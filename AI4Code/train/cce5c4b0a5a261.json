{"cell_type":{"54b0a089":"code","37322bcb":"code","11cd4024":"code","c8f5799e":"code","213e9cc5":"code","1d6ffb37":"code","339d2f76":"code","f818204a":"code","8a512a38":"code","a3bb4d04":"code","be495757":"code","bd5107a8":"code","60b1f804":"code","dddce725":"code","a025ab51":"code","e61f00e9":"code","526bb5ff":"code","bf10fc4c":"code","354fa41b":"code","012b882e":"code","9de86476":"code","37df6430":"code","17306b9a":"code","5fba04c5":"code","405af81c":"code","753da178":"code","52d03e3c":"code","704f9950":"code","b315973d":"markdown","54ebec1d":"markdown","8988a5fb":"markdown","c87d1452":"markdown","092dde3f":"markdown","250e0ddb":"markdown","1b047b1f":"markdown","e699e671":"markdown","bee61c33":"markdown","64d3ef58":"markdown","52fd55aa":"markdown","4f47ff62":"markdown","1e99fc5d":"markdown","15b44f3b":"markdown","f5128418":"markdown","20925778":"markdown","4fdecfd7":"markdown","315baf17":"markdown","58ad4762":"markdown"},"source":{"54b0a089":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37322bcb":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets\nmnist = tensorflow_datasets.load('mnist')\ndef sgd(cost, params, lr=np.float32(0.01)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        updates.append(param.assign(param - lr*g_param))\n    return updates\n\n\ndef sgd_clip(cost, params, lr=np.float32(0.01), thld=np.float32(1.0)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        g_param = tf.where(tf.greater(tf.abs(g_param), thld), thld\/tf.norm(g_param, ord=1)*g_param, g_param)\n        updates.append(param.assign(param - lr*g_param))\n    return updates\n\n\ndef momentum(cost, params, lr=np.float32(0.01), gamma=np.float32(0.9)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        v = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='v')\n        updates.append(v.assign(gamma*v - lr*g_param))\n        with tf.control_dependencies(updates):\n            updates.append(param.assign(param + v))\n    return updates\n\n\ndef nesterov_momentum(cost, params, lr=np.float32(0.01), gamma=np.float32(0.9)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        v = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='v')\n        updates.append(v.assign(gamma*v - lr*g_param))\n        with tf.control_dependencies(updates):\n            updates.append(param.assign(param + (gamma**2)*v - (1 + gamma)*lr*g_param))\n    return updates\n\n\ndef adagrad(cost, params, lr=np.float32(0.01), eps=np.float32(1e-8)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        g2_sum = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='g2_sum')\n        updates.append(g2_sum.assign(g2_sum + g_param**2))\n        with tf.control_dependencies(updates):\n            updates.append(param.assign(param - lr\/tf.sqrt(g2_sum + eps)*g_param))\n    return updates\n\n\ndef adadelta(cost, params, gamma=np.float32(0.95), eps=np.float32(1e-8)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        ms_g = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='ms_g')\n        ms_d = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='ms_d')\n        d_param = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='d_param')\n        updates.append(ms_g.assign(gamma*ms_g + (1. - gamma)*g_param**2))\n        with tf.control_dependencies(updates):\n            updates.append(d_param.assign(-tf.sqrt((ms_d + eps)\/(ms_g + eps))*g_param))\n        with tf.control_dependencies(updates):\n            updates.append(param.assign(param + d_param))\n        with tf.control_dependencies(updates):\n            updates.append(ms_d.assign(gamma*ms_d + (1. - gamma)*d_param**2))\n    return updates\n\n\ndef rmsprop(cost, params, lr=np.float32(0.001), gamma=np.float32(0.9), eps=np.float32(1e-8)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        ms_g = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='ms_g')\n        updates.append(ms_g.assign(gamma*ms_g + (1. - gamma)*g_param**2))\n        with tf.control_dependencies(updates):\n            updates.append(param.assign(param - lr\/tf.sqrt(ms_g + eps)*g_param))\n    return updates\n\n\ndef adam(cost, params, alpha=np.float32(0.001), beta_1=np.float32(0.9), beta_2=np.float32(0.999), eps=np.float32(1e-8)):\n    g_params = tf.gradients(cost, params)\n    t = tf.Variable(0.0, dtype=tf.float32, name='t')\n    updates = []\n    updates.append(t.assign(t + 1))\n    with tf.control_dependencies(updates):\n        for param, g_param in zip(params, g_params):\n            m = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='m')\n            v = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='v')\n            alpha_t = alpha*tf.sqrt(1. - beta_2**t)\/(1. - beta_1**t)\n            updates.append(m.assign(beta_1*m + (1. - beta_1)*g_param))\n            with tf.control_dependencies(updates):\n                updates.append(v.assign(beta_2*v + (1. - beta_2)*g_param**2))\n            with tf.control_dependencies(updates):\n                updates.append(param.assign(param - alpha_t*m\/(tf.sqrt(v) + eps)))\n    return updates\n\n\ndef adamax(cost, params, alpha=np.float32(0.002), beta_1=np.float32(0.9), beta_2=np.float32(0.999), eps=np.float32(1e-8)):\n    g_params = tf.gradients(cost, params)\n    t = tf.Variable(1.0, dtype=tf.float32, name='t')\n    updates = []\n    updates.append(t.assign(t + 1))\n    with tf.control_dependencies(updates):\n        for param, g_param in zip(params, g_params):\n            m = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='m')\n            u = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='u')\n            updates.append(m.assign(beta_1*m + (1. - beta_1)*g_param))\n            with tf.control_dependencies(updates):\n                updates.append(u.assign(tf.maximum(beta_2*u, tf.abs(g_param))))\n            with tf.control_dependencies(updates):\n                updates.append(param.assign(param - (alpha\/(1. - beta_1**t))*(m\/(u + eps))))\n    return updates\n\n\ndef smorms3(cost, params, lr=np.float32(0.001), eps=np.float32(1e-16)):\n    g_params = tf.gradients(cost, params)\n    updates = []\n    for param, g_param in zip(params, g_params):\n        m = tf.Variable(np.ones(param.get_shape(), dtype='float32'), name='m')\n        g = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='g')\n        g2 = tf.Variable(np.zeros(param.get_shape(), dtype='float32'), name='g2')\n        r = 1.\/(m + 1.)\n        updates.append(g.assign((1. - r)*g + r*g_param))\n        updates.append(g2.assign((1. - r)*g2 + r*g_param**2))\n        with tf.control_dependencies(updates):\n            x = g**2\/(g2 + eps)\n            updates.append(param.assign(param - tf.minimum(lr, x)\/tf.sqrt(g2 + eps)*g_param))\n            updates.append(m.assign(1. + (1. - x)*m))\n    return updates","11cd4024":"class Dense:\n    def __init__(self, in_dim, out_dim, function=lambda x: x):\n        # Xavier initialization\n        rng = np.random.RandomState(1234)\n        self.W = tf.Variable(rng.uniform(\n            low=-np.sqrt(6\/(in_dim + out_dim)),\n            high=np.sqrt(6\/(in_dim + out_dim)),\n            size=(in_dim, out_dim)\n        ).astype('float32'), name='W')\n        self.b = tf.Variable(np.zeros([out_dim]).astype('float32'), name='b')\n        self.function = function\n        self.params = [self.W, self.b]\n\n    def f_prop(self, x):\n        return self.function(tf.matmul(x, self.W) + self.b)\n\ndef tf_log(x):\n    return tf.log(tf.clip_by_value(x, 1e-10, 1.0))\n\ndef f_props(layers, x):\n    params = []\n    for layer in layers:\n        x = layer.f_prop(x)\n        params += layer.params\n    return x, params\n\n","c8f5799e":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport tensorflow.compat.v1 as tf\nimport numpy as np\ntf.disable_v2_behavior()\n\nrandom_state = 42\ndef load_mnist(one_hot=True):\n    (X_train, y_train), (X_test, y_test)=tf.keras.datasets.mnist.load_data()\n    X_train = X_train\/255 \n    X_test = X_test\/255\n    y_train = tf.keras.utils.to_categorical(y_train, 10)\n    y_test = tf.keras.utils.to_categorical(y_test, 10)\n    X_train=X_train.reshape(X_train.shape[0],784)\n    return X_train,y_train    \n\n\nmnist_X, mnist_y = load_mnist()\ntrain_X, valid_X, train_y, valid_y = train_test_split(mnist_X, mnist_y, test_size=0.1, random_state=random_state)\n\nx = tf.placeholder(tf.float32, [None, 784])\nt = tf.placeholder(tf.float32, [None, 10])\n\nlayers = [\n    Dense(784, 256, tf.nn.relu),\n    Dense(256, 256, tf.nn.relu),\n    Dense(256, 10, tf.nn.softmax)\n]\n\ny, params = f_props(layers, x)\n\ncost = -tf.reduce_mean(tf.reduce_sum(t*tf_log(y), axis=1))\n\n# Choose an optimizer from sgd, sgd_clip, momentum, nesterov_momentum, adagrad, adadelta, rmsprop, adam, adamax, smorms3\nupdates = adam(cost, params)\n\ntrain = tf.group(*updates)\ntest  = tf.argmax(y, axis=1)\n\nn_epochs = 10\nbatch_size = 100\nn_batches = train_X.shape[0]\/\/batch_size\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(n_epochs):\n        train_X, train_y = shuffle(train_X, train_y, random_state=random_state)\n        for i in range(n_batches):\n            start = i*batch_size\n            end = start + batch_size\n            sess.run(train, feed_dict={x: train_X[start:end], t: train_y[start:end]})\n        pred_y, valid_cost = sess.run([test, cost], feed_dict={x: valid_X, t: valid_y})\n        print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' % (epoch + 1, valid_cost, f1_score(np.argmax(valid_y, 1).astype('int32'), pred_y, average='macro')))\n","213e9cc5":"import tensorflow as tf\nimport mnist\n\nclass Convolution():\n    def __init__(self, num_filters):\n        self.num_filters = num_filters\n        self.fan=9\n        self.filters = np.random.randn(num_filters, 3, 3)\/self.fan\n    \n    def iterate_regions(self, image):\n        #generates all possible 3*3 image regions using valid padding\n        h,w = image.shape\n        for i in range(h-2):\n            for j in range(w-2):\n                im_region = image[i:(i+3), j:(j+3)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        self.last_input = input\n        h,w = input.shape\n        output = np.zeros((h-2, w-2, self.num_filters))\n        for im_regions, i, j in self.iterate_regions(input):\n            output[i, j] = np.sum(im_regions * self.filters, axis=(1,2))\n        return output\n    \n    def backprop(self, deriv_out, learn_rate):\n        derivative_filters = np.zeros(self.filters.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            for f in range(self.num_filters):\n                derivative_filters[f] += deriv_out[i,j,f] * im_region\n        self.filters -= learn_rate * derivative_filters\n        return self.filters\n    \nclass MaxPool():\n    def iterate_regions(self, image):\n        h, w, _ = image.shape\n        new_h = h \/\/ 2\n        new_w = w \/\/ 2\n        for i in range(new_h):\n            for j in range(new_w):\n                im_region = image[(i*2):(i*2+2), (j*2):(j*2+2)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        self.last_input = input\n        h, w, num_filters = input.shape\n        output = np.zeros((h\/\/2, w\/\/2, num_filters))\n        for im_region, i, j in self.iterate_regions(input):\n            output[i,j] = np.amax(im_region,axis=(0,1))\n            \n        return output\n    \n    def backprop(self, deriv_out):\n        deriv_input = np.zeros(self.last_input.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0,1))\n\n            for i2 in range(h):\n                for j2 in range(w):\n                    for f2 in range(f):\n                        #if the pixel was the max value, copy the gradient to it\n                        if(im_region[i2,j2,f2] == amax[f2]):\n                            deriv_input[i*2+i2, j*2+j2 ,f2] = deriv_out[i, j, f2]\n                            break;\n        return deriv_input\n    \nclass Softmax():\n    def __init__(self, input_len, nodes):\n        self.weights = np.random.randn(input_len, nodes)\/input_len\n        self.biases = np.zeros(nodes)\n    \n    def forward(self, input):\n        \n        self.last_input_shape = input.shape\n        input = input.flatten()\n        self.last_input = input\n        input_len, nodes = self.weights.shape\n        totals = np.dot(input, self.weights) + self.biases\n        self.last_totals = totals\n        exp = np.exp(totals)\n        return(exp\/np.sum(exp, axis=0)) \n    \n    def backprop(self, d_l_d_out, learn_rate):\n        for i, gradient in enumerate(d_l_d_out):\n            if(gradient == 0):\n                continue\n            \n            t_exp = np.exp(self.last_totals)\n            S = np.sum(t_exp)\n            #gradients of out[i] against totals\n            d_out_d_t = -t_exp[i] * t_exp\/ (S**2)\n            d_out_d_t[i] = t_exp[i] * (S-t_exp[i]) \/(S**2)\n            # Gradients of totals against weights\/biases\/input\n            d_t_d_w = self.last_input\n            d_t_d_b = 1\n            d_t_d_inputs = self.weights\n            #Gradients of loss against totals\n            d_l_d_t = gradient * d_out_d_t\n            #Gradients of loss against weights\/biases\/input\n            d_l_d_w = d_t_d_w[np.newaxis].T @ d_l_d_t[np.newaxis]\n            d_l_d_b = d_l_d_t * d_t_d_b  \n            d_l_d_inputs = d_t_d_inputs @ d_l_d_t\n            #update weights\/biases\n            self.weights -= learn_rate * d_l_d_w\n            self.biases -= learn_rate * d_l_d_b\n            return d_l_d_inputs.reshape(self.last_input_shape)\n","1d6ffb37":"conv = Convolution(8)\npool = MaxPool()\nsoftmax = Softmax(13 * 13 * 8, 10)\nepochs=10\ndef forward(image, label):\n    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n    # to work with. This is standard practice.\n    out = conv.forward((image\/255) - 0.5)\n    out = pool.forward(out)\n    out = softmax.forward(out)\n    #calculate cross-entropy loss and accuracy\n    loss = -np.log(out[label])\n    acc = 1 if(np.argmax(out) == label) else 0\n    return out, loss, acc\n\n\ndef train(im, label, lr=0.005):\n    #forward\n    out,loss,acc = forward(im, label)\n    #calculate initial gradient\n    gradient = np.zeros(10)\n    gradient[label] = -1\/out[label]\n    #Backprop\n    gradient = softmax.backprop(gradient, lr)\n    gradient = pool.backprop(gradient)\n    gradient = conv.backprop(gradient, lr)\n    return loss, acc\n    \n\nif __name__=='__main__':\n    train_images = mnist.train_images()[:1000]\n    train_labels = mnist.train_labels()[:1000]\n    test_images = mnist.test_images()[:1000]\n    test_labels = mnist.test_labels()[:1000]\n    for epoch in range(epochs):\n        print('----EPOCH %d ---'%(epoch+1))\n        #shuffle the training data\n        permutation = np.random.permutation(len(train_images))\n        train_images = train_images[permutation]\n        train_labels = train_labels[permutation]\n        loss = 0\n        num_correct = 0\n        for i, (im, label) in enumerate(zip(train_images, train_labels)):\n            if(i>0 and i %100 == 99):\n                print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss \/ 100, num_correct))\n                loss = 0\n                num_correct = 0\n            l, acc = train(im, label)\n            loss += l\n            num_correct += acc\n\n","339d2f76":"#Basic convolution with tf on mnist\nimport tensorflow as tf\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n#hyperparameters\nnum_classes = 10 # total classes (0-9 digits).\n# Training parameters.\nlearning_rate = 1e-3\ntraining_steps = 20\nbatch_size = 256\ndisplay_step = 10\n# Network parameters.\nconv1_filters = 32 # number of filters for 1st conv layer.\nconv2_filters = 64 # number of filters for 2nd conv layer.\nfeedforward_units = 1024 # number of neurons for 1st fully-connected layer.\n\ndef loader():\n    (x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data()\n    # Normalize images value from [0, 255] to [0, 1].\n    x_train=np.array(x_train\/255.,np.float32)\n    x_test=np.array(x_test\/255.,np.float32)\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n    return train_data,x_train,y_train,x_test,y_test\n\nclass Convolution(tf.keras.Model):\n    def __init__(self):\n        super(Convolution,self).__init__()\n        # Convolution Layer with 32 filters and a kernel size of 5.\n        self.conv1 = tf.keras.layers.Conv2D(conv1_filters, kernel_size=5, activation=tf.nn.relu)\n        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2. \n        self.maxpool1 = tf.keras.layers.MaxPool2D(2, strides=2)\n        # Convolution Layer with 64 filters and a kernel size of 3.\n        self.conv2 = tf.keras.layers.Conv2D(conv2_filters, kernel_size=3, activation=tf.nn.relu)\n        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2. \n        self.maxpool2 = tf.keras.layers.MaxPool2D(2, strides=2)\n        # Flatten the data to a 1-D vector for the fully connected layer.\n        self.flatten = tf.keras.layers.Flatten()\n\n        # Fully connected layer.\n        self.feedforward = tf.keras.layers.Dense(feedforward_units)\n        # Apply Dropout (if is_training is False, dropout is not applied).\n        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n        # Output layer, class prediction.\n        self.out = tf.keras.layers.Dense(num_classes)\n        \n    def call(self, x, is_training=False):\n        x = tf.reshape(x, [-1, 28, 28, 1])\n        x = self.conv1(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.maxpool2(x)\n        x = self.flatten(x)\n        x = self.feedforward(x)\n        x = self.dropout(x, training=is_training)\n        x = self.out(x)\n        if not is_training:\n            # tf cross entropy expect logits without softmax(sum-ylog(y)), so only\n            # apply softmax when not training.\n            x = tf.nn.softmax(x)\n        return x\n    \ndef cross_entropy_loss(x, y):\n    y = tf.cast(y, tf.int64)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n    return tf.reduce_mean(loss)\n\ndef accuracy(y_pred, y_true):\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n\ndef run_optimization(x, y,conv_net):\n    #Computation Graph with gradient tape\n    with tf.GradientTape() as g:\n        # Forward pass.\n        pred = conv_net(x, is_training=True)\n        # Compute loss.\n        loss = cross_entropy_loss(pred, y)\n    # Variables to update, i.e. trainable variables.\n    trainable_variables = conv_net.trainable_variables\n    # Compute gradients.->jacobian\n    gradients = g.gradient(loss, trainable_variables)\n    # Update W and b following gradients.\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n    \ndef plotter(n_images,x_test,conv_net):\n    test_images = x_test[:n_images]\n    predictions = conv_net(test_images)\n    for i in range(n_images):\n        plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n        plt.show()\n        print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))\n\n    \nif __name__=='__main__':\n    \n    optimizer = tf.optimizers.Adam(learning_rate)\n    model=Convolution()\n    train_data,x_train,y_train,x_test,y_test=loader()\n    print(\"=====================Optimization=========================\")\n\n\n    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n        # Run the optimization to update W and b values.\n        run_optimization(batch_x, batch_y,model)\n        if step % display_step == 0:\n            pred = model(batch_x)\n            loss = cross_entropy_loss(pred, batch_y)\n            acc = accuracy(pred, batch_y)\n            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n    \n    print(model.summary())\n    tf.keras.utils.plot_model(\n        model,\n        to_file=\"Basic_Conv2d_model.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )\n    \n    \n    print(\"=====================Accuracy=========================\")\n\n    pred = model(x_test)\n    print(\"Test Accuracy: %f\" % accuracy(pred, y_test)) \n    \n    print(\"=====================Plotting=========================\")\n\n    n_images=10\n    plotter(n_images,x_test,model)\n    \n    \n","f818204a":"import os\nimport shutil\nimport glob\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf \nimport cv2\n\nNUM_CLASSES = 2\n\n#helper functions for input preprocessing\n#we are using pneumonia patients x ray scan images for cnns\ndef visualize():\n    img_normal = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/NORMAL\/IM-0131-0001.jpeg')\n    img_penumonia_bacteria = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA\/person1017_bacteria_2948.jpeg')\n    img_penumonia_virus = plt.imread('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA\/person1021_virus_1711.jpeg')\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1,3,1).set_title('NORMAL')\n    plt.imshow(img_normal, cmap='gray')\n\n    plt.subplot(1,3,2).set_title('PNEUMONIA')\n    plt.imshow(img_penumonia_bacteria, cmap='gray')\n\n    plt.subplot(1,3,3).set_title('PNEUMONIA')\n    plt.imshow(img_penumonia_virus, cmap='gray')\n    plt.tight_layout()\n\n#utility function to convert the images to np arrays and label them\n\nlabels = ['PNEUMONIA', 'NORMAL']\nimg_size = 150\ndef converter(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)\n    y = np.asarray(y_train)\n    return x, y\n\ndef add_(train,test,val):\n    #reshapes,modifies and appends in numpy arrays\n    x_train = []\n    y_train = []\n    x_val = []\n    y_val = []\n    x_test = []\n    y_test = []\n\n    for feature, label in train:\n        x_train.append(feature)\n        y_train.append(label)\n\n    for feature, label in test:\n        x_test.append(feature)\n        y_test.append(label)\n\n    for feature, label in val:\n        x_val.append(feature)\n        y_val.append(label)\n    x_train = np.array(x_train) \/ 255\n    x_val = np.array(x_val) \/ 255\n    x_test = np.array(x_test) \/ 255\n    x_train = x_train.reshape(-1, img_size, img_size, 1)\n    y_train = np.array(y_train)\n\n    x_val = x_val.reshape(-1, img_size, img_size, 1)\n    y_val = np.array(y_val)\n\n    x_test = x_test.reshape(-1, img_size, img_size, 1)\n    y_test = np.array(y_test)\n    return x_train,y_train,x_val,y_val,x_test,y_test\n\nvisualize()\ntrain = converter('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = converter('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = converter('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')\nx_train,y_train,x_val,y_val,x_test,y_test=add_(train,test,val)\nprint(x_train.shape,y_train.shape,x_val.shape,y_val.shape)","8a512a38":"#Variations of Convolutions\n\nclass VGG():\n    \n    def model(self):\n        _input = tf.keras.layers.Input((150,150,1)) \n        conv1  = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n        conv2  = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n        pool1  = tf.keras.layers.MaxPooling2D((2, 2))(conv2)\n\n        conv3  = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n        conv4  = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n        pool2  = tf.keras.layers.MaxPooling2D((2, 2))(conv4)\n\n        conv5  = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n        conv6  = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n        conv7  = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n        pool3  = tf.keras.layers.MaxPooling2D((2, 2))(conv7)\n\n        conv8  = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n        conv9  = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n        conv10 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n        pool4  = tf.keras.layers.MaxPooling2D((2, 2))(conv10)\n\n        conv11 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n        conv12 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n        conv13 = tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n        pool5  = tf.keras.layers.MaxPooling2D((2, 2))(conv13)\n\n        flat   = tf.keras.layers.Flatten()(pool5)\n        dense1 = tf.keras.layers.Dense(4096, activation=\"relu\")(flat)\n        dense2 = tf.keras.layers.Dense(1024, activation=\"relu\")(dense1)\n        output = tf.keras.layers.Dense(1000, activation=\"softmax\")(dense2)\n        output = tf.keras.layers.Dropout(0.5, name='dropout2')(output)\n        output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n\n        vgg16_model  = tf.keras.Model(inputs=_input, outputs=output)\n        \n        return vgg16_model\n\nvgg=VGG()\nmodel=vgg.model()\nprint(model.summary())\ntf.keras.utils.plot_model(\n        model,\n        to_file=\"vgg16.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )   \n","a3bb4d04":"model.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)\nhistory = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])","be495757":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, ZeroPadding2D,\\\n     Flatten, BatchNormalization, AveragePooling2D, Dense, Activation, Add \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import activations\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\ndef res_identity(x, filters): \n  #resnet block where dimension doesnot change.\n  #The skip connection is just simple identity conncection\n  #we will have 3 blocks and then input will be added\n\n    x_skip = x # this will be used for addition with the residual block \n    f1, f2 = filters\n\n    #first block \n    x = tf.keras.layers.Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n\n    #second block # bottleneck (but size kept same with padding)\n    x = tf.keras.layers.Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n\n    # third block activation used after adding the input\n    x = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    # x = Activation(activations.relu)(x)\n\n    # add the input \n    x = tf.keras.layers.Add()([x, x_skip])\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n    return x\n\ndef res_conv(x, s, filters):\n    '''here the input size changes''' \n    x_skip = x\n    f1, f2 = filters\n\n    # first block\n    x = tf.keras.layers.Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n    # when s = 2 then it is like downsizing the feature map\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n\n    # second block\n    x = tf.keras.layers.Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n\n    #third block\n    x = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    # shortcut \n    x_skip = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n    x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n\n    # add \n    x = tf.keras.layers.Add()([x, x_skip])\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n\n    return x\n\ndef resnet50():\n\n    input_im = tf.keras.layers.Input(shape=(150,150,1)) \n    x = tf.keras.layers.ZeroPadding2D(padding=(3, 3))(input_im)\n    # 1st stage\n    # here we perform maxpooling, see the figure above\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    #2nd stage \n    # frm here on only conv block and identity block, no pooling\n\n    x = res_conv(x, s=1, filters=(64, 256))\n    x = res_identity(x, filters=(64, 256))\n    x = res_identity(x, filters=(64, 256))\n\n    # 3rd stage\n\n    x = res_conv(x, s=2, filters=(128, 512))\n    x = res_identity(x, filters=(128, 512))\n    x = res_identity(x, filters=(128, 512))\n    x = res_identity(x, filters=(128, 512))\n\n    # 4th stage\n\n    x = res_conv(x, s=2, filters=(256, 1024))\n    x = res_identity(x, filters=(256, 1024))\n    x = res_identity(x, filters=(256, 1024))\n    x = res_identity(x, filters=(256, 1024))\n    x = res_identity(x, filters=(256, 1024))\n    x = res_identity(x, filters=(256, 1024))\n\n    # 5th stage\n\n    x = res_conv(x, s=2, filters=(512, 2048))\n    x = res_identity(x, filters=(512, 2048))\n    x = res_identity(x, filters=(512, 2048))\n\n    # ends with average pooling and dense connection\n\n    x = tf.keras.layers.AveragePooling2D((2, 2), padding='same')(x)\n\n    x = tf.keras.layers.Flatten()(x)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    #binary-class\n    resnet_model  = tf.keras.Model(inputs=input_im, outputs=output)\n    return resnet_model\n\nresnet=resnet50()\nprint(resnet.summary())\ntf.keras.utils.plot_model(\n        model,\n        to_file=\"Resnet_50.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )\n\n","bd5107a8":"model=resnet\nmodel.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)\nhistory = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])","60b1f804":"from tensorflow import keras\ndef AlexNet():\n    model = keras.models.Sequential([\n    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(150,150,1)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(4096, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(4096, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])\n    return model\nalexnet=AlexNet()\nhistory = alexnet.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])\ntf.keras.utils.plot_model(\n        model,\n        to_file=\"AlexNet.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )\n\n    ","dddce725":"from tensorflow import keras\n\ndef inception_module(x,\n                     filters_1x1,\n                     filters_3x3_reduce,\n                     filters_3x3,\n                     filters_5x5_reduce,\n                     filters_5x5,\n                     filters_pool_proj,\n                     name=None):\n    kernel_init=keras.initializers.GlorotUniform()\n    bias_init=keras.initializers.Zeros()\n    conv_1x1 = keras.layers.Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n    conv_3x3 = keras.layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n    conv_3x3 = keras.layers.Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n\n    conv_5x5 = keras.layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n    conv_5x5 = keras.layers.Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n\n    pool_proj = keras.layers.MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n    pool_proj = keras.layers.Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n\n    output = keras.layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n    return output\n\n\n\ndef Inception():\n    kernel_init=keras.initializers.GlorotUniform()\n    bias_init=keras.initializers.Zeros()\n    \n    input_layer = keras.layers.Input(shape=(150, 150, 1))\n    x = keras.layers.Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7\/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n    x = keras.layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3\/2')(x)\n    x = keras.layers.Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3\/1')(x)\n    x = keras.layers.Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3\/1')(x)\n    x = keras.layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3\/2')(x)\n    x = inception_module(x,\n                         filters_1x1=64,\n                         filters_3x3_reduce=96,\n                         filters_3x3=128,\n                         filters_5x5_reduce=16,\n                         filters_5x5=32,\n                         filters_pool_proj=32,\n                         name='inception_3a')\n\n    x = inception_module(x,\n                         filters_1x1=128,\n                         filters_3x3_reduce=128,\n                         filters_3x3=192,\n                         filters_5x5_reduce=32,\n                         filters_5x5=96,\n                         filters_pool_proj=64,\n                         name='inception_3b')\n\n    x = keras.layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3\/2')(x)\n\n    x = inception_module(x,\n                         filters_1x1=192,\n                         filters_3x3_reduce=96,\n                         filters_3x3=208,\n                         filters_5x5_reduce=16,\n                         filters_5x5=48,\n                         filters_pool_proj=64,\n                         name='inception_4a')\n\n\n    x1 = keras.layers.AveragePooling2D((5, 5), strides=3)(x)\n    x1 = keras.layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n    x1 = keras.layers.Flatten()(x1)\n    x1 = keras.layers.Dense(1024, activation='relu')(x1)\n    x1 = keras.layers.Dropout(0.7)(x1)\n    x1 = keras.layers.Dense(10, activation='softmax', name='auxilliary_output_1')(x1)\n\n    x = inception_module(x,\n                         filters_1x1=160,\n                         filters_3x3_reduce=112,\n                         filters_3x3=224,\n                         filters_5x5_reduce=24,\n                         filters_5x5=64,\n                         filters_pool_proj=64,\n                         name='inception_4b')\n\n    x = inception_module(x,\n                         filters_1x1=128,\n                         filters_3x3_reduce=128,\n                         filters_3x3=256,\n                         filters_5x5_reduce=24,\n                         filters_5x5=64,\n                         filters_pool_proj=64,\n                         name='inception_4c')\n\n    x = inception_module(x,\n                         filters_1x1=112,\n                         filters_3x3_reduce=144,\n                         filters_3x3=288,\n                         filters_5x5_reduce=32,\n                         filters_5x5=64,\n                         filters_pool_proj=64,\n                         name='inception_4d')\n\n\n    x2 = keras.layers.AveragePooling2D((5, 5), strides=3)(x)\n    x2 = keras.layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n    x2 = keras.layers.Flatten()(x2)\n    x2 = keras.layers.Dense(1024, activation='relu')(x2)\n    x2 = keras.layers.Dropout(0.7)(x2)\n    x2 = keras.layers.Dense(10, activation='softmax', name='auxilliary_output_2')(x2)\n\n    x = inception_module(x,\n                         filters_1x1=256,\n                         filters_3x3_reduce=160,\n                         filters_3x3=320,\n                         filters_5x5_reduce=32,\n                         filters_5x5=128,\n                         filters_pool_proj=128,\n                         name='inception_4e')\n\n    x = keras.layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3\/2')(x)\n\n    x = inception_module(x,\n                         filters_1x1=256,\n                         filters_3x3_reduce=160,\n                         filters_3x3=320,\n                         filters_5x5_reduce=32,\n                         filters_5x5=128,\n                         filters_pool_proj=128,\n                         name='inception_5a')\n\n    x = inception_module(x,\n                         filters_1x1=384,\n                         filters_3x3_reduce=192,\n                         filters_3x3=384,\n                         filters_5x5_reduce=48,\n                         filters_5x5=128,\n                         filters_pool_proj=128,\n                         name='inception_5b')\n\n    x = keras.layers.GlobalAveragePooling2D(name='avg_pool_5_3x3\/1')(x)\n    x = keras.layers.Dropout(0.4)(x)\n    x = keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n    model=keras.Model(inputs=input_layer, outputs=x)\n    return model\n\ninception=Inception()\ninception.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nlearning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)\nhistory = inception.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])\nprint(inception.summary())\nkeras.utils.plot_model(\n        inception,\n        to_file=\"Inception.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )\n","a025ab51":"### Demo Toy implementation ---\nimport tensorflow as tf\nimport tensorflow.keras\n\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Model, model_from_json, Sequential\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, SeparableConv2D, UpSampling2D, BatchNormalization, Input, GlobalAveragePooling2D\n\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import SGD, RMSprop\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\n\ndef entry_flow(inputs) :\n\n    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(64,3,padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    previous_block_activation = x\n\n    for size in [128, 256, 728] :\n\n        x = Activation('relu')(x)\n        x = SeparableConv2D(size, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n\n        x = Activation('relu')(x)\n        x = SeparableConv2D(size, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n\n        x = MaxPooling2D(3, strides=2, padding='same')(x)\n\n        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n\n        x = tensorflow.keras.layers.Add()([x, residual])\n        previous_block_activation = x\n\n    return x\n\ndef middle_flow(x, num_blocks=8) :\n\n    previous_block_activation = x\n\n    for _ in range(num_blocks) :\n\n        x = Activation('relu')(x)\n        x = SeparableConv2D(728, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n\n        x = Activation('relu')(x)\n        x = SeparableConv2D(728, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n\n        x = Activation('relu')(x)\n        x = SeparableConv2D(728, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n\n        x = tensorflow.keras.layers.Add()([x, previous_block_activation])\n        previous_block_activation = x\n\n    return x\n\ndef exit_flow(x) :\n\n    previous_block_activation = x\n\n    x = Activation('relu')(x)\n    x = SeparableConv2D(728, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = Activation('relu')(x)\n    x = SeparableConv2D(1024, 3, padding='same')(x) \n    x = BatchNormalization()(x)\n\n    x = MaxPooling2D(3, strides=2, padding='same')(x)\n\n    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n    x = tensorflow.keras.layers.Add()([x, residual])\n\n    x = Activation('relu')(x)\n    x = SeparableConv2D(728, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = Activation('relu')(x)\n    x = SeparableConv2D(1024, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1, activation='linear')(x)\n\n    return x\nshape_x=150\nshape_y=150\ninputs = Input(shape=(shape_x, shape_y, 1))\noutputs = exit_flow(middle_flow(entry_flow(inputs)))\nxception = Model(inputs, outputs)\nxception.summary()\nkeras.utils.plot_model(\n        xception,\n        to_file=\"Xception.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )","e61f00e9":"xception.compile(optimizer = \"adam\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nlearning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)\nhistory = xception.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])\nprint(xception.summary())\nkeras.utils.plot_model(\n        xception,\n        to_file=\"Xception.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96,\n    )\n","526bb5ff":"ComputeLB = False\n\nimport os, gc, zipfile\nimport numpy as np, pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nif ComputeLB: PATH = '..\/input\/cats-faces-64x64-for-generative-models\/cats\/cats\/'\nelse: PATH = '..\/input\/cats-faces-64x64-for-generative-models\/cats\/cats\/'\nIMAGES = os.listdir(PATH)\nprint('There are',len(IMAGES),'images. Here are 5 example filesnames:')\nprint(IMAGES[:5])\n\nos.mkdir('..\/tmp')\nos.mkdir('..\/tmp\/images')\n\nfor i in range(500000):\n    img = Image.open(PATH + IMAGES[i%len(IMAGES)])\n    img = img.resize(( 100,int(img.size[1]\/(img.size[0]\/100) )), Image.ANTIALIAS)\n    w = img.size[0]; h = img.size[1]; a=0; b=0\n    if w>64: a = np.random.randint(0,w-64)\n    if h>64: b = np.random.randint(0,h-64)\n    img = img.crop((a, b, 64+a, 64+b))\n    img.save('..\/tmp\/images\/'+str(i)+'.png','PNG')\n    if i%100000==0: print('created',i,'cropped images')\nprint('created 500000 cropped images')","bf10fc4c":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nBATCH_SIZE = 256; EPOCHS = 10\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_batches = train_datagen.flow_from_directory('..\/tmp\/',\n                                                  target_size=(64,64), shuffle=True, class_mode='input', batch_size=BATCH_SIZE)\n\n","354fa41b":"# ENCODER\ninput_img = Input(shape=(64, 64, 3))  \nx = Conv2D(48, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(96, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(192, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nencoded = Conv2D(32, (1, 1), activation='relu', padding='same')(x)\n\n# LATENT SPACE\nlatentSize = (8,8,32)\n\n# DECODER\ndirect_input = Input(shape=latentSize)\nx = Conv2D(192, (1, 1), activation='relu', padding='same')(direct_input)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(192, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(96, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(48, (3, 3), activation='relu', padding='same')(x)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n# COMPILE\nencoder = Model(input_img, encoded)\ndecoder = Model(direct_input, decoded)\nautoencoder = Model(input_img, decoder(encoded))\n\nautoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\nprint(autoencoder.summary())\nhistory = autoencoder.fit_generator(train_batches,\n        steps_per_epoch = train_batches.samples \/\/ BATCH_SIZE,\n        epochs = EPOCHS, verbose=2)","012b882e":"images = next(iter(train_batches))[0]\n\nfor i in range(5):\n\n    plt.figure(figsize=(15,5))\n    plt.subplot(1,3,1)\n    \n    # ORIGINAL IMAGE\n    orig = images[i,:,:,:].reshape((-1,64,64,3))\n    img = Image.fromarray( (255*orig).astype('uint8').reshape((64,64,3)))\n    plt.title('Original')\n    plt.imshow(img)\n\n    # LATENT IMAGE\n    latent_img = encoder.predict(orig)\n    mx = np.max( latent_img[0] )\n    mn = np.min( latent_img[0] )\n    latent_flat = ((latent_img[0] - mn) * 255\/(mx - mn)).flatten(order='F')\n    img = Image.fromarray( latent_flat[:2025].astype('uint8').reshape((45,45)), mode='L') \n    plt.subplot(1,3,2)\n    plt.title('Latent')\n    plt.xlim((-10,55))\n    plt.ylim((-10,55))\n    plt.axis('off')\n    plt.imshow(img)\n\n    # RECONSTRUCTED IMAGE\n    decoded_imgs = decoder.predict(latent_img[0].reshape((-1,latentSize[0],latentSize[1],latentSize[2])))\n    img = Image.fromarray( (255*decoded_imgs[0]).astype('uint8').reshape((64,64,3)))\n    plt.subplot(1,3,3)\n    plt.title('Reconstructed')\n    plt.imshow(img)\n    \n    plt.show()\n","9de86476":"### RNN\n\n# load text data\n\ntxt_data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz \" # input data\n# txt_data = open('input.txt', 'r').read() # test external files\nchars = list(set(txt_data)) # split and remove duplicate characters. convert to list.\nnum_chars = len(chars) # the number of unique characters\ntxt_data_size = len(txt_data)\nprint(\"unique characters : \", num_chars) # You can see the number of unique characters in your input data.\nprint(\"txt_data_size : \", txt_data_size)\n\n# one hot encode\nchar_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\nint_to_char = dict((i, c) for i, c in enumerate(chars))\nprint(char_to_int)\nprint(\"----------------------------------------------------\")\nprint(int_to_char)\nprint(\"----------------------------------------------------\")\n# integer encode input data\ninteger_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\nprint(integer_encoded)\nprint(\"----------------------------------------------------\")\nprint(\"data length : \", len(integer_encoded))\n\n\n# hyperparameters\n\niteration = 5000\nsequence_length = 10\nbatch_size = round((txt_data_size \/sequence_length)+0.5) # = math.ceil\nhidden_size = 100  # size of hidden layer of neurons.  \nlearning_rate = 1e-1\n\n\n# model parameters\n\nW_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \nW_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\nW_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n\nb_h = np.zeros((hidden_size, 1)) # hidden bias\nb_y = np.zeros((num_chars, 1)) # output bias\n\nh_prev = np.zeros((hidden_size,1)) # h_(t-1)\n\n\n\ndef forwardprop(inputs, targets, h_prev):\n        \n    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n    loss = 0 # loss initialization\n    \n    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n        \n        xs[t] = np.zeros((num_chars,1)) \n        xs[t][inputs[t]] = 1\n        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n        ps[t] = np.exp(ys[t]) \/ np.sum(np.exp(ys[t])) # probabilities for next chars. \n        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n \n        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n\n#         y_class = np.zeros((num_chars, 1)) \n#         y_class[targets[t]] =1\n#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n\n    return loss, ps, hs, xs\n\n\ndef backprop(ps, inputs, hs, xs):\n\n    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n\n    # reversed\n    for t in reversed(range(len(inputs))):\n        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss\/dy\"\n        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n        dWhy += np.dot(dy, hs[t].T)\n        dby += dy \n        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n        dbh += dhraw\n        dWxh += np.dot(dhraw, xs[t].T)\n        dWhh += np.dot(dhraw, hs[t-1].T)\n        dhnext = np.dot(W_hh.T, dhraw)\n    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n    \n    return dWxh, dWhh, dWhy, dbh, dby\n\ndata_pointer = 0\n\n# memory variables for Adagrad\nmWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\nmbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n\n\nfor i in range(iteration):\n    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n    data_pointer = 0 # go from start of data\n    \n    for b in range(batch_size):\n        \n        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n            \n        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data. \n#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n\n\n        # forward\n        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n#         print(loss)\n    \n        # backward\n        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n        \n        \n    # perform parameter update with Adagrad\n        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n                                    [dWxh, dWhh, dWhy, dbh, dby], \n                                    [mWxh, mWhh, mWhy, mbh, mby]):\n            mem += dparam * dparam # elementwise\n            param += -learning_rate * dparam \/ np.sqrt(mem + 1e-8) # adagrad update      \n    \n        data_pointer += sequence_length # move data pointer\n        \n    if i % 100 == 0:\n        print ('iter %d, loss: %f' % (i, loss)) # print progress","37df6430":"import os\nimport time\nimport tensorflow as tf\nimport numpy as np\nfrom glob import glob\nimport datetime\nimport random\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\n%matplotlib inline\nimport urllib\nimport tarfile\nimport xml.etree.ElementTree as ET\nfrom imageio import imread, imsave, mimsave\nimport shutil\nimport cv2\nimport glob\nfrom imageio import imread, imsave, mimsave\nimport numpy as np, pandas as pd, os\nimport matplotlib.pyplot as plt, zipfile \nfrom PIL import Image \nfrom glob import glob\n\n\nroot_images = \"..\/input\/generative-dog-images\/all-dogs\/all-dogs\/\"\nroot_annots = \"..\/input\/generative-dog-images\/annotation\/Annotation\/\"\nINPUT_DATA_DIR = \"..\/input\/generative-dog-images\/all-dogs\/all-dogs\/\"\nIMG_DIR = \"images\"\nComputeLB = False\nDogsOnly = True\nIMAGE_SIZE = 64\nNOISE_SIZE = 100\nLR_D = 0.0005\nLR_G = 2e-4\nBATCH_SIZE = 64\nEPOCHS = 100\nBETA1_G = 0.5\nBETA1_D = 0.5\nWEIGHT_INIT_STDDEV = 0.02\nMOMENTUM = 0.9\nEPSILON = 0.0005\nSAMPLES_TO_SHOW = 5 # each epoch\n\n### plotter\n\n\nROOT = '..\/input\/generative-dog-images\/'\nif not ComputeLB: ROOT = '..\/input\/'\nIMAGES = os.listdir(ROOT + 'all-dogs\/all-dogs\/')\nbreeds = os.listdir(ROOT + 'annotation\/Annotation\/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,64,64,3))\n\n# CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n# https:\/\/www.kaggle.com\/paulorzp\/show-annotations-and-breeds\nif DogsOnly:\n    for breed in breeds:\n        for dog in os.listdir(ROOT+'annotation\/Annotation\/'+breed):\n            try: img = Image.open(ROOT+'all-dogs\/all-dogs\/'+dog+'.jpg') \n            except: continue           \n            tree = ET.parse(ROOT+'annotation\/Annotation\/'+breed+'\/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                img2 = img2.resize((64,64), Image.ANTIALIAS)\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                #if idxIn%1000==0: print(idxIn)\n                namesIn.append(breed)\n                idxIn += 1\n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n    imagesIn = imagesIn[idx,:,:,:]\n    namesIn = np.array(namesIn)[idx]\n    \n# RANDOMLY CROP FULL IMAGES\nelse:\n    x = np.random.choice(np.arange(20579),10000)\n    for k in range(len(x)):\n        img = Image.open(ROOT + 'all-dogs\/all-dogs\/' + IMAGES[x[k]])\n        w = img.size[0]\n        h = img.size[1]\n        sz = np.min((w,h))\n        a=0; b=0\n        if w<h: b = (h-sz)\/\/2\n        else: a = (w-sz)\/\/2\n        img = img.crop((0+a, 0+b, sz+a, sz+b))  \n        img = img.resize((64,64), Image.ANTIALIAS)   \n        imagesIn[idxIn,:,:,:] = np.asarray(img)\n        namesIn.append(IMAGES[x[k]])\n        if idxIn%1000==0: print(idxIn)\n        idxIn += 1\n    \n# DISPLAY CROPPED IMAGES\nx = np.random.randint(0,idxIn,25)\nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n        else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()\n    \n#### Generator\n\ndef generator(z, output_channel_dim, training):\n    with tf.variable_scope(\"generator\", reuse= not training):\n        \n        # 4x4x512\n        fully_connected = tf.layers.dense(z, 4*4*512)\n        fully_connected = tf.reshape(fully_connected, (-1, 4, 4, 512))\n        fully_connected = tf.nn.leaky_relu(fully_connected)\n\n        # 4x4x512 -> 8x8x256\n        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\n                                                 filters=256,\n                                                 kernel_size=[5,5],\n                                                 strides=[2,2],\n                                                 padding=\"SAME\",\n                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                                 name=\"trans_conv1\")\n        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\n                                                          training=training,\n                                                          epsilon=EPSILON,\n                                                          name=\"batch_trans_conv1\")\n        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\n                                           name=\"trans_conv1_out\")\n        \n        # 8x8x256 -> 16x16x128\n        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\n                                                 filters=128,\n                                                 kernel_size=[5,5],\n                                                 strides=[2,2],\n                                                 padding=\"SAME\",\n                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                                 name=\"trans_conv2\")\n        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\n                                                          training=training,\n                                                          epsilon=EPSILON,\n                                                          name=\"batch_trans_conv2\")\n        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\n                                           name=\"trans_conv2_out\")\n        \n        # 16x16x128 -> 32x32x64\n        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\n                                                 filters=64,\n                                                 kernel_size=[5,5],\n                                                 strides=[2,2],\n                                                 padding=\"SAME\",\n                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                                 name=\"trans_conv3\")\n        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\n                                                          training=training,\n                                                          epsilon=EPSILON,\n                                                          name=\"batch_trans_conv3\")\n        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\n                                           name=\"trans_conv3_out\")\n        \n\n        # 32x32x64 -> 64x64x32\n        trans_conv4 = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\n                                                 filters=32,\n                                                 kernel_size=[5,5],\n                                                 strides=[2,2],\n                                                 padding=\"SAME\",\n                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                                 name=\"trans_conv4\")\n        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4,\n                                                          training=training,\n                                                          epsilon=EPSILON,\n                                                          name=\"batch_trans_conv4\")\n        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4,\n                                           name=\"trans_conv4_out\")\n        \n        # 64x64x32 -> 64x64x3\n        logits = tf.layers.conv2d_transpose(inputs=trans_conv4_out,\n                                            filters=3,\n                                            kernel_size=[5,5],\n                                            strides=[1,1],\n                                            padding=\"SAME\",\n                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                            name=\"logits\")\n        out = tf.tanh(logits, name=\"out\")\n        return out\n    \n    \n### discriminator\ndef discriminator(x, reuse):\n    with tf.variable_scope(\"discriminator\", reuse=reuse): \n        \n        # 64x64x3 -> 32x32x32\n        conv1 = tf.layers.conv2d(inputs=x,\n                                 filters=32,\n                                 kernel_size=[5,5],\n                                 strides=[2,2],\n                                 padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                 name='conv1')\n        batch_norm1 = tf.layers.batch_normalization(conv1,\n                                                    training=True,\n                                                    epsilon=EPSILON,\n                                                    name='batch_norm1')\n        conv1_out = tf.nn.leaky_relu(batch_norm1,\n                                     name=\"conv1_out\")\n        \n        # 32x32x32 -> 16x16x64\n        conv2 = tf.layers.conv2d(inputs=conv1_out,\n                                 filters=64,\n                                 kernel_size=[5, 5],\n                                 strides=[2, 2],\n                                 padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                 name='conv2')\n        batch_norm2 = tf.layers.batch_normalization(conv2,\n                                                    training=True,\n                                                    epsilon=EPSILON,\n                                                    name='batch_norm2')\n        conv2_out = tf.nn.leaky_relu(batch_norm2,\n                                     name=\"conv2_out\")\n        \n        # 16x16x64 -> 8x8x128\n        conv3 = tf.layers.conv2d(inputs=conv2_out,\n                                 filters=128,\n                                 kernel_size=[5, 5],\n                                 strides=[2, 2],\n                                 padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                 name='conv3')\n        batch_norm3 = tf.layers.batch_normalization(conv3,\n                                                    training=True,\n                                                    epsilon=EPSILON,\n                                                    name='batch_norm3')\n        conv3_out = tf.nn.leaky_relu(batch_norm3,\n                                     name=\"conv3_out\")\n        \n        # 8x8x128 -> 8x8x256\n        conv4 = tf.layers.conv2d(inputs=conv3_out,\n                                 filters=256,\n                                 kernel_size=[5, 5],\n                                 strides=[1, 1],\n                                 padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                 name='conv4')\n        batch_norm4 = tf.layers.batch_normalization(conv4,\n                                                    training=True,\n                                                    epsilon=EPSILON,\n                                                    name='batch_norm4')\n        conv4_out = tf.nn.leaky_relu(batch_norm4,\n                                     name=\"conv4_out\")\n        \n        # 8x8x256 -> 4x4x512\n        \n        conv5 = tf.layers.conv2d(inputs=conv4_out,\n                                filters=512,\n                                kernel_size=[5, 5],\n                                strides=[2, 2],\n                                padding=\"SAME\",\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n                                name='conv5')\n        batch_norm5 = tf.layers.batch_normalization(conv5,\n                                                    training=True,\n                                                    epsilon=EPSILON,\n                                                    name='batch_norm5')\n        conv5_out = tf.nn.leaky_relu(batch_norm5,\n                                     name=\"conv5_out\")\n\n        flatten = tf.reshape(conv5_out, (-1, 4*4*512))\n        logits = tf.layers.dense(inputs=flatten,\n                                 units=1,\n                                 activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n    \n ###loss    \ndef model_loss(input_real, input_z, output_channel_dim):\n    g_model = generator(input_z, output_channel_dim, True)\n\n    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\n                                                     mean=0.0,\n                                                     stddev=random.uniform(0.0, 0.1),\n                                                     dtype=tf.float32)\n    \n    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\n    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n    \n    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n                                                                         labels=tf.zeros_like(d_model_fake)))\n    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n                                                                    labels=tf.ones_like(d_model_fake)))\n    return d_loss, g_loss\n##optimiser\ndef model_optimizers(d_loss, g_loss):\n    t_vars = tf.trainable_variables()\n    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n    \n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n    \n    with tf.control_dependencies(gen_updates):\n        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1_D).minimize(d_loss, var_list=d_vars)\n        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1_G).minimize(g_loss, var_list=g_vars)  \n    return d_train_opt, g_train_opt\n\ndef model_inputs(real_dim, z_dim):\n    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\n    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\n    return inputs_real, inputs_z, learning_rate_G, learning_rate_D\n\ndef show_samples(sample_images, name, epoch):\n    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = sample_images[index].astype('uint8') \n        axis.imshow(image_array)\n    plt.show()\n    plt.close()\n\ndef save_samples(sample_images, name, epoch):\n    # save images\n    for index,img in enumerate(sample_images):\n        image = Image.fromarray(img.astype('uint8') )\n        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n        \ndef test(sess, input_z, out_channel_dim, epoch):\n    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n    show_samples(sample_images, IMG_DIR + \"samples\", epoch)\n    \ndef generate (sess, input_z, out_channel_dim):\n    print (\">> Generating 1k images ...\")\n    for i in tqdm(range(100)):\n        example_z = np.random.uniform(-1, 1, size=[100, 100]).astype(np.float32)\n        imgs = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n        imgs = [((img + 1.0) * 127.5).astype(np.uint8) for img in imgs]\n        for j in range(len(imgs)):\n            imsave(os.path.join(IMG_DIR, f'dog_{i}_{j}.png'), imgs[j])\n            \ndef summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n    minibatch_size = int(data_shape[0]\/\/BATCH_SIZE)\n    print(\"Epoch {}\/{}\".format(epoch, EPOCHS),\n          \"\\nDuration: {:.5f}\".format(duration),\n          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n    \n    fig, ax = plt.subplots()\n    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n    plt.plot(g_losses, label='Generator', alpha=0.6)\n    plt.title(\"Losses\")\n    plt.legend()\n    #plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n    plt.show()\n    plt.close()\n    test(sess, input_z, data_shape[3], epoch)\n    \n ## batch generation   \ndef get_batches(data):\n    batches = []\n    for i in range(int(data.shape[0]\/\/BATCH_SIZE)):\n        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n        augmented_images = []\n        for img in batch:\n            image = Image.fromarray(img.astype('uint8'))\n            if random.choice([True, False]):\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            augmented_images.append(np.asarray(image))\n        batch = np.asarray(augmented_images)\n        normalized_batch = (batch \/ 127.5) - 1.0\n        batches.append(normalized_batch)\n    return batches\n\n\ndef train(get_batches, data_shape, LR_G = 2e-4, LR_D = 0.0005):\n    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\n    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\n    d_opt, g_opt = model_optimizers(d_loss, g_loss)\n    generator_epoch_loss = 0\n    train_d_losses = []\n    train_g_losses = []\n    generator_epoch_loss = 999\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        epoch = 0\n        iteration = 0\n        d_losses = []\n        g_losses = []\n        \n        for epoch in tqdm(range(EPOCHS)):        \n            epoch += 1\n            start_time = time.time()\n                \n            for batch_images in get_batches:\n                iteration += 1\n                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n                g_losses.append(g_loss.eval({input_z: batch_z}))\n\n            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)\n            minibatch_size = int(data_shape[0]\/\/BATCH_SIZE)\n            generator_epoch_loss = np.mean(g_losses[-minibatch_size:])\n            train_d_losses.append(np.mean(d_losses[-minibatch_size:]))\n            train_g_losses.append(np.mean(g_losses[-minibatch_size:]))\n            \n            if epoch == EPOCHS:\n                generate (sess, input_z, out_channel_dim=3)\n            \n    fig, ax = plt.subplots()\n    plt.plot(train_d_losses, label='Discriminator', alpha=0.5)\n    plt.plot(train_g_losses, label='Generator', alpha=0.5)\n    plt.title(\"Training Losses\")\n    plt.legend()\n    plt.savefig('train_losses.png')\n    plt.show()\n    plt.close()\n    \n### train start\n\nstart = time.time()\n\nprint(\">> Start training...\")\nwith tf.Graph().as_default():\n    train(get_batches(imagesIn), imagesIn.shape)\n    \nprint(\">> train time = \",time.time() - start)","17306b9a":"## show images\nimgs = os.listdir('\/kaggle\/working\/images')\nprint (\"images = \",len(imgs))\n\nplt.figure(figsize=(10,10))\nfor i,image in enumerate(imgs):\n    im= Image.open('images\/'+image)\n    plt.subplot(3,3,i+1)\n    plt.axis(\"off\")\n    plt.imshow(im)    \n    if(i==8):\n        break","5fba04c5":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport tensorflow_addons as tfa\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nautotune = tf.data.AUTOTUNE\n\n# Load the horse-zebra dataset using tensorflow-datasets.\ndataset, _ = tfds.load(\"cycle_gan\/horse2zebra\", with_info=True, as_supervised=True)\ntrain_horses, train_zebras = dataset[\"trainA\"], dataset[\"trainB\"]\ntest_horses, test_zebras = dataset[\"testA\"], dataset[\"testB\"]\n\n# Define the standard image size.\norig_img_size = (286, 286)\n# Size of the random crops to be used during training.\ninput_img_size = (256, 256, 3)\n# Weights initializer for the layers.\nkernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n# Gamma initializer for instance normalization.\ngamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\nbuffer_size = 256\nbatch_size = 1\n\n\ndef normalize_img(img):\n    img = tf.cast(img, dtype=tf.float32)\n    # Map values in the range [-1, 1]\n    return (img \/ 127.5) - 1.0\n\n\ndef preprocess_train_image(img, label):\n    # Random flip\n    img = tf.image.random_flip_left_right(img)\n    # Resize to the original size first\n    img = tf.image.resize(img, [*orig_img_size])\n    # Random crop to 256X256\n    img = tf.image.random_crop(img, size=[*input_img_size])\n    # Normalize the pixel values in the range [-1, 1]\n    img = normalize_img(img)\n    return img\n\n\ndef preprocess_test_image(img, label):\n    # Only resizing and normalization for the test images.\n    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n    img = normalize_img(img)\n    return img\n\ntrain_horses = (\n    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(batch_size)\n)\ntrain_zebras = (\n    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(batch_size)\n)\n\n# Apply the preprocessing operations to the test data\ntest_horses = (\n    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(batch_size)\n)\ntest_zebras = (\n    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(batch_size)\n)\n\n\n_, ax = plt.subplots(4, 2, figsize=(10, 15))\nfor i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\n    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n    ax[i, 0].imshow(horse)\n    ax[i, 1].imshow(zebra)\nplt.show()","405af81c":"class ReflectionPadding2D(layers.Layer):\n    \"\"\"Implements Reflection Padding as a layer.\n\n    Args:\n        padding(tuple): Amount of padding for the\n        spatial dimensions.\n\n    Returns:\n        A padded tensor with the same type as the input tensor.\n    \"\"\"\n\n    def __init__(self, padding=(1, 1), **kwargs):\n        self.padding = tuple(padding)\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n\n    def call(self, input_tensor, mask=None):\n        padding_width, padding_height = self.padding\n        padding_tensor = [\n            [0, 0],\n            [padding_height, padding_height],\n            [padding_width, padding_width],\n            [0, 0],\n        ]\n        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n\n\ndef residual_block(\n    x,\n    activation,\n    kernel_initializer=kernel_init,\n    kernel_size=(3, 3),\n    strides=(1, 1),\n    padding=\"valid\",\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    dim = x.shape[-1]\n    input_tensor = x\n\n    x = ReflectionPadding2D()(input_tensor)\n    x = layers.Conv2D(\n        dim,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = activation(x)\n\n    x = ReflectionPadding2D()(x)\n    x = layers.Conv2D(\n        dim,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = layers.add([input_tensor, x])\n    return x\n\n\ndef downsample(\n    x,\n    filters,\n    activation,\n    kernel_initializer=kernel_init,\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=\"same\",\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    x = layers.Conv2D(\n        filters,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x\n\n\ndef upsample(\n    x,\n    filters,\n    activation,\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=\"same\",\n    kernel_initializer=kernel_init,\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    x = layers.Conv2DTranspose(\n        filters,\n        kernel_size,\n        strides=strides,\n        padding=padding,\n        kernel_initializer=kernel_initializer,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x\n","753da178":"def get_resnet_generator(\n    filters=64,\n    num_downsampling_blocks=2,\n    num_residual_blocks=9,\n    num_upsample_blocks=2,\n    gamma_initializer=gamma_init,\n    name=None,\n):\n    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n        x\n    )\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = layers.Activation(\"relu\")(x)\n\n    # Downsampling\n    for _ in range(num_downsampling_blocks):\n        filters *= 2\n        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n\n    # Residual blocks\n    for _ in range(num_residual_blocks):\n        x = residual_block(x, activation=layers.Activation(\"relu\"))\n\n    # Upsampling\n    for _ in range(num_upsample_blocks):\n        filters \/\/= 2\n        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n\n    # Final block\n    x = ReflectionPadding2D(padding=(3, 3))(x)\n    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n    x = layers.Activation(\"tanh\")(x)\n\n    model = keras.models.Model(img_input, x, name=name)\n    return model\ndef get_discriminator(\n    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n):\n    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n    x = layers.Conv2D(\n        filters,\n        (4, 4),\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=kernel_initializer,\n    )(img_input)\n    x = layers.LeakyReLU(0.2)(x)\n\n    num_filters = filters\n    for num_downsample_block in range(3):\n        num_filters *= 2\n        if num_downsample_block < 2:\n            x = downsample(\n                x,\n                filters=num_filters,\n                activation=layers.LeakyReLU(0.2),\n                kernel_size=(4, 4),\n                strides=(2, 2),\n            )\n        else:\n            x = downsample(\n                x,\n                filters=num_filters,\n                activation=layers.LeakyReLU(0.2),\n                kernel_size=(4, 4),\n                strides=(1, 1),\n            )\n\n    x = layers.Conv2D(\n        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n    )(x)\n\n    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n    return model\n\n\n# Get the generators\ngen_G = get_resnet_generator(name=\"generator_G\")\ngen_F = get_resnet_generator(name=\"generator_F\")\n\n# Get the discriminators\ndisc_X = get_discriminator(name=\"discriminator_X\")\ndisc_Y = get_discriminator(name=\"discriminator_Y\")\n\n","52d03e3c":" \nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        generator_G,\n        generator_F,\n        discriminator_X,\n        discriminator_Y,\n        lambda_cycle=10.0,\n        lambda_identity=0.5,\n    ):\n        super(CycleGan, self).__init__()\n        self.gen_G = generator_G\n        self.gen_F = generator_F\n        self.disc_X = discriminator_X\n        self.disc_Y = discriminator_Y\n        self.lambda_cycle = lambda_cycle\n        self.lambda_identity = lambda_identity\n\n    def compile(\n        self,\n        gen_G_optimizer,\n        gen_F_optimizer,\n        disc_X_optimizer,\n        disc_Y_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n    ):\n        super(CycleGan, self).compile()\n        self.gen_G_optimizer = gen_G_optimizer\n        self.gen_F_optimizer = gen_F_optimizer\n        self.disc_X_optimizer = disc_X_optimizer\n        self.disc_Y_optimizer = disc_Y_optimizer\n        self.generator_loss_fn = gen_loss_fn\n        self.discriminator_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n\n    def train_step(self, batch_data):\n        # x is Horse and y is zebra\n        real_x, real_y = batch_data\n\n        # For CycleGAN, we need to calculate different\n        # kinds of losses for the generators and discriminators.\n        # We will perform the following steps here:\n        #\n        # 1. Pass real images through the generators and get the generated images\n        # 2. Pass the generated images back to the generators to check if we\n        #    we can predict the original image from the generated image.\n        # 3. Do an identity mapping of the real images using the generators.\n        # 4. Pass the generated images in 1) to the corresponding discriminators.\n        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n        # 6. Calculate the discriminators loss\n        # 7. Update the weights of the generators\n        # 8. Update the weights of the discriminators\n        # 9. Return the losses in a dictionary\n\n        with tf.GradientTape(persistent=True) as tape:\n            # Horse to fake zebra\n            fake_y = self.gen_G(real_x, training=True)\n            # Zebra to fake horse -> y2x\n            fake_x = self.gen_F(real_y, training=True)\n\n            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n            cycled_x = self.gen_F(fake_y, training=True)\n            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n            cycled_y = self.gen_G(fake_x, training=True)\n\n            # Identity mapping\n            same_x = self.gen_F(real_x, training=True)\n            same_y = self.gen_G(real_y, training=True)\n\n            # Discriminator output\n            disc_real_x = self.disc_X(real_x, training=True)\n            disc_fake_x = self.disc_X(fake_x, training=True)\n\n            disc_real_y = self.disc_Y(real_y, training=True)\n            disc_fake_y = self.disc_Y(fake_y, training=True)\n\n            # Generator adverserial loss\n            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n\n            # Generator cycle loss\n            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n\n            # Generator identity loss\n            id_loss_G = (\n                self.identity_loss_fn(real_y, same_y)\n                * self.lambda_cycle\n                * self.lambda_identity\n            )\n            id_loss_F = (\n                self.identity_loss_fn(real_x, same_x)\n                * self.lambda_cycle\n                * self.lambda_identity\n            )\n\n            # Total generator loss\n            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n\n            # Discriminator loss\n            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n\n        # Get the gradients for the generators\n        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n\n        # Get the gradients for the discriminators\n        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n\n        # Update the weights of the generators\n        self.gen_G_optimizer.apply_gradients(\n            zip(grads_G, self.gen_G.trainable_variables)\n        )\n        self.gen_F_optimizer.apply_gradients(\n            zip(grads_F, self.gen_F.trainable_variables)\n        )\n\n        # Update the weights of the discriminators\n        self.disc_X_optimizer.apply_gradients(\n            zip(disc_X_grads, self.disc_X.trainable_variables)\n        )\n        self.disc_Y_optimizer.apply_gradients(\n            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n        )\n\n        return {\n            \"G_loss\": total_loss_G,\n            \"F_loss\": total_loss_F,\n            \"D_X_loss\": disc_X_loss,\n            \"D_Y_loss\": disc_Y_loss,\n        }\n\nclass GANMonitor(keras.callbacks.Callback):\n    \"\"\"A callback to generate and save images after each epoch\"\"\"\n\n    def __init__(self, num_img=4):\n        self.num_img = num_img\n\n    def on_epoch_end(self, epoch, logs=None):\n        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n        for i, img in enumerate(test_horses.take(self.num_img)):\n            prediction = self.model.gen_G(img)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n            ax[i, 0].imshow(img)\n            ax[i, 1].imshow(prediction)\n            ax[i, 0].set_title(\"Input image\")\n            ax[i, 1].set_title(\"Translated image\")\n            ax[i, 0].axis(\"off\")\n            ax[i, 1].axis(\"off\")\n\n            prediction = keras.preprocessing.image.array_to_img(prediction)\n            prediction.save(\n                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n            )\n        plt.show()\n        plt.close()\n        \n# Loss function for evaluating adversarial loss\nadv_loss_fn = keras.losses.MeanSquaredError()\n\n# Define the loss function for the generators\ndef generator_loss_fn(fake):\n    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n    return fake_loss\n\n\n# Define the loss function for the discriminators\ndef discriminator_loss_fn(real, fake):\n    real_loss = adv_loss_fn(tf.ones_like(real), real)\n    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n    return (real_loss + fake_loss) * 0.5\n\n\n# Create cycle gan model\ncycle_gan_model = CycleGan(\n    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n)\n\n# Compile the model\ncycle_gan_model.compile(\n    gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n    gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n    gen_loss_fn=generator_loss_fn,\n    disc_loss_fn=discriminator_loss_fn,\n)\n# Callbacks\nplotter = GANMonitor()\ncheckpoint_filepath = \".\/model_checkpoints\/cyclegan_checkpoints.{epoch:03d}\"\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath\n)\n\n# Here we will train the model for just one epoch \ncycle_gan_model.fit(\n    tf.data.Dataset.zip((train_horses, train_zebras)),\n    epochs=1,\n    callbacks=[plotter, model_checkpoint_callback],\n)","704f9950":"import tensorflow as tf\nimport numpy as np\nclass RBM(object):\n    def __init__(self, visible_dim, hidden_dim, learning_rate, number_of_iterations):\n        \n        self._graph = tf.Graph()\n        \n        #Initialize graph\n        with self._graph.as_default():\n            \n            self._num_iter = number_of_iterations\n            self._visible_biases = tf.Variable(tf.random.uniform([1, visible_dim], 0, 1, name = \"visible_biases\"))\n            self._hidden_biases = tf.Variable(tf.random.uniform([1, hidden_dim], 0, 1, name = \"hidden_biases\"))\n            self._hidden_states = tf.Variable(tf.zeros([1, hidden_dim], tf.float32, name = \"hidden_biases\"))\n            self._visible_cdstates = tf.Variable(tf.zeros([1, visible_dim], tf.float32, name = \"visible_biases\"))\n            self._hidden_cdstates = tf.Variable(tf.zeros([1, hidden_dim], tf.float32, name = \"hidden_biases\"))\n            self._weights = tf.Variable(tf.random.normal([visible_dim, hidden_dim], 0.01), name=\"weights\")\n            self._leraning_rate =  tf.Variable(tf.fill([visible_dim, hidden_dim], learning_rate), name = \"learning_rate\")\n            \n            self._input_sample = tf.compat.v1.placeholder(tf.float32, [visible_dim], name = \"input_sample\")\n            \n            # Gibbs Sampling\n            input_matrix = tf.transpose(tf.stack([self._input_sample for i in range(hidden_dim)]))\n            _hidden_probabilities = tf.sigmoid(tf.add(tf.multiply(input_matrix, self._weights), tf.stack([self._hidden_biases[0] for i in range(visible_dim)])))\n            self._hidden_states = self.callculate_state(_hidden_probabilities)\n            _visible_probabilities = tf.sigmoid(tf.add(tf.multiply(self._hidden_states, self._weights), tf.transpose(tf.stack([self._visible_biases[0] for i in range(hidden_dim)]))))\n            self._visible_cdstates = self.callculate_state(_visible_probabilities)\n            self._hidden_cdstates = self.callculate_state(tf.sigmoid(tf.multiply(self._visible_cdstates, self._weights) + self._hidden_biases))\n            \n            #CD\n            positive_gradient_matrix = tf.multiply(input_matrix, self._hidden_states)\n            negative_gradient_matrix = tf.multiply(self._visible_cdstates, self._hidden_cdstates)\n            print(\"Contrastive Divergence\", positive_gradient_matrix)\n            \n            new_weights = self._weights\n            new_weights.assign_add(tf.multiply(positive_gradient_matrix, self._leraning_rate))\n            new_weights.assign_sub(tf.multiply(negative_gradient_matrix, self._leraning_rate))\n\n            self._training = tf.compat.v1.assign(self._weights, new_weights) \n            \n            #Initilize session and run it\n            self._sess = tf.compat.v1.Session()\n            initialization = tf.compat.v1.global_variables_initializer()\n            self._sess.run(initialization)\n        \n    def train(self, input_vects):\n        for iter_no in range(self._num_iter):\n            print(f\"Running for {iter_no} iterations\")\n            for input_vect in input_vects:\n                print(self._sess.run(self._training,feed_dict={self._input_sample: input_vect}))\n    \n    def callculate_state(self, probability):\n        state= tf.floor(probability + tf.random.uniform(tf.shape(probability), 0, 1))\n        print(\"Hidden activated states\",state)\n        return state\n\ntest = np.array([[0,1,1,0], [0,1,0,0], [0,0,1,1]])\nrbm = RBM(4, 3, 0.1, 120)\nrbm.train(test)","b315973d":"## GAN\n\n\n<img src=\"https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2019\/05\/Summary-of-the-Generative-Adversarial-Network-Training-Algorithm-1024x669.png\">","54ebec1d":"### Optimizers in TF\n\nIn this case , we will be building our own optimizer from [tensorflow](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/python\/keras\/optimizer_v2) and testing it on [MNIST](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/datasets\/mnist)\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRRfhjR1qbTR_CIsgJ5v4ClnFTZRAhKskP-pQ&usqp=CAU\">","8988a5fb":"### Xception Network\n\n<img src=\"https:\/\/maelfabien.github.io\/assets\/images\/xception.jpg\">\n\nXCeption is an efficient architecture that relies on two main points :\n\n- Depthwise Separable Convolution\n- Shortcuts between Convolution blocks as in ResNet\n\n#### Depthwise Separable Convolutions\n\nDepthwise Separable Convolutionsare alternatives to classical convolutions that are supposed to be much more efficient in terms of computation time.Depthwise Convolution is a first step in which instead of applying convolution of size \n$d\u00d7d\u00d7C$, we apply a convolution of size $d\u00d7d\u00d71$. In other words, we don\u2019t make the convolution computation over all the channels, but only 1 by 1.\n\n#### Pointwise Convolution\n\nPointwise convolution operates a classical convolution, with size $1\u00d71\u00d7N$ over the $K\u00d7K\u00d7C$ volume. This allows creating a volume of shape $K\u00d7K\u00d7N$, as previously.\n\n\n<img src=\"https:\/\/maelfabien.github.io\/assets\/images\/XCeption3.jpg\">\n","c87d1452":"## DCGAN (Deep convolutional generative adversarial networks)\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*yeBUm7SfMaHjnFd2.gif\">\n\n\n[Paper](https:\/\/arxiv.org\/abs\/1511.06434)\n\nOne of the most interesting parts of GANs is the design of the Generator. The Generator network is able to take random noise and map it into images such that the discriminator cannot tell which images came from the dataset and which images came from the generator.\n\nThis is a very interesting application of neural networks. Typically neural nets map input into a binary output, (1 or 0), maybe a regression output, (some real-valued number), or even multiple categorical outputs, (such as MNIST or CIFAR-10\/100). We will see how a neural net maps from random noise to an image matrix and how using Convolutional Layers in the generator network produces better results.\n\nIn addition, although GAN is known for its difficulty in learning, this paper proposed by Radford. introduces various techniques for successful learning:\n\n- Convert max-pooling layers to convolution layers\n- Convert fully connected layers to global average pooling layers in the discriminator\n- Use batch normalization layers in the generator and the discriminator\n- Use leaky ReLU activation functions in the discriminator\n\nDCGAN is one of the popular and successful network design for GAN. It mainly composes of convolution layers without max pooling or fully connected layers. It uses convolutional stride and transposed convolution for the downsampling and the upsampling. The figure below is the network design for the generator.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*rdXKdyfNjorzP10ZA3yNmQ.png\">\n\nThis is the DCGAN generator presented in the LSUN scene modeling paper. This network takes in a 100x1 noise vector, denoted z, and maps it into the G(Z) output which is 64x64x3.\n\nThis architecture is especially interesting the way the first layer expands the random noise. The network goes from 100x1 to 1024x4x4! This layer is denoted \u2018project and reshape\u2019.\n\nWe see that following this layer, classical convolutional layers are applied which reshape the network with the (N+P\u200a\u2014\u200aF)\/S + 1 equation classically taught with convolutional layers. In the diagram above we can see that the N parameter, (Height\/Width), goes from 4 to 8 to 16 to 32, it doesn\u2019t appear that there is any padding, the kernel filter parameter F is 5x5, and the stride is 2. You may find this equation to be useful for designing your own convolutional layers for customized output sizes.\n\nWe see the network goes from:\n\n100x1 \u2192 1024x4x4 \u2192 512x8x8 \u2192 256x16x16 \u2192 128x32x32 \u2192 64x64x3\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*bNjBJm6827sRXvzmcjGTDQ.png\">\n\nHere is the summary of DCGAN:\n\n- Replace all max pooling with convolutional stride\n- Use transposed convolution for upsampling.\n- Eliminate fully connected layers.\n- Use Batch normalization except the output layer for the generator and the input layer of the discriminator.\n- Use ReLU in the generator except for the output which uses tanh.\n- Use LeakyReLU in the discriminator.\n\nResource:\n\n- [TF](https:\/\/keras.io\/examples\/generative\/)\n- [Jason](https:\/\/machinelearningmastery.com\/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras\/)\n- [dcgan](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)\n- [kernel](https:\/\/www.kaggle.com\/cdeotte\/dog-memorizer-gan)\n\n","092dde3f":"#### Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far. Forward pass of Classical RNNs have the following formula :\n\n\n#### Forward Pass Formula \n\n\n For the hidden gates: <img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/12\/06005300\/eq2.png\">\n \n For the output gate: <img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/12\/06005750\/outeq.png\">\n \n \nGenerally for the output of the forward pass, we generally use a softmax activation on the output.\n\n\n#### Backward Pass Equations and BPTT\n\n[Backpropagation Through Time](http:\/\/www.wildml.com\/2015\/10\/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients\/) is done in RNNs which allows flow of gradients through each hidden time step. The effective loss function for RNNs is : \n\n<img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cbegin%7Baligned%7D++E_t%28y_t%2C+%5Chat%7By%7D_t%29+%26%3D+-+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D+%5C%5C++E%28y%2C+%5Chat%7By%7D%29+%26%3D%5Csum%5Climits_%7Bt%7D+E_t%28y_t%2C%5Chat%7By%7D_t%29+%5C%5C++%26+%3D+-%5Csum%5Climits_%7Bt%7D+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nour goal is to calculate the gradients of the error with respect to our parameters U, V and W and then learn good parameters using Stochastic Gradient Descent. Just like we sum up the errors, we also sum up the gradients at each time step for one training example: <img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cfrac%7B%5Cpartial+E%7D%7B%5Cpartial+W%7D+%3D+%5Csum%5Climits_%7Bt%7D+%5Cfrac%7B%5Cpartial+E_t%7D%7B%5Cpartial+W%7D&bg=ffffff&fg=000&s=1\">\n\nTo calculate these gradients we use the chain rule of differentiation. That\u2019s the backpropagation algorithm when applied backwards starting from the error. For the rest of this post we\u2019ll use E_3 as an example, just to have concrete numbers to work with.\n\n<img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+V%7D+%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+z_3%7D%5Cfrac%7B%5Cpartial+z_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%28%5Chat%7By%7D_3+-+y_3%29+%5Cotimes+s_3+%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nEffectively the logic behind the chain rule is denoted by the following formula:\n<img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nBPTT can be understood clearly with this image\n\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTBrnLrCdE0ae14fv4Wc7ThY61Ikr6lzsyJSQ&usqp=CAU\">\n\n\n\n#### Classical RNN image\n\n\nA classic RNN consists of the following image:\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/627\/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\n\nSome resources:\n    \n- [Blog](http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\/)\n- [Video](https:\/\/youtu.be\/-eBjweSRgFc)\n- [Blog](https:\/\/towardsdatascience.com\/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/introduction-to-recurrent-neural-networks\/)\n- [Documentation](https:\/\/keras.io\/api\/layers\/recurrent_layers\/simple_rnn\/)\n\n#### Drawbacks of RNNs\n\n\nVanishing Gradients: The chain rule of differentiation of the weight vectors often lead to shrinkage in the change in the weights of the gradients for each iteration. This leads to  a slower convergence and many times it leads to an oscillation around local minimas. The chain rule formula: <img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nNote that <img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D+&bg=ffffff&fg=000&s=1\">  is a chain rule in itself! For example, <img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_1%7D+%3D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_2%7D%5Cfrac%7B%5Cpartial+s_2%7D%7B%5Cpartial+s_1%7D&bg=ffffff&fg=000&s=1\">. Also note that because we are taking the derivative of a vector function with respect to a vector, the result is a matrix (called the Jacobian matrix) whose elements are all the pointwise derivatives. We can rewrite the above gradient:\n\n<img src=\"http:\/\/s0.wp.com\/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D++%5Cleft%28%5Cprod%5Climits_%7Bj%3Dk%2B1%7D%5E%7B3%7D++%5Cfrac%7B%5Cpartial+s_j%7D%7B%5Cpartial+s_%7Bj-1%7D%7D%5Cright%29++%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nExploding Gradients: The chain rule (mainly due to tanh activation) often leads to overshooting of the gradient weights.This may lead to gradients which are really large at each iteration of the training process.\n\n","250e0ddb":"## CycleGAN\n\nCycleGAN is a model that aims to solve the image-to-image translation problem. The goal of the image-to-image translation problem is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, obtaining paired examples isn't always feasible. CycleGAN tries to learn this mapping without requiring paired input-output images, using cycle-consistent adversarial networks.\n\n- [Code](https:\/\/github.com\/junyanz\/pytorch-CycleGAN-and-pix2pix)\n- [Kernel](https:\/\/www.kaggle.com\/amyjang\/monet-cyclegan-tutorial\/notebook)\n\n### What is CycleGAN?\n\nFrom the authors:\n\nWe present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X \u2192 Y, such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y \u2192 X and introduce a cycle consistency loss to push F(G(X)) \u2248 X (and vice versa).\n\nIn essence it maps and image to a given domaind, if you are turning horses into zebra the image will be the horse and the domain is the zebras, in our case the photos are the image and the domain are the Monet paintings.\n\nTurning horses into zebras and zebras into horses:\n\n<img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/I%E2%80%99m%20Something%20of%20a%20Painter%20Myself\/cyclegan_horse-zebra.jpg\">","1b047b1f":"## Residual Networks\n\n#### Degradation Problem :\n\nThe main motivation of the ResNet original work was to address the degradation problem in a deep network. Adding more layers to a sufficiently deep neural network would first see saturation in accuracy and then the accuracy degrades. He et.al. presented the following picture of train and test error with Cifar-10 data-set using vanilla net--\n\n<img src=\"https:\/\/miro.medium.com\/max\/1000\/1*5jesyboQzDqOxddUCccqpw.png\">\n\n\nAs we can see the training (left) and test errors (right) for the deeper network (56 layer) are higher than the 20 layer network. More the depth and with increasing epochs, the error increases. At first, it appears that as the number of layers increase, the number of parameters increase, thus this is a problem of overfitting. But it is not, let\u2019s understand.\nOne way of thinking about the problem is to consider a sufficiently DNN that calculates a sufficiently strong set of features that is necessary for the task in hand (ex: Image classification). If we add one more layer of the network to this already very DNN, what will this additional layer do? If already the network could calculate strong features, then this additional layer doesn\u2019t need to calculate any extra features, rather, just copy the already calculated features i.e. perform an identity mapping (kernels in the added layer produce exact same features to that of the previous kernel). This seems to be a very simple operation but within a deep neural net this is far from our expectations.\n\n#### residual blocks\n\nThe idea of a residual block is completely based on the intuition that was explained before. Simpler function (shallower network) should be a subset of Complex function (deeper network) so that degradation problem can be addressed. Let us consider input x and the desired mapping from input to output is denoted by g(x). Instead of dealing with this function we will deal with a simpler function f(x) = g(x)-x. The original mapping is then recast to f(x)+x. In the ResNet paper He et al. hypothesized that it is easier to optimize the residual f(x) than the original g itself. Also optimizing the residual takes care of the fact that we don\u2019t need to bother about the dreaded identity mapping f(y)\u2192 y in a very deep network. Let\u2019s see the schematic of the residual block below \u2014\n\n<img src=\"https:\/\/miro.medium.com\/max\/1000\/1*gjoenc3yvXhPMRpoPn4xtA.png\">\n\nThe residual learning formulation ensures that when identity mappings are optimal (i.e. g(x) = x), the optimization will drive the weights towards zero of the residual function. ResNet consists of many residual blocks where residual learning is adopted to every few (usually 2 or 3 layers) stacked layers. The building block is shown in Figure 2 and the final output can be considered as y = f(x, W) + x. Here W\u2019s are the weights and these are learned during training. The operation f + x is performed by a shortcut (\u2018skip\u2019 2\/3 layers) connection and element-wise addition. This is the simplest block where no additional parameters are involved in the skip connection. Element-wise addition is only possible when the dimension of f and x are same, if this is not the case then, we multiply the input x by a projection matrix Ws, so that dimensions of f and x matches. In this case the output will change from the previous equation to y = f(x, W) + Ws * x. The elements in the projection matrix will also be trainable.\n\n\n### Building ResNet and 1\u00d7 1 Convolution:\n\nWe will build the ResNet with 50 layers following the method adopted in the original paper by He. et al. The architecture adopted for ResNet-50 is different from the 34 layers architecture. The shortcut connection skips 3 blocks instead of 2 and, the schematic diagram below will help us clarify some points-\n\n<img src=\"https:\/\/miro.medium.com\/max\/1000\/1*kwQcNkPe7guJy1qC0mpENA.png\">\n\nIn ResNet-50 the stacked layers in the residual block will always have 1\u00d71, 3\u00d73, and 1\u00d71 convolution layers. The 1\u00d71 convolution first reduces the dimension and then the features are calculated in bottleneck 3\u00d73 layer and then the dimension is again increased in the next 1\u00d71 layer. Using 1\u00d71 filter for reducing and increasing the dimension of feature maps before and after the bottleneck layer was described in the GoogLeNet model by Szegedy et al. in their Inception paper. Since there\u2019s no pooling layer within the residual block, the dimension is reduced by 1\u00d71 convolution with strides 2. With these points in mind let\u2019s build ResNet-50 using TensorFlow 2.0.\n\n#### Architecture for Resnet 50\n\n<img src=\"https:\/\/miro.medium.com\/max\/2000\/1*YQgpwj-Wde7r8bFqXy2ruw.png\">","e699e671":"## Energy Based Model\n\nEBM approach\nInstead of trying to classify x\u2019s to y\u2019s, we would like to predict if a certain pair of (x, y) fit together or not. Or in other words, find a y compatible with x. We can also pose the problem as finding a y for which some F(x,y) is low. For example:\n\nIs y an accurate high-resolution image of x ?\nIs text A a good translation of text B?\n\n### Definition\n\nWe define an energy function F: ${X}*{Y} ->{R}F:X\u00d7Y\u2192R$ where F(x,y) describes the level of dependency between (x,y) pairs. (Note that this energy is used in inference, not in learning.) The inference is given by the following equation:\n\n$y^{`} = argmin_{y}{ F(x,y)}$\n\n\n### Solution: gradient-based inference\n\nWe would like the energy function to be smooth and differentiable so that we can use it to perform the gradient-based method for inference. In order to perform inference, we search this function using gradient descent to find compatible yy\u2019s. There are many alternate methods to gradient methods to obtain the minimum.\n\nAside: Graphical models are a special case of Energy-Based models. The energy function decomposes as a sum of energy terms. Each energy terms take into account a subset of variables that we are dealing with. If they organize in a particular form, there are efficient inference algorithms to find the minimum of the sum of the terms with respect to the variable that we are interested in inferring.\n\n\n### Botlzmann Machines\n\n\n\nEnergy-Based Models are a set of deep learning models which utilize physics concept of energy. They determine dependencies between variables by associating a scalar value, which represents the energy to the complete system. To be more precise, this scalar value actually represents a measure of the probability that the system will be in a certain state. The Boltzmann Machine is just one type of Energy-Based Models.\n\nThey consist of symmetrically connected neurons. These neurons have a binary state, i.e they can be either on or off. The decision regarding the state is made stochastically. Since all neurons are connected to each other, calculating weights for all connections is resource-demanding, so this architecture needed to be optimized.\n\n<img src=\"https:\/\/i0.wp.com\/i.imgur.com\/sbc3T9f.png\">\n\nIn the end, we ended up with the Restricted Boltzmann Machine, an architecture which has two layers of neurons \u2013 visible and hidden, as you can see on the image below. The hidden neurons are connected only to the visible ones and vice-versa, meaning there are no connections between layers in the same layer. This architecture is simple and pretty flexible. It is important to note that data can go both ways, from the visible layer to hidden, and vice-versa.\n\nThe learning process of the Restricted Boltzmann Machine is separated into two big steps: Gibbs Sampling and Contrastive Divergence. First, we need to calculate the probabilities that neuron from the hidden layer is activated based on the input values on the visible layer \u2013 Gibbs Sampling. Using this value, we will either turn the neuron on or not.\n\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/y2l0yLZ.png\">\n\n\n### Theoretical Understanding\n\nWhen neuron i is given the opportunity to change its binary state, it first calculates the total input on connections of all active neurons and adds its own bias to it,\n\n<img src=\"https:\/\/i0.wp.com\/i.imgur.com\/OedBIFe.png?resize=197%2C59&ssl=1\">\n\nwhere $b_{i}$ represents the aforementioned mentioned bias, $s_{j}$  the current state of the neuron that is connected, and wij  the weight on the connection between neurons i and j. This means that there is the probability that neuron i will be active, meaning it will have a state one is given by the formula:\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/BE6o4o7.png?resize=283%2C77&ssl=1\">\n\nIf neurons update their state in any given order, this means that the network will eventually reach a Boltzmann distribution, which states that probability of a state vector v is determined by the \u201cenergy\u201d of that state vector relative to energies of all possible binary state vectors:\n\n<img src=\"https:\/\/i2.wp.com\/i.imgur.com\/BmGtBLQ.png?resize=241%2C67&ssl=1\">\n\nThe energy of a state vector in the Boltzmann machine is defined as\n\n<img src=\"https:\/\/i2.wp.com\/i.imgur.com\/8EQJekS.png?resize=311%2C63&ssl=1\">\n\nwhere si is the state assigned to neuron i by state vector v. From this equation we can see the dependency between the energy of the system and the weighted connections. In a nutshell, the goal of the learning process is to find a set of weights that will minimize the energy. However, the Boltzmann machine\u2019s architecture is resource-demanding. Since every neuron is connected to every other neuron, calculations can take a long time. That is why Restricted Boltzmann Machines (RBM) came into the picture.\n\n### Restricted Boltzmann machines\n\n\nThe only difference in the architecture between RBMs and the standard Boltzmann machines is that visible and hidden neurons are not connected among each other, i.e. visible neurons are only connected to hidden neurons. That looks something like this:\n\n<img src=\"https:\/\/i2.wp.com\/i.imgur.com\/y2l0yLZ.png?w=1080&ssl=1\">\n\n\n\nThis way the number of connections is reduced and a learning process of this kind of networks is demanding. As you can see, these kinds of networks have a very simple architecture, which is the main building block of deep belief neural networks. Energy function for RBM is defined by this formula,\n\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/KEbJrMG.png?resize=416%2C69&ssl=1\">\n\n\nwhere ai is the bias of the visible neuron i that is in state vi, bj is a bias of the hidden vector j that is in state hj and wij is the weight of the connection between neuron i and j. This function is just a specialization of the previous formula.\n\nThe probability of the whole system can be presented using the states of neurons in the hidden layer \u2013 h as well as the states of the neurons in the visible layer \u2013 v\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/HWZ7eCI.png?resize=226%2C109&ssl=1\">\n\n\nwhere Z summs all possible pairs of visible and hidden vectors and it is called the partition function. This again, is just a specialization of the Boltzmann distribution to the RBM\u2019s architecture. This formula provides us with an opportunity to calculate the probability that any neuron is activated. Given an input vector v, the probability for a single hidden neuron j being activated is\n\n\n<img src=\"https:\/\/i0.wp.com\/i.imgur.com\/lmlygsL.png?resize=461%2C63&ssl=1\">\n\nwhere \u03c3 is the Sigmoid function. Analogous to this the probability that visible neuron i is activated can be calculated like this:\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/EMrhzM3.png?resize=449%2C61&ssl=1\">\n\nAs you can imagine, the learning process of RBM greatly differs from the one that is in place with the feed-forward neural networks. It has two major processes:\n\n- [Gibbs sampling](https:\/\/en.wikipedia.org\/wiki\/Gibbs_sampling)\n- [Contrastive Divergence](https:\/\/www.robots.ox.ac.uk\/~ojw\/files\/NotesOnCD.pdf)\n\nGibbs sampling is a sub-process that itself consists of two parts. First, the input values are set to the visible layer, and then based on that, states of the neurons in the hidden layer are calculated. That is done using probability functions p(h|v) from the previous chapter. Once these values are available, the other function p(v|h) is used to predict new input values for the visible layer. This process can be repeated k times, but in practice, it is repeated once or twice for every input sample. In the end, we get the other input vector vk which was recreated from original input values v0.\n\nContrastive Divergence is a sub-process during which the weights are updated. Vectors v0 and vk are used to calculate activation probabilities for the hidden layers h0 and hk, again using the formula for p(h|v). Finally, we can get the matrix which will define delta for which weight needs to be updated:\n\n<img src=\"https:\/\/i1.wp.com\/i.imgur.com\/ckHmnhM.png?resize=358%2C48&ssl=1\">\n\nAfter Gibbs Sampling is performed, we will use the Contrastive Divergence to update the weights.  Let\u2019s consider the situation in which we have the visible layer with four nodes in the visible layer and a hidden layer with three nodes. For example, let\u2019s say that input values on the visible layer are [0, 1, 1, 0].\n\nUsing the formulas from [this article](https:\/\/rubikscode.net\/2018\/10\/01\/introduction-to-restricted-boltzmann-machines\/), we will calculate the activation probability for each neuron in the hidden layer. If this probability is high, the neuron from the hidden layer will be activated; otherwise, it will be off. For example, based on current weights and biases we get that values of the hidden layer are [0, 1, 1]. This is the moment when we calculate the so-called positive gradient using the outer product of layer neuron states [0, 1, 1, 0] and the hidden layer neuron states [0, 1, 1]. Outer product is defined like this:\n\n```\nv[0]*h[0] v[0]*h[1] v[0]*h[2] \nv[1]*h[0] v[1]*h[1] v[1]*h[2] \nv[2]*h[0] v[2]*h[1] v[2]*h[2] \nv[3]*h[0] v[3]*h[1] v[3]*h[2]\n```\n\nwhere v represents a neuron from the visible layer and h represents a neuron from the hidden layer. As a result, we get these values for our example:\n\n```\n0 0 0\n0 1 1\n0 1 1\n0 0 0\n```\n\nThis matrix is actually corresponding to all connections in this system, meaning that the first element can be observed as some kind of property or action on the connection between v[0] and h[0]. In fact, it is exactly that! Wherever we have value 1 in the matrix we add the learning rate to the weight of the connection between two neurons. So, in our example we will do so for connections between v[1]h[1], v[1]h[2], v[2]h[1] and v[2]h[2].Now, we are once again using formulas from [this article](https:\/\/rubikscode.net\/2018\/10\/01\/introduction-to-restricted-boltzmann-machines\/) to calculate probabilities for the neurons in the visible layer, using values from the hidden layer. Based on these probabilities we calculate the temporary Contrastive Divergence states for the visible layer \u2013 v'[n]. For example, we get the values [0, 0, 0, 1]. Finally, we calculate probabilities for the neurons in the hidden layer once again, only this time we use the Contrastive Divergence states of the visible layer calculated previously. We calculate the Contrastive Divergence states for the hidden layer \u2013  \u2013 h'[n], and for this example get the results [0,  0, 1].\n\nThis time we use the outer product of visible layer neuron Contrastive Divergence states [0, 0, 0, 1] and hidden layer neuron states [0,  0, 1] to get this so-called negative gradient:\n\n\n```\n0 0 0\n0 0 0\n0 0 0\n0 0 1\n```\n\nAs we described previously, first we calculate the possibilities for the hidden layer based on the input values and values of the weights and biases. Based on that probability, with the help of calculate_state function, we get the states of the hidden layer. After that probability for the visible layer is calculated,  and temporary Contrastive Divergence states for the visible layer are defined. Then the process is done for the Contrastive Divergence states of the hidden layer as well. Once this is performed we can calculate the positive and negative gradient and update the weights.\n\n[Excellent Implementation](https:\/\/github.com\/echen\/restricted-boltzmann-machines\/blob\/master\/rbm.py)\n\n\n\n","bee61c33":"## Lenet ,Proto Convolution networks\n\n[Lenet](https:\/\/paperswithcode.com\/method\/lenet)\nInspired by Fukushima\u2019s work on visual cortex modelling, using the simple\/complex cell hierarchy combined with supervised training and backpropagation lead to the development of the first CNN at University of Toronto in \u201888-\u201889 by Prof. Yann LeCun. The experiments used a small dataset of 320 \u2018mouser-written\u2019 digits. Performances of the following architectures were compared:\n\n- Single FC(fully connected) Layer\n- Two FC Layers\n- Locally Connected Layers w\/o shared weights\n- Constrained network w\/ shared weights and local connections\n- Constrained network w\/ shared weights and local connections 2 (more feature maps)\n\nThe most successful networks (constrained network with shared weights) had the strongest generalizability, and form the basis for modern CNNs. Meanwhile, singler FC layer tends to overfit.\n\n#### First \u201creal\u201d ConvNets at Bell Labs\n\nAfter moving to Bell Labs, LeCunn\u2019s research shifted to using handwritten zipcodes from the US Postal service to train a larger CNN:\n\n- 256 (16\u00d716) input layer\n- 12 5\u00d75 kernels with stride 2 (stepped 2 pixels): next layer has lower resolution\n- NO separate pooling\n- Convolutional network architecture with pooling\n\nThe next year, some changes were made: separate pooling was introduced. Separate pooling is done by averaging input values, adding a bias, and passing to a nonlinear function (hyperbolic tangent function). The 2\u00d72 pooling was performed with a stride of 2, hence reducing resolutions by half.\n\n<img src=\"https:\/\/atcold.github.io\/pytorch-Deep-Learning\/images\/week03\/03-2\/detailed_convNet.png\">\n\n\n#### Overall architecture breakdown\n\nGeneric CNN architecture can be broken down into several basic layer archetypes:\n\n- Normalisation\n    - Adjusting whitening (optional)\n    - Subtractive methods e.g. average removal, high pass filtering\n    - Divisive: local contrast normalisation, variance normalisation\n- Filter Banks\n    - Increase dimensionality\n    - Projection on overcomplete basis\n    - Edge detections\n- Non-linearities\n    - Sparsification\n    - Typically Rectified Linear Unit (ReLU):$ReLU(x)=max(x,0)$\n- Pooling\n    - Aggregating over a feature map\n    - Max Pooling: $MAX=Max_{i}\u200b(X_{i})$\n    - LP-Norm Pooling: $Lp=(\\sum_{i=1}^n \u2225X_{i}\u2225^p)^{1\/p}$ \n    - Log-Prob Pooling: $Prob=1\/b(\\sum_{i=1}^n e^{bX_{i}})$\n \n \n \n <img src=\"https:\/\/atcold.github.io\/pytorch-Deep-Learning\/images\/week03\/03-2\/various_input.gif\">\n \n#### Usefulness of CNNs\n\nCNNs are good for natural signals that come in the form of multidimensional arrays and have three major properties:\n\n- Locality: The first one is that there is a strong local correlation between values. If we take two nearby pixels of a natural image, those pixels are very likely to have the same colour. As two pixels become further apart, the similarity between them will decrease. The local correlations can help us detect local features, which is what the CNNs are doing. If we feed the CNN with permuted pixels, it will not perform well at recognizing the input images, while FC will not be affected. The local correlation justifies local connections.\n\n- Stationarity: Second character is that the features are essential and can appear anywhere on the image, justifying the shared weights and pooling. Moreover, statistical signals are uniformly distributed, which means we need to repeat the feature detection for every location on the input image.\n\n- Compositionality: Third character is that the natural images are compositional, meaning the features compose an image in a hierarhical manner. This justifies the use of multiple layers of neurons, which also corresponds closely with Hubel and Weisel\u2019s research on simple and complex cells.\n    ","64d3ef58":"## Variations of CNN architectures\n\n####  VGG16\n\n[VGG16](https:\/\/arxiv.org\/abs\/1409.1556) was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:\n\nThis network contains total 16 layers in which weights and bias parameters are learnt.\nA total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.\nThe number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).\nThe informative features are obtained by max pooling layers applied at different steps in the architecture.\nThe dense layers comprises of 4096, 4096, and 1000 nodes each.\nThe cons of this architecture are that it is slow to train and produces the model with very large size.\nThe VGG16 architecture is given below:\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png\">","52fd55aa":"## Building A Convolution Network\n\n<img src=\"https:\/\/i.ytimg.com\/vi\/9nGVp0j4KH4\/maxresdefault.jpg\">\n\n#### Convolution\n\nConvolutional neural networks, short for \u201cCNN\u201d, is a type of feed-forward artificial neural networks, in which the connectivity pattern between its neurons is inspired by the organization of the visual cortex system. The primary visual cortex (V1) does edge detection out of the raw visual input from the retina. The secondary visual cortex (V2), also called prestriate cortex, receives the edge features from V1 and extracts simple visual properties such as orientation, spatial frequency, and color. The visual area V4 handles more complicated object attributes. All the processed visual features flow into the final logic unit, inferior temporal gyrus (IT), for object recognition. The shortcut between V1 and V4 inspires a special type of CNN with connections between non-adjacent layers: Residual Net ([He, et al. 2016](http:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)) containing \u201cResidual Block\u201d which supports some input of one layer to be passed to the component two layers later.\n\n<img src=\"https:\/\/lilianweng.github.io\/lil-log\/assets\/images\/visual_cortex_system.png\">\n\nConvolution is a mathematical term, here referring to an operation between two matrices. The convolutional layer has a fixed small matrix defined, also called kernel or filter. As the kernel is sliding, or convolving, across the matrix representation of the input image, it is computing the element-wise multiplication of the values in the kernel matrix and the original image values. [Specially designed kernels](http:\/\/setosa.io\/ev\/image-kernels\/) can process images for common purposes like blurring, sharpening, edge detection and many others, fast and efficiently.\n\n<img src=\"https:\/\/lilianweng.github.io\/lil-log\/assets\/images\/lenet.png\">\n\n\n#### Kernel Convolutions\n\nKernel convolution is not only used in CNNs, but is also a key element of many other Computer Vision algorithms. It is a process where we take a small matrix of numbers (called kernel or filter), we pass it over our image and transform it based on the values from filter. Subsequent feature map values are calculated according to the following formula, where the input image is denoted by f and our kernel by h. The indexes of rows and columns of the result matrix are marked with m and n respectively.\n\n<img src=\"https:\/\/miro.medium.com\/max\/588\/1*EJBQVsY0g6XqBaGgU8hYfA.gif\">\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*32zCSTBi3giSApz1oQV-zA.gif\">\n\n#### Valid and Same Convolution\n\nWhen we perform convolution over the 6x6 image with a 3x3 kernel, we get a 4x4 feature map. This is because there are only 16 unique positions where we can place our filter inside this picture. Since our image shrinks every time we perform convolution, we can do it only a limited number of times, before our image disappears completely. What\u2019s more, if we look at how our kernel moves through the image we see that the impact of the pixels located on the outskirts is much smaller than those in the center of image. This way we lose some of the information contained in the picture. Below you can see how the position of the pixel changes its influence on the feature map.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*P1vkUXqHJbAbUQWDrxg8rA.gif\">\n\nTo solve both of these problems we can pad our image with an additional border. For example, if we use 1px padding, we increase the size of our photo to 8x8, so that output of the convolution with the 3x3 filter will be 6x6. Usually in practice we fill in additional padding with zeroes. Depending on whether we use padding or not, we are dealing with two types of convolution \u2014 Valid and Same. Naming is quite unfortunate, so for the sake of clarity: Valid \u2014 means that we use the original image, Same \u2014 we use the border around it, so that the images at the input and output are the same size. In the second case, the padding width, should meet the following equation, where p is padding and f is the filter dimension (usually odd).\n\n<img src=\"https:\/\/miro.medium.com\/max\/124\/1*RT4wahrezNLWl4DeqWDOjw.gif\">\n\nStrided Convolution\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*itcofCIVsGe7rBmciJcmVw.gif\">\n\n The dimensions of the output matrix - taking into account padding and stride - can be calculated using the following formula.\n \n<img src=\"https:\/\/miro.medium.com\/max\/323\/1*HoOLVDujrz1TKJJ6ijR6Bw.gif\">\n\n\nConvolution over volume is a very important concept, which will allow us not only to work with color images, but even more importantly to apply multiple filters within a single layer. The first important rule is that the filter and the image you want to apply it to, must have the same number of channels. Basically, we proceed very much like in the example from Figure 3, nevertheless this time we multiply the pairs of values from the three-dimensional space. If we want use multiple filters on the same image, we carry out the convolution for each of them separately, stack the results one on top of the other and combine them into a whole. The dimensions of the received tensor (as our 3D matrix can be called) meet the following equation, in which: n \u2014 image size, f \u2014 filter size, nc \u2014 number of channels in the image, p \u2014used padding, s \u2014 used stride, nf \u2014 number of filters.\n\n<img src=\"https:\/\/miro.medium.com\/max\/809\/1*oto8zpybb-uWB_7Uu7Nk8w.gif\">\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*Ukb2msCjU3G5eS4a45f-lg.png\">\n\n#### Parameter Sharing\n\nForm the below image we can see that not all neurons in the two consecutive layers are connected to each other. For example, unit 1 only affects the value of A. Secondly, we see that some neurons share the same weights. Both of these properties mean that we have much less parameters to learn.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*6S1ltWsTUIdULzRxqueWiA.gif\">\n\n\n\n\n","4f47ff62":"## Gradient Descent\n\nGradient descent is an optimization algorithm based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. Gradient Descent iteratively reduces a loss function by moving in the direction opposite to that of steepest ascent. It is dependent on the derivatives of the loss function for finding minima. uses the data of the entire training set to calculate the gradient of the cost function to the parameters which requires large amount of memory and slows down the process.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*I47BK0AUgESr4M3hlvUwnQ.png\">\n\n### Stochastic gradient descent\n\nStochastic gradient descent (SGD) in contrast performs a parameter update for each training example \n\n$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$$\n\nBatch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.\nSGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in Image:\n\n<img src=\"https:\/\/ruder.io\/content\/images\/2016\/09\/sgd_fluctuation.png\">\n\n\nWe can visualize what happens to a single weight $w$ in a cost function $C(w)$(same as J). Naturally, what happens is that we find the derivative of the parameter $\\theta$, which is $w$ in this case, and we update the parameter accordingly to the equation above.\n\n\n<img src=\"https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/gradient-descent-optimized--1-.gif\">\n\n\nWhile batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.\n\n### Mini-batch gradient descent\nMini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \nn training examples:\n\n$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$$\n\nThis way, it a) reduces the variance of the parameter updates, which can lead to more stable convergence; and b) can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used.\n\n### Momentum based approaches\n\nMomentum SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image\n\n<img src=\"https:\/\/ruder.io\/content\/images\/2015\/12\/without_momentum.gif\">\n\nMomentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Image. It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector:\n\n$$\\begin{align} \n\\begin{split} \nv_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\ \n\\theta &= \\theta - v_t \n\\end{split} \n\\end{align}$$\n\nThis [blog](https:\/\/distill.pub\/2017\/momentum\/) has a great description behind the intuition of momentums:\n\n<img src=\"https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/increasing-momentum.gif\">\n\n\n### Adagrad\nAdagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates\n(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data. Dean et al. have found that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, which -- among other things -- learned to recognize cats in Youtube videos. Moreover, Pennington et al. used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.\n\nPreviously, we performed an update for all parameters $\\theta$ at once as every parameter \n$\\theta_{i}$ used the same learning rate $\\eta$. As Adagrad uses a different learning rate for every parameter $\\theta_{i}$ at every time step t, we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we use $g_{t}$ to denote the gradient at time step $t$. $g_{t,i}$ is then the partial derivative of the objective function w.r.t. to the parameter $\\theta_{i}$ at time step {t}\n\n$$g_{t, i} = \\nabla_\\theta J( \\theta_{t, i} )$$\n\nThe SGD update for every parameter $\\theta_{i}$ at each time step $t$ then becomes:\n\n$$\\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t, i}$$\n\nIn its update rule, Adagrad modifies the general learning rate $\\eta$ at each time step $t$ for every parameter $\\theta_{i}$ based on the past gradients that have been computed for $\\theta$:\n\n$$\\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i}$$\n\n\nAs $G_{t}$contains the sum of the squares of the past gradients w.r.t. to all parameters $\\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product \u2299 between $G_{t}$ and $g_{t}$:\n\n$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}$$\n\n\n### Adadelta\n\nAdadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \nw.\n\nInstead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $E[g^{2}]t$ at time step $t$ then depends (as a fraction \n$\\gamma$ similarly to the Momentum term) only on the previous average and the current gradient:\n\n$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g^2_t$$\n\nWe set $\\gamma$ to a similar value as the momentum term, around 0.9. For clarity, we now rewrite our vanilla SGD update in terms of the parameter update vector $\\Delta \\theta_t$:\n\n$$\\begin{align} \n\\begin{split} \n\\Delta \\theta_t &= - \\eta \\cdot g_{t, i} \\\\ \n\\theta_{t+1} &= \\theta_t + \\Delta \\theta_t \\end{split} \n\\end{align}$$\n\nThe parameter update vector of Adagrad that we derived previously thus takes the form:\n\n$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}$$\n\nWe now simply replace the diagonal matrix $G_{t}$ with the decaying average over past squared gradients $E[g^{2}]t$\n$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t}$$\nAs the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:\n\n$$\\Delta \\theta_t = - \\dfrac{\\eta}{RMS[g]_{t}} g_t$$\n\n\n\n\n\n### RMS Prop\n\nRMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:\n\n$$\\begin{align} \n\\begin{split} \nE[g^2]_t &= 0.9 E[g^2]_{t-1} + 0.1 g^2_t \\\\ \n\\theta_{t+1} &= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t} \n\\end{split} \n\\end{align}$$\n\n[RMSprop](http:\/\/www.cs.toronto.edu\/~tijmen\/csc321\/slides\/lecture_slides_lec6.pdf) as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests $\\gamma$ to be set to 0.9, while a good default value for the learning rate \n$\\eta$ is 0.001.\n\n<img src=\"https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/rmsprop.gif\">\n\n\n### Adam\n\nAdaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \n$v_{t}$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients \n$m_{t}$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_{t}$ and $v_{t}$ respectively as follows:\n\n$$\\begin{align} \n\\begin{split} \nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ \nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \n\\end{split} \n\\end{align}$$\n\n$m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small \n\n<img src=\"https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/saddle.gif\">\n\n\n$$\\begin{align} \n\\begin{split} \n\\hat{m}_t &= \\dfrac{m_t}{1 - \\beta^t_1} \\\\ \n\\hat{v}_t &= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split} \n\\end{align}$$\n\nThey then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:\n$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n\n\n\nFor other optimizers \n\n- [NADAM,AMSGrad,Adamax](https:\/\/ruder.io\/optimizing-gradient-descent\/)\n- [Jason's blog](https:\/\/machinelearningmastery.com\/tour-of-optimization-algorithms\/)\n- [Adam](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/)\n\n<img src=\"https:\/\/ruder.io\/content\/images\/2016\/09\/contours_evaluation_optimizers.gif\">\n","1e99fc5d":"## Samples for GAN variations\n\n[Keras blog](https:\/\/keras.io\/examples\/generative\/)\n\n[PixelGAN](https:\/\/keras.io\/examples\/generative\/pixelcnn\/)\n\n[Stylegan](https:\/\/keras.io\/examples\/generative\/stylegan\/)  : Generative Adversarial Networks, or GANs for short, are effective at generating large high-quality images.\n\nMost improvement has been made to discriminator models in an effort to train more effective generator models, although less effort has been put into improving the generator models.\n\nThe Style Generative Adversarial Network, or StyleGAN for short, is an extension to the GAN architecture that proposes large changes to the generator model, including the use of a mapping network to map points in latent space to an intermediate latent space, the use of the intermediate latent space to control style at each point in the generator model, and the introduction to noise as a source of variation at each point in the generator model.\n\nThe resulting model is capable not only of generating impressively photorealistic high-quality photos of faces, but also offers control over the style of the generated image at different levels of detail through varying the style vectors and noise.\n\n<img src=\"https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2019\/06\/Summary-of-the-StyleGAN-Generator-Model-Architecture.png\">\n\n\nBlocks\n\n-  Baseline Progressive GAN\nThe StyleGAN generator and discriminator models are trained using the progressive growing GAN training method.\n\nThis means that both models start with small images, in this case, 4\u00d74 images. The models are fit until stable, then both discriminator and generator are expanded to double the width and height (quadruple the area), e.g. 8\u00d78.\n\nA new block is added to each model to support the larger image size, which is faded in slowly over training. Once faded-in, the models are again trained until reasonably stable and the process is repeated with ever-larger image sizes until the desired target image size is met, such as 1024\u00d71024.\n\n- Bilinear Sampling\nThe progressive growing GAN uses nearest neighbor layers for upsampling instead of transpose convolutional layers that are common in other generator models.\n\nThe first point of deviation in the StyleGAN is that bilinear upsampling layers are unused instead of nearest neighbor.\n\n- Mapping Network and AdaIN\nNext, a standalone mapping network is used that takes a randomly sampled point from the latent space as input and generates a style vector.\n\nThe mapping network is comprised of eight fully connected layers, e.g. it is a standard deep neural network.The style vector is then transformed and incorporated into each block of the generator model after the convolutional layers via an operation called adaptive instance normalization or AdaIN.\n\nThe AdaIN layers involve first standardizing the output of feature map to a standard Gaussian, then adding the style vector as a bias term.\n\n<img src=\"https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2019\/06\/Calculation-of-the-adaptive-instance-normalization-AdaIN-in-the-StyleGAN-300x55.png\">\n\n- Removal of Latent Point Input\nThe next change involves modifying the generator model so that it no longer takes a point from the latent space as input.\n\nInstead, the model has a constant 4x4x512 constant value input in order to start the image synthesis process.\n\n- Addition of Noise\nThe output of each convolutional layer in the synthesis network is a block of activation maps.\n\nGaussian noise is added to each of these activation maps prior to the AdaIN operations. A different sample of noise is generated for each block and is interpreted using per-layer scaling factors.\n\n- Mixing regularization\nMixing regularization involves first generating two style vectors from the mapping network.\n\nA split point in the synthesis network is chosen and all AdaIN operations prior to the split point use the first style vector and all AdaIN operations after the split point get the second style vector.\n\n[TF](https:\/\/github.com\/NVlabs\/stylegan)\n\n[Resource](https:\/\/github.com\/PacktPublishing\/Hands-On-Image-Generation-with-TensorFlow-2.0\/blob\/master\/Chapter07\/ch7_stylegan.ipynb)\n","15b44f3b":"## Backpropagation in Convolutions\n\nAs in the case of parametric Dense layers, chain partial derivatives are computed w.r.t weight tensors and the gradient is accumulated to successive levels.We want to assess the influence of the change in the parameters on the resulting features map, and subsequently on the final result. \n\n<img src=\"https:\/\/miro.medium.com\/max\/620\/1*3k9R6aUwekXXEHVrGbMitw.gif\">\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*1ma11FU4okzC-csMEAU0kQ.png\">\n\nOur task is to calculate $dW[l]$ and $db[l]$ - which are derivatives associated with parameters of current layer, as well as the value of $dA[l -1]$ - which will be passed to the previous layer. As shown in Figure 10, we receive the $dA[l]$ as the input. Of course, the dimensions of tensors dW and W, db and b as well as dA and A respectively are the same. The first step is to obtain the intermediate value $dZ[l]$ by applying a derivative of our activation function to our input tensor. According to the chain rule, the result of this operation will be used later.\n\n<img src=\"https:\/\/miro.medium.com\/max\/231\/1*UBilkUfJ6OCUpWAJlPsavg.gif\">\n\nNow, we need to deal with backward propagation of the convolution itself, and in order to achieve this goal we will utilise a matrix operation called full convolution \u2014 which is visualised below. Note that during this process we use the kernel, which we previously rotated by 180 degrees. This operation can be described by the following formula, where the filter is denoted by W, and $dZ[m,n]$ is a scalar that belongs to a partial derivative obtained from the previous layer.\n\n<img src=\"https:\/\/miro.medium.com\/max\/304\/1*soIA9mwFw1UMaU2M8eAGEA.gif\">\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*SYJpq-rOn6idEYbljdwT7w.gif\">\n\n#### BP in Pooling layers\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*qImgD2KGZw7ETjw3mOxNyg.gif\">\n\nBesides convolution layers, CNNs very often use so-called pooling layers. They are used primarily to reduce the size of the tensor and speed up calculations. This layers are simple - we need to divide our image into different regions, and then perform some operation for each of those parts. For example, for the Max Pool Layer, we select a maximum value from each region and put it in the corresponding place in the output. As in the case of the convolution layer, we have two hyperparameters available \u2014 filter size and stride. Last but not least, if you are performing pooling for a multi-channel image, the pooling for each channel should be done separately.\n\n\n\nIn this case, article we will discuss only max pooling backpropagation, but the rules that we will learn \u2014 with minor adjustments \u2014 are applicable to all types of pooling layers. Since in layers of this type, we don\u2019t have any parameters that we would have to update, our task is only to distribute gradients appropriately. As we remember, in the forward propagation for max pooling, we select the maximum value from each region and transfer them to the next layer. It is therefore clear that during back propagation, the gradient should not affect elements of the matrix that were not included in the forward pass. In practice, this is achieved by creating a mask that remembers the position of the values used in the first phase, which we can later utilize to transfer the gradients.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*jhW0G_vJsYCQpphu50yj2Q.gif\">\n","f5128418":"#### LSTMs\n\nFor LSTM networks from scratch please refer to the base implementation [here](https:\/\/github.com\/abhilash1910\/DeepGenerator) (inside deepgenerator script).\n\n\n#### LSTM- Long Short Term Memory\n\n[LSTMs](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/) are gated recurrent networks having 4 gates with (tanh\/sigmoid) activation units. These architectures are the the building blocks of all the transformer architectures that we see, and the 4 gates combine input from different time stamps to produce the output. In a LSTM, there are typically 3 input and output signals: The h (hidden cell output from the previous timestep), c (the signal from previous cell), and the x(input vectors). Outputs involve the updated ht+1(hidden cell output of current block) value, ct+1, (updated c signal from the present cell) and the output(o).\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png\">\n\n\n#### Operation Steps - LSTM\n\nThe first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at ht\u22121 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct\u22121. A 1 represents \u201ccompletely keep this\u201d while a 0 represents \u201ccompletely get rid of this.\u201d\n\nLet\u2019s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-f.png\"> \n\n\nThe next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.\n\nIn the example of our language model, we\u2019d want to add the gender of the new subject to the cell state, to replace the old one we\u2019re forgetting.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-i.png\">\n\nIt\u2019s now time to update the old cell state, Ct\u22121, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it\u2217C~t. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we\u2019d actually drop the information about the old subject\u2019s gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh (to push the values to be between \u22121 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that\u2019s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that\u2019s what follows next.\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-o.png\">\n\n\n\nResources:\n\n- [NLP](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-2)\n- [Paper](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43905.pdf)","20925778":"## AlexNet\n\nAlexNet is an important milestone in the visual recognition tasks in terms of available hardware utilization and several architectural choices. After its publication in 2012 by Alex Krizhevsky et al.[1] the popularity of deep learning and, in specific, the popularity of CNNs grew drastically. Below you can see the architecture of AlexNet:\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*hhd-belzmau_TMcF.png\">\n\nAfter the big success of LeNet in handwritten digit recognition, computer vision applications using deep learning came to a halt. Instead of feature extraction methods by filter convolutions, researchers preferred more handcrafted image processing tasks such as wavelets, Gabor filters, and many more. There were several reasons why the community did not push CNNs for many years to undertake more complex tasks. First of all, LeNet was good for MNIST but very few people believed that it is capable of dealing with more challenging data. After a while, it was not very interesting to work with MNIST over and over again. This links us to the second reason which was the lack of annotated data. Training neural networks requires an abundance of data and the need for more data increases as the model complexity (model size) increases. Since it is expensive to annotate data and CNNs were not really popular, large-scale datasets were not available at the time. Last but not least, training large models (assuming enough training data is provided) is a computationally expensive task. Each added layer of a deep learning model increases the number of parameters to be trained which significantly slows down the training. The hardware used in the \u201990s took huge amounts of time to train even basic models and it was not feasible to design larger models.\nAll those reasons prevented new breakthroughs in the deep learning community for a long time. However, the last decade witnessed many exciting new projects and it feels like we are just getting started. In 2009, the ImageNet dataset is released with the title of ImageNet: A large-scale hierarchical image database. Initially having 3.2 million annotated images, ImageNet now has more than 14 million images that are labeled in more than 20 thousand categories. That much of data encouraged many groups worldwide and excited them for the ImageNet Large Scale Visual Recognition Challenge (ILSRVC) which took place between 2010 and 2017. On the other hand, improvements were accelerated in hardware at the dawn of the new millennium. NVIDIA released the first commercial GPU GeForce 256 in 1999 and GPU technology was first used for deep learning in 2006 by Kumar Chellapilla et al. Later on in 2009, Raina et al. took one step further for the popularity of GPUs on deep learning.\nHowever, most of the credit goes to AlexNet in terms of the prevalence of GPU supported computation in deep learning literature. Winning the ILSRVC in 2012 by a quite large margin compared to the previous year\u2019s winner, AlexNet proved that from now on, it is a necessity to employ state-of-the-art hardware for training the state-of-the-art models for the most complex task. AlexNet had more layers than LeNet has which brings a greater learning capacity. It is originally trained on the ImageNet dataset.","4fdecfd7":"### Creating a Dense Network\n\nIn this case, we are creating a Dense neural perceptron.Multi-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let\u2019s start with the building block of this network which is a perceptron. Think of perceptron\/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.\n\n<img src=\"https:\/\/miro.medium.com\/max\/501\/0*BcQ26EY75dXdztUF.png\">\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/874\/0*HB2hrDZDqs8l5KFA.jpeg\">","315baf17":"## AutoEncoder \n\n\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n\n1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n\n2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n\n3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n\nTo build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding\/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It's simple! And you don't even need to understand any of these words to start using autoencoders in practice.\n\n#### Are they good at data compression?\n\nUsually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: you can only use them on data that is similar to what they were trained on, and making them more general thus requires lots of training data. But future advances might change this, who knows.\n\n#### What are autoencoders good for?\n\nThey are rarely used in practical applications. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks , but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning .\n\nToday two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\nFor 2D visualization specifically, t-SNE (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32-dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on Github. Otherwise scikit-learn also has a simple and practical implementation.\n\nSo what's the big deal with autoencoders?\nTheir main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n\nOtherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts \"invented\" by humans such as \"dog\", \"car\"...). In fact, one may argue that the best features in this regard are those that are the worst at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).\n\nIn self-supervized learning applied to vision, a potentially fruitful alternative to autoencoder-style input reconstruction is the use of toy tasks such as jigsaw puzzle solving, or detail-context matching (being able to match high-resolution but small patches of pictures with low-resolution versions of the pictures they are extracted from). The following paper investigates jigsaw puzzle solving and makes for a very interesting read: Noroozi and Favaro (2016) Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. Such tasks are providing the model with built-in assumptions about the input data which are missing in traditional autoencoders, such as \"visual macro-structure matters more than pixel-level details\".\n\n<img src=\"https:\/\/blog.keras.io\/img\/ae\/autoencoder_schema.jpg\">\n\n- [Keras Blog](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n","58ad4762":"## Inception\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*uW81y16b-ptBDV8SIT1beQ.png\">\n\nIt performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*DKjGRDd_lJeUfVlY50ojOA.png\">\n\ndeep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*U_McJnp7Fnif-lw9iIC5Bw.png\">\n\n### Inception v4 Resnet\n\nInception v4 and Inception-ResNet were introduced in the same [paper](https:\/\/arxiv.org\/pdf\/1602.07261.pdf).\n\nThe Premise\n\n- Make the modules more uniform. The authors also noticed that some of the modules were more complicated than necessary. - This can enable us to boost performance by adding more of these uniform modules.\n\nThe Solution\n\n- The \u201cstem\u201d of Inception v4 was modified. The stem here, refers to the initial set of operations performed before introducing the Inception blocks.\n\n<img src=\"https:\/\/miro.medium.com\/max\/808\/1*cYjhQ05zLXdHn363TsPrLQ.jpeg\">\n\n- They had three main inception modules, named A,B and C (Unlike Inception v2, these modules are infact named A,B and C). They look very similar to their Inception v2 (or v3) counterparts.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*KrBAIZjcrlXu6JPiPQj2vQ.jpeg\">\n\n- Inception v4 introduced specialized \u201cReduction Blocks\u201d which are used to change the width and height of the grid. The earlier versions didn\u2019t explicitly have reduction blocks, but the functionality was implemented.\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*2Hdo2wG3ILUoYaorJdlR-Q.jpeg\">\n\n\n### Inception-Resnet v1\/v2\n\nInspired by the performance of the ResNet, a hybrid inception module was proposed. There are two sub-versions of Inception ResNet, namely v1 and v2. Before we checkout the salient features, let us look at the minor differences between these two sub-versions.\n\n- Inception-ResNet v1 has a computational cost that is similar to that of Inception v3.\n- Inception-ResNet v2 has a computational cost that is similar to that of Inception v4.\n- They have different stems, as illustrated in the Inception v4 section.\n- Both sub-versions have the same structure for the modules A, B, C and the reduction blocks. Only difference is the hyper-parameter settings. \n\nFor residual addition to work, the input and output after convolution must have the same dimensions. Hence, we use 1x1 convolutions after the original convolutions, to match the depth sizes (Depth is increased after convolution).\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*WyqyCKA4mP1jsl8H4eHrjg.jpeg\">\n\nThe pooling operation inside the main inception modules were replaced in favor of the residual connections. However, you can still find those operations in the reduction blocks. Reduction block A is same as that of Inception v4\n\n<img src=\"https:\/\/miro.medium.com\/max\/3000\/1*QY-g6oMF_6-v7N668HNvvA.jpeg\">\n\n\nNetworks with residual units deeper in the architecture caused the network to \u201cdie\u201d if the number of filters exceeded 1000. Hence, to increase stability, the authors scaled the residual activations by a value around 0.1 to 0.3.\n\n<img src=\"https:\/\/miro.medium.com\/max\/318\/1*UJONYV4aJqfpkH8LoV9vwg.png\">\n\nThe original paper didn\u2019t use BatchNorm after summation to train the model on a single GPU (To fit the entire model on a single GPU).\nIt was found that Inception-ResNet models were able to achieve higher accuracies at a lower epoch.\nThe final network layout for both Inception v4 and Inception-ResNet are as follows:\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*Ed8AfmerIrBtNgsTFZs--A.jpeg\">\n\nResources:\n\n- [Blog](https:\/\/miro.medium.com\/max\/875\/1*Ed8AfmerIrBtNgsTFZs--A.jpeg)\n- [Jason](https:\/\/machinelearningmastery.com\/how-to-implement-major-architecture-innovations-for-convolutional-neural-networks\/)"}}