{"cell_type":{"b1798278":"code","25a6a070":"code","65502e81":"code","9da4d596":"code","a0aacf58":"code","4cf6fe23":"code","2eaff482":"code","bfb04f9e":"code","f609d8a7":"code","0168e34b":"code","5f454e22":"code","6cf425a6":"code","d9dcc5eb":"code","6565a388":"code","394bece4":"code","1f841b43":"code","0de80cb4":"code","693d50d5":"code","90ce13df":"code","9d131059":"code","352ccf67":"code","eecb4d44":"code","b68c4395":"code","bfd934ae":"code","5e79a35f":"code","8871224d":"code","12034065":"code","29aa9fa2":"code","d79014a4":"code","7e278b64":"code","44cf7e6e":"code","330eab62":"code","4d6817ff":"code","df07d115":"code","66835efa":"code","e31554ab":"code","240af662":"code","f37be371":"code","1ed9ee0d":"code","a266fef9":"code","d986abf6":"code","e87ac617":"code","fc85da6f":"code","bae3389d":"code","2abb901e":"code","37922cdb":"code","92e4ca8a":"markdown","c2184590":"markdown","cdb30a07":"markdown","c1ccac1c":"markdown","ad3347bc":"markdown","cb3fe792":"markdown","090a363e":"markdown","83ad46b6":"markdown","0202295c":"markdown","5d1f39ad":"markdown","b7bbccb8":"markdown"},"source":{"b1798278":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25a6a070":"\n!pip install gensim\n!pip install nltk","65502e81":"!pip install ftfy","9da4d596":"import random\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nfrom tqdm import tqdm as tqdm\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport io\n\n\n\n","a0aacf58":"import ftfy\nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument","4cf6fe23":"import tensorflow as tf\nfrom tensorflow.keras import Sequential, regularizers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.python.framework import ops\nfrom tensorflow.keras.callbacks import EarlyStopping","2eaff482":"\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve","bfb04f9e":"train = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\") ","f609d8a7":"import pandas_profiling as pp\nimport seaborn as sns\nimport warnings\nimport os\n\nwarnings.filterwarnings('ignore')","0168e34b":"train.info()","5f454e22":"train.describe()","6cf425a6":"train.shape","d9dcc5eb":"pp.ProfileReport(train)","6565a388":"train.drop_duplicates(subset='review', keep='first',inplace=True)","394bece4":"def prepros(text):\n    txt=text.replace(\"<br \/>\",\" \") #retirando tags\n    txt=fix_text(txt) #consertando Mojibakes (Ver https:\/\/pypi.org\/project\/ftfy\/)\n    txt=txt.lower() #passando tudo para min\u00fasculo\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontua\u00e7\u00e3o\n    txt=txt.replace(\" \u2014 \", \" \") #retirando h\u00edfens\n    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os n\u00fameros\n    txt=re.sub(' +', ' ', txt) #deletando espa\u00e7os extras\n    return txt","1f841b43":"X_train = train['review']","0de80cb4":"X_train.shape\n","693d50d5":"x_prep=X_train.apply(prepros)","90ce13df":"x_prep.head()","9d131059":"x_doc=x_prep.iloc[:].tolist()","352ccf67":"x_doc=[word.split() for word in x_doc]","eecb4d44":"x_doc[:5]","b68c4395":"d2v = Doc2Vec.load('..\/input\/sentiment-analysis-pmr3508\/doc2vec')","bfd934ae":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=20)\n    \n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","5e79a35f":"%%time\n\ntrainx = [emb(x, d2v) for x in tqdm(x_doc)] \ntrainx = np.array(trainx)","8871224d":"!pip install pycaret","12034065":"%%time\ntrainy = np.array(train.loc[:,'positive'].tolist())\nmlp = MLPClassifier(early_stopping=True)\nparameters= {'hidden_layer_sizes': [x for x in range(10, 101, 10)], 'alpha':[0.001,0.01,0.1,1]}\n\nA = GridSearchCV(mlp, parameters, n_jobs=4, verbose=1, scoring=\"roc_auc\")\nA.fit(trainx, trainy)","29aa9fa2":"\nmd1 = MLPClassifier(early_stopping=True, alpha=A.best_params_[\"alpha\"],hidden_layer_sizes=A.best_params_[\"hidden_layer_sizes\"])\nmd1.fit(trainx, trainy)","d79014a4":"%%time\n\nsizes = [(x,y) for x in range(40, 101, 10) for y in range(20, x, 10)]\n\nmlp_b = MLPClassifier(random_state=40,early_stopping=True)\nparameters= {'hidden_layer_sizes': sizes,'alpha': [0.001,0.01,0.1],'learning_rate': ['constant','adaptive']}\n\n\n\nmlp_b = RandomizedSearchCV(mlp_b, parameters, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_b.fit(trainx, trainy)","7e278b64":"search_mlp.best_score_","44cf7e6e":"search_mlp.best_params_, search_mlp.best_score_\n\nB = MLPClassifier(random_state=40,early_stopping=True, alpha=search_mlp.best_params_[\"alpha\"], hidden_layer_sizes=search_mlp.best_params_[\"hidden_layer_sizes\"])\nB.fit(trainx, trainy)","330eab62":"from pycaret.nlp import *","4d6817ff":"train.head()","df07d115":"X_train_train, X_train_validation, Y_train_train, Y_train_validation = train_test_split(trainx, trainy, test_size=0.25, random_state=5)\n\nprint(X_train_train.shape, X_train_validation.shape, Y_train_train.shape, Y_train_validation.shape)","66835efa":"n_iter=100\nn_features=X_train_train.shape[1]\nneurons=[]\nreg=[]\n\n#Sorteando valores\nfor i in range(n_iter):\n    h1=random.randrange(25, 100, 1)\n    h2=random.randrange(20, h1, 1)\n    neurons.append((h1, h2))\n    \n    l1=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    l2=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    reg.append((l1, l2))\n    \n#DataFrame\nparams = {'neurons': neurons, 'reg': reg, 'epochs': n_iter*[None], 'auc': n_iter*[None]}\nparams = pd.DataFrame(params)\nparams = params[['neurons', 'reg', 'epochs', 'auc']]\n\nparams.head()","e31554ab":"def create_model(neurons=(10,10), reg=(.001, .001)):\n    \n    ops.reset_default_graph() #\u00e9 importante resetar os grafos das redes neurais j\u00e1 criadas para n\u00e3o tornam o processo muito lento \n    \n    #Criando modelo\n    model = Sequential()\n    model.add(Dense(neurons[0], input_shape=(n_features,), activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(neurons[1], activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    model.add(Dense(1, activation='sigmoid', \n                    kernel_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1]), \n                    bias_regularizer=regularizers.l1_l2(l1=reg[0], l2=reg[1])))\n    \n    #Compilando\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    return model","240af662":"%%time\nfor j in tqdm(range(n_iter)):\n    model = create_model(params.loc[j,'neurons'], params.loc[j,'reg'])\n    \n    es = EarlyStopping(monitor='val_auc', patience=10)\n    \n    history = model.fit(X_train_train, Y_train_train, \n                        validation_split=.25,\n                        epochs=50, \n                        batch_size=100, \n                        shuffle=True, \n                        verbose=False,\n                        callbacks=[es]) \n    \n    params.loc[j,'epochs'] = len(history.history['val_auc'])\n    params.loc[j,'auc'] = history.history['val_auc'][-1]\n\n","f37be371":"hyper = params.iloc[np.argsort(params.loc[:,'auc']),:]\nhyper.tail(10)","1ed9ee0d":"#Melhores valores\nneurons = params.iloc[-1,0]\nreg = params.iloc[-1,1]\nepochs = params.iloc[-1,2]\n\n#Criando modelo\nmlp_tf = create_model(neurons, reg)\n\n#Treinando modelo\nmlp_tf.fit(X_train_train, Y_train_train, \n                validation_split=0,\n                epochs=epochs, \n                batch_size=100, \n                shuffle=True, \n                verbose=True)","a266fef9":"test = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\").drop_duplicates(subset='review', keep='first')","d986abf6":"X_val = test.loc[:,'review'].tolist()\ny_val = np.array(test.loc[:,'positive'].tolist())\n\nX_val = [prepros(x) for x in tqdm(X_val)]\nX_val = [x.split() for x in X_val]\n\nX_val = [emb(x, d2v) for x in tqdm(X_val)] \nX_val = np.array(X_val)","e87ac617":"from sklearn.metrics import roc_auc_score\n\naucs = [0,0,0]\naucs[0] = roc_auc_score(y_val, A.predict_proba(X_val)[:,1])\naucs[1] = roc_auc_score(y_val, B.predict_proba(X_val)[:,1])\naucs[2] = roc_auc_score(y_val, mlp_tf.predict(X_val).squeeze())\n\nprint('Resultado das comparacoes')\nprint(\"AUC SciKit 1 camada:\", aucs[0])\nprint(\"AUC SciKit 2 camadas:\", aucs[1])\nprint(\"AUC Tensorflow:\", aucs[2])","fc85da6f":"submission = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\")\nsubmission.head()","bae3389d":"sub_x = submission.loc[:, \"review\"].tolist()\n\n# cleaning data\nsub_x = [prepros(x) for x in tqdm(sub_x)]\nsub_x = [x.split() for x in tqdm(sub_x)]\n\nsub_x = [emb(x, d2v) for x in tqdm(sub_x)]\nsub_x = np.array(sub_x)","2abb901e":"melhormod = np.argmax(aucs)\npredict = []\n\nif melhormod == 0:\n    predict = A.predict_proba(sub_x)[:,1]\nelif melhormod == 1:\n    predict = B.predict_proba(sub_x)[:,1]\nelse:\n    predict = mlp_tf.predict(sub_x).squeeze()\n\noutput = pd.DataFrame({'positive': predict})\noutput.head()","37922cdb":"output.to_csv(\"submission.csv\", index = True, index_label = 'Id')","92e4ca8a":"Processamento dos textos\n","c2184590":"Implementacao de redes neurais com uma e duas camadas","cdb30a07":"Importacao dos dados de treino","c1ccac1c":"# Implementacao das redes neurais e comparativo entre modelos","ad3347bc":"Visao Geral da Base de dados","cb3fe792":"ML libs e NLP","090a363e":"Importacao das libs","83ad46b6":"Comparacao e envio para submissao","0202295c":"Duas Camadas","5d1f39ad":"Rede Neural 3B","b7bbccb8":"Pre-Processamento E treinamento dos dados"}}