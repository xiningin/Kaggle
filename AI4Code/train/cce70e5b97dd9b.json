{"cell_type":{"ca174085":"code","e79ab5e9":"code","107f21bd":"code","ce722be8":"code","f7ce4eba":"code","fb3cd4db":"code","00ca1ea3":"code","63908e73":"code","67f0f7e9":"code","7960dbc4":"code","7ff71162":"code","19fbafe0":"code","3b6c54b2":"code","5bba5d86":"code","f7ec7352":"code","6596405a":"code","e35cc410":"code","80f00bfe":"code","42311c56":"code","3c4e55d9":"code","3fa0a87d":"code","136caec9":"code","09b6bed8":"code","c704ba79":"code","0761b20a":"code","259d85a0":"code","a6abb353":"code","7f625847":"code","8945cf50":"code","1f2af0c2":"code","1ef79ea4":"code","c4109ce3":"code","060dab96":"code","a3d613ef":"code","0347b81a":"code","8243f682":"code","7c7eb846":"code","ea1575ab":"code","7320e12d":"code","b8da99a4":"code","925f4f30":"code","d01ecbab":"code","124c9880":"markdown","228f7661":"markdown","37ed75a7":"markdown","2c4aa972":"markdown","8b90a5e9":"markdown","b5ccbf00":"markdown","ba4d17e5":"markdown","605d87f0":"markdown","63eada70":"markdown","2402a4eb":"markdown","622fd3a6":"markdown","12fa13a6":"markdown","f75c36b6":"markdown","c8d56952":"markdown","cfa44b4b":"markdown","e93b69ca":"markdown","5a5bada0":"markdown","237488b3":"markdown","883e2444":"markdown","0eafcf8e":"markdown","c34ed203":"markdown"},"source":{"ca174085":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(200,200)})\nsns.set(font_scale=3)\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"..\/input\"))\n\n","e79ab5e9":"%%time\ntrain = load_df()\ntest = load_df(\"..\/input\/test.csv\")","107f21bd":"train.describe(include='all')","ce722be8":"train.head()","f7ce4eba":"print(len(train.columns))\nprint(train.columns)","fb3cd4db":"sns.set(rc={'figure.figsize':(50,50)})","00ca1ea3":"train[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float')\ntotal_rev_by_visid = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(total_rev_by_visid.shape[0]), np.sort(np.log1p(total_rev_by_visid[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","63908e73":"nzi = pd.notnull(train[\"totals.transactionRevenue\"]).sum()\nnzr = (total_rev_by_visid[\"totals.transactionRevenue\"]>0).sum()\nprint(\"Number of instances in train set with non-zero revenue : \", nzi, \" and ratio is : \", nzi \/ train.shape[0] * 100 , \"%\")\nprint(\"Number of unique customers with non-zero revenue : \", nzr, \"and the ratio is : \", nzr \/ total_rev_by_visid.shape[0] * 100 , \"%\")","67f0f7e9":"print(\"Number of unique visitors in train set : \",train.fullVisitorId.nunique(), \" out of rows : \",train.shape[0])\nprint(\"Number of unique visitors in test set : \",test.fullVisitorId.nunique(), \" out of rows : \",test.shape[0])\nprint(\"Number of common visitors in train and test set : \",len(set(train.fullVisitorId.unique()).intersection(set(test.fullVisitorId.unique())) ))","7960dbc4":"const_cols = [c for c in train.columns if train[c].nunique(dropna=False)==1 ]\nconst_cols","7ff71162":"imp_columns = set(train.columns.tolist()) - set(const_cols)","19fbafe0":"imp_columns","3b6c54b2":"np.set_printoptions(suppress=True)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","5bba5d86":"train[\"totals.pageviews\"] = train[\"totals.pageviews\"].astype(\"float\")\ntotal_rev_by_visid = train.groupby(\"totals.pageviews\")[\"totals.transactionRevenue\"].agg([\"count\",\"sum\",\"mean\",\"size\"])\ntotal_rev_by_visid.reset_index(inplace=True)\ntotal_rev_by_visid[\"bin\"] = total_rev_by_visid[\"totals.pageviews\"] \/\/ int(5)","f7ec7352":"sns.set(font_scale=3)\ntotal_rev_by_pageview_bin = total_rev_by_visid.groupby(\"bin\")[\"mean\"].agg([\"mean\"])\ntotal_rev_by_pageview_bin.reset_index(inplace=True)\ntotal_rev_by_pageview_bin[\"bin\"] = total_rev_by_pageview_bin[\"bin\"].astype(int)\nsns.barplot(x=total_rev_by_pageview_bin[\"bin\"],y=total_rev_by_pageview_bin[\"mean\"])\n","6596405a":"total_rev_by_pageview_bin = total_rev_by_visid.groupby(\"bin\")[\"sum\"].agg([\"sum\"])\ntotal_rev_by_pageview_bin.reset_index(inplace=True)\ntotal_rev_by_pageview_bin[\"bin\"] = total_rev_by_pageview_bin[\"bin\"].astype(int)\n# total_rev_by_pageview_bin.plot(kind=\"bar\")\nsns.barplot(x=total_rev_by_pageview_bin[\"bin\"],y=total_rev_by_pageview_bin[\"sum\"])","e35cc410":"total_rev_by_pageview_bin = total_rev_by_visid.groupby(\"bin\")[\"count\"].agg([\"sum\"])\ntotal_rev_by_pageview_bin.reset_index(inplace=True)\ntotal_rev_by_pageview_bin[\"bin\"] = total_rev_by_pageview_bin[\"bin\"].astype(int)\n# total_rev_by_pageview_bin.plot(kind=\"bar\")\nsns.barplot(x=total_rev_by_pageview_bin[\"bin\"],y=total_rev_by_pageview_bin[\"sum\"])","80f00bfe":"train[\"channelGrouping\"].value_counts()","42311c56":"rev_by_channelgrouping = train.groupby(\"channelGrouping\")[\"totals.transactionRevenue\"].agg([\"count\",\"size\",\"mean\",\"sum\"])\nrev_by_channelgrouping.reset_index(inplace=True)","3c4e55d9":"sns.barplot(x=rev_by_channelgrouping[\"channelGrouping\"],y=rev_by_channelgrouping[\"mean\"])","3fa0a87d":"sns.barplot(x=rev_by_channelgrouping[\"channelGrouping\"],y=rev_by_channelgrouping[\"sum\"])","136caec9":"sns.barplot(x=rev_by_channelgrouping[\"channelGrouping\"],y=rev_by_channelgrouping[\"count\"])","09b6bed8":"total_rev_by_visid = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()","c704ba79":"total_rev_by_visid_non_zero = total_rev_by_visid[total_rev_by_visid[\"totals.transactionRevenue\"] > 0]","0761b20a":"non_zero_rev_vistor_id = total_rev_by_visid_non_zero.fullVisitorId.tolist()","259d85a0":"len(non_zero_rev_vistor_id)","a6abb353":"rev_by_channelgrouping_non_zero_rev_vistors = train[train[\"fullVisitorId\"].isin(non_zero_rev_vistor_id)].groupby(\"channelGrouping\")[\"totals.transactionRevenue\"].agg([\"count\",\"size\",\"mean\",\"sum\"])\nrev_by_channelgrouping_non_zero_rev_vistors.reset_index(inplace=True)","7f625847":"sns.barplot(x=rev_by_channelgrouping_non_zero_rev_vistors[\"channelGrouping\"],y=rev_by_channelgrouping_non_zero_rev_vistors[\"mean\"])","8945cf50":"sns.barplot(x=rev_by_channelgrouping_non_zero_rev_vistors[\"channelGrouping\"],y=rev_by_channelgrouping_non_zero_rev_vistors[\"sum\"])","1f2af0c2":"sns.barplot(x=rev_by_channelgrouping_non_zero_rev_vistors[\"channelGrouping\"],y=rev_by_channelgrouping_non_zero_rev_vistors[\"size\"])","1ef79ea4":"transaction_rev_by_device_browser = train.groupby(\"device.browser\")[\"totals.transactionRevenue\"].agg([\"sum\",\"mean\",\"count\",\"size\"]).reset_index()","c4109ce3":"transaction_rev_by_device_browser_non_zero_sum = transaction_rev_by_device_browser[transaction_rev_by_device_browser[\"sum\"] > 0].sort_values(by=[\"sum\",\"mean\",\"count\",\"size\"])","060dab96":"transaction_rev_by_device_browser_non_zero_sum[\"buy_ratio\"] = transaction_rev_by_device_browser_non_zero_sum[\"count\"].astype(\"float\") * 100 \/ transaction_rev_by_device_browser_non_zero_sum[\"size\"].astype(\"float\")","a3d613ef":"transaction_rev_by_device_browser_non_zero_sum","0347b81a":"imp_columns","8243f682":"print(\"Variables not in test but in train : \", set(train.columns).difference(set(test.columns)))","7c7eb846":"cols_to_drop = const_cols + ['sessionId']\n\ntrain = train.drop(cols_to_drop + [\"trafficSource.campaignCode\"], axis=1)\ntest = test.drop(cols_to_drop, axis=1)\n","ea1575ab":"train['date'] = pd.to_datetime(train['date'],format=\"%Y%m%d\")","7320e12d":"# Impute 0 for missing target values\nimport datetime\nfrom sklearn import preprocessing\ntrain[\"totals.transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train[\"totals.transactionRevenue\"].values\ntrain_id = train[\"fullVisitorId\"].values\ntest_id = test[\"fullVisitorId\"].values\n\n\n# label encode the categorical variables and convert the numerical variables to float\ncat_cols = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n            \"trafficSource.adwordsClickInfo.adNetworkType\", \n            \"trafficSource.adwordsClickInfo.gclId\", \n            \"trafficSource.adwordsClickInfo.page\", \n            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n            \"trafficSource.keyword\", \"trafficSource.medium\", \n            \"trafficSource.referralPath\", \"trafficSource.source\",\n            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\nfor col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))\n\n\nnum_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \nfor col in num_cols:\n    train[col] = train[col].astype(float)\n    test[col] = test[col].astype(float)\n\n# Split the train dataset into development and valid based on time \ndev_df = train[train['date']<=datetime.date(2017,5,31)]\nval_df = train[train['date']>datetime.date(2017,5,31)]\ndev_y = np.log1p(dev_df[\"totals.transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n\ndev_X = dev_df[cat_cols + num_cols] \nval_X = val_df[cat_cols + num_cols] \ntest_X = test[cat_cols + num_cols] ","b8da99a4":"# custom function to run light gbm model\nimport lightgbm as lgb\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y\n\n# Training the model #\npred_test, model, pred_val = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n","925f4f30":"from sklearn import metrics\npred_val[pred_val<0] = 0\nval_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\nval_pred_df[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\nval_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n#print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))\nval_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\nprint(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))","d01ecbab":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"baseline_lgb.csv\", index=False)","124c9880":"Now let us build a baseline model on this dataset. Before we start building models, let us look at the variable names which are there in train dataset and not in test dataset.","228f7661":"^^ This looks very interesting , the sum of transaction for chrome is higher and the buy ratio is also higher ","37ed75a7":"Now let us prepare the submission file similar to validation set.","2c4aa972":"^^ The above charts show that there the total rev is dependent on the channel","8b90a5e9":"\n### Lets look at how each bin of page views contribute to mean total transaction value","b5ccbf00":"## Lets Look at device related stats","ba4d17e5":"### Lets look at the same for just visitor with non zero revenue","605d87f0":"## Lets look at the \"channelGrouping\" column ","63eada70":"# Lets look at each of above variables and their relation to total trasaction values","2402a4eb":"\n\nNow let us create development and validation splits based on time to build the model. We can take the last two months as validation sample.\n","622fd3a6":"^^ The graph is similar compared to all visitors","12fa13a6":"# Lets load the data and peek through","f75c36b6":"### Looking at the above plots the number of pages views has a influance in terms of total trasaction value , Lets move further","c8d56952":"### The above plots proves us that the most transaction value comes from only few of users","cfa44b4b":"# Given that the data has some json fields , lets use a json function ","e93b69ca":"### Relation between bins of page views and sum of their transaction value","5a5bada0":"## Lets look into the page views vs total transaction","237488b3":"Now let us compute the evaluation metric on the validation data as mentioned in [this](https:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction\/discussion\/66737) new discussion thread. So we need to do a sum for all the transactions of the user and then do a log transformation on top. Let us also make the values less than 0 to 0 as transaction revenue can only be 0 or more.","883e2444":"### Baseline ","0eafcf8e":"\n\nSo apart from target variable, there is one more variable \"trafficSource.campaignCode\" not present in test dataset. So we need to remove this variable while building models. Also we can drop the constant variables which we got earlier.\n\nAlso we can remove the \"sessionId\" as it is a unique identifier of the visit.\n","c34ed203":"# Lets look into the total trasactions per vistor "}}