{"cell_type":{"886633f7":"code","0ccaf9c5":"code","e3ce9358":"code","6a34f600":"code","bff4d9d1":"code","910c27a9":"code","64d34ddd":"code","30ef8a5f":"code","5c71db8b":"code","e880753e":"code","681a1de2":"code","836b2ea3":"code","7e21dbcf":"code","51ca9095":"code","a6464698":"code","0771d3cb":"code","3910c128":"code","cc4e287a":"code","f0fb82b5":"code","7df3d972":"code","d957f982":"code","b8340c41":"code","50d8ecf8":"code","75bec638":"code","5728fe08":"code","e5b64db0":"code","c8bfbd4d":"markdown","93c4ad39":"markdown","c82893cb":"markdown","551f1395":"markdown","bfbabd09":"markdown","660041cc":"markdown","324276da":"markdown","21e72c0e":"markdown","ac78e63f":"markdown","b593576f":"markdown","c3db0e9e":"markdown","4a8c7902":"markdown","93b850d9":"markdown","a6f4eb0b":"markdown","3172bc4e":"markdown","d0c5b452":"markdown","4f0cad97":"markdown","ba3bec30":"markdown","65104769":"markdown","e5d53e82":"markdown","477fdc9e":"markdown","5507affc":"markdown"},"source":{"886633f7":"import os, warnings, folium\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom geopy.distance import great_circle\nfrom shapely.geometry import MultiPoint\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import pairwise_distances, mean_squared_error\nfrom sklearn.preprocessing import minmax_scale, MultiLabelBinarizer\nfrom sklearn.decomposition import NMF\nfrom random import randint\nfrom download_csv_file import create_download_link\nfrom ml_metrics import mapk, apk\n\n# Pandas and Numpy configs\npd.set_option('display.max_columns', 20)\nnp.set_printoptions(suppress=True)\n\n# Input data files are available in the \"..\/input\/\" directory.\nmain_path = '..\/input'\n    \n# Init ploty in offline mode\nplot_template = 'plotly_white'\n\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(main_path))","0ccaf9c5":"### Path of file to read\ndata_file_path = f'{main_path}\/flickr_london\/london_20k.csv'\n\n# Change data types\ndata_type = {\n    'photo_id': 'object',\n    'owner': 'object',\n    'faves': 'float16',\n    'lat': 'float32',\n    'lon': 'float32',\n    'taken': 'datetime64'\n}\n\n# Read file into a variable data\nraw = pd.read_csv(data_file_path, \n                  engine='python', \n                  sep=',', \n                  encoding='utf-8', \n                  dtype=data_type, \n                  decimal=',')\ndata_dim = raw.shape\n\nprint(f'Dataframe dimentions: {data_dim}', f'\\n{\"-\"*50}\\nData Types:\\n{raw.dtypes}')\n\n# Show head\nraw.head()","e3ce9358":"# Find total missing values\ndata = raw[['photo_id','owner','lat','lon','taken']]\nmissing_nan = data.isna().sum()\n\nprint('TOTAL MISSINGS:', missing_nan, sep='\\n')\n\n# Remove missing values\ndata = data.dropna(subset=['lat','lon'])\nnew_size = len(data.index)\nprint(f'{\"-\"*50}\\n{data_dim[0]-new_size} empty rows are removed.')","6a34f600":"# Create dataframe filled with DBSCAN params and clusters\ndef paramsClusters(data, eps_range, minPts_range):\n    m_per_rad = 6371.0088 * 1000\n    df = pd.DataFrame(columns=['eps','min_pts','num_clusters'])\n    for m in minPts_range:\n        for e in eps_range:\n            eps_rad = e\/m_per_rad\n            eps_rad = eps_rad\n            #  DBSCAN based on Haversine metric\n            db = DBSCAN(eps=eps_rad, min_samples=m, algorithm='ball_tree', metric='haversine').fit(np.radians(data[['lat','lon']]))\n            c = len(set(db.labels_ + 1))\n            df = df.append({'eps': e, 'min_pts': m, 'num_clusters': c}, ignore_index=True)\n    \n    return df\n\n# DBSCAN trend - epsilons and clusters\nepsilons = list(map(lambda n: n*20, range(1,16)))\nmin_points = list(map(lambda n: n*10, range(1,6)))\npc = paramsClusters(data, epsilons, min_points)\nEVC = go.Figure()\n\nfor m in pc.min_pts.unique():\n    df = pc[pc.min_pts == m]\n    EVC.add_trace(go.Scatter(\n        x=df.eps,\n        y=df.num_clusters,\n        name=f'Min Samples: {m} points',\n        mode='lines+markers',\n        marker=dict(size=8),\n        line=dict(width=2),\n        line_shape='spline'\n    ))\n    \nEVC.update_layout(\n    title='The number of detected clusters with different valuses of MinSamples',\n    xaxis=dict(title='Epsilon', zeroline=False, dtick=40),\n    yaxis=dict(title='Number of Cluster', zeroline=False),\n    template=plot_template\n)\n\n# DBSCAN trend - samples and clusters\nepsilons = list(map(lambda n: n*20, range(1,5)))\nmin_points = list(map(lambda n: n*5, range(1,11)))\npc = paramsClusters(data, epsilons, min_points)\nMVC = go.Figure()\n    \nfor e in pc.eps.unique():\n    df = pc[pc.eps == e]\n    MVC.add_trace(go.Scatter(\n        x=df.min_pts,\n        y=df.num_clusters,\n        name=f'Epsilon: {e} m',\n        mode='lines+markers',\n        marker=dict(size=8),\n        line=dict(width=2),\n        line_shape='spline'\n    ))\n\nMVC.update_layout(\n    title='The number of detected clusters with different valuses of Eps',\n    xaxis=dict(title='The number of samples in neighborhood', zeroline=False, dtick=5),\n    yaxis=dict(title='Number of Cluster', zeroline=False),\n    template=plot_template\n)\n\nEVC.show()\nMVC.show()","bff4d9d1":"# Calculate DBSCAN based on Haversine metric    \ndef HDBSCAN(df, epsilon, minPts, x='lat', y='lon'):\n    \n    # Find most centered sample in a cluster\n    def getCenterMostPts(cluster):\n        centroid = (MultiPoint(cluster.values).centroid.x, MultiPoint(cluster.values).centroid.y)\n        centermost_point = min(cluster.values, key=lambda point: great_circle(point, centroid).m)\n        return tuple(centermost_point)\n\n    m_per_rad = 6371.0088 * 1000\n    eps_rad = epsilon\/m_per_rad\n    photo_coords = df.loc[:, {x,y}]\n    photo_coords = photo_coords[['lat','lon']]\n    db = DBSCAN(eps=eps_rad, min_samples=minPts, algorithm='ball_tree', metric='haversine').fit(np.radians(photo_coords))\n    cluster_labels = db.labels_ + 1\n    num_clusters = len(set(cluster_labels))\n\n    # Put clusters and their subset of coords in an array\n    clusters = pd.Series([photo_coords[cluster_labels==n] for n in range(num_clusters)])\n\n    # Find centroid of each cluster\n    centroids = clusters.map(getCenterMostPts)\n    \n    # Pull rows from original data frame where row numbers match the clustered data\n    rows = clusters.apply(lambda c: c.index.values)\n    clustered_df = rows.apply(lambda row_num: df.loc[row_num])\n    \n    # Append cluster numbers and centroid coords to each clustered dataframe\n    lats,lons = zip(*centroids)\n    new_df = []\n    for i, v in clustered_df.iteritems():\n        v.loc[:, 'cluster_num'] = i\n        v.loc[:, 'cent_lat'] = lats[i]\n        v.loc[:, 'cent_lon'] = lons[i]\n        new_df.append(v)    \n    new_df = pd.concat(new_df)\n    \n    return new_df\n    \ncdata = HDBSCAN(data, epsilon=120, minPts=10)\nprint(f'Number of clusters: {len(cdata.cluster_num.unique())}')","910c27a9":"# Convet matplotlib colormap to plotly\ndef matplotlibToPlotly(cmap, pl_entries):\n    h = 1.0\/(pl_entries-1)\n    pl_colorscale = []\n    \n    for k in range(pl_entries):\n        C = list(map(np.uint8, np.array(cmap(k*h)[:3])*255))\n        pl_colorscale.append('rgb'+str((C[0], C[1], C[2])))\n        \n    return pl_colorscale\n\n\n# Show plot\nunique_labels = cdata.cluster_num.unique()\ncolors = matplotlibToPlotly(plt.cm.Spectral, len(unique_labels))\nDB = go.Figure()\nleaflet_map = folium.Map(location=[51.514205,-0.104371], zoom_start=12, tiles='Cartodb Positron')\n\nfor k,col in zip(unique_labels, colors):\n    # Check if label number is 0, then create noisy points \n    if k == 0:\n        col = 'gray'\n        df = cdata[cdata.cluster_num == 0]\n        \n        DB.add_trace(go.Scatter(\n            x=df.lat,\n            y=df.lon,\n            mode='markers',\n            name='noise',\n            marker=dict(size=3, color=col),\n            hoverinfo='none'\n        ))\n        \n    # Check the remaining clusters\n    else:\n        col = col\n        df = cdata[cdata.cluster_num == k]\n        lat = df.lat\n        lon = df.lon\n        cent_lat = df.cent_lat.unique()\n        cent_lon = df.cent_lon.unique()\n        \n        # Bokeh plot\n        DB.add_trace(go.Scatter(\n            x=lat,\n            y=lon,\n            mode='markers',\n            name='point',\n            marker=dict(size=5, color=col),\n            text=df.photo_id.apply(lambda id: f'photo_id: {id}'),\n            hoverinfo='none',\n            showlegend=False\n        ))\n        DB.add_trace(go.Scatter(\n            x=cent_lat,\n            y=cent_lon,\n            mode='markers',\n            name='centroid',\n            text=f'cluster: {k}',\n            marker=dict(\n                size=12,\n                color=col,\n                line=dict(color='gray', width=1)\n            ),\n            hoverinfo='x+y+name+text'\n        ))\n        \n        # Map plot\n        folium.Marker(\n            location=[cent_lat, cent_lon],\n            icon=folium.Icon(icon='map-marker')\n        ).add_to(leaflet_map)\n        \n        \nDB.update_layout(\n    title='DBSCAN based on Haversine including center most points',\n    hovermode='closest',\n    showlegend=False,\n    xaxis=dict(title='Latitude', zeroline=False),\n    yaxis=dict(title='Longitude', zeroline=False),\n    template=plot_template\n)\n\nDB.show()\nleaflet_map","64d34ddd":"# Remove noise cluster from the training set\nclean_data = cdata[cdata.cluster_num!=0]\n\n# Distribution plot\ndef chunk(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\ndistrib_df = clean_data.groupby(['cluster_num'])['cluster_num'].count().reset_index(name='photo_num')\nchunked_distrib = chunk(distrib_df, 50)\n\nfor df in chunked_distrib:\n    cluster_distrib = go.Figure([go.Bar(x=df.cluster_num, y=df.photo_num)])\n    cluster_distrib.update_layout(\n        title='Distribution of photos among clusters',\n        xaxis=dict(title='Cluster id', dtick=1),\n        yaxis=dict(title='Number of images'),\n        template=plot_template\n    )\n    cluster_distrib.show()","30ef8a5f":"# Find most frequent string in array\ndef mostFreqStr(array):\n    array = [i for i in array if str(i) != 'nan']\n    if len(array) != 0:\n        counts = np.unique(array, return_counts=True)[1]\n        max_index = np.argmax(counts)\n        freq_bin = array[max_index]\n        return freq_bin\n    else:\n        return np.nan\n\n# Find median of array included Timestamps\ndef medTimestamps(array):\n    if len(array) == 1:\n        return array[0]\n    else:\n        if len(array) % 2 == 0:\n            delta = array[int(len(array)\/2)] - array[int(len(array)\/2-1)]\n            median = pd.Timestamp(array[int(len(array)\/2-1)] + delta)\n        else:\n            time = pd.Timestamp(array[int(len(array)\/2)]).time()\n            ser = pd.Series(array)\n            date = pd.Timestamp.fromordinal(int(ser.apply(lambda x: pd.to_datetime(x).toordinal()).median(skipna=True))).date()\n            median = pd.Timestamp.combine(date,time)     \n        return median\n\n# Create database of locations\nPOI = pd.DataFrame(columns=['location_id', 'user_id', 'lat', 'lon', 'visit_time'])\nthreshold = np.timedelta64(6, 'h')\n\nfor i,g in clean_data.groupby(by='cluster_num'):\n    l = {}\n    l['location_id'] = randint(100000,999999)\n    l['lat'] = g.cent_lat.unique()[0]\n    l['lon'] = g.cent_lon.unique()[0]\n    \n    for u in g.owner.unique():\n        l['user_id'] = u\n        taken = g.loc[g.owner == u, 'taken'].sort_values()\n        t_indices = taken.keys()\n        t_values = taken.values\n        visit_times = []\n        \n        if len(t_values) == 1:\n            l['visit_time'] = pd.Timestamp(t_values[0])\n            POI = POI.append(l, ignore_index=True)\n        \n        else:\n            for t in range(1, len(t_values)):\n                if t_values[t]-t_values[t-1] < threshold:\n                    visit_times.append(t_values[t-1])\n                else:\n                    visit_times.append(t_values[t-1])\n                    l['visit_time'] = medTimestamps(visit_times)\n                    POI = POI.append(l, ignore_index=True)\n                    visit_times = []\n\ndisplay(POI.head(10))\n                    \n# Create a lint to download\ncreate_download_link(POI, filename='prefiltered.csv')","5c71db8b":"# Path of file to read\nprefiltered_file_path = f'{main_path}\/flickr_london_prefiltered\/prefiltered.csv'\n\n# Change data types\ndata_type = {\n    'faves': 'float16',\n    'lat': 'float32',\n    'lon': 'float32',\n    'visit_time': 'datetime64'\n}\n\n# Read csv file and convert it to a Multiindex\nLPD = pd.read_csv(prefiltered_file_path, engine='python', sep=',', encoding='utf-8', dtype=data_type, decimal=',')\nLPD = LPD.set_index(keys=['user_id', 'location_id'])\ndisplay(LPD.head(10))\n\n# Split dataset\nvisit_limit = LPD.groupby(level=[0,1])['visit_time'].count()\nvisit_limit = visit_limit[visit_limit>3]\nmask = LPD.index.isin(visit_limit.index) == True\nX = LPD[mask]\ny = X.index.get_level_values(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=70)","e880753e":"# Find ratings\ntrain_rating = X_train.groupby(['location_id','user_id'])['visit_time'].count().reset_index(name='rating')\ntrain_rating.head(10)","681a1de2":"def normalize(df):\n    # Normalize number of visit into a range of 1 to 5\n    df['rating'] = minmax_scale(df.rating, feature_range=[1,5])\n    return df\n\nr_df = normalize(train_rating)\n\n# Create a rating matrix\nr_df = train_rating.pivot_table(\n    index='user_id', \n    columns='location_id', \n    values='rating', \n    fill_value=0\n)\n    \n# Calculate the sparcity percentage of matrix\ndef calSparcity(m):\n    m = m.fillna(0)\n    non_zeros = np.count_nonzero(m)\/np.prod(m.shape) * 100\n    sparcity = 100 - non_zeros\n    print(f'The sparcity percentage of matrix is %{round(sparcity,2)}')\n\ndisplay(r_df.head())\ncalSparcity(r_df)","836b2ea3":"# Create user-user similarity matrix\ndef improved_asym_cosine(m, mf=False,**kwarg):\n    # Cosine similarity matrix distance\n    cosine = cosine_similarity(m)\n\n    # Asymmetric coefficient\n    def asymCo(X,Y):\n        co_rated_item = np.intersect1d(np.nonzero(X),np.nonzero(Y)).size\n        coeff = co_rated_item \/ np.count_nonzero(X)\n        return coeff\n    asym_ind = pairwise_distances(m, metric=asymCo)\n\n    # Sorensen similarity matrix distance\n    sorensen = 1 - pairwise_distances(np.array(m, dtype=bool), metric='dice')\n\n    # User influence coefficient\n    def usrInfCo(m):\n        binary = m.transform(lambda x: x >= x[x!=0].mean(), axis=1)*1\n        res = pairwise_distances(binary, metric=lambda x,y: (x*y).sum()\/y.sum() if y.sum()!=0 else 0)\n        return res       \n    usr_inf_ind = usrInfCo(m)\n\n    similarity_matrix = np.multiply(np.multiply(cosine,asym_ind),np.multiply(sorensen,usr_inf_ind))\n\n    usim = pd.DataFrame(similarity_matrix, m.index, m.index)\n    \n    # Check if matrix factorization was True\n    if mf:\n        # Binary similarity matrix\n        binary = np.invert(usim.values.astype(bool))*1\n        model = NMF(**kwarg)\n        W = model.fit_transform(usim)\n        H = model.components_\n        factorized_usim = np.dot(W,H)*binary + usim\n        usim = pd.DataFrame(factorized_usim, m.index, m.index)\n                \n    return usim\n\ns_df = improved_asym_cosine(r_df)\ndisplay(s_df.head())\ncalSparcity(s_df)","7e21dbcf":"# Find probability of contexts\ncontexts = X_train.filter(['season','daytime','weather']).apply(lambda x: (x.season,x.daytime,x.weather), axis=1).reset_index(name='context')\nIF = contexts.groupby(['location_id','context'])['context'].count()\/contexts.groupby(['context'])['context'].count()\nIDF = np.log10(contexts.groupby(['location_id','user_id'])['user_id'].count().sum()\/contexts.groupby(['location_id'])['user_id'].count())\ncontexts_weight = (IF * IDF).to_frame().rename(columns={0: 'weight'})\n\n# Create a context-location matrix\nlc_df = contexts_weight.pivot_table(\n    index='context', \n    columns='location_id', \n    values='weight',\n    fill_value=0\n)\n\n\ndisplay(lc_df.head())\ncalSparcity(lc_df)","51ca9095":"cs_df = pd.DataFrame(cosine_similarity(lc_df), index=lc_df.index, columns=lc_df.index)\ndisplay(cs_df.head())\ncalSparcity(cs_df)","a6464698":"def CF(user_id, location_id, s_matrix):\n    r = np.array(r_df)\n    s = np.array(s_matrix)\n    users = r_df.index\n    locations = r_df.columns\n    l = np.where(locations==location_id)[0]\n    u_idx = np.where(users==user_id)[0]\n        \n    # Means of all users\n    means = np.array([np.mean(row[row!=0]) for row in r])\n    \n    # Check if l is in r_rating\n    if location_id in r_df:\n        # Find similar users rated the location that target user hasn't visited\n        idx = np.nonzero(r[:,l])[0]\n        sim_scores = s[u_idx,idx].flatten()\n        sim_users = zip(idx,sim_scores)\n    \n        # Check if there is any similar user to target user\n        if idx.any():\n            sim_ratings = r[idx,l]\n            sim_means = means[idx]\n            numerator = (sim_scores * (sim_ratings - sim_means)).sum()\n            denominator = np.absolute(sim_scores).sum()\n            weight = (numerator\/denominator) if denominator!=0 else 0\n            wmean = means[u_idx] + weight\n            wmean_rating = wmean[0]\n            \n    else:\n        wmean_rating = 0\n\n    return wmean_rating","0771d3cb":"# Collaborative filtering with post-filtered contexts\ndef CaCF_Post(user_id, location_id, s_matrix, c_current, delta):\n    \n    # Calculate cf\n    initial_pred = CF(user_id, location_id, s_matrix)\n    \n    if location_id in r_df:\n        r = np.array(r_df)\n        users = r_df.index\n        locations = r_df.columns\n        l = np.where(locations==location_id)[0]\n        c_profile = contexts\n        all_cnx = contexts.context.unique().tolist()\n        c = np.array(c_profile)\n        u_idx = np.where(users==user_id)[0]\n        c_current = tuple(c_current)\n        \n        # Get contexts of similar users visited the location\n        l_cnx = np.array(c_profile.loc[c_profile.location_id==location_id,['user_id','context']])\n                \n        if c_current in all_cnx:\n            # Find similarity of the current context to location contexts\n            cnx_scores = np.array([[uid, cs_df[c_current][cx]] for uid,cx in l_cnx])\n\n            # Filter users whose similarity bigger than delta\n            filtered_scores = cnx_scores[cnx_scores[:,1].astype(float)>delta]\n\n            # Location popularity based on current context\n            visit_prob = len(filtered_scores) \/ len(cnx_scores)\n            \n        else:\n            visit_prob = 1\n\n        return initial_pred * visit_prob\n\n    else:\n        return initial_pred","3910c128":"# Find ratings\ntest_rating = X_test.groupby(['location_id','user_id'])['visit_time'].count().reset_index(name='rating')\ntest_rating = normalize(test_rating)\nr_df_test = test_rating.pivot_table(index='user_id', columns='location_id', values='rating', fill_value=0)\n\n# Proposed approach\ndef EACOS_CaCF_Post(user_id, location_id, c_current, delta):\n    res = CaCF_Post(user_id, location_id, s_df, c_current, delta)\n    return res\n\n# Recommendation\ndef predict(target_user, model, option=None):\n    true = r_df_test.loc[target_user]\n    \n    # Check if model is context-aware \n    if option:\n        pred_val = []\n        for l in true.index:\n            delta = option.get('delta')\n            c_current = tuple(X_test.xs(target_user)[['season','daytime','weather']].head(1).values[0])\n            r = model(user_id=target_user, location_id=l, c_current=c_current, delta=delta)\n            pred_val.append(r)\n    else:\n        pred_val = [model(user_id=target_user, location_id=l) for l in true.index]\n\n    pred = pd.Series(pred_val, index=true.index)\n\n    return pred","cc4e287a":"user = '41087279@N00'\noptions = {\n    'delta': .3\n}\n\ndef item_relevancy(col):\n    relevant = 1\n    r_color = 'background-color: lime'\n    nr_color = 'background-color: red'\n    res = []\n    for v in col:\n        if v > relevant:\n            res.append(r_color)\n        elif (v > 0) & (v <= relevant):\n            res.append(nr_color)\n        else:\n            res.append('')\n    return res\n    \ntrue = r_df_test.loc[user]\npred = predict(user, EACOS_CaCF_Post, option=options)\n\nwith pd.option_context(\"display.max_rows\", None):\n    prediction = pd.DataFrame({'true': true, 'pred': pred})\n    display(prediction.style.apply(lambda col: item_relevancy(col)))","f0fb82b5":"# Top 10 recommendations\ntop_10 = prediction.nlargest(10, 'pred')\ntop_10.style.apply(lambda col: item_relevancy(col))","7df3d972":"def rmse(true, pred):\n    return np.sqrt(mean_squared_error(true, pred))\n\ndef mean_average_precision(true, pred, k=10):\n    relevant = 1\n    sort_rates = lambda s: s.sort_values(ascending=False)\n    true = [r[1].where(r[1]>relevant).dropna().index.tolist() for r in true.iterrows()]\n    pred = [sort_rates(r[1].where(r[1]>relevant).dropna()).index.tolist() for r in pred.iterrows()]\n    map_score = mapk(true, pred, k)\n    return map_score","d957f982":"def predict_all(model, option=None):\n    users = r_df_test.index\n    locations = r_df_test.columns\n    pred = np.zeros(r_df_test.shape)\n    \n    for i in range(0,len(users)):\n        uid = users[i]\n        for j in range(0,len(locations)):\n            lid = locations[j]\n            # Check if model is context-aware \n            if option:\n                delta = option.get('delta')\n                c_current = X_test.xs(uid)[['season','daytime','weather']].head(1).values[0]\n                pred[i,j] = model(user_id=uid, location_id=lid, c_current=c_current, delta=delta)\n            else:\n                pred[i,j] = model(user_id=uid, location_id=lid)\n                        \n    return pd.DataFrame(pred, index=users, columns=locations)","b8340c41":"deltas = np.arange(0.1, 1, 0.1)\neval_scores = []\n\nfor d in deltas:\n    options['delta'] = d\n    pred = predict_all(EACOS_CaCF_Post, option=options)\n    precision = mean_average_precision(r_df_test,pred)\n    eval_scores.append(precision)\n    \nd_eval = pd.DataFrame(eval_scores, index=deltas, columns=['precision'])\n\n# Delta influence on the prediction and racall\nd_precision = go.Figure([go.Scatter(\n    name='MAP', \n    x=d_eval.index, \n    y=d_eval.precision, \n    text=d_eval.precision,\n    line_shape='spline'\n)])\n\nd_precision.update_layout(\n    title='The impact of similarity threshold on the recommendation quality',\n    xaxis=dict(title='Threshold of context similarity (\\u03B4)', autorange='reversed'), \n    yaxis=dict(title='MAP'),\n    template=plot_template\n)\n\nd_precision.show()","50d8ecf8":"## Non context-aware methodologies with asymetric similarity measure\n# Asymmetric cosine similarity\ndef asymmetric_cosine(m, mf=False, **kwarg):\n    # Cosine similarity matrix distance\n    cosine = cosine_similarity(m)\n    # Asymmetric coefficient\n    def asymCo(X,Y):\n        co_rated_item = np.intersect1d(np.nonzero(X),np.nonzero(Y)).size\n        coeff = co_rated_item \/ np.count_nonzero(X)\n        return coeff\n    asym_ind = pairwise_distances(m, metric=asymCo)\n    # Sorensen similarity matrix distance\n    sorensen = 1 - pairwise_distances(np.array(m, dtype=bool), metric='dice')\n    # Final similarity matrix\n    usim = np.multiply(np.multiply(cosine,asym_ind),sorensen)\n    # Check if matrix factorization was True\n    if mf:\n        binary = np.invert(usim.astype(bool))*1\n        model = NMF(**kwarg)\n        W = model.fit_transform(usim)\n        H = model.components_\n        factorized_usim = np.dot(W,H)*binary + usim\n        usim = factorized_usim\n            \n    return pd.DataFrame(usim, index=m.index, columns=m.index)\n\n# Calculate user similarities\nasym_cos = asymmetric_cosine(r_df)\nmf_asym_cos = asymmetric_cosine(r_df, mf=True, solver='mu')\n\n# Methods\ndef ACOS(user_id, location_id):\n    res = CF(user_id, location_id, asym_cos)\n    return res\n\ndef MF_ACOS(user_id, location_id):\n    res = CF(user_id, location_id, mf_asym_cos)\n    return res","75bec638":"## Context-aware methodologies symmetric similarity measure\n# Similarity measure based on location popularity\ndef loc_pop_sim(df, dist_method='correlation'):\n    df = df.reset_index()\n    # Calculate location pop\n    loc_idf = np.log10(df.groupby('location_id')['user_id'].count().sum()\n                    \/df.groupby('location_id')['user_id'].count()\n                   ).reset_index(name='idf_score')\n    loc_idf = df.merge(loc_idf)\n    \n    # Create location popularity matrix\n    r_df = loc_idf.pivot_table(\n        index='user_id', \n        columns='location_id', \n        values='idf_score', \n        fill_value=0\n    )\n    \n    # Calculate user similarities\n    if dist_method == 'dice':\n        dist = 1 - pairwise_distances(r_df.values, metric=dist_method)\n    else:\n        dist = pairwise_distances(r_df.values, metric=dist_method)\n    return pd.DataFrame(dist, r_df.index, r_df.index)\n\n# Calculate user similarities\nsym_locpop_pearson = loc_pop_sim(X_train)\nsym_locpop_sorensen = loc_pop_sim(X_train, dist_method='dice')\n\n# Methods\ndef PR(user_id, location_id):\n    res = CF(user_id, location_id, sym_locpop_pearson)\n    return res\n\ndef CSR(user_id, location_id, c_current, delta):\n    initial_pred = CF(user_id, location_id, sym_locpop_pearson)\n    if location_id in r_df:\n        r = np.array(r_df)\n        users = r_df.index\n        locations = r_df.columns\n        l = np.where(locations==location_id)[0]\n        c_profile = contexts\n        c = np.array(c_profile)\n        u_idx = np.where(users==user_id)[0]\n        c_current = tuple(c_current)\n\n        # Find users who visit the location in the current context \n        exact_match = contexts[(contexts.location_id==location_id)&(contexts.context==c_current)].user_id.unique()\n        \n        if exact_match.size != 0:\n            idx = np.where(users.isin(exact_match))\n\n            # Calculate visit probability in exact-match context\n            visit_match_prob = r[idx,l].sum() \/ r[:,l].sum()\n\n            # Calculate visit probability of location\n            visit_loc_prob = r[:,l].sum() \/ r.sum()\n\n            # Calculate visit probability in current context\n            visit_cnx_prob = contexts[contexts.context==c_current].location_id.count()\/r.sum()\n\n            visit_prob = (visit_loc_prob * visit_match_prob) \/ visit_cnx_prob\n        \n            return initial_pred * visit_prob\n        \n        else:\n            return initial_pred\n    \n    else:\n        return initial_pred\n\ndef Sorensen_CaCF_Post(user_id, location_id, c_current, delta):\n    res = CaCF_Post(user_id, location_id, sym_locpop_sorensen, c_current, delta=.3)\n    return res","5728fe08":"models = [PR, ACOS, MF_ACOS, CSR, Sorensen_CaCF_Post, EACOS_CaCF_Post]\nk_range = [5,10,15,20]\neval_scores = {}\ntrue = r_df_test\noptions['delta'] = .3\n\nfor model in models:\n    option = None if model.__name__ in ['ACOS', 'PR', 'MF_ACOS'] else options\n    val = []\n    for k in k_range:\n        pred = predict_all(model, option)\n        mapk_score = mean_average_precision(true, pred, k)\n        val.append(mapk_score)\n        \n    eval_scores[model.__name__] = val\n    \nmap_at_k = pd.DataFrame(eval_scores, index=k_range)\n\nmapk_comp = go.Figure()\n\nfor model, ser in map_at_k.iteritems():\n    mapk_comp.add_trace(go.Bar(\n        name=model,\n        x=ser.index,\n        y=ser.values,\n        width=.6\n    ))\n    \nmapk_comp.update_layout(\n    barmode='group',\n    title='Comparision of the proposed method with the benchmarking methods (MAP@k)',\n    xaxis=dict(title='NUmber of recommendations'),\n    yaxis=dict(title='MAP@k', range=[.7,.9]),\n    template=plot_template\n)\n\nmapk_comp.show()","e5b64db0":"rmse_eval = []\n\nfor model in models:\n    option = None if model.__name__ in ['ACOS', 'PR', 'MF_ACOS'] else options\n    pred = predict_all(model, option)\n    rmse_score = rmse(true, pred)\n    rmse_eval.append([model.__name__, rmse_score])\n    \nrmse_perf = pd.DataFrame(rmse_eval, columns=['model','value'])\n\nrmse_comp = go.Figure([go.Bar(\n    x=rmse_perf.model, \n    y=rmse_perf.value,\n    width=.5,\n    text=round(rmse_perf.value,2),\n    textposition='outside', \n    marker=dict(color=rmse_perf.index, colorscale='Viridis')\n)])\n\nrmse_comp.update_layout(\n    barmode='group',\n    title='Comparision of the proposed method with the benchmarking methods (RMSE)',\n    yaxis=dict(title='RMSE'),\n    template=plot_template\n)\n\nrmse_comp.show()","c8bfbd4d":"## Finding DBSCAN Parameters\n\nWe need to know the best performing parameters of DBSCAN clustering. So, through a trial and error approach we determine the appropriate values for <code>eps<\/code> and <code>min_pts<\/code>\/<code>min_samples<\/code>.","93c4ad39":"<span id=\"model\"><\/span>\n# Recommendation Model\n---\nThe <code>POI<\/code> dataset including contextual factors is imported. This dataset is assumed as <code>LPD<\/code>, Location Profile Dataframe. For making recommendation model, only tourists who have visited at least 4 distinct locations were selected. Data then is split into training data and test data. ","c82893cb":"<h1 style=\"text-align: center\">A Context-Aware Recommender System<\/h1>\nIn this notebook, we will attempt at implementing a context-aware recommender system. The proposed approach uses a hybrid collaborative filtering method in order to recommend locations in a city for users, based on their history of visits, and users' contexts.<br>\n\n1. <a href=\"#intro\">Introduction<\/a>\n2. <a href=\"#prefiltering\">Pre-filtering Locations<\/a>\n3. <a href=\"#model\">Building Recommendation Model<\/a>\n4. <a href=\"#finalrecom\">Generate Final Recommendations<\/a>\n5. <a href=\"#eval\">Evaluation<\/a>","551f1395":"### Creating User-User Similarity Matrix\nNow let's create the user-user similarity matrix.","bfbabd09":"# Conclusion\nIn this kernel, we create a tourism recommendation system based on contexts and geo-tagged photos. Our hybrid approach first looks for similarity among users using an asymmetric similarity metric, and then uses collaborative filtering to predict the item ratings. The proposed method ultimately uses a context-aware post-filtering approach to determine the final recommendations. The system is able to understand various contextual conditions such as location, time of visit, day\/night, season and weather conditions of the venue at the time of visit.\n\n\nReferences:\n1. [P. Pirasteh, D. Hwang, and J. J. Jung, \u201cExploiting matrix factorization to asymmetric user similarities in recommendation systems,\u201d Knowledge-Based Systems, vol. 83, pp. 51- 57, 2015.](http:\/\/dx.doi.org\/10.1016\/j.knosys.2015.03.006)\n2. [H.Huang,\u201cContext-Aware Location Recommendation Using Geotagged Photos in Social Media,\u201d ISPRS International Journal of Geo-Information, vol. 5, no. 11, p. 195, 2016.](https:\/\/www.researchgate.net\/publication\/309541764_Context-Aware_Location_Recommendation_Using_Geotagged_Photos_in_Social_Media)\n3. [A. Majid, L. Chen, H. T. Mirza, I. Hussain, and G. Chen, \u201cA system for mining in- teresting tourist locations and travel sequences from public geo-tagged photos,\u201d Data & Knowledge Engineering, vol. 95, pp. 66-86, 2015.](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0169023X14000962)","660041cc":"Five different models have been selected from previous studies to compare with the proposed model. They are classified into non-contextual and context- aware categories.\n\n* <strong>Collaborative filtering using asymmetric cosine similarity (ACOS):<\/strong><br>This method uses an asymmetric cosine similarity measure to find similarities among users, then by collaborative filtering ratings can be predicted [[1]](http:\/\/dx.doi.org\/10.1016\/j.knosys.2015.03.006).\n* <strong>Collaborative filtering using asymmetric cosine similarity and matrix factorization (MF_ACOS):<\/strong><br>The difference between this model and the cosine similarity approach is to eliminate the sparsity of the similarity matrix by using the matrix factorization [[1]](http:\/\/dx.doi.org\/10.1016\/j.knosys.2015.03.006).\n* <strong>Popularity Ranking (PR):<\/strong><br>The basic idea of this method is to rank tourist locations based on the popularity of each location [[3]](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0169023X14000962).\n* <strong>Context-aware significant tourist locations recommendations (CSR):<\/strong><br>The basis of this model is to predict the ranking of the tourist location based on the context of the target user. This method uses the likelihood of visiting the destination exactly in the context of the target user to filter out the tourist destinations [[3]](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0169023X14000962).\n* <strong>Context-aware collaborative filtering using Sorensen-Dice coef- ficient (Sorensen CaCF Post):<\/strong><br>This method uses the Sorensen Dice coefficient to find similarity among users. It also uses a probability ratio to find the visit probability of locations similar to the target user\u2019s context, which is applied to the collaborative filtering results as a post-filtering [[2]](https:\/\/www.researchgate.net\/publication\/309541764_Context-Aware_Location_Recommendation_Using_Geotagged_Photos_in_Social_Media).","324276da":"<span id=\"prefiltering\"><\/span>\n# Pre-Filtering\n---\nUsing DBSCAN clustering, we try to remove noise coordinates and find potential locations can be considered as tourist venues. This includes the steps below:\n1. Determining <code>eps<\/code> and <code>min_sample<\/code> parameters of DBSCAN clustering.\n2. Clustering <code>data<\/code> with DBSCAN and find potential points in each cluster.\n3. Finding the center of each cluster:<br>Because of the arbitrary shape of the clusters in DBSCAN method, we find the reference point of each cluster by assuming the summation of latitudes and longitudes divided by numbers of points inside of each cluster. Center is the coordinate of the nearest point to the reference point.\n4. Profiling locations:<br>This step identifies tourist locations, their time of visits as well as visited pattern of each location in terms of different contextual factors.","21e72c0e":"The visit probability of each candidate location is calculated in the current contexts, such that the probability of visiting the location $i$ equals the fraction of the users who visited the location $i$ in contexts similar to the target user, and the similarity of the current contexts with the visiting context of the neighbors is larger than a threshold like <code>delta<\/code>.\n\nAfter calculating the visiting probability, the final rating associated with each candidate location is obtained by:\n<br><br>\n\\begin{equation}\nscore(u_a,i) = (collaborative\\ filtering\\ rate) \\times (visit\\ probability)\n\\end{equation}","ac78e63f":"### Creating Context-Context Matrix","b593576f":"If you enjoyed reading the kernel, hit the upvote button. Please leave your feedbacks and suggestions below as well.","c3db0e9e":"### <span id=\"eval\"><\/span>\n# Evaluation\n---\nIn the final step, we evaluate the proposed method based on common evaluation metrics in recommendation systems, MAP and RMSE. We also compare the performance of proposed model against some other recommendation methods.","4a8c7902":"As an example, let's see the result of rating prediction related to a user visited different locations in London city. Top-10 locations with the highest ratings would be a list of recommendations.","93b850d9":"### Creating Context-Location Matrix\nThe matrix below shows the number of visits from venues in various contexts. For example, a venue has been visited 3 times in the context <code>(1,1,9)<\/code> which means this context is equal to:\n\n* season: spring\n* daytime: partly-cloudy-day\n* weather: day\n\n> To make it easier to work with the data, the value of each context has turned into numbers:\n> * Weather: (1=clear-day, 2=clear-night, 3=rain, 4=snow, 5=sleet, 6=wind, 7=fog, 8=cloudy, 9=partly-cloudy-day, 10=partly-cloudy-night)\n> * Season: (1=spring, 2=summer, 3=autumn, 4=winter)\n> * Daytime: (1=day, 2=night, 3=midnight)\n\n<br><br>\nWe use a method based on TF-IDF to find the visit probability of locations [[2]](https:\/\/www.researchgate.net\/publication\/309541764_Context-Aware_Location_Recommendation_Using_Geotagged_Photos_in_Social_Media).\n<br><br>\n> We use the term frequency-inverse document frequency (TF-IDF) measure to compute the usage of a location in a specific situation $w_l^c$. TF-IDF is used in the field of information retrieval to measure how important a word is to a document in a collection or corpus. It increases proportionally with the number of times a word appears in the document, is offset by the frequency of the word in the corpus [[2]](https:\/\/www.researchgate.net\/publication\/309541764_Context-Aware_Location_Recommendation_Using_Geotagged_Photos_in_Social_Media).\n<br><br>\n\n\\begin{equation}\nw_l^c = TF_l \\times IDF_l = \\frac{N_{c,l}}{N_{c,\\oslash}} \\times \\log\\frac{N_{\\oslash,\\oslash}}{N_{\\oslash,l}}\n\\end{equation}\n<br><br>\n$N_{c,l}$ is the number of visits in context $c$ that visited location $l$. $N_{c,\\oslash}$ shows the number of visits to all locations in the context $c$. $N_{\\oslash,\\oslash}$ is the total number of visits to all locations, and $N_{\\oslash,l}$ represents the total number of visits to the location $l$.","a6f4eb0b":"## Clusters Analysis\nThe plots below illustrate all clusters and their centroids in two geographical and non-geographical views. The output of DBSCAN is a set of photo clusters $L=\\{l_1,l_2,...,l_n\\}$. Each member of this set is a tourist venue that can be considered as $l_i=\\{P_{l_i},g_{l_i}\\}$. $P_{l_i}$ is the set of all photos taken in location $l_i$, in which the geographical coordinate of the centroid is $g_{l_i}$.","3172bc4e":"## Loading Data\nLet's load data into the <code>raw<\/code> dataframe. The dataset contains photo records including meta tags related to photos and users who took them.","d0c5b452":"## DBSCAN\nWe choose <code>eps = 120<\/code> and <code>min_sample = 10<\/code> to the data.","4f0cad97":"<span id=\"finalrecom\"><\/span>\n# Final Recommendation\n---\nBased on the tourist location profiles and user similarities, the locations which have not been rated by the user could be predicted by applying user-based collaborative filtering. A post-filtering approach is used to adjust predicted ratings according to contextual information.\n\nWe use the user-based collaborative filtering to predict the initial ratings.","ba3bec30":"## Location Profiling\nAt this step, we are looking for features related to each tourist location, extracted from the previous step. Each location identified in the clustering contains information such as the geographical location, id of the user who visited the venue, visiting time and visiting contexts. It is important to know that a user may take more than one photo of the place while visiting a venue. Therefore, if the duration between the time-stamps of two photos taken by a user at the same location is less than visit duration threshold (<code>threshold<\/code>), we can cosider that both photos belong to a same location. If not, the <code>median<\/code> of timestamps can be considered as the time of the visit with new contexts.\n<br><br>\nNext, the outcome dataframe, <code>POI<\/code>, is exported in order to extract contextual features based on photos' taken times, then it will be imported again.","65104769":"## Creating User-Location Matrix\nTo create rating matrix, we need to know the number of times when each user has visited different venues. Ratings is also normilized by the min-max normalization method.","e5d53e82":"<span id=\"intro\"><span>\n# Introduction\n---\nThis soultion includes two major steps, <strong>pre-filtering geo-tagged data<\/strong> in order to detect tourists venues, and <strong>building a recommendation model<\/strong> considering users' asymmetric similarities and visit probablity in the user's current context.","477fdc9e":"## Data Validation\nIn this step, we find and remove rows including coordinates (latitude and longitude) with <code>NA<\/code>\/<code>Null<\/code> value.","5507affc":"In the next step, the user similarity should be calculated. According to the asymmetric similarity concept:\n> Most of the traditional similarity metrics assign equal value for the similarity relation between two users, This means, these methods are based on the assumption that $sim(u,v) = sim(v,u)$. Traditional methods cannot differentiate between these two users. However based on asymmetric similarity user $u$ is similar to user $v$, but not vice versa [[1]](http:\/\/dx.doi.org\/10.1016\/j.knosys.2015.03.006).\n\nThe user's similarity based on an asymmetric cosine similarity proposed by [Pirasteh et al [1]](http:\/\/dx.doi.org\/10.1016\/j.knosys.2015.03.006) is as follow:\n<br><br>\n\\begin{equation}\nACOS(u,v)=\\frac{\\overrightarrow{r_1}.\\overrightarrow{r_2}}{||\\overrightarrow{r_1}||.||\\overrightarrow{r_2}||}.\\frac{|u \\cap v|}{|u|}.\\frac{2*|u \\cap v|}{|u| + |v|}\n\\end{equation}\n<br>\nIn case of having same numbers of items\/venues rated by both users, the $ACOS(u_1,u_2)$ will be equal to $ACOS(u_2,u_1)$. So to address this problem a weighted user's influence coefficient is suggested.<br><br>\n\\begin{equation}\n\\bar{r_{u}} = \\frac{\\sum_{j=1}^{n}r_{i,j}}{n_i}\n\\end{equation}\n<br>\n\\begin{equation}\nr_{u}^\\prime = \n    \\begin{cases}\n    1, \\qquad if \\quad r_u \\geq \\bar{r_{u}} \\\\\n    0, \\qquad otherwise\n    \\end{cases}\n\\end{equation}\n<br>\n\\begin{equation}\nW_{u,v}^\\prime = \\frac{\\sum_{i=1}^{n} r_{u,i}^\\prime \\times r_{v,i}^\\prime}{\\sum_{i=1}^{n} r_{v,i}^\\prime}\n\\end{equation}\n<br><br>\nWe propose an extended version of $ACOS(u,v)$:\n<br><br>\n\\begin{equation}\nACOS(u,v)=\\frac{\\overrightarrow{r_u}.\\overrightarrow{r_v}}{||\\overrightarrow{r_u}||.||\\overrightarrow{r_v}||}.\\frac{|u \\cap v|}{|u|}.\\frac{2*|u \\cap v|}{|u| + |v|}.\\frac{\\sum_{i=1}^{n} r_{u,i}^\\prime \\times r_{v,i}^\\prime}{\\sum_{i=1}^{n} r_{v,i}^\\prime}\n\\end{equation}\n<br><br>"}}