{"cell_type":{"711c716c":"code","3009d7a8":"code","fdd79b41":"code","2899295d":"code","2086de30":"code","e2fbe1a6":"code","4fcbe213":"code","d19bf1fa":"code","81748863":"code","a5e6d3b4":"code","ba3631b4":"code","7cb637d6":"code","b9e486fc":"code","6afb9d42":"code","e84d3b1a":"code","4e1098c1":"code","649238cf":"code","bf5d8ebc":"code","dfa0be2d":"code","0fd605e1":"markdown","49b68c95":"markdown","28ef0cca":"markdown","8eea3c8e":"markdown","08d274e4":"markdown","7f0c99c0":"markdown","0e506028":"markdown","783bf7ad":"markdown","f0c18c6a":"markdown","c3a654ee":"markdown","c4a53c98":"markdown"},"source":{"711c716c":"!pip install -U skorch -q","3009d7a8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nsns.set(font_scale= 1.0)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport torch\nfrom torch import nn\nfrom torch.nn.utils import weight_norm\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom skorch import NeuralNetClassifier, NeuralNet\nfrom skorch.callbacks import EpochScoring\nfrom skorch.callbacks import LRScheduler, EarlyStopping\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(0)","fdd79b41":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', index_col = 'id')\ntest_pred = test.values.astype('float32')\n\n\nX = train.drop('target', axis = 1).values.astype('float32')\n\nlencoder = LabelEncoder()\ny = lencoder.fit_transform(train['target']).astype('int64')","2899295d":"feature_dictionary_size = 360\nnum_features = 75\n\ndef residual_block(in_features, out_features, p_drop, non_linear = nn.ReLU(), *args, **kwargs):  \n        return nn.Sequential(\n            nn.Dropout(p = p_drop),\n            weight_norm(nn.Linear(in_features, out_features)),\n            non_linear)\n\n\nclass TPSResidual(nn.Module):\n    def __init__(self, num_class = 9, dropout = 0.3, linear_nodes = 32, linear_out = 16, emb_output = 4, num_block = 3):\n        super(TPSResidual, self).__init__()\n        \n        self.num_block = num_block\n        self.final_module_list = nn.ModuleList()\n    \n        \n        self.embedding = nn.Embedding(feature_dictionary_size, emb_output)\n        self.flatten = nn.Flatten()\n\n        self.linear = weight_norm(nn.Linear(emb_output * num_features, linear_nodes))\n        \n        for res_num in range(self.num_block):\n            self.non_linear = nn.ELU() if res_num % 2 else nn.ReLU()\n            self.lin_out = linear_out if res_num == (self.num_block - 1) else linear_nodes\n            self.final_module_list.append(residual_block(emb_output * num_features + (res_num + 1) * linear_nodes, self.lin_out, dropout, self.non_linear))\n            \n        self.out = nn.Linear(linear_out, num_class)\n        \n        # nonlinearity - activation function\n        self.selu = nn.SELU()\n        \n        self.dropout = nn.Dropout(p = dropout)\n    \n   \n    def forward(self, x):\n        x = torch.tensor(x).to(torch.int64)\n        \n        # Embedding \n        e = self.embedding(x)\n        e = self.flatten(e)\n        \n        h1 = self.dropout(e)\n        h1 = self.linear(h1)\n        h1 = self.selu(h1)\n        \n        ri = torch.cat((e, h1), 1)\n        \n        for res_num in range(self.num_block):          \n            rx = self.final_module_list[res_num](ri)\n            ri = torch.cat((ri, rx), 1)\n        \n        return  F.softmax(self.out(rx), dim = -1)","2086de30":"lr_scheduler = LRScheduler(policy = ReduceLROnPlateau, monitor = 'valid_loss', mode = 'min', patience = 3, factor = 0.1, verbose = True)\nearly_stopping = EarlyStopping(monitor='valid_loss', patience = 10, threshold = 0.0001, threshold_mode='rel', lower_is_better=True)","e2fbe1a6":"net = NeuralNetClassifier(TPSResidual, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping])","4fcbe213":"# if you can search for hyperparamteters set OPTIM = True\n# I ran many experiments (270 runs) o local machine. Results provided below.\n\nOPTIM = False","d19bf1fa":"# Each parameter you can define as you want. For testing purposes and limited Kaggle notebook uptime I decided to compute them on my local machine and use it here.  \n\n# This is demo only so I dropped some of parameters (it is only subset, full list of params (about 270 runs) will be trained locally and then I will provide here)\ngrid_params = {\n    'net__module__dropout': [0.2, 0.3], \n    'net__optimizer': [optim.AdamW, optim.Adam, optim.RMSprop],\n    'net__module__linear_nodes' : [64, 32, 16],\n    'net__module__emb_output' : [2, 4, 6],\n    'net__module__linear_out' : [16],\n    'net__module__num_block' : [2, 3, 4]\n} \n\n\n\n# Here you can define steps in Pipeline (eg. data preprocessing). In this example there is no such needs.\nsteps = [('net', net)]\npipeline = Pipeline(steps)\n\n\ngrid_net = GridSearchCV(pipeline, grid_params, cv = 5, refit = True, verbose = 1)","81748863":"if OPTIM:\n    result = grid_net.fit(X,y)\n\n# You can look into training history below","a5e6d3b4":"def report(results, n_top=5):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\nif OPTIM:\n    report(grid_net.cv_results_,5)","ba3631b4":"if OPTIM:\n    grid_net.best_estimator_","7cb637d6":"if OPTIM:\n    epochs = [i for i in range(len(grid_net.best_estimator_[0].history))]\n    train_loss = grid_net.best_estimator_[0].history[:,'train_loss']\n    valid_loss = grid_net.best_estimator_[0].history[:,'valid_loss']\n\n    plt.plot(epochs,train_loss,'g-');\n    plt.plot(epochs,valid_loss,'r-');\n    plt.title('Training Loss Curves');\n    plt.xlabel('Epochs');\n    plt.ylabel('Mean Squared Error');\n    plt.legend(['Train','Validation']);","b9e486fc":"# This parameters were taken from TOP3 searchgridcv estimators (computed on local machine) \n\nnet_params = [{'net__module__dropout': 0.3, 'net__module__emb_output': 2, \n               'net__module__linear_nodes': 16, 'net__module__linear_out': 16, \n               'net__module__num_block': 2, 'net__optimizer': optim.Adam},\n             \n              {'net__module__dropout': 0.3, 'net__module__emb_output': 2, \n              'net__module__linear_nodes': 32, 'net__module__linear_out': 16, \n              'net__module__num_block': 2, 'net__optimizer': optim.Adam},\n              \n              {'net__module__dropout': 0.2, 'net__module__emb_output': 2, \n              'net__module__linear_nodes': 16, 'net__module__linear_out': 16, \n              'net__module__num_block': 3, 'net__optimizer': optim.Adam},\n             \n              {'net__module__dropout': 0.3, 'net__module__emb_output': 2, \n               'net__module__linear_nodes': 16, 'net__module__linear_out': 16, \n               'net__module__num_block': 3, 'net__optimizer': optim.Adam},\n              \n              {'net__module__dropout': 0.3, 'net__module__emb_output': 2, \n               'net__module__linear_nodes': 16, 'net__module__linear_out': 16, \n               'net__module__num_block': 4, 'net__optimizer': optim.Adam}]\n\ndef get_estimator(net_params):\n    return NeuralNetClassifier(TPSResidual, \n                               device = device, \n                               lr = 3e-2, \n                               max_epochs = 50, \n                               optimizer = optim.AdamW,\n                               callbacks = [lr_scheduler, early_stopping],\n                               module__dropout = net_params['net__module__dropout'],\n                               module__emb_output = net_params['net__module__emb_output'],\n                               module__linear_nodes = net_params['net__module__linear_nodes'],\n                               module__linear_out = net_params['net__module__linear_out'],\n                               module__num_block = net_params['net__module__num_block'],\n                               iterator_train__shuffle = True\n                             )","6afb9d42":"if not OPTIM:\n    NUM_MODELS = len(net_params)\n    y_pred = np.zeros((100000,9))\n\n    for net_config in net_params:\n        print(f'MODEL PARMAETERS {net_config}\\n')\n        net = get_estimator(net_config)\n        net.fit(X,y)\n        y_pred += net.predict_proba(test_pred) \/ NUM_MODELS\n        print(\"\\n\")","e84d3b1a":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(y_pred, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\", \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\" ])\npredictions_df['id'] = sub['id']","4e1098c1":"predictions_df.head(5)","649238cf":"palette = itertools.cycle(sns.color_palette())\n\nplt.figure(figsize=(16, 8))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    c = next(palette)\n    sns.histplot(predictions_df, x = f'Class_{i+1}', color=c)\nplt.suptitle(\"CLASS PREDICTION - DISTRIBUTION\")","bf5d8ebc":"predictions_df.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","dfa0be2d":"predictions_df.to_csv(\"TPS06-pytorch_residual_submission.csv\", index = False)","0fd605e1":"#### I really appreciate any feedback and support. Thank you very much!","49b68c95":"# Pytorch (+skorch) RESIDUAL (skipped connection) with CROSS VALIDATION and HYPERPARAMETER SEARCH (GridSearchCV)\n\n\n<div class=\"alert alert-success\">\n  <strong>Content of this notebook<\/strong>\n    <ul>\n        <li>Notebook contribution<\/li>\n        <li>Model definition<\/li>\n        <li>Callback definition<\/li>\n        <li>Pilepine for data preprocessing and model run<\/li>\n        <li>Hyperparameter search<\/li>\n        <ul>\n            <li>number of residual modules in NN<\/li>\n            <li>number of embedding dimention<\/li>\n<li>best network optimizer<\/li>\n            <li>optimal linear nodes number<\/li>\n<li>dropout <\/li>\n<li>and many more (I just provided framework for searching the best params for NN)<\/li><\/ul>\n        <li>Model ranking + optimal parameters<\/li>\n        <li>Prediction smoothing - using many models<\/li>\n        <li>Submission<\/li>\n    <\/ul>\n<\/div>\n<\/br>\n<hr class=\"background-color: #fff; border-top: 2px dotted #8c8b8b;\">\n<\/br>\nScope of seach defined as a standard GridSearchCV\n<code>\n    grid_params = {\n    'net__module__dropout': [0.2, 0.3], \n    'net__optimizer': [optim.Adam], \n    'net__module__linear_nodes' : [64, 32, 16],\n    'net__module__emb_output' : [2, 4, 6],\n    'net__module__linear_out' : [16],\n    'net__module__num_block' : [2, 3, 4]\n} \n <\/code>\n\n\n<div>\n   <strong>Contributions<\/strong>\n    <ul>\n        <li><a href = \"https:\/\/www.kaggle.com\/oxzplvifi\/tabular-residual-network\">Tabular Residual Network by @oxzplvifi<\/a><\/li>\n        <li><a href = \"https:\/\/www.kaggle.com\/oxzplvifi\/tabular-residual-network\">Python keras NN (residual) by @alexryzhkov<\/a><\/li>\n    <\/ul>\n<\/div>\n\n<div>\n<strong>List of experiments which have not provide better result<\/strong>\n<ul>\n    <li>I have used different ways to normalize weight - batch norm, weight norm and layer norm - the best so far is weight norm<\/li>\n    <li> Mixing many activation function (ReLU, ELU, SELU) works fine. If using only ReLU score has not improved.<\/li>\n<\/ul>\n<\/div>","28ef0cca":"## LEARNING HISTORY ","8eea3c8e":"## MODEL DEFINITION \n\nThis model is prepared for maximum tuning by scikit GridSearchCV. Certainly you can add more feature to optimize. ","08d274e4":"#### TOP3 - AFTER 270 RUNS THE BEST PARAMETERS SO FAR:\n**Model with rank: 1**\nMean validation score: -1.744 (std: 0.004)\nParameters: {'net__module__dropout': 0.3, 'net__module__emb_output': 2, 'net__module__linear_nodes': 16, 'net__module__linear_out': 16, 'net__module__num_block': 2, 'net__optimizer': <class 'torch.optim.adam.Adam'>}\n\n**Model with rank: 2**\nMean validation score: -1.745 (std: 0.003)\nParameters: {'net__module__dropout': 0.3, 'net__module__emb_output': 2, 'net__module__linear_nodes': 32, 'net__module__linear_out': 16, 'net__module__num_block': 2, 'net__optimizer': <class 'torch.optim.adam.Adam'>}\n\n**Model with rank: 3**\nMean validation score: -1.745 (std: 0.004)\nParameters: {'net__module__dropout': 0.2, 'net__module__emb_output': 2, 'net__module__linear_nodes': 16, 'net__module__linear_out': 16, 'net__module__num_block': 3, 'net__optimizer': <class 'torch.optim.adam.Adam'>}","7f0c99c0":"## ESTIMATOR LEARNING AND BLENDING","0e506028":"## CALLBACKS \nI decided to define to callback (1) Learning Rate Scheduler and (2) Early Stopping. You can look for it using GridSearch as well but I think that this is a way better approach (dynamic search).","783bf7ad":"## TOP5 MODELS RANKING ","f0c18c6a":"## SUBMISSION","c3a654ee":"## HYPERPARAMETER SEARCH PARAMETERS ","c4a53c98":"## BEST ESTIMATOR "}}