{"cell_type":{"f9001dd4":"code","b5c721cd":"code","77064111":"code","e9d2a8d9":"code","0699de46":"code","8cd4d7b6":"code","1e8b105e":"code","482d80ae":"code","523d44b4":"markdown","e8fd4f8d":"markdown","fb9a6b60":"markdown","528b24ca":"markdown","7fd18a12":"markdown","5889edc1":"markdown","f00bf82b":"markdown","89478ef0":"markdown","e41a7e68":"markdown","923ace08":"markdown"},"source":{"f9001dd4":"import pandas as pd\nimport numpy as np\nimport re\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier","b5c721cd":"train_d = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_d = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_d.head()","77064111":"data = [train_d, test_d]\n\n# Extracting the title of passengers (Dealing with 'Name' coulmn)\ndef title(name):\n    t_search = re.search(\"([A-Za-z]+)\\.\", name)\n    if t_search:\n        return t_search.group(1)\n    return \" \"\n# Creating a dictionary (direct method can also be used without assigning)\n#train\/test['Title'].unique() was used to get different titles before making dict\nT = {'Mr': 1, 'Mrs':2, 'Miss':3, 'Master':4, 'Special':5}        \nfor i in data:\n    i['Title'] = i['Name'].apply(title)\n    i['Title'] = i['Title'].replace('Ms', 'Miss')\n    i['Title'] = i['Title'].replace('Mlle', 'Miss')\n    i['Title'] = i['Title'].replace('Mme', 'Mrs')\n    i['Title'] = i['Title'].replace(['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess','Jonkheer', 'Dona'], 'Special')       #train_d['Title'].unique() was used get all titles, 'Dona' is from test data \n    #Mapping Title\n    i['Title'] = i['Title'].map(T).astype(int)     #To check result train\/test['Age'].unique() can be used. o\/p - [1,2,3,4,5]\n\n\n#Dealing with 'Sex' coulmn\nS = {'male': 1, 'female': 0}                       #Creating dictionary\nfor i in data:                                     #Mapping 'Sex' coulmn\n    i['Sex'] = i['Sex'].map(S).astype(int)         #To check result train\/test['Age'].unique() can be used. o\/p - [0,1]\n    \n#Dealing with Age coulmn\n#Replacing missing values \nfor i in data:\n    age_mean = i['Age'].mean()\n    age_std = i['Age'].std()\n    null_count = i['Age'].isna().sum()\n    null_random_age = np.random.randint(age_mean - age_std, age_mean + age_std, size=null_count)    \n    i['Age'][np.isnan(i['Age'])] = null_random_age         #Replacing NaN values with random values\n    i['Age'] = i['Age'].astype(int)           #initially Age is float type\n    \n#Categorizing values into different age groups(i did randomly)\n    i.loc[ i['Age'] <= 16, 'Age'] = 0\n    i.loc[(i['Age'] > 16) & (i['Age'] <= 30), 'Age'] = 1\n    i.loc[(i['Age'] > 30) & (i['Age'] <= 46), 'Age'] = 2\n    i.loc[(i['Age'] > 46) & (i['Age'] <= 64), 'Age'] = 3\n    i.loc[ i['Age'] > 64, 'Age'] = 4              #To check result train\/test['Age'].unique() can be used. o\/p - [0,1,2,3,4]\n    \n#Moving onto SibSp(Siblings\/Spouse) & Parch(Parent\/Child) coulmn\nfor i in data:\n    i['Family_size'] = 1 + i['SibSp'] + i['Parch']    #Extracting total members of a family\n    i['IsAlone'] = 0\n    i.loc[i['Family_size'] == 1, 'IsAlone'] = 1       #Extracting lonley people that are expendables(JK)\n\n#Dealing with Ticket coulmn\n#multiple encoding functions can be used to encode into integer(ML readable) values. I used SklearnLabelEncoder (it's basic and simple)\nfrom sklearn.preprocessing import LabelEncoder        #You can also import it at start \nencoder=LabelEncoder()\nfor i in data:\n    i['Ticket'] = encoder.fit_transform(i['Ticket'])  #Ticket values are transformed into 3 digit int \n\n#Dealing with Ticket coulmn\ntest_d['Fare'].fillna(int('0'), inplace = True)       #test data had one missing value so i just replaced it with 0\nfor i in data:\n    i.loc[ i['Fare'] <= 8, 'Fare'] = 0                #Creating Fare groups and replacing em with 4 int vals\n    i.loc[(i['Fare'] > 8) & (i['Fare'] <= 15), 'Fare'] = 1\n    i.loc[(i['Fare'] > 15) & (i['Fare'] <= 30), 'Fare'] = 2\n    i.loc[ i['Fare'] > 30, 'Fare'] = 3                #train\/test['Fare'].max()\/min()\/mean() can be used to identify group size (max val was 512 i guess, hope that fella got money worth,JK don't mind me)\n    \n    i['Fare'] = i['Fare'].astype(int)                 #Again to check result train\/test['Age'].unique() can be used. o\/p - [0,1,2,3]\n    \n#After Fare we have Cabin\nfor i in data:\n    i['Had_Cabin'] = i['Cabin'].apply(lambda x: 1 if type(x) == str else 0)  #Non-Empty values were str and rest NaN so it was easy\n\n#Lastly Embarked column\n#(C = Cherbourg; Q = Queenstown; S = Southampton) these were port names \ntrain_d['Embarked'].fillna('S', inplace = True)      #Only 2 vals were missing so i replaced em with 'S'\nE = {'S': 0, 'C': 1, 'Q': 2}\nfor i in data:\n    i['Embarked'] = i['Embarked'].map(E)             #Mapping Embarked with E dict\n                                                     #Again to check result train\/test['Age'].unique() can be used. o\/p - [0,1,2]\n\n#Columns 'PassengerId', 'Name', 'SibSp', 'Cabin' are useless now so lets drop em into the sea(Parch is important because infants get priority)\nD = ['PassengerId', 'Name', 'SibSp', 'Cabin']\ntrain_d_final = train_d.drop(D, axis = 1)            #Here i made new DataFrame but you can also override it\ntest_d_final = test_d.drop(D, axis = 1)\n\n#Now lets summon head\ntrain_d_final.head()","e9d2a8d9":"#Seperating Dependent and Independent data\nx_train = train_d_final.drop(['Survived'], axis = 1)\ny_train = train_d_final['Survived']\nx_test = test_d_final\n\n#Using Random Forest Classifier to predict the survival of titanic passengers\nP_Id = test_d['PassengerId']\nactual_data = pd.read_csv('..\/input\/titanic\/gender_submission.csv')      #Loading actual data to check accuracy\ntrain_fit = RandomForestClassifier().fit(x_train, y_train)\nRFC_Prediction = train_fit.predict(x_test)\ndf = pd.DataFrame({'PassengerId': P_Id, 'Survived':RFC_Prediction})      #Creating dataframe comparable with actual dataframe\nprint('Training score : {}'.format(train_fit.score(x_train,y_train)))    #trainig data accuracy\nprint('Test score : {}'.format(metrics.r2_score(actual_data, df)))       #test data accuracy","0699de46":"#Visualizing Survival rate of women and men who survived and who didn't survive\nS = 'Survived'\nN = \"Didn't survived\"\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(18, 9))\nmen = train_d[train_d['Sex'] == 'male']                 #This part was done before Data Cleaning to show more graphical data(because after cleaning age is [1,2,3,4] only)\nwomen = train_d[train_d['Sex'] == 'female']\n\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(), bins=42, label = S, ax = axes[0], kde = False, color=\"green\")    \nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins=20, label = N, ax = axes[0], kde = False, color=\"red\")\nax.legend()\nax.set_title('Male Survival Rate')\n\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(), bins=42, label = S, ax = axes[1], kde =False, color=\"green\")\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins=20, label = N, ax = axes[1], kde =False, color=\"red\")\nax.legend()\nax.set_title('Female Survival Rate')","8cd4d7b6":"#Heatmap of relation of data with different data\nplt.figure(figsize = (16, 16))\nplt.title('Feature Correlation using heatmap', size=15)\nsns.heatmap(train_d_final.astype(float).corr(), linewidths=0.1,vmax=.8, \n            square=True, linecolor='white', annot=True)","1e8b105e":"#Using Randomized Search Cross Validation increasing the test (r2_score)\nfrom sklearn.model_selection import RandomizedSearchCV\n#Using some parameters\nRs_cv={\n    'n_estimators':[int(x) for x in np.linspace(start=100,stop=2000,num=5)],\n    'max_features':['auto','sqrt', 'log2'],\n    'max_depth':[int(x) for x in np.linspace(5,40,num=5)],\n    'min_samples_split':[5,10,15,42,100]\n    }\n\n#Fitting and Deoloying RSCV Model\nRS_CV = RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=Rs_cv,cv=5,verbose=2,n_jobs=-1)\nrscv_fit = RS_CV.fit(x_train, y_train)\nrscv_predict = rscv_fit.predict(x_test)\nrscv_df = pd.DataFrame({'PassengerId': P_Id, 'Survived':rscv_predict})\nprint('Training score : {}'.format(rscv_fit.score(x_train,y_train)))    #trainig data accuracy\nprint('Test score : {}'.format(metrics.r2_score(actual_data, rscv_df))) #test data accuracy","482d80ae":"#Lastly making output csv file\nrscv_df.to_csv('Titanic_RFC.csv', index = False)","523d44b4":"# 2. Dealing with Data \nI am going to clean the data column vise starting from Name to Embarked. All the data handling is going to done here. I will leave out the unnecessary code and to make this notebook simple an understandable i'll mention them in comments.","e8fd4f8d":"# Introduction\n**This** notebook is shared by Jasvir Suman(also my first upload). Regarding the notebook i had some help from other notebooks as well. Nonetheless i forged it myself with my own methods and it took me more than a day to complete it. In this notebook i'm going to use Random Forest Classifier only(will update in later entries with other algos) in oversimplified way(with training score 0.99 or 99% training data accuracy, 68% test accuracy). Randomized Search Cross Validation was used to increase test score(from 0.68 to 0.80 test score). Language i used is simple but informal. If i did something wrong in this notebook be sure to point it out(also any spelling mistakes) and i will try my best to resolve them quickly as possible. If you liked something do upvote or something(idk i'm new here).","fb9a6b60":"**2nd.**","528b24ca":"# 3. Visualizing Data\nInstead of plotting relations b\/w all the data (Different fragmrnts), I only used two plots for this notebook,\n* **1st** is distribution plot of \"Survival rate in men in relation to their age and Survival rate in women in relation to their age\".\n* **2nd** is Heatmap of correlation b\/w different feature of training data.","7fd18a12":"## 1. Importing Libraries","5889edc1":"# 4. Cross Validation\n**By Cross Validating** model accuracy of test score can be increased. I am going to use Radomized Search CV but there are also other (probably better) CV techniques that can be used.","f00bf82b":"## Keypoints collected from graphic data\n    1.More Women survived compared to men regardless of age\n    2.There is no constant\/strong correlation b\/w data ","89478ef0":"### Loading the Data","e41a7e68":"**1st.**","923ace08":"### Deploying survival prediction model using Random Forest Classifier "}}