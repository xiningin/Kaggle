{"cell_type":{"3cefd3b3":"code","54b14e0a":"code","6f71685c":"code","86cc06f2":"code","59b48762":"code","dc54d7be":"code","ff928c6d":"code","108dbd4a":"code","1e07e48a":"code","1de056fa":"code","1704025a":"code","f6687803":"code","6cfb9021":"code","b0175480":"code","718b3520":"code","63e31a61":"code","e576c1d0":"code","301804d9":"code","f6e44dc4":"code","765cb840":"code","79616d65":"code","a4620e6a":"code","2fddfdb3":"code","895e5677":"code","48768bc1":"code","5c5ba61d":"code","014f0021":"code","b5e74cfd":"code","43c1cd62":"code","3fae53e0":"code","7a4c484a":"code","3e450620":"code","2f932c7d":"code","19876ddc":"code","3df379b7":"code","5cf6201c":"code","8472de39":"code","fb4a1351":"code","058fab42":"code","e139260b":"code","dcc37277":"code","61d036ad":"code","03d2ab5b":"code","df0fe258":"code","9b1af5f4":"code","076bd390":"code","e7d64485":"code","b42f90b7":"code","0b1b8b11":"code","b900507c":"code","f45ca69e":"code","fb4933c8":"code","39cd5f09":"code","3001ea7c":"code","2872303d":"code","4abea29b":"code","e8523c67":"code","da4900bf":"code","cb4c3996":"code","fd025325":"code","985127f9":"code","61b6b70e":"code","e85ae1a8":"code","7bcaad6d":"code","935060f9":"code","00c747c7":"code","e095517c":"code","d67675f0":"code","c470b7aa":"code","79e410e2":"code","58c477bd":"code","6317a3c9":"code","9fe905fe":"code","64452cfd":"code","ff4a77e9":"code","8ad284a0":"code","e1a33463":"code","b2d8e6a1":"code","45d71543":"code","0248512f":"code","248f5985":"code","5df028d0":"code","903c26dd":"code","40a9d059":"code","7b1bc179":"code","c1978ca8":"code","7628aa74":"code","808cd041":"code","59d9b344":"code","cb85b13d":"code","cdfa3e6a":"code","aefe3787":"code","8b7662e3":"code","2f35d4b7":"code","c0ce0d47":"code","67c4dcfe":"code","5d54e923":"code","83b1c58a":"code","fa2e9267":"code","c95f6fa8":"markdown","d1ac995f":"markdown","b8750772":"markdown","7bb3bbd4":"markdown","08408e1e":"markdown","1a5e5e1b":"markdown","b174acfe":"markdown","036ca388":"markdown","eb4c710e":"markdown","be2760eb":"markdown","0389cf31":"markdown","bc6b101f":"markdown","53e050b4":"markdown","0b35b4df":"markdown","9d4f5931":"markdown","a0de53c0":"markdown","ec2e4901":"markdown","9f861412":"markdown","fb82301d":"markdown","ce68de45":"markdown","550c020b":"markdown","410364da":"markdown","f06b1d87":"markdown","5919636f":"markdown","0046793c":"markdown","8d586f74":"markdown","87ca1a8d":"markdown","6cd20489":"markdown","44959911":"markdown","e6cc5124":"markdown","76aad22a":"markdown","70842c84":"markdown","630c1eb7":"markdown","77515751":"markdown","25dc3d08":"markdown","4a26283f":"markdown","03c6d792":"markdown","5bb01fcd":"markdown","6d4c3bc7":"markdown","f745d4ef":"markdown","853909b7":"markdown","2fc41894":"markdown","e0453dd6":"markdown","a5ae1639":"markdown","02bb0fa7":"markdown","a4b32d8b":"markdown","a6596f44":"markdown","0d93e5fc":"markdown","6aeed86d":"markdown","e4fb288f":"markdown","426620c6":"markdown","8905a4af":"markdown","a84881ca":"markdown","b87f4474":"markdown","af89d24d":"markdown","e7d6c78f":"markdown","812da410":"markdown","9fb78aa4":"markdown","4afa4f7f":"markdown","b40c5873":"markdown","a1b74f57":"markdown","2bd94975":"markdown","6e112465":"markdown","ede15491":"markdown","7364d81d":"markdown"},"source":{"3cefd3b3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","54b14e0a":"train = pd.read_csv(\"..\/input\/are-your-employees-burning-out\/train.csv\")\ntest = pd.read_csv(\"..\/input\/are-your-employees-burning-out\/test.csv\")","6f71685c":"train.head()","86cc06f2":"train.info()","59b48762":"# number of unique values per variable\ntrain.nunique()","dc54d7be":"# number of empty values\ntrain.isnull().sum()","ff928c6d":"# number of empty values (test)\ntest.isnull().sum()","108dbd4a":"# cosmetic configurations\nsns.set_style(style=\"whitegrid\")\n\ntitle_font = {\"family\":\"sans-serif\", \n              \"weight\":\"bold\", \n              \"color\":\"darkgreen\", \n              \"size\":16}\n\naxis_font = {\"family\":\"serif\", \n             \"weight\":\"normal\", \n             \"color\":\"darkgreen\", \n             \"size\":14}","1e07e48a":"is_male = pd.get_dummies(train.Gender, drop_first=True)\nis_service = pd.get_dummies(train[\"Company Type\"], drop_first=True)\nwfh_available = pd.get_dummies(train[\"WFH Setup Available\"], drop_first=True)","1de056fa":"for loc, column in enumerate([\"is_male\", \"is_service\", \"wfh_available\"], start=2):\n    train.insert(loc=loc, column=column, value=eval(column))\n\ntrain.drop(columns=[\"Gender\", \"Company Type\", \"WFH Setup Available\"], axis=1, inplace=True)","1704025a":"train.head()","f6687803":"is_male = pd.get_dummies(test.Gender, drop_first=True)\nis_service = pd.get_dummies(test[\"Company Type\"], drop_first=True)\nwfh_available = pd.get_dummies(test[\"WFH Setup Available\"], drop_first=True)\n\nfor loc, column in enumerate([\"is_male\", \"is_service\", \"wfh_available\"], start=2):\n    test.insert(loc=loc, column=column, value=eval(column))\n\ntest.drop(columns=[\"Gender\", \"Company Type\", \"WFH Setup Available\"], \n          axis=1, \n          inplace=True)","6cfb9021":"%%time\nmonth_dict = {\"01\":31, \"02\":29, \"03\":31, \n              \"04\":30, \"05\":31, \"06\":30, \n              \"07\":31, \"08\":31, \"09\":30, \n              \"10\":31, \"11\":30, \"12\":31}\n\nfor dataset in (train, test):\n    for i in range(len(dataset)):\n        x = dataset[\"Date of Joining\"][i].split(\"-\")    # sample output: ['2008', '09', '30']\n        x = eval(f\"{int(x[1])} * {month_dict[x[1]]} + {int(x[2])}\")    # sample output: 300\n        dataset.loc[i, \"Date of Joining\"] = x\n\n    dataset[\"Date of Joining\"] = dataset[\"Date of Joining\"].astype(\"float32\")\n    dataset[\"Date of Joining\"] = abs(dataset[\"Date of Joining\"] - dataset[\"Date of Joining\"].max())\n    dataset[\"Date of Joining\"] \/= dataset[\"Date of Joining\"].max()","b0175480":"# there are NaNs in the most critical variables\n# let's dive into the details\nresource_nan = train[\"Resource Allocation\"].isna()\nmental_nan = train[\"Mental Fatigue Score\"].isna()\nburnrate_nan = train[\"Burn Rate\"].isna()\n\n# No \"Burn Rate\" & \"Mental Fatigue Score\" values\ntrain[(burnrate_nan) & (mental_nan)]","718b3520":"# Index values of NaN values of \"Burn Rate\" & \"Mental Fatigue Score\"\ndouble_nan_indices = train[(burnrate_nan) & (mental_nan)].index\n\ntrain.drop(index=double_nan_indices, axis=0, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\ndel double_nan_indices, is_male, is_service, wfh_available","63e31a61":"# okay, what else?\ntrain.isna().sum()","e576c1d0":"train_corr = train.corr()","301804d9":"plt.figure(figsize=(8,5))\nsns.heatmap(data=train_corr, \n            annot=True, \n            fmt=\".2f\", \n            vmin=-1, \n            vmax=1, \n            cmap=\"inferno\")\nplt.yticks(rotation=0)\nplt.show()","f6e44dc4":"# We have very high correlation between some of the variables.\n# Let's focus on \"Mental Fatigue Score\" and \"Burn Rate\"\nplt.figure(figsize=(9, 5))\nplt.scatter(\"Mental Fatigue Score\", \"Burn Rate\", data=train, s=0.3, c=\"darkgreen\")\nplt.plot([0,10], [0, 1], linewidth=2, c=\"darkred\")\nplt.xlabel(\"Mental Fatigue Score\", fontdict=axis_font)\nplt.ylabel(\"Burn Rate\", fontdict=axis_font)\nplt.xticks(range(0,11))\nplt.show()","765cb840":"LINEAR_CONST = 7 \/ 10","79616d65":"fig, (ax1, ax2) = plt.subplots(1,2, sharex=True, sharey=True, figsize=(18 ,5))\nfig.suptitle(\"Mental Fatigue Score vs. Burn Rate\", color=\"black\", size=22)\n\nax1.scatter(\"Mental Fatigue Score\", \n            \"Burn Rate\", \n            data=train, \n            s=0.3, \n            c=\"darkgreen\")\nax1.plot([0,10], [0, 1], linewidth=2, c=\"darkred\")\nax1.set_title(\"Original Graph\")\nplt.xticks(range(0,11))\n\nax2.scatter(train[\"Mental Fatigue Score\"], \n            train[\"Burn Rate\"] ** LINEAR_CONST, \n            s=0.3, \n            c=\"green\")\nax2.plot([0,10], [0, 1], linewidth=2, c=\"darkred\")\nax2.set_title(\"Transformed Graph\")\n\nfig.text(x=0.09, y=0.5, \n         s=\"Burn Rate\", \n         va=\"center\", rotation=\"vertical\", size=20, c=\"darkred\")\nfig.text(x=0.42, y=0.03, \n         s=\"Mental Fatigue Score\", \n         va=\"center\", rotation=\"horizontal\", size=20, c=\"darkred\")\nplt.show()","a4620e6a":"# Note that we are not changing \"Burn Out\". It's our target variable. \n# We need to modify the fatigue score. Thus -1.\ntrain[\"Mental Fatigue Score\"] **= (LINEAR_CONST**-1)\ntest[\"Mental Fatigue Score\"] **= (LINEAR_CONST**-1)","2fddfdb3":"# to keep things safe, let's play on a copy of our train dataset\ntrain_copy = pd.DataFrame()\nfor i in train.columns[-3:]:\n    fill_with = train[i].interpolate(method=\"linear\")\n    train_copy[i] = train[i].fillna(fill_with, inplace=False)\n\nplt.figure(figsize=(9, 5))\nplt.scatter(\"Mental Fatigue Score\", \"Burn Rate\", \n            data=train_copy, \n            s=0.3, \n            c=\"darkgreen\")\nplt.plot([0, train[\"Mental Fatigue Score\"].max()], [0, 1], \n         linewidth=2, \n         c=\"darkred\")\nplt.xlabel(\"Mental Fatigue Score\", fontdict=axis_font)\nplt.ylabel(\"Burn Rate\", fontdict=axis_font)\nplt.show()\ndel train_copy","895e5677":"describe_original = train.describe()\ndescribe_original","48768bc1":"from scipy import interpolate","5c5ba61d":"# Create the copy of our dataset and\n# make sure that there are no NaN values in\n# \"Mental Fatigue Score\" & \"Burn Rate\" variables\n\ntrain_copy = train.copy(deep=True)\n\nnot_na1 = train_copy[\"Mental Fatigue Score\"].notna()\nnot_na2 = train_copy[\"Burn Rate\"].notna()\n\ntrain_copy = train_copy[(not_na1) & (not_na2)]\ndel not_na2, not_na1","014f0021":"# function to find the burn rate\nfn_burn = interpolate.interp1d(x=train_copy[\"Mental Fatigue Score\"], \n                               y=train_copy[\"Burn Rate\"], \n                               kind=\"linear\", \n                               fill_value=None)\n# function to find the mental fatigue score\nfn_mental = interpolate.interp1d(y=train_copy[\"Mental Fatigue Score\"], \n                                 x=train_copy[\"Burn Rate\"], \n                                 kind=\"linear\", \n                                 fill_value=None)","b5e74cfd":"# get rid of these NaNs\nfor i in train[train[\"Burn Rate\"].isna()].index:\n    train.loc[i, \"Burn Rate\"] = fn_burn(train.loc[i, \"Mental Fatigue Score\"])\n    \nfor i in train[train[\"Mental Fatigue Score\"].isna()].index:\n    train.loc[i, \"Mental Fatigue Score\"] = fn_mental(train.loc[i, \"Burn Rate\"])","43c1cd62":"# well, what now?\ntrain.isna().sum()","3fae53e0":"# remember that \"Resource Allocation\" is also highly correlated with Designation.\nplt.figure(figsize=(6, 5))\nplt.scatter(\"Designation\", \"Resource Allocation\", data=train, c=\"darkred\")\nplt.xlabel(\"Designation\", fontdict=axis_font)\nplt.ylabel(\"Resource Allocation\", fontdict=axis_font)\nplt.yticks(range(0,11))\nplt.show()","7a4c484a":"# We will get the mean of each \"Designation\" value and replace the NaN with them \n# Here, an example: \ntemp_value = train[\"Resource Allocation\"][train[\"Designation\"]==1].mean()\n\nprint(f\"The mean value of Resource Allocation where \\\nDesignation == 1 is: {temp_value:.2f}\")","3e450620":"for i in range(6):    # 0 to 5, Designation values\n    mean_value = train[\"Resource Allocation\"][train[\"Designation\"]==i].mean()\n    \n    # condition: \"Designation\" == i AND \"Resource Allocation\" is NaN\n    condition = (train[\"Designation\"]==i) & (train[\"Resource Allocation\"].isna())\n    \n    # all NaN values are converted to mean values:\n    train.loc[condition, \"Resource Allocation\"] = mean_value\n\ndel temp_value, train_copy, fn_burn, fn_mental","2f932c7d":"# ...and it's gone!\ntrain.isna().sum()","19876ddc":"describe_botox = train.describe()\n\nprint(\"--------------- Original Data ---------------\")\ndisplay(describe_original.iloc[:3, -3:])\nprint(\"\\n--------------- Cleaned Data ---------------\")\ndisplay(describe_botox.iloc[:3, -3:])","3df379b7":"# !pip install -U pandas-profiling","5cf6201c":"# from pandas_profiling import ProfileReport","8472de39":"# profile = ProfileReport(train, title=\"Profiling Report\", explorative=True)\n# profile.to_notebook_iframe()","fb4a1351":"# profile.to_file(\".\/burning_out_report.html\")","058fab42":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","e139260b":"# 25% of the train set is split as a validation set\nX_train, X_val, \\\ny_train, y_val = train_test_split(train.iloc[:, 1:-1],\n                                         train.iloc[:, -1], \n                                         test_size=0.25, \n                                         shuffle=True, \n                                         random_state=19)\nX_test = test.iloc[:, 1:]\ny_test = None\n\nprint(\"Shape of the train set:\\nX_train:\", X_train.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"\\nShape of the validation set:\\nX_val:\", X_val.shape)\nprint(\"y_val:\", y_val.shape)\nprint(\"\\nShape of the test set:\\nX_test:\", X_test.shape)","dcc37277":"scale = StandardScaler()\nnormalize = MinMaxScaler((0, 1))\n\n# scaled set: mean=0, standard deviation=1\nX_train_std = scale.fit_transform(X_train)\nX_val_std = scale.fit_transform(X_val)\nX_test_std = scale.fit_transform(X_test)\n\n# normalized set: values are between [0, 1]\nX_train_norm = normalize.fit_transform(X_train)\nX_val_norm = normalize.fit_transform(X_val)\nX_test_norm = normalize.fit_transform(X_test)","61d036ad":"import statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom statsmodels.tools.eval_measures import mse, rmse","03d2ab5b":"# Let me create a function for error values\ndef show_errors(y_validations, y_predictions):\n    \"\"\"Function to show error statistics of the validation part.\n    y_validations: Validation values\n    y_predictions: Predicted values\"\"\"\n    mae_fn = mae(y_validations, y_predictions)\n    mse_fn = mse(y_validations, y_predictions)\n    rmse_fn = rmse(y_validations, y_predictions)\n\n    print(\"\\n---------Error Statistics of Validation Part---------\")\n    print(f\"Mean Absolute Error (MAE)             : {mae_fn:.4f}\")\n    print(f\"Mean Square Error (MSE)               : {mse_fn:.4f}\")\n    print(f\"Root Mean Square Error (RMSE)         : {rmse_fn:.4f}\\n\")","df0fe258":"X_train_ols = sm.add_constant(X_train)\nols_results_model = sm.OLS(y_train, X_train_ols)\nols_results = ols_results_model.fit()","9b1af5f4":"ols_results.summary()","076bd390":"# validation data\nX_val_ols = sm.add_constant(X_val)\nols_results_val_model = sm.OLS(y_val, X_val_ols)\nols_results_val = ols_results_val_model.fit()\nols_results_val.summary()","e7d64485":"show_errors(y_val, \n            ols_results_val.predict(sm.add_constant(X_val_std))\n           )","b42f90b7":"def show_comparison(y_validations, y_predictions):\n    \"\"\"To see the comparison, please provide:\n    y_validations and y_predictions\"\"\"\n    plt.figure(figsize=(12,6))\n    plt.scatter(y_validations, \n                y_predictions, \n                color=\"k\", alpha=0.6, s=10, label=\"Predicted\")\n    plt.plot(y_validations, y_validations, \"r--\", label=\"True\")\n    plt.xlabel(\"True Values\", fontdict=axis_font)\n    plt.ylabel(\"Predicted Values\", fontdict=axis_font)\n    plt.title(\"True vs. Predicted Values\", fontdict=title_font)\n    plt.legend(fontsize=\"large\", loc=\"best\")\n    plt.show()","0b1b8b11":"show_comparison(y_val, \n                ols_results_val.predict(sm.add_constant(X_val_std)))","b900507c":"# Assumption: The relationship between the dependent (Y) and independent (X)\n# variables should be linear.\n# That's why we linearized the fatigue score to get a better result.","f45ca69e":"# Assumption: Error term should be near zero in means\npredict = ols_results.predict(X_train_ols)\nerrors = y_train - predict\nprint(\"The average error between the predicted & real value: \\\n{:.3g}\".format(np.mean(errors)))","fb4933c8":"# Assumption: Homoscedasticity \n# The error variance mustn't systematically change across the observations,\n# i.e it should be the same for all the values of independent variables, X\nplt.figure(figsize=(12,6))\nplt.scatter(predict, errors, color=\"darkgreen\", s=3, alpha=0.75)\nplt.xlabel('Predicted Value', fontdict=axis_font)\nplt.ylabel('Residuals', fontdict=axis_font)\nplt.axhline(y=0, color=\"r\")\nplt.title('Residuals vs. Predict', fontdict=title_font)\nplt.show()","39cd5f09":"from scipy.stats import bartlett\nfrom scipy.stats import levene\n\nbart_stats = bartlett(predict, errors)\nlev_stats = levene(predict, errors)\n\nprint(\"Bartlett test statistic value is {0:.2f} \\\nand p-value is {1:.3g}\".format(bart_stats[0], bart_stats[1]))\nprint(\"Levene test statistic value is {0:.2f} \\\nand p-value is {1:.3g}\".format(lev_stats[0], lev_stats[1]))","3001ea7c":"# Assumption: Low multicollinearity\n# We saw that there are high collinearities among the features :(","2872303d":"# Assumption: Error terms should be uncorrelated with one another\nplt.figure(figsize=(14,4))\nplt.subplot(1,2,1)\nplt.plot(errors, linewidth=0.1)\n\nplt.subplot(1,2,2)\nfrom statsmodels.tsa.stattools import acf\nacf_data = acf(errors)\nplt.plot(acf_data[1:], color=\"red\")\nplt.show()","4abea29b":"# Assumption: Features also shouldn't be correlated with the errors (exogeneity)\nrand_nums = np.random.normal(np.mean(errors), \n                             np.std(errors), \n                             len(errors))\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nplt.scatter(rand_nums, errors, color=\"darkblue\", s=3, alpha=0.75)\nplt.xlabel(\"Normally Distributed Random Variables\", fontdict=axis_font)\nplt.ylabel(\"Errors of the Model\", fontdict=axis_font)\nplt.title(\"QQ plot\", fontdict=title_font)\n\nplt.subplot(1,2,2)\nplt.hist(errors, bins=30, color=\"darkred\")\nplt.xlabel(\"Errors\", fontdict=axis_font)\nplt.title(\"Histogram of the Errors\", fontdict=title_font)\n\nplt.tight_layout()\nplt.show()","e8523c67":"from scipy.stats import jarque_bera\nfrom scipy.stats import normaltest\njb_stats = jarque_bera(errors)\nnorm_stats = normaltest(errors)\n\nprint(\"Jarque-Bera test statistics is {0:.2f} and \\\np value is {1:.5g}\".format(jb_stats[0], jb_stats[1]))\nprint(\"Normality test statistics is {0:.2f} and p \\\nvalue is {1:5g}\".format(norm_stats[0], norm_stats[1]))","da4900bf":"from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV","cb4c3996":"lasso_CV_model = LassoCV(alphas=np.logspace(-8, 6, num=30, base=10.0), \n                         cv=10, \n                         n_jobs=-1)\nlasso_CV = lasso_CV_model.fit(X_train_std, y_train)\n\nlasso_CV_score = lasso_CV.score(X_train_std, y_train)\nprint(f\"Adjusted R-square value of train set: {lasso_CV_score:.3f}\")\nlasso_CV_score = lasso_CV.score(X_val_std, y_val)\nprint(f\"Adjusted R-square value of validation set: {lasso_CV_score:.3f}\")\n\nshow_errors(y_val, lasso_CV.predict(X_val_std))","fd025325":"ridge_CV_model = RidgeCV(alphas=np.logspace(-8, 6, num=30, base=10.0), \n                         cv=10)\nridge_CV = ridge_CV_model.fit(X_train_std, y_train)\n\nridge_CV_score = ridge_CV.score(X_train_std, y_train)\nprint(f\"Adjusted R-square value of train set: {ridge_CV_score:.3f}\")\nridge_CV_score = ridge_CV.score(X_val_std, y_val)\nprint(f\"Adjusted R-square value of validation set: {ridge_CV_score:.3f}\")\n\nshow_errors(y_val, ridge_CV.predict(X_val_std))","985127f9":"elasticNet_CV = ElasticNetCV(alphas=np.logspace(-6, 6, 30), \n                             l1_ratio=0.5, \n                             cv=10).fit(X_train_std, y_train)\nelasticNet_score = elasticNet_CV.score(X_train_std, y_train)\n\nelasticNet_CV_score = elasticNet_CV.score(X_train_std, y_train)\nprint(f\"Adjusted R-square value of train set: {elasticNet_CV_score:.3f}\")\nelasticNet_CV_score = elasticNet_CV.score(X_val_std, y_val)\nprint(f\"Adjusted R-square value of validation set: {elasticNet_CV_score:.3f}\")\n\nshow_errors(y_val, elasticNet_CV.predict(X_val_std))","61b6b70e":"from sklearn.preprocessing import PolynomialFeatures","e85ae1a8":"import warnings; warnings.filterwarnings(\"ignore\")\n\npoly = PolynomialFeatures(degree=4, \n                          interaction_only=False, \n                          order=\"F\")\nX_train_poly = poly.fit_transform(X_train_std)\nX_val_poly = poly.fit_transform(X_val_std)\n\nlasso_CV_poly = LassoCV(alphas=np.logspace(-8, 6, num=30, base=10.0), \n                        cv=10, \n                        n_jobs=-1).fit(X_train_poly, y_train)\n\nlasso_score_poly = lasso_CV_poly.score(X_train_poly, y_train)\nprint(f\"Adjusted R-square value of train set: {lasso_score_poly:.3f}\")\nlasso_score_poly = lasso_CV_poly.score(X_val_poly, y_val)\nprint(f\"Adjusted R-square value of validation set: {lasso_score_poly:.3f}\")\n\nshow_errors(y_val, lasso_CV_poly.predict(X_val_poly))","7bcaad6d":"show_comparison(y_val, lasso_CV_poly.predict(X_val_poly))","935060f9":"import datetime\ndef export_output(algorithm, \n                  test_data, \n                  name=datetime.datetime.now().strftime(\"hour%H%M\") + \"_output\"):\n    \"\"\"This function exports the predicted test data as a .csv file.\n    Three input needs to be provided:\n    1) The model you wish to use e.g \"lasso_CV_poly\",\n    2) Predictions created by X_test, X_test_std or something similar,\n    3) (optional) Name for the output e.g 'lasso_output'.\"\"\"\n    test_predict = algorithm.predict(test_data)\n    predicted_output = pd.concat([pd.Series(test.iloc[:, 0]), \n                                            pd.Series(test_predict)],\n                                 axis=1)\n    wrong_column_name = predicted_output.columns[-1]\n    predicted_output.rename(columns = {wrong_column_name:\"Burn Rate\"},\n                            inplace=True)\n    predicted_output.to_csv(path_or_buf=(name + \".csv\"), \n                            index=False)\n    predicted_output.head()","00c747c7":"export_output(lasso_CV_poly, poly.fit_transform(X_test_std), \n              name = \"lassoPoly_output\")","e095517c":"# %whos    # to check who is getting in our way","d67675f0":"# Let's clean up the place before we continue.\ndel lasso_CV_score, ridge_CV_score, elasticNet_CV_score, poly, LassoCV, RidgeCV,\\\n    lasso_score_poly, warnings, jarque_bera, bartlett,\\\n    norm_stats, jb_stats, rand_nums, acf_data, X_val_ols, bart_stats,\\\n    lev_stats, predict, errors, X_train_ols, ols_results_model, ols_results,\\\n    ols_results_val_model, ols_results_val, lasso_CV_model, lasso_CV, ridge_CV,\\\n    ridge_CV_model, resource_nan, mental_nan, burnrate_nan, acf, ElasticNetCV,\\\n    month_dict, fill_with, elasticNet_CV, elasticNet_score, train_corr, levene,\\","c470b7aa":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV","79e410e2":"knn_model = KNeighborsRegressor(n_jobs=-1)\nknn_params = {\"n_neighbors\":range(10,20), \n              \"weights\":(\"uniform\", \"distance\"), \n              \"metric\":(\"euclidean\", \"minkowski\", \"manhattan\")\n              }","58c477bd":"def train_model(estimator, parameters, cv=10, verbose=1, X_data=X_train_std):\n    \"\"\"Function to train your model with a grid search.\n    Just provide the function an estimator and its parameters.\n    'estimator' example: knn_model\n    'parameters' example: knn_params\n    Optionally, you can change 'cv', 'verbose' or X_input as well.\"\"\"\n    gridCV = GridSearchCV(estimator=estimator, \n                          param_grid=parameters, \n                          n_jobs=-1, \n                          cv=cv, \n                          verbose=verbose)\n    gridCV.fit(X_data, y_train)\n\n    print(\"\\nBest training parameters:\", gridCV.best_params_)\n    print(\"Best training score: {:.4f}\".format(gridCV.best_score_))\n    show_errors(y_val, gridCV.predict(X_val_std))\n    return gridCV","6317a3c9":"knn_gridCV = train_model(knn_model, knn_params)","9fe905fe":"knn_results = pd.DataFrame(knn_gridCV.cv_results_)\nknn_results = knn_results[[\"param_metric\", \"param_n_neighbors\",\n                           \"param_weights\", \"mean_test_score\"]]\nknn_results = knn_results.sort_values(by=\"mean_test_score\", ascending=False)\nknn_results.head()","64452cfd":"# import pydotplus, graphviz\n# from IPython.display import Image\nfrom sklearn.tree import DecisionTreeRegressor    #, export_graphviz","ff4a77e9":"tree_model = DecisionTreeRegressor(random_state=19)\ntree_params = {\"max_depth\":range(8,10), \n               \"min_samples_split\":range(2,5), \n               \"splitter\":(\"best\", \"random\"), \n               \"min_samples_leaf\":range(5,15)\n              }\n\ntree_gridCV = train_model(tree_model, tree_params)","8ad284a0":"tree_results = pd.DataFrame(tree_gridCV.cv_results_)\ntree_results = tree_results[[\"param_max_depth\", \n                             \"param_min_samples_split\",\n                             \"param_splitter\", \n                             \"param_min_samples_leaf\", \n                             \"mean_test_score\"]]\ntree_results = tree_results.sort_values(by=\"mean_test_score\", \n                                        ascending=False)\ntree_results.head()","e1a33463":"export_output(algorithm = tree_gridCV, test_data = X_test_std,\n              name = \"decisionTree_output\")","b2d8e6a1":"import time\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, cross_val_predict","45d71543":"# These values are from a series of everlasting grid searches.\nforest_model = RandomForestRegressor(n_estimators=200, \n                                     max_depth=8, \n                                     criterion=\"mae\", \n                                     min_samples_leaf=5, \n                                     min_samples_split=2, \n                                     random_state=19)","0248512f":"forest_scoreCV = cross_val_score(estimator=forest_model, \n                                 X=X_train_std, \n                                 y=y_train, \n                                 cv=5, \n                                 verbose=2, \n                                 n_jobs=-1)","248f5985":"print(f\"Mean score of Random Forest model: {forest_scoreCV.mean():.3f}\")","5df028d0":"forest_predictCV = cross_val_predict(estimator=forest_model, \n                                     X=X_val_std, \n                                     y=y_val, \n                                     cv=5, \n                                     verbose=2, \n                                     n_jobs=-1)","903c26dd":"show_errors(y_val, forest_predictCV)","40a9d059":"# Variable named 'sp' stands for 'significant pairs'\nforest_model.fit(X_val_std, y_val)\n\nsp = dict(zip(X_train.columns, forest_model.feature_importances_))\n# sorting w.r.t their significance values:\nsp = {k: v for k, v in sorted(sp.items(), key=lambda i: i[1], reverse=True)}\n\nsignificant_names = list(sp.keys())\nsignificant_values = list(sp.values())\n\nsignificance = pd.Series(data=significant_values,\n                         index=significant_names).sort_values()\nplt.figure(dpi=75)\nsignificance.plot(kind=\"barh\", color=\"darkred\")\nplt.title(\"Significance of The Features\")\nplt.show()\ndel sp, significant_names, significant_values, significance","7b1bc179":"from sklearn.svm import SVR","c1978ca8":"time_start = time.time()\nsvm_model = SVR(C=0.01, kernel=\"linear\")    # grid searched prior to this\nsvm_params = {\"epsilon\":np.arange(0.01, 0.1, 0.02)}\n\nsvm_gridCV = train_model(svm_model, svm_params, cv=5)\nprint(\"\\nTraining is completed in {:.2f} seconds\".format(time.time() - time_start))","7628aa74":"svm_results = pd.DataFrame(svm_gridCV.cv_results_)\nsvm_results = svm_results[[\"param_epsilon\", \"mean_test_score\"]]\nsvm_results = svm_results.sort_values(by=\"mean_test_score\", \n                                            ascending=False)\nsvm_results.head()","808cd041":"import xgboost as xgb","59d9b344":"xgb_model = xgb.XGBRegressor(booster=\"gbtree\", \n                             gamma=0, \n                             max_depth=6, \n                             alpha=0, \n                             eta=0.1)\n# again, after a series of grid search, this one is the last one:\nxgb_params = {\"objective\":[\"reg:squarederror\"],\n              \"n_estimators\":[47], \n              \"min_child_weight\":[7]}\n\nxgb_gridCV = train_model(xgb_model, xgb_params, cv=20, verbose=1)","cb85b13d":"xgb_results = pd.DataFrame(xgb_gridCV.cv_results_)\nxgb_results = xgb_results[[\"param_objective\",  \n                           \"param_n_estimators\", \n                           \"param_min_child_weight\", \n                           \"mean_test_score\"]]\nxgb_results = xgb_results.sort_values(by=\"mean_test_score\", \n                                            ascending=False)\nxgb_results.head()","cdfa3e6a":"# let's save this model too\nexport_output(xgb_gridCV, X_test_std, name=\"xgb_output\")","aefe3787":"# feature importance\nxgb_model2 = xgb.train(params=xgb_gridCV.best_params_, \n                      dtrain=xgb.DMatrix(data=X_train_std, label=y_train))\nax = xgb.plot_importance(xgb_model2)\nax.figure.set_size_inches(10, 8)\nplt.show()","8b7662e3":"from keras import models, layers\nfrom keras.optimizers import Adam, schedules","2f35d4b7":"# to avoid repeating the layers\ndef dense_layer(unit):\n    model.add(layers.Dense(units=unit))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))","c0ce0d47":"model = models.Sequential()\nmodel.add(layers.Dense(units=16,\n                       input_shape=(X_train_norm.shape[1], ), \n                       kernel_initializer=\"GlorotUniform\", \n                       name=\"Hidden_Layer1\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Activation(\"relu\"))\n#dense_layer(64)\nmodel.add(layers.Dropout(0.05))\n#dense_layer(128)\nmodel.add(layers.Dense(1, name=\"Output_Layer\"))\nmodel.summary()","67c4dcfe":"lr_schedule = schedules.ExponentialDecay(initial_learning_rate=1e-3,\n                                         decay_steps=10000,\n                                         decay_rate=0.9)\n\nopt = Adam(learning_rate=lr_schedule)\nmodel.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])\nmodel_history = model.fit(X_train_norm, y_train, \n                          epochs=30, \n                          batch_size=32, \n                          validation_data=(X_val_norm, y_val), \n                          shuffle=False, \n                          verbose=0)","5d54e923":"show_errors(y_val, model.predict(X_val_norm).reshape(-1))","83b1c58a":"history_dict = model_history.history\n\ntrain_loss = history_dict[\"loss\"]\nval_loss = history_dict[\"val_loss\"]\ntrain_mae = history_dict[\"mae\"]\nval_mae = history_dict[\"val_mae\"]","fa2e9267":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nplt.plot(range(1, len(train_mae)+1), train_mae, \n         color=\"darkgreen\", linewidth=2, label=\"Train Set\")\n\nplt.plot(range(1, len(val_mae)+1), val_mae, \n        \"r--\", linewidth=2, label=\"Valdation Set\")\n\nplt.xlabel(\"Epochs\", fontdict=axis_font)\nplt.ylabel(\"Mean Absolute Error\", fontdict=axis_font)\nplt.legend(fontsize=\"medium\", loc=(0.7,0.5))\n\nplt.subplot(1,2,2)\nplt.plot(range(1, len(train_loss)+1), train_loss, \n         color=\"darkgreen\", linewidth=2, label=\"Train Set\")\n\nplt.plot(range(1, len(val_loss)+1), val_loss, \n        \"r--\", linewidth=2, label=\"Valdation Set\")\n\nplt.xlabel(\"Epochs\", fontdict=axis_font)\nplt.ylabel(\"Loss Value\", fontdict=axis_font)\nplt.legend(fontsize=\"medium\", loc=(0.7,0.5))\n\nplt.show()","c95f6fa8":"#### 2.2.1.1 Train Set","d1ac995f":"#### 2.2.1.2 Test Set","b8750772":"Let's go over the steps quickly and get the party started:<br>\n* Import the essential modules and the dataset (duh!).\n* Check out the NaN and unique values and also the value types.\n* Perform one-hot encoding if necessary to make things easier for the models.\n  * ...because there are a couple of binary features as well as some discrete & continuous ones.\n* Modify the features to get some useful information out of them.\n  * Feature engineering, e.g how about we inspect WFH option by gender?\n* Fill in the blanks (NaNs) with proper values.\n  * Though we might just get rid of them as well.\n* Create a baseline model to see where we are.\n* Try different models (with different features) and tune their hyperparameters\n* Dance with the best model till sunrise.","7bb3bbd4":"### 3.2.5 Lasso Regression with Polynomial Features","08408e1e":"# HackerEarth Machine Learning Challenge\n## Predict the employee burn out rate","1a5e5e1b":"## 3.1 Data Preparation","b174acfe":"***\n**Participant:** [Mert G\u00fcl](https:\/\/www.linkedin.com\/in\/gulmert89\/)<br>\nHackathon Link: [HackerEarth](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-predict-burnout-rate\/)<br>\nMore information on the dataset: [Kaggle](https:\/\/www.kaggle.com\/blurredmachine\/are-your-employees-burning-out)\n***","036ca388":"### 3.2.1 Ordinary Least Squares (As a baseline model)","eb4c710e":"### 2.2.2 The Column: \"Date of Joining\"\nAll the employees in the dataset seem to join in 2008. Thus, omiting the year and converting the dates to integers (just to days) ***might*** help us a bit along the way. Here what we are going to do is:<br>\n* **Remove the year**\n* **Convert month + day to only days**\n    * *Example-1:* join date of 01-20 (Jan 20th) will have $1*31 + 20 = 51$ days\n    * *Example-2:* join date of 12-30 (Dec 30th) will have $2*31 + 30 = 402$ days\n* **Inverse the sorting** because the smaller the integers are, the more experienced employee will be.\n    * *Example:* So, the newest guy will have the maximum, the oldest will have the minimum days, right?\n    * To inverse the situation, a simple calculation will be performed:\n        * At the end, the oldest guy will have $|newGuysDays - oldGuysDays|$ in total w.r.t the new guy.\n        * And the newest guy will have 0 days.\n* Finally, **Normalize the numbers** to work with them better.<br>\n\n**Note:** All these process would be in vain because it might not affect the model at all. We will see :)","be2760eb":"Dataset:\n* **Employee ID**: The unique ID allocated for each employee (example: fffe390032003000)\n* **Date of Joining**: The date-time when the employee has joined the organization (example: 2008-12-30)\n* **Gender**: The gender of the employee (Male\/Female)\n* **Company Type**: The type of company where the employee is working (Service\/Product)\n* **WFH Setup Available**: Is the work from home facility available for the employee (Yes\/No)\n* **Designation**: The designation of the employee of work in the organization.\n    * In the range of [0.0, 5.0] bigger is higher designation.\n* **Resource Allocation**: The amount of resource allocated to the employee to work, ie. number of working hours.\n    * In the range of [1.0, 10.0] (higher means more resource)\n* **Mental Fatigue Score**: The level of fatigue mentally the employee is facing.\n    * In the range of [0.0, 10.0] where 0.0 means no fatigue and 10.0 means completely fatigue.\n* **Burn Rate**: The value we need to predict for each employee telling the rate of Bur out while working.\n    * In the range of [0.0, 1.0] where the higher the value is more is the burn out.","0389cf31":"Before we move on, it would be a wise practice to check the correlations of the variables to form a solid idea on what to do with the NaN values.","bc6b101f":"1) I have a confession to make: If we skipped the whole NaN value modification and just killed them all with <code>pandas.DataFrame.dropna()<\/code> function, we would have had better results in all of the models including our *champ* XGBoost. Removing all the NaNs and running the dataset on XGBoost gave me 92.98% submission score, slightly higher than the former but unfortunatelly it's not the one I submitted before the deadline. These are the best XGBoost scores from this notebook (You saw the 1st one. 2nd is the result of <code>.dropna()<\/code> trick):\n\n\n![The Winner XGBOOST](https:\/\/i.pinimg.com\/originals\/4e\/e6\/ff\/4ee6ff59edbbcab85a462002435f1ef6.png)","53e050b4":"### 3.2.4 ElasticNet Regression","0b35b4df":"Let's keep the data with polynomial features. It seems quite okay. We submit the data and see what happens later.<br> ***(PS: We almost hit $92.7\\% \\space R^2$ value. I'm among the first 50 even at this point.)***","9d4f5931":"4) Obviously, there is still some room to make improvements over those bullet points but for me, that's all folks! This was an easy dataset since \"Mental Fatigue Score\" dominated our prediction process and left little room for other features but I hope this notebook shines a little bit of light on the things you didn't know before. Wish you a better luck than I had! :) If you think there seems a better way (another model, a different hyperparameter or an interesting way of feature engineering), let me know. We are all here in this world to learn!","a0de53c0":"## 1.1 Imports & Dataset","ec2e4901":"This is a module that I've recently discovered so you may consider this section as a note to myself and skip it. As title stated, it's a \"fancier\" way of exploratory data analysis with an interactive GUI.\n\n*Note-1: The module needs to be updated to be used on Colab.<br>Please use the command and restart the runtime:*\n\n<code>!pip install -U pandas-profiling<\/code>\n\n*Note-2: I commented out the codes because GitHub doesn't show the interactive stuff on its page even though it keeps the files on the cache. So please uncomment them and feel free to explore our cleaned data.*","9f861412":"## 2.3 Bonus: A Fancy Way of EDA","fb82301d":"***\n# 2\\. Preprocessing","ce68de45":"**ABOUT CHALLENGE** <br>\n> *To win in the marketplace, you must first win in the workplace.*\n~ Doug Conant\n\n> And rightly so! Happy and healthy employees are indisputably more productive at work, and in turn, help the business flourish profoundly.\n\n> However, since working from home has become the new strange normal, over 69% of employees have been showing burnout symptoms globally (source: Monster poll). This rate of burnout is indeed alarming and the likes of Google and Cisco (even HackerEarth!) are taking steps that ensure wellness and reduce burnout among employees.\n\n> On the occasion of World Mental Health Day this October 10th, your organization has stepped in to help its employees plan their weekly calendars appropriately so as to prevent burning out. Your task, as a Machine Learning engineer, is to build a Machine Learning model that predicts burnout rate based on numerous factors such as WFH setup, resources, mental fatigue score, and the like.\n","550c020b":"***","410364da":"We can roughly predict the **Burn Rate** by **Mental Fatigue Score** (and vice-versa) but the absence of both of these values gives us a hard time and trying to predict the **Burn Rate** would be an example of overestimation. Thus we had better remove these double-NaN values to make things simpler.","f06b1d87":"### 2.2.1 One-Hot Encoding","5919636f":"It would be useful to convert these features,\n* Gender\n* Company Type\n* WFH Setup Available<br>\n\nto binary values by **one-hot encoding**.","0046793c":"#### 3.2.1.1 Linear Regression Assumptions","8d586f74":"As can be seen below, standard deviations are almost the same. Yay!","87ca1a8d":"***","6cd20489":"## 2.2 Processing The Variables","44959911":"Update & import the module:","e6cc5124":"(Optional) Export the report as an html file:","76aad22a":"From the first figure, we will see that the errors are uncorrelated where the second one implies very low autocorrelation among the errors (see the extremely low values). By the way, *autocorrelation function*, **acf()** computes the variables' correlation with each other.","70842c84":"Even though the variance seems almost constant, **Bartlett** and the **Levene** tests will confirm this assumption. The null hypothesis for both tests is that the errors are ***homoscedastic***. If the p-values < 0.05, the hypothesis will be rejected (i.e **heteroscedastic**).","630c1eb7":"Close but not enough!","77515751":"### 3.2.3 Ridge Regression","25dc3d08":"We need no more than 30 epochs. It's a waste of time. Trust me, I tried.","4a26283f":"## 3.2 Linear Regression Models","03c6d792":"## 3.4 Decision Trees","5bb01fcd":"### 3.2.2 Lasso Regression","6d4c3bc7":"3) Remember the insignificant features? The score also changed a little when they were out. For this dataset, the roadmap should have been this way:<br>\n* Remove all the NaN values.\n* Perform one-hot encoding\n* Try some feature engineering\n* Get rid of insignificant features (because they unnecessarily occupy weights as well)\n* Determine the best models\n* Hyperparameter tuning of best models\n\n$92.98\\% $ is the score that I got after these steps (after the deadline).","f745d4ef":"XGBoost is the winner at this point. We improved our score and hit almost $92.9\\% \\space R^2$ value on the submission.","853909b7":"We are still slightly behind **Lasso with polynomial features** model. The submitted score confirmed this :( Maybe a better hyperparameter tuning would succeed to the throne but it would take many hours to find out. Let's display the most significant variables to turn this all \"random forest time waste\" into something useful. So we can make sure which features have the most impact on our model.","2fc41894":"If these assumptions don't hold, our predicted values would be biased. Thankfully, blue cloud shows us the independency between our variables and the errors. Also the histogram tells us whether the errors have normal distribution or not. It seems so but we need to check them statistically. **Jarque Bera** and **normal** tests tell us the truth. The null hypothesis of these tests are the errors being normally distributed.","e0453dd6":"Just one hidden layer with 16 neurons did the job without an overfit. (Note that we put the BatchNormalization in between to normalize the transition from linearity (dense layer) to non-linearity (activation function).","a5ae1639":"After a few trial and error, we came up with a LINEAR_CONST of 0.7. Let's implement it to our datasets.","02bb0fa7":"# 1\\. Introduction","a4b32d8b":"***\n# 3\\. Training & Validation","a6596f44":"Decision tree results seem pretty good. Let's keep it as well.<br>\n***PS: (After submission, we had a score slightly worse (~0.2%) than the Lasso model.)***","0d93e5fc":"The correlation is almost linear. How about we really *linearize* it and fit into our model because you know, models like to assume that the relationship between the features and target variables is linear. Then later, we may fill the gaps by using this high correlation by interpolating the points we have in our hand.","6aeed86d":"***\n## 3.8 Artificial Neural Networks","e4fb288f":"We have an $adjusted \\space R^2$ value of $0.919$, which seems nice but pretty bad error & prediction values as seen above. Also, as we predicted, **Date of Joining** and **Company Type** features are unfortunatelly not statically significant since **p-values** > 0.05 (including *gender* for the validation set). Before we move on to other models, let's check the linear regression assumptions for fun (or you can directly skip to the secion **3.2.2 Lasso Regression**). Some of these assumptions may be satisfied and some may not :( <br>Anyways, we will keep trying the other models and comment at the end. ","426620c6":"Since **Lasso** will do the feature selection for us *(remember the statistically insignificant features)*, it would be a wise choice to apply polynomial features over it.","8905a4af":"It's time to fill the holes.","a84881ca":"## 3.7 XGBoost","b87f4474":"I have high hopes for XGBoost.","af89d24d":"2) Yes, **XGBoost** won the game. I couldn't manage to go beyond 93% **(to be exact: 92.89% with NaN transformation, 92.98% with NaN slaughter)** while the first place of the leaderboard is occupied by a score of 93.12%. Yeah, sometimes you all need a 0.14% bump and you just can't make it. The hackathon has come to an end as I'm writing this and got the 58th place -*48th with that* <code>.dropna()<\/code> thing- *(out of 500+ attendees, which doesn't seem bad)*. That little difference got me fall behind 50+ people. I also tried some feature engineering (as *Polynomial Feature* function did by itself up to 4th degree) by hand but no avail.","e7d6c78f":"## 3.5 Random Forest","812da410":"## 3.3 k-Nearest Neighbors","9fb78aa4":"### 2.2.3 NaN Values","4afa4f7f":"Random interpolation over the data made the dataset unnecessarily noisy. That why we should consider the correlations between the variables.<br>The things we are going to do are:<br>\n* Create a copy of the dataset with *NaN-free* values on \"Mental Fatigue Score\" & \"Burn Rate\" variables\n    * because the interpolation will calculate the relationship between them.\n* Bake the interpolation function, i.e $y = f(x)$\n* Predict the NaN values by the function and replace them with the new values\n\n...but first, let's look at the original values for the last time:","b40c5873":"**Profile Report**","a1b74f57":"## 3.6 Support Vector Machines","2bd94975":"## 2.1 Examine The Dataset","6e112465":"XGBoost seems to use our features more efficiently. We had a similar significance graph as random forest.<br>Now comes my favorite part: *Deep Learning* algorithm!<br>I'm not sure if we can beat XGBoost or not but we surely will try hard!","ede15491":"...and the null hypotheses are rejected. They do not have normal distributions. Shoot... \u00af\\\\_(\u30c4)_\/\u00af\n\nFailure in these assumptions affect our estimations: OLS coefficients become biased, variance of predictions increases and less reliable model we face.","7364d81d":"# 4\\. Final Remarks"}}