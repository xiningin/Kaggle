{"cell_type":{"6f1f50d6":"code","10ee720e":"code","019cd64b":"code","a70bc33a":"code","f9d5e39d":"code","84862066":"code","e694fc97":"code","f5db26d7":"code","2e2f613c":"code","5f5c734e":"code","ec879f10":"code","fa0b666b":"code","b20ab172":"code","d6e742b7":"code","2c1840b1":"code","8dd1ec9d":"markdown"},"source":{"6f1f50d6":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom catboost import CatBoostClassifier","10ee720e":"#import and split the data","019cd64b":"diabetes = pd.read_csv(\"..\/input\/diabetes\/diabetes.csv\")\ndf = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","a70bc33a":"#set and fit the model","f9d5e39d":"catb_model=CatBoostClassifier().fit(X_train,y_train)","84862066":"#test error without tuning","e694fc97":"y_pred=catb_model.predict(X_test)\naccuracy_score(y_pred,y_test)","f5db26d7":"#model tuning","2e2f613c":"#important parameters","5f5c734e":"catb_params = {\n    'iterations': [200,500],\n    'learning_rate': [0.01,0.05],\n    'depth': [3,5] }","ec879f10":"catb_cv_model= GridSearchCV(catb_model,catb_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)","fa0b666b":"catb_cv_model.best_params_","b20ab172":"catb_tuned_model= CatBoostClassifier(iterations = 200, \n                          learning_rate = 0.01, \n                          depth = 8).fit(X_train, y_train)\ny_pred=catb_tuned_model.predict(X_test)\n","d6e742b7":"accuracy_score(y_test,y_pred)","2c1840b1":"# We found 0.766 by Logistic Regression\n#          0.775 by Naive Bayes \n#          0.731 by KNN\n#          0.744 by Linear SVC\n#          0.735 by Nonlinear SVC Steps\n#          0.735  by ANN\n#          0.753 by CART\n#          0.735 by Random Forests\n#          0.735 by GBM\n#          0.757 by XG Boost\n#          0.744 by Light GBM\n#And now,  0.753 by CatBoost\n#Naive Bayes is the best model for this dataset.","8dd1ec9d":" Thanks to https:\/\/github.com\/mvahit\/DSMLBC"}}