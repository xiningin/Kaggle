{"cell_type":{"5d6fa847":"code","759212bf":"code","4af73328":"code","69e624b2":"code","4219c015":"code","63373a6d":"code","6a67a590":"code","fc393f81":"code","57c3703a":"code","958eae64":"markdown"},"source":{"5d6fa847":"#importing all the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n#importing dataset using panda\n\ndf = pd.read_csv('..\/input\/kc_house_data.csv')\ndf.head()","759212bf":"#checking if any value is missing\n# print(dataset.isnull().any())\ndf.isnull().any()","4af73328":"#checking for categorical data\ndf.dtypes","69e624b2":"#dropping the id and date column\ndf = df.drop(['id', 'date'], axis = 1)","4219c015":"df.info()","63373a6d":"%%time\n#understanding the distribution with seaborn\nwith sns.plotting_context(\"notebook\",font_scale=2.5):\n    g = sns.pairplot(df[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], \n                 hue='bedrooms', palette='tab20',size=6)\ng.set(xticklabels=[]);\n","6a67a590":"%%time\n#separating independent and dependent variable\nX = df.iloc[:,1:].values\ny = df.iloc[:,0].values\n#splitting dataset into training and testing dataset\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 12)","fc393f81":"%%time\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = regressor.predict(X_test)\nprint(y_pred)","57c3703a":"%%time\n#Backward Elimination\nimport statsmodels.formula.api as sm\ndef backwardElimination(x, SL):\n    numVars = len(x[0])\n    temp = np.zeros((21613,19)).astype(int)\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n        if maxVar > SL:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    temp[:,j] = x[:, j]\n                    x = np.delete(x, j, 1)\n                    tmp_regressor = sm.OLS(y, x).fit()\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n                    if (adjR_before >= adjR_after):\n                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n                        x_rollback = np.delete(x_rollback, j, 1)\n                        print (regressor_OLS.summary())\n                        return x_rollback\n                    else:\n                        continue\n    regressor_OLS.summary()\n    return x\n \nSL = 0.05\nX_opt = X[:, [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]]\nX_Modeled = backwardElimination(X_opt, SL)","958eae64":"# House Prices using Backward Elimination\n\nJust started with machine learning. I have used backward Elimination to check the usefulness of dependent variables."}}