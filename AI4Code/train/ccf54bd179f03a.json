{"cell_type":{"7d421aef":"code","9ad4b36c":"code","6a48cfe9":"code","43579f17":"code","f49d6c33":"code","c05755af":"code","4e6978c4":"code","4652707b":"code","656a5f99":"code","a5de4d67":"code","ee584dae":"code","b0e77a7f":"code","192fe2f1":"code","6fb532f7":"code","97041db2":"code","00a0549f":"code","ea7f7485":"code","0ede56a1":"code","aef1bc4f":"code","0e72bd95":"code","8e64cce5":"code","03bf4ddc":"code","7615b627":"code","157be534":"code","193cfc72":"code","3b2b1a7e":"code","61d28fc3":"code","495cfb7d":"code","19c6ce6f":"code","1fc5f9f1":"code","4579030f":"code","4f867643":"code","01f5c410":"code","e0a2b24f":"code","64287be2":"code","4c29b01c":"code","466953f4":"code","8e932956":"code","e35e73b3":"code","ca480313":"code","f3772398":"code","d01d8203":"code","d73f4461":"code","4ce3cccf":"markdown","8feeaec8":"markdown","b4769453":"markdown","722351e8":"markdown","b4582947":"markdown","daef6875":"markdown","f430ad8d":"markdown","03aab293":"markdown","2274d4a7":"markdown","d65be0e4":"markdown","dfa87d63":"markdown","fae0bc0c":"markdown","b568f633":"markdown","96de0228":"markdown"},"source":{"7d421aef":"#Importing necessary libraries.\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport os","9ad4b36c":"#Importing warnings library so as to remove warnings from the output.\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","6a48cfe9":"#Reading the data set.\nbcancer = pd.read_csv(\"..\/input\/data.csv\")\nbcancer.head(4)","43579f17":"#Checking the dimensions of the data set.\nbcancer.shape","f49d6c33":"#Checking the data types of the variables.\nbcancer.dtypes","c05755af":"bcancer.drop('Unnamed: 32', axis = 1, inplace = True)","4e6978c4":"#Dropping the ID column as it is not important.\nbcancer.drop('id',axis = 1, inplace=True)","4652707b":"#checking for missing values.\nbcancer.isnull().sum()","656a5f99":"#Checking the summary statistics of the object attribute 'diagnosis'.\nbcancer.describe(include='object')","a5de4d67":"#Checking the summary statistics of the numeric attributes.\nbcancer.describe()","ee584dae":"bcancer.groupby('diagnosis').mean()","b0e77a7f":"#Converting the target variable in to integer where B is 0 and M is 1.\nbcancer['diagnosis'] = (bcancer['diagnosis'] == 'M').astype('int')","192fe2f1":"#Checking the count of Malignant as well as Beningn observations.\nbcancer['diagnosis'].value_counts()","6fb532f7":"#Visualizing the count of values in diagnosis variable.\nsns.countplot(x='diagnosis',data = bcancer,palette='hls')\nplt.show()","97041db2":"#Checking the distribution of the variable 'diagnosis.'\nsns.distplot(bcancer['diagnosis'])","00a0549f":"#Checking the distribution of the attribute radius mean.\nsns.distplot(bcancer['radius_mean'])","ea7f7485":"#Checking the distribution of the attribute perimeter mean.\nsns.distplot(bcancer['perimeter_mean'])","0ede56a1":"#Nucleus features vs diagnosis\nfeatures_mean=list(bcancer.columns[1:11])\n# split dataframe into two based on diagnosis\nbcancer_M = bcancer[bcancer['diagnosis'] == 1]\nbcancer_B = bcancer[bcancer['diagnosis'] == 0]","aef1bc4f":"#Genrating a scatter plot matrix with the \"mean\" columns\ncols = ['diagnosis',\n        'radius_mean', \n        'texture_mean', \n        'perimeter_mean', \n        'area_mean', \n        'smoothness_mean', \n        'compactness_mean', \n        'concavity_mean', \n        'symmetry_mean']\n\nsns.pairplot(data = bcancer[cols], hue = 'diagnosis', palette = 'RdBu')","0e72bd95":"#Stacking the data\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(bcancer[features_mean[idx]]) - min(bcancer[features_mean[idx]]))\/50\n    ax.hist([bcancer_M[features_mean[idx]],bcancer_B[features_mean[idx]]], bins=np.arange(min(bcancer[features_mean[idx]]), \\\n                        max(bcancer[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked=True, \\\n                        normed = True, label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","8e64cce5":"x=bcancer.iloc[:,1:31]\ny=bcancer['diagnosis']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = .2, random_state=10) ","03bf4ddc":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7615b627":"#Displaying all the columns.\npd.options.display.max_columns = None\nx_train.head(3)","157be534":"#Creating an instance of logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogistic_model1 = LogisticRegression()\n\n#We fit our model to data\nfitted_model1 = logistic_model1.fit(x_train,y_train)\n\n#We use predict_proba() to predict the probabilities\npredictedvalues1 = fitted_model1.predict(x_test)\n\n#We print the probabilites to take a glance\nprint(predictedvalues1)","193cfc72":"#Checking the accuracy of the above model.\nprint('Accuracy of logistic regression classifier on test set: {:.3f}'.format(logistic_model1.score(x_test, y_test)))","3b2b1a7e":"#Generating the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix1 = confusion_matrix(y_test,predictedvalues1)\nprint(confusion_matrix1)","61d28fc3":"#Calculating sensitivity and specificity\ntotal=sum(sum(confusion_matrix1))\n\nsensitivity1 = confusion_matrix1[0,0]\/(confusion_matrix1[0,0]+confusion_matrix1[1,0])\nprint('Sensitivity : ', sensitivity1 )\n\nspecificity1 = confusion_matrix1[1,1]\/(confusion_matrix1[1,1]+confusion_matrix1[0,1])\nprint('Specificity : ', specificity1)","495cfb7d":"#Generating the roc and calculating the auc.\nfpr, tpr, thresholds = roc_curve(y_test, predictedvalues1)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nprint(\"Area under the curve: {:.3f}\".format(auc(fpr,tpr)))","19c6ce6f":"#Generating the classification report.\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictedvalues1))","1fc5f9f1":"#Generating the correlation matrix.\nfig, ax=plt.subplots(figsize=(20,20))\ncorrelation=bcancer.corr()\nsns.heatmap(correlation,square=True, vmin=-0.2, vmax=0.8,cmap=\"YlGnBu\", annot=True)","4579030f":"#Dropping all \"worst\" columns. \ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst', \n        'symmetry_worst']      \nbcancer = bcancer.drop(cols, axis = 1)","4f867643":"#Dropping the perimeter and area attributes.\ncols1 = ['area_se', 'perimeter_se', 'perimeter_mean', 'area_mean']\nbcancer = bcancer.drop(cols1, axis = 1)","01f5c410":"#Dropping the concavity attributes.\ncols2 = ['concavity_mean', 'concavity_se']\nbcancer = bcancer.drop(cols2, axis = 1)","e0a2b24f":"#Drawing the heatmap again, with the new correlation matrix\ncorr = bcancer.corr().round(2)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap='YlGnBu', vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.tight_layout()","64287be2":"x = bcancer.drop('diagnosis', axis = 1)\ny = bcancer['diagnosis']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)","4c29b01c":"#Checking the shape of train as well as test data.\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","466953f4":"#Displaying all the columns.\npd.options.display.max_columns = None\nx_train.head(3)","8e932956":"#Creating an instance of logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogistic_model2 = LogisticRegression()\n\n#Fitting our model to data\nfitted_model2 = logistic_model2.fit(x_train,y_train)\n\n#We use predict_proba() to predict the probabilities\npredictedvalues2 = fitted_model2.predict(x_test)\n\n#We print the probabilites to take a glance\nprint(predictedvalues2)","e35e73b3":"#Checking the accuracy of the above model.\nprint('Accuracy of logistic regression classifier on test set: {:.3f}'.format(logistic_model2.score(x_test, y_test)))","ca480313":"#Generating the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix2 = confusion_matrix(y_test,predictedvalues2)\nprint(confusion_matrix2)","f3772398":"#Calculating sensitivity and specificity\ntotal=sum(sum(confusion_matrix1))\n\nsensitivity2 = confusion_matrix2[0,0]\/(confusion_matrix2[0,0]+confusion_matrix2[1,0])\nprint('Sensitivity : ', sensitivity2 )\n\nspecificity2 = confusion_matrix2[1,1]\/(confusion_matrix2[1,1]+confusion_matrix2[0,1])\nprint('Specificity : ', specificity2)","d01d8203":"#Generating the roc and calculating the auc.\nfpr, tpr, thresholds = roc_curve(y_test, predictedvalues2)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nprint(\"Area under the curve: {:.3f}\".format(auc(fpr,tpr)))","d73f4461":"#Generating the classification report.\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, predictedvalues2))","4ce3cccf":"### Data Description-","8feeaec8":"Here we can see the mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\nMean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the above histograms there are no noticeable large outliers that requires further cleanup.","b4769453":"There are almost perfectly linear patterns between the radius, perimeter and area attributes which hint at the presence of multicollinearity between these variables. Another set of variables that possibly imply multicollinearity are the concavity, concave_points and compactness.\n\nI have also genrated a correlation matrix in the cells below to show multicollinearity among variables. ","722351e8":"Since a cell's **radius** is the basic building block of its size. Therefore, it is reasonable to choose radius as our attribute to represent the size of a cell.","b4582947":"Also here we can see that there is multicollinearity between \"mean\" columns and the \"worst\" column. For instance, the radius_mean column has a correlation of 0.97 with the radius_worst column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its \"mean\" and \"worst\" columns. This shows that the \"worst\" columns are essentially just a subset of the \"mean\" columns; the \"worst\" columns are also the \"mean\" of some values (the three largest values among all observations). Therefore, I think we can discard the \"worst\" columns from our analysis and only focus on the \"mean\" columns.","daef6875":"### So now we will remove the unnecessary columns.","f430ad8d":"## Building the logistic model with all the attributes.","03aab293":"<br>**1) ID number\n<br>**2) Diagnosis** (M = malignant, B = benign)\n<br>Ten real-valued features are computed for each cell nucleus:\n\n<br>**a) radius** (mean of distances from center to points on the perimeter)\n<br>**b) texture** (standard deviation of gray-scale values)\n<br>**c) perimeter\n<br>**d) area\n<br>**e) smoothness** (local variation in radius lengths)\n<br>**f) compactness** (perimeter^2 \/ area - 1.0)\n<br>**g) concavity** (severity of concave portions of the contour)\n<br>**h) concave points** (number of concave portions of the contour)\n<br>**i) symmetry**","2274d4a7":"So here we will drop all \"worst\" columns from the dataset, and pick only one of the three attributes that describe the size of cells. ","d65be0e4":"Here we can see after eliminating the multicollinear attributes the accuracy reduced as well as the AUC dropped from 0.945 to 0.909","dfa87d63":"## Rebuilding the logistic regression model.","fae0bc0c":"As seen in the heatmap above- radius_mean, perimeter_mean, texture_mean, area_mean, radius_worst, perimeter_worst are highly correlated.","b568f633":"## Breast Cancer Prediction","96de0228":"Also there is multicollinearity between the attributes compactness, concavity. So similarly what we did with the size attributes, here also we should pick only one of these three attributes that contain information on the shape of the cell. So we will take compactness attribute as it somewhat describes the size of the cell, and remove the other attribute."}}