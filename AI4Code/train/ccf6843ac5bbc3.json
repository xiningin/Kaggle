{"cell_type":{"a5a7ea40":"code","c3b8ada5":"code","65452979":"code","35e9c2bc":"code","a93e24a7":"code","cc2e933e":"code","be84c5a4":"code","349f4462":"code","e7b38bbd":"code","fbbba9eb":"code","b48ddb20":"code","400eb495":"code","9b8e87f9":"code","f9a253a5":"markdown"},"source":{"a5a7ea40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c3b8ada5":"print(os.listdir(\"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"))","65452979":"WORK_DIR = \"..\/working\/\"\nos.listdir(WORK_DIR)","35e9c2bc":"# import module we'll need to import our custom module\nimport shutil\n\n# copy our file into the working directory\nshutil.copytree(\"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\/pytorch_pretrained_bert\", os.path.join(WORK_DIR, \"pytorch_pretrained_bert\"))","a93e24a7":"os.listdir('..\/input\/bert-pretrained-models\/cased_l-12_h-768_a-12\/cased_L-12_H-768_A-12')","cc2e933e":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n","be84c5a4":"BERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None)\n\n# Tokenized input\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n\n# Mask a token that we will try to predict back with `BertForMaskedLM`\nmasked_index = 8\ntokenized_text[masked_index] = '[MASK]'\nprint(tokenized_text)","349f4462":"# Convert token to vocabulary indices\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n\n# Convert inputs to PyTorch tensors\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])","e7b38bbd":"os.listdir(BERT_MODEL_PATH)","fbbba9eb":"convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')","b48ddb20":"shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","400eb495":"# Load pre-trained model (weights)\nmodel = BertModel.from_pretrained(\n    WORK_DIR)\nmodel.eval()\n\n# If you have a GPU, put everything on cuda\ntokens_tensor = tokens_tensor.to('cuda')\nsegments_tensors = segments_tensors.to('cuda')\nmodel.to('cuda')\n\n# Predict hidden states features for each layer\nwith torch.no_grad():\n    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n# We have a hidden states for each of the 12 layers in model bert-base-uncased\nassert len(encoded_layers) == 12","9b8e87f9":"[e.size() for e in encoded_layers]","f9a253a5":"- import pytorch-pretrained-bert from source in dataset\n\nthe source is cloned from [huggingface\/pytorch\\-pretrained\\-BERT: \ud83d\udcd6The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google\/CMU Transformer\\-XL\\.](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT)\n\n- convert tf checkpoints to pytorch model\n\n\n- feature extraction with BERT like [huggingface\/pytorch\\-pretrained\\-BERT: \ud83d\udcd6The Big\\-&\\-Extending\\-Repository\\-of\\-Transformers: Pretrained PyTorch models for Google's BERT, OpenAI GPT & GPT\\-2, Google\/CMU Transformer\\-XL\\.](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT)\n\nThis kernel is based off of [Import functions from Kaggle script](https:\/\/www.kaggle.com\/rtatman\/import-functions-from-kaggle-script)"}}