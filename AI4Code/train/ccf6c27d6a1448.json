{"cell_type":{"4d40182e":"code","201d8e8a":"code","2c1185d6":"code","062cf9a3":"code","a920304e":"code","44207d76":"code","6b5b1bde":"code","ed6d0ded":"code","d22cc5bf":"code","3e9c46e2":"code","dbe7fd08":"code","f98539d6":"code","f1373f15":"markdown","fe348cd3":"markdown","34435397":"markdown"},"source":{"4d40182e":"import re\nimport string\nimport numpy as np \nimport pandas as pd\nimport seaborn as sb\nfrom PIL import Image\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nfrom nltk import pos_tag,word_tokenize, stem","201d8e8a":"dt = pd.read_csv('..\/input\/joe-rogan-experience-1169-elon-musk\/joe-rogan-experience-1169-elon-musk.csv')\ndt.info()","2c1185d6":"dt['Timestamp'] = dt['Timestamp'].apply(lambda x: re.findall('[\\d:]+',x)[0])\ndt['Timestamp'] = pd.to_datetime(dt['Timestamp'])","062cf9a3":"dt.info()","a920304e":"plt.rcParams[\"figure.figsize\"] = (10,5)\ndt['TextLength'] = dt['Text'].apply(lambda x: len(x))\ng1 = dt.groupby('Speaker')['TextLength'].sum().apply(lambda x: x\/sum(dt['TextLength'])).sort_values().plot.barh()","44207d76":"plt.rcParams[\"figure.figsize\"] = (20,15)\ng1 = dt.groupby(pd.Grouper(key='Timestamp',freq='5Min'))['TextLength'].sum().plot.bar()\nxlabels = g1.get_xticklabels()\nnew_labels = [datetime.strptime(xlabel.get_text(),'%Y-%m-%d %H:%M:%S').time() for xlabel in xlabels]\ng1.set_xticklabels(new_labels,rotation=45, fontsize=16)\nplt.title('Amount of words said per 5 minutes')\nplt.ylabel('Amount of words')\nplt.show()","6b5b1bde":"plt.rcParams[\"figure.figsize\"] = (30,20)\ndt1 = dt.groupby([pd.Grouper(key='Timestamp',freq='5Min'),'Speaker'])['TextLength'].mean().reset_index()\ndt1['Timestamp'] = dt1['Timestamp'].dt.time\ng2 = sb.barplot(x='Timestamp',y='TextLength',hue='Speaker',data=dt1)\ng2.set_xticklabels(g2.get_xticklabels(), rotation=40, fontsize=16)\nplt.title('Average of words said per Speaker and per 5 minutes')\nplt.ylabel('Average of words')\nplt.xlabel('TimeStamp',fontsize=16)\nplt.show()","ed6d0ded":"stopWords = stopwords.words('english')\ndt_elon = dt.loc[dt['Speaker'] == 'Elon Musk']['Text'].str.lower()\nwordcloud = WordCloud(height=700,width=700,margin=0,max_words=1500,stopwords=stopWords,colormap='prism').generate(str(dt_elon.values))\nplt.imshow(wordcloud)\nplt.title(\"Elons WordCloud\")\nplt.show()","d22cc5bf":"stopWords = stopwords.words('english')\ndt_joe = dt.loc[dt['Speaker'] == 'Joe Rogan']['Text'].str.lower()\nwordcloud = WordCloud(height=700,width=700,margin=0,max_words=1500,stopwords=stopWords,colormap='prism').generate(str(dt_joe.values))\nplt.imshow(wordcloud)\nplt.title(\"Joes WordCloud\")\nplt.show()","3e9c46e2":"def classes_dataframe(series,speaker): # simple function for returning classes of words \n    tag_dicts = defaultdict(int)\n    for sentence in series.values:\n        tokens = word_tokenize(sentence)\n        tags = pos_tag(tokens,tagset='universal')\n        for tag in tags:\n            if tag[1] != '.':\n                tag_dicts[tag[1]] += 1\n            else:\n                pass\n    tag_df = pd.DataFrame(tag_dicts.items(),columns=['WordClass','Count'])\n    tag_df['Speaker'] = speaker\n    return tag_df.sort_values(by='Count',ascending=False)","dbe7fd08":"elon_wordclass,joe_wordclass = classes_dataframe(dt_elon,'Elon Musk'), classes_dataframe(dt_joe,'Joe Rogan')\nwordclass = pd.concat([elon_wordclass,joe_wordclass])","f98539d6":"g3 = sb.barplot(x='WordClass',y='Count',hue='Speaker',data=wordclass)\ng3.set_xticklabels(g3.get_xticklabels(), fontsize=18)\nplt.title(\"WordClass per Speaker\")\nplt.ylabel('WordClass Amount',fontsize=15)\nplt.xlabel('WordClass',fontsize=15)\nplt.show()","f1373f15":"Looks like they have very similar word clouds. With that in mind let's see the most common class of word each one uses. ","fe348cd3":"Let's use the nltk <code>pos_tag<\/code> function to classify the word classes. For reference here is the tager.\n![image.png](attachment:image.png)","34435397":"That's all for today folks. If there are any questions do let me know."}}