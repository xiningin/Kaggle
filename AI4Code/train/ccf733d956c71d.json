{"cell_type":{"f6a743a4":"code","6e3834fc":"code","46c828b3":"code","5570662c":"code","059b1c1d":"code","9478e70c":"code","63999a80":"code","5866594d":"code","09ff86e5":"code","177d50a8":"code","4852d521":"code","8885cce6":"code","9d56db0a":"code","dad1d8bd":"code","866bee81":"code","33fd2019":"code","b82b6686":"code","381e2865":"code","82aa6d64":"markdown","175019b2":"markdown","b1103859":"markdown","9205db8b":"markdown","6d841a4e":"markdown"},"source":{"f6a743a4":"import pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd \nimport numpy as np\nfrom tqdm import tqdm \nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.torch import ToTensor \nimport torch.utils.data as data \n","6e3834fc":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","46c828b3":"class TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","5570662c":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num\n","059b1c1d":"sample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\ntest_data_folder = \"..\/input\/severstal-steel-defect-detection\/test_images\"","9478e70c":"# initialize test dataloader\nbest_threshold = 0.5\nnum_workers = 2\nbatch_size = 4\nprint('best_threshold', best_threshold)\nmin_size = 3500\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)","63999a80":"from torchvision.models.resnet import ResNet\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import Bottleneck\n# from pretrainedmodels.models.torchvision_models import pretrained_settings\n\n\nclass ResNetEncoder(ResNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.fc\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu(x0)\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)\n\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    'resnet18': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet18'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [2, 2, 2, 2],\n        },\n    },\n\n    'resnet34': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet34'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet50': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet101': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n        },\n    },\n\n    'resnet152': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 8, 36, 3],\n        },\n    },\n\n    'resnext50_32x4d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'https:\/\/download.pytorch.org\/models\/resnext50_32x4d-7cdf4587.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n            'groups': 32,\n            'width_per_group': 4\n        },\n    },\n\n    'resnext101_32x8d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'https:\/\/download.pytorch.org\/models\/resnext101_32x8d-8ba56ff5.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            },\n            'instagram': {\n                'url': 'https:\/\/download.pytorch.org\/models\/ig_resnext101_32x8-c38310e5.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 8\n        },\n    },\n\n    'resnext101_32x16d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram':{\n                'url': 'https:\/\/download.pytorch.org\/models\/ig_resnext101_32x16-c6f796b0.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 16\n        },\n    },\n\n    'resnext101_32x32d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram': {\n                'url': 'https:\/\/download.pytorch.org\/models\/ig_resnext101_32x32-e4b90b00.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 32\n        },\n    },\n\n    'resnext101_32x48d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram': {\n                'url': 'https:\/\/download.pytorch.org\/models\/ig_resnext101_32x48-3e41cc8a.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 48\n        },\n    },\n}","5866594d":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\n__all__ = ['DPN', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107']\n\npretrained_settings = {\n    'dpn68': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn68-4af7d88d2.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    },\n    'dpn68b': {\n        'imagenet+5k': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn68b_extra-363ab9c19.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    },\n    'dpn92': {\n        # 'imagenet': {\n        #     'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn68-66bebafa7.pth',\n        #     'input_space': 'RGB',\n        #     'input_size': [3, 224, 224],\n        #     'input_range': [0, 1],\n        #     'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n        #     'std': [1 \/ (.0167 * 255)] * 3,\n        #     'num_classes': 1000\n        # },\n        'imagenet+5k': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn92_extra-fda993c95.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    },\n    'dpn98': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn98-722954780.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    },\n    'dpn131': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn131-7af84be88.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    },\n    'dpn107': {\n        'imagenet+5k': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/dpn107_extra-b7f9f4cc9.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [124 \/ 255, 117 \/ 255, 104 \/ 255],\n            'std': [1 \/ (.0167 * 255)] * 3,\n            'num_classes': 1000\n        }\n    }\n}\n\ndef dpn68(num_classes=1000, pretrained='imagenet'):\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn68'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\ndef dpn68b(num_classes=1000, pretrained='imagenet+5k'):\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn68b'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\ndef dpn92(num_classes=1000, pretrained='imagenet+5k'):\n    model = DPN(\n        num_init_features=64, k_r=96, groups=32,\n        k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn92'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\ndef dpn98(num_classes=1000, pretrained='imagenet'):\n    model = DPN(\n        num_init_features=96, k_r=160, groups=40,\n        k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn98'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\ndef dpn131(num_classes=1000, pretrained='imagenet'):\n    model = DPN(\n        num_init_features=128, k_r=160, groups=40,\n        k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn131'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\ndef dpn107(num_classes=1000, pretrained='imagenet+5k'):\n    model = DPN(\n        num_init_features=128, k_r=200, groups=50,\n        k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings['dpn107'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_chs, activation_fn=nn.ReLU(inplace=True)):\n        super(CatBnAct, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n\n    def forward(self, x):\n        x = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        return self.act(self.bn(x))\n\n\nclass BnActConv2d(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size, stride,\n                 padding=0, groups=1, activation_fn=nn.ReLU(inplace=True)):\n        super(BnActConv2d, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=False)\n\n    def forward(self, x):\n        return self.conv(self.act(self.bn(x)))\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, num_init_features, kernel_size=7,\n                 padding=3, activation_fn=nn.ReLU(inplace=True)):\n        super(InputBlock, self).__init__()\n        self.conv = nn.Conv2d(\n            3, num_init_features, kernel_size=kernel_size, stride=2, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(num_init_features, eps=0.001)\n        self.act = activation_fn\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.pool(x)\n        return x\n\n\nclass DualPathBlock(nn.Module):\n    def __init__(\n            self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, groups, block_type='normal', b=False):\n        super(DualPathBlock, self).__init__()\n        self.num_1x1_c = num_1x1_c\n        self.inc = inc\n        self.b = b\n        if block_type is 'proj':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type is 'down':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type is 'normal'\n            self.key_stride = 1\n            self.has_proj = False\n\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=2)\n            else:\n                self.c1x1_w_s1 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=1)\n        self.c1x1_a = BnActConv2d(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=1, stride=1)\n        self.c3x3_b = BnActConv2d(\n            in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=3,\n            stride=self.key_stride, padding=1, groups=groups)\n        if b:\n            self.c1x1_c = CatBnAct(in_chs=num_3x3_b)\n            self.c1x1_c1 = nn.Conv2d(num_3x3_b, num_1x1_c, kernel_size=1, bias=False)\n            self.c1x1_c2 = nn.Conv2d(num_3x3_b, inc, kernel_size=1, bias=False)\n        else:\n            self.c1x1_c = BnActConv2d(in_chs=num_3x3_b, out_chs=num_1x1_c + inc, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x_in = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        if self.has_proj:\n            if self.key_stride == 2:\n                x_s = self.c1x1_w_s2(x_in)\n            else:\n                x_s = self.c1x1_w_s1(x_in)\n            x_s1 = x_s[:, :self.num_1x1_c, :, :]\n            x_s2 = x_s[:, self.num_1x1_c:, :, :]\n        else:\n            x_s1 = x[0]\n            x_s2 = x[1]\n        x_in = self.c1x1_a(x_in)\n        x_in = self.c3x3_b(x_in)\n        if self.b:\n            x_in = self.c1x1_c(x_in)\n            out1 = self.c1x1_c1(x_in)\n            out2 = self.c1x1_c2(x_in)\n        else:\n            x_in = self.c1x1_c(x_in)\n            out1 = x_in[:, :self.num_1x1_c, :, :]\n            out2 = x_in[:, self.num_1x1_c:, :, :]\n        resid = x_s1 + out1\n        dense = torch.cat([x_s2, out2], dim=1)\n        return resid, dense\n\n\nclass DPN(nn.Module):\n    def __init__(self, small=False, num_init_features=64, k_r=96, groups=32,\n                 b=False, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                 num_classes=1000, test_time_pool=False):\n        super(DPN, self).__init__()\n        self.test_time_pool = test_time_pool\n        self.b = b\n        bw_factor = 1 if small else 4\n\n        blocks = OrderedDict()\n\n        # conv1\n        if small:\n            blocks['conv1_1'] = InputBlock(num_init_features, kernel_size=3, padding=1)\n        else:\n            blocks['conv1_1'] = InputBlock(num_init_features, kernel_size=7, padding=3)\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv2_1'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, 'proj', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks['conv2_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv3_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks['conv3_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv4_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks['conv4_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) \/\/ (64 * bw_factor)\n        blocks['conv5_1'] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'down', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks['conv5_' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, 'normal', b)\n            in_chs += inc\n        blocks['conv5_bn_ac'] = CatBnAct(in_chs)\n\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.last_linear = nn.Conv2d(in_chs, num_classes, kernel_size=1, bias=True)\n\n    def logits(self, features):\n        if not self.training and self.test_time_pool:\n            x = F.avg_pool2d(features, kernel_size=7, stride=1)\n            out = self.last_linear(x)\n            # The extra test time pool should be pooling an img_size\/\/32 - 6 size patch\n            out = adaptive_avgmax_pool2d(out, pool_type='avgmax')\n        else:\n            x = adaptive_avgmax_pool2d(features, pool_type='avg')\n            out = self.last_linear(x)\n        return out.view(out.size(0), -1)\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\"\"\" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * 'avg' - Average pooling\n    * 'max' - Max pooling\n    * 'avgmax' - Sum of average and max pooling re-scaled by 0.5\n    * 'avgmaxc' - Concatenation of average and max pooling along feature dim, doubles feature dim\nBoth a functional and a nn.Module version of the pooling is provided.\nAuthor: Ross Wightman (rwightman)\n\"\"\"\n\ndef pooling_factor(pool_type='avg'):\n    return 2 if pool_type == 'avgmaxc' else 1\n\n\ndef adaptive_avgmax_pool2d(x, pool_type='avg', padding=0, count_include_pad=False):\n    \"\"\"Selectable global pooling function with dynamic input kernel size\n    \"\"\"\n    if pool_type == 'avgmaxc':\n        x = torch.cat([\n            F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        ], dim=1)\n    elif pool_type == 'avgmax':\n        x_avg = F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        x = 0.5 * (x_avg + x_max)\n    elif pool_type == 'max':\n        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n    else:\n        if pool_type != 'avg':\n            print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n    return x\n\n\nclass AdaptiveAvgMaxPool2d(torch.nn.Module):\n    \"\"\"Selectable global pooling layer with dynamic input kernel size\n    \"\"\"\n    def __init__(self, output_size=1, pool_type='avg'):\n        super(AdaptiveAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n        self.pool_type = pool_type\n        if pool_type == 'avgmaxc' or pool_type == 'avgmax':\n            self.pool = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size), nn.AdaptiveMaxPool2d(output_size)])\n        elif pool_type == 'max':\n            self.pool = nn.AdaptiveMaxPool2d(output_size)\n        else:\n            if pool_type != 'avg':\n                print('Invalid pool type %s specified. Defaulting to average pooling.' % pool_type)\n            self.pool = nn.AdaptiveAvgPool2d(output_size)\n\n    def forward(self, x):\n        if self.pool_type == 'avgmaxc':\n            x = torch.cat([p(x) for p in self.pool], dim=1)\n        elif self.pool_type == 'avgmax':\n            x = 0.5 * torch.sum(torch.stack([p(x) for p in self.pool]), 0).squeeze(dim=0)\n        else:\n            x = self.pool(x)\n        return x\n\n    def factor(self):\n        return pooling_factor(self.pool_type)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + 'output_size=' + str(self.output_size) \\\n               + ', pool_type=' + self.pool_type + ')'\nclass DPNEncorder(DPN):\n\n    def __init__(self, feature_blocks, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.feature_blocks = np.cumsum(feature_blocks)\n        self.pretrained = False\n        \n        del self.last_linear\n\n    def forward(self, x):\n\n        features = []\n\n        input_block = self.features[0]\n\n        x = input_block.conv(x)\n        x = input_block.bn(x)\n        x = input_block.act(x)\n        features.append(x)\n\n        x = input_block.pool(x)\n\n        for i, module in enumerate(self.features[1:], 1):\n            x = module(x)\n            if i in self.feature_blocks:\n                features.append(x)\n\n        out_features = [\n            features[4],\n            F.relu(torch.cat(features[3], dim=1), inplace=True),\n            F.relu(torch.cat(features[2], dim=1), inplace=True),\n            F.relu(torch.cat(features[1], dim=1), inplace=True),\n            features[0],\n        ]\n\n        return out_features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\ndpn_encoders = {\n    'dpn68': {\n        'encoder': DPNEncorder,\n        'out_shapes': (832, 704, 320, 144, 10),\n#         'pretrained_settings': pretrained_settings['dpn68'],\n        'params': {\n            'feature_blocks': (3, 4, 12, 4),\n            'groups': 32,\n            'inc_sec': (16, 32, 32, 64),\n            'k_r': 128,\n            'k_sec': (3, 4, 12, 3),\n            'num_classes': 1000,\n            'num_init_features': 10,\n            'small': True,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn68b': {\n        'encoder': DPNEncorder,\n        'out_shapes': (832, 704, 320, 144, 10),\n#         'pretrained_settings': pretrained_settings['dpn68b'],\n        'params': {\n            'feature_blocks': (3, 4, 12, 4),\n            'b': True,\n            'groups': 32,\n            'inc_sec': (16, 32, 32, 64),\n            'k_r': 128,\n            'k_sec': (3, 4, 12, 3),\n            'num_classes': 1000,\n            'num_init_features': 10,\n            'small': True,\n            'test_time_pool': True,\n        },\n    },\n\n    'dpn92': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1552, 704, 336, 64),\n#         'pretrained_settings': pretrained_settings['dpn92'],\n        'params': {\n            'feature_blocks': (3, 4, 20, 4),\n            'groups': 32,\n            'inc_sec': (16, 32, 24, 128),\n            'k_r': 96,\n            'k_sec': (3, 4, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 64,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn98': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1728, 768, 336, 96),\n        'pretrained_settings': pretrained_settings['dpn98'],\n        'params': {\n            'feature_blocks': (3, 6, 20, 4),\n            'groups': 40,\n            'inc_sec': (16, 32, 32, 128),\n            'k_r': 160,\n            'k_sec': (3, 6, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 96,\n            'test_time_pool': True,\n        },\n    },\n\n    'dpn107': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 2432, 1152, 376, 128),\n#         'pretrained_settings': pretrained_settings['dpn107'],\n        'params': {\n            'feature_blocks': (4, 8, 20, 4),\n            'groups': 50,\n            'inc_sec': (20, 64, 64, 128),\n            'k_r': 200,\n            'k_sec': (4, 8, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 128,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn131': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1984, 832, 352, 128),\n#         'pretrained_settings': pretrained_settings['dpn131'],\n        'params': {\n            'feature_blocks': (4, 8, 28, 4),\n            'groups': 40,\n            'inc_sec': (16, 32, 32, 128),\n            'k_r': 160,\n            'k_sec': (4, 8, 28, 3),\n            'num_classes': 1000,\n            'num_init_features': 128,\n            'test_time_pool': True\n        },\n    },\n\n}","09ff86e5":"from collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\nclass SENetEncoder(SENet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        \n        del self.last_linear\n        del self.avg_pool\n\n    def forward(self, x):\n        for module in self.layer0[:-1]:\n            x = module(x)\n\n        x0 = x\n        x = self.layer0[-1](x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nsenet_encoders = {\n    'senet154': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['senet154'],\n        'out_shapes': (2048, 1024, 512, 256, 128),\n        'params': {\n            'block': SEBottleneck,\n            'dropout_p': 0.2,\n            'groups': 64,\n            'layers': [3, 8, 36, 3],\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet50': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet101': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet152': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 8, 36, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext50_32x4d': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnext50_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext101_32x4d': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnext101_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n}","177d50a8":"import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n__all__ = ['InceptionResNetV2', 'inceptionresnetv2']\n\npretrained_settings = {\n    'inceptionresnetv2': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/inceptionresnetv2-520b38e4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1000\n        },\n        'imagenet+background': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/inceptionresnetv2-520b38e4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 299, 299],\n            'input_range': [0, 1],\n            'mean': [0.5, 0.5, 0.5],\n            'std': [0.5, 0.5, 0.5],\n            'num_classes': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(160, 192, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(224, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionResNetV2, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def features(self, input):\n        x = self.conv2d_1a(input)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool_1a(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef inceptionresnetv2(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"InceptionResNetV2 model architecture from the\n    `\"InceptionV4, Inception-ResNet...\" <https:\/\/arxiv.org\/abs\/1602.07261>`_ paper.\n    \"\"\"\n    if pretrained:\n        settings = pretrained_settings['inceptionresnetv2'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n\n        # both 'imagenet'&'imagenet+background' are loaded from same parameters\n        model = InceptionResNetV2(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n\n        if pretrained == 'imagenet':\n            new_last_linear = nn.Linear(1536, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = InceptionResNetV2(num_classes=num_classes)\n    return model\n\n\nclass InceptionResNetV2Encoder(InceptionResNetV2):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.pretrained = False\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.avgpool_1a\n        del self.last_linear\n\n    def forward(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x0 = x\n\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x1 = x\n\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x2 = x\n\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x3 = x\n\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        x4 = x\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\ninception_encoders = {\n    'inceptionresnetv2': {\n        'encoder': InceptionResNetV2Encoder,\n#         'pretrained_settings': pretrained_settings['inceptionresnetv2'],\n        'out_shapes': (1536, 1088, 320, 192, 64),\n        'params': {\n            'num_classes': 1000,\n        }\n\n    }\n}","4852d521":"import torch.nn as nn\nimport torch.nn.functional as F\nimport re\nimport torch.nn as nn\n\n# from pretrainedmodels.models.torchvision_models import pretrained_settings\nfrom torchvision.models.densenet import DenseNet\n\nfrom torchvision.models.resnet import ResNet\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import Bottleneck\n# from pretrainedmodels.models.torchvision_models import pretrained_settings\n\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\nclass SENetEncoder(SENet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        \n        del self.last_linear\n        del self.avg_pool\n\n    def forward(self, x):\n        for module in self.layer0[:-1]:\n            x = module(x)\n\n        x0 = x\n        x = self.layer0[-1](x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nsenet_encoders = {\n    'senet154': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['senet154'],\n        'out_shapes': (2048, 1024, 512, 256, 128),\n        'params': {\n            'block': SEBottleneck,\n            'dropout_p': 0.2,\n            'groups': 64,\n            'layers': [3, 8, 36, 3],\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet50': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet101': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet152': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 8, 36, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext50_32x4d': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnext50_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext101_32x4d': {\n        'encoder': SENetEncoder,\n#         'pretrained_settings': pretrained_settings['se_resnext101_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n}\n\n\nclass ResNetEncoder(ResNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.fc\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu(x0)\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)\n\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    'resnet18': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet18'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [2, 2, 2, 2],\n        },\n    },\n\n    'resnet34': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet34'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet50': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet101': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n        },\n    },\n\n    'resnet152': {\n        'encoder': ResNetEncoder,\n#         'pretrained_settings': pretrained_settings['resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 8, 36, 3],\n        },\n    },\n}\n\n\nclass DenseNetEncoder(DenseNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.classifier\n        self.initialize()\n\n    @staticmethod\n    def _transition(x, transition_block):\n        for module in transition_block:\n            x = module(x)\n            if isinstance(module, nn.ReLU):\n                skip = x\n        return x, skip\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n\n        x = self.features.conv0(x)\n        x = self.features.norm0(x)\n        x = self.features.relu0(x)\n        x0 = x\n\n        x = self.features.pool0(x)\n        x = self.features.denseblock1(x)\n        x, x1 = self._transition(x, self.features.transition1)\n\n        x = self.features.denseblock2(x)\n        x, x2 = self._transition(x, self.features.transition2)\n\n        x = self.features.denseblock3(x)\n        x, x3 = self._transition(x, self.features.transition3)\n\n        x = self.features.denseblock4(x)\n        x4 = self.features.norm5(x)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict):\n        pattern = re.compile(\n            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        # remove linear\n        state_dict.pop('classifier.bias')\n        state_dict.pop('classifier.weight')\n\n        super().load_state_dict(state_dict)\n\n\ndensenet_encoders = {\n    'densenet121': {\n        'encoder': DenseNetEncoder,\n#         'pretrained_settings': pretrained_settings['densenet121'],\n        'out_shapes': (1024, 1024, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 24, 16),\n        }\n    },\n\n    'densenet169': {\n        'encoder': DenseNetEncoder,\n#         'pretrained_settings': pretrained_settings['densenet169'],\n        'out_shapes': (1664, 1280, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 32, 32),\n        }\n    },\n\n    'densenet201': {\n        'encoder': DenseNetEncoder,\n#         'pretrained_settings': pretrained_settings['densenet201'],\n        'out_shapes': (1920, 1792, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 48, 32),\n        }\n    },\n\n    'densenet161': {\n        'encoder': DenseNetEncoder,\n#         'pretrained_settings': pretrained_settings['densenet161'],\n        'out_shapes': (2208, 2112, 768, 384, 96),\n        'params': {\n            'num_init_features': 96,\n            'growth_rate': 48,\n            'block_config': (6, 12, 36, 24),\n        }\n    },\n\n}\n\nencoders = {}\nencoders.update(resnet_encoders)\n# encoders.update(dpn_encoders)\n# encoders.update(vgg_encoders)\nencoders.update(senet_encoders)\nencoders.update(densenet_encoders)\nencoders.update(inception_encoders)\n\n\ndef get_encoder(name, encoder_weights=None):\n    Encoder = encoders[name]['encoder']\n    encoder = Encoder(**encoders[name]['params'])\n    encoder.out_shapes = encoders[name]['out_shapes']\n\n    if encoder_weights is not None:\n        settings = encoders[name]['pretrained_settings'][encoder_weights]\n        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_params(encoder_name, pretrained='imagenet'):\n    settings = encoders[encoder_name]['pretrained_settings']\n\n    if pretrained not in settings.keys():\n        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n    \n    formatted_settings = {}\n    formatted_settings['input_space'] = settings[pretrained].get('input_space')\n    formatted_settings['input_range'] = settings[pretrained].get('input_range')\n    formatted_settings['mean'] = settings[pretrained].get('mean')\n    formatted_settings['std'] = settings[pretrained].get('std')\n    return formatted_settings\n\n\ndef get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n    return functools.partial(preprocess_input, **params)\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\nclass EncoderDecoder(Model):\n\n    def __init__(self, encoder, decoder, activation):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == 'softmax':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError('Activation should be \"sigmoid\"\/\"softmax\"\/callable\/None')\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s `encoder` and `decoder` (return logits!)\"\"\"\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)`\n        and apply activation function (if activation is not `None`) with `torch.no_grad()`\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n            if self.activation:\n                x = self.activation(x)\n\n        return x","8885cce6":"class Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass SCSEModule(nn.Module):\n    def __init__(self, ch, re=16):\n        super().__init__()\n        self.cSE = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                 nn.Conv2d(ch, ch\/\/re, 1),\n                                 nn.ReLU(inplace=True),\n                                 nn.Conv2d(ch\/\/re, ch, 1),\n                                 nn.Sigmoid()\n                                )\n        self.sSE = nn.Sequential(nn.Conv2d(ch, ch, 1),\n                                 nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\ndef _upsample(x, size):\n    return F.interpolate(x, size=size, mode='bilinear', align_corners=True)\n\n\nclass PyramidStage(nn.Module):\n\n    def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n        super().__init__()\n        if pool_size == 1:\n            use_bathcnorm = False\n        self.pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n            Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n        )\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        x = self.pool(x)\n        x = _upsample(x, size=(h, w))\n        return x\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n        super().__init__()\n\n        self.stages = nn.ModuleList([\n            PyramidStage(in_channels, in_channels \/\/ len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n        ])\n\n    def forward(self, x):\n        xs = [stage(x) for stage in self.stages] + [x]\n        x = torch.cat(xs, dim=1)\n        return x\n\n\nclass AUXModule(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = F.adaptive_max_pool2d(x, output_size=(1, 1))\n        x = x.view(-1, x.size(1))\n        x = self.linear(x)\n        return x\n\n\nclass PSPDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            downsample_factor=8,\n            use_batchnorm=True,\n            psp_out_channels=512,\n            final_channels=21,\n            aux_output=False,\n            dropout=0.2,\n    ):\n        super().__init__()\n        self.downsample_factor = downsample_factor\n        self.out_channels = self._get(encoder_channels)\n        self.aux_output = aux_output\n        self.dropout_factor = dropout\n\n        self.psp = PSPModule(\n            self.out_channels,\n            sizes=(1, 2, 3, 6),\n            use_bathcnorm=use_batchnorm,\n        )\n\n        self.conv = Conv2dReLU(\n            self.out_channels * 2,\n            psp_out_channels,\n            kernel_size=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n        if self.dropout_factor:\n            self.dropout = nn.Dropout2d(p=dropout)\n\n        self.final_conv = nn.Conv2d(psp_out_channels, final_channels,\n                                    kernel_size=(3, 3), padding=1)\n\n        if self.aux_output:\n            self.aux = AUXModule(self.out_channels, final_channels)\n\n        self.initialize()\n\n    def _get(self, xs):\n        if self.downsample_factor == 4:\n            return xs[3]\n        elif self.downsample_factor == 8:\n            return xs[2]\n        elif self.downsample_factor == 16:\n            return xs[1]\n        else:\n            raise ValueError('Downsample factor should bi in [4, 8, 16], got {}'\n                             .format(self.downsample_factor))\n\n    def forward(self, x):\n\n        features = self._get(x)\n        x = self.psp(features)\n        x = self.conv(x)\n        if self.dropout_factor:\n            x = self.dropout(x)\n        x = self.final_conv(x)\n        x = F.interpolate(\n            x,\n            scale_factor=self.downsample_factor,\n            mode='bilinear',\n            align_corners=True\n        )\n\n        if self.training and self.aux_output:\n            aux = self.aux(features)\n            x = [x, aux]\n\n        return x\n\n\nclass PSPNet(EncoderDecoder):\n    \"\"\"PSPNet_ is a fully convolution neural network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model used as feature\n                extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        psp_in_factor: one of 4, 8 and 16. Downsampling rate or in other words backbone depth\n            to construct PSP module on it.\n        psp_out_channels: number of filters in PSP block.\n        psp_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n                is used.\n        psp_aux_output: if ``True`` add auxiliary classification output for encoder training\n        psp_dropout: spatial dropout rate between 0 and 1.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n    Returns:\n        ``torch.nn.Module``: **PSPNet**\n    .. _PSPNet:\n        https:\/\/arxiv.org\/pdf\/1612.01105.pdf\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            psp_in_factor=8,\n            psp_out_channels=512,\n            psp_use_batchnorm=True,\n            psp_aux_output=False,\n            classes=21,\n            dropout=0.2,\n            activation='softmax',\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = PSPDecoder(\n            encoder_channels=encoder.out_shapes,\n            downsample_factor=psp_in_factor,\n            psp_out_channels=psp_out_channels,\n            final_channels=classes,\n            dropout=dropout,\n            aux_output=psp_aux_output,\n            use_batchnorm=psp_use_batchnorm,\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = 'psp-{}'.format(encoder_name)","9d56db0a":"\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True, attention_type=None):\n        super().__init__()\n        if attention_type is None:\n            self.attention1 = nn.Identity()\n            self.attention2 = nn.Identity()\n        elif attention_type == 'scse':\n            self.attention1 = SCSEModule(in_channels)\n            self.attention2 = SCSEModule(out_channels)\n\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.block(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(DecoderBlock):\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UnetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels=(256, 128, 64, 32, 16),\n            final_channels=1,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None\n    ):\n        super().__init__()\n\n        if center:\n            channels = encoder_channels[0]\n            self.center = CenterBlock(channels, channels, use_batchnorm=use_batchnorm)\n        else:\n            self.center = None\n\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4],\n                                   use_batchnorm=use_batchnorm, attention_type=attention_type)\n        self.final_conv = nn.Conv2d(out_channels[4], final_channels, kernel_size=(1, 1))\n\n        self.initialize()\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        if self.center:\n            encoder_head = self.center(encoder_head)\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n\nclass Unet(EncoderDecoder):\n    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        center: if ``True`` add ``Conv2dReLU`` block on encoder head (useful for VGG models)\n        attention_type: attention module used in decoder of the model\n            One of [``None``, ``scse``]\n    Returns:\n        ``torch.nn.Module``: **Unet**\n    .. _Unet:\n        https:\/\/arxiv.org\/pdf\/1505.04597\n    \"\"\"\n\n    def __init__(\n            self,\n            encoder_name='resnet34',\n            encoder_weights='imagenet',\n            decoder_use_batchnorm=True,\n            decoder_channels=(256, 128, 64, 32, 16),\n            classes=1,\n            activation='sigmoid',\n            center=False,  # usefull for VGG models\n            attention_type=None\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = UnetDecoder(\n            encoder_channels=encoder.out_shapes,\n            decoder_channels=decoder_channels,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n            center=center,\n            attention_type=attention_type\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = 'u-{}'.format(encoder_name)","dad1d8bd":"model = Unet('se_resnext50_32x4d', classes=4, encoder_weights=None, activation='softmax') ","866bee81":"import os\nos.listdir('\/kaggle\/input\/weight-segmentation\/')\n","33fd2019":"# Initialize mode and load trained weights\nckpt_path = \"\/kaggle\/input\/weight-segmentation\/se_resnext50_32x4d_Unet_checkpoint_185.pth\"\ndevice = torch.device(\"cuda\")\n# model = Unet(\"resnet18\", encoder_weights=None, classes=4, activation=None)\n# model = deeplabv3_resnet50(num_classes=4, pretrained_backbone=False)\n# model = smp.PSPNet('densenet121', classes=4, activation='softmax', encoder_weights=None)\nmodel = model.to(device)\nmodel.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel.load_state_dict(state[\"state_dict\"])","b82b6686":"# start prediction\npredictions = []\nfor i, batch in enumerate(tqdm(testset)):\n    fnames, images = batch\n    batch_preds = torch.sigmoid(model(images.to(device)))\n    batch_preds = batch_preds.detach().cpu().numpy()\n    for fname, preds in zip(fnames, batch_preds):\n        for cls, pred in enumerate(preds):\n            pred, num = post_process(pred, best_threshold, min_size)\n            rle = mask2rle(pred)\n            name = fname + f\"_{cls+1}\"\n            predictions.append([name, rle])\n\n# save predictions to submission.csv\ndf = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\ndf.to_csv(\"submission.csv\", index=False)\n","381e2865":"df.head()","82aa6d64":"### 1.1 resnet","175019b2":"### 1.3 SEnet","b1103859":"### 2. Decoder","9205db8b":"### 1. Encoder","6d841a4e":"### 1.2 DPN"}}