{"cell_type":{"b6abab3a":"code","0901df3e":"code","93fb292e":"code","67ba75e6":"code","4b75624a":"code","12116263":"code","7f0689d5":"code","1a17f7a6":"code","efd4c46b":"code","0b75a521":"code","80db7fe5":"code","35ad18c5":"code","dbf5c707":"code","209792e4":"code","24f9e2e4":"code","589e22f1":"code","9963bea4":"code","a92bbc43":"code","618c92cf":"code","4821e238":"code","33391835":"code","723cf37c":"code","bb22330c":"code","142d3044":"code","1370d9e5":"code","42c3c95e":"code","18ef997b":"code","0be11148":"code","25a10833":"code","4ab8506a":"code","2c9b8959":"code","e6c72f63":"markdown","af5f2c1b":"markdown","4332f21b":"markdown","d8e55398":"markdown","5624f9a3":"markdown","015b5f41":"markdown","e7c326fe":"markdown","0546348a":"markdown","e59d5531":"markdown","81fa1e70":"markdown","fae0fa77":"markdown","4ee5deee":"markdown","34c90d90":"markdown","b1fc5fef":"markdown","74311218":"markdown","b37d14e9":"markdown","d753f031":"markdown"},"source":{"b6abab3a":"# import basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","0901df3e":"# import the dataset\ndf = pd.read_csv('..\/input\/ucl-statistics\/ucl_stats.csv')","93fb292e":"df.head()","67ba75e6":"df.shape","4b75624a":"df.info()","12116263":"#let's see the descriptive statistics of th data\ndf.describe().T","7f0689d5":"# check for null values\ndf.isna().sum()","1a17f7a6":"# number of sample in each class\ndf.champions.value_counts().plot(kind = 'bar');","efd4c46b":"# new features\ndf['win_match_ratio'] = (df['wins'] + 1)\/ df['match_played']\ndf['gs_match_ratio'] = (df['goals_scored'] + 1)\/ df['match_played']\ndf['gc_match_ratio'] = (df['goals_conceded'] + 1)\/ df['match_played']\ndf['win_gs_ratio'] = (df['wins'] + 1)\/(df['goals_scored'] + 1)\ndf['win_lost_ratio'] = (df['wins'] + 1)\/(df['losts'] + 1)\ndf['gs_gc'] = (df['goals_scored'] - df['goals_conceded']) + 0.1\ndf['wins_draws_ratio'] = (df['wins'] +  1) \/ (df['draws'] + 1)\ndf['gs_gd'] = (df['goals_scored'] + 1) + (df['gd'])","0b75a521":"# dropping columns that are not important for the model.\ndf.drop(['year', 'team'], axis = 1, inplace = True)","80db7fe5":"# independent and dependent matrix of feature\nx = df.drop('champions', axis = 1)\ny = df['champions']","35ad18c5":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","dbf5c707":"# mutual info regression scores\nmi_score = make_mi_scores(x, y)\nprint(mi_score)","209792e4":"plot_mi_scores(mi_score)","24f9e2e4":"# balancing the dataset\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nx_sm, y_sm= smote.fit_resample(x, y)","589e22f1":"pd.DataFrame(y_sm).champions.value_counts().plot(kind = 'bar');","9963bea4":"# splitting into train and test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_sm, y_sm,random_state = 31)","a92bbc43":"from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve\ndef report(classifier, x_test = x_test, y_test = y_test):\n    print(\"Classification Report: \\n\")\n    y_preds = classifier.predict(x_test)\n    print(classification_report(y_test, y_preds))\n    print(\"Confusion Matrix: \\n\")\n    plot_confusion_matrix(classifier, x_test, y_test, cmap=plt.cm.Blues)\n    print(\"ROC: \\n\")\n    plot_roc_curve(classifier, x_test, y_test) \n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')","618c92cf":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nlog_clf = LogisticRegression(max_iter = 100,\n                             C = 5.428675439323859,\n                             penalty='l1',\n                             solver='liblinear',\n                             random_state = 41).fit(x_train, y_train)\n\ncv_lr = cross_val_score(log_clf, x_train, y_train, cv = 10)\nlr_score = np.mean(cv_lr)\nprint(\"LR Score: \", lr_score)","4821e238":"report(log_clf)","33391835":"from sklearn.neighbors import KNeighborsClassifier\n\n# parameter tuned by gridsearchcv\nknn_clf = KNeighborsClassifier(algorithm='auto',\n                                leaf_size=10,\n                                n_neighbors=2,\n                                p = 1).fit(x_train, y_train)\n\ncv_knn = cross_val_score(knn_clf, x_train, y_train, cv = 10)\nknn_score = np.mean(cv_knn)\nprint(\"KNN Score: \", knn_score)","723cf37c":"report(knn_clf)","bb22330c":"from sklearn import svm\n\n# parameter tuned by gridsearchcv\nsvc_clf = svm.SVC(C = 233.57214690901213,\n                  degree = 2,\n                  kernel = 'rbf',\n                  random_state = 7).fit(x_train, y_train)\n\ncv_svc = cross_val_score(knn_clf, x_train, y_train, cv = 10)\nsvc_score = np.mean(cv_svc)\nprint(\"SVC Score: \", svc_score)","142d3044":"report(svc_clf)","1370d9e5":"from sklearn.ensemble import RandomForestClassifier\n\n# parameters were taken by randomizedsearchcv\nrand_clf = RandomForestClassifier(n_estimators=1000,\n                                 min_samples_split = 4,\n                                 min_samples_leaf = 1,\n                                 max_depth = None,\n                                 random_state = 35).fit(x_train, y_train)\n\ncv_rf = cross_val_score(rand_clf, x_train, y_train, cv = 10)\nrf_score = np.mean(cv_rf)\nprint(\"RF Score: \", rf_score)","42c3c95e":"report(rand_clf)","18ef997b":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc_clf = GradientBoostingClassifier(learning_rate=0.1,\n                                 loss='deviance',\n                                 max_depth=2,\n                                 min_samples_leaf=5,\n                                 min_samples_split=2,\n                                 n_estimators=500,\n                                 random_state=31).fit(x_train, y_train)\n\ncv_gbc = cross_val_score(gbc_clf, x_train, y_train, cv = 10)\ngbc_score = np.mean(cv_gbc)\nprint(\"GBC Score: \", gbc_score)","0be11148":"report(gbc_clf)","25a10833":"from lightgbm import LGBMClassifier\nlgbm = LGBMClassifier().fit(x_train, y_train)\ncv_lgbm = cross_val_score(lgbm, x_train, y_train, cv = 10)\nlgbm_score = np.mean(cv_lgbm)\nprint(\"Lgbm Score: \", lgbm_score)","4ab8506a":"report(lgbm)","2c9b8959":"score_dict= {'Logistic Regression':lr_score,\n             'KNN':knn_score,\n             'SVC':svc_score,\n             'Random Forest': rf_score,\n             'Gradient Boost':gbc_score,\n             'LightBGM':lgbm_score}\npd.DataFrame.from_dict(score_dict, orient = 'index', columns = ['Score'])","e6c72f63":"# Score Analysis","af5f2c1b":"# Gradient Boost","4332f21b":"We clearly dealing with Imbalance distribution problem. We must solve to get better model. Here is the [link](https:\/\/www.kaggle.com\/bakar31\/a-guide-to-imbalance-distribution-problem-in-ml) of a notebook in that I talked about various methods of balancing a dataet.","d8e55398":"# Basic info about data","5624f9a3":"## KNN","015b5f41":"# LightBGM","e7c326fe":"# Table of Contents\n\n* [EDA](https:\/\/www.kaggle.com\/bakar31\/ucl-stats-analysis)\n* [Basic data Info](#Basic-info-about-data)\n    - [Solve imbalance distribution problem](https:\/\/www.kaggle.com\/bakar31\/a-guide-to-imbalance-distribution-problem-in-ml)\n* [Feature Enginnering](#Feature-Engineering)\n* [Feature Selection](#Feature-Selection)\n* [Solve Imbalance Problem](#Balancing-Imbalance-Distribution-Problem)\n* [Modeling](#Modeling)\n    - [Logistic Regression](#Logistic-Regression)\n    - [KNN](#KNN)\n    - [SVC](#SVC)\n    - [Random Foresr](#Random-Forest)\n    - [GradientBoost](#Gradient-Boost)\n    - [LightBGM](#LightBGM)\n* [Score Analysis](#Score-Analysis![4D6N2F.gif](attachment:db4a4db7-3598-4b17-8bce-8074b5bbc41e.gif))","0546348a":"# Feature engineering\n\nFeature engineerning is a very important and fruitful steps in every machine learning pipeline. Feature Enginnering enables us to create more powerful features from the existing feature that can help us making a better mode.","e59d5531":"# Random Forest","81fa1e70":"### An upvote will be very appreciated ","fae0fa77":"# Balancing Imbalance Distribution Problem","4ee5deee":"## Logistic Regression","34c90d90":"# Overview:\n\nUCL stands for UEFA Champions League. This is the biggest Title race in the club football. I have done a very deep analysis in [this notebook](https:\/\/www.kaggle.com\/bakar31\/ucl-stats-analysis). Feel free to check this out.What are we gonna do is to develop a model with the highest accuracy, precision and F1-score to predict the new season champion. But to predict we need current season's data like number of matches played, number of wins, number of losses, goal difference, how many goals scored etc. Let's build our model","b1fc5fef":"**Yay!** Our imbalance distribtion problem is solved.","74311218":"# Feature Selection","b37d14e9":"# SVC","d753f031":"# Modeling"}}