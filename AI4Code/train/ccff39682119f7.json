{"cell_type":{"a75e8c19":"code","d9716a01":"code","d127d547":"code","2d5bd9df":"code","e397e86a":"code","841b63b4":"code","00bd0b14":"code","a6dad156":"code","71d4f770":"code","cd75c493":"code","e4b28895":"code","1d29d2bb":"code","776dfb4d":"code","6de6376d":"code","609d09ea":"code","e3993937":"code","4e19231a":"code","ddc36711":"code","3b7b2c9e":"markdown","229f19bb":"markdown","819fc701":"markdown","a792b7ba":"markdown","f1904ec0":"markdown","42dee910":"markdown","d39489fa":"markdown","f9aabc62":"markdown","bd4432c6":"markdown"},"source":{"a75e8c19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom catboost import Pool, CatBoostClassifier\n\nfrom scipy.stats import pearsonr, chi2_contingency\nfrom itertools import combinations\nfrom statsmodels.stats.proportion import proportion_confint","d9716a01":"# Read all the data\nall_data = pd.read_csv(\n    '..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True)\n# Only process data in a 1 year period from beginning of 2018 to beginning of 2019\nall_data = all_data[(all_data.issue_d >= '2018-01-01 00:00:00') & (all_data.issue_d < '2019-01-01 00:00:00')]\nall_data.head()","d127d547":"# Read in the LCDataDictionary.xls file which contains all the features available to investors and their descriptions\nbrowse_notes = pd.read_excel('..\/input\/lending-club-loan-data\/LCDataDictionary.xlsx',sheet_name=1)\n\n# Read in the features file\nbrowse_features = browse_notes['BrowseNotesFile'].dropna().values\n\n# Reformat the feature names to match the ones in all_data\nbrowse_features = [re.sub('(?<![0-9_])(?=[A-Z0-9])', '_', x).lower().strip() for x in browse_features]\n\n# There are some features in both datasets that mean the same thing are different words\n# are used for them. Change those feautre names accordingly\nwrong = ['is_inc_v', 'mths_since_most_recent_inq', 'mths_since_oldest_il_open',\n         'mths_since_recent_loan_delinq', 'verified_status_joint']\ncorrect = ['verification_status', 'mths_since_recent_inq', 'mo_sin_old_il_acct',\n           'mths_since_recent_bc_dlq', 'verification_status_joint']\n\nbrowse_features = np.setdiff1d(browse_features, wrong)\nbrowse_features = np.append(browse_features, correct)\n\n# Remove all the feauture columns from all_data that are not part of browse_features\nall_data_feaures = all_data.columns.values\navailble_features = np.intersect1d(browse_features, all_data_feaures)\navailble_features = np.append(availble_features, ['loan_status']);\ndata_with_available_features = all_data[availble_features].copy()\n\n# Get info regarding the dataframe to confirm appropriate features are removed\ndata_with_available_features.info()","2d5bd9df":"# Change appropriate columns to float type and remove rows where string cannot be converted to float\ndata_with_available_features['emp_length'] = data_with_available_features['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\ndata_with_available_features['emp_length'] = data_with_available_features['emp_length'].str.extract('(\\d+)').astype('float')\n\n# Change appropriate columns to datetime type\ndata_with_available_features['earliest_cr_line'] = pd.to_datetime(data_with_available_features['earliest_cr_line'], infer_datetime_format=True)\ndata_with_available_features['sec_app_earliest_cr_line'] = pd.to_datetime(data_with_available_features['sec_app_earliest_cr_line'], infer_datetime_format=True)\n\n# Remove columns with no values\ndata_with_available_features = data_with_available_features.drop(['desc', 'member_id', 'id'], axis=1, errors='ignore')\n\n# Get info regarding the dataframe to confirm appropriate features are removed\ndata_with_available_features.info()","e397e86a":"# Define columns that need to be filled with an empty value, max, or min value\nfill_empty = ['emp_title', 'verification_status_joint']\nfill_max = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n            'mths_since_last_major_derog', 'mths_since_last_record',\n            'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n            'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n            'pct_tl_nvr_dlq','sec_app_mths_since_last_major_derog']\nfill_min = np.setdiff1d(data_with_available_features.columns.values, np.append(fill_empty, fill_max))\n\ndata_with_available_features[fill_empty] = data_with_available_features[fill_empty].fillna('')\ndata_with_available_features[fill_max] = data_with_available_features[fill_max].fillna(data_with_available_features[fill_max].max())\ndata_with_available_features[fill_min] = data_with_available_features[fill_min].fillna(data_with_available_features[fill_min].min())","841b63b4":"# Remove all features that only hold one value or all unqiue values\ndata_with_available_features = data_with_available_features.drop(['num_tl_120dpd_2m', 'url', 'emp_title'], axis=1, errors='ignore')\n\n# Calculate the correlation between all pairs of numeric features\n# Pearson's R correlation coefficient was used\nnum_feat = data_with_available_features.select_dtypes('number').columns.values\ncomb_num_feat = np.array(list(combinations(num_feat, 2)))\ncorr_num_feat = np.array([])\nfor comb in comb_num_feat:\n    isNaN = np.logical_or(np.isnan(data_with_available_features[comb[0]]), np.isnan(data_with_available_features[comb[1]]))\n    isINF = np.logical_or(np.isinf(data_with_available_features[comb[0]]), np.isinf(data_with_available_features[comb[1]]))\n    if isNaN is False and isINF is False:\n        corr = pearsonr(data_with_available_features[comb[0]], data_with_available_features[comb[1]])[0]\n        corr_num_feat = np.append(corr_num_feat, corr)\n# for comb in comb_num_feat:\n#     corr = pearsonr(data_with_available_features[comb[0]], data_with_available_features[comb[1]])[0]\n#     corr_num_feat = np.append(corr_num_feat, corr)\n\n# Drop the first one in the pair with coefficient >= 0.9\nhigh_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.9]\ndata_with_available_features = data_with_available_features.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')\n\n# Calculate the correlation between all pairs of categorical features\n# Cramer's V correlation coefficient was used\ncat_feat = data_with_available_features.select_dtypes('object').columns.values\ncomb_cat_feat = np.array(list(combinations(cat_feat, 2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(data_with_available_features, values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0] \/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)\n\n# Drop the second one in the pair with coefficient >= 0.9\nhigh_corr_cat = comb_cat_feat[corr_cat_feat >= 0.9]\ndata_with_available_features = data_with_available_features.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')\n","00bd0b14":"data_to_use = data_with_available_features.copy()\ndata_to_use.head()","a6dad156":"#example of non-float column\ndata_to_use['verification_status'].value_counts()\n","71d4f770":"from sklearn import preprocessing\nfor (columnName, columnData) in data_to_use.iteritems():\n    if(columnData.dtype is not 'float64'):\n        if columnData.dtype == 'object':\n            le = preprocessing.LabelEncoder()\n            le.fit(columnData.values.astype(str))\n            data_to_use[columnName] = le.transform(columnData.values.astype(str))\n        else:\n            le = preprocessing.LabelEncoder()\n            le.fit(columnData.values)\n            data_to_use[columnName] = le.transform(columnData.values)\n        print('done')\n#     if(isinstance((columnData.values[0]),float) != True):\n#         print('Colunm Name : ', columnName)\n#         le = preprocessing.LabelEncoder()\n#         le.fit(columnData.values)\n#         data_to_use[columnName] = le.transform(columnData.values)\n#         print(\"done\")","cd75c493":"data_to_use['loan_status'].value_counts()","e4b28895":"data_with_available_features['loan_status'].value_counts()","1d29d2bb":"# Create the expected_output dataframe. Classifying 1\/3\/4 as the same \nexpected_output = data_to_use['loan_status'].copy()\nexpected_output = expected_output.isin(['1', '3', '4']).astype('int')\n\n# remove the int_rate field since its highly correlated with the grading field and loan_status since we saved it in expected_output\ndataset = data_to_use.drop(['int_rate', 'loan_status'], axis=1, errors='ignore')\nexpected_results = expected_output[dataset.index]\n\n# Split the dataset and the expected_result set into training and testing\ndataset_train, dataset_test, expected_results_train, expected_results_test = train_test_split(dataset, expected_results, stratify=expected_results, random_state=0)\n\n\nexpected_results_train.head()\n","776dfb4d":"# import the necessary module\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n#create an object of the type GaussianNB\ngnb = GaussianNB()\n#train the algorithm on training data and predict using the testing data\npred = gnb.fit(dataset_train, expected_results_train).predict(dataset_test)\n#print(pred.tolist())\n#print the accuracy score of the model\nprint(\"Naive-Bayes accuracy : \",accuracy_score(expected_results_test, pred, normalize = True))","6de6376d":"# actual_results = classifier.predict(dataset_test)\nactual_results = pred\nprint(\"And Voila!\")\n\n# expected_results_test vs dataset_test\nacc_test = accuracy_score(expected_results_test, actual_results)\nprec_test = precision_score(expected_results_test, actual_results)\nrec_test = recall_score(expected_results_test, actual_results)\nprint(f'''Accuracy (test): {acc_test:.3f}\nPrecision (test): {prec_test:.3f}\nRecall (test): {rec_test:.3f}''')","609d09ea":"#Quick Check With The Original Dataset\nprint(expected_results_test)","e3993937":"print(data_with_available_features.loc[466264].loan_status)\nprint(data_with_available_features.loc[511723].loan_status)\nprint(data_with_available_features.loc[1437966].loan_status)\n\n#last one to check if did not successfully pay\nprint(data_with_available_features.loc[465298].loan_status)\n\n","4e19231a":"# Generate the confusion matrix\ncm = confusion_matrix(expected_results_test, actual_results)\nax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","ddc36711":"print(data_to_use['loan_status'])","3b7b2c9e":"We only want the features that are available to investors, ie. feautures known before the loan.\nA list of all feautres known to investors before the loan is provided in  LCDataDictionary.xls.\nRemove all columns from all_data that are not in this file and save in a new dataset. Should be 109 columns after filtering.","229f19bb":"# Logistic Regression\n","819fc701":"Change the field values of some columns depending on if the missing value should be replaced with a empty, max, or min value.","a792b7ba":"Checking the encoding was done correctly for one of the columns ","f1904ec0":"Change the types of some columns for the model and remove columns with no values.","42dee910":"Delete all feautres with a very high correlation coefficient to improve model accuracies. ","d39489fa":"Read the whole lending club dataset. There are 151 columns but some are not needed.","f9aabc62":"Encoding values that were not floats to be used for Logistics Regression model","bd4432c6":"Create new dataframe with 1 column which is the expected_output dataframe. 1 means loan is success, 0 means its not. Current, Fully Paid, and In Grace Period loan statuses will count as success.\n\n"}}