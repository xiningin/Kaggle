{"cell_type":{"d201b983":"code","d236cd41":"code","aa35f055":"code","9035297f":"code","7cb5143e":"code","b08d77f1":"code","ddbeb93f":"code","ced2d2af":"code","4fe7d8b5":"code","84f445e3":"code","863b36c8":"code","f5652098":"markdown","05cd2637":"markdown","1cd8ad4c":"markdown"},"source":{"d201b983":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d236cd41":"import pandas as pd","aa35f055":"base_path = \"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/\"\ntest_path = base_path + \"test.csv\"\ntrain_path = base_path + \"train.csv\"\n\ntest_set = pd.read_csv(test_path)\ntrain_set = pd.read_csv(train_path)","9035297f":"# Add End Index\ntrain_set['answer_end'] = train_set['answer_text'].str.len() + train_set['answer_start']\ntrain_set.head()","7cb5143e":"# Find Max Context Length to be used as a hyperparameter\n# Don't have enough memory for training so we only get to look at the first 1660 characters\n\n# To-do: Improve this pipeline by splitting text into smaller pieces for processing.\nmax_sequence_length = int(train_set['context'].map(lambda x: len(x)).max() \/ 30 ) \nprint(max_sequence_length)","b08d77f1":"# Using Keras for text preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","ddbeb93f":"# Setting the char_level value to true because the\n# size of the word_index is smaller and this saves memory when training the model.\n\n# To-Do: Optimize pipeline so words are used instead of char_level\ntokenizer = Tokenizer(char_level = True)\ntokenizer.fit_on_texts(train_set['context'])\nmax_word_index = len(tokenizer.word_index) + 1\nprint(max_word_index)","ced2d2af":"def prep_text(texts, tokenizer, max_sequence_length):\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(text_sequences, maxlen=max_sequence_length)\n\n# Convert each of the texts into sequences.\ntrain_context_sequence = prep_text(train_set['context'], tokenizer, max_sequence_length)\ntrain_question_sequence = prep_text(train_set['question'], tokenizer, max_sequence_length)\ntest_context_sequence = prep_text(test_set['context'], tokenizer, max_sequence_length)\ntest_question_sequence = prep_text(test_set['question'], tokenizer, max_sequence_length)","4fe7d8b5":"from keras.models import Model\nfrom keras.layers import Embedding, SpatialDropout1D, LSTM, concatenate, Dense\nfrom keras import Input\n\ntext_vocabulary_size = 1000\nquestion_vocabulary_size = 1000\nanswer_vocabulary_size = 1\n\ntext_input = Input(shape=(None,), dtype='int32', name='text')\nembedded_text = Embedding(max_word_index, text_vocabulary_size)(text_input)\nembedded_text = SpatialDropout1D(0.2)(embedded_text)\nencoded_text = LSTM(32)(embedded_text)\n\nquestion_input = Input(shape=(None,), dtype='int32', name='question')\nembedded_question = Embedding(max_word_index, question_vocabulary_size)(question_input)\nembedded_question = SpatialDropout1D(0.2)(embedded_question)\nencoded_question = LSTM(32)(embedded_question)\n\nconcatenated = concatenate([encoded_text, encoded_question])\n\nstart_index = Dense(1, activation='softmax')(concatenated)\n\nend_index = Dense(1, activation='softmax')(concatenated)\n\nmodel = Model([text_input, question_input], outputs=[start_index, end_index])\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])","84f445e3":"model.fit([train_context_sequence, train_question_sequence], [train_set['answer_start'], train_set['answer_end']], epochs=10, batch_size=128)","863b36c8":"predictions = model.predict([test_context_sequence, test_question_sequence])","f5652098":"# Preprocessing\n\nFirst load the dataset and tokenize the text.\n\nWe must also calculate the End Index from the start index and the answer text.\n\nTo-Do: Split the Context at predefined sizes in a way that allows the model to train.","05cd2637":"# Model\n\nA test model with two inputs and two outputs.\nThe Inputs are the context and questions as integer sequences.\nInstead of One-Hot encoding the model learns the embeddings at training time, for both inputs.\nAn LSTM processes the input's Embeddings and their results are contatenated.\nThe concatenated layer values are passed to a Dense Layer for predictions of the start and end indicies.\n\nTo-Do: Replace the Dense Layers with something better.","1cd8ad4c":"# Introduction\n\nA novice attempt at Question and Answering in Hindi and Tamil. Will be improved over time.\n\n# Understanding the Problem\n\nGiven a pair of inputs, Context and Question, return a String that Answers the question for the given context. The Answers are drawn directly from the Context (Answers are a subset of Context). The Answer includes punctuation.\n\nAnswers will be evaluated using the word-level Jaccard Score as provided by the competition."}}