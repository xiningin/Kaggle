{"cell_type":{"e32827ce":"code","2c93fbf6":"code","6598f074":"code","f4184b70":"code","a0867dcf":"code","f7671732":"code","9ff17f44":"code","83540194":"code","bb385967":"code","f9d7ff5f":"code","00c2c7a8":"code","df78e96b":"code","31deafbc":"code","b487d7bf":"markdown","2d1e06ec":"markdown","9908d61c":"markdown","29544604":"markdown","8e9cd472":"markdown","f16b296f":"markdown","22f60d41":"markdown","de68a332":"markdown","28c1bb78":"markdown","bb10141a":"markdown","b93da8c9":"markdown","bf0bdf51":"markdown","7501c79a":"markdown","d7d86465":"markdown","42dbd829":"markdown"},"source":{"e32827ce":"import pandas as pd\nimport numpy as np","2c93fbf6":"pd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 5000)\npd.set_option('display.width', 10000)\ndata = pd.read_csv('..\/input\/train.csv')","6598f074":"data.shape","f4184b70":"print(data.head()) #similar case to data.tail(), with output being the last 5 rows.","a0867dcf":"print(data.dtypes)","f7671732":"data.isnull().sum()","9ff17f44":"print(data.PassengerId.nunique())","83540194":"print(data.describe())","bb385967":"print(data.Pclass.value_counts())","f9d7ff5f":"data.sort_values('PassengerId', ascending=False, inplace=True)\n#data = data.sort_values('PassengerId', ascending=False) #other alternative, same output\nprint(data.head())","00c2c7a8":"data.drop_duplicates('Sex', keep='first', inplace=True)\n#data = data.drop_duplicates('Sex', keep='first') #also alretnatives for the absence of 'inplace=True'\nprint(data.shape)\nprint(\"Here are all the data left:\\n\")\nprint(data)","df78e96b":"data = pd.read_csv('..\/input\/train.csv')\nprint(\"Number of null entries in Age column before fill in is: \"+ str(data['Age'].isnull().sum()) + \"\\n\")\navr_age = float(data['Age'].mean())\ndata['Age'].fillna(avr_age, inplace=True) #avr_age arguments contains the float value for the replacement\nprint(\"Number of null entries in Age column after fill in is: \"+ str(data['Age'].isnull().sum()))","31deafbc":"data_gp = data.groupby(['Survived', 'Sex']).agg({'PassengerId': 'count',\n                                      'Age': 'mean'\n                                      })\ndata_gp.reset_index(inplace=True) #to retain the index numbering for the resulting table\nprint(data_gp)","b487d7bf":"For **Data sorting**, we could use the command '-.sort_values('ColumnName', ascending=(True\/False), inplace=True)'. The inplace argument is merely there to substitute the other way to run this code, both will be shown below:","2d1e06ec":"As you can see, 'data' now only contains two people, with different sex. This is because every same sexes are dropped after the first two unique sexes are found. The argument 'keep=first' makes sure that the program will keep the first unique row. The other possible argument is 'keep=last', which will let the computer drop every other duplicates except for the last one. \n\n**Filling missing values** can be done using the '-.fillna(SubsValue, inplace=True), example shown below. But first, I'd have to re-import 'data' since it was left with only 2 rows from last command. In the code below, I will replace every null values with the average age of the rest of the passengers.","9908d61c":"As you can see, the head() function is now listed in descending order, since we sorted the PassengerId to descend in value. We specifued the sort_values ascending argument to be False (ascending = false).\n**Dropping Duplicate Data** is simply done by using the command '-.drop_duplicates('ColumnName', keep= 'first\/last'. In the code snippet below, I will drop duplicate Sex (which is not the best idea, but its for the sake of learning).","29544604":"I would then fill in the proprietary panda command to configure the display option. The appropriate path \/ directory to the relevant csv file: train.csv is also set.","8e9cd472":"The value matches with the amount of row (recall that the data shape was 891 x 12), which means there are no duplicate PassengerId in the file. \n\nNow, we'll move on to the **data distribution check**. To begin, I will first see the overall view, using the command '-.describe()' as shown below.","f16b296f":"The **data types** for each individual column can be checked with 'data.dtypes'.","22f60d41":"The 'Survived' value above is user-defined, and the data author uses 0 to signify death and 1 means alive.  Notice the arrangement of the groupby function; we put in 'Survived' and 'Sex' column respectively. This order will classify the number of Females & Males that died and survived.","de68a332":"To check for **duplicate data**, I consider PassengerId as the key, and checked to see if they're unique with below code:","28c1bb78":"I could then view various properties for each column. The formatting of the code is simply '[DatasetName].[ColumnName].[Command()]'. For example, the code below classify the numbers of people in their corresponding ship class.","bb10141a":"**End of Note** that is all that I've learned for today, hope you learned someting new from this kernel.","b93da8c9":"# Introduction to Panda: Basic commands\nThis is my very first post \/ kernel on kaggle, and I'm entirely new to Data Science \/ Data Analytics, so there might be mistakes here and there throughout this article. Inputs and comments are greatly appreciated, and I'll try to improve this post overtime. Since I'm new to this, I'll post comments and explanation on basic commands that might seem obvious by nature, but its more for my sake of learning. I'll start off with importing the 2 basic libraries that will be used throughout.","bf0bdf51":"The function '-.head(n)' is used to show the first n rows, with the default value for n being 5. The opposite is true for '-.tail(n)', to show the last n rows.","7501c79a":"**Group By** in Panda is somewhat similar to the method I've used in mySQL. I will explain the code below on the following paragraph.","d7d86465":"The function '-.shape' is used to tell use the number of rows and columns.","42dbd829":"To **check the missing values**, I can use the '-.isnull()' command, and combine it with '-.sum()' to count the amount of null data in every column."}}