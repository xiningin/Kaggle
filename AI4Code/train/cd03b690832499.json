{"cell_type":{"d0ca40d3":"code","6a258406":"code","feddd7b9":"code","be0e6fdc":"code","e5f60585":"code","9ea64c83":"code","cb12b0ce":"code","1e4baa06":"code","87a51cb6":"code","e9661f36":"code","8a934742":"code","faed3a16":"code","20af4a7c":"code","4f83294e":"code","afb1cf43":"code","8cb52f9d":"code","4c8d0d3e":"code","878cdbab":"code","e897b739":"code","9e859411":"code","5239afcd":"code","831cd9d1":"code","5548f650":"code","df51a111":"code","8447d4e3":"code","8702bbd1":"code","db96e13d":"code","97e30e7f":"code","06b8060e":"code","e7f5ea09":"code","615439cb":"code","62b321fe":"code","3977e42e":"code","4ec662de":"code","79090fc1":"code","fa98186e":"code","5c1c9e7c":"code","288bfbe0":"code","f6e46e78":"code","0bf22e8f":"code","76809bb4":"code","0a9a079e":"code","4e274153":"code","21bbd9ff":"code","224e902a":"code","51f6aa85":"code","dfe6886c":"code","a34d52ae":"code","e37de091":"code","88d94271":"code","23bfb81d":"code","61a93e97":"code","720bca22":"code","0c078e37":"code","b12ffacf":"code","65eefb0c":"code","94d4f9a0":"code","39de46e2":"code","64d5ad48":"code","f19a0be4":"code","f06f8ae0":"code","ef128f26":"code","95646063":"code","5aa43f79":"code","7f8f10f3":"code","84d21169":"code","15999208":"markdown","2a0b2934":"markdown","198b3d43":"markdown","ccd3667a":"markdown","605be420":"markdown","d50d1987":"markdown","94d8abba":"markdown","d2c9c630":"markdown","f93484f0":"markdown","b016d726":"markdown","82163503":"markdown","fa5e4132":"markdown","e5709016":"markdown","0ab28e06":"markdown","e536831b":"markdown","fd1c7c28":"markdown","4f10cd75":"markdown","e8bfa018":"markdown","22bd4522":"markdown","ad2a4d73":"markdown","dfbcfedc":"markdown","18593423":"markdown"},"source":{"d0ca40d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6a258406":"train_df = pd.read_csv('..\/input\/shopee-sentiment-analysis\/train.csv')\ntest_df = pd.read_csv('..\/input\/shopee-sentiment-analysis\/test.csv')\nprint(train_df.shape, test_df.shape)","feddd7b9":"train_df.head()","be0e6fdc":"test_df.head()","e5f60585":"dup_df = train_df[train_df['review'].duplicated()]\nprint(f'No. of duplicate reviews on train data: {dup_df.shape[0]}')","9ea64c83":"dup_df['check'] = dup_df.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_df['check'].duplicated().sum(),'of duplicate reviews have the same rating')","cb12b0ce":"train_df.drop_duplicates(subset = 'review', inplace = True)","1e4baa06":"train_df['rating'].value_counts()","87a51cb6":"count_df = train_df.groupby(['rating']).count()\ncount_df.drop(['review_id'], axis = 1, inplace = True)\ncount_df['percentage'] = 100 * count_df['review']  \/ count_df['review'].sum()\ncount_df","e9661f36":"train_df['rating'].hist()","8a934742":"from nltk import word_tokenize\n\ndef count_len(text):\n    return len(word_tokenize(text))","faed3a16":"train_df['len'] = train_df['review'].apply(count_len)\ntest_df['len'] = test_df['review'].apply(count_len)","20af4a7c":"train_df['len'].hist()","4f83294e":"test_df['len'].hist()","afb1cf43":"# kaggle.com\/liuhh02\/test-labelled\n# old test leak labelled\ntest_labelled = pd.read_csv('..\/input\/test-labelled\/test_labelled.csv')\ntest_labelled","8cb52f9d":"dup_testlab = test_labelled[test_labelled['review'].duplicated()]\nprint(f'No. of duplicate reviews: {dup_testlab.shape[0]}')\ndup_testlab['check'] = dup_testlab.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_testlab['check'].duplicated().sum(),'of duplicate reviews have the same rating')","4c8d0d3e":"test_labelled.drop_duplicates(subset = 'review', inplace = True)","878cdbab":"# kaggle.com\/shymammoth\/shopee-reviews\n# scraped shopee reviews\nscraped_reviews = pd.read_csv('..\/input\/shopee-reviews\/shopee_reviews.csv')\nscraped_reviews","e897b739":"scraped_reviews.rename(columns = {'text': 'review', 'label': 'rating'}, inplace = True)\nscraped_reviews.info()","9e859411":"scraped_reviews['rating'].value_counts()","5239afcd":"scraped_reviews = scraped_reviews[scraped_reviews['rating'] != 'label']\nscraped_reviews['rating'] = scraped_reviews['rating'].astype(int)\nscraped_reviews['rating'].value_counts()","831cd9d1":"dup_scraped = scraped_reviews[scraped_reviews['review'].duplicated()]\nprint(f'No. of duplicate reviews: {dup_scraped.shape[0]}')","5548f650":"train_df = train_df.append(test_labelled, ignore_index = True)\ntrain_df = train_df.append(scraped_reviews, ignore_index = True)\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)","df51a111":"dup_train = train_df[train_df['review'].duplicated()]\nprint(f'No. of duplicate reviews on train data: {dup_train.shape[0]}')\ndup_train['check'] = dup_train.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_train['check'].duplicated().sum(),'of duplicate reviews have the same rating')","8447d4e3":"train_df","8702bbd1":"train_df['rating'].value_counts()","db96e13d":"train_df = train_df.drop(train_df[train_df['rating'] == 5].sample(frac = .6).index)\nprint(train_df['rating'].value_counts())\nprint(train_df.shape)","97e30e7f":"# adding column 'rating' to test dataset\ntest_df['rating'] = -1 # flag to separate train and test\n\n# joining train and test datasets\nreviews = pd.concat([train_df, test_df], ignore_index = True)\nreviews","06b8060e":"from nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\n\nstemmer = SnowballStemmer('english')\nlemma = WordNetLemmatizer()\n\nfrom string import punctuation","e7f5ea09":"def clean_review(review_col):\n    review_corpus=[]\n    \n    for i in range(0, len(review_col)):\n        review = str(review_col[i])\n        review = re.sub('[^a-zA-Z]', ' ', review)\n        review = [lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review = ' '.join(review)\n        \n        review_corpus.append(review)\n        \n    return review_corpus","615439cb":"import emoji  \n\nhave_emoji_train_idx = []\n\nfor idx, review in enumerate(reviews['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)","62b321fe":"def emoji_cleaning(text):\n    \n    # change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","3977e42e":"# emoji_cleaning\nreviews.loc[have_emoji_train_idx, 'review'] = reviews.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)","4ec662de":"def review_cleaning(text):\n    \n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n#     text = text.replace(\"n't\", ' not')\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n#     # delete punctuation\n#     text = re.sub('[^a-z0-9 ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])","79090fc1":"reviews['review'] = reviews['review'].apply(review_cleaning)","fa98186e":"repeated_rows_train = []\n\nfor idx, review in enumerate(reviews['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)","5c1c9e7c":"def delete_repeated_char(text):\n    \n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text","288bfbe0":"reviews.loc[repeated_rows_train, 'review'] = reviews.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)","f6e46e78":"def recover_shortened_words(text):\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","0bf22e8f":"%%time\nreviews['review'] = reviews['review'].apply(recover_shortened_words)","76809bb4":"# cleaning round 2, lemmatization\nreviews['review'] = clean_review(reviews['review'].values)\nreviews","0a9a079e":"train_df = reviews[reviews.rating != -1]\ntrain_df.drop(['review_id', 'len'], axis = 1, inplace = True)\ntrain_df.head()","4e274153":"test_df = reviews[reviews.rating == -1]\ntest_df.drop(['rating', 'len'], axis = 1, inplace = True)\ntest_df['review_id'] = test_df['review_id'].astype(int)\ntest_df.head()","21bbd9ff":"train_df[['review', 'rating']].to_csv('clean_extended_train.csv', index = False)\ntest_df[['review_id', 'review']].to_csv('clean_test_up.csv', index = False)","224e902a":"# train_df = pd.read_csv('..\/input\/shopee-code-league-2020-sentiment-analysis\/clean_extended_train.csv').fillna('')\n# test_df = pd.read_csv('..\/input\/shopee-code-league-2020-sentiment-analysis\/clean_test_up.csv').fillna('')\n# print(train_df.shape, test_df.shape)","51f6aa85":"train_df.head()","dfe6886c":"test_df.head()","a34d52ae":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)","e37de091":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","88d94271":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token) # 5 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","23bfb81d":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","61a93e97":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMODEL = 'jplu\/tf-xlm-roberta-large' # bert-base-multilingual-uncased","720bca22":"# since keras takes 0 as the reference, our category should start from 0 not 1\nrating_mapper_encode = {1: 0,\n                        2: 1,\n                        3: 2,\n                        4: 3,\n                        5: 4}\n\n# convert back to original rating after prediction later\nrating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)","0c078e37":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train_df['rating'], num_classes=5)","b12ffacf":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=1111)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","65eefb0c":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","94d4f9a0":"MAX_LEN = 104 # chosen from EDA\n\nX_train = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df['review'].values, tokenizer, maxlen=MAX_LEN)","39de46e2":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)","64d5ad48":"%%time\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","f19a0be4":"n_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","f06f8ae0":"plt.style.use('fivethirtyeight')\n\n# Get training and test loss histories\ntraining_loss = train_history.history['loss']\ntest_loss = train_history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","ef128f26":"pred = model.predict(test_dataset, verbose=1)","95646063":"# for ensemble\nnp.save('xlm-roberta', pred)","5aa43f79":"pred_sentiment = np.argmax(pred, axis=1)\n\nprint(pred_sentiment)","7f8f10f3":"submission = pd.DataFrame({'review_id': test_df['review_id'],\n                           'rating': pred_sentiment})","84d21169":"submission['rating'] = submission['rating'].map(rating_mapper_decode)\nsubmission.to_csv('submission.csv', index=False)\nsubmission['rating'].value_counts()","15999208":"**Length statistics**","2a0b2934":"Check if the duplicate reviews have the same rating","198b3d43":"**Load data**","ccd3667a":"**Thanks to:**\n* indralin\/text-processing-augmentation-tpu-baseline-0-4544\n* garyongguanjie\/eda-sentiment-shopee","605be420":"**Check for duplicate reviews**","d50d1987":"**Separating train and test sets**","94d8abba":"Looking at the histogram, a length of 100 is enough to cover most (more than 90 pecent) of the data","d2c9c630":"# Convert rating values back to (1 - 5)","f93484f0":"Trim train set by half to minimize training time","b016d726":"Change column names for merging","82163503":"Since most of the duplicate reviews have the same rating, we could choose to drop them","fa5e4132":"Save to file: clean extended train and test data","e5709016":"# Data Preprocessing","0ab28e06":"**Extend train data with publicly, freely available external data**","e536831b":"**Data cleaning**","fd1c7c28":"## Modeling","4f10cd75":"# Import clean extended train and test","e8bfa018":"Final check for duplicates","22bd4522":"**Rating statistics**","ad2a4d73":"**Change range of rating from (1 to 5) to (0 to 4)**","dfbcfedc":"Join train and test for cleaning","18593423":"# EDA"}}