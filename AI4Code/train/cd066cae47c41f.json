{"cell_type":{"ab97e3c6":"code","fb821fa6":"code","7a9f9459":"code","9befa148":"code","fb08ebce":"code","9787fdc7":"code","4fb9b25a":"code","e9f1cd42":"code","f76e399a":"code","fc603f81":"code","0aef35f4":"code","b8f81b1d":"code","8c32f412":"code","f38d3262":"code","abde522f":"code","4fb55355":"code","11cb23bf":"code","e4e47800":"code","096a1992":"code","76f07794":"code","727e7d37":"code","9cf028bf":"code","40d51387":"code","f9ce078a":"code","93358c8a":"code","6bbcff11":"code","cb16c937":"code","e6223845":"code","d7819000":"code","2d9851a8":"code","78568538":"code","447152ce":"code","88925297":"code","750d95eb":"code","1e170e20":"code","2e57653f":"code","53bc4a75":"code","5a0d21d5":"code","d8e46f74":"code","ba9f2ffa":"code","c7115a60":"code","e0151f7c":"code","b56fc710":"code","aceb102b":"code","b6ff05d8":"code","dc7817be":"code","c40ef5f8":"code","d63a7dea":"code","184e843e":"code","f00cb86e":"code","c5506c4c":"code","273d1d9f":"code","a15c5c07":"code","3f0762de":"markdown","9ae11c3c":"markdown","3c732ccd":"markdown","14464e51":"markdown","1b890a42":"markdown","ff7499a3":"markdown","c5131c6f":"markdown","1216ea19":"markdown","67e5a532":"markdown","1a522830":"markdown","6e514bca":"markdown","44a217d4":"markdown","8133e717":"markdown","e67d8083":"markdown","2aa394e7":"markdown","d09a582b":"markdown","bedab63e":"markdown","c55664d9":"markdown","5143009f":"markdown","0dcd95ec":"markdown","e9d82a41":"markdown","e91875ea":"markdown","984324c8":"markdown","585e73d2":"markdown","831bf44c":"markdown","b2c4f93a":"markdown","a2fe27a0":"markdown","0c8c2c48":"markdown","1bc95871":"markdown","61861bd3":"markdown","202f3c72":"markdown","b79141ab":"markdown","d464a37c":"markdown","1aefe2bc":"markdown","f327e3ed":"markdown","3e3ca7a1":"markdown","368fb1b3":"markdown","347672a0":"markdown","19404f9c":"markdown","1ced70ec":"markdown","365149a9":"markdown","cfe4f6d8":"markdown"},"source":{"ab97e3c6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","fb821fa6":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")","7a9f9459":"df.head()","9befa148":"df.shape","fb08ebce":"df.info()","9787fdc7":"df.describe().T","4fb9b25a":"df.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)","e9f1cd42":"df.isna().sum()","f76e399a":"df.drop(\"Cabin\", axis=1, inplace=True)\n\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(df[\"Embarked\"].mode().values[0], inplace=True)","fc603f81":"df.duplicated().sum()","0aef35f4":"df.drop_duplicates(inplace=True)","b8f81b1d":"sns.pairplot(df, hue=\"Survived\")","8c32f412":"num_cols_viz = [\"Age\", \"Fare\"]\ncat_cols_viz = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\nsns.set()\n\nfor num_col in num_cols_viz:\n    plt.figure(figsize=(12,8))\n    sns.distplot(df[num_col])\n    plt.title(f\"{num_col}\", size=15)\n    plt.show()\n\nfor cat_col in cat_cols_viz:\n    plt.figure(figsize=(10,8))\n    sns.countplot(df[cat_col])\n    plt.title(f\"{cat_col}\", size=15)\n    plt.show()","f38d3262":"for num_col in num_cols_viz:\n    plt.figure(figsize=(12,8))\n    sns.violinplot(x=df[\"Survived\"], y=df[num_col])\n    plt.title(f\"{num_col} vs Survived\", size=15)\n    plt.show()\n\nfor cat_col in cat_cols_viz:\n    plt.figure(figsize=(12,8))\n    sns.barplot(x=df[cat_col], y=df[\"Survived\"])\n    plt.title(f\"Survived vs {cat_col}\", size=15)\n    plt.show()","abde522f":"plt.figure(figsize=(12,8))\nsns.heatmap(df[[\"Survived\", \"Age\", \"Fare\"]].corr(), annot=True, cmap=\"Blues\")\nplt.title(\"Correlations Between Variables\", size=16)\nplt.show()","4fb55355":"X = df.drop(\"Survived\", axis=1)\ny = df[\"Survived\"]","11cb23bf":"X = pd.get_dummies(X, columns=[\"Embarked\", \"Sex\"])","e4e47800":"scaler = StandardScaler()\nX[[\"Age\", \"Fare\"]] = scaler.fit_transform(X[[\"Age\", \"Fare\"]])","096a1992":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","76f07794":"models = pd.DataFrame(columns=[\"Model\", \"Accuracy Score\"])","727e7d37":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\npredictions = log_reg.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"LogisticRegression\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","9cf028bf":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"RandomForestClassifier\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","40d51387":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npredictions = gbc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"GradientBoostingClassifier\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","f9ce078a":"svc = SVC()\nsvc.fit(X_train, y_train)\npredictions = svc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"SVC\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","93358c8a":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"KNeighborsClassifier\", \"Accuracy Score\": score}\nmodels = models.append(new_row, ignore_index=True)","6bbcff11":"models.sort_values(by=\"Accuracy Score\", ascending=False)","cb16c937":"plt.figure(figsize=(12,8))\nsns.barplot(x=models[\"Model\"], y=models[\"Accuracy Score\"])\nplt.title(\"Models' Accuracy Scores\", size=15)\nplt.xticks(rotation=30)\nplt.show()","e6223845":"def visualize_roc_auc_curve(model, model_name):\n    pred_prob = model.predict_proba(X_test)\n    fpr, tpr, thresh = roc_curve(y_test, pred_prob[:,1], pos_label=1)\n\n    random_probs = [0 for i in range(len(y_test))]\n    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)  # tpr = fpr\n\n    score = roc_auc_score(y_test, pred_prob[:,1])\n    print(\"ROC AUC Score:\", score)\n\n    plt.figure(figsize=(10,8))\n    plt.plot(fpr, tpr, linestyle='--',color='orange')\n    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\n    plt.title(f'{model_name} ROC curve', size=15)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.show()","d7819000":"def f_importances(model, model_name):\n    f_imp = pd.DataFrame({\"Feature Importances\": model.feature_importances_}, index=X.columns)\n\n    plt.figure(figsize=(12,8))\n    sns.barplot(x=f_imp[\"Feature Importances\"], y=f_imp.index)\n    plt.title(f\"{model_name} Feature Importances\", size=15)\n    plt.show()","2d9851a8":"tuned_models = pd.DataFrame(columns=[\"Model\", \"Accuracy Score\"])","78568538":"param_grid_log_reg = {\"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), param_grid_log_reg, cv=5, scoring=\"accuracy\", verbose=0, n_jobs=-1)\n\ngrid_log_reg.fit(X_train, y_train)","447152ce":"log_reg_params = grid_log_reg.best_params_\n\nlog_reg = LogisticRegression(**log_reg_params)\nlog_reg.fit(X_train, y_train)\npredictions = log_reg.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"LogisticRegression\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","88925297":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Logistic Regression\", size=15)\nplt.show()","750d95eb":"visualize_roc_auc_curve(log_reg, \"Logistic Regression\")","1e170e20":"param_grid_rfc = {\"max_depth\": [None],\n                  \"max_features\": [1, 3, 10],\n                  \"min_samples_split\": [2, 3, 10],\n                  \"min_samples_leaf\": [1, 3, 10],\n                  \"n_estimators\" :[100, 200, 500]}\n\ngrid_rfc = GridSearchCV(RandomForestClassifier(), param_grid_rfc, cv=5, scoring=\"accuracy\", verbose=0, n_jobs=-1)\n\ngrid_rfc.fit(X_train, y_train)","2e57653f":"rfc_params = grid_rfc.best_params_\n\nrfc = RandomForestClassifier(**rfc_params)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"RandomForestClassifier\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","53bc4a75":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Random Forest\", size=15)\nplt.show()","5a0d21d5":"visualize_roc_auc_curve(rfc, \"Random Forest\")","d8e46f74":"f_importances(rfc, \"Random Forest\")","ba9f2ffa":"param_grid_gbc = {'n_estimators' : [100, 200, 500],\n                  'learning_rate': [0.1, 0.05, 0.01],\n                  'max_depth': [2, 3, 6],\n                  'min_samples_leaf': [1, 2, 5]}\n\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid_gbc, cv=5, scoring=\"accuracy\", verbose=0, n_jobs=-1)\n\ngrid_gbc.fit(X_train, y_train)","c7115a60":"gbc_params = grid_gbc.best_params_\n\ngbc = GradientBoostingClassifier(**gbc_params)\ngbc.fit(X_train, y_train)\npredictions = gbc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"GradientBoostingClassifier\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","e0151f7c":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Gradient Boosting\", size=15)\nplt.show()","b56fc710":"visualize_roc_auc_curve(gbc, \"Gradient Boosting\")","aceb102b":"f_importances(gbc, \"Gradient Boosting\")","b6ff05d8":"param_grid_svc = {'gamma': [ 0.001, 0.01, 0.1, 1, 10],\n                  'C': [1, 10, 50, 100, 200, 300, 500, 1000]}\n\ngrid_svc = GridSearchCV(SVC(), param_grid_svc, cv=5, scoring=\"accuracy\", verbose=0, n_jobs=-1)\n\ngrid_svc.fit(X_train, y_train)","dc7817be":"svc_params = grid_svc.best_params_\n\nsvc = SVC(**svc_params)\nsvc.fit(X_train, y_train)\npredictions = svc.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"SVC\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","c40ef5f8":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of Support Vector Machines\", size=15)\nplt.show()","d63a7dea":"param_grid_knn = {\"n_neighbors\": range(1,20),\n                  \"leaf_size\": range(1,50, 5),\n                  \"p\": [1, 2]}\n\ngrid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring=\"accuracy\", verbose=0, n_jobs=-1)\n\ngrid_knn.fit(X_train, y_train)","184e843e":"knn_params = grid_knn.best_params_\n\nknn = KNeighborsClassifier(**knn_params)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(\"Accuracy Score:\", score)\n\nnew_row = {\"Model\": \"KNeighborsClassifier\", \"Accuracy Score\": score}\ntuned_models = tuned_models.append(new_row, ignore_index=True)","f00cb86e":"plt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True, cmap=\"Blues\", fmt='d')\nplt.title(\"Confusion Matrix of K-Nearest Neighbors\", size=15)\nplt.show()","c5506c4c":"visualize_roc_auc_curve(knn, \"K-Nearest Neighbors\")","273d1d9f":"tuned_models.sort_values(by=\"Accuracy Score\", ascending=False)","a15c5c07":"plt.figure(figsize=(12, 8))\nsns.barplot(x=tuned_models[\"Model\"], y=tuned_models[\"Accuracy Score\"])\nplt.title(\"Models' Accuracy Scores After Hyperparameter Tuning\", size=15)\nplt.xticks(rotation=30)\nplt.show()","3f0762de":"<h3>X, y Split<\/h3>","9ae11c3c":"<h3>Relationship Between Each Variable and Target Variable (Survived)<\/h3>","3c732ccd":"# <center>Titanic Survival Prediction \ud83d\udea2<\/center>","14464e51":"# Detecting Missing Values and Duplicates","1b890a42":"<h1 style=\"font-family: Times New Roman;\">Thank you so much for reading notebook. Preparing notebooks is taking a great deal of time. If you liked it, please do not forget to give an upvote. Peace Out \u270c\ufe0f ...<\/h1>","ff7499a3":"***Checking the shape\u2014i.e. size\u2014of the data.***","c5131c6f":"<h3>Plotting the Values of Each Variable<\/h3>","1216ea19":"# Machine Learning Models","67e5a532":"# Importing the Essential Libraries, Metrics, Tools and Models","1a522830":"***Learning the dtypes of columns' and how many non-null values there are in those columns.***","6e514bca":"<h3>Tuning the Support Vector Machines<\/h3>","44a217d4":"<h3>One-Hot Encoding<\/h3>","8133e717":"<h3>Model Comparison Before Hyperparameter Tuning<\/h3>","e67d8083":"<h3>Data Standardization<\/h3>","2aa394e7":"<img src=\"https:\/\/images.unsplash.com\/photo-1558431571-4a9f128e135f?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1032&q=80\">","d09a582b":"***Defining a couple of visualization functions for the convenience of evaluation***","bedab63e":"# Exploratory Data Analysis","c55664d9":"<h3>Random Forest Classifier<\/h3>","5143009f":"# Loading the Data","0dcd95ec":"***Taking a look at the first 5 rows of the dataset.***","e9d82a41":"***Visualizing the Correlation between the numerical variables using pairplot visualization.***","e91875ea":"<h3>Tuning the K-Nearest Neighbors<\/h3>","984324c8":"<h3>After hyperparameter tuning, we can see that the model which is yielding the best accuracy score is Support Vector Machines with the accuracy score of 0.832618.<\/h3>","585e73d2":"# About the Dataset","831bf44c":"<h3>Logistic Regression<\/h3>","b2c4f93a":"<h3>Gradient Boosting Classifier<\/h3>","a2fe27a0":"***Visualizing the linear correlations between variables using Heatmap visualization. The measure used for finding the linear correlation between each variable is Pearson Correlation Coefficient.***","0c8c2c48":"<h3>Train-Test Split<\/h3>","1bc95871":"***If we take a look at the official technical documentation of SVC, we can observe that predict_proba() function may be inconsistent with predict() function. It sucks especially on the small datasets, that's why we don't plot ROC AUC Curve for SVC model.***","61861bd3":"# Model Comparison After Hyperparameter Tuning","202f3c72":"# Conclusion","b79141ab":"<h3>Tuning the Gradient Boosting Classifier<\/h3>","d464a37c":"<h3>K-Nearest Neighbors<\/h3>","1aefe2bc":"<h3>Support Vector Machines<\/h3>","f327e3ed":"<h3>Tuning the Random Forest<\/h3>","3e3ca7a1":"# Data Visualization","368fb1b3":"# Data Preprocessing","347672a0":"# Hyperparameter Tuning","19404f9c":"***It seems that we have some missing values in our data. We have to drop the \"Cabin\" column because there are a lot of missing values to fix. As for other columns, we are imputing the missing values in categorical columns with mode of that particular column and missing values in numerical columns with median value of that column.***","1ced70ec":"* **Survived** - Survival (0 = No, 1 = Yes) ---> *Output Variable*\n* **Pclass** - Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) ---> *Input Variable*\n* **Sex** - Sex of the passenger ---> *Input Variable*\n* **Age** - Age in years ---> Input Variable\n* **Sibsp** - number of siblings\/spouses aboard the Titanic ---> *Input Variable*\n* **Parch** - number of parents\/children aboard the Titanic ---> *Input Variable*\n* **Ticket** - Ticket number ---> *Input Variable*\n* **Fare** - Passenger fare ---> *Input Variable*\n* **Cabin** - Cabin number ---> *Input Variable*\n* **Embarked** - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) ---> *Input Variable*","365149a9":"***Getting the statistical summary of dataset.***","cfe4f6d8":"<h3>Tuning the Logistic Regression<\/h3>"}}