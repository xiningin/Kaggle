{"cell_type":{"78bc199a":"code","39877699":"code","181acaec":"code","c0eb6cbb":"code","8015b282":"code","205ac024":"code","7a711b11":"code","8a05cb03":"code","e1823283":"code","d6f2bd25":"code","82178e80":"code","ab563b40":"code","a8e39a3a":"code","8d7369db":"code","185d99f5":"code","be0a7c98":"code","8304f41c":"code","89e19561":"code","c7d35011":"code","30ea3e55":"code","f53cfeb9":"code","ec499d8d":"code","9f85c273":"code","9712690e":"code","b010a54f":"code","0fa6037a":"code","4f20a25a":"code","0922d4c1":"code","e6a1fbf8":"code","45549bcf":"code","a1048e58":"code","8c4f5319":"code","d9f3a1af":"code","f707ed82":"code","23488e61":"code","e03e642a":"code","81cba67e":"code","33af5c72":"code","2bfdd894":"code","fad3d064":"code","91188fff":"code","06a27618":"code","dfa5dd09":"code","32166a37":"code","e043c6d9":"code","81b6d8f8":"code","0ffed63c":"code","f3c976be":"code","b06818d8":"code","0c79b8f3":"code","d76ba630":"code","e744ffd8":"code","377541ea":"code","e362575c":"code","6efcff07":"code","11a96d67":"code","b6dd6717":"code","80d092c5":"code","217af8f2":"code","ec703cb5":"markdown","7964cf0e":"markdown","0d3484bf":"markdown","1f730689":"markdown","4cc31634":"markdown","60f789d7":"markdown","ffbbd68e":"markdown","a6e2582c":"markdown","ee81708f":"markdown","b73f4b92":"markdown","f2780bfe":"markdown","f8107ec8":"markdown","7d049564":"markdown","aa720c8c":"markdown","3469f554":"markdown","5a890314":"markdown","c6002f40":"markdown","b4931450":"markdown","050ab329":"markdown","9be62bc4":"markdown","001de523":"markdown","272ca33e":"markdown","d41eeab7":"markdown","aff748a0":"markdown","9908431f":"markdown","e2c4e555":"markdown","33e11d74":"markdown","411a7a3d":"markdown","70fe29d2":"markdown","15fe6c5c":"markdown","1836cbd6":"markdown","2522c3fc":"markdown","999f8163":"markdown","76b4043b":"markdown","8a583924":"markdown","cab0e94d":"markdown","597c2b42":"markdown","2884bcde":"markdown","d3893b60":"markdown","558f2742":"markdown","fa4e5cf8":"markdown","7a366135":"markdown","59924137":"markdown","12ffb2e0":"markdown","0dee89b4":"markdown","b39155cc":"markdown","129a1ed4":"markdown","2c71b15c":"markdown","3ea409cc":"markdown","19d813f9":"markdown"},"source":{"78bc199a":"import warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import Image\n\n#Libraries used\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport plotly_express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom scipy.stats import norm\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","39877699":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","181acaec":"df.columns","c0eb6cbb":"df.info()","8015b282":"df.describe()","205ac024":"group = df.groupby('Class')['Time'].count()\nfig = go.Figure()\nfig.add_trace(go.Bar(name='counts',x=group.index, y=group.values, marker_line_color='black', marker_line_width=2,\n                     text=group.values,textposition='outside'))\nfig.update_yaxes(title_text='Number of Cases')\nfig.update_xaxes(title_text='Case',nticks=3)\nfig.update_layout(template='seaborn',hovermode='closest',title='Number of Fraud vs Non-Fraud Cases',\n                 width=700,height=400,xaxis=dict(mirror=True,linecolor='black',linewidth=2),\n                 yaxis=dict(mirror=True,linecolor='black',linewidth=2),margin=dict(t=50,b=0,l=70,r=0))\nfig.show()","7a711b11":"print('Percentage of No Frauds: {}%'.format(round(df.Class.value_counts()[0]\/len(df) * 100.0,2)))\nprint('Percentage of Frauds: {}%'.format(round(df.Class.value_counts()[1]\/len(df) * 100.0,2)))","8a05cb03":"fig = make_subplots(rows=1,cols=2,subplot_titles=['Distribution of Time', 'Distribution of Amount'])\nfig.add_trace(go.Histogram(name='Time',histnorm='probability',x=df.Time),1,1)\nfig.add_trace(go.Histogram(name='Amount',x=df.Amount),1,2)\nfig.update_xaxes(mirror=True,linecolor='black',linewidth=2,row=1,col=1)\nfig.update_xaxes(mirror=True,linecolor='black',linewidth=2,row=1,col=2)\nfig.update_yaxes(mirror=True,linecolor='black',linewidth=2,row=1,col=1)\nfig.update_yaxes(mirror=True,linecolor='black',linewidth=2,row=1,col=2)\nfig.update_layout(template='seaborn',width=700,height=300,margin=dict(t=50,b=0,l=0,r=0))\nfig.show()","e1823283":"#Feature Scaling\nrob_scaler = RobustScaler()\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf.drop(['Time','Amount'], axis=1, inplace=True)","d6f2bd25":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\n#scaled_amount and scaled_time are added to the starting of the dataframe\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\ndf.head()","82178e80":"#Random Under Sampling\nX = df.drop('Class', axis=1)\ny = df['Class']\nrus = RandomUnderSampler(random_state=42)\nX_rs, y_rs = rus.fit_sample(X,y)","ab563b40":"#Random Under-sampled Dataframe\ndf_rs = pd.DataFrame(np.hstack((X_rs,y_rs[:, None])), columns=df.columns)\ndf_rs.Class = df_rs.Class.astype(int)\ndf_rs.head()","a8e39a3a":"#Frauds vs. Non-Frauds for the new dataframe\ngroup = df_rs.groupby('Class')['scaled_time'].count()\nfig = go.Figure()\nfig.add_trace(go.Bar(name='counts',x=group.index, y=group.values, marker_line_color='black', marker_line_width=2,\n                     text=group.values,textposition='outside'))\nfig.update_yaxes(title_text='Number of Cases')\nfig.update_xaxes(title_text='Case',nticks=3)\nfig.update_layout(template='seaborn',hovermode='closest',title='Number of Fraud vs Non-Fraud Cases',\n                 width=700,height=400,xaxis=dict(mirror=True,linecolor='black',linewidth=2),\n                 yaxis=dict(mirror=True,linecolor='black',linewidth=2),margin=dict(t=50,b=0,l=70,r=0))\nfig.show()","8d7369db":"print('Percentage of No Frauds: {}%'.format(round(df_rs.Class.value_counts()[0]\/len(df_rs) * 100.0,2)))\nprint('Percentage of Frauds: {}%'.format(round(df_rs.Class.value_counts()[1]\/len(df_rs) * 100.0,2)))","185d99f5":"fig = make_subplots(rows=2, cols=1, \n                    subplot_titles=['Correlation Matrix for the original dataframe','Correlation Matrix for the undersampled dataframe'])\n\nfig.add_trace(go.Heatmap(name='original df',z=df.corr().values,x=df.corr().index,y=df.corr().index,\n                         coloraxis='coloraxis'),1,1)\nfig.add_trace(go.Heatmap(name='undersampled df',z=df_rs.corr().values,x=df_rs.corr().index,y=df_rs.corr().index,\n                         coloraxis='coloraxis'),2,1)\nfig.update_layout(height=800,width=700,coloraxis = {'colorscale':'YlOrRd'})\nfig.show() ","be0a7c98":"rows = 2\ncols = 4\nfig = make_subplots(rows=rows,cols=cols,vertical_spacing=0.15,\n                    subplot_titles=['V2 Distribution', 'V4 Distribution', 'V11 Distribution', 'V19 Distribution',\n                                                 'V10 Distribution', 'V12 Distribution', 'V14 Distribution', 'V17 Distribution'])\nfeatures = ['V2','V4','V11','V19','V10','V12','V14','V17']\nfor r in range(1,rows+1):\n    for c in range(1, cols+1):\n        fig.add_trace(go.Histogram(name=features[r+c-2],x=df.loc[df.Class==1,features[r+c-2]]),r,c)\n        fig.update_xaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\n        fig.update_yaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\nfig.update_layout(template='seaborn',title='Feature Distributions for Fraud Transactions')\nfig.show()","8304f41c":"#Box-Plots for Positive Correlation\nrows=1\ncols=4\nfeatures = ['V2','V4','V11','V19']\nfig = make_subplots(rows=rows,cols=cols,subplot_titles=['V2 vs Class','V4 vs Class','V11 vs Class','V19 vs Class'])\nfor r in range(1,rows+1):\n    for c in range(1,cols+1):\n        fig.add_trace(go.Box(name=features[r+c-2],x=df_rs['Class'],y=df_rs[features[r+c-2]]),r,c)\n        fig.update_xaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\n        fig.update_yaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\nfig.update_layout(width=700,template='seaborn',title='Boxplots for Positive Correlations')","89e19561":"#Box-Plots for Negative Correlation\nrows=1\ncols=4\nfeatures = ['V10','V12','V14','V17']\nfig = make_subplots(rows=rows,cols=cols,subplot_titles=['V10 vs Class','V12 vs Class','V14 vs Class','V17 vs Class'])\nfor r in range(1,rows+1):\n    for c in range(1,cols+1):\n        fig.add_trace(go.Box(name=features[r+c-2],x=df_rs['Class'],y=df_rs[features[r+c-2]]),r,c)\n        fig.update_xaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\n        fig.update_yaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=c)\nfig.update_layout(width=700,template='seaborn',title='Boxplots for Negative Correlations')","c7d35011":"#Outlier Removal\ndef outlier_removal(df,feature, fraud):\n    array = df[feature].loc[df['Class'] == fraud].values \n    q25, q75 = np.percentile(array, 25), np.percentile(array,75)\n    print('25th percentile: {} | 75th percentile: {}'.format(q25,q75))\n    iqr = q75 - q25\n    print('Interquartile Range: {}'.format(iqr))\n    cutoff = iqr*1.5\n    lower_threshold, upper_threshold = q25 - cutoff, q75 + cutoff\n    print('Cutoff: {}'.format(cutoff))\n    print('Lower Threshold: {} | Upper Threshold: {}'.format(lower_threshold, upper_threshold))\n    outliers = [a for a in array if a < lower_threshold or a > upper_threshold]\n    print('{} Outliers: {}'.format(feature,outliers))\n    print('Number of outliers detected for feature {}: {}'.format(feature,len(outliers)))\n    df = df.drop(df[(df[feature] > upper_threshold) | (df[feature] < lower_threshold)].index)\n    print('Number of records after outlier removal: {}'.format(len(df)))\n    print('-'*117)\n    return df","30ea3e55":"#Removal of outliers for features V10 and V2. Since, these two features seem to have the most number of outliers.\ndf_rs_out = outlier_removal(df_rs,'V10', 1)\ndf_rs_out = outlier_removal(df_rs_out,'V2',1)","f53cfeb9":"rows=2\ncols=2\nfeatures=['V2','V10']\nfig = make_subplots(rows=rows,cols=cols,\n                    shared_yaxes=True,subplot_titles=[features[0]+' vs Class <br> (Before Outlier Removal)',\n                                                        features[0]+' vs Class <br> (After Outlier Removal)',\n                                                        features[1]+' vs Class <br> (Before Outlier Removal)',\n                                                        features[1]+' vs Class <br> (After Outlier Removal)'])\nfor r in range(1,rows+1):\n    fig.add_trace(go.Box(name=features[r-1]+'(Before)',x=df_rs['Class'],y=df_rs[features[r-1]]),r,1)\n    fig.add_trace(go.Box(name=features[r-1]+'(After)',x=df_rs_out['Class'],y=df_rs_out[features[r-1]]),r,2)\n    fig.update_xaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=1)\n    fig.update_yaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=1)\n    fig.update_xaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=2)\n    fig.update_yaxes(mirror=True,linewidth=2,linecolor='black',row=r,col=2)\nfig.update_layout(width=700,template='seaborn',title='Comparison of Box Plots after outlier removal - V10 & V2')\nfig.show()","ec499d8d":"def naive_predictor(df):\n    TP = df.Class.count() - np.sum(df.Class)\n    FP = np.sum(df.Class)\n    TN = 0\n    FN = 0\n    \n    # TODO: Calculate accuracy, precision and recall\n    accuracy = (TP+TN)\/(TP+TN+FP+FN)\n    recall = TP\/(TP+FN)\n    precision = TP\/(TP+FP)\n    \n    # TODO: Calculate F-score using the formula above for beta = 1 and correct values for precision and recall.\n    fscore = (2*precision*recall)\/(precision + recall)\n    \n    # Print the results \n    print(\"[Accuracy score: {:.4f}, precision: {:.4f}, recall: {:.4f}, f1-score: {:.4f}]\".format(accuracy, precision, recall, fscore))\n    \nprint('Naive predictor for original dataset:')\nnaive_predictor(df)\nprint('-'*117)\nprint('Naive predictor for undersampled dataset:')\nnaive_predictor(df_rs)","9f85c273":"# Undersampling before cross validation(prove to overfit)\nX = df_rs.drop('Class', axis=1)\ny = df_rs['Class']","9712690e":"#Split the data using train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b010a54f":"#Convert to arrays to feed to the classifications algorithms\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Support Vector Classifier\": SVC(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"KNearest\": KNeighborsClassifier()\n}","0fa6037a":"#Calculate the cross-validation score for each classifier\nprint('Cross-Validation Scores:-')\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    cv_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print('{}: {}'.format(key,round(cv_score.mean()*100.0, 2)))","4f20a25a":"params = {\n    \"Logistic Regression\": {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n    \"Support Vector Classifier\": {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']},\n    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))},\n    \"KNearest\": {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n}","0922d4c1":"#Using Grid search for finding the most optimal hyperparameters\ndef gridsearch(classifier, params):\n    grid_classifier = GridSearchCV(classifier, params)\n    grid_classifier.fit(X_train, y_train)\n    best_classifier = grid_classifier.best_estimator_\n    return best_classifier","e6a1fbf8":"#Scores after applying Grid Search\nprint('Cross-Validation Scores after applying GridSearch:-')\nfor key, classifier in classifiers.items():\n    classifier = gridsearch(classifier,params[key])\n    cv_score = cross_val_score(classifier, X_train, y_train, cv=3)\n    print('{}: {}'.format(key,round(cv_score.mean()*100.0, 2)))","45549bcf":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Plot the learning curve for the estinmator.\"\"\"\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(name='Training score - Standard Deviation',\n                            x=train_sizes,\n                            y=train_scores_mean+train_scores_std,\n                            mode='lines',\n                            showlegend=False,\n                            marker=dict(color='green')))\n    fig.add_trace(go.Scatter(name='Training score',\n                            x=train_sizes,\n                            y=train_scores_mean,\n                            fill='tonexty',\n                            mode='lines+markers',\n                            marker=dict(color='green')))\n    fig.add_trace(go.Scatter(name='Training score + Standard Deviation',\n                            x=train_sizes,\n                            y=train_scores_mean-train_scores_std,\n                            mode='lines',\n                            fill='tonexty',\n                            showlegend=False,\n                            marker=dict(color='green')))\n    fig.add_trace(go.Scatter(x=train_sizes,\n                            y=test_scores_mean+test_scores_std,\n                            mode='lines',\n                            showlegend=False,\n                            marker=dict(color='red')))\n    fig.add_trace(go.Scatter(name='Validation Score',\n                            x=train_sizes,\n                            y=test_scores_mean,\n                            mode='lines+markers',\n                            fill='tonexty',\n                            marker=dict(color='red')))\n    fig.add_trace(go.Scatter(x=train_sizes,\n                            y=test_scores_mean-test_scores_std,\n                            mode='lines',\n                            fill='tonexty',\n                            showlegend=False,\n                            marker=dict(color='red')))\n\n    fig.update_layout(width=700,height=400,template='seaborn',title=title,\n                        margin=dict(l=60,r=0,b=0,t=40),legend=dict(orientation='h',x=0.5,y=1),\n                        xaxis=dict(title='Training examples',mirror=True,linecolor='black',linewidth=2),\n                        yaxis=dict(title='Scores',range=ylim if ylim is not None else None,\n                        mirror=True,linecolor='black',linewidth=2))\n    return fig","a1048e58":"# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n#                         n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n#     #plt.figure()\n#     plt.title(title)\n#     if ylim is not None:\n#         ax.ylim(*ylim)\n#     plt.xlabel(\"Training examples\")\n#     plt.ylabel(\"Score\")\n#     train_sizes, train_scores, test_scores = learning_curve(\n#         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n#     train_scores_mean = np.mean(train_scores, axis=1)\n#     train_scores_std = np.std(train_scores, axis=1)\n#     test_scores_mean = np.mean(test_scores, axis=1)\n#     test_scores_std = np.std(test_scores, axis=1)\n \n#     plt.grid()\n\n#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n#                      train_scores_mean + train_scores_std, alpha=0.1,\n#                      color=\"r\")\n#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n#                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n#              label=\"Training score\")\n#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n#              label=\"Cross-validation score\")\n\n#     plt.legend(loc=\"best\")\n#     return plt","8c4f5319":"#Variables\nn_jobs = 10\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n\n#Classifiers\nlog_reg = gridsearch(LogisticRegression(), params['Logistic Regression'])\nsvc = gridsearch(SVC(), params['Support Vector Classifier'])\ndecision_tree = gridsearch(DecisionTreeClassifier(), params['Decision Tree'])\nknearest = gridsearch(KNeighborsClassifier(), params['KNearest'])\n\nplot_learning_curve(log_reg,'Logistic Regression', X_train, y_train, n_jobs=10)","d9f3a1af":"plot_learning_curve(svc, 'Support Vector Classifier', X_train, y_train, n_jobs=10)","f707ed82":"plot_learning_curve(decision_tree, 'Decision Tree Classifier', X_train, y_train, n_jobs=10)","23488e61":"plot_learning_curve(knearest, 'KNearest Classifier', X_train, y_train, n_jobs=10)","e03e642a":"# #Variables\n# n_jobs = 10\n# cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n\n# #Classifiers\n# log_reg = gridsearch(LogisticRegression(), params['Logistic Regression'])\n# svc = gridsearch(SVC(), params['Support Vector Classifier'])\n# decision_tree = gridsearch(DecisionTreeClassifier(), params['Decision Tree'])\n# knearest = gridsearch(KNeighborsClassifier(), params['KNearest'])\n\n# plt.figure(figsize=(15,15))\n# plt.subplot(2,2,1)\n# plot_learning_curve(log_reg, 'Logistic Regression', X_train, y_train, n_jobs=10)\n# plt.subplot(2,2,2)\n# plot_learning_curve(svc, 'Support Vector Classifier', X_train, y_train, n_jobs=10)\n# plt.subplot(2,2,3)\n# plot_learning_curve(decision_tree, 'Decision Tree Classifier', X_train, y_train, n_jobs=10)\n# plt.subplot(2,2,4)\n# plot_learning_curve(knearest, 'KNearest Classifier', X_train, y_train, n_jobs=10)\n# plt.show()","81cba67e":"#ROC Curve\n'''log_pred = cross_val_predict(log_reg, X_train, y_train, cv=5)\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5)\ntree_pred = cross_val_predict(decision_tree, X_train, y_train, cv=5)\nknear_pred = cross_val_predict(knearest, X_train, y_train, cv=5)'''\n\nlog_pred = log_reg.predict(X_test)\nsvc_pred = svc.predict(X_test)\ntree_pred = decision_tree.predict(X_test)\nknear_pred = knearest.predict(X_test)\n\nlog_fpr, log_tpr, log_threshold = roc_curve(y_test, log_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_test, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_test, tree_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_test, knear_pred)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(name='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_test, log_pred)),\n                         x=log_fpr,y=log_tpr,mode='lines'))\nfig.add_trace(go.Scatter(name='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_test, svc_pred)),\n                         x=svc_fpr,y=svc_tpr,mode='lines'))\nfig.add_trace(go.Scatter(name='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_test, tree_pred)),\n                         x=tree_fpr,y=tree_tpr,mode='lines'))\nfig.add_trace(go.Scatter(name='K-Nearest Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_test, knear_pred)),\n                         x=knear_fpr,y=knear_tpr,mode='lines'))\nfig.add_trace(go.Scatter(name='AUC-ROC=0.5',x=[0,1],y=[0,1],line=dict(dash='dot'),showlegend=False))\nfig.update_layout(\n    width=700,xaxis=dict(mirror=True,linewidth=2,linecolor='black'),\n    yaxis=dict(mirror=True,linewidth=2,linecolor='black'),\n    title='ROC Curve<br>(All Classifiers)',\n    template='seaborn',\n    legend=dict(\n        x=0.46,\n        y=0,\n        traceorder=\"normal\",\n        font=dict(\n            family=\"sans-serif\",\n            size=12,\n            color=\"black\"\n        ),\n        bgcolor=\"Lightgray\",\n        bordercolor=\"Black\",\n        borderwidth=2\n    ),\n     annotations=[\n        dict(\n            x=0.5,\n            y=0.5,\n            xref=\"x\",\n            yref=\"y\",\n            text=\"Minimum ROC Score of 50% <br> (This is the minimum score to get)\",\n            showarrow=True,\n            arrowhead=7,\n            ax=40,\n            ay=50\n        )\n    ]\n)\nfig.show()","33af5c72":"labels = ['No Fraud', 'Fraud']\nprint('Logistic Regression:')\nprint(classification_report(y_test, log_pred, target_names=labels))\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, svc_pred, target_names=labels))\nprint('Decision Tree:')\nprint(classification_report(y_test, tree_pred, target_names=labels))\nprint('KNearest Neighbours:')\nprint(classification_report(y_test, knear_pred, target_names=labels))","2bfdd894":"original_X = df.drop('Class',axis=1)\noriginal_y = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(original_X, original_y):\n    print(\"Train: {} Test: {}\".format(train_index, test_index))\n    original_Xtrain, original_Xtest = original_X.iloc[train_index], original_X.iloc[test_index]\n    original_ytrain, original_ytest = original_y.iloc[train_index], original_y.iloc[test_index]\n    \n#Turn into arrays    \noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n#Check if the labels are equally distibuted among the train and test set.\ntrain_unique_label, train_count_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_count_label = np.unique(original_ytest, return_counts=True)\n\nprint('\\n Label Distribution:')\nprint('Train: {}'.format(train_count_label\/len(original_ytrain)))\nprint('Test: {}'.format(test_count_label\/len(original_ytest)))","fad3d064":"#Lists\nundersample_accuracy_lst = []\nundersample_precision_lst = []\nundersample_recall_lst = []\nundersample_f1_lst = []\nundersample_auc_lst = []\n\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(original_X.values, original_y.values)\nprint('Near Miss Distribution: {}'.format(np.unique(y_nearmiss, return_counts=True)))\n\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(NearMiss(ratio='majority'), log_reg)\n    undersample_model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    prediction = undersample_model.predict(original_Xtrain[test])\n    \n    undersample_accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision_lst.append(precision_score(original_ytrain[test], prediction))\n    undersample_recall_lst.append(recall_score(original_ytrain[test], prediction))\n    undersample_f1_lst.append(f1_score(original_ytrain[test], prediction))\n    undersample_auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('accuracy: {}'.format(np.mean(undersample_accuracy_lst)))\nprint('precision: {}'.format(np.mean(undersample_precision_lst)))\nprint('recall: {}'.format(np.mean(undersample_recall_lst)))\nprint('f1_score: {}'.format(np.mean(undersample_f1_lst)))","91188fff":"labels = ['No Fraud', 'Fraud']\nundersample_prediction = undersample_model.predict(original_Xtest)\nprint(classification_report(original_ytest, undersample_prediction, target_names=labels))","06a27618":"rand_log_reg = RandomizedSearchCV(LogisticRegression(), params['Logistic Regression'], n_iter=4)\n\n#Lists\noversample_accuracy_lst = []\noversample_precision_lst = []\noversample_recall_lst = []\noversample_f1_lst = []\noversample_auc_lst = []\n\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(ratio='minority'), rand_log_reg)\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    oversample_accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    oversample_precision_lst.append(precision_score(original_ytrain[test], prediction))\n    oversample_recall_lst.append(recall_score(original_ytrain[test], prediction))\n    oversample_f1_lst.append(f1_score(original_ytrain[test], prediction))\n    oversample_auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n\nprint('accuracy: {}'.format(np.mean(oversample_accuracy_lst)))\nprint('precision: {}'.format(np.mean(oversample_precision_lst)))\nprint('recall: {}'.format(np.mean(oversample_recall_lst)))\nprint('f1_score: {}'.format(np.mean(oversample_f1_lst)))","dfa5dd09":"smote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","32166a37":"undersample_prediction = undersample_model.predict(X_nearmiss)\nundersample_score = accuracy_score(y_nearmiss, undersample_prediction)\nsmote_prediction = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, smote_prediction)\n\ndata = {'Technique': ['Undersampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=data)\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\nfinal_df","e043c6d9":"n_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs,), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","81b6d8f8":"undersample_model.summary()","0ffed63c":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","f3c976be":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","b06818d8":"undersample_predictions = undersample_model.predict(X_test, batch_size=200, verbose=0)","0c79b8f3":"undersample_fraud_predictions = undersample_model.predict_classes(X_test, batch_size=200, verbose=0)","d76ba630":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","e744ffd8":"from sklearn.metrics import confusion_matrix\nundersample_cm = confusion_matrix(y_test, undersample_fraud_predictions)\nactual_cm = confusion_matrix(y_test, y_test)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","377541ea":"n_inputs = original_Xtrain.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","e362575c":"oversample_model.summary()","6efcff07":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","11a96d67":"oversample_model.fit(original_Xtrain, original_ytrain, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","b6dd6717":"oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)","80d092c5":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","217af8f2":"oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","ec703cb5":"# <a id='neuralnet'>Neural Network testing Random undersampled data vs Oversampled data(SMOTe)<\/a>\nIn this section I'll be using a neural network with single hidden layer to test the performance in case of random undersampled and SMOTe oversampled data. I'll be using Keras library to create my neural network.\n- **Neural Network Structure**: As stated previously, this will be a simple model composed of one input layer (where the number of nodes equals the number of features) plus bias node, one hidden layer with 32 nodes and one output node composed of two possible results 0 or 1 (No fraud or fraud).\n- **Other characteristics**: The learning rate will be 0.001, the optimizer we will use is the AdamOptimizer, the activation function that is used in this scenario is \"Relu\" and for the final outputs we will use sparse categorical cross entropy, which gives the probability whether an instance case is no fraud or fraud (The prediction will pick the highest probability between the two.)\n\nThe metric I'll be using is a **confusion matrix**.\n\n## <a id='conf'>Confusion Matrix<\/a>\nA confusion matrix is a summary of prediction results on a classification problem.\nThe number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\nThe confusion matrix shows the ways in which your classification model is confused when it makes predictions.\nIt gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n<img src='https:\/\/static.packt-cdn.com\/products\/9781838555078\/graphics\/C13314_06_05.jpg' height=200 width=400\/>\nHere,\n- Class 1 : Positive\n- Class 2 : Negative","7964cf0e":"I'll split the data using a train-test split of 0.8:0.2 meaning training set has 80% of the records and the rest 20% in the testing set.","0d3484bf":">Here TP, FP, TN & FN stand for True Positive, False Positive, True Negative & False Negative.","1f730689":"## <a id='crossval'>Cross-Validation<\/a>\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word \u201cexperiment\u201d is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.\n\n<img src='https:\/\/scikit-learn.org\/stable\/_images\/grid_search_workflow.png' height=300 width=500\/>","4cc31634":"## <a id='box'>Boxplots<\/a>\nA boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n\nYou need to have information on the variability or dispersion of the data. A boxplot is a graph that gives you a good indication of how the values in the data are spread out. Although box lots may seem primitive in comparison to a histogram or density plot, they have the advantage of taking up less space, which is useful when comparing distributions between many groups or datasets.\n\nBelow I have plotted boxplots for all the features which show positive and negative correlations with Class. The boxplot representation for each feature is shown separately for each of the class(0 & 1).","60f789d7":"## <a id='test_over'>Testing on Oversampled data(SMOTe Technique)<\/a>","ffbbd68e":"Now, I will generate the Classification report for all the 4 Classifers so that we can judge which one is the best of the lot in terms of the metrics: *precision, recall, f1-score and support*. The Classification report will show all these metrics for both the classes(Fraud and Non-Fraud)","a6e2582c":"#### *Specificity*\n<img src='https:\/\/miro.medium.com\/max\/492\/1*f7NmMcQtfes1ng7jtjNtHQ.png'\/>","ee81708f":"On using info() function we can check if any of the columns contain null values. Looking at the output it can be seen that all the 31 columns have non-null values.","b73f4b92":"I will try to remove the outliers for features *V10* and *V2* as through the above boxplots we can see that these have the maximum number of outliers. The code will show the outliers detected and their total number and then finally remove all the outliers.","f2780bfe":"## <a id='hist'>Histrograms<\/a>\nBelow I have plotted the histograms of the features listed above. The first row of histograms shows the distribution of positively correlated features with Class: *V2, V4, V11 & V19*, while the second row shows the distribution of negatively correlated features with Class: *V10, V12, V14 & V17*. \n> All the feature distributions are for fraudulant transactions i.e Class = 1.","f8107ec8":"## <a id='test_under'>Testing on Random undersampled data<\/a>","7d049564":"## <a id='learn' > Learning Curve<\/a>\nA learning curve refers to a plot of the prediction accuracy\/error vs. the training set size (i.e: how better does the model get at predicting the target as you the increase number of instances used to train it). Usually both the training and test\/validation performance are plotted together so we can diagnose the **bias-variance tradeoff** (i.e determine if we benefit from adding more training data, and assess the model complexity by controlling regularization or number of features).\n<img src='https:\/\/histalk2.com\/wp-content\/uploads\/2018\/12\/image-30.png' height=400 width=200\/>","aa720c8c":"## <a id='eda'>Exploratory Data Analysis<\/a>\nNow I will perform EDA on the dataset to study the various features of it. Since, it is an imbalanced dataset, I would first undersample the majority class(i.e. Non-Fraud in this case) so that I can get an equal distribution of Fraud and Non-Fraud cases. For undersampling I have used Random under sampling before crossvalidation. Although it is prone to overfit , but for EDA it can be performed. ","3469f554":"The following are the names of the columns of the dataset. In total there are 31 columns.","5a890314":"## <a id='over'>Oversampling (SMOTe Technique)<\/a>\nSynthetic Minority Oversampling Technique(SMOTe) has the following description:-\n- SMOTe is a technique based on nearest neighbors judged by Euclidean Distance between data points in feature space.\n- There is a percentage of Over-Sampling which indicates the number of synthetic samples to be created and this percentage parameter of Over-sampling is always a multiple of 100. If the percentage of Over-sampling is 100, then for each instance, a new sample will be created. Hence, the number of minority class instances will get doubled. Similarly, if the percentage of Over-sampling is 200, then the total number of minority class samples will get tripled.\n\nIn SMOTe,\n1. For each minority instance, k number of nearest neighbors are found such that they also belong to the same class where,<br>\n**k = (SMOTEe %)\/100**\n2. The difference between the feature vector of the considered instance and the feature vectors of the k nearest neighbors are found. So, k number of difference vectors are obtained.\n3. The k difference vectors are each multiplied by a random number between 0 and 1 (excluding 0 and 1).\n4. Now, the difference vectors, after being multiplied by random numbers, are added to the feature vector of the considered instance (original minority instance) at each iteration.","c6002f40":"### Heatmap 1:\nThis heatmap is built on top of the original dataset. The original dataset is an imbalanced dataset with 99.83:0.17 ratio of Non-fraudulant is to Fraudulant transactions respectively. Hence, it is not a good indicator of what features influence a transaction to be fraudulant. The same can be seen through the heapmap as well as majority of the features show no correlation at all. ","b4931450":"Let's analyse the number of Fraud vs Non-Fraud cases. It can be clearly seen from the image below that the number of non-fraudulent transactions clearly outnumber the number of fraudulent transactions.","050ab329":"## <a id='corr'>Correlation Matrices<\/a>\nIn order to get the essence of the data I'll be plotting correlation matrices. Since, there are 31 features it seems to be a better option to go for a heatmap kind of correlation matrix where different colours would indicate the correlation between any two features of the dataset. My main aim is to how the different features of the dataset help in determining a fraudulent or a non-fraudulent transaction.","9be62bc4":"My main aim is to find the features which influence a transaction to be fraudulant. In order to achieve this, I will focus on the last column of the plot where the correlation between various features and the Class is computed. The following is my analysis:-\n- **Positive Correlation:** Features *V2, V4, V11 & V19* show positive correlation with class. The higher the values of these features, the higher the chances of a transaction being fraudulant.\n- **Negative Correlation:** Features *V10, V12, V14 & V17* show negative correlation with class. The lower the values of these features, the higher the chances of a transaction being fraudulant.","001de523":"Below I have created a function *plot_leaning_curve()* which takes in different input parameters like the estimator, title, e.t.c and plots the learning curve for that estimator.","272ca33e":"The describe() function generates descriptive statistics that summarize the central tendency,dispersion and shape of a dataset's distribution, excluding ``NaN`` values.","d41eeab7":"## <a id='roc'>AUC-ROC Curve<\/a>\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n<img src='https:\/\/miro.medium.com\/max\/722\/1*pk05QGzoWhCgRiiFbz-oKQ.png'\/>","aff748a0":"> Since, the dataset contains no null or missing values and all the features seem to be scaled properly, there seems to be no need of any preprocessing. Only the features Time & Amount are required to be scaled.","9908431f":"# Table of Contents\n- [Credit Card Fraud Detection](#creditcard)\n- [Dataset and Preprocessing](#dataset)\n    - [Feature Scaling](#feature)\n    - [Exploratory Data Analysis](#eda)\n    - [Correlation Matrices](#corr)\n    - [Histograms](#hist)\n    - [Boxplots](#box)\n    - [Outlier Detection and Removal](#outlier)\n- [Naive Predictor Performance](#naive)\n- [Classifiers (Undersampling)](#classifier)\n    - [Cross-Validation](#crossval)\n    - [Learning Curve](#learn)\n    - [AUC-ROC Curve](#roc)\n- [Undersampling vs Oversampling](#sampling)\n    - [Undersampling](#under)\n    - [Oversampling(SMOTe Technique)](#over)\n- [Neural Network testing Random undersampled data vs Oversampled data(SMOTe)](#neuralnet)\n    - [Confusion Matrix](#conf)\n    - [Testing on Random undersampled data](#test_under)\n    - [Testing on Oversampled data(SMOTe Technique)](#test_over)\n- [Conclusion](#conc)\n- [References](#ref)","e2c4e555":"#### *TPR (True Positive Rate) \/ Recall \/Sensitivity*\n<img src='https:\/\/miro.medium.com\/max\/710\/1*HgxNKuUwXk9JHYBCt_KZNw.png'\/>","33e11d74":"I will be using the classifers with the hyperparameters that came out as a result of grid search as these are the most optimal models.","411a7a3d":"## <a id='under'>Undersampling<\/a>\nAbove for the analysis I have random undersampled the majority class of the dataset and did undersampling before cross-validation. That is not the ideal scenario as it is prone to overfit. Here I'll perform undersampling during cross-validation and use Nearmiss technique for undersampling.\n\n**Nearmiss Technique** : In order to attack the issue of potential information loss, \u201cnear neighbor\u201d method and its variations have been proposed. The basic algorithms of the near neighbor family are this: first, the method calculates the distances between all instances of the majority class and the instances of the minority class. Then k instances of the majority class that have the smallest distances to those in the minority class are selected. If there are n instances in the minority class, the \u201cnearest\u201d method will result in k*n instances of the majority class.\n\n\u201cNearMiss-1\u201d selects samples of the majority class that their average distances to three closest instances of the minority class are the smallest. \u201cNearMiss-2\u201d uses three farthest samples of the minority class. \u201cNearMiss-3\u201d selects a given number of the closest samples of the majority class for each sample of the minority class.","70fe29d2":"# <a id='classifier'> Classifiers (Undersampling)<\/a>\nIn this section I'll be using 4 different classifiers to classify the transactions as Fraudulant or Non-Fraudulant in the randomly undersampled dataset. My aim is to compare the performance of the Naive predictor(Benchmark model) with that of the classifiers that I choose. The following are the classifiers that I'll be using:-\n- **Logistic Regression**\n- **Support Vector Classifier**\n- **Decision Tree**\n- **KNearest Classifier**\n\n**Please Note** the randomly undersampled dataset used for the analysis is undersampled before cross validation and hence, is prove to overfit. To get the best model undersampling should be done along with cross validation.","15fe6c5c":"# <a id='conc'>Conclusion<\/a>\nIn this project, I tried to find a model which would help me predict whether a transaction is fraudulent or non-fraudulent. In order to achieve this, I downloaded a dataset from kaggle whose information is available [here](#dataset). I then performed preprocessing on the dataset and later on did exploratory data analysis on it to study all of its features and to check which all influence a transaction to be fraudulent or non-fraudulent . Initially, I defined a [Naive predictor](#naive) which would act as the benchmark model for all other models that I put to test. After that, I took four different classifiers and tested them on the randomly undersampled dataset and evaluated several metrics on it: accuracy, precision, recall, f1-score. I also plotted learning curves and AUC-ROC curve to get a better understanding of the performance of the 4 classifiers. Oversampling using SMOTe technique was also done on the dataset and then also the metrics were evaluated. Lastly, I used a simple neural network to test the randomly undersampled and oversampled data. In the end, it can be concluded that the neural network performed the best on the oversampled dataset and will be the best model for fraud detection.\n","1836cbd6":"## <a id='feature'>Feature Scaling<\/a>\nThrough the above dataset analysis it can be seen that all the columns are scaled except the `Amount` & `Time` features. Most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.","2522c3fc":"# <a id='naive'>Naive Predictor Performance<\/a>\nMy Naive Predictor will be a model which predicts all the transactions as Non-Fraudulant. The following will be the definitions for the model:-\n- \"Fraud(Class = 1)\" is a **negative class**.\n- \"Non-Fraud(Class = 0)\" is a **Positive class**.\n\n** Please note ** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place you could start from.\n\n> *When we have a model that always predicts '0' (i.e. the transaction is non-fraudulant) then our model will have no **True Negatives(TN)** or **False Negatives(FN)** as we are not making any negative(Fraud '1' value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives\/(True Positives + False Positives)) as every prediction that we have made with value '0' that should have '1' becomes a False Positive; therefore our denominator in this case is the total number of records we have in total.\nOur Recall score(True Positives\/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives.*\n\nThe metrics that I'll be using are:  *Accuracy, Precision, Recall & f1-score* ","999f8163":"### Heatmap 2:\nIn order to get a better view of what features influence a transaction to be fraudulant, it would be best to plot a heatmap on top of the randomly undersampled dataset where the class distribution(fraud vs non-fraud) is equal. As can be seen through the figure this heatmap shows a lot of correlation between the various features. Hence, this graph can prove to be handy in studying the features.","76b4043b":"**Grid-search** is used to find the optimal hyperparameters of a model which results in the most \u2018accurate\u2019 predictions. I have performed grid search on the listed classifiers below. Logistic Regression classifer still produces the best score out of the 4 classifiers.","8a583924":"Below we can see the boxplots representing the before and after state of the outliers for features *V10* and *V2*. It is clearly evident that the number of outliers have reduced after going through outlier removal. ","cab0e94d":"An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.","597c2b42":"The above plots show the following cases(These graphs are better understood when seen upside down as we are plotting the error function against number of instances):-\n- `Plot 1` : This is a case of **Underfitting(High Bias)** as it has very high training and cross-validation error meaning that the model performs badly on both training and cross-validation set.\n- `Plot 2` : This is the **Ideal** case as it has low training and cross-validation errors and they seem to converge as the number of instances increase. We strive to find such model for our predictions.\n- `Plot 3` : This is a case of **Overfitting(High Variance)** as it has very low training error and pretty high cross-validation error meaning that the model has learned each and every detail of the training data that if anything different fed to it then it won't be able to perform well. And the the training and cross-validation error lines don't seem to converge at any point of time.","2884bcde":"Below I have calculated the cross validation score for the 4 classifiers. *Logistic Regression and Support Vector Classifer* produce the best score.","d3893b60":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*2c21SkzJMf3frPXPAR_gZA.png' height=100 width=500\/>\nBoxplots are a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d).<br>\n- **median (Q2\/50th Percentile)**: the middle value of the dataset.\n- **first quartile (Q1\/25th Percentile)**: the middle number between the smallest number (not the \u201cminimum\u201d) and the median of the dataset.\n- **third quartile (Q3\/75th Percentile)**: the middle value between the median and the highest value (not the \u201cmaximum\u201d) of the dataset.\n- **interquartile range (IQR)**: 25th to the 75th percentile.\n- **maximum**: Q3 + 1.5xIQR\n- **minimum**: Q1 -1.5xIQR\n- **whiskers**(shown in blue)\n- **outliers**(shown as green circles): Anything below the minimum value and above the maximum value is an outlier\n\nBelow I have deviced a method, which will detect the outliers and then remove them from the dataset.","558f2742":"As can be seen through the learning curves above, **Logistic Regression** and **Support Vector Classifier** show the best score in training and cross-validation set and are pretty close to the ideal case.","fa4e5cf8":"#### *FPR(False Positive Rate)*\n<img src='https:\/\/miro.medium.com\/max\/490\/1*3GhDfiuhvINF5-9eL8g6Pw.png'\/>","7a366135":"Below are the histogram distributions of the columns `Time` and `Amount` respectively.","59924137":"# <a id='ref'>References<\/a>\n- https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets","12ffb2e0":"# <a id='sampling'>Undersampling vs Oversampling<\/a>\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*P93SeDGPGw0MhwvCcvVcXA.png' \/>","0dee89b4":"Now if we see the number of fraud vs non-fraud cases are equally distributed.","b39155cc":"From the above curve it can be seen that the **KNearest Neighbors Classifier** has the highest area under the ROC curve and hence, seems to be the best model according to this metric.","129a1ed4":"Below I have added the classifiers to a dictionary for making the coding easier. If any changes are required to the classifiers then this is the only place that needs to be changed.","2c71b15c":"# <a id='dataset'>Dataset and Preprocessing<\/a>\nThe dataset is provided by Kaggle and contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and more background information about the data could not be provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n","3ea409cc":"# <a id=\"creditcard\"> Credit Card Fraud Detection<\/a>\n\n<img src='https:\/\/wallpaperaccess.com\/full\/2666312.jpg' height=700 width=500\/>\n<br>\n\nCredit card fraud is a wide-ranging term for theft and fraud committed using or involving a payment card, such as a credit card or debit card, as a fraudulent source of funds in a transaction. The purpose may be to obtain goods without paying or to obtain unauthorized funds from an account. Credit card fraud is also an adjunct to identity theft.\nAlthough incidences of credit card fraud are limited to about 0.1% of all card transactions, they have resulted in huge financial losses as the fraudulent transactions have been large value transactions. It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. What we need is an algorithm, which could classify a transaction as fraudulent or non-fraudulent. Doing so will benefit both the credit card companies and the customers who have to go through the ordeal.","19d813f9":"A **model hyperparameter** is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example, c in Support Vector Machines, k in k-Nearest Neighbors, the number of hidden layers in Neural Networks.\nIn contrast, a **parameter** is an internal characteristic of the model and its value can be estimated from data. Example, beta coefficients of linear\/logistic regression or support vectors in Support Vector Machines.\n\nBelow are the hyperparameters for the classifiers:-"}}