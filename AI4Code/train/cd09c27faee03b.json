{"cell_type":{"2296b6ce":"code","b8b51a54":"code","b4884b7a":"code","23099d32":"code","1d934970":"code","3ef16615":"code","a6e86adb":"code","18e10751":"code","e95f45ac":"code","2e72f96b":"code","6f4cf8a2":"code","1ebc22c7":"code","bf49fae3":"code","54bfb263":"code","f4ca9f79":"code","4351fc14":"code","5c836ab2":"code","af6e8d7c":"code","cd83761f":"code","58a26c77":"code","1bfde2d0":"code","c0737fa7":"code","f857b1ae":"code","445dd11d":"code","5f347747":"code","0636d328":"markdown","192fc167":"markdown","07c11eaa":"markdown","a63dc22c":"markdown","6150a4b7":"markdown","d649a2f5":"markdown","f06c67cd":"markdown","3456ebb4":"markdown","fe8745ff":"markdown","a31855c7":"markdown","05796b16":"markdown","aedf9227":"markdown","62a34aea":"markdown","33b5194a":"markdown","4fa5bac5":"markdown","450d63d7":"markdown","54345929":"markdown","69c5e3a5":"markdown","c40da8f6":"markdown","d25f2fe2":"markdown","6f8c88dd":"markdown","be0af612":"markdown","b0e02ad8":"markdown"},"source":{"2296b6ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#basic eda packages\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 7, 5\npd.options.display.max_columns = 25\nsns.set(style='darkgrid')\n\n#validation\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\n#models \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b8b51a54":"df = pd.read_csv('\/kaggle\/input\/kc-housesales-data\/kc_house_data.csv')","b4884b7a":"df.head()","23099d32":"print(\"Shape of the data :\",df.shape)","1d934970":"df.info()","3ef16615":"df.describe()","a6e86adb":"pd.DataFrame(df.isna().sum()).T","18e10751":"pd.DataFrame(df.corr()['price']).sort_values(by='price',ascending=False)","e95f45ac":"print(Counter(df.bedrooms))\nsns.countplot(df.bedrooms,order=df.bedrooms.value_counts().index);\nplt.title(\"No of Bedrooms count\");","2e72f96b":"plt.xticks(rotation=90)\nsns.countplot(df.bathrooms,order=df.bathrooms.value_counts().index);\nplt.title('No of bathroom Counts');","6f4cf8a2":"print(Counter(df.floors))\nsns.countplot(df.floors,order=df.floors.value_counts().index);\nplt.title(\"Number of Floors\");","1ebc22c7":"print(Counter(df.waterfront))\nplt.pie(df.waterfront.value_counts(),explode=[0,0.5],\\\n        autopct=\"%01.1f\",labels=df.waterfront.unique(),shadow=True,startangle=10,colors=['gold','red'],\\\n        textprops={'fontsize': 14});\nplt.axis(\"equal\");\nplt.title('Waterfront in Percentage');","bf49fae3":"print(Counter(df.view))\nsns.countplot(df.view);","54bfb263":"sns.barplot(list(Counter(df.grade).keys()),list(Counter(df.grade).values()));\nplt.xlabel(\"Grade\");\nplt.ylabel(\"Count of Grades\");\nplt.title(\"Types of Grades\");","f4ca9f79":"unwanted = ['id','date']\ndf.drop(unwanted,axis=1,inplace=True)","4351fc14":"df.head() #id and data variable coll is droped","5c836ab2":"df['built_age'] = 2020 - df.yr_built \ndf.drop('yr_built',axis=1,inplace=True)","af6e8d7c":"df.head(1)","cd83761f":"X = list(df.iloc[:,1:].values) # independent variables\ny = df.price.values # dependent variable","58a26c77":"sn = StandardScaler()\nX = sn.fit_transform(X)\nX","1bfde2d0":"sns.distplot(y);\nplt.xticks(rotation=90);\nplt.title(\"Before normalizing the dependent variable\");","c0737fa7":"y = np.log10(y)\n#we just normalized the y variable using log10 which is available in numpy package \n#now lets plot the data\n\nsns.distplot(y);\nplt.xticks(rotation=90);\nplt.title(\"After normalizing the dependent variable\");\nprint(\"This is also called as normal gaussian distribution\")","f857b1ae":"X_train ,X_test , y_train ,y_test = train_test_split(X,y,test_size=0.2,random_state=10)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape) #printing the shape of splited data","445dd11d":"model_line = LinearRegression(normalize=True,fit_intercept=True,n_jobs=1)\nmodel_line.fit(X_train,y_train)\n\ny_train_pred = model_line.predict(X_train)\ny_pred = model_line.predict(X_test)\n\n\nprint(\"Train score:\",r2_score(y_train,y_train_pred))\nprint(\"Test score:\",r2_score(y_test,y_pred))","5f347747":"model = RandomForestRegressor(n_estimators=190,max_depth=100,random_state=25,max_features='auto',n_jobs=1)\nmodel.fit(X_train,y_train)\n\ny_train_pred = model.predict(X_train)\ny_pred = model.predict(X_test)\n\n\nprint(\"Train score:\",r2_score(y_train,y_train_pred))\nprint(\"Test score:\",r2_score(y_test,y_pred))","0636d328":"Here we'll calculate the number of year's between present year and mentioned year in the data","192fc167":"- We'll check the distrubution of the dependent variable ","07c11eaa":"Then, we split them to train and test sets","a63dc22c":"Lets have handson on pie chart as well!!","6150a4b7":"### Feature engineering \n- and yea it's a vast topic but i just went with basics here ","d649a2f5":"lets drop the non contribution variables here","f06c67cd":"# Machine learning\n> here i have taken 2 regression algorithms i.e\n- LinearRegression\n- RandomForestRegressor\n\n\n- Here we'er calculating both train and test data so that we can check if the data is overfitted , underfitted of optimal in state","3456ebb4":"# 2.RandomForestRegressor","fe8745ff":"> # Basic eda checks","a31855c7":"#### if you find this kernal helpfull please vote :)","05796b16":"# Preprocessing\n- We start machine learning by setting the features and target:\n\n- Features: x\n- Target: y","aedf9227":"# lets plot the data","62a34aea":"Here their are no null values found!!","33b5194a":"## Split the data","4fa5bac5":"# Scaling the data","450d63d7":"### Import Libraries","54345929":"**correlation** w.r.t price ","69c5e3a5":"- Here we can see that data is \"Highly positively skewed\"\n- Lets normalize the y variable (dependent variable)","c40da8f6":"#### Let's get started!","d25f2fe2":"## If you made it this far, thank you for your attention.\n### In order to improve my score, I will keep updating this kernel by:\n\n- Creating new features\n- normalizing the data \n- I will bring more updates, stay tuned!","6f8c88dd":"# 1. LinearRegression","be0af612":"# KC_Housesales_Data\n- Predict House sale prices using Multi-Linear Regression\n- Predict the sales of houses in King County with an accuracy of at least 75-80% and \n- understand which factors are responsible for higher property value  \"$650K and above.\"\n","b0e02ad8":"# Data Cleaning"}}