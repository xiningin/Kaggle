{"cell_type":{"c1625893":"code","2aa16f66":"code","f81d0616":"code","3d9ad167":"code","4e203be1":"code","85dc5889":"code","78d6f465":"code","1a04d7a6":"code","ec158c88":"code","c8858154":"code","d46c8a23":"code","ea8c7e8f":"code","3737976a":"code","70b3fde4":"markdown","fabe2be1":"markdown","0d4df01c":"markdown","66357c22":"markdown","b97e8e1a":"markdown","7cbcdcd5":"markdown","1d619053":"markdown","4e4886bf":"markdown","9e8aa8d8":"markdown"},"source":{"c1625893":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer #word stemmer class\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding","2aa16f66":"embeddings_index = dict()\nf = open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","f81d0616":"df_total = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ndisplay(df_total.head())","3d9ad167":"def preprocess_text(tweet):\n    tweet = tweet.lower() # Convert to lowercase\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet) # Remove words with non-ASCII characters\n    words = tweet.split()\n    words = filter(lambda x: x[0]!= '@' , tweet.split()) # Remove user tags\n    words = [word for word in words if word not in set(stopwords.words('english'))] # Remove stop words\n    tweet = \" \".join(words)\n    return tweet","4e203be1":"df_total['preprocessedTweet'] = df_total.tweet.apply(preprocess_text)\ndisplay(df_total.head())","85dc5889":"df_total.isna().sum()","78d6f465":"max_length = df_total.preprocessedTweet.apply(lambda x: len(x.split())).max()\n\nt = Tokenizer()\nt.fit_on_texts(df_total.preprocessedTweet)\nvocab_size = len(t.word_index) + 1\nencoded_tweets = t.texts_to_sequences(df_total.preprocessedTweet)\npadded_tweets = pad_sequences(encoded_tweets, maxlen=max_length, padding='post')\n\nvocab_size = len(t.word_index) + 1","1a04d7a6":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","ec158c88":"x_train, x_test, y_train, y_test = train_test_split(padded_tweets, df_total.label, test_size=0.2, stratify=df_total.label)","c8858154":"model_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size, 50, input_length=max_length, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","d46c8a23":"## Fit train data\nmodel_glove.fit(x_train, y_train, epochs = 10)","ea8c7e8f":"y_pred = model_glove.predict(x_test)","3737976a":"pr, rc, thresholds = precision_recall_curve(y_test, y_pred)\nplt.plot(thresholds, pr[1:])\nplt.plot(thresholds, rc[1:])\nplt.show()\ncrossover_index = np.max(np.where(pr == rc))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_recall = rc[crossover_index]\nprint(\"Crossover at {0:.2f} with recall {1:.2f}\".format(crossover_cutoff, crossover_recall))\nprint(classification_report(y_test, y_pred > crossover_cutoff))","70b3fde4":"We may have to deal with NaNs.","fabe2be1":"# Train the model\nSince we are dealing with sequences, I found a better performance with bidirectional recurrent neural networks.","0d4df01c":"# Tokenize\nTo apply the GloVe embeddings, we have to first convert our text to sequences. We can use keras to define a vocabulary in which each word will have a unique index.\nWe will pad shorter sentences to the max length (length of longest tweet after preprocessing).","66357c22":"Since many tweets contain accents that add no value to the sentiment, we will remove these.\nWe also remove stop words and user tags for the same reason.","b97e8e1a":"# Why GloVe?\n* GloVe word embeddings are generated from a huge text corpus like Wikipedia and are able to find a meaningful vector representation for each word in our twitter data.\n* This allows me to use Transfer learning and train further over our data.\n* I will use the 50-dimensional data.\n* When used with a BiLSTM, the results seem to be better than BoW and Td-Idf vectorization methods.\n* Possible inaccuracies can occur because the tweets are filled with misspelt words and internet slang. Which is why I have made the Embedding layer trainable.","7cbcdcd5":"# Read GloVe Embeddings\nWe will read the GloVe embeddings and make a dictionary that maps a word to its vector.","1d619053":"# Evaluate\nSince our dataset is not well balanced, we must see the recall score rather than accuracy.","4e4886bf":"# Tweet preprocessing\n\n","9e8aa8d8":"Now we map each unique word index with its GloVe vector."}}