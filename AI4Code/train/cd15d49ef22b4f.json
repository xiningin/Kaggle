{"cell_type":{"c64d14cc":"code","ddde3b73":"code","c59df6f5":"code","9e4b0b83":"code","d145db4f":"code","69ac9340":"code","1b2740a2":"code","17a20ddc":"code","c78801f6":"code","627f2cad":"code","543a2671":"code","880920ac":"code","17727e85":"code","68c973fc":"code","5b597299":"code","da54f9b8":"code","1f21f212":"code","1c0dd84d":"code","a4142b6a":"markdown","97c6bf41":"markdown"},"source":{"c64d14cc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras import Model, Sequential\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport re\nimport string","ddde3b73":"import nltk\nnltk.download('stopwords')","c59df6f5":"# Load Dataset\ndataset = pd.read_csv('..\/input\/emotions-in-text\/Emotion_final.csv')\ndataset.tail()","9e4b0b83":"# check the missing values\ndef missing_value(data):\n    missing_data = pd.DataFrame({\n        'Missing_count':data.isnull().sum(),\n        'Missing_part (%)':(data.isnull().sum()\/len(data))*100\n    })\n    missing_data = missing_data[missing_data['Missing_count']!=0]\n\n    if len(missing_data) > 0:\n        return missing_data\n    else:\n        return \"No value is lost!!!\"","d145db4f":"missing_value(dataset)","69ac9340":"missing_value(dataset)","1b2740a2":"# Encoded Sentiment columns\nencoder = LabelEncoder()\ndataset['Label'] = encoder.fit_transform(dataset['Emotion'])\ndataset.tail()","17a20ddc":"num_classes = dataset.Label.nunique()\nprint(num_classes)","c78801f6":"def cleaning_text(text):\n    stop_words = stopwords.words(\"english\")\n\n    text = re.sub(r'http\\S+', \" \", text)    # remove urls\n    text = re.sub(r'@\\w+',' ',text)         # remove mentions\n    text = re.sub(r'#\\w+', ' ', text)       # remove hastags\n    text = re.sub('r<.*?>',' ', text)       # remove html tags\n    \n    # remove stopwords \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stop_words])\n\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, \"\")\n    \n    return text\n\ndataset['Text'] = dataset['Text'].apply(lambda x: cleaning_text(x))","627f2cad":"for i in range(5):\n    print('----------------------------------------------')\n    random_number=np.random.randint(0,len(dataset)-1)\n    print(dataset.Text[random_number])\n    print('----------------------------------------------\\n')","543a2671":"# Maximum sentence length\nmax_len_words = max(list(dataset['Text'].apply(len)))\nprint(max_len_words)","880920ac":"sns.countplot(dataset.Emotion)\nplt.show()","17727e85":"def tokenizer(x_train, y_train, max_len_word):\n    # because the data distribution is imbalanced, \"stratify\" is used\n    X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, \n                                                      test_size=.2, shuffle=True, \n                                                      stratify=y_train, random_state=0)\n\n    # Tokenizer\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(X_train)\n    sequence_dict = tokenizer.word_index\n    word_dict = dict((num, val) for (val, num) in sequence_dict.items())\n\n    # Sequence data\n    train_sequences = tokenizer.texts_to_sequences(X_train)\n    train_padded = pad_sequences(train_sequences,\n                                 maxlen=max_len_word,\n                                 truncating='post',\n                                 padding='post')\n    \n    val_sequences = tokenizer.texts_to_sequences(X_val)\n    val_padded = pad_sequences(val_sequences,\n                                maxlen=max_len_word,\n                                truncating='post',\n                                padding='post', )\n    \n    print(train_padded.shape)\n    print(val_padded.shape)\n    print('Total words: {}'.format(len(word_dict)))\n    return train_padded, val_padded, y_train, y_val, word_dict\n\nX_train, X_val, y_train, y_val, word_dict = tokenizer(dataset.Text, dataset.Label, 100)","68c973fc":"model = Sequential([\n    layers.Embedding(5000, 100, input_length=100),\n    layers.Bidirectional(layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4)),\n    #layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4),\n    #layers.BatchNormalization(),\n    layers.GlobalAveragePooling1D(),    # or layers.Flatten()\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(num_classes, activation='softmax')\n])","5b597299":"model.summary()","da54f9b8":"model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              metrics=['accuracy'])","1f21f212":"import time\nstart = time.perf_counter()\nearly_stopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                         mode =\"min\", patience=3)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=50, \n                    validation_data=(X_val, y_val),\n                    callbacks=[early_stopping], \n                    shuffle=True)\n\nelapsed = time.perf_counter() - start\nprint('Elapsed %.3f seconds.' % elapsed)","1c0dd84d":"# Plotting accuracy and val_accuracy\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(1, len(val_acc)+1)\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.xlim(1, len(val_acc)+1)\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.xlim(1, len(val_acc)+1)\nplt.title('Training and Validation Loss')\nplt.show()","a4142b6a":"### Preprocessing","97c6bf41":"### Model"}}