{"cell_type":{"4e87adf6":"code","0293e7c4":"code","81b193a7":"code","3b2d55fd":"code","22881537":"code","32e825e7":"code","58bc57f7":"code","16169670":"code","222fcad3":"code","5027a26c":"code","a59e0eda":"code","ad2b9853":"code","814bcb4a":"code","e29cd91a":"code","824cef43":"code","7a43b1c3":"code","11f5f61d":"code","6547a0cc":"code","146c24b3":"code","110a1a2a":"code","5ed82547":"code","b0d9c992":"code","06fca8db":"code","3d99d13a":"code","90ae2486":"code","b246980c":"code","d5f242bb":"code","80379504":"code","51c1c5cc":"markdown","a86c5a04":"markdown","4ddb3d6f":"markdown","a018cfdf":"markdown","473613e2":"markdown","58711bd8":"markdown","8aeccbb6":"markdown","ab39d6e8":"markdown","bea5da5b":"markdown","37f1147c":"markdown","6fe9ef20":"markdown","4b23b921":"markdown","b69e4713":"markdown","d41fbe9d":"markdown","efaaef15":"markdown","68a0141b":"markdown","6eb094fd":"markdown","82126e8d":"markdown","071df727":"markdown","c43c1f2a":"markdown","0151ca0d":"markdown","b3f27a7a":"markdown","e746c45a":"markdown","34eb6d26":"markdown","372e91aa":"markdown","1c599d69":"markdown","c86d9bed":"markdown"},"source":{"4e87adf6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","0293e7c4":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","81b193a7":"train_df.head(5)","3b2d55fd":"for question in train_df.question_text[:5]:\n    print(question)","22881537":"labels = np.array(train_df.target)\n\nprint('Number of training samples:', len(train_df))\nprint('Percentage of insincere questions: {:.2%}'.format(labels.sum()\/len(train_df)))\nprint('Number of test samples:', len(test_df))","32e825e7":"ins_idx = np.where(np.array(train_df.target)==1)[0]\nfor question in train_df.question_text[ins_idx[:5]]:\n    print(question)","58bc57f7":"from string import punctuation\n\ndef lower_eliminate_punctuation(sentence):\n    '''\n    Function that takes an string as input, lower it and get rid of its punctuation\n    '''\n    filtered_sentence = ''.join([c for c in sentence if c not in punctuation])\n    return filtered_sentence.lower()\n\n# Getting rid of punctuation in both datasets\ntrain_df.question_text = train_df.question_text.apply(lower_eliminate_punctuation)\ntest_df.question_text = test_df.question_text.apply(lower_eliminate_punctuation)","16169670":"# Training set\nempty_idx_train = []\nfor i, question in enumerate(train_df.question_text):\n    if question == '':\n        print('Empty question at index', i, 'with label ', train_df.target[i])\n        empty_idx_train.append(i)","222fcad3":"# Test set\nempty_idx_test = []\nfor i, question in enumerate(test_df.question_text):\n    if question == '':\n        print('Empty question at index', i, 'with label ', test_df.target[i])\n        empty_idx_test.append(i)","5027a26c":"# Eliminating sample with empty question in the training set\ntrain_df = train_df.drop(empty_idx_train, axis=0)\nlabels = np.delete(labels, empty_idx_train)","a59e0eda":"# Getting all words in both samples\nall_text_list = list(train_df.question_text) + list(test_df.question_text)\nall_text = ' '.join(all_text_list)\nwords = set(all_text.split())\n\nprint('Number of unique words:', len(words))","ad2b9853":"# Dictionary that maps words to integers\nword_to_int = {word: i for i, word in enumerate(words, 1)}","814bcb4a":"def tokenize(sentence):\n    '''\n    Function that tokenize a sentence using the word_to_int dictionnaire and \n    return a list of tokens\n    '''\n    tokens = []\n    for word in sentence.split():\n        tokens.append(word_to_int[word])\n    return tokens\n\n\ntrain_tokens = train_df.question_text.apply(tokenize)\ntest_tokens = test_df.question_text.apply(tokenize)","e29cd91a":"print('Size of longest question:')\nprint('Training set:', max(train_tokens.apply(len)))\nprint('Test set:', max(test_tokens.apply(len)))","824cef43":"seq_length = max(train_tokens.apply(len)) # == 132","7a43b1c3":"def pad(questions, seq_length):\n    '''\n    This function pad the questions fed as series of tokens with 0 at left\n    and returns a numpy array\n    '''\n    \n    features = np.zeros((len(questions), seq_length), dtype=int)\n    for i, sentence in enumerate(questions):\n        features[i, -len(sentence):] = sentence\n    \n    return features","11f5f61d":"train = pad(train_tokens, seq_length)\ntest = pad(test_tokens, seq_length)","6547a0cc":"x_train, x_val, label_train, label_val = train_test_split(train, labels, test_size=0.1, random_state=0) ","146c24b3":"# Create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(label_train))\nvalid_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(label_val))\ntest_data = TensorDataset(torch.from_numpy(test))\n\n# Create Dataloaders\nbatch_size = 56\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)","110a1a2a":"# Checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif train_on_gpu:\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","5ed82547":"class RNN_model(nn.Module):\n    \"\"\"\n    The RNN model that will be used for our classification task\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers\n        \"\"\"\n        super(RNN_model, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim       \n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # GRU layer\n        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout = drop_prob)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        # Fully-connected layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n        # Sigmoid layer\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        \n        batch_size = x.size(0)\n        \n        # Deal with cases were the current batch_size is different from general batch_size\n        # It occurrs at the end of iteration with the Dataloaders\n        if hidden.size(1) != batch_size:\n            hidden = hidden[:, :batch_size, :].contiguous()\n        \n        # Apply embedding\n        x = self.embedding(x)\n        \n        # GRU Layer\n        out, hidden = self.gru(x, hidden)\n        \n        # Stack up GRU outputs --> preparation for the fully-connected layer\n        out = out.contiguous().view(-1, self.hidden_dim)\n        \n        # Dropout and fully-connected layers\n        out = self.dropout(out)\n        sig_out = self.sigmoid(self.fc(out))\n        \n        # Unstack outputs to come back to correct dimensions per sample (batch_size, seq_length)\n        sig_out = sig_out.contiguous().view(batch_size, -1)\n        \n        # return last sigmoid output and hidden state\n        return sig_out[:, -1], hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create a new tensor with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero\n        \n        weight = next(self.parameters()).data\n        \n        if train_on_gpu:\n            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda()\n            \n        else:\n            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n        \n        return hidden\n        ","b0d9c992":"vocab_size = len(word_to_int) + 1 # including token 0\noutput_size = 1 # binary classification task \nembedding_dim = 256\nhidden_dim = 256\nn_layers = 1\n\n# Initiating the model\nmodel = RNN_model(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)","06fca8db":"# Training parameters\n\nepochs = 4\n\nprint_every = 1000\nclip = 5 # gradient clipping - to avoid gradient explosion\n\nlr=0.001\n\n# Defining loss and optimization functions\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","3d99d13a":"def train_model(model, train_loader, valid_loader, batch_size, epochs, optimizer, criterion, print_every, clip):\n    \n    # move model to GPU, if available\n    if(train_on_gpu):\n        model.cuda()\n    \n    counter = 0\n    \n    # Model in training mode\n    model.train()\n    breaker = False\n    for e in range(epochs):\n\n        # Batch loop\n        for inputs, labels in train_loader:\n            counter += 1\n\n            # move data to GPU, if available\n            if(train_on_gpu):\n                inputs, labels = inputs.cuda(), labels.cuda()\n\n            # Initialize hidden state\n            h = model.init_hidden(batch_size)\n\n            # Setting accumulated gradients to zero before backward step\n            model.zero_grad()\n\n            # Output from the model\n            output, _ = model(inputs, h)\n\n            # Calculate the loss and perform backprop\n            loss = criterion(output.squeeze(), labels.float())\n            loss.backward()\n\n            # Clipping the gradient to avoid explosion\n            nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n            # Backpropagation step\n            optimizer.step()\n\n            # Validation stats\n            if counter % print_every == 0:\n\n                with torch.no_grad():\n\n                    # Get validation loss and F1-score on validation set\n\n                    val_losses = []\n                    all_val_labels = []\n                    all_val_preds = []\n\n                    # Model in evaluation mode\n                    model.eval()\n                    for inputs, labels in valid_loader:\n\n                        all_val_labels += list(labels)\n\n                        # Sending data to GPU\n                        if(train_on_gpu):\n                            inputs, labels = inputs.cuda(), labels.cuda()\n\n                        # Initiating hidden state for the validation set\n                        val_h = model.init_hidden(batch_size)\n\n                        output, _ = model(inputs, val_h)\n\n                        # Computing validation loss\n                        val_loss = criterion(output.squeeze(), labels.float())\n\n                        val_losses.append(val_loss.item())\n\n                        # Computing validation F1-score\n\n                        preds = torch.round(output.squeeze())  # 1 if output probability >= 0.5\n                        preds = np.squeeze(preds.numpy()) if not train_on_gpu else np.squeeze(preds.cpu().numpy())\n                        all_val_preds += list(preds)\n\n                current_loss = np.mean(val_losses)\n                \n                print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}...\".format(current_loss),\n                      \"F1-score: {:.3%}\".format(f1_score(all_val_labels, all_val_preds)))\n                \n                # Saving the best model and stopping if there is no improvement after 10 evaluations\n                \n                if counter == print_every: # first evaluation\n                    best_loss = current_loss\n                    counter_eval = 0  \n                    \n                if current_loss < best_loss:\n                    best_loss = current_loss\n                    torch.save(model.state_dict(), 'checkpoint.pth')\n                    counter_eval = 0 \n                    \n                counter_eval += 1\n                if counter_eval == 10:\n                    breaker = True\n                    break\n\n                # Put model back to training mode\n                model.train()\n        \n        # breaking outer loop on epochs\n        if breaker:\n            break\n    \n    # Loading best model\n    state_dict = torch.load('checkpoint.pth')\n    model.load_state_dict(state_dict)","90ae2486":"train_model(model, train_loader, valid_loader, batch_size, epochs, optimizer, criterion, print_every, clip)","b246980c":"# Model in evaluation mode\nmodel.eval()\n\nwith torch.no_grad():\n    all_test_preds = []\n\n    for inputs in test_loader:\n        inputs = inputs[0]\n        \n        # Sending data to GPU\n        if(train_on_gpu):\n            inputs = inputs.cuda()\n            \n        test_h = model.init_hidden(batch_size)\n        output, _ = model(inputs, test_h)\n        \n        preds = torch.round(output.squeeze())  # 1 if output probability >= 0.5\n        preds = np.squeeze(preds.numpy()) if not train_on_gpu else np.squeeze(preds.cpu().numpy())\n        all_test_preds += list(preds.astype(int))","d5f242bb":"sub = pd.DataFrame({\n    'qid': test_df.qid,\n    'prediction': all_test_preds\n})\n\n# Make sure the columns are in the correct order\nsub = sub[['qid', 'prediction']]","80379504":"sub.to_csv('submission.csv', index=False, sep=',')","51c1c5cc":"### The model\n\nThe structure of the model is simple:\n1. Embedding layer. As said above, we are not going to use pre-trained embeddings.\n2. 1-Layer GRU\n3. Dropout Layer to avoid overfitting\n4. Fully connected layer followed by the application of a sigmoid\n5. Use the output of the last position of the setence as prediction probability","a86c5a04":"Now, let's create Dataloaders for the datasets that will help us with batch iteration.","4ddb3d6f":"## Import of useful libraries","a018cfdf":"# Challenge Kaggle - Quora Insincere Questions Classification - Simple Version","473613e2":"Visualizing a few examples of insincere questions:","58711bd8":"### Defining hyperparameters and initiating the model","8aeccbb6":"### Eliminate punctuation and lower the sentences\n\nFor this simpler notebook, I am only going to deal with the words in the sentences, without taking into account the punctuation. It is however important to notice that ponctuation might play a big role in this task, as insincere questions are likely to have some particular patterns of punctuation.","ab39d6e8":"### Splitting training data and creating dataloaders","bea5da5b":"This notebook presents a simple solution for the Kaggle Challenge: [Quora Insincere Questions Classification](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification).\n\nThis problem can be understood as a sentiment analysis problem, one of the most common downstream tasks in NLP.\n\nThe objective is to build a simple RNN model step-by-step in order to understand how such problem can be tackled.\n\nI am going to use the PyTorch library as deep-learning framework.\n\nIn this solution, I am not going to use one of pre-trained embedding made available by the challenge. In order to present a more didactic solution, I am going to hand-craft the tokenization and we are going to add a embedding layer for training in the model. Please note that this approach will increase the training time, as the model will need to learn the paramaters of the embeddings, which quantity can be huge given the size of the vocabulary.","37f1147c":"Checking the size of the longest question in both datasets","6fe9ef20":"### Tokenizing and padding","4b23b921":"### Dealing with outliers\n\nIf there are empty sentences in the datasets, we should not take them into account","b69e4713":"Now that we have processesed the data, I am going to build a RNN model using 1-Layer GRUs. Such RNN structure is pretty simple and it usually give good results. One can choose to use LSTM instead, I chose GRU because it will have to train less parameters while keeping good performance.","d41fbe9d":"It can be noted that the data set is very unballanced (only 6.19% of the training samples are labeled 1), therefore accuracy is not a good metric to evalutate performance. The metric chosen by the competition, F1-score, is a good metric for evluation of unbalaced datasets.","efaaef15":"## Training\n\nWe are going to use binary cross-entropy loss (BCELoss()) as loss function and the ADAM optimizer.\n\nWe are algo going to print the F1-score on valuation set each 1000 steps to follow the progress of the training. Please note that, for simplicity, we are considering a threshold of 0.5 for prediction of a positive label. This is also a hyperparameter that can be learned by cross-validation.\n\nWe are also going to clip the gradient whenever its norm is higher than 5 in order to avoid the explosion gradient effect that can happen often in RNNs.","68a0141b":"### Training on GPU or CPU","6eb094fd":"### Loading data and visualizing data","82126e8d":"Tokeninzing questions in both datasets:","071df727":"## Data preprocessing","c43c1f2a":"### Creating vocabulary\n\nIn other to tokenize the sentences, we need to create a vocabulary. I will use a dictionnaire to map word and integer. \n\nNote: the first number of the vocabulary will be a 1, as we will use 0 for the padding of the sentences.","0151ca0d":"## Predictions on test set","b3f27a7a":"In order to avoid overfitting, we need to use some data as validation during the training phase.\n\nI am going to use a 90\/10 ratio for the split of training and validation sets, in order to get a training dataset the closest as possible to the original training dataset","e746c45a":"## Building the model","34eb6d26":"Let's visualizing the structure of the dataframes and the Quora's questions","372e91aa":"As the questions are not too long, we can set the sequence length of the samples to the highest value and we will not need to deal with truncating. \n\nI will then pad the questions at the left using the token 0.","1c599d69":"Some statitistics of the datasets:","c86d9bed":"\n## Loading the data"}}