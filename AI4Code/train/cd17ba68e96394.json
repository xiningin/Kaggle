{"cell_type":{"8f8cd47b":"code","55546491":"code","ff251062":"code","29957d50":"code","0efc3c7c":"code","190ea8ee":"code","2dd73b79":"code","27ee870f":"code","71e3123c":"code","6a62292a":"code","b1d73f83":"code","53c5f772":"code","423cb814":"code","c98e2d83":"code","1bd343d5":"code","700cc1bd":"code","f9cc2435":"code","41943ea7":"code","f6188e31":"code","8566cbd1":"code","5c9e371c":"code","56105840":"code","46021e99":"code","5aaee65d":"code","e8bbdbed":"code","cd791b45":"code","f44383bf":"code","0d5a6e73":"code","4a2d2fad":"code","10ee27b7":"code","609ba37a":"code","09a6d8b4":"code","ee3eed66":"code","3d4b340a":"code","85f56584":"code","2db1c016":"code","888139a4":"code","72b7fa9d":"code","2b611972":"code","d2e1b052":"code","67ac96ec":"code","7c9bfa88":"code","68bc2f7e":"markdown","a7d41a9f":"markdown","6f0ed534":"markdown","71673592":"markdown","58232197":"markdown","7d4fc9a4":"markdown","a4623440":"markdown","1c9d9fbe":"markdown","19e6b183":"markdown","239e88eb":"markdown","5061783d":"markdown","e4fb3420":"markdown","88cee64a":"markdown","8e0f8816":"markdown","dd9b16da":"markdown","ec9bbc0a":"markdown","c21cec49":"markdown","f9fbb65b":"markdown","30456a24":"markdown","0ef18038":"markdown","2dc265c6":"markdown","041e6f0f":"markdown","fdb11196":"markdown","ac084324":"markdown","9549f794":"markdown","be06c919":"markdown","8ab25bbe":"markdown","d8fb878c":"markdown","5cc1acaf":"markdown","6afb0523":"markdown"},"source":{"8f8cd47b":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","55546491":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator):\n    def __init__(self, columns ):\n        self.scaler = StandardScaler()\n        self.columns = columns\n        self.mean_ = None\n        self.std_ = None\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.std_ = np.std(X[self.columns])\n        return self\n    \n    def transform(self, X, y=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns,index=X.index)\n        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","ff251062":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","29957d50":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n#     if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n#         columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","0efc3c7c":"import matplotlib.pyplot as plt # plotting\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn import metrics","190ea8ee":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2dd73b79":"df = pd.read_csv('\/kaggle\/input\/Boston-house-price-data.csv', delimiter=',')\ndf.sample(5)","27ee870f":"df.info()","71e3123c":"print('total number of null values : {0}'.format(df.isna().sum().sum()))","6a62292a":"df.describe()","b1d73f83":"plt.figure(figsize=(11,9))\ncorr = df.corr().round(2)\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# # Want diagonal elements as well\n# mask[np.diag_indices_from(mask)] = False\n\nsns.heatmap(data=corr, annot=True,cmap='coolwarm',mask=mask)\nplt.xticks(rotation=90)\nplt.show()","53c5f772":"for col in df.columns:\n    fig,ax = plt.subplots(1,2,figsize=(15,1.5))\n    if len(np.unique(df[col]))<10:\n        sns.countplot(df[col],ax=ax[0])\n    else:\n        sns.distplot(df[col],bins=50 if len(np.unique(df[col]))>50 else None,ax=ax[0])\n        \n    sns.boxplot(df[col],ax=ax[1])\n    plt.suptitle(col,fontsize=20,y=1.2)\n    plt.show()","423cb814":"columns = [col for col in df.columns if len(np.unique(df[col]))>50]\ncolumns.remove('MEDV')\ncolumns","c98e2d83":"for col in columns:\n    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,1.5))\n    \n    sns.distplot(df[col],bins=50,ax=ax[0])\n    ax[0].set_title('original')\n    \n    quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal',n_quantiles=int(len(df)\/20), random_state=0)\n    X_trans = quantile_transformer.fit_transform(df[col].values.reshape((len(df),1)))\n    sns.distplot(X_trans,bins=50,ax=ax[1])\n    ax[1].set_title('normalized')\n    \n    plt.suptitle(col,fontsize=20,y=1.2)\n    plt.show()","1bd343d5":"columns","700cc1bd":"c = columns.copy()\nc.append('MEDV')\nX = df[c]\nX_train = X.copy()","f9cc2435":"for k, v in X_train.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(X_train)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","41943ea7":"len(X_train)","f6188e31":"Q1 = X_train.quantile(0.25)\nQ3 = X_train.quantile(0.75)\nIQR = Q3 - Q1\n\nX_train = X_train[~((X_train < (Q1 - 1.5 * IQR)) |(X_train > (Q3 + 1.5 * IQR))).any(axis=1)]","8566cbd1":"len(X_train)","5c9e371c":"sns.distplot(X_train['MEDV']);plt.show()","56105840":"cols = 3\nrows = int(len(X_train.drop('MEDV',axis=1).columns)\/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.drop('MEDV',axis=1).columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","46021e99":"X.columns","5aaee65d":"X_train, y_train = X_train.drop('MEDV',axis=1), X_train['MEDV']","e8bbdbed":"y_train = np.log(y_train)","cd791b45":"quantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal',n_quantiles=int(len(X_trans)\/20), random_state=0)\nX_train.loc[:,columns] = quantile_transformer.fit_transform(X_train[columns].values.reshape((len(X_train),len(columns))))","f44383bf":"cols = 3\nrows = int(len(X_train.columns)\/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","0d5a6e73":"scaler = CustomScaler(columns)#check at the start of the book to find the CustomScaler\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","4a2d2fad":"cols = 3\nrows = int(len(X_train.columns)\/cols)\n\nplt.figure(figsize=(15,10))\nfor i,col in enumerate(X_train.columns):\n    ax = plt.subplot(rows, cols, i+1)\n    sns.distplot(X_train[col],ax=ax)","10ee27b7":"X = X_train.copy()\nX.loc[:,'MEDV']=y_train\nQ1 = X.quantile(0.25)\nQ3 = X.quantile(0.75)\nIQR = Q3 - Q1\n\nX = X[~((X < (Q1 - 1.5 * IQR)) |(X > (Q3 + 1.5 * IQR))).any(axis=1)]\ny_train = X['MEDV']\nX_train = X.drop('MEDV',axis=1)","609ba37a":"for k, v in X_train.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(X_train)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","09a6d8b4":"scores_map={}","ee3eed66":"from sklearn.linear_model import LinearRegression\nLR_model = LinearRegression()\nscores = cross_val_score(LR_model,X_train,y_train,cv=10,n_jobs=-1,scoring='neg_mean_squared_error')\nscores_map['LR']=scores\nprint('Logistic Regression negative RMSE {:.3f} (+\/- {:.3f})'.format(scores.mean(),scores.std()))","3d4b340a":"from sklearn.svm import SVR\n\n\nsvr_rbf = SVR(kernel='rbf')\ngrid = GridSearchCV(svr_rbf, cv=10, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","85f56584":"svr_rbf = SVR(kernel='rbf',C=10,gamma=0.01)\n\nscores = cross_val_score(svr_rbf,X_train,y_train,cv=10,n_jobs=-1,scoring='neg_mean_squared_error')\nscores_map['SVR']=scores\nprint('SVR negative RMSE {:.3f} (+\/- {:.3f})'.format(scores.mean(),scores.std()))","2db1c016":"from sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(random_state=0)\ngrid = GridSearchCV(tree, cv=10, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters : \", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","888139a4":"tree = DecisionTreeRegressor(max_depth=7)\nscores = cross_val_score(tree, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['DTree'] = scores\nprint(\"D.Tree negative RMSE {:.3f} (+\/- {:.3f})\".format(scores.mean(),scores.std()))","72b7fa9d":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor()\n\ngrid = GridSearchCV(knn, cv=10, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best parameters :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","2b611972":"knn = KNeighborsRegressor(n_neighbors=4)\nscores = cross_val_score(knn, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['KNN'] = scores\nprint(\"KNN negative RMSE {:.3f} (+\/- {:.3f})\".format(scores.mean(),scores.std()))","d2e1b052":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(random_state=0)\nparam_grid={'n_estimators':[50,100,150, 200], 'learning_rate': [0.5,0.1,0.05,0.02,0.001]\n            , 'max_depth':[2, 3,4,5,6,7,8], 'min_samples_leaf':[3,5,9,11,14,16]\n            ,'min_samples_split':[2,4,6,8,10], 'alpha':[0.05,0.1,0.3,0.5]}\n# grid = GridSearchCV(gbr, cv=10, param_grid=param_grid, scoring='neg_mean_squared_error')\ngrid = RandomizedSearchCV(gbr, cv=10, param_distributions=param_grid, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\nprint(\"Best params :\", grid.best_params_)\nprint(\"Best Score :{:.3f}\".format(grid.best_score_))","67ac96ec":"gbr = GradientBoostingRegressor(n_estimators=200,min_samples_split=2,min_samples_leaf=3,max_depth=8,learning_rate=0.02,alpha=0.05,   random_state=0)\nscores = cross_val_score(gbr, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\nscores_map['GBR'] = scores\nprint(\"GBR negative RMSE {:.3f} (+\/- {:.3f})\".format(scores.mean(),scores.std()))","7c9bfa88":"plt.figure(figsize=(15, 7))\nscores_map = pd.DataFrame(scores_map)\nsns.boxplot(data=scores_map)\nplt.xticks(fontsize=30)\nplt.show()","68bc2f7e":"now the data has been scaled","a7d41a9f":"**Analysis**\n* columns RAD & TAX are highly positively correlated \n    * **inference**: as the accessibility to radial highways increases so does the porperty TAX\n* columns DIS is highly negatively correlated with INDUS, NOX, AGE\n    * **inference**: as the distances to five boston employment centres increases\n    * proportion of non-retail business acres per town decreases\n    * nitric oxides concentration (parts per 10 million) decreases\n    * proportion of owner-occupied units built prior to 1940 decreases\n* column LSTAT & MEDV are highly negatively correlated\n    * **inference**: as the % lower status of the population increases \n    * Median value of owner-occupied homes decreases","6f0ed534":"# Modeling","71673592":"# Univariate Analysis","58232197":"# Standardization","7d4fc9a4":"There is 1 csv file in the current version of the dataset:\n","a4623440":"### Now lets select few columns from do some normalization techniques","1c9d9fbe":"# Support Vector Machine Regressor (SVR)","19e6b183":"# Operations on dataset","239e88eb":"lets do normalization on the train and transform test data with it","5061783d":"## cutting outliers again after normalization","e4fb3420":"as the data is small lets not split for the validation data instead go for cross validation","88cee64a":"### Clipping outliers from train data","8e0f8816":"### Target Variable MEDV","dd9b16da":"# Performance Comparisions","ec9bbc0a":"### Independent variables (INPUTS)","c21cec49":"**Analysis**\n* almost all regressors are performing sholder to sholder","f9fbb65b":"### LOG Transform target variable for better results","30456a24":"**Analysis**\n* here if we observe standard-deviation is much larger than mean for few of the columns which we need to normalize\n* we need to check the distribution by ploting the data, and do the required normalizations","0ef18038":"# Multivariate Analysis","2dc265c6":"# Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","041e6f0f":"**Analysis:**\n* here we observe that all columns are of numeric datatype\n* we can also observe that all of non-null column values have 506, which is fortunately equals to total rows in dataframe\n* i.e., we dont have any null values in this dataframe","fdb11196":"# Normalization","ac084324":"# Gradient Boosting","9549f794":"# Decision Tree Regressor","be06c919":"## Exploratory Analysis","8ab25bbe":"# K Nearest Neighbours Regression (KNN)","d8fb878c":"# Linear Regression","5cc1acaf":"# Normalization","6afb0523":"<img src=\"https:\/\/i.imgur.com\/J4QlqZu.jpg\"\/>\n\n\n## Introduction\nGreetings starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.\n\n"}}