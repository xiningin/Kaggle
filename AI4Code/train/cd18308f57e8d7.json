{"cell_type":{"6351df92":"code","6d387f93":"code","fcc49801":"code","486cfb36":"code","7ab859b6":"code","f4148cdd":"code","2805db57":"code","c7ceb2a3":"code","8a47dbfe":"code","e72ffe25":"code","bcf23586":"code","8c1e391b":"code","623c063b":"code","6b2d3430":"code","0e517d4d":"markdown","b5bcf99e":"markdown","49eb2be8":"markdown","edbbd073":"markdown","0d11d40d":"markdown","0dd708e8":"markdown","dc5de74a":"markdown","0a2b9fa6":"markdown","85931de0":"markdown","d947e548":"markdown","05a44204":"markdown","d2d24f99":"markdown","abe00755":"markdown"},"source":{"6351df92":"from time import time\nfrom datetime import timedelta\nfrom copy import deepcopy\n\nimport random\nimport numpy as np\nimport pandas as pd\nfrom ml_metrics import mapk\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertForMultipleChoice\n\n# Random seed\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# CUDA device\nuse_cuda_device = 0\ntorch.cuda.set_device(use_cuda_device)\nprint(\"Using CUDA device: %d\" % torch.cuda.current_device())","6d387f93":"# Input files\ndocument_csv_path = '..\/input\/ntust-ir2020-homework6\/documents.csv'\ntraining_csv_path = '..\/input\/ntust-ir2020-homework6\/train_queries.csv'\ntesting_csv_path = '..\/input\/ntust-ir2020-homework6\/test_queries.csv'\n\n# Input limitation\nmax_query_length = 64\nmax_input_length = 512\nnum_negatives = 3   # num. of negative documents to pair with a positive document\n\n# Model finetuning\nmodel_name_or_path = \"bert-base-uncased\"\nmax_epochs = 1\nlearning_rate = 3e-5\ndev_set_ratio = 0.2   # make a ratio of training set as development set for rescoring weight sniffing\nmax_patience = 0      # earlystop if avg. loss on development set doesn't decrease for num. of epochs\nbatch_size = 2    # num. of inputs = 8 requires ~9200 MB VRAM (num. of inputs = batch_size * (num_negatives + 1))\nnum_workers = 2   # num. of jobs for pytorch dataloader\n\n# Save paths\nsave_model_path = \"models\/bert_base_uncased\"  # assign `None` for not saving the model\nsave_submission_path = \"bm25_bert_rescoring.csv\"\nK = 1000   # for MAP@K","fcc49801":"# Build and save BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(model_name_or_path)\nif save_model_path is not None:\n    save_tokenizer_path = \"%s\/tokenizer\" % (save_model_path)\n    tokenizer.save_pretrained(save_tokenizer_path)\n\n# Collect mapping of all document id and text\ndoc_id_to_text = {}\ndoc_df = pd.read_csv(document_csv_path)\ndoc_df.fillna(\"<Empty Document>\", inplace=True)\nid_text_pair = zip(doc_df[\"doc_id\"], doc_df[\"doc_text\"])\nfor i, pair in enumerate(id_text_pair, start=1):\n    doc_id, doc_text = pair\n    doc_id_to_text[doc_id] = doc_text\n    \n    print(\"Progress: %d\/%d\\r\" % (i, len(doc_df)), end='')\n    \ndoc_df.tail()","486cfb36":"train_df = pd.read_csv(training_csv_path)\ndev_df, train_df = np.split(train_df, [int(dev_set_ratio*len(train_df))])\ndev_df.reset_index(drop=True, inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\nprint(\"train_df shape:\", train_df.shape)\nprint(\"dev_df shape:\", dev_df.shape)\ntrain_df.tail()","7ab859b6":"%%time\ndoc_id_to_token_ids = {}\ndef preprocess_df(df):\n    ''' Preprocess DataFrame into training instances for BERT. '''\n    instances = []\n    \n    # Parse CSV\n    for i, row in df.iterrows():\n        query_id, query_text, pos_doc_ids, bm25_top1000, _ = row\n        pos_doc_id_list = pos_doc_ids.split()\n        pos_doc_id_set = set(pos_doc_id_list)\n        bm25_top1000_list = bm25_top1000.split()\n        bm25_top1000_set = set(bm25_top1000_list)\n\n        # Pair BM25 neg. with pos. samples\n        labeled_pos_neg_list = []\n        for pos_doc_id in pos_doc_id_list:\n            neg_doc_id_set = bm25_top1000_set - pos_doc_id_set\n            neg_doc_ids = random.sample(neg_doc_id_set, num_negatives)\n            pos_position = random.randint(0, num_negatives)\n            pos_neg_doc_ids = neg_doc_ids\n            pos_neg_doc_ids.insert(pos_position, pos_doc_id)\n            labeled_sample = (pos_neg_doc_ids, pos_position)\n            labeled_pos_neg_list.append(labeled_sample)\n            \n        # Make query tokens for BERT\n        query_tokens = tokenizer.tokenize(query_text)\n        if len(query_tokens) > max_query_length:  # truncation\n            query_tokens = query_tokens[:max_query_length]\n        query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n        query_token_ids.insert(0, tokenizer.cls_token_id)\n        query_token_ids.append(tokenizer.sep_token_id)\n\n        # Make input instances for all query\/doc pairs\n        for doc_ids, label in labeled_pos_neg_list:\n            paired_input_ids = []\n            paired_attention_mask = []\n            paired_token_type_ids = []\n            \n            # Merge all pos\/neg inputs as a single sample\n            for doc_id in doc_ids:\n                if doc_id in doc_id_to_token_ids:\n                    doc_token_ids = doc_id_to_token_ids[doc_id]\n                else:\n                    doc_text = doc_id_to_text[doc_id]\n                    doc_tokens = tokenizer.tokenize(doc_text)\n                    doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\n                    doc_id_to_token_ids[doc_id] = doc_token_ids\n                doc_token_ids.append(tokenizer.sep_token_id)\n\n                # make input sequences for BERT\n                input_ids = query_token_ids + doc_token_ids\n                token_type_ids = [0 for token_id in query_token_ids]\n                token_type_ids.extend(1 for token_id in doc_token_ids)\n                if len(input_ids) > max_input_length:  # truncation\n                    input_ids = input_ids[:max_input_length]\n                    token_type_ids = token_type_ids[:max_input_length]\n                attention_mask = [1 for token_id in input_ids]\n                \n                # convert and collect inputs as tensors\n                input_ids = torch.LongTensor(input_ids)\n                attention_mask = torch.FloatTensor(attention_mask)\n                token_type_ids = torch.LongTensor(token_type_ids)\n                paired_input_ids.append(input_ids)\n                paired_attention_mask.append(attention_mask)\n                paired_token_type_ids.append(token_type_ids)\n            label = torch.LongTensor([label]).squeeze()\n            \n            # Pre-pad tensor pairs for efficiency\n            paired_input_ids = pad_sequence(paired_input_ids, batch_first=True)\n            paired_attention_mask = pad_sequence(paired_attention_mask, batch_first=True)\n            paired_token_type_ids = pad_sequence(paired_token_type_ids, batch_first=True)\n\n            # collect all inputs as a dictionary\n            instance = {}\n            instance['input_ids'] = paired_input_ids.T  # transpose for code efficiency\n            instance['attention_mask'] = paired_attention_mask.T\n            instance['token_type_ids'] = paired_token_type_ids.T\n            instance['label'] = label\n            instances.append(instance)\n\n        print(\"Progress: %d\/%d\\r\" % (i+1, len(df)), end='')\n    print()\n    return instances\n\ntrain_instances = preprocess_df(train_df)\ndev_instances = preprocess_df(dev_df)\n\nprint(\"num. train_instances: %d\" % len(train_instances))\nprint(\"num. dev_instances: %d\" % len(dev_instances))\nprint(\"input_ids.T shape:\", train_instances[0]['input_ids'].T.shape)\ntrain_instances[0]['input_ids'].T","f4148cdd":"class TrainingDataset(Dataset):\n    def __init__(self, instances):\n        self.instances = instances\n    \n    def __len__(self):\n        return len(self.instances)\n        \n    def __getitem__(self, i):\n        instance = self.instances[i]\n        input_ids = instance['input_ids']\n        attention_mask = instance['attention_mask']\n        token_type_ids = instance['token_type_ids']\n        label = instance['label']\n        return input_ids, attention_mask, token_type_ids, label\n    \ndef get_train_dataloader(instances, batch_size=2, num_workers=4):\n    def collate_fn(batch):\n        input_ids, attention_mask, token_type_ids, labels = zip(*batch)\n        input_ids = pad_sequence(input_ids, batch_first=True).transpose(1,2).contiguous()  # re-transpose\n        attention_mask = pad_sequence(attention_mask, batch_first=True).transpose(1,2).contiguous()\n        token_type_ids = pad_sequence(token_type_ids, batch_first=True).transpose(1,2).contiguous()\n        labels = torch.stack(labels)\n        return input_ids, attention_mask, token_type_ids, labels\n    \n    dataset = TrainingDataset(instances)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, \\\n                            batch_size=batch_size, num_workers=num_workers)\n    return dataloader\n\n# Demo\ndataloader = get_train_dataloader(train_instances)\nfor batch in dataloader:\n    input_ids, attention_mask, token_type_ids, labels = batch\n    break\n    \nprint(input_ids.shape)\ninput_ids","2805db57":"model = BertForMultipleChoice.from_pretrained(model_name_or_path)\nmodel.cuda()\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\noptimizer.zero_grad()","c7ceb2a3":"def validate(model, instances):\n    total_loss = 0\n    model.eval()\n    dataloader = get_train_dataloader(instances, batch_size=batch_size, num_workers=num_workers)\n    for batch in dataloader:\n        batch = (tensor.cuda() for tensor in batch)\n        input_ids, attention_mask, token_type_ids, labels = batch\n        with torch.no_grad():\n            loss = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)[0]\n        curr_batch_size = input_ids.shape[0]\n        total_loss += loss * curr_batch_size\n    avg_loss = total_loss \/ len(instances)\n    return avg_loss","8a47dbfe":"patience, best_dev_loss = 0, 1e10\nbest_state_dict = model.state_dict()\n\nstart_time = time()\ndataloader = get_train_dataloader(train_instances, batch_size=batch_size, num_workers=num_workers)\nfor epoch in range(1, max_epochs+1):\n    model.train()\n    for i, batch in enumerate(dataloader, start=1):\n        batch = (tensor.cuda() for tensor in batch)\n        input_ids, attention_mask, token_type_ids, labels = batch\n        \n        # Backpropogation\n        loss = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)[0]\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Progress bar with timer ;-)\n        elapsed_time = time() - start_time\n        elapsed_time = timedelta(seconds=int(elapsed_time))\n        print(\"Epoch: %d\/%d | Batch: %d\/%d | loss=%.5f | %s      \\r\" \\\n              % (epoch, max_epochs, i, len(dataloader), loss, elapsed_time), end='')\n        \n    # Save parameters of each epoch\n    if save_model_path is not None:\n        save_checkpoint_path = \"%s\/epoch_%d\" % (save_model_path, epoch)\n        model.save_pretrained(save_checkpoint_path)\n        \n    # Get avg. loss on development set\n    print(\"Epoch: %d\/%d | Validating...                           \\r\" % (epoch, max_epochs), end='')\n    dev_loss = validate(model, dev_instances)\n    elapsed_time = time() - start_time\n    elapsed_time = timedelta(seconds=int(elapsed_time))\n    print(\"Epoch: %d\/%d | dev_loss=%.5f | %s                      \" \\\n          % (epoch, max_epochs, dev_loss, elapsed_time))\n    \n    # Track best checkpoint and earlystop patience\n    if dev_loss < best_dev_loss:\n        patience = 0\n        best_dev_loss = dev_loss\n        best_state_dict = deepcopy(model.state_dict())\n        if save_model_path is not None:\n            model.save_pretrained(save_model_path)\n    else:\n        patience += 1\n    \n    if patience > max_patience:\n        print('Earlystop at epoch %d' % epoch)\n        break\n        \n# Restore parameters with best loss on development set\nmodel.load_state_dict(best_state_dict)","e72ffe25":"class TestingDataset(Dataset):\n    def __init__(self, instances):\n        self.instances = instances\n    \n    def __len__(self):\n        return len(self.instances)\n        \n    def __getitem__(self, i):\n        instance = self.instances[i]\n        input_ids = instance['input_ids']\n        attention_mask = instance['attention_mask']\n        token_type_ids = instance['token_type_ids']\n        input_ids = torch.LongTensor(input_ids)\n        attention_mask = torch.FloatTensor(attention_mask)\n        token_type_ids = torch.LongTensor(token_type_ids)\n        return input_ids, attention_mask, token_type_ids, \n    \ndef get_test_dataloader(instances, batch_size=8, num_workers=4):\n    def collate_fn(batch):\n        input_ids, attention_mask, token_type_ids = zip(*batch)\n        input_ids = pad_sequence(input_ids, batch_first=True).unsqueeze(1)  # predict as single choice\n        attention_mask = pad_sequence(attention_mask, batch_first=True).unsqueeze(1)\n        token_type_ids = pad_sequence(token_type_ids, batch_first=True).unsqueeze(1)\n        return input_ids, attention_mask, token_type_ids\n    \n    dataset = TestingDataset(instances)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=False, \\\n                            batch_size=batch_size, num_workers=num_workers)\n    return dataloader","bcf23586":"def predict_query_doc_scores(model, df):\n    model.eval()\n    start_time = time()\n\n    # Parse CSV\n    query_id_list = df[\"query_id\"]\n    query_text_list = df[\"query_text\"]\n    bm25_top1000_list = df[\"bm25_top1000\"]\n\n    # Treat {1 query, K documents} as a dataset for prediction\n    query_doc_scores = []\n    query_doc_ids = []\n    rows = zip(query_id_list, query_text_list, bm25_top1000_list)\n    for qi, row in enumerate(rows, start=1):\n        query_id, query_text, bm25_top1000 = row\n        bm25_doc_id_list = bm25_top1000.split()\n        query_doc_ids.append(bm25_doc_id_list)\n\n        #################################################\n        #    Collect all instances of query\/doc pairs\n        #################################################\n        query_instances = []\n\n        # Make query tokens for BERT\n        query_tokens = tokenizer.tokenize(query_text)\n        if len(query_tokens) > max_query_length:  # truncation\n            query_tokens = query_tokens[:max_query_length]\n        query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n        query_token_ids.insert(0, tokenizer.cls_token_id)\n        query_token_ids.append(tokenizer.sep_token_id)\n\n        # Make input instances for all query\/doc pairs\n        for i, doc_id in enumerate(bm25_doc_id_list, start=1):\n            if doc_id in doc_id_to_token_ids:\n                doc_token_ids = doc_id_to_token_ids[doc_id]\n            else:\n                doc_text = doc_id_to_text[doc_id]\n                doc_tokens = tokenizer.tokenize(doc_text)\n                doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\n                doc_id_to_token_ids[doc_id] = doc_token_ids\n            doc_token_ids.append(tokenizer.sep_token_id)\n\n            # make input sequences for BERT\n            input_ids = query_token_ids + doc_token_ids\n            token_type_ids = [0 for token_id in query_token_ids]\n            token_type_ids.extend(1 for token_id in doc_token_ids)\n            if len(input_ids) > max_input_length:  # truncation\n                input_ids = input_ids[:max_input_length]\n                token_type_ids = token_type_ids[:max_input_length]\n            attention_mask = [1 for token_id in input_ids]\n\n            # convert and collect inputs as tensors\n            input_ids = torch.LongTensor(input_ids)\n            attention_mask = torch.FloatTensor(attention_mask)\n            token_type_ids = torch.LongTensor(token_type_ids)\n\n\n            # collect all inputs as a dictionary\n            instance = {}\n            instance['input_ids'] = input_ids\n            instance['attention_mask'] = attention_mask\n            instance['token_type_ids'] = token_type_ids\n            query_instances.append(instance)\n\n        #################################################################\n        #    Predict relevance scores for all BM25-top-1000 documents\n        #################################################################\n        doc_scores = np.empty((0,1))\n\n        # Predict scores for each document\n        dataloader = get_test_dataloader(query_instances, batch_size=batch_size*(num_negatives+1), num_workers=num_workers)\n        for di, batch in enumerate(dataloader, start=1):\n            batch = (tensor.cuda() for tensor in batch)\n            input_ids, attention_mask, token_type_ids = batch\n            with torch.no_grad():\n                scores = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n\n            # merge all scores into a big numpy array\n            scores = scores.cpu().numpy()\n            doc_scores = np.vstack((doc_scores, scores))\n\n            # Progress bar with timer ;-)\n            elapsed_time = time() - start_time\n            elapsed_time = timedelta(seconds=int(elapsed_time))\n            print(\"Query: %d\/%d | Progress: %d\/%d | %s      \\r\" \\\n                  % (qi, len(df), di, len(dataloader), elapsed_time), end='')\n\n        # merge all query\/BM25 document pair scores\n        query_doc_scores.append(doc_scores)\n    query_doc_scores = np.hstack(query_doc_scores).T\n\n    print()  # new line fomr previous \\r :-U\n    return query_doc_scores, query_doc_ids","8c1e391b":"dev_query_doc_scores, dev_query_doc_ids = predict_query_doc_scores(model, dev_df)\n\nprint('---- Grid search weight for \"BM25 + weight * BERT\" ----')\nbest_map_score, best_bert_weight = -100, 0.0\nbert_scores = dev_query_doc_scores\nn_query = dev_query_doc_scores.shape[0]\n\n# Get MAP@K of BM25 baseline\nquery_pos_doc_ids = dev_df['pos_doc_ids'].values.tolist()\nactual = [doc_ids.split() for doc_ids in query_pos_doc_ids]\nbm25_predicted = [doc_id_list[:K] for doc_id_list in dev_query_doc_ids]\nmap_score = mapk(actual, bm25_predicted, k=K)\nbest_map_score = map_score\nprint(\"weight=%.1f: %.5f  (BM25 baseline)\" % (0, 100*map_score))\n\n# Collect BM25 scores into same format of BERT scores\nbm25_scores = [scores.split() for scores in dev_df[\"bm25_top1000_scores\"]]  # parse into 2d list of string\nbm25_scores = [[float(score) for score in scores] for scores in bm25_scores]  # convert to float\nbm25_scores = np.array(bm25_scores)\n\n# Grid search for BM25 + BERT rescoring\nlow_bound, high_bound, scale = 0, 5, 1000\ngrids = [i \/ scale for i in range(low_bound * scale+1, high_bound * scale+1)]\nfor weight in grids:\n    weighted_scores = bm25_scores + weight * bert_scores\n    rescore_argsort = np.flip(weighted_scores.argsort(), axis=1)\n    predicted = []\n    for i in range(n_query):  # num. of queries\n        predicted.append([dev_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K])\n    map_score = mapk(actual, predicted, k=K)\n    \n    # show part of results for human evaluation\n    if weight * 10 % 2 == 0:\n        print(\"weight=%.1f: %.5f\" % (weight, 100*map_score))\n        \n    # track weight with best MAP@10\n    if map_score > best_map_score:\n        best_map_score = map_score\n        best_bert_weight = weight\nprint(\"\\nHighest MAP@%d = %.5f found at weight=%.3f\" % (K, best_map_score, best_bert_weight))","623c063b":"# Predict BERT scores for testing set\ntest_df = pd.read_csv(testing_csv_path)\nquery_id_list = test_df[\"query_id\"]\nn_query = len(query_id_list)\ntest_query_doc_scores, test_query_doc_ids = predict_query_doc_scores(model, test_df)\nbert_scores = test_query_doc_scores","6b2d3430":"# Rescore query\/document score with BM25 + BERT\nbm25_scores = [scores.split() for scores in test_df[\"bm25_top1000_scores\"]]  # parse into 2d list of string\nbm25_scores = [[float(score) for score in scores] for scores in bm25_scores]  # convert to float\nbm25_scores = np.array(bm25_scores)\nweighted_scores = bm25_scores + best_bert_weight * bert_scores\n\n# Rerank document ids with new scores\nrescore_argsort = np.flip(weighted_scores.argsort(), axis=1)\nranked_doc_id_list = []\nfor i in range(n_query):  # num. of queries\n    ranked_doc_id_list.append([test_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K])\nranked_doc_ids = [' '.join(doc_id_list) for doc_id_list in ranked_doc_id_list]\n\n# Save reranked results for submission\ndata = {'query_id': query_id_list, 'ranked_doc_ids': ranked_doc_ids}\nsubmission_df = pd.DataFrame(data)\nsubmission_df.reset_index(drop=True, inplace=True)\nsubmission_df.to_csv(save_submission_path, index=False)\nprint(\"Saved submission file as `%s`\" % save_submission_path)","0e517d4d":"## Preparing","b5bcf99e":"## Initialize and finetune BERT","49eb2be8":"## Find best weight of BERT for BM25 rescoring on training set","edbbd073":"### Define validation function for earlystopping","0d11d40d":"## Settings","0dd708e8":"## Rescore testing set with BERT for submission","dc5de74a":"## Build dataset and dataloader for PyTorch","0a2b9fa6":"## Split a ratio of training set as development set","85931de0":"### Let's train this beeg boy ;-)","d947e548":"## Define function to predict BERT scores","05a44204":"## Build instances for training\/development set","d2d24f99":"# Training","abe00755":"# Testing"}}