{"cell_type":{"1d2d7a08":"code","acb381e6":"code","2b57fff4":"code","46c6fede":"code","f6e00a28":"code","2bbe6bde":"code","d8502926":"code","cfbaa599":"code","39797541":"code","a154f699":"code","a0aef807":"code","5afcd13e":"code","e4fc443a":"code","68c57732":"code","d9ef4a52":"code","3bcfc0bd":"code","7cee39e0":"code","35416994":"code","087b7240":"code","5deb15cd":"code","32c99edd":"code","1f7388c4":"code","514d825c":"code","f918f922":"markdown","e99d0075":"markdown","8e4cdb63":"markdown","81812b1a":"markdown","d45d00b6":"markdown","86c543f2":"markdown","da3a957f":"markdown","bb39a033":"markdown"},"source":{"1d2d7a08":"import numpy as np\nimport pandas as pd\n\nimport sys\nfrom sklearn import preprocessing\nimport collections\nfrom tqdm import tqdm\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport warnings\nwarnings.filterwarnings('ignore')","acb381e6":"from __future__ import unicode_literals, print_function\nfrom spacy.lang.en import English\nimport spacy","2b57fff4":"# needs internet or package as input data\n# pip install textstat","46c6fede":"# not using internet\n\n# Access to textstat files\nsys.path.append(\"..\/input\/textstat\")\nsys.path.append(\"..\/input\/textstat\/textstat-master\")\n\n# Access to pyphen files\nsys.path.append(\"..\/input\/pyphen\")\nsys.path.append(\"..\/input\/pyphen\/Pyphen-master\")","f6e00a28":"import pyphen\nimport textstat","2bbe6bde":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain_df.head()","d8502926":"train_original = train_df[['excerpt','target']]\ntrain_original","cfbaa599":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df","39797541":"train_original.shape","a154f699":"%%time\ndef feat_eng(df):\n    df = df.copy() # .head(3) # head for testing\/debugging\n    \n    df['syllable_count'] = [textstat.syllable_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['lexicon_count'] = [textstat.lexicon_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['sentence_count'] = [textstat.sentence_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['flesch_reading_ease'] = [textstat.flesch_reading_ease(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['flesch_kincaid_grade'] = [textstat.flesch_kincaid_grade(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['gunning_fog'] = [textstat.gunning_fog(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['smog_index'] = [textstat.smog_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['automated_readability_index'] = [textstat.automated_readability_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['coleman_liau_index'] = [textstat.coleman_liau_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    \n    df['linsear_write_formula'] = [textstat.linsear_write_formula(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['linsear_write_formula'] = round(df['linsear_write_formula'], 3)\n    \n    df['dale_chall_readability_score'] = [textstat.dale_chall_readability_score(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['crawford'] = [textstat.crawford(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    \n    df['text_standard'] = [textstat.text_standard(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    le = preprocessing.LabelEncoder()\n    df['text_standard'] = le.fit_transform(df['text_standard']) # categorical feature\n    \n    df['word_count'] = [len(df.excerpt.iloc[i].split(' ')) for i in range(0, len(df))]\n    \n    for row in tqdm(range(0, len(df))):\n        nlp = English()\n        complicated_signals = nlp(':;-&')\n        \n        full_text = []\n        full_text = df.excerpt.iloc[row] # or full_text = df.loc[row, 'excerpt']\n        doc = nlp(full_text)\n        words_ns = [token.text for token in doc if not token.is_stop and not token.is_punct] # not stopwords & not punct\n        df.loc[row, 'word_count_ns'] = len(words_ns)\n        \n        punct_count = [token.text for token in doc if token.is_punct] \n        df.loc[row, 'punctuation_count'] = len(punct_count)\n        \n        complicated = [token.text for token in doc if token in complicated_signals]\n        df.loc[row, 'complicated_signals'] = len(complicated)\n        \n        df.loc[row, 'vocab_len'] = len(doc.vocab)\n        \n        \n        # POS: Parts of Speech\n        all_tags_in_a_row = []\n        nlp = spacy.load('en_core_web_sm') # load model\n        doc = nlp(df.loc[row, 'excerpt'])\n        all_tags_in_a_row.append([token.pos_ for token in doc]) # list with tags (POS)\n        row_dict = collections.Counter(all_tags_in_a_row[0]) # Counter object\n#         print(row_dict)\n\n        # create columns accessing Counter object\n        df.loc[row, 'n_ADJ'] = row_dict['ADJ']\n        df.loc[row, 'n_ADP'] = row_dict['ADP']\n        df.loc[row, 'n_ADV'] = row_dict['ADV']\n        df.loc[row, 'n_AUX'] = row_dict['AUX']\n        df.loc[row, 'n_CCONJ'] = row_dict['CCONJ']\n        df.loc[row, 'n_DET'] = row_dict['DET']\n        df.loc[row, 'n_INTJ'] = row_dict['INTJ']\n        df.loc[row, 'n_NOUN'] = row_dict['NOUN']\n        df.loc[row, 'n_NUM'] = row_dict['NUM']\n        df.loc[row, 'n_PART'] = row_dict['PART']\n        df.loc[row, 'n_PRON'] = row_dict['PRON']\n        df.loc[row, 'n_PROPN'] = row_dict['PROPN']\n#         df.loc[row, 'n_PUNCT'] = row_dict['PUNCT'] # same as 'punctuation_count' column\n        df.loc[row, 'n_SCONJ'] = row_dict['SCONJ']\n        df.loc[row, 'n_VERB'] = row_dict['VERB']\n        \n        \n        # sentences\n        nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n        sentences = [sent.string.strip() for sent in doc.sents]\n        df.loc[row, 'n_sentences'] = len(sentences)\n        \n        df.loc[row, 'avg_words_per_sentence'] = df.loc[row, 'word_count']\/len(sentences)\n        df['avg_words_per_sentence'] = round(df['avg_words_per_sentence'], 2)\n        df.loc[row, 'avg_words_ns_per_sentence'] = df.loc[row, 'word_count_ns']\/len(sentences)\n        df['avg_words_ns_per_sentence'] = round(df['avg_words_ns_per_sentence'], 2)\n        \n        \n        for sentence in sentences:\n            flesch_list = []\n            flesch_list.append(textstat.flesch_reading_ease(sentence))\n            df.loc[row, 'max_flesch_per_sentence'] = max(flesch_list)\n            df.loc[row, 'min_flesch_per_sentence'] = min(flesch_list)\n            \n            flesch_kincaid_list = []\n            flesch_kincaid_list.append(textstat.flesch_kincaid_grade(sentence))\n            df.loc[row, 'max_flesch_kincaid_per_sentence'] = max(flesch_kincaid_list)\n            df.loc[row, 'min_flesch_kincaid_per_sentence'] = min(flesch_kincaid_list)            \n            \n        \n\n    \n    df['percentage_stopwords'] = round(100*(df['word_count'] - df['word_count_ns'])\/df['word_count'], 2)\n    \n\n\n    ### add more features?\n    # percentage of verbs, nouns...?\n    # max ADJ per sentence...\n    # avg, max, min words\n    # max\/min flesch (ok)\n    # max\/min other textstat features    \n    \n    return df\n    \n\n#### Uncomment these 3 lines below to generate the features\n# train = feat_eng(train_original)\n# train.to_csv('.\/train_features.csv', index=False) # export train features to csv file\n# train.shape\n\n#### ~39 min to generate train features","a0aef807":"# reading train features created in previous run (saves ~40 min)\ntrain = pd.read_csv('..\/input\/train-features-40\/train_features_40.csv')","5afcd13e":"train.columns","e4fc443a":"train\n# train[['min_flesch_per_sentence', 'min_flesch_kincaid_per_sentence']]\n# train[['punctuation_count', 'n_PUNCT']]","68c57732":"features = [i for i in train.columns if i not in ['excerpt', 'target']]\nlen(features)","d9ef4a52":"train.groupby(['text_standard'])['target'].count() # imbalance?? still need to check these categories...","3bcfc0bd":"X_train, X_test, y_train, y_test = train_test_split(train[features], train['target'], test_size=0.1, random_state=42)","7cee39e0":"## check data types\n# train.info()","35416994":"%%time\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state = 42)\n\nxgb_reg.fit(X_train, y_train)\n\npreds = xgb_reg.predict(X_test)\n\nrmse = np.sqrt(mse(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","087b7240":"%%time\nxgb_reg.fit(train[features], train['target'])\n\ntest = feat_eng(test_df)\ntest_pred = xgb_reg.predict(test[features])\ntest_pred","5deb15cd":"test.shape, test_pred.shape","32c99edd":"from xgboost import plot_importance\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (22, 16)\nplot_importance(xgb_reg)\nplt.show()","1f7388c4":"xgb_reg.feature_importances_","514d825c":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)\npredictions","f918f922":"# Imports","e99d0075":"[Textstat package](https:\/\/pypi.org\/project\/textstat\/)","8e4cdb63":"# Test data","81812b1a":"# Feature engineering","d45d00b6":"# XGB Regressor","86c543f2":"# Submission","da3a957f":"# About this notebook\n\n- Feature engineering: 40 features created, most of them using Textstat and Spacy\n- XGB train\n- Feature importance analysis\n- Submission","bb39a033":"# Train data"}}