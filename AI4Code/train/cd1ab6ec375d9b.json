{"cell_type":{"93a54191":"code","ee6f500f":"code","a7899f3e":"code","660c5aa0":"code","b75cf312":"code","6ed4c348":"code","60fe7e57":"code","5ef72e8d":"code","1d2fd819":"code","b7e86327":"code","93ff5874":"code","09bda378":"code","794d709f":"code","33a2dd6a":"code","b399aee1":"code","6fdc3f5b":"code","8187bed0":"code","f12d1e84":"code","9b96c3ce":"code","a0eda000":"code","d7da37ba":"code","32b7e8b8":"code","a7e323cb":"code","018f0ca3":"code","3816343b":"code","181c1c67":"code","fec4e17d":"code","60d1fe15":"code","0a68fb5b":"code","6de2e7ff":"code","9a0e2783":"code","d63ed1ef":"code","681fb490":"code","9e424bc3":"markdown","1ddc4f44":"markdown","fe01b992":"markdown","d1e1f703":"markdown","02fb9480":"markdown","7468ff47":"markdown","ecceca57":"markdown","11e57c14":"markdown","180d7e31":"markdown","956695e8":"markdown","49a6ec85":"markdown","ba801022":"markdown","d396646c":"markdown","5ef36448":"markdown"},"source":{"93a54191":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ee6f500f":"data = pd.read_csv('\/kaggle\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv')\ndata = data.drop(['Unnamed: 0'], axis=1)## This was the original index column, there are a few missing entries which cause some issues when attempting to call a row by index, so I chose to reindex\ndata.tail()","a7899f3e":"crit_risk = data['Critical Risk'].value_counts()\ncrit_risk","660c5aa0":"## Changing the string representation of incident level to its integer value for easier plotting and comparison \nlevel_map = {'I': 1, 'II': 2,'III': 3 , 'IV' : 4, 'V': 5, 'VI' : 6}\ndata['Accident Level'] = pd.Series([level_map[x] for x in data['Accident Level']], index=data.index)\ndata['Potential Accident Level'] = pd.Series([level_map[x] for x in data['Potential Accident Level']], index=data.index)\ndata['Risk_differnce'] = data['Potential Accident Level'] - data['Accident Level']","b75cf312":"import matplotlib.pyplot as plt\nimport seaborn as sns","6ed4c348":"sns.countplot(data['Accident Level'])","60fe7e57":"sns.countplot(data['Risk_differnce'])","5ef72e8d":"acc_level = data['Accident Level'].value_counts()\nacc_level","1d2fd819":"risk_delta = data['Risk_differnce'].value_counts()\nrisk_delta","b7e86327":"potential_acc_level = data['Potential Accident Level'].value_counts()\npotential_acc_level","93ff5874":"# This shows us the average value for accident level\n# Notice that Accident level and Risk difference share their mean, standard deviation, 25%, 50%, and 75% values\ndata.describe()","09bda378":"## The first plot below shows the distribution of incidents\n## The blue line represents the Severity of the accident\n## the yellow line represents the potential severity\n## The second plot we added a line to indicate the difference in risk \n\n\nplt.figure(figsize=(10,8))\nsns.lineplot(y=acc_level, x=acc_level.index)\nsns.lineplot(y=potential_acc_level, x=potential_acc_level.index)\nplt.legend(labels=['Accident Level', 'Potential Accident Level'])\n\n\nplt.figure(figsize=(10,8))\nsns.lineplot(y=acc_level, x=acc_level.index)\nsns.lineplot(y=potential_acc_level, x=potential_acc_level.index)\nsns.lineplot(y=risk_delta, x=risk_delta.index)\nplt.legend(labels=['Accident Level', 'Potential Accident Level', 'Risk Delta'])","794d709f":"print('Incedents by possible factor: Countries')\ndisplay(data['Countries'].value_counts())\n\n\nfig, ax =plt.subplots(1,3)\nax[0].set_title('Country_01')\nsns.countplot(data['Accident Level'].loc[data['Countries'] == 'Country_01'], ax=ax[0])\nax[1].set_title('Country_02')\nsns.countplot(data['Accident Level'].loc[data['Countries'] == 'Country_02'], ax=ax[1])\nax[2].set_title('Country_03')\nsns.countplot(data['Accident Level'].loc[data['Countries'] == 'Country_02'], ax=ax[2])","33a2dd6a":"print('Incedents by possible factor: Local')\ndisplay(data['Local'].value_counts())\n\ndisplay(data.loc[data['Local'] == 'Local_09'])\ndisplay(data.loc[data['Local'] == 'Local_11'])\n\nprint(data.Description.iloc[119])\nprint('###')\nprint(data.Description.iloc[212])","b399aee1":"print('Incedents by possible factor: Industry Sector')\ndisplay(data['Industry Sector'].value_counts())\n\n\nfig, ax =plt.subplots(1,3)\nax[0].set_title('Mining')\nsns.countplot(data['Accident Level'].loc[data['Industry Sector'] == 'Mining'], ax=ax[0])\nax[1].set_title('Metals')\nsns.countplot(data['Accident Level'].loc[data['Industry Sector'] == 'Metals'], ax=ax[1])\nax[2].set_title('Others')\nsns.countplot(data['Accident Level'].loc[data['Industry Sector'] == 'Others'], ax=ax[2])","6fdc3f5b":"print('Incedents by possible factor: Genre')\ndisplay(data['Genre'].value_counts())\n\nfig, ax =plt.subplots(1,2)\nax[0].set_title('Male')\nsns.countplot(data['Accident Level'].loc[data['Genre'] == 'Male'], ax=ax[0])\nax[1].set_title('Female')\nsns.countplot(data['Accident Level'].loc[data['Genre'] == 'Female'], ax=ax[1])","8187bed0":"print('Incedents by possible factor: Employee or Third Party')\ndisplay(data['Employee or Third Party'].value_counts())\n\nfig, ax =plt.subplots(1,3)\nax[0].set_title('Third Party')\nsns.countplot(data['Accident Level'].loc[data['Employee or Third Party'] == 'Third Party'], ax=ax[0])\nax[1].set_title('Employee')\nsns.countplot(data['Accident Level'].loc[data['Employee or Third Party'] == 'Employee'], ax=ax[1])\nax[2].set_title('Third Party (Remote)')\nsns.countplot(data['Accident Level'].loc[data['Employee or Third Party'] == 'Third Party (Remote)'], ax=ax[2])","f12d1e84":"import nltk\nfrom nltk.util import ngrams\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\nimport re","9b96c3ce":"# This creates one long string to perform n-gram operations on\nsuper_string = data['Description'].sum()\nsuper_string = re.sub(r'[^\\w\\s]','',super_string)\nsuper_string = super_string.split(' ')\nstop_words = set(nltk.corpus.stopwords.words('english'))\nsuper_string = [word for word in super_string if word not in stop_words]\nlen(super_string)","a0eda000":"# Notice how frequently hands are mentioned\nword_fd = nltk.FreqDist(super_string)\nbigram_fd = nltk.FreqDist(nltk.bigrams(super_string))\n\nbigram_fd.most_common(10)","d7da37ba":"bigram_fd = nltk.FreqDist(nltk.trigrams(super_string))\n\nbigram_fd.most_common(10)","32b7e8b8":"bigram_fd = nltk.FreqDist(nltk.ngrams(super_string,7))\n\nbigram_fd.most_common(9)","a7e323cb":"# single word counts, note how frequent the word 'hand' occurs.\nbigram_fd = nltk.FreqDist(nltk.ngrams(super_string,1))\n\nbigram_fd.most_common(9)","018f0ca3":"# Attempt with skip-grams, did not find anything signifigant\n\nbigram_fd = nltk.FreqDist(nltk.skipgrams(super_string, n=4, k=3))\n\nbigram_fd.most_common(9)\n","3816343b":"crit_acc = data.loc[data['Accident Level'] >= 4]\ncrit_acc.shape","181c1c67":"super_hr_string = crit_acc['Description'].sum()\nsuper_hr_string = re.sub(r'[^\\w\\s]','',super_hr_string)\nsuper_hr_string = super_hr_string.split(' ')\nsuper_hr_string = [word for word in super_hr_string if word not in stop_words]\nlen(super_hr_string)","fec4e17d":"bigram_fd = nltk.FreqDist(nltk.ngrams(super_hr_string,1))\n\nbigram_fd.most_common(10)","60d1fe15":"bigram_fd = nltk.FreqDist(nltk.ngrams(super_hr_string,2))\n\nbigram_fd.most_common(10)","0a68fb5b":"print(data.Description.iloc[393])","6de2e7ff":"print(data.Description.iloc[407])","9a0e2783":"print(data.Description.iloc[383])","d63ed1ef":"## Now to generate the graphic that we started with\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","681fb490":"text = \" \".join(super_string)\nmask = np.array(Image.open(\"\/kaggle\/input\/caution-hands\/Hand_crush_grey_3.png\"))\nwordcloud_hands = WordCloud(width=1200,height=1200, prefer_horizontal=0.5,scale=2,colormap='Reds',\n                            collocations=False, background_color='white', mode=\"RGB\", min_font_size=8,\n                            max_words=2500, mask=mask, contour_width=3,  contour_color='yellow').generate(text)\n\nplt.figure(figsize=[20,20])\nplt.imshow(wordcloud_hands,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","9e424bc3":"# N-Grams\n\nN-grams are a tool in nlp for finding sets of commonly co-occuring words, where *n* refers to the number of consecutive occuring words. for instance *'near', 'drainage', 'pump'* would be a 3 word n-gram or trigram. ","1ddc4f44":" ### Theme 3: Excess force\n\nApplying excess force to dislodge something or move a part that is stiff","fe01b992":"# Notes on Critical Risk:\n\nMore than half the incidents fall under the category 'Others'. A possible explanation might be that the entries for this database were entered through a form or program that had a limited number of valid inputs, so 'Others' was the most valid bin for the incident. It could also be that these entries were changed to 'Others' as part of the anonymization process. Or it also could be that the details were lost in translation. ","d1e1f703":"# Features\n\nThe section below is a description of the dataset which I copied directly from the Kaggle page. The data was collected from 12 different plants in 3 different countries.\n\n    Columns description\n        Data: timestamp or time\/date information\n        Countries: which country the accident occurred (anonymized)\n        Local: the city where the manufacturing plant is located (anonymized)\n        Industry sector: which sector the plant belongs to\n        Accident level: from I to VI, it registers how severe was the accident (I means not severe but VI means very severe)\n        Potential Accident Level: Depending on the Accident Level, the database also registers how severe the accident could have been (due to other factors involved in the accident)\n        Genre: if the person is male of female\n        Employee or Third Party: if the injured person is an employee or a third party\n        Critical Risk: some description of the risk involved in the accident\n        Description: Detailed description of how the accident happened.\n\nIt is worth noting that components of this dataset were anonymized to hide the location and name of each facility.","02fb9480":"### Theme 2: Using hands to secure something that should be secured with a more appropriate tool\n\nThere were many instances where hands were caught in moving parts that should have had a more appropriate tool to secure them.","7468ff47":"### Theme 1: Hands near moving parts\n\nNot being mindful of where one's hands are in relation to moving parts ","ecceca57":"# Notes from n-grams\n\nA frequent occurrence in the n-grams are references to hands. In fact, hand is the fourth most common word in the dataset behind left, right, and causing.\n\nTo get a better feel for the data I read through about a quarter of the entries in the dataset. Hand injuries are one of the most common stories in the data set. Three themes emerge when reading through the incident descriptions.","11e57c14":"# Task: Main cause of accidents\n\nThe goal of this notebook is to explore the leading causes of accidents. This notebook features N-grams and excerpts from accident descriptions, the examples displayed won't be graphic.\n\nThanks to:\n* IHM Stefanini for the data set https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database\n* Tinotenda Mhlanga for the task https:\/\/www.kaggle.com\/tinotendamhlanga\n* Robert L. Lorber and Gregory M. Anderson, https:\/\/safety247.org\/ , authors of the book safety 24\/7 which inspired more than a few of the points brought up in this notebook ","180d7e31":"![week_9_task.png](attachment:week_9_task.png)\n\n### TLDR\n\nThe conclusion reached by this notebook is that the largest cause of incidents in this data set is Non-Safety Conscious Hand Operations. While we chose this as the leading cause of incidents, there were many other factors. See below for the full exploratory data analysis and the other conclusions we reached.","956695e8":"### Observations: country\n\nNotice above that country 1 has more incidents than the other two countries combined. By itself, this isn't necessarily significant. For instance Country 1 could have twice as many workers as either of the other two groups. However, in country 1 the rate of level 4 incidents are much higher (23 incidents in 251, where there are only 7 in the other 174). To get a good birds eye view of the ratios of incident severities we are using Seaborn\u2019s countplot function, keep in mind that each plot isn't using the same measurement for each tick mark. ","49a6ec85":"# Population dependant features:\n\nThe catagories Countries, Local, Industry Sector, Genre(Gender), and Employee or Third Party can provide some insight into incedent risk. Without seeing the ratio of population size to incedent occurances it can lead to some false conclusions. ","ba801022":"# Final Thoughts:\n<br \/>Through a combination of basic NLP and rooting around in the data a reasonable case that Non-Safety Conscious Hand Operations are the main cause of incidents. By being mindful of moving parts, using the appropriate tool for the job, and reframing from using excess force you can reduce the amount of incidents that occur. To paraphrase from safety 24\/7 \"Before performing a task that has risk, take a moment to consider the risk and what you can do to stay safe\" (couldn't find the exact quote online and I no longer have the book).\n### All accidents and injuries are preventable.\n\nWhen exploring the data, particularly n-grams, many alternate causes popped up that warrant further exploration, and could even contain a greater cause of incidents than what we found. Given that this notebook is part of an ongoing project I've titled \"task of the week '' there may be more to find in the dataset outside what we uncovered. Feel free to fork this notebook and dive into the data yourself.\n\nThis year I've challenged myself to complete one task on Kaggle per week, in order to develop a larger Data Science portfolio. If you found this notebook useful or interesting please give it an upvote. I'm always open to constructive feedback. If you have any questions, comments, concerns, or if you would like to collaborate on a future task of the week feel free to leave a comment here or message me directly. For past TOTW check out the link to my page on github for this ongoing project<br \/>\nhttps:\/\/github.com\/Neil-Kloper\/Weekly-Kaggle-Task\/wiki","d396646c":"Notice in the graph above there is a significant difference between the severity of an incident, and the potential severity of the incident. This ties very neatly into a concept from safety 24\/7.\n\n\n# The Safety Pyramid\n![safety24_7.PNG](attachment:safety24_7.PNG)\n\nThe concept is fairly simple. At the bottom of the pyramid you have at-risk behaviors, and as these unsafe behaviors continue the outcome starts to work up the pyramid.<br \/>An example of such a behavior might be speeding, most of the time when someone is driving too fast nothing happens, so they continue to drive too fast. This causes bullet-proof thinking: \"Nothing bad has happened when I did this before, nothing will happen this time\". Given enough time and a near miss will occur, a close call that maybe won't have happened if they were driving more reasonably. Eventually this moves a bit higher up on the pyramid and someone gets hurt.\n\nThe fact that this data was collected and published for the world to view and provide critical feedback on suggests that this organization probably has a strong safety culture (at least at the corperate level), one that doesn't tolerate bullet-proof thinking.","5ef36448":" ### Local 11\n \nSometimes it helps to take a closer look at an outlier, such as Local 11. Notice how there are only 2 entries, one occurred off property and the other was from an employee visiting from another location. This could mean that this location doesn't follow proper reporting procedures, or it could be that they have a very safe workspace, or possibly they only have a staff of one or two people for day to day operations. "}}