{"cell_type":{"5a6d636d":"code","0d7058fe":"code","28ca5ce9":"code","414b62a1":"code","f6a6bf23":"code","50900c44":"code","136f9b27":"code","2f5a6b52":"code","5a123d1b":"code","c88f8ff1":"code","7128edbb":"code","aa4b0fdd":"code","4b5cca7d":"code","2c2ef55b":"code","03620a2d":"code","4bd4817b":"code","3cd8bd41":"code","45edee88":"code","cc711e90":"code","bf199508":"code","c72403c7":"code","9926eacc":"code","5a0ec9fa":"code","baa459e4":"markdown","20377721":"markdown","2cc50d9d":"markdown","2b635621":"markdown","774282c0":"markdown","ec95b265":"markdown","43cb207b":"markdown","16963aca":"markdown","5d418a63":"markdown","476da9cb":"markdown","1d453661":"markdown"},"source":{"5a6d636d":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd  \nimport seaborn as sns \nfrom sklearn.datasets import load_boston\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","0d7058fe":"train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","28ca5ce9":"# found that one of the test data is in the train set and removed it\n# Dropping the last row which is 2011-01-01 00:00:00\ntrain=train.loc[~(train['date_time']=='2011-01-01 00:00:00')].reset_index(drop=True)","414b62a1":"print('train shape:',train.shape)","f6a6bf23":"all_data = pd.concat([train, test])\n# convert to datatime format\nall_data['date_time'] = pd.to_datetime(all_data['date_time'])\nall_data.head()","50900c44":"fig,ax=plt.subplots(4,2,figsize=(20,15))\nfor i,col in enumerate(train.columns[1:9]):\n    ax[i%4][i\/\/4].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%4][i\/\/4].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%4][i\/\/4].set_xlabel(f'{col}')\n    ax[i%4][i\/\/4].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","136f9b27":"fig,ax=plt.subplots(3,1,figsize=(8,10))\nfor i,col in enumerate(train.columns[9:12]):\n    ax[i%3].hist(train[col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%3].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%3].set_xlabel(f'{col}')\n    ax[i%3].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","2f5a6b52":"# \u5b57\u4e32\u8f49\u63db\u65e5\u671f\u683c\u5f0f\ntrain['date_time']=pd.to_datetime(train['date_time'],format='%Y-%m-%d %H:%M:%S')\ntest['date_time']=pd.to_datetime(test['date_time'],format='%Y-%m-%d %H:%M:%S')\n# Following code is inspired from - https:\/\/www.kaggle.com\/nroman\/eda-for-ashrae\nfig,ax=plt.subplots(1,1,figsize=(12,6))\ntrain[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","5a123d1b":"fig,ax=plt.subplots(1,1,figsize=(12,6))\ntest[['date_time','deg_C']].set_index('date_time').resample('D').mean()['deg_C'].plot(ax=ax,label='by hour(train)',alpha=1,color='blue').set_ylabel('deg C',fontsize=10)\nax.set_title('Trend of Mean deg_C by Day',fontsize=12)\nax.set_xlabel('')","c88f8ff1":"all_data.isnull().sum()","7128edbb":"# The months will be used for folds split\nmonths = all_data[\"date_time\"].dt.month[:len(train)]\n## New idea\nall_data[\"hour\"] = all_data[\"date_time\"].dt.hour\nall_data[\"working_hours\"] =  all_data[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nall_data[\"is_weekend\"] = (all_data[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nall_data.drop(columns = 'hour', inplace = True)\n# convert datetime to timestamp(s)\nall_data['time'] = all_data['date_time'].astype(np.int64)\/\/10**9\nall_data.drop(columns = 'date_time', inplace = True)","aa4b0fdd":"X=all_data[:len(train)].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\ny=all_data[:len(train)][['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\ny_log=np.log1p(y)\nX_test=all_data[len(train):].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\nprint('X_train shape:', X.shape)\nprint('y_train shape:', y.shape)\nprint('X_test shape:', X_test.shape)","4b5cca7d":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)\nX_scaled = scaler.transform(X)\nX_test_scaled = scaler.transform(X_test)","2c2ef55b":"fig,ax=plt.subplots(4,2,figsize=(20,15))\nfor i in range(8):\n    col=train.columns[1:9][i]\n    ax[i%4][i\/\/4].hist(X_test_scaled[:,i],bins=40,color='darkblue',label=f'{col}')\n    ax[i%4][i\/\/4].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%4][i\/\/4].set_xlabel(f'{col}')\n    ax[i%4][i\/\/4].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","03620a2d":"fig,ax=plt.subplots(3,1,figsize=(8,10))\nfor i in range(3):\n    col=train.columns[9:12][i]\n    ax[i%3].hist(y_log[:][col],bins=40,color='darkblue',label=f'{col}')\n    ax[i%3].set_title(f'Distribution of {col}',fontsize=15)\n    ax[i%3].set_xlabel(f'{col}')\n    ax[i%3].set_ylabel('Dist')\n    plt.subplots_adjust(hspace=0.45)","4bd4817b":"# load submission\npreds = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","3cd8bd41":"# %%time\n# from sklearn.metrics import mean_squared_log_error\n# from sklearn.model_selection import StratifiedKFold\n# import xgboost as xgb\n\n# all_fi = []\n# splits = 10\n\n# for i, target in enumerate(y_log.columns):\n#     print(f\"\\nTraining for {target}...\")\n#     skf = StratifiedKFold(n_splits=splits, shuffle = True, random_state = 42)\n#     oof_preds = np.zeros((X_scaled.shape[0],))\n#     model_preds = 0\n#     model_fi = 0\n#     for num, (train_idx, valid_idx) in enumerate(skf.split(X_scaled, months)):\n#         X_train, X_valid = X_scaled[[train_idx]], X_scaled[[valid_idx]]\n#         y_train, y_valid = y_log.loc[train_idx, target], y_log.loc[valid_idx, target]\n#         model = xgb.XGBRegressor(max_depth=6,\n#                        n_estimators=10000,\n#                        random_state=42)\n#         model.fit(X_train, y_train,\n#                   eval_set=[(X_valid, y_valid)],\n#                   verbose=False,early_stopping_rounds=500,)\n#         model_preds += np.expm1(model.predict(X_test_scaled)) \/ splits\n#         model_fi += model.feature_importances_\n#         oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n#         print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n#     print(f\"\\nOverall RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_log[target]), oof_preds))}\")    \n#     preds[target] = model_preds\n#     all_fi.append(model_fi)","45edee88":"%%time\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import LeaveOneGroupOut\nimport xgboost as xgb\n\nall_fi = []\nsplits = 10\ntarget_names=y_log.columns\n\nfor i, target in enumerate(target_names):\n    print(f\"\\nTraining for {target}...\")\n    logo = LeaveOneGroupOut()\n    oof_preds = np.zeros((X_scaled.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(logo.split(X_scaled, y_log, months)):\n        X_train, X_valid = X_scaled[[train_idx]], X_scaled[[valid_idx]]\n        y_train, y_valid = y_log.loc[train_idx, target], y_log.loc[valid_idx, target]\n        model = xgb.XGBRegressor(max_depth=6,\n                       n_estimators=10000)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  verbose=False,early_stopping_rounds=500,)\n        model_preds += np.expm1(model.predict(X_test_scaled)) \/ splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n        print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n    print(f\"\\nOverall RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_log[target]), oof_preds))}\")    \n    preds[target] = model_preds\n    all_fi.append(dict(zip(X.columns, model_fi)))","cc711e90":"# Creating feature list from feature importance dictionaries\nfeature_list = set()\nfor i in np.arange(len(all_fi)):\n    feature_list = set.union(feature_list, set(all_fi[i].keys()))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","bf199508":"# Combining feature importances of different models into one dataframe\ndf = pd.DataFrame(columns=[\"Feature\"])\ndf[\"Feature\"] = list(feature_list)\nfor i in np.arange(len(all_fi)):\n    for key in all_fi[i].keys():\n        df.loc[df[\"Feature\"] == key, \"Importance_\" + str(i+1)] = all_fi[i][key] \/ 1000\ndf.fillna(0, inplace=True)\ndf.sort_values(\"Importance_1\", axis=0, ascending=False, inplace=True)","c72403c7":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 9))\nbars1 = ax.barh(x-height, df[\"Importance_1\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Importance_2\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Importance_3\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\nax.legend(fontsize=13, loc=\"lower right\")\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","9926eacc":"preds.head()","5a0ec9fa":"preds.to_csv('submission.csv', index=False)","baa459e4":"## Save Predict File","20377721":"## StratifiedKFold\nPublic Score: 0.22277","2cc50d9d":"## LeaveOneGroupOut\nPublic Score: 0.21989\n\ntrain Score:0.198500193","2b635621":"### \u5206\u6790\u6bcf\u5929\u7684\u8cc7\u8a0a\nLooking at the day wise trend,we see that there has been sudden peak and dips for certain days over the month.While the temperatures have been above 20 deg after late may, there is a dip in temperature less than 15 dec after Nov but there is a sudden increase in mid december.","774282c0":"## Train Model","ec95b265":"For the test set, if we try to compare between March month of train, we could see the temperatures have started from approx 3 deg and increased above 15 deg.","43cb207b":"## Data preprocessing\n\u9032\u4e00\u6b65\u8655\u7406\u4e4b\u524d\u5148\u78ba\u8a8d\u662f\u5426\u6709\u7f3a\u5931\u503c\uff1a","16963aca":"## Feature importances\nThanks to: @Maxim Kazantsev https:\/\/www.kaggle.com\/maximkazantsev\/tps-07-21-eda-catboost","5d418a63":"## EDA\n- The distribution of deg_C shows peaks between 20 to 30 deg.\n- There is a dip in relative humidity at 40% and there are two peaks at 30% and 45% approx.\n- The absolute humidity value shows peaks at 0.25g\/m3(i have assumed it to be g\/m3.Data info did not explicitly mention any units).\n- The distribution of sensor_1,2,3 & 5 appears to be left skewed whereas sensor-4 is normal with outliers at 500.","476da9cb":"## XGBoost_11Feature_LeaveOneGroupOut","1d453661":"### Standardization \u5e73\u5747&\u8b8a\u7570\u6578\u6a19\u6e96\u5316\n\u5c07\u6240\u6709\u7279\u5fb5\u6a19\u6e96\u5316\uff0c\u4e5f\u5c31\u662f\u9ad8\u65af\u5206\u4f48\u3002\u4f7f\u5f97\u6578\u64da\u7684\u5e73\u5747\u503c\u70ba0\uff0c\u65b9\u5dee\u70ba1\u3002\n\u9069\u5408\u7684\u4f7f\u7528\u6642\u6a5f\u65bc\u7576\u6709\u4e9b\u7279\u5fb5\u7684\u65b9\u5dee\u904e\u5927\u6642\uff0c\u4f7f\u7528\u6a19\u6e96\u5316\u80fd\u5920\u6709\u6548\u5730\u8b93\u6a21\u578b\u5feb\u901f\u6536\u6582\u3002"}}