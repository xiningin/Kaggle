{"cell_type":{"6a7b4579":"code","c1d857ce":"code","91e6be89":"code","72ef2152":"code","64918589":"code","cf09dbec":"code","42aa12b6":"code","b75166b1":"code","f30f1e4e":"code","1b897bc0":"code","fd1e9086":"code","ff3a7aab":"code","e6e31a8d":"code","50a5e3b6":"code","bced1ad2":"code","0a23b1aa":"code","8adbeaf3":"code","972aaf4f":"code","b596f126":"code","089463ec":"code","44bb5a1e":"code","85a0bd81":"code","5e2cc4f4":"code","5feb4508":"code","a0063909":"code","45cb1c97":"code","2c0cc41c":"code","6b2c0af7":"code","e2a98ea7":"code","5e1c570a":"code","01a0f714":"code","9d774dae":"code","53d027ea":"code","b505b0bf":"code","0a94d8eb":"markdown","467bb499":"markdown","3739ea8a":"markdown","085ba484":"markdown","ccece9bc":"markdown","91df3c13":"markdown","d4f4439a":"markdown","094e90f9":"markdown","543354cf":"markdown","84931ca1":"markdown","3c6ac8aa":"markdown","452b3a4e":"markdown","fd13f06a":"markdown","49c31d55":"markdown","26f6d015":"markdown"},"source":{"6a7b4579":"import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport warnings, time, gc\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file\n\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.manifold import TSNE\n\nfrom wordcloud import WordCloud\n\nnp.random.seed(32)\ncolor = sns.color_palette(\"Set2\")\nwarnings.filterwarnings(\"ignore\")\nstop_words = set(stopwords.words(\"english\"))\npunctuations = string.punctuation\noutput_notebook()\n\n%matplotlib inline\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")","c1d857ce":"train.head()","91e6be89":"train.isna().sum()","72ef2152":"target_count = train[\"target\"].value_counts()\n\nplt.figure(figsize = (8, 5))\nax = sns.barplot(target_count.index, target_count.values)\nrects = ax.patches\nlabels = target_count.values\nfor rect, label in zip(rects, labels):\n    ax.text(rect.get_x() + rect.get_width()\/2, rect.get_height() + 5,\n           label, ha = \"center\", va = \"bottom\")\nplt.show()","64918589":"train[\"quest_len\"] = train[\"question_text\"].apply(lambda x: len(x.split()))","cf09dbec":"sincere = train[train[\"target\"] == 0]\ninsincere = train[train[\"target\"] == 1]\n\nplt.figure(figsize = (15, 8))\nsns.distplot(sincere[\"quest_len\"], hist = True, label = \"sincere\")\nsns.distplot(insincere[\"quest_len\"], hist = True, label = \"insincere\")\nplt.legend(fontsize = 10)\nplt.title(\"Questions Length Distribution by Class\", fontsize = 12)\nplt.show()","42aa12b6":"#https:\/\/drive.google.com\/file\/d\/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM\/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","b75166b1":"# Credit: https:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda\n\nlem = WordNetLemmatizer()\ntokenizer = TweetTokenizer()\n\ndef clean_text(question):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    question = question.lower()\n    #remove \\n\n    question = re.sub(\"\\\\n\", \"\", question)\n    #remove disteacting single quotes\n    question = re.sub(\"\\'\", \"\", question)\n    # remove new line characters\n#     question = re.sub('s+', \" \", question)\n    \n    #Split the sentences into words\n    words = tokenizer.tokenize(question)\n    \n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words = [APPO[word] if word in APPO else word for word in words]\n    words = [lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if w not in stop_words and w not in punctuations]\n\n    clean_sent = \" \".join(words)\n    # remove any non alphanum, digit character\n#     clean_sent = re.sub(\"\\W+\", \" \", clean_sent)\n#     clean_sent = re.sub(\"  \", \" \", clean_sent)\n    \n    return clean_sent","f30f1e4e":"sincere[\"clean_question_text\"] = sincere[\"question_text\"].apply(lambda question: clean_text(question))\ninsincere[\"clean_question_text\"] = insincere[\"question_text\"].apply(lambda question: clean_text(question))","1b897bc0":"insincere.head()","fd1e9086":"cv = CountVectorizer(min_df = 10,\n                     max_features = 100000,\n                     analyzer = \"word\",\n                     ngram_range = (1, 2),\n                     stop_words = \"english\",\n                     token_pattern = '[a-zA-Z]')\n\ncount_vectors = cv.fit_transform(insincere[\"clean_question_text\"])","ff3a7aab":"# params = {\"n_components\": [5, 10, 20, 30, 40, 50]}\n\n# lda_model = LatentDirichletAllocation(n_components = n_topics, \n#                                       # we choose a small n_components for time convenient\n#                                       # will find a appropriate n_components later \n#                                       learning_method = \"online\",\n#                                       batch_size = 128,\n#                                       evaluate_every = -1,\n#                                       max_iter = 20,\n#                                       random_state = 32,\n#                                       n_jobs = -1)\n\n# model = GridSearchCV(lda_model, param_grid = params)\n# model.fit(count_vectors)\n\n# best_lda_model = model.best_estimator_\n# best_lda_model","e6e31a8d":"n_topics = 8\nlda_model = LatentDirichletAllocation(n_components = n_topics, \n                                      learning_method = \"online\",\n                                      batch_size = 128,\n                                      evaluate_every = -1,\n                                      max_iter = 20,\n                                      random_state = 32,\n                                      n_jobs = -1)\n\nquestion_topics = lda_model.fit_transform(count_vectors)\ntemp = question_topics","50a5e3b6":"print(\"Log Likelihood: {} \\nPerplexity: {}\".format(lda_model.score(count_vectors), \n                                                   lda_model.perplexity(count_vectors)))","bced1ad2":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda = tsne_model.fit_transform(question_topics)","0a23b1aa":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics\/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(insincere[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df = pd.DataFrame(tsne_lda, columns = [\"x\", \"y\"])\ntsne_lda_df[\"qid\"] = insincere[\"qid\"].values\ntsne_lda_df[\"question\"] = insincere[\"question_text\"].values\ntsne_lda_df[\"topics\"] = lda_keys\ntsne_lda_df[\"topics\"] = tsne_lda_df[\"topics\"].map(int)","8adbeaf3":"import random\n\ndef generate_color():\n    color = \"#{:02x}{:02x}{:02x}\".format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color","972aaf4f":"colormap = np.array([generate_color() for t in range(n_topics)])","b596f126":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df[\"x\"], y = tsne_lda_df[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df[\"qid\"],\n                         question = tsne_lda_df[\"question\"],\n                         topics = tsne_lda_df[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\",\"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","089463ec":"threshold = 0.5\nidx = np.amax(temp, axis = 1) >= threshold\nquestion_topics = temp[idx]","44bb5a1e":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda2 = tsne_model.fit_transform(question_topics)","85a0bd81":"new_insincere = insincere[[\"qid\", \"question_text\"]].copy()\nnew_insincere = new_insincere[idx]","5e2cc4f4":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics\/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(new_insincere[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df2 = pd.DataFrame(tsne_lda2, columns = [\"x\", \"y\"])\ntsne_lda_df2[\"qid\"] = new_insincere[\"qid\"].values\ntsne_lda_df2[\"question\"] = new_insincere[\"question_text\"].values\ntsne_lda_df2[\"topics\"] = lda_keys\ntsne_lda_df2[\"topics\"] = tsne_lda_df2[\"topics\"].map(int)","5feb4508":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df2[\"x\"], y = tsne_lda_df2[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df2[\"qid\"],\n                         question = tsne_lda_df2[\"question\"],\n                         topics = tsne_lda_df2[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","a0063909":"idx = np.amax(temp, axis = 1) < threshold\nquestion_topics = temp[idx]","45cb1c97":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda3 = tsne_model.fit_transform(question_topics)","2c0cc41c":"new_insincere2 = insincere[[\"qid\", \"question_text\"]].copy()\nnew_insincere2 = new_insincere2[idx]","6b2c0af7":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics\/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(new_insincere2[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df3 = pd.DataFrame(tsne_lda3, columns = [\"x\", \"y\"])\ntsne_lda_df3[\"qid\"] = new_insincere2[\"qid\"].values\ntsne_lda_df3[\"question\"] = new_insincere2[\"question_text\"].values\ntsne_lda_df3[\"topics\"] = lda_keys\ntsne_lda_df3[\"topics\"] = tsne_lda_df2[\"topics\"].map(int)","e2a98ea7":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df3[\"x\"], y = tsne_lda_df3[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df3[\"qid\"],\n                         question = tsne_lda_df3[\"question\"],\n                         topics = tsne_lda_df3[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","5e1c570a":"def create_wordcloud(i, data):\n#     plt.subplot(int(\"52{}\".format(ax+1)))\n    wc =  WordCloud(max_words = 1000, stopwords = stop_words)\n    wc.generate(\" \".join(data))\n    ax[int(i\/2)][i%2].axis(\"off\")\n    ax[int(i\/2)][i%2].set_title(\"Words Frequented in Topic {}\".format(i), fontsize = 15)\n    ax[int(i\/2)][i%2].imshow(wc)\n    \nfig, ax = plt.subplots(4, 2, figsize = (25, 25))\nfor i in range(n_topics):\n    text = tsne_lda_df[tsne_lda_df[\"topics\"] == int(i)][\"question\"]\n    create_wordcloud(int(i), text)","01a0f714":"import networkx as nx\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist, squareform\n\ncor = squareform(pdist(tsne_lda2, metric = \"euclidean\"))","9d774dae":"cor = squareform(pdist(tsne_lda2[:100], metric = \"euclidean\"))","53d027ea":"labels = {}\n\nfor l, i in enumerate(tsne_lda_df2[\"qid\"]):\n    labels[l] = i","b505b0bf":"G = nx.Graph()\n\nfor i in range(cor.shape[0]):\n    for j in range(cor.shape[1]):\n        if i == j:\n            G.add_edge(i, j, weight = 0)\n        else:\n            G.add_edge(i, j, weight = 1.0\/cor[i, j])\n            \nG = nx.relabel_nodes(G, labels)\n            \nedges = [(i, j) for i, j, w in G.edges(data = True) if w[\"weight\"] > 0.8]\nedge_weight = dict([((u, v, ), int(d[\"weight\"])) for u, v, d in G.edges(data = True)])\n\npos = nx.spring_layout(G)\n\nplt.figure(figsize = (10, 8))\nnx.draw_networkx_nodes(G, pos, node_size = 100, alpha = 0.5)\nnx.draw_networkx_edges(G, pos, edgelist = edges, width = 1)\nnx.draw_networkx_labels(G, pos, font_size = 8, font_family = \"sans-serif\")\nplt.show()","0a94d8eb":"## Question Length Distribution","467bb499":"## Target Distrinbution","3739ea8a":"#### Topic Probability => 0.5","085ba484":"## Insincere Questions Topic Modeling","ccece9bc":"The objective of this notebook is to discover Quora **insincere** questions' topics, aka target = 1.","91df3c13":"Related paper: [Topic Modeling and Network Visualization to\nExplore Patient Experiences](http:\/\/faculty.washington.edu\/atchen\/pubs\/Chen_Sheble_Eichler_VAHC2013.pdf)","d4f4439a":"## Insincere Topic Wordcloud","094e90f9":"After applying Grid Search, we found the optimial **n_components** is between 5 to 10. In this case, we pick the 'mean' which is 8.","543354cf":"To get a better LDA model, we need to maximize log likelihood and minimize perplexity.","84931ca1":"We get a much better visualization after using probability threshold.","3c6ac8aa":"## Topic Network","452b3a4e":"# To Be Continued ...","fd13f06a":"Although we can see some patterns in the visualization from above, the graph is difficult to interpret.  The very reason for that is our model is unable to confidently assign a topic to every questions. This means that there are questions being assigned a low probability to a probable topic. To filter out such questions, we simply add a threshold factor.","49c31d55":"## Data Cleaning","26f6d015":"#### Topic Probability < 0.5"}}