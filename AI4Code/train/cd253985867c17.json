{"cell_type":{"7a19d573":"code","ecd41f93":"code","5c63db44":"code","dc52e12d":"code","311b9cfa":"code","57e8e303":"code","8b89770d":"code","2c52e969":"code","43c72315":"code","95da2d99":"code","79bb6cdc":"code","0ce73342":"code","116a870a":"code","8547282b":"code","d5dbbfac":"code","0836a558":"code","8a871578":"code","deaec335":"code","975b05ce":"code","ee1a0819":"code","eda3c6d1":"code","058d6e67":"markdown","39fb6f17":"markdown","88180789":"markdown","74d25a07":"markdown","c3bb98df":"markdown","2fce313e":"markdown","adfd178e":"markdown","8d139eeb":"markdown","86517b60":"markdown","43b7de5a":"markdown","8de0295d":"markdown","fa26275a":"markdown","530cfce6":"markdown","3d1198b2":"markdown","bde0f087":"markdown","a13e81dc":"markdown","5a55e9df":"markdown","0d13b5b6":"markdown","973cbeb9":"markdown","8b635cf4":"markdown","18e20ecf":"markdown"},"source":{"7a19d573":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport nltk\nfrom nltk import NaiveBayesClassifier\nfrom nltk.metrics.scores import f_measure, precision, recall\nimport collections\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfor filename in os.listdir(\"..\/input\"):\n    print(filename)\n\n# Any results you write to the current directory are saved as output.","ecd41f93":"import re\nfrom itertools import islice\n\ndef load_tsv(data_file, n):\n    data_features = list()\n    data = list()\n    infile = open(data_file, encoding='utf-8')\n    for line in infile:\n        if not line.strip():\n            continue\n        label, text = line.split('\\t')\n        text_features = process_text(text, n)\n        if text_features:\n            data_features += text_features\n            data.append((text_features, label))\n    return data, data_features\n\ndef process_text(text, n=1,\n                 remove_vowel_marks=False,\n                 remove_repeated_chars=False,\n                 ):\n    clean_text = text\n    if remove_vowel_marks:\n        clean_text = remove_diacritics(clean_text)\n    if remove_repeated_chars:\n        clean_text = remove_repeating_char(clean_text)\n\n    if n == 1:\n        return clean_text.split()\n    else:\n        tokens = clean_text.split()\n        grams = tokens\n        for i in range(2, n + 1):\n            grams += [  ' '.join(g) for g in list(window(tokens, i))  ]\n        return grams\n\n\n\ndef window(words_seq, n):\n    \"\"\"Returns a sliding window (of width n) over data from the iterable\"\"\"\n    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n    it = iter(words_seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n\n\ndef remove_repeating_char(text):\n    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n\ndef document_features(document, corpus_features):\n    document_words = set(document)\n    features = {}\n    for word in corpus_features:\n        features['has({})'.format(word)] = (word in document_words)\n    return features","5c63db44":"pos_train_file = '..\/input\/train_Arabic_tweets_positive_20190413.tsv'\nneg_train_file = '..\/input\/train_Arabic_tweets_negative_20190413.tsv'\n\npos_test_file = '..\/input\/test_Arabic_tweets_positive_20190413.tsv'\nneg_test_file = '..\/input\/test_Arabic_tweets_negative_20190413.tsv'\nprint('data files')\nprint('train file (pos)', pos_train_file)\nprint('train file (neg)', neg_train_file)\nprint('test file (pos)', pos_test_file)\nprint('test file (neg)', neg_test_file)","dc52e12d":"print('parameters')\nn = 1\nprint('n grams:', n)","311b9cfa":"print('loading train data ....')\npos_train_data, pos_train_feat = load_tsv(pos_train_file, n)\nneg_train_data, neg_train_feat = load_tsv(neg_train_file, n)\nprint('loading test data ....')\npos_test_data, pos_test_feat = load_tsv(pos_test_file, n)\nneg_test_data, neg_test_feat = load_tsv(neg_test_file, n)","57e8e303":"print('train data info')\ntrain_data = pos_train_data + neg_train_data\nprint('train data size', len(train_data))\nprint('# of positive', len(pos_train_data))\nprint('# of negative', len(neg_train_data))","8b89770d":"import random\nsample_size = 100\nprint('{} random tweets .... '.format(sample_size))\nfor s in random.sample(train_data, sample_size):\n    print(s)","2c52e969":"print('test data info')\ntest_data = pos_test_data + neg_test_data\nprint('test data size', len(train_data))\nprint('# of positive', len(pos_test_data))\nprint('# of negative', len(neg_test_data))","43c72315":"print('merging all features ... ')\nall_features = pos_train_feat + neg_train_feat + \\\n               pos_test_feat + pos_test_feat\nprint('len(all_features):', len(all_features))","95da2d99":"print('{} sample features ...'.format(sample_size))\nprint(random.sample(all_features, sample_size))","79bb6cdc":"all_features_count = {}\nfor w in all_features:\n    all_features_count[w] = all_features_count.get(w, 0) + 1","0ce73342":"print('sample frequencies')\nprint(random.sample(list(all_features_count.items()), 30))\nword = '\u0641\u064a'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\nword = '\u0641\u0649'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\nword = '\u0645\u0646'\nprint('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))","116a870a":"print('size of training data:',  len(train_data))\nmin_df = int(0.001 * len(train_data))\nmax_df = int(0.98 * len(train_data))\nprint('min document frequency:', min_df)\nprint('max document frequency:', max_df)","8547282b":"# remove features that have frequency below\/above the threshold\nmy_features = set([word for word, freq in all_features_count.items() if  max_df > freq > min_df ])\nprint(len(my_features), 'are kept out of', len(all_features))","d5dbbfac":"print('{} sample of selected features:'.format(sample_size))\nprint(random.sample(list(my_features), sample_size))","0836a558":"feature_sets = [(document_features(d, my_features), c) for (d, c) in train_data]","8a871578":"classifier = nltk.NaiveBayesClassifier.train(feature_sets)\nprint('training is done')","deaec335":"classifier.show_most_informative_features(40)","975b05ce":"test_features = [(document_features(d, my_features), c) for (d, c) in test_data]","ee1a0819":"ref_sets = collections.defaultdict(set)\ntest_sets = collections.defaultdict(set)\n\nfor i, (feats, label) in enumerate(test_features):\n    ref_sets[label].add(i)\n    observed = classifier.classify(feats)\n    test_sets[observed].add(i)","eda3c6d1":"print('accuracy: ', nltk.classify.accuracy(classifier, test_features))\nprint('pos precision: ', precision(ref_sets['pos'], test_sets['pos']))\nprint('pos recall:', recall(ref_sets['pos'], test_sets['pos']))\nprint('neg precision: ', precision(ref_sets['neg'], test_sets['neg']))\nprint('neg recall:', recall(ref_sets['neg'], test_sets['neg']))\nprint('positive f-score:', f_measure(ref_sets['pos'], test_sets['pos']))\nprint('negative f-score:', f_measure(ref_sets['neg'], test_sets['neg']))","058d6e67":"# Parameters (ngrams)","39fb6f17":"# Sample of selected features ","88180789":"# loading train data .... ","74d25a07":"# Selecting Features ","c3bb98df":"# Load corpus","2fce313e":"# Test data info","adfd178e":"# Sample Frequency","8d139eeb":"# Sample features ","86517b60":"# generating features for training documents ...","43b7de5a":"# define functions ","8de0295d":"# Compute Threshold","fa26275a":"# Most informative features ","530cfce6":"# Results ","3d1198b2":"# compute frequencies","bde0f087":"# merging all features ...","a13e81dc":"# training ...","5a55e9df":"# Sample training data ","0d13b5b6":"# Arabic Sentiment Analysis in tweets using Naive Bayes Machine learning Algorithm and unigram features","973cbeb9":"# generating features for test documents ...","8b635cf4":"# classify test instances ","18e20ecf":"# Training data information"}}