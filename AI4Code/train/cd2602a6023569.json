{"cell_type":{"a7ae2544":"code","4f499120":"code","58f24548":"code","4f48e3a3":"code","f93bacbe":"code","7aabae40":"code","0aef7936":"code","d6a6f1e4":"code","3a01844e":"code","445d53c0":"code","8cf1d1e9":"code","fe4c05de":"code","e1d461b6":"code","b3b1ba23":"code","cf0727b7":"markdown","85e8474d":"markdown","1e4a37f2":"markdown","fe69a86a":"markdown","ed926e4b":"markdown","fbad72e1":"markdown","c9b73868":"markdown","064f5d84":"markdown","bc212d75":"markdown","711a3b60":"markdown"},"source":{"a7ae2544":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","4f499120":"X_train = pd.read_csv('..\/input\/titanic\/train.csv')\nX_test = pd.read_csv('..\/input\/titanic\/test.csv')\ny_train = X_train['Survived']\ndf = pd.concat((X_train, X_test)).reset_index(drop=True)\n# Store our passenger ID for easy access\nPassengerId = X_test['PassengerId']","58f24548":"# Creat new feature based on passengers's titles\ndf['Title'] = df['Name'].str.split(', ', expand = True)[1].str.split('.',expand=True)[0]\ntitle_changes = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'the Countess':'Mrs','Ms': 'Miss', 'Dona': 'Mrs'}\ndf.replace({'Title': title_changes}, inplace=True)\n\n# Fill up missing values of 'Age', group by 'Title' first\ndf['Age'] = df.groupby('Title')['Age'].apply(lambda x: x.fillna(x.median()))\n\n# Create new feature - Family Size\ndf['Family_Size'] = df['Parch'] + df['SibSp']\n\n# Create new feature - Last name\ndf['Last_Name'] = df['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Fill up missing values of Fare\ndf['Fare'] = df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))\n\n# Bin fare and age values\ndf['FareBin'] = pd.qcut(df['Fare'], 5)\ndf['AgeBin'] = pd.qcut(df['Age'], 4)","4f48e3a3":"DEFAULT_SURVIVAL_VALUE = 0.5\ndf['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):   \n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 0\nprint(\"Number of passengers with family survival information:\", \n      df.loc[df['Family_Survival']!=0.5].shape[0])\n\nfor _, grp_df in df.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 0                      \nprint(\"Number of passenger with family\/group survival information: \" \n      +str(df[df['Family_Survival']!=0.5].shape[0]))","f93bacbe":"df.drop(['Survived', 'Age', 'Fare', 'Title', 'Last_Name', 'Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked'], axis = 1, inplace = True)","7aabae40":"# Encode categorical features\nnon_numeric_features = ['Sex', 'FareBin', 'AgeBin']\nfor feature in non_numeric_features:        \n    df[feature] = LabelEncoder().fit_transform(df[feature])","0aef7936":"# Separate train and test sets\nX_train = df.iloc[:len(y_train), :]\nX_test = df.iloc[len(y_train):, :]\nX_train.shape, X_test.shape","d6a6f1e4":"# Scale train and test data\nstd_scaler = StandardScaler()\nX_train = std_scaler.fit_transform(X_train)\nX_test = std_scaler.transform(X_test)","3a01844e":"# Just initialize the pipeline with any estimator you like    \npipe = Pipeline(steps=[('classifier', SVC())])\n\n# Add a dict of classifier and related parameters in list\nparams_grid = [{\n                'classifier':[SVC()],\n                'classifier__C': [0.1, 1, 10, 100],\n                'classifier__gamma': [0.001, 0.0001]\n                },\n              {\n                'classifier': [DecisionTreeClassifier()],\n                'classifier__max_depth': [2,3,4],\n                'classifier__max_features': [None, \"auto\", \"sqrt\", \"log2\"]\n              },\n              {\n                'classifier': [KNeighborsClassifier()],\n                'classifier__n_neighbors': [6,7,8,9,10,11,12,14,16,18,20],\n                'classifier__algorithm': ['auto'],\n                'classifier__weights': ['uniform', 'distance'],\n                'classifier__leaf_size': list(range(1,50,5))\n              }, \n              {\n                'classifier': [LogisticRegression()],\n                'classifier__C': [0.1, 1, 10, 100],\n              },\n              {\n                'classifier': [RandomForestClassifier()],\n                'classifier__n_estimators': [50, 100, 150, 500],\n                'classifier__max_depth': [2, 3, 4, 5],\n              },\n              {\n                'classifier': [GradientBoostingClassifier()],\n                'classifier__learning_rate': [0.1, 0.2, 0.5],\n                'classifier__n_estimators': [50, 100, 150],\n                'classifier__max_depth': [2,3]\n              },\n              {\n                'classifier': [XGBClassifier()],\n                'classifier__learning_rate': [0.1, 0.2, 0.3],\n                'classifier__n_estimators': [100, 150, 500, 1000],\n                'classifier__max_depth': [3,4,5],\n                'classifier__gamma': [0, 0.2, 0.5]  \n              },\n              {\n                'classifier': [GaussianNB()]\n              }]","445d53c0":"gd = GridSearchCV(pipe, params_grid, cv=10, scoring='roc_auc')\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","8cf1d1e9":"gd.best_estimator_.fit(X_train, y_train)\ny_pred_1 = gd.best_estimator_.predict(X_test)","fe4c05de":"best_model = KNeighborsClassifier(algorithm='auto', \n                                  leaf_size=26,\n                                  metric='minkowski', \n                                  metric_params=None,\n                                  n_neighbors=18, \n                                  p=2,\n                                  weights='uniform')","e1d461b6":"best_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)","b3b1ba23":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = PassengerId\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions.csv', header=True, index=False)","cf0727b7":"# 3 Final prediction and submission","85e8474d":"The best estimator was used to predict test data.","1e4a37f2":"# 2 Models","fe69a86a":"# Import dataset","ed926e4b":"# Feature Engineering","fbad72e1":"Grid search was performed based on AUC score for all models. The best estimator was printed.","c9b73868":"* Several different models was fitted: random forest, logistic regression, support vector machine, gradient boosting, decision tree, naive bayes, XGBoost and KNN.\n* A pipeline was set up for hyperparameter tuning using grid search.","064f5d84":"# Introduction\n* This note book performed advanced feature engineering based on my previous [kernel](https:\/\/www.kaggle.com\/scpitt\/titanic-notebook-for-beginners).\n* Some feature engineering ideas came from this wonderful [notebook](https:\/\/www.kaggle.com\/acsrikar279\/titanic-higher-score-using-kneighborsclassifier)\n* Eight popular models were set up.\n* Hyperparameters were tuned using grid search by pipeline.\n\nPlease feel free to comment and share your thoughts. Thanks!","bc212d75":"# Import libraries","711a3b60":"Even though decision tree has higher cross validation score, KNN turns out to be a better model for final prediction on test set."}}