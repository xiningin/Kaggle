{"cell_type":{"b9a44dca":"code","722bd6d4":"code","55fc1b85":"code","00542145":"code","0aa44f4c":"code","b52ba541":"code","0e4377ab":"code","daa41c60":"code","2464ab07":"code","acee531f":"code","98cd95c7":"code","2e8c8f1a":"code","311f04ba":"code","00d1d9e7":"code","6c503868":"code","38c9b75a":"code","9f4cd4ef":"code","468cdcf8":"code","3266392e":"code","e7538296":"code","c79e42a2":"code","c7136331":"code","bd02ae26":"code","7ecd8bb6":"code","c6a6fec1":"code","56939429":"code","3f876e29":"code","cf90589b":"code","c90c53cd":"code","cc8a953b":"code","0cfeb733":"code","b74bc2c9":"code","15ea9b7f":"code","2d46f631":"code","d8d2c22d":"code","127c66f3":"code","5d178f82":"code","90d5aed8":"code","3d376162":"code","766df981":"code","f820a778":"code","a9dd1b19":"code","8b4110a8":"code","92e2fa69":"code","1fbf01e7":"code","5ad050f1":"code","af104468":"markdown","e6fea44d":"markdown","5032c32a":"markdown","33284c8d":"markdown","f53479ea":"markdown","c446ffe2":"markdown","3449492c":"markdown","1f7a17d6":"markdown","b7d5b21e":"markdown","65130987":"markdown"},"source":{"b9a44dca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","722bd6d4":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab","55fc1b85":"# importing the dataset \ndf_train = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/train.txt', header =None, sep =';', names = ['Input','Sentiment'], encoding='utf-8')\ndf_test = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/test.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')\ndf_val=pd.read_csv('..\/input\/emotions-dataset-for-nlp\/val.txt',header=None,sep=';',names=['Input','Sentiment'],encoding='utf-8')","00542145":"df_full = pd.concat([df_train,df_test,df_val], axis = 0)\ndf_full","0aa44f4c":"!pip install text_hammer","b52ba541":"import text_hammer as th","0e4377ab":"%%time\n\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef text_preprocessing(df,col_name):\n    column = col_name\n    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n    df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_stopwords(x))\n\n    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n    return(df)","daa41c60":"df_cleaned = text_preprocessing(df_full,'Input')","2464ab07":"df_cleaned = df_cleaned.copy()","acee531f":"df_cleaned['num_words'] = df_cleaned.Input.apply(lambda x:len(x.split()))","98cd95c7":"# changing the data type to the category to encode into codes \ndf_cleaned['Sentiment'] = df_cleaned.Sentiment.astype('category')\n","2e8c8f1a":"df_cleaned.Sentiment","311f04ba":"df_cleaned.Sentiment.cat.codes","00d1d9e7":"encoded_dict  = {'anger':0,'fear':1, 'joy':2, 'love':3, 'sadness':4, 'surprise':5}","6c503868":"df_cleaned['Sentiment']  =  df_cleaned.Sentiment.cat.codes\ndf_cleaned.Sentiment","38c9b75a":"df_cleaned.num_words.max()","9f4cd4ef":"from sklearn.model_selection import train_test_split\ndata_train,data_test = train_test_split(df_cleaned, test_size = 0.3, random_state = 42, stratify = df_cleaned.Sentiment)","468cdcf8":"data_train.shape","3266392e":"data_test.shape","e7538296":"from tensorflow.keras.utils import to_categorical","c79e42a2":"to_categorical(data_train.Sentiment)","c7136331":"from transformers import AutoTokenizer,TFBertModel\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nbert = TFBertModel.from_pretrained('bert-base-cased')\n","bd02ae26":"tokenizer.save_pretrained('bert-tokenizer')\nbert.save_pretrained('bert-model')\n# for saving model locally and we can load it later on ","7ecd8bb6":"import shutil\nshutil.make_archive('bert-tokenizer', 'zip', 'bert-tokenizer')","c6a6fec1":"shutil.make_archive('bert-model','zip','bert-model')","56939429":"### we can use distilbert its lighter cheaper and similar performance \n\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\ndbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n","3f876e29":"tokenizer('hello this me abhishek')","cf90589b":"# Tokenize the input (takes some time) \n# here tokenizer using from bert-base-cased\nx_train = tokenizer(\n    text=data_train.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n\nx_test = tokenizer(\n    text=data_test.Input.tolist(),\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n","c90c53cd":"x_test['input_ids']","cc8a953b":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical","0cfeb733":"import tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')","b74bc2c9":"max_len = 70\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\nout = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\n\ny = Dense(6,activation = 'sigmoid')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True\n# for training bert our lr must be so small","15ea9b7f":"optimizer = Adam(\n    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss =CategoricalCrossentropy(from_logits = True)\nmetric = CategoricalAccuracy('balanced_accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","2d46f631":"model.summary()","d8d2c22d":"tf.config.experimental_run_functions_eagerly(True)\ntf.config.run_functions_eagerly(True)\n","127c66f3":"train_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = to_categorical(data_train.Sentiment),\n    validation_data = (\n    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, to_categorical(data_test.Sentiment)\n    ),\n  epochs=1,\n    batch_size=36\n)","5d178f82":"model.save_weights('sentiment_weights.h5')\n","90d5aed8":"# max_len = 70\n# import tensorflow as tf\n# from tensorflow.keras.layers import Input, Dense\n\n# input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n# input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n# # embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\n# embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)\n# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n# out = Dense(128, activation='relu')(out)\n# out = tf.keras.layers.Dropout(0.1)(out)\n# out = Dense(32,activation = 'relu')(out)\n\n# y = Dense(6,activation = 'sigmoid')(out)\n    \n# new_model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n# new_model.layers[2].trainable = True\n# # for training bert our lr must be so small\n\n# new_model.load_weights('sentiment_weights.h5')","3d376162":"predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})","766df981":"predicted_raw[0]","f820a778":"y_predicted = np.argmax(predicted_raw, axis = 1)","a9dd1b19":"data_test.Sentiment","8b4110a8":"from sklearn.metrics import classification_report\n","92e2fa69":"print(classification_report(data_test.Sentiment, y_predicted))","1fbf01e7":"texts = input(str('input the text'))\n\nx_val = tokenizer(\n    text=texts,\n    add_special_tokens=True,\n    max_length=70,\n    truncation=True,\n    padding='max_length', \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True) \nvalidation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\nvalidation","5ad050f1":"for key , value in zip(encoded_dict.keys(),validation[0]):\n    print(key,value)","af104468":"### for prediction lets","e6fea44d":"### model.save doesn't work in this case \nso we need to save the weights files and then we need to make the same model architecture and then load with the weights","5032c32a":"lets create a new model and then load the weights ","33284c8d":"## Now lets load the model ","f53479ea":"## MultiClass classification by using SOTA BERT model and then fine tuning on Tensorflow \n1. ### We will use tokenizer\n2. ### We will convert our data in the form of BERT \n3. ### We will see how to use pretrained BERT architecture \n4. ### We will then finetune the bert model\n5. ### Model evaluation \n\n\nAuthor @ Abhishek jaiswal (kaggle @ https:\/\/www.kaggle.com\/preatcher)\n","c446ffe2":"#### here we are doing some text preprocessing \n\n","3449492c":"### Prediction Part","1f7a17d6":"### so far we have cleaned our text data now we need to encode our output in some labels , \nhere we have two method to encode ","b7d5b21e":"### loading some libraries ","65130987":"#### model fitting and then evaluation"}}