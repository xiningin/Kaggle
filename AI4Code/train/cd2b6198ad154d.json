{"cell_type":{"8457c037":"code","6ea0fe07":"code","f6bb3731":"code","b4909577":"code","e260ee0b":"code","21259f66":"code","ac75ddb9":"code","72b4d4e6":"code","6ba97989":"code","9273f800":"code","90338800":"code","c45db8c0":"code","041375a1":"code","4578363f":"code","142cf8f6":"code","ebd2fff0":"code","5dac54b7":"code","1e5f4a01":"code","50c9a8e0":"code","1fd6a832":"code","ded50dd4":"code","e15bab2f":"code","cd8a7fdb":"code","c608e2ff":"code","9170facf":"code","e90459f4":"code","5446fc46":"code","428f238b":"markdown","38d6ef32":"markdown","3161cde5":"markdown","810d9796":"markdown","b8f9128e":"markdown","0682ef3e":"markdown","068f8adf":"markdown","9e0e0e66":"markdown","8509f2bc":"markdown","38d24cbb":"markdown","5cb14971":"markdown","6bb6491d":"markdown"},"source":{"8457c037":"# Import required libraries\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6ea0fe07":"# Import data\n\ndata = pd.read_csv(\"\/kaggle\/input\/fish-market\/Fish.csv\")","f6bb3731":"# Information about the data\n\ndata.info()","b4909577":"# Check if there is any missing data\n\ndata.isnull().sum()","e260ee0b":"data.head()","21259f66":"# Summary statistics\n\ndata.describe()","ac75ddb9":"data[\"Species\"].value_counts()","72b4d4e6":"sns.countplot(\"Species\", data=data)","6ba97989":"grp = data.groupby(\"Species\").mean()\ngrp","9273f800":"# Barplot for mean weight of species\n\nsns.barplot(grp.index, \"Weight\", data=grp)\nplt.title(\"Mean Weight of Species\")","90338800":"# Box plot for weight of species\n\nsns.boxplot(\"Species\", \"Weight\", data=data)","c45db8c0":"# Calculate correlation and store it\ncorr = data.corr()\n# Color palette for heatmap\ncmap = sns.diverging_palette(20, 220, as_cmap=True)\nsns.heatmap(corr, vmin=-1, vmax=1, cmap=cmap, annot=True)\nplt.title(\"Correlation Heatmap\")","041375a1":"# Detect outliers\n\n\ndef outliers(data):\n    \"\"\"\n    Plot a boxplot and return outliers of the given data\n    \"\"\"\n    # plot data\n    sns.boxplot(data)\n\n    # find outliers\n    q1 = data.quantile(0.25)\n    q3 = data.quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    outliers = data[(data < lower_bound) | (data > upper_bound)]\n    if len(outliers) != 0:\n        return outliers\n    return \"No outliers found\"\n\n","4578363f":"outliers(data[\"Weight\"])","142cf8f6":"outliers(data[\"Length1\"])","ebd2fff0":"outliers(data[\"Length2\"])","5dac54b7":"outliers(data[\"Length3\"])","1e5f4a01":"outliers(data[\"Width\"])","50c9a8e0":"# Drop outliers\n\ndata.drop([142, 143, 144], inplace=True)","1fd6a832":"# Plot linear relationship of each variable with Weight\n\nsns.pairplot(data=data, x_vars=[\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"], y_vars=[\n             \"Weight\"], kind=\"reg\", height=8, aspect=.5)","ded50dd4":"# Split the data into train and test data\nx = data[[\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"]]\ny = data[\"Weight\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    x, y, random_state=0, test_size=0.2)\n","e15bab2f":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train, y_train)\ny_hat = lin_reg.predict(X_test)\n","cd8a7fdb":"# Calculate R-Squared and Mean Squared Error\n\nprint(f\"R-Squared: {r2_score(y_test, y_hat)}\")\nprint(f\"MSE: {mean_squared_error(y_test, y_hat)}\")","c608e2ff":"Input = [('polynomial', PolynomialFeatures(degree=2)),\n         ('model', LinearRegression())]\npipe = Pipeline(Input)\npipe.fit(X_train, y_train)\ny_hat_pipe = pipe.predict(X_test)","9170facf":"# Calculate R-Squared and Mean Squared Error\n\nprint(f\"R-Squared: {r2_score(y_test, y_hat_pipe)}\")\nprint(f\"MSE: {mean_squared_error(y_test, y_hat_pipe)}\")","e90459f4":"# Scatterplot for linear regression\nplt.figure(figsize=(10, 5))\nplt.subplot(\"121\")\nplt.scatter(y_test, y_hat)\nplt.xlabel(\"Actual Value\")\nplt.ylabel(\"Predicted Value\")\nplt.title(\"Linear Regression\")\n# Scatterplot for polynomial regression\nplt.subplot(\"122\")\nplt.scatter(y_test, y_hat_pipe)\nplt.xlabel(\"Actual Value\")\nplt.ylabel(\"Predicted Value\")\nplt.title(\"Polynomial Regression\")\n\nplt.suptitle(\"Linear vs Polynomial Regression\")\nplt.tight_layout()","5446fc46":"# Create a distribution plot\n# Distribution plot for linear regression\nplt.figure(figsize=(10, 5))\nplt.subplot(\"121\")\nsns.distplot(y_test, hist=False, label=\"Actual Values\")\nsns.distplot(y_hat, hist=False, label=\"Predicted Values\")\nplt.title(\"Linear Regression\")\n# Distribution plot for polynomial regression\nplt.subplot(\"122\")\nsns.distplot(y_test, hist=False, label=\"Actual Values\")\nsns.distplot(y_hat_pipe, hist=False, label=\"Predicted Values\")\nplt.title(\"Polynomial Regression\")\n\nplt.suptitle(\"Linear vs Polynomial Regression\")\nplt.tight_layout()","428f238b":" Heatmap shows us that variables have high correlation coefficients.\n \n \"Weight\" and \"Length1\": 0.92,\n \"Weight\" and \"Length2\": 0.92,\n \"Weight\" and \"Length3\": 0.92,\n \"Weight\" and \"Height\": 0.72,\n \"Weight\" and \"Width\": 0.89","38d6ef32":" According to our barplot, Pike has the highest mean weight whereas Smelt has the lowest.","3161cde5":" We can look at the distribution of the fitted values and compare it to the distribution of the actual values.","810d9796":" The results above suggest that variables have linear relationship with Weight.","b8f9128e":" Let's group the data by Species and calculate the mean values of each continuous variable.","0682ef3e":" Index number 142,143,144 are outliers.\n We can remove those entries.","068f8adf":" ## Polynomial Regression (Degree = 2)","9e0e0e66":" The last two visualizations suggest that polynomial regression model is able to predict more accurately compared to linear regression model.\n \n R-Squared value that we calculated from polynomial regression model is higher than the R-Squared value from linear regression model.\n\n $R^2: 0.9168025741580008$ (linear regression)\n\n $R^2: 0.9653660833845066$ (polynomial regression)\n\n In addition, MSE value from polynomial regression model is lower compared to MSE value from linear regression model.\n \n $MSE: 7838.981939855566$ (linear regression)\n \n $MSE: 3263.2577763994027$ (polynomial regression)","8509f2bc":" There is only one categorical variable which is \"Species\".\n We can see the number of occurrences of each category.","38d24cbb":" ## Linear Regression","5cb14971":" We can see that Perch is the most common one and Whitefish is the least.","6bb6491d":" Let's calculate the correlation for continuous variables and plot a correlation heatmap for visualization. By this way, we can see how strong the association between the variables."}}