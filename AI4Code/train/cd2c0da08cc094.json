{"cell_type":{"cbc4d73f":"code","31eb5789":"code","73ff0c36":"code","ae3f3d67":"markdown","9c9f5d35":"markdown","d5d4498a":"markdown"},"source":{"cbc4d73f":"!pip install --upgrade transformers \n!pip install nb_black\n%load_ext nb_black","31eb5789":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom transformers import pipeline\n\n# load BART summarizer\nsummarizer = pipeline(task=\"summarization\")\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf = pd.read_csv(\n    \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\",\n    usecols=[\"title\", \"abstract\", \"authors\", \"doi\", \"publish_time\"],\n)\nprint(df.shape)\n# drop duplicates\n# df=df.drop_duplicates()\ndf = df.drop_duplicates(subset=\"abstract\", keep=\"first\")\n# drop NANs\ndf = df.dropna()\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()\n# show 5 lines of the new dataframe\nprint(df.shape)\ndf.head()","73ff0c36":"import functools\nfrom IPython.core.display import display, HTML\nfrom nltk import PorterStemmer\nfrom tqdm.notebook import tqdm_notebook\n\ntqdm_notebook.pandas()\n\n# list of lists for topic words realting to tasks\ndisplay(HTML(\"<h1>COVID-19 Risk Factors<\/h1>\"))\ndisplay(\n    HTML(\n        \"<h3>Table of Contents (ctrl f to search for the hash tagged words below to find that data table)<\/h3>\"\n    )\n)\ntasks = [\n    [\"comorbidities\"],\n    [\"risk\", \"factors\"],\n    [\"lung\", \"cancer\"],\n    [\"hypertension\"],\n    [\"heart\", \"disease\"],\n    [\"chronic\", \"bronchitis\"],\n    [\"cerebral\", \"infarction\"],\n    [\"diabetes\"],\n    [\"copd\"],\n    [\"blood type\", \"type\"],\n    [\"smoking\"],\n    [\"effective\", \"reproductive\", \"number\"],\n    [\"incubation\", \"period\", \"days\"],\n]\n\n\ndef summarize(row):\n    summary = \"\"\n\n    abstract = row[\"abstract\"].split(\"abstract \")[-1]\n\n    summary = summarizer(abstract, min_length=50, max_length=200)\n    summary = summary[0][\"summary_text\"]\n\n    if summary != \"\":\n        authors = row[\"authors\"].split(\" \")\n        link = row[\"doi\"]\n        title = row[\"title\"]\n        linka = \"https:\/\/doi.org\/\" + link\n        linkb = title\n        summary = (\n            '<p align=\"left\">'\n            + \"<strong>Summary:<\/strong><br>\"\n            + summary\n            + \"<br><br>\"\n            + \"<strong>Original:<\/strong><br>\"\n            + abstract\n            + \"<\/p>\"\n        )\n        final_link = '<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka, linkb)\n        to_append = [\n            row[\"publish_time\"],\n            authors[0] + \" et al.\",\n            final_link,\n            summary,\n        ]\n        df_length = len(df_table)\n        df_table.loc[df_length] = to_append\n\n\n# function to stem keywords into a common base word\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles = []\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\n\nfor z, search_words in enumerate(tqdm_notebook(tasks)):\n    df_table = pd.DataFrame(columns=[\"pub_date\", \"authors\", \"title\", \"excerpt\"])\n    str1 = \"\"\n    # a make a string of the search words to print readable search\n    str1 = \" \".join(search_words)\n    search_words = stem_words(search_words)\n    # add cov to focus the search the papers and avoid unrelated documents\n    search_words.append(\"covid\")\n    # search the dataframe for all the keywords\n    dfa = df[\n        functools.reduce(\n            lambda a, b: a & b, (df[\"abstract\"].str.contains(s) for s in search_words)\n        )\n    ]\n    search_words.pop()\n    search_words.append(\"-cov-\")\n    dfb = df[\n        functools.reduce(\n            lambda a, b: a & b, (df[\"abstract\"].str.contains(s) for s in search_words)\n        )\n    ]\n    # remove the cov word for sentence level analysis\n    search_words.pop()\n    # combine frames with COVID and cov and drop dups\n    frames = [dfa, dfb]\n    df1 = pd.concat(frames)\n    df1 = df1.drop_duplicates()\n    df1 = df1.reset_index()\n\n    display(HTML(\"<h3>Task Topic: \" + str1 + \"<\/h3>\"))\n    display(HTML(\"# \" + str1 + \" <a><\/a>\"))\n\n    print(df1.shape)\n    # SUMMARIZATION for all abstracts\n    #     df.progress_apply(summarize, axis=1, result_type=\"expand\")\n    # SUMMARIZATION for the first abstract in each category\n\n    summarize(df1.loc[0])\n\n    filename = str1 + \".csv\"\n    df_table.to_csv(filename, index=False)\n    df_table = HTML(df_table.to_html(escape=False, index=False))\n    display(df_table)","ae3f3d67":"### Run BART summarization\n\nThis code only runs the summarization for the first paper in each category.","9c9f5d35":"## Most of the code comes from this kernel https:\/\/www.kaggle.com\/mlconsult\/summary-page-covid-19-risk-factors\n\n## I added the [BART summarizer](https:\/\/github.com\/huggingface\/transformers\/tree\/5b396457e5035a8b16ddee14b205c098598fe6bb\/examples\/summarization\/bertabs)\nPaper to BART \n\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\" - [arXiv:1910.13461](https:\/\/arxiv.org\/abs\/1910.13461)\n\n### Notes\n- Compute times are incredibly slow (15-20sec\/it = __~200-250 hours__ for 45k)\n- Multi-GPU\/TPU is not supported yet as mentioned [here](https:\/\/github.com\/huggingface\/transformers\/blob\/5b396457e5035a8b16ddee14b205c098598fe6bb\/examples\/summarization\/bertabs\/README.md#reproduce-the-authors--rouge-score)","d5d4498a":"### Load abstracts"}}