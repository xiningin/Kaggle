{"cell_type":{"1be0ee3d":"code","fcec06aa":"code","dd93872e":"code","39f3a1ae":"code","0c8b5e7b":"code","c6df03a8":"code","5cb31229":"code","98c197db":"code","87def46b":"code","f685f33b":"code","ea32ca31":"code","a0f02528":"code","8c71c4d2":"code","853fc26b":"code","12c60263":"code","2e198a3e":"code","8ffeb9cc":"code","aa2f8371":"code","a591ac4e":"code","1c6b8234":"code","c95fb7f0":"code","98a140b3":"code","2379444c":"code","d641ff74":"code","6cec9bac":"code","2fe184b2":"code","298a4ad1":"code","a630a664":"code","84a1167c":"code","c64971f4":"code","1cca2143":"code","8e23c14a":"code","e8feddbb":"code","918556a5":"code","361f8e82":"code","01b632e1":"code","679480d7":"code","41369caf":"code","972dedd2":"code","26181859":"code","6b838098":"code","a07f273c":"code","1015bf03":"code","f44e8e98":"code","e87aa436":"code","46269d0c":"code","51a5a4dd":"code","794a1c12":"code","2edd9937":"code","b88cf8f6":"code","a54a8c0b":"code","4b15758a":"code","770b9d93":"code","05f5c6a4":"code","a55839f9":"code","dce58193":"code","5a2de1d1":"code","deac1bcb":"code","33f5a93d":"code","efc4b913":"code","b4a903c4":"code","4d8af581":"code","410d3603":"code","eb7d3a4a":"code","81f0afde":"code","489e7a8b":"code","de939452":"code","6e97e1a1":"code","7f5edf69":"code","567150be":"code","4fb8ca09":"code","de85dcb8":"code","34714ad4":"code","af19b52b":"code","fca20172":"code","d112ad69":"code","53eb4992":"code","64e4b54f":"code","1bf75e62":"code","6c327ba0":"code","a65c71fd":"code","8889fd5c":"code","b4bba56f":"code","4647de58":"code","75030215":"code","8f8bdfe4":"code","15b42c94":"code","3b6ac9a4":"code","2aeae38e":"code","298a7fa8":"code","495912d7":"code","b94091b5":"code","14524e85":"code","4db4c872":"code","7105f971":"code","3ea6d8f6":"code","42ec0e9e":"code","fa67b8af":"code","9b7f0a89":"markdown","28638fb4":"markdown","3492ae0e":"markdown","e6654143":"markdown","71803f24":"markdown","7448a50e":"markdown","44b013b5":"markdown","2ce88b21":"markdown","5b904ce6":"markdown","684c1389":"markdown","a405b6dc":"markdown","6e103fd1":"markdown","666959c6":"markdown","63390239":"markdown","e3101bb1":"markdown","ab83bba6":"markdown","cd024199":"markdown","729a6e23":"markdown","b502c274":"markdown","ebbe37f7":"markdown","08d21dee":"markdown","7d232206":"markdown","2c2dec34":"markdown"},"source":{"1be0ee3d":"# Importing libraries necessary for the study\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","fcec06aa":"# reading the dataset\nHouseAu = pd.read_csv('..\/input\/train.csv')","dd93872e":"# Head\nHouseAu.head()","39f3a1ae":"#transposing the Data table to view the variables better\nHouseAu.head(12).transpose()","0c8b5e7b":"# summary of the dataset: 1460 rows, 81 columns\nprint(HouseAu.info())","c6df03a8":"# Let's check the dimensions of the dataframe\nHouseAu.shape","5cb31229":"#Cleaning up variable Alley (Replacing NA => No Alley Access)\nHouseAu['Alley'].replace({np.nan:'No Alley Access'},inplace=True)\n100*(HouseAu['Alley'].value_counts()\/HouseAu['Alley'].count())","98c197db":"# As 93.8% of Alley is \"No Alley access\" it can be considered as a single value attribute and hence can be dropped\nHouseAu=HouseAu.drop(['Alley'],axis=1)","87def46b":"#Cleaning up variable BsmtQual (Replacing NA => No Basement) to reduce the features\nHouseAu['BsmtQual'].replace({np.nan:'No Basement'},inplace=True)\nprint(100*(HouseAu['BsmtQual'].value_counts()\/HouseAu['BsmtQual'].count()))\n# Three levels can be combined as \"Others\" (Fa, No BAsement, Ex)\nHouseAu['BsmtQual'].replace({'Fa':'Others'},inplace=True)\nHouseAu['BsmtQual'].replace({'Ex':'Others'},inplace=True)\nHouseAu['BsmtQual'].replace({'No Basement':'Others'},inplace=True)\nprint(100*(HouseAu['BsmtQual'].value_counts()\/HouseAu['BsmtQual'].count()))\n","f685f33b":"#Cleaning up variable BsmtCond (Replacing NA => No Basement)\nHouseAu['BsmtCond'].replace({np.nan:'No Basement'},inplace=True)\n100*(HouseAu['BsmtCond'].value_counts()\/HouseAu['BsmtCond'].count())\n# Three levels of fair\/good quality can be combined as OK \nHouseAu['BsmtCond'].replace({'Fa':'OK'},inplace=True)\nHouseAu['BsmtCond'].replace({'TA':'OK'},inplace=True)\nHouseAu['BsmtCond'].replace({'Gd':'OK'},inplace=True)\n# Two levels of poor quality can be combined as NOK (Po, No Basement)\nHouseAu['BsmtCond'].replace({'Po':'NOK'},inplace=True)\nHouseAu['BsmtCond'].replace({'No Basement':'NOK'},inplace=True)\nprint(100*(HouseAu['BsmtCond'].value_counts()\/HouseAu['BsmtCond'].count()))\n","ea32ca31":"#Can be considered as single value and can be dropped from dataset\nHouseAu=HouseAu.drop(['BsmtCond'],axis=1)","a0f02528":"#Cleaning up variable BsmtExposure (Replacing NA => No Basement)\nHouseAu['BsmtExposure'].replace({np.nan:'No Basement'},inplace=True)\n100*(HouseAu['BsmtExposure'].value_counts()\/HouseAu['BsmtExposure'].count())","8c71c4d2":"#Cleaning up variable BsmtFinType1 (Replacing NA => No Basement)\nHouseAu['BsmtFinType1'].replace({np.nan:'No Basement'},inplace=True)\n100*(HouseAu['BsmtFinType1'].value_counts()\/HouseAu['BsmtFinType1'].count())","853fc26b":"#Cleaning up variable BsmtFinType2 (Replacing NA => No Basement)\nHouseAu['BsmtFinType2'].replace({np.nan:'No Basement'},inplace=True)\n100*(HouseAu['BsmtFinType2'].value_counts()\/HouseAu['BsmtFinType2'].count())","12c60263":"#Taking a deep dive into the Basement related attributes to understand the correlations\nHouseAu_Basement=HouseAu[['BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']]\nHouseAu_Basement.head()\nHouseAu_Basement.info()\n##HouseAu['BsmtQual']=HouseAu['BsmtQual'].values.astype(np.int64)","2e198a3e":"# pairwise scatter plot to explore Basement attributes\n\nplt.figure(figsize=(20, 10))\nsns.pairplot(HouseAu_Basement)\nplt.show()","8ffeb9cc":"#Dropping of correlated variables and keeping only TotalBsmtSF as this is the key one remaining are related to it.\nHouseAu=HouseAu.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis=1)","aa2f8371":"#Plotting the categorical variables related to Basement to find which ones have correlation and can be dropped\nplt.figure(figsize=(20, 12))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'BsmtQual', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'BsmtExposure', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'BsmtFinType1',y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'BsmtFinType2',y = 'SalePrice', data = HouseAu)","a591ac4e":"#Dropping variables BsmtFinType1 and BsmtFinType2 as two do not seem to have a strong influence on sale price\nHouseAu=HouseAu.drop(['BsmtFinType1','BsmtFinType2'],axis=1)","1c6b8234":"#Cleaning up variable FireplaceQu (Replacing NA => No Fireplace)\nHouseAu['FireplaceQu'].replace({np.nan:'No Fireplace'},inplace=True)\nprint(100*(HouseAu['FireplaceQu'].value_counts()\/HouseAu['FireplaceQu'].count()))\n#Imputing level values of FireplaceQu\nHouseAu['FireplaceQu'].replace({'Fa':'OK Fireplace'},inplace=True)\nHouseAu['FireplaceQu'].replace({'TA':'OK Fireplace'},inplace=True)\nHouseAu['FireplaceQu'].replace({'Gd':'OK Fireplace'},inplace=True)\nHouseAu['FireplaceQu'].replace({'Ex':'OK Fireplace'},inplace=True)\nHouseAu['FireplaceQu'].replace({'Po':'OK Fireplace'},inplace=True)\nprint(100*(HouseAu['FireplaceQu'].value_counts()\/HouseAu['FireplaceQu'].count()))","c95fb7f0":"#Plotting the categorical variables related to FireplaceQu and checking correlation with SalePrice\nplt.figure(figsize=(20, 12))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'FireplaceQu', y = 'SalePrice', data = HouseAu)\n#Clearly Fireplace presence drives Sale price to some extent","98a140b3":"#Cleaning up variable GarageType (Replacing NA => No Garage)\nHouseAu['GarageType'].replace({np.nan:'No Garage'},inplace=True)\n100*(HouseAu['GarageType'].value_counts()\/HouseAu['GarageType'].count())","2379444c":"#Cleaning up variable GarageFinish (Replacing NA => No Garage)\nHouseAu['GarageFinish'].replace({np.nan:'No Garage'},inplace=True)\n100*(HouseAu['GarageFinish'].value_counts()\/HouseAu['GarageFinish'].count())","d641ff74":"#Cleaning up variable GarageQual (Replacing NA => No Garage)\nHouseAu['GarageQual'].replace({np.nan:'No Garage'},inplace=True)\nprint(100*(HouseAu['GarageQual'].value_counts()\/HouseAu['GarageQual'].count()))\n#Imputing level values of GarageQual\nHouseAu['GarageQual'].replace({'TA':'OK Garage'},inplace=True)\nHouseAu['GarageQual'].replace({'Fa':'OK Garage'},inplace=True)\nHouseAu['GarageQual'].replace({'Gd':'OK Garage'},inplace=True)\nHouseAu['GarageQual'].replace({'Ex':'OK Garage'},inplace=True)\nHouseAu['GarageQual'].replace({'Po':'No Garage'},inplace=True)\nprint(100*(HouseAu['GarageQual'].value_counts()\/HouseAu['GarageQual'].count()))","6cec9bac":"#Cleaning up variable GarageCond (Replacing NA => No Garage)\nHouseAu['GarageCond'].replace({np.nan:'No Garage'},inplace=True)\nprint(100*(HouseAu['GarageCond'].value_counts()\/HouseAu['GarageCond'].count()))\n#Imputing level values of GarageCond\nHouseAu['GarageCond'].replace({'TA':'OK'},inplace=True)\nHouseAu['GarageCond'].replace({'Fa':'OK'},inplace=True)\nHouseAu['GarageCond'].replace({'Gd':'OK'},inplace=True)\nHouseAu['GarageCond'].replace({'Ex':'OK'},inplace=True)\nHouseAu['GarageCond'].replace({'Po':'No Garage'},inplace=True)\nprint(100*(HouseAu['GarageCond'].value_counts()\/HouseAu['GarageCond'].count()))","2fe184b2":"#Plotting the categorical variables related to Garage and checking correlation with SalePrice\nplt.figure(figsize=(20, 12))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'GarageCond', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'GarageQual', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'GarageFinish', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'GarageType', y = 'SalePrice', data = HouseAu)\n","298a4ad1":"#GarageCond and GarageQual seem to be same in influence on SalePrice, one can be dropped\nHouseAu=HouseAu.drop(['GarageQual'],axis=1)\n#Garage type - CarPort, No Garage, Basement, 2Types can be combined as \"Others\"\n#Imputing values to \"Others\"\nHouseAu['GarageType'].replace({'CarPort':'No Garage'},inplace=True)\nHouseAu['GarageType'].replace({'Basment':'No Garage'},inplace=True)\nHouseAu['GarageType'].replace({'No Garage':'No Garage'},inplace=True)\nHouseAu['GarageType'].replace({'2Types':'No Garage'},inplace=True)\nprint(100*(HouseAu['GarageType'].value_counts()\/HouseAu['GarageType'].count()))","a630a664":"#Cleaning up variable PoolQC (Replacing NA => No Pool)\nHouseAu['PoolQC'].replace({np.nan:'No Pool'},inplace=True)\nprint(100*(HouseAu['PoolQC'].value_counts()\/HouseAu['PoolQC'].count()))\n#Imputing level values of PoolQC\nHouseAu['PoolQC'].replace({'Fa':'OK'},inplace=True)\nHouseAu['PoolQC'].replace({'Gd':'OK'},inplace=True)\nHouseAu['PoolQC'].replace({'Ex':'OK'},inplace=True)\nprint(100*(HouseAu['PoolQC'].value_counts()\/HouseAu['PoolQC'].count()))","84a1167c":"#Plotting the categorical variables related to PooQC to find which ones have correlation and can be dropped\nplt.figure(figsize=(10,10))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'PoolQC', y = 'SalePrice', data = HouseAu)\n#PoolQC is only 0.4% of the houses so a small subset of data","c64971f4":"#Sale Price is not strongly changing with Pool or No Pool, effects can be captured with Pool Area. Do dropping PoolQC\nHouseAu=HouseAu.drop(['PoolQC'],axis=1)","1cca2143":"#Cleaning up variable Fence (Replacing NA => No Fence)\nHouseAu['Fence'].replace({np.nan:'No Fence'},inplace=True)\nprint(100*(HouseAu['Fence'].value_counts()\/HouseAu['Fence'].count()))\n#Imputing level values of Fence\nHouseAu['Fence'].replace({'MnPrv':'Fence'},inplace=True)\nHouseAu['Fence'].replace({'GdPrv':'Fence'},inplace=True)\nHouseAu['Fence'].replace({'GdWo':'Fence'},inplace=True)\nHouseAu['Fence'].replace({'MnWw':'Fence'},inplace=True)\nprint(100*(HouseAu['Fence'].value_counts()\/HouseAu['Fence'].count()))","8e23c14a":"#Cleaning up variable MiscFeature (Replacing NA => No Fence)\nHouseAu['MiscFeature'].replace({np.nan:'None'},inplace=True)\n100*(HouseAu['MiscFeature'].value_counts()\/HouseAu['MiscFeature'].count())","e8feddbb":"#Plotting the categorical variables related to MiscFeature to find which ones have correlation and can be dropped\nplt.figure(figsize=(20,20))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'MiscFeature', y = 'SalePrice', data = HouseAu)\n#MiscFeature levels are a minor subset in the dataset but seem to have a good influence on sale price.","918556a5":"#Taking a deep dive into the Basement related attributes to understand the correlations\nHouseAu_Porch=HouseAu[['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']]\nprint(HouseAu_Porch.head())\nprint(HouseAu_Porch.info())\n# pairwise scatter plot\nplt.figure(figsize=(20, 10))\nsns.pairplot(HouseAu_Porch)\nplt.show()","361f8e82":"#From the correlation pairplots, Out of four variables on Porch, we can capture key effects from Open Porch and Enclosed Porch\nHouseAu=HouseAu.drop(['ScreenPorch','3SsnPorch'],axis=1)","01b632e1":"print(100*(HouseAu['Neighborhood'].astype('category').value_counts()\/HouseAu['Neighborhood'].count()))\n#Imputing values of the minor category levels in Neighborhood\nHouseAu['Neighborhood'].replace({'ClearCr':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'SWISU':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'StoneBr':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'Blmngtn':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'MeadowV':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'BrDale':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'Veenker':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'NPkVill':'Others'},inplace=True)\nHouseAu['Neighborhood'].replace({'Blueste':'Others'},inplace=True)\nprint(100*(HouseAu['Neighborhood'].astype('category').value_counts()\/HouseAu['Neighborhood'].count()))","679480d7":"#Binning of the Year built variable\n#Creating bins to define the year periods - 1872-1925, 1925-1950,1950-1975, 1976-1990, 1991-2000,2001-2010\nbins=[1872,1925,1950,1976,1991,2001,2010]\nslot_names=['1872-1925','1925-1950','1950-1975','1976-1990','1991-2000','2001-2010']\nHouseAu['YearBuilt']=pd.cut(HouseAu['YearBuilt'],bins,labels=slot_names,include_lowest=True)\nprint(100*(HouseAu['YearBuilt'].value_counts()\/HouseAu['YearBuilt'].count()))","41369caf":"#Binning of the YearRemodAdd variable\n#Creating bins to define the year periods - 1872-1925, 1925-1950,1950-1975, 1976-1990, 1991-2000,2001-2010\nbins=[1872,1950,1976,1991,2001,2010]\nslot_names=['1872-1950','1950-1975','1976-1990','1991-2000','2001-2010']\nHouseAu['YearRemodAdd']=pd.cut(HouseAu['YearRemodAdd'],bins,labels=slot_names,include_lowest=True)\n100*(HouseAu['YearRemodAdd'].value_counts()\/HouseAu['YearRemodAdd'].count())","972dedd2":"#Plotting the categorical variables related toYear Built and Year Remodified to find which ones have correlation and can be dropped\nplt.figure(figsize=(20,15))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'YearBuilt', y = 'SalePrice', data = HouseAu)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'YearRemodAdd', y = 'SalePrice', data = HouseAu)","26181859":"# percentage of missing values in each column\nround(HouseAu.isnull().sum()\/len(HouseAu.index), 2)*100","6b838098":"# missing values in rows\nHouseAu.isnull().sum(axis=1)","a07f273c":"#Converting the binned year columns as object datatype\nHouseAu['YearBuilt']=HouseAu['YearBuilt'].values.astype(np.object)\nHouseAu['YearRemodAdd']=HouseAu['YearRemodAdd'].values.astype(np.object)","1015bf03":"#Cleaning up variable LotFrontage (Replacing NA => 0)\nHouseAu['LotFrontage'].replace({np.nan:'0'},inplace=True)\nHouseAu['LotFrontage']=HouseAu['LotFrontage'].values.astype(np.int64)\n100*(HouseAu['LotFrontage'].value_counts()\/HouseAu['LotFrontage'].count())\nHouseAu.info()","f44e8e98":"#MasVnrArea: Masonry veneer area in square feet\n100*(HouseAu['MasVnrType'].astype('category').value_counts()\/HouseAu['MasVnrType'].count())","e87aa436":"HouseAu=HouseAu.drop(['GarageYrBlt'],axis=1) # As it is same as Year Built","46269d0c":"#Replacing missing value with Unknown\nHouseAu['Electrical'].replace({np.nan:'Unknown'},inplace=True)\nprint(100*(HouseAu['Electrical'].value_counts()\/HouseAu['Electrical'].count()))\n#Imputing the minor category levels of Electrical\nHouseAu['Electrical'].replace({'FuseA':'Other'},inplace=True)\nHouseAu['Electrical'].replace({'FuseF':'Other'},inplace=True)\nHouseAu['Electrical'].replace({'FuseP':'Other'},inplace=True)\nHouseAu['Electrical'].replace({'Mix':'Other'},inplace=True)\nHouseAu['Electrical'].replace({'Unknown':'Other'},inplace=True)\nprint(100*(HouseAu['Electrical'].value_counts()\/HouseAu['Electrical'].count()))","51a5a4dd":"# checking whether some rows have more than 1 missing values\nlen(HouseAu[HouseAu.isnull().sum(axis=1) > 1].index)","794a1c12":"#NULL Rows in MasVnrType \nHouseAu=HouseAu.dropna(how='any',axis=0)","2edd9937":"#Dropping column MasVnrarea and LotFrontage as these are not adding value\nHouseAu=HouseAu.drop(['MasVnrArea','LotFrontage'],axis=1)","b88cf8f6":"# percentage of missing values in each column\nround(HouseAu.isnull().sum()\/len(HouseAu.index), 2)*100","a54a8c0b":"HouseAu.describe().transpose()","4b15758a":"#finding uniqness in records we see there is no attribute column with a single value\nHouseAu.nunique().sort_values(ascending =True)","770b9d93":"HouseAu.shape","05f5c6a4":"# all numeric (float and int) variables in the dataset\nHouseAu_numeric = HouseAu.select_dtypes(include=['float64', 'int64'])\nHouseAu_numeric.head()","a55839f9":"# dropping ID column \nHouseAu_numeric = HouseAu_numeric.drop(['Id'], axis=1)\nHouseAu_numeric.head()","dce58193":"# pairwise scatter plot\nplt.figure(figsize=(20, 10))\nsns.pairplot(HouseAu_numeric)\nplt.show()","5a2de1d1":"# correlation matrix\ncor = HouseAu_numeric.corr()\ncor","deac1bcb":"# plotting correlations on a heatmap\n# figure size\nplt.figure(figsize=(16,8))\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()","33f5a93d":"#Target variable: sale price of house\nsns.distplot(HouseAu['SalePrice'])\nplt.show()\n#Normally distributed SalePrice","efc4b913":"# Predictor variable: LotArea --> Lot size in square feet\nsns.distplot(HouseAu['LotArea'])\nplt.show()\n#Normally distributed --- LotArea","b4a903c4":"# Predictor variable: GrLivArea--> Above grade (ground) living area square feet\nsns.distplot(HouseAu['GrLivArea'])\nplt.show()\n#Normally distributed but slightly bimodal","4d8af581":"# Predictor variable: TotalBsmtSF ---> Total square feet of basement area\nsns.distplot(HouseAu['TotalBsmtSF'])\nplt.show()","410d3603":"# Predictor variable: 1stFlrSF: First Floor square feet\nsns.distplot(HouseAu['1stFlrSF'])\nplt.show()\n#Normally distributed with bimodaldistribution","eb7d3a4a":"# Predictor variable: 2ndFlrSF: Second floor square feet\nsns.distplot(HouseAu['2ndFlrSF'])\nplt.show()\n#Normally distributed ","81f0afde":"HouseAu.info()","489e7a8b":"# split into X and y\nX = HouseAu.loc[:, ['MSSubClass','MSZoning','LotArea','Street','LotShape','LandContour','Utilities',\n                    'LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','OverallQual',\n                    'OverallCond','YearBuilt','YearRemodAdd','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\n                    'ExterQual','ExterCond','Foundation','BsmtQual','BsmtExposure','TotalBsmtSF','Heating','HeatingQC','CentralAir','Electrical',\n                    '1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                    'BedroomAbvGr','KitchenAbvGr','KitchenQual','TotRmsAbvGrd','Functional','Fireplaces','FireplaceQu',\n                    'GarageType','GarageFinish','GarageCars','GarageArea','GarageCond','PavedDrive',\n                    'WoodDeckSF','OpenPorchSF','EnclosedPorch','PoolArea','Fence','MiscFeature',\n                    'MiscVal','MoSold','YrSold','SaleType','SaleCondition']]\n\ny = HouseAu['SalePrice']","de939452":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\nHouseAu_categorical = X.select_dtypes(include=['object'])\nHouseAu_categorical.head()","6e97e1a1":"# convert categorical variables into dummies\nHouseAu_dummies = pd.get_dummies(HouseAu_categorical, drop_first=True)\nHouseAu_dummies.head()","7f5edf69":"# drop categorical variables \nX = X.drop(list(HouseAu_categorical.columns), axis=1)","567150be":"# concat dummy variables with X\nX = pd.concat([X, HouseAu_dummies], axis=1)","4fb8ca09":"X.shape","de85dcb8":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","34714ad4":"# split into train and test\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","af19b52b":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n# cross validation\nfolds = 5\n\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","fca20172":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","d112ad69":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","53eb4992":"model_cv.best_params_","64e4b54f":"alpha =100\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","1bf75e62":"#Extracting the coefficients and model equation from lasso regression\nlasso.coef_","6c327ba0":"# lasso model parameters generation\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 1) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nprint(list(zip(cols, model_parameters)))","a65c71fd":"# model with optimal alpha\n# lasso regression\nlm1 = Lasso(alpha=100)\n#lm1 = Lasso(alpha=0.001)\nlm1.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score\n# predict\ny_train_pred = lm1.predict(X_train)\n#print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm1.predict(X_test)\n#print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))\n\n#New Code for R2\nr_square_score=r2_score(y_test,y_test_pred)\nprint(\"R Square score:{}\".format(r_square_score))","8889fd5c":"#AIC and BIC Calculation\nn= 1016 # n is equal to total datapoints on which model is built\nk= 100 # k is equal to number of predictor variables in model built\nresid=np.subtract(y_test_pred,y_test)\nrss=np.sum(np.power(resid,2))\nprint(\"RSS:{}\".format(rss))\naic=n*np.log(rss\/n)+2*k\nprint(\"AIC:{}\".format(aic))\nbic=n*np.log(rss\/n)+k*np.log(n)\nprint(\"BIC:{}\".format(bic))","b4bba56f":"#Predictor Variables from the Model built using Lasso Regression:\n[('constant', 180762.1), ('MSSubClass', -2299.5), ('LotArea', 6976.5), ('OverallQual', 12762.5), ('OverallCond', 5932.6), ('TotalBsmtSF', 13352.4), ('1stFlrSF', 0.0), ('2ndFlrSF', 6658.1), ('LowQualFinSF', -2033.2), ('GrLivArea', 27001.7), ('BsmtFullBath', 6507.3), ('BsmtHalfBath', 0.0), ('FullBath', 1629.3), ('HalfBath', 1682.0), ('BedroomAbvGr', -4823.0), ('KitchenAbvGr', -4531.2), ('TotRmsAbvGrd', 1524.2), ('Fireplaces', 2298.4), ('GarageCars', 5535.4), ('GarageArea', 322.1), ('WoodDeckSF', 582.8), ('OpenPorchSF', -0.0), ('EnclosedPorch', -1227.0), ('PoolArea', 4833.0), ('MiscVal', 1988.5), ('MoSold', 0.0), ('YrSold', 208.3), ('MSZoning_FV', 4201.8), ('MSZoning_RH', -53.1), ('MSZoning_RL', 2131.6), ('MSZoning_RM', 0.0), ('Street_Pave', 2159.7), ('LotShape_IR2', -282.6), ('LotShape_IR3', 25.1), ('LotShape_Reg', -822.2), ('LandContour_HLS', 1103.4), ('LandContour_Low', -1905.3), ('LandContour_Lvl', 0.0), ('Utilities_NoSeWa', -192.7), ('LotConfig_CulDSac', 2034.0), ('LotConfig_FR2', -1105.6), ('LotConfig_FR3', -753.2), ('LotConfig_Inside', -551.2), ('LandSlope_Mod', 458.8), ('LandSlope_Sev', -4745.6), ('Neighborhood_CollgCr', -925.1), ('Neighborhood_Crawfor', 3167.2), ('Neighborhood_Edwards', -1462.2), ('Neighborhood_Gilbert', -1587.3), ('Neighborhood_IDOTRR', 348.6), ('Neighborhood_Mitchel', -2035.4), ('Neighborhood_NAmes', -2336.2), ('Neighborhood_NWAmes', -1841.2), ('Neighborhood_NoRidge', 6125.6), ('Neighborhood_NridgHt', 3541.9), ('Neighborhood_OldTown', -2038.7), ('Neighborhood_Others', 2368.6), ('Neighborhood_Sawyer', -0.0), ('Neighborhood_SawyerW', 602.9), ('Neighborhood_Somerst', -7.3), ('Neighborhood_Timber', -971.6), ('Condition1_Feedr', 1509.1), ('Condition1_Norm', 5238.4), ('Condition1_PosA', 1450.9), ('Condition1_PosN', 1925.5), ('Condition1_RRAe', -334.5), ('Condition1_RRAn', 1224.4), ('Condition1_RRNe', 0.0), ('Condition1_RRNn', 557.4), ('Condition2_Feedr', -0.0), ('Condition2_Norm', 0.0), ('Condition2_PosA', 1055.2), ('Condition2_PosN', -15909.2), ('Condition2_RRAe', -0.0), ('Condition2_RRAn', -0.0), ('Condition2_RRNn', 0.0), ('BldgType_2fmCon', 0.0), ('BldgType_Duplex', -936.7), ('BldgType_Twnhs', -2828.2), ('BldgType_TwnhsE', -2544.1), ('HouseStyle_1.5Unf', 982.8), ('HouseStyle_1Story', 3463.2), ('HouseStyle_2.5Fin', -2102.8), ('HouseStyle_2.5Unf', -588.6), ('HouseStyle_2Story', -765.1), ('HouseStyle_SFoyer', 54.0), ('HouseStyle_SLvl', 955.6), ('YearBuilt_1925-1950', 2674.2), ('YearBuilt_1950-1975', 1812.2), ('YearBuilt_1976-1990', 3304.6), ('YearBuilt_1991-2000', 6088.8), ('YearBuilt_2001-2010', 7061.1), ('YearRemodAdd_1950-1975', 1089.8), ('YearRemodAdd_1976-1990', 0.0), ('YearRemodAdd_1991-2000', 1295.7), ('YearRemodAdd_2001-2010', 1487.3), ('RoofStyle_Gable', -0.0), ('RoofStyle_Gambrel', 73.5), ('RoofStyle_Hip', 651.7), ('RoofStyle_Mansard', 183.3), ('RoofStyle_Shed', 2116.0), ('RoofMatl_CompShg', 0.0), ('RoofMatl_Membran', -0.0), ('RoofMatl_Metal', 1124.1), ('RoofMatl_Roll', -141.8), ('RoofMatl_Tar&Grv', -934.8), ('RoofMatl_WdShake', -427.2), ('RoofMatl_WdShngl', 63.9), ('Exterior1st_AsphShn', -0.0), ('Exterior1st_BrkComm', -161.8), ('Exterior1st_BrkFace', 2355.7), ('Exterior1st_CBlock', -30.6), ('Exterior1st_CemntBd', 0.0), ('Exterior1st_HdBoard', -574.3), ('Exterior1st_ImStucc', -0.0), ('Exterior1st_MetalSd', 0.0), ('Exterior1st_Plywood', -393.2), ('Exterior1st_Stone', -99.7), ('Exterior1st_Stucco', -549.7), ('Exterior1st_VinylSd', -0.0), ('Exterior1st_Wd Sdng', -0.0), ('Exterior1st_WdShing', 238.0), ('Exterior2nd_AsphShn', 544.7), ('Exterior2nd_Brk Cmn', 0.0), ('Exterior2nd_BrkFace', -785.7), ('Exterior2nd_CBlock', -4.5), ('Exterior2nd_CmentBd', 830.9), ('Exterior2nd_HdBoard', 0.0), ('Exterior2nd_ImStucc', 862.6), ('Exterior2nd_MetalSd', 593.2), ('Exterior2nd_Other', 359.6), ('Exterior2nd_Plywood', -897.3), ('Exterior2nd_Stone', 398.1), ('Exterior2nd_Stucco', -494.5), ('Exterior2nd_VinylSd', -0.0), ('Exterior2nd_Wd Sdng', -1708.3), ('Exterior2nd_Wd Shng', -919.7), ('MasVnrType_BrkFace', 0.0), ('MasVnrType_None', 0.0), ('MasVnrType_Stone', 2715.7), ('ExterQual_Fa', -723.7), ('ExterQual_Gd', -8167.3), ('ExterQual_TA', -8529.6), ('ExterCond_Fa', -267.7), ('ExterCond_Gd', -0.0), ('ExterCond_Po', -0.0), ('ExterCond_TA', 430.7), ('Foundation_CBlock', 2254.4), ('Foundation_PConc', 1657.6), ('Foundation_Slab', 389.7), ('Foundation_Stone', 335.5), ('Foundation_Wood', -1002.7), ('BsmtQual_Others', 6617.0), ('BsmtQual_TA', 1466.7), ('BsmtExposure_Gd', 4489.8), ('BsmtExposure_Mn', -1743.4), ('BsmtExposure_No', -4071.3), ('BsmtExposure_No Basement', 298.7), ('Heating_GasA', -0.0), ('Heating_GasW', 407.8), ('Heating_Grav', 315.7), ('Heating_OthW', -1628.0), ('Heating_Wall', 186.3), ('HeatingQC_Fa', -796.3), ('HeatingQC_Gd', -265.3), ('HeatingQC_Po', -189.7), ('HeatingQC_TA', -0.0), ('CentralAir_Y', 595.5), ('Electrical_SBrkr', -176.6), ('KitchenQual_Fa', -3268.2), ('KitchenQual_Gd', -10980.7), ('KitchenQual_TA', -10858.8), ('Functional_Maj2', -786.1), ('Functional_Min1', 2032.5), ('Functional_Min2', 861.8), ('Functional_Mod', -290.7), ('Functional_Sev', -854.9), ('Functional_Typ', 5678.9), ('FireplaceQu_OK Fireplace', -1838.5), ('GarageType_BuiltIn', 766.3), ('GarageType_Detchd', 232.7), ('GarageType_No Garage', -468.6), ('GarageFinish_No Garage', 3184.3), ('GarageFinish_RFn', -1373.5), ('GarageFinish_Unf', -1305.2), ('GarageCond_OK', 1108.2), ('PavedDrive_P', -990.9), ('PavedDrive_Y', 195.3), ('Fence_No Fence', 171.3), ('MiscFeature_None', 0.0), ('MiscFeature_Othr', 371.7), ('MiscFeature_Shed', -0.0), ('MiscFeature_TenC', -2114.1), ('SaleType_CWD', 521.8), ('SaleType_Con', 643.6), ('SaleType_ConLD', 376.3), ('SaleType_ConLI', 1153.8), ('SaleType_ConLw', 562.1), ('SaleType_New', 5113.1), ('SaleType_Oth', 756.3), ('SaleType_WD', 0.0), ('SaleCondition_AdjLand', 441.1), ('SaleCondition_Alloca', -574.3), ('SaleCondition_Family', 478.6), ('SaleCondition_Normal', 2011.1), ('SaleCondition_Partial', -0.0)]","4647de58":"# split into X and y, X being selected from predictor variables found in Lasso model\nX = HouseAu.loc[:, ['MSSubClass','LotArea','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','OverallQual',\n                    'OverallCond','YearBuilt','YearRemodAdd','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\n                    'ExterQual','Foundation','BsmtQual','BsmtExposure','TotalBsmtSF','Heating','HeatingQC','CentralAir','Electrical',\n                    '2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                    'KitchenAbvGr','KitchenQual','TotRmsAbvGrd','Functional','Fireplaces','FireplaceQu',\n                    'GarageType','GarageFinish','GarageCars','GarageCond','WoodDeckSF','EnclosedPorch','PoolArea',\n                    'SaleType','SaleCondition']]\n\ny = HouseAu['SalePrice']","75030215":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\nHouseAu_categorical = X.select_dtypes(include=['object'])\nHouseAu_categorical.head()","8f8bdfe4":"# convert categorical variables into dummies\nHouseAu_dummies = pd.get_dummies(HouseAu_categorical, drop_first=True)\nHouseAu_dummies.head()","15b42c94":"# drop categorical variables \nX = X.drop(list(HouseAu_categorical.columns), axis=1)","3b6ac9a4":"# concat dummy variables with X\nX = pd.concat([X, HouseAu_dummies], axis=1)","2aeae38e":"X.shape","298a7fa8":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","495912d7":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","b94091b5":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=200]\ncv_results.head()","14524e85":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","4db4c872":"model_cv.best_params_","7105f971":"alpha = 10\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\n#Predictor Variables from the Model built using Ridge Regression:\nridge.coef_","3ea6d8f6":"# ridge model parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))\n#Predictor Variables from the Model built using Ridge Regression:","42ec0e9e":"# model with optimal alpha\n# Ridge regression\nlm2 = Ridge(alpha=10)\n#lm2 = Ridge(alpha=0.001)\nlm2.fit(X_train, y_train)\n\nfrom sklearn.metrics import r2_score\n# predict\ny_train_pred = lm2.predict(X_train)\n#print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm2.predict(X_test)\n#print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))\n\n#New Code for R2\nr_square_score=r2_score(y_test,y_test_pred)\nprint(\"R Square score:{}\".format(r_square_score))","fa67b8af":"#AIC and BIC Calculation\nn= 1016 # n is equal to total datapoints on which model is built\nk= 50 # k is equal to number of predictor variables in model built\nresid=np.subtract(y_test_pred,y_test)\nrss=np.sum(np.power(resid,2))\nprint(\"RSS:{}\".format(rss))\naic=n*np.log(rss\/n)+2*k\nprint(\"AIC:{}\".format(aic))\nbic=n*np.log(rss\/n)+k*np.log(n)\nprint(\"BIC:{}\".format(bic))","9b7f0a89":"####Univariate Analysis on Key predictor variables and the target variable","28638fb4":"### ----------HOUSING PRICE PREDICTION ----------","3492ae0e":"##### This is quite hard to read, and we can rather plot correlations between variables. Also, a heatmap is pretty useful to visualise multiple correlations in one plot.","e6654143":"#### Step 4: Data Preparation for Model Building","71803f24":"#### Based on the predictor variables having high coefficients in Lasso Regression model, we will shortlist these and perform Ridge regression to further regularize the regression model","7448a50e":"###### Let's now prepare the data and build the model.","44b013b5":"####### A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual value and flip them at a higher price. For the same purpose, the company has collected a data set from house sales in Australia. The company is looking at prospective properties to buy to enter the market. The obective of the study is to build a regression model using regularization, so as to predict the actual value of the prospective properties and decide whether to invest in them or not.","2ce88b21":"#######Purpose of this study is as follows:\n\n1. Build a model which the company can use to predict Sale Price of houses in Australia\n2. Find out which variables are significant in predicting the price of a house\n3. How well those variables describe the price of a house\n4. Determine the optimal value of lambda for ridge and lasso regression.","5b904ce6":"##### Let's now make a pairwise scatter plot and observe linear relationships.","684c1389":"####### The solution is divided into the following sections:\n####### 1.Data understanding and exploration; \n####### 2.Data cleaning; \n####### 3.Data preparation; \n####### 4.Model building and evaluation.","a405b6dc":"#### Ridge and Lasso Regression\u00b6","6e103fd1":"### Model Building and Evaluation","666959c6":"#### Step 1: Importing Libraries and Importing Original Dataset","63390239":"#### Step 2: Data understanding and exploration","e3101bb1":"#### As Alpha parameter in scikit learn is nothing but theory lambda parameter, so we have from Lasson Rigression, alpha = 100 and from Ridge Regression, alpha = 10. These are the optimal values of lambda or the hyerparameters for regularization of the regression model.","ab83bba6":"#### Based on both the Ridge and LAsso Models, the key predictor variables for predicting the Sale Price of Houses in Australia are as follows:\n 'MSSubClass', 'LotArea', 'OverallQual','OverallCond','TotalBsmtSF','BsmtFullBath','LowQualFinSF','BsmtHalfBath','YearBuilt_2001-2010',\n'HeatingQC','SaleType','SaleCondition','GarageCond','Exterior1st','Neighborhood','Fireplaces','Condition1','RoofStyle', 'Exterior2nd_Stone','KitchenQual',\n'Functional_Mod','GarageType_BuiltIn','Heating_Wall','Heating'","cd024199":"#### First Lasso regression will be done to reduce the number of predictor variables","729a6e23":"#### Ridge Regression","b502c274":"#### From the Heatmap we can observe that attributes like OverQual, GrLivArea, GarageCars,GarageArea,FullBath and few other variables clearly have a strong correlation with the dependant variable SalePrice","ebbe37f7":"#### Step 3: Data Cleaning - Missing Value, Duplicates, Imputing, Dropping, Deleting, Exploration","08d21dee":"##### From the R square value we see that the model \nbuilt using lasso regression can have 65% accuracy in its prediction capability.\nR-squared (R2) explains the proportion of variation in the outcome (Sale Price) that is explained by the predictor variables. ","7d232206":"### Data Exploration\nTo perform linear regression, the (numeric) target variable should be linearly related to at least one another numeric variable. Let's see whether that's true in this case.\nWe'll first subset the list of all (independent) numeric variables, and then make a pairwise plot.","2c2dec34":"#### The answer to how well these variables describe the price of a house, is determined from the value of the R2-score and AIC, BOC values.\nThe model prediction capability is only 69%. Which is moderately OK. R2 score provides a measure of how well future samples are likely to be predicted by the model. \nIn estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.In this model, the AIC is 21000 whic is high. Also the BIC which provides a measure of penalty for additional variables. It is 21000 again high."}}