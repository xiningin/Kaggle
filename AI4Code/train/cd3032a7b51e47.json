{"cell_type":{"0849a78a":"code","8917dc1b":"code","69e88e8c":"code","c4f203f9":"code","77e86da1":"code","0dc8ab56":"code","12b9d3a0":"code","a7030191":"code","54f596a7":"code","3e281726":"code","8b63ee3f":"code","8dd6ec78":"code","b0f165b1":"code","22b3b970":"code","b6e470e0":"code","e63ff62b":"code","9f2895cc":"code","16b8b0fb":"code","c8b9c16a":"code","a163d955":"code","b23832ff":"code","e584dc7a":"code","7615f041":"code","a30f94ea":"code","8e428053":"code","1415a5ed":"code","cd66210d":"code","4bff5469":"code","c7ee2239":"code","01becc21":"code","93186910":"code","4193ae25":"code","bbb494d8":"code","9eac4055":"code","4fb766ec":"code","f95f343b":"code","cd630452":"code","fa9ce9cb":"code","6d79f015":"code","b3f90324":"code","c30b01da":"code","f5ea6899":"code","c20b41b5":"code","dbeef27a":"code","813f1a28":"code","768d9475":"code","38e8e4d5":"code","13f6345a":"code","a769c324":"code","278309ff":"code","73c3a7d0":"code","99ef946c":"code","1b091d64":"code","e9d935a3":"code","ece40616":"code","d17e3662":"code","a5b1efb0":"code","c2101b9b":"code","27432be4":"code","a1fdf9a1":"code","1b412718":"code","728ee61e":"code","fdb6dc80":"code","fa8ff987":"code","2d9ea501":"code","6c54a021":"code","674374ad":"code","3c829691":"code","8edcffe6":"markdown","80e6feac":"markdown","ff147f91":"markdown","8d1e1c1f":"markdown","e66daeed":"markdown","90deec40":"markdown","16bc6f9a":"markdown","4401e365":"markdown","08f871a0":"markdown","c7111443":"markdown","1c4556c2":"markdown","4ad1d472":"markdown","033c028c":"markdown","fbbf26bb":"markdown","60b76566":"markdown","cc3e8f8c":"markdown","79acf3ff":"markdown","a52c26df":"markdown","48d0a9e6":"markdown","1d020e79":"markdown","4bf8238f":"markdown","e9ae5355":"markdown","9caf63b2":"markdown","5f31c15a":"markdown","9576a10e":"markdown","a6364c2a":"markdown","cbbc1c8e":"markdown","fff3daa6":"markdown","401904a1":"markdown","5a2890a8":"markdown","753dfb85":"markdown","f60c5213":"markdown","58ebf9e1":"markdown","84d9b07f":"markdown","763c62e7":"markdown","9c8b4d39":"markdown","630aa62f":"markdown","20d44662":"markdown","8cb0b17b":"markdown","a646f910":"markdown","c91fd465":"markdown","f887e3ac":"markdown","c28db287":"markdown","b1b1fd37":"markdown","8fd8f3b0":"markdown","f1641f7d":"markdown","cdbe4dac":"markdown","120850ce":"markdown","8a329189":"markdown","a897e1ba":"markdown","86e718e3":"markdown","5fbfc2a8":"markdown","86c56e0d":"markdown","3c7ff4f1":"markdown","e4915182":"markdown","5a9b0caa":"markdown","a99057f5":"markdown","722f98f2":"markdown","d0d4a1b2":"markdown","bbd8bd91":"markdown","7e2e139d":"markdown","e4136595":"markdown","3acc6780":"markdown","688b4671":"markdown","fe17c0c6":"markdown","9af5c2a7":"markdown","017f3a1c":"markdown","14231d08":"markdown","7d59078d":"markdown","deb90829":"markdown","b4ca63e3":"markdown","2b7b8a19":"markdown","0b011a90":"markdown"},"source":{"0849a78a":"!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java","8917dc1b":"import os\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","69e88e8c":"!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","c4f203f9":"COVID_INDEX = '..\/input\/luceneindexcovidparagraph20200410\/lucene-index-covid-paragraph-2020-04-10'","77e86da1":"searcher = pysearch.SimpleSearcher(COVID_INDEX)","0dc8ab56":"def get_articles(query):\n    hits = searcher.search(query)\n    #print(len(hits))\n    # Prints the first 10 hits\n    return hits","12b9d3a0":"query = 'range of incubation periods for COVID-19'\nhits = get_articles(query)\nfor i in range(0, 10):\n    #print some relevant fields\n    print(f'{i+1} {hits[i].docid} {hits[i].score} {hits[i].lucene_document.get(\"title\")} {hits[i].lucene_document.get(\"doi\")}')","a7030191":"hits[0].contents.split('\\n')","54f596a7":"import json\ndef get_para_results(query):\n    hits = searcher.search(query,10) \n    print(len(hits))\n    temp = {} # to store the doi of the articles being returned so we know if the article is repeated\n    i = 0\n    output = []\n    while i<len(hits) and i<10:\n        outJson = {}\n        outJson['rank'] = i+1\n        # check if the current article has a paragraph returned or not ('has_full_text' in the dataset)\n        if '.' in hits[i].docid:\n            doc_id = hits[i].docid.split('.')[0]\n            para_id = hits[i].docid.split('.')[1]\n            doi = hits[i].lucene_document.get('doi')\n            paragraph = {}\n            paragraph['score'] = hits[i].score\n            paragraph['text'] = hits[i].contents.split('\\n')[-1] # get the last element, since the contents are sorted as [title, abstract, paragraph]\n            paragraph['id'] = para_id\n            # check if the doi (same article) has not appeared before in the list\n            if doi not in temp:\n                outJson['abstract'] = hits[i].lucene_document.get('abstract') # include abstract if new article\n                article_data = json.loads(searcher.doc(doc_id).lucene_document().get('raw')) # get all the relevant data from the dataset \n                if 'body_text' in article_data:\n                    outJson['body_text'] = article_data['body_text'] # include 'body_text' in case needed later\n                temp[doi] = i\n            outJson['paragraphs'] = []\n            outJson['paragraphs'].append(paragraph)\n        else:\n            # no paragraph present, which means article does not have full text available\n            outJson['abstract'] = hits[i].lucene_document.get('abstract')\n            outJson['score'] = hits[i].score\n        outJson['title'] = hits[i].lucene_document.get('title')\n        outJson['sha'] = hits[i].lucene_document.get('sha')\n        outJson['doi'] = hits[i].lucene_document.get('doi')\n        output.append(outJson)\n        i+=1\n    return output","3e281726":"query = 'range of incubation periods for COVID-19'\ni = 1\nfor item in get_para_results(query):\n    if i>10:\n        break\n    print(item)\n    i+=1","8b63ee3f":"def information_retrieval(file_name, topk = 10):\n\n    with open(file_name) as f:\n        json_file = json.load(f)\n    subtasks = json_file[\"sub_task\"]\n    \n    all_results = []\n    data_for_qa = []\n    for item in subtasks:\n        questions = item[\"questions\"]\n        for query in questions:\n            result_item = {\"question\" : query}\n            retri_result = get_para_results(query)\n            result_item[\"data\"] = retri_result\n\n            qa_item = {\"question\": query}\n            context = []\n            titles = []\n            doi = []\n            count = 1\n            for item in retri_result:\n                if count>topk:\n                    break\n                if 'abstract' in item and len(item['abstract']) > 0:\n                    context.append(item['abstract'])\n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n                if 'paragraphs' in item:\n                    context.append(item['paragraphs'][0]['text'])   \n                    doi.append(item[\"doi\"])\n                    titles.append(item[\"title\"])\n                    count+=1\n\n            qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n            all_results.append(result_item)\n            data_for_qa.append(qa_item)\n\n    return data_for_qa\n\ndef parse_ir_results(query, retri_result, topk = 10):\n    all_results = []\n    data_for_qa = []\n    qa_item = {\"question\": query}\n    result_item = {\"question\" : query}\n    result_item[\"data\"] = retri_result\n    context = []\n    titles = []\n    doi = []\n    count = 1\n    for item in retri_result:\n        if count>topk:\n            break\n        if 'abstract' in item and len(item['abstract']) > 0:\n            context.append(item['abstract'])\n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n        if 'paragraphs' in item:\n            context.append(item['paragraphs'][0]['text'])   \n            doi.append(item[\"doi\"])\n            titles.append(item[\"title\"])\n            count+=1\n    qa_item[\"data\"] = {\"answer\": \"\", \"context\": context, \"doi\": doi, \"titles\": titles}\n\n    all_results.append(result_item)\n    data_for_qa.append(qa_item)    \n\n    return all_results, data_for_qa\n\n    \ndef information_retrieval_query(query):\n\n    retri_result = get_para_results(query)\n    all_results, data_for_qa = parse_ir_results(query, retri_result ,topk = 20)\n    \n    return all_results, data_for_qa","8dd6ec78":"### 3.1 install the prerequisite\nimport os\nimport sys\nimport json\n\n!pip uninstall tensorflow -y\n!pip uninstall tensorflow-gpu -y\n!pip install tensorflow==1.13.1\n!pip install caireCovid==0.1.8","b0f165b1":"import tensorflow as tf\nimport caireCovid\nfrom caireCovid import QaModule\nfrom caireCovid.qa_utils import stop_words\nimport math","22b3b970":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","b6e470e0":"### 3.2 Check all version\nprint(tf.__version__)","e63ff62b":"# QA System\nclass QA_System():\n    def _init_(self):\n        # Load the QA models. Please refer to [Github](https:\/\/github.com\/yana-xuyan\/caireCovid) for details.\n        self.model = QaModule(['mrqa', 'biobert'], [\"\/kaggle\/input\/pretrained-qa-models\/mrqa\/1564469515\", \"\/kaggle\/input\/pretrained-qa-models\/biobert\/1585470591\"], \\\n                              \"\/kaggle\/input\/xlnetlargecased\/xlnet_cased_L-24_H-1024_A-16\/spiece.model\", \"\/kaggle\/input\/pretrained-qa-models\/bert_config.json\", \\\n                              \"\/kaggle\/input\/bert-base-cased\/vocab.txt\")\n    def getAnswer(self, query):\n        _, data_for_qa = information_retrieval_query(query)\n        answers =  self.model.getAnswers(data_for_qa)\n        return answers\n    def getAnswers(self, filename):\n        _, data_for_qa = information_retrieval(query)\n        answers = self.model.getAnswers(data_for_qa)\n        return answers\n    def makeFormatAnswers(self, answers):\n        format_answers = []\n        for i in range(len(answers[0]['data']['answer'])):\n                format_answer = {}\n                format_answer['question'] = answers[0]['question']\n                format_answer['answer'] = answers[0]['data']['answer'][i]\n                format_answer['context'] = answers[0]['data']['context'][i]\n                format_answer['doi'] = answers[0]['data']['doi'][i]\n                format_answer['title'] = answers[0]['data']['title'][i]\n                format_answer[\"confidence\"] = answers[0]['data']['confidence'][i]\n                format_answer[\"raw\"] = answers[0]['data']['raw'][i]\n                format_answers.append(format_answer)\n        return format_answers\n\ndef get_QA_answer_api(query):\n    url = \"http:\/\/eez114.ece.ust.hk:5000\/query_qa\"\n    payload = \"{\\n\\t\\\"text\\\": \\\"\"+query+\"\\\"\\n}\"\n    headers = {\n        'Content-Type': \"application\/json\",\n        'cache-control': \"no-cache\",\n        'Postman-Token': \"696fa512-5fed-45ca-bbe7-b7a1b4d19fe4\"\n    }\n    response = requests.request(\"POST\", url, data=payload, headers=headers)\n    response = response.json()\n    return response","9f2895cc":"import argparse\nimport sys\nimport pandas as pd\nimport csv\nimport requests\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.tokenize import sent_tokenize # use sentence tokenize\nfrom IPython.core.display import display, HTML","16b8b0fb":"from nltk import word_tokenize, pos_tag, sent_tokenize\nfrom caireCovid.qa_utils import stop_words\nstop_words.append('including')\n\ndef rankAnswers(answers):\n    for item in answers:\n        query = item[\"question\"]\n        context = item['context']\n        # make new query with only n. and adj.\n        tokens = word_tokenize(query.lower())\n        tokens = [word for word in tokens if word not in stop_words]\n        tagged = pos_tag(tokens)\n        query_token = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n        text = context.lower()\n        count = 0\n        text_words = word_tokenize(text)\n        for word in text_words:\n            if word in query_token:\n                count += 1\n            \n        match_number = 0\n        for word in query_token:\n            if word == 'covid-19':\n                continue\n            if word in text_words:\n                match_number += 1\n        matching_score = count \/ (1 + math.exp(-len(text_words)+50))\/ 5 + match_number*10\n        item['matching_score'] = matching_score\n        item['rerank_score'] = matching_score + 0.5 * item['confidence']\n    \n    # sort QA results\n    answers.sort(key=lambda k: k[\"rerank_score\"], reverse=True)\n#     print([item['rerank_score'] for item in answers])\n    return answers\n\ndef highlight_qaresult(qaresult):\n    if qaresult == []:\n        print('API broken')\n        return 1\n    ## tokenize query\n    query = qaresult[0]['question']\n    query_tokens = word_tokenize(query.lower())\n    query_tokens = [word for word in query_tokens if word not in stop_words]\n    tagged = pos_tag(query_tokens)\n    query_tokens = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n\n    ## highlihgt answer\n    for i in range(len(qaresult)):\n        context_1 = \"<style type='text\/css'>mark { background-color:yellow; color:black; } <\/style>\"\n        golden = qaresult[i]['answer']\n        context = qaresult[i]['context']\n        context_sents = sent_tokenize(context)\n        golden_sents = sent_tokenize(golden)\n        for sent in context_sents:\n            if sent not in golden:\n                context_1 += sent\n            else:\n                context_1 += \"<mark>\"\n                for word in sent.split():\n                    word_tokens = word_tokenize(word)\n                    if len(word_tokens) > 1:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \"<\/b>\"\n                            else:\n                                context_1 = context_1 + j\n                        context_1 = context_1 + \" \"\n                    else:\n                        for j in word_tokens:\n                            if j.lower() in query_tokens:\n                                context_1 = context_1 + \"<b>\" + j + \" <\/b>\"\n                            else:\n                                context_1 = context_1 + j + \" \"\n                context_1 += \" <\/mark>\"\n        qaresult[i]['context'] = context_1\n    return qaresult\n\ndef display_QA(result):\n    result = highlight_qaresult(result)\n    pdata = []\n    count = 0\n    for i in range(len(result)):\n        count += 1\n        line = []\n        context_1 = \"<div> \"\n        context = result[i]['context']\n        context_1 = context_1 + context\n        context_1 += \" <\/div>\"\n        line.append(context_1)\n        context_2 = '<a href= \"https:\/\/doi.org\/'\n        context_2 += result[i]['doi']\n        context_2 += ' target=\"_blank\">'\n        context_2 += result[i]['title']\n        context_2 += '<\/a>'\n        line.append(context_2)\n        pdata.append(line)\n        if count > 5:\n            break\n    df = pd.DataFrame(pdata, columns = ['QA results', 'title'])\n    df = df.style.set_properties(**{'text-align': 'left','mark-color': 'red'})\n    display(df)","c8b9c16a":"!pip install easydict\n!pip install covidSumm==0.1.4\n!pip install fairseq","a163d955":"import covidSumm\nimport requests\nimport json\nimport os\nimport argparse","b23832ff":"from covidSumm.abstractive_utils import get_ir_result, result_to_json, get_qa_result\nfrom covidSumm.abstractive_model import abstractive_summary_model\nfrom covidSumm.abstractive_config import set_config\nfrom covidSumm.abstractive_bart_model import *","e584dc7a":"def get_summary_list(article_list, abstractive_model):\n    summary_list = []\n    for i in range(len(article_list)):\n        article = article_list[i]\n        summary_results = abstractive_model.generate_summary(article)\n        result = \"\"\n        for item in summary_results:\n            result += item.replace('\\n', ' ')\n        summary_list.append(result)\n    return summary_list\n\ndef get_answer_summary(query, abstractive_model):\n    paragraphs_list = get_qa_result(query, topk = 3)\n    answer_summary_list = abstractive_model.generate_summary(paragraphs_list)\n    answer_summary = \"\"\n    for item in answer_summary_list:\n        answer_summary += item.replace('\\n', ' ')\n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary\n    answer_summary_json['question'] = query\n    return answer_summary_json\n\ndef get_article_summary(query, abstractive_summary_model):\n    article_list, meta_info_list = get_ir_result(query, topk = 10)  \n    summary_list = get_summary_list(article_list, abstractive_summary_model)\n    summary_list_json = []\n    \n    for i in range(len(summary_list)):\n        json_summary = {}\n        json_summary = result_to_json(meta_info_list[i], summary_list[i])\n        summary_list_json.append(json_summary)\n\n    return summary_list_json\n\ndef get_bart_answer_summary_from_qa(query, qa_result, bart_model):\n    # we select top3\n    paragraphs_list = []\n    topk = 3\n\n    for i in range(topk):\n        if 'context' in qa_result[i].keys():\n            one_line = {}\n            one_line['src'] = qa_result[i]['context']\n            one_line['tgt'] = \"\"\n            paragraphs_list.append(one_line)\n    \n    answer_summary_list = bart_model.bart_generate_summary(paragraphs_list)\n    answer_summary_result = \"\"\n    for item in answer_summary_list:\n        answer_summary_result += item.replace('\\n', ' ')\n    \n    answer_summary_json = {}\n    answer_summary_json['summary'] = answer_summary_result\n    answer_summary_json['question'] = query\n    return answer_summary_json","7615f041":"args = set_config()\nargs['model_path'] = '\/kaggle\/input\/carieabssummmodel\/'\nsummary_model_1 = abstractive_summary_model(config = args)","a30f94ea":"model_path = \"\/kaggle\/input\/bartsumm\/bart.large.cnn\"\nsummary_model_2 = Bart_model(model_path)","8e428053":"from IPython.core.display import display, HTML\nimport pandas as pd\n\ndef display_summary(ans_summary_json, model_type):\n    question = ans_summary_json['question']\n    text = ans_summary_json['summary']\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n    display(HTML(question_HTML))\n\n    execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>' + model_type + ' Abstractive Summary:<\/b>: '+text+'<\/div>'\n    display(HTML(execSum_HTML))\n\ndef display_article_summary(result, query):\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+query+'<\/div>'\n    pdata = []\n    abstract = \"\"\n    summary = \"\"\n    for i in range(len(result)):\n        if 'abstract' in result[i].keys():\n            line = []\n            context_2 = '<a href= \"https:\/\/doi.org\/'\n            context_2 += result[i]['doi']\n            context_2 += ' target=\"_blank\">'\n            context_2 += result[i]['title']\n            context_2 += '<\/a>'\n            line.append(context_2)\n            \n            abstract = \"<div> \" \n            abstract += result[i]['abstract']\n            abstract += \" <\/div>\"\n            line.append(abstract)\n            summary = \"<div> \" + result[i]['summary'] + \" <\/div>\"\n            line.append(summary)\n\n\n            pdata.append(line)\n    display(HTML(question_HTML))\n    df = pd.DataFrame(pdata, columns = ['Title','Abstract','Summary'])\n    HTML(df.to_html(render_links=True, escape=False))\n#     display(HTML(df.to_html(render_links=True, escape=False)))\n    df = df.style.set_properties(**{'text-align': 'left'})\n    display(df)\n","1415a5ed":"query = \"How incubation period for COVID-19 varies across age?\"","cd66210d":"def run_example(query):\n    # Given one query, we retrieve the relevant paragraphs and feed the (paragraph, query) pairs into the QA system \n    qa_result = get_QA_answer_api(query)\n    # Answer Reranking\n    qa_result = rankAnswers(qa_result)\n    \n    # Input \"summary_model_2\" is the BART summarization model.\n    # Function \"get_bart_answer_summary\" is loaded from covidSumm.abstractive_bart_model\n    # Given one query, we take top-3 reranked paragraphs from the QA module and summarize them into one paragraph\n    answer_summary_2 = get_bart_answer_summary_from_qa(query, qa_result, summary_model_2)\n    display_summary(answer_summary_2, 'BART')\n    display_QA(qa_result)","4bff5469":"run_example(query)","c7ee2239":"query = \"What do we know about diagnostics and surveillance?\"\nrun_example(query)","01becc21":"query = \"What has been published concerning systematic, holistic approach to diagnostics?\"\nrun_example(query)","93186910":"query = \"how widespread current exposure is to be able to make immediate policy recommendations on mitigation measures?\"\nrun_example(query)","4193ae25":"query = \"denominators for testing and a mechanism for rapidly sharing demographics, to the extent possible?\"\nrun_example(query)","bbb494d8":"query = \"use of serosurveys such as convalescent samples?\"\nrun_example(query)","9eac4055":"query = \"use of screening of neutralizing antibodies such as ELISAs?\"\nrun_example(query)","4fb766ec":"query = \"increase capacity on existing diagnostic platforms and tap into existing surveillance platforms?\"\nrun_example(query)","f95f343b":"query = \"how states might leverage universities and private laboratories for testing purposes?\"\nrun_example(query)","cd630452":"query = \"development of a rapid influenza test and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy?\"\nrun_example(query)","fa9ce9cb":"query = \"targeted surveillance experiments using PCR in a defined area to start testing and report to a specific entity?\"\nrun_example(query)","6d79f015":"query = \"separation of assay development issues from instruments?\"\nrun_example(query)","b3f90324":"query = \"role of the private sector to help quickly migrate assays onto those devices?\"\nrun_example(query)","c30b01da":"query = \"efforts to genetic drift or mutations?\"\nrun_example(query)","f5ea6899":"query = \"avoid locking into specific reagents and surveillance\/detection schemes?\"\nrun_example(query)","c20b41b5":"query = \"latency issues?\"\nrun_example(query)","dbeef27a":"query = \"when there is sufficient viral load to detect the pathogen?\"\nrun_example(query)","813f1a28":"query = \"understanding of what is needed in terms of biological and environmental sampling?\"\nrun_example(query)","768d9475":"query = \"use of host response markers cytokines?\"\nrun_example(query)","38e8e4d5":"query = \"policies and protocols for screening and testing?\"\nrun_example(query)","13f6345a":"query = \"policies to mitigate the effects on swabs?\"\nrun_example(query)","a769c324":"query = \"policies to mitigate the effects on swabs?\"\nrun_example(query)","278309ff":"query = \"technology roadmap for diagnostics?\"\nrun_example(query)","73c3a7d0":"query = \"barriers to developing and scaling up market forces?\"\nrun_example(query)","99ef946c":"query = \"what the literature reports about CRISPR to improve response times and employ more holistic approaches to COVID-19 and future diseases?\"\nrun_example(query)","1b091d64":"query = \"coupling genomics and diagnostic testing on a large scale?\"\nrun_example(query)","e9d935a3":"query = \"enhance capabilities for rapid sequencing?\"\nrun_example(query)","ece40616":"query = \"enhance capabilities for bioinformatics to target regions of the genome?\"\nrun_example(query)","d17e3662":"query = \"people for sequencing with advanced analytics for unknown pathogens?\"\nrun_example(query)","a5b1efb0":"query = \"technology for sequencing with advanced analytics for unknown pathogens?\"\nrun_example(query)","c2101b9b":"query = \"data for sequencing with advanced analytics for unknown pathogens?\"\nrun_example(query)","27432be4":"query = \"explore capabilities for distinguishing naturally-occurring pathogens from intentional?\"\nrun_example(query)","a1fdf9a1":"query = \"potential sources of future spillover or ongoing exposure for evolutionary hosts (e.g., bats)?\"\nrun_example(query)","1b412718":"query = \"potential sources of future spillover or ongoing exposure for heavily trafficked?\"\nrun_example(query)","728ee61e":"query = \"potential sources of future spillover or ongoing exposure for heavily trafficked?\"\nrun_example(query)","fdb6dc80":"query = \"potential sources of future spillover or ongoing exposure for domestic food?\"\nrun_example(query)","fa8ff987":"query = \"potential sources of future spillover or ongoing exposure for companion species?\"\nrun_example(query)","2d9ea501":"query = \"what about One Health surveillance of humans?\"\nrun_example(query)","6c54a021":"query = \"what about potential sources of future spillover or ongoing exposure for environmental risk factors?\"\nrun_example(query)","674374ad":"query = \"what about potential sources of future spillover or ongoing exposure for demographic risk factors?\"\nrun_example(query)","3c829691":"query = \"what about potential sources of future spillover or ongoing exposure for occupational risk factors?\"\nrun_example(query)","8edcffe6":"# what the literature reports about development of a rapid influenza test and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy?","80e6feac":"Getting the pyserini library, which is anserini wrapped with python:","ff147f91":"# what the literature reports about coupling genomics and diagnostic testing on a large scale?\n","8d1e1c1f":"# 3. Abstractive Summerization","e66daeed":"Write down whatever question you are interested in below. For example:","90deec40":"# what the literature reports about potential sources of future spillover or ongoing exposure for companion species?","16bc6f9a":"# what the literature reports about use of host response markers cytokines to detect early disease or predict severe disease progression\uff1f","4401e365":"# what the literature reports about the role of the private sector to help quickly migrate assays onto those devices?","08f871a0":"# what the literature reports about enhance capabilities for rapid sequencing?","c7111443":"# what about potential sources of future spillover or ongoing exposure for demographic risk factors?","1c4556c2":"We use [Anserini](https:\/\/github.com\/castorini\/anserini) to create the search engine to retrieve a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine **Lucene**. Although Lucene has been used widely to build industry search engine applications, its complex indexing and lack of documentation for ad hoc experimentation and testing on standard test sets, has made it less popular in the information retrieval community. Anserini uses the Lucene indexing to create an easy-to-understand module. Standard ranking algorithms(e.g. bag of words, BM25) have been implemented in the module, which enables us to use Lucene for our application. Thanks to Jimmy Lin, we make this platform based on his [notebook](https:\/\/github.com\/castorini\/anserini-notebooks\/blob\/master\/pyserini_covid19_paragraph.ipynb). Since the disk is not large enough for saving the whole dataset with index and other models, we use his API to get the information retrieval results.","4ad1d472":"# what the literature reports about policies to mitigate the effects on reagents?","033c028c":"## 1.2 Search Engine","fbbf26bb":"## 2.2 Highlights Generation","60b76566":"# what the literature reports about technology for sequencing with advanced analytics for unknown pathogens?","cc3e8f8c":"# what about potential sources of future spillover or ongoing exposure for occupational risk factors\n","79acf3ff":"# What do we know about diagnostics and surveillance?","a52c26df":"In this project, for sake of efficiency, ***we build an API for the Search Engine module to integrate it with the Question Answering module into this system***.","48d0a9e6":"# what the literature reports about efforts to genetic drift or mutations?","1d020e79":"* Now we initiate our **Summerization model UniLM**.","4bf8238f":"* We initiate our **Summerization model BART**.","e9ae5355":"In this part, we introduce the word matching highlight strategy. There are two main components in this part: (1) word matching score calculate. (2) rerank and display. The input is the Question Answering result and for the output we display the most relevant paragraph with the highlighted answer.\n+ **POS-tag based scoring:** \nWe calculate a similarity score between a QA result and a given query based on keyword matching. To obtain this score, we first select important keywords based on POS-tagging - we consider words with {NN(noun), VB (verb), JJ (adjective)} POS-tags to be important keywords. Based on the set of filtered keywords, we count the word-match between QA-result keywords and query keywords. Higher the count is, more similar the QA-result and the query are. To penalize the matching scores of short retrieved paragraphs, we normalize them with sigmoid value computed from paragraph length. Moreover, we reward the paragraphs with more diverse keywords from query, which is the major factor of matching scores.\n\n+ **rerank and display:**\nThe re-ranking score is based on both the word matching score above and the confidence score from the QA system. The QA results are again ranked and displayed. ","9caf63b2":"The first two are title and abstract, and the last element is the matched paragraph. We can see that the paragraph matched is actually quite good at answering the query. The next step would be to get an answer in a concised form by passing the matched paragraphs to a QA model.","5f31c15a":"### 1.2.1 install Python dependencies and pre-built index","9576a10e":"# what the literature reports about separation of assay development issues from instruments?","a6364c2a":"### 1.2.2 example for using Anserini\nHere we type query 'range of incubation periods for COVID-19' in the search engine and it will return the top10 revelant items Anserini gets in the dataset.","cbbc1c8e":"To accelerate our QA system, instead of running corresponding process in notebook, we leverage an API to make better use of the local GPUs. We also make our QA system public to the community by making python package for downloading with the following command:\n> pip install caireCovid\n\nFor anyone who may be interested in the implementation details, lease refer to our [Github repository](https:\/\/github.com\/yana-xuyan\/caireCovid). Some examples and other useful resources are also available there.","fff3daa6":"# what the literature reports about efforts to avoid locking into specific reagents and surveillance\/detection schemes?","401904a1":"# what the literature reports about data for sequencing with advanced analytics for unknown pathogens?","5a2890a8":"# what the literature reports about technology roadmap for diagnostics?","753dfb85":"# what the literature reports about policies and protocols for screening and testing.","f60c5213":"# what the literature reports about denominators for testing and a mechanism for rapidly sharing demographics, to the extent possible? ","58ebf9e1":"# what the literature reports about when there is sufficient viral load to detect the pathogen?","84d9b07f":"We now make another function to get the best matched paragraphs results from the dataset to our given query. If the article in concern does not have full text available then only the abstract is indexed. Since we know the doi field is unique to each article, we check if the article has already appeared before in the list returned. To avoid repetitions, we only include the 'abstract' and the 'body_text' fields form the dataset if the article is new and not a repeated article from before. The function is shown below:","763c62e7":"The objective of this sub-module is to break down a user\u2019s query and rephrase complex query sentences into several shorter and simpler questions that convey the same meaning.  In this way,  the search engine and the question answering modules will be able to find more relevant and less redundant results. \n\nFor this task 8, we convert the task into several questions:\n\n* What do we know about diagnostics and surveillance for covid-19?\n* What has been published concerning systematic, holistic approach to diagnostics?\n* What is the public health surveillance perspective to be able to predict clinical outcomes?\n* how widespread current exposure is to be able to make immediate policy recommendations on mitigation measures?\n* what the literature reports about denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible? \n* what the literature reports about sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs)?\n* what the literature reports about efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\n* what the literature reports about recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\n* what the literature reports about national guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\n* what the literature reports about development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\n* what the literature reports about rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity?\n* what the literature reports about separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\n* what the literature reports about efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\n* what the literature reports about latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\n* what the literature reports about use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression\uff1f\n* what is the policies and protocols for screening and testing of covid-19?\n* what are policies to mitigate the effects on supplies associated with mass testing, swabs and reagents.\n* What is about technology roadmap for diagnostics of covid-19.\n* barriers to developing and scaling up new diagnostic tests (e.g., market forces)? \n* what the literature reports about how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\n* what the literature reports about new platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\n* what the literature reports about coupling genomics and diagnostic testing on a large scale.\n* what the literature reports about enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\n* what the literature reports about enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\n* what the literature reports about potential sources of future spillover or ongoing exposure for evolutionary hosts (e.g., bats)\n* what the literature reports about potential sources of future spillover or ongoing exposure for transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species)\n* what the literature reports about potential sources of future spillover or ongoing exposure for environmental, demographic, and occupational risk factors.\n","9c8b4d39":"#  What has been published concerning systematic, holistic approach to diagnostics?\n","630aa62f":"# what about potential sources of future spillover or ongoing exposure for environmental risk factors?","20d44662":"# Project Description","8cb0b17b":"For the question answering (QA) module, we have leveraged the BioBERT QA model which is finetuned on the SQuAD dataset and [our generalized QA model](http:\/\/https:\/\/github.com\/yana-xuyan\/caireCovid) for MRQA@EMNLP 2019 Shared Task[1]. Instead of fine-tuning the QA models on COVID-19 related datasets, we focus more on maintaining the generalization ability of our system so it can be easily applied to other similar tasks. For the MRQA model, we utilized six datasets, which vary from each other in terms of data source, context lengths, whether multi-hop reasoning is needed, strategies for data augmentation to reduce overfitting to the training data in order to enable generalization to out-of-domain data. Multi-task learning over six datasets is used to fine-tune large pre-trained language model XLNet[2] and it helped achieve promising results. To make the answers more readable, instead of providing small spans of answers, we provide the whole sentences and the surrounding context.  \n\nTo better evaluate the question answering results, we leverage the prediction probability of the QA models as the confidence score. The final answers of our system are re-ranked using this score as one of the factors, which will be talked about later in Section 2.2.\n \n\n[1]Su, Dan, et al. \"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning.\" Proceedings of the 2nd Workshop on Machine Reading for Question Answering. 2019.\n[2]Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems. 2019.","a646f910":"# what about One Health surveillance of humans?","c91fd465":"# what the literature reports about people for sequencing with advanced analytics for unknown pathogens?\n","f887e3ac":"# 1. Document retrieval\n## 1.1 Query Paraphrasing","c28db287":"# what the literature reports about potential sources of future spillover or ongoing exposure for domestic food?","b1b1fd37":"# what the literature reports about exploring capabilities for distinguishing naturally-occurring pathogens from intentional?","8fd8f3b0":"We can build the lucene index of the COVID-19 dataset from scratch, or get one of the pre-built indexes. Using the paragraph indexing which indexes each paragraph of an article (already uploaded the index as a dataset to use), can be downloaded from: https:\/\/www.dropbox.com\/s\/ivk87journyajw3\/lucene-index-covid-paragraph-2020-04-10.tar.gz","f1641f7d":"Let's write some code to print the results, which are top 10 articles matching a given query, along with the best matched paragraph. We are printing some of the fields corresponding to each article, a complete list of fields can be found [here](https:\/\/github.com\/castorini\/anserini\/blob\/master\/src\/main\/java\/io\/anserini\/index\/generator\/CovidGenerator.java#L46).","cdbe4dac":"# what the literature reports about policies to mitigate the effects on swabs?","120850ce":"![image.png](attachment:image.png)\nIn this section, we will elaborate on the building blocks of each module in our system.\n \n1) Document Retriever\n+ **Query Paraphrasing**: It converts a long\/complicated query from a user to several shorter and simpler questions for search;\n+ **Search Engine**: We use Anserini with Lucene to retrieve publications from the candidate pool with high coverage. \n \n2) Relevant Snippet Selector: \n+ **Question Answering(QA)**: This sub-module looks for and integrates evidence from one or multiple paragraphs. We leverage an ensemble of two neural-based QA models which are pre-trained on SQuAD style QA datasets. Here we consider the QA module as a supporting fact selector to provide relevant snippets from the retrieved documents.\n+ **Answer Re-ranking & Highlight Generation**:  We rerank the retrieved result by a word matching score based on part-of-speech tagging as well as the QA system confidence score. We also highlight the answer span in order to enable easier reading of the QA results.\n\n3) Multi-document Summarizer:\n+ **Abstractive Summarization**: Another output of our system is an abstractive summary that synthesizes the answer from multiple retrieved snippets. This step aims to generate short pieces of fluent summaries based on the top relevant results. Using the neural-based summarizer, we generate summaries to improve the legibility of the results and help the user to have an overview of the relevant snippets in a short time.","8a329189":"In response to the COVID-19 pandemic, a lot of scholarly articles have been published recently and made freely available. At the same time, there are emerging requests from both the medical research community and the broader society to find answers to various questions regarding COVID-19. A system that can provide reliable answers to the COVID-19 related questions from the latest academic resources is crucial, especially for the medical community in the current time-critical race to treat patients and to find a cure for the virus. \n \nTo address the aforementioned requests by the medical community, we propose a machine learning-based system that uses state-of-the-art natural language processing(NLP) question answering(QA) techniques combined with summarization for mining the available scientific literature. The system is an end-to-end neural network-based open-domain QA system that can answer COVID-19 related questions, such as those questions proposed in the COVID-19 Kaggle task. Through our system, users can get two versions of the outcome:\n1. A ranked list of relevant snippets from the literature given a query;\n1. A fluent summary of the relevant results. We provide the paragraph-level summaries, which takes the paragraphs where the top three relevant snippets are located as input, to enable a more efficient way of understanding of the content.\n \nOur system consists of three different modules: **1) Document Retriever, 2) Relevant Snippet Selector, and 3) Multi-Document Summarizer**. The first module pre-processes a user\u2019s query and retrieves the most relevant k number of academic publications. The second module outputs a list of the most relevant answer snippets from the retrieved documents. It also highlights the relevant keywords. The last module is for generating the second output, namely a concise summary of the top-ranked retrieved relevant paragraphs in the two previous modules.\n\nWe have launched our [CAiRE-Covid website](https:\/\/caire.ust.hk\/covid), which showcases our results for each user query in real-time, so people can further experiment with our system.","a897e1ba":"Automatic text summarization is a common problem in machine learning and natural language processing (NLP). Basically there are two main types of how to summarize text in NLP:\n* **Extraction-based summarization**, which involves pulling key phrases from the source document and combining them to make a summary, and;\n* **Abstraction-based summarization**, which creates new phrases and sentences that relay the most useful information from the original text \u2014 just like humans do. \nIn general, the abstractive method is a much harder task but performs better than an extractive method.\n\nIn our project, considering the requirements that people may still want to further read each paragraph containing the predicted QA answer spans, we summarize the top-k  (top-3) paragraphs that QA module passes, to generate a **paragraph-level abstractive summary**. \n\nOur model is based on two different abstractive summarization models: [Unilm](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/s2s-ft) and [BART](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart), both of which have obtained SOTA results on the summarization tasks ([CNN\/DM datasets](https:\/\/cs.nyu.edu\/~kcho\/DMQA\/), and [XSUM](https:\/\/github.com\/EdinburghNLP\/XSum\/tree\/master\/XSum-Dataset) data). UniLM model is a unified pre-trained model for language understanding and generation. BART is a sequence-to-sequence model trained with denoising as a pre-training objective for language generation, translation, and comprehension.\n\nWe fine-tuned the UniLM model using [SumOnGraph](https:\/\/github.com\/coshiang\/SumOnGraph) biology dataset which includes literature for 5 types of diseases including Cancer, Cardiovascular Disease, Diabetes, Allergy, and Obesity. Original data is from PubMed which is a free resource supporting the search and retrieval of biomedical and life sciences literature with the aim of improving health\u2013both globally and personally. We used the BART model fine-tuned on CNN\/DailMail dataset. \n\nWe generate a summary for each answer-related paragraph from the QA module, then concatenate them directly to form our final **paragraph-level** answer summary.\n\n*(Actually we also implement **article-level summary**, even though in this Kaggle project, for the simplicity and legibility, we only display **the paragraph-level results** of the summarization models. It takes the whole article as input, and generate a summary for each sections (eg. Introductions section, Methodologies section)  of the articles, and then concatenate them together as a more fine-grained **article-level summary**, as complementary to the abstracts. You can refer to this [github repository](https:\/\/github.com\/Iamfinethanksu\/covidSumm) for more details.)*\n\nIf anyone is interested in our article-level results, please utilize ```summary = get_article_summary(query, abstractive_summary_model)``` to obtain the results from our system. ","86e718e3":"# what the literature reports about how widespread current exposure is to be able to make immediate policy recommendations on mitigation measures?\n","5fbfc2a8":"# what the literature reports about potential sources of future spillover or ongoing exposure for evolutionary hosts (e.g., bats)?\n","86c56e0d":"# 2. Relevant Snippet Selector\n## 2.1 Question Answering","3c7ff4f1":"### Please try it!","e4915182":"# what the literature reports about efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.","5a9b0caa":"# what the literature reports about enhance capabilities for bioinformatics to target regions of the genome?","a99057f5":"We can see some repititions in the results above. This can either be due to multiple paragraphs in the same article being matched with the query, or one article appearing more than once in the CORD-19 dataset, due to different sources. Let's now try printing out the actual paragraph that is being matched with each of the returned hits. First we print out contents of the indexing of the first hit for example:","722f98f2":"# what the literature reports about use of screening of neutralizing antibodies such as ELISAs?","d0d4a1b2":"# what the literature reports about potential sources of future spillover or ongoing exposure for farmed wildlife? ","bbd8bd91":"The indexing is done based on each paragraph merged with the title and abstract. Given an article with id *doc_id*, the index will be as follows:\n* *doc_id* : title + abstract\n* *doc_id.00001* : title + abstract + 1st paragraph\n* *docid.00002*: title + abstract + 2nd paragraph\n* *docid.00003*: title + abstract + 3rd paragraph","7e2e139d":"# what the literature reports about how states might leverage universities and private laboratories for testing purposes?","e4136595":"# what the literature reports about CRISPR to improve response times and employ more holistic approaches to COVID-19 and future diseases?","3acc6780":"# Let's try now","688b4671":"# what the literature reports about latency issues?\n","fe17c0c6":"Following the lucene+answerini information retrieval as described in: https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md\n\nSetting up JAVA sdk 11 first:","9af5c2a7":"* install the prerequisite packages\n(The covidSumm packages are from [here](https:\/\/github.com\/Iamfinethanksu\/covidSumm).","017f3a1c":"# what the literature reports about barriers to developing and scaling up new diagnostic tests (e.g., market forces)? \n","14231d08":"# what the literature reports about understanding of what is needed in terms of biological and environmental sampling?","7d59078d":"# what the literature reports about potential sources of future spillover or ongoing exposure for heavily trafficked?","deb90829":"# what the literature reports about targeted surveillance experiments using PCR in a defined area to start testing and report to a specific entity?","b4ca63e3":"# what the literature reports about use of serosurveys such as convalescent samples?","2b7b8a19":"We can try running the function for the previous query, and check if the results are what we want.","0b011a90":"# System Architecture Overview"}}