{"cell_type":{"78fc8cdb":"code","ee2ced8e":"code","bcbfc4c2":"code","65be0110":"code","13e4450a":"code","9a8c811b":"code","13b3fbed":"code","3b43aef0":"code","8219aa67":"code","4ce45632":"code","dbfebce6":"code","76bc7d4a":"code","5ddf9e03":"markdown","a73f155e":"markdown","31082d14":"markdown","8cfe2690":"markdown","a2f24f91":"markdown","b7f082a0":"markdown","7c39505f":"markdown","19459cba":"markdown","773b868e":"markdown","81115900":"markdown"},"source":{"78fc8cdb":"import cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nfrom skimage import io\nfrom shutil import copyfile\nimport sys\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array","ee2ced8e":"train = pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/train.csv\")\n#test = pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/test.csv\")","bcbfc4c2":"train.head()","65be0110":"#convert bbox into XMin XMax YMin YMax\n#train[\"bbox\"][0][1:-1].split(', ')\nbox_dict = {'XMin':[], 'XMax':[], 'YMin':[], 'YMax':[]}\ndef bbox(x):\n    #x = X[1:-1].split(', ')\n    #min value\n    #print(x)\n    box_dict['XMin'].append(int(x[0]))\n    box_dict['YMin'].append(int(x[1]))\n    \n    #Max value\n    box_dict['XMax'].append(int(x[0])+int(x[2]))\n    box_dict['YMax'].append(int(x[1])+int(x[3]))\n    \n\nb = train[\"bbox\"].apply(lambda X: bbox(list(map(float,X[1:-1].split(', ')))))\nbox = pd.DataFrame(box_dict)","13e4450a":"train = pd.concat([train,box],axis=1)\ntrain.drop(\"bbox\",axis=1,inplace=True)","9a8c811b":"dir_train = os.listdir(\"\/kaggle\/input\/global-wheat-detection\/train\/\")\ndir_test = os.listdir(\"\/kaggle\/input\/global-wheat-detection\/test\/\")","13b3fbed":"len(dir_test)","3b43aef0":"#Creating full path for each image ID\ndef creatingPath(x):\n    return (\"\/kaggle\/input\/global-wheat-detection\/train\/\"+x+\".jpg\")\n\ntrain[\"path\"] = train[\"image_id\"].apply(creatingPath)","8219aa67":"\n#Plot some of the images\ndef plot_bbox(img_id):\n  img_url = train.loc[train[\"image_id\"]==img_id]['path'].values[0]\n  img = io.imread(img_url)\n  height, width, channel = img.shape\n  print(f\"Image: {img.shape}\")\n  bboxs = train[train['image_id']==img_id]\n  for index, row in bboxs.iterrows():\n      xmin = row['XMin']\n      xmax = row['XMax']\n      ymin = row['YMin']\n      ymax = row['YMax']\n      #xmin = int(xmin*width)\n      #xmax = int(xmax*width)\n      #ymin = int(ymin*height)\n      #ymax = int(ymax*height)\n      label_name = row['source']\n      class_series = train[train[\"source\"]==label_name]\n      class_name = class_series[\"source\"].values[0]\n      print(f\"Coordinates: {xmin,ymin}, {xmax,ymax}\")\n      cv2.rectangle(img, (xmin,ymin), (xmax,ymax), (255,0,0), 5)\n      font = cv2.FONT_HERSHEY_SIMPLEX\n      cv2.putText(img, class_name, (xmin,ymin-10), font, 1, (0,255,0), 5)\n  plt.figure(figsize=(15,10))\n  plt.title('Image with Bounding Box')\n  plt.imshow(img)\n  plt.axis(\"off\")\n  plt.show()","4ce45632":"least_objects_img_ids = train[\"image_id\"].value_counts().tail(50).index.values\nfor img_id in random.sample(list(least_objects_img_ids), 5):\n  plot_bbox(img_id)","dbfebce6":"train[\"source\"].value_counts()","76bc7d4a":"test_df = train[int(train.shape[0]*0.9):]\n\n# for test\nwith open(\"test_annotation.txt\", \"w+\") as f:\n  for idx, row in test_df.iterrows():\n      sys.stdout.write(str(idx) + '\\r')\n      sys.stdout.flush()\n      img = cv2.imread(row['path'])\n      height, width = img.shape[:2]\n      x1 = int(row['XMin'])\n      x2 = int(row['XMax'])\n      y1 = int(row['YMin'])\n      y2 = int(row['YMax'])\n      \n      #google_colab_file_path = 'drive\/My Drive\/AI\/Dataset\/Open Images Dataset v4 (Bounding Boxes)\/test'\n      fileName = row['path']\n      className = row['source']\n      f.write(fileName + ',' + str(x1) + ',' + str(y1) + ',' + str(x2) + ',' + str(y2) + ',' + className + '\\n')","5ddf9e03":"#Lets Check the class distribution","a73f155e":"#Training dATASET annotation","31082d14":"#Testing Dataset annotation","8cfe2690":"There are total 7 classes , out of 7 classes 3 are in minority and 4 are in majority classes.","a2f24f91":"Part 1: https:\/\/www.kaggle.com\/kishor1210\/eda-and-data-processing\n\nPart 2: https:\/\/www.kaggle.com\/kishor1210\/train-faster-rcnn-using-keras\n\nPart 3: comming soon....","b7f082a0":"train_df = train[:int(train.shape[0]*0.9)]\n\n# for training\nwith open(\"annotation.txt\", \"w+\") as f:\n  for idx, row in train_df.iterrows():\n      img = cv2.imread(row['path'])\n      height, width = img.shape[:2]\n      x1 = int(row['XMin'])\n      x2 = int(row['XMax'])\n      y1 = int(row['YMin'])\n      y2 = int(row['YMax'])\n      \n      #google_colab_file_path = 'drive\/My Drive\/AI\/Dataset\/Open Images Dataset v4 (Bounding Boxes)\/train'\n      fileName = row['path']\n      className = row['source']\n      f.write(fileName + ',' + str(x1) + ',' + str(y1) + ',' + str(x2) + ',' + str(y2) + ',' + className + '\\n')","7c39505f":"#Load the datasets","19459cba":"#Combine two dataframe image_id with annotation","773b868e":"This Kernal Will explain all the EDA and data preprocessing for Faster Rcnn.\n\npart 2: https:\/\/www.kaggle.com\/kishor1210\/train-faster-rcnn-with-90-of-images","81115900":"#Load the Required libraries"}}