{"cell_type":{"62967d94":"code","7f4e4cee":"code","6fa9763b":"code","5c1407b0":"code","6b039449":"code","67b014aa":"code","ebb02436":"code","e41deb9b":"code","33d48645":"code","42203838":"code","fb71a28d":"code","ae7ae0ff":"code","741e3ed8":"code","ea7b76c1":"markdown","47f348c3":"markdown"},"source":{"62967d94":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7f4e4cee":"!head -n 2 \/kaggle\/input\/tweets-from-hyderabadindia\/hyd.json","6fa9763b":"import dask.bag as db\nimport json","5c1407b0":"b = db.read_text('\/kaggle\/input\/tweets-from-hyderabadindia\/hyd.json').map(json.loads)","6b039449":"type(b)","67b014aa":"b.take(5)","ebb02436":"b.visualize()","e41deb9b":"print('Columns for the dataset are:', b.take(1)[0].keys())","33d48645":"## get first 5 tweets\nb.map(lambda d: d['tweet']).take(5)","42203838":"print('Size of data:', b.count().compute())","fb71a28d":"b.take(1)[0].keys()","ae7ae0ff":"def flatten(record):\n    return {\n        'id': record['id'],\n        'created_at': record['created_at'],\n        'date': record['date'],\n        'time': record['time'],\n        'tweet': record['tweet'],\n        'username': record['username'],\n        'mentions': record['mentions'],\n        'urls': record['urls'],\n        'replies_count': record['replies_count'],\n        'retweets_count': record['retweets_count'],\n        'likes_count': record['likes_count'],\n        'hashtags': record['hashtags'],\n        'link': record['link'],\n        'reply_to': record['reply_to'],\n    }\n\nb.map(flatten).take(20)","741e3ed8":"df = b.map(flatten).to_dataframe()\ndf.head()","ea7b76c1":"## For reading large dataset, we will be using dask bags which are good and more efficient form like Dask Dataframes. \n## For documentation for dask bags you can check this [link](https:\/\/examples.dask.org\/bag.html)","47f348c3":"##### Here we make a function to flatten down our nested data structure, map that across our records, and then convert that to a Dask Dataframe."}}