{"cell_type":{"48a64dd0":"code","973753ae":"code","eebcb7eb":"code","c29c7b0c":"code","2c29dc79":"code","997f9053":"markdown","ea22e4f3":"markdown","d1f76873":"markdown","23251283":"markdown","d99606a2":"markdown"},"source":{"48a64dd0":"import pandas as pd\n\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\").sample(frac=1.)\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","973753ae":"from transformers import *\nfrom gensim.utils import simple_preprocess\nimport numpy as np\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nX_train = np.array([tokenizer.encode(\" \".join(simple_preprocess(text, min_len=2, max_len=15)),\n                                     add_special_tokens=True, \n                                     max_length=40, \n                                     pad_to_max_length=True) for text in train[\"text\"]])\nX_test = np.array([tokenizer.encode(\" \".join(simple_preprocess(text, min_len=2, max_len=15)), \n                           add_special_tokens=True, \n                           max_length=40, \n                           pad_to_max_length=True) for text in test[\"text\"]])","eebcb7eb":"from tensorflow.keras.initializers import glorot_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import int32\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import plot_model\n\n# Input layer\ninput_ids = Input((40,), dtype=int32)\n# RoBERTa layer\nlm = TFRobertaModel.from_pretrained('roberta-base')\nsequence, cls = lm(input_ids) \n# Parallel mean and max global pooling\nmean_pooling = GlobalAveragePooling1D()(sequence)\nmax_pooling = GlobalMaxPooling1D()(sequence)\npooling = concatenate([mean_pooling, max_pooling])\n# Dropout layer\ndropout = Dropout(0.5, name=\"dropout\")(pooling)\n# Classification layer\nclassification = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_normal(seed=1), bias_initializer=glorot_normal(seed=1), name=\"classification\")(pooling)\nmodel = Model(input_ids, classification)\nplot_model(model)","c29c7b0c":"from tensorflow.keras.callbacks import LearningRateScheduler\n\ndef rate(epoch):\n    return 1.5e-5\/(epoch + 1)\n\nscheduler = LearningRateScheduler(rate)\nmodel.compile(optimizer=Adam(beta_1=0.9, beta_2=0.999), \n              loss=\"binary_crossentropy\", \n              metrics=[\"accuracy\"])\nlog = model.fit(X_train, train[\"target\"].values, \n                callbacks=[scheduler],\n                batch_size=12, \n                epochs=3, \n                verbose=1)","2c29dc79":"test[\"proba\"] = model.predict(X_test)\ntest[\"target\"] = test[\"proba\"].apply(lambda p: int(p > 0.5))\ntest[[\"id\", \"target\"]].to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","997f9053":"## Tokenize the tweets\n\nWe clean the tweets a little by removing tokens that are shorter than 2 characters or longer than 15 characters.","ea22e4f3":"## RoBERTa with mean and max pooling\n\nWe embed the words using RoBERTa, we perform mean and max pooling over the embedded sequences ","d1f76873":"We train the model, with a linearly decaying learning rate.","23251283":"# RoBERTa with mean and max pooling\n\nThis notebook shows you how to set up a classifier based on RoBERTa, with mean and max pooling.\n\n## Read the data\n\nWe load the train and test datasets.","d99606a2":"## Submit predictions"}}