{"cell_type":{"ac294dce":"code","99f81fa1":"code","1e853aad":"code","95509ab8":"code","6e1c4c6b":"code","616a397f":"code","a7c5672b":"code","867a841e":"code","482252b9":"code","eaa5dc6d":"code","f70ee063":"code","3057cfba":"code","410ef7c8":"code","6b057c3d":"code","765dce57":"code","340e1d13":"code","b44e5a6c":"code","f95fc09b":"code","5d7a09a8":"code","acca32fd":"code","8087ff3a":"code","987bab4b":"code","b7869481":"code","555ca8d4":"code","83f4d403":"code","4b64da49":"code","70a625b4":"markdown","ad116d43":"markdown","0ea9cae2":"markdown","3df3d18f":"markdown","b6c2e117":"markdown","2949b22b":"markdown","83c07d83":"markdown","2bf9a37a":"markdown","d5158b80":"markdown","f307f65b":"markdown"},"source":{"ac294dce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99f81fa1":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","1e853aad":"train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')","95509ab8":"train.info()","6e1c4c6b":"train.describe(include='all')","616a397f":"train.head(5)","a7c5672b":"train=train.drop('id',axis=1)\ntrain.head()","867a841e":"train=train.drop('language',axis=1)\ntest=test.drop('language',axis=1)\ntrain.head(5)","482252b9":"sns.countplot(x='lang_abv', data=train)","eaa5dc6d":"plt.figure(figsize=(9,9))\ntrain.groupby('lang_abv').size().plot(kind='pie', autopct='%1.1f%%')","f70ee063":"from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer\nmodel_name ='joeddav\/xlm-roberta-large-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","3057cfba":"def encode_premise_sentence(s):\n    tokens = []\n    tokens.append('[CLS]')\n    tokens+=list(tokenizer.tokenize(s))\n    return tokenizer.convert_tokens_to_ids(tokens)","410ef7c8":"def encode_hypothesis_sentence(s):\n    tokens = []\n    tokens.append('[SEP]')\n    tokens+=list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","6b057c3d":"tokenized=[]\nfor each in range(len(train)):\n    pre=encode_premise_sentence(train['premise'][each])\n    hyp=encode_hypothesis_sentence(train['hypothesis'][each])\n    tokenized.append(pre+hyp)\ntrain['tokenized'] = tokenized\ntrain.head(5)","765dce57":"mask=[]\nfor each in range(len(train)):\n    padded_sequences = tokenizer(train['premise'][each],train['hypothesis'][each], padding=True,add_special_tokens = True)\n    mask.append(padded_sequences)\ntrain['masked'] = mask\ntrain.head(5)","340e1d13":"train['masked'][0]","b44e5a6c":"#This should be the length of the longest token in test data so that you can apply the model for prediction without any error\n# or you could get the longest token of the whole data and set this max_len\nmax_len=237 \n\ndef build_model():\n    bert_encoder = TFAutoModel.from_pretrained('joeddav\/xlm-roberta-large-xnli')\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    #input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","f95fc09b":"def input_convert(data):\n    inputs   = {\n            'input_word_ids' :[],\n            'input_mask'     :[]  }\n    for each in data:\n        inputs['input_word_ids'].append(each['input_ids'])\n        inputs['input_mask'].append(each['attention_mask'])\n        #inputs['input_type_ids'].append(each['token_type_ids'])\n    \n    inputs['input_word_ids']= tf.ragged.constant( inputs['input_word_ids']).to_tensor()\n    inputs['input_mask']= tf.ragged.constant( inputs['input_mask']).to_tensor()\n    #inputs['input_type_ids']= tf.ragged.constant( inputs['input_type_ids']).to_tensor()\n    \n    return inputs","5d7a09a8":"train_input= input_convert(train['masked'].values)\nfor key in train_input.keys():\n    train_input[key] = train_input[key][:,:max_len]","acca32fd":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    model.fit(train_input, train['label'].values, epochs = 5, verbose = 1, batch_size = 128, validation_split = 0.1 ,callbacks=[early_stop])","8087ff3a":"mask=[]\nfor each in range(len(test)):\n    padded_sequences = tokenizer(test['premise'][each],test['hypothesis'][each], padding=True,add_special_tokens = True)\n    mask.append(padded_sequences)\ntest['masked'] = mask\ntest.head(5)","987bab4b":"test_input= input_convert(test['masked'].values)\nfor key in test_input.keys():\n    test_input[key] = test_input[key][:,:max_len]","b7869481":"predictions = [np.argmax(i) for i in model.predict(test_input)]","555ca8d4":"test.head()","83f4d403":"submission = test['id'].copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)","4b64da49":"submission.head()","70a625b4":"# Pretrained Model using RoBERTa\nWe feed the input of the data we have into the model and add a layers as the output of 3 neurons to represent in percentage that it belong to one of 3 possible output of our model. <br>\nThe one with the highest percentage would be chosen as the prediction ( for the prediction in test set)","ad116d43":"We can recon that the majority of our data is in english, with 11 languagues that is consider minority. However, we still need the others for the model to learn different languages.","0ea9cae2":"# Load Data","3df3d18f":"# Prediction ","b6c2e117":"### Remove ID column\nThe column seems to be independence to the result since each data has it own ID. Thus, I remove it\n","2949b22b":"# Data Preprocessing","83c07d83":"### Language\nColumn language and lang_abv is the same. We only need one of them and I would prefer lang_abv only","2bf9a37a":"# RoBERTa Embedding\nIn this step I use pretrain model of word embedding roBERTa to get the word vector of each word","d5158b80":"# Tokenize\nWhile tokenize, we add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. <br>\nThis step is for demonstration of how a sentence after tokenized would look like\n","f307f65b":"# Attention Mask and Token Type ID:\nAttention Mask is to guarantee that the length of 2 sentences are the same and avoid bias in our model  <br>\nToken type IDs helps with distinguish between premise and hypothesis sentence ( Optional) <br>\nInput ID: Instead of showing the whole vector represent the word, this only display the ID of that word in the pretrained model dictionary"}}