{"cell_type":{"fd68967a":"code","8d7bfe91":"code","b4b8558b":"code","24540d9b":"code","df6c95ea":"code","f24cbc0e":"code","538680c5":"code","e45340b4":"code","35838ed0":"code","6ec41e93":"code","a4e677db":"code","ebea7e7a":"code","5a5f1c61":"code","34974d22":"code","c41d6a0a":"code","1a0ed679":"code","2742300d":"code","d84c6798":"code","f05b1bcc":"code","66faecff":"code","c210363d":"code","57ade35f":"code","19eb7555":"code","7f29ad95":"code","92f366fd":"code","fce09871":"code","c308ca17":"code","6c74a512":"code","d61e5ada":"code","96fbd7b7":"code","e193075c":"code","9c7ac949":"code","d9085465":"code","e563d9bb":"code","7e557193":"markdown","62aed985":"markdown","78191188":"markdown","a26e5055":"markdown","aa7eee46":"markdown","d99bef45":"markdown","2b4d6b1c":"markdown","eb028071":"markdown","e4048007":"markdown","4f99d856":"markdown","09d553e6":"markdown","cb8e459b":"markdown","ca7bfb3d":"markdown","1900f424":"markdown","e1d211fa":"markdown","bc446c19":"markdown","8367c0c8":"markdown","6d6a37ba":"markdown"},"source":{"fd68967a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","8d7bfe91":"## import the datasets","b4b8558b":"from sklearn.datasets import load_digits","24540d9b":"dataset = load_digits()","df6c95ea":"dataset.data[0].shape","f24cbc0e":"dataset.target[0]","538680c5":"dataset.images","e45340b4":"plt.imshow(dataset.images[0])","35838ed0":"dataset.images[0].shape","6ec41e93":"X,y = dataset.data,dataset.target","a4e677db":"i=0\nfor item1,item2 in zip(X,y):\n    plt.imshow(item1.reshape(8,8))\n    plt.show()\n    print(item2)\n    i+=1\n    if i==10:\n        break","ebea7e7a":"y_binary_imbalanced = y.copy()\ny_binary_imbalanced[y_binary_imbalanced != 1] = 0","5a5f1c61":"y_binary_imbalanced","34974d22":"## we can run a simple bin count\nnp.bincount(y_binary_imbalanced)","c41d6a0a":"print('Original labels:\\t', y[1:30])\nprint('New binary labels:\\t', y_binary_imbalanced[1:30])","1a0ed679":"## train test split","2742300d":"X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n\n# Accuracy of Support Vector Machine classifier\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\nsvm.score(X_test, y_test)","d84c6798":"from sklearn.dummy import DummyClassifier\n\n# Negative class (0) is most frequent\n## so we classify with the most frequent\n## you will see this classifier will show all the output is zero\n## because most of it is negative and still it gives a good accuracy\ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n# Therefore the dummy 'most_frequent' classifier always predicts class 0\ny_dummy_predictions = dummy_majority.predict(X_test)\n\ny_dummy_predictions","f05b1bcc":"dummy_majority.score(X_test, y_test)","66faecff":"from sklearn.metrics import confusion_matrix\n\n# Negative class (0) is most frequent\ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\ny_majority_predicted = dummy_majority.predict(X_test)\nconfusion = confusion_matrix(y_test, y_majority_predicted)\n\nprint('Most frequent class (dummy classifier)\\n', confusion)","c210363d":"dummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\ny_classprop_predicted = dummy_classprop.predict(X_test)\nconfusion = confusion_matrix(y_test, y_classprop_predicted)\n\nprint('Random class-proportional prediction (dummy classifier)\\n', confusion)","57ade35f":"svm = SVC(kernel='linear', C=1).fit(X_train, y_train)\nsvm_predicted = svm.predict(X_test)\nconfusion = confusion_matrix(y_test, svm_predicted)\n\nprint('Support vector machine classifier (linear kernel, C=1)\\n', confusion)","19eb7555":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression().fit(X_train, y_train)\nlr_predicted = lr.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\n\nprint('Logistic regression classifier (default settings)\\n', confusion)","7f29ad95":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\ntree_predicted = dt.predict(X_test)\nconfusion = confusion_matrix(y_test, tree_predicted)\n\nprint('Decision tree classifier (max_depth = 2)\\n', confusion)","92f366fd":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n# Accuracy = TP + TN \/ (TP + TN + FP + FN)\n# Precision = TP \/ (TP + FP)\n# Recall = TP \/ (TP + FN)  Also known as sensitivity, or True Positive Rate\n# F1 = 2 * Precision * Recall \/ (Precision + Recall) \nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))","fce09871":"# Combined report with all above metrics\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, tree_predicted, target_names=['not 1', '1']))","c308ca17":"print('Random class-proportional (dummy)\\n', \n      classification_report(y_test, y_classprop_predicted, target_names=['not 1', '1']))\nprint('SVM\\n', \n      classification_report(y_test, svm_predicted, target_names = ['not 1', '1']))\nprint('Logistic regression\\n', \n      classification_report(y_test, lr_predicted, target_names = ['not 1', '1']))\nprint('Decision tree\\n', \n      classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))","6c74a512":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.dummy import DummyRegressor\n\ndiabetes = datasets.load_diabetes()\n\nX = diabetes.data[:, None, 6]\ny = diabetes.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nlm = LinearRegression().fit(X_train, y_train)","d61e5ada":"lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n\ny_predict = lm.predict(X_test)\ny_predict_dummy_mean = lm_dummy_mean.predict(X_test)","96fbd7b7":"print('Linear model, coefficients: ', lm.coef_)\nprint(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(y_test, \n                                                                     y_predict_dummy_mean)))\nprint(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\nprint(\"r2_score (dummy): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\nprint(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))\n","e193075c":"# Plot outputs\nplt.scatter(X_test, y_test,  color='black')\nplt.plot(X_test, y_predict, color='green', linewidth=2)\nplt.plot(X_test, y_predict_dummy_mean, color='red', linestyle = 'dashed', \n         linewidth=2, label = 'dummy')\n\nplt.show()","9c7ac949":"from sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\ndataset = load_digits()\n# again, making this a binary problem with 'digit 1' as positive class \n# and 'not 1' as negative class\nX, y = dataset.data, dataset.target == 1\nclf = SVC(kernel='linear', C=1)\n\n# accuracy is the default scoring metric\nprint('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=5))\n# use AUC as scoring metric\nprint('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=5, scoring = 'roc_auc'))\n# use recall as scoring metric\nprint('Cross-validation (recall)', cross_val_score(clf, X, y, cv=5, scoring = 'recall'))","d9085465":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\ndataset = load_digits()\nX, y = dataset.data, dataset.target == 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = SVC(kernel='rbf')\ngrid_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n\n# default metric to optimize over grid parameters: accuracy\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\ngrid_clf_acc.fit(X_train, y_train)\ny_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)\n\n# alternative metric to optimize over grid parameters: AUC\ngrid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\ngrid_clf_auc.fit(X_train, y_train)\ny_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test) \n\nprint('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))\nprint('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\nprint('Grid best score (AUC): ', grid_clf_auc.best_score_)\n","e563d9bb":"from sklearn.metrics.scorer import SCORERS\n\nprint(sorted(list(SCORERS.keys())))","7e557193":"* ## we use the load digit for the evluation \n* ## and we make this dataset as a binary dataset\n* ## that make this data highly imbalanced\n* ## then we see how the dummy class and the actual class\n* ## behave with the imbalanced data","62aed985":"###  we make the classifier imbalanced so we make this data set as a 1 detection only the target  1 will be 1 all the other will be 0\n### we will make a binary classifier","78191188":"# same thing can be done with Regressor","a26e5055":"* ## if your data is imbalanced this could happen\n* ## now lets classify with the most-frequent","aa7eee46":"## lets see the how the label changes","d99bef45":"### we make a binary classifier it wont make a decision insted we will tell him what to do in the hyparameter to show how the classifier can behave in imbalanced data.so we get a understanding how a classifier can give false result even the accuracy showing good","2b4d6b1c":"1. ## use cross validation with different scoring","eb028071":"## thats a pretty good result based on r2 result","e4048007":"## this will divide the data with the cv value and then compare one with other this is a very good and reliable system","4f99d856":"* ## replacing everything into zero unless the data is 1 .\n* ## then we replace it with the 1\n* ## so this dataset will only indicate which one is 1","09d553e6":"## this are the scores that you can use","cb8e459b":"## if we only use the mean in the Regresion classifier","ca7bfb3d":"# Evalueate with precision recall and F1_score","1900f424":"* ## so we can see that the 1.6 k data is zero only 182 data is one \n* ## so the data is imbalanced","e1d211fa":"# now lets find with a  actual clasifier","bc446c19":"## we can create a combined score card","8367c0c8":"## we can use stratified # produces random predictions w\/ same class proportion as training set.this can give a good accuracy even this is a dummy classifier because this is used partial data","6d6a37ba":"## we use grid search to find the best parameter but it takes a lot of time"}}