{"cell_type":{"7b317e4f":"code","5af3af16":"code","db5cf2f0":"code","b29120e9":"code","eb9a550d":"code","d55c2ea6":"code","ac9098c4":"code","7e6f4904":"code","9fdddece":"code","d0cc3a66":"code","c42b03c0":"code","d3418764":"code","db197cfd":"code","3ac54a46":"code","48a34334":"code","7790e71a":"code","1a347ca1":"markdown","a5fa8bf8":"markdown","72d58a69":"markdown","5efd2b09":"markdown","8dfd0b2c":"markdown","380132c2":"markdown","9d2a02e4":"markdown","25f360aa":"markdown","5ee718dc":"markdown","4e707f54":"markdown","c1f06ca4":"markdown","8ca2e8f5":"markdown","229ff5b4":"markdown","3b54dbd5":"markdown"},"source":{"7b317e4f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import StratifiedKFold # For creating folds\nfrom sklearn.metrics import log_loss # Evaluation metrics\nimport copy","5af3af16":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","db5cf2f0":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nss = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","b29120e9":"print(f\"Shape of train : {train.shape}\")\nprint(f\"Shape of test : {test.shape}\")\nprint(f\"Shape of sample submission : {ss.shape}\")","eb9a550d":"train.head()","d55c2ea6":"train.isnull().sum()\/train.shape[0]","ac9098c4":"test.isnull().sum()\/test.shape[0]","7e6f4904":"train.info()","9fdddece":"cat_features =['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49']","d0cc3a66":"train[\"kfold\"] = -1\ntrain = train.sample(frac=1).reset_index(drop=True)\ny = train.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=train,y=y)):\n  train.loc[v_,\"kfold\"] = f","c42b03c0":"cat = CatBoostClassifier(task_type='GPU',\n                         iterations=1000,\n                         loss_function='MultiClass',\n                         random_state = 42,\n                         verbose=100\n                         )","d3418764":"df = copy.deepcopy(train)\nlogloss = []\npreds = []\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    #Creating pool\n    train_pool = Pool(data=X_train,label=y_train,cat_features=cat_features)\n    valid_pool = Pool(data=X_valid,label=y_valid,cat_features=cat_features)\n    \n    #Fitting the model\n    cat.fit(train_pool, eval_set=valid_pool,verbose=100)\n    \n    #Predicting for valid and test datasets\n    valid_preds = cat.predict_proba(X_valid)\n    preds.append(cat.predict_proba(X_test)) #Appending the predicted probablities for test data into a list\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","db197cfd":"np.array(preds).shape","3ac54a46":"for i in range(50000):\n    input_val = [preds[0][i], preds[1][i], preds[2][i], preds[3][i], preds[4][i]]\n    avg_pred = [np.mean(x) for x in zip(*input_val)]","48a34334":"ss[\"Class_1\"] = pred[0]\nss[\"Class_2\"] = pred[1]\nss[\"Class_3\"] = pred[2]\nss[\"Class_4\"] = pred[3]","7790e71a":"ss.to_csv(\"base_line_sub.csv\", index=False)","1a347ca1":"Creating folds for the train dataset, so that we can train the model for the n folds, to avoid overfitting.","a5fa8bf8":"avg_pred has the average of the 5 predictions. Lets write it to the ss","72d58a69":"Traing and evaluating the model in the 5 folds cross validation manner.","5efd2b09":"Defining a variable with all the categorical features to pass to catboost classifier","8dfd0b2c":"# Basline model","380132c2":"Lets check the shape of preds","9d2a02e4":"**The average log loss is 1.0926706588358275**","25f360aa":"Now we have the predictions from 5 models, one from each folds. We are going the take the avearge of these. The result will be more reliable since we have predicted in the cross validated manner. ","5ee718dc":"We have a 5 two dimentional array in the shape of sample submission. Lets take the average of it and write to sample submission to create a submission file.","4e707f54":"There are **no missing values** in the both train and test datasets.","c1f06ca4":"**Since it is a baseline\/starter model,I am not doing EDA and directly moving onto model building part.**","8ca2e8f5":"# Reading the train, test and sample submission file","229ff5b4":"This is a very basic code and have lot of room for improvement.\n1. We can check for outliers or do feature engineering to adjust the data.\n2. We can tune the hyper parameters for the CatBoostClassifier to get better scores.\n\nThank you.","3b54dbd5":"# Next steps"}}