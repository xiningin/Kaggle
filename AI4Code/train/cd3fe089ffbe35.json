{"cell_type":{"aec9d7e7":"code","fa1b9c3d":"code","240116d4":"code","d9866d16":"code","aa6142f8":"code","22f42f92":"code","99e142cf":"code","6ca5a509":"code","93338e6b":"code","01f94950":"code","63f84dc4":"code","1702d922":"code","a5023479":"code","dd0a550f":"code","c8113dd5":"code","50a69b60":"markdown","c124eff0":"markdown","0acbcba5":"markdown","c0045a7d":"markdown","d59846c2":"markdown","a50a45c4":"markdown","85f96453":"markdown","45c0ac04":"markdown"},"source":{"aec9d7e7":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl >\/dev\/null\n!pip install \"..\/input\/pytorch-tabnet\/pytorch_tabnet-1.2.0-py3-none-any.whl\" >> quit","fa1b9c3d":"# Used most of coding from this kernel \nimport random\nimport os\nimport operator\nimport riiideducation\n\nimport datatable as dt\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\n\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')","240116d4":"class CFG:\n    START_IDX = 90000000\n    SEED = 42\n    TEST_SIZE = 0.2\n    N_EPOCHS = 5\n    BATCH_SZ = 1024\n    PATIENCE = 3\n    VIRTUAL_BS = 128\n    LR = 0.01\n    ND = 8  # Width of the decision prediction layer. Bigger values gives more capacity to the model with the risk of overfitting. \n    NA = 8  # Width of the attention embedding for each mask. According to the paper n_d=n_a is usually a good choice. \n    N_STEPS = 3 # Number of steps in the architecture (usually between 3 and 10)\n    GAMMA = 1.3 # This is the coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. \n    #Values range from 1.0 to 2.0.\n    N_INDEPENDENT = 1 # Number of independent Gated Linear Units layers at each step. Usual values range from 1 to 5.\n    LAMBDA = 0\n    N_SHARED = 3 # Number of shared Gated Linear Units at each step Usual values range from 1 to 5\n    MOMENTUM = 0.1\n    CLIP = 1.0\n    MASK_TYPE = 'sparsemax' #(default='sparsemax') Either \"sparsemax\" or \"entmax\" : this is the masking function to use for selecting features","d9866d16":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(CFG.SEED)","aa6142f8":"env = riiideducation.make_env()\ntrain = dt.fread(\"..\/input\/riiid-test-answer-prediction\/train.csv\").to_pandas()\n\n# Remove lectures\ntrain = train[train.content_type_id == False]\n\n# Arrange by timestamp\ntrain = train.sort_values(['timestamp'], ascending=True)\n\n# Drop useless columns\ntrain.drop(['timestamp','content_type_id'], axis=1, inplace=True)","22f42f92":"# Average of correct answers for each content_id\nresults_c = train[['content_id', 'answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content\"]\n\n# Number of correct answers for each suser\nresults_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","99e142cf":"# Select 10% of the training set (last interactions)\nX = train.iloc[CFG.START_IDX:,:]\n\n#del train\n\n# Merge with features previously computer\nX = pd.merge(X, results_u, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_c, on=['content_id'], how=\"left\")\n\n# Remove all lectures\nX = X[X.answered_correctly!= -1 ]\nX = X.sort_values(['user_id'])\n\n# Get ground truth\nY = X[[\"answered_correctly\"]]\nX = X.drop([\"answered_correctly\"], axis=1)\n\n# Categorical encoding\nlb_make = LabelEncoder()\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])\n\n# Keep relevant features\nX = X[['answered_correctly_user', 'answered_correctly_content', 'sum', 'prior_question_elapsed_time', 'prior_question_had_explanation_enc']] \nX.fillna(0.5, inplace=True)\n\nX.head()","6ca5a509":"#X = X[:2000]\n#Y = Y[:2000]\nXt, Xv, Yt, Yv = train_test_split(X, Y, test_size = CFG.TEST_SIZE, shuffle = False, random_state=CFG.SEED)","93338e6b":"cat_idxs = Xt.columns.get_loc('prior_question_had_explanation_enc')\ncat_dims = Xt['prior_question_had_explanation_enc'].nunique()\n\nprint(cat_idxs, cat_dims)","01f94950":"model = TabNetClassifier(\n    n_d = CFG.ND,\n    n_a = CFG.NA,\n    n_steps = CFG.N_STEPS,\n    gamma = CFG.GAMMA, \n    n_independent = CFG.N_INDEPENDENT,\n    n_shared = CFG.N_SHARED,\n    cat_dims=[cat_dims],\n    cat_emb_dim=1,\n    optimizer_params=dict(lr=CFG.LR),\n    momentum=CFG.MOMENTUM,\n    cat_idxs=[cat_idxs],\n    verbose=1,\n    #scheduler_params=dict(milestones=[20, 50, 80], gamma=0.5), \n    #scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n    mask_type = CFG.MASK_TYPE,\n    lambda_sparse = CFG.LAMBDA,\n    clip_value = CFG.CLIP\n)\n\nmodel.fit(\n    X_train = Xt.values, \n    y_train = Yt['answered_correctly'].values,\n    X_valid = Xv.values, \n    y_valid = Yv['answered_correctly'].values,\n    max_epochs = CFG.N_EPOCHS, \n    patience = CFG.PATIENCE,\n    batch_size = CFG.BATCH_SZ, \n    virtual_batch_size = CFG.VIRTUAL_BS,\n    num_workers = 0,\n    weights = 1,\n    drop_last = False\n)","63f84dc4":"# Plot losses\n#plt.plot(model.history['train']['loss'])\n#plt.plot(model.history['valid']['loss'])","1702d922":"# Plot learning rate\n#plt.plot(model.history['train']['lr'])","a5023479":"# Plot metric\n#plt.plot(np.array(model.history['train']['metric']) * -1)\n#plt.plot(np.array(model.history['valid']['metric']) * -1)","dd0a550f":"iter_test = env.iter_test()","c8113dd5":"for (test_df, sample_prediction_df) in iter_test:\n    test_df = pd.merge(test_df, results_u, on=['user_id'], how=\"left\")\n    test_df = pd.merge(test_df, results_c, on=['content_id'], how=\"left\")\n    test_df['answered_correctly_user'].fillna(0.5, inplace=True)\n    test_df['answered_correctly_content'].fillna(0.5, inplace=True)\n    test_df['sum'].fillna(0, inplace=True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    \n    test_ = test_df[['answered_correctly_user', 'answered_correctly_content', 'sum','prior_question_elapsed_time','prior_question_had_explanation_enc']]\n    test_.fillna(0.5, inplace=True)   # should be modified !\n    test_df['answered_correctly'] = model.predict(test_.values)\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","50a69b60":"# Config","c124eff0":"# MODEL","0acbcba5":"# UTILS","c0045a7d":"# Preprocess","d59846c2":"# PREPARE TRAIN SET","a50a45c4":"TabNet (https:\/\/github.com\/dreamquark-ai\/tabnet\/) starter kernel for RiiiD challenge\n\nFeature engineering is based on:\n- https:\/\/www.kaggle.com\/code1110\/riiid-gbdt-pipeline-baseline\n- https:\/\/www.kaggle.com\/lgreig\/simple-lgbm-baseline\n- https:\/\/www.kaggle.com\/jsylas\/riiid-lgbm-starter","85f96453":"# TRAIN SPLIT","45c0ac04":"# SUBMIT"}}