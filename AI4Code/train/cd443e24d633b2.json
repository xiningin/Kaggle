{"cell_type":{"42db91c2":"code","cd577a11":"code","b38a0fc5":"code","4b48ca5d":"code","ac5f1370":"code","a61778af":"code","aa88dbfc":"code","a1f0e00f":"code","bcfdd0d0":"code","e7d1fefc":"code","fbfedc30":"code","53095eec":"code","809bb920":"code","c9d2f40c":"code","c21165bc":"code","47eb5083":"code","7a4f2d97":"code","5a75af95":"code","93d7049e":"code","d0032d11":"code","eff139ac":"code","9752b3b7":"code","b2c06f80":"code","9d18577f":"code","a375acc4":"code","7d40c528":"code","f024b799":"code","0cf4ab0f":"code","75166dd6":"code","7b9c60f9":"code","1a43d7f6":"code","f94073ca":"code","9726bae6":"code","6fe41904":"code","5ba36b51":"code","6aa5d782":"code","bfd5a58d":"code","ccc5e16e":"code","659573bd":"code","73e08db5":"code","6d42ad87":"code","a69a9e8c":"code","bf411124":"code","af0008f3":"code","829e8547":"code","c8f231d6":"markdown","e0354e63":"markdown","8d3ff752":"markdown","a2baf95f":"markdown","1c4c914c":"markdown","4839be93":"markdown","b541770b":"markdown","53c9d6c4":"markdown","d097015e":"markdown","4f76c591":"markdown","9e8121b2":"markdown","3f03f07b":"markdown","b8f234ab":"markdown","5c2f1615":"markdown","06ad145a":"markdown","7a53c71a":"markdown","795e0571":"markdown","b1f004b2":"markdown","ba105c51":"markdown","2e8fe568":"markdown","1206397f":"markdown","61cf72d5":"markdown","e9ecd00f":"markdown","8528caa4":"markdown","2ddad6de":"markdown","47d9ea0a":"markdown","cd37fa6b":"markdown","59b75c91":"markdown","61abe5a7":"markdown","38fb60d7":"markdown","287cf40e":"markdown","2514f7ca":"markdown","fa5be37b":"markdown","3e36da77":"markdown","c6e514b3":"markdown","449f5c75":"markdown","80270308":"markdown","7aabffb3":"markdown","e691fa73":"markdown","03de82d6":"markdown","97d66fef":"markdown","72359d2f":"markdown","fe38aa55":"markdown","4bf2a3cc":"markdown","ba2e38b2":"markdown","3e40275b":"markdown","c75bf863":"markdown","12e5c183":"markdown","43af3488":"markdown","437b2734":"markdown","693e6a33":"markdown"},"source":{"42db91c2":"#!pip install pandas\n#!pip install seaborn\n#!pip install xgboost\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport sklearn \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import classification_report\n\nfrom scipy.stats import expon, gamma, randint, uniform\nfrom sklearn.utils.fixes import loguniform","cd577a11":"trainDT = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv', index_col = ['Id'], na_values = '?')\n\ntestDT = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv', index_col = ['Id'], na_values = '?')","b38a0fc5":"trainDT.head(7)","4b48ca5d":"trainDT.shape","ac5f1370":"testDT.head(7)","a61778af":"testDT.shape","aa88dbfc":"trainDT['income'] = LabelEncoder().fit_transform(trainDT['income'])\n\ntrainDT.head(7)","a1f0e00f":"sns.barplot(x = 'workclass', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","bcfdd0d0":"sns.barplot(x = 'education', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","e7d1fefc":"sns.barplot(x = 'marital.status', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","fbfedc30":"sns.barplot(x = 'occupation', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","53095eec":"sns.barplot(x = 'relationship', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","809bb920":"sns.barplot(x = 'race', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","c9d2f40c":"sns.barplot(x = 'sex', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","c21165bc":"sns.barplot(x = 'native.country', y = 'income', data = trainDT)\nplt.xticks(rotation=90)\nplt.show()","47eb5083":"print((trainDT['native.country'].value_counts()\/trainDT.shape[0])*100)","7a4f2d97":"trainDT.describe()","5a75af95":"sns.boxplot(x = 'income', y = 'age', data = trainDT)\nplt.show()","93d7049e":"sns.boxplot(x = 'income', y = 'education.num', data = trainDT)\nplt.show()","d0032d11":"sns.boxplot(x = 'income', y = 'hours.per.week', data = trainDT)\nplt.show()","eff139ac":"sns.boxplot(x = 'income', y = 'capital.gain', data = trainDT)\nplt.show()","9752b3b7":"sns.boxplot(x = 'income', y = 'capital.loss', data = trainDT)\nplt.show()","b2c06f80":"sns.heatmap(trainDT.corr(), square = True, annot = True, vmin = -0.1, vmax = 1)\nplt.show()","9d18577f":"trainDT.isna().sum()","a375acc4":"missing_data_trainDT = pd.concat([trainDT.isna().sum().sort_values(ascending = False), \n                          (trainDT.isna().sum()\/trainDT.isna().count()*100).sort_values(ascending = False)],\n                           axis = 1, keys = ['Total', '%'])\nmissing_data_trainDT.head(3)","7d40c528":"missing_data_testDT = pd.concat([testDT.isna().sum().sort_values(ascending = False), \n                          (testDT.isna().sum()\/testDT.isna().count()*100).sort_values(ascending = False)],\n                           axis = 1, keys = ['Total', '%'])\nmissing_data_testDT.head(3)","f024b799":"trainDT = trainDT.drop(['native.country', 'fnlwgt', 'education'], axis = 1)\n\ntrainDT.head()","0cf4ab0f":"testDT = testDT.drop(['native.country', 'fnlwgt', 'education'], axis = 1)\n\ntestDT.head()","75166dd6":"trainDT[['occupation', 'workclass']] = SimpleImputer(strategy = 'most_frequent').fit_transform(trainDT[['occupation','workclass']])\n\ntrainDT.isna().sum()","7b9c60f9":"testDT[['occupation', 'workclass']] = SimpleImputer(strategy = 'most_frequent').fit_transform(testDT[['occupation','workclass']])\n\ntestDT.isna().sum()","1a43d7f6":"trainDT[['age', 'education.num', 'hours.per.week']] = StandardScaler().fit_transform(trainDT[['age', 'education.num', 'hours.per.week']])\n#testDT[['age', 'education.num', 'hours.per.week']] = StandardScaler().fit_transform(testDT[['age', 'education.num', 'hours.per.week']])\n\n#trainDT[['age', 'education.num', 'hours.per.week','capital.gain','capital.loss']] = StandardScaler().fit_transform(trainDT[['age', 'education.num', 'hours.per.week','capital.gain','capital.loss']])\n#testDT[['age', 'education.num', 'hours.per.week','capital.gain','capital.loss']] = StandardScaler().fit_transform(testDT[['age', 'education.num', 'hours.per.week','capital.gain','capital.loss']])\n\ntrainDT.describe()","f94073ca":"trainDT = pd.get_dummies(trainDT, columns = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex'], drop_first = True)\n\ntrainDT.head()","9726bae6":"testDT = pd.get_dummies(testDT, columns = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex'], drop_first = True)\n\ntestDT.head()","6fe41904":"YtrainDT = trainDT.pop('income')\nXtrainDT = trainDT","5ba36b51":"YtrainDT.head()","6aa5d782":"XtrainDT.head()","bfd5a58d":"#Classificador:\n\nrf = RandomForestClassifier()\n\n\n#Defini-se o intervalo de par\u00e2metros:\n\ninterval_parameters_rf = {'n_estimators': np.arange(100, 500),\n   'criterion': ['gini', 'entropy'],\n   'max_depth': np.arange(1, 50),\n    'max_features':['sqrt', 5, 7, 9]}\n\n\n#Executa-se o comando RandomizedSearchCV:\n\nrf_param = RandomizedSearchCV(rf, interval_parameters_rf , cv=5, scoring='accuracy', n_iter=20, random_state=5)\n%timeit -n 1 -r 1 rf_param.fit(XtrainDT, YtrainDT)\n\n\n#Resultado:\n\nprint(f'Melhor par\u00e2metro = {rf_param.best_params_} | Acur\u00e1cia: {rf_param.best_score_}')\n\n\n#Aplica\u00e7\u00e3o:\n\nrf_pred = rf_param.predict(XtrainDT)","ccc5e16e":"#Classificador:\n\ndt = DecisionTreeClassifier()\n\n\n#Defini-se o intervalo de par\u00e2metros:\n\ninterval_parameters_dt = {'max_depth': list(range(5,100))}\n\n\n#Executa-se o comando RandomizedSearchCV:\n\ndt_param = RandomizedSearchCV(dt, interval_parameters_dt, cv=5, scoring='accuracy', n_iter=20, random_state=5)\n%timeit -n 1 -r 1 dt_param.fit(XtrainDT, YtrainDT)\n\n\n#Resultado:\n\nprint(f'Melhor par\u00e2metro = {dt_param.best_params_} | Acur\u00e1cia: {dt_param.best_score_}')\n\n\n#Aplica\u00e7\u00e3o:\n\ndt_pred = dt_param.predict(XtrainDT)","659573bd":"#Classificador:\n\nxgb = XGBClassifier(objective='binary:logistic', eval_metric='error', use_label_encoder=False)\n\n\n#Defini-se o intervalo de par\u00e2metros:\n\ninterval_parameters_xgb = {\n    'n_estimators': randint(100, 1000),\n    'learning_rate': loguniform(1e-3, 1),\n    'max_depth': randint(1, 20),\n    'reg_alpha': loguniform(1e-7, 1e1),\n    'reg_lambda': loguniform(1e-7, 1e1),\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n    'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n}\n\n\n#Executa-se o comando RandomizedSearchCV:\n\nxgb_param = RandomizedSearchCV(xgb, interval_parameters_xgb, cv=5, scoring='accuracy', n_iter=20, random_state=5)\n%timeit -n 1 -r 1 xgb_param.fit(XtrainDT, YtrainDT)\n\n\n#Resultado:\n\nprint(f'Melhor par\u00e2metro = {xgb_param.best_params_} | Acur\u00e1cia: {xgb_param.best_score_}')\n\n\n#Aplica\u00e7\u00e3o:\n\nxgb_pred = xgb_param.predict(XtrainDT)","73e08db5":"#Classificador:\n\nnb = GaussianNB()\n\n\n#Defini-se o intervalo de par\u00e2metros:\n\ninterval_parameters_nb = {'var_smoothing': np.logspace(0,-9, num=100)}\n\n\n#Executa-se o comando RandomizedSearchCV:\n\nnb_param = RandomizedSearchCV(nb, interval_parameters_nb, cv=5, scoring='accuracy', n_iter=30, random_state=5)\n%timeit -n 1 -r 1 nb_param.fit(XtrainDT, YtrainDT)\n\n\n#Resultado:\n\nprint(f'Melhor par\u00e2metro = {nb_param.best_params_} | Acur\u00e1cia: {nb_param.best_score_}')\n\n\n#Aplica\u00e7\u00e3o:\n\nnb_pred = nb_param.predict(XtrainDT)","6d42ad87":"#Classificador:\n\nnn = MLPClassifier()\n\n\n#Defini-se o intervalo de par\u00e2metros:\n\ninterval_parameters_nn = {'hidden_layer_sizes': [(2 ** n_1, 2 ** n_2) for n_1 in np.arange(4, 7) for n_2 in np.arange(4, 7)],\n                    'alpha': [1e-6, 1e-4, 1e-2],\n                    'learning_rate': ['constant','adaptive']}\n\n\n#Executa-se o comando RandomizedSearchCV:\n\nnn_param = RandomizedSearchCV(nn, interval_parameters_nn, cv=5, scoring='accuracy', n_iter=20, random_state=5)\n%timeit -n 1 -r 1 nn_param.fit(XtrainDT, YtrainDT)\n\n\n#Resultado:\n\nprint(f'Melhor par\u00e2metro = {nn_param.best_params_} | Acur\u00e1cia: {nn_param.best_score_}')\n\n\n#Aplica\u00e7\u00e3o:\n\nnn_pred = nn_param.predict(XtrainDT)","a69a9e8c":"print(\"Random Forest\")\nprint()\nprint(classification_report(YtrainDT, nb_pred, digits = 6))\n\nprint(\"Decision Tree\")\nprint()\nprint(classification_report(YtrainDT, dt_pred, digits = 6))\n\nprint(\"XGBoost\")\nprint()\nprint(classification_report(YtrainDT, xgb_pred, digits = 6))\n\nprint(\"Naive Bayes\")\nprint()\nprint(classification_report(YtrainDT, nb_pred, digits = 6))\n\nprint(\"Redes neurais\")\nprint()\nprint(classification_report(YtrainDT, nn_pred, digits = 6))","bf411124":"best_model = xgb_param.best_estimator_\nbest_model.fit(XtrainDT, YtrainDT)\n\nYtest = best_model.predict(testDT)\nYtest","af0008f3":"incomes = []\nfor income in Ytest:\n    if income == 0:\n        incomes.append('<=50K')\n    else:\n        incomes.append('>50K')\n\nsubmissao = pd.DataFrame({'Id': testDT.index, 'income': incomes})\nsubmissao","829e8547":"submissao.to_csv('submission.csv', index = False)","c8f231d6":"### Aplica\u00e7\u00e3o do melhor modelo e submiss\u00e3o","e0354e63":"### Escolha do melhor modelo\n\nPara escolha do melhor classificador \u00e9 aplicado a fun\u00e7\u00e3o \"classification_report\" para melhor compara\u00e7\u00e3o.","8d3ff752":"## 3 - An\u00e1lise da base de dados de treino","a2baf95f":"#### Redes neurais","1c4c914c":"#### Random Forest","4839be93":"Assim como a *feature* 'capital.gain', a 'capital.loss' tamb\u00e9m possui muitos valores concentrados no valor zero e uma grande vari\u00e2ncia com *outliers*. Tamb\u00e9m se faz pertinente tratar esta vari\u00e1vel antes de executar o algor\u00edtimo.","b541770b":"### Submiss\u00e3o  ","53c9d6c4":"Foi feito tamb\u00e9m um *heatmap* para evidenciar a rela\u00e7\u00e3o de cada vari\u00e1vel num\u00e9rica com a vari\u00e1vel *target* 'income':","d097015e":"\n## 2 - Importa\u00e7\u00e3o da base de dados de treino","4f76c591":"Observa-se que para a *feature* 'sex' o *income* \u00e9 maior para homens do que para mulheres, evidenciando a desigualdade de g\u00eanero no quesito de renda.","9e8121b2":"A an\u00e1lise do gr\u00e1fico acima mostra que a *feature* 'capital.gain' possui muitos valores concentrados no valor zero, e possui uma grande vari\u00e2ncia com *outliers*. Portanto se faz pertinente tratar esta vari\u00e1vel antes de executar o algor\u00edtimo.","3f03f07b":"Observa-se no gr\u00e1fico acima que em m\u00e9dia indiv\u00edduos com uma maior idade possuem uma renda maior.","b8f234ab":"Observa-se que para a *feature* 'race' o *income* \u00e9 maior para indiv\u00edduos de etnia branca (classe 'white') e americanos asi\u00e1ticos\/americanos das ilhas do Pac\u00edfico (classe 'Asian-Pac-Islander'). ","5c2f1615":"Observa-se que para a *feature* 'education' o *income* \u00e9 maior para individuos com maior grau de escolaridade, visto \u00e0s classes 'Prof-school', 'Doctorate' e 'Masters'. ","06ad145a":"Tratando agora os dados faltantes, como uma das *features* que possuiam dados faltantes n\u00e3o ser\u00e1 utilizada, a 'native.country', ela n\u00e3o ser\u00e1 tratada. As duas *features* que possuem os dados faltantes s\u00e3o categ\u00f3ricas, por isso esses dados ser\u00e3o substituidos pela moda da da respectiva *feature*.","7a53c71a":"Nota-se que para as *features* 'capital.gain' e 'capital.loss' podem possuir *outliers* devido ao valor m\u00e1ximo ser muito maior que suas m\u00e9dias e tamb\u00e9m possuem uma grande vari\u00e2ncia, visto seus valores elevados de desvios padr\u00e3o. ","795e0571":"#### Decision Tree","b1f004b2":"### Tranforma\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas para num\u00e9ricas","ba105c51":"### Exclus\u00e3o de *features* n\u00e3o utilizadas","2e8fe568":"### Tratamento de dados faltantes","1206397f":"Observa-se que para a *feature* 'occupation' o *income* \u00e9 maior para as classes 'Exec-managerial' e 'Prof-specialty'. ","61cf72d5":"Para utilizar a informa\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas no modelo preditivo se faz necess\u00e1rio transformar estas em num\u00e9ricas. Optou pelo m\u00e9todo do *One-hot enconding* pois as veri\u00e1veis categ\u00f3ricas n\u00e3o possuem nenhuma l\u00f3gica de ordena\u00e7\u00e3o entre si e que se fosse apenas numerar as classes est\u00e1ria incluindo uma informa\u00e7\u00e3o n\u00e3o verdadeira no processo.  ","e9ecd00f":"Em geral nota-se que todas as *features* categ\u00f3ricas se mostram relevantes e podem ser aplicadas para a execu\u00e7\u00e3o do algor\u00edtimo kNN.  ","8528caa4":"### Dados faltantes","2ddad6de":"Nota-se que 3 *features* categ\u00f3ricas possuem dados faltantes: 'workclass', 'occupation' e 'native.country'.\n\nAnalisa-se ent\u00e3o se essa quantidade \u00e9 parte significativa perante os dados totatis, calculando a porcentagem de dados faltantes:","47d9ea0a":"### Separa\u00e7\u00e3o da *feature* target  das demais","cd37fa6b":"Nota-se que aproximadamente 90% da base \u00e9 de indiv\u00edduos dos Estados Unidos, isso evidencia um desbalanceamento da *feature* em quest\u00e3o. Logo esta ter\u00e1 que ser tratada ou eliminada no momento da aplica\u00e7\u00e3o do algor\u00edtimo kNN. ","59b75c91":"#### Naive Bayes","61abe5a7":"## 5 - Modelos preditivos\n\n### Aplica\u00e7\u00e3o de diferentes classificadores para an\u00e1lise de efic\u00e1cia\n\nModelos:\n\n- Random Forest\n- Decision Tree\n- XGBoost \n- Naive Bayes\n- Redes neurais\n\nA seguir foi feito o Tuning para uma sele\u00e7\u00e3o otimizada dos hiperpar\u00e2metros de cada classificador. \n","38fb60d7":"Somando os valores isso representa aproximadamente 13% de dados faltantes o que \u00e9 uma parte consider\u00e1vel. Logo \u00e9 pertinente tratar esses dados faltantes de alguma forma.","287cf40e":"### Normaliza\u00e7\u00e3o de *features* com valores discrepantes","2514f7ca":"Observa-se que para a *feature* 'marital.status' o *income* \u00e9 maior para indiv\u00edduos com o estado civ\u00edl de casado, para c\u00f4njuge civil (classe 'Married-civ-spouse') e para c\u00f4njuge das for\u00e7as armadas (classe 'Married-AF-spouse') apesar deste possuir uma incerteza maior, provavelmente por apresentar um menor n\u00famero de casos desta classe. ","fa5be37b":"Primeiro para a base de treino:","3e36da77":"Observa-se que para a *feature* 'workclass' o *income* \u00e9 maior paras as classes 'Self-emp-inc' e 'federal-gov'. ","c6e514b3":"Primeiramente ser\u00e1 examinado as *features* categ\u00f3ricas. Para isso foram gerados gr\u00e1ficos de barras para cada feature com rela\u00e7\u00e3o a vari\u00e1vel *targert* \"income\". Note que cada barra nos gr\u00e1ficos possuem um tra\u00e7ado preto, este tra\u00e7ado indica o desvio padr\u00e3o.","449f5c75":"#### XGBoost ","80270308":"Observa-se que essa base de dados engloba 15 *features* das quais 6 features; \"age\", \"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\" e \"hours.per.week\"; compreendem vari\u00e1veis num\u00e9ricas e as demais 8; \"workclass\", \"education\", \"marital.status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native.country\" e \"income\" s\u00e3o vari\u00e1veis categ\u00f3ricas. \n\nO presente exerc\u00edcio tem como objetivo prever se um indiv\u00edduo na base de teste recebe mais que 50 mil no ano ou menos\/igual que 50 mil no ano (informa\u00e7\u00e3o presente na coluna \"income\" na base de treino) baseando-se nas outras vari\u00e1veis. Portanto, na pr\u00f3xima se\u00e7\u00e3o a base de dados de treino ser\u00e1 analisada mais afundo, explorando a rela\u00e7\u00e3o da vari\u00e1vel \"income\" com as demais.\n\nPara melhor an\u00e1lise da base de dados e a vari\u00e1vel *target* \"income\", esta ser\u00e1 tranformada de vari\u00e1vel categ\u00f3rica, para uma vari\u00e1vel num\u00e9rica, especificamente de forma bin\u00e1ria. Logo o valor de 1 ser\u00e1 atribuido para quem recebe mais de 50 mil anual e o valor de 0 ser\u00e1 atribuido para quem recebe menos\/igual de 50 mil anual.","7aabffb3":"Nota-se pelo gr\u00e1fico acima que indiv\u00edduos com trabalham mais horas por semana, em m\u00e9dia, possuem uma maior renda. ","e691fa73":"# PMR3508 - KNN for Dataset Adult - Parte 2\n\n## 1 - Importa\u00e7\u00e3o de bibliotecas:","03de82d6":"Verificando dados faltantes na base teste, tem-se:","97d66fef":"## 4 - Tratamento dos dados e sele\u00e7\u00e3o das *features*","72359d2f":"Prosseguindo, a seguir ser\u00e1 examinado as *features* num\u00e9ricas. Para isso foram gerados diagrama de caixa para cada feature com rela\u00e7\u00e3o a vari\u00e1vel *targert* \"income\". \n\nAlguns detalhes das vari\u00e1veis num\u00e9ricas s\u00e3o exibidos a seguir:","fe38aa55":"Nota-se que todas as vari\u00e1veis possuem alguma rela\u00e7\u00e3o\/influ\u00eancia para com a vari\u00e1vel 'income', o que foi verificado na an\u00e1lise dos diagramas de caixas anteriormente, com exce\u00e7\u00e3o da 'fnlwgt',o que se mostra pertinente n\u00e3o utilizar essa *feature* na execu\u00e7\u00e3o do algor\u00edtimo. ","4bf2a3cc":"A seguir, para a base de teste:","ba2e38b2":"Observando o gr\u00e1fico acima da *feature* native.country, nota-se que todos os pa\u00edses aprentam um elevado desvio padr\u00e3o exceto os Estados Unidos. Explora-se, ent\u00e3o, um pouco mais essa *feature* calculando o percentual de cada classe na base de dados de treino:","3e40275b":"Para sele\u00e7\u00e3o das *features*, diantes do que foi analisado, 3 *features* n\u00e3o ser\u00e3o utilizadas para a aplica\u00e7\u00e3o do modelo preditivo. Estas s\u00e3o 'native.country', 'fnlwgt' e 'education'. A 'native.country' n\u00e3o ser\u00e1 utilizada devido ao seu grande desbalanciamento, a 'fnlwgt' n\u00e3o ser\u00e1 utilizada pois verificou-se que n\u00e3o a reela\u00e7\u00e3o com a vari\u00e1vel *target* e a 'education' n\u00e3o ser\u00e1 utilizada pois a informa\u00e7\u00e3o que esta carrega ja est\u00e1 compreendida pela vari\u00e1vel n\u00famerica 'education.num', o que seria redundante sua utiliza\u00e7\u00e3o.   \n\nS\u00e3o tiradas da tab\u00e9la de dados de treino e de teste essas *features*:","c75bf863":"Para a base de teste, os dados faltantes representam aproximadamente 13,5% o que tamb\u00e9m \u00e9 uma parcela consider\u00e1vel. ","12e5c183":"Observa-se no gr\u00e1fico acima que em m\u00e9dia indiv\u00edduos com uma maior escolaridade possuem uma renda maior, isso condiz com o que foi observado na an\u00e1lise da *feature* categ\u00f3rica 'education'.","43af3488":"Ademais, \u00e9 importante na an\u00e1lise do banco de dados de treino verificar se existem dados faltantes e como tratar esse problema. A seguir foi exibido a existencia de dados faltantes:","437b2734":"Observa-se que para a *feature* 'relationship' o *income* \u00e9 maior para indiv\u00edduos sendo marido (classe 'hunband') e esposa (classe 'wife'). Isso condiz com a observa\u00e7\u00e3o da *feature* 'marital.status', a qual mostrou que indiv\u00edduos casados possuem um maior *income*.   ","693e6a33":"Avalia-se qual modelo teve o melhor desempenho dentre os 5 modelos treinados. Para essa an\u00e1lise, foi considerado o melhor modelo que tivesse a maior acur\u00e1cia m\u00e9dia nos conjuntos de dados da valida\u00e7\u00e3o.\n\nNota-se pelos dados gerados acima que o melhor classificador obtido foi o \"XGBoost\". A seguir, este modelo \u00e9 aplicado para a base de teste e submetida para avalia\u00e7\u00e3o. "}}