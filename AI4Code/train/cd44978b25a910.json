{"cell_type":{"1464cc85":"code","8d06121f":"code","d74de7e2":"code","a5b266a7":"code","fad2a174":"code","aafe7d9d":"code","eb7f3d7f":"code","8aa24f86":"code","5a32c1cc":"code","f62fce41":"code","366f6ece":"code","1d87302d":"code","11c94fd4":"code","164ab0b9":"code","8713993b":"code","3cd4b0c1":"code","bbcc2ed6":"code","8d250fd5":"code","35eb31af":"code","c8cd704e":"code","19a27f91":"code","3d45356f":"code","f9f6a6c0":"code","489e9303":"code","69b69dc5":"code","5d064c59":"code","33bb8a6e":"code","36bd970e":"code","c94dc610":"code","538ed983":"code","f5a5824f":"code","5c31f8f7":"code","ce270763":"code","f8b09657":"markdown","24059f26":"markdown","bba0d58a":"markdown","a2bd43ed":"markdown","66267018":"markdown","07c2b0dd":"markdown","f8f82ba0":"markdown","59d9a9de":"markdown","25b1da53":"markdown","a211d612":"markdown","dec942db":"markdown","cc9020e5":"markdown","d159f74d":"markdown","6988a7ef":"markdown","6c62b0a9":"markdown","b6d587df":"markdown","958cd3cd":"markdown","40debfe6":"markdown","a4ebcdae":"markdown","f644c92e":"markdown","845d1635":"markdown","88393656":"markdown","7da3b3c1":"markdown","33324adc":"markdown","5daa958f":"markdown","9f071824":"markdown","b3389005":"markdown","9f07c9a0":"markdown","f260e440":"markdown","17ded898":"markdown","811c60ce":"markdown","040b7f75":"markdown","33dc3ba0":"markdown","1625447a":"markdown","1d88fde4":"markdown","3b4aa4de":"markdown","d103681f":"markdown","dfef0b3c":"markdown","b3e3dda0":"markdown","8365ca23":"markdown","83c40427":"markdown"},"source":{"1464cc85":"import pandas as pd\nimport numpy as np\nfrom pandas import plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\nfrom sklearn import metrics\nimport statsmodels.api as sm\nimport statsmodels.stats.diagnostic as smd\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import gaussian_kde\nfrom statsmodels.graphics.gofplots import ProbPlot\nfrom scipy import stats","8d06121f":"# Load CSV file data into a dataframe.\ndf = pd.read_csv(\"\/kaggle\/input\/pga-tour-20102018-data\/2019_data.csv\")\ndf.head()","d74de7e2":"# Transpose the statistic variables such that there is 1 golfer per row, each with columns for\n# every statistic variable.\ndf = df.set_index(['Player Name', 'Variable', 'Date'])['Value'].unstack('Variable').reset_index()\n\n# Typecast the Date column to datetime objects so they can be quantified.\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Select data from 8\/25\/19, which was the end of the 2018-2019 PGA season.\ndf = df[(df['Date'] == '2019-08-25')]\n\n# Typecast data points to numeric data types, except Player Name and Date columns.\ndf.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce').fillna(0)\n\ndf.head()","a5b266a7":"# Check all of the statistic variables available in the dataset.\nfor col in df.columns[2:]:\n    print(col)","fad2a174":"# Visualize the distribution and colinearity for a sample of random variables.\n\nsample_df = df.iloc[:, 2:].sample(n=8, axis=1)\n\nfig1 = pd.plotting.scatter_matrix(sample_df, figsize=(24, 24))\n\nfor x in range(len(sample_df.columns)):\n    for y in range(len(sample_df.columns)):\n        ax = fig1[x, y]\n        ax.xaxis.label.set_rotation(45)\n        ax.yaxis.label.set_rotation(45)\n        ax.yaxis.labelpad = 100","aafe7d9d":"selected_var = 'Greens in Regulation Percentage - (%)'","eb7f3d7f":"# Only keep the average, percentage, and points variables, otherwise there will be many 1.0 co-efficients cluttering the analysis.\nfor col in df.columns:\n    if 'AVG' not in col and '%' not in col and 'POINTS' not in col:\n        del df[col]","8aa24f86":"# Create a sorted Pierson correlation matrix to understand variable pair relationships.\n\nmatrix_df = df.corr().abs()\nunstack_matrix = matrix_df.unstack()\nsorted_matrix = unstack_matrix.sort_values(kind='quicksort', ascending=False, na_position='first').dropna()\n  \nprint('ALL CORRELATIONS ARE BETWEEN \\\"{}\\\" AND AN ARBITRARY VARIABLE'.format(selected_var))\nprint('='*95+'\\n')\n    \ncount = 0\nfor pair, val in sorted_matrix.items():\n    if pair[1] == selected_var and count < 10:\n        print('{:68} PIERSON CO-EFF.'.format(pair[0] + ' ,'))\n        print('{:68} {}'.format(pair[1], val))\n        print('-'*88)\n        count += 1","5a32c1cc":"# Select the highly correlated pairs that contain the selected variable.\n\npairs = []\n\nfor pair, val in sorted_matrix.items():\n    var1, var2 = pair\n    if var2 == selected_var:\n        pairs.append([var1, var2, val])","f62fce41":"# Test the significance of the correlations with the selected variable using p-values of the co-effs.\n\nlin_regress_dict = {}\n\nfor pair in pairs:\n    var1_list = df[pair[0]].values.tolist()\n    var2_list = df[pair[1]].values.tolist()\n    (slope, intercept, r_value, p_value, std_err) = linregress(var1_list, var2_list)\n    \n    key_name = \"{}, {}\".format(pair[0], pair[1])\n    lin_regress_dict[key_name] = ((slope, intercept, r_value, p_value, std_err))\n    \n# Keep the most significantly correlated pairs\nfor key, val in list(lin_regress_dict.items()):\n    if val[3] > 0.05:  # p-value > 0.05\n        del lin_regress_dict[key]","366f6ece":"# Sort the correlated pairs by p-value.\nsorted_pvalues = sorted(lin_regress_dict.items(), key=lambda x: x[1][3])\n\n# Print the most significantly correlated variables to the selected variable.\nprint('VARIABLE CORRELATIONS TO \\\"{}\\\", SORTED BY P-VALUE\\n'.format(selected_var))\nprint('{:58} {:13} {}'.format('NAME', 'R-VALUE', 'P-VALUE'))\nprint('='*81)\n\nfor pair in sorted_pvalues[:100]:\n    var1, var2 = pair[0].split(', ')\n    #split1, split2 = name1.split(' - ', 1)\n    slope, intercept, r_value, p_value, std_err = pair[1]\n    \n    print('{:52}   |   {:6.4f}   |   {:4}'.format(var1, r_value.round(4), p_value))\n    print('-'*81)  ","1d87302d":"independent_var = 'Driving Distance - (AVG.)'\ndependent_var = 'Greens in Regulation Percentage - (%)'\n\nx = pd.DataFrame(df[independent_var])\ny = pd.DataFrame(df[dependent_var])","11c94fd4":"# Split data for later validation.\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\n# Concatenate the variables into one dataframe for easier reference.\ntrain_df = pd.concat([x_train, y_train], axis=1)","164ab0b9":"# Generate an Ordinary Least Squares regression model.\n\nX_train = sm.add_constant(x_train)\n\nols_model = sm.OLS(y_train, X_train).fit()\n\nprint(ols_model.summary())","8713993b":"# Cross validate with the test set of data.\n\nX_test = sm.add_constant(x_test)\n\nols_model_test = sm.OLS(y_test, X_test).fit()\n\nprint(ols_model_test.summary())","3cd4b0c1":"# Set variables for info from the model, to use for analysis.\n\n# Model fitted values.\nols_model_fitted_y = ols_model.fittedvalues\n\n# Model residuals.\nols_model_residuals = ols_model.resid\n\n# Normalized residuals.\nols_model_norm_residuals = ols_model.get_influence().resid_studentized_internal\n\n# Absolute squared normalized residuals.\nols_model_norm_residuals_abs_sqrt = np.sqrt(np.abs(ols_model_norm_residuals))\n\n# Leverage.\nols_model_leverage = ols_model.get_influence().hat_matrix_diag","bbcc2ed6":"# Regression plot.\n\nplt.style.use('seaborn')\n\nfig2, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nax1.scatter(x_train, y_train, alpha=0.5)\nax1.plot(x_train, ols_model_fitted_y, color='red', linewidth=2)\n\nax1.set_title('Regression')\nax1.set_xlabel(independent_var)\nax1.set_ylabel(dependent_var)\nax1.text(200, 10,'y = 0.2259x + 0.0053', fontsize=20)\n\nfig3, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nax1.scatter(x_train, y_train, alpha=0.8)\nax1.plot(x_train, ols_model_fitted_y, color='red', linewidth=2)\n\nax1.set_title('Regression - (Magnified View)')\nax1.set_xlabel(independent_var)\nax1.set_ylabel(dependent_var)\nax1.set_xbound(min([x for x in x_train[independent_var] if x!=0])-5, \\\n               max(x_train[independent_var])+5)\nax1.set_ybound(min([y for y in y_train[dependent_var] if y!=0])-5, \\\n               max(y_train[dependent_var])+5)","8d250fd5":"# Residuals density plot\n\nmean = np.mean(ols_model_residuals)\nstd = np.std(ols_model_residuals)\n\nkde = gaussian_kde(ols_model_residuals)\ncovf = kde.covariance_factor()\nbw = covf * std\n     \nfig4, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.distplot(ols_model_residuals, kde_kws={'bw': bw})\n\nax1.set_title('Residual Density')\nax1.text(2.5, 1.25, \"mean = {:.4f}\\nstd = {:.4f}\".format(mean, std), fontsize=18)","35eb31af":"# Normal Q-Q plot.\n\nfig5, ax1 = plt.subplots(figsize=(12, 8))\n\nQQ = ProbPlot(ols_model_norm_residuals)\n\nQQ.qqplot(line='45', alpha=0.3, lw=1, color='#4c72b0', ax=ax1)\n\nax1.set_title('Normal Q-Q')\nax1.set_xlabel('Theoretical Quantiles')\nax1.set_ylabel('Standardized Residuals')","c8cd704e":"# Residuals vs Fitted plot.\n\nfig6, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.residplot(ols_model_fitted_y, train_df[dependent_var], lowess=False, ax=ax1, \\\n              scatter_kws={'alpha': 0.6}, line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_title('Residuals vs Fitted')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Residuals');\n\nfig7, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.residplot(ols_model_fitted_y, y_train[dependent_var], lowess=False, ax=ax1, \\\n              scatter_kws={'alpha': 0.8}, line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_title('Residuals vs Fitted - (Magnified View)')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Residuals');\nax1.set_xbound(min([x for x in ols_model_fitted_y if x>1])-3, \\\n               max(y_train[dependent_var])+2)","19a27f91":"# Scale-Location plot.\n\nfig8, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.regplot(ols_model_fitted_y, ols_model_norm_residuals_abs_sqrt, ci=False, \\\n            lowess=False, scatter_kws={'alpha': 0.6}, fit_reg=False, ax=ax1, \\\n            line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_title('Scale-Location')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Standardized Residuals')\n\nfig9, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.regplot(ols_model_fitted_y, ols_model_norm_residuals_abs_sqrt, ci=False, \\\n            lowess=False, scatter_kws={'alpha': 0.8}, fit_reg=False, ax=ax1, \\\n            line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})   \n\nax1.set_title('Scale-Location - (Magnified View)')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Standardized Residuals')\nax1.set_xbound(60, 74)","3d45356f":"# Residuals vs Leverage plot.\n\nfig10, ax1 = plt.subplots(figsize=(12,8))\n\nplt.scatter(ols_model_leverage, ols_model_norm_residuals, alpha=0.5)\n\nsns.regplot(ols_model_leverage, ols_model_norm_residuals, ax=ax1, \\\n              scatter=False, ci=False, lowess=False, fit_reg=False, \\\n              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nax1.set_xlim(0, max(ols_model_leverage)+0.001)\nax1.set_title('Residuals vs Leverage')\nax1.set_xlabel('Leverage')\nax1.set_ylabel('Standardized Residuals')","f9f6a6c0":"new_train_df = train_df.copy()\n\nfor idx, row in new_train_df.iterrows():\n    if row[1] <= 0:\n        new_train_df.drop(idx, axis=0, inplace=True)\n\nnew_x = pd.DataFrame(new_train_df[independent_var])\nnew_y = pd.DataFrame(new_train_df[dependent_var])\n\nnew_x_train, new_x_test, new_y_train, new_y_test = train_test_split(new_x, new_y, test_size=0.2)\n\nnew_X_train = sm.add_constant(new_x_train)\n\nnew_ols_model = sm.OLS(new_y_train, new_X_train).fit()\n\nprint(new_ols_model.summary())","489e9303":"# Cross validate with the test set of data.\n\nnew_X_test = sm.add_constant(new_x_test)\n\nnew_ols_model_test = sm.OLS(new_y_test, new_X_test).fit()\n\nprint(new_ols_model_test.summary())","69b69dc5":"# Set variables for info from the model, to use for analysis.\n\n# Model fitted values.\nnew_ols_model_fitted_y = new_ols_model.fittedvalues\n\n# Model residuals.\nnew_ols_model_residuals = new_ols_model.resid\n\n# Normalized residuals.\nnew_ols_model_norm_residuals = new_ols_model.get_influence().resid_studentized_internal\n\n# Absolute squared normalized residuals.\nnew_ols_model_norm_residuals_abs_sqrt = np.sqrt(np.abs(new_ols_model_norm_residuals))\n\n# Leverage.\nnew_ols_model_leverage = new_ols_model.get_influence().hat_matrix_diag","5d064c59":"# New Regression plot.\n\nfig11, ax1 = plt.subplots(1, 1, figsize=(12, 8))\nfig11.suptitle('NEW')\n\nax1.scatter(new_x_train, new_y_train, alpha=0.8)\nax1.plot(new_x_train, new_ols_model_fitted_y, color='red', linewidth=2)\n\nax1.set_title('Regression')\nax1.set_xlabel(independent_var)\nax1.set_ylabel(dependent_var)\nax1.text(275, 61,'y = 0.06x + 47.45', fontsize=20)\n\n# Compare with the original regression plot.\nfig2","33bb8a6e":"# New Residuals density plot\n\nmean = np.mean(new_ols_model_residuals)\nstd = np.std(new_ols_model_residuals)\n\nkde = gaussian_kde(new_ols_model_residuals)\ncovf = kde.covariance_factor()\nbw = covf * std\n     \nfig12, ax1 = plt.subplots(1, 1, figsize=(12, 8))\nfig12.suptitle('NEW')\n\nsns.distplot(new_ols_model_residuals, kde_kws={'bw': bw}, ax=ax1)\n\nax1.text(5, 0.15, \"mean = {:.4f}\\nstd = {:.4f}\".format(mean, std), fontsize=18)\nax1.set_title('Residual Density')\n\n# Compare with original residual density plot.\nfig4","36bd970e":"# Test if residuals are normally distributed using a test that factors skew and kurtosis.\n\ns, pval = stats.normaltest(new_ols_model_residuals)\n\nif pval < 0.05:\n    print('new_ols_model.resid is not normally distributed.')\nelse:\n    print('new_ols_model.resid is normally distributed.')","c94dc610":"# Residuals vs Fitted plot.\n\nfig13, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.residplot(new_ols_model_fitted_y, new_y_train, lowess=True, ax=ax1, \\\n              scatter_kws={'alpha': 0.6}, line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_title('Residuals vs Fitted')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Residuals');\n\n# Compare with the original residuals vs fitted plot\nfig6","538ed983":"# Use Breush-Pagan Test to check for heteroskedasticity.\n\ntest = smd.het_breuschpagan(new_ols_model_residuals, new_ols_model.model.exog)\n\nif test[1] > 0.05:\n    print('There is not enough evidence to conclude that there is heteroskedasticity in the data.')\nelse:\n    print('There is enough evidence to conclude that there is heteroskedasticity in the data.')","f5a5824f":"# New Normal Q-Q plot.\n\nfig13, ax1 = plt.subplots(figsize=(12, 8))\n\n\nQQ = ProbPlot(new_ols_model_residuals)\n\nQQ.qqplot(line='45', alpha=0.3, lw=1, color='#4c72b0', ax=ax1)\n\nax1.set_title('Normal Q-Q')\nax1.set_xlabel('Theoretical Quantiles')\nax1.set_ylabel('Standardized Residuals')\n\n# Compare to original Normal QQ plot\nfig5","5c31f8f7":"# New Scale-Location plot.\n\nfig14, ax1 = plt.subplots(1, 1, figsize=(12, 8))\nfig14.suptitle('NEW')\n\nsns.regplot(new_ols_model_fitted_y, new_ols_model_norm_residuals_abs_sqrt, ci=False, \\\n            lowess=True, scatter_kws={'alpha': 0.6}, ax=ax1, \\\n            line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_title('Scale-Location')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Standardized Residuals')\n\n# Compare with original scale-location plot.\nfig8","ce270763":"# Residuals vs Leverage plot.\n\nfig15, ax1 = plt.subplots(figsize=(12,8))\nfig15.suptitle('NEW')\n\nplt.scatter(new_ols_model_leverage, new_ols_model_norm_residuals, alpha=0.6)\n\nsns.regplot(new_ols_model_leverage, new_ols_model_norm_residuals, ax=ax1, \\\n              scatter=False, ci=False, lowess=True, \\\n              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nax1.set_xlim(0, max(new_ols_model_leverage)+0.001)\nax1.set_title('Residuals vs Leverage')\nax1.set_xlabel('Leverage')\nax1.set_ylabel('Standardized Residuals')\n\n# Compare with original residuals vs leverage plot.\nfig10","f8b09657":"<a id='OLS'><\/a>","24059f26":"<a id='OLS_analysis'><\/a>","bba0d58a":"The lowess line has some arch in it, so there may be some bias in the data that's causing this.\nNext we'll check for heteroskedasticity using a test.","a2bd43ed":"As expected the cluster of 0 values do not have any leverage on the model because the regression line was plotted onto those points. The remaining non-zero\npredictors have slighly more leverage but not nearly enough to be considered outliers. The x-scale is only a range of 0.008, so very insignificant leverage.","66267018":"The new regression plot is essentially the entire cluster of non-zero points seen in the original regression. The difference is that now the model doesn't fit\nthe cluster of zeros so the regression line will be more true to all of the other predictors besides 0.","07c2b0dd":"# Ordinary Least Squares Regression\n[Back to Table of Contents](#ToC)","f8f82ba0":"Before, the model was fit in favor of the 0-cluster. This consequently gave more leverage to non-zero points since they were\nfurther away from the regression line. Now that the model better fits those non-zero points, they don't have as much leverage.\n","59d9a9de":"In the new plot, the line is much straighter. This illustrates that the distribution of residuals over the quantiles is more normal.\nThe line is straight, which leads us to believe that the residuals are normally distributed. Ideally, the line would follow the 45 \ndegree red line, but I don't think that it's much of an issue in this case. More than 95% of the standardized residuals still fall within\n2 standard deviations of the mean, with standard deviation being 1. Then since there aren't really any clusters and the line is straight,\nthere aren't residuals values that are contributing much more than others.","25b1da53":"<a id='new_OLS_analysis'><\/a>","a211d612":"Now that we can visualize the distribution of the residuals, it's clear that although the data is centered, most of it is located within half of a standard deviation from the mean, \nso it isn't normally distributed.","dec942db":"This plot shows that the residuals are evenly spread among the predictors. My one concern is that the amount of\n0's is actually making this distribution skewed towards them. I can't tell from this plot alone, how many 0's there are.","cc9020e5":"<a id='data_clean'><\/a>","d159f74d":"The adjusted r^2 value is very high which tells us that the model fits the data well. In other words, 99.8% of the variation in Greens in Regulation is\nexplained by the model. I do believe that there's bias among this because there are so many 0 values and the model fits through 0. So by default, there are\ngoing to be many predicted values on the regression line. Looking at skew and kurtosis, it seems the residuals aren't very off center, slightly negatively skewed.\nThe kurtosis is very high which means there's a large peak in the distribution, which must be attributed to the large cluster of 0 values in the predictor data.\nThis high kurtosis translates to a large value for the Jarque-Bera test, which indicates that there definitely isn't a normal distribution of the residuals. ","6988a7ef":"# Ordinary Least Squares Regression With Zeros Removed\n[Back to Table of Contents](#ToC)\n\nNow I'll remove the 0 values from the data and generate another OLS model to see if performance is improved.","6c62b0a9":"<a id='ToC'><\/a>","b6d587df":"<a id='corr_analysis'><\/a>","958cd3cd":"The data seemed homoskedastic before, apart from the cluster of zeros. The new plot appears to be even more homoskedastic. I can't know for certain\nbecause I couldn't fit a regress line on the old plot due to all the replicant zero values.","40debfe6":"I choose a variable to focus on for the correlation analysis. This is the potential dependent\nvariable. We'll be able to compare other variables with it to determine if there's a\nlinear relationship. This information will be used to decide dependent and independent variables.","a4ebcdae":"The results of fitting an OLS model using the test data are very similar to the train data. Therefore the model we plan to use is generalized for the range of the independent variable.\n","f644c92e":"There seems to be many variables that are highly correlated with each other. Also the distribution for many variables is zero-inflated.\n\nThis multicolinearity and zero-inflated distribution should negatively influence the model.","845d1635":"<a id='summary'><\/a>","88393656":"# Table of Contents\n* [Data Exploration](#data_explore)\n    * [Data Cleanup](#data_clean)\n    * [Correlation Analysis](#corr_analysis)\n* [Ordinary Least Squares Regression Model](#OLS)\n    * [OLS Model Analysis](#OLS_analysis)    \n* [Ordinary Least Squares Regression Model With Zeros Removed](#new_OLS)\n    * [New OLS Model Analysis](#new_OLS_analysis)    \n* [Summary](#summary)  \n   ","7da3b3c1":"<a id='data_explore'><\/a>","33324adc":"The coefficient of determination drastically decreased. The original model accounted for 99.8% of the variance in Green in Regulation,\nwhere as the new model only explains 8.8% of variance in the dependent variable. This is due to the bias from the cluster of zeros. The trade off is that now the residuals are perhaps normally\ndistributed. Skew is slightly more centered and kurtosis is very much lowered. Also the co-efficient of the independent variable is much lower\nthan the original model. This is because the cluster of 0 has leveraged the slope of the regression line to be more steep so that it would reach\nall of those points.","5daa958f":"The distribution of the residuals now appears to be normal after removing the 0 values. To be more sure, we'll run the residuals through a test.","9f071824":"This plot illustrates the increasing linear relationship between Greens in Regulation and Driving Distance. Notice how the model fits in the cluster of 0's.\nThis is costing accuracy for all remaining y-values. The regression line cannot best fit the big cluster of points, shown in the bottom plot, if the same line\nalso needs to account for those 0 values. Since the line appears to split the cluster at a higher angle than it should, some predicted y values will be overfit, then\nothers will be underfit as the values of the independent variable increase.\n","b3389005":"# Data Exploration\n[Back to Table of Contents](#ToC)\n\nThis dataset consists of many measures for individual golfers on the PGA Tour. Each golfer has data for nearly 1500 variables, which is also grouped by date. \nSo after each week, each golfer's data is updated and a new entry is made for them.","9f07c9a0":"## OLS Model Analysis\n[Back to Table of Contents](#ToC)","f260e440":"## Correlation Analysis\n[Back to Table of Contents](#ToC)","17ded898":"<a id='new_OLS'><\/a>","811c60ce":"This plot further confirms that the distribution of the residuals is not normal. The horizontal cluster of points represents all of the \n0 values that are fit perfectly onto the regression line. There are so many of them that they cover roughly 3.5 'quantiles', which is most\nof the total distribution of the residuals. Since the model weighs all of these 0s the same as the other points, the fit is going to be\nskewed towards them. So those 0 values will have 0 residuals, since the model fit the regression line directly through those points. Because\nof this, the remaining points can't fulfil the necessary residual values that are needed to make the residual distribution normal. If I understand\nthis correctly, a model that could either remove the 0 values or give them less weight or importance, could fit a line through the data that would\nreduce the range of the residual values and also reduce the amount of 0 valued residuals. This would potentially result in a normal distribution\nof the residuals and thus a better model overall.","040b7f75":"The number of observations is very small, after removing values of zero. So I don't think that the differences in the train and test\nsets are very significant.","33dc3ba0":"There are many extremely correlated variables, which again emphasizes the significant multicollinearity of this data.","1625447a":"The original correlation analysis was inaccurate because the data was zero inflated. This directly inflated the Pierson co-efficients for\nperhaps every pair of variables. As a result, I inaccurately chose the dependent and independent variables because I thought they were more\ncorrelated than they truly were. After generating an OLS model which used data without zeros, it was determined that there wasn't a linear\nrelationship between the chosen variables. As a result of a poor linear relationship, the fit of the model wasn't very good. The co-efficient\nof determination dropped from 99.8% to merely 8.8%. Overall, the second model provided a more realistic fit for the data, although it wasn't\nnearly as accurate.\n\nInstead of continuing this notebook further, I'll make a new one that will account for the zero-inflation from the start. If I can analyze the \ncorrelation of variables without the bias from the zeros, then I should be able to choose better variables for the model and thus the model\nshould fit the data better. \n\n[Click here to see the updated OLS model](https:\/\/www.kaggle.com\/ryanalbertson\/ols-model-using-pga-tour-data)","1d88fde4":"## New OLS Model Analysis\n[Back to Table of Contents](#ToC)","3b4aa4de":"This notebook is going to be used for learning and practicing the use of an ordinary least squares regression model in Python.\nMore specifically, I want to learn and understand the fundamental statistics within linear regression.\n\n*I will greatly appreciate any feedback regarding the code or statistics, as I'm trying to learn as much as I can. Thanks.*","d103681f":"*The lowess lines wouldn't plot because there are too many replicant values in the data.*","dfef0b3c":"With this information, I chose the dependent and independent variables. I kept the selected variable as the dependent variable. I then chose\n\"Driving Distance - (AVG.)\" as the independent variable because the correlation with \"Greens in Regulation Percentage - (%)\" is high, as is the \nsignificance indicated by the p-value. This provides a strong linear relationship for the regression model.","b3e3dda0":"I believe the variation in the residuals is homoskedastic, because the distribution seems to be uniform. The model appears to overfit values\nin the low-60s to mid-60s, while underfitting values in the high-60s to low-70s. I think this is because the cluster of 0's is leveraging the slope\nof the regression line.","8365ca23":"# Summary\n[Back to Table of Contents](#ToC)","83c40427":"## Data Cleanup\n[Back to Table of Contents](#ToC)"}}