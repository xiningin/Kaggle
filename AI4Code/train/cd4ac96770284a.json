{"cell_type":{"ac48a655":"code","66f30258":"code","593020d9":"code","6e10aa94":"code","ff6b5b9c":"code","946c9820":"code","a36fdcef":"code","4d9b8fc0":"code","22a42875":"code","230ab82c":"code","c4c17e14":"code","23424c16":"code","57c197ef":"code","3a746052":"code","76c45ddd":"code","29802156":"code","bd0ba235":"code","07172f59":"code","98dfd036":"code","df010d9e":"code","e1297257":"code","cbef3ede":"code","9cde500e":"code","56623234":"code","058c1601":"code","98577a1c":"code","d9bc2fd7":"code","cb34ea64":"code","964f8d9e":"code","d792576d":"code","7583e7ae":"code","58737994":"code","2e72ab41":"code","f366dffb":"code","48a22734":"code","2c389f45":"code","84ee03f8":"code","8b87683e":"code","7a4d5ed5":"markdown","14ba39c5":"markdown","8f2eea52":"markdown","4444b44b":"markdown","2a503628":"markdown","90062167":"markdown","0e87e21e":"markdown","8db6cc5c":"markdown","d4874530":"markdown","1b79ba58":"markdown","9f2e526c":"markdown","ff33174e":"markdown","04e283bb":"markdown","3c6695e0":"markdown","73b65d5c":"markdown","220acaa4":"markdown","1142ee8f":"markdown","decea66a":"markdown","68bffd87":"markdown","f77c4f30":"markdown"},"source":{"ac48a655":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline","66f30258":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","593020d9":"train.head()","6e10aa94":"all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","ff6b5b9c":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\nprices.hist()","946c9820":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","a36fdcef":"all_data = pd.get_dummies(all_data)","4d9b8fc0":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","22a42875":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","230ab82c":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","c4c17e14":"model_ridge = Ridge()","23424c16":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","57c197ef":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","3a746052":"cv_ridge.min()","76c45ddd":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)","29802156":"rmse_cv(model_lasso).mean()","bd0ba235":"coef = pd.Series(model_lasso.coef_, index = X_train.columns)","07172f59":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","98dfd036":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])","df010d9e":"matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","e1297257":"#let's look at the residuals as well:\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","cbef3ede":"import xgboost as xgb","9cde500e":"\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","56623234":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","058c1601":"model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(X_train, y)","98577a1c":"xgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))","d9bc2fd7":"predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")","cb34ea64":"preds = 0.7*lasso_preds + 0.3*xgb_preds","964f8d9e":"solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"ridge_sol.csv\", index = False)","d792576d":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.regularizers import l1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","7583e7ae":"X_train = StandardScaler().fit_transform(X_train)","58737994":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)","2e72ab41":"X_tr.shape","f366dffb":"X_tr","48a22734":"model = Sequential()\n#model.add(Dense(256, activation=\"relu\", input_dim = X_train.shape[1]))\nmodel.add(Dense(1, input_dim = X_train.shape[1], W_regularizer=l1(0.001)))\n\nmodel.compile(loss = \"mse\", optimizer = \"adam\")","2c389f45":"model.summary()","84ee03f8":"hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val))","8b87683e":"pd.Series(model.predict(X_val)[:,0]).hist()","7a4d5ed5":"Nice! The lasso performs even better so we'll just use this one to predict on the test set. Another neat thing about the Lasso is that it does feature selection for you - setting coefficients of features it deems unimportant to zero. Let's take a look at the coefficients:","14ba39c5":"# *Regularized Linear Models | House Prices:*  \nDavid Rivas, Ph.D.  \n  \n# Summary\n\nSimple regularized linear regression models are used to solve house prices data. Surprisingly one of them does really well with very little feature engineering. The key point is to log_transform the numeric variables since most of them are skewed. These results are stacked to xgboost and compared with keras models.  \n\nThis work is based on the references and will be further extended in the future. In particular, more tuning will be performed on xgboost and more research will be done to try to improve the performace of the Keras model (by using the secont reference).","8f2eea52":"The most important positive feature is `GrLivArea` - Above grade (ground) living area square feet. This definitely makes sense. Then a few other  location and quality features contributed positively. Some of the negative features make less sense and would be worth looking into more - it seems like they might come from unbalanced categorical variables.\n\n Also note that unlike the feature importance you'd get from a random forest these are _actual_ coefficients in your model - so you can say precisely why the predicted price is what it is. The only issue here is that we log_transformed both the target and the numeric features so the actual magnitudes are a bit hard to interpret. ","4444b44b":"Let's add an xgboost model to our linear model to see if we can improve our score:","2a503628":"For future research: Here a Standard Scalar transformation was made prior to splitting the data below, the reverse order might produce better results as it would prevent leakage.","90062167":"Many times it makes sense to take a weighted average of uncorrelated results - this usually imporoves the score although in this case it doesn't help that much.  \nFor future research: improve the modeling and tuning of xgboost, as xgboost has a lot of potential. ","0e87e21e":"# > xgboost  \n### Adding an xgboost model:","8db6cc5c":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above.","d4874530":"# > Keras\n### Trying out keras?\n\nFeedforward Neural Nets doesn't seem to work well at all...more research is required.","1b79ba58":"# > Ridge  ","9f2e526c":"The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.","ff33174e":"So for the Ridge regression we get a rmsle of about 0.127  \n  \n# > Lasso\n\nLet' try out the Lasso model. We will do a slightly different approach here and use the built in Lasso CV to figure out the best alpha for us. For some reason the alphas in Lasso CV are really the inverse or the alphas in Ridge.","04e283bb":"# Data preprocessing:   \nWe're not going to do anything fancy here: \n \n- First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal    \n- Create Dummy variables for the categorical features    \n- Replace the numeric missing values (NaN's) with the mean of their respective columns","3c6695e0":"For future research: Here not all the numerical features are transformed, perhaps to be consistent we should transform all of them, or perhaps because the features are assumed to be linearly independent it will not matter.","73b65d5c":"Good job Lasso.  One thing to note here however is that the features selected are not necessarily the \"correct\" ones - especially since there are a lot of collinear features in this dataset. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is.","220acaa4":"We can also take a look directly at what the most important coefficients are:","1142ee8f":"The residual plot looks pretty good.To wrap it up let's predict on the test set and submit on the leaderboard:","decea66a":"# Models\n\nNow we are going to use regularized linear regression models from the scikit learn module. I'm going to try both l_1(Lasso) and l_2(Ridge) regularization. I'll also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning par","68bffd87":"# Reading the data","f77c4f30":"# References used  \n[Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) by Alexandru Papiu  \n\n[XGboost + Ridge + Lasso](https:\/\/www.kaggle.com\/zoupet\/xgboost-ridge-lasso) by Julien Heiduk"}}