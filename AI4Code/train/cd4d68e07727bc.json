{"cell_type":{"a4e7830d":"code","e27af399":"code","2a7ab273":"code","2e77395f":"code","8a7abf01":"code","0418e1b4":"code","cd7e51b9":"code","4f04d17b":"code","1474a1c6":"code","c5fae2dc":"code","0e523c2f":"code","d35acee7":"code","8601a55f":"code","38a8767e":"code","7f9508ab":"code","a8017389":"code","a12a1793":"code","66eb4bdd":"code","3930e785":"code","f875bd3e":"code","64c60cca":"code","7bc1e4c8":"code","e18ad1b1":"code","c81060a0":"code","d44789d7":"code","57269d6b":"code","9ce59cb7":"code","04c45554":"code","515020bb":"code","1a835530":"code","4d7077d4":"code","74f0a428":"code","77ffb16f":"code","c48202e5":"code","d9cf0e19":"code","2c531a59":"code","d37d373c":"code","531f07a1":"code","a55ae78d":"code","20f8754f":"code","5c76f32f":"code","d047e364":"code","ab2a44dc":"code","66bc1dd9":"code","d614a8be":"code","f02042d3":"code","914dcf5f":"code","c9a4fc9f":"code","b325f7de":"code","2c2490c3":"code","b5be47ae":"code","a423aef3":"code","1ae337fd":"code","b1623c56":"code","ec08f9ae":"code","a413d72d":"code","29c3a493":"code","56c5b7df":"code","9faa67b6":"code","0709b8cf":"code","cc58093f":"code","63606a0b":"markdown","9ba846f8":"markdown","5cfb760c":"markdown","21889c42":"markdown","bd59af41":"markdown","faa98fe6":"markdown","8076a467":"markdown","c41f7a49":"markdown","856bae8b":"markdown"},"source":{"a4e7830d":"import pandas as pd\nimport re\nimport time\nimport datetime\nimport warnings\nfrom wordcloud import WordCloud, STOPWORDS\nfrom emoji import demojize\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n\nstopwords = set(STOPWORDS)\nwarnings.filterwarnings(\"ignore\", 'This pattern has match groups')","e27af399":"def clean_text(row):\n    words = []\n    for w in row.split():\n        word = re.sub('[^A-Za-z0-9]+', '', str(w))\n        words.append(word.lower())\n    return words\n\ndef get_words(df,text_label='text',target_label='is_offensive',value=0):\n    words = []\n    for row in df[df[target_label]==value].iloc[:][text_label]:\n        words += clean_text(row)\n    return ' '.join(words)    \n\ndef get_wordcloud(df,text_label='text',target_label='is_offensive',value=0):\n    words = get_words(df,text_label=text_label,target_label=target_label,value=value)\n    wc = WordCloud(background_color='white',\n                   max_words=500,\n                   stopwords=stopwords)\n    wc.generate(words)\n    return wc","2a7ab273":"df = pd.read_csv(\"..\/input\/d\/brandonbenton\/botezlive-chat-classification\/botezlive_data.csv\")","2e77395f":"'''\nfilter_words = ['kekw','lul','lol','omegalul','pog',\n                'kappa','sadge','lulw','monkaw','monkas',\n                'd','pogu','pepega','yep','wut','catjam',\n                'jebaited','pogo','yes','no','pepelaugh',\n                'wutface','f','pepejam','lmao','ez',\n                'waytoodank','dansgame','gg','5head',\n                'pepehands','hi','clap','widepeepohappy',\n                'pogchamp','notlikethis','weirdchamp',\n                'pepepls','pogchamp','monkaeyes','pausechamp',\n                'peepohey','poggers','forsencd','wtf','song',\n                'kapp','hello','nice','true','uptime','kkomrade',\n                'chess','omg','subs','rip','xd','what',\n                'l','peped','wow','kkona','4head',\n                'muted','hahaa','biblethump','ayaya',\n                'feelsbadman','hahaha','ttours',\n                'jammies','pogchamps','dream','yo',\n                'time','monkahmm','angelthump','lmfao',\n                'oof','o','haha','pepog','nam','bruh','ok',\n                'hahahaha','me','hey','playlist','bye',\n                'peepoclap','kkatjam','yeah','nope',\n                'discord','modcheck','partyparrot','seemsgood',\n                'heyguys','louder','fake','alienpls3','lmfaooo',\n                'saved','thanks','xqcl','lmaooo','skip']\nfilter_words = ['ppjedi','babyrage','andrealul','raid']\nfor w in filter_words:\n    df = df[df['text'].apply(process_text)!=w]\n'''","8a7abf01":"def process_text(text):\n    return re.sub(r'[^\\w\\s]','',text.lower()).strip()","0418e1b4":"def contains_words(row,words=[]):\n    text = process_text(row).split()\n    return all(x in text for x in words)","cd7e51b9":"single_word_dict = {}\nfor row in df['text'][:]:\n    text = process_text(row)\n    if len(text.split())==1:\n        word = text\n        if word not in single_word_dict:\n            single_word_dict[word] = 1\n        else: \n            single_word_dict[word] += 1","4f04d17b":"words = []\ncounts = []\nfor w in single_word_dict:\n    words.append(w)\n    counts.append(single_word_dict[w])","1474a1c6":"df_words = pd.DataFrame.from_dict({'word':words,'count':counts})","c5fae2dc":"df_words.sort_values(by='count',inplace=True,axis=0,ascending=False)","0e523c2f":"fig = plt.figure(figsize=(20,15))\ndf_words.head(20).plot(kind='bar',x='word',y='count')\nplt.show()","d35acee7":"pd.set_option('display.max_colwidth',None)","8601a55f":"df.loc[df[(df['text'].apply(contains_words,words=['hot','sauce'])) & \n          (df['is_offensive']==1)].index]","38a8767e":"#df.loc[df[(df['text'].apply(contains_words,words=['hot','sauce'])) & \n#          (df['is_offensive']==1)].index[1],['is_offensive']]=0","7f9508ab":"df.loc[df[(df['text'].apply(contains_words,words=['hot','out'])) & \n          (df['is_offensive']==1)].index]","a8017389":"#df.loc[df[(df['text'].apply(contains_words,words=['hot','out'])) & \n#          (df['is_offensive']==1)].index[6],['is_offensive']]=0","a12a1793":"df.loc[df[(df['text'].apply(contains_words,words=['im','hot'])) & \n          (df['is_offensive']==1)].index]","66eb4bdd":"df.loc[df[(df['text'].apply(contains_words,words=['sort','by','hot'])) & \n          (df['is_offensive']==1)].index]","3930e785":"df.loc[df[(df['text'].apply(contains_words,words=['sort','by','hot'])) & \n          (df['is_offensive']==1)].index]","f875bd3e":"df.loc[df[(df['text'].apply(contains_words,words=['hot','dog'])) & \n          (df['is_offensive']==1)].index]","64c60cca":"#df.loc[df[(df['text'].apply(contains_words,words=['hot','dog'])) & \n#          (df['is_offensive']==1)].index[1:3],'is_offensive']=0","7bc1e4c8":"df.loc[df[(df['text'].apply(contains_words,words=['hot','tub'])) & \n          (df['is_offensive']==1)].index]","e18ad1b1":"#df.loc[df[(df['text'].apply(contains_words,words=['hot','tub'])) & \n#          (df['is_offensive']==1)].index[1:5],['is_offensive']]=0","c81060a0":"#df.loc[df[(df['text'].apply(contains_words,words=['hot','mic'])) & \n#          (df['is_offensive']==1)].index,['is_offensive']]=0","d44789d7":"df.loc[df[(df['text'].apply(contains_words,words=['its','so','hot'])) & \n          (df['is_offensive']==1)].index]","57269d6b":"#df.loc[df[(df['text'].apply(contains_words,words=['its','so','hot'])) & \n#          (df['is_offensive']==1)].index[1],['is_offensive']]=0","9ce59cb7":"df.loc[df[(df['text'].apply(process_text)=='andrea') & \n          (df['is_offensive']==1)].index,['is_offensive']]=0","04c45554":"df.loc[df[(df['text'].apply(process_text)=='alex') & \n          (df['is_offensive']==1)].index,['is_offensive']]=0","515020bb":"df.loc[df[(df['text'].apply(contains_words,words=['i','lost','xqcK'])) & \n          (df['is_offensive']==1)].index,['is_offensive']]=1","1a835530":"vectorizer = TfidfVectorizer()\nvectorizer.fit(df['text'])","4d7077d4":"n_samples = 10000\nn_components = 20","74f0a428":"df_ones = df[df['is_offensive']==1].sample(n_samples)\ndf_zeros = df[df['is_offensive']==0].sample(n_samples)\ndf_samp = pd.concat([df_zeros,df_ones])","77ffb16f":"X = vectorizer.transform(df_samp['text'])\ny = df_samp['is_offensive']","c48202e5":"pca = TruncatedSVD(n_components=n_components)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,\n                           columns = ['%s' %(i) for i in range(n_components)])\nprincipalDf['is_offensive'] = y.values\n\npdf_zeros = principalDf[principalDf['is_offensive']==0]\npdf_ones = principalDf[principalDf['is_offensive']==1]","d9cf0e19":"features = [x for x in principalDf.columns]\nfeatures.remove('is_offensive')\nkmeans = KMeans(n_clusters=2,random_state=0)\nkmeans.fit(principalDf[features])\nprincipalDf['kmean cluster'] = kmeans.labels_\ndf_samp['kmean cluster'] = kmeans.labels_","2c531a59":"corrs = principalDf[features].corr()","d37d373c":"sns.heatmap(corrs)\nplt.show()","531f07a1":"corrs_df = pd.DataFrame(columns=['row','col','corr'])\n\nfor i in range(corrs.shape[0]):\n    for j in range(i+1,corrs.shape[1]):\n        corrs_df = corrs_df.append({'row':i,\n                                    'col':j,\n                                    'corr':abs(corrs.iloc[i,j])},\n                                   ignore_index=True)\n\ncorrs_df.sort_values(by='corr',ascending=True,inplace=True)","a55ae78d":"def plot_set(labels):\n    fig = plt.figure(figsize=(20,30))\n    fig_num = 12#min(len(set(agg.labels_)),12)\n    cols = 2\n    rows = fig_num\/\/cols\n    for idx in range(fig_num):\n        row_idx = int(corrs_df.iloc[idx]['row'])\n        col_idx = int(corrs_df.iloc[idx]['col'])\n        dims = [row_idx,col_idx,9]\n        grouped_data = {}\n        ax = fig.add_subplot(rows,cols,idx+1)\n        for x,y,z,l in zip(principalDf[str(dims[0])],principalDf[str(dims[1])],principalDf[str(dims[2])],labels):\n            if l not in grouped_data:\n                grouped_data[l] = {'x':[x],'y':[y],'z':[z]}\n            else:\n                grouped_data[l]['x'].append(x)\n                grouped_data[l]['y'].append(y)\n                grouped_data[l]['z'].append(z)\n\n\n        ax.set_xlabel(str(dims[0]))\n        ax.set_ylabel(str(dims[1]))\n        for l in grouped_data:\n            plt.scatter(grouped_data[l]['x'],\n                        grouped_data[l]['y'],\n                        #c=l,\n                        label=l)\n            plt.scatter(centers[l][dims[0]],\n                        centers[l][dims[1]],\n                        c='black', marker='x'), \n                        #label='1st center',s=50)\n\n#plt.xlim(-0.01,0.03)\n#plt.ylim(-0.01,0.03)\n        plt.title(\"%s,%s\" %(row_idx,col_idx))\n    #ax.legend()\n    plt.show()","20f8754f":"dims = [2,4,9]\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(121, projection = '3d')\n\nax.set_xlabel(str(dims[0]))\nax.set_ylabel(str(dims[1]))\nax.set_zlabel(str(dims[2]))\n\nax.scatter(pdf_zeros[str(dims[0])], pdf_zeros[str(dims[1])], pdf_zeros[str(dims[2])], c='b', label='zero',alpha=0.5)\nax.scatter(pdf_ones[str(dims[0])], pdf_ones[str(dims[1])], pdf_ones[str(dims[2])], c='r', label='one',alpha=0.5)\nax.scatter(kmeans.cluster_centers_[0][dims][0],\n           kmeans.cluster_centers_[0][dims][1],\n           kmeans.cluster_centers_[0][dims][2],\n           c='black', marker='x', label='1st center',s=50)\nax.scatter(kmeans.cluster_centers_[1][dims][0],\n           kmeans.cluster_centers_[1][dims][1],\n           kmeans.cluster_centers_[1][dims][2],\n           c='black', marker='*', label='2nd center',s=50)\n\n#plt.xlim(-0.02,0.1)\n#plt.ylim(-0.01,0.1)\n#plt.zlim(-0.02,0.5)\nax.legend()\n#plt.show()\n\nax = fig.add_subplot(122)\nsns.scatterplot(x=str(dims[0]),y=str(dims[1]), data=pdf_zeros, label='0')\nsns.scatterplot(x=str(dims[0]),y=str(dims[1]), data=pdf_ones, label='1')\nplt.scatter(kmeans.cluster_centers_[0][dims][0],\n            kmeans.cluster_centers_[0][dims][1],\n            c='black', marker='x', label='1st center',s=50)\nplt.scatter(kmeans.cluster_centers_[1][dims][0],\n            kmeans.cluster_centers_[1][dims][1],\n            c='black', marker='*', label='1st center',s=50)\n#plt.xlim(-0.01,0.03)\n#plt.ylim(-0.01,0.03)\nax.legend()\nplt.show()","5c76f32f":"fig = plt.figure(figsize=(20,30))\nfig_num = 12\ncols = 2\nrows = fig_num\/\/cols\nfor idx in range(fig_num):\n    row_idx = int(corrs_df.iloc[idx]['row'])\n    col_idx = int(corrs_df.iloc[idx]['col'])\n    dims = [row_idx,col_idx]    \n    ax = fig.add_subplot(rows,cols,idx+1)\n    sns.scatterplot(x=str(dims[0]),y=str(dims[1]), \n                    #data=principalDf[principalDf['kmean cluster']==0], label='0')\n                    data=pdf_zeros, label='0')\n    sns.scatterplot(x=str(dims[0]),y=str(dims[1]), \n                    #data=principalDf[principalDf['kmean cluster']==1], label='1')\n                    data=pdf_ones, label='1')\n    plt.scatter(kmeans.cluster_centers_[0][dims][0],\n                kmeans.cluster_centers_[0][dims][1],\n                c='black', marker='x', label='1st center',s=50)\n    plt.scatter(kmeans.cluster_centers_[1][dims][0],\n                kmeans.cluster_centers_[1][dims][1],\n                c='black', marker='*', label='2nd center',s=50)\n    #plt.xlim(-0.01,0.03)\n    #plt.ylim(-0.01,0.03)\n    plt.title(\"%s,%s\" %(row_idx,col_idx))\n    ax.legend()\nplt.show()","d047e364":"centers = {}\nfor l in set([0,1]):\n    centers[l] = principalDf.loc[principalDf['kmean cluster']==l,features].mean().to_list()","ab2a44dc":"plot_set(kmeans.labels_)","66bc1dd9":"dbscan = DBSCAN(eps=0.25).fit(principalDf[features])\nprincipalDf['dbscan cluster'] = dbscan.labels_\ndf_samp['dbscan cluster'] = dbscan.labels_\nprint(\"dbscan label count: %s\" %(len(set(dbscan.labels_))))","d614a8be":"centers = {}\nfor l in set(dbscan.labels_):\n    centers[l] = principalDf.loc[principalDf['dbscan cluster']==l,features].mean().to_list()","f02042d3":"dims = [2,4,9]\nfig = plt.figure(figsize=(15,10))\nax = fig.add_subplot(221, projection = '3d')\n\nax.set_xlabel(str(dims[0]))\nax.set_ylabel(str(dims[1]))\nax.set_zlabel(str(dims[2]))\n\nax.scatter(pdf_zeros[str(dims[0])], pdf_zeros[str(dims[1])], pdf_zeros[str(dims[2])], c='b', label='0',alpha=0.5)\nax.scatter(pdf_ones[str(dims[0])], pdf_ones[str(dims[1])], pdf_ones[str(dims[2])], c='orange', label='1',alpha=0.5)\nax.legend()\n\nax = fig.add_subplot(222)\nsns.scatterplot(x=str(dims[0]),y=str(dims[1]),data=pdf_zeros,label='0')\nsns.scatterplot(x=str(dims[0]),y=str(dims[1]),data=pdf_ones,label='1')\nplt.legend()\n\ngrouped_data = {}\n\nax = fig.add_subplot(223, projection = '3d')\n\nax.set_xlabel(str(dims[0]))\nax.set_ylabel(str(dims[1]))\nax.set_zlabel(str(dims[2]))\n\nfor x,y,z,l in zip(principalDf[str(dims[0])],principalDf[str(dims[1])],principalDf[str(dims[2])],dbscan.labels_):\n    if l not in grouped_data:\n        grouped_data[l] = {'x':[x],'y':[y],'z':[z]}\n    else:\n        grouped_data[l]['x'].append(x)\n        grouped_data[l]['y'].append(y)\n        grouped_data[l]['z'].append(z)\n\nfor l in grouped_data:\n    ax.scatter(grouped_data[l]['x'],\n               grouped_data[l]['y'],\n               grouped_data[l]['z'],\n               #c=l,\n               label=l)\n\n    ax.scatter(centers[l][dims[0]],\n               centers[l][dims[1]],\n               centers[l][dims[2]],\n               c='black', marker='*')#, label='2nd center',s=50)\n\n#plt.xlim(-0.02,0.1)\n#plt.ylim(-0.01,0.1)\n#plt.zlim(-0.02,0.5)\n#ax.legend()\n#plt.show()\n\nax = fig.add_subplot(224)\nax.set_xlabel(str(dims[0]))\nax.set_ylabel(str(dims[1]))\nfor l in grouped_data:\n    plt.scatter(grouped_data[l]['x'],\n                grouped_data[l]['y'],\n                #c=l,\n                label=l)\n    plt.scatter(centers[l][dims[0]],\n                centers[l][dims[1]],\n                c='black', marker='x'), \n                #label='1st center',s=50)\n\n#plt.xlim(-0.01,0.03)\n#plt.ylim(-0.01,0.03)\n#ax.legend()\nplt.show()","914dcf5f":"plot_set(dbscan.labels_)","c9a4fc9f":"fig = plt.figure(figsize=(15,10))\n\nfig.add_subplot(1,2,1)\nwc = get_wordcloud(df_samp,text_label='text',target_label='is_offensive',value=0)\n   #fig = plt.figure(figsize=(10,5))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('%s = %s' %('cluster',0))\n    #plt.show()\n    #return fig\n\nfig.add_subplot(1,2,2)\nwc = get_wordcloud(df_samp,text_label='text',target_label='is_offensive',value=1)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('%s = %s' %('cluster',1))\nplt.show()","b325f7de":"fig = plt.figure(figsize=(15,10))\n\nfig.add_subplot(1,2,1)\nwc = get_wordcloud(df_samp,text_label='text',target_label='kmean cluster',value=0)\n   #fig = plt.figure(figsize=(10,5))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('%s = %s' %('cluster',0))\n    #plt.show()\n    #return fig\n\nfig.add_subplot(1,2,2)\nwc = get_wordcloud(df_samp,text_label='text',target_label='kmean cluster',value=1)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.title('%s = %s' %('cluster',1))\nplt.show()","2c2490c3":"correct_vals = df_samp[df_samp['kmean cluster']==df_samp['is_offensive']].shape[0]\nfalse_vals = df_samp[df_samp['kmean cluster']!=df_samp['is_offensive']].shape[0]\naccuracy = correct_vals\/(correct_vals+false_vals)\naccuracy","b5be47ae":"labels = list(set(dbscan.labels_))\nsubplot_number = len(labels)","a423aef3":"label_accuracy_df = pd.DataFrame(columns=['label','score'])\nmatches = df_samp[(df_samp['dbscan cluster']==0) & (df_samp['is_offensive']==0)].shape[0]\ntotal = df_samp[df_samp['is_offensive']==0].shape[0]\nlabel_accuracy_df = label_accuracy_df.append({'label':0,'score':matches\/total},ignore_index=True)","1ae337fd":"for l in labels[1:]:\n    matches = df_samp[(df_samp['dbscan cluster']==l) & (df_samp['is_offensive']==1)].shape[0]\n    total = df_samp[df_samp['dbscan cluster']==l].shape[0]\n    label_accuracy_df = label_accuracy_df.append({'label':l,'score':matches\/total},ignore_index=True)","b1623c56":"label_accuracy_df.sort_values(by='score',ascending=False,inplace=True)","ec08f9ae":"fig = plt.figure(figsize=(15,10))\nlabels = list(set(dbscan.labels_))\nprint(\"dbscan label count: %s\" %len(labels))\nsubplot_number = min(20,len(labels))\ncols = 2\nrows = subplot_number\/\/cols\n\nfor i in range(rows*cols):\n    fig.add_subplot(rows,cols,i+1)\n    try:\n        wc = get_wordcloud(df_samp,text_label='text',target_label='dbscan cluster',value=labels[i])\n   #fig = plt.figure(figsize=(10,5))\n        plt.imshow(wc, interpolation='bilinear')\n        plt.axis('off')\n        plt.title('%s = %s' %('cluster',labels[i]))\n    except:\n        pass\n    \nplt.show()","a413d72d":"df_samp[df_samp['dbscan cluster']==0]","29c3a493":"agg = AgglomerativeClustering()\nagg.fit(principalDf[features])\nprincipalDf['agg cluster'] = agg.labels_\ndf_samp['agg cluster'] = agg.labels_\nprint(\"agg label count: %s\" %(len(set(agg.labels_))))","56c5b7df":"centers = {}\nfor l in set(agg.labels_):\n    centers[l] = principalDf.loc[principalDf['agg cluster']==l,features].mean().to_list()","9faa67b6":"plot_set(agg.labels_)","0709b8cf":"fig = plt.figure(figsize=(15,10))\nlabels = list(set(agg.labels_))\nsubplot_number = min(20,len(labels))\ncols = 2\nrows = subplot_number\/\/cols\n\nfor i in range(rows*cols):\n    fig.add_subplot(rows,cols,i+1)\n    #try:\n    wc = get_wordcloud(df_samp,text_label='text',target_label='agg cluster',value=labels[i])\n   #fig = plt.figure(figsize=(10,5))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('%s = %s' %('cluster',labels[i]))\n    #except:\n     #   pass\n    \nplt.show()","cc58093f":"correct_vals = df_samp[df_samp['agg cluster']==df_samp['is_offensive']].shape[0]\nfalse_vals = df_samp[df_samp['agg cluster']!=df_samp['is_offensive']].shape[0]\naccuracy = correct_vals\/(correct_vals+false_vals)\naccuracy","63606a0b":"**Correct some misclassifications**","9ba846f8":"**Agglomerative clustering**","5cfb760c":"**DBSCAN plots**","21889c42":"**Filter some single word lines**","bd59af41":"**Word clouds for kmean clustering with 2 clusters**","faa98fe6":"**KMeans plots**","8076a467":"**Look at clustering approach**","c41f7a49":"**Word clouds for original classification**","856bae8b":"**Word clouds for dbscan clustering**"}}