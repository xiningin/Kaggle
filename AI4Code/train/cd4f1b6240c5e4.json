{"cell_type":{"e3c91c0a":"code","e8ce5185":"code","0221e6fa":"code","5f2dda65":"code","d250bbaf":"code","7fe42be7":"code","5598186c":"code","ef1a7990":"code","7904c728":"code","6cb5ebea":"code","5c2b515c":"code","15a1cb68":"code","05e87068":"code","33c2ac9e":"code","fa380c0a":"code","50ad9d29":"code","29829a20":"code","faacf16e":"code","d598b834":"code","4e8e7d9f":"code","66b903df":"code","5ee7e33f":"code","255fbede":"code","78931a94":"code","152e570c":"code","85c1d649":"code","46b4a307":"code","8b62d511":"code","474b912d":"code","9c8f657c":"code","e4320102":"code","de804c82":"code","30e17eee":"code","815fe000":"markdown","5b644ec6":"markdown","bb54d904":"markdown","191c049e":"markdown","e5b2d2f8":"markdown","8e979f1b":"markdown","1cd7dd73":"markdown","cb3ed380":"markdown","e0d361f6":"markdown","43b38780":"markdown","0229473c":"markdown","b9b3cb2e":"markdown","20c1b7ee":"markdown"},"source":{"e3c91c0a":"import torch","e8ce5185":"def activation(x, func=None):\n  \"\"\"\n  Arguments\n  ---------\n  x: torch.tensor\n  \"\"\"\n  if func == 'softmax':\n\n    #dim =1 takes sum over the rows.\n    return torch.exp(x)\/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n    \n  return 1\/(1+torch.exp(-x))","0221e6fa":"#Generate data for example.\ntorch.manual_seed(7)\n\n#features\nfeatures = torch.randn((1, 5))\n\n#weights\nweights = torch.randn_like(features)\n#bias\nbias = torch.randn((1, 1))","5f2dda65":"print(f\"F:{features},\\nW:{weights},\\nb:{bias}\")","d250bbaf":"#Using matrix multuplication change the shape of second tensor to match with number of columns of features.\n#Functions that can be used torch.reshape(), torch.resize(), torch.view()\n\nactivation((torch.matmul(features, weights.T))+bias)","7fe42be7":"weights.shape","5598186c":"activation(torch.sum(features*weights)+bias)","ef1a7990":"#Torch has a function torch.from_numpy() to convert numpy array to tensors.\nimport numpy as np\n\nnp.random.seed(13)\ntorch.from_numpy(np.random.randint(10,size=(5, 2)))","7904c728":"from torchvision import datasets, transforms\n\n#Normalizing the data using transform\n\ntransform= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n\n#Download and load the MNIST dataset\n\ntrainset = datasets.MNIST('MNIST_data\/',download=True, train=True, transform = transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)","6cb5ebea":"import warnings\nwarnings.filterwarnings('ignore')\niter_data = iter(trainloader)\nimages, labels = next(iter_data)\nprint(images.shape)\nprint(labels.shape)","5c2b515c":"import matplotlib.pyplot as plt\n%matplotlib inline","15a1cb68":"plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')","05e87068":"#Input images of size 64*784 where each row is of size 28*28 = 784\ntorch.manual_seed(13)\nimg_input = torch.flatten(images, start_dim=1)\n\n#Assiging weights to the first layer which will have 684 weights with respect to each hidden units.\nW_1 = torch.randn((784, 256))\nbias = torch.randn((256))\n\n#Assiging weights to the output layer\n\nW_2 = torch.randn((256, 10))\nbias1 = torch.randn((10))\n\n#Using sigmoid as activation and output function\nout = activation(torch.matmul(activation(torch.matmul(img_input, W_1)+bias), W_2)+bias1)\n\n#Using softmax for activation and output function\nout1 = activation(torch.matmul(activation(torch.matmul(img_input, W_1)+bias, func='softmax'), W_2)+bias1, func='softmax')\n\n#Using sigmoid for activation and softmax for final ouput.\nout2 = activation(torch.matmul(activation(torch.matmul(img_input, W_1)+bias), W_2)+bias1, func='softmax')","33c2ac9e":"torch.sum(torch.matmul(img_input, W_1)+bias, dim=0).shape","fa380c0a":"torch.flatten(images, start_dim=1).shape","50ad9d29":"import seaborn as sns","29829a20":"fig, ax = plt.subplots(nrows=1, ncols=4,figsize=(16, 8))\n\n\n#First input image of the 64 images.\nax[0].imshow(img_input[1].numpy().reshape((28, 28)).squeeze(),cmap='Reds')\n\n#Proba class of network without training\nsns.barplot(y=list(range(10)), x=out[:1,:].numpy().squeeze(), ax=ax[1], orient='h')\nax[1].set_title('Class Probability --- Logistic')\nax[1].set_xlabel('Probabiity')\nax[1].set_ylabel('Class')\n\n#Proba class using softmax.\nsns.barplot(y=list(range(10)), x=out1[:1, :].numpy().squeeze(), ax=ax[2], orient='h')\nax[2].set_title('Class Probability --- Softmax')\nax[2].set_xlabel('Probabiity')\nax[2].set_ylabel('Class')\n\n\n#Proba class using softmax.\nsns.barplot(y=list(range(10)), x=out2[:1, :].numpy().squeeze(), ax=ax[3], orient='h')\nax[3].set_title('Class Probability --- Logistic-Softmax')\nax[3].set_xlabel('Probabiity')\nax[3].set_ylabel('Class')","faacf16e":"out2.sum(dim=1)","d598b834":"from torch import nn\n\n#Build a forward network\n#Note that the input is a batch of 64 flattened images, that is matrix of size (64, 784).\n#Each column of the matrix corresponds to weights associated to the neuron.\n#(64, 784)*(784, 128)*(128, 64)*(64, 10)=(64, 10)\n#The result is probability for each image in the batch.\n\nmodel = nn.Sequential(nn.Linear(784, 128),nn.ReLU(),nn.Linear(128, 64),nn.ReLU(),nn.Linear(64, 10), nn.LogSoftmax(dim=1))\n\n#Loss function\ncriterion = nn.NLLLoss()\n\n#data\nimages, labels = next(iter(trainloader))\n\n#Flatten images\nimages = images.view(images.shape[0], -1)\n\nlogits = model(images)\n\n#loss calculation\nloss =  criterion(logits, labels)\n\nprint(loss)","4e8e7d9f":"logits.shape","66b903df":"#Autograd\n#To keep  a track of operations that created the tensor we have to set requires_grad = True.\nx = torch.randn(2, 2, requires_grad=True)\nx","5ee7e33f":"y= x**2\ny","255fbede":"#The operation that created y can be seen as follows\ny.grad_fn","78931a94":"z = torch.exp(y)","152e570c":"z.grad_fn","85c1d649":"z1=z.mean()\nz1","46b4a307":"#Currently no value is given to grad attribute. As we haven't called the backward method on the variable\nprint(x.grad)","8b62d511":"z1.backward(retain_graph=True)\nprint(x.grad)","474b912d":"x = torch.tensor([1., 4.], requires_grad=True)\ny = x**2","9c8f657c":"y.backward()","e4320102":"y.backward(gradient=torch.tensor([1., 4.]), retain_graph=True)","de804c82":"print(x.grad)","30e17eee":"t=torch.tensor([1.,2.], requires_grad=True)\n\n#function (x1, x2) --> (x1^3, x2^3)\nz=t**3\n\n#gradient in the direction of (1, 1)\nz.backward(gradient=torch.tensor([1., 1.]), retain_graph=True)\n\n#check whether it is correct it must give (3, 12)\nprint(t.grad==torch.tensor([3., 12.]))","815fe000":"The above shows that it is exp function.","5b644ec6":"Plotting the probabilities of the above network which is not trained.","bb54d904":"# Tensors","191c049e":"\nBelow explains the view(it only changes the shape for veiwing) but in memory the shape is retained. A value of negative 1 means the shape if inferred from the tensor.\n```\nIn [21]: a\nOut[21]:\ntensor([[ 1,  0,  0],\n        [ 1,  1,  1],\n        [-1,  1,  1],\n        [ 2,  3,  4]])\n\nIn [22]: a.sum(dim=1).view(-1, 1)\nOut[22]:\ntensor([[1],\n        [3],\n        [1],\n        [9]])\n\nIn [23]: a\/a.sum(dim=1).view(-1, 1)\nOut[23]:\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3333,  0.3333,  0.3333],\n        [-1.0000,  1.0000,  1.0000],\n        [ 0.2222,  0.3333,  0.4444]])\n\nIn [24]:\n```\n\n","e5b2d2f8":"In the following we see that` <PowBackward0 at 0x7f8dadefa7d0>` is the function as we raised 2 to original tensor.\n\n","8e979f1b":"The above error was thrown as y is not a scalar. So in this case we need to give the direction for the gradient evaluation. Suppose we want to evaluate it in the direction of $(1, 4)$. So that would be $Jv^t$ where $J$ is the *jacobian*. Hence, we have the following\n\n\\begin{equation}\nJ=\n\\begin{pmatrix}\n2 & 0\\\\\n0 & 8\\\\\n\\end{pmatrix}\n%\n\\begin{pmatrix}\n1\\\\\n4\n\\end{pmatrix}\n%\n=\n\\begin{pmatrix}\n2\\\\\n32\n\\end{pmatrix}\n\\end{equation}","1cd7dd73":"NOTE: If any value is changed in-place for one object then the other value changes as well. Both have a shared memory.","cb3ed380":"## Example 3","e0d361f6":"The above shows that it is powerbackward function.","43b38780":"#Neural Networks\n\nWe will use the MNIST dataset of handwritten examples of digits in postcards which is pre-processed and formatted. This dataset can be loaded with `torchvision`in pytorch.","0229473c":"# Autograd\n\nAutograd is reverse automatic differentiation system and a good introduction is provided on the pytorch [page](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html).\n\nWe require error associated with output of each neuron and to rectify it we require certain partial derivatives and autograd simplies this task by keeping track of the operations associated with scalar. In simple terms autograd stores the jacobian, it is simply the derivative(linear transformation) of multivariable function and give any vector $v$ (the direction of derivative) it gives out $Jv$.\n\n\n\n\n## Mechanics\n\nNote that partial derivatives can be represented by a tree\n\n![chain_rule.png](data:image\/png;base64,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)\n\nSo if we want to calculate $\\partial z\/\\partial s$ we get this by adding up the values in the above tree. So it would be\n\\begin{equation}\n\\frac{\\partial z}{\\partial s}= \\frac{\\partial z}{\\partial x} \\frac{\\partial x}{\\partial s} +\n\\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial s}\n\\end{equation}\nIn a similar manner autograd calculates the partial derivatives for the requires function.\nFrom the documentaion:\n\n*Autograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe.*\n\nIn python we don't need to worry because GIL.\n\n## Example\n\nIn the following example we use the flag `requires_grad=True` to keep track of the operations on the tensor","b9b3cb2e":"Tensor of size (64, 1, 28, 28) that it is a batch of 64 images with 1 channel of 28 * 28 pixel. Note that an (d1, d2, ..., dn)-dimensional array\/tensor is represented as follows.\n\nEach $d_i$ represents the length of the list we will have \"list of $d_1$ lists each containing $d_2$ lists with $d_3$ lists and so on...\"\n\n","20c1b7ee":"## Example 2"}}