{"cell_type":{"0ee03b88":"code","fcd4483c":"code","b72c2a85":"code","3391836c":"code","c5d598a1":"code","386efd78":"code","5491ece1":"code","0b8b2e00":"code","55df7bda":"code","a7316f9b":"code","fedf00b4":"code","349b3116":"code","4e058c02":"code","f01e3e17":"code","a0da34ce":"code","101772b6":"code","1388ebf3":"code","dab3aed4":"code","b486aabf":"code","da6e7de8":"code","e3f5c656":"code","e3979b45":"code","5a30eb21":"code","ef9d9b6f":"code","de7fd623":"code","372ff609":"code","1dfe5b35":"code","c23c6f79":"code","f0836e7d":"code","bbb4e954":"code","1f580c12":"code","c9a7c0d6":"code","7b2b5768":"code","f08e3c8e":"code","77a8036e":"code","e1857c49":"code","76c44075":"code","1dc5baae":"code","edcaf28b":"code","8f197458":"code","746eebc2":"code","60296c49":"code","1bc35b88":"code","64c3b84e":"markdown","3e363dc7":"markdown","c6d656fe":"markdown","e169fe61":"markdown","8dd09cf3":"markdown"},"source":{"0ee03b88":"import os\nimport glob\nimport gc\nimport time\nimport warnings\nimport math\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nfrom math import floor\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm","fcd4483c":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import (\n    Input, layers, activations, initializers, optimizers, models, regularizers, callbacks\n)\nimport lightgbm as lgb","b72c2a85":"def reduce_memory_usage(df):\n    integers = ['int16', 'int32', 'int64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in integers:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n                \n        elif col_type == 'float64':\n            df[col] = df[col].astype(np.float32)\n            \n    \n    return df","3391836c":"def convert_long_to_wide_format(df):\n    \"\"\"Convert book\/trade DataFrame from long format (panel data) to wide format\"\"\"\n    df = df.pivot(index='time_id', columns='seconds_in_bucket')\n    multi_col = pd.MultiIndex.from_product([df.columns.levels[0], \n                                            df.columns.levels[1].astype('str')])\n    df.columns = multi_col.map('_'.join)\n    df.reset_index(inplace=True)\n    \n    return df","c5d598a1":"ROOT_PATH = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\ntrain_target = pd.read_csv(ROOT_PATH + 'train.csv')\nMIN_TARGET = train_target['target'].min()","386efd78":"def add_book_features(df):\n    df['wap1'] = ((df['bid_price1'] * df['ask_size1'] + \n                   df['ask_price1'] * df['bid_size1']) \/ \n                  (df['bid_size1'] + df['ask_size1']))\n    df['wap2'] = ((df['bid_price2'] * df['ask_size2'] + \n                   df['ask_price2'] * df['bid_size2']) \/ \n                  (df['bid_size2'] + df['ask_size2']))\n\n    df['log_ret1'] = df.groupby('time_id')['wap1'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['log_ret2'] = df.groupby('time_id')['wap2'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n\n    df['bid_ask_spread'] = df['ask_price1'] \/ df['bid_price1'] - 1\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] \/ df['bid_price2'] - 1\n    df['ask_spread'] = df['ask_price2'] \/ df['ask_price1'] - 1\n    df['wap_spread'] = df['wap2'] \/ df['wap1'] - 1\n    df['bid_ask_depth_ratio'] = (df['bid_size1'] + df['bid_size2']) \/ (df['ask_size1'] + df['ask_size2'])\n    df['bid_ask_depth_ratio1'] = df['bid_size1'] \/ df['ask_size1']\n    df['bid_ask_depth_ratio2'] = df['bid_size2'] \/ df['ask_size2']\n    df['total_depth'] = df['bid_size1'] + df['bid_size2'] + df['ask_size1'] + df['ask_size2']\n    \n    return df\n\ndef add_agg_book_features(df):\n    df_agg = pd.DataFrame()\n    df_agg['sigma1'] = df.groupby('time_id')['log_ret1'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['sigma2'] = df.groupby('time_id')['log_ret2'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['book_update_times'] = df.groupby('time_id')['seconds_in_bucket'].count()\n\n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n               'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']:\n        df_agg[col + '_update_times'] = df.groupby('time_id')[col].unique().apply(lambda x: x.shape[0])\n    \n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n                'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2',\n                'wap1', 'wap2', 'bid_ask_spread', 'price_spread', \n                'bid_spread', 'ask_spread', 'wap_spread', 'bid_ask_depth_ratio',\n                'bid_ask_depth_ratio1', 'bid_ask_depth_ratio2', 'total_depth']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    \n    for col in ['log_ret1', 'log_ret2']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    return df_agg\n\ndef fetch_book_one_stock(stock_id, partition='train', seconds_lapse=10):\n    \"\"\"Fetch book data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}book_{partition}.parquet\/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df = add_book_features(df)\n    df_agg = add_agg_book_features(df)\n    \n    # add missing seconds to book data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600, seconds_lapse)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    \n    # forward fill book on missing seconds (missing = order book unchanged)\n    df = df.reindex(multi_index, method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    df.reset_index(inplace=True)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    assert not df.isna().any().any()\n\n    return df","5491ece1":"def add_trade_features(df):\n    df['real_log_ret'] = df.groupby('time_id')['price'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['volume'] = df['price'] * df['size']\n    df['volume_per_order'] = (df['volume'] \/ df['order_count']).fillna(0)\n    df['volume_last_seconds'] = df['volume'] * df['seconds_in_bucket'] \/ 600\n    df['whale_last_seconds'] = df['volume_per_order'] * df['seconds_in_bucket'] \/ 600\n\n    return df\n\ndef add_agg_trade_features(df, df_agg):\n    df_agg['real_sigma'] = df.groupby('time_id')['real_log_ret'].apply(lambda x: np.sqrt(np.sum(x**2)))\n\n    for col in ['price', 'size', 'order_count', 'volume', \n                'volume_per_order', 'volume_last_seconds', 'whale_last_seconds']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    \n    return df_agg\n    \ndef fetch_trade_one_stock(stock_id, partition='train', seconds_lapse=15):\n    \"\"\"Fetch trade data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}trade_{partition}.parquet\/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df_agg = pd.DataFrame()\n    df_agg['trade_times'] = df.groupby('time_id')['seconds_in_bucket'].unique().apply(lambda x: x.shape[0])\n\n    # add ALL missing seconds (0, 1, 2,..., 599, 600) to trade data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    df = df.reindex(multi_index)\n    df.reset_index(inplace=True)\n    # forward fill price in missing seconds (missing = no trade happens)\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    # fill size and order_count with 0\n    df['size'] = df['size'].fillna(value=0).astype(int)\n    df['order_count'] = df['order_count'].fillna(value=0).astype(int)\n    \n    # build another bucket of seconds (1 bucket = seconds_lapse)\n    # e.g. seconds_lapse = 5; then seconds 0, 1, ..., 4 fall into bucket 5; seconds 5 - 9 fall into bucket 10; etc.\n    df['seconds_in_bucket'] = df['seconds_in_bucket'].apply(lambda x: (floor(x \/ seconds_lapse) + 1) * seconds_lapse)\n    # now compute the average price traded in each bucket of seconds again \n    # & total number shares traded + order_count\n    df['price'] = df['price'] * df['size']\n    df = df.groupby(['time_id', 'seconds_in_bucket']).sum()\n    df['price'] = df['price'] \/ df['size']\n    # fill missing price due to no trades happened in an entire bucket\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')\n    df.reset_index(inplace=True)\n    \n    df = add_trade_features(df)\n    df_agg = add_agg_trade_features(df, df_agg)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    \n    assert not df.isna().any().any()\n\n    return df","0b8b2e00":"def fetch_data_one_stock(stock_id, partition='train', book_seconds_lapse=10, trade_seconds_lapse=15):\n    pd.options.mode.chained_assignment = None\n    book_df = fetch_book_one_stock(stock_id, partition, book_seconds_lapse)\n    trade_df = fetch_trade_one_stock(stock_id, partition, trade_seconds_lapse)\n    # merge order books and trade histories into 1 DataFrame\n    features_df = book_df.merge(trade_df, on=['stock_id', 'time_id'], how='left')    \n    del book_df, trade_df\n    gc.collect()\n    features_df.fillna(0, inplace=True)\n\n    if partition == 'train':\n        target_df = train_target[train_target['stock_id'] == stock_id]\n        target_df['target'] = target_df['target'].astype('float32')\n        full_df = target_df.merge(features_df, on=['stock_id', 'time_id'], how='inner')\n        del features_df, target_df\n        gc.collect()\n        return reduce_memory_usage(full_df)\n    \n    elif partition == 'test':\n        cols = list(features_df.columns)\n        cols.insert(0, cols.pop(cols.index('stock_id')))\n        features_df = features_df.loc[:, cols]\n        return reduce_memory_usage(features_df)","55df7bda":"class ProgressParallel(Parallel):\n    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):\n        self._use_tqdm = use_tqdm\n        self._total = total\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:\n            return Parallel.__call__(self, *args, **kwargs)\n\n    def print_progress(self):\n        if self._total is None:\n            self._pbar.total = self.n_dispatched_tasks\n        self._pbar.n = self.n_completed_tasks\n        self._pbar.refresh()\n\n        \ndef fetch_data_all_stocks(stock_ids, partition=\"train\", book_seconds_lapse=10, trade_seconds_lapse=15):\n    df = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n        delayed(fetch_data_one_stock)(\n            stock_id, partition, book_seconds_lapse, trade_seconds_lapse\n        ) for stock_id in stock_ids\n    )\n    df = pd.concat(df, ignore_index=True)\n    gc.collect()\n    \n    assert not df.isna().any().any()\n    return reduce_memory_usage(df)","a7316f9b":"def predict_one_stock(stock_id, model, scaler, model_type):\n    X_num_test_df = fetch_data_one_stock(stock_id, partition='test')\n    submission = pd.DataFrame()\n    submission['row_id'] = X_num_test_df['stock_id'].astype(str) + '-' + X_num_test_df['time_id'].astype(str)\n    \n    X_cat_test = X_num_test_df[CAT_FEATURES].to_numpy()\n    X_num_test_df.drop((CAT_FEATURES + ['time_id']), axis=1, inplace=True)\n    X_num_test = X_num_test_df.to_numpy(dtype='float32')\n    del X_num_test_df\n    gc.collect()\n    scaler.transform(X_num_test)\n    \n    if model_type == 'lgbm':\n        X_test = np.concatenate((X_cat_test, X_num_test), axis=1)\n        del X_num_test, X_cat_test\n        gc.collect()\n        Y_test = np.clip(model.predict(X_test), MIN_TARGET\/2, None)\n        del X_test\n        gc.collect()\n        \n    elif model_type == 'nn':\n        Y_test = np.clip(model.predict((X_cat_test, X_num_test)), MIN_TARGET\/2, None)\n        del X_cat_test, X_num_test\n        gc.collect()\n    \n    submission['target_' + model_type] = Y_test\n    del Y_test\n    gc.collect()\n\n    return submission\n    \ndef predict_multiple_stocks(stock_ids, model, scaler, model_type, parallel=False):\n    if not parallel:\n        submission = [predict_one_stock(stock_id, model, scaler, model_type) for stock_id in stock_ids]\n    \n    else:\n        submission = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n            delayed(predict_one_stock)(stock_id, model, scaler, model_type) for stock_id in stock_ids\n        )\n    submission = pd.concat(submission, ignore_index=True)\n    gc.collect()\n    return submission","fedf00b4":"%%time\nX_num = pd.read_parquet('\/kaggle\/input\/prep-opt-v10\/prep_opt_v10\/full_dataset.parquet')  # book=10, trade=15, 1704 features\nX_num.info(memory_usage='deep')","349b3116":"FEATURE_NAMES = [X_num.columns[0]] + list(X_num.columns[3:])\nFEATURE_NAMES[:10]","4e058c02":"X_num.shape","f01e3e17":"CAT_FEATURES = ['stock_id']\nX_cat = X_num[CAT_FEATURES].to_numpy()\nY = X_num['target'].to_numpy('float32')\nX_num.drop((CAT_FEATURES + ['time_id', 'target']), axis=1, inplace=True)\ngc.collect()\nprint('X_cat shape: ', X_cat.shape)\nprint('X_num shape:', X_num.shape)\nprint('Y shape:', Y.shape)","a0da34ce":"RANDOM_STATE = 86\n\ndef reset_seed():\n    np.random.seed(RANDOM_STATE)\n    tf.random.set_seed(RANDOM_STATE)\n    \nreset_seed()","101772b6":"X_cat_train, X_cat_dev, X_num_train_df, X_num_dev_df, Y_train, Y_dev = train_test_split(\n    X_cat, X_num, Y, test_size=10000, random_state=RANDOM_STATE\n)\ndel X_num, Y\ngc.collect()\n\nX_num_train = X_num_train_df.to_numpy(dtype='float32')\ndel X_num_train_df\ngc.collect()\n\nX_num_dev = X_num_dev_df.to_numpy(dtype='float32')\ndel X_num_dev_df\ngc.collect()\n\nprint('X_cat_train shape: ', X_cat_train.shape)\nprint('X_num_train shape:', X_num_train.shape)\nprint('X_cat_dev shape:', X_cat_dev.shape)\nprint('X_num_dev shape:', X_num_dev.shape)","1388ebf3":"scaler = MinMaxScaler(copy=False)\n\nscaler.fit_transform(X_num_train)\nscaler.transform(X_num_dev);","dab3aed4":"X_train = np.concatenate((X_cat_train, X_num_train), axis=1)\ndel X_num_train, X_cat_train\ngc.collect()\nX_dev = np.concatenate((X_cat_dev, X_num_dev), axis=1)\ndel X_num_dev, X_cat_dev\ngc.collect()\n\nprint('X_train shape: ', X_train.shape)\nprint('X_dev shape:', X_dev.shape)","b486aabf":"d_train = lgb.Dataset(X_train, label=Y_train, feature_name=FEATURE_NAMES, \n                      categorical_feature=CAT_FEATURES, free_raw_data=True)\ndel X_train, Y_train\ngc.collect()\nd_dev = lgb.Dataset(X_dev, label=Y_dev, feature_name=FEATURE_NAMES, \n                    categorical_feature=CAT_FEATURES, free_raw_data=True)\ndel X_dev, Y_dev\ngc.collect()","da6e7de8":"def rmspe_np(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n\ndef fobj_rmpse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    grad = -2 * (y_true - y_pred) \/ (y_true ** 2)\n    hess = 2 \/ (y_true ** 2)\n    return grad, hess\n \n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE: ', round(rmspe_np(y_true, y_pred), 4), False","e3f5c656":"eval_result = {}\nrecord_eval_cb = lgb.record_evaluation(eval_result)\n\nparams = {\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'num_leaves': 40,\n    'max_bin': 255,\n    'min_data_in_leaf': 750,\n    'learning_rate' : 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'seed': RANDOM_STATE,\n    'n_jobs': -1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id': -1,\n    'gpu_device_id': -1,\n    'gpu_use_dp': False,\n}\n\nmodel = lgb.train(params, d_train, num_boost_round=1000, \n                  valid_sets=[d_train, d_dev], valid_names=['Train', 'Dev'], \n                  early_stopping_rounds=50, fobj=fobj_rmpse, feval=feval_rmspe, \n                  verbose_eval = 10, categorical_feature=CAT_FEATURES,\n                  callbacks=[record_eval_cb])","e3979b45":"lgb.plot_importance(model, max_num_features=20);","5a30eb21":"val_loss_lgb = np.array(eval_result[\"Dev\"][\"RMSPE: \"])\nmin_val_loss_lgb = np.min(val_loss_lgb)\nbest_epoch_lgb = np.argmin(val_loss_lgb) + 1\nprint(\"LGBM's minimum val_loss: \", min_val_loss_lgb, \", achieved at Round\", best_epoch_lgb)","ef9d9b6f":"list_order_book_test = glob.glob(ROOT_PATH + 'book_test.parquet\/*')\ntest_ids = [int(path.split('=')[1]) for path in list_order_book_test]\n\nsubmission = predict_multiple_stocks(test_ids, model=model, scaler=scaler, model_type='lgbm', parallel=True)\nsubmission.head()","de7fd623":"del model, eval_result, record_eval_cb, d_train, d_dev, scaler\ngc.collect()","372ff609":"def add_book_features(df):\n    df['wap1'] = ((df['bid_price1'] * df['ask_size1'] + \n                   df['ask_price1'] * df['bid_size1']) \/ \n                  (df['bid_size1'] + df['ask_size1']))\n    df['wap2'] = ((df['bid_price2'] * df['ask_size2'] + \n                   df['ask_price2'] * df['bid_size2']) \/ \n                  (df['bid_size2'] + df['ask_size2']))\n\n    df['log_ret1'] = df.groupby('time_id')['wap1'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['log_ret2'] = df.groupby('time_id')['wap2'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n\n    df['bid_ask_spread'] = df['ask_price1'] \/ df['bid_price1'] - 1\n    df['bid_spread'] = df['bid_price1'] \/ df['bid_price2'] - 1\n    df['ask_spread'] = df['ask_price2'] \/ df['ask_price1'] - 1\n    df['wap_spread'] = df['wap2'] \/ df['wap1'] - 1\n    df['bid_ask_depth_ratio'] = (df['bid_size1'] + df['bid_size2']) \/ (df['ask_size1'] + df['ask_size2'])\n    df['bid_ask_depth_ratio1'] = df['bid_size1'] \/ df['ask_size1']\n    df['bid_ask_depth_ratio2'] = df['bid_size2'] \/ df['ask_size2']\n    df['total_depth'] = df['bid_size1'] + df['bid_size2'] + df['ask_size1'] + df['ask_size2']\n    \n    return df\n\ndef add_agg_book_features(df):\n    df_agg = pd.DataFrame()\n    df_agg['sigma1'] = df.groupby('time_id')['log_ret1'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['sigma2'] = df.groupby('time_id')['log_ret2'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['book_update_times'] = df.groupby('time_id')['seconds_in_bucket'].count()\n\n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n               'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']:\n        df_agg[col + '_update_times'] = df.groupby('time_id')[col].unique().apply(lambda x: x.shape[0])\n    \n    return df_agg\n\ndef fetch_book_one_stock(stock_id, partition='train', seconds_lapse=5):\n    \"\"\"Fetch book data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}book_{partition}.parquet\/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df = add_book_features(df)\n    df_agg = add_agg_book_features(df)\n    \n    # add missing seconds to book data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600, seconds_lapse)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    \n    # forward fill book on missing seconds (missing = order book unchanged)\n    df = df.reindex(multi_index, method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    df.reset_index(inplace=True)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    del df_agg\n    gc.collect()\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    assert not df.isna().any().any()\n\n    return df","1dfe5b35":"def add_trade_features(df):\n    df['real_log_ret'] = df.groupby('time_id')['price'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['volume'] = df['price'] * df['size']\n    df['volume_per_order'] = (df['volume'] \/ df['order_count']).fillna(0)\n    df['volume_last_seconds'] = df['volume'] * df['seconds_in_bucket'] \/ 600\n    df['whale_last_seconds'] = df['volume_per_order'] * df['seconds_in_bucket'] \/ 600\n\n    return df\n\ndef add_agg_trade_features(df, df_agg):\n    df_agg['real_sigma'] = df.groupby('time_id')['real_log_ret'].apply(lambda x: np.sqrt(np.sum(x**2)))\n\n    for col in ['price', 'size', 'order_count', 'volume', 'volume_per_order', 'volume_last_seconds', 'whale_last_seconds']:\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n    \n    return df_agg\n    \ndef fetch_trade_one_stock(stock_id, partition='train', seconds_lapse=5):\n    \"\"\"Fetch trade data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}trade_{partition}.parquet\/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df_agg = pd.DataFrame()\n    df_agg['trade_times'] = df.groupby('time_id')['seconds_in_bucket'].unique().apply(lambda x: x.shape[0])\n\n    # add ALL missing seconds (0, 1, 2,..., 599, 600) to trade data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    df = df.reindex(multi_index)\n    df.reset_index(inplace=True)\n    # forward fill price in missing seconds (missing = no trade happens)\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    # fill size and order_count with 0\n    df['size'] = df['size'].fillna(value=0).astype(int)\n    df['order_count'] = df['order_count'].fillna(value=0).astype(int)\n    \n    # build another bucket of seconds (1 bucket = seconds_lapse)\n    # e.g. seconds_lapse = 5; then seconds 0, 1, ..., 4 fall into bucket 5; seconds 5 - 9 fall into bucket 10; etc.\n    df['seconds_in_bucket'] = df['seconds_in_bucket'].apply(lambda x: (floor(x \/ seconds_lapse) + 1) * seconds_lapse)\n    # now compute the average price traded in each bucket of seconds again \n    # & total number shares traded + order_count\n    df['price'] = df['price'] * df['size']\n    df = df.groupby(['time_id', 'seconds_in_bucket']).sum()\n    df['price'] = df['price'] \/ df['size']\n    # fill missing price due to no trades happened in an entire bucket\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')\n    df.reset_index(inplace=True)\n    \n    df = add_trade_features(df)\n    df_agg = add_agg_trade_features(df, df_agg)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    del df_agg\n    gc.collect()\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    \n    assert not df.isna().any().any()\n\n    return df","c23c6f79":"def fetch_data_one_stock(stock_id, partition='train', book_seconds_lapse=10, trade_seconds_lapse=15):\n    pd.options.mode.chained_assignment = None\n    book_df = fetch_book_one_stock(stock_id, partition, book_seconds_lapse)\n    trade_df = fetch_trade_one_stock(stock_id, partition, trade_seconds_lapse)\n    # merge order books and trade histories into 1 DataFrame\n    features_df = book_df.merge(trade_df, on=['stock_id', 'time_id'], how='left')    \n    del book_df, trade_df\n    gc.collect()\n    features_df.fillna(0, inplace=True)\n\n    if partition == 'train':\n        target_df = train_target[train_target['stock_id'] == stock_id]\n        target_df['target'] = target_df['target'].astype('float32')\n        full_df = target_df.merge(features_df, on=['stock_id', 'time_id'], how='inner')\n        del features_df, target_df\n        gc.collect()\n        return reduce_memory_usage(full_df)\n    \n    elif partition == 'test':\n        cols = list(features_df.columns)\n        cols.insert(0, cols.pop(cols.index('stock_id')))\n        features_df = features_df.loc[:, cols]\n        return reduce_memory_usage(features_df)","f0836e7d":"class ProgressParallel(Parallel):\n    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):\n        self._use_tqdm = use_tqdm\n        self._total = total\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:\n            return Parallel.__call__(self, *args, **kwargs)\n\n    def print_progress(self):\n        if self._total is None:\n            self._pbar.total = self.n_dispatched_tasks\n        self._pbar.n = self.n_completed_tasks\n        self._pbar.refresh()\n\n        \ndef fetch_data_all_stocks(stock_ids, partition=\"train\", book_seconds_lapse=10, trade_seconds_lapse=15):\n    df = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n        delayed(fetch_data_one_stock)(\n            stock_id, partition, book_seconds_lapse, trade_seconds_lapse\n        ) for stock_id in stock_ids\n    )\n    df = pd.concat(df, ignore_index=True)\n    gc.collect()\n    \n    assert not df.isna().any().any()\n    return reduce_memory_usage(df)","bbb4e954":"def predict_one_stock(stock_id, model, scaler, model_type):\n    X_num_test_df = fetch_data_one_stock(stock_id, partition='test')\n    submission = pd.DataFrame()\n    submission['row_id'] = X_num_test_df['stock_id'].astype(str) + '-' + X_num_test_df['time_id'].astype(str)\n    \n    X_cat_test = X_num_test_df[CAT_FEATURES].to_numpy()\n    X_num_test_df.drop((CAT_FEATURES + ['time_id']), axis=1, inplace=True)\n    X_num_test = X_num_test_df.to_numpy(dtype='float32')\n    del X_num_test_df\n    gc.collect()\n    scaler.transform(X_num_test)\n    \n    if model_type == 'lgbm':\n        X_test = np.concatenate((X_cat_test, X_num_test), axis=1)\n        del X_num_test, X_cat_test\n        gc.collect()\n        Y_test = np.clip(model.predict(X_test), MIN_TARGET\/2, None)\n        del X_test\n        gc.collect()\n        \n    elif model_type == 'nn':\n        Y_test = np.clip(model.predict((X_cat_test, X_num_test)), MIN_TARGET\/2, None)\n        del X_cat_test, X_num_test\n        gc.collect()\n    \n    submission['target_' + model_type] = Y_test\n    del Y_test\n    gc.collect()\n\n    return submission\n    \ndef predict_multiple_stocks(stock_ids, model, scaler, model_type, parallel=False):\n    if not parallel:\n        submission = [predict_one_stock(stock_id, model, scaler, model_type) for stock_id in stock_ids]\n    \n    else:\n        submission = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n            delayed(predict_one_stock)(stock_id, model, scaler, model_type) for stock_id in stock_ids\n        )\n    submission = pd.concat(submission, ignore_index=True)\n    gc.collect()\n    return submission","1f580c12":"%%time\nX_num = pd.read_parquet('\/kaggle\/input\/prep-opt-v5\/prep_opt\/full_dataset.parquet')  # book=10, trade=15, 1541 features\nX_num.info(memory_usage='deep')","c9a7c0d6":"X_cat = X_num[CAT_FEATURES].to_numpy()\nMAX_STOCK_ID = max(X_cat)[0] + 1\nY = X_num['target'].to_numpy('float32')\nX_num.drop((CAT_FEATURES + ['time_id', 'target']), axis=1, inplace=True)\ngc.collect()\nprint('X_cat shape: ', X_cat.shape)\nprint('X_num shape:', X_num.shape)\nprint('Y shape:', Y.shape)","7b2b5768":"reset_seed()","f08e3c8e":"X_cat_train, X_cat_dev, X_num_train_df, X_num_dev_df, Y_train, Y_dev = train_test_split(\n    X_cat, X_num, Y, test_size=10000, random_state=RANDOM_STATE\n)\ndel X_num, Y\ngc.collect()\n\nX_num_train = X_num_train_df.to_numpy(dtype='float32')\ndel X_num_train_df\ngc.collect()\n\nX_num_dev = X_num_dev_df.to_numpy(dtype='float32')\ndel X_num_dev_df\ngc.collect()\n\nprint('X_cat_train shape: ', X_cat_train.shape)\nprint('X_num_train shape:', X_num_train.shape)\nprint('X_cat_dev shape:', X_cat_dev.shape)\nprint('X_num_dev shape:', X_num_dev.shape)","77a8036e":"scaler = MinMaxScaler(copy=False)\n\nscaler.fit_transform(X_num_train)\nscaler.transform(X_num_dev);","e1857c49":"keras.backend.clear_session()\nreset_seed()\n\ndef rmspe(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square((y_true - y_pred)\/y_true)))\n\ndef optiver_fnn(tpu=True):\n    # params\n    num_units = 64\n    num_layers = 8\n    embedding_size = 96\n    drop_out = 0\n    activation = activations.swish\n    activation_out = activations.swish\n    kernel_initializer = initializers.LecunUniform(seed=RANDOM_STATE)\n    batch_norm = False\n    optimizer = optimizers.Adam\n    learning_rate = 0.0005\n    regularizer = None\n    \n    # build models\n    hidden_units = [num_units] * num_layers\n    quarter_num_layers = int(num_layers \/ 4)\n    drop_out_rates = ([drop_out] * quarter_num_layers + \n                      [drop_out \/ 2] * quarter_num_layers + \n                      [drop_out \/ 4] * quarter_num_layers + \n                      [0] * quarter_num_layers)\n\n    if not len(hidden_units) == len(drop_out_rates):\n        raise Exception(\"Length of hidden units must be equal to length of drop-out rates.\")\n\n    inp_cat = Input(shape=(1,), name='stock_id')\n    inp_num = Input(shape=X_num_train.shape[1], name='num_data')\n    \n    stock_embedded = layers.Embedding(MAX_STOCK_ID, embedding_size, \n                                      input_length=1, name='stock_id_embedding')(inp_cat)\n    stock_flattened = layers.Flatten()(stock_embedded)\n    x = layers.Concatenate()([stock_flattened, inp_num])\n\n    if batch_norm:\n        \n        x = layers.Dense(hidden_units[0], kernel_initializer=kernel_initializer, \n                         kernel_regularizer=regularizer)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(drop_out_rates[0], seed=RANDOM_STATE)(x)\n        \n        for i in range(1, len(hidden_units)):\n            x = layers.Dense(hidden_units[i], kernel_initializer=kernel_initializer, \n                             kernel_regularizer=regularizer)(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.Activation(activation)(x)\n            x = layers.Dropout(drop_out_rates[i], seed=RANDOM_STATE)(x)\n            \n    else:\n        \n        x = layers.Dense(hidden_units[0], kernel_initializer=kernel_initializer, \n                         kernel_regularizer=regularizer)(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(drop_out_rates[0], seed=RANDOM_STATE)(x)\n\n        for i in range(1, len(hidden_units)):\n            x = layers.Dense(hidden_units[i], kernel_initializer=kernel_initializer, \n                             kernel_regularizer=regularizer)(x)\n            x = layers.Activation(activation)(x)\n            x = layers.Dropout(drop_out_rates[i], seed=RANDOM_STATE)(x)    \n    \n    x = layers.Dense(1)(x)\n    out = layers.Activation(activation_out)(x)\n\n    model = models.Model(inputs=[inp_cat, inp_num], outputs=out)\n    \n    def lr_normalizer(lr, optimizer):\n        if optimizer == optimizers.Adam:\n            return lr\n        elif optimizer == optimizers.Nadam:\n            return lr * 2\n        else:\n            raise Exception(str(optimizer) + 'is neither Adam or Nadam.')\n            \n    model.compile(loss=rmspe, optimizer = optimizer(lr_normalizer(learning_rate, optimizer)))        \n    return model\n\n\nmodel = optiver_fnn()\nearlystop_cb = callbacks.EarlyStopping(patience=20, restore_best_weights=True)\nhistory = model.fit((X_cat_train, X_num_train), Y_train, epochs=400, \n                    validation_data=((X_cat_dev, X_num_dev), Y_dev), batch_size=2048, callbacks=[earlystop_cb]);","76c44075":"val_loss_nn = history.history[\"val_loss\"]\nmin_val_loss_nn = np.min(val_loss_nn)\nbest_epoch_nn = np.argmin(val_loss_nn) + 1\nprint(\"NN's minimum val_loss: \", min_val_loss_nn, \", achieved at Epoch\", best_epoch_nn)","1dc5baae":"del X_cat_train, X_num_train, Y_train, X_cat_dev, X_num_dev, Y_dev, earlystop_cb, history\ngc.collect()","edcaf28b":"submission_nn = predict_multiple_stocks(test_ids, model=model, scaler=scaler, model_type='nn')\nsubmission_nn.head()","8f197458":"del model, scaler\ngc.collect()","746eebc2":"submission = submission.merge(submission_nn, how='left', on='row_id')\ndel submission_nn\ngc.collect()","60296c49":"base_rmspe = 0.21\nerror_ratio = (min_val_loss_nn - base_rmspe) \/ (min_val_loss_lgb - base_rmspe)\nlgb_weight = error_ratio \/ (error_ratio + 1)\nnn_weight = 1 - lgb_weight\n\nprint('LGBM weight: ', lgb_weight)\nprint('NN weight: ', nn_weight)\nsubmission['target'] = (submission['target_nn'] * nn_weight + \n                        submission['target_lgbm'] * lgb_weight)\nsubmission.drop(['target_nn', 'target_lgbm'], axis=1, inplace=True)","1bc35b88":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","64c3b84e":"# NN","3e363dc7":"# Light GBM","c6d656fe":"# Util functions & Global variables","e169fe61":"# Ensemble","8dd09cf3":"# Changelog\n* v.6\n    * No features engineering. `book_seconds_lapse=5` & `trade_seconds_lapse=5` (data: `prep-opt-v6`)\n    * Simple NN\n* v.7\n    * Add new features: `stock_id`, multiple `book` and `trade` features. `book_seconds_lapse=10` & `trade_seconds_lapse=15` (data: `prep-opt-v5`)\n    * Add parallel processing\n    * Debugged prediction\n* v.8\n    * v.6 but NN w\/ embedding layers (data: `prep-opt-v6`)\n* v.9\n    * v.7 but `book_seconds_lapse=5` (try to fit in the memory) (data: `prep-opt-v7`)\n* v.10\n    * v.7 but MinMaxScaler instead of StandardScaler (data: prep-opt-v5)\n* v.11\n    * v.9 but `trade_seconds_lapse=20` (data: `prep-opt-v8`), since v.9 can't fit in memory when training\n* v.12\n    * v.10 but \n    * Start trying out GB models: XGBoost, LGBM, CatBoost\n* v.13\n    * v.10 but\n    * Emsemble LGBM + NN\n    * Added progress bar for parallel processing\n* v.14\n    * v.13 (MinMaxScaler, `book_seconds=10`, `trade_seconds=15`)\n    * Checked the situation of stock 31\n    * Added a bunch of features: `is_stock_31`, `trading_halted`, multiple book and trade features (data: `prep-opt-v9`)\n* v.15\n    * v.14, removed `is_stock_31` & `trading_halted`, useless features (data: `prep-opt-v10`)\n    * NN `prep-opt-v5` + LGBM `prep-opt-v10`\n    * Predict in batches\n    \n# (Relative) comparision of datasets:\n* Memory: `prep-opt-v7` > `prep-opt-v8` > `prep-opt-v9` > `prep-opt-v10` > `prep-opt-v5` > `prep-opt-v6`\n* Feature creativeness: `prep-opt-v9` = `prep-opt-v10` > `prep-opt-v7` > `prep-opt-v8` >  `prep-opt-v5` > `prep-opt-v6`"}}