{"cell_type":{"66274315":"code","903fb423":"code","8f59f804":"code","fefc6895":"code","e543cd0a":"code","7b5d81c4":"code","724c443b":"code","aea3edb3":"code","1dc32b91":"code","a075b007":"code","b52b0166":"code","e3bae3a1":"code","154741b4":"code","3d3683ce":"code","bb177d20":"code","498c0be1":"code","d7bc358b":"code","05fdcb2a":"markdown","6a02d509":"markdown","18b53208":"markdown","b7322ea1":"markdown","43951e50":"markdown","8982e7fc":"markdown","0238cbd1":"markdown","1be65dfc":"markdown","e841ff99":"markdown","b8d16c04":"markdown","ce5615c8":"markdown","d65e23b4":"markdown"},"source":{"66274315":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n\n%matplotlib inline","903fb423":"    \ndef load_dataset():\n    train_dataset = h5py.File('..\/input\/catnotcat\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('..\/input\/catnotcat\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","8f59f804":"# Loading the data (cat\/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()","fefc6895":"print(classes[:])","e543cd0a":"# Example of a picture\nindex = 13\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[train_set_y[0, index]].decode(\"utf-8\") +  \"' picture.\")","7b5d81c4":"\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n\n\nprint (f\"Number of training examples: m_train = {m_train}\")\nprint (f\"Number of testing examples: m_test = {m_test}\")\nprint (f\"Height\/Width of each image: num_px = {num_px}\")\nprint (f\"Each image is of size:  ({num_px} ,{num_px}, 3)\")\nprint (f\"train_set_x shape: {train_set_x_orig.shape}\")\nprint (f\"train_set_y shape: {train_set_y.shape}\")\nprint (f\"test_set_x shape: {test_set_x_orig.shape}\")\nprint (f\"test_set_y shape: {test_set_y.shape}\")","724c443b":"# Array.T gives the transpose \ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n\n\nprint (f\"train_set_x_flatten shape: {train_set_x_flatten.shape}\")\nprint (f\"train_set_y shape: {train_set_y.shape}\")\nprint (f\"test_set_x_flatten shape: {test_set_x_flatten.shape}\")\nprint (f\"test_set_y shape: {test_set_y.shape}\")","aea3edb3":"train_set_x = train_set_x_flatten\/255.\ntest_set_x = test_set_x_flatten\/255.","1dc32b91":"def sigmoid(z):\n    s = 1.\/(1 + np.exp(-z))\n    return s","a075b007":"def initialize_with_zeros(dim):\n    W = np.zeros((dim, 1))\n    B = 0\n    # assert tests if a condition is true \n    assert(W.shape == (dim, 1)) \n    assert(isinstance(B, float) or isinstance(B, int))\n    \n    return W, B","b52b0166":"dim = 5\nW, B = initialize_with_zeros(dim)\nprint (f\"W = {W}\")\nprint (f\"B = {B}\")","e3bae3a1":"def propagate(W, b, X, Y):\n \n    m = X.shape[1]\n \n    A = sigmoid(np.dot(W.T, X) + b)                                    # activation computation\n    cost = (-1.\/ m)*np.sum(Y*np.log(A) + (1 - Y)*np.log(1 - A))        # cost computation\n   \n    dW = (1.\/ m)*np.dot(X, (A - Y).T)\n    db = (1.\/ m)*np.sum(A - Y)\n  \n\n    assert(dW.shape == W.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dW\": dW,\n             \"db\": db}\n    \n    return grads, cost","154741b4":"\ndef optimize(W, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    costs = []\n    \n    for i in range(num_iterations):\n        grads, cost = propagate(W, b, X, Y)\n        dW = grads[\"dW\"]\n        db = grads[\"db\"]\n        W = W - learning_rate*dW\n        b = b - learning_rate*db\n            \n        costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (f\"Cost after iteration {i}: {cost}\")\n    \n    params = {\"W\": W,\n              \"b\": b}\n    \n    grads = {\"dW\": dW,\n             \"db\": db}\n    \n    return params, grads, costs","3d3683ce":"def predict(W, b, X):\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    W = W.reshape(X.shape[0], 1)\n    \n    \n    A = sigmoid(np.dot(W.T, X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[0,i] > 0.5:\n            Y_prediction[0, i] = 1\n        else:\n            Y_prediction[0, i] = 0\n    \n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","bb177d20":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n   \n    W, b = initialize_with_zeros(X_train.shape[0])\n\n    \n    parameters, grads, costs = optimize(W, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n    \n    # Retrieve parameters W and b from dictionary \"parameters\"\n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n    \n    # Predict test\/train set examples\n    Y_prediction_test = predict(W, b, X_test)\n    Y_prediction_train = predict(W, b, X_train)\n\n    # Print train\/test Errors\n    \"\"\" \n        So how will we calculate accuracy? Lets see the difference if real values and predicted values are same : (1-1) = 0, (0-0) = 0. So if they are\n        different ie (1-0) = 1 or (0-1) = -1. So if we take the absolute difference the absolute difference of the corresponding real and wrong prediction\n        will be 1. As the value is 1, therefore if we take the sum of all the elements ie sum of the count of all wrong predictions as corresponding correct \n        value will be 0 which will not add any value. So, the mean will give loss fraction. Multiply it by 100 and we have loss percentage. Subtracting it from\n        100 will give the corresponding accuracy\n    \"\"\"\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\n            \"costs\": costs,\n             \"Y_prediction_test\": Y_prediction_test, \n             \"Y_prediction_train\" : Y_prediction_train, \n             \"W\" : W, \n             \"b\" : b,\n             \"learning_rate\" : learning_rate,\n             \"num_iterations\": num_iterations\n    }\n    \n    return d","498c0be1":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)","d7bc358b":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","05fdcb2a":"### X will be the input matrix, which will have individual flatten x (flatten individual input) as columns.","6a02d509":"### Learning (IMHO it's calculating) the W & B such that the cost function J is minimum. For a parameter X, the update rule is X - lr * X, where lr is the learning rate","18b53208":"# A simple neural network using linear regression to differentiate a cat image and a non cat image\n### Importing useful packages by thier standard aliases","b7322ea1":"### To flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape(a, b * c * d) is to use\n#### X_flatten = X.reshape(X.shape[0], -1)  (-1 tells the pythone to calculate the remaining dimensions on its own)","43951e50":"### Function to load dataset","8982e7fc":"### Some useful inbuilt numpy functions: (u,v are arrays)\n* u = np.exp(v)\n* u = np.log(v)\n* u = np.ab(v)         (absolute value)\n* u = np.maximum(v,0)","0238cbd1":"### Creating model which will assemble all the required functions","1be65dfc":"### We are using matrices because numpy has many functions which are vectorized, therefore they are more compute efficient than using loops like for","e841ff99":"### Though sigmoid is the worst activation function, I will soon implement tanh and RELU.\n#### References : http:\/\/www.wildml.com\/2015\/09\/implementing-a-neural-network-from-scratch\/\n#### Upvote it if you learnt something from it \ud83d\ude0a","b8d16c04":"### Function for initializing parameters","ce5615c8":"### Getting familiar with datset","d65e23b4":"### Using Broadcasting to divide every row of the dataset by 255. Neural Networks work better if values are in-between 0-1. The reason is in statistics which idk."}}