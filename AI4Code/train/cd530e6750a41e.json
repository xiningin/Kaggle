{"cell_type":{"cb78e556":"code","753683cc":"code","3a172f00":"code","34d1a77e":"code","16cbaf3a":"code","413e2607":"code","9c4cf8cf":"code","fdb8753b":"code","8bb175f8":"code","4b6293e3":"code","dcdbb22c":"code","6efd83be":"code","f850d909":"code","05eafcd1":"code","f517df2b":"code","66e40fca":"code","666f37f3":"code","9c19e38c":"code","5beb2a27":"code","5cca7bda":"code","a9cb029d":"code","d051acd9":"code","26f6ea9d":"code","d13c633d":"markdown","f919483e":"markdown","ef555750":"markdown","8cb97d92":"markdown","9505388d":"markdown","5deeaca4":"markdown","23fc7184":"markdown","41ae28ba":"markdown","b97f6c54":"markdown","7b7a8f9c":"markdown","93f75ce7":"markdown","59ad181b":"markdown","57b406cd":"markdown","4e4c69e0":"markdown","5da4eadb":"markdown","41b1187f":"markdown","3ac4a33f":"markdown","1c0c177b":"markdown","af8bd9d8":"markdown","af6e5794":"markdown","ad8ecf5d":"markdown","6e672413":"markdown","82f782ba":"markdown","feee0b25":"markdown","5e1beac8":"markdown","db4caed9":"markdown","02267af6":"markdown","de9645e0":"markdown","96cc2bf1":"markdown","bfbd005d":"markdown"},"source":{"cb78e556":"#importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\n\n#removing warnings\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n    \n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report \n\n# to depict tree_prediction\n! pip install pydotplus\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# to remove warnings\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","753683cc":"#reading data\ntry:\n    df = pd.read_csv('income_evaluation.csv')\nexcept:\n    df = pd.read_csv('\/kaggle\/input\/income-classification\/income_evaluation.csv')\n    \ndf.head()","3a172f00":"def data_descr(data, data_name=''):\n    print(f'The dataset is: {data_name}', end='\\n\\n')\n    display(data.info())\n    \n    print(f'Statistical information', end='\\n\\n')\n    display(data.describe(include='all'))\n    \n    duplicates = data.duplicated().sum()\n    if duplicates > 0:\n        print('The number of duplicates is', data.duplicated().sum(), '.')\n    else:\n        print('There are no duplicates in the data')\n        \n    print('The number of missing values per column in % ')\n    report = data.isna().sum() \/ data.shape[0] * 100\n    report = report.to_frame()\n    report.rename(columns={0:'missing values in % of total'}, inplace =True)\n    display(report)\n\ndata_descr(df, data_name='Income groups')","34d1a77e":"plt.figure(figsize=(15,6))\nplt.title(f'The boxcharts for the age and hours-per-week columns')\nsns.boxplot( data=df[['age', ' hours-per-week']], orient='h')\nplt.show()","16cbaf3a":"plt.figure(figsize=(15,6))\nplt.title(f'The histogram for the capital-gain and capital-loss columns')\nsns.histplot( data=df[[' capital-gain',' capital-loss']])\nplt.ylabel('The number of entries')\nplt.xlabel('Capital in $')\nplt.show()","413e2607":"#studying duplicates\ndisplay(df.loc[df.duplicated()].head())\n\n#it seems like there is no relationship among values, that's why we will remove duplicates\ndf.drop_duplicates(inplace=True)\nprint('The number of duplicates in the data is ', df.duplicated().sum())","9c4cf8cf":"# rename columns name\ndf.columns = df.columns.str.replace(' ', '')\ndf.columns = df.columns.str.replace('-', '_')\ndf.columns","fdb8753b":"# adding new the categorical column\nbins = [16, 24, 64, 90]\nlabels = [\"young\",\"adult\",\"old\"]\ndf['age_types'] = pd.cut(df['age'], bins=bins, labels=labels)\ndf['income_num'] = np.where(df['income'] == ' >50K', 1, 0).astype('int16')\n\n# making two lists of columns name\nnumeric_columns = ['age', 'capital_gain','capital_loss', 'hours_per_week', 'income_num']\ncategorical = ['workclass', 'education','age_types',  'education_num', 'marital_status','occupation','relationship','race','sex', 'native_country','income']","8bb175f8":"#changing categorical dtypes\nfor i in categorical:\n    df[i] = df[i].astype(\"category\")\n    \ndisplay(df.info())","4b6293e3":"plt.figure(figsize=(15,6))\nplt.title(f'The boxplot for the capital_gain and capital_loss columns')\nsns.boxplot( data=df[['capital_gain','capital_loss']], orient='h')\nplt.ylabel('The number of entries')\nplt.xlabel('Capital in $')\nplt.show()","dcdbb22c":"# Who has more than 90000 capital-gain?\nQ3 = df['capital_gain'].quantile(0.75)\ncapital_g_high = df.query('capital_gain > @Q3')\ncapital_g_max = df.query('capital_gain > 99000')\n\nQ3 = df['capital_loss'].quantile(0.9)\ncapital_l_high = df.query('capital_loss > @Q3')\ncapital_l_max = df.query('capital_loss >= 4356')\n\ndisplay(capital_g_max.describe(include='all'))","6efd83be":"# How do these columns impact on the target column?\n\ndata_list = {'Original DataFrame':df, \n             'Capital loss max (4356)' : capital_l_max,\n             'High capital loss':capital_l_high,\n             'High capital gain':capital_g_high,\n             'Capital gain max (99 999)': capital_g_max}\n\n\nfig, axs = plt.subplots(1, 5, sharey=True, figsize=(16,4))\nfig.suptitle('Relation between the target and capital gain and loss')\n\nfor i, data in enumerate(data_list):\n    data_list[data].groupby('income')['income'].count().plot.pie(autopct=\"%.1f%%\", ax=axs[i])\n    axs[i].set_title(data)","f850d909":"# histograms\nparam_graphs = df.hist(numeric_columns, figsize=(16, 10), bins=20,)\nfor axis in param_graphs.flatten():\n    axis.set_ylabel('frequency')\nplt.show()","05eafcd1":"sns.pairplot(data=df, hue=\"income\")\nplt.title('Distributions for each variable')\nplt.show()","f517df2b":"fig, axs = plt.subplots(3, 2, figsize=(20,20))\naxs = axs.flatten()\nfig.suptitle('Relation between the categorical features and income')\n\ncategorical2 = ['workclass', 'marital_status','occupation','relationship','race','sex']\nfor ax, i in enumerate(categorical2):\n    plt.legend( bbox_to_anchor=(1.1, 1.1), loc='upper left')\n    sns.countplot(x='income', alpha=0.7, hue=i, data=df, ax=axs[ax])","66e40fca":"# check the mean and meadian ages for  marital_status, income groups\ndisplay(df.groupby('marital_status').agg({'age' : ['mean', 'median']}))\ndisplay(df.groupby('income').agg({'age' : ['mean', 'median']}))","666f37f3":"# calculating the correlation matrix\ncorr = df.corr()\nmatrix = np.triu(corr)\nsns.heatmap(corr, vmax=1.0, vmin=-1.0, \n            fmt='.1g', annot=True, mask = matrix)\n\nplt.title('Correlation matrix')\nplt.show()","9c19e38c":"#dividing data\nY = df['income'].copy()\nX = df.drop(categorical, axis=1).copy()\n\n# defining feature selection\nrf_selector = SelectFromModel(RandomForestClassifier(n_estimators=80), max_features=5)\nrf_selector.fit(X, Y)\n\nrf_support = rf_selector.get_support()\nrf_features = X.loc[:,rf_support].columns.tolist()\nprint(str(len(rf_features)), 'selected feature:', *rf_features)","5beb2a27":"# scalling features\nnumeric_columns = ['age', 'capital_gain','capital_loss', 'fnlwgt', 'hours_per_week']\ndf_scalled = df[numeric_columns].copy()\n# fit scaler on data\nscale = StandardScaler().fit(df_scalled)\n# transform data\ndf_scalled = scale.transform(df_scalled)\ndf_scalled = pd.DataFrame(df_scalled, columns=df[numeric_columns].columns)\ndf_scalled = df_scalled.merge(df[categorical], on=df.index)\ndf_scalled.drop('key_0', axis = 1, inplace=True)\ndf_scalled","5cca7bda":"X, Y = df_scalled.drop('income', axis=1),  df_scalled['income'].copy()\ncategorical = ['workclass', 'education','age_types',  'education_num', 'marital_status','occupation','relationship','race','sex', 'native_country']\nle = LabelEncoder()\n\nle.fit(Y)\nY, Y_names = le.transform(Y), le.classes_\nprint(f'Income classes are {Y_names}')\nfor i in X[categorical]:\n    try:\n        le.fit(X[i])\n        X[i] = le.transform(X[i])\n    except:\n        pass\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, Y, test_size=0.3, random_state=0)","a9cb029d":"# Decision tree\ntree = DecisionTreeClassifier(criterion='gini', \n                              max_depth=4, \n                              random_state=1)\ntree.fit(X_train, y_train)\n\ny_pred_tree = tree.predict(X_test)\nprint(classification_report(y_test, y_pred_tree))\n\n\ndot_data = export_graphviz(tree,\n                           filled=True, \n                           rounded=True,\n                           class_names=Y_names,\n                           feature_names=list(X.columns),\n                           out_file=None) \ngraph = graph_from_dot_data(dot_data)\ngraph.write_png('tree.png')\nImage(filename = 'tree.png')","d051acd9":"# SVM","26f6ea9d":"#RF","d13c633d":"<a id=\"section-three\"><\/a>\n# Step 2: EDA ","f919483e":"### Conclusion:\n\nNow we obtain the following impormation about our dataset:\n\nGeneral:\n- There are 32561 entries and 15 columns;\n- It seems like there are no errors in the data (for example, too high ot too low age values);\n- There are no missing values in the data;\n- There are 25 duplicates, which were removed from the data;\n- The 'capital-gain','capital-loss' columns has some outliers, which has to be examined properly at the EDA stage;\n- One new column (`age-types`) was added to the data; \n- We have ten categorical including the target column (income) and six quantitative columns. \n\nThe main:\n\n- The min age is 17, the max age is 90;\n- The top worklass is 'Private';\n- the most frequent education type is \"Prof-specialty\";\n- White race and male sex are top in the corresponding columns;\n- The mean value and 50% of hours-per-week are 40.44 and 40.00$ (it is like normal distribution);\n- The most common type of income is less than or equals to 50k.\n","ef555750":"<a id=\"section-one\"><\/a>\n# Introduction\n### The dataset contains information about people and their income.\n\n### The data columns:\n\n* age: continuous.\n* workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n* fnlwgt: continuous.\n* education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n* education-num: continuous.\n* marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n* occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n* relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n* race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n* sex: Female, Male.\n* capital-gain: continuous.\n* capital-loss: continuous.\n* hours-per-week: continuous.\n* native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n\n\n\n### Our goal \n**Our goal is to classify people into different income groups (more or less 50k\/per year) depending on features above.**\nThis analysis is divided into the following steps:\n\n1. Studying and preparing data;\n2. Exploratory Data Analysis;\n3. Classification;\n4. The overall conclusion.\n\n___________\nP.S:\n\nHello everyone! I'm beginner in the Kaggle and try to become more skilled data scientist.\n\nI hope, you will find my work  informative. Please, upwote the notebook if it could be usefull for you, thanks!\n\nAnd now let's go \u27a1","8cb97d92":"There are mainly 20 - 50 y.o. according to the `age` column hist, it is left skewed;\n\nLooks like `capital_gain` is distributed as  `capital_loss` with bigger outliers (see above);\n\nThe majority af `hours per week` no more than 50 h\/w, and the most value is about 40 (8 hours per day).","9505388d":"<a id=\"sub-22\"><\/a>\n### Preproccessing stage\n- There are now missing values, but we have to check duplicates and remove them;\n- It could be better to rename data columns to have more comfortable df for the further research;\n- Also it would be usefull to add the new columns and check&change dtypes.","5deeaca4":"### Model Evaluation","23fc7184":"# The table of Contents\n* [Introduction](#section-one)\n* [Step 1: Studying and preparing data](#section-two)\n    - [Basic information](#sub-21)\n    - [Data preproccessing](#sub-22)\n\n* [Step 2: EDA](#section-three)\n    - [Investigation of outliers in 'capital-gain','capital-loss' columns](#sub-31)\n    - [Quntitative columns](#sub-31)\n    - [Categorical columns](#sub-33)\n    - [The correlation matrix](#sub-34)\n\n* [Step 3: Feature Selection](#section-four)\n    - [Feature Selection](#subsection-one)\n    - [Feature Scaling](#anything-you-like)\n\n* [Step 4: Building model](#section-five)\n    - [Subsection 1](#subsection-one)\n    - [Subsection 2](#anything-you-like)\n\n* [Step 5: Overall conclusion](#section-six)\n","41ae28ba":"<a id=\"sub-41\"><\/a>\n### Feature Selection\n\n>     `Feature selection is primarily focused on removing non-informative or redundant predictors from the model.`\n\n        \u2014 Page 488, Applied Predictive Modeling, 2013.\n\nAlso the performance of some models can degrade when including input variables that are not relevant to the target variable. We have small dataset, but nevertheless it would be usefull to master this skill. There are two main ways to select variables:\n\n- Unsupervised;\n- Supervised.\n\n\nWe choose the last one, which is divided into 3 methods:\n\n- Intrinsic (Trees);\n- Wrapper methods (RFE);\n- Filter methods (Stats & Feature Importance).\n\nWe use RandomForest to select features based on feature importance, which is the average of all decision tree feature importance in this method.","b97f6c54":"# Step 1: Studying and preparing data","7b7a8f9c":"<a id=\"section-five\"><\/a>\n# Step 4: Building model\n\nModels to build:\n- Linear;\n- Decision Tree;\n- Support Vector Machine;\n- Random Forest.","93f75ce7":"### Conclusion: \n\n- outliers in columns `capital_loss` and `capital_gain` were investigated, now it isn't needed to drop them, as it can make sense;\n- it is like husbands and men are more likely to have the higher income (see categorical investigation stage);\n- there is a weak positive correlation between hours_per_week,capital_loss,capital_gain, age and the income;\n- most features have no linear correlation between each other.","59ad181b":"Observation: all entries with the highest capital gain have >50k income value. That's why we couldn't delete these values","57b406cd":"Conclusion: we see a strong relationship between considered features: the higher capital gain the higher income and vice versa.\n- It is needed to scale all features before building a model,\n- It is could be better to make a corresponding conclusion after checking correlation and distribution (the next step),\n- We can't delete outliers here, as it can lead to the result distortion, but it could be useful to check the difference.","4e4c69e0":"Now lets take a look at how the quntitative variables are distributed","5da4eadb":"<a id=\"sub-32\"><\/a>\n### Quntitative columns","41b1187f":"<a id=\"sub-31\"><\/a>\n### Investigation of outliers in 'capital-gain','capital-loss' columns\n'\nQuestions:\n- Who has >90000 capital-gain?\n- How do these columns impact on the target column?","3ac4a33f":"- Federal and Local gov. occupations are more likely to get the higher salary;\n\n- Occupation `Private house serving` is less likely to aim >50k income;\n\n- `Never married` people has low chances to get the higher salary in comparison to the <50k count value;\n\n- Husbands have big chances to get >50k income (also check `sex` barplot);\n\n- Most people with high income are in family or were in family, it is related to the fact the mean age is higher and it is more likey that you're not `Not-in-Family`.\n","1c0c177b":"### Usefull links:\n\n- Thank [this source](https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/) for the information about feature importance;","af8bd9d8":"<a id=\"sub-21\"><\/a>\n### Basic info\n#### Studying data","af6e5794":"<a id=\"sub-34\"><\/a>\n### The correlation matrix\n\nCorrelation matrix depicts the correlation coefficients between all pairs of features in the data.\n\n\nWe use the Pearson correlation coefficient, which is a measure of the linear association between two variables. It has a value between -1 and 1 where:\n\n- -1 indicates a perfectly negative linear correlation between two variables\n- 0 indicates no linear correlation between two variables\n- 1 indicates a perfectly positive linear correlation between two variables\n","ad8ecf5d":"People, who are less than 40 y.o. are most likely to have higher income.\n\nAll features have different ranges, so it is needed to skale it later.","6e672413":"<a id=\"sub-33\"><\/a>\n### Categorical columns","82f782ba":"#### Checking outliers","feee0b25":"<a id=\"conclusion-3\"><\/a>\n### Conclusion:\n\n- there is no Gaussian distribution, so we chose normalization to scale the data;","5e1beac8":"<a id=\"section-four\"><\/a>\n# Step 3: Feature Selection and Scaling","db4caed9":"<a id=\"section-six\"><\/a>\n# Step 5: Overall conclusion\n\n1. At the first step we have studied data in detail;\n2. At the second step ","02267af6":"Okay, maybe this step requires an additional research.","de9645e0":"<a id=\"sub-34\"><\/a>\n### Feature Scaling\n\n**Why Should we Use Feature Scaling?**\n\nThe first thing is some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it.\n\n**There are two ways: normalization and standartization**\n\n- Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n- Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n\nOur data does not follow a Gaussian distribution, so we're going to normalize.","96cc2bf1":"### Building models","bfbd005d":"<a id=\"conclusion-4\"><\/a>\n### Conclusion:"}}