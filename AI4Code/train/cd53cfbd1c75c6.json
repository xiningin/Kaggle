{"cell_type":{"a226f1b0":"code","6e29192e":"code","e2b33ef6":"code","29983624":"code","6956113e":"code","3d91ddc4":"code","94d057a4":"code","9d565501":"code","e7107167":"code","6d5fd22b":"code","3217b3c8":"code","4c86bef7":"code","f72dce53":"code","aec2934a":"code","c9da0053":"code","c8f1f732":"code","cc14b495":"code","182e736e":"code","34d2db0c":"code","12283ae9":"code","0ba9649c":"code","4c9a3707":"code","9c285699":"code","3c76bb91":"code","d3779c40":"code","cfd46b34":"code","00192b83":"code","f65c0ba5":"code","83dad0a8":"code","ae215858":"code","d3869ef0":"code","86afcd1c":"code","4cab2122":"code","4f0141a6":"code","f1978aba":"code","73b6bd00":"code","ab2e1499":"code","785f0d17":"code","4881a2af":"code","0697fe29":"code","0f28d394":"code","e9ea8a87":"code","1a230f17":"code","46ab7056":"code","791d9acd":"code","b326f7a6":"code","880c2991":"code","b80daba7":"code","561a0795":"code","76001667":"code","d37ad6f4":"code","0ca1e907":"code","4d33c26d":"code","1fa7272d":"code","51a0afe6":"code","99a1afbc":"code","ae617654":"code","8e17353a":"code","cde702ef":"code","240f54c5":"code","6ceab6bc":"code","168441f6":"code","939e6857":"code","e5274056":"code","37a3f8c9":"code","b85b4b3e":"code","b240c59e":"code","fe6ddf61":"code","6bd9d966":"code","77c999fc":"code","74805b8a":"code","eb5d31b7":"code","6fee353a":"code","81293410":"code","5d1858f8":"code","991028cb":"code","0632a45d":"code","b2303505":"code","b7f7bcf9":"code","844d4721":"code","1df4b1ca":"code","3a606f8c":"code","59a97da0":"code","7a316d9b":"code","9ef47f59":"code","77a09dfd":"code","b0019d16":"code","2a80dbec":"code","60f91f64":"code","1ee48d4b":"code","d01afbf1":"code","3348b467":"code","cd4a5e92":"code","a00d19d3":"code","1c6d5540":"code","dd7fcc46":"code","0ed9addd":"markdown","789dc24c":"markdown","dd3b7853":"markdown","6a700850":"markdown","d653f7f2":"markdown","77194c0b":"markdown","da3bdc33":"markdown","2dcf6782":"markdown","34432246":"markdown","b9cd99a2":"markdown","4c0c6c12":"markdown","ed2675f6":"markdown","7ea8dbdb":"markdown","c265dea6":"markdown","59647ca5":"markdown","99bf50b1":"markdown","97cce9f0":"markdown","d9f46631":"markdown","2a9eb704":"markdown","e5218b0a":"markdown","7184d4bf":"markdown","f22aace4":"markdown","b908ff11":"markdown","1c58b41b":"markdown","feedc75e":"markdown","6321192e":"markdown","b2b335ea":"markdown","18cede4e":"markdown","f83c319a":"markdown","0ae2665f":"markdown","9c91a52d":"markdown","4266c9ec":"markdown","a18174fe":"markdown","07ee1e1d":"markdown","bd8dc8a3":"markdown","b62d3852":"markdown","2bd3ac54":"markdown","3641b50a":"markdown","407d5c01":"markdown","77b1fbc3":"markdown","139d13a8":"markdown","6440baaa":"markdown","f3a6377e":"markdown","5d9ef1d6":"markdown","94a12a2f":"markdown","3fc78606":"markdown","b6669867":"markdown","fbe4d8e1":"markdown","c31b7b79":"markdown","35dbad9d":"markdown","4d3a9c82":"markdown","3d4568d3":"markdown","d3b52b60":"markdown","e4b82257":"markdown","6e39da06":"markdown","91b71476":"markdown","bca07b61":"markdown","b687b89d":"markdown","e7dda454":"markdown","6c22ed0d":"markdown","28f38ff8":"markdown","a37f3a1d":"markdown","da5c5fe2":"markdown","acd77ac1":"markdown","e6204caa":"markdown","1c59d564":"markdown","c664e380":"markdown","9aca928c":"markdown","7d54de38":"markdown","194650c8":"markdown"},"source":{"a226f1b0":"#Import the necessary libraries.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n#from sklearn.model_selection import train_test_split\n#from sklearn.linear_model import LinearRegression","6e29192e":"data = '\/kaggle\/input\/cars4u\/used_cars_data.csv'\ndata1 = pd.read_csv(data) #load and read the csv file as a dataframe\ndf = data1.copy() #making a copy to avoid changes to the data\ndf.head()  # load the first five rows in the dataset","e2b33ef6":"df.tail() #load the last five rows","29983624":"print(f'There are {df.shape[0]} rows and {df.shape[1]} columns in this dataset.')  # f-string to get the shape of dataset","6956113e":"np.random.seed(68) #let's take a look at 10 random rows in the dataset\ndf.sample(n=10) #setting random seed to '68', so that we get same 10 rows every time","3d91ddc4":"df.drop(['S.No.'],axis=1,inplace=True) #dropping the S.No column","94d057a4":"df.info() ","9d565501":"df.drop([\"New_Price\"],axis=1,inplace=True)","e7107167":"from scipy.stats import norm\nnum_col = df.select_dtypes(include=np.number).columns.tolist()\n\nfor col in num_col:\n    df[col]=df[col].replace(0.0,np.nan)","6d5fd22b":"num_col","3217b3c8":"num_values = []\n#the loop will add all the columns we want to convert form object to numerical into a list\n# we can then use this list for conversion\nfor colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns\n    if df[colname].str.endswith('pl').any() or df[colname].str.endswith('kg').any() or df[colname].str.endswith('CC').any() or df[colname].str.endswith('bhp').any() or df[colname].str.endswith('Lakh').any():  \n        # using `.str` to use an element-wise string method to select the required columns\n        num_values.append(colname)\n    \nprint(num_values)","4c86bef7":"#Writing a function that will help split the string from the numerical values in the columns\n#This function will also drop the string and convert to float datatype.\n#This function will ensure a clean and faster code\ndef obj_to_num(n):\n    if isinstance(n,str): #checks if the columns are string datatype\n        if n.endswith('kmpl'):\n            return float(n.split('kmpl')[0])     \n        elif n.endswith('km\/kg'):                   \n            return float(n.split('km\/kg')[0])\n        elif n.endswith('CC'):\n            return float(n.split('CC')[0])\n        elif n.startswith('null'):     #replaces values that have string 'null bhp' to Nan\n            return(np.nan)          \n        elif n.endswith('bhp'):\n             return float(n.split('bhp')[0])\n    else: \n        return np.nan\n\nfor colname in num_values:\n    df[colname] = df[colname].apply(obj_to_num)#applying above function to the column list    \n    df[colname]=df[colname].replace(0.0,np.nan)","f72dce53":"df[\"Name\"]=df[\"Name\"].astype(\"category\")\ndf[\"Location\"]=df[\"Location\"].astype(\"category\")\ndf[\"Fuel_Type\"]=df[\"Fuel_Type\"].astype(\"category\")\ndf[\"Transmission\"]=df[\"Transmission\"].astype(\"category\")\ndf[\"Owner_Type\"]=df[\"Owner_Type\"].astype(\"category\") ","aec2934a":"np.random.seed(68) \ndf.sample(n=10) #re-checking the same 10 random rows","c9da0053":"df.info()  #re-checking the dataset again","c8f1f732":"df.describe(include=[\"category\"]).T","cc14b495":"df[['Car_Brand','Model']] = df.Name.str.split(n=1,expand=True) #splitting the Brand and the car model","182e736e":"Brand_name=df['Car_Brand'].unique()\nModel=df['Model'].unique() # Model names are unique to the Car Brands. ","34d2db0c":"Brand_name #Checking car brand names ","12283ae9":"df['Car_Brand']=df['Car_Brand'].replace('Land','Land_Rover') \ndf['Car_Brand']=df['Car_Brand'].replace('ISUZU','Isuzu')  #correcting the brands\ndf['Car_Brand'].value_counts()","0ba9649c":"numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\nnumeric_columns","4c9a3707":"numeric_columns.remove('Price') #It's the dependent variable\nmedianFiller = lambda x: x.fillna(x.median())\ndf[numeric_columns] = df[numeric_columns].apply(medianFiller,axis=0)","9c285699":"Median1=[] #creating an empty list to add the median Price of Cars per Brand\nfor i in range(len(Brand_name)):\n    x=df['Price'][df['Car_Brand']==Brand_name[i]].median()\n    Median1.append(x)","3c76bb91":"Median2=[] #Creating an empty list to add the median price of cars per Car model\nfor i in range(len(Model)):\n    x=df['Price'][df['Model']==Model[i]].median()\n    Median2.append(x)","d3779c40":"df['Price']= df['Price'].fillna(0.0) #replacing the missing values with float 0.0","cfd46b34":"for i in range(len(df)):  #running a loop to check every row in df dataset\n    if df.Price[i]==0.00:\n        for j in range(len(Model)):  \n            if df.Model[i]==Model[j]:  #Comparing the Car model  names in both datasets\n                df.Price[i]=Median2[j]  #replacing the Price of the car with the median price of its subsequent model\n            ","00192b83":"df.info() #rechecking for missing values","f65c0ba5":"df[df['Price'].isna()]\n  #checking the rows with missing Price Value","83dad0a8":"df['Price']= df['Price'].fillna(0.0) #replacing the missing values with float 0.0\nfor i in range(len(df)):  #running a loop to check every row in df dataset\n    if df.Price[i]==0.00:\n        for j in range(len(Brand_name)):  \n            if df.Car_Brand[i]==Brand_name[j]:  #Comparing the brand names in both datasets\n                df.Price[i]=Median1[j]     #replacing with corresponding missing values","ae215858":"df[df['Price'].isna()]","d3869ef0":"#Dropping the above two cars as there are only one of each per brand\n#Also we do not have any further information to calculate its price\ndf.dropna(axis=0,inplace=True)\ndf.shape  #we now have 7251 rows and 14 columns","86afcd1c":"pd.set_option('display.float_format', lambda x: '%.3f' % x) # to display numbers in digits\ndf.describe().T","4cab2122":"#Performing Univariate Analysis to study the central tendency and dispersion\n#Plotting histogram to study distribution\nfrom scipy.stats import norm\nUni_num = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(17,75))\nfor i in range(len(Uni_num)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    sns.histplot(df[Uni_num[i]],kde=False)\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","4f0141a6":"#Plotting a box plot to study central tendency\nplt.figure(figsize=(15,35))\nfor i in range(len(Uni_num)):\n    plt.subplot(10,3,i+1)\n    sns.boxplot(df[Uni_num[i]],showmeans=True, color='yellow')\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","f1978aba":"regions ={'Delhi':'North','Jaipur':'North',\n          'Chennai':'South','Coimbatore':'South','Hyderabad':'South','Bangalore':'South','Kochi':'South',\n        'Kolkata':'East',\n         'Mumbai':'West','Pune':'West','Ahmedabad':'West'}\ndf['Region']=df['Location'].replace(regions)","73b6bd00":"df.drop([\"Car_Brand\",\"Model\"],axis=1,inplace=True) # no longer needed for Analysis\ndf['Car_Type'] = pd.cut(df['Price'],[-np.inf,5.5,10.5,20.5,45.0,75.0,np.inf],\n                       labels=[\"Tier1\",\"Tier2\",\"Tier3\",\"Tier4\",\"Tier5\",\"Tier6\"])\n\ndf['Car_Type'].value_counts()","ab2e1499":"df.sample()","785f0d17":"#Univariate Analysis on Categorical Variables\ncategorical_val = df.select_dtypes(exclude=np.number).columns.tolist()\ncategorical_val.remove('Name')\ncategorical_val.remove('Location')","4881a2af":"plt.figure(figsize=(17,75))\nfor i in range(len(categorical_val)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    ax=sns.countplot(df[categorical_val[i]],palette='Dark2')\n    plt.tight_layout()\n    plt.title(categorical_val[i],fontsize=25)\n    total = len (df[categorical_val[i]])\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total) # percentage of each class of the category\n        x = p.get_x() + (p.get_width() \/ 2)-0.1  # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 13.5,color='black') # To annonate\n\nplt.show()\n","0697fe29":"df[df['Fuel_Type']=='Electric']","0f28d394":"corr= df.corr().sort_values(by=['Price'],ascending=False) #coorelation matrix with respect to dependent variable Price\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,annot= True,vmin=0,vmax=1, cmap='coolwarm',linewidths=0.75)\nplt.show()","e9ea8a87":"#Analysis of variables that have high correlation with Price\n#Price Vs Engine Vs Region\nplt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Price',x='Engine',hue='Region')\nplt.show()","1a230f17":"#Price Vs Power Vs Region\nplt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Price',x='Power',hue='Region')\nplt.show()","46ab7056":"plt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Engine',x='Power',hue='Car_Type')\nplt.show()","791d9acd":"#Price Vs Mileage Vs Region\nplt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Price',x='Mileage',hue='Region')\nplt.show()","b326f7a6":"#How does Manufacture Year affect Price?\nplt.figure(figsize=(15,7))\nsns.lineplot(x='Year', y='Price',\n             data=df);","880c2991":"#Kilometers_Driven Vs Year\n#Since The range for Kilometers is very wide, we will log transform to a manageable scale\nplt.figure(figsize=(15,7))\nsns.lineplot(x='Year', y=np.log(df['Kilometers_Driven']),\n             data=df)","b80daba7":"#Engine Vs Mileage Vs Car_Type\nplt.figure(figsize=(15,7))\nsns.scatterplot(x='Engine', y='Mileage',hue='Car_Type',\n             data=df)","561a0795":"#Does type of ownership affect Car price?\ndf_hm =df.pivot_table(index = 'Region',columns ='Owner_Type',values =\"Price\",aggfunc=np.median)\n# Draw a heatmap \nplt.subplots(figsize=(10,7))\nsns.heatmap(df_hm,cmap='copper',linewidths=.5, annot=True);","76001667":"#Does type of Fuel affect car price?\nplt.figure(figsize=(7,5))\nsns.barplot(data=df,x='Fuel_Type',y='Price')","d37ad6f4":"# Lets treat outliers by flooring and capping\ndef treat_outliers(df,col):\n   \n    Q1=df[col].quantile(0.25) # 25th quantile\n    Q3=df[col].quantile(0.75)  # 75th quantile\n    IQR=Q3-Q1\n    Lower_Whisker = Q1 - 1.5*IQR \n    Upper_Whisker = Q3 + 1.5*IQR\n    df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker \n                                                            # and all the values above upper_whishker will be assigned value of upper_Whisker \n    return df\n\ndef treat_outliers_all(df, col_list):\n    \n    for c in col_list:\n        df = treat_outliers(df,c)\n        \n        \n    return df    ","0ca1e907":"df2=df.copy() #making the first copy\nnumerical_col = df2.select_dtypes(include=np.number).columns.tolist()\nnumerical_col.remove('Year')\nnumerical_col.remove('Mileage')\nnumerical_col.remove('Seats')  #Dropping Year,Mileage and Seats as they dont have very high outliers\nnumerical_col\n","4d33c26d":"df2 = treat_outliers_all(df2,numerical_col) #treating outliers ","1fa7272d":"#checking if the outliers are treated\nplt.figure(figsize=(15,35))\nfor i in range(len(numerical_col)):\n    plt.subplot(10,3,i+1)\n    sns.boxplot(df2[numerical_col[i]],showmeans=True, color='yellow')\n    plt.tight_layout()\n    plt.title(numerical_col[i],fontsize=25)\n\nplt.show()","51a0afe6":"df2.head()","99a1afbc":"#Defining X and y variables","ae617654":"X = df2.drop(['Name','Fuel_Type','Location','Price'], axis=1)\n#dropping Name as we bins via Car_Type\n#dropping Fuel_Type to not affect the accuracy of the model\ny = df2[['Price']]\n\nprint(X.shape)\nprint(y.shape)","8e17353a":"#Creating Dummy Variabls for the Categorical Columns\n#Dummy variable will be used as independent variables and will not impose any ranking\nX = pd.get_dummies(X, columns=['Transmission','Owner_Type','Region','Car_Type'], drop_first=True)\nX.head()","cde702ef":"#split the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56) # keeping random_state =56 ensuring datasplit remains consistent\nX_train.head()","240f54c5":"#Fitting linear model\n\nfrom sklearn.linear_model import LinearRegression\nlinearregression = LinearRegression()                                    \nlinearregression.fit(X_train, y_train)\nprint(\"Intercept of the linear equation:\", linearregression.intercept_) \nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, linearregression.coef_[0][idx]))\n                         ","6ceab6bc":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\npred = linearregression.predict(X_test) ","168441f6":"# Mean Absolute Error on test\nmean_absolute_error(y_test, pred)","939e6857":"# RMSE on test data\nmean_squared_error(y_test, pred)**0.5","e5274056":"# R-squared on test\nr2_score(y_test, pred)","37a3f8c9":"# Training Score\n\nlinearregression.score(X_train, y_train)  # 70 % data ","b85b4b3e":"# Testing score\n\nlinearregression.score(X_test, y_test) # unseen data","b240c59e":"df3=df.copy() #making the second copy\ndf3.head()","fe6ddf61":"X1 = df3.drop(['Name','Fuel_Type','Location','Price'], axis=1)\n#dropping Name as we bins via Car_Type\n#dropping Fuel_Type to not affect the accuracy of the model\ny1 = df3[['Price']]\n\nprint(X1.shape)\nprint(y1.shape)","6bd9d966":"#Creating Dummy Variabls for the Categorical Columns\n#Dummy variable will be used as independent variables and will not impose any ranking\nX1 = pd.get_dummies(X1, columns=['Transmission','Owner_Type','Region','Car_Type'], drop_first=True)\nX1.head()","77c999fc":"#split the data into train and test\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=56)\nX1_train.head()","74805b8a":"#Fitting linear model\nfrom sklearn.linear_model import LinearRegression\nlinearregression = LinearRegression()                                    \nlinearregression.fit(X1_train, y1_train)                                  \nprint(\"Intercept of the linear equation:\", linearregression.intercept_) \nfor idx, col_name in enumerate(X1_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, linearregression.coef_[0][idx]))                         ","eb5d31b7":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\npred1 = linearregression.predict(X1_test)","6fee353a":"# Mean Absolute Error on test\nmean_absolute_error(y1_test, pred1)","81293410":"# RMSE on test data\nmean_squared_error(y1_test, pred1)**0.5","5d1858f8":"# R-squared on test\nr2_score(y1_test, pred1)","991028cb":"# Training Score\n\nlinearregression.score(X1_train, y1_train)  # 70 % data ","0632a45d":"# Testing score\n\nlinearregression.score(X1_test, y1_test) # unseen data","b2303505":"# Lets us build linear regression model using statsmodel \nimport statsmodels.api as sm\nX = sm.add_constant(X)\nX_train1, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56)\n\nolsmod0 = sm.OLS(y_train, X_train1) #y_train remains same \nolsres0 = olsmod0.fit()\nprint(olsres0.summary())","b7f7bcf9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif_series1 = pd.Series([variance_inflation_factor(X_train1.values,i) for i in range(X_train1.shape[1])],index=X_train1.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series1))","844d4721":"X_train2 = X_train1.drop('Power', axis=1)\nvif_series2 = pd.Series([variance_inflation_factor(X_train2.values,i) for i in range(X_train2.shape[1])],index=X_train2.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series2))","1df4b1ca":"olsmod1 = sm.OLS(y_train, X_train2)\nolsres1 = olsmod1.fit()\nprint(olsres1.summary())","3a606f8c":"X_train3 = X_train1.drop('Engine', axis=1)\nvif_series3 = pd.Series([variance_inflation_factor(X_train3.values,i) for i in range(X_train3.shape[1])],index=X_train3.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series3))","59a97da0":"olsmod2 = sm.OLS(y_train, X_train3)\nolsres2 = olsmod2.fit()\nprint(olsres2.summary())","7a316d9b":"X_train4 = X_train3.drop('Kilometers_Driven', axis=1)\nolsmod3 = sm.OLS(y_train, X_train4)\nolsres3 = olsmod3.fit()\nprint(olsres3.summary())","9ef47f59":"residual= olsres3.resid\nnp.mean(residual)","77a09dfd":"residual=olsres3.resid\nfitted=olsres3.fittedvalues #predicted values","b0019d16":"sns.set_style(\"whitegrid\")\nsns.residplot(fitted,residual,color=\"olive\",lowess=True)\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual PLOT\")\nplt.show()","2a80dbec":"#Histogram of Residuals\nsns.distplot(residual) \nplt.title('Normality of residuals')\nplt.show()","60f91f64":"# Q-Q plot to check the normal probability of residuals.\n# It should approximately follow a straight line\nimport pylab\nimport scipy.stats as stats\nstats.probplot(residual,dist=\"norm\",plot=pylab)\nplt.show()","1ee48d4b":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residual, X_train4)\nlzip(name, test) #returns a list of values\n","d01afbf1":"X_train4.columns","3348b467":"X_test_final = X_test[X_train4.columns]\nX_test_final.head()","cd4a5e92":"y_pred = olsres3.predict(X_test_final)","a00d19d3":"#Checking root mean squared error on both train and test set  \n\nfrom sklearn.metrics import mean_squared_error\nrms = np.sqrt(mean_squared_error(y_train, fitted))\nprint('Train error:',rms)\n\nrms1 = np.sqrt(mean_squared_error(y_test, y_pred))\nprint('Test error:',rms1)","1c6d5540":"olsmodtest = sm.OLS(y_test, X_test_final)\nolsrestest = olsmodtest.fit()\nprint(olsrestest.summary())","dd7fcc46":"print(olsres3.summary())","0ed9addd":"## Outliers Treatment","789dc24c":"- For further processing we have to make the data more manageable.\n- Let's group the cars by Brand and Model","dd3b7853":"**Observations**:\n- Most cars in Tier1 have less Engine CC and therefore Higher Mileage","6a700850":"- The VIF scores have reduced for Engine and there is no more collinearity in the model","d653f7f2":"**Observations**:\n- The Adj. $R^2$ is 0.951 for olsres2, which is better than the olsres1 at 0.949\n- Hence we will proceed further with olsres2 for further analysis.\n- Now that there is no multi-collinerity, we check the p-values of the predictor variables for insignificance\n\n\n**Observations**:\n* Kilometers_Driven, Owner_Type_Fourth & Above and Owner_Type_Second p-value greater than 0.05 and therefore is not significant.\n* We will only be dropping Kilometers_Driven and not the other two despite high p-values. \n* Owner_Type_Fourth & Above and Owner_Type_Second are part of the categorical variable Owner_Type and there are other significant values in this category.","77194c0b":"**Observations**:\n* Year:\n    - Mean Year of car's is 2013 which is one year short of median. Year starts from 1998 till 2019 implying older to latest car models\n* Kilometers_Driven: \n    - The Mean is slightly higher than the median, but the max value is very hight, suggesting outliers\n* Mileage: \n    - The Mean and Median of Mileage are fairly close\n* Engine and Power & Price\n    - The Mean value is significantly higher than the median for all three variables. \n    - Average Price is at 9.33 Lakhs. The Variance is greater than the Mean which suggests wide distribution(skewness) of data. ","da3bdc33":"* Mean of Residuals is very close to 0.\n","2dcf6782":"**Observations:**\n- We see that ther are 2041 total unique Cars\n- More cars are sold in Mumbai and Diesel is the preffered Fuel Type\n- Most of the cars sold are Manual Transmission and have only had one previous owner.","34432246":"**Observations**:\n* We see that as Engine capacity increases Price of cars also increase.\n* We also notice several exceptions to the above case","b9cd99a2":"**Observations**\n* There are 7253 entries across 13 columns\n* Mileage, Engine, Power and New_Price columns are in Object datatype.These columns need to be converted to Numerical. \n* The rest of the object datatype need to be converted to Category.\n    `coverting \"objects\" to \"category\" reduces the space required to store the dataframe. It also helps in analysis`\n* We can also see that New_Price column has only 1006 entries. \n* The Price column also has significant missing values.\n* Power, Mileage, Seats and Engine have comparatively lesser missing values than the above two.","4c0c6c12":"**Observations:** From Both Histogram and Box plots :\n\n* Only Mileage has a somewhat normal distribution\n* Year is left-skewed and has comparatively less outliers in the lower end.\n* Engine & Power:\n    - Both columns are right-skwed with a moderate Inter-Quartile Range and several outliers at the higher scale.\n      Power has more outliers comapred to Engine. \n* Kilometer_Driven and Price:\n    - Both these columns are heavily right-skewed, with Kilometers_Driven having a very small IQR and one large outlier in the max end. Price column also has several outliers in the higher end. \n    We will treat these outliers as they might have adverse effect in the accuracy of the prediction. But sometimes outliers might have independent significance to the data.\n    So, We will also the building model to decide on the outlier treatment","ed2675f6":"**Observations**:\n* Mean Price of cars decrease as number of ownership of cars increases across all regions\n* The South region also has the highest Median Price for Cars with only one previous owner, followed by West then North and East\n* We also see that in East there are only two Owner_types\n* This suggests that type of ownership does affects overall car price.","7ea8dbdb":"## Checking the Linear Regression Assumptions:\n-  No Multicollinearity\n-  Mean of residuals should be 0\n-  No Heteroscedacity\n-  Linearity of variables\n-  Normality of error terms","c265dea6":"### We will build a Predictive model with and without treating the Outliers and compare it's performances to decide if the outliers have any adverse impact to the linear model.","59647ca5":"### TEST FOR HOMOSCEDASTICITY:\n* The assumption is that the variance of the residuals  is equal\/same across all values of independent variabels for the final data. i.e The Error term doesnt vary too much when the Indipendent(Predictor) variable changes, Homoscedastic\n* If the variance is not equal then the data is Hetroscedastic\n    - Null Hypothesis : Residuals are equal across independent variables\n    - Alternate Hypothesis : Residuals are not equal across independent variables","99bf50b1":"* The Price Column also has missing values(1234) that needs to be treated. \n* Hence we will calculate Median Price per Brand and per Brand's model and replace the missing values in Price column","97cce9f0":"**Observations**:\n- From the plot, we see that all three variables have a positive correlation.\n- This also suggest multicolinearity between Engine and Power, which must be addressed later","d9f46631":"#### Checking if Mean of residuals should be 0 for OLSres3\n* Residual is the difference between the observed x-value and the fitted x-value to the best fit line.","2a9eb704":"## Feature Engineering:\n\n### Grouping Location by Regions","e5218b0a":"### Coorelation Matrix:","7184d4bf":"* The p-value 0.202 is greater than level of confidence. i.e p-value>0.05.\n* Hence we fail to reject the Null Hypothesis. Thus the Residuals are equal (Homoscedastic) across all independent variables.\n\n#### All Linear Regression Assumptions have been satisfied.\n\n\n### Predicting on Test Data:","f22aace4":"* Let's replace any possible corrupt values like, 0.0 to Nan before proceeding.","b908ff11":"**Insights**:\n* Price has high positive correlation with Engine and Power and a lower positive correlation with Year.\n* Price has a lower negative correlation with Mileage \n* Engine and Power have a very high positive correlation.\n* Mileage has a high negative correlation with Engine and Power","1c58b41b":"## Model Building 2 - Without Treating Outliers","feedc75e":"**Insights**:\n- The P-Value of the variable indicates if the its significant or not.\n- The level of significance is 0.05 and any p-value less than 0.05 , then that variable would be considered significant.\n","6321192e":"* From the above details, we see that New_Price column has almost 80% of data missing. This may impact the performance of the model to caluculate the price. Therefore we will drop this column for further analysis.\n","b2b335ea":"* All datatypes are now fixed and the memory useage has reduced.\n* We noticed that the number of missing values has also increased","18cede4e":"## Model Building 1 - With Treated Outliers","f83c319a":"* Ajd.$R^2$ is 0.952 which is close to the traing data Ajd.$R^2$ 0.951","0ae2665f":"**Observations**:\n* Price and Mileage have a negative correlation with a few exceptions","9c91a52d":"### Summary of Numerical Columns","4266c9ec":"**Observations**:\n- The Adj.$R^2$ has reduced from 0.952 to 0.949 - which is still good.","a18174fe":"**Observations**:\n* Year and Kilomertes_driven have a negative correlation\n* This is to be expected as lastest model used cars probably have less useage before being sold.","07ee1e1d":"# Key Questions:\n- Does the various factors in the dataset really affect the pricing?\n- What are the key\/main variables affecting the pricing for Pre-Owned cars?\n- What factors have no influence in the Pricing? \n- What are the key recommendations in improving business profits?\n\n# Assumptions:\n\n- The Used Cars data is a random sample from the population data","bd8dc8a3":"- The VIF scores have reduced for Power and there is no more collinearity in the model","b62d3852":"**Observations**:\n- Maruti and Hyundai are the most popular cars brands\n- Honda and Toyota are the next most popular brands\n- We also see that the expensive luxury car brands are very few","2bd3ac54":"### Binning the Car Names by different Price Levels:\n- We have 33 car brands and even higher individual models. \n- To manage the data subesequently, we will bin them according to their Price Ranges; from lower\/economic cars to luxury\/expensive cars\n- This will reduce total categories of Cars to just six.","3641b50a":"**Observations:**\n- The Outliers for Engine, Price, Power and Kilometers_driven is treated\n- We will build a model with this treated dataset to analyse the Price","407d5c01":"* The Q-Q plot is approximately straight line\n* Hence the Test for Linearity is satisfied","77b1fbc3":"### Missing Value Treatment:\n- We will replace the missing values in Power,Engine,Mileage and Seats with its median value.","139d13a8":"**Observations**:\n- From the above model we see that the  $R^2$ is 0.921, that explains 92.1% of total variation in dataset. Though this model is a decent fit, its less than the $R^2$ value from Model 1. \n- Also we see that the Training and Testing Scores for this model are 94.5% and 92.1% which has a marginal difference.\n-  Hence we shall proceed with the Model 1 for further analysis and  Stats model.","6440baaa":"### Test for Linearity:\n* To check if there is a linear (Straight-line) relationship between the dependent and independent variables.\n* To check, we will plot between Fitted values Vs Residuals\n* Fitted or Predicted value describes where the particular x-value fits in the best fit line.","f3a6377e":"## Interpreting the Regression Results:\n\n1. **Adjusted. R-squared**: It reflects the fit of the model and ranges from 0 to 1\n    - A high Adjusted R-Squared values indicated a good fit. In this model, the Adj. R-squared is **0.953**, which is good!\n2. **const coefficient** is the Y-intercept.\n    - If all the independent variables are zero, then the expected output will be equal to const coefficient, which in this case is **-40.48**\n3. **std err**: It reflects the level of accuracy of the coefficients.\n      - The lower it is, the higher is the level of accuracy.\n5. **P >|t|**: It is p-value.\n    -  This shows that for each independent feature there is a null hypothesis and alternate hypothesis \n\n    Ho : Independent variable is not significant \n\n    Ha : Independent variable is significant\n    - If p-value is less than 0.05 , then the variable is considered to be statistically significant.\n  \n6. **Confidence Interval**: It represents the range in which our coefficients are likely to fall.\n    - The current confidence interval is at 95% ","5d9ef1d6":"**Observations**:\n* 82.1% of all cars only have One previous owner.\n* 71% of the cars are of Manual Transmission and 47.6% of cars sold in South region.\n* We also see that about 49.6% of cars are in Tier1 i.e at Price below 5.5 Lakhs INR.\n* Diesel is the most Preferred Fuel_type at 53.1% followed by Petrol 45.8%.CNG and LPG(Gas-reliant) together make 1% of all cars. We are see that the Electric Fuel_Type is at 0.0%, Let's check that","94a12a2f":"### TEST FOR NORMALITY:\n* The Residuals should be normally distributed.\n* We will perform the test for Normality in the following steps:\n    - Histogram of Residuals\n    - Q-Q plot \n* Further analysis of data will be performed if any the above tests fail.","3fc78606":"### Load the data to read and explore ","b6669867":"**Observations**:\n- We see that Land Rover is mentioned as Land and the Brand Isuzu is mentioned twice","fbe4d8e1":"## Conclusion:\n* We conclude that olsres3 is a good model for prediction and inference at 95.1% Ajd.$R^2$.\n* Only Transmission and Onwer_type have a negative correlation to Price; ie. As Manual Transmissions lower the overall Pricing of used Cars than Automatic.\n* As ownership level increases, the Pricing of used cars drop. \n* Year, Mileage, Power and Seats have positive assosiation with Pricing. \n* The above variables are the main features that impact the Price of a Used car","c31b7b79":"## Exploratory Data Analysis:\n### Univariate Analysis Each Numerical Column:","35dbad9d":"#### Since there are no more p-values greater than 0.05, olsres 3 is the final model and X_train4 is the final data.\n\n**Observations**:\n- The Adjusted R-Squared for the model is 0.951. This shows that the model is able to explain 95.1% of the variance.\n- The Adjusted R-Squared in OLSres0 was 95.2%.This shows that the dropped variables did not affect the model very much.\n- Hence this model is a good fit.","4d3a9c82":"### Bivariate and Multivariate Analysis:","3d4568d3":"### Checking for Multicollinearity using VIF Scores:\n- Multicollinearity occurs when there is correlation between the predictor variables.\n- Since the variables are required to be independent,having a correlation will lead to inaccuracy in the model.\n- VIF(Variance Inflation Factor) scores measures how much the variance of an estimated regression coefficient is increased by collinearity. VIF scores quantify the severity of multi-collinearity in OLS stats model.\n- If VIF value exceeds or is close to 5 then we there is moderate correlation.\n- IF VIF value exceed or is close to 10 then it shows high multi-collinearity","d3b52b60":"* The above mentioned cars appear only once in the dataset. Hence we dont have a median price value per its model.\n* Therefore we will replace the missing Price of these cars with the median Price of its corresponding Brand, that was calculated earlier. ","e4b82257":"* We can drop the S.No column as its repetitive of the index and not required in further analysis.\n* The Mileage, Engine and Power column are represented as strings when they should be in numerical.\n* Mileage has a 0.0 value that should be replace by Nan\n* Power has string 'null bhp' which should be replaced by Nan\n* The above random sample shows that some columns have a lot of missingness, so that needs to be analysed later and New_price particularly has a lot missing values.\n* Price is the dependent variable","6e39da06":"- There are only two cars running in Electric Fuel in this dataset","91b71476":"### Checking the performace of Train and test data using RMSE metric\n* Root Mean Squared Error (RMSE) is the Standard Deviation (S.D) of residuals. \n* Lower RMSE values indicate a good model fit","bca07b61":"## Processing Columns:\n- Before getting the summary statistics of the data to analyse the distribution, we must convert them to numerical columns.","b687b89d":"### Model Performances:","e7dda454":"# Stats Model:\n- Using Stats Model in Python, we will get an list of statistical results for each estimator.\n- Stats Model is also used to further conduct tests and statistical data exploration","6c22ed0d":"* The Scatterplot shows that the distribution between Residuals(errors) and Fitted values has no pattern.\n* Hence Linearity assumption is satisfied","28f38ff8":"### Checking the data types and missing values in the columns.","a37f3a1d":"**Observations**:\n* Engine and Power have VIF scores greater than 5. This suggests that there is moderate to high collinearity suggesting the 2 variables are correlated to each other.\n* This makes sense as High Engine efficiency leads to high Power in a vehicle.\n* Hence to remove multi-collinearity, we will drop Power column first as it has a higher score.\n","da5c5fe2":"**Observations**: \n* Electric Car's have an equal Price range compared to Diesel.\n* We know that there are only two cars with Electric Fuel_Type in this data, which is a very small sample size.\n* Hence we will drop Fuel_Type while building the ML model, as it might affect the accuracy.","acd77ac1":"**Observations**:\n- From the above model we see that the  $R^2$ is 0.953, that explains 95.3% of total variation in dataset. This model is a good fit.","e6204caa":"**Observations**:\n* Overall as Manufacture Year rises, Price of Car also increases.","1c59d564":"### Summary of Categorical Variabels","c664e380":"**Observations**:\n- Price does increase with Power, but we can also see several exceptions.","9aca928c":"## Fixing Datatypes","7d54de38":"### Import libraries","194650c8":"**Observations**:\n* The Train and Test Errors are comparable and quite low.\n* This suggests that the model does not suffer from either over-fitting(noise + information) or under-fitting(less information)"}}