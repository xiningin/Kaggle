{"cell_type":{"cb4c90d7":"code","a3d00b9b":"code","406d686d":"code","a9836f59":"code","598a2c75":"code","b33d474a":"code","0ef78587":"code","a80aaabb":"code","50840620":"code","c44a434e":"code","3d442886":"code","25444b59":"code","efc6d158":"code","60f5655c":"code","68b303c1":"code","4b8a93ec":"code","3e61fa63":"code","1b8951db":"code","c48a94b9":"code","52992930":"code","c32d4527":"markdown"},"source":{"cb4c90d7":"#Import libraries\n\n# data anlysis\nimport numpy as np\nimport pandas as pd\n\n# data visualization\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data preparation for modelling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\nimport time\nimport tensorflow as tf\nimport pandas as pd\nimport os\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import Sequential\nfrom keras import layers\nfrom keras import backend as K\nfrom keras.layers.core import Dense\nfrom keras import regularizers\nfrom keras.layers import Dropout , BatchNormalization\nfrom keras.constraints import max_norm\n\n# ensure comparability of different runs\nnp.random.seed(2020)","a3d00b9b":"# df = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_for_use.pkl\")\n# df_fe = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_fe.pkl\")","406d686d":"df = pd.read_pickle('..\/input\/searching-for-bad-loan-data-preprocessing\/df_pp.pkl')","a9836f59":"# X = df.drop('Loan_status', axis=1)\n# # x_dd = X.copy()\n# # from sklearn.preprocessing import StandardScaler\n# # from sklearn.preprocessing import MinMaxScaler\n# # minMaxScaler = MinMaxScaler()\n# # minMaxScaler.fit(X)\n# # X_scaled = minMaxScaler.transform(X)\n\n# # X= pd.DataFrame(X_scaled, columns=x_dd.columns)\n# # X = reduce_mem_usage(X)\n# y = df['Loan_status']\n\n# from sklearn import preprocessing as pp\n# featuresToScale = X.columns\n# sX = pp.MinMaxScaler(copy=True)\n# X.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])\n\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","598a2c75":"data = df.copy()","b33d474a":"data.head()","0ef78587":"# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent',)),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])","a80aaabb":"data.columns","50840620":"cat_cols = [ 'home_ownership_cat', 'term_cat', 'application_type_cat', 'purpose_cat', 'region','emp_length_int', 'grade_cat', 'interest_payment_cat', 'income_cat' ]","c44a434e":"num_cols = ['loan_amount', 'annual_inc', 'interest_rate', 'dti', 'total_pymnt', 'recoveries', 'installment']","3d442886":"data.head()","25444b59":"X = data.drop('Loan_status', axis=1)\n# x_dd = X.copy()\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import MinMaxScaler\n# minMaxScaler = MinMaxScaler()\n# minMaxScaler.fit(X)\n# X_scaled = minMaxScaler.transform(X)\n\n# X= pd.DataFrame(X_scaled, columns=x_dd.columns)\n# X = reduce_mem_usage(X)\ny = data['Loan_status']\n\nfrom sklearn import preprocessing as pp\nfeaturesToScale = X.columns\nsX = pp.MinMaxScaler(copy=True)\nX.loc[:,featuresToScale] = sX.fit_transform(X[featuresToScale])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","efc6d158":"input_dim = X_train.shape[1]\ninput_dim","60f5655c":"# Add RUC metric to monitor NN\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","68b303c1":"model = Sequential()\nmodel.add(Dense(64, input_dim = 31 , kernel_initializer = 'uniform', kernel_regularizer=regularizers.l2(0.005),activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,  kernel_initializer = 'uniform',kernel_regularizer=regularizers.l2(0.005),activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16,  kernel_initializer = 'uniform',kernel_regularizer=regularizers.l2(0.005),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_logloss', optimizer='adam', metrics=['accuracy' ,auc])\nmodel.summary()","4b8a93ec":"from keras.callbacks import EarlyStopping","3e61fa63":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\nearlystopper = EarlyStopping(monitor='val_loss', patience = 20, verbose=1)\nstart = time.time() \nmodel.fit(X_train, y_train, epochs=1000,validation_split=0.2,  batch_size = 512, callbacks=[earlystopper] )\nnn_runtime = time.time() - start","1b8951db":"print( ' Neural Net Runtime : {0:.4f}'.format(nn_runtime ))","c48a94b9":"from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report ,average_precision_score\n\npred = model.predict(X_test)\npred = pd.DataFrame(data = pred , dtype=np.float32)\ny_test =  pd.DataFrame(data = y_test)\ny_test.reset_index(inplace = True)\ny_test.drop('index',axis=1,inplace=True)\npreds = pd.concat([y_test,pred], axis=1)\npreds.columns = ['trueLabel','prediction']\n# predictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","52992930":"NN_roc_auc_score = roc_auc_score(y_test, model.predict(X_test))\n\nprint( 'NeuralNet_ROC_AUC : {0:.4f} '.format(NN_roc_auc_score))","c32d4527":"Categorical Features in the dataset:, home_ownership, term, application_type, purpose, region.\n\nOrdinal Features in the dataset :emp_length_int, grade, interest_payments, income_category\n\nContinous Features in the dataset: loan_amount, annual_inc, interest_rate, dti, total_pymnt, recoveries, installment"}}