{"cell_type":{"a7f9f247":"code","9a81d992":"code","4e5aabaf":"code","ba15d8e0":"code","ffa78b43":"code","57ad67fa":"code","e02a9f79":"code","5d3d29d9":"code","fc95aac9":"code","8316620f":"code","2d717777":"code","4f3371bc":"code","031da909":"code","b7cb559d":"code","0088d537":"code","0433fda6":"code","648478a8":"code","407751e4":"code","676b5f5e":"code","adb408fc":"code","0f10df67":"code","3a5c5a64":"code","2b6db64a":"code","51b1363b":"code","4aafc289":"code","6c4c1459":"code","d8db1483":"code","e7a78d09":"code","6b372a69":"code","f153a1c7":"code","f0769d29":"code","016b5e6e":"code","9837e6c8":"code","6298160f":"code","959fe765":"code","c3baf429":"code","55f2268e":"code","476ffecf":"code","418743aa":"code","5517cb19":"code","4600bf17":"code","e8dd52dc":"code","bb5f7b95":"code","d9786be4":"markdown","4231519a":"markdown","7a9dd841":"markdown","a60bb9e7":"markdown","f62ffc14":"markdown","7e4d35c3":"markdown","8a04d7fe":"markdown","b41d1c3f":"markdown","ceb2b218":"markdown","c78d884e":"markdown","0f1d6d7f":"markdown","cfe1aa91":"markdown","9a4b446a":"markdown","5ea54ba5":"markdown","6a1fcb07":"markdown","b34e6aef":"markdown","db683d3c":"markdown","df05e071":"markdown","0f7b09a7":"markdown","9aa61c4b":"markdown","420f4ad6":"markdown","34e7927a":"markdown","249bc2f7":"markdown","71e9bd80":"markdown","a236ace1":"markdown","54a85605":"markdown","89f57eeb":"markdown","ec67e225":"markdown","6c47436e":"markdown","299bb48b":"markdown","f14a1319":"markdown","0c0c2809":"markdown","f526ff8f":"markdown","61669a0a":"markdown","e4252340":"markdown","17676951":"markdown","214eda7e":"markdown","c005bb56":"markdown","847c4e69":"markdown","68df3960":"markdown","8b6ee003":"markdown","0dfbf526":"markdown","65f91a0c":"markdown","02c2d092":"markdown","0654c167":"markdown","401d7a9e":"markdown","428c185f":"markdown","02fa5be6":"markdown","ca45348c":"markdown","6af5c49d":"markdown","972236ce":"markdown","93fb7c5d":"markdown","169a8634":"markdown","a984bfc9":"markdown","810cc19d":"markdown","6ecfc06a":"markdown","385df861":"markdown","0e7d083f":"markdown","f37680f4":"markdown","83eaee93":"markdown"},"source":{"a7f9f247":"# Core\nimport pandas as pd\nimport numpy as np\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9a81d992":"# Path of the file to read.\nfile_path = '..\/input\/train.csv'\n\n# Load data into a pandas DataFrame. Note: 1st column is ID\nhome_data = pd.read_csv(file_path, index_col=0)","4e5aabaf":"home_data.tail()\n# home_data.head()","ba15d8e0":"home_data.shape","ffa78b43":"# List of numerical attributes\nhome_data.select_dtypes(exclude=['object']).columns","57ad67fa":"len(home_data.select_dtypes(exclude='object').columns)","e02a9f79":"home_data.select_dtypes(exclude=['object']).describe().round(decimals=2)","5d3d29d9":"home_data.select_dtypes(include=['object']).columns","fc95aac9":"len(home_data.select_dtypes(include='object').columns)","8316620f":"home_data.select_dtypes(include=['object']).describe()","2d717777":"target = home_data.SalePrice\nplt.figure()\nsns.distplot(target)\nplt.title('Distribution of SalePrice')\nplt.show()","4f3371bc":"sns.distplot(np.log(target))\nplt.title('Distribution of Log-transformed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","031da909":"print('SalePrice has a skew of ' + str(target.skew().round(decimals=2)) + \n      ' while the log-transformed SalePrice improves the skew to ' + \n      str(np.log(target).skew().round(decimals=2)))","b7cb559d":"num_attributes = home_data.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\n\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(num_attributes.iloc[:,i].dropna())\n    plt.xlabel(num_attributes.columns[i])\n\nplt.tight_layout()\nplt.show()","0088d537":"fig = plt.figure(figsize=(12, 18))\n\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.boxplot(y=num_attributes.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","0433fda6":"f = plt.figure(figsize=(12,20))\n\nfor i in range(len(num_attributes.columns)):\n    f.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_attributes.iloc[:,i], target)\n    \nplt.tight_layout()\nplt.show()","648478a8":"correlation = home_data.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()\n\n## Heatmap with annotation of correlation values\n# sns.heatmap(home_data.corr(), annot=True)","407751e4":"correlation['SalePrice'].sort_values(ascending=False).head(15)","676b5f5e":"num_columns = home_data.select_dtypes(exclude='object').columns\ncorr_to_price = correlation['SalePrice']\nn_cols = 5\nn_rows = 8\nfig, ax_arr = plt.subplots(n_rows, n_cols, figsize=(16,20), sharey=True)\nplt.subplots_adjust(bottom=-0.8)\nfor j in range(n_rows):\n    for i in range(n_cols):\n        plt.sca(ax_arr[j, i])\n        index = i + j*n_cols\n        if index < len(num_columns):\n            plt.scatter(home_data[num_columns[index]], home_data.SalePrice)\n            plt.xlabel(num_columns[index])\n            plt.title('Corr to SalePrice = '+ str(np.around(corr_to_price[index], decimals=3)))\nplt.show()","adb408fc":"# Show columns with most null values:\nnum_attributes.isna().sum().sort_values(ascending=False).head()","0f10df67":"cat_columns = home_data.select_dtypes(include='object').columns\nprint(cat_columns)","3a5c5a64":"var = home_data['KitchenQual']\nf, ax = plt.subplots(figsize=(10,6))\nsns.boxplot(y=home_data.SalePrice, x=var)\nplt.show()","2b6db64a":"f, ax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=home_data.SalePrice, x=home_data.Neighborhood)\nplt.xticks(rotation=40)\nplt.show()","51b1363b":"## Count of categories within Neighborhood attribute\nfig = plt.figure(figsize=(12.5,4))\nsns.countplot(x='Neighborhood', data=home_data)\nplt.xticks(rotation=90)\nplt.ylabel('Frequency')\nplt.show()","4aafc289":"home_data[cat_columns].isna().sum().sort_values(ascending=False).head(17)","6c4c1459":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","d8db1483":"# Create copy of dataset  ====================================\nhome_data_copy = home_data.copy()\n\n# Dealing with missing\/null values ===========================\n# Numerical columns:\nhome_data_copy.MasVnrArea = home_data_copy.MasVnrArea.fillna(0)\n# HOW TO TREAT LotFrontage - 259 missing values??\n\n# Categorical columns:\ncat_cols_fill_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                     'MasVnrType']\nfor cat in cat_cols_fill_none:\n    home_data_copy[cat] = home_data_copy[cat].fillna(\"None\")\n    ","e7a78d09":"# Check for outstanding missing\/null values\n# Scikit-learn's Imputer will be used to address these\nhome_data_copy.isna().sum().sort_values(ascending=False).head()","6b372a69":"# Remove outliers based on observations on scatter plots against SalePrice:\nhome_data_copy = home_data_copy.drop(home_data_copy['LotFrontage']\n                                     [home_data_copy['LotFrontage']>200].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['LotArea']\n                                     [home_data_copy['LotArea']>100000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['BsmtFinSF1']\n                                     [home_data_copy['BsmtFinSF1']>4000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['TotalBsmtSF']\n                                     [home_data_copy['TotalBsmtSF']>6000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['1stFlrSF']\n                                     [home_data_copy['1stFlrSF']>4000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy.GrLivArea\n                                     [(home_data_copy['GrLivArea']>4000) & \n                                      (target<300000)].index)\nhome_data_copy = home_data_copy.drop(home_data_copy.LowQualFinSF\n                                     [home_data_copy['LowQualFinSF']>550].index)\n","f153a1c7":"home_data_copy['SalePrice'] = np.log(home_data_copy['SalePrice'])\nhome_data_copy = home_data_copy.rename(columns={'SalePrice': 'SalePrice_log'})","f0769d29":"transformed_corr = home_data_copy.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(transformed_corr)","016b5e6e":"# Remove attributes that were identified for excluding when viewing scatter plots & corr values\nattributes_drop = ['SalePrice_log', 'MiscVal', 'MSSubClass', 'MoSold', 'YrSold', \n                   'GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd'] # high corr with other attributes\n\nX = home_data_copy.drop(attributes_drop, axis=1)\n\n# Create target object and call it y\ny = home_data_copy.SalePrice_log\n\n# One-hot-encoding to transform all categorical data\nX = pd.get_dummies(X)\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Normalisation - to be added later\n# normaliser = StandardScaler()\n# train_X = normaliser.fit_transform(train_X)\n# val_X = normaliser.transform(val_X)\n\n# Final imputation of missing data - to address those outstanding after previous section\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\nval_X = my_imputer.transform(val_X)","9837e6c8":"from sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","6298160f":"def inv_y(transformed_y):\n    return np.exp(transformed_y)\n\n# Series to collate mean absolute errors for each algorithm\nmae_compare = pd.Series()\nmae_compare.index.name = 'Algorithm'\n\n# Specify Model ================================\n# iowa_model = DecisionTreeRegressor(random_state=1)\n# # Fit Model\n# iowa_model.fit(train_X, train_y)\n\n# # Make validation predictions and calculate mean absolute error\n# val_predictions = iowa_model.predict(val_X)\n# val_mae = mean_absolute_error(inv_y(val_predictions), inv_y(val_y))\n# mae_compare['DecisionTree'] = val_mae\n# # print(\"Validation MAE for Decision Tree when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Decision Tree. Using best value for max_leaf_nodes ==============\n# iowa_model = DecisionTreeRegressor(max_leaf_nodes=90, random_state=1)\n# iowa_model.fit(train_X, train_y)\n# val_predictions = iowa_model.predict(val_X)\n# val_mae = mean_absolute_error(inv_y(val_predictions), inv_y(val_y))\n# mae_compare['DecisionTree_opt_max_leaf_nodes'] = val_mae\n# # print(\"Validation MAE for Decision Tree with best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Random Forest. Define the model. =============================\nrf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(inv_y(rf_val_predictions), inv_y(val_y))\n\nmae_compare['RandomForest'] = rf_val_mae\n# print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n\n\n# XGBoost. Define the model. ======================================\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n              eval_set=[(val_X,val_y)], verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(inv_y(xgb_val_predictions), inv_y(val_y))\n\nmae_compare['XGBoost'] = xgb_val_mae\n# print(\"Validation MAE for XGBoost Model: {:,.0f}\".format(xgb_val_mae))\n\n# Linear Regression =================================================\nlinear_model = LinearRegression()\nlinear_model.fit(train_X, train_y)\nlinear_val_predictions = linear_model.predict(val_X)\nlinear_val_mae = mean_absolute_error(inv_y(linear_val_predictions), inv_y(val_y))\n\nmae_compare['LinearRegression'] = linear_val_mae\n# print(\"Validation MAE for Linear Regression Model: {:,.0f}\".format(linear_val_mae))\n\n# Lasso ==============================================================\nlasso_model = Lasso(alpha=0.0005, random_state=5)\nlasso_model.fit(train_X, train_y)\nlasso_val_predictions = lasso_model.predict(val_X)\nlasso_val_mae = mean_absolute_error(inv_y(lasso_val_predictions), inv_y(val_y))\n\nmae_compare['Lasso'] = lasso_val_mae\n# print(\"Validation MAE for Lasso Model: {:,.0f}\".format(lasso_val_mae))\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002, random_state=5)\nridge_model.fit(train_X, train_y)\nridge_val_predictions = ridge_model.predict(val_X)\nridge_val_mae = mean_absolute_error(inv_y(ridge_val_predictions), inv_y(val_y))\n\nmae_compare['Ridge'] = ridge_val_mae\n# print(\"Validation MAE for Ridge Regression Model: {:,.0f}\".format(ridge_val_mae))\n\n# ElasticNet ===========================================================\nelastic_net_model = ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7)\nelastic_net_model.fit(train_X, train_y)\nelastic_net_val_predictions = elastic_net_model.predict(val_X)\nelastic_net_val_mae = mean_absolute_error(inv_y(elastic_net_val_predictions), inv_y(val_y))\n\nmae_compare['ElasticNet'] = elastic_net_val_mae\n# print(\"Validation MAE for Elastic Net Model: {:,.0f}\".format(elastic_net_val_mae))\n\n# KNN Regression ========================================================\n# knn_model = KNeighborsRegressor()\n# knn_model.fit(train_X, train_y)\n# knn_val_predictions = knn_model.predict(val_X)\n# knn_val_mae = mean_absolute_error(inv_y(knn_val_predictions), inv_y(val_y))\n\n# mae_compare['KNN'] = knn_val_mae\n# # print(\"Validation MAE for KNN Model: {:,.0f}\".format(knn_val_mae))\n\n# Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, \n                                      max_depth=4, random_state=5)\ngbr_model.fit(train_X, train_y)\ngbr_val_predictions = gbr_model.predict(val_X)\ngbr_val_mae = mean_absolute_error(inv_y(gbr_val_predictions), inv_y(val_y))\n\nmae_compare['GradientBoosting'] = gbr_val_mae\n# print(\"Validation MAE for Gradient Boosting Model: {:,.0f}\".format(gbr_val_mae))\n\n# # Ada Boost Regression ================================================\n# ada_model = AdaBoostRegressor(n_estimators=300, learning_rate=0.05, random_state=5)\n# ada_model.fit(train_X, train_y)\n# ada_val_predictions = ada_model.predict(val_X)\n# ada_val_mae = mean_absolute_error(inv_y(ada_val_predictions), inv_y(val_y))\n\n# mae_compare['AdaBoost'] = ada_val_mae\n# # print(\"Validation MAE for Ada Boost Model: {:,.0f}\".format(ada_val_mae))\n\n# # Support Vector Regression ===========================================\n# svr_model = SVR(kernel='linear')\n# svr_model.fit(train_X, train_y)\n# svr_val_predictions = svr_model.predict(val_X)\n# svr_val_mae = mean_absolute_error(inv_y(svr_val_predictions), inv_y(val_y))\n\n# mae_compare['SVR'] = svr_val_mae\n# print(\"Validation MAE for SVR Model: {:,.0f}\".format(svr_val_mae))\n\nprint('MAE values for different algorithms:')\nmae_compare.sort_values(ascending=True).round()","959fe765":"from sklearn.model_selection import cross_val_score\n\nimputer = SimpleImputer()\nimputed_X = imputer.fit_transform(X)\nn_folds = 10\n\n# =========================================================================\nscores = cross_val_score(lasso_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nlasso_mae_scores = np.sqrt(-scores)\n\nprint('For LASSO model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(lasso_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(lasso_mae_scores.std().round(decimals=3)))","c3baf429":"scores = cross_val_score(gbr_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\ngbr_mae_scores = np.sqrt(-scores)\n\nprint('For Gradient Boosting model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(gbr_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(gbr_mae_scores.std().round(decimals=3)))","55f2268e":"scores = cross_val_score(xgb_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For XGBoost model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","476ffecf":"scores = cross_val_score(rf_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For Random Forest model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","418743aa":"# Grid search for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# # Tuning XGBoost\n# param_grid = [{'n_estimators': [1000, 1500], \n#                'learning_rate': [0.01, 0.03] }]\n# #               'max_depth': [3, 6, 9]}]\n\n# top_reg = XGBRegressor()\n\n# Tuning Lasso\nparam_grid = [{'alpha': [0.0007, 0.0005, 0.005]}]\ntop_reg = Lasso()\n\n# -------------------------------------------------------\ngrid_search = GridSearchCV(top_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error')\n\ngrid_search.fit(imputed_X, y)\n\ngrid_search.best_params_","5517cb19":"# path to file you will use for predictions\ntest_data_path = '..\/input\/test.csv'\n\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)","4600bf17":"# create test_X which to perform all previous pre-processing on\ntest_X = test_data.copy()\n\n# Repeat treatments for missing\/null values =====================================\n# Numerical columns:\ntest_X.MasVnrArea = test_X.MasVnrArea.fillna(0)\n\n# Categorical columns:\nfor cat in cat_cols_fill_none:\n    test_X[cat] = test_X[cat].fillna(\"None\")\n\n# Repeat dropping of chosen attributes ==========================================\nif 'SalePrice_log' in attributes_drop:\n    attributes_drop.remove('SalePrice_log')\n\ntest_X = test_data.drop(attributes_drop, axis=1)\n\n# One-hot encoding for categorical data =========================================\ntest_X = pd.get_dummies(test_X)\n\n\n# ===============================================================================\n# Ensure test data is encoded in the same manner as training data with align command\nfinal_train, final_test = X.align(test_X, join='left', axis=1)\n\n# Imputer for all other missing values in test data. Note: Do not 'fit_transform'\nfinal_test_imputed = my_imputer.transform(final_test)\n","e8dd52dc":"# Create model - on full set of data (training & validation)\n# Best model = Lasso?\nfinal_model = Lasso(alpha=0.0005, random_state=5)\n# final_model = XGBRegressor(n_estimators=1500, learning_rate=0.03)\nfinal_train_imputed = my_imputer.fit_transform(final_train)\n\n# Fit the model using all the data - train it on all of X and y\nfinal_model.fit(final_train_imputed, y)","bb5f7b95":"# make predictions which we will submit. \ntest_preds = final_model.predict(final_test_imputed)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\n# Reminder: predictions are in log(SalePrice). Need to inverse-transform.\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': inv_y(test_preds)})\n\noutput.to_csv('submission.csv', index=False)","d9786be4":"<a id='transform_skew'><\/a>","4231519a":"### Import modules","7a9dd841":"### Assess correlations amongst attributes\nThe linear correlation between two columns of data is shown below. There are various correlation calculation methods, but the Pearson correlation is often used and is the default method. It may be useful to note that:\n1. A combination of the correlation figure and a scatter plot can support the understanding of whether there is a non-linear correlation (i.e. depending on the data, this may result in a low value of linear correlation, but the variables may still be strongly correlated in a non-linear fashion)\n2. Correlation values may be heavily influenced by single outliers! \n<br><br>\n\nSeveral authors have suggested that _\"to use linear regression for modelling, it is necessary to remove correlated variables to improve your model\"_, and _\"it's a good practice to remove correlated variables during feature selection\"_<br><br>\n\nBelow is a heatmap of the correlation of the numerical columns:","a60bb9e7":"<a id='top'><\/a>","f62ffc14":"### Import machine learning modules","7e4d35c3":"### Reading in input file","8a04d7fe":"<a id='feature_eng'><\/a>","b41d1c3f":"## 2.3 Transforming data to reduce skew\nFor the moment, this is restricted to the target variable.\n[Back to contents](#top)","ceb2b218":"__Notes for Data Cleaning & Preprocessing:__\n* Perform log transformation on SalePrice\n* Feature variables that are skewed should also be investigated to assess whether they require transformation","c78d884e":"# Introduction & Approach Taken\nThis kernel represents my first personal exploration of an (almost) end-to-end data science process. While starting off as a kernel to complete the Kaggle Learn Machine Learning exercise, it has now been significantly modified to serve a broader learning experience for myself. If you find any errors, lapses in logic, or just have advice for me, please do post them. Any feedback would certainly be appreciated. <br>\n\nThe challenge is based on the Kaggle Housing Prices Competition that provides a dataset with different house attributes together with the price label. The aim is to develop a model capable of predicting the house price based on the data available.\n\nDifferent data scientists espouse various approaches for the process of applied data science. The following is an adaptation of some that I have come across, which shall form the structure of this kernel and my approach taken for this particular competition:\n\n[1. Exploratory Data Analysis](#explore) <br>\n> [1.1 Preliminary observations](#prelim_explore) <br>\n   [1.2 Exploring numerical attributes](#explore_num_columns) <br>\n   [1.3 Exploring categorical attributes](#explore_cat_columns)  <br>\n\n[2. Data Cleaning & Preprocessing](#data_clean)\n> [2.1 Dealing with missing\/null values](#fix_nans) <br>\n   [2.2 Addressing outliers](#address_outliers)<br>\n   [2.3 Transforming data to reduce skew](#transform_skew) <br>\n\n[3. Feature Selection & Engineering](#feature_eng)\n\n[4. Preliminary Assessment of Machine Learning Algorithms](#algorithms)\n\n[5. Selection of Best Algorithm(s) and Fine-Tuning](#fine_tune)\n","0f1d6d7f":"<a id='fix_nans'><\/a>","cfe1aa91":"# 1. Exploratory Data Analysis\nAmongst others, the data is explored to:\n1. Gain a preliminary understanding of available data\n2. Check for missing or null values\n3. Find potential outliers\n4. Assess correlations amongst attributes\/features\n5. Check for data skew\n\n[Back to contents](#top)","9a4b446a":"### Cross-validation\nUse scikit-learn's cross_val_score to try K-fold cross-validation. ","5ea54ba5":"__Notes for Data Cleaning & Preprocessing__ <br>\nBased on a first viewing of the scatter plots against SalePrice, there appears to be:\n* A few outliers on the LotFrontage (say, >200) and LotArea (>100000) data.\n* BsmtFinSF1 (>4000) and TotalBsmtSF (>6000)\n* 1stFlrSF (>4000)\n* GrLivArea (>4000 AND SalePrice <300000)\n* LowQualFinSF (>550) <br>\n\nReference: [quick link to the implementation](#address_outliers) <br>","6a1fcb07":"### Missing\/null values in categorical columns\n\n[Back to contents](#top)","b34e6aef":"<a id='data_clean'><\/a>","db683d3c":"__Notes for Data Cleaning & Processing:__\n* Not yet clear what to do with LotFrontage missing values. Simple imputation with median? LotFrontage correlation with Neigborhood?\n* GarageYrBlt is highly correlated with YearBuilt, and as an after-note, it is discarded before the machine learning step. Hence no action required.\n* MasVnrArea has 8 missing values, the same number as missing MasVnrType values. Likely not to have masonry veneer. Hence, fill with 0 ","df05e071":"<a id='fine_tune'><a>","0f7b09a7":"## 1.1 Preliminary observations\nExamples from the dataset are shown below. <br><br>\n[Back to contents](#top)","9aa61c4b":"### Numerical columns within the dataset","420f4ad6":"### Skew of target column\nIt appears to be good practice to minimise the skew of the dataset. The reason often given is that skewed data adversely affects the prediction accuracy of regression models. <br>\nNote: While important for linear regression, correcting skew is not necessary for Decisions Trees and Random Forests.","34e7927a":"# 5. Selection of Best Algorithm(s) & Fine-Tuning\n### Create a Model and Make Predictions for the Competition\nRead the file of \"test\" data. And apply best model to make predictions\n\n__Reminders:__\n* __Need to perform all transformations and normalisation, etc. to test data similar to when training the data__\n* __Make sure to inverse transform predictions to get predicted SalePrice__\n\n[Back to contents](#top)","249bc2f7":"### Finding Outliers\nVisualisation of data may support the discovery of possible outliers within the data. Examples of how this can be done include:\n1. Within univariate analysis, for example through using box plots. Outliers are observations more than a multiple (1.5-3) of the IQR (inter-quartile range) beyond the upper or lower quartile. (If data is skewed, it may be helpful to transform them first to a more symmetric distribution shape)\n2. Within bivariate analysis, for example scatterplots. Outliers have y-values that are unusual in relation to other observations with similar x-values. Alternatively, plots of the residuals from fitted least square line of bivariate regression can also indicate outliers.\n\nThe consensus is that all outliers should be carefully examined:\n* Go back to original data to check for recording or transcription errors\n* If no such errors, look carefully for unusual features of the individual unit to explain difference. This may lead to new theory\/discoveries \n* If data cannot be checked further, outlier is usually (often) dropped from the dataset.\n\nThe scatterplots of SalePrice against each numerical attribute is shown below, with the aim of employing method 2 above with bivariate analysis.\n\n[Back to contents](#top)","71e9bd80":"<a id='check_null'><\/a>","a236ace1":"<a id='explore_num_columns'><\/a>","54a85605":"__Notes for Feature Selection & Engineering:__\nBased on the scatter plots and correlation figures above, consider:\n* Excluding GarageArea - highly (0.88) correlated with GarageCars, which has a higher corr with Price\n* Excluding GarageYrBlt - highly (0.83) correlated with YearBuilt\n* Excluding all attributes with low corr with Price and unclear non-linear correlation - e.g. MSSubClass, MoSold, YrSold, MiscVal, BsmtFinSF2, BsmtUnfSF, LowQualFinSF?","89f57eeb":"### Create final model","ec67e225":"<a id='notes_outliers'><\/a>","6c47436e":"__Reminder: target is now log(SalePrice). After prediction call, need to inverse-transform to obtain SalePrice!__","299bb48b":"# 4. Preliminary Assessment of Machine Learning Algorithms\n[Back to contents](#top)","f14a1319":"### Repeat pre-processing defined previously","0c0c2809":"### Considering highly-correlated features\nFeeding highly-correlated features to machine algorithms may cause a reduction in performance. Hence, these are addressed below:","f526ff8f":"<a id='address_outliers' ><\/a>","61669a0a":"# 2. Data Cleaning & Preprocessing\n\n[Back to contents](#top)<br><br>","e4252340":"__Univariate analysis - box plots for numerical attributes__","17676951":"<a id='explore'><\/a>","214eda7e":"__Notes for Data Cleaning & Preprocessing:__\n* For the moment, assume that PoolQC to Bsmt attributes are missing as the houses do not have them (pools, basements, etc.). Hence, the missing values could be filled in with \"None\".\n* MasVnrType has 8 missing values, the same number as missing MasVnrArea values. Likely not to have masonry veneer. Hence, fill with 'None' ","c005bb56":"## 2.2 Addressing outliers\n[Back to contents](#top)<br>\nReference: [Quick link to notes](#notes_outliers)","847c4e69":"### Distributions of attributes","68df3960":"### Missing\/null values in numerical columns\n\n[Back to contents](#top)","8b6ee003":"# 3. Feature Selection & Engineering\n\n[Back to contents](#top)","0dfbf526":"Show scatter plots for each numerical attribute (again, but different, less-efficient code) and show correlation value:","65f91a0c":"## 1.2 Exploring numerical columns\n[Back to contents](#top)","02c2d092":"__Bivariate analysis - scatter plots for target versus numerical attributes__","0654c167":"### Perform feature selection, and encoding of categorical columns","401d7a9e":"# Future Study\n### Exploratory Data Analysis\n* How to assess outliers for categorical columns?\n* How to check for correlation amongst categorical columns?\n\n### Data Cleaning & Preprocessing\n* Add code to perform data normalisation\/standardisation.\n* Create custom scikit-learn preprocessing class so that this can be easily used later with pipelines.\n* Consider scikit-learn's PowerTransformer preprocessing module to fit and transform columns to be more Gaussian-like. \n\n### Feature Selection & Engineering\n* Think about possible combination of attributes to create useful new features, including polynomials.\n* Explore pros and cons of other methods to encode categorical attributes, besides one-hot encoding\n* Consider further advanced preprocessing techniques, such as Principal Components Analysis for dimensionality reduction, to create new features\n\n### Preliminary Assessment of ML Algorithms\n* Use scikit-learn's pipelines!\n* Learn how best to utilise ensemble methods.\n* Explore choices for performance criteria.\n* Plots to diagnose bias vs. variance, and learning curves\n\n### Selection of Best Algorithm(s) & Fine-tuning\n* Test newly engineered features as hyperparameters in grid-search cross-validation.\n* Partial-dependence plots for understanding the final model better.\n\n[Back to contents](#top)\n\n\n","428c185f":"With reference to the target SalePrice, the top correlated attributes are:","02fa5be6":"<a id='algorithms'><\/a>","ca45348c":"## 1.3 Exploring categorical columns\n[Back to contents](#top)\n### Examples of box plots of SalePrice versus categorical values","6af5c49d":"The above shows that there appears to be 37 numerical columns, including the target, SalePrice. <br>\nNotes: <br>\n* Some potential anomalies in certain rows of the dataset may cause the column data type to become an 'object'. This may lead to an error in distinguishing between numerical and categorical columns. How can this be checked efficiently?\n* It is possible that there are numerical columns that have data in the form of discrete, and limited number of values. Such columns may also be interpreted as categorical data.\n<br>\n\nThe 37 numerical columns have the following general characteristics:","972236ce":"Highly-correlated attributes include (left attribute has higher correlation with SalePrice_log):\n* GarageCars and GarageArea (0.882)\n* YearBuilt and GarageYrBlt (0.826)\n* GrLivArea_log1p and TotRmsAbvGrd (0.826)\n* TotalBsmtSF and 1stFlrSF_log1p (0.780)\n\nPerhaps choose to drop the column with the lower correlation against SalePrice_log from the above pairs with more than 0.8 correlation.","93fb7c5d":"There are 43 categorical columns, with the following characteristics:","169a8634":"### Categorical columns within the dataset","a984bfc9":"The \"shape\" of the dataset shows that it has 1460 rows\/instances, with data from 80 attributes. <br>\nOut of the 80 attributes, one is the target (SalePrice) that the model should predict. <br>\nHence, there are 79 attributes that may be used for feature selection\/engineering.","810cc19d":"__Notes for Data Cleaning & Preprocessing:__ <br>\nUni-modal, skewed distributions could potentially be log transformed: <br>\n> LotFrontage, LotArea, 1stFlrSF, GrLivArea, OpenPorchSF<br>\n\nAfter-note: This will be a future addition.","6ecfc06a":"<a id='prelim_explore'><\/a>","385df861":"### Use scikit-learn's function to grid search for the best combination of hyperparameters","0e7d083f":"### Make predictions for submission","f37680f4":"<a id='explore_cat_columns'><\/a>","83eaee93":"## 2.1 Dealing with missing\/null values\n[Back to contents](#top)"}}