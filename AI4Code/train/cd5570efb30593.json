{"cell_type":{"8477d17b":"code","efc87060":"code","3ad84771":"code","34663bf4":"code","4b6e61cf":"code","73e67abf":"code","53621c6e":"code","5600da2e":"code","dfe79ccc":"code","657e2267":"code","49ceba94":"code","cd7b8a34":"code","6cd58ab6":"code","e2941f70":"code","eb7450c1":"code","fd7107bd":"code","2b714481":"code","6b818c3a":"code","54030ab4":"code","7f1d68fb":"code","d0f375a3":"code","c8b43aeb":"code","f47f4afc":"code","f1747346":"code","3621f641":"code","af1a9f9f":"code","800ab30f":"code","d659900c":"code","3b4833cc":"code","62345479":"code","6bd17875":"code","45001b23":"code","35572e13":"code","241d1216":"code","ed944810":"code","14603e87":"code","0e29585f":"code","7c6997fb":"code","6c486de9":"code","b8ef0b32":"markdown","08b71d5a":"markdown","a154e13b":"markdown","43461e7d":"markdown","a9682e3e":"markdown","d651a3b2":"markdown","f024e75c":"markdown","88d992c3":"markdown","7f87ee2e":"markdown","74872a6a":"markdown","080ab3c0":"markdown","628afede":"markdown","5edfd60e":"markdown","5e48a1ed":"markdown","3b64a355":"markdown","6d789c25":"markdown","8aee6f4b":"markdown","3a6f120b":"markdown","6db26102":"markdown","93dda092":"markdown"},"source":{"8477d17b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","efc87060":"# importing libraries and magic functions\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n%config InlineBackend.figure_format ='retina'\n%matplotlib inline","3ad84771":"# read data\ndf = pd.read_csv('\/kaggle\/input\/framingham-heart-study-dataset\/framingham.csv')\n\n# first glimpse at data\ndf.head(20)\n\n# data shape\ndf.shape\n\n# data types\ndf.dtypes","34663bf4":"# check for dupicates\nduplicate_df = df[df.duplicated()]\nduplicate_df","4b6e61cf":"# checking for missing values\ndf.isna().sum()\nnull = df[df.isna().any(axis=1)]\nnull","73e67abf":"# checking distributions using histograms\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndf.hist(ax = ax)","53621c6e":"# checking which features are correlated with each other and are correlated with the outcome variable\ndf_corr = df.corr()\nsns.heatmap(df_corr)","5600da2e":"# Dropping columns education and glucose\ndf = df.drop(['education'], axis=1)","dfe79ccc":"# Checking for more missing data \ndf.isna().sum()","657e2267":"# Dropping all rows with missing data\ndf = df.dropna()\ndf.isna().sum()\ndf.columns","49ceba94":"# Identify the features with the most importance for the outcome variable Heart Disease\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# separate independent & dependent variables\nX = df.iloc[:,0:14]  #independent columns\ny = df.iloc[:,-1]    #target column i.e price range\n\n# apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(11,'Score'))  #print 10 best features","cd7b8a34":"featureScores = featureScores.sort_values(by='Score', ascending=False)\nfeatureScores","6cd58ab6":"# visualizing feature selection\nplt.figure(figsize=(20,5))\nsns.barplot(x='Specs', y='Score', data=featureScores, palette = \"GnBu_d\")\nplt.box(False)\nplt.title('Feature importance', fontsize=16)\nplt.xlabel('\\n Features', fontsize=14)\nplt.ylabel('Importance \\n', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","e2941f70":"# selecting the 10 most impactful features for the target variable\nfeatures_list = featureScores[\"Specs\"].tolist()[:10]\nfeatures_list","eb7450c1":"# Create new dataframe with selected features\n\ndf = df[['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male','TenYearCHD']]\ndf.head()","fd7107bd":"# Checking correlation again\ndf_corr = df.corr()\nsns.heatmap(df_corr)","2b714481":"# Checking for outliers\ndf.describe()\nsns.pairplot(df)","6b818c3a":"# Zooming into cholesterin outliers\n\nsns.boxplot(df.totChol)\noutliers = df[(df['totChol'] > 500)] \noutliers","54030ab4":"# Dropping 2 outliers in cholesterin\ndf = df.drop(df[df.totChol > 599].index)\nsns.boxplot(df.totChol)","7f1d68fb":"df_clean = df","d0f375a3":"scaler = MinMaxScaler(feature_range=(0,1)) \n\n#assign scaler to column:\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_clean), columns=df_clean.columns)\n","c8b43aeb":"df_scaled.describe()\ndf.describe()","f47f4afc":"# clarify what is y and what is x label\ny = df_scaled['TenYearCHD']\nX = df_scaled.drop(['TenYearCHD'], axis = 1)\n\n# divide train test: 80 % - 20 %\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=29)","f1747346":"len(X_train)\nlen(X_test)","3621f641":"# Checking balance of outcome variable\ntarget_count = df_scaled.TenYearCHD.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\nsns.countplot(df_scaled.TenYearCHD, palette=\"OrRd\")\nplt.box(False)\nplt.xlabel('Heart Disease No\/Yes',fontsize=11)\nplt.ylabel('Patient Count',fontsize=11)\nplt.title('Count Outcome Heart Disease\\n')\nplt.savefig('Balance Heart Disease.png')\nplt.show()","af1a9f9f":"# Shuffle df\nshuffled_df = df_scaled.sample(frac=1,random_state=4)\n\n# Put all the fraud class in a separate dataset.\nCHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n\n#Randomly select 492 observations from the non-fraud (majority class)\nnon_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=611,random_state=42)\n\n# Concatenate both dataframes again\nnormalized_df = pd.concat([CHD_df, non_CHD_df])\n\n# check new class counts\nnormalized_df.TenYearCHD.value_counts()\n\n# plot new count\nsns.countplot(normalized_df.TenYearCHD, palette=\"OrRd\")\nplt.box(False)\nplt.xlabel('Heart Disease No\/Yes',fontsize=11)\nplt.ylabel('Patient Count',fontsize=11)\nplt.title('Count Outcome Heart Disease after Resampling\\n')\n#plt.savefig('Balance Heart Disease.png')\nplt.show()","800ab30f":"y_train = normalized_df['TenYearCHD']\nX_train = normalized_df.drop('TenYearCHD', axis=1)\n\nfrom sklearn.pipeline import Pipeline\n\nclassifiers = [LogisticRegression(),SVC(),DecisionTreeClassifier(),KNeighborsClassifier(2)]\n\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('classifier', classifier)])\n    pipe.fit(X_train, y_train)   \n    print(\"The accuracy score of {0} is: {1:.2f}%\".format(classifier,(pipe.score(X_test, y_test)*100)))\n","d659900c":"# logistic regression again with the balanced dataset\n\nnormalized_df_reg = LogisticRegression().fit(X_train, y_train)\n\nnormalized_df_reg_pred = normalized_df_reg.predict(X_test)\n\n# check accuracy: Accuracy:\u00a0Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_reg_pred)\nprint(f\"The accuracy score for LogReg is: {round(acc,3)*100}%\")\n\n# f1 score: The\u00a0F1 score\u00a0can be interpreted as a weighted average of the precision and recall, where an\u00a0F1 score\u00a0reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_reg_pred)\nprint(f\"The f1 score for LogReg is: {round(f1,3)*100}%\")\n\n# Precision score:\u00a0When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_reg_pred)\nprint(f\"The precision score for LogReg is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall):\u00a0When it\u2019s actually yes, how often does it predict yes? True Positive Rate =\u00a0True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_reg_pred)\nprint(f\"The recall score for LogReg is: {round(recall,3)*100}%\")","3b4833cc":"# plotting confusion matrix LogReg\n\ncnf_matrix_log = confusion_matrix(y_test, normalized_df_reg_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix_log), annot=True,cmap=\"Reds\" , fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix Logistic Regression\\n', y=1.1)","62345479":"# Support Vector Machine\n\n#initialize model\nsvm = SVC()\n\n#fit model\nsvm.fit(X_train, y_train)\n\nnormalized_df_svm_pred = svm.predict(X_test)\n\n# check accuracy: Accuracy:\u00a0Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_svm_pred)\nprint(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n\n# f1 score: The\u00a0F1 score\u00a0can be interpreted as a weighted average of the precision and recall, where an\u00a0F1 score\u00a0reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_svm_pred)\nprint(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n\n# Precision score:\u00a0When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_svm_pred)\nprint(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall):\u00a0When it\u2019s actually yes, how often does it predict yes? True Positive Rate =\u00a0True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_svm_pred)\nprint(f\"The recall score for SVM is: {round(recall,3)*100}%\")\n","6bd17875":"# plotting confusion matrix SVM\n\ncnf_matrix_svm = confusion_matrix(y_test, normalized_df_svm_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix_svm), annot=True,cmap=\"Reds\" , fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix SVM\\n', y=1.1)","45001b23":"# Decision Tree\n\n#initialize model\ndtc_up = DecisionTreeClassifier()\n\n# fit model\ndtc_up.fit(X_train, y_train)\n\nnormalized_df_dtc_pred = dtc_up.predict(X_test)\n\n# check accuracy: Accuracy:\u00a0Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_dtc_pred)\nprint(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n\n# f1 score: The\u00a0F1 score\u00a0can be interpreted as a weighted average of the precision and recall, where an\u00a0F1 score\u00a0reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_dtc_pred)\nprint(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n\n# Precision score:\u00a0When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_dtc_pred)\nprint(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall):\u00a0When it\u2019s actually yes, how often does it predict yes? True Positive Rate =\u00a0True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_dtc_pred)\nprint(f\"The recall score for DTC is: {round(recall,3)*100}%\")","35572e13":"# plotting confusion matrix Decision Tree\n\ncnf_matrix_dtc = confusion_matrix(y_test, normalized_df_dtc_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix_dtc), annot=True,cmap=\"Reds\" , fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix Decision Tree\\n', y=1.1)\n","241d1216":"# KNN Model\n\n#initialize model\nknn = KNeighborsClassifier(n_neighbors = 2)\n\n#fit model\nknn.fit(X_train, y_train)\n\n# prediction = knn.predict(x_test)\nnormalized_df_knn_pred = knn.predict(X_test)\n\n\n# check accuracy: Accuracy:\u00a0Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_knn_pred)\nprint(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n\n# f1 score: The\u00a0F1 score\u00a0can be interpreted as a weighted average of the precision and recall, where an\u00a0F1 score\u00a0reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_knn_pred)\nprint(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n\n# Precision score:\u00a0When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_knn_pred)\nprint(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall):\u00a0When it\u2019s actually yes, how often does it predict yes? True Positive Rate =\u00a0True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_knn_pred)\nprint(f\"The recall score for KNN is: {round(recall,3)*100}%\")","ed944810":"# Check overfit of the KNN model\n# accuracy test and train\nacc_test = knn.score(X_test, y_test)\nprint(\"The accuracy score of the test data is: \",acc_test*100,\"%\")\nacc_train = knn.score(X_train, y_train)\nprint(\"The accuracy score of the training data is: \",round(acc_train*100,2),\"%\")\n\n","14603e87":"# Perform cross validation\n'''Cross Validation is used to assess the predictive performance of the models and and to judge \nhow they perform outside the sample to a new data set'''\n\ncv_results = cross_val_score(knn, X, y, cv=5) \n\nprint (\"Cross-validated scores:\", cv_results)\nprint(\"The Accuracy of Model with Cross Validation is: {0:.2f}%\".format(cv_results.mean() * 100))","0e29585f":"# plotting confusion matrix KNN\n\ncnf_matrix_knn = confusion_matrix(y_test, normalized_df_knn_pred)\n\nax= plt.subplot()\nsns.heatmap(pd.DataFrame(cnf_matrix_knn), annot=True,cmap=\"Reds\" , fmt='g')\n\nax.set_xlabel('Predicted ');ax.set_ylabel('True'); \n","7c6997fb":"# AU ROC CURVE KNN\n'''the AUC ROC Curve is a measure of performance based on plotting the true positive and false positive rate \nand calculating the area under that curve.The closer the score to 1 the better the algorithm's ability to \ndistinguish between the two outcome classes.'''\n\nfpr, tpr, _ = roc_curve(y_test, normalized_df_knn_pred)\nauc = roc_auc_score(y_test, normalized_df_knn_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.box(False)\nplt.title ('ROC CURVE KNN')\nplt.show()\n\nprint(f\"The score for the AUC ROC Curve is: {round(auc,3)*100}%\")","6c486de9":"def start_questionnaire():\n    my_predictors = []\n    parameters=['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male']\n    \n    print('Input Patient Information:')\n    \n    age = input(\"Patient's age: >>> \") \n    my_predictors.append(age)\n    male = input(\"Patient's gender. male=1, female=0: >>> \") \n    my_predictors.append(male)\n    cigsPerDay = input(\"Patient's smoked cigarettes per day: >>> \") \n    my_predictors.append(cigsPerDay)\n    sysBP = input(\"Patient's systolic blood pressure: >>> \") \n    my_predictors.append(sysBP)\n    diaBP = input(\"Patient's diastolic blood pressure: >>> \")\n    my_predictors.append(diaBP)\n    totChol = input(\"Patient's cholesterin level: >>> \") \n    my_predictors.append(totChol)\n    prevalentHyp = input(\"Was Patient hypertensive? Yes=1, No=0 >>> \") \n    my_predictors.append(prevalentHyp)\n    diabetes = input(\"Did Patient have diabetes? Yes=1, No=0 >>> \") \n    my_predictors.append(diabetes)\n    glucose = input(\"What is the Patient's glucose level? >>> \") \n    my_predictors.append(diabetes)\n    BPMeds = input(\"Has Patient been on Blood Pressure Medication? Yes=1, No=0 >>> \")\n    my_predictors.append(BPMeds)\n    \n    my_data = dict(zip(parameters, my_predictors))\n    my_df = pd.DataFrame(my_data, index=[0])\n    scaler = MinMaxScaler(feature_range=(0,1)) \n   \n    # assign scaler to column:\n    my_df_scaled = pd.DataFrame(scaler.fit_transform(my_df), columns=my_df.columns)\n    my_y_pred = knn.predict(my_df)\n    print('\\n')\n    print('Result:')\n    if my_y_pred == 1:\n        print(\"The patient will develop a Heart Disease.\")\n    if my_y_pred == 0:\n        print(\"The patient will not develop a Heart Disease.\")\n        \nstart_questionnaire()","b8ef0b32":"### 2. SVM","08b71d5a":"We can see that the proportion is 5.57:1 which is not well balanced.\nOne of the major issues when dealing with unbalanced datasets relates to the metrics used to evaluate a model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.","a154e13b":"### UNDERSAMPLING METHOD","43461e7d":"## Applying the model <a name=\"paragraph7\"><\/a>","a9682e3e":"Undersampling aims to decrease the number of instances from the overrepresented class in the data set. In our case, these techniques will decrease the number of fraudulent transactions in our data to approximately 50:50. If we do not balance the number of instances, most classification algorithms will heavily focus on the majority class. As a result, it might seem like your algorithm is achieving superb results when, in reality, it is simply always predicting the majority class.\n\nThe easiest way to do so is to randomly select observations from the majority class and remove them from the data set until we achieve a balance between the majority and minority class.\n","d651a3b2":"> ## **Model Pipeline** <a name=\"paragraph5\"><\/a>","f024e75c":"### 1. Logistic Regression","88d992c3":"### 3. Decision Tree","7f87ee2e":"## Feature Scaling <a name=\"paragraph2\"><\/a>\nSince we want to try out different models, and also these that use distance as a measure, we will scale our features.","74872a6a":"We will only keep those features that have the strongest relationship with the output variable. These features are:\n- Systolic Blood Pressure\n- Glucose\n- Age\n- Cholesterin\n- Cigarettes per Day\n- Diastolic Blood Pressure\n- Hypertensive\n- Diabetes\n- Blood Pressure Medication\n- Gender","080ab3c0":"## Exploratory Data Analysis *<a name=\"paragraph1\"><\/a>*","628afede":"## Resampling imbalanced Dataset <a name=\"paragraph4\"><\/a>","5edfd60e":"### 4. KNN","5e48a1ed":"## Test - Train Split <a name=\"paragraph3\"><\/a>","3b64a355":"# Heart Disease Prediction\n### Will a patient have a 10 year risk of developing a cardio vascular diseases?\n\n### Table of contents\n1. [Introduction](#introduction)\n2. [Exploratory Data Analysis](#paragraph1)\n3. [Feature Selection](#paragraph2)\n4. [Feature Scaling](#paragraph3)\n5. [Test - Train Split](#paragraph4)\n6. [Resampling](#paragraph5)\n7. [Model Pipeline](#paragraph6)\n8. [Modelling & Evaluation](#paragraph7)\n9. [Apply model](#paragraph8)\n\n## Introduction <a name=\"introduction\"><\/a>\n\n**Problem:**\nThe World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant\/risk factors of heart disease as well as predict the overall risk using logistic regression Data Preparation\n\n**Source:**\nThe dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes. Variables Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n\n**Attributes:**\n\n#### Demographic: \n* Sex: male or female(Nominal) \n* Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous) \n* Education: no further information provided\n\n#### Behavioral: \n* Current Smoker: whether or not the patient is a current smoker (Nominal) \n* Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.) \n\n#### Information on medical history: \n* BP Meds: whether or not the patient was on blood pressure medication (Nominal) \n* Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal) \n* Prevalent Hyp: whether or not the patient was hypertensive (Nominal) \n* Diabetes: whether or not the patient had diabetes (Nominal) \n\n#### Information on current medical condition: \n* Tot Chol: total cholesterol level (Continuous) \n* Sys BP: systolic blood pressure (Continuous) \n* Dia BP: diastolic blood pressure (Continuous) \n* BMI: Body Mass Index (Continuous) \n* Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.) \n* Glucose: glucose level (Continuous) \n\n#### Target variable to predict: \n* 10 year risk of coronary heart disease (CHD) - (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n","6d789c25":"**Conclusions from Heatmap:**\n\nWe are dropping the column *education* because a doctor would have to decide on which education level to put a patient and this could result in very subjective outcomes and it is also not very handy to put in practice.\n\nThe two features are not correlated to the outcome variable. In that case we would have kept them.","8aee6f4b":"**The scores for test and training data for the KNN model are similar. Therefore we do not expect the model to overfit.**","3a6f120b":"## Feature Selection *<a name=\"paragraph2\"><\/a>*","6db26102":"### Result: The KNN model has the highest accuracy score","93dda092":"## Modelling & Evaluation (without Pipeline) <a name=\"paragraph6\"><\/a>"}}