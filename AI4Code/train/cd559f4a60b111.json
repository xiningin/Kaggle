{"cell_type":{"c77c1840":"code","b54e4538":"code","dc6987ea":"code","5a042eac":"code","8821d097":"code","ad029b3f":"code","52e5360c":"code","4fdb28aa":"code","84c8630f":"code","9b742e6f":"code","4fbc6f0b":"code","c51e7898":"code","c1b95876":"code","478a4254":"code","3806cde7":"code","9bc2b2d2":"markdown","5e87e316":"markdown","87245d97":"markdown","1e3504c2":"markdown","25a65ed3":"markdown","644faf81":"markdown","e22d5f6a":"markdown","6273dd09":"markdown","efa23ee0":"markdown","ea02a99e":"markdown","5ecef693":"markdown"},"source":{"c77c1840":"# Importing required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score","b54e4538":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dc6987ea":"# Let's load the data\n\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","5a042eac":"# We'll sepearte features and labels now\nX = train_df.drop(['label'], axis = 1)\ny = train_df['label']\n\n# And now, normalize the features\nX = X\/255.0\ntest_df = test_df\/255.0\n\n# Converting the data to np.array\nX = X.values\ntest_df = test_df.values","8821d097":"print(X.shape, y.shape)","ad029b3f":"# Let's split the available dataset into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 89)\n\nprint(X_train.shape, X_val.shape)\nprint(y_train.shape, y_val.shape)","52e5360c":"# Let's visualize some samples\ndef drawImg(sample):\n    sample = sample.reshape((28,28))\n    plt.imshow(sample, cmap = 'gray')\n\nimg = X_train[10]\ndrawImg(img)","4fdb28aa":"# Function to calculate the Euclidean distance between two points\n\ndef dist(x1, x2):\n    return np.sqrt(sum((x1-x2)**2))","84c8630f":"## The Algorithm begins ##\n\ndef KNN(X, y, queryPoint, k = 5):\n    \n    vals = []\n    m = X.shape[0] # get the number of data points\n    \n    for i in range(m):\n        d = dist(queryPoint, X[i]) # Calculate the distance with all data points\n        vals.append((d, y.iloc[i])) # Store all distances and corresponding labels\n        \n    vals = sorted(vals) # Sort all the distances in increasing order\n    vals = vals[:k] # Keep only the first K distances \n    \n    new_vals = np.unique([v[1] for v in vals], return_counts=True) # Get the count for each label \n    \n    idx = new_vals[1].argmax() # Get the index for the label with max counts\n    pred = new_vals[0][idx] # Get the predicted label too\n    \n    return pred\n    ","9b742e6f":"# Let's first check on some sample images\n\nimg = X_val[87]\n\npred = KNN(X_train, y_train, img, k = 3)\n\ndrawImg(img)\nprint(\"Predcition: \", pred)","4fbc6f0b":"# Using the KNeighborsClassifier to make predictions\n\nknn = KNeighborsClassifier()\n\n# Fit the algorithm with the train data\nknn.fit(X_train,y_train)\n\ndrawImg(X_val[89].reshape((28,28)))\nknn.predict([X_val[89]])[0]","c51e7898":"predictions = knn.predict(X_val)\n\naccuracy_score(y_val, predictions)","c1b95876":"submit_predictions = knn.predict(test_df)","478a4254":"submission = pd.DataFrame({'ImageId' : range(1,28001), 'Label' : list(submit_predictions)})\nsubmission.head()","3806cde7":"submission.to_csv('submisson.csv', index = False)","9bc2b2d2":"## It's Submission Time!!!","5e87e316":"## Load and prepare the data","87245d97":"For our simple KNN algorithm, we won't be reshaping our images into 2D array and let them remain 1-dimensional so as to easily calculate the distance between two points which is the basis for this algorithm","1e3504c2":"## The KNN Algorithm","25a65ed3":"In this notebook, I'll be implementing the K-Nearest Neighbors algorithm for scratch to try and recognize MNIST Digits. The algorithm is the simplest of all the machine learning algorithms. This algorithm doesn't train on the dataset and therefore requires O(1) or constant time.\n\nYou can try to use this notebook for MNIST digits recognition as well as other image recognition cases. If you have any ideas please share them in the comments, it would be interesting if you could share your approach.","644faf81":"## Time to make some Predictions!","e22d5f6a":"## Well, how accurate will be this algorithm be?\n\nLet's find it out ourselves...\n\nThis time, we will use the scikit-learn implementation of KNN to determine the accuracy and to submit our predictions, as our algorithm is very slow and will require much time to run on all the validation points","6273dd09":"Now, it's time to implement the K-Nearest Neighbors algorithm. \n\nThis algorithm classifies any data point based on the label of it's surrounding neighbors. The K in the name signifies the number of nearest neighbors it chooses to predict the label. \n\nFor example, if K=3, then the 3 nearest neighbors will be checked and the label with the majority will be the predicted label for the new data point.\n\nAs mentioned earlier, this algorithm functions in constant time O(1), as it does not require to train the data points over and over again like other machine learning algorithms.\n","efa23ee0":"96.7% !!!!!!!!!!!\n\nThis simple algorithm does work reallyyyy well.","ea02a99e":"## Introduction","5ecef693":"We did not make any significant transformations or conversions of the data. We rescale pixel values from the range (0, 255) to the range (0, 1) as it is the best format for neural network models. Through this transformation, we also reduce the effect of illumination's differences, which is not as relevant for our case but can generally be helpful when working with the photo.\n\nScaling data to the range (0, 1) is usually called **normalization** and, in our case, is achieved by dividing the value of each pixel by 255 (normalization coefficient 1\/255 = ~0.0039)."}}