{"cell_type":{"2015c607":"code","10335c4c":"code","eaa21980":"code","5f69df10":"code","99071e2f":"code","046192aa":"code","7e538cf8":"code","9f3abfbc":"code","520d6fc6":"code","d4ac1391":"code","4c14f8cb":"code","f2d75f53":"code","b5ee8e20":"code","bc8f59af":"code","40a06d27":"code","2c002d88":"code","b3cd8710":"code","86edd09f":"code","be07bb0f":"code","acddc18c":"code","16187f41":"code","54de4b72":"code","96bf5a22":"code","1cfb3aca":"code","de8140d7":"code","b612cabd":"code","a96b5203":"code","cebb56f6":"code","95a8e427":"code","dd3ce1d6":"code","1c3f7dc2":"code","4b3f3f47":"code","13f08c5b":"code","97ee6de7":"code","785f50b4":"code","d99c7a1d":"code","634b1c00":"code","f76560e7":"code","c26c972a":"code","5f36caeb":"code","1da79c51":"code","9de571b2":"code","d0968654":"code","754d1c2a":"code","af2cdac8":"code","3531d083":"code","b12b0135":"code","407aa1ac":"code","9244bfb7":"code","80b82c15":"code","d836e1c4":"code","ad368bc1":"code","293bad61":"code","d3d4dd83":"code","bc866875":"code","93c1e364":"code","b2407355":"markdown","3d2879db":"markdown","5cec9e7c":"markdown","32c5e0fe":"markdown","3ef0cdd5":"markdown","08a63b49":"markdown","833c9382":"markdown","570fca4e":"markdown","b93645ac":"markdown","5f9dc071":"markdown","db8708ef":"markdown","5edbfd5a":"markdown","ada99389":"markdown","475df699":"markdown","13afe78e":"markdown","664d8e34":"markdown","533deeb0":"markdown","64c6efe6":"markdown","28a05d15":"markdown","65d331e6":"markdown","e392d85d":"markdown","6d2e4bc8":"markdown","82aaa10c":"markdown","1539d17e":"markdown","b5291b1e":"markdown","287680b2":"markdown","eabbfb0e":"markdown","540c2e5c":"markdown","19e8f346":"markdown","84f2ee6e":"markdown","ea90682e":"markdown","e033807c":"markdown","741486ab":"markdown","fad7304f":"markdown","d9bf78f7":"markdown","d1795f9e":"markdown","db5edd22":"markdown","86a08ad8":"markdown","82163275":"markdown","52ae7326":"markdown","355fb28f":"markdown","9e97a77f":"markdown","fdd51b44":"markdown","10b090e2":"markdown","64445551":"markdown","4ab5ff5b":"markdown","13b714f2":"markdown","fce82d7a":"markdown","673413b1":"markdown","64d29f4b":"markdown","9005c299":"markdown","ccbe66f5":"markdown","58c20fd8":"markdown","15f020b3":"markdown","8e79eaee":"markdown","38ef95d7":"markdown","d6f0b7b4":"markdown","eec8029f":"markdown","c05c4998":"markdown","65bbc6c0":"markdown","a6bebad7":"markdown","958dbc6f":"markdown","09db8ac4":"markdown","0a511da2":"markdown"},"source":{"2015c607":"!pip install seaborn --upgrade","10335c4c":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport warnings\nimport re\nimport os\nsns.set()\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\nwarnings.filterwarnings(\"ignore\")\n\nsns.__version__","eaa21980":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f69df10":"data = pd.read_csv('\/kaggle\/input\/marketing-data\/marketing_data.csv', index_col='ID', parse_dates=['Dt_Customer'])\n\ndata = data.rename(columns={\n    'Dt_Customer':'Enrollment date',\n    'Recency':'Days since last purchase',\n    ' Income ':'Income'})\n\ndata['ID'] = data.index\n\ndata.head()","99071e2f":"print('Number of columns :',data.shape[1])\nprint('Number of records :',data.shape[0])","046192aa":"# Another way\ndata['Income'].str.replace(r'[$,]','')","7e538cf8":"def extract(x):\n    if x is np.nan: return np.nan\n    return float(re.sub(r'[$,]', \"\", str(x)))\n\ndata['Income'] = data['Income'].apply(extract)","9f3abfbc":"# Store customer's information\nCustomers = data.loc[:,:'Days since last purchase'].join(data[['Country']])  \n\n# Store product's information\nProducts = data.loc[:,'MntWines':'MntGoldProds']     \n\n# Store Purchases' information\nPurchases = data.loc[:,'NumDealsPurchases':'NumWebVisitsMonth']    \n\n# Store campaign's information\nCampaigns = data.loc[:,'AcceptedCmp3':'AcceptedCmp2']     \nMisc = data.loc[:,['Response','Complain']]","520d6fc6":"category = data.select_dtypes(include='object')\nnumeric = data.select_dtypes(exclude='object')","d4ac1391":"pd.DataFrame(data.isnull().sum(), columns=['#Null values']).T","4c14f8cb":"for f in category.columns:\n    print(category[f].value_counts())\n    print('***********************************')","f2d75f53":"df = numeric.describe()\n\ndef custom_style(row):\n    \n    color = 'white'\n    if row.name == 'min' or row.name == 'max':\n        color = 'darkkhaki'\n\n    return ['background-color: %s' % color]*len(row.values)\n\ndf.style.apply(custom_style, axis=1)","b5ee8e20":"'''\nfrom sklearn.impute import KNNImputer\n\nnumeric_before_impute = numeric.drop(['Year_Birth','Enrollment date','ID'], axis=1).copy()\n\nimputer = KNNImputer(missing_values=np.nan)\nnumeric_imputed = imputer.fit_transform(numeric_before_impute)\n\nnumeric_imputed = pd.DataFrame(numeric_imputed, \n                       index=numeric_before_impute.index, \n                       columns=numeric_before_impute.columns).join(numeric[['Year_Birth','Enrollment date','ID']])\n\n'''","bc8f59af":"def report(feature):\n    fig, ax = plt.subplots(1,2)\n    fig.set_size_inches(16,4)\n    fig.suptitle(feature, fontsize=16)\n    sns.histplot(data=numeric, x=feature, kde=True, ax=ax[0])\n    sns.boxplot(data=numeric, x=feature, ax=ax[1])\n    plt.show()\n\n    print(numeric[feature].describe())","40a06d27":"from datetime import date\n\nAge = date.today().year-numeric['Year_Birth']\nnumeric.insert(1, 'Age', Age,)\n\nEnroll_at_age = numeric['Enrollment date'].dt.year - numeric['Year_Birth']\nnumeric.insert(6, 'Enroll_at_age', Enroll_at_age)","2c002d88":"for col in numeric.columns:\n    if col in ['Year_Birth','Enrollment date']: continue\n    if col == 'AcceptedCmp3' : break\n    report(col)","b3cd8710":"numeric[numeric['Age']>80]","86edd09f":"temp = numeric[numeric['Age']>120].index\nnumeric.loc[temp, [\"Year_Birth\", \"Age\", \"Enroll_at_age\"]] = np.nan","be07bb0f":"numeric[numeric['Income']>160000]","acddc18c":"plt.bar(height = numeric['Enrollment date'].dt.year.value_counts()[[2012,2013,2014]], x=['2012','2013','2014'])\nplt.title('Number of enrollment in each year')\nplt.show()","16187f41":"df = pd.pivot_table(numeric, values='ID', index='Enrollment date', aggfunc='count')\ndf['count'] = df['ID'].rolling(10).mean()\ndf['Year'] = df.index.year.astype('category')\n\nfig, ax = plt.subplots()\nfig.set_size_inches(20,6)\n\nsns.lineplot(data=df, x='Enrollment date', y='count', ax=ax, hue='Year')\nax.set_ylabel('Average enrollments')\nax.xaxis.set_major_locator(mdates.MonthLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nplt.xticks(rotation=40)\nplt.show()","54de4b72":"# This time, we need to exclude the outlier in \"Income\" column before caculating any statistics.\nnumeric_analysis = numeric[numeric['Income']!=666666]\n\ndf = pd.pivot_table(numeric_analysis.join(category[['Country']]), \n                     values='Income', \n                     index='Country', \n                     aggfunc={'Income':['mean','median']})\ndf.plot(kind='bar')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.ylabel('Income')\nplt.show()","96bf5a22":"temp = numeric_analysis[['Income','Age']].join(category[['Country']])\nsns.lmplot(data=temp, y='Income', x='Age', col='Country', col_wrap=4, line_kws={'color': 'darkorange'}, scatter_kws={'color':'teal'})\nplt.ylim(0,200000)\nplt.show()","1cfb3aca":"df = pd.pivot_table(numeric_analysis.join(category[['Education']]), \n               values='Income', \n               index='Education', \n               aggfunc={'Income':['count','mean','median']})\n\ndf[['mean','median']].plot(kind='bar')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.ylabel('Income')\nplt.xlabel('')\nplt.title('Income in each education level')\nplt.show()\n\ndf","de8140d7":"del numeric_analysis","b612cabd":"numeric['Total products amount'] = np.sum(Products, axis=1)","a96b5203":"total_purchase_each = np.sum(Purchases.iloc[:,:-1], axis=0)\n\npercent_purchase_each = total_purchase_each\/np.sum(total_purchase_each)*100\n\ndef plot_pie_chart(labels, sizes: pd.Series, title):\n    fig, ax = plt.subplots()\n    fig.suptitle(title, fontsize=16)\n    fig.set_size_inches(5,5)\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    ax.axis('equal')  \n    plt.show()\n    print(sizes.sort_values(ascending=False))\n    \n\nplot_pie_chart(labels=['Deal','Web','Catalog','Store'], sizes=percent_purchase_each, title='Purchases in each channel')","cebb56f6":"if 'Total purchase' not in Purchases.columns:\n    Purchases['Total purchase'] = np.sum(Purchases, axis=1)\n\nPurchase_category = Purchases.join(category)\n\nPurchase_country_summary = pd.pivot_table(Purchase_category, \n                                          values='Total purchase', \n                                          index='Country', \n                                          aggfunc={'Total purchase':['sum']})\n\nPurchase_country_summary.plot(kind='bar')\nplt.title('Total purchases in each country')\nplt.show()","95a8e427":"sum_each_product = np.sum(Products, axis=0)\n\nplot_pie_chart(sizes=sum_each_product\/np.sum(sum_each_product)*100, \n               labels=['Wine','Fruit','Meat','Fish','Sweet','Gold'],\n              title='Overall %Products')","dd3ce1d6":"if 'Total' not in Products.columns:\n    Products['Total'] = np.sum(Products, axis=1)\n\nEach_ID_Products = Products.apply(lambda x:x\/x[-1]*100, axis=1)\n\nAvg_Each_ID_Products = np.mean(Each_ID_Products, axis=0)","1c3f7dc2":"plot_pie_chart(sizes = Avg_Each_ID_Products[:-1], \n               labels=['Wine','Fruit','Meat','Fish','Sweet','Gold'],\n               title='Average Product for each ID')","4b3f3f47":"each_campaign = np.sum(Campaigns,axis=0)\n\nCR_each_canpaign = each_campaign\/len(Campaigns)*100","13f08c5b":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(15,4)\n\ncam_color = ['steelblue','peru','olivedrab','teal','sienna']\n\nax[0].barh(y=['Campaign 3','Campaign 4','Campaign 5','Campaign 1','Campaign 2'], \n           width=each_campaign.values, color=cam_color)\nax[0].set_xlabel('# success')\n\nax[1].pie(x=CR_each_canpaign, labels=['Campaign 3','Campaign 4','Campaign 5','Campaign 1','Campaign 2'],\n         autopct='%1.1f%%', shadow=True, startangle=90, colors=cam_color)\nax[1].axis('equal')\nax[1].set_xlabel('Overall each campaign\\'s success rate')\n\nplt.show()","97ee6de7":"fig, ax = plt.subplots(1,len(Campaigns.columns), sharey=True)\ni=0\nfig.set_size_inches(20,6)\n\nfor campaign in Campaigns.columns:\n    sns.histplot(data=numeric[numeric[campaign]==1], x = 'Age', ax=ax[i])\n    ax[i].set_ylim(0,40)\n    ax[i].set_title(campaign)    \n    i+=1\n    \nplt.show()","785f50b4":"Campaigns_category = Campaigns.join(category)\nCampaigns_category['Total accept'] = np.sum(Campaigns, axis=1)\n\nsummary_country = pd.pivot_table(Campaigns_category, \n                                   values='Total accept', \n                                   index='Country', \n                                   aggfunc={'Total accept':['sum','count']})\n\nsummary_country['CR'] = summary_country['sum']\/summary_country['count']\nsummary_country.rename(columns={'count':'#customers', 'sum':'Total accept'}).style.background_gradient(sns.light_palette('khaki', as_cmap=True), \n                                                                                                       subset=pd.IndexSlice[:, ['CR']])","d99c7a1d":"temp = Campaigns_category.groupby(by='Country').agg(['mean'])\ntemp.drop(['ME'], axis=0, inplace=True)  # since \"ME\" have only 3 observations, I decided to drop it in this table.\ntemp.style.background_gradient(sns.light_palette('green', as_cmap=True))","634b1c00":"# Building analysis dataset\nnumeric_analsis = numeric[numeric['Income']!=666666]\n\n# Let's clear redundant features\nX_numeric = numeric_analsis.drop(['NumStorePurchases','Enrollment date','Year_Birth','ID','Total products amount'], axis=1)\n\n# Focus on number of store purchases\ny = numeric_analsis[['NumStorePurchases']]","f76560e7":"from sklearn.linear_model import Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.feature_selection import f_regression, f_classif, chi2, RFE, VarianceThreshold, SelectPercentile\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.impute import KNNImputer\n\n# 1) Preoprocess\n# Impute null values\nImputer = KNNImputer(missing_values=np.nan)\n\n# Standardize numeric columns\nScaler = StandardScaler()\n\nnumeric_pipe = Pipeline([(\"Impute\", Imputer),\n                         (\"Scale\", Scaler)])\n\nX_numeric_preprocessed = numeric_pipe.fit_transform(X_numeric)\n\n# 2) Building the models\nlasso = Lasso(alpha=0.01).fit(X_numeric_preprocessed, y)\nridge = Ridge().fit(X_numeric_preprocessed, y)\nF, p = f_regression(X_numeric_preprocessed, y)  # Statistically check how each predictor & target are linearly correlated","c26c972a":"index = [\n    ['Features','Variance','Lasso','Ridge','F-test','F-test'],\n    ['','','Coef','Coef','F-value','p-value']\n]\n\nStorePurchases_effect_num = pd.DataFrame(list(zip(X_numeric.columns,\n                                          X_numeric.var()\/np.nanmean(X_numeric, axis=0),\n                                          lasso.coef_, \n                                          ridge.coef_[0], \n                                          F, p))\n                                          ,columns=index).set_index('Features')\n\nStorePurchases_effect_num.sort_values(by=('F-test','p-value')).style.background_gradient(sns.light_palette('khaki', as_cmap=True), \n                                                                                                       subset=pd.IndexSlice[:, [('Lasso','Coef'),\n                                                                                                                                ('Ridge','Coef'),\n                                                                                                                                ('F-test','F-value')]])","5f36caeb":"plt.figure(figsize=(7,7))\nStorePurchases_effect_num['Lasso']['Coef'].sort_values().plot(kind='barh')\nplt.xlabel('Lasso coef')\nplt.title('LASSO analysis')\nplt.show()","1da79c51":"plt.figure(figsize=(7,7))\nStorePurchases_effect_num['F-test']['F-value'].sort_values().plot(kind='barh')\nplt.xlabel('F-value')\nplt.title('F-test')\nplt.show()","9de571b2":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\n\nrfe = RFE(estimator = rf, n_features_to_select = 1.0, verbose=1).fit(X_numeric_preprocessed, y)","d0968654":"pd.Series(dict(zip(X_numeric.columns, rfe.ranking_)), name='Rank').to_frame().sort_values(by='Rank')","754d1c2a":"category_NumStore = numeric[['NumStorePurchases']].join(category)\ncategory_NumStore.head()","af2cdac8":"sns.catplot(kind='box', data=category_NumStore.query(\"Country!='ME'\"), col='Country', x='Education', y='NumStorePurchases',  col_wrap=4)\nplt.show()","3531d083":"sns.catplot(kind='bar', data=category_NumStore.query(\"(Country!='ME') and (Marital_Status not in ['YOLO','Alone','Absurd'])\"), \n            col='Country', x='Marital_Status', y='NumStorePurchases',  col_wrap=4)\nplt.show()","b12b0135":"fig, ax = plt.subplots(1,3, sharey=True)\nfig.set_size_inches(15,5)\n\nfor i,col in list(enumerate(['Education', 'Marital_Status', 'Country'])):\n    sns.countplot(data=data, x='Education',  ax=ax[i])\n\nfor ax_i in ax:\n    ax_i.xaxis.set_label_coords(0.5, 1.05)\n    plt.setp( ax_i.xaxis.get_majorticklabels(), rotation=40 )\n\nplt.show()","407aa1ac":"temp = Purchases.join(data[['Country','ID']])\n\nfig, ax = plt.subplots(2,4)\nfig.set_size_inches(20,7)\ni=0\n\nimport itertools \naxes = list(itertools.chain(ax[0],ax[1]))\n\nfor c in temp.Country.unique():\n    sns.histplot(data=temp[temp['Country'] == c], x='Total purchase', label=c, ax=axes[i], bins=15)\n    axes[i].legend()\n    i+=1\nplt.show()","9244bfb7":"summary_purchase_country = np.sum(Purchases, axis=1).to_frame('Total purchases').join(category[['Country']]).groupby(by='Country').agg(['count','sum','mean','median','std'])\n\nsummary_purchase_country","80b82c15":"from scipy.stats import norm\n\nUS_mean = summary_purchase_country.loc['US', ('Total purchases','mean')]\nUS_std = summary_purchase_country.loc['US', ('Total purchases','std')]\nUS_n = summary_purchase_country.loc['US', ('Total purchases','count')]\n\nfor country in summary_purchase_country.index:\n    if country == 'US':break\n    other_mean = summary_purchase_country.loc[country, ('Total purchases','mean')]\n    x = (US_mean-other_mean)\/(US_std\/US_n**0.5)\n    print('mean \"US\" > mean \"'+country.upper()+'\" p-value = '+str(norm.sf(x)))\n    ","d836e1c4":"Gold_avg = np.mean(Products['MntGoldProds'])\n\nmask = Products['MntGoldProds']>=Gold_avg\n\nAbove_gold = Products[mask].index\nBelow_gold = Products[~mask].index\n\nfig,ax = plt.subplots(1,2)\nfig.set_size_inches(15,5)\nsns.histplot(ax=ax[0], data=Purchases[mask], x='NumStorePurchases', kde=True, label='Purchase Gold Above Avg.', color='indigo',element='step')\nsns.histplot(ax=ax[0], data=Purchases[~mask], x='NumStorePurchases', kde=True, label='Below', color='darkorange', element='step')\n\ntemp = Purchases.join(mask)\ntemp['MntGoldProds'] = temp['MntGoldProds'].replace({True:'Gold above avg.', False:'Gold below avg.'})\nsns.boxplot(ax=ax[1], data=temp, y='NumStorePurchases', x='MntGoldProds', palette={'Gold above avg.':'indigo','Gold below avg.':'darkorange'})\n\nax[0].legend()\nplt.show()","ad368bc1":"PhD_Married = (category['Education']=='PhD') & (category['Marital_Status']=='Married')\n\nnumeric.loc[PhD_Married,['MntFishProducts']].describe().join(numeric.loc[~PhD_Married,['MntFishProducts']].describe(),\n                                                            lsuffix='_PhD&Marrid',\n                                                            rsuffix='_Not')","293bad61":"Customers.drop(['Year_Birth','Enrollment date'], axis=1)\\\n.join([Purchases, Products, Campaigns])\\\n.corr()[['MntFishProducts']]\\\n.style.background_gradient(sns.light_palette('green', as_cmap=True))","d3d4dd83":"fig, ax = plt.subplots(1,3, sharey=True)\nfig.set_size_inches(20,7)\nsns.boxplot(data=data[data['Country']!='ME'], \n            x='Country', y='MntFishProducts',\n            ax=ax[0])\n\nsns.boxplot(data=data.query('Marital_Status not in  [\"YOLO\",\"Alone\",\"Absurd\"]'), \n            x='Marital_Status', y='MntFishProducts',\n            ax=ax[1])\n\nsns.boxplot(data=data, x='Education', y='MntFishProducts',ax=ax[2])\n\nfig.tight_layout()\nfig.suptitle('Relationship between each categorical variable\\nand amount spent on Fish', fontsize=18, y=1.07)\nplt.show()","bc866875":"sns.catplot(kind='box', data = data.query('(Marital_Status not in [\"YOLO\",\"Absurd\",\"Alone\"]) and (Country != \"ME\")'), \n            x ='Marital_Status', \n            y ='MntFishProducts', \n            col='Country', col_wrap=4)\nplt.suptitle('In each country, \\nAmount spent on Fish in each marital status', fontsize=18, y=1.07)\n\nplt.show()","93c1e364":"sns.catplot(kind='box', data = data.query('(Marital_Status not in [\"YOLO\",\"Absurd\",\"Alone\"]) and (Country != \"ME\")'), \n            x ='Education', \n            y ='MntFishProducts', \n            col='Country', col_wrap=4)\nplt.suptitle('In each country, \\nAmount spent on Fish in each education level', fontsize=18, y=1.07)\n\nplt.show()","b2407355":"### 3) Analysis <br>\nAfter we have looked into missing values and outliers, we then analyze all of the features to get some insight into them.\n\nConsidering **Enrollment date**, we can see the total number of enrollment in each year in the following bar graph.","3d2879db":"From the **\"Relationship between each categorical variable and amount spent on Fish\"** graphs we see that:\n- In 'Country' variable, we see no difference in each country on 'MntFishProducts'.\n- In 'Marital_Status' variable, Widow tends to have more 'MntFishProducts'.\n- In 'Education' variable, 'Graduation' and '2n Cycle' tends to have more 'MntFishProducts'.","5cec9e7c":"# Prepare the data","32c5e0fe":"So, what is the key factor related to amount spent on fish? Let's first see the correlation between other numerical features and 'MntFishProducts'.","3ef0cdd5":"## - Product","08a63b49":"Create table for each section of data","833c9382":"From the **\"In each country, Amount spent on Fish in each marital status\"** graphs we see that:\n- **Widows** from **\"GER\" and \"IND\"** tend to spend on fish significantly more than others. While the **Divorced** from those country have a opposite trend.","570fca4e":"## - Income <br>\n**Mean and Median of Income** in **each country**","b93645ac":"Obviously, we can see that **'Basic' education level** seems to have **the lowest number of store purchases in all countries**. \n\nOn the other hand, there is no obvious trend in marital status in each country as shown in the plot below. However, from the plot below, we might conclude that **widows in the US** are **less likely to purchase in-store** while those in AUS, IND, GER are tended to purchase in-store.","5f9dc071":"Change \"Income\" column format from \"\\\\$84,835.00\" (String) to 84835.00 (Float) <br>\nBy replacing comma(\",\") and \"$\" with empty string(\"\").","db8708ef":"However in 2012, the data is gathered from 2012\/08 and ,in 2014, data is gathered until 2014\/07. Thus, that makes sense that 2013 have the greatest number of enrollment. The graph below shows the average number of enrollment over time.","5edbfd5a":"Apart from statistical methods(LASSO and F-test) that measure te linear effect, we'll also perform Recursive Feature Elimination which is a feature selection method based on ML.","ada99389":"From the correlation table above, the darker the color shade is the greater relation on 'MntFishProducts' each variable has.","475df699":"We can conclude from many methods we performed before that **'MntWines' is the most significant factor related to the number of store purchases.**","13afe78e":"Lastly, we have to look up to the average number of store purchases in each country and in each education level to see if there's some bias in the number of samples.","664d8e34":"- Does **US** fare significantly better than the Rest of the World in terms of **total purchases?** <br>\n\nI'll do **X-test** to test whether mean of total purchases in US is significantly higher than other country by stating null-hypothesis and alternative hypothesis under the significance level of 0.05 as following.<br>\nIn each country : $C$ <br>\n$H_0 : \\mu_{us} \\leq \\mu_{c}$ <br>\n$H_a : \\mu_{us} > \\mu_{c}$\n$, \\alpha = 0.05$\n\nLet's see the distribution of purchases in each country and then <br>\na table that summary important statistics of purchases in each country.","533deeb0":"\n## 1) Null values","64c6efe6":"**-> Income**","28a05d15":"Once we know that 'Marital_Status' and 'Education' have some relation to 'MntFishProducts', we'll go deeper to find more specific insight.\n\nFirst, see 'Marital_Status' in each country.","65d331e6":"Next, **Income** in each **education level**.","e392d85d":"For simplicity, we plot the **coefficient of LASSO** and **F-value of F-test** in the bar graphs below.","6d2e4bc8":" We wanted to know which product was the most popular. So, we'll look into **Overall proportion of products purchased**. <br>\n We see that 50.2% of all products purchased by all customers is wine and the second place(27.6%) is meat.","82aaa10c":"The table below shows a summary of the models we have built. LASSO's coefficient and F-value can tell the importance of each variable. <br>\nI also included the variance of each variable to see the spread of value in each variable.","1539d17e":"We'll looking into **proportion** of the number of **purchases in each channel** ('Deal', 'Web', 'Catalog', 'Store') to see the performance of each channel. <br>\nWe see that 39% of all purchase is in store, 27.5% in web, 15.6% in deal, and 17.9% by catalog. We can conclude that ,from the data, more than half of the customers purchased in store and website. ","b5291b1e":"**Section 01: Exploratory Data Analysis**\n\n- Are there any **null values or outliers**? How will you wrangle\/handle them? <br>\n-> There're nullvalues and outliers in 'Income' column. They will be imputed by sklearn's KNNImputer\n\n- Are there any variables that warrant **transformations**? <br>\n-> We can transform year columns into age columns.\n\n- Are there any useful variables that you can engineer with the given data? <br>\n-> Total_product_purchases, Conversion rate of campaign acception and many others will be shown inside the notebook.\n\n- Do you notice any **patterns or anomalies** in the data? Can you plot them? <br>\n-> There're anomaly in customers' age and extramly high value in income columns. There also are highly right-skewed distribution in product purchases, income, channel purchase columns\n\n\n\n**Section 02: Statistical Analysis**\n\n- What factors are significantly related to the **number of store purchases?** <br>\n-> MntWines\n\n- Does **US** fare significantly better than the Rest of the World in terms of **total purchases?** <br>\n-> *No, we can't conclude like that*\n\n- Your supervisor insists that people who buy gold are more conservative. \nTherefore, people who spent an **above average amount on gold** in the last 2 years would have **more in store purchases**. Justify or refute this statement using an appropriate statistical test <br>\n-> *Yes*\n- Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do **\"Married PhD candidates\"** have a significant relation with **amount spent on fish**? What **other factors** are significantly related to amount spent on fish? <br>\n-> *Not at all*\n\n- Is there a significant relationship between **geographical regional** and success of a **campaign**? <br>\n-> *Yes, many campaigns are most successful in Spain*\n\n\n**Section 03: Data Visualization**\n\nThis notebook will not explicitly have this section. However, throughout this notebook, I did quite a lot of visualization to answer the questions of all the sections.\n\n- Which marketing **campaign** is most successful? <br>\n-> *Campaign4 have the greatest number of acceptance while canpaign2 have the least.*\n- What does the average customer look like for this company? <br>\n-> *Middle-age, Spainish, Have degree(s), Have family, Mostly purchase wine*\n- Which products are performing best? <br>\n-> *wine*\n- Which channels are underperforming? <br>\n-> *Deal(Purchase by discount deal)*\n\n\n___","287680b2":"Moreover, we need to see if there are some categorical variables that can be related to 'MntFishProducts'.","eabbfb0e":"We're gonna using these libraries.","540c2e5c":"# Section 01: Exploratory Data Analysis\n- Are there any null values or outliers? How will you wrangle\/handle them?\n- Are there any variables that warrant transformations?\n- Are there any useful variables that you can engineer with the given data?\n- Do you notice any patterns or anomalies in the data? Can you plot them?\n\nI'll do my best to answer above questions. This first section is separated into 3 parts: <br>\n    **1) Null values** : In this part, I'll try to find all the missing values and impute them. <br>\n    **2) Outliers** : In this part, I'll plot all the distribution of numerical data in order to detect some outliers and try to make sense of them.<br> \n    **3) Analysis** in each feature : In this part, I'll try to seek some insights into the data by creating perceptive pivot tables, graphs, and other visualizations. \n\n","19e8f346":"We'll go deeper by looking into relationship between **age** and **income** in **each country**. <br>\nWe noticed that \"ME\" country have only 3 observations. We won't drop them. Instead, we'll keep that caution in mind while we're doing analysis.","84f2ee6e":"Next, we'll see the **average performance** of **each campaign** in **each country**. <br>\nKeep in mind that *\\\"ME\\\" has only 3 observations*.","ea90682e":"# Data cleaning","e033807c":"After we get some sense of distribution of each numerical column, next, we'll analyze them. <br>\n___\n\n\n**-> Age** <br>\nThere're 3 people with ages 128, 122, and 121 whose Enrollment_at_age are 113, 114, and 121 respectively which is quite impossible.","741486ab":"From the **\"In each country, Amount spent on Fish in each education level\"** graphs we see that:\n\n- Only 'Graduation' and '2n Cycle' in 'SP', 'US', and 'SA' spend on fish more than others. But not for other countries.","fad7304f":"Inspecting each numerical columns -> There's no weirdly low or high value.\n\n\n### We can conclude that there are only have missing values in \"Income\" column","d9bf78f7":"Let's import our data.","d1795f9e":"- people who spent an above average amount on gold in the last 2 years would have more in store purchases.\n\nFrom the histogram and boxplot below, we can conclude that people who spent an **above-average amount on gold** have more **in-store purchases**.","db5edd22":"- do **\"Married PhD candidates\"** have a significant relation with amount spent on **fish**? What other factors are significantly related to amount spent on fish?","86a08ad8":"Next, looking into total number of **purchases** in each **country**. <br>\nWe see that most of the customers are from Spain","82163275":"We want to know if 'Age' have some noticeable effect on campaign acceptance. <br>\nAlthough there's no obvious trend, we still can see that campaign3 is likely to be accepted in younger customers than campaign4 while other campaigns are uniformly accepted in different ages.","52ae7326":"*Make sure the version of seaborn is 0.11.0","355fb28f":"Moreover, we want to know the average purchase behavior of each customer. We'll looking into **Average proportion of each product purchased by one ID** <br>\nFrom 100% of all product each person puurchased*, we see that 45.8% will be wine, 24.95% meat product, 12% gold ptoduct, 7% fish product, 5% sweet, 4.9% fruit product.","9e97a77f":"If we ignore \"ME\", because of its too few observation, we still can not conclude that US has significantly better than the Rest of the World in term of total purchase since p-value of $\\mu_{us}>\\mu_{ca}$ is 0.0554 which is not less than 0.05.","fdd51b44":"Next, we'll see if there's that effect in different countries. The table below shows the **overall conversion rate** in **each country**. <br>\n\nThe overall conversion rate is **around 30%**. <br>\nWe see that '*ME'* has the best conversion rate but there is only 3 observation in this country. So, *it's not significant*. <br>\nOther than 'ME', campaigns in **'SP' and 'CA'** get the **best conversion rate(32.4%)**. <br>\nThe worst rate is in 'AUS' which is 21.8%.","10b090e2":"Inspecting each value counts in categorical columns -> there're no more missing value.","64445551":"## - Campaign <br>\nWe wanted to know the performance of each campaign we conducted. Below, the bar graph and pie-chart show the number of acceptance and success rates in each campaign. <br>\nWe see that camapign2 might have some problems because it's very less accepted while other campaigns are accepted at a similar rate.","4ab5ff5b":"From the summary table above, we clearly see that \"Married PhD candidates\" doesn't have a significant relation with amount spent on fish. <br>","13b714f2":"**First**, We'll see the *linear* effect of numerical variables on \"number of store purchases\" by using statistical models such as regression and F-test. <br>\nBut before that we need to immpute the missing values. <br>\n\nI'll impute these null values by KNN imputation. KNN imputation is an approach to fill the missing data by using a model to predict the missing values. A range of different models can be used, although a simple k-nearest neighbor (KNN) model has proven to be effective in experiments. The use of a KNN model to predict or fill missing values is referred to as \u201cNearest Neighbor Imputation\u201d or \u201cKNN imputation.\u201d <br>\n\n[this link](https:\/\/machinelearningmastery.com\/knn-imputation-for-missing-values-in-machine-learning\/) provides you a great stuff about KNN imputation. Make sure you check it out!","fce82d7a":"We can't confidentially tell that **'Basic' education level** have **the lowest number of store purchases in all countries** because we don't have enough data to say so. But these visualization give us a roughly say. If we have more data, we'll have more confident to that.","673413b1":"## - Purchase","64d29f4b":"Create table for each type of data ","9005c299":"Considering Lasso coefficient, we see that \n- Features having positive effect to the number of store purchases in decreasing order is *'MntWines', 'NumDealsPurchases', 'NumWebPurchases', 'MntFruits', 'MntFishProducts'*.\n- Features having negative effect to the number of store purchases in decreasing order is *NumWebVisitsMonth, Kidhome, Response*.","ccbe66f5":"### 2) Outliers <br>\nCreate report() function to describe and visualize the numerical data.","58c20fd8":"# Section 02: Statistical Analysis\n\n- What **factors** are significantly related to the **number of store purchases?** <br>\nI'll figure out this question by trying some feature selection methods including L1-Regularization, ANOVA F-test, Recursive feature elimination.","15f020b3":"Once we see the effect of our numerical variables on the number of store purchases, we also have to see the effect of categorical variables.<br>","8e79eaee":"There're 24 detectable null values in \"Income\" column. However, we still need to check for other missing values since sometimes missing values are denoted as, for example, \"Unknown\" for categorical data or -1 for numerical data. <br>\n","38ef95d7":"Then, print the distribution report for numerical columns.","d6f0b7b4":"# Note :<br>\n\n- Major update : (18-Jan-2021) <br>\n    1) Added content in section2 \"significant factor related to the number of store purchases.\" <br>\n    2) Add highlight color in dataframe for better visibility  <br>\n    3) Added KNN imputation explanation\n- I'll keep updating some little details that I miss. Once I think this notebook is perfectly done, I will delete this note. <br>\n- However, if I made some mistakes or you want to leave some advice, please do not hesitate to comment in the comment section below. <br>\n- Any comments are welcomed :)\n___","eec8029f":"I'll create new column named \"Age\" and \"Enroll_at_age\" derived from \"Year_Birth\" and \"Enrollment date\" respectively.","c05c4998":"I'll create new column **\"Total products amount\"** : define a total number of products purchased by each customer.","65bbc6c0":"According to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/List_of_the_verified_oldest_people), there're no person alives at that age in 20 century. <br>\nWe can conclude that there were some mistakes in these records. So, I'll mark their \"Year_Birth\", \"Age\", \"Enroll_at_age\" Null.","a6bebad7":"There is one person with ID 9432 that has very high income which is not impossible. Moreover, when consider age and enrollment date, it seems OK. So, I decide to do not thing with him.\n\n**Amount of product, number of purchases features** are exponentially distributed which not unusual. <br>","958dbc6f":"And then, see 'Education' in each country.","09db8ac4":"In each column, the darkest green shade one is the country that performed the best in each campaign. <br>\nWe see that:\n- In campaign 3, \"GER\", \"IND\", \"SP\", \"US\" perform quite well.\n- In campaign 4, \"SP\", \"IND\", \"GER\", \"CA\" perform quite well.\n- In campaign 5, \"AUS\", \"CA\", \"SP\" perform quite well.\n- In campaign 1, \"SP\", \"CA\" perform quite well.\n- Campaign 2 was doing not very well in any country.\n- For the overall average performance, \"CA\", \"GER\", \"SP\" are the best.","0a511da2":"---"}}