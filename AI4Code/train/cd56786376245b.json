{"cell_type":{"d357d0a0":"code","337ee1db":"code","4df4dd5b":"code","1c029a1b":"code","82c4750c":"code","1ca89bbf":"code","80f03be2":"code","473943df":"code","9b3b2793":"code","b5351756":"code","5c0e053b":"code","d5e87688":"code","335580b7":"code","415f2a6f":"code","7a43408c":"code","df8885c4":"code","2187fb19":"code","95a58d80":"code","73f73e0d":"code","ff942859":"code","0ee9facd":"code","a31261a7":"code","45c7bae8":"code","0e8172ec":"code","32b4e465":"code","456f869d":"code","32742ee5":"code","e0f7fb7a":"code","06a46c83":"code","56f8c748":"code","b3847166":"code","8773a5b2":"code","c86a3bf4":"code","1a99e3c3":"code","920bf8ed":"code","c7fb82fc":"code","62c88daf":"code","d9b2bada":"code","3a242ee8":"code","8f5a3b56":"code","43dd581e":"markdown","f598914b":"markdown","830ed591":"markdown","379353b8":"markdown","c54393d8":"markdown","81e44531":"markdown","c59ea04f":"markdown","60cba739":"markdown","8b9adf03":"markdown","303a6045":"markdown","33081ecc":"markdown","a8116e8e":"markdown","b11e8e03":"markdown","73c64fc1":"markdown","214627d1":"markdown","42622634":"markdown","efce8f41":"markdown","4a0866f5":"markdown"},"source":{"d357d0a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","337ee1db":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv');print(df.shape)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv');print(test.shape)\n\n# Lets combine train set( df) and test together so its easier to process everything in one go\n# Just remember test set doesnt have 'Survived' column, so everything in NaN when we combine them\ndf = df.append(test, ignore_index=True)\n\n# Lets just drop 'PassengerID' cause it won't be useful in this case\ndf.drop(columns='PassengerId',inplace=True)\ndf.info()","4df4dd5b":"df.Name.head(10)","1c029a1b":"# From what I see aove, we can extract the title by getting text that follows by a dot(.)\n# Now lets get the title for each person\n\nfor name_string in df['Name']:\n    df['Title']=df['Name'].str.extract('([A-Za-z]+)\\.',expand=True) # Use '([A-Za-z]+)\\.' here to get string follow by dot\ndf['Title'].value_counts()","82c4750c":"# Okay, after looking at several other kernels, I realised that Don, Sir, Col, Major, Jonkheer, Capt can be map as Mr. \n# Similarly, Mlle, Mme, Ms - Miss, and Lady, Countess, Dona as Mrs.\n\n# Mapping by changing several similar titles to common ones (ie: Don to Mr)\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\n# So replace the title that we want using the mapping dictionary\ndf.replace({'Title': mapping}, inplace=True)\n\n# Get the new title count\ndf.Title.value_counts()","1ca89bbf":"# Now lets start the imputation stuff. First we impute the age using the median of each title. \n# Get the median age in each categorical title, then impute it to the null value in age\n\n# From above we get the following titles:\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\n# So use this to get the median age \nfor title in titles:\n    listOfMedianAgeForEachTitle = df.groupby('Title')['Age'].median() # Get the median for each title and put it in a list\n    age_to_impute = listOfMedianAgeForEachTitle[titles.index(title)] # For the speciic title in the current loop, get the median age using the index\n    print(title, age_to_impute) # Print the title and age for the current loop\n    \n    # So if the age of the current title is null, replace is to the age_to_impute\n    df.loc[(df['Age'].isnull()) & (df['Title'] == title), 'Age'] = age_to_impute\n    \n    # End Loop\n\n# Check how which columns still null\ndf.isnull().sum()","80f03be2":"# From what I observed in the data description in Kaggle, seems like ticket number varies for each person\n# meaning that it might just be a non-meaningfull number. Lets doublecheck the number of values\ndf.Ticket.value_counts()","473943df":"# Ticket got 929 distinct values in 1300++ entries, so won't be useful for the machine learning. Drop em'\n# We already used Name and replace it to title, so lets drop em'\n# Cabin got heaps of NaN. So lets just drop em'\ndf.drop(['Cabin','Name','Ticket'],axis=1,inplace=True)\n\ndf.info()","9b3b2793":"# So embarked & fare got null entries (Survived too but we know that test fle doesnt have em so lets ignore it now)\n# Fare is numeric so can easily fill NaN as mean value\n# Embarked is an object so lets check the value counts\ndf.Embarked.value_counts()","b5351756":"# Fill the NaN in Embarked column with the highest value count and Fare with the mean()\ndf.Embarked.fillna(df.Embarked.value_counts().argmax(),inplace=True)\ndf.Fare.fillna(df.Fare.mean(),inplace=True)\ndf.info()","5c0e053b":"# I learned that some category is easier if we convert them from category to Numeric. \n# So lets figure out the number of distinct values in each object columns\n\nfor col in df.columns:\n    if df[col].dtype not in ['int64','float64']:\n        totalDistinct = df[col].value_counts().count()\n        print(col,totalDistinct)\n","d5e87688":"# Okay so we have 3 in embarked, 2 in sex and 6 in title. maybe lets convert the embarked and sex first\n# SEX - Convert female = 0, male = 1\ndf['Sex'] = df.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\n#EMBARKED - Convert Q=0, C=1, S=2\ndf['Embarked'] = df.Embarked.apply(lambda x: 0 if x == \"Q\" else (1 if x==\"C\" else 2))\n\nprint(df.Sex.value_counts())\nprint(df.Embarked.value_counts())","335580b7":"# So wait, parch is the number of parents aboard & SibSp is the number of childern aboard.\n# Maybe its better if we add both of them to a column 'NoOfFamilies'. Arguable here but I wanna take this route \n# to simplify things\n\ndf['NoOfFamilies']=df['SibSp']+df['Parch']\n\ndf.NoOfFamilies.value_counts()","415f2a6f":"# Not bad, 790 were single and the rest got families aboard. I could split them into category of 2 class, single & taken \n# but not too sure maybe theres somebody who came with 1 parent or something like that. \n# I'll maybe try and split them to 0-no family and 1-have family. Will come back to see if that improves the accuracy\nhaveFamilies = (df.NoOfFamilies>0) # For later. Not sure whether i'll use it or not\n\n# Drop the Parch & SibSp columns cause I alread have the NoOfFamilies column\ndf.drop(columns=['Parch','SibSp'],inplace=True)\n\n# Sweet, now i think we are readdy to move. Lets just double check that everything is good\ndf.info()","7a43408c":"# Convert all object to category (in our case just the title actually)\nfor col in df.columns:\n    if df[col].dtype not in ['int64','float64']:\n        df[col]=df[col].astype('category')","df8885c4":"# Checking the correlation & put it in a heatmap plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrmat =df.corr()\ncorrInd = corrmat.index[abs(corrmat['Survived'])>0]\nplt.figure(figsize=(8,8))\nsns.heatmap(df.corr(),\n            annot=True, cmap = 'Blues',\n            vmin=0,vmax=1,square=True\n           )\n","2187fb19":"# Use describe just to inspect the values from each numeric columns. So I know whether column is boolean or not, whats the mean and std\ndf.describe()","95a58d80":"# Plot Embarked countplot with Survived as the hue. Recall 0-Q, 1-C, 2-S\nsns.countplot('Embarked',data=df,hue='Survived')\nplt.show()\n\n# What I can say here is that embarked may not be a huge factor for predicting the survival rate. You can see that\n# the percentage of survival in each case is pretty much 50-50 except for ","73f73e0d":"# Dropping the embarked column\ndf.drop(columns='Embarked',inplace=True)","ff942859":"# Plot the sex with survived as hue. Female-0, Male-1\nsns.countplot('Sex',data=df,hue='Survived')\nplt.show()\n\n# The plot kinda makes sense as female were prioritised compared to male. So less female died & more male died","0ee9facd":"# Plot the age and survived as hue\nsns.countplot('Age',data=df,hue='Survived')\nplt.show()","a31261a7":"bins = [0, 6, 15, 21, 35, 50, 100]\nnames = ['Baby', 'Kids', 'Teenage', 'YoungAdult', 'Adult','Senior']\n\ndf['AgeRange'] = pd.cut(df['Age'], bins, labels=names)\n\nprint(df.AgeRange.value_counts())","45c7bae8":"# Plot the ageRange and survived as hue\nsns.countplot('AgeRange',data=df,hue='Survived')\nplt.show()\n\n# Sweet and lets delete the age column cause its not necessary anymore\ndf.drop(columns='Age',inplace=True)","0e8172ec":"# So I know fare will be the same as age. so I need to sepearte them into groups first. \n# But not sure whats the group to divide them into. According to this wiki page,\n# $0-40 for 3rd class, $60 for 2nd class and $150 for 1st class. Check the link here:\n# http:\/\/www.jamescamerononline.com\/TitanicFAQ.htm \n\nbins = [-1, 50, 149, 1000] # (-1,51) (51,149) (150,1000)\nnames = [3, 2, 1]\n\ndf['FareRange'] = pd.cut(df['Fare'], bins, labels=names)\n\nprint(df.FareRange.value_counts())\nprint(df.Pclass.value_counts())\n\n# So compare between both FareRange and Pclass, there were a bit difference here. Maybe some of them paid discounted price\n# or early birds, so they got cheap price for a good Pclass.","32b4e465":"# # This part was here because I previously set the lower limit in bins as 0 (see above) \n# # and got several NaN values in FareRange.\n# # After looking at the Fare value of the ones that appear as NaN in the FareRange, I noticed all was 0.\n# # So change the lower limit from 0 to -1 and it solves the issue. \n\n# Null_FareRange=df.FareRange[df.FareRange.isnull()==True]\n# df.Fare.loc[Null_FareRange.index]","456f869d":"# Similarly, I think Fare is not important anymore as we have FareRange now. Lets drop Fare\n\ndf.drop(columns='Fare',inplace=True)","32742ee5":"# countplot for FareRange with survived as hue\nsns.countplot('FareRange',data=df,hue='Survived')\nplt.show()\n\n# Both FareRange 1 & 2 showed higher percentage of survival compaed to FareRange 3 (lowest range).\n# If theres anything we learn here, its a good idea to pay for better class when you go for a cruise holiday.\n# At least its evident here that they have better chance of survival!","e0f7fb7a":"sns.countplot('NoOfFamilies',data=df,hue='Survived')","06a46c83":"# Looks to me that we can divide them into 3 groups:\n# 0 - NoFamily (0)\n# 1 - SmallFamily(1-3)\n# 2 - BigFamily (4-10)\n\nbins = [-1,0,3,1000] # (-1,0) (1,3) (4,1000)\nnames = [0,1,2]\n\ndf['FamilySize'] = pd.cut(df['NoOfFamilies'], bins, labels=names)\n\nsns.countplot('FamilySize',data=df,hue='Survived')\nplt.show()","56f8c748":"# Okay we can drop NoOfFamilies column\n\ndf.drop(columns='NoOfFamilies',inplace=True)","b3847166":"# Import necessary tools\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nencoder = LabelEncoder()\nsc = StandardScaler()\n\ndef labelNscale(df):\n    # Find the columns of all category data and put it in list\n    colsCateg = [col for col in df.columns if df[col].dtype not in ['int64','float64']]\n    # For all category data, apply LabelEncoder on them\n    for col in colsCateg:\n        df[col]=encoder.fit_transform(df[col])\n    \n    # Now apply scaler on all data (numeric & category)\n    df_ = sc.fit_transform(df)\n    df = pd.DataFrame(data=df_,columns=df.columns)\n    return df\n\ndf_R=labelNscale(df)\ndf_R.describe()","8773a5b2":"# Recall we have train and test set? \n# Train has a shape of 891 rows & test has 418. \n# Now lets split them \ntrainSet = df_R.loc[:890,:]\nfor_y = df.loc[:890,:]\ntestSet = df_R.loc[891:,:]\n\nprint(trainSet.shape)\nprint(testSet.shape)\n\n# Doublecheck train info\ntrainSet.info()\n\n# Drop the testSet survived column\ntestSet.drop(columns='Survived',inplace=True)\ntestSet.info()\n","c86a3bf4":"# Sweet everythings good. Now split the trainSet into X & y\ny = for_y['Survived']\nX = trainSet.drop(columns = ['Survived']).copy()\nprint(X.shape)\ny","1a99e3c3":"# Splitting the train set into train_train and train_test.\n# Im using StratifiedShuffleSplit cause I want the train_Train set to have \n# equal survival percentage values to the train_Test set. \nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","920bf8ed":"# Double check percentage split of y_train and y_test\nprint((y_train.value_counts())\/len(y_train))\nprint(y_test.value_counts()\/len(y_test))","c7fb82fc":"# Use knn & XGB as first guess model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\nacc_knn=accuracy_score(y_test,y_pred)\nprint('Score using knn is: ',acc_knn)\n\n\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)\nacc_xgb=accuracy_score(y_test,y_pred_xgb)\nprint('Score using XGB is: ',acc_xgb)\n\nsvm=SVC()\nsvm.fit(X_train,y_train)\ny_pred_svm = svm.predict(X_test)\nacc_svm=accuracy_score(y_test,y_pred_xgb)\nprint('Score using SVM is: ',acc_svm)","62c88daf":"# Define the process to split and predict as function with several input that we want to change\n# This wil return the accuracy of teh XGB model\n\ndef findOptimalXGB(X,y,nsplit,testsize):\n\n    sss = StratifiedShuffleSplit(n_splits=nsplit, test_size=testsize, random_state=1)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    xgb = XGBClassifier()\n    xgb.fit(X_train,y_train)\n    y_pred_xgb = xgb.predict(X_test)\n    acc_xgb=accuracy_score(y_test,y_pred_xgb)\n    \n    return acc_xgb","d9b2bada":"print('Optimal test size')\nfor i in np.arange(0.1,0.35,0.05):\n    acc = findOptimalXGB(X,y,10,i)\n    print('{:.2f}:{:.4f}'.format(i,acc))\n\nprint('Optimal nsplits')\nfor j in range(2,17,2):\n    acc = findOptimalXGB(X,y,j,0.3)\n    print('{:.1f}:{:.4f}'.format(j,acc))\n\n# Now repeat for optimal settings so that its default for our test prediction next\nacc = findOptimalXGB(X,y,10,0.3)","3a242ee8":"y_test = xgb.predict(testSet)\n","8f5a3b56":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nPassengerId = test['PassengerId']\n\nsubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': y_test })\nsubmission.to_csv(path_or_buf =\"Titanic_Submission.csv\", index=False)\nprint(\"Submission file is formed\")\n","43dd581e":"<a href=\"Titanic_Submission.csv\"> Download File <\/a>","f598914b":"Fare","830ed591":"**3.1 Splitting data into training and test set**\n<br><br>Now that I have labelise them, its time to split the data into training & test sets","379353b8":"You can see that its a bit struggling to identify the age because the gap is huge. So what I'm thinking is converting this into categorical data where:<br>\n<= 5 is Baby<br> \n6-14 is Kids<br>\n15-20 is Teenage<br>\n21-34 is YoungAdult<br>\n35-49 is Adult<br>\n50 is Senior\n","c54393d8":"Sex","81e44531":"Embarked","c59ea04f":"So using default option with n_splits of 10 and tset_size of 0.3 gave the best prediction of 86%. Lets stick with that. ","60cba739":"<br><br><br><br><br>\n**a) Importing data**","8b9adf03":"<br><br><br><br><br><br><br><br><br>\n**2. A bit of statistics on the data**<br><br>\nOkay, all the data cleaning is done. Let's have a look at the stats. Here I will look at:<br>1. The count of each variables with 'Survived' set as hue. This is to understand how different class\/group affects the chance of survival<br>2. For continous data, I will segregate them into different groups particularly for Age & Fare. <br><br>\nSo we'll take a closer look at each column starting from correlation plot, then Embarked, Sex, Age, Fare and NoOfFamilies. Other columns may not be necessary at this stage but will certainly be included later to see potential ways to improve accuracy","303a6045":"<br><br><br><br><br><br>\n**1. Data Cleaning**<br><br>\nAfter importing & inspecting the train.csv, I've noticed that Cabin has more than half of null object. So I decided to remove it from the df. <br><br>Name and ticket can also be dropped. But wait, maybe good idea to categorise the name according to the persons title<br><br>The next part will be on cleaning the data & converting the object to category","33081ecc":"Not bad with 84% of accuracy using the XGB. Definitely can get better accuracy if I put more effort on segregating the data. But happy with this right now. Lets try improve model?","a8116e8e":"Certainly not the best model but I'm satisfied enough by how I understand the data and use manpulate it to predict unseen data. Wht I'll do next is to broaden my knowledge on making pipelines & perhaps use pd.get_dummies on some columns. Then I'll come back and see if it can improves the accuracy of the model. Cheers!","b11e8e03":"What I observed here is that :<br>\n* 'Survived' is positively correlated with 'Fare'<br>\n* 'Survived' is negatively correlated with sex, pclass & age. (kinda logic here cause females (sex), younglings (age) were the priorities.)<br>\n* 'Survived' is negatively correlated with Embarked. Recall emabarked Q-0, C-1 & S-2. Seems passenger emarked from Q(queenstown) have better chances of survival. But also maybe because theres less passenger from Q. Lets look at it later <br>\n<br>Pclass 1 (1st class) seem to survive better here. Also makes sense, you pay more for better insurance I guess.\n<br><br> The rest had minor correlation, but maybe still useful. I think its best if we make a plot for each column against the survived","73c64fc1":"<br><br><br><br><br><br><br><br><br>\n**5. Predict the test and put into submission file **","214627d1":"<br><br><br><br><br><br><br><br><br>\n**4. Now apply the ML on them. Classifier not Regressor.**\n<br><br>Alright, now let's see how it goes when I apply several models to predict them.\nI used regressor at first and got really low accuracy. Only then realised I should use classifier instead<br><br> The outline here is:<br>\na) Apply Knn & XGB, see whos more accurate<br>\nb) Compile the process of splitting data into function so we can loop over and see if changing the split size or nplits affect the accuracy by how much. Then choose the appropriate one.","42622634":"<br><br><br><br><br><br><br><br><br>\n**3. Turn the category data to numeric (and scaling)**\n<br><br>Okay, done with the stats stuff, now lets get to the real meat! <br> \nI'm going to introduce LabelEncoder to labelise the category<br><br>\nI dont think we need StandardScaler cause the min and max for each colum range between 0 and 5. But maybe I'll try use it later and see if it improves the accuracy","efce8f41":"NoOfFamilies","4a0866f5":"Age"}}