{"cell_type":{"48e9e286":"code","b794e9c6":"code","248ddf34":"code","63568d11":"code","5c3ed768":"code","26464ffc":"code","1444948e":"code","8ddc7dad":"code","7de71b99":"code","503a1f2d":"code","8ab04f27":"code","8e997dba":"code","56920132":"code","30d408e0":"markdown","a58fd195":"markdown","6e8675b7":"markdown","0dae50c9":"markdown","67ca70ee":"markdown","cfa52d3e":"markdown","6c71c565":"markdown","d23fb04e":"markdown","05a3976e":"markdown","25ebbf01":"markdown","740eae1d":"markdown","5fb346e9":"markdown","c38384b2":"markdown","958c894e":"markdown","5590166f":"markdown","c0c733c5":"markdown","e268cced":"markdown","84805000":"markdown"},"source":{"48e9e286":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nimport sklearn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n!pip install --upgrade kaleido\nimport kaleido\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\nimport transformers\nimport tokenizers\n\nimport datetime\nimport json\nimport IPython\nfrom collections import Counter\nfrom IPython.display import display, HTML, IFrame","b794e9c6":"def sample_without_replacement(prob_dist, nb_samples):\n    \"\"\"Sample integers in the range [0, N), without replacement, according to the probability\n       distribution `prob_dist`, where `N = prob_dist.shape[0]`.\n    \n    Args:\n        prob_dist: 1-D tf.float32 tensor.\n    \n    Returns:\n        selected_indices: 1-D tf.int32 tensor\n    \"\"\"\n\n    nb_candidates = tf.shape(prob_dist)[0]\n    logits = tf.math.log(prob_dist)\n    z = -tf.math.log(-tf.math.log(tf.random.uniform(shape=[nb_candidates], minval=0, maxval=1)))\n    _, selected_indices = tf.math.top_k(logits + z, nb_samples)\n\n    return selected_indices","248ddf34":"for i in range(10):\n    print('sample {}: {}'.format(i + 1, sample_without_replacement(tf.constant([0.1, 0.2, 0.3, 0.4]), nb_samples=3).numpy()))","63568d11":"def get_masked_lm_fn(tokenizer, mlm_mask_prob=0.15, mask_type_probs=(0.8, 0.1, 0.1), token_counts=None, predict_special_tokens=False, mlm_smoothing=0.7):\n    \"\"\"\n    Prepare the batch: from the input_ids and the lenghts, compute the attention mask and the masked label for MLM.\n\n    Args:\n\n        tokenizer: A Hugging Face tokenizer.  \n        \n        token_counts: A list of integers of length `tokenizer.vocab_size`, which is the token counting in a dataset\n            (usually, the huge dataset used for pretraing a LM model). This is used for giving higher probability\n            for rare tokens to be masked for prediction. If `None`, each token has the same probability to be masked.\n\n        mlm_mask_prob:  A `tf.float32` scalar tensor. The probability to <mask> a token, inclding\n            actually masking, keep it as it is (but to predict it), and randomly replaced by another token.\n        \n        mask_type_probs: A `tf.float32` tensor of shape [3]. Among the sampled tokens to be <masked>, \n\n            mask_type_probs[0]: the proportion to be replaced by the mask token\n            mask_type_probs[1]: the proportion to be kept as it it\n            mask_type_probs[2]: the proportion to be replaced by a random token in the tokenizer's vocabulary\n        \n        predict_special_tokens: bool, if to mask special tokens, like cls, sep or padding tokens. Default: `False`\n        \n        mlm_smoothing: float, smoothing parameter to emphasize more rare tokens (see `XLM` paper, similar to word2vec).\n        \n    Retruns:\n\n        prepare_masked_lm_batch: a function that masks a batch of token sequences.\n    \"\"\"\n\n    if token_counts is None:\n        \"\"\"\n        Each token has the same probability to be masked.\n        \"\"\"\n        token_counts = [1] * tokenizer.vocab_size\n\n    # Tokens with higher counts will be masked less often.\n    # If some token has count 1, it will have freq 1.0 in this frequency list, which is the highest value.\n    # However, since it never appears in the corpus used for pretraining, there is no effect of this high frequency.\n    token_mask_freq = np.maximum(token_counts, 1) ** -mlm_smoothing\n\n    # NEVER to mask\/predict padding tokens.\n    token_mask_freq[tokenizer.pad_token_id] = 0.0\n    \n    if not predict_special_tokens:\n        for special_token_id in tokenizer.all_special_ids:\n            \"\"\"\n            Do not to predict special tokens, e.g. padding, cls, sep and mask tokens, etc.\n            \"\"\"\n            token_mask_freq[special_token_id] = 0.0\n\n    # Convert to tensor.\n    token_mask_freq = tf.constant(token_mask_freq, dtype=tf.float32)        \n\n    mlm_mask_prob = tf.constant(mlm_mask_prob)\n    mask_type_probs = tf.constant(mask_type_probs)\n    \n    vocab_size = tf.constant(tokenizer.vocab_size)\n    pad_token_id = tf.constant(tokenizer.pad_token_id)\n    mask_token_id = tf.constant(tokenizer.mask_token_id)\n    \n    def prepare_masked_lm_batch(inputs):\n        \"\"\"\n        Prepare the batch: from the input_ids and the lenghts, compute the attention mask and the masked label for MLM.\n\n        Args:\n            \n            inputs: a dictionary of tensors. Format is:\n            \n                {\n                    'input_ids': `tf.int32` tensor of shape [batch_size, seq_len] \n                    : `tf.int32` tensor of shape [batch_size, seq_len] \n                }            \n                \n                Optionally, it could contain extra keys 'attention_mask' and `token_type_ids` with values being\n                `tf.int32` tensors of shape [batch_size, seq_len] \n             \n        Returns:\n        \n            result: a dictionary. Format is as following:\n\n                {\n                    'inputs': A dictionary of tensors, the same format as the argument `inputs`.\n                    'mlm_labels': shape [batch_size, seq_len]\n                    'mask_types': shape [batch_size, seq_len]\n                    'original_input_ids': shape [batch_size, seq_len]\n                    'nb_tokens': shape [batch_size]\n                    'nb_non_padding_tokens': shape [batch_size]\n                    'nb_tokens_considered': shape [batch_size]\n                    'nb_tokens_masked': shape [batch_size]\n                }\n                \n                The tensors associated to `number of tokens` are the toekn countings in the whole batch, not\n                in individual examples. They are actually constants, but reshapped to [batch_size], because\n                `tf.data.Dataset` requires the batch dimension to be consistent. These are used only for debugging,\n                except 'nb_tokens_masked, which is used for calculating the MLM loss values.\n        \"\"\"\n\n        input_ids = inputs['input_ids']\n        \n        batch_size, seq_len = input_ids.shape\n\n        attention_mask = None\n        if 'attention_mask' in inputs:\n            attention_mask = inputs['attention_mask']\n\n        # Compute `attention_mask` if necessary\n        if attention_mask is None:\n            attention_mask = tf.cast(input_ids != pad_token_id, tf.int32)            \n\n        # The number of tokens in each example, excluding the padding tokens. \n        # shape = [batch_size]\n        lengths = tf.reduce_sum(attention_mask, axis=-1)\n                \n        # The total number of tokens, excluding the padding tokens.\n        nb_non_padding_tokens = tf.math.reduce_sum(lengths)\n\n        # For each token in the batch, get its frequency to be masked from the 1-D tensor `token_mask_freq`.\n        # We keep the output to remain 1-D, since it's easier for using sampling method `sample_without_replacement`.\n        # shape = [batch_size * seq_len], 1-D tensor.\n        freq_to_mask = tf.gather(params=token_mask_freq, indices=tf.reshape(input_ids, [-1]))\n\n        # Normalize the frequency to get a probability (of being masked) distribution over tokens in the batch.\n        # shape = [batch_size * seq_len], 1-D tensor.\n        prob_to_mask = freq_to_mask \/ tf.reduce_sum(freq_to_mask)\n\n        tokens_considered = tf.cast(attention_mask, tf.bool)\n        if not predict_special_tokens:\n            for special_token_id in tokenizer.all_special_ids:\n                tokens_considered = tf.logical_and(tokens_considered, input_ids != special_token_id)\n        nb_tokens_considered = tf.reduce_sum(tf.cast(tokens_considered, dtype=tf.int32))\n        \n        # The number of tokens to be masked.\n        # type = tf.float32\n        # nb_tokens_to_mask = tf.math.ceil(mlm_mask_prob * tf.cast(nb_non_padding_tokens, dtype=tf.float32))\n        nb_tokens_to_mask = tf.math.ceil(mlm_mask_prob * tf.cast(nb_tokens_considered, dtype=tf.float32))\n        \n        # round to an integer\n        nb_tokens_to_mask = tf.cast(nb_tokens_to_mask, tf.int32)\n\n        # Sample `nb_tokens_to_mask` of different indices in the range [0, batch_size * seq_len).\n        # The sampling is according to the probability distribution `prob_to_mask`, without replacement.\n        # shape = [nb_tokens_to_mask]\n        indices_to_mask = sample_without_replacement(prob_to_mask, nb_tokens_to_mask)\n \n        # Create a tensor of shape [batch_size * seq_len].\n        # At the indices specified in `indices_to_mask`, it has value 1. Otherwise, the value is 0.\n        # This is a mask (after being reshaped to 2D tensor) for masking\/prediction, where `1` means that, at that place,\n        # the token should be masked for prediction. \n        # (For `tf.scatter_nd`, check https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/scatter_nd)\n        pred_mask = tf.scatter_nd(\n            indices=indices_to_mask[:, tf.newaxis],  # This is necessary for making `tf.scatter_nd` work here. Check the documentation.\n            updates=tf.cast(tf.ones_like(indices_to_mask), tf.bool),\n            shape=[batch_size * seq_len]\n        )\n\n        # Change to 2-D tensor.\n        # The mask for masking\/prediction.\n        # shape = [batch_size, seq_len]\n        pred_mask = tf.reshape(pred_mask, [batch_size, seq_len])\n\n        # Get token ids at the places where to mask tokens.\n        # 1-D tensor, shape = [nb_tokens_to_mask].\n        _input_ids_real = input_ids[pred_mask]\n\n        # randomly select token ids from the range [0, vocab_size)\n        # 1-D tensor, shape = [nb_tokens_to_mask]\n        _input_ids_rand = tf.random.uniform(shape=[nb_tokens_to_mask], minval=0, maxval=vocab_size, dtype=tf.int32)\n\n        # A constant tensor with value `mask_token_id`.\n        # 1-D tensor, shape = [nb_tokens_to_mask]\n        _input_ids_mask = mask_token_id * tf.ones_like(_input_ids_real, dtype=tf.int32)\n\n        # For each token to be masked, we decide which type of transformations to apply:\n        #     0: masked, 1: keep it as it is, 2: replaced by a random token\n        \n        # Detail: we need to pass log probability (logits) to `tf.random.categorical`,\n        #    and it has to be 2-D. The output is also 2-D, and we just take the 1st row.\n        # shape = [nb_tokens_to_mask]\n        mask_types = tf.random.categorical(logits=tf.math.log([mask_type_probs]), num_samples=nb_tokens_to_mask)[0]\n\n        # These are token ids after applying masking.\n        # shape = [nb_tokens_to_mask]\n        masked_input_ids = (\n            _input_ids_mask * tf.cast(mask_types == 0, dtype=tf.int32) + \\\n            _input_ids_real * tf.cast(mask_types == 1, dtype=tf.int32) + \\\n            _input_ids_rand * tf.cast(mask_types == 2, dtype=tf.int32)\n        )\n\n        # Put the masked token ids into a 2-D tensor (initially zeros) of shape [batch_size, seq_len].\n        # remark: `tf.where(pred_mask)` is of shape [nb_tokens_to_mask, 2].\n        token_ids_to_updates = tf.scatter_nd(indices=tf.where(pred_mask), updates=masked_input_ids, shape=[batch_size, seq_len])\n\n        # At the places where we don't mask, just keep the original token ids.\n        # shape = [batch_size, seq_len]\n        token_ids_to_keep = input_ids * tf.cast(~pred_mask, tf.int32)\n        \n        # The final masked token ids used for training\n        # shape = [batch_size, seq_len]\n        masked_input_ids = token_ids_to_updates + token_ids_to_keep\n        \n        # At the places where we don't predict, change the labels to -100\n        # shape = [batch_size, seq_len]\n        mlm_labels = input_ids * tf.cast(pred_mask, dtype=tf.int32) + -100 * tf.cast(~pred_mask, tf.int32)\n\n        masked_lm_batch = {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask\n        }\n        if 'token_type_ids' in inputs:\n            masked_lm_batch['token_type_ids'] = inputs['token_type_ids']\n\n        # The total number of tokens\n        nb_tokens = tf.reduce_sum(tf.cast(input_ids > -1, dtype=tf.int32))\n\n        # Used for visualization\n        # 0: not masked, 1: masked, 2: keep it as it is, 3: replaced by a random token, 4: padding - (not masked)\n        # shape = [batch_size, seq_len]\n        _mask_types = tf.scatter_nd(tf.where(pred_mask), updates=mask_types + 1, shape=[batch_size, seq_len])\n        _mask_types = tf.cast(_mask_types, dtype=tf.int32)\n        _mask_types += 4 * tf.cast(input_ids == pad_token_id, tf.int32)\n\n        result = {\n            'inputs': masked_lm_batch,\n            'mlm_labels': mlm_labels,\n            'mask_types': _mask_types,\n            'original_input_ids': input_ids   \n        }\n\n        return result\n\n    return prepare_masked_lm_batch","5c3ed768":"dummy_sentence_pairs = [\n    ('i am so hungry', 'need some food'),\n    ('kaggle once a day', 'keep the doctors away'),\n    ('training is too slow?', 'try tensor processing unit!'),\n    ('nlp is interesting', 'join us and become a data scientist'),\n    ('want to learn bert?', 'let\\'s play with masked language model')\n]","26464ffc":"colors = np.array(\n    [\n        [ 44, 255, 119],   # green - unchanged tokens        \n        [192, 192, 192],  #  gray - masked tokens\n        [255, 255, 102],  # yellow - kept as it is, but to predict        \n        [255, 169, 242],  #  pink - randomly replaced tokens\n        # [137 ,209, 254 ],  #  blue - padding - unchanged tokens\n        [207, 238, 250 ],  #  blue - padding - unchanged tokens\n    ]) \/ 255\ncmap = matplotlib.colors.ListedColormap(colors)\nmask_categories = ['not to predict', 'masked', 'to predict as it is', 'randomly replaced', 'padding']\n\ndef set_ax(ax, vocab, input_ids, mask_types, batch_size, seq_len, title='', show_title=True, show_text=True, show_legend=True):\n\n    ax.set_xticks(range(-1, seq_len + 1))\n    ax.set_yticks(range(-1, batch_size + 1))\n    \n    ax.grid(color='k', linewidth=4)\n    \n    if show_text:\n        for (row_id, col_id), token_id in np.ndenumerate(input_ids):\n            ax.text(\n                col_id,\n                row_id,\n                '{}'.format(vocab[token_id]),\n                ha='center', va='center',\n                position=(0.5 * (2 * col_id + 1), 0.5 * (2 * row_id + 1)),\n                fontsize=14\n            )\n\n    extent = (0, seq_len, batch_size, 0)\n    ax.imshow(mask_types, cmap=cmap, extent=extent)\n    \n    ax.tick_params(\n        axis='both',          # changes apply to the x-axis\n        which='both',         # both major and minor ticks are affected\n        bottom=False,         # ticks along the bottom edge are off\n        top=False,            # ticks along the top edge are off\n        left=False,           # ticks along the bottom edge are off\n        right=False,          # ticks along the top edge are off    \n        labelbottom=False,\n        labeltop=False,\n        labelleft=False,\n        labelright=False\n    )\n    \n    if show_title:\n        ax.set_title(title, color='black', fontsize=20, position=(0.5, 1.05))\n    \n    if show_legend:\n        \n        # Used for legend\n        patches = [matplotlib.patches.Patch(color=colors[i], label=\"{}\".format(mask_categories[i]) ) for i in range(len(colors))]\n        # put those patched as legend-handles into the legend\n        ax.legend(handles=patches, bbox_to_anchor=(0, 1.20, 1, 0.2), loc=2, borderaxespad=0., fontsize=16, edgecolor='black')\n    \ndef plot_mlm_batch(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.0, plot_original=False, show_title=True, show_tokens=True, show_legend=True):\n        \n    original_input_ids = original_input_ids.numpy()\n    masked_input_ids = masked_input_ids.numpy()\n    mask_types = mask_types.numpy()\n    \n    batch_size, seq_len = original_input_ids.shape\n    \n    nb_axes = 1\n    if plot_original:\n        nb_axes = 2\n    \n    # size is (width, height)    \n    fig = plt.figure(figsize=(round(seq_len * scaling), round(nb_axes * batch_size * scaling)))\n    \n    gs = matplotlib.gridspec.GridSpec(nb_axes, 1)\n\n    ax_id = 0\n    if plot_original:\n        ax = fig.add_subplot(gs[ax_id, 0])\n        set_ax(ax, vocab, original_input_ids, mask_types, batch_size, seq_len, title='original tokens', show_title=show_title, show_text=show_tokens, show_legend=show_legend)\n        ax_id += 1\n    \n    ax = fig.add_subplot(gs[ax_id, 0])\n    set_ax(ax, vocab, masked_input_ids, mask_types, batch_size, seq_len, title='masked tokens', show_title=show_title, show_text=show_tokens, show_legend=show_legend and not plot_original)\n\n    plt.show()\n    \nmask_categories_abv = ['n\/a', 'masked', 'kept', 'rand. repl.', 'n\/a: padding']\ncolorscale = [\n     'rgb(44, 255, 119)',\n     'rgb(192, 192, 192)',\n     'rgb(255, 255, 102)',\n     'rgb(255, 169, 242)',\n     'rgb(207, 238, 250)'\n]\ndef plot_mlm_batch_plotly(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.0, title=''):\n    \"\"\"Use plotly to display the mlm batch when it is too large: No text is displayed, but information is shown when hovering on it.\n       This is also much faster.\n    \"\"\"\n\n    original_input_ids = original_input_ids.numpy()\n    masked_input_ids = masked_input_ids.numpy()\n    mask_types = mask_types.numpy()\n\n    original_tokens = np.array(\n        [[vocab[x] for x in token_ids] for token_ids in original_input_ids[::-1]]\n    )\n    masked_tokens = np.array(\n        [[vocab[x] for x in token_ids] for token_ids in masked_input_ids[::-1]]\n    )\n    mask_type_text = np.array(\n        [[mask_categories_abv[x] for x in mask_type] for mask_type in mask_types[::-1]]\n    )    \n    \n    customdata = np.dstack([original_tokens, masked_tokens, mask_type_text])    \n    \n    batch_size, seq_len = original_input_ids.shape\n\n    side_min = 300\n    width_max = 600\n    \n    if seq_len >= batch_size:\n        \n        width = max(100 * seq_len, side_min)\n        width = min(width, width_max)\n        \n        height = min(100, width \/ seq_len) * batch_size            \n        height += 200 * (1 - batch_size \/ seq_len)\n                        \n    elif seq_len < batch_size:\n        \n        width = max(100 * seq_len, side_min)\n        width = min(width, width_max)        \n        \n        height = min(100, width \/ seq_len) * batch_size            \n        height += 200 * (1 - batch_size \/ seq_len)\n    \n    data = mask_types[::-1]\n\n    fig = go.Figure(go.Heatmap(\n        z=data,\n        customdata=customdata,\n        colorscale=colorscale,\n        hovertemplate=' original: %{customdata[0]}<br>   masked: %{customdata[1]}<br>mask type: %{customdata[2]}',\n        name='', showscale=False, xgap=3, ygap=3))\n\n    title_text = None\n    if title is not None:\n        title_text = title\n        \n    # https:\/\/stackoverflow.com\/questions\/54826436\/how-to-remove-axes-and-numbers-from-plotly\n    fig.update_layout(\n        title_text=title_text,\n        plot_bgcolor=\"black\",\n        xaxis=dict(zeroline=False, constrain='domain', constraintoward='left', showticklabels=False, showgrid=False,\n                   visible=False, scaleanchor='y', scaleratio=1),\n        yaxis=dict(zeroline=False, constrain='domain', constraintoward='top', showticklabels=False, showgrid=False,\n                   visible=False),\n        width=width,\n        height=height,\n        autosize=False,\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=50,\n            pad=0\n        ),\n        hoverlabel=dict(\n            bgcolor=\"black\", \n            font_size=16, \n            font_family=\"Courier New, Monospace\"\n        )        \n    )\n\n    fig.show()","1444948e":"tokenizer_name = 'distilbert-base-uncased'\ntokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\nvocab = {v: k for k, v in tokenizer.get_vocab().items()}\n\nmlm_mask_prob = 0.35\nmask_type_probs=[0.5, 0.25, 0.25]\n\nprepare_masked_lm_batch = get_masked_lm_fn(\n    tokenizer=tokenizer,    \n    mlm_mask_prob=mlm_mask_prob,\n    mask_type_probs=mask_type_probs,\n)","8ddc7dad":"dummy_inputs = tokenizer.batch_encode_plus(dummy_sentence_pairs, max_length=16, padding='max_length', truncation=True)\ndummy_inputs = dict(dummy_inputs)\ndummy_ds = tf.data.Dataset.from_tensor_slices({'inputs': dummy_inputs}).batch(len(dummy_sentence_pairs))\ndummy_batch = next(iter(dummy_ds))\ndummy_batch","7de71b99":"r = prepare_masked_lm_batch(dummy_batch['inputs'])\noriginal_input_ids, masked_input_ids, mask_types = r['original_input_ids'], r['inputs']['input_ids'], r['mask_types']\nplot_mlm_batch(vocab, original_input_ids, masked_input_ids, mask_types, scaling=1.75, plot_original=True)","503a1f2d":"plot_mlm_batch_plotly(vocab, original_input_ids, masked_input_ids, mask_types, title='Hover the grid to see information.')","8ab04f27":"tokenizer_name = 'jplu\/tf-xlm-roberta-base'\ntokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\nvocab = {v: k for k, v in tokenizer.get_vocab().items()}\n\nmlm_mask_prob = 0.35\nmask_type_probs=[0.5, 0.25, 0.25]\n\nprepare_masked_lm_batch = get_masked_lm_fn(\n    tokenizer=tokenizer,    \n    mlm_mask_prob=mlm_mask_prob,\n    mask_type_probs=mask_type_probs\n)","8e997dba":"dummy_inputs = tokenizer.batch_encode_plus(dummy_sentence_pairs, max_length=14, padding='max_length', truncation=True)\ndummy_inputs = dict(dummy_inputs)\ndummy_ds = tf.data.Dataset.from_tensor_slices({'inputs': dummy_inputs}).batch(len(dummy_sentence_pairs))\ndummy_batch = next(iter(dummy_ds))\ndummy_batch","56920132":"r = prepare_masked_lm_batch(dummy_batch['inputs'])\noriginal_input_ids, masked_input_ids, mask_types = r['original_input_ids'], r['inputs']['input_ids'], r['mask_types']\nplot_mlm_batch(vocab, original_input_ids[:, :], masked_input_ids[:, :], mask_types[:, :], scaling=1.75, plot_original=True)","30d408e0":"### Sample without replacement in TensorFlow<a id='sample-no-replacement'><\/a>\n\nIn PyTorch, we can use\n\n```\n    torch.multinomial(..., replacement=False)\n```\n\nto perform sampling without replacement. In TensorFlow, there is no direct API to perform this operation.\n\nFollowing the discussion [here on GitHub](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/9260#issuecomment-437875125),\nwe use the [Gumbel-max trick]( https:\/\/timvieira.github.io\/blog\/post\/2014\/07\/31\/gumbel-max-trick\/) to implement it.","a58fd195":"#### Interactive\n\nWhen the batch becomes large, it is impossible to display the tokens directly as in the above figure. In this case, we will use [Plotly](https:\/\/plotly.com\/) to draw the batch over which you can hover to see several information, just like the figure in the next cell.\n\n![Annotation%202020-08-21%20152241.png](attachment:Annotation%202020-08-21%20152241.png)","6e8675b7":"### Visualize the original and masked tokens<a id='visualize-mlm-batch'><\/a>","0dae50c9":"# Preview token masking in Masked Language Models (MLM)\n\n#### Here is how a masked batch of tokens looks like:\n\nIt consists:\n* tokens not to predict\n  1. word tokens: green\n  2. (optional) special tokens: green (we can choose to predict or not speical tokens like `[CLS]` or `[SEP]`, etc.)\n  3. padding tokens: blue\n* tokens to predict:\n  1. the token is masked: gray\n  2. the token is kept as it is: yellow\n  3. the token is replaced by a random tokens in the vocabulary: pink\n\nHere we use a higher masking frequency (`0.35`), so you may find that too many tokens are masked for training MLM. This is only for visualization. In the experiment, we will use a frequency of `0.15`, which is used for traininig [Bert](https:\/\/arxiv.org\/pdf\/1810.04805.pdf).\n\nTo visualize (in an interactive way) a real batch of masked tokens used for training, see [Visualize a batch of MLM finetuning dataset](#check-mlm-fine-tuning-batch). The tokens are not displayed directly, but you can hover the grid to see the token information.\n\n### ![mlm.png](attachment:mlm.png)","67ca70ee":"#### Import","cfa52d3e":"### Check with another tokenizer\n\n#### Roberta tokenizer","6c71c565":"### Create a tokenizer and get the masking method\n\n#### Let's try with Bert tokenizer first.","d23fb04e":"## Visualize mask language model<a id='visualize-mlm'><\/a>\n\n#### Let's visualize MLM with a simple example!","05a3976e":"## Implementation of masking tokens<a id='imp-mlm'><\/a>","25ebbf01":"#### Bert<a id='visualize-mlm-batch-bert'><\/a>","740eae1d":"#### Sanity check","5fb346e9":"# Masked Language Model<a id='mlm'><\/a>","c38384b2":"### Tokenize the examples","958c894e":"#### Roberta<a id='visualize-mlm-batch-roberta'><\/a>\n\nDue to some missing glyphs in the font, we get some tokens rendered as\n\n![missing.PNG](attachment:missing.PNG)","5590166f":"### Mask tokens<a id='mask-tokens'><\/a>\n\n#### This is the main method that peforms token masking for training MLM language models. It is a translation into [TensorFlow](https:\/\/www.tensorflow.org\/) from [Hugging Face](https:\/\/github.com\/huggingface\/transformers\/blob\/390c1285925dd119705e69a266202ef04490d012\/examples\/distillation\/distiller.py)'s code, which is originally in [PyTorch](https:\/\/pytorch.org\/).\n\nThe `mlm_smoothing` argument is used to emphasize more rare tokens, see [Cross-lingual Language Model Pretraining (XLM)](https:\/\/arxiv.org\/pdf\/1901.07291.pdf). If `token_counts` is `None`, it will be a uniform distribution over the vocabulary, except for some special tokens.","c0c733c5":"<center><img src=\"https:\/\/raw.githubusercontent.com\/chiapas\/kaggle\/master\/competitions\/contradictory-my-dear-watson\/header.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Visualize Bert's token masking<\/h1><\/center>\n<br>\n\n#### In this notebook, we implement and visualize token masking in Masked Language Model (MLM), which is used for training [Bert model](https:\/\/arxiv.org\/pdf\/1810.04805.pdf) and [(XLM)-Roberta model](https:\/\/arxiv.org\/pdf\/1907.11692.pdf). The implemention is a translation into [TensorFlow](https:\/\/www.tensorflow.org\/) from [Hugging Face](https:\/\/github.com\/huggingface\/transformers\/blob\/390c1285925dd119705e69a266202ef04490d012\/examples\/distillation\/distiller.py)'s code, which is originally in [PyTorch](https:\/\/pytorch.org\/). We also visualize the effects of token masking.\n\n#### Here are some features of the implementation:\n  * <p style=\"color:blue\">dynamic token masking in TensorFlow operations, therefore it could be used as a tf.data.Dataset transformation.<\/p>\n  * <p style=\"color:blue\">the number of tokens to mask is calculated based on non-padding tokens (and optionally, excluding other special tokens)<\/p>\n  * <p style=\"color:blue\">including a smoothing option to mask rare tokens more frequently<\/p>\n  \nThis notebook is a simplified version of another notebook [Masked, My Dear Watson - MLM with TPU](https:\/\/www.kaggle.com\/yihdarshieh\/masked-my-dear-watson-mlm-with-tpu), where you can find how to use token masking to fine tune models with MLM objective, and a comparision between without and with using MLM finetuning.","e268cced":"### Visualization helpers","84805000":"### Create dummy examples"}}