{"cell_type":{"b73eb5ad":"code","2af488d4":"code","0a7f4060":"code","eab0a53b":"code","1ebd558a":"code","649f3e09":"code","9d259a39":"code","eda9cb7a":"code","65f0fd89":"code","0c856adf":"code","8149b845":"code","067df79a":"code","5be4bd0a":"code","e1ac7415":"code","e254a4c8":"code","4cee14d2":"code","8441931e":"code","d1a5bdeb":"code","5e98160b":"code","ea94b62f":"code","bad21a13":"code","d52b11a4":"code","60dcee1b":"code","f3753586":"code","975341cd":"code","51affbe6":"code","fa7c78ef":"code","c9f7ed08":"code","67110800":"markdown","33565ad9":"markdown","b68addfc":"markdown","ec91cd91":"markdown","1d458e86":"markdown","b6dc5226":"markdown","9d022c17":"markdown","76527c8b":"markdown","f89fbb5c":"markdown","3bde5592":"markdown","79e1c046":"markdown","07a719c8":"markdown","97e4b7ae":"markdown","2345075d":"markdown","87904d6b":"markdown","7aebe510":"markdown","cf9dbf60":"markdown","37cc78a2":"markdown","59042307":"markdown","3e82a051":"markdown","8a874f53":"markdown"},"source":{"b73eb5ad":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import cross_val_score\n","2af488d4":"# Import dataset\ndf = pd.read_csv(\"\/kaggle\/input\/kickstarter-campaigns-dataset-20\/Kickstarter Campaigns DataSet.csv\")\ndf.head()\ndf.describe()\nnRow, nCol = df.shape\n\nprint(f'There are {nRow} rows and {nCol} columns')\n\ndf.columns\ndf.info()\ndf.describe()","0a7f4060":"df=df.drop(['Unnamed: 0'], axis=1)\nprint(\"duplicates: \", df.duplicated().sum())\ndf.drop_duplicates(inplace=True, ignore_index=True)\n\n# original row-length was 217245. check after removal of duplicates\nprint(\"length of dataset after removal of duplicates is \", len(df))","eab0a53b":"# id column is unuseful\n\n# We will also have no use for creator_id\n\n# Currency data is partially represented in the country data, uniting European countries to Euro currency. Therefore we will remove\n# the currency column.\n\n# Also, the column usd_pledged can be treated just the same as the 'status' target column. \n# If amount pledged is greater than the goal, obviously the status will be success. If it is less than that goal\n# the status will be failed, so this column will be ommited.\n\n# This dataset is quite large, and the blurb column is different for each row. Since we have the blurb_length column we will remove \n# the blurb column.\n\n# Since we have campaign duration in days, we will drop the end date in the column 'deadline'\n\n\ncolumns_to_drop = ['id', 'creator_id', 'currency', 'usd_pledged', 'blurb', 'deadline' ]\n\ndf=df.drop(columns=columns_to_drop, axis=1)\n","1ebd558a":"# from 'launched_at' column we will only take the year information and ignore the month and day info \n \ndf['launched_at'] = df['launched_at'].str[:4]\n\n# Let's see how the kikstart projects were spread over the years\n\nyear_count = df['launched_at'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(year_count.index, year_count.values, alpha=0.9)\nplt.title('Kickstarter Distribution of Launch Year')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Year', fontsize=12)\nplt.xticks(rotation=90, horizontalalignment=\"center\")\nplt.show()\n\n# These will be now be treated as categories","649f3e09":"# Now let's see how many countries there are and how they are spread\n\ncountry_count = df['country'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(country_count.index, country_count.values, alpha=0.9)\nplt.title('Kickstarter Distribution of Countries')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xticks(rotation=90, horizontalalignment=\"center\")\nplt.xlabel('Country', fontsize=12)\nplt.show()","9d259a39":"# Let's check cities \nprint(\"The projects come from {} different cities\".format(df['city'].nunique()))\ndf['city'].value_counts()\n\n# Since there are 13409 different cities, and as city data is highly specific, we will drop that column and keep only the countries.\ndf=df.drop(columns='city')\n","eda9cb7a":"# Now let's see how many main-categories and sub-categories there are \nprint(\"The projects are divided into {} different main categories\".format(df['main_category'].nunique()))\nprint(\"They are also divided into {} different sub-categories\".format(df['sub_category'].nunique()))\n\ndf['main_category'].value_counts()\ndf['sub_category'].value_counts()\nsub_category_count = df['sub_category'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(sub_category_count.index, sub_category_count.values, alpha=0.9)\nplt.title('Kickstarter Distribution of Projects by Sub-Category')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Sub-Category', fontsize=12)\nplt.xticks(rotation=90, horizontalalignment=\"center\")\nplt.show()\n\n# It seens that sub categories allow for a more generalized classification. Therefore\n# we will keep the sub_category and drop the main_category\ndf = df.drop(columns = 'main_category')","65f0fd89":"# Transforming object type columns to numerical:","0c856adf":"# The name feauture is not very indicative, we will transform this feature to its length (number of characters)\n\ndf['name_length'] = df['name'].apply(lambda x: len(x))\ndf=df.drop(['name'], axis=1)\n\n# For the slug feature we will use the number of words that compose it\n\ndf['slug_length'] = df['slug'].str.count('-') + 1\ndf=df.drop(['slug'], axis=1)\n","8149b845":"# Check which columns need more handling, start with 'status' which will also serve as our target\ndf.head()","067df79a":"# Change the status column values to successful -> 1, failed -> 0\n\ndf.loc[df['status'] == 'successful', 'status'] = 1\ndf.loc[df['status'] == 'failed', 'status'] = 0\ndf['status'].value_counts()\n\n# Canceled projects will be deleted \n\ndf= df[df.status!=('canceled')]\n\n# Live projects are still ongoing, delete these as see \n# (these instances can be kept and prediction can be made, which will be checked later on when new outcome is released)\n\ndf_live=df[df.status==('live')]\ndf= df[df.status!=('live')]\n\ndf['status'] = df['status'].astype('int')\ndf['status'].value_counts()","5be4bd0a":"df['goal_usd'].min()\ndf['goal_usd'].max()\nprint (\"The minimum goal_USD is {}, the maximum goal_USD is {}\".format((df['goal_usd'].min()), (df['goal_usd'].max())))\n\n# The gap between max and min is enormous. It will also be easier to treat this column after the log transformation\n\ndf['goal_usd']=np.log(df['goal_usd'])\nprint (\"The minimum goal_USD after log transformation is {}, the maximum goal_USD after log transformation is {}\".format((df['goal_usd'].min()), (df['goal_usd'].max())))\n\ndf['goal_usd'].plot(kind='box',xlim=(-5,20), vert=False, figsize=(25,2))\n\n\nplt.title(\"Goal USD\", fontsize=18)\nplt.xlabel(\"Values after log transform\", fontsize=14)\n","e1ac7415":"# outliers:\nlow_outs = len(df[(df['goal_usd'] <= 0)])\nprint (\"There are {} low outliers\".format(low_outs))\n\nhigh_outs = len(df[(df['goal_usd'] >15)])\nprint (\"There are {} high outliers\".format(high_outs))\n\n# both of these can be removed\ndf.drop(df[(df['goal_usd'] <= 0) | (df['goal_usd'] > 15)].index, axis=0, inplace=True)\n\n# check number of rows remaining\nprint (\"There are {} rows in the datset after outlier removal\".format(len(df)))","e254a4c8":"print(\"The minimum number of backers is: {}\".format(df['backers_count'].min()))\nprint(\"The maximum number of backers is: {}\".format(df['backers_count'].max()))\nprint(\"The average number of backers is: {}\".format(df['backers_count'].mean()))\n\n# Projects that were successful and have 0 backers will be considered outliers and \n# Check if there are any: \nlen(df[(df['backers_count'] <= 0) & (df['status'] > 0)])","4cee14d2":"# country column: binary encoding\n\n\nencoder= ce.BinaryEncoder(cols=['country'],return_df=True)\n#Fit and Transform Data\ndf=encoder.fit_transform(df)\ndf.head()\n\n# sub_category: count\/frequency encoding\n\nsub_category_Dict = df['sub_category'].value_counts(normalize=True)\ndf['encoded_sub_category'] = df['sub_category'].map(sub_category_Dict)\n# drop original sub-category column\ndf = df.drop(['sub_category'], axis=1)\n\n# launched_at: one-hot encoding\n\ndf = pd.get_dummies(df)\n\ndf.info()\n","8441931e":"df['status'].value_counts(normalize=True)\n","d1a5bdeb":"df.head()","5e98160b":"# Continue on to prepare target and data matrix:\n\n# target\ny = np.array(df.iloc[:,7])\ndf=df.drop(['status'], axis=1)\n# data matrix\nX=np.array(df.iloc[:,0:25])\n\nprint(\"the shape of the data X matrix is {}, the shape of the target vector is {}\".format(X.shape, y.shape))","ea94b62f":"# prepare train and test data \nXtrain, Xtest, yTrain, yTest = train_test_split(X, y)","bad21a13":"k = 10\nkf = KFold(n_splits=k, shuffle = True, random_state=42)\nmodel = GaussianNB()\nresult = cross_val_score(model , X, y, cv = kf)\n \nprint(\"Avg accuracy Naive Bayes with KFold: {}\".format(result.mean()))\n\n# try Bernoulli NB as well since this is a binary classification problem, it might be better\nmodel = BernoulliNB()\nresult = cross_val_score(model , X, y, cv = kf)\n\nprint(\"Avg accuracy Bernoulli Naive Bayes with KFold: {}\".format(result.mean()))","d52b11a4":"k = 10\nkf = KFold(n_splits=k, shuffle = True, random_state=42)\nmodel = KNeighborsClassifier(5)    \n# result = cross_val_score(model , X, y, cv = kf)\n# print(\"Avg accuracy Naive Bayes with KFold: {}\".format(result.mean()))\n\n# long run-time...... resulted in:\n# Avg accuracy : 0.9213982539803947\n\n# Run KNN without KFold, check 3 neighbors as well:\n\nKNN_classifier = KNeighborsClassifier(5)    \nKNN_classifier.fit(Xtrain, yTrain)\npreds_KNN = KNN_classifier.predict(Xtest)\n\nprint('MAE KNN:',mean_absolute_error(yTest, preds_KNN))\nprint('Accuracy KNN :',accuracy_score(yTest,preds_KNN))\nprint('Classification report KNN:',classification_report(yTest, preds_KNN))\n\n\nKNN_classifier2 = KNeighborsClassifier(3)    \nKNN_classifier2.fit(Xtrain, yTrain)\npreds_KNN2 = KNN_classifier2.predict(Xtest)\n\n\nprint('MAE KNN:',mean_absolute_error(yTest, preds_KNN2))\nprint('Accuracy KNN :',accuracy_score(yTest,preds_KNN2))\nprint('Classification report KNN:',classification_report(yTest, preds_KNN2))","60dcee1b":"# Decision Tree - 2 versions with KFold 10\n\nk = 10\nkf = KFold(n_splits=k, shuffle = True, random_state=42)\n\n# 1. max_depth = 4\n\nmodel = DecisionTreeClassifier(max_depth=4, random_state=42)\nresult = cross_val_score(model , X, y, cv = kf)\n \nprint(\"Avg accuracy decision tree with KFold and max_depth 4: {}\".format(result.mean()))\n\n# 2. max_depth = 8\n\nmodel = DecisionTreeClassifier(max_depth=8, random_state=43)\nresult = cross_val_score(model , X, y, cv = kf)\n \nprint(\"Avg accuracy decision tree with KFold and max_depth 8: {}\".format(result.mean()))\n\n","f3753586":"\nmodel = RandomForestClassifier(random_state=42)\nresult = cross_val_score(model , X, y, cv = kf)\nprint(\"Avg accuracy random forest with KFold: {}\".format(result.mean()))\n","975341cd":"# XGBOOST\nXGB_classifier = XGBClassifier()\nXGB_classifier.fit(Xtrain, yTrain)\nprint(XGB_classifier)\n\n# make predictions for test data\nXGB_preds = XGB_classifier.predict(Xtest)\npredictions = [round(value) for value in XGB_preds]\n\n# evaluate predictions\naccuracy = accuracy_score(yTest, predictions)\nprint(\"Accuracy of XGB classifier: %.2f%%\" % (accuracy * 100.0))\n","51affbe6":"# XGBOOST\n\n# get importance\nimportance = XGB_classifier.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()\n\n","fa7c78ef":"random_forest_classifier = RandomForestClassifier(random_state=42)\nrandom_forest_classifier.fit(Xtrain, yTrain)\npreds_rfc = random_forest_classifier.predict(Xtest)\n\n# get importance\nimportance = random_forest_classifier.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","c9f7ed08":"# no need to change target vector, just data matrix\n\nX2 = np.array(df[['backers_count', 'goal_usd']])\nprint(\"the new data matrix size is: {}\".format(X2.shape))\nXtrain2, Xtest2, yTrain2, yTest2 = train_test_split(X2, y)\n\n# models using 2 features only:\n\n\nKNN_classifier_2features = KNeighborsClassifier(5)    \nKNN_classifier_2features.fit(Xtrain2, yTrain2)\npreds_KNN2 = KNN_classifier_2features.predict(Xtest2)\n\n\nprint('MAE :',mean_absolute_error(yTest2, preds_KNN2))\nprint('Accuracy KNN:',accuracy_score(yTest2,preds_KNN2))\n\n\ndecision_tree_classifier_2features = DecisionTreeClassifier(max_depth=4, random_state=42)\ndecision_tree_classifier_2features.fit(Xtrain2, yTrain2)\npreds_decision_tree2 = decision_tree_classifier_2features.predict(Xtest2)\n\n\nprint('MAE :',mean_absolute_error(yTest2, preds_decision_tree2))\nprint('Accuracy decision tree:',accuracy_score(yTest2,preds_decision_tree2))\n\n\n# random forest:\nrandom_forest_classifier_2features = RandomForestClassifier(random_state=42)\nrandom_forest_classifier_2features.fit(Xtrain2, yTrain2)\npreds_random_forest2 = random_forest_classifier_2features.predict(Xtest2)\n\n\nprint('MAE :',mean_absolute_error(yTest2, preds_random_forest2))\nprint('Accuracy random forest:',accuracy_score(yTest2,preds_random_forest2))\n","67110800":"Before preparing the data matrix and the target vector it is important to check that our data is stilil balanced target-wise, after removals of duplicate rows etc. We need to make sure we have at least a 70:30 ratio between success(1) and failure(0) or we will have to balance the data. We see that we are OK.","33565ad9":"3 more categorical features now need to be encoded: Launched_at, sub_category, and country.\nThis will be done in a number of methods:\n\nCountry column will be encoded using binary encoding.\nBinary encoding combines Hash encoding and one-hot encoding. The country feature will first be converted into numerical using an ordinal encoder, and then the numbers will be transformed to binary numbers. The binary value will be split into different columns.\nBinary encoding was chosen since it works well when there are a high number of categories and it is efficient in feature incrementation.\n\nSub_category column will be encoded using count\/frequency encoding, replacing the category by the frequency of observations in the dataset. \n\nLaunched_at will be encoded with one-hot encoding.\n\n\nAfterwards double-check all features are numerical","b68addfc":"Top two most influential features are the backers_count and the goal_usd, which seems to make sense. \nNonetheless, the feature_importance method is biased and prefers features with high cardinality,\nwhich these two features have.\nChecking the random forest as well:\n","ec91cd91":"Well, the decision tree achieved 93% success with max_depth 4 and 94% with max_depth 8. We will continue on and check random forest:\n","1d458e86":"Move on to checking more columns, as the goal is to transform them all to numerical data for the prediction models.","b6dc5226":"In this project we would like to make a classification regarding the success\/failure of Kickstarter Capaigns.\nData will be read and some preprocessing will be done before applying various prediction models.\n\nBegin by importing useful libraries for use in the project:","9d022c17":"We can see there are no nulls, some columns are numeric and others are of type object.\n\nLet's check for duplicated rows and remove them. In order to do this, first we wll remove column 'Unnamed: 0 ' as it is an index that will not allow us to detect duplicates: ","76527c8b":"Accuracy of random forest is 94.4%.\nLast model to check is gradient boosting:\n\n","f89fbb5c":"Some columns will not be useful in our prediction models. These will be removed:","3bde5592":"This model does not classify our data properly. Naive Bayesian models are particularly useful for small & medium sized data sets, and this dataset is quite large. That might be one reason. Also, another limitation of Naive Bayes is the assumption of independent predictors. When the Naive assumption does not hold true we may get poor results. \nLet's try working with KNN model: ","79e1c046":"Begin with Naive Bayes model, using KFold with k=10","07a719c8":"We will now run some models on the data.\nIt is preferable to use KFold cross-validation, but for some of the models this is quite \nheavy in computation time. Therefore the data will also be split using train-test split \nand some predictions will be made without KFold as well.","97e4b7ae":"Now let's read the data, check its size and get acquainted with it","2345075d":"We notice that the distribution of countries is highly skewed, since the vast majority of projects originates from the US.\nWe will tend to this later. ","87904d6b":"In the case of random forest, the top two features are the same.\nLet's do one more experiment, and try to make the predictions based on just these two features (backers_count, goal_usd)","7aebe510":"There is definitely one outlier, the -5 valued goal_usd, but actually all negative values represent values that are 1 or bellow. \nThese might be typos in the original data (perhaps values that are 1000 were written as 1.000 as is the syntax in some countries).\nHowever, we cannot determine the reason for the goal_usd to be of such a low value as 1. \nIn addition, values over 15 also represent extremely high goal_usd's.\nLet's check how many instances satisfy either ofthese two conditions: ","cf9dbf60":"Results are quite good. It seems that the models rely heavily on these two features,\nand the other features just add some fine-tuning which allow us to raise the accuracy by just 1.5% more.\nThis is possible largely because the dataset is large, and would not work on a small dataset.\n","37cc78a2":"The goal_usd is extremely different in its values from the rest of the columns.\nWe will apply a logarithm transformation on it. There are quite a few benefits to using log transform:\nIt helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\nIt also decreases the effect of the outliers due to the normalization of magnitude differences and the model become more robust.\nThe data on which log transform is applied must have only positive values, therefore the goal_usd fits this transformation.","59042307":"Accuracy is 91.1% with 5 neighbors, 90.7% with 3 neighbors. Much better than the Naive Bayes model. Let's see if we can do better, using a decision tree in two versions, one with max_depth 4 and a deeper one with max_depth 8:","3e82a051":"It seems that this is the best accuracy we are reaching at the moment.  \nTo end this analysis we would like to check the feature importance in a couple of the models,\nin order to get an idea which are the most influential features of the models:","8a874f53":"One more column where outliers might appear is the backers_count. Let's check the values\nthat it contains:"}}