{"cell_type":{"ee3bcf7b":"code","94f308c0":"code","e3f2d556":"code","0864b557":"code","e1393fba":"code","338885a4":"code","b13ac680":"code","b4704635":"code","9fb59983":"code","33967274":"code","547aba53":"code","f43d9066":"code","62d1d605":"code","e70111ca":"code","d09c42f3":"code","73f81175":"code","bb94f34b":"code","c2bb009a":"code","a029a17f":"code","6eca9574":"code","0f67b440":"markdown","77c9f368":"markdown","7996bbcb":"markdown","bdefca72":"markdown","7e4c4f85":"markdown","e666fa0e":"markdown","371a333d":"markdown","27adb213":"markdown","5553768e":"markdown","cfd73a93":"markdown","34f72445":"markdown"},"source":{"ee3bcf7b":"# Very few imports. This is a pure torch solution!\nimport cv2\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN","94f308c0":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBASE_DIR = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/\"\n\nNUM_EPOCHS = 20","e3f2d556":"df = pd.read_csv(\"..\/input\/reef-cv-strategy-subsequences-dataframes\/train-validation-split\/train-0.1.csv\")\n\n# Turn annotations from strings into lists of dictionaries\ndf['annotations'] = df['annotations'].apply(eval)\n\n# Create the image path for the row\ndf['image_path'] = \"video_\" + df['video_id'].astype(str) + \"\/\" + df['video_frame'].astype(str) + \".jpg\"\n\ndf.head()","0864b557":"df_train, df_val = df[df['is_train']], df[~df['is_train']]","e1393fba":"# The model doesn't support images with no annotations\n# It raises an error that suggest that it just doesn't support them:\n# V    alueError: No ground-truth boxes available for one of the images during training\n# I'm dropping those images for now\n# https:\/\/discuss.pytorch.org\/t\/fasterrcnn-images-with-no-objects-present-cause-an-error\/117974\/3\ndf_train = df_train[df_train.annotations.str.len() > 0 ].reset_index(drop=True)\ndf_val = df_val[df_val.annotations.str.len() > 0 ].reset_index(drop=True)","338885a4":"df_train.shape[0], df_val.shape[0]","b13ac680":"class ReefDataset:\n\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.transforms = transforms\n\n    def can_augment(self, boxes):\n        \"\"\" Check if bounding boxes are OK to augment\n        \n        \n        For example: image_id 1-490 has a bounding box that is partially outside of the image\n        It breaks albumentation\n        Here we check the margins are within the image to make sure the augmentation can be applied\n        \"\"\"\n        \n        box_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\n        return not box_outside_image\n\n    def get_boxes(self, row):\n        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n        \n        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n        \n        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        return boxes\n    \n    def get_image(self, row):\n        \"\"\"Gets the image for a given row\"\"\"\n        \n        image = cv2.imread(f'{BASE_DIR}\/{row[\"image_path\"]}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        return image\n    \n    def __getitem__(self, i):\n\n        row = self.df.iloc[i]\n        image = self.get_image(row)\n        boxes = self.get_boxes(row)\n        \n        n_boxes = boxes.shape[0]\n        \n        # Calculate the area\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        \n        \n        target = {\n            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n            'area': torch.as_tensor(area, dtype=torch.float32),\n            \n            'image_id': torch.tensor([i]),\n            \n            # There is only one class\n            'labels': torch.ones((n_boxes,), dtype=torch.int64),\n            \n            # Suppose all instances are not crowd\n            'iscrowd': torch.zeros((n_boxes,), dtype=torch.int64)            \n        }\n\n        if self.transforms and self.can_augment(boxes):\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            if n_boxes > 0:\n                target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        else:\n            image = ToTensorV2(p=1.0)(image=image)['image']\n\n        return image, target\n\n    def __len__(self):\n        return len(self.df)","b4704635":"def get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","9fb59983":"# Define datasets\nds_train = ReefDataset(df_train, get_train_transform())\nds_val = ReefDataset(df_val, get_valid_transform())","33967274":"# Let's get an interesting one ;)\ndf_train[df_train.annotations.str.len() > 12].head()","547aba53":"image, targets = ds_train[2200]\nimage","f43d9066":"targets","62d1d605":"boxes = targets['boxes'].cpu().numpy().astype(np.int32)\nimg = image.permute(1,2,0).cpu().numpy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(img,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(img);","e70111ca":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndl_train = DataLoader(ds_train, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\ndl_val = DataLoader(ds_val, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)","d09c42f3":"def get_model():\n    # load a model; pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    num_classes = 2  # 1 class (starfish) + background\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    model.to(DEVICE)\n    return model\n\nmodel = get_model()","73f81175":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.0015, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\nn_batches, n_batches_val = len(dl_train), len(dl_val)\nvalidation_losses = []\n\n\nfor epoch in range(NUM_EPOCHS):\n    time_start = time.time()\n    loss_accum = 0\n    \n    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n        \n        images = list(image.float().to(DEVICE) for image in images)\n        targets = [{k: v.to(torch.float32).to(DEVICE) if \"box\" in k else v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        # Predict\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_accum += loss_value\n\n        # Back-prop\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    # Validation \n    val_loss_accum = 0\n        \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n            images = list(image.float().to(DEVICE) for image in images)\n            targets = [{k: v.to(torch.float32).to(DEVICE) if \"box\" in k else v.to(DEVICE) for k, v in t.items()} for t in targets]\n            \n            val_loss_dict = model(images, targets)\n            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n            val_loss_accum += val_batch_loss.item()\n    \n    # Logging\n    val_loss = val_loss_accum \/ n_batches_val\n    train_loss = loss_accum \/ n_batches\n    validation_losses.append(val_loss)\n    \n    # Save model\n    chk_name = f'fasterrcnn_resnet50_fpn-e{epoch}.bin'\n    torch.save(model.state_dict(), chk_name)\n    \n    \n    elapsed = time.time() - time_start\n    \n    print(f\"[Epoch {epoch+1:2d} \/ {NUM_EPOCHS:2d}] Train loss: {train_loss:.3f}. Val loss: {val_loss:.3f} --> {chk_name}  [{elapsed:.0f} secs]\")   ","bb94f34b":"validation_losses","c2bb009a":"np.argmin(validation_losses)","a029a17f":"# idx = 0\n\n# images, targets = next(iter(dl_val))\n# images = list(img.to(DEVICE) for img in images)\n# targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n# boxes = targets[idx]['boxes'].cpu().numpy().astype(np.int32)\n# sample = images[idx].permute(1,2,0).cpu().numpy()\n\n# model.eval()\n\n# outputs = model(images)\n# outputs = [{k: v.detach().cpu().numpy() for k, v in t.items()} for t in outputs]","6eca9574":"# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# # Red for ground truth\n# for box in boxes:\n#     cv2.rectangle(sample,\n#                   (box[0], box[1]),\n#                   (box[2], box[3]),\n#                   (220, 0, 0), 3)\n\n    \n# # Green for predictions\n# # Print the first 5\n# for box in outputs[idx]['boxes'][:5]:\n#     cv2.rectangle(sample,\n#                   (box[0], box[1]),\n#                   (box[2], box[3]),\n#                   (0, 220, 0), 3)\n\n# ax.set_axis_off()\n# ax.imshow(sample);","0f67b440":"# **Load df** \n# from [CoTS - CV strategy: subsequences](https:\/\/www.kaggle.com\/warotjanpinitrat\/cv-strategy-subsequences)","77c9f368":"### For understanding and learning purpose, I will leave the link to model document [click here](https:\/\/pytorch.org\/vision\/stable\/_modules\/torchvision\/models\/detection\/faster_rcnn.html)","7996bbcb":"# **Import**","bdefca72":"# **Model**","7e4c4f85":"# **Dataset Class**","e666fa0e":"# **DataLoader**","371a333d":"# **Reference**\n###  [\ud83d\udc20 Reef- Starter Torch FasterRCNN Train [LB=0.416]](https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-416)\n###  [Pytorch Starter - FasterRCNN Train](https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train)","27adb213":"#### You can criticize my work or give your suggestion, your comment is a treasure of knowledge for me\n##### P.S. sorry for a poor grammar","5553768e":"# **Augmentation**","cfd73a93":"# **Check result**","34f72445":"# **About author: I'm a beginner in this field trying to learn and discovering the enjoyment of Data Science.**\n### Note1: This notebook is a copy version plus some editing and experimenting for my own understanding and learning.\n### Note2: If this notebook is useful for you in anyway, please give an upvote or commenting your gratitude on the notebook in the reference section. "}}