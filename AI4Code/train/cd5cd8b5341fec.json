{"cell_type":{"4b4d23d0":"code","eba95032":"code","60841fc5":"code","79fdebbc":"code","5c186931":"code","e8fd6579":"code","5b61b002":"code","ed5653d3":"code","3d6a2771":"code","c88064f4":"code","9b464d29":"markdown","9fe66b0a":"markdown","411e3c10":"markdown","8fa7a1a2":"markdown","3c2e682b":"markdown","d79cea17":"markdown","7b0df43b":"markdown","3325739d":"markdown","dd840617":"markdown"},"source":{"4b4d23d0":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import ensemble\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_validate,GridSearchCV\nimport lightgbm as lgbm\nfrom collections import Counter","eba95032":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","60841fc5":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n\n# 1.detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n\n# 2.get train row size \ntrain_len = len(train)\n\n# 3.concat train + test \ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n\n# 4. empty data => np.nan\ndataset = dataset.fillna(np.nan)\n\n# 5.column encoding and create new features\n# 5.1.Fare\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].mean())\n# dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n# 5.2.Embarked\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(dataset[\"Fare\"].mode()[0])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")\n\n# 5.3.Sex\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n# dataset = pd.get_dummies(dataset, columns = [\"Sex\"])\n\n# 5.4.Age\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].mean()\n    dataset['Age'].iloc[i] = age_med\n#     age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n#     if not np.isnan(age_pred) :\n#         dataset['Age'].iloc[i] = age_pred\n#     else :\n#         dataset['Age'].iloc[i] = age_med\n        \n        \n# 5.5.Name         \n# dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n# dataset[\"Title\"] = pd.Series(dataset_title)\n# dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n# dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n# dataset[\"Title\"] = dataset[\"Title\"].astype(int)\n# dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\n# dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n\n# 5.6.SibSp(Number of Siblings\/Spouses Aboard) + Parch(Number of Parents\/Children Aboard) =>Fsize\ndataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\ndataset['IsSingle'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n# dataset['IsSmallFamily'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n# dataset['IsMedFamily'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n# dataset['IsLargeFamily'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\n# #5.7.Cabin\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")\n\n# #5.8.Ticket \nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")\n\n# 5.9.Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\n\n# 5.10.PassengerId\ndataset.drop(labels = [\"Name\",\"PassengerId\"], axis = 1, inplace = True)\n\n# 6.devide train test \ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","79fdebbc":"train.info()","5c186931":"y = train['Survived']\nX = train.drop(['Survived'], axis=1)\nX_test = test","e8fd6579":"params={\"n_estimators\":np.arange(50,200,10),\n        \"max_depth\":np.arange(1,11,1),\n        \"learning_rate\":[0.1,0.01,0.001]\n       }","5b61b002":"xgb_est=lgbm.LGBMClassifier( \n                            random_state=42, \n                              objective='binary',\n                              eval_metric=\"auc\"\n               \n        )\n\ngr_xgb_est=GridSearchCV( xgb_est,param_grid=params,cv=5,n_jobs=-1,verbose=10)\n\ngr_xgb_est.fit(X,y)","ed5653d3":"gr_xgb_est.best_estimator_.get_params()","3d6a2771":"pred_test = gr_xgb_est.predict(X_test)","c88064f4":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = (pred_test > 0.5).astype(int)\nsubmission.to_csv('sub.csv', index=False)\nsubmission.head()","9b464d29":"# split data ( input data \/ label data )","9fe66b0a":"# import libraries","411e3c10":"# load data ","8fa7a1a2":"# preprocessing","3c2e682b":"# submit result","d79cea17":"# predict test data using model","7b0df43b":"# evaluate model","3325739d":"gbm = LGBMClassifier(learning_rate=0.01, first_metric_only = True)\n\ngbm.fit(train_X, train_Y,eval_set =[(test_X,test_Y)] , eval_metric=['auc'],\n        early_stopping_rounds=10,verbose = 2)","dd840617":"# GridSearch Ensembel "}}