{"cell_type":{"5b62d059":"code","22ffc3e2":"code","e7cac959":"code","6f7245dd":"code","7491ed6d":"code","2935277d":"code","7543b6da":"code","491c0487":"code","e207d358":"code","d3e3ddba":"code","afa6bac3":"code","072333e9":"code","c15b2ab6":"code","30bb8661":"code","71b69cb0":"code","fe117731":"code","8441d6eb":"code","53961acb":"code","0d0cc52a":"markdown","3c582e51":"markdown","63488c14":"markdown","4550804f":"markdown","06591950":"markdown","c36a3d58":"markdown","eb2d5390":"markdown","66217395":"markdown","1cf6d7c3":"markdown","baa79d8c":"markdown","77bb5ada":"markdown","0edde188":"markdown","8648934a":"markdown","19143220":"markdown","03b78127":"markdown","797fef23":"markdown","b650d59f":"markdown","3dbf1051":"markdown","6756ddc7":"markdown","dedb71fd":"markdown"},"source":{"5b62d059":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","22ffc3e2":"# load data\nx_l = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nimg_size = 64","e7cac959":"# sample data representation\nplt.subplot(1, 2, 1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')\nplt.show()","6f7245dd":"X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","7491ed6d":"# data division train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\nprint('X_train: ',X_train.shape)\nprint('X_test: ',X_test.shape)\nprint('Y_train: ',Y_train.shape)\nprint('Y_test: ',Y_test.shape)","2935277d":"# Converting the 3D matrix to 2D matrix\nprint('X_train: ',X_train.shape)\nprint('X_test: ',X_test.shape)\n\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n\nX_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\n\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)\n\nx_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\n\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","7543b6da":"# initialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\n        'weight1':np.random.randn(3,x_train.shape[0]) * 0.1,\n        'bias1':np.zeros((3,1)),\n        'weight2':np.random.randn(y_train.shape[0],3) * 0.1,\n        'bias2':np.zeros((y_train.shape[0],1))\n    }\n    return parameters","491c0487":"def sigmoid_func(z):\n    \"\"\"  \n    Sigmoid function f(z) = 1 \/ 1 + e^-z\n    \"\"\"\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","e207d358":"# Forward Propagation\ndef forward_propagation_NN(x_train, parameters):\n    Z1 = np.dot(parameters['weight1'],x_train) + parameters['bias1']\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters['weight2'],A1) + parameters['bias2']\n    A2 = sigmoid_func(Z2)\n    \n    cache = {\n        'Z1':Z1,\n        'A1':A1,\n        'Z2':Z2,\n        'A2':A2\n    }\n    \n    return A2, cache","d3e3ddba":"def compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = - np.sum(logprobs) \/ Y.shape[1]\n    return cost","afa6bac3":"# Backward Propagations\ndef backward_propagation_NN(parameters, cache, X, Y):\n    dZ2 = cache['A2'] - Y\n    dW2 = np.dot(dZ2, cache['A1'].T) \/ X.shape[1]\n    db2 = np.sum(dZ2, axis = 1, keepdims = True) \/ X.shape[1]\n    dZ1 = np.dot(parameters['weight2'].T, dZ2) * (1 - np.power(cache['A1'], 2))\n    dW1 = np.dot(dZ1, X.T) \/ X.shape[1]\n    db1 = np.sum(dZ1, axis = 1, keepdims = True) \/ X.shape[1]\n    grads = {\n        'dweight1':dW1,\n        'dbias1':db1,\n        'dweight2':dW2,\n        'dbias2':db2,\n    }\n    return grads","072333e9":"# Update Function\ndef update_parameters_NN(parameters, grads, learning_rate = 0.03):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    return parameters","c15b2ab6":"# Prediction\ndef predict_NN(parameters,x_test):\n    A2, cache = forward_propagation_NN(x_test, parameters)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction","30bb8661":"def two_layer_neural_network(x_train, y_train, x_test, y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n    \n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation_NN(x_train, parameters)\n        cost = compute_cost_NN(A2, y_train, parameters)\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    y_prediction_test = predict_NN(parameters, x_test)\n    y_prediction_train = predict_NN(parameters, x_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters","71b69cb0":"parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","fe117731":"x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","8441d6eb":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library","53961acb":"def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \", mean)\nprint(\"Accuracy variance: \", variance)","0d0cc52a":"# Converting the 3D matrix to 2D matrix\n* We have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first.\n* Our label array (Y) is already flatten(2D) so we leave it like that.","3c582e51":"# INTRODUCTION\n* I wouldn't try my first deep learning experience.\n* I first tried to create the neural network model by hand. Then I did the same with the keras library.\n* I wish you good readings. The conclusion is about to meet again in the chapter.","63488c14":"# Data split train and test\n* With the train_test_split method we import from sklearn library, we divide the data into two as train and test.\n* We will train our model with the train part of the data we have divided and we will test our model with the test part.\n* We split the data to 80% train and 20% test.","4550804f":"We write to update function for weight and bias update.","06591950":"# Sample data representation\n* We are plotting our sample data.","c36a3d58":"* We write to sigmoid function","eb2d5390":"* We write to cost funtion","66217395":"# Conclusion\n* A notebook I've made to try what I've learned more about deep learning and try to better understand deep learning.\n* I'm new to programming. I'm even more new in data science, machine learning, deep learning and artificial intelligence. But I am working. And I'il be an artificial intelligence developer. Your comments are very important to me.\n* Thank you for reading my notebook. Waiting for your criticism.","1cf6d7c3":"* x_train and y_train parameter as a parameter and layer sizes to initialize the function of writing.","baa79d8c":"* In order to create image array, we concatenate zero sign and one sign arrays\n* Then we create label array 0 for zero sign images and 1 for one sign images.","77bb5ada":"* We write to forward propagation function","0edde188":"* Train and test data reshaping","8648934a":"* Using the keras library, we create our Neural Network model and put it into the cross_val_score method.\n* When I set the cv parameter of cross_val_score method to 4, accuracy is 74%, and 5 is 54%.","19143220":"# Content\n* Introduction\n* Import library\n* Load data\n* Sample data representation\n* Data split train and test\n* Converting the 3D matrix to 2D matrix\n* Manually writing Neural Network functions\n    * Intialize Parameters And Layer Sizes\n    * Sigmoid Function\n    * Forward Propagation\n    * Loss Function And Cost Function\n    * Backward Propagation\n    * Update Parameters Function\n    * Prediction Function\n    * Create Model\n* Neural Network With Keras Library\n* Conclusion","03b78127":"* Write to create model function.","797fef23":"# Load data\n* In this data there are 2062 sign language digits images.\n* At the beginning of tutorial we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1).","b650d59f":"* We write to prediction function for to test our model","3dbf1051":"* Create Model\n        * Since the learning rate was 0.01 when it was too slow, my model didn't learn very well and accuracy remained at 54%.\n        * When I increased the learning rate to 0.03, my model was well learned and accuracy increased to 96%.","6756ddc7":"* We write to backward propagation function","dedb71fd":"* Import keras library\n* From sklearn.model_selection import cross_val_score method "}}