{"cell_type":{"d94007ed":"code","624911f7":"code","614710d3":"code","4b3cf06e":"code","99ed6eba":"code","c8277f2a":"code","20a20c51":"code","f23a4046":"code","b045f69b":"code","0fc2dbbd":"code","68d6153a":"code","8ae0e8f6":"code","a166ce35":"code","9190a4ac":"code","305cbb75":"code","2034fba7":"code","3318a94d":"code","80cef190":"code","a6b63d67":"code","fc3a634a":"code","12276ecf":"code","d4921f5b":"code","16b64efa":"code","6d36ab39":"code","f5af634b":"code","e07a578d":"code","586d6459":"code","8492f373":"code","728cbc19":"code","d1cafb33":"code","03dd3ae7":"markdown","1105e8be":"markdown","c6526c4d":"markdown","5a9e7bef":"markdown","354a387d":"markdown","e9055628":"markdown","d46cdfe9":"markdown","fe36d61d":"markdown","9217a10b":"markdown","9b20295b":"markdown","a5c32b95":"markdown","a89fb2f3":"markdown","34a96cd6":"markdown","f065cea6":"markdown","7f9fe838":"markdown","e835d40f":"markdown"},"source":{"d94007ed":"conda install -c conda-forge hdbscan","624911f7":"conda install -c conda-forge kneed","614710d3":"#Import necessary libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nimport hdbscan\nimport folium\nimport re\nfrom tqdm import tqdm\nfrom kneed import KneeLocator","4b3cf06e":"#Load the data\ndf = pd.read_csv('..\/input\/taxi_data.csv')","99ed6eba":"#Check the contents of the data\nprint('-----First five lines of the data-----')\nprint(df.head())\nprint(\"\")\nprint('-----Data Shape-----')\nprint(df.shape)\nprint(\"\")\nprint('-----Information-----')\nprint(df.info())\nprint(\"\")\nprint('-----Check for correctness of data-----')\nprint(df.describe())","c8277f2a":"print('Duplicates: ',df.duplicated(subset=['LON','LAT']).values.sum())\nprint('Null values: ',df.isna().values.sum())","20a20c51":"#Remove duplicates and null values\nprint('Before removal of duplicates and null values: ',df.shape)\ndf.drop_duplicates(subset=['LON','LAT'],keep='first',inplace=True)\ndf.dropna(inplace=True)\nprint('After removal of duplicates and null values: ',df.shape)","f23a4046":"df.head()","b045f69b":"#Let us visualize the geographical data using folium\nX = df[['LAT','LON']].values\nmap = folium.Map(location=[X[:,0].mean(),X[:,1].mean()],tiles='Stamen Toner',zoom_start=10)","0fc2dbbd":"#Let us fill-in this map with the datapoints\nfor _,row in df.iterrows():\n    folium.CircleMarker(\n        location=[row.LAT,row.LON],\n        radius=5,\n        popup=re.sub(r'\\W+',' ',row.NAME),\n        fill=True\n    ).add_to(map)\n#Remember to remove non-word characters in popup since this result in a broken map \ntitle = '''<h3 align=\"center\" style=\"font-size:30px\"><b>Given Data<\/b><\/h3>'''\nmap.get_root().html.add_child(folium.Element(title))\nmap","68d6153a":"inertia=[]\nfor i in tqdm(range(1,101)):\n    kmeans = KMeans(n_clusters=i,random_state=0).fit(X)\n    inertia.append(kmeans.inertia_)\nplt.plot(range(1,101),inertia)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.xticks(np.arange(0,101,5))\nplt.show()","8ae0e8f6":"score=-1\nk=0\nfor i in tqdm(range(2,101)):\n    kmeans = KMeans(n_clusters=i,random_state=0).fit(X)\n    score2 = silhouette_score(X,kmeans.predict(X))\n    if score2>score:\n        score=score2\n        k=i\nprint('The best number of clusters based on silhouette score is {} with a score of {}.'.format(k,score))","a166ce35":"elbow_point = KneeLocator(range(1,101),inertia,curve='convex',direction='decreasing')\nprint(elbow_point.knee)","9190a4ac":"kmeans = KMeans(n_clusters=11,random_state=0).fit(X)\nkmeans_prediction = kmeans.predict(X)\ndf['Cluster_KMeans'] = kmeans_prediction\nprint(df.head())","305cbb75":"#Visualize the KMeans result\n\n#First create an array of color schemes\ncolors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', \n          'olive', 'goldenrod', 'lightcyan', 'navy','springgreen','midnightblue',\n         'red','brown','limegreen','lime','pink','orchid','crimson','m']*10\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\ndf['Colors_KMeans'] = vectorizer(kmeans_prediction)\n\ndef create_map(cluster_column, colors_column, title):\n    map = folium.Map(location=[X[:,0].mean(),X[:,1].mean()],tiles='Stamen Toner',zoom_start=8.5)\n    for _,row in df.iterrows():\n        folium.CircleMarker(\n            location=[row.LAT,row.LON],\n            radius=5,\n            popup=row[cluster_column],\n            fill=True,\n            color=row[colors_column],\n            fill_color=row[colors_column],\n        ).add_to(map)\n\n    print(title)\n    return map\n\ncreate_map('Cluster_KMeans','Colors_KMeans','KMeans Clustering')","2034fba7":"print('Number of clusters: 11')\n\nprint('Silhouette score: {}'.format(round(silhouette_score(X,kmeans_prediction),2)))","3318a94d":"# The eps is proportional to the expected number of neighbours, we can use the nearest neighbors to reach a fair estimation for eps.\n# We still use the kneed library to get the elbow point.\n\nnearest_neighbors = NearestNeighbors(n_neighbors=6)\nneighbors = nearest_neighbors.fit(X)\ndistances, indices = neighbors.kneighbors(X)\ndistances = np.sort(distances[:,5], axis=0)\ni = np.arange(len(distances))\nknee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n# We use the polynomial interp_method in this one to get a smooth line\nfig = plt.figure(figsize=(5, 5))\nknee.plot_knee()\nplt.xlabel(\"Points\")\nplt.ylabel(\"Distance\")\nplt.show()\nprint('The elbow point is at around {}'.format(round(distances[knee.knee],3)))","80cef190":"dbscan = DBSCAN(eps=0.045)\ndbscan.fit(X)\ndbscan_predictions = dbscan.labels_\ndf['Cluster_DBSCAN'] = dbscan_predictions\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\ndf['Colors_DBSCAN'] = vectorizer(dbscan_predictions)","a6b63d67":"create_map('Cluster_DBSCAN','Colors_DBSCAN','DBSCAN (eps=0.045)')","fc3a634a":"print('Number of clusters found: {}'.format(len(np.unique(dbscan_predictions))))\nprint('Number of outliers found: {}'.format(len(dbscan_predictions[dbscan_predictions==-1])))\n\noutliers=[(counter+2)*x if x==-1 else x for counter,x in enumerate(dbscan_predictions)]\nprint('Silhouette score ignoring outliers: {}'.format(silhouette_score(X[dbscan_predictions!=-1],\n                                                                       dbscan_predictions[dbscan_predictions!=-1])))\nprint('Silhouette score with outliers as singletons: {}'.format(silhouette_score(X,outliers)))","12276ecf":"scores_no_outlier=[]\nscores_with_outlier=[]\nmax_score=0\nbest_eps=0\nfor i in np.arange(0.15,0,-0.005):\n    dbscan = DBSCAN(eps=i)\n    dbscan.fit(X)\n    dbscan_predictions = dbscan.labels_\n    score_without_outlier=silhouette_score(X[dbscan_predictions!=-1], dbscan_predictions[dbscan_predictions!=-1])\n    scores_no_outlier.append(score_without_outlier)\n    outliers=[(counter+2)*x if x==-1 else x for counter,x in enumerate(dbscan_predictions)]\n    scores_with_outlier.append(silhouette_score(X,outliers))\n    \n    if score_without_outlier>max_score:\n        max_score = score_without_outlier\n        best_eps = i\n    \nplt.figure(figsize=(10,6))    \nplt.subplot(1,2,1)\nplt.plot(np.arange(0.15,0,-0.005),scores_no_outlier)\nplt.xlabel('Epsilon')\nplt.ylabel('Silhouette Score')\nplt.title('Scores Without Outlier')\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(0.15,0,-0.005),scores_with_outlier)\nplt.xlabel('Epsilon')\nplt.ylabel('Silhouette Score')\nplt.title('Scores With Outlier as Singletons')\nplt.show()\n\nprint('The best score {} is attained at epsilon of {}'.format(round(max_score,3),round(best_eps,3)))","d4921f5b":"dbscan = DBSCAN(eps=0.01)\ndbscan.fit(X)\ndbscan_predictions = dbscan.labels_\ndf['Cluster_DBSCAN_Silhouette'] = dbscan_predictions\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\ndf['Colors_DBSCAN_Silhouette'] = vectorizer(dbscan_predictions)\n\ncreate_map('Cluster_DBSCAN_Silhouette','Colors_DBSCAN_Silhouette','DBSCAN (eps=0.01)')","16b64efa":"print('Number of clusters found: {}'.format(len(np.unique(dbscan_predictions))))\nprint('Number of outliers found: {}'.format(len(dbscan_predictions[dbscan_predictions==-1])))\n\noutliers=[(counter+2)*x if x==-1 else x for counter,x in enumerate(dbscan_predictions)]\nprint('Silhouette score ignoring outliers: {}'.format(silhouette_score(X[dbscan_predictions!=-1],\n                                                                       dbscan_predictions[dbscan_predictions!=-1])))\nprint('Silhouette score with outliers as singletons: {}'.format(silhouette_score(X,outliers)))","6d36ab39":"hdbscan_model = hdbscan.HDBSCAN(cluster_selection_epsilon=0.01)\nhdbscan_predictions = hdbscan_model.fit_predict(X)","f5af634b":"df['Cluster_HDBSCAN'] = hdbscan_predictions\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\ndf['Colors_HDBSCAN'] = vectorizer(hdbscan_predictions)\n\ncreate_map('Cluster_HDBSCAN','Colors_HDBSCAN','HDBSCAN')","e07a578d":"print('Number of clusters found: {}'.format(len(np.unique(hdbscan_predictions))))\nprint('Number of outliers found: {}'.format(len(hdbscan_predictions[hdbscan_predictions==-1])))\n\noutliers=[(counter+2)*x if x==-1 else x for counter,x in enumerate(hdbscan_predictions)]\nprint('Silhouette score ignoring outliers: {}'.format(silhouette_score(X[hdbscan_predictions!=-1],\n                                                                       hdbscan_predictions[hdbscan_predictions!=-1])))\nprint('Silhouette score with outliers as singletons: {}'.format(silhouette_score(X,outliers)))","586d6459":"# First we get our training and prediction data\nX_train = df[df['Cluster_HDBSCAN']!=-1][['LON','LAT']]\ny_train = df[df['Cluster_HDBSCAN']!=-1]['Cluster_HDBSCAN']\n\nX_pred = df[df['Cluster_HDBSCAN']==-1][['LON','LAT']]","8492f373":"# Before we use the model, let us first check the best number of neighbors to use\nscores=[]\nfor i in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    scores.append(knn.score(X_train,y_train))\n\nplt.plot(range(1,11),scores)\nplt.xticks(np.arange(1,11,4))\nplt.show()","728cbc19":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\n\nknn_predictions = knn.predict(X_pred)\ndf['Cluster_Hybrid'] = df['Cluster_HDBSCAN']\ndf.loc[df['Cluster_HDBSCAN']==-1,'Cluster_Hybrid'] = knn_predictions\n\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])\ndf['Colors_Hybrid'] = vectorizer(df['Cluster_Hybrid'].values)\n\ncreate_map('Cluster_Hybrid','Colors_Hybrid','Hybrid (HDBSCAN + KNN)')","d1cafb33":"print('Number of clusters found: {}'.format(len(np.unique(df['Cluster_Hybrid']))))\nprint('Silhouette score: {}'.format(round(silhouette_score(X,df['Cluster_Hybrid']),2)))","03dd3ae7":"# **Clustering Geolocation Data**\n### The dataset given contains taxi rank locations. We want to define key clusters of these taxis where we can build service stations for all taxis operating in that region.","1105e8be":"# 4. DBSCAN\n### DBSCAN is another clustering algorithm which I find appropriate to use in this problem since we are dealing with geographical data which can have varying cluster shape and density. This algorithm also excludes noise\/outliers unlike KMeans which uses the whole dataset. \n\n### But before we proceed with DBSCAN calculation, we need to first find the most important parameter, epsilon, which is the maximum distance between points to create a cluster. We can find this using the NearestNeighbors algorithm.","c6526c4d":"### So it seems that we can use n_neighbors = 1","5a9e7bef":"### Since the elbow method and silhouette score fails in giving us the best number of clusters, this may mean that KMeans algorithm is not sufficient to use.\n### For the purpose of finding the elbow point, we use the kneed library. ","354a387d":"### The data is simple, it only contains 3 features. The min and max shows that there is no incorrect data.\n\n### Let us check if there are duplicates and null values.","e9055628":"# 5. HDBSCAN\n### We use another clustering algorithm called HDBSCAN which extends DBSCAN by converting it into a hierarchical clustering algorithm. In addition to being better for data with varying density, it\u2019s also faster than regular DBSCAN.","d46cdfe9":"### With HDBSCAN, we formed more clusters and less outliers are present.","fe36d61d":"# **7. Final Thoughts**\n\n### In this problem we used different clustering techniques. We started with KMeans algorithm which is the simplest and fastest clustering algorithm. We showed that DBSCAN performs well on geographical data but selecting the epsilon value is critical. The HDBSCAN algorithm is better to use on data with varying density which is applicable here since taxi density is higher around the city and is less away from the city.\n### We should also remember that the Silhouette score with -1 as worst and 1 as best should only be used as a guide in tuning hyperparameters and should not be taken as the absolute metric to say if the cluster is good or not. Such as the case with DBSCAN and HDBSCAN, although HDBSCAN has a bit lower Silhouette score, it was able to cluster more data and contains fewer outliers. Visualization of the result is also important to check if we obtain good result.\n### Another thing to point out is that combination of unsupervised and supervised algorithm is possible to handle outliers in the data.","9217a10b":"### So let us use n_clusters=11 in the KMeans algorithm.","9b20295b":"# 3. KMeans Clustering\n### We first use the simplest clustering method which is the KMeans algorithm.\n### First let us find the best number of clusters using Elbow method.","a5c32b95":"# 2. Data Visualization","a89fb2f3":"### The score using eps=0.045 which was derived using NearestNeighbor estimation is very low. Let us use another method to derive eps by checking the score at specified iterations.","34a96cd6":"### We get a better score by estimating the eps value using the Silhouette plot. More clusters are identified as well as outliers.","f065cea6":"### There is no obvious elbow point using the elbow method. Let us try to check using the silhouette value.","7f9fe838":"# 6. Taking care of the Outliers\n### We can address the presence of outliers by using a hybrid algorithm, HDBSCAN + KNearestNeighbors. The idea here is we combine an unsupervised with supervised learning algorithm. Since we already have cluster groups obtained from HDBSCAN, we will classify the outliers using KNN.","e835d40f":"# 1. Exploratory Data Analysis"}}