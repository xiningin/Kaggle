{"cell_type":{"220af013":"code","7743ec78":"code","c575749b":"code","e2255b4f":"code","97013acc":"code","96231353":"code","30cf7997":"code","e4ab6698":"code","efe7c522":"code","e6327233":"code","7cec319d":"code","d172fc5b":"code","fbf378ae":"code","a9e32928":"code","63c335fc":"code","a2abb0fe":"code","c2bcba06":"code","060e2943":"code","198c0b57":"code","e64910bf":"code","bd32b853":"code","73af4def":"code","e15c1754":"code","03a4c959":"markdown","8a030bf4":"markdown","5883d86c":"markdown","54f6bf00":"markdown","37f1f7f9":"markdown","a1dd8453":"markdown","f4345942":"markdown","bbffe89d":"markdown","a92a0f9c":"markdown","34e114dd":"markdown","02b0f4bd":"markdown","351b3b6a":"markdown","5f2fae22":"markdown","8947e556":"markdown","48142eba":"markdown","866c71a7":"markdown","15b7e685":"markdown"},"source":{"220af013":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","7743ec78":"df=pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv', sep=',')\ndf.head()","c575749b":"df['species'].value_counts()","e2255b4f":"df.columns","97013acc":"X=df.iloc[:,0:4].values\ny=df.iloc[:,4].values","96231353":"label_dict={1: 'Iris-setosa',\n           2: 'Iris-virginica',\n           3: 'Iris-versicolor'}\nfeature_dict={0: 'sepal_length',1: 'sepal_width',2: 'petal_length',3: 'petal_width'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for i in range(4):\n        plt.subplot(2,2,i+1)\n        for lab in ('Iris-setosa','Iris-virginica','Iris-versicolor'):\n            plt.hist(X[y==lab, i],\n                    label=lab,\n                    bins=10,\n                    alpha=0.3)\n        plt.xlabel(feature_dict[i])\n    plt.legend(loc='upper right', fancybox=True,fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()\n        ","30cf7997":"X_std.shape[0]-1","e4ab6698":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","efe7c522":"mean_vec=np.mean(X_std,axis=0)\ncov_mat=(X_std-mean_vec).T.dot((X_std-mean_vec))\/(X_std.shape[0]-1)\nprint(\"Covariance Matrix \\n%s\" %cov_mat)\n                 ","e6327233":"print(\"Numpy Covariance matrix \\n%s\" %np.cov(X_std.T))","7cec319d":"cov_mat=np.cov(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(cov_mat)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","d172fc5b":"corr_mat1=np.corrcoef(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(corr_mat1)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","fbf378ae":"cor_mat2=np.corrcoef(X.T)\neig_vals, eig_vecs=np.linalg.eig(cor_mat2)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)\n","a9e32928":"u, s, v=np.linalg.svd(X_std.T)\nu","63c335fc":"for ev in eig_vecs.T:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint(\"Everithing is ok\")","a2abb0fe":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","c2bcba06":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)","060e2943":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","198c0b57":"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('Matrix W:\\n', matrix_w)","e64910bf":"Y = X_std.dot(matrix_w)\n","bd32b853":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y[y==lab, 0],\n                    Y[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","73af4def":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","e15c1754":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y_sklearn[y==lab, 0],\n                    Y_sklearn[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='upper center')\n    plt.tight_layout()\n    plt.show()\n","03a4c959":"# Projection Onto the New Feature Space\nn this last step we will use the 4\u00d72-dimensional projection matrix W to transform our samples onto the new subspace via the equation\nY=X\u00d7W, where Y is a 150\u00d72 matrix of our transformed samples.","8a030bf4":"# We can clearly see that all three approaches yield the same eigenvectors and eigenvalue pairs:\n\n* Eigendecomposition of the covariance matrix after standardizing the data.\n* Eigendecomposition of the correlation matrix.\n* Eigendecomposition of the correlation matrix after standardizing the data.","5883d86c":"# Correlation Matrix","54f6bf00":"# Projection Matrix","37f1f7f9":"Next, we perform an eigendecomposition on the covariance matrix:","a1dd8453":"# PCA and Dimensionality Reduction\n* he desired goal is to reduce the dimensions of a d-dimensional dataset by projecting it onto a (k)-dimensional subspace (where k<d)\n*  in order to increase the computational efficiency while retaining most of the information\n* An important question is \u201cwhat is the size of k that represents the data \u2018well\u2019?\u201d\n* Later, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix\n* If some eigenvalues have a significantly larger magnitude than others, then the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the \u201cless informative\u201d eigenpairs is reasonable.\n","f4345942":"# Principal Component Analysis\n* Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more\n* In this tutorial, we will see that PCA is not just a \u201cblack box\u201d, and we are going to unravel its internals in 3 basic steps.\n* Large data  is useful but on computation it causes a lot of issue\n* PCA helps us to retain the correlation among the most important feature of the dataset\n# Linear Decomposition Analysis\n* LDA is linear transformation methods\n* LDA helps to find the direction which maximize the separation of different classes.\n* It helps in pattern classification\n* LDA determines the suitable feature of the dataset\n    ","bbffe89d":"# Shortcut - PCA in scikit-learn","a92a0f9c":"The plot above clearly shows that most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information.","34e114dd":"# Explained Variance: \nAfter sorting the eigenpairs, the next question is \u201chow many principal components are we going to choose for our new feature subspace?\u201d A useful measure is the so-called \u201cexplained variance,\u201d which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components.","02b0f4bd":"# Singular Value Decomposition","351b3b6a":"# Sorting Eigenpairs\nThe typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code","5f2fae22":"While the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Value Decomposition (SVD) to improve the computational efficiency. So, let us perform an SVD to confirm that the result are indeed the same:","8947e556":"# Eigendecomposition of the raw data based on the correlation matrix","48142eba":"# Conclusion \n* It helps us to reduce the dimesion of tha dataset\n* Here the helpful resources \n>https:\/\/sebastianraschka.com\/Articles\/2015_pca_in_3_steps.html","866c71a7":"# A Summary of the PCA Approach\n* Standardize the data.\n* Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition.\n* Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k\u2264d).\n* Construct the projection matrix W from the selected k eigenvectors.\n* Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.","15b7e685":"It\u2019s about time to get to the really interesting part: The construction of the projection matrix that will be used to transform the Iris data onto the new feature subspace. Although, the name \u201cprojection matrix\u201d has a nice ring to it, it is basically just a matrix of our concatenated top k eigenvectors."}}