{"cell_type":{"1a4b1482":"code","383aa058":"code","d08b5942":"code","e29af017":"code","5fdc9f36":"code","3c966d15":"code","5df84143":"code","8c2af66d":"code","d3f9b908":"code","e9482dc1":"code","7803cbd0":"code","eb704b13":"code","2a63459d":"code","5ddfc4a5":"code","78fbfb0e":"code","4d577790":"code","b14c789f":"code","75b6152b":"code","d139d7fb":"code","91b179e8":"code","7d66acbf":"code","841c9d77":"code","d5f02c88":"code","d28aa168":"markdown","ddce998d":"markdown","e843f822":"markdown","283b4c22":"markdown","a05423a7":"markdown","781cbe4d":"markdown","b64c10b4":"markdown","a5d5f37b":"markdown","7dc27e24":"markdown","a3a4acbe":"markdown","98e7197b":"markdown","9ba87bee":"markdown","5a386325":"markdown"},"source":{"1a4b1482":"# importation des librairies\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.polynomial.polynomial as nppol\n\n# lecture du fichier contenant les anomalies de temp\u00e9ratures (origine : Universit\u00e9 de Berkeley)\nfile = '..\/input\/estimated-global-landsurface-temperature-average\/Complete_TAVG_summary.txt'\ndata = np.loadtxt(file, skiprows=22, usecols=(0, 1, 2))\n\n# d\u00e9finition des variables n\u00e9cessaires au calcul d'inversion\nt = data[:, 0] # premi\u00e8re colonne = les ann\u00e9es\ntemp = data[:, 1] # deuxi\u00e8me colonne = la valeur de l'anomalie de temp\u00e9rature en \u00b0C\nunc = data[:,2] # incertitude sur le calcul des anomalies (intervalle de confiance 95%) en \u00b0C\n\n#Affichage de la s\u00e9rie temporelle\nplt.figure(figsize=(15, 8))\nplt.errorbar(t, temp, yerr=unc) \nplt.ylabel(\"Anomalie de temp\u00e9rature (\u00b0C)\")\nplt.xlabel('Ann\u00e9e')\nplt.title(\"Anomalie de temp\u00e9rature annuelle moyenne sur la surface des continents\")\nplt.show()\n\n\n\n\n\n","383aa058":"#Premi\u00e8re m\u00e9thode pour la r\u00e9gression lin\u00e9aire.\n\nP, param = nppol.polyfit(t, temp, 1, full=True)\n\nprint(P)\nprint(param)\n\na_pol = P[1]\nb_pol = P[0]\n\nresidu_pol = param[0][0] \/ len(t)\nprint(f\"a_pol = {a_pol.round(2)} , b_pol = {b_pol.round(2)} ; residu pond\u00e9r\u00e9 = {residu_pol.round(2)}\")\n\n# affichage\nplt.figure(figsize=(15, 8))\nplt.plot(t, temp, 'b', label='Donn\u00e9es mesur\u00e9es', markersize=5)  # nuage de points mesur\u00e9s\n# utilisation de la fonction polyval du module polynomial de numpy afin de tracer le poly\u00f4me de coeffs P en fonction de x\nplt.plot(t, nppol.polyval(t, P), 'r', label='Polyn\u00f4me ajust\u00e9')  # Polyn\u00f4me d'ajustement\nplt.xlabel('temps en ann\u00e9es')\nplt.ylabel('anomalies de temp\u00e9rature')\nplt.legend()\nplt.title(\"Ajustement polynomial des donn\u00e9es d'anomalies de temp\u00e9ratures entre 1880 et 2019\")\nplt.errorbar(t, temp, yerr=unc)\nplt.show()","d08b5942":"#Seconde m\u00e9thode pour la r\u00e9gression lin\u00e9aire.\n\n# d\u00e9finition de la matrice \"donn\u00e9es\" \nd = temp.T\nx = np.linspace(1879,2019,140)\n\n# d\u00e9finition de la matrice \"loi physique\"  \nG = np.vstack([x, np.ones(x.size)]).T\n\n# r\u00e9solution par m\u00e9thode des moindres carr\u00e9s\nm = np.linalg.lstsq(G, d, rcond=None)\nprint (m)\n\n# d\u00e9finition des param\u00e8tres (a et b) optimaux issu de l'inversion\na_opt, b_opt = m[0]\n# d\u00e9finition du r\u00e9sidu\nres = m[1][0]\n# d\u00e9finiton du r\u00e9sidu pond\u00e9r\u00e9 par le nombre de points n\nres_w = res\/len(temp)\nprint(f\"a_opt = {a_opt.round(2)} , b_opt = {b_opt.round(2)} ; r\u00e9sidu = {res.round(2)} ; r\u00e9sidu pond\u00e9r\u00e9 = {res_w.round(2)}\")\n\n# affichage\nplt.figure(figsize=(15, 8))\nplt.plot(t, temp, 'b', label=\"Anomalie de temp\u00e9rature (\u00b0C)\", markersize=5)  # Temp\u00e9ratures\nplt.plot(t, a_opt * x + b_opt, 'r', label='Droite ajust\u00e9e')  # droite de r\u00e9gression \/ mod\u00e8le ajust\u00e9\nplt.xlabel('Ann\u00e9e')\nplt.ylabel(\"Anomalie de temp\u00e9rature (\u00b0C)\")\nplt.legend()\nplt.title(\"R\u00e9gression sur l'anomalie de temp\u00e9rature annuelle moyenne sur la surface des continents\")\nplt.errorbar(t, temp, yerr=unc)\nplt.show()","e29af017":"model = temp - nppol.polyval(t, P)\n\n# affichage\nplt.figure(figsize=(15, 8))\nplt.plot(t, temp, 'b', label='Donn\u00e9es mesur\u00e9es', markersize=5)  # nuage de points mesur\u00e9s\nplt.plot(t, model, 'r', label='Polyn\u00f4me corrig\u00e9 de la regression')  # Polyn\u00f4me d'ajustement\nplt.xlabel('temps en ann\u00e9es')\nplt.ylabel('anomalies de temp\u00e9rature')\nplt.legend()\nplt.title(\"Diff\u00e9rence entre les donn\u00e9es et le mod\u00e8le d'anomalie de temp\u00e9ratures moyennes\")\nplt.errorbar(t, temp, yerr=unc)\nplt.show()","5fdc9f36":"#Voici une possibilit\u00e9 pour am\u00e9liorer notre mod\u00e8le : faire une r\u00e9gression quadratique.\n\nP, param = nppol.polyfit(t, temp, 2, full=True)\n\nprint(P)\nprint(param)\n\na_pol = P[2]\nb_pol = P[1]\nc_pol = P[0]\nresidu_pol = param[0][0] \/ len(t)\nprint(f\"a_pol = {a_pol.round(2)} , b_pol = {b_pol.round(2)}, c_pol = {c_pol.round(2)} ; residu pond\u00e9r\u00e9 = {residu_pol.round(2)}\")\n\n\nmodel_quad = temp - nppol.polyval(t, P)\n\n\n# affichage\nplt.figure(figsize=(15, 8))\nplt.plot(t, temp, 'b', label='Donn\u00e9es mesur\u00e9es', markersize=5)  # nuage de points mesur\u00e9s\n# utilisation de la fonction polyval du module polynomial de numpy afin de tracer le poly\u00f4me de coeffs P en fonction de x\nplt.plot(t, nppol.polyval(t, P), 'r', label='Polyn\u00f4me ajust\u00e9')  # Polyn\u00f4me d'ajustement\nplt.plot(t, model_quad, 'r', label='Polyn\u00f4me corrig\u00e9 de la regression')  # Polyn\u00f4me d'ajustement\nplt.xlabel('temps en ann\u00e9es')\nplt.ylabel('anomalies de temp\u00e9rature')\nplt.legend()\nplt.title(\"Ajustement polynomial des donn\u00e9es d'anomalies de temp\u00e9ratures entre 1880 et 2019\")\nplt.errorbar(t, temp, yerr=unc)\nplt.show()","3c966d15":"#Early signifie que l'on est avant 1970\nP_early, param_early = nppol.polyfit(t[0:94], temp[0:94], 1, full=True);\n\nprint(P_early)\nprint(param_early)\n\na_pol_early = P_early[1];\nb_pol_early = P_early[0];\n\nresidu_pol_early = param_early[0][0] \/ len(t[0:95]);\nprint(f\"a_pol_early = {a_pol_early.round(2)} , b_pol_early = {b_pol_early.round(2)} ; residu pond\u00e9r\u00e9_early = {residu_pol_early.round(2)}\")\n\n#Late signifie que l'on est apr\u00e8s 1970\n\nP_late, param_late = nppol.polyfit(t[95:140], temp[95:140], 1, full=True)\n\nprint(P_late)\nprint(param_late)\n\na_pol_late = P_late[1];\nb_pol_late = P_late[0];\n\nresidu_pol_late = param_late[0][0] \/ len(t[95:140]);\nprint(f\"a_pol_late = {a_pol_late.round(2)} , b_pol_late = {b_pol_late.round(2)} ; residu pond\u00e9r\u00e9_late = {residu_pol_late.round(2)}\")\n\npoly_late = nppol.polyval(t, P_late)\n# affichage\nplt.figure(figsize=(15, 8))\nplt.plot(t, temp, 'b', label='Donn\u00e9es mesur\u00e9es', markersize=5)  # nuage de points mesur\u00e9s\n# utilisation de la fonction polyval du module polynomial de numpy afin de tracer le poly\u00f4me de coeffs P en fonction de x\nplt.plot(t[0:94], nppol.polyval(t[0:94], P_early[0:94]), 'r', label='Polyn\u00f4me ajust\u00e9')  # Polyn\u00f4me d'ajustement\nplt.plot(t[94:], poly_late[94:], 'k', label='Polyn\u00f4me ajust\u00e9')  # Polyn\u00f4me d'ajustement\nplt.xlabel('temps en ann\u00e9es')\nplt.ylabel('anomalies de temp\u00e9rature')\nplt.legend()\nplt.title(\"Ajustement polynomial des donn\u00e9es d'anomalies de temp\u00e9ratures entre 1880 et 2019\")\nplt.errorbar(t, temp, yerr=unc)\nplt.show()\n\n#print (nppol.polyval(t, P_late))","5df84143":"#d\u00e9coupage du jeu de donn\u00e9es avec une ann\u00e9e de coupe \u00e9gale \u00e0 1974. \ntemp_early= data[0:95, 1]\ntemp_late = data[95:140, 1]\ndate_early = data[0:95, 0]\ndate_late = data[95:140, 0]\n\n#sigmainf = np.std(tempinf)\n#sigmasup = np.std(tempsup)\nsigma_early = data[0:95, 2]\nsigma_late= data[95:140, 2]\nn_early = len(temp_early)\nn_late = len(temp_late)\n\nprint(len(sigma_early))\nprint(len(sigma_late))\n\nx_early = np.linspace(1,95,95)\nx_late = np.linspace(95, 140, 45)\n\nprint(x_late)","8c2af66d":"# D\u00e9finition du g\u00e9n\u00e9rateur de nombres pseudo-al\u00e9atoires \u00e0 partir d'une graine\/seed (=n'importe quel entier)\nnp.random.seed(123)\n#np.random.seed(456)","d3f9b908":"\n# * Fonctions *\n# *************\ndef droite(x_early, a, b):\n    return a * x_early + b\n\n# fonction densit\u00e9 de probabilit\u00e9\ndef L(temp_early, yfit, n_early, sigma_early):\n    return np.exp( -(1\/n_early) * np.sum(((temp_early - yfit) \/ sigma_early ) ** 2) )\n\n# fonction r\u00e9sidu\ndef res(temp_early, yfit, n_early, sigma_early):\n    return (1\/n_early) * np.sum(((temp_early - yfit) \/ sigma_early ) ** 2)","e9482dc1":"\n# information a priori sur a :\na_early_inf = 0 #la pente ne peut pas \u00eatre n\u00e9gative\na_early_sup = 0.01 #elle ne peut pas \u00eatre sup\u00e9rieure \u00e0 celle dans le cas d'une seule droite de r\u00e9gression\n\n# information a priori sur b :\nb_early_inf = -1\nb_early_sup = -0.3","7803cbd0":"# a) d\u00e9finition du mod\u00e8le initial\n\n# param\u00e8tre a0 du mod\u00e8le initial\na_0_early = a_early_inf + np.random.uniform() * (a_early_sup - a_early_inf)\n\n# param\u00e8tre b0 du mod\u00e8le initial\nb_0_early = b_early_inf + np.random.uniform() * (b_early_sup - b_early_inf)\n\n# b) Calcul de la fonction de densit\u00e9 de probabilit\u00e9 associ\u00e9e\n\n# calcul du mod\u00e8le initial (=y_0) \u00e0 partir de l'\u00e9quation y = ax + b avec les param\u00e8tres a_0 et b_0\ny_0_early = droite(x_early, a_0_early, b_0_early)\nL_0_early = L(temp_early, y_0_early, n_early, sigma_early)\nprint(a_0_early, b_0_early, L_0_early)","eb704b13":"# cr\u00e9ation de la liste qui contiendra toutes les valeurs des param\u00e8tres test\u00e9s\nparam_opt_early = list()\n\n# nombre d'it\u00e9ration \nn_iter = int(4.5e6)\n\n# on se d\u00e9place dans l'espace des param\u00e8tres en perturbant le mod\u00e8le initial ant\u00e9rieur \nfor i in range(n_iter):\n\n    # Param\u00e9trisation de la perturbation des param\u00e8tres a et b:\n    pas_perturbation = 0.2\n    delta_a_early = pas_perturbation * (a_early_sup + a_early_inf ) \/ 2\n    delta_b_early = pas_perturbation * (b_early_sup + b_early_inf ) \/ 2\n\n    # on cr\u00e9e un nouveau mod\u00e8le y_p avec les param\u00e8tres a_p et b_p\n    # ces param\u00e8tres sont obtenus par perturbation du mod\u00e8le initial ant\u00e9rieur d\u00e9fini par les param\u00e8tres a_0 et b_0\n    a_p_early = a_0_early + np.random.uniform(-1, 1) * delta_a_early\n    b_p_early = b_0_early + np.random.uniform(-1, 1) * delta_b_early\n    y_p_early = droite(x_early, a_p_early, b_p_early)\n\n    # on calcule la densit\u00e9 de probabilit\u00e9 du nouveau mod\u00e8le\n    L_p_early = L(temp_early, y_p_early, n_early, sigma_early)\n    # on calcul la valeur du r\u00e9sidu du nouveau mod\u00e8le\n    res_p_early = res(temp_early, y_p_early, n_early, sigma_early)\n    # on stocke la valeur des param\u00e8tres, la valeur de la densit\u00e9 de probabilit\u00e9 et la valeur du r\u00e9sidu\n    param_opt_early.append((a_p_early, b_p_early, L_p_early, res_p_early))\n\n    # on compare sa densit\u00e9 de probabilit\u00e9 \u00e0 celle du mod\u00e8le initial ant\u00e9rieur\n    # si L_p > L_0, le mod\u00e8le y_p devient le nouveau mod\u00e8le initial\n    # si L_p < L_0, le mod\u00e8le y_0 reste le mod\u00e8le initial\n    if L_p_early > L_0_early or np.random.uniform() < L_p_early \/ L_0_early:\n        a_0_early = a_p_early\n        b_0_early = b_p_early\n        L_0_early = L_p_early\n\n# on transforme la liste des param\u00e8tres test\u00e9s en matrice        \nparam_opt_early = np.asarray(param_opt_early)\n\n# d\u00e9finition des param\u00e8tres test\u00e9s et densit\u00e9 de probabilit\u00e9\/r\u00e9sidu associ\u00e9s\na_opt_early = param_opt_early[:,0]\nb_opt_early = param_opt_early[:,1]\nL_opt_early = param_opt_early[:,2]\nL_opt_max_early = param_opt_early[:,2].max()\nres_opt_early = param_opt_early[:,3]\nres_opt_max_early = param_opt_early[:,3].max()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les)\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt_early),5))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt_early),5))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt_early),5))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt_early),5))       ","2a63459d":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt obtenues au cours de l'inversion\nplt.figure(1)\nplt.hist(a_opt_early, 10, histtype='step', lw=3, label='a_early')\nplt.hist(b_opt_early, 10, histtype='step', lw=3, label='b_early')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la valeurs du r\u00e9sidu des mod\u00e8les test\u00e9s\n# le r\u00e9sidu est pond\u00e9r\u00e9 par le r\u00e9sidu maximal obtenu dans l'inversion\nplt.figure(1)\nplt.hist(res_opt_early\/res_opt_max_early, 10, histtype='step', lw=3, label='r\u00e9sidu_early')\nplt.xlabel('Valeurs du r\u00e9sidu')\nplt.title('Distribution du r\u00e9sidu des mod\u00e8les test\u00e9s')\nplt.legend()\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\nplt.figure(1)\nplt.scatter(a_opt_early, b_opt_early, s=L_opt_early\/L_opt_max_early, marker='^', c=L_opt_early\/L_opt_max_early)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()","5ddfc4a5":"# d\u00e9finition de la fonction d'autocorr\u00e9lation\ndef acf(samples, lags):\n    corr = [1. if lag1 == 0 else np.corrcoef(samples[lag1:], samples[:-lag1])[0][1] for lag1 in range(lags)]\n    return np.array(corr)\n\n# fonction d'autocorr\u00e9lation des param\u00e8tres a et b\nplt.plot(acf(a_opt_early, lags=700), label='a') # modifier lags = pour trouver acf=0\nplt.plot(acf(b_opt_early, lags=700), label='b') # modifier lags = pour trouver acf=0\nplt.xlabel(\"Lag\")\nplt.ylabel('ACF')\nplt.title(\"ACF des param\u00e8tres a et b\")\nplt.grid()\nplt.legend()\nplt.show()\n# valeur du lag pour acf = 0 - utilis\u00e9e pour r\u00e9-\u00e9chantillonner\nlag1=500 # modifier la valeur du lag \n\n# \u00e9volution de la valeur des param\u00e8tres a et b ind\u00e9pendants pendant l'inversion\nplt.figure(1)\nplt.plot(a_opt_early[::lag1], 'r-', markersize=10) \nplt.plot(b_opt_early[::lag1], 'b-', markersize=10) \nplt.xlabel(\"Nombre de valeurs ind\u00e9pendantes\")\nplt.ylabel('Valeurs des param\u00e8tres')\nplt.title(\"Valeurs des param\u00e8tres a et b ind\u00e9pendants durant l'inversion\")\nplt.show()","78fbfb0e":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt ind\u00e9pendants obtenues au cours de l'inversion\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.hist(a_opt_early[::lag1], 10, histtype='step', lw=3, label='a_early')\nplt.hist(b_opt_early[::lag1], 10, histtype='step', lw=3, label='b_early')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s et ind\u00e9pendants')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres ind\u00e9pendants constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.scatter(a_opt_early[::lag1], b_opt_early[::lag1], s=L_opt_early[::lag1]\/L_opt_max_early, marker='^', c=L_opt_early[::lag1]\/L_opt_max_early)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s et ind\u00e9pendants\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les) ind\u00e9pendants\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt_early[::lag1]),5))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt_early[::lag1]),5))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt_early[::lag1]),5))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt_early[::lag1]),5))   ","4d577790":"# * Fonctions *\n# *************\ndef droite(x_late, a, b):\n    return a * x_late + b\n\n# fonction densit\u00e9 de probabilit\u00e9\ndef L(temp_late, yfit, n_late, sigma_late):\n    return np.exp( -(1\/n_late) * np.sum(((temp_late - yfit) \/ sigma_late ) ** 2) )\n\n# fonction r\u00e9sidu\ndef res(temp_late, yfit, n_late, sigma_late):\n    return (1\/n_late) * np.sum(((temp_late - yfit) \/ sigma_late ) ** 2)","b14c789f":"# information a priori sur a :\na_late_inf = 0.\na_late_sup = 0.5\n\n# information a priori sur b :\nb_late_inf = -8\nb_late_sup = -5.5","75b6152b":"# a) d\u00e9finition du mod\u00e8le initial\n\n# param\u00e8tre a0 du mod\u00e8le initial\na_0_late = a_late_inf + np.random.uniform() * (a_late_sup - a_late_inf)\n\n# param\u00e8tre b0 du mod\u00e8le initial\nb_0_late = b_late_inf + np.random.uniform() * (b_late_sup - b_late_inf)\n\n# b) Calcul de la fonction de densit\u00e9 de probabilit\u00e9 associ\u00e9e\n\n# calcul du mod\u00e8le initial (=y_0) \u00e0 partir de l'\u00e9quation y = ax + b avec les param\u00e8tres a_0 et b_0\ny_0_late = droite(x_late, a_0_late, b_0_late)\nL_0_late = L(temp_late, y_0_late, n_late, sigma_late)\nprint(a_0_late, b_0_late, L_0_late)","d139d7fb":"# cr\u00e9ation de la liste qui contiendra toutes les valeurs des param\u00e8tres test\u00e9s\nparam_opt_late = list()\n\n# nombre d'it\u00e9ration \nn_iter = int(2.5e6)\n\n# on se d\u00e9place dans l'espace des param\u00e8tres en perturbant le mod\u00e8le initial ant\u00e9rieur \nfor i in range(n_iter):\n\n    # Param\u00e9trisation de la perturbation des param\u00e8tres a et b:\n    pas_perturbation = 0.15\n    delta_a_late = pas_perturbation * (a_late_sup + a_late_inf ) \/ 2\n    delta_b_late = pas_perturbation * (b_late_sup + b_late_inf ) \/ 2\n\n    # on cr\u00e9e un nouveau mod\u00e8le y_p avec les param\u00e8tres a_p et b_p\n    # ces param\u00e8tres sont obtenus par perturbation du mod\u00e8le initial ant\u00e9rieur d\u00e9fini par les param\u00e8tres a_0 et b_0\n    a_p_late= a_0_late + np.random.uniform(-1, 1) * delta_a_late\n    b_p_late = b_0_late + np.random.uniform(-1, 1) * delta_b_late\n    y_p_late = droite(x_late, a_p_late, b_p_late)\n\n    # on calcule la densit\u00e9 de probabilit\u00e9 du nouveau mod\u00e8le\n    L_p_late = L(temp_late, y_p_late, n_late, sigma_late)\n    # on calcul la valeur du r\u00e9sidu du nouveau mod\u00e8le\n    res_p_late = res(temp_late, y_p_late, n_late, sigma_late)\n    # on stocke la valeur des param\u00e8tres, la valeur de la densit\u00e9 de probabilit\u00e9 et la valeur du r\u00e9sidu\n    param_opt_late.append((a_p_late, b_p_late, L_p_late, res_p_late))\n\n    # on compare sa densit\u00e9 de probabilit\u00e9 \u00e0 celle du mod\u00e8le initial ant\u00e9rieur\n    # si L_p > L_0, le mod\u00e8le y_p devient le nouveau mod\u00e8le initial\n    # si L_p < L_0, le mod\u00e8le y_0 reste le mod\u00e8le initial\n    if L_p_late > L_0_late or np.random.uniform() < L_p_late \/ L_0_late:\n        a_0_late = a_p_late\n        b_0_late = b_p_late\n        L_0_late = L_p_late\n\n# on transforme la liste des param\u00e8tres test\u00e9s en matrice        \nparam_opt_late = np.asarray(param_opt_late)\n\n# d\u00e9finition des param\u00e8tres test\u00e9s et densit\u00e9 de probabilit\u00e9\/r\u00e9sidu associ\u00e9s\na_opt_late = param_opt_late[:,0]\nb_opt_late = param_opt_late[:,1]\nL_opt_late = param_opt_late[:,2]\nL_opt_max_late = param_opt_late[:,2].max()\nres_opt_late = param_opt_late[:,3]\nres_opt_max_late = param_opt_late[:,3].max()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les)\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt_late),5))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt_late),5))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt_late),5))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt_late),5))       ","91b179e8":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt obtenues au cours de l'inversion\nplt.figure(1)\nplt.hist(a_opt_late, 10, histtype='step', lw=3, label='a')\nplt.hist(b_opt_late, 10, histtype='step', lw=3, label='b')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la valeurs du r\u00e9sidu des mod\u00e8les test\u00e9s\n# le r\u00e9sidu est pond\u00e9r\u00e9 par le r\u00e9sidu maximal obtenu dans l'inversion\nplt.figure(1)\nplt.hist(res_opt_late\/res_opt_max_late, 10, histtype='step', lw=3, label='r\u00e9sidu')\nplt.xlabel('Valeurs du r\u00e9sidu')\nplt.title('Distribution du r\u00e9sidu des mod\u00e8les test\u00e9s')\nplt.legend()\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\nplt.figure(1)\nplt.scatter(a_opt_late, b_opt_late, s=L_opt_late\/L_opt_max_late, marker='^', c=L_opt_late\/L_opt_max_late)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()","7d66acbf":"# d\u00e9finition de la fonction d'autocorr\u00e9lation\ndef acf(samples, lags):\n    corr = [1. if lag2 == 0 else np.corrcoef(samples[lag2:], samples[:-lag2])[0][1] for lag2 in range(lags)]\n    return np.array(corr)\n\n# fonction d'autocorr\u00e9lation des param\u00e8tres a et b\nplt.plot(acf(a_opt_late, lags=1000), label='a') # modifier lags = pour trouver acf=0\nplt.plot(acf(b_opt_late, lags=1000), label='b') # modifier lags = pour trouver acf=0\nplt.xlabel(\"Lag\")\nplt.ylabel('ACF')\nplt.title(\"ACF des param\u00e8tres a et b\")\nplt.grid()\nplt.legend()\nplt.show()\n# valeur du lag pour acf = 0 - utilis\u00e9e pour r\u00e9-\u00e9chantillonner\nlag2=750 # modifier la valeur du lag \n\n# \u00e9volution de la valeur des param\u00e8tres a et b ind\u00e9pendants pendant l'inversion\nplt.figure(1)\nplt.plot(a_opt_late[::lag2], 'r-', markersize=10) \nplt.plot(b_opt_late[::lag2], 'b-', markersize=10) \nplt.xlabel(\"Nombre de valeurs ind\u00e9pendantes\")\nplt.ylabel('Valeurs des param\u00e8tres')\nplt.title(\"Valeurs des param\u00e8tres a et b ind\u00e9pendants durant l'inversion\")\nplt.show()","841c9d77":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt ind\u00e9pendants obtenues au cours de l'inversion\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.hist(a_opt_late[::lag2], 10, histtype='step', lw=3, label='a_late')\nplt.hist(b_opt_late[::lag2], 10, histtype='step', lw=3, label='b_late')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s et ind\u00e9pendants')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres ind\u00e9pendants constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.scatter(a_opt_late[::lag2], b_opt_late[::lag2], s=L_opt_late[::lag2]\/L_opt_max_late, marker='^', c=L_opt_late[::lag2]\/L_opt_max_late)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s et ind\u00e9pendants\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les) ind\u00e9pendants\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt_late[::lag2]),2))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt_late[::lag2]),2))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt_late[::lag2]),2))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt_late[::lag2]),2))   ","d5f02c88":"ainf = np.mean(a_opt_early[::lag1])\nasup = np.mean(a_opt_late[::lag2])\nbinf = np.mean(b_opt_early[::lag1])\nbsup = np.mean(b_opt_late[::lag2])\n\nplt.figure(figsize=(15, 8))\nplt.errorbar (t, temp, yerr=unc)\nplt.plot(date_early, ainf * x_early + binf)\nplt.plot (date_late, asup * x_late + bsup)\nplt.xlabel (\"Ann\u00e9e\")\nplt.ylabel (\"Anomalie de temp\u00e9rature (\u00b0C)\")\nplt.title (\"R\u00e9gressions lin\u00e9aires sur l'anomalie de temp\u00e9rature annuelle moyenne sur la surface des continents \")","d28aa168":"### <font color = blue> Ce mod\u00e8le explique-t-il correctement selon vous l'ensemble de la s\u00e9rie temporelle (i.e. la p\u00e9riode 1880-2019)? \n On remarque que le r\u00e9sidu pond\u00e9r\u00e9 est relativement faible, le mod\u00e8le semble donc bien coller aux donn\u00e9es. Cependant, en soustrayant le mod\u00e8le des donn\u00e9es, on voit que les valeurs restantes ne sont pas \u00e9gales \u00e0 0 bien que l'intervalle des ordonn\u00e9es soit r\u00e9duit et centr\u00e9 autour de 0. Or si le mod\u00e8le collait parfaitement aux donn\u00e9es, on devrait obtenir une droite en y = 0. ","ddce998d":"### <font color = blue> Inversion des donn\u00e9es avant 1974 :","e843f822":"Les deux derniers blocs de codes nous permettent d'obtenir la m\u00eame regression lin\u00e9aire en utilisant la m\u00e9thode des moindre carr\u00e9s. Cependant, le param\u00e8tre b semble varier tr\u00e8s l\u00e9g\u00e8rement d'une m\u00e9thode \u00e0 l'autre, bien que la repr\u00e9sentation graphique ne le montre pas de mani\u00e8re flagrante.\n\nLes deux m\u00e9thodes semblent \u00e9quivalentes, par la suite nous favoriserons l'utilisation de la premi\u00e8re m\u00e9thode, afin d'utiliser la fonction polyfit de numpy, et ainsi effectuer des regressions polynomiales de degr\u00e9 2 ou sup\u00e8rieures si besoin.","283b4c22":"# Inversion des donn\u00e9es apr\u00e8s 1974 :","a05423a7":"Votre r\u00e9sultat est-il coh\u00e9rent avec l'augmentation de la temp\u00e9rature totale enregistr\u00e9e ces 50 derni\u00e8res ann\u00e9es \u00e0 la surface de la Terre? \n\nOn peut noter une rupture de pente tr\u00e8s nette vers 1975, en effet, les anomalies de temp\u00e9ratures se mettent \u00e0 augmenter de fa\u00e7on beaucoup plus forte. On a presque un facteur 10 entre la pente des deux r\u00e9gression lin\u00e9aires (coefficient directeur de la r\u00e9gression lin\u00e9aire de 0.03 apr\u00e8s 1975 contre 0.004 avant).","781cbe4d":"En regardant la r\u00e9gression lin\u00e9aire, on constate facilement qu'il y a au moins 2 tendances qui se d\u00e9gagent. On pourrait alors chercher \u00e0 augmenter le degr\u00e9 de la r\u00e9gression, afin de mieux coller \u00e0 nos donn\u00e9es, et de voir s'il n'y a pas un effet non lin\u00e9aire dans l'\u00e9volutions des temp\u00e9ratures. On pourrait \u00e9galement distinguer 2 p\u00e8riodes, dans lesquelles on pense pouvoir d\u00e9celer un comportement lin\u00e9aire distinct qui expliquerait \u00e9galement l'allure de nos donn\u00e9es.\n\nNous allon donc voir ces deux options ci-dessous.","b64c10b4":"Cet exercice a pour objectif de mod\u00e9liser et d'analyser \u00e0 l'aide des **m\u00e9thodes d'inversion** pr\u00e9sent\u00e9es en cours **l'\u00e9volution de la temp\u00e9rature moyenne annuelle mesur\u00e9e \u00e0 la surface des continents entre 1880 et 2019**. Dans l'exercice, les temp\u00e9ratures sont exprim\u00e9es en Celsius et sont report\u00e9es sous forme d'anomalies relatives \u00e0 la moyenne des temp\u00e9ratures mesur\u00e9es \u00e0 la surface des continents entre janvier 1951 et d\u00e9cembre 1980.\n\nL'augmentation de la temp\u00e9rature \u00e0 la surface du globe depuis le 19\u00e8me si\u00e8cle fait consensus dans la communaut\u00e9 scientifique. **La difficult\u00e9 actuelle r\u00e9side dans la quantification de cette augmentation.** En effet, la valeur de l'augmentation d\u00e9pend fortement des intervalles de temps consid\u00e9r\u00e9s, comme le montre cet exercice. Celui-ci s'organise en trois parties :\n1. Dans un premier temps, vous estimerez **l'augmentation de l'anomalie de temp\u00e9rature par an** \u00e0 la surface des continents pour la **p\u00e9riode 1880-2019** en d\u00e9finissant la **droite y = ax + b** passant au plus pr\u00e8s de l'ensemble des points. Vous utiliserez la m\u00e9thode des **moindres carr\u00e9s**. \n2. Dans un deuxi\u00e8me temps, vous calculerez et tracerez la **diff\u00e9rence entre l'anomalie de temp\u00e9rature moyenne annuelle et le mod\u00e8le** obtenu dans la partie 1 (i.e. la droite)? Ce mod\u00e8le explique-t-il correctement selon vous **l'ensemble de la s\u00e9rie temporelle** (i.e. la p\u00e9riode 1880-2019)? Vous justifierez votre r\u00e9ponse.\n3. Les climatologues s'accordent sur le fait que l'anomalie de temp\u00e9rature moyenne annuelle pr\u00e9sente en r\u00e9alit\u00e9 **deux tendances** en fonction de l'intervalle de temps que nous consid\u00e9rons et non une seule. En s\u00e9parant la s\u00e9rie temporelle en deux parties, on peut d\u00e9finir ces deux tendances \u00e0 l'aide de **deux droites ind\u00e9pendantes**. En utilisant **la m\u00e9thode MCMC**, vous d\u00e9finirez **s\u00e9paremment** chacune de ces deux droites et trouverez **quelle combinaison de droites** explique au mieux l'ensemble des donn\u00e9es (i.e. la p\u00e9riode 1880-2019). De cette mani\u00e8re, vous estimerez **l'augmentation de l'anomalie de temp\u00e9rature par an** associ\u00e9e \u00e0 ces deux intervalles de temps et **l'ann\u00e9e \u00e0 partir de laquelle la tendance se modifie**. Votre r\u00e9sultat est-il coh\u00e9rent avec l'augmentation de la temp\u00e9rature totale enregistr\u00e9e ces 50 derni\u00e8res ann\u00e9es \u00e0 la surface de la Terre?\n\n**Important :** Pour chaque \u00e9tape, vous devez \u00e9crire le code associ\u00e9 en vous inspirant fortement des notebooks du cours, expliquer vos choix au cours de l'inversion (espace des param\u00e8tres, pond\u00e9ration, pas d'it\u00e9ration, nombre d'it\u00e9rations, lag, etc.), d\u00e9crire et discuter les r\u00e9sultats obtenus (param\u00e8tres (moyenne, \u00e9cart-type), valeur du r\u00e9sidu, etc.) \u00e0 l'aide de graphes et de textes. L'exercice est \u00e0 r\u00e9aliser sur ce notebook que vous partagerez une fois le travail termin\u00e9. Les donn\u00e9es sont directement lues par les premi\u00e8res lignes du code ci-dessous.","a5d5f37b":"### <font color= blue> Estimez l'augmentation de l'anomalie de temp\u00e9rature par an \u00e0 la surface des continents pour la p\u00e9riode 1880-2019 :\n\n<font color = blue> L'augmentation de l'anomalie de temp\u00e9rature par an correspond au coefficient directeur de la droite soit 0.01\u00b0C par an. ","7dc27e24":"On divise le jeu de donn\u00e9es en 2. Pour cela, on remarque que l'ann\u00e9e 1975 semble \u00eatre un point de basculement dans la tendance d'augmentation des anomalies.","a3a4acbe":"## 3. Mod\u00e8le et analyse de la s\u00e9rie temporelle : deux tendances","98e7197b":"# 2. Mod\u00e8le et analyse de la s\u00e9rie temporelle : une tendance","9ba87bee":"### En utilisant la m\u00e9thode MCMC, vous d\u00e9finirez s\u00e9paremment chacune de ces deux droites et trouverez quelle combinaison de droites explique au mieux l'ensemble des donn\u00e9es (i.e. la p\u00e9riode 1880-2019). De cette mani\u00e8re, vous estimerez l'augmentation de l'anomalie de temp\u00e9rature par an associ\u00e9e \u00e0 ces deux intervalles de temps et l'ann\u00e9e \u00e0 partir de laquelle la tendance se modifie.\n\n","5a386325":"# Probl\u00e8mes inverses : *Devoir maison \u00e0 rendre le vendredi 08 janvier 2021 (minuit)*\n\n# Mod\u00e8les et analyses de l'augmentation de la temp\u00e9rature globale au cours du temps\n# 1. Pr\u00e9sentation de l'\u00e9tude"}}