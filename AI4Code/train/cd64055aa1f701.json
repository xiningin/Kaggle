{"cell_type":{"6921df7e":"code","d3f87fa2":"code","a95fd515":"code","43473d65":"code","39b6295b":"code","59ff142c":"code","b2d35196":"code","74ee1862":"code","8fc99c6f":"code","015af553":"code","02949d72":"code","3cb4f4f5":"code","97ab23fe":"code","f5b5935a":"code","065c0d78":"code","d3a8ffd3":"code","b5926c52":"code","cf290c44":"code","8cddfb6d":"code","cc8fdfd7":"code","aa5a713d":"code","9129509c":"code","9863fad8":"code","f173c438":"code","64d70f00":"code","e7d0461e":"code","c03ef9a2":"code","f7181947":"code","74a16b8d":"code","b347460c":"code","c237c04d":"code","cd3d1027":"code","90169e04":"code","f9fd43ea":"code","d225fef8":"code","172a6073":"code","d2d69106":"code","5578a439":"code","229d576e":"code","0544331e":"code","828d4548":"code","7bc3e5f1":"code","99c6f81b":"code","862214cc":"code","32962bb5":"code","884d99b7":"code","005b1e58":"code","440deccd":"code","a639a71f":"code","ac3485df":"markdown","bfa7466d":"markdown","b2e77159":"markdown","83a9a237":"markdown","37892da1":"markdown","a19c295e":"markdown","9193f19d":"markdown","b6d03e78":"markdown","99853f1f":"markdown","bf6e5d66":"markdown","32aa361d":"markdown","2e724505":"markdown","79010054":"markdown","822f1138":"markdown","0829e6b5":"markdown","f3e28128":"markdown","35c312a3":"markdown","14d64514":"markdown","71d49d66":"markdown","0bb61be7":"markdown","94f0921a":"markdown"},"source":{"6921df7e":"# Let's import libraries\n\n# for data analysis \nimport numpy as np\nimport pandas as pd\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d3f87fa2":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","a95fd515":"# Let's have a look at the data\ntrain.head()","43473d65":"train.info()","39b6295b":"sex_vs_survived = pd.crosstab(train.Sex, train.Survived)\nprint(sex_vs_survived)\nplt.figure(figsize=(25,10))\nsex_vs_survived.plot.bar(stacked=True)","59ff142c":"survived_vs_pclass = pd.crosstab([train.Pclass, train.Sex], train.Survived)\nprint(survived_vs_pclass)\nplt.figure(figsize=(25,15))\nsurvived_vs_pclass.plot.bar(stacked = True)","b2d35196":"sib = pd.crosstab([train.SibSp, train.Sex], train.Survived)\nprint(sib)\nplt.figure(figsize=(25,10))\nsib.plot.bar(stacked = True)","74ee1862":"emb = pd.crosstab(train.Embarked, train.Survived)\nprint(emb)\nemb.plot.bar(stacked = True)","8fc99c6f":"emb1 = pd.crosstab([train.Embarked, train.Survived], train.Pclass)\nprint(emb1)\nemb1.plot.bar(stacked = True)","015af553":"train.head()","02949d72":"name_train = train['Name']\nname_train['Title'] = 0\nfor i in train['Name']:\n    name_train['Title']=train['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n    \nname_test = test['Name']\nname_test['Title'] = 0\nfor i in test['Name']:\n    name_test['Title']=test['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","3cb4f4f5":"print(name_train.Title.unique())\nprint(name_test.Title.unique())","97ab23fe":"name_train['Title'] = name_train['Title'].replace(['Miss', 'Ms', 'Mlle', 'Lady'], 'Miss')\nname_test['Title'] = name_test['Title'].replace(['Miss', 'Ms', 'Mlle', 'Lady'], 'Miss')\nname_test['Title'] = name_test['Title'].replace('Dona', 'Don')","f5b5935a":"print(name_train.Title.unique())\nprint(name_test.Title.unique())","065c0d78":"train['Title'] = name_train['Title']\ntest['Title'] = name_test['Title']\n\ntitle_mean = train.groupby('Title')['Age'].mean()","d3a8ffd3":"title_mean","b5926c52":"map_title_mean = title_mean.to_dict()\nmap_title_mean","cf290c44":"# fill missing values in the Age column according to title\ntrain.Age = train.Age.fillna(train.Title.map(map_title_mean))\ntest.Age = test.Age.fillna(train.Title.map(map_title_mean))","8cddfb6d":"print(train.head(15))\nprint(test.head(15))","cc8fdfd7":"train.info()","aa5a713d":"test.info()","9129509c":"train.drop('Cabin', axis = 1, inplace = True)\ntrain.drop('Name', axis = 1, inplace = True)\ntrain.drop('Ticket', axis = 1, inplace = True)\ntrain.drop('Fare', axis=1, inplace = True)\n\ntest.drop('Cabin', axis = 1, inplace = True)\ntest.drop('Name', axis = 1, inplace = True)\ntest.drop('Ticket', axis = 1, inplace = True)\ntest.drop('Fare', axis=1, inplace = True)","9863fad8":"print(train.head(10))\nprint(test.head(10))","f173c438":"title_survival = pd.crosstab(train.Title, train.Survived)\nprint(title_survival)\n\nplt.figure(figsize=(25,10))\nsns.barplot(x='Title', y='Survived', data = train)\nplt.xticks(rotation=90);","64d70f00":"# peaks for survived\/not survived passengers by their age\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age', shade = True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()","e7d0461e":"sex_mapping = {\"male\": 0, \"female\": 1}","c03ef9a2":"embarked_mapping = {'S':0, 'C':1, 'Q':2}","f7181947":"title_mapping = {'Capt': 1,\n 'Col': 2,\n 'Countess': 3,\n 'Don': 4,\n 'Dr': 5,\n 'Jonkheer': 6,\n 'Major': 7,\n 'Master': 8,\n 'Miss': 9,\n 'Mme': 10,\n 'Mr': 11,\n 'Mrs': 12,\n 'Rev': 13,\n 'Sir': 14}","74a16b8d":"train['Sex'] = train['Sex'].map(sex_mapping)\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntrain['Title'] = train['Title'].map(title_mapping)\n\ntest['Sex'] = test['Sex'].map(sex_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\ntest['Title'] = test['Title'].map(title_mapping)","b347460c":"print(train.head(10))\nprint(test.head(10))","c237c04d":"train.Embarked = train.Embarked.fillna(0)\ntest.Embarked = test.Embarked.fillna(0)","cd3d1027":"test.head()","90169e04":"train.Age = pd.Series(train.Age).astype(int)\ntrain.Embarked = pd.Series(train.Embarked).astype(int)\n\ntest.Age = pd.Series(test.Age).astype(int)\ntest.Embarked = pd.Series(test.Embarked).astype(int)","f9fd43ea":"train.info()\nprint(train.head(10))\nprint(test)","d225fef8":"from sklearn.model_selection import train_test_split","172a6073":"predictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.2, random_state = 0)","d2d69106":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","5578a439":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","229d576e":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier(learning_rate = 0.05, n_estimators = 3000)\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","0544331e":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","828d4548":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","7bc3e5f1":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","99c6f81b":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","862214cc":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(n_estimators = 1000)\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","32962bb5":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","884d99b7":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","005b1e58":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","440deccd":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","a639a71f":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission2.csv', index=False)","ac3485df":"# First visualization say that women and children survive better than men and \n\n# second visualization probably speaks of people of age group 30 to 40 took the responsibility of making sure children and women are safe and children survive more and elderly don't have much of a chance","bfa7466d":"## converting float point variables to integer variables","b2e77159":"# Let's mark some nominal values to Sex, Embarked and Title","83a9a237":"Thank you very much for viewing my work. I am thankful to Kaggle community for teaching me many concepts. \n\nCredits: I am thankful to https:\/\/www.kaggle.com\/rochellesilva\/simple-tutorial-for-beginners , https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner and many others as I have understood their code and added my own insights to improve the model.","37892da1":"# Can't say much but it looks like C has better chance of survival. Let's explore further","a19c295e":"# Titanic is sinking. Lifeboats are limited and a decision need to be made. \n\n![](https:\/\/www.google.com\/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwip_fLdzarhAhVXcCsKHTwED3sQjRx6BAgBEAU&url=https%3A%2F%2Fwww.goodhousekeeping.com%2Flife%2Fg19809308%2Ftitanic-facts%2F&psig=AOvVaw2W19MLBaByqF5BcG5l0xLA&ust=1554060770519155)\n# What are those decisions? Let's see if we can get some insights from the data and replicate the thought process of 10th April, 1912.","9193f19d":"# Probably more first class tickets were sold in C as a percentage compared to Q and S stations which justifies higher survival in C","b6d03e78":"### Age column missing values are now filled with relevant data in train and test datasets","99853f1f":"# Let's see which ML model could replicate year 1912 decision better\n","bf6e5d66":"Let's map the above age values in the missing NAN columns in age of train and test data","32aa361d":"As Logistic Regression gave better score, we shall extract the predictions from that data for competition","2e724505":"# Let us now fill the missing age values in train and test data\n","79010054":"We can fill age in two ways. \n\n1. Take the mean age across the age column and fill it in missing values\n\n2. impute mean age with respect to title of the person (Mr. , Master, Miss, etc ) and fill them respectively. \n\n\nOption two would give better age approximation, so let's go with that method ","822f1138":"# We can't find any pattern here, atleast visually","0829e6b5":"# Now, Let's remove columns which are not useful in analysis.","f3e28128":"# Insight 2: Females of first class and second class survived more than females of third class\n# Insight 3: Males of first class has better survival compared to males of second and third class","35c312a3":"## It is done!","14d64514":"We can see same observations with different names such as Miss, Ms, Mlle, Lady. We may group them into familier titiles","71d49d66":"* # Initial Observations:\n\n\nPassenger ID is an integer (Nominal data) with no random text to filter out\n\nSurvived is an integer (Nominal data) with only two values - 0 and 1\n\nPclass is an integer (Nominal data) with only three values - 1, 2 and 3\n\nSibSp is an integer (Ordinal data) \n\nParch is an integer (Ordinal data)\n\nAge has some missing values \n\nCabin has many missing values \n\nEmbarked has a few missing values","0bb61be7":"### Unnecessary Columns are removed from the train and test dataframe","94f0921a":"# Insight 1: Female gender has better survival than male gender"}}