{"cell_type":{"170b027e":"code","54fa45cb":"code","04ecc567":"code","53eb674c":"code","b63c213a":"code","c9fb5d52":"code","788fb504":"code","8718480c":"code","1ba7ecc3":"code","f1c0e498":"code","210063bb":"code","c7c43805":"code","bf9bcf40":"code","80f80482":"code","3044428c":"code","1449c792":"code","c952d62f":"markdown","0e604a7e":"markdown","f933358c":"markdown","24ecad47":"markdown","3fce0149":"markdown","53b502a2":"markdown","adc5667e":"markdown","1caa9146":"markdown","e3874f8b":"markdown","78a0fdd6":"markdown","2870de44":"markdown","cfeb02ac":"markdown","d3392fb7":"markdown","c28cea0c":"markdown","bce45c32":"markdown","d2c1c84c":"markdown","415de518":"markdown","0cb3e5cf":"markdown","6242a00b":"markdown","d86236d1":"markdown","ee391658":"markdown","24c0c481":"markdown"},"source":{"170b027e":"!pip install transformers","54fa45cb":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","04ecc567":"!pip install --upgrade -q wandb\n\n# Install timm \n!pip install -q timm","53eb674c":"import wandb\nfrom pytorch_lightning.loggers import WandbLogger\n\nwandb.login()","b63c213a":"import os\nimport timm\nimport gc\nimport copy\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm\nimport pytorch_lightning as pl\nimport time\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nfrom transformers import BertTokenizer,BertForSequenceClassification, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom collections import defaultdict\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","c9fb5d52":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","788fb504":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","8718480c":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","1ba7ecc3":"class CONFIG:\n    seed = 42\n    max_len = 331\n    train_batch = 16\n    valid_batch = 32\n    epochs = 10\n    learning_rate = 2e-5\n    splits = 5\n    model='bert-base-cased'\n    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n    tokenizer.save_pretrained('.\/tokenizer')\n    accum=1\n    ","f1c0e498":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","210063bb":"class BERTDataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n\n        return {\n            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            'label': torch.tensor(self.target[index], dtype=torch.float)\n        }","c7c43805":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train)\n    valid_dataset = BERTDataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","bf9bcf40":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))","80f80482":"class Model(pl.LightningModule):\n\n    def __init__(self,fold):\n        super(Model, self).__init__()\n        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1,output_attentions = False,output_hidden_states = False)\n        self.model = model\n        train_dataloader, val_dataloader= get_data(fold)\n        self._train_dataloader = train_dataloader\n        self._val_dataloader = val_dataloader\n        self.all_targets=[]\n        self.all_preds=[]\n        self.automatic_optimization = True\n        \n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"gamma\", \"beta\"]\n        optimizer_grouped_parameters = [\n                {\n                    \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                    \"weight_decay_rate\": 0.01\n                    },\n                {\n                    \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                    \"weight_decay_rate\": 0.0\n                    },\n                ]\n        optimizer = AdamW(\n                optimizer_grouped_parameters,\n                lr=CONFIG.learning_rate,\n                )\n        \n        # Defining LR Scheduler\n        scheduler = get_linear_schedule_with_warmup( optimizer, \n        num_warmup_steps=0, \n        num_training_steps=len(self._train_dataloader )*CONFIG.epochs)\n        self.scheduler=scheduler\n        self.optimizer=optimizer\n        return {\n            'optimizer': self.optimizer,\n            'lr_scheduler': {\n                'scheduler': self.scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def training_step(self, batch, batch_idx):\n        labels = batch[\"label\"]\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        token_type_ids = batch[\"token_type_ids\"]\n        \n        loss= self.model(\n                input_ids,\n                attention_mask=attention_mask\n                )\n        output=loss['logits']\n        loss=loss_fn(output,labels)\n        self.train_loss +=(loss.item() * len(labels))\n        self.t_data_size+=len(labels)\n        epoch_loss=self.train_loss\/self.t_data_size\n        self.log('train_loss', epoch_loss, on_epoch=True, prog_bar=True, logger=True)\n        tqdm_dict={'train_loss':loss}\n        sch=self.scheduler\n        sch.step()\n        output = OrderedDict({\n            \"loss\": loss,\n            \"progress_bar\": tqdm_dict,\n            \"log\": tqdm_dict\n            })\n\n        return output\n        \n    def validation_step(self, batch, batch_idx):\n        labels = batch[\"label\"]\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        token_type_ids = batch[\"token_type_ids\"]\n        loss= self.model(\n                input_ids,\n                attention_mask=attention_mask\n                )\n        outputs=loss['logits']\n        loss=loss_fn(outputs,labels)\n        self.val_loss +=(loss.item() * len(labels))\n        self.v_data_size+=len(labels)\n        epoch_loss=self.val_loss\/self.v_data_size\n        self.all_targets.extend(labels.detach().squeeze(-1).cpu().numpy())\n        self.all_preds.extend(outputs.detach().squeeze(-1).cpu().numpy())\n        val_rmse = mean_squared_error(self.all_targets,self.all_preds,squared=False)\n        \n        logs = {\n                \"val_loss\": epoch_loss,\n                \"val_rmse\": val_rmse,\n                }\n        self.log_dict(logs, on_epoch=True, prog_bar=True, logger=True)\n        output = OrderedDict({\n            \"val_loss\": loss,\n            \"preds\":self.all_preds,\n            \"labels\":self.all_targets,\n            \"batch_size\": len(labels)\n            })\n        return output\n\n    def validation_end(self, outputs):\n        val_loss = sum([out[\"val_loss\"] for out in outputs]) \/ len(outputs)\n        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n        return result\n   \n    def train_dataloader(self):\n        return self._train_dataloader\n\n    def val_dataloader(self):\n        return self._val_dataloader\n","3044428c":"lr_monitor = LearningRateMonitor(logging_interval='step')\n# Checkpoint\ncheckpoint_callback = ModelCheckpoint(monitor='val_rmse',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='checkpoint\/{epoch:02d}-{val_loss:.4f}',\n                                      verbose=False,\n                                      mode='min')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor='val_rmse', patience=5, mode='min')","1449c792":"#fold1 train\nmodel = Model(1)\n# instrument experiment with W&B\nwandb_logger = WandbLogger(project='CommonlitReadabilityTrain_', log_model='all',job_type='train')\ntrainer = pl.Trainer(logger=wandb_logger,max_epochs=CONFIG.epochs,accumulate_grad_batches=CONFIG.accum,callbacks=[earlystopping,lr_monitor],checkpoint_callback =checkpoint_callback ,tpu_cores=1)\n# log gradients and model topology\nwandb_logger.watch(model)\ntrainer.fit(model)\nwandb.finish()","c952d62f":"# <p style=\"color:#159364; font-family:cursive;\">CALLBACKS<\/center><\/p>\n* A callback is a piece of code that you\u2019d like to be executed at various parts of training. In Lightning callbacks are reserved for non-essential code such as logging or something not related to research code. This keeps the research code super clean and organized.","0e604a7e":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS<\/center><\/p>","f933358c":"![download (82).png](attachment:39b3319a-d191-49ef-a5e7-c2c557bb6d91.png)","24ecad47":"Code taken from:https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds","3fce0149":"# <p style=\"color:#159364; font-family:cursive;\">MODEL CLASS <\/center><\/p>\nIt is important to note that we are not extending from \u201cnn.Module\u201d as we would commonly do in a pure PyTorch model, in this case we are extending from \u201cpl.LightningModule\u201d.","53b502a2":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS<\/center><\/p>","adc5667e":"# <p style=\"color:#159364; font-family:cursive;\">RUN THE MODEL WITH PYTORCH LIGHTNING TRAINER and EVALUATE IT WITH WANDB<\/center><\/p>\n* In Lightning, you can train your model on CPUs, GPUs, Multiple GPUs, or TPUs without changing a single line of your PyTorch code.\n* no TPU specific code\n* no .to(device)\n* no .cuda()\n* Just 1 argument in the trainer,and the rest is automated ","1caa9146":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION<\/center><\/p>","e3874f8b":" <h1 style=\"font-family:verdana;\"> <center>BERT FINETUNING WITH \u26a1PYTORCH LIGHTNING AND TRACKING THE \"WEIGHTS AND BIASES(WANDB)\" LOGS<\/center> <\/h1>","78a0fdd6":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA<\/center><\/p>","2870de44":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY<\/center><\/p>\n","cfeb02ac":"PyTorch Lightning is a PyTorch extension for the prototyping of the training, evaluation and testing phase of PyTorch models. Also, PyTorch Lightning provides a simple, friendly and intuitive structure to organize each component of the training phase of a PyTorch model. On the other hand, PyTorch Lightning provides a great variety of functionalities and flags for a detailed customization of the training of our model. In short, PyTorch Lightning came to organize, simplify and compact the components that involve a training phase of a deep learning model such as: training, evaluation, testing, metrics tracking, experiment organization and logs.","d3392fb7":"# IMPORT WANDB LOGGER\nCoupled with Weights & Biases integration,we can quickly train and monitor models","c28cea0c":"\ud83d\udcccOTHER NOTEBOOKS FOR THE COMPETITION:\n* [PyTorch Prompt-Tuning BERT](https:\/\/www.kaggle.com\/shreyasajal\/prompt-tuning-bert-commonlit-readability)  \n* [Pytorch BERT BASELINE & LR SCHEDULERS' GUIDE](https:\/\/www.kaggle.com\/shreyasajal\/pytorch-bert-baseline-lr-schedulers-guide)  \n* [PyTorch OpenAI GPT-2 FineTuning](https:\/\/www.kaggle.com\/shreyasajal\/pytorch-openai-gpt2-commonlit-readability)","bce45c32":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES<\/center><\/p>","d2c1c84c":"<p style=\"color:#159364; font-family:cursive;\">Installing PyTorch XLA Frameworks so that we can work with TPUs.<\/center><\/p>","415de518":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY<\/center><\/p>","0cb3e5cf":"![1_kswFi_ipCKuwR9vWpYlqxg-min.jpeg](attachment:8751229f-91d1-4820-ba21-3413abbff599.jpeg)","6242a00b":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)\n","d86236d1":"\n# TRACK YOUR EXPERIMENTS\n\n## LOSSES\n    \n![img](https:\/\/imgur.com\/sgLd13Z.gif)\n\n\n## LEARNING RATE\n![img](https:\/\/imgur.com\/a4DbLom.gif)\n\n\n## SYSTEM LOGS\n![img](https:\/\/imgur.com\/MEO8vRm.gif)\n","ee391658":"So far everything is normal, the previous steps we would also have done if we were to train a pure PyTorch model. The big difference comes next,when we initialize our model, as we can see, we are only passing the fold as a parameter.The \u201cTrainer\u201d of PyTorch Lightning is the key piece for the training. We can pass a large number of arguments to the \u201cTrainer\u201d in order to have a detailed customization of our training phase, in this case we are only defining the maximum number of epochs,logger,callbacks and tpu_cores","24c0c481":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING<\/center><\/p>"}}