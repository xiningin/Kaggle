{"cell_type":{"78dcfa53":"code","f7b302bc":"code","76b62207":"code","9b9ac294":"code","9aa5389b":"code","2755270f":"code","0628bb8b":"code","d810b7ec":"code","6f8d0691":"code","93a8e1f1":"code","a3b76170":"code","19eed12b":"code","fc3b7eda":"code","5a7182c7":"code","3728bb27":"code","f523de16":"code","abe34c68":"code","de20305a":"code","f286df52":"code","2d533c8b":"code","4d43f249":"code","28bc9e05":"code","3addbef5":"code","6bdd4eb1":"code","ff35a63f":"code","7b565830":"code","2ec7e53e":"code","22261211":"code","b4dbebb9":"code","5da5bc84":"code","a393ef5f":"code","5994325b":"code","e86235b6":"code","fb609809":"code","1cfebda4":"code","48520e9f":"code","cabe5a6c":"code","36587165":"code","6abb5b50":"code","6cb24800":"code","8883efa4":"code","9b967f28":"markdown","c376ad7d":"markdown","06e1bbf7":"markdown","d9745d8d":"markdown","00c3d03e":"markdown","b3c4cb99":"markdown","4fe7d646":"markdown","f1e33000":"markdown","572192ab":"markdown","fae8784c":"markdown","cab69cea":"markdown","bb0d0064":"markdown","85f33cfd":"markdown"},"source":{"78dcfa53":"from sklearn.metrics import confusion_matrix ,classification_report,precision_score, recall_score ,f1_score, roc_auc_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","f7b302bc":"\n#\/kaggle\/input\/mobile-price-classification\/train.csv\n#\/kaggle\/input\/mobile-price-classification\/test.csv\n\ntraindata = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')\ntestdata = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/test.csv')","76b62207":"traindata.head()","9b9ac294":"traindata.tail()","9aa5389b":"traindata.describe()","2755270f":"traindata.info()","0628bb8b":"data_num = traindata[['battery_power',  'clock_speed' , 'fc','int_memory','m_dep', 'mobile_wt','n_cores', 'pc',\n                      'px_height','px_width','ram', 'sc_h', 'sc_w', 'talk_time']]\n\ndata_cat = traindata[['blue','dual_sim', 'four_g','three_g','touch_screen', 'wifi']]","d810b7ec":"for i in data_num.columns:\n    plt.hist(data_num[i])\n    plt.title(i)\n    plt.show()","6f8d0691":"sns.heatmap(data_num.corr())","93a8e1f1":"pd.pivot_table(traindata, index='price_range', values=['battery_power',  'clock_speed' , 'fc','int_memory', 'mobile_wt', \n                                                       'pc', 'px_height','px_width','ram', 'sc_h', 'sc_w', 'talk_time'])","a3b76170":"for i in data_cat.columns:\n    sns.barplot(data_cat[i].value_counts().index,data_cat[i].value_counts()).set_title(i)\n    plt.show()","19eed12b":"# fc, px_height, three_g","fc3b7eda":"for i in data_cat:\n    print(pd.pivot_table(traindata,index='price_range',columns=i, values='ram'))\n    print(\"==\"*20)","5a7182c7":"# dealing with outliers values\nfor i in data_num.columns:\n    sns.boxplot(data_num[i])\n    plt.title(i)\n    plt.show()","3728bb27":"def outlinefree(dataCol):     \n      \n    sorted(dataCol)                          # sort column\n    Q1,Q3 = np.percentile(dataCol,[25,75])   # getting 25% and 75% percentile\n    IQR = Q3-Q1                              # getting IQR \n    LowerRange = Q1-(1.5 * IQR)              # getting Lowrange\n    UpperRange = Q3+(1.5 * IQR)              # getting Upperrange \n    \n    colname = dataCol.tolist()               # convert column into list  \n    newlist =[]                              # empty list for store new values\n    for i in range(len(colname)):\n        \n        if colname[i] > UpperRange:          # list number > Upperrange \n            colname[i] = UpperRange          # then number = Upperrange\n            newlist.append(colname[i])       # append value to empty list\n        elif colname[i] < LowerRange:        # list number < Lowrange \n            colname[i] = LowerRange          # then number = Lowrange\n            newlist.append(colname[i])       # append value to empty list \n        else:\n            colname[i]                       # list number\n            newlist.append(colname[i])       # append value to empty list\n            \n        \n\n    return newlist","f523de16":"for i in range(len(data_num.columns)):\n    new_list =  outlinefree(traindata.loc[:,data_num.columns[i]]) # retrun new list\n    traindata.loc[:,data_num.columns[i]] = new_list               # new list = data.columns","abe34c68":"data_final_num = traindata[['battery_power',  'clock_speed' , 'fc','int_memory','m_dep', 'mobile_wt','n_cores', 'pc',\n                      'px_height','px_width','ram', 'sc_h', 'sc_w', 'talk_time']]","de20305a":"\nfor i in data_final_num.columns:\n    sns.boxplot(data_final_num[i])\n    plt.title(i)\n    plt.show()","f286df52":"sns.pairplot(traindata)","2d533c8b":"ct = pd.crosstab(traindata['wifi'],traindata['price_range'])\nfrom scipy.stats import chi2_contingency\nstat,pvalue,dof,expected_R = chi2_contingency(ct)\nprint(\"pvalue : \",pvalue)\n\nif pvalue <= 0.1:\n    print(\"Alternate Hypothesis passed. int_memory and price_range have Relationship\")\nelse:\n    print(\"Null hypothesis passed. int_memory and price_range doesnot have  Relationship\")","4d43f249":"features = traindata.loc[:,[\"battery_power\",\"int_memory\", \"ram\",\"sc_w\"]].values\nlabel = traindata.iloc[:,-1].values","28bc9e05":"#------------------------LogisticRegression-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=167)\n\nclassimodel= LogisticRegression(solver=\"liblinear\")  \nclassimodel.fit(X_train, y_train)\ntrainscore =  classimodel.score(X_train,y_train)\ntestscore =  classimodel.score(X_test,y_test)  \n\nprint(\"test score: {}\".format(testscore),'\\n')\ny_predlogi =  classimodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predlogi,average='micro'),'\\n')\nprint(confusion_matrix(y_test, y_predlogi))\n","3addbef5":"print(' precision score: ',precision_score(y_test, y_predlogi,average='micro'),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predlogi,average='micro'),'\\n')\nprint(classification_report(y_test, y_predlogi))","6bdd4eb1":"#------------------------------naive bayes---------------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=120) \n\nNBmodel = GaussianNB()  \nNBmodel.fit(X_train, y_train) \n\ntrainscore =  NBmodel.score(X_train,y_train)\ntestscore =  NBmodel.score(X_test,y_test)  \n\nprint(\"test score: {} train score: {}\".format(testscore,trainscore),'\\n')\ny_predNB =  NBmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predNB,average='micro'),'\\n')\nprint(confusion_matrix(y_test, y_predNB))","ff35a63f":"print(' precision score: ',precision_score(y_test, y_predNB,average='micro'),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predNB,average='micro'),'\\n')\nprint(classification_report(y_test, y_predNB))","7b565830":"#-------------------------------- support vector classification -------------------------------------  \nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=39) \n\nsvcmodel = SVC(probability=True)  \nsvcmodel.fit(X_train, y_train) \n\ntrainscore =  svcmodel.score(X_train,y_train)\ntestscore =  svcmodel.score(X_test,y_test)  \n\nprint(\"test score: {} \".format(testscore),'\\n')\n\ny_predsvc =  svcmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predsvc,average='micro'),'\\n')\nprint(confusion_matrix(y_test, y_predsvc))","2ec7e53e":"print(' precision score: ',precision_score(y_test, y_predsvc,average='micro'),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predsvc,average='micro'),'\\n')\nprint(classification_report(y_test, y_predsvc))","22261211":"#------------------------Decision Tree-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=194)\n\nDTmodel=  DecisionTreeClassifier(max_depth=4)  \nDTmodel.fit(X_train, y_train)\ntrainscore =  DTmodel.score(X_train,y_train)\ntestscore =  DTmodel.score(X_test,y_test)\ny_predDT =  DTmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predDT,average='micro'),'\\n')\nprint(confusion_matrix(y_test, y_predDT))","b4dbebb9":"print(' precision score: ',precision_score(y_test, y_predDT,average='micro'),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predDT,average='micro'),'\\n')\nprint(classification_report(y_test, y_predDT))","5da5bc84":"#------------------------Random Forest-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=39)\n\nRFmodel=  RandomForestClassifier(criterion='entropy',max_depth=4) \nRFmodel.fit(X_train, y_train)\ntrainscore =  RFmodel.score(X_train,y_train)\ntestscore =  RFmodel.score(X_test,y_test)  \ny_predRF =  RFmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predRF,average='micro'),'\\n')\nprint(confusion_matrix(y_test, y_predRF))","a393ef5f":"print(' precision score: ',precision_score(y_test, y_predRF,average='micro'),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predRF,average='micro'),'\\n')\nprint(classification_report(y_test, y_predRF))","5994325b":"#-------------------------------------- LogisticRegression -------------------------------------\nprobabilityValues = classimodel.predict_proba(features)\nauc = roc_auc_score(label,probabilityValues,multi_class ='ovr')\nprint(auc)","e86235b6":"#-------------------------------------- naive bayes -------------------------------------\nprobabilityValues = NBmodel.predict_proba(features)\nauc = roc_auc_score(label,probabilityValues,multi_class ='ovr')\nprint(auc)","fb609809":"#-------------------------------------- support vector classification -------------------------------------\nprobabilityValues = svcmodel.predict_proba(features)\nauc = roc_auc_score(label,probabilityValues,multi_class ='ovr')\nprint(auc)","1cfebda4":"#-------------------------------------- Decision Tree -------------------------------------\nprobabilityValues = DTmodel.predict_proba(features)\nauc = roc_auc_score(label,probabilityValues,multi_class ='ovr')\nprint(auc)","48520e9f":"#-------------------------------------- Random Forest -------------------------------------\nprobabilityValues = RFmodel.predict_proba(features)\nauc = roc_auc_score(label,probabilityValues,multi_class ='ovr')\nprint(auc)","cabe5a6c":"finaltestdata = testdata.loc[:,[\"battery_power\",\"int_memory\", \"ram\",\"sc_w\"]]","36587165":" predicted_price = DTmodel.predict(finaltestdata)","6abb5b50":"predicted_price","6cb24800":"testdata['price_range']=predicted_price","8883efa4":"testdata.head()","9b967f28":"## Context\nBob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc.\n\nHe does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.\n\nBob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.\n\nIn this problem you do not have to predict actual price but a price range indicating how high the price is\n\n\n![](https:\/\/s3b.cashify.in\/gpro\/uploads\/2019\/07\/09100223\/mobile-phone-evolution.jpg)\n","c376ad7d":"## Data Normalization\n\n1. interquartile range: It is hlep us to find **outlier values** in th columns.\n2. outlinefree() : It is a customise function that help us to figureout and work on outlier values in columns. meanly, it is used to **remove outlires** values from dataset.    \n3. for-loop: with the help of for-loop, we are checking the **outlinefree()** function worked properly or not.","06e1bbf7":"## Feature Selection with hypothesis test\n1. Chi-test: It is help to figure-out relation between features and label with **\"pvalue <= 0.1\"**","d9745d8d":"## Conclusion\nI will choose a **Decision Tree** algorithm for this test-dataset.\n\nDecision Tree \n1. **f1_score: 0.834**\n2. **auc: 0.9551051666666667**","00c3d03e":"## Know Dataset Nature\n1. head() : It is used to get the **first 5 rows** of the dataframe.\n2. tail() : It is used to get the **last 5 rows** of the dataframe.\n3. describe() : It is used to view some **basic statistical details** like percentile, mean, std etc.\n4. info() : It is used to print a **concise summary** of a DataFrame. including the **index dtype and column dtypes, non-null values and memory usage.**","b3c4cb99":"## Receiver Operating Characteristic Score (ROC AUC)\nhere we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.\n\n1. Logistic Regression (auc: 0.9362343333333334)\n2. naive bayes (auc: 0.9429046666666667)\n3. support vector classification (auc: 0.9649543333333334)\n4. DecisionTreeClassifier (auc: 0.9551051666666667)\n5. **RandomForestClassifier (auc: 0.9655039999999999)**","4fe7d646":"## Applying Algorithm\n1. we have to separet relational columns from the test dataset that will be columns assign to a new dataset.\n2.  now we are ready for applying the decision tree algorithm on the test dataset.\n3. now we have model-predicted prices and we can assign a price column to the test dataset.","f1e33000":"## Feature Selection\n\n1. seaborn.pairplot(): It is help to figure-out relation between features and label.","572192ab":"## Model Buliding\nhere we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.\n\n1. Logistic Regression (f1 score: 0.782 )\n2. naive bayes (f1 score: 0.278)\n3. support vector classification (f1 score: 0.266 )\n4. **DecisionTreeClassifier (f1 score: 0.834)**\n5. RandomForestClassifier (f1 score: 0.262)","fae8784c":"# Mobile Price Classification\nclassify mobile price range\n\n**Best results f1_score : 0.834 % accuracy : 0.9551051666666667**\n\n## Overview \n### 1) Context\n\n### 2) Describtion \n\n### 3) Used Python Libraries\n\n### 4) Exploratory Data Analysis (EDA) \n\n### 5) Data Normalization\n\n### 6) Feature Selection with Visualization\n\n### 7) Feature Selection with Hypothesis test \n\n### 8) Model Buliding \n\n### 9) Receiver Operating Characteristic Score (ROC AUC) \n\n### 10) Conclusion \n\n### 11) Applying Algorithm \n\n\n\n\n","cab69cea":"## Describtion\n\n1. Id : id\n2. battery_power: Total energy a battery can store in one time measured in mAh\n3. blue     : Has bluetooth or not \n4. clock_speed : speed at which microprocessor executes instructions \n5. dual_sim : Has dual sim support or not \n6. fc : Front Camera mega pixels \n7. four_g  : Has 4G or not \n8. int_memory : Internal Memory in Gigabytes \n9. m_dep :   Mobile Depth in cm \n10. mobile_wt : Weight of mobile phone \n11. n_cores :  Number of cores of processor \n12. pc : Primary Camera mega pixels \n13. px_height : Pixel Resolution Height \n14. px_width : Pixel Resolution Width \n15. ram : Random Access Memory in Megabytes \n16. sc_h : Screen Height of mobile in cm \n17. sc_w : Screen Width of mobile in cm \n18. talk_time  : longest time that a single battery charge will last when you are \n19. three_g : Has 3G or not \n20. touch_screen : Has touch screen or not \n21. wifi  : Has wifi or not \n22. price_range : This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost). \n","bb0d0064":"## Light Data Exploration\n### 1) For numeric data \n* Made histograms to understand distributions \n* Corrplot \n* Pivot table comparing survival rate across numeric variables \n\n\n### 2) For Categorical Data \n* Made bar charts to understand balance of classes \n* Made pivot tables to understand relationship with survival ","85f33cfd":"## Used Python Libraries"}}