{"cell_type":{"2ec73e1d":"code","ba7cdfaa":"code","d54d2caf":"code","a93fdf5a":"code","7dfa8af9":"code","c2aed336":"code","278dc02a":"code","d3980937":"code","af775dab":"code","cd51551f":"code","49de2ec6":"code","cc8d9dbe":"code","94b73402":"code","e1520004":"code","f1ba48d6":"markdown","a0970e82":"markdown","c7e85819":"markdown","6755bb99":"markdown","84f704c3":"markdown","641b8d51":"markdown","91b739f0":"markdown","6e5deb26":"markdown","02f9e93e":"markdown","e3a90116":"markdown","4209b452":"markdown","5954f9fa":"markdown","b1fc15cc":"markdown","aa2ba8e7":"markdown","cd3b5318":"markdown"},"source":{"2ec73e1d":"import numpy as np\nnp.random.seed(5)\n\n#from keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.utils import np_utils","ba7cdfaa":"def load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(X_train, y_train), (X_test, y_test) = load_data('..\/input\/mnist-numpy\/mnist.npz')","d54d2caf":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","a93fdf5a":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as cm\n\nfig, ax = plt.subplots(ncols=10, nrows=1, figsize=(10, 5))\namostra = np.random.choice(60000, 10) #escolhe 10 imagens dentre as 60000\n\nfor i in range(len(amostra)):\n    imagem = np.array(X_train[amostra[i]])\n    ax[i].imshow(imagem, cmap = cm.Greys_r)\n    ax[i].get_xaxis().set_ticks([])\n    ax[i].get_yaxis().set_ticks([])\n    ax[i].set_title(y_train[amostra[i]]) # Coloca o label como t\u00edtulo da figura.\nplt.show()","7dfa8af9":"X_train = X_train.astype('float32') \/ 255.\nX_test = X_test.astype('float32') \/ 255.","c2aed336":"n_classes = 10 #s\u00e3o 10 classes: n\u00fameros de 0 a 9\nY_train = np_utils.to_categorical(y_train, n_classes)\nY_test = np_utils.to_categorical(y_test, n_classes)","278dc02a":"X_train_flat = X_train.reshape(60000, 784)\nX_test_flat = X_test.reshape(10000, 784)","d3980937":"#Par\u00e2metros\nnb_epoch = 15\nbatch_size = 128","af775dab":"model = Sequential()\nmodel.add(layers.Dense(512, input_shape=(784,)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.Dense(10))\nmodel.add(layers.Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","cd51551f":"model2 = Sequential()\nmodel2.add(layers.Dense(512, input_shape=(784,)))\nmodel2.add(layers.Activation('relu'))\n## Nova camada\nmodel2.add(layers.Dense(512))\nmodel2.add(layers.Activation('relu'))\n\nmodel2.add(layers.Dense(10))\nmodel2.add(layers.Activation('softmax'))\n\nmodel2.summary()\n\nmodel2.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model2.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model2.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","49de2ec6":"model3 = Sequential()\nmodel3.add(layers.Dense(512, input_shape=(784,)))\nmodel3.add(layers.Activation('relu'))\nmodel3.add(layers.Dropout(0.3)) # percentual de neur\u00f4nios que ser\u00e3o zerados durante o aprendizado\nmodel3.add(layers.Dense(512))\nmodel3.add(layers.Activation('relu'))\nmodel3.add(layers.Dropout(0.3)) # percentual de neur\u00f4nios que ser\u00e3o zerados durante o aprendizado\nmodel3.add(layers.Dense(10))\nmodel3.add(layers.Activation('softmax'))\n\nmodel3.summary()\n\nmodel3.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model3.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model3.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","cc8d9dbe":"img_rows = 28\nimg_cols = 28\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)","94b73402":"## Par\u00e2metros\nbatch_size = 2048\nn_filters = 32 #n\u00famero de filtros\nn_pool = 2 #Tamanho da camada de pooling\nn_conv = 3 #Tamanho da kernel do filtro ","e1520004":"model4 = Sequential()\nmodel4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel4.add(layers.Dropout(0.5))\nmodel4.add(layers.MaxPooling2D((2, 2)))\nmodel4.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel4.add(layers.Dropout(0.5))\nmodel4.add(layers.MaxPooling2D((2, 2)))\nmodel4.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel4.add(layers.Dropout(0.5))\nmodel4.add(layers.Flatten())\nmodel4.add(layers.Dense(64, activation='relu'))\nmodel4.add(layers.Dense(10, activation='softmax'))\nmodel4.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n\nmodel4.summary()\n\nmodel4.fit(X_train, Y_train, batch_size=batch_size, epochs=30, \n          verbose=1, validation_data=(X_test, Y_test))\nscore = model4.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","f1ba48d6":"Os dados est\u00e3o em valores entre 0 e 255. Vamos reescalon\u00e1-los para valores entre 0 e 1.","a0970e82":"Agora montamos nossa rede neural","c7e85819":"# Laborat\u00f3rio 2 - Classifica\u00e7\u00e3o de imagens\n\nNesse laborat\u00f3rio vamos utilizar um conjunto de dados p\u00fablico de imagens de d\u00edgitos (de 0 a 9), o <a href=\"https:\/\/en.wikipedia.org\/wiki\/MNIST_database\">MNIST<\/a>.  N\u00f3s exploraremos algumas arquiteturas de redes neurais para identificar corretamente um d\u00edgito.","6755bb99":"Primeiro formatamos os dados para em vez de ser uma matriz 28x28, ser um vetor de 784 valores.","84f704c3":"Com nosso primeiro modelo, conseguimos um resultado j\u00e1 muito bom, de 98,18% de acur\u00e1cia e um Categorical log-loss de 0.073. Vamos acrescentar mais uma camada na nossa rede neural e ver como fica.","641b8d51":"## Parte 2 - Usando uma rede neural tradicional\n\nNa nossa primeira tentativa, vamos usar uma rede neural tradicional com apenas uma camada de neur\u00f4nios escondida.","91b739f0":"Com a mudan\u00e7a de arquitetura, podemos ver que de quase 50% no Categorical Log-loss (de 0,06 para 0,04) e a acur\u00e1cia chegou a 99,2%","6e5deb26":"Agora vamos explorar os dados","02f9e93e":"## Parte 1 - Carregando os dados\n\nCarregando os datasets MNIST dispon\u00edveis ","e3a90116":"Note que temos imagens de 28x28 pixels. S\u00e3o 60000 imagens de treinamento e 10000 para teste. Agora vamos visualizar uma imagem e seu r\u00f3tulo.","4209b452":"## Parte 3 - Rede Neural Convolucional\n\nAgora vamos mudar de t\u00e9cnica, e tentar usar uma <a href=\"http:\/\/cs231n.github.io\/convolutional-networks\/\">rede neural convolucional<\/a>.\n\nPrimeiro temos de formatar os dados para um array com dimens\u00f5es (n_exemplos, n_pixel_x, n_pixel_y, n_cores)","5954f9fa":"O nosso resultado piorou. Isso pode ocorrer por alguns motivos. O primeiro \u00e9 que aumentando o n\u00famero de par\u00e2metros a serem aprendidos na nossa rede, precisar\u00edamos de mais \u00e9pocas para trein\u00e1-la. O segundo \u00e9 que a nossa rede pode estar se adaptando demais aos dados de treino e sendo incapaz de generalizar nos dados de teste (*Overfiting*). H\u00e1 outros motivos, relacionados aos par\u00e2metros, mas vamos nos ater a esses por enquanto.\n\nPara o segundo problema, podemos usar a t\u00e9cnica de <a href=\"http:\/\/www.jmlr.org\/papers\/volume15\/srivastava14a.old\/source\/srivastava14a.pdf\">Dropout<\/a>, que tem se mostrado muito efetiva na preven\u00e7\u00e3o de *overfitting*. ","b1fc15cc":"Agora montamos nossa rede convolucional","aa2ba8e7":"Com nosso terceiro modelo, obtivemos um resultado um pouco melhor de Log-loss e um pouco pior de acur\u00e1cia que o primeiro modelo.  Claro que \u00e9 uma compara\u00e7\u00e3o simplista, pois poder\u00edamos variar uma s\u00e9rie de par\u00e2metros para fazer essas compara\u00e7\u00f5es.","cd3b5318":"E convertemos o vetor com o n\u00famero representado em cada imagem num formato apropriado para o Keras"}}