{"cell_type":{"a68bf67b":"code","3ff3b4d2":"code","183b7b5c":"code","5d6be8a2":"code","e01c1c06":"code","0f68e55b":"code","14ab7be3":"code","02ac780c":"code","9cc8c67a":"code","cbfb098e":"code","294b76d8":"code","6188ee75":"code","5e3c01e8":"code","d52f43f3":"code","c655e2d3":"code","13ab05f2":"code","f853c54c":"code","a6525a92":"code","4cffdcd4":"code","08d61100":"code","8edcd4d1":"code","cb600d12":"code","a7ce0e59":"code","a3d2921d":"code","4aaf613a":"code","eb6f0f29":"code","b57c56aa":"code","e1164323":"code","d7388c86":"code","59b840c0":"code","08eada6c":"code","34ad477b":"code","b70fe4f8":"code","9ba53a19":"code","9b1c9468":"code","35657cdf":"code","c88c112d":"code","18fecbdd":"code","99436fb2":"code","d17afe43":"code","3a7e2a7a":"code","a7323978":"code","564555e1":"code","9be775e3":"code","37e160dc":"code","d28bc6dc":"code","b97281a5":"code","000f7829":"code","6d372d31":"code","5c9cafd0":"code","6d461e9c":"code","c0d073a6":"code","4121cffb":"code","102d7cff":"code","a0abc614":"code","c9dbfec7":"markdown","a0e6ad2e":"markdown","a333b0ac":"markdown","0d02e004":"markdown","4dae806d":"markdown","b73289f1":"markdown","2f94f1df":"markdown","b585959f":"markdown","3a56a336":"markdown","ec769d1f":"markdown","b9435799":"markdown","fc92ddd8":"markdown","adb11dc8":"markdown","9179d428":"markdown","91ea115e":"markdown","2769a1cc":"markdown","caaa5700":"markdown","56ad9d9c":"markdown","2c4bf618":"markdown","860d3274":"markdown"},"source":{"a68bf67b":"#Importing libraries\n\nimport os\nimport pandas as pd \nimport numpy as np\n\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\ncolors = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\nsns.set_palette(palette = colors, n_colors = 4)\n\nprint('Data Manipulation, Mathematical Computation and Visualisation packages imported!')\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nprint('Deprecation warning will be ignored!')","3ff3b4d2":"#Uploading the 5 excel sheets consisting the GTD data\n\nxl_1=pd.ExcelFile(r\"..\/input\/gtd1993_0718dist.xlsx\") #Uploading Excel sheet1\nxl_2=pd.ExcelFile(r\"..\/input\/gtd_14to17_0718dist.xlsx\") #Uploading Excel sheet2\nxl_3=pd.ExcelFile(r\"..\/input\/gtd_70to95_0718dist.xlsx\") #Uploading Excel sheet3\nxl_4=pd.ExcelFile(r\"..\/input\/gtd_96to13_0718dist.xlsx\") #Uploading Excel sheet4\n#xl_5=pd.ExcelFile(r\"C:\\Users\\aniru\\OneDrive\\Desktop\\Studies\\Data Science\\Fellowship.AI\\data5.xlsx\") #Uploading Excel sheet5\n\nprint('Excel Files of GTD Uploaded!')","183b7b5c":"#Parsing the excel sheets to be suitable for pandas dataframe. \n#The sheets are named as \"Data\" in all the five GTD excel files.\n\ndf_1= xl_1.parse(\"Data\") #Excel sheet1 parsed into dataframe1 (df_1)\ndf_2= xl_2.parse(\"Data\") #Excel sheet2 parsed into dataframe2 (df_2)\ndf_3= xl_3.parse(\"Data\") #Excel sheet3 parsed into dataframe3 (df_3)\ndf_4= xl_4.parse(\"Data\") #Excel sheet4 parsed into dataframe4 (df_4)\n#df_5= xl_5.parse(\"Data\") #Excel sheet5 parsed into dataframe5 (df_5)\n\nprint('Excel sheets converted to DataFrames!')","5d6be8a2":"#Appending all the 5 dataframes into a single dataframe\n\n#df_2 has been appended into df_1\ndf_1=df_1.append(df_2, ignore_index=True)\n#df_3 has been appended into df_1\ndf_1=df_1.append(df_3, ignore_index=True)\n#df_4 has been appended into df_1\ndf_1=df_1.append(df_4, ignore_index=True)\n\n","e01c1c06":"# The number of rows and columns of terror_df, found out by pd.Dataframe.shape\n\n#df_1 acts as a backup dataframe \nterror_df=df_1.copy()\n\nterror_df.head(10)\n\n#terror_df is the copy of df1 where all other dataframes have been appended","0f68e55b":"terror_df.shape","14ab7be3":"# To avoid confusion,the dataset is restricted to only attacks that were of terrorist nature.\n\nterror_df = terror_df[(terror_df.crit1 == 1) & (terror_df.crit2 == 1) & (terror_df.crit3 == 1) & (terror_df.doubtterr == 0)]","02ac780c":"#missing value counts in each of these columns\n\nmiss = terror_df.isnull().sum()\/len(terror_df)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace=True)\nmiss\n","9cc8c67a":"# Removing columns having more than 20% of missing values\n\nfrac = len(terror_df) * 0.8\nterror_df=terror_df.dropna(thresh=frac, axis=1)\nterror_df= terror_df.replace(-9,0)\nterror_df= terror_df.drop([\"city\"], axis=1)\nprint('Removing columns having 20% of missing values')","cbfb098e":"#missing value counts in each of these columns after removing columns having more than 20% of missing values\n\nmiss = terror_df.isnull().sum()\/len(terror_df)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace=True)\nmiss","294b76d8":"#replacing few strings with generalised strings to be common for all rows\n\nterror_df.weaptype1_txt.replace(\n    'Vehicle (not to include vehicle-borne explosives, i.e., car or truck bombs)',\n    'Vehicle', inplace = True)\n\nterror_df.gname = terror_df.gname.str.lower()","6188ee75":"# replacing 'nkill', 'nwound', 'ncasualties' with has_casualities boolean\n\nterror_df['ncasualties'] = terror_df['nkill'] + terror_df['nwound']\nterror_df['has_casualties'] = terror_df['ncasualties'].apply(lambda x: 0 if x == 0 else 1)\nterror_df= terror_df.drop(['nkill', 'nwound', 'ncasualties'], axis=1)","5e3c01e8":"#visualising missing values\n\nmiss = terror_df.isnull().sum()\/len(terror_df)\nmiss = miss[miss > 0]\nmiss = miss.to_frame()\nmiss.columns = ['count']\nmiss.index.names = ['Name']\nmiss['Name'] = miss.index\n\n#plot the missing value count\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Name', y = 'count', data=miss)\nplt.xticks(rotation = 90)\nplt.show()","d52f43f3":"# imputing with mode\nterror_df['specificity'] = terror_df['specificity'].fillna(terror_df['specificity'].mode()[0])\nterror_df['natlty1'] = terror_df['natlty1'].fillna(terror_df['natlty1'].mode()[0])\n\n#imputing with value '0'\nterror_df['guncertain1'] = terror_df['guncertain1'].fillna(terror_df['guncertain1'].fillna(0))\nterror_df['targsubtype1'] = terror_df['targsubtype1'].fillna(terror_df['targsubtype1'].fillna(0))\nterror_df['weapsubtype1'] = terror_df['weapsubtype1'].fillna(terror_df['weapsubtype1'].fillna(0))\nterror_df['ishostkid'] = terror_df['ishostkid'].fillna(terror_df['ishostkid'].fillna(0))\nterror_df['latitude'] = terror_df['latitude'].fillna(terror_df['latitude'].fillna(0))\nterror_df['longitude'] = terror_df['longitude'].fillna(terror_df['longitude'].fillna(0))\n\nprint(\"Numerical variables Imputed!\")","c655e2d3":"for col in (\"target1\",\"natlty1_txt\", \"provstate\", \"targsubtype1_txt\", \"weapsubtype1_txt\"):\n    terror_df[col] = terror_df[col].fillna(\"None\")\nprint(\"Categorical variables Imputed!\")\n","13ab05f2":"unknown_df= terror_df[terror_df['gname'] == \"unknown\"]","f853c54c":"# removing all unknown_df values from terror_df\n\nterror_df=terror_df[~terror_df.isin(unknown_df)]\nterror_df= terror_df.dropna()","a6525a92":"# dropping \"iyear\", \"eventid\", \"imonth\", \"iday\", \"extended\" as they are redundant.\n\nterror_df=terror_df.drop([\"iyear\", \"eventid\", \"imonth\", \"iday\", \"extended\"], axis=1)\nterror_df= terror_df.dropna()\n\nunknown_df=unknown_df.drop([\"iyear\", \"eventid\", \"imonth\", \"iday\", \"extended\"], axis=1)\nunknown_df= unknown_df.dropna()","4cffdcd4":"# All the gname stored into an another dataset Y\n\nY = pd.DataFrame()\nY = terror_df[\"gname\"]\nY_count= Y.value_counts()","08d61100":"print(Y_count)","8edcd4d1":"# Binning whole gname values based on their frequency as explained in above steps.\n\nY_bins=pd.qcut(Y_count, [0, .5, .75, 1], labels=[\"Small Org\", \"Medium Org\" ,\"Dangerous Groups\"])\nY_bins= Y_bins.to_dict()\n\nY=Y.reset_index()\nY=Y.drop([\"index\"], axis=1)\n\nLabel = []\n\nfor x in Y[\"gname\"]:\n    for key, value in Y_bins.items():\n           if x==key:\n            Label.append(value)\n\nY[\"Label\"]= pd.Series(Label)","cb600d12":"# Example of dataset after Binning \n\nY['Label'].head(5)","a7ce0e59":"# Verifying the Binning process with barchart\n\nplt.subplot(1, 3, 3)\nsns.barplot(x=\"Label\", y=Y.index, data=Y);","a3d2921d":"# Label encoding the \"Small Org\", \"Medium Org\", and \"Dangerous Groups\" into integers as they are ordinal variables and as it will be easier for training the model\n\nY['Label'] = Y['Label'].map({\"Small Org\":1, \"Medium Org\":2, \"Dangerous Groups\":3})\nY['Label'].shape","4aaf613a":"#Storing the binned and encoded terror organisation names back to terror_df dataset.\n\nterror_df[\"Label\"]= Y[\"Label\"].values","eb6f0f29":"# As gname has been binned and encoded into \"Label\" column, \"gname\" is dropped. \"Label\" now acts as a label dataset while training.\n\nterror_df= terror_df.drop([\"gname\"], axis=1)","b57c56aa":"#preparing the correlation matrix\n\ncorr = terror_df.corr()\nplt.subplots(figsize=(30, 30))\ncmap = sns.diverging_palette(150, 250, as_cmap=True)\nsns.heatmap(corr, cmap=\"RdYlBu\", vmax=1, vmin=-0.6, center=0.2, square=True, linewidths=0, cbar_kws={\"shrink\": .5}, annot = True);","e1164323":"print (corr['Label'].sort_values(ascending=False)[:15], '\\n') #top 15 values\nprint ('----------------------')\nprint (corr['Label'].sort_values(ascending=False)[-5:]) #last 5 values`","d7388c86":"#Dropping the gname column from the unknown_df dataset. Its filled with \"unknown\" values thus its no use for us.\n\nunknown_df= unknown_df.drop([\"gname\"], axis= 1)","59b840c0":"# Storing Label column from terror_df as our Label Dataset for training process\n\ny_train = terror_df.Label.values","08eada6c":"# Dropping Label from terror_df as we will later convert this dataset into train dataset.\n\nterror_df= terror_df.drop([\"Label\"], axis= 1)","34ad477b":"# Concating terror_df and unknown_df into one column for encoding categorical variables and name it as all_data\n\n# Storing shape of train dataset and test dataset for later use\nntrain = terror_df.shape[0] # terror_df is train dataset\nntest = unknown_df.shape[0] # unknown_df is test dataset\n\nall_data= pd.concat((terror_df, unknown_df)).reset_index(drop=True)\n\nprint(\"all_data shape: {}\".format(all_data.shape))","b70fe4f8":"cols = terror_df.columns\n\n# List all columns having categorical variables\n\nnum_cols = terror_df._get_numeric_data().columns\nlist(set(cols) - set(num_cols))","9ba53a19":"# One-hot encoding all nominal variables\n\nall_data[\"targtype1_txt\"] = pd.get_dummies(all_data[\"targtype1_txt\"])\nall_data[\"region_txt\"] = pd.get_dummies(all_data[\"region_txt\"])\nall_data[\"country_txt\"] = pd.get_dummies(all_data[\"country_txt\"])\nall_data[\"dbsource\"] = pd.get_dummies(all_data[\"dbsource\"])\nall_data[\"provstate\"] = pd.get_dummies(all_data[\"provstate\"])","9b1c9468":"#Label encoding all categorical variables\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(all_data[\"attacktype1_txt\"])\nlist(le.classes_)\nall_data[\"attacktype1_txt\"]= pd.Series(list(le.transform(all_data[\"attacktype1_txt\"]))) \n\nle_2 = preprocessing.LabelEncoder()\nle_2.fit(all_data[\"weapsubtype1_txt\"])\nlist(le_2.classes_)\nall_data[\"weapsubtype1_txt\"]= pd.Series(list(le_2.transform(all_data[\"weapsubtype1_txt\"]))) \n\nle_3 = preprocessing.LabelEncoder()\nle_3.fit(all_data[\"target1\"])\nlist(le_3.classes_)\nall_data[\"target1\"]= pd.Series(list(le_3.transform(all_data[\"target1\"]))) \n\nle_4 = preprocessing.LabelEncoder()\nle_4.fit(all_data[\"weaptype1_txt\"])\nlist(le_4.classes_)\nall_data[\"weaptype1_txt\"]= pd.Series(list(le_4.transform(all_data[\"weaptype1_txt\"]))) \n\nle_5 = preprocessing.LabelEncoder()\nle_5.fit(all_data[\"targsubtype1_txt\"])\nlist(le_5.classes_)\nall_data[\"targsubtype1_txt\"]= pd.Series(list(le_5.transform(all_data[\"targsubtype1_txt\"]))) \n\nle_6 = preprocessing.LabelEncoder()\nle_6.fit(all_data[\"natlty1_txt\"])\nlist(le_6.classes_)\nall_data[\"natlty1_txt\"]= pd.Series(list(le_6.transform(all_data[\"natlty1_txt\"]))) ","35657cdf":"# Scaling all the values so that they are in the same scale\n\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nall_data=sc.fit_transform(all_data)\n","c88c112d":"# As scaling turns all_data to numpy, we convert it back to pandas for later use.\n\nall_data = pd.DataFrame(all_data)","18fecbdd":"#creating train and test datasets\n\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\nprint(train.shape)\nprint(test.shape)","99436fb2":"from xgboost import XGBClassifier\nfrom xgboost import plot_importance\nmodel = XGBClassifier()\nmodel.fit(train, y_train)","d17afe43":"# Sort feature importances from GBC model trained earlier\nindices = np.argsort(model.feature_importances_)[::-1]\nindices = indices[:75]\n\n# Visualise these with a barplot\nplt.subplots(figsize=(20, 15))\ng = sns.barplot(y=cols[indices], x = model.feature_importances_[indices], orient='h', palette = colors)\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"XGB feature importance\");","3a7e2a7a":"# Storing and copying train and test values for safety and maybe later use\n\ntrain_2 = train.copy()\ntest_2 = test.copy()","a7323978":"from sklearn.feature_selection import SelectFromModel\n\n# Allow the feature importances attribute to select the most important features\nxgb_feat_red = SelectFromModel(model, prefit = True)\n\n# Reduce estimation, validation and test datasets\ntrain = xgb_feat_red.transform(train)\ntest = xgb_feat_red.transform(test)\n\n\nprint(\"Results of 'feature_importances_':\")\nprint('X_train: ', train.shape, '\\nX_test: ', test.shape)","564555e1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Dividing train dataset and y_train dataset to X_train, X_test, Y_train, Y_test respectively for training the model.\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, y_train, test_size=0.3, random_state=42, stratify=y_train)","9be775e3":"print('X_train: ', X_train.shape, '\\nX_test: ', X_test.shape, '\\nY_train: ', Y_train.shape, '\\nY_test: ', Y_test.shape)","37e160dc":"# Importing Statistical packages!\n# Importing Metrics packages!\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nprint('Validation and metric libraries imported')","d28bc6dc":"# Importing Algorithm packages!\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nprint('Classification Models imported')","b97281a5":"names = [\"K-Nearest Neighbors\", \"Linear SVM\",  \"Naive Bayes\", \"Decision Tree\", \"Neural Net\"]\n\nmodels= [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), GaussianNB(), DecisionTreeClassifier(max_depth=5), MLPClassifier(alpha=1)]","000f7829":"# First I will use ShuffleSplit as a way of randomising the cross validation samples.\nshuff = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\n#create table to compare MLA metrics\ncolumns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy']\nbefore_model_compare = pd.DataFrame(columns = columns)\n\n#index through models and save performance to table\nrow_index = 0\nfor alg in models:\n\n    #set name and parameters\n    model_name = alg.__class__.__name__\n    before_model_compare.loc[row_index, 'Name'] = model_name\n    before_model_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n    \n    alg.fit(X_train, Y_train)\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    training_results = cross_val_score(alg, X_train, Y_train, cv = shuff, scoring= 'accuracy').mean()\n    test_results = accuracy_score(Y_test, alg.predict(X_test)).mean()\n    \n    before_model_compare.loc[row_index, 'Train Accuracy Mean'] = (training_results)*100\n    before_model_compare.loc[row_index, 'Test Accuracy'] = (test_results)*100\n    \n    row_index+=1\n    print(row_index, alg.__class__.__name__, 'trained...')\n\ndecimals = 3\nbefore_model_compare['Train Accuracy Mean'] = before_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\nbefore_model_compare['Test Accuracy'] = before_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\nbefore_model_compare","6d372d31":"models= [ KNeighborsClassifier(), SVC(), GaussianNB(), DecisionTreeClassifier(), MLPClassifier()]\n\nKNN_param_grid = {'n_neighbors': [1,2,3]}\nSVC_param_grid = {'C': [0.025, 0.030, 0.020]}\nG_param_grid = {'priors': [None, None]}\nDTC_param_grid = {'max_depth': range(5, 30, 2)}\nMLP_param_grid = {'alpha': [1, 2, 3]}\n\nparams_grid = [KNN_param_grid, SVC_param_grid, G_param_grid, DTC_param_grid, MLP_param_grid]\n#create table to compare MLA metrics\ncolumns = ['Name', 'Parameters', 'Train Accuracy Mean', 'Test Accuracy']\nafter_model_compare = pd.DataFrame(columns = columns)\n\nrow_index = 0\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring= 'accuracy', n_jobs=-1)\n    \n    params_grid.pop(0)\n    #set name and parameters\n    model_name = alg.__class__.__name__\n    after_model_compare.loc[row_index, 'Name'] = model_name\n    \n    gs_alg.fit(X_train, Y_train)\n    gs_best = gs_alg.best_estimator_\n    after_model_compare.loc[row_index, 'Parameters'] = str(gs_alg.best_params_)\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    \n    after_training_results = gs_alg.best_score_\n    after_test_results = accuracy_score(Y_test, gs_alg.predict(X_test)).mean()\n    \n    after_model_compare.loc[row_index, 'Train Accuracy Mean'] = (after_training_results)*100\n    after_model_compare.loc[row_index, 'Test Accuracy'] = (after_test_results)*100\n    \n    row_index+=1\n    print(row_index, alg.__class__.__name__, 'trained...')\n\ndecimals = 3\nafter_model_compare['Train Accuracy Mean'] = after_model_compare['Train Accuracy Mean'].apply(lambda x: round(x, decimals))\nafter_model_compare['Test Accuracy'] = after_model_compare['Test Accuracy'].apply(lambda x: round(x, decimals))\nafter_model_compare\n\n","5c9cafd0":"models= [ KNeighborsClassifier(), SVC(), GaussianNB(), DecisionTreeClassifier(), MLPClassifier()]\n\nKNN_param_grid = {'n_neighbors': [1,2,3]}\nSVC_param_grid = {'C': [0.025, 0.030, 0.020]}\nG_param_grid = {'priors': [None, None]}\nDTC_param_grid = {'max_depth': range(5, 30, 2)}\nMLP_param_grid = {'alpha': [1, 2, 3]}\n\nparams_grid = [KNN_param_grid, SVC_param_grid, G_param_grid, DTC_param_grid, MLP_param_grid]\n\nstacked_validation_train = pd.DataFrame()\nstacked_test_train = pd.DataFrame()\n\nrow_index=0\n\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'accuracy', n_jobs=-1)\n    params_grid.pop(0)\n    \n    gs_alg.fit(X_train, Y_train)\n    gs_best = gs_alg.best_estimator_\n    stacked_validation_train.insert(loc = row_index, column = names[0], value = gs_best.predict(X_test))\n    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking validation dataset...')\n    \n    stacked_test_train.insert(loc = row_index, column = names[0], value = gs_best.predict(test))\n    print(row_index+1, alg.__class__.__name__, 'predictions added to stacking test dataset...')\n    print(\"-\"*50)\n    names.pop(0)\n    \n    row_index+=1\n    \nprint('Done')","6d461e9c":"stacked_validation_train.head(5)","c0d073a6":"stacked_test_train.head(5)","4121cffb":"# First drop the Lasso results from the table, as we will be using Lasso as the meta-model\ndrop = ['K-Nearest Neighbors']\nstacked_validation_train.drop(drop, axis=1, inplace=True)\nstacked_test_train.drop(drop, axis=1, inplace=True)\n\n# Now fit the meta model and generate predictions\nmeta_model = make_pipeline(RobustScaler(),KNeighborsClassifier(1))\nmeta_model.fit(stacked_validation_train, Y_test)\n\nmeta_model_pred = meta_model.predict(stacked_test_train)\nprint(\"Meta-model trained and applied!...\")","102d7cff":"models= [ KNeighborsClassifier(), SVC(), GaussianNB(), DecisionTreeClassifier(), MLPClassifier()]\nnames = [\"K-Nearest Neighbors\", \"Linear SVM\",  \"Naive Bayes\", \"Decision Tree\", \"Neural Net\"]\nKNN_param_grid = {'n_neighbors': [1,2,3]}\nSVC_param_grid = {'C': [0.025, 0.030, 0.020]}\nG_param_grid = {'priors': [None, None]}\nDTC_param_grid = {'max_depth': range(5, 30, 2)}\nMLP_param_grid = {'alpha': [1, 2, 3]}\n\nparams_grid = [KNN_param_grid, SVC_param_grid, G_param_grid, DTC_param_grid, MLP_param_grid]\n\n\nfinal_predictions = pd.DataFrame()\n\nrow_index=0\n\nfor alg in models:\n    \n    gs_alg = GridSearchCV(alg, param_grid = params_grid[0], cv = shuff, scoring = 'accuracy', n_jobs=-1)\n    params_grid.pop(0)\n    \n    gs_alg.fit(stacked_validation_train, Y_test)\n    gs_best = gs_alg.best_estimator_\n    final_predictions.insert(loc = row_index, column = names[0], value = gs_best.predict(stacked_test_train))\n    print(row_index+1, alg.__class__.__name__, 'final results predicted added to table...')\n    names.pop(0)\n    \n    row_index+=1\n\nprint(\"-\"*50)\nprint(\"Done\")\n    \nfinal_predictions.head()","a0abc614":"ensemble = meta_model_pred*(2.5\/10)+ final_predictions[\"K-Nearest Neighbors\"]*(1.5\/10)+final_predictions[\"Linear SVM\"]*(1.5\/10)+ final_predictions[\"Naive Bayes\"]*(1.5\/10)+ final_predictions[\"Decision Tree\"]*(1.5\/10)+ final_predictions[\"Neural Net\"]*(1.5\/10)\n\n#Final predicted values for the attacks that have been not attrbuted any attacks.\n\nunknown_df[\"gname\"]= ensemble.values\n\nunknown_df.head(5)","c9dbfec7":"We can see that each of the models performs with varying ability, with DecisionTreeClassifier having the best accuracy score on the training dataset and accuracy on the validation dataset.\n\n### Optimisation\n\nAs you can see from the above table, the accuracy for these models is not quite as good as it could be.\nThis is because we use the default configuration of parameters for each of the algorithms.\nSo now, we will use GridSearchCV to find the best combinations of parameters to produce the highest scoring models.\n","a0e6ad2e":"## 1.\n### Import packages","a333b0ac":"### 3.\n### Exploratory Data Analysis\n\n### Correlation matrix\n\nNow that missing values and outliers have been treated, I will analyse each feature in more detail. This will give guidance on how to prepare this feature for modelling. I will analyse the features based on the different aspects of the terror organisation available in the dataset.","0d02e004":"### In order to impute missing numeric values we do the following\n\n#### 1. Replace missing 'specificity' and 'natlty1' with the mode, i.e replacing with the values which occur the most.\n\n#### 2. Replace other missing numeric values with 0 as replacing it with any other imputing methods (for eg: mean, median , mode) will result in identifying wrong characteristic\/ behaviour of a terror organisation.","4dae806d":"### A missing value is an entry in a column that has no assigned value. This can mean multiple things:\n\n1. A missing value may be the result of an error during the production of the dataset. \nThis could be a human error, or machinery error depending on where the data comes from.\n2. A missing value in some cases, may just mean a that a 'zero' should be present. In which case, it can be replaced by a 0. The data description provided helps to address situations like these.\n3. However, missing values represent no information. Therefore, does the fact that you don't know what value to assign an entry, mean that filling it with a 'zero' is always a good fit?\n4. Some algorithms do not like missing values. Some are capable of handling them, but others are not. Therefore since we are using a variety of algorithms, it's best to treat them in an appropriate way.\n\nIf we have missing values, we have two options:\n\n1. Delete the entire row\n2. Fill the missing entry with an imputed value\n\nLets see, what I have done with this dataset.","b73289f1":"## 2.\n\n### Dataset Preparation","2f94f1df":"### Ensemble\n\nAs our models having varying degrees of accuracy we also use Ensemble methods.\n\nThey work on the idea that many weak learners, can produce a strong learner.\n\n\nTherefore, using the meta-model that I will create, I will also combine this with the results of the individual optimised models to create an ensemble.\n\n\nIn order to create this ensemble, I must collect the final predictions of each of the optimised models. I will do this now.","b585959f":"1. As you can see, each of the models produces results that vary quite widely. This is the beauty of using a combination of many different models.\n\n2. Some models will be much better at catching certain signals in the data, whereas others may perform better in other situations.\n\n3. By creating an ensemble of all of these results, it helps to create a more generalised model that is resistant to noise.\n\n## 5.\n\n### Finish\n\n### Now, I will finish by creating an ensemble of the meta-model and optimised models.","3a56a336":"All the attacks have not been attributed to a particular terrorist group are collected to a unknown_df dataset which will later act as test dataset.","ec769d1f":"The GTD database is known for its lots of missing values. We check percentage of missing values in each columns and remove columns having more than 20% of missing data.","b9435799":"The most common relationship we may think of between two variables, would be a straight line or a linear relationship. \n\nWhat this means is that if we increase the predictor by 1 unit, the response always increases by X units. \n\nHowever, not all data has a linear relationship and therefore it may be necessary for our model to fit the more complex relationships in the data.\n\nThere are a variety of curve-fitting methods we can choose from to help us with this.\n\nThe most common way to fit curves to the data is to include polynomial terms, such as squared or cubed predictors.\n\n\nTypically, we choose the model order by the number of bends you need in your line. Each increase in the exponent produces one more bend in the curved fitted line. It\u2019s very rare to use more than a cubic term.\n\nAs all the variables in most of the top columns (from correlation matrix) have values 0 and 1, its not necessary to follow this step but in order to explain my thinking process, I have described this step.\n\nAlso, If the response data follows a pattern that descends down to a lower bound, or ascends up to an upper bound, we can fit this type of relationship by including the reciprocal (1\/x) of one or more predictor variables in the model, (For eg: INT_MISC in this dataset)\n","fc92ddd8":"Overall we can see that the training and test scores for each of the models have increased, which is what we want.\n\n#### Now we have a set of highly tuned algorithms to use for \"STACKING\".\n\n### Stacking\nNow that we have a set of highly tuned algorithms, a rather famous and successful technique to further improve the accuracy of these models, is to use Stacking.\n\n\n#### The steps for this technique are shown below:\n\n1. Create a set of algorithms ready for stacking - We've done this...\n2. Split the original training data into a training and validation sample - We've done this too...\n3. Train the algorithms on the training sample - Also done this...\n4. For each algorithm, apply the trained models to the validation dataset and create a set of predictions, 1 column for each model, as a new table. Call this the new training dataset.\n5. Also apply the trained algorithm to the test dataset and create a final set of predictions, 1 column for each model, as a new table. Call this new test dataset.\n6. For the new training dataset, we have labeled outputs, in the form of Y_test. Now we must train another model on these two feature sets: new training dataset and Y_test.\n7. Use this newly trained model to predict values for new test dataset.\n","adb11dc8":"### Imputing missing categorical values with 'None'","9179d428":"### I will use these two datasets to train and produce predictions for the meta-model,K-Nearest Neighbors","91ea115e":"### 2.\n## Load data\n\nThe Pandas package helps us work with our datasets.\n","2769a1cc":"Before I start with the stacking, I need to decide which algorithms to use as my base estimators, and which to use as the meta-model.\n\nSince KNeighborsClassifier performed the best after optimisation, I chose this to be the meta-model. All other models will be used as base estimators.\n\nSo now, I will cycle through each optimised estimator, train them on the training dataset, apply to them the validation and test datasets, then finally outputting the predictions for validation and test into two new datasets: stacked_validation_train and stacked_test_train.","caaa5700":"### Training\n\n#### We are finally ready to train our models. For this analysis I am using 5 different algorithms:\n\n1. \"K-Nearest Neighbors\"\n2. \"Linear SVM\"\n3. \"Naive Bayes\"\n4. \"Decision Tree\"\n5. \"Neural Net\"\n\n#### The method of measuring accuracy was chosen to be \"Accuracy\"","56ad9d9c":"## 4. \n## Modeling\n\n### Preparation of data\nNow that our dataset is ready for modeling, we must prepare it from training, testing and prediction. One of the vital steps here is to reduce the number of features. I will do this using XGBoost's inbuilt feature importance functionality.","2c4bf618":"### This step acts as a crucial step in this Label pre-processing step. \n\n### In this problem the column \"gname\" acts as a Label in the training steps.\n\n#### There are a high number of terrorist organisations (3058) as shown in below step named in GTD which are also attributed to multiple terrorist attacks. This will also be difficult to formulate as a classification problem as it will need lots of computation power to predict name of each individual terrorist organisation behind a attack. Thus in order to fit this problem to computation of 4GB RAM local machine following steps have been undertaken.\n\nAll the organisations are classified as Dangerous Groups, Medium Organisations and Small Organisations based on their attack frequency and count by using \"BINNING\".\n\n1. Organisations having 75% of attacks attributed are classified as Dangerous Groups\n2. Organisations having 75% of attacks attributed are classified as Medium Org\n3. Organisations having 75% of attacks attributed are classified as Small Org","860d3274":"## \"What are the characteristics of a terror attack which could predict which terror organisation may have been responsible for an incident ? \"\n\n### 1. Motive : Motive may be political\n### 2. Description of the attack\n### 3. Area or Region of attack\n### 4. Weapons used\n### 5. Pattern of attacks (duration of attacks)\n\nThese insights will guide the feature engineering process.\n\n\n#### Problem Formulation: Lots of missing values is a spefic charateristic of Global Terrorist Database. Thus even lots of attacks have not been attributed to a particular terrorist group. This also leads to the problem formulation of this challenge solution.\n\n1.The attacks have not been attributed to a particular terrorist group are assigned to the Test Dataset.\n\n2.The models that have been attributed to a particular terrorist group acts as a Train Dataset.\n\n#### Challenge Goal: To accurately train a model to predict the terrorist organisations behind the attacks that have been not been attributed to a terrorist group. \n\n"}}