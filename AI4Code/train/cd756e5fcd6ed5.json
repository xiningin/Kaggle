{"cell_type":{"45015d5e":"code","98e3c341":"code","06091cef":"code","f492556a":"code","c1fcdb2d":"code","b8dfa750":"code","c0ecbf70":"code","ae949331":"code","28e8a5b3":"code","11fb5e6f":"code","2ba0da34":"code","c70df01f":"code","07765224":"code","b4941a1f":"code","479b30bf":"code","1c3a3045":"code","1c0b69a4":"code","d40f1999":"code","d3712ed3":"code","793b76f6":"code","4b4ab576":"code","00d2ba9a":"code","d638aec5":"markdown","06b2c95d":"markdown","ddf13277":"markdown","fb4360f9":"markdown","25a274d5":"markdown","9b797604":"markdown","43e77b93":"markdown","8ec913a6":"markdown","a631e2ba":"markdown","b5c42f80":"markdown","ed535154":"markdown"},"source":{"45015d5e":"# Import necessary modules\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport datetime\nimport seaborn as sns\n\npd.options.display.latex.repr=True\n\nfrom scipy.stats import randint, geom\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import ParameterGrid\n\nfrom imblearn.metrics import geometric_mean_score\n\n","98e3c341":"store = pd.HDFStore('..\/input\/io.h5', 'r')\nstore.open()\nstore.keys()\n\ndsets = ['train',\n         'test',\n         'kaggle']\n\nX = dict.fromkeys(dsets)\ny = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    X[ds] = store['\/X\/' + str(ds)]\n    y[ds] = store['\/y\/' + str(ds)]\n\nstore.close()\nstore.is_open","06091cef":"skew_train = ((y['train'].count() - y['train'].sum())\/y['train'].sum())\n\nskew_test = ((y['test'].count() - y['test'].sum())\/y['test'].sum())\n\nprint('Ratio of negative class to positive class: Skew = %.4f' % skew_train)","f492556a":"rfc = RandomForestClassifier(n_estimators=35,\n                             n_jobs=-1,\n                             oob_score=True,\n                             random_state=20190603)","c1fcdb2d":"# current grid\n# (5,3) 'min_impurity_decrease' vs 'min_samples_leaf' or 'max_features'\nparam1 = 'min_impurity_decrease'\nparam2 = 'min_samples_leaf'\n# param2 = 'max_features'\n\nparam_grid = {\n    param1: np.geomspace(1e-8, 1e-6, 5),\n    param2: np.around(np.geomspace(12, 48, 3)).astype(int)\n#    param2: np.arange(6, 9)    \n}","b8dfa750":"# # Initial coarse search parameters (superset) from another notebook:\n# param_grid = {\n# #    \"max_depth\": [4, 8, 16, 32], # does not seem helpful\n#     \"max_features\": [5, 6, 7, 8, 9],\n#     \"min_samples_leaf\": [4, 8, 16, 32],\n#     \"criterion\": [\"gini\", \"entropy\"] # just use gini, should not matter\n# }","c0ecbf70":"# # (5,3) 'min_impurity_decrease' vs 'max_features'\n# param1 = 'min_impurity_decrease'\n# param2 = 'max_features'\n\n# param_grid = {\n#     param1: np.geomspace(1e-8, 1e-6, 5),\n#     param2: np.arange(6, 9)\n# }","ae949331":"# # (3,3) 'min_impurity_decrease' vs 'max_features'\n# param1 = 'min_impurity_decrease'\n# param2 = 'max_features'\n\n# param_grid = {\n#     param1: np.geomspace(1e-8, 1e-6, 3),\n#     param2: np.arange(6, 9)\n# }","28e8a5b3":"# # testing plots\n# param1 = 'min_impurity_decrease'\n# param2 = 'max_features'\n\n# param_grid = {\n#     param1: [1e-6, 1e-5],\n#     param2: [6, 7]\n# }","11fb5e6f":"# Double check which param_grid we set:\nparam_grid","2ba0da34":"rows = []\ncols = []\ndecisions = []\nsizes = []\n\nfor params in ParameterGrid(param_grid):\n    print(datetime.datetime.now())\n    print(params)\n    rfc.set_params(**params)\n    rfc.fit(X['train'], y['train'].values.ravel())\n    rows.append(params[param1])\n    cols.append(params[param2])\n    decisions.append(rfc.oob_decision_function_[:, 1])\n    sizes.append([rfc.estimators_[i].tree_.node_count\n                  for i in range(len(rfc.estimators_))])\n    \nprint(datetime.datetime.now())","c70df01f":"print('The number of NaNs is: %i' % np.isnan(decisions[0]).sum())\nprint('The percentage of NaNs is: {:.4%}'.format(np.isnan(decisions[0]).sum()\/len(decisions[0])))","07765224":"# Create a dictionary of metrics to compute multiple scores\n\nmetrics_dict = {}\n\n\nmetrics_dict['auc_roc'] = {'fcn' : metrics.roc_auc_score,\n                        'name': 'AUC-ROC',\n                        'thr' : False}\n\nmetrics_dict['auc_pr'] = {'fcn' : metrics.average_precision_score,\n                        'name': 'AUC-PR',\n                        'thr' : False}\n\nmetrics_dict['log_loss'] = {'fcn' : metrics.log_loss,\n                        'name': 'Log Loss',\n                        'thr' : False}\n\nmetrics_dict['prec'] = {'fcn' : metrics.precision_score,\n                        'name': 'Precision',\n                        'thr' : True}\n\nmetrics_dict['rec'] = {'fcn' : metrics.recall_score,\n                        'name': 'Recall',\n                        'thr' : True}\n\nmetrics_dict['f1'] = {'fcn' : metrics.f1_score,\n                        'name': 'F1 Score',\n                        'thr' : True}\n\nmetrics_dict['bal_acc'] = {'fcn' : metrics.balanced_accuracy_score,\n                        'name': 'Balanced Accuracy',\n                        'thr' : True}\n\nmetrics_dict['g_mean'] = {'fcn' : geometric_mean_score,\n                        'name': 'Geometric Mean',\n                        'thr' : True}\n\nmetrics_dict['kappa'] = {'fcn' : metrics.cohen_kappa_score,\n                        'name': 'Cohen\\'s Kappa',\n                        'thr' : True}","b4941a1f":"def compute_score(y_true, y_proba, metric, threshold=0.5):\n    \"\"\"Computes score given metric dict as above\n    (i.e. metric.keys() == ('fcn', 'name', 'thr'))\"\"\"\n    \n    y_proba_nonan = y_proba[~np.isnan(y_proba)]\n    y_true_nonan = y_true[~np.isnan(y_proba)]\n    \n    if metric['thr'] == True:\n        return metric['fcn'](y_true_nonan, (y_proba_nonan >= threshold))\n    elif metric['thr'] == False:\n        return metric['fcn'](y_true_nonan, y_proba_nonan)\n    else:\n        return np.NaN\n    # try\/except?","479b30bf":"from scipy.optimize import brentq","1c3a3045":"# Find the binary threshold which reproduces skew_train\n# for decisions array.\n# Empirically, a sample of 100,000 seems sufficient for \n# three figures of precision in threshold\n\nthresholds = []\n\ndef threshold_skewer(x, decision, sample=100000):\n    decisions_sample = np.random.choice(decisions[decision], sample)\n    return sum(decisions_sample < x) \/ (sum(decisions_sample >= x)) - skew_train\n\nfor k in range(len(decisions)):\n    thresholds.append(brentq(threshold_skewer, 0, 1, args=k))","1c0b69a4":"# Compute scores and concatenate with corresponding parameter values\n\nnames = []\nscores = []\n\nfor key, val in metrics_dict.items():\n    names.append(val['name'])    \n    for k in range(len(decisions)):\n        scores.append(compute_score(y['train'].values.ravel(),\n                                    decisions[k],\n                                    val,\n                                    thresholds[k]))\n\nscores = np.array(scores).reshape((len(metrics_dict.keys()),\n                                   len(decisions))).T\n\nscores_df = pd.concat([pd.DataFrame(data=np.array([rows, cols]).T,\n             columns=[param1, param2]), \n    pd.DataFrame(data=scores,\n             columns=names)],\n            axis=1).melt(id_vars=[param1, param2])\n\nscores_df.rename(columns={'variable': 'score'}, inplace=True)","d40f1999":"def draw_heatmap(index, columns, values, **kwargs):\n    data = kwargs.pop('data')\n    d = data.pivot(index=index,\n                   columns=columns,\n                   values=values)\n    sns.heatmap(d, **kwargs)\n\nfg = sns.FacetGrid(scores_df,\n                   col='score',\n                   col_wrap=3,\n                   height=4)\n\nfg.map_dataframe(draw_heatmap,\n                 index=param1, \n                 columns=param2,\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f')\n\nfg.set_axis_labels(param2, param1)\n\nplt.show()","d3712ed3":"# Construct long DataFrames from (samples of) p-r curve arrays\npr_dfs_list = []\ny_true = y['train'].values.ravel()\n\nfor counter, params in enumerate(ParameterGrid(param_grid)):\n    pr_dict = {}\n    y_proba = decisions[counter]\n    y_proba_nonan = y_proba[~np.isnan(y_proba)]\n    y_true_nonan = y_true[~np.isnan(y_proba)]\n    precision, recall, _ = metrics.precision_recall_curve(y_true_nonan, y_proba_nonan)\n    pr_dict[param1] = params[param1]\n    pr_dict[param2] = params[param2]\n    pr_dict['precision'] = precision[::1000]\n    pr_dict['recall'] = recall[::1000]\n    pr_dfs_list.append(pd.DataFrame(pr_dict))","793b76f6":"palette = sns.color_palette(\"Set2\", len(param_grid[param1]))\n\nplt.figure(figsize=(12,12))\nsns.lineplot(data=pd.concat(pr_dfs_list),\n             x='recall',\n             y='precision',\n             style=param2,\n             hue=param1,\n             palette=palette\n             )\nplt.title('Precision-Recall Curves')\nplt.show()","4b4ab576":"# Construct long DataFrames from (samples of) roc curve arrays\nroc_dfs_list = []\ny_true = y['train'].values.ravel()\n\nfor counter, params in enumerate(ParameterGrid(param_grid)):\n    roc_dict = {}\n    y_proba = decisions[counter]\n    y_proba_nonan = y_proba[~np.isnan(y_proba)]\n    y_true_nonan = y_true[~np.isnan(y_proba)]\n    fpr, tpr, _ = metrics.roc_curve(y_true_nonan, y_proba_nonan)\n    roc_dict[param1] = params[param1]\n    roc_dict[param2] = params[param2]\n    roc_dict['fpr'] = fpr[::1000]\n    roc_dict['tpr'] = tpr[::1000]\n    roc_dfs_list.append(pd.DataFrame(roc_dict))","00d2ba9a":"palette = sns.color_palette(\"Set2\", len(param_grid[param1]))\n\nplt.figure(figsize=(12,12))\nsns.lineplot(data=pd.concat(roc_dfs_list),\n             x='fpr',\n             y='tpr',\n             style=param2,\n             hue=param1,\n             palette=palette\n             )\nplt.title('Receiver Operating Characteristic Curves')\nplt.show()","d638aec5":"# Instacart: Random Forest ParameterGrid Search","06b2c95d":"Random forests have a convenient property which we exploited in\nhyperparameter tuning. Bootstrap samples are chosen so that [approximately 1\/3 of instances are left out](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating#Description_of_the_technique). Therefore, for any \n$\\gamma \\in \\Gamma_\\text{train}$, the observation\n$(x^\\gamma, y^\\gamma) \\in X_\\text{train} \\times y_\\text{train}$ is\nleft out of approximately 1\/3 of the samples.\n\nThe motivation for using OOB estimates instead of $N$-fold\ncross-validation is two-fold. First, this simplifies the phases of the\noverall project. Second, $N$-fold cross-validation with\n[`sklearn.model_selection.GridSearchCV`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)\nrequires scikit-learn to split and copy the dataset $N$ times, which,\nin addition to parallelization, causes a memory spike that the 16GB\nKaggle Kernel cannot handle, even using $N=3$, for a feature set of\nreasonable size.","ddf13277":"[A Stack Overflow post](https:\/\/datascience.stackexchange.com\/a\/30408)\nwas insightful in devising a strategy to calculate OOB estimates using\ndifferent metrics. To use OOB estimates with the `sklearn` interface, we\nconstruct the hyperparameter search somewhat manually with\n[`sklearn.model_selection.ParameterGrid`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ParameterGrid.html),\nwhich essentially creates a Python iterator from a dictionary of\nparameters, `param_grid`. Once we have trained a random forest\nclassifier on a combination of parameters in `param_grid`, we expose the\nprobabilistic OOB estimates with the\n[`RandomForestClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\nattribute `oob_decision_function_`. Then we can score all OOB estimates\nusing the metrics described in the next section.","fb4360f9":"We have a slight data imbalance:","25a274d5":"Not all of the metrics we compute below are necessarily relevant. In particular, Log\nLoss and Cohen's Kappa are included 1) out of curiosity and 2) because\nseven is prime; we should not pay much attention to these metrics except\nas an exploration into the behavior of the metrics themselves.","9b797604":"[This paper](https:\/\/www.pitt.edu\/~jeffcohn\/biblio\/Jeni_Metrics.pdf) offers some suggestions on which metrics are useful for scoring classifiers on imbalanced data. A skew of 10 may not necessarily mean that the data is imbalanced with respect to classifier choice; but we should make sure to use metrics whose scores will not be inflated by the ease of correctly classifying the negative class.","43e77b93":"Run ParameterGrid search:","8ec913a6":"`'max_features'` seems like fine-tuning. Just use default `'sqrt'` for now. Past `param_grid`s for quick reference:","a631e2ba":"Despite any warnings about missing OOB scores, we likely have enough trees for reliable estimates since there are very few missing scores, relatively speaking. NaNs are samples which were missed by RF bootstrap sampling:","b5c42f80":"[Heatmap FacetGrid Example](https:\/\/stackoverflow.com\/questions\/41471238\/how-to-make-heatmap-square-in-seaborn-facetgrid)\n\nThe first row of plots consists of rank metrics. We should pay particular attention to AUC-PR.","ed535154":"Display threshold scores for threshold which results in the same skew as $y_\\text{train}$."}}