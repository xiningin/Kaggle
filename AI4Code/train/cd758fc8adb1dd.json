{"cell_type":{"d188c6ff":"code","33aa0f45":"code","c3394761":"code","e0325fd5":"code","44333760":"code","0c78ad3d":"code","89ba2cc6":"code","859637b1":"code","a407870b":"code","886d726d":"code","4ca3cecb":"code","080e84c6":"code","fb2420b6":"code","ecc6e4aa":"code","acfee283":"code","b4d5ceab":"code","64cfe059":"code","e161a73d":"code","edce97b9":"code","93a9f7f1":"code","a719b3e9":"code","766e3bf7":"code","1f5c3eaa":"code","c8233032":"code","c6faee98":"code","98424109":"code","11639b61":"code","4666eb00":"code","d15f91a4":"code","bf4fa0a7":"code","03185bc5":"code","425b71b7":"code","a05c1515":"code","a8ed4655":"code","6552f3cd":"code","c11550a2":"code","5a995248":"code","7cd1875b":"code","559706e8":"code","bdc188a8":"code","6dc40857":"code","4b86086e":"code","f52e9b82":"code","507c329c":"markdown","04659df7":"markdown","0b97433c":"markdown","6d7b1036":"markdown","601c8fac":"markdown","2ddb4c53":"markdown","a8ef9ff4":"markdown","8b0b4e49":"markdown","4c4f6182":"markdown","9b14db8d":"markdown","ee6052b5":"markdown","c1626d28":"markdown","95d6b557":"markdown","f788e434":"markdown","3be1867c":"markdown","76ba5f23":"markdown","3c805a4f":"markdown","63aa7ed6":"markdown","37331a78":"markdown","f4f820e3":"markdown","c45b0257":"markdown","25770aab":"markdown","f2b8fb70":"markdown","49890b13":"markdown","6bdb3489":"markdown","6ce18d76":"markdown","d456fe8e":"markdown","28c4dd3c":"markdown","40b5a213":"markdown","62aada52":"markdown","7aa9d7e5":"markdown","42273199":"markdown","e7002a28":"markdown","93fd25d6":"markdown","11976dbc":"markdown","69b506dd":"markdown","f25952b7":"markdown"},"source":{"d188c6ff":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport tensorflow as tf\nfrom math import pi","33aa0f45":"train_data_path = '..\/input\/train.csv'\ntest_data_path = \"..\/input\/test.csv\"\n\ndf = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)\nfull_data = [df, test_df]\n","c3394761":"df.info()","e0325fd5":"df.head()","44333760":"df.describe()","0c78ad3d":"def find_nan_percentage(df):\n    total_row_count = df.shape[0]\n    non_nan_row_count =  df.dropna().shape[0]\n    nan_row_count = total_row_count - non_nan_row_count\n    \n    print(\"Total number of DATA rows\", total_row_count)\n    print(\"Total number of DATA rows that have NaN\/Null values: \", nan_row_count)\n    print(\"Total numver of DATA rows that doesn't have NaN\/Null values\", non_nan_row_count)\n    \n    #Draw a pie chart to represent the above values\n    plt.figure(figsize=(8,6))\n    labels = 'Fully filled Rows', 'NaN Rows'\n    sizes = [non_nan_row_count, nan_row_count]\n    colors = ['skyblue', 'yellowgreen']\n    explode = (0.1, 0)\n\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%', shadow=True, explode=explode)\n    return (nan_row_count\/total_row_count)*100\n","89ba2cc6":"nan_percentage = find_nan_percentage(df)\nprint(\"{0:.2f}% of the data have NaN values\".format(nan_percentage))","859637b1":"df.columns","a407870b":"print(\"Number of rows with NaN is: \", df.Pclass.isna().sum())\npd.crosstab(df.Pclass, df.Survived).plot(kind='bar', figsize = (20,10))\nplt.xlabel(\"Class\")\nplt.ylabel(\"Survival frequency\")\nprint (df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\n","886d726d":"print(\"Number of rows with NaN is: \", df.Sex.isna().sum())\npd.crosstab(df.Sex, df.Survived).plot(kind='bar', figsize = (20,10))\nprint (df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())","4ca3cecb":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())\npd.crosstab(df.FamilySize, df.Survived).plot(kind='bar', figsize=(10,10))","080e84c6":"print(\"Number of rows with NaN is: \", df.Name.isna().sum())\ndf.Name.head(5)","fb2420b6":"import re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)","ecc6e4aa":"pd.crosstab(df['Title'], df['Sex'])","acfee283":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint (df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","b4d5ceab":"print(\"Number of rows with NaN is: \", df.Embarked.isna().sum())","64cfe059":"df.Embarked.value_counts()","e161a73d":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","edce97b9":"pd.crosstab(df.Embarked, df.Survived).plot(kind='bar', figsize=(10,10))","93a9f7f1":"print(\"Number of rows with NaN is: \", df.Fare.isna().sum())\ndf.plot(kind='scatter', x='Fare', y='Survived')\n","a719b3e9":"for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(df['Fare'].median())\ndf['CategoricalFare'] = pd.qcut(df['Fare'], 4)\nprint (df[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","766e3bf7":"pd.crosstab(df.CategoricalFare,df.Survived).plot(kind='bar', figsize=(10,10))","1f5c3eaa":"print(\"Number of rows with NaN is: \", df.Age.isna().sum())","c8233032":"for dataset in full_data:\n    age_avg        = dataset['Age'].mean()\n    age_std        = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ndf['CategoricalAge'] = pd.cut(df['Age'], 5)\n\nprint (df[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\npd.crosstab(df.CategoricalAge, df.Survived).plot(kind='bar', figsize=(10,10))","c6faee98":"sns.violinplot(x=df.CategoricalAge, y=df.Survived)","98424109":"for dataset in full_data:\n    # Mapping Sex\n    name_map = {'female': 0, 'male': 1}\n    dataset['Sex'] = dataset['Sex'].map(name_map).astype(int)\n    \n    # Mapping titles\n    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_map)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n    #dataset['Embarked'] = dataset['Embarked'].map(embarked_map).astype(int)\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_map).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare']  = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4","11639b61":"df.head()","4666eb00":"PassengerId = test_df['PassengerId']\ncolumns_to_be_dropped = ['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin', ]\ndf = df.drop(columns_to_be_dropped, axis=1)\ndf = df.drop(['CategoricalAge', 'CategoricalFare'], axis=1)\ntest_df = test_df.drop(columns_to_be_dropped, axis=1)","d15f91a4":"df.head()","bf4fa0a7":"test_df.head()","03185bc5":"\ntrain_a = pd.get_dummies(df['Pclass'], prefix = \"Pclass\")\ntrain_b = pd.get_dummies(df['Fare'], prefix = \"Fare\")\ntrain_c = pd.get_dummies(df['Title'], prefix = \"Title\")\ntrain_frames = [df, train_a, train_b, train_c]\ndf = pd.concat(train_frames, axis = 1)\n\ntest_a = pd.get_dummies(test_df['Pclass'], prefix = \"Pclass\")\ntest_b = pd.get_dummies(test_df['Fare'], prefix = \"Fare\")\ntest_c = pd.get_dummies(test_df['Title'], prefix = \"Title\")\ntest_frames = [test_df, test_a, test_b, test_c]\ntest_df = pd.concat(test_frames, axis = 1)\n\nto_be_dropped = ['Pclass', 'Fare', 'Title']\ndf = df.drop(to_be_dropped, axis=1)\ntest_df = test_df.drop(to_be_dropped, axis=1)","425b71b7":"df.head()","a05c1515":"features = df.drop(\"Survived\", axis=1)\ntargets = df.Survived.values","a8ed4655":"from sklearn.model_selection import train_test_split\ntrain_features,test_features,train_targets,test_targets = train_test_split(features,targets,test_size = 0.20,random_state = 42)\n","6552f3cd":"# Imports\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n\n# Building the model\nmodel = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(train_features.shape[1],)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compiling the model\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['acc', 'mae'])\nmodel.summary()","c11550a2":"from keras.callbacks import ModelCheckpoint   \n\n# train the model\ncheckpointer = ModelCheckpoint(filepath='test.model.best.hdf5', \n                               verbose=1, save_best_only=True)\n\nhistory = model.fit(train_features, train_targets, validation_split=0.2, epochs=250, batch_size=32, verbose=0, callbacks=[checkpointer], shuffle=True)\n\n#Load the Model with the Best Classification Accuracy on the Validation Set\nmodel.load_weights('test.model.best.hdf5')","5a995248":"#print(vars(history))\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n","7cd1875b":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import accuracy_score,mean_squared_error\n\ny_pred = model.predict(test_df)\ny_pred = y_pred.flatten()\nprint(y_pred.shape)\ny_pred = np.where(y_pred<.5,0,1)\n\ntest_submission = \"..\/input\/gender_submission.csv\"\nsubmission_df = pd.read_csv(test_submission)\nsubmission_targets = submission_df['Survived'].values\nplt.plot(submission_targets)\nplt.plot(y_pred)\nplt.title('Prediction')\nprint(\"Accuracy: \", accuracy_score(submission_targets,y_pred))\n","559706e8":"from sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error","bdc188a8":"classifiers=[['Logistic Regression :',LogisticRegression()],\n       ['Decision Tree Classification :',DecisionTreeClassifier()],\n       ['Random Forest Classification :',RandomForestClassifier(n_estimators=30, max_features=7, max_depth=None, min_samples_split=2)],\n       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n       ['Ada Boosting Classification :',AdaBoostClassifier(n_estimators=100)],\n       ['Extra Tree Classification :', ExtraTreesClassifier()],\n       ['K-Neighbors Classification :',KNeighborsClassifier(n_neighbors=7)],\n       ['Support Vector Classification :',SVC()],\n       ['Gausian Naive Bayes :',GaussianNB()],\n       ['XGBoost Classification :', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, n_estimators=20, al_metric=[\"auc\", \"error\"])],]\ncla_pred=[]\nmax_accuracy = float('-inf')\nfor index, (name,model) in enumerate(classifiers):\n    model=model\n    model.fit(train_features,train_targets)\n    predictions = model.predict(test_features)\n    cla_pred.append(accuracy_score(test_targets,predictions))\n    accuracy_ = accuracy_score(test_targets,predictions)\n    if accuracy_ > max_accuracy:\n        max_index = index\n    print(name, accuracy_)","6dc40857":"y_ax=['Logistic Regression' ,\n      'Decision Tree Classifier',\n      'Random Forest Classifier',\n      'Gradient Boosting Classifier',\n      'Ada Boosting Classifier',\n      'Extra Tree Classifier' ,\n      'K-Neighbors Classifier',\n      'Support Vector Classifier',\n      'Gaussian Naive Bayes',\n      'XGBoost Classification']\nx_ax=cla_pred\nsns.barplot(x=x_ax,y=y_ax)\nplt.xlabel('Accuracy')","4b86086e":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import accuracy_score,mean_squared_error\n\ny_pred = classifiers[max_index][1].predict(test_df)\ny_pred = y_pred.flatten()\nprint(y_pred.shape)\ny_pred = np.where(y_pred<.5,0,1)\n\ntest_submission = \"..\/input\/gender_submission.csv\"\nsubmission_df = pd.read_csv(test_submission)\nsubmission_targets = submission_df['Survived'].values\nplt.plot(submission_targets)\nplt.plot(y_pred)\nplt.title('Prediction')\nprint(\"Accuracy: \", accuracy_score(submission_targets,y_pred))\n","f52e9b82":"# Generate Submission File \nSubmission = pd.DataFrame({'PassengerId': PassengerId,\n                            'Survived': y_pred })\nSubmission.to_csv(\"Submission.csv\", index=False)\n\n","507c329c":"#### - We coud generalize all the trivial titles to a common basket","04659df7":"### Inference\nThe higher the class, the higher the ratio of survival (1 being the highest class). It's quite clear","0b97433c":"## Analyzing NaN\/Null values <a class=\"anchor\" id=\"fourth-bullet\"><\/a>\n\n- Find out how many NaN\/Null values are present in the data set\n- Remove them if they are a trival % ","6d7b1036":"### Interference\nHigher the category, higher the survival rate!!!","601c8fac":"## Neural network architecture  <a class=\"anchor\" id=\"ninth-bullet\"><\/a>\nFinally we have prepared our data. Now it's time to train it with neural nets !!!","2ddb4c53":"### Inference\n\nPeople who started from Cherbourg survived more than people who started from anywehre else!!!","a8ef9ff4":"### Inference\nThe survival rate was certainly high for youngsters","8b0b4e49":"### 5. Embarked","4c4f6182":"### Inference\nQuite certain that females survived more than males","9b14db8d":"## Analyzing the columns  <a class=\"anchor\" id=\"fifth-bullet\"><\/a>","ee6052b5":"## Splitting into train and test features <a class=\"anchor\" id=\"eighth-bullet\"><\/a>","c1626d28":"#### - First, let us see how the names are listed","95d6b557":"## Data Cleaning  <a class=\"anchor\" id=\"sixth-bullet\"><\/a>","f788e434":"### Dropping the unnecessary columns","3be1867c":"### Inference\n\n- Around 80% of the rows aren't completely filled. So we can't omit all of them\n- We need to further dive into the data and analyze more","76ba5f23":"### 6. Fare","3c805a4f":"## One hot Encoding <a class=\"anchor\" id=\"seventh-bullet\"><\/a>","63aa7ed6":"# Table of contents\n- [Data Information](#first-bullet)\n- [Importing the packages needed](#second-bullet)\n- [Load and prepare the data](#third-bullet)\n- [Analyzing NaN\/Null values](#fourth-bullet)\n- [Analyzing the columns](#fifth-bullet)\n- [Data Cleaning](#sixth-bullet)\n- [One hot Encoding](#seventh-bullet)\n- [Splitting into train and test features](#eighth-bullet)\n- [Neural Network Architecture](#ninth-bullet)\n- [Evaluating the model](#tenth-bullet)\n- [Predicting the model](#eleventh-bullet)\n- [Other Classification techniques](#twelfth-bullet)\n- [Submission](#thirteenth-bullet)","37331a78":"### Inference\nThe scatter plot tells that fare doesn't make any influence on survival ","f4f820e3":"#### - We will get the title alone to see if they make any value","c45b0257":"### 1. Pclass","25770aab":"# Introduction\n\nThis notebook is built to analyze and identify the factors that helped people to survive the horrible shipwreck.\n\nWe also would use ML techniques to predict if a person would survive or not!!\n\nThis notebook is inspired from [Titanic best working Classifier](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\/data) and [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n\n ","f2b8fb70":"## Load and prepare the data <a class=\"anchor\" id=\"third-bullet\"><\/a>\n","49890b13":"### Inference\n- This plot was made to check if families were saved or survived more than induviduals. It doesn't look like that\n- We could assume that families of mid sizes survived considerably well when compared to the others","6bdb3489":"#### - Categorize the fare and check if it makes more sense","6ce18d76":"## Predicting the model <a class=\"anchor\" id=\"eleventh-bullet\"><\/a>","d456fe8e":"## Other Classification techniques <a class=\"anchor\" id=\"twelfth-bullet\"><\/a>","28c4dd3c":"### 7. Age","40b5a213":"- We have plenty of missing values in Age section\n- We generate random numbers between (mean - std) and (mean + std)\n- We then categorize the age in ranges","62aada52":"### 2.Sex","7aa9d7e5":"#### We would replace the NaN values with the most common Embarked place -- S","42273199":"## Importing the packages needed <a class=\"anchor\" id=\"second-bullet\"><\/a>","e7002a28":"### 4. Name","93fd25d6":"## Data Infromation <a class=\"anchor\" id=\"first-bullet\"><\/a>\n- Survival\t0 = No, 1 = Yes\n- pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n- sex\tMale\/Female\t\n- Age\tAge in years\t\n- sibsp\t# of siblings \/ spouses aboard the Titanic\t\n- parch\t# of parents \/ children aboard the Titanic\t\n- ticket\tTicket number\t\n- fare\tPassenger fare\t\n- cabin\tCabin number\t\n- embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n","11976dbc":"## Evaluating the model <a class=\"anchor\" id=\"tenth-bullet\"><\/a>","69b506dd":"### 3. Total Family","f25952b7":"# Submission <a class=\"anchor\" id=\"thirteenth-bullet\"><\/a>"}}