{"cell_type":{"e3a9fe82":"code","212e312f":"code","c907a07a":"code","21f475be":"code","8afa67e1":"code","fecc1ef3":"code","f77ebf4f":"code","d06e7e7a":"code","8ba40021":"code","44eecd3c":"code","8e7662f4":"code","32a41f0b":"code","c6671fac":"code","842ce19a":"code","2e6b87df":"code","772c629e":"code","0b6136f6":"code","5d401a04":"code","70c362a3":"code","49a9de3b":"code","95138560":"code","ea94f1c2":"code","1ab188ad":"code","7b82f0fe":"code","69e0441f":"code","8c3baef1":"code","511aa276":"code","641721d6":"code","aa1e18bc":"code","e6871a61":"code","4eb26e84":"code","4e9b1089":"code","1d24b74a":"code","ddca70cb":"code","fff7b869":"code","430fc5fd":"code","41ac5382":"code","a0cff625":"code","6d97ecf2":"markdown","ea5a0611":"markdown","3dd0e8d0":"markdown","2780265e":"markdown","f2d2ecd3":"markdown","49b3ba24":"markdown","4eeebf93":"markdown","ac3912a1":"markdown","a849b153":"markdown","0ef15f76":"markdown","5523b8d2":"markdown","658533c3":"markdown","41d813d4":"markdown","0b8ba162":"markdown","f5524802":"markdown","4df523e1":"markdown","d812ca14":"markdown","5089ad66":"markdown","49eeab45":"markdown","71ed37f3":"markdown","45ad3f30":"markdown","fd170e28":"markdown","55cdd50d":"markdown","881db833":"markdown","54f6b73a":"markdown","94da8bb9":"markdown","133d9edd":"markdown","7f688eeb":"markdown","c143baee":"markdown","4056ea3a":"markdown","23cabe6e":"markdown","cad6350e":"markdown","6da02434":"markdown","35c1ff3c":"markdown"},"source":{"e3a9fe82":"import warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.cluster import DBSCAN\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\nfrom scipy.stats import *\nfrom scipy.special import boxcox1p\nfrom scipy.optimize import minimize_scalar\nimport statsmodels.api as sm\nimport statsmodels.stats.outliers_influence as oi\n\n\nfrom lightgbm import LGBMRegressor\nimport scipy.stats as stats\n\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#Options\nrandom_state = 10\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\nwarnings.filterwarnings(\"ignore\")","212e312f":"def col_types(df, drop_feats = []):\n    data_describe = df.describe(include=\"all\")\n    if len(drop_feats):\n        cat_cols = [c for c in df.drop(drop_feats, axis=1).columns if df[c].dtype.name == 'object']\n        num_cols = [c for c in df.drop(drop_feats, axis=1).columns if df[c].dtype.name != 'object']\n    else:\n        cat_cols = [c for c in df.columns if df[c].dtype.name == 'object']\n        num_cols = [c for c in df.columns if df[c].dtype.name != 'object']\n        \n    bin_cols    = [c for c in cat_cols if data_describe[c]['unique'] == 2]\n    nonbin_cols = [c for c in cat_cols if data_describe[c]['unique'] > 2]\n    return cat_cols, num_cols, bin_cols, nonbin_cols","c907a07a":"path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n#path= \"\"   #local path\n\ntrain = pd.read_csv(path + 'train.csv', index_col='Id')\n\nX_test_s = pd.read_csv(path + \"test.csv\", index_col='Id')\ny_test_s = pd.read_csv(path + \"sample_submission.csv\", index_col='Id')\ntest = pd.merge(y_test_s, X_test_s, how='inner', left_index=True, right_index=True)\n\ntrain_idx = train.index\ntest_idx = test.index\n\ndata = pd.concat([train, test], axis=0, sort=False)","21f475be":"fig, ax = plt.subplots(figsize=(20,4), nrows=1, ncols=2)\nax[0].set_title(\"SalePrice\")\nax[1].set_title(\"QQ Plot\")\nsns.distplot(data.loc[train_idx, \"SalePrice\"], bins=40, ax=ax[0], norm_hist=True, kde=True).grid()\nfig = sm.qqplot(data.loc[train_idx, \"SalePrice\"], stats.t, fit=True, line='45', ax=ax[1])","8afa67e1":"data[\"SalePrice\"] = np.log(data[\"SalePrice\"])","fecc1ef3":"fig, ax = plt.subplots(figsize=(20,4), nrows=1, ncols=2)\nax[0].set_title(\"SalePrice\")\nax[1].set_title(\"QQ Plot\")\nsns.distplot(data.loc[train_idx, \"SalePrice\"], bins=40, ax=ax[0], norm_hist=True, kde=True).grid()\nfig = sm.qqplot(data.loc[train_idx, \"SalePrice\"], stats.t, fit=True, line='45', ax=ax[1])","f77ebf4f":"feats = [\"MSZoning\", \"Electrical\", \"Functional\", \"Utilities\", \"SaleType\", \"KitchenQual\", \"Exterior2nd\", \"Exterior1st\"]\nfor feat in feats:\n    data[feat].fillna(data[feat].mode()[0], inplace=True)","d06e7e7a":"feats = [\"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageCars\", \"GarageArea\", \"GarageQual\", \"GarageCond\", \n         \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \n         \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\", \"BsmtFullBath\", \"Fireplaces\", \"FireplaceQu\", \"MasVnrArea\", \"MasVnrType\",\n        \"Alley\", \"Fence\", \"MiscFeature\", \"MiscVal\", \"PoolQC\", \"PoolArea\", \"LotFrontage\"]\nfor feat in feats:\n    data[feat].fillna(0, inplace=True)","8ba40021":"data['LotFrontage'] = data.groupby(['MSSubClass', \"MSZoning\"])['LotFrontage'].transform(lambda x: x.fillna(x.mean()))","44eecd3c":"data[\"hasGarage\"] = 1\ndata.loc[data[\"GarageArea\"]==0, [\"hasGarage\"]]=0\ndata[\"hasGarage\"] = data[\"hasGarage\"].astype(object)\n\ndata[\"hasBsmt\"] = 1\ndata.loc[data[\"TotalBsmtSF\"]==0, [\"hasBsmt\"]]=0\ndata[\"hasBsmt\"] =  data[\"hasBsmt\"].astype(object)\n\ndata[\"hasPool\"] = 1\ndata.loc[data[\"PoolArea\"]==0, [\"hasPool\"]]=0\ndata[\"hasPool\"] = data[\"hasPool\"].astype(object)\n\ndata[\"has2Floor\"] = 0\ndata.loc[data[\"2ndFlrSF\"]>0, [\"has2Floor\"]]=1\ndata[\"has2Floor\"] = data[\"has2Floor\"].astype(object)","8e7662f4":"data['LivingAreaSF'] = data[\"GrLivArea\"] + data[\"TotalBsmtSF\"] + data[\"MasVnrArea\"] ","32a41f0b":"#drop outliars \ntrain_idx = train_idx.drop(train[train[\"GrLivArea\"]>4500].index)","c6671fac":"data[\"AgeOnSoldMoment\"] = data[\"YrSold\"]-data[\"YearBuilt\"]\ndata[\"RemodAgeOnSoldMoment\"] = data[\"YrSold\"]-data[\"YearRemodAdd\"]\ndata[\"GarageOnSoldMoment\"] = data[\"YrSold\"]-data[\"GarageYrBlt\"]\n\n\n#fix some negative values\ndata.loc[data[\"AgeOnSoldMoment\"]<0,\"AgeOnSoldMoment\"]=0\ndata.loc[data[\"RemodAgeOnSoldMoment\"]<0,\"RemodAgeOnSoldMoment\"]=0\ndata.loc[data[\"GarageOnSoldMoment\"]<0,\"GarageOnSoldMoment\"]=0\ndata.loc[data[\"GarageOnSoldMoment\"]>250,\"GarageOnSoldMoment\"]=0","842ce19a":"data[\"Baths\"] = data[\"HalfBath\"] + data[\"FullBath\"]  + data[\"BsmtFullBath\"] + data[\"BsmtHalfBath\"]","2e6b87df":"data[\"PorchSF\"] = data[\"OpenPorchSF\"]+data[\"ScreenPorch\"] +data[\"3SsnPorch\"] +data[\"EnclosedPorch\"]\n\ndata[\"hasPorch\"] = 0\ndata.loc[data[\"PorchSF\"]>0, [\"hasPorch\"]]=1\ndata[\"hasPorch\"] = data[\"hasPorch\"].astype(object)","772c629e":"data[\"hasWoodDeck\"] = 0\ndata.loc[data[\"WoodDeckSF\"]>0, [\"hasWoodDeck\"]]=1\ndata[\"hasWoodDeck\"] = data[\"hasWoodDeck\"].astype(object)","0b6136f6":"#data.drop([\"GrLivArea\", \"TotalBsmtSF\", \"MasVnrArea\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"OpenPorchSF\", \"ScreenPorch\",\"3SsnPorch\", \"EnclosedPorch\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"YearBuilt\", \"YearRemodAdd\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"HalfBath\", \"FullBath\",\"BsmtFullBath\", \"BsmtHalfBath\"], axis=1, inplace=True, errors='ignore')","5d401a04":"data[\"MSSubClass\"] = data[\"MSSubClass\"].astype(object)\ndata[\"MoSold\"] = data[\"MoSold\"].astype(object)\ndata[\"YrSold\"] = data[\"YrSold\"].astype(object)","70c362a3":"def optimize_skew(df, edge=0.5):\n    coeffs = {}\n    feat_skew = df.skew()\n    correction_list = feat_skew[abs(feat_skew)>edge].index.tolist()\n    for feat in correction_list:\n        coef = boxcox_normmax(df[feat] + 1)\n        coeffs[feat] = coef\n    return coeffs\n\ndef skew_corr_info(df, target):\n    skew_value = np.mean(np.abs(df.skew()))\n    corr_value = 0.0\n    df_corr = pd.merge(df, target, how='outer', left_index=True, right_index=True)\n    df_corr = df_corr.corr().iloc[-1,:][:-1]\n    corr_value = np.mean(np.abs(df_corr))\n    print(\"Corr: {0:.4}  |  Skew: {1:.4}\".format(corr_value, skew_value))\n    #return skew_value, corr_value\n    ","49a9de3b":"#My solution may seem a little unusual, but I tried several options and in final left some of the universal code to correct the skewness\n\ncat_cols, num_cols, bin_cols, nonbin_cols = col_types(data, drop_feats=['SalePrice'])\n\nskew_corr_info(data.loc[train_idx, num_cols], data.loc[train_idx, \"SalePrice\"])\ncoeffs = optimize_skew(data.loc[train_idx, num_cols], edge=0.5)\n\nfor key in coeffs:\n    data[key] = boxcox1p(data[key], coeffs[key])\n    \nskew_corr_info(data.loc[train_idx, num_cols], data.loc[train_idx, \"SalePrice\"])","95138560":"display(data.head(10))","ea94f1c2":"cat_cols, num_cols, bin_cols, nonbin_cols = col_types(data, drop_feats=['SalePrice'])\n\ndisplay(data[num_cols].head())\ndisplay(data[bin_cols].head())\ndisplay(data[nonbin_cols].head())","1ab188ad":"data_work = data.copy()\n\nscaler = RobustScaler()\n\ndata_X = data_work.drop(['SalePrice'], axis=1)\ndata_y = data_work['SalePrice']\n\nscaler.fit(data_X.loc[train_idx, num_cols].values)\n\ndata_X[num_cols] = scaler.transform(data_X[num_cols].values)\n\ndata_X = pd.get_dummies(data_X, columns=cat_cols) #, drop_first=True\n\n\nX = data_X.loc[train_idx, :]\ny = data_y.loc[train_idx]\n\ntest_X = data_X.loc[test_idx, :]\ntest_y = data_y.loc[test_idx]\n\ndisplay(X.head())\ndisplay(test_X.head())","7b82f0fe":"#Hyperopt functions\n\ndef rmsle_func(y_test, y_pred):\n    return np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred)))\n\nrmsle_scorer = make_scorer(rmsle_func)\n    \ndef hyperopt_run(X_, y_, params):\n    try:\n        Model = globals()[params.pop(\"model\")]\n        \n        if 'feature_mask' in params:\n            mask = np.array(params.pop('feature_mask'))\n            feats = X_.columns[mask]\n        else:\n            feats = X_.columns\n        \n        if 'sample_mask' in params:\n            idx = np.array(params.pop('sample_mask'))\n        else:\n            idx = X_.index\n        \n        model = Model(**params)\n        score = cross_val_score(model, X_.loc[idx, feats], y_.loc[idx], cv=4, scoring=rmsle_scorer)\n        return score.mean()\n    \n    except Exception as ex :\n        print(ex)\n        return np.inf\n\ndef f_model(params):\n    global best\n    global best_params\n    acc = hyperopt_run(X.copy(), y, params.copy())\n    if (acc < best):\n        best = acc\n        best_params = params\n        print(\"new best: {0:.4}\".format(best, params))\n    return {'loss': acc, 'status': STATUS_OK}","69e0441f":"space = {\n        'model': 'Lasso',\n        'alpha': hp.uniform('alpha', 0, 2.0)\n    }\n\nbest,  best_params = np.inf,None \nres = fmin(f_model, space, algo=tpe.suggest, max_evals=200, rstate=np.random.RandomState(random_state))\nprint(\"---------------------------\\nBest_params: \\n\", best_params)\n\nModel = globals()[best_params.pop(\"model\")]\nmp_alpha = best_params[\"alpha\"]\nlasso = Model(random_state=random_state, **best_params)\nlasso.fit(X,y)\nfeat_mask = lasso.coef_ != 0\n","8c3baef1":"fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\nlasso_ = Model(random_state=random_state, **best_params)\nlasso_.fit(X_train,y_train)\n\ny_pred = lasso_.predict(X_test)\nresid = y_test - y_pred\n\nax[0] = sns.scatterplot(x=y_pred, y=y_test, ax=ax[0], hue = X_test.loc[:, \"OverallQual\"])\nax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = X_test.loc[:, \"OverallQual\"])\nfig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])","511aa276":"model = sm.OLS(y, X.loc[:,feat_mask])\nfitted = model.fit()\nprint(fitted.summary())","641721d6":"influence = fitted.get_influence()\n(c, p) = influence.cooks_distance\ndistances = pd.DataFrame(c, index=train_idx)\n\n#fig, ax = plt.subplots(figsize=(24,8), ncols=1)\n#ax.set_yscale('log')\n#plt.stem(np.arange(len(c)), c, markerfmt=\",\")","aa1e18bc":"drop_idx = distances[distances[0]>0.01].index\nprint(drop_idx)\ntrain_idx = train_idx.drop(drop_idx)\n\nX, y = X.loc[train_idx,:], y.loc[train_idx]","e6871a61":"space = {\n        'model': 'Lasso',\n        'alpha': hp.uniform('alpha', 0, 0.001)\n    }\n\nbest,  best_params = np.inf,None \nres = fmin(f_model, space, algo=tpe.suggest, max_evals=20, rstate=np.random.RandomState(random_state))\n\nModel = globals()[best_params.pop(\"model\")]\nmp_alpha = best_params[\"alpha\"]\n\nprint(\"---------------------------\\nBest_params: \\n\", best_params)\n\n","4eb26e84":"lasso = Model(random_state=random_state, **best_params)\nlasso_fitted = lasso.fit(X,y)","4e9b1089":"fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\nlasso_ = Model(random_state=random_state, **best_params)\nlasso_.fit(X_train,y_train)\n\ny_pred = lasso_.predict(X_test)\nresid = y_test - y_pred\n\nax[0] = sns.scatterplot(x=y_pred, y=y_test, ax=ax[0], hue = X_test.loc[:, \"OverallQual\"])\nax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = X_test.loc[:, \"OverallQual\"])\nfig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])","1d24b74a":"#func\n\ndef model_tune(space, X, y, graph=True, random_state=random_state, iters=10):\n    print(space[\"model\"])\n    global best\n    global best_params\n    best, best_params = np.inf, None \n    res = fmin(f_model, space, algo=tpe.suggest, max_evals=iters, rstate=np.random.RandomState(random_state))\n    \n    Model = globals()[best_params.pop(\"model\")]\n    print(\"\\nBest_params: \\n\", best_params)\n    \n    model = Model(random_state=random_state, **best_params)\n    \n    if graph:\n        fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n        fig.suptitle(space[\"model\"])\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\n        model_fitted_ = model.fit(X_train,y_train)\n\n        y_pred = model_fitted_.predict(X_test)\n        resid = y_test - y_pred\n\n        ax[0] = sns.scatterplot(x=y_pred, y=y_test, ax=ax[0], hue = X_test.loc[:, \"OverallQual\"])\n        ax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = X_test.loc[:, \"OverallQual\"])\n        fig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])\n    print(\"--------------------------------------------------\")\n    return model\n","ddca70cb":"#spaces\n\nspace_xgbr = {\n        'model': 'XGBRegressor',\n\n        'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n        'max_depth':        hp.choice('max_depth',        np.arange(3, 12, 1, dtype=int)),\n        'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n        'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n        'subsample':        hp.uniform('subsample', 0.6, 1),\n        'n_estimators':     130,\n        'eval_metric': 'rmse',\n        'objective': 'reg:squarederror',\n        'nthread': 4,\n        'early_stopping_rounds': 10,\n    }\n\nspace_lgbmr = {\n        'model': 'LGBMRegressor',\n    \n        'max_depth':        hp.choice('max_depth',        np.arange(2, 12, 1, dtype=int)),\n        'min_data_in_leaf': hp.choice('min_data_in_leaf',        np.arange(5, 35, 5, dtype=int)),\n        'feature_fraction': hp.uniform('feature_fraction', 0.6, 0.9),\n        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n        'lambda': hp.uniform('lambda', 0, 1),\n        'task': 'train',\n        'learning_rate': hp.uniform('learning_rate', 0.00001, 0.2),\n        'metric': 'rmse',\n        'num_threads': 4,    \n    }\n\nspace_rf = {\n    'model': 'RandomForestRegressor',\n    'n_estimators': hp.choice('n_estimators', range(80,180)),\n    'max_depth':    hp.choice('max_depth', range(15,40)),\n    'max_features': hp.choice('max_features', range(20,60)),\n    'min_samples_split': hp.choice('min_samples_split', range(2,8)),\n    'min_samples_leaf': hp.choice('min_samples_leaf', range(1,10)),\n    'n_jobs': 4\n}\n\nspace_ada = {\n    'model': 'AdaBoostRegressor',\n    'base_estimator':    hp.choice('base_estimator', [DecisionTreeRegressor(max_depth=2), DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=5)]),\n    'n_estimators': hp.choice('n_estimators', range(20,180)),\n    'learning_rate': hp.uniform('learning_rate', 0.00001, 0.5),\n    'loss':    hp.choice('loss', [\"linear\",\"square\",\"exponential\"]),\n}\n\nspace_lasso = {\n        'model': 'Lasso',\n        'alpha': hp.uniform('alpha', 0.00001, 0.001)\n    }\n\nspace_ridge = {\n        'model': 'Ridge',\n        'alpha': hp.uniform('alpha', 8, 15)\n    }","fff7b869":"#model_tuning\nridge_model = model_tune(space_ridge, X, y, iters=100)  #ok\nlasso_model = model_tune(space_lasso, X, y, iters=50)   #ok\nxgbr_model = model_tune(space_xgbr, X, y, iters=30)     #ok\nlgbmr_model = model_tune(space_lgbmr, X, y, iters=30)   #ok\nrf_model = model_tune(space_rf, X, y, iters=30)         #ok","430fc5fd":"kf = KFold(n_splits=4,  shuffle=False, random_state=random_state)\n\nmodels = [ridge_model, lasso_model, xgbr_model, lgbmr_model, rf_model] \nweights = [4,5,1,1,0]  #[3,4,1,1,0]\n\ndef model_blending(models, weights, X):\n    weights = weights\/np.sum(weights)\n    result = []\n    for i in range(len(models)):\n        result.append(weights[i] * models[i].predict(X))\n    return(np.sum(result, axis=0))\n\nfinal_result = []\nfor train_index, test_index in kf.split(X):\n    blend_result = []\n    for model in models:\n        model.fit(X.iloc[train_index,:],y.iloc[train_index])\n        y_pred = model_blending(models, weights, X.iloc[test_index,:])    \n        blend_result.append(rmsle_func(y.iloc[test_index], y_pred))\n    final_result.append(np.mean(blend_result))\n\nprint(np.mean(final_result))","41ac5382":"#fit models\nfor model in models:\n    model.fit(X,y)","a0cff625":"#predict y test\ny_pred = model_blending(models, weights, test_X)\nres = pd.DataFrame(np.exp(y_pred), index = test_X.index, columns=['SalePrice'])\nres.to_csv('Submission_blend.csv')\nprint(\">>>subm file saved<<<\")","6d97ecf2":"<a class=\"anchor\" id=\"52\"><\/a>\n## Model fitting\n[on top](#0)","ea5a0611":"### hasFeature","3dd0e8d0":"<a class=\"anchor\" id=\"11\"><\/a>\n## Imports\n[on top](#0)","2780265e":"# A bit more models...","f2d2ecd3":"<a class=\"anchor\" id=\"42\"><\/a>\n## Missing values\n[on top](#0)","49b3ba24":"### With 0","4eeebf93":"### With mode","ac3912a1":"<a class=\"anchor\" id=\"45\"><\/a>\n## Skew correction\n[on top](#0)","a849b153":"<a class=\"anchor\" id=\"53\"><\/a>\n## Residual analysis\n[on top](#0)","0ef15f76":"<a class=\"anchor\" id=\"41\"><\/a>\n## Target variable \n[on top](#0)","5523b8d2":"### Porch","658533c3":"### Living Area","41d813d4":"<a class=\"anchor\" id=\"51\"><\/a>\n## Dataset\n[on top](#0)","0b8ba162":"<a class=\"anchor\" id=\"60\"><\/a>\n# Outliars... and model fitting #2\n\nTo increase the accuracy of the model, I want to remove the samples \u200b\u200bthat have the most influence - outliers. This will help me Cook's distance .\n\nTo do this, I fitted OLS regression on the features obtained in the previous step.\n\nThe threshold for removing outliers I selected experimentally, this is another possible way to improve the model. \n\n\n[on top](#0)","f5524802":"<a class=\"anchor\" id=\"62\"><\/a>\n## Model fitting\n[on top](#0)","4df523e1":"### Year group","d812ca14":"<a class=\"anchor\" id=\"40\"><\/a>\n# Data transformation\nIn this section I transformed the dataset using the knowledge gained from performing EDA\n\nInformation about the year (build, remod), I replaced the age at the time of sale. It seems to me that this approach made it possible to better predict the final price.\n\n[on top](#0)","5089ad66":"<a class=\"anchor\" id=\"50\"><\/a>\n# Modelling #1... and feature selection\nFirst approach.\n\nI chose a model with regularization - lasso regression. In this section, I will simply train the model and select the best alpha for adjusting the regularization strength. The coefficients of the model will allow us to select the best features that we use in the next section.\n\n\n[on top](#0)","49eeab45":"### Drop duplicate features","71ed37f3":"<a class=\"anchor\" id=\"12\"><\/a>\n## Useful functions\n[on top](#0)","45ad3f30":"<a class=\"anchor\" id=\"0\"><\/a>\n## TABLE OF CONTENTS\n\n1. [Preparing](#10)\n    1. [Imports](#11)\n    1. [Useful functions](#12)\n1. [Read the data](#20)\n1. [EDA](#30)\n1. [Data transformation](#40)\n    1. [Target variable](#41)\n    1. [Missing values](#42)\n    1. [New features](#43)\n    1. [Types](#44)\n    1. [Skew correction](#45)\n    1. [Result](#46)\n1. [Modelling #1... and feature selection](#50)    \n    1. [Dataset](#51)\n    1. [Model fitting](#52)\n    1. [Residual analysis](#53)\n1. [Outliars... and model fitting #2](#60)    \n    1. [Outliars](#61)\n    1. [Model fitting](#62)\n    1. [Residual analysis](#63)\n1. [Predictions](#70)","fd170e28":"<a class=\"anchor\" id=\"43\"><\/a>\n## New features\n[on top](#0)","55cdd50d":"### WoodDeck","881db833":"### With speciefic values","54f6b73a":"<a class=\"anchor\" id=\"63\"><\/a>\n## Residual analysis\n[on top](#0)","94da8bb9":"<a class=\"anchor\" id=\"20\"><\/a>\n# Read the data\n[on top](#0)","133d9edd":"<a class=\"anchor\" id=\"44\"><\/a>\n## Types\n[on top](#0)","7f688eeb":"<a class=\"anchor\" id=\"46\"><\/a>\n## Result\n[on top](#0)","c143baee":"<a class=\"anchor\" id=\"30\"><\/a>\n# EDA\nYou can find my exploratory data analysis (EDA) by clicking on [this link](https:\/\/www.kaggle.com\/zosimovaa\/house-prices-eda).\n\nI did not cite EDA here, so as not to amplify the volume of the notebook. Based on the conclusions of EDA, I built a training dataset.\n\n[on top](#0)","4056ea3a":"# Houses prices with model blending - TOP 7%","23cabe6e":"<a class=\"anchor\" id=\"10\"><\/a>\n# Preparing \n\nNothing special - just imports and function defining. [on top](#0)","cad6350e":"<a class=\"anchor\" id=\"61\"><\/a>\n## Outliars\n[on top](#0)","6da02434":"### Baths","35c1ff3c":"# Introducing\n\n### Hi everyone!\n\nIn the beginning I would like to say a few words about my solution.\n\nThe main idea of my solution is - get Lasso regression, tune it with hyperopt, select features and remove the outliers according to the Cook's distance value, fit Lasso regression again. This allowed me to enter the **TOP 8%** results. Not perfect, but not so bad :)\nAnd the last iteration - i trained several different models and blended them. As result - **TOP 6%** .\n\nIn each section you can find a brief description of what is happening in this block.\n\nI would be glad for your comments and criticism, that will be very helful for me!\n\nThanks for your time"}}