{"cell_type":{"2e7ba5f3":"code","0346ac83":"code","3c0d89de":"code","70d56f85":"code","8ce262bb":"code","3325ddfc":"code","8563357b":"code","8c14e409":"code","8f2cceec":"code","f2d81b01":"code","a9ec8a55":"code","55cd6443":"code","0f5fbdc7":"code","86d4b24f":"code","0cda114a":"code","374b42e4":"code","41a22e92":"code","7a658f71":"code","3261f876":"code","d05b5cea":"code","b9e54222":"code","e0153f5c":"code","80bf9e7f":"code","79ed6d93":"code","17598070":"code","859ec0d0":"code","d8555f99":"code","b0e22132":"code","b2b93296":"code","1eef374e":"code","b91355d9":"code","8381855a":"code","48bab0a6":"code","cdca0919":"code","1bb4a760":"code","480d99ae":"code","596bb76c":"code","19e67313":"code","295e601d":"code","ffb7c716":"code","e359a0f9":"code","aaeb7463":"code","ff1b18c9":"code","153f0c8d":"code","7d18648c":"code","f86d97fd":"code","8424ab98":"code","c270456d":"code","53d177d5":"code","f7da217f":"code","937bb730":"code","8b3c91ec":"code","0319805c":"code","0aabaa2a":"markdown","a0a2ca04":"markdown","65daf383":"markdown","a6afda16":"markdown","19bb3291":"markdown","7fd2463d":"markdown","2fed18da":"markdown","6b9f8d76":"markdown","979f20fa":"markdown","2cf7e33f":"markdown","be3cc10f":"markdown","e1abc04b":"markdown","0d1e9faa":"markdown","b13eaad3":"markdown","a9a8ce51":"markdown","056737ca":"markdown","44032478":"markdown","56a4ee55":"markdown","5e04dfcc":"markdown","3a535ab1":"markdown","29809c96":"markdown","6540b3d0":"markdown","8c8d0146":"markdown"},"source":{"2e7ba5f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom datetime import datetime, date\nfrom pylab import rcParams\nimport seaborn as sns\nimport haversine as hs\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0346ac83":"# load the data\nfraudtest = pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTest.csv')\nfraudtrain = pd.read_csv('\/kaggle\/input\/fraud-detection\/fraudTrain.csv')\n\n# drop the unnamed column\nfraudtest.drop('Unnamed: 0', axis=1, inplace=True)\nfraudtrain.drop('Unnamed: 0', axis=1, inplace=True)\n\n# concanate both \ndata = pd.concat([fraudtrain,fraudtest])","3c0d89de":"def dataset_overview(df):\n    display(df.head())\n    variables = df.shape[1]\n    observations = df.shape[0]\n    missings = df.isnull().sum().sum()\n    missings_per = round(100 * df.isnull().sum().sum()\/len(df),3)\n    duplicated = df.duplicated().sum()\n    duplicated_per = round(100 * df.duplicated().sum()\/len(df),3)\n    categ = len(df.select_dtypes(include=[np.number]).columns.values)\n    numer = len(df.select_dtypes(exclude=[np.number]).columns.values)\n    #names = ['Variables', 'Observations', 'Missings cells', 'Missing cells(%)','Duplicated rows','Duplicated rows(%)','Num cols','Categ cols']\n    stats = list([variables, observations, missings, missings_per,duplicated,duplicated_per, categ, numer])\n    over_df = pd.DataFrame({'General Overview':['Variables', 'Observations', 'Missings cells', 'Missing cells(%)','Duplicated rows','Duplicated rows(%)','Num cols','Categ cols'],\n                           '': stats})\n    over_df = over_df.set_index('General Overview')\n    print(over_df)\n        #################### categorical columns  ###############################################################\n    print('\\nVariable overview\\n')\n    print('Categorical variables')\n    for x in list(data.select_dtypes(exclude=[np.number]).columns.values):\n        print(x,'\\n')\n        print(f'unique values:         {len(df[x].unique())}')\n        print(f'Missing values:        {df[x].isnull().sum()}')\n        print(f'Missing values(%):     {df[x].isnull().sum()\/len(data[x])}%')\n        print(f'Mode:                  {df[x].mode()[0]}')\n        print(f'Frequency:             {df[x].value_counts()[0]}\\n')\n        print(f'Data type:             {df[x].dtype}')\n    \n    #################### numerical columns  ###############################################################\n    print('\\nNumerical variables\\n')\n    for y in list(data.select_dtypes(include=[np.number]).columns.values):\n        print(y,'\\n')\n        print(f'unique values:     {len(df[x].unique())}')\n        print(f'Missing values:    {df[y].isnull().sum()}')\n        print(f'Missing values(%): {df[y].isnull().sum()\/len(data[y])}%')\n        print(f'Minimum:           {df[y].min()}')\n        print(f'Median:            {df[y].median()}')\n        print(f'Mean:              {df[y].mean()}')\n        print(f'Max:               {df[y].max()}')\n        print(f'Data type:         {df[x].dtype}')","70d56f85":"dataset_overview(data)","8ce262bb":"# Function to calculate the distance between two adress\ndef haversine_vectorize(lon1, lat1, lon2, lat2):\n\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    newlon = lon2 - lon1\n    newlat = lat2 - lat1\n\n    haver_formula = np.sin(newlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon\/2.0)**2\n\n    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n    km = 6367 * dist #6367 for distance in KM for miles use 3958\n    return km","3325ddfc":"# clean date of DOB and trans_date_trans_time\ndates_list = ['trans_date_trans_time','dob']\nfor x in dates_list:\n    data[x] = pd.to_datetime(data[x])","8563357b":"# get hours from the transaction\ndata['trans_hour'] = data['trans_date_trans_time'].dt.hour\n# days when the transaction occured \ndata['day_of_week'] =data['trans_date_trans_time'].dt.day_name()\n# period when the transaction occured\ndata['year_month'] =data['trans_date_trans_time'].dt.to_period('M')\n# the age of the client when the transaction occured\ndata['age'] = (np.round((data['trans_date_trans_time'] - data['dob'])\/np.timedelta64(1,'Y')))\n# get the full name \ndata['names'] = data['first'] + ' ' + data['last']\ndata.drop(['first','last'], axis=1, inplace=True)\n# create the column where the if the population is less than 25% to be rural, 25-50% ssemi-urban, and more than 50% urban\ndata['residence'] = pd.qcut(data.city_pop, q=[0, .25, .75, 1], labels=['rural', 'semi_urban', 'urban'])\n\n# concanate the lat and longitude of client into one column and the same for the merchant location\ndata['lat_long'] = tuple(zip(*data[['lat','long']].values.T))\ndata['merch_ad'] = tuple(zip(*data[['merch_lat','merch_long']].values.T))","8c14e409":"# create the distance column\ndata['distance'] = haversine_vectorize(data['long'],data['lat'],data['merch_long'],data['merch_lat'])","8f2cceec":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 30]] # For displaying purposes, pick columns that have between 1 and 30 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","f2d81b01":"sns.set(style='whitegrid')\nplotDistribution(data, 10, 3)","a9ec8a55":"data[(data['is_fraud']==1)].category.value_counts(normalize= True, ascending= False).plot(kind='bar');","55cd6443":"data[(data['is_fraud']==0)].category.value_counts(normalize= True, ascending= False).plot(kind='bar');","0f5fbdc7":"\nfig, ax = plt.subplots(1,3,figsize=(20,5))\nax[0].hist(data[data['amt']<=1000]['amt'], bins=50)\nax[1].hist(data[(data['is_fraud']==0) & (data['amt']<=1000)]['amt'], bins=50)\nax[2].hist(data[(data['is_fraud']==1) & (data['amt']<=1000)]['amt'], bins=50)\n\nax[0].set_title('Overall Amount Distribution')\nax[1].set_title('Non Fraud Amount Distribution')\nax[2].set_title('Fraud Amount Distribution')\n\nax[0].set_xlabel('Transaction Amount')\nax[0].set_ylabel('number of Transactions')\n\nax[1].set_xlabel('Transaction Amount')\nax[2].set_xlabel('Transaction Amount')\nplt.show()","86d4b24f":"bins = np.linspace(200, 2000, 100)\nplt.hist(data[(data['is_fraud']==0)]['amt'], bins,alpha=1, density=True, label='Non Fraud' )\nplt.hist(data[(data['is_fraud']==1)]['amt'], bins,alpha=1, density=True, label='Fraud')\n\nplt.title('Amount by percentage of transactions')\n\nplt.xlabel('Transaction Amount')\nplt.ylabel('Percentage of Transactions')\nplt.show()","0cda114a":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(data['distance'], ax= ax[0])#age distributio\nsns.distplot(data[(data['is_fraud']==0)].distance, ax= ax[1]) # age distribution for fraudulent transaction\nsns.distplot(data[(data['is_fraud']==1)].distance, ax= ax[2]) # age distribution for non fraudulent transaction \n\nax[0].set_title('Overall transaction vs distance Distribution')\nax[1].set_title('Non Fraud transaction vs distance Distribution')\nax[2].set_title('Fraud transaction vs distance Distribution')\n\nplt.show()","374b42e4":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(data['unix_time'], ax= ax[0])#age distributio\nsns.distplot(data[(data['is_fraud']==0)].unix_time, ax= ax[1]) # age distribution for fraudulent transaction\nsns.distplot(data[(data['is_fraud']==1)].unix_time, ax= ax[2])\nax[0].set_title('Overall transaction vs time between another transaction Distribution')\nax[1].set_title('Non Fraud transaction vs time between another transaction Distribution')\nax[2].set_title('Fraud transaction vs time between another transaction Distribution')\n\nplt.show()","41a22e92":"fraud = data[(data['is_fraud']==1)] # fraud df\nnot_fraud = data[(data['is_fraud']==0)] # non fraud dataframe\n\nax = fraud.groupby(fraud['year_month'])['trans_num'].nunique().reset_index().set_index('year_month').plot.bar(figsize=(20,10))\nax1 = not_fraud.groupby(not_fraud['year_month'])['trans_num'].nunique().reset_index().sort_values(by=['trans_num']).plot.bar(figsize=(20,10))\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(fraud)*100),\n            ha=\"center\", fontsize=12)\n    \nfor t in ax1.patches:\n    height = t.get_height()\n    ax1.text(t.get_x()+t.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(not_fraud)*100),\n            ha=\"center\", fontsize=12) \nax.set_title('fraudulent transaction by period')\nax1.set_title('overall transaction by period')\nplt.show()","7a658f71":"ax = data.groupby(data['day_of_week'])['trans_num'].nunique().reset_index().set_index('day_of_week').plot.bar(figsize=(20,10))\nax.set_ylabel('# of all the transactions')\nax.set_xlabel('days of a week')\nax.set_title('Week days vs all transaction')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(data)*100),\n            ha=\"center\", fontsize=15)\nplt.show()","3261f876":"ax = fraud.groupby(fraud['day_of_week'])['trans_num'].nunique().reset_index().set_index('day_of_week').plot.bar(figsize=(20,10))\nax.set_ylabel('# of fraudulent transactions')\nax.set_xlabel('days of a week')\nax.set_title('Week days vs fraudulent transaction')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(fraud)*100),\n            ha=\"center\", fontsize=15)\nplt.show()","d05b5cea":"ax = fraud.groupby(fraud['residence'])['trans_num'].nunique().reset_index().set_index('residence').plot.bar(figsize=(20,10))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(fraud)*100),\n            ha=\"center\", fontsize=15)\nax.set_title('residence vs transaction numbers')\nplt.show()","b9e54222":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(), annot=True,cmap=\"YlGnBu\")\nplt.show()","e0153f5c":"import plotly.express as px \n\ndf2_fraud = data[data['is_fraud'] == 1]\n\nfig = px.scatter_mapbox(df2_fraud, lat=\"lat\", lon=\"long\", hover_name=\"city\",\n                         zoom=3, height=500,\n                         color=\"is_fraud\",  color_discrete_sequence=px.colors.cyclical.IceFire)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","80bf9e7f":"dfm_fraud = data[data['is_fraud'] == 1]\n\nfig = px.scatter_mapbox(dfm_fraud, lat=\"merch_lat\", lon=\"merch_long\", hover_name=\"city\",\n                         zoom=3, height=500,\n                         color=\"is_fraud\",  color_discrete_sequence=px.colors.cyclical.IceFire)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","79ed6d93":"dframe = data.copy()\ndframe = dframe.drop(['cc_num','trans_date_trans_time','names', 'merchant','trans_num','street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop','dob', 'merch_lat', 'merch_long','lat_long',\n       'merch_ad','job','year_month'],axis=1)","17598070":"# creating a dummy variable for ome of the categoorical variables and drop the first ones\ndummy_var1 = pd.get_dummies(dframe[['category', 'day_of_week', 'gender', 'residence']], drop_first= True)\n# adding the resultss to the master dataframe\ndframe = pd.concat([dframe, dummy_var1], axis=1)\n#dropping the repeated variables\ndframe = dframe.drop(['category', 'day_of_week', 'gender', 'residence'],1)","859ec0d0":"from sklearn.preprocessing import RobustScaler\n# select columns to scale \nto_scale = [col for col in dframe.columns if dframe[col].max()>1]\nscaler = RobustScaler()\nscaled =scaler.fit_transform(dframe[to_scale])\nscaled = pd.DataFrame(scaled, columns=to_scale)\n\n# replace original columns with scaled columns\nfor col in scaled:\n    dframe[col] = scaled[col]","d8555f99":"#make a copy of this dataframe\ndf = dframe.copy()","b0e22132":"# import libraries needed for this step\nfrom sklearn.model_selection import train_test_split # train-test split\nfrom sklearn.metrics import confusion_matrix, classification_report,roc_auc_score,roc_curve # classification metrics\nfrom imblearn.over_sampling import SMOTE # SMOTE\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler # scaling methods\n\nimport sklearn.neighbors\nfrom sklearn.model_selection import GridSearchCV # grid search cross validation\nfrom sklearn.model_selection import RandomizedSearchCV # randomized search cross validation\n\n# supervised learning algorithms\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbbors\nfrom sklearn.naive_bayes import GaussianNB # Gaussain Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier # Adaptive Boosting Classifier\nfrom sklearn.ensemble import BaggingClassifier # Bootstrap Aggregating Classifier\nfrom xgboost import XGBClassifier\nimport statsmodels.api as sm # estimates statistical models\nfrom sklearn.feature_selection import RFE #Recursive Feature Elimination for feature selection\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","b2b93296":"X = df.drop(['is_fraud'],axis=1) \ny = df['is_fraud'] #target variable","1eef374e":"#split the dataset into training set and testing set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=42)","b91355d9":"smote = SMOTE()\nX_train_new, y_train_new = smote.fit_resample(X_train, y_train.ravel())\n\n\n# to demonstrate the effect of SMOTE over imbalanced datasets\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize =(15, 5))\nax1.set_title('Before SMOTE')\npd.Series(y_train).value_counts().plot.pie(autopct='%.1f%%',ax=ax1)\nax1.yaxis.set_major_formatter(mtick.PercentFormatter())\nax2.set_title('After SMOTE')  \npd.Series(y_train_new).value_counts().plot.pie(autopct='%.1f%%',ax=ax2)\nax2.yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()","8381855a":"# let's see the correlation matrix\nplt.figure(figsize= (30, 10))\nsns.heatmap(df.corr(), annot=True, cmap= 'GnBu')\nplt.show()","48bab0a6":"# logistic regression model\nlogml = sm.GLM(y_train_new,(sm.add_constant(X_train_new)), family=sm.families.Binomial())\nlogml.fit().summary()","cdca0919":"# look for best describiing features for the logistic regression model using rfe\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X_train_new, y_train_new)","1bb4a760":"logreg.fit(X_train_new, y_train_new)\ny_test_pred = logreg.predict(X_test)","480d99ae":"# have a look on the list of features and the ranking they gets \nlist(zip(X_train_new.columns, rfe.support_, rfe.ranking_))","596bb76c":"# extract all the columns selected by rfe as best variable for our model\ncol = X_train_new.columns[rfe.support_]","19e67313":"#remove those with less important features\nX_train_new.columns[~rfe.support_]","295e601d":"feature_imp = pd.DataFrame(list(zip(X_train_new.columns, rfe.support_, rfe.ranking_)), columns=['variables','selected','rank'])\nfeature_imp = feature_imp.set_index('variables')\n\nplt.figure(figsize=[15,6])\nplt.title('Features Importance')\nsns.barplot(x='rank', y=feature_imp.index.values, data=feature_imp, \n            order=feature_imp.sort_values(by='rank', ascending=False).index.values, palette='rocket', )\nplt.xlabel('Feature Name');","ffb7c716":"# build a logistic regression using only the variables selected using rfe\nX_train_sm = sm.add_constant(X_train_new[col])\n\nlogml2 = sm.GLM(y_train_new, X_train_sm, family=sm.families.Binomial())\n\nres = logml2.fit()\n\nres.summary()","e359a0f9":"## getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred =y_train_pred.values.reshape(-1)","aaeb7463":"# put in the dataframe the probability calculated for fraud\ny_train_pred_final = pd.DataFrame({'fraud':y_train_new, 'fraud_Prob': y_train_pred})\ny_train_pred_final.head()","ff1b18c9":"# creating new column 'predicted' with 1 if churn_prob > 0.5 else 0\n# https:\/\/smallbiztrends.com\/2019\/12\/payment-fraud-statistics.html\ny_train_pred_final['predicted'] = y_train_pred_final.fraud_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.sample(50)","153f0c8d":"print(confusion_matrix(y_train_pred_final.fraud, y_train_pred_final.predicted))\nprint(classification_report(y_train_pred_final.fraud, y_train_pred_final.predicted))","7d18648c":"print(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))","f86d97fd":"#Building Decision Tree Model\nfrom sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier(criterion = 'gini', max_depth = 20, random_state=0)\ndt_clf.fit(X_train_new, y_train_new)\npred_train = dt_clf.predict(X_train_new)\n\nprint(confusion_matrix(y_train_new, pred_train))\nprint(classification_report(y_train_new, pred_train))","8424ab98":"pred_test = dt_clf.predict(X_test)\n\nprint(confusion_matrix(y_test, pred_test))\nprint(classification_report(y_test, pred_test))","c270456d":"#Building Random Forest Model\nrf_clf = RandomForestClassifier(n_estimators = 50,max_depth = 20,verbose = 1)\nrf_clf.fit(X_train_new, y_train_new)\npred_train = rf_clf.predict(X_train_new)\n\nprint(confusion_matrix(y_train_new, pred_train))\nprint(classification_report(y_train_new, pred_train))","53d177d5":"pred_test = rf_clf.predict(X_test)\n\nprint(confusion_matrix(y_test, pred_test))\nprint(classification_report(y_test, pred_test))","f7da217f":"#Building XG Boost Model\n\nxbt_model = XGBClassifier(n_estimators = 100, learning_rate = 0.1, max_depth = 3, verbose = 1)\nxbt_model.fit(X_train_new, y_train_new)\n\npred_train = xbt_model.predict(X_train_new)\nprint(confusion_matrix(y_train_new, pred_train))\nprint(classification_report(y_train_new, pred_train))","937bb730":"pred_test = xbt_model.predict(X_test)\nprint(confusion_matrix(y_test, pred_test))\nprint(classification_report(y_test, pred_test))","8b3c91ec":"data['ques'] = 1","0319805c":"print(f\"Average number of transactions per month: {round(data.groupby(['year_month','ques'])['ques'].sum().mean(),2)}\")\nprint(f\"Average number of fraudulent transaction per month: {round(fraud.groupby(['year_month','is_fraud'])['is_fraud'].count().mean(),2)}\")\nprint(f\"Average amount per fraud transaction: {round(fraud.amt.sum()\/len(fraud),2)}\")","0aabaa2a":"## resampling using SMOTE","a0a2ca04":"## Bivariate Analysis\u00b6","65daf383":"From the above we can observe that we have 22 columns and 1852394 observations. \n\nNo missing variables, no duplicates, having some data types that doesn't correspond to the variables for example on date columns (trans_date_trans_time and dob).\n\nThe next step, will be to change those data types, and create some columns which can help us in our analysis","a6afda16":"# Capstone Project: Credit card fraud detection using ML\n# Understanding data","19bb3291":"# Exploratory Data Analysis (EDA)\n## Univariate Analysis","7fd2463d":"# Data Cleansing","2fed18da":"- Gas and transport is where there was the highest number of transaction,  and travel the lowest\n- Female made fewer transaction than male\n- we can see that there are an imbalance between this dataset between the fraudulent and non-fraudulent transaction\n- we can see that more transaction are made in the night\n- during the week mre transaction are made monday and sunday\n- we can see that at the end of year there more transaction made\n- The last but not least we can see that our columns are all right skewed","6b9f8d76":"We can see that there arent any variable with a high correlation\n## Logistic Regression","979f20fa":"From the code above we can observe that from the age variable is really important with the rank have over 12, and frim categorykids pets up to category misc pos have a really low rank","2cf7e33f":"- From the above figures we observed that the POS and ne t purchases have the high fraudulent compared o others \n- There are not a big difference on the distribution of the fraudulent transaction and non fraudulent transacation\n- From period transaction, there were more fraudulent transaction in 2019\n- In a week, there were  more fraudulent transaction on monday\n- In the comparison, using the area, there were more fraudulent transaction in the semi-urban than others","be3cc10f":"# Business Impact","e1abc04b":"## Test Train Split","0d1e9faa":"## Decision Tree","b13eaad3":"## Multivariate analysis","a9a8ce51":"# Data Preparation","056737ca":"What we can observe from the above plot is that more fraudulent transaction occurs in the east of the united states, where there were more merchant as well","44032478":"## Random-Forest Classifier","56a4ee55":"## Cost Benefit Analysis","5e04dfcc":"# Model Building\n__Attention:__ <\/br>Here,instead of Accuracy we are very much interested on the recall score, because that is the metric that will help us try to capture the most fraudulent transactions.","3a535ab1":"### Feature selection using RFE","29809c96":"The above all p values are statistically significant <\/br>\nThe next step, we are going to check which variables that has high importance for this model than others","6540b3d0":"as we can see here there are a high correlation between location (longititude and latitude of the clients and merchant location)","8c8d0146":"### Gradient Boosting Model"}}