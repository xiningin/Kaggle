{"cell_type":{"a55fa3a6":"code","b312f26a":"code","c3ef8b1a":"code","cd817088":"code","5d6ed62b":"code","f122770f":"code","c7c28b6d":"code","6ac6eeac":"code","0acd8f8a":"code","e70df824":"code","7ac7d84a":"code","960967eb":"code","63002086":"code","d50cb117":"code","119b0f62":"code","5f9ec119":"code","9e4be966":"code","31f33bc9":"code","03371af3":"code","fa1f547b":"code","d7b2f38c":"code","4202c24d":"code","e5a98d01":"code","70c99e2d":"code","e4881601":"code","8daa70d4":"code","fdd26c02":"code","b088da19":"code","fe742d52":"markdown","0ad05ab1":"markdown","3162c5bf":"markdown","34e9edee":"markdown","dcf9e599":"markdown"},"source":{"a55fa3a6":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nimport random","b312f26a":"random.seed(100)","c3ef8b1a":"train_data = pd.read_csv('..\/input\/lishmy\/train_data_exc.csv') \ntest_data = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')","cd817088":"train_data.head()","5d6ed62b":"top_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\nprint(f'length of top features: {len(top_feats)}')\nprint('top features df: ')\ntrain_data.drop('sig_id',axis=1).iloc[:,top_feats].head()","f122770f":"genes = [col for col in train_data if col[0:2]=='g-']\ncells = [col for col in train_data if col[0:2]=='c-']","c7c28b6d":"temp = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\nfeatures = temp.drop(['sig_id'],axis=1).columns\ntemp = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntargets = temp.drop('sig_id',axis=1).columns\n\ndel temp","6ac6eeac":"test_data.head(3)","0acd8f8a":"train_data.head(3)","e70df824":"from sklearn.preprocessing import StandardScaler","7ac7d84a":"# Class to prepare data for training.\nclass MoaDataPrepare:\n    def __init__(self,train,test):\n        self.train_data = train.copy()\n        self.test_data = test.copy()\n        \n        self.train_data[\"cp_time\"]= self.train_data[\"cp_time\"].map({24:'24hr',48:'48hr',72:'72hr'})\n        self.train_data[\"cp_dose\"]= self.train_data[\"cp_dose\"].map({'D1':0,'D2':1})\n        \n      \n        self.test_data[\"cp_time\"]= self.test_data[\"cp_time\"].map({24:'24hr',48:'48hr',72:'72hr'})\n        self.test_data[\"cp_dose\"]= self.test_data[\"cp_dose\"].map({'D1':0,'D2':1})\n        \n        \"\"\"self.train_targets = self.train_data.iloc[:,876:]\n        self.train_data = self.train_data.drop('sig_id',axis=1).iloc[:,top_feats]\n        self.test_data = self.test_data.drop('sig_id',axis=1).iloc[:,top_feats]\"\"\"\n        \n        genes = [col for col in self.train_data if col[0:2]=='g-']\n        cells = [col for col in self.train_data if col[0:2]=='c-']\n        \n        #gene_data = self.train_data.loc[:,genes]\n        cell_data = self.train_data.loc[:,cells]\n        #gene_data_test = self.test_data.loc[:,genes]\n        cell_data_test = self.test_data.loc[:,cells]\n        \n        pca=PCA(0.98)\n        tran = pca.fit_transform(cell_data,self.train_data.loc[:,targets])\n        conv_cell_data = pd.DataFrame(tran,columns=['pca-'+str(i) for i in range(tran.shape[1])])\n        tran = pca.transform(cell_data_test)\n        conv_cell_test = pd.DataFrame(tran,columns=['pca-'+str(i) for i in range(tran.shape[1])])\n        \n        self.train_data = self.train_prepare()\n        self.test_data = self.test_prepare()\n        \n        self.train_data = pd.concat([self.train_data.iloc[:,:3+len(genes)],conv_cell_data,self.train_data.iloc[:,3+len(genes)+conv_cell_data.shape[1]:]],axis=1)\n        self.test_data = pd.concat([self.test_data.iloc[:,:3+len(genes)],conv_cell_test],axis=1)\n        \n        sc = StandardScaler()\n        self.train_data.loc[:,genes] = sc.fit_transform(self.train_data.loc[:,genes])\n        self.test_data.loc[:,genes] = sc.transform(self.test_data.loc[:,genes])\n        \n        self.features = self.train_data.columns[:3+len(genes)+conv_cell_data.shape[1]]\n        self.targets = targets\n        \n    def train_prepare(self):\n        self.train_data = self.train_data.drop(['sig_id','cp_type'],axis=1)\n        cp_time = pd.get_dummies(self.train_data.cp_time,drop_first = True)\n    \n        self.train_data = pd.concat([cp_time,self.train_data.drop(['cp_time'],axis=1)],axis=1)\n        \n        return self.train_data\n    \n    def test_prepare(self):\n        self.test_data = self.test_data.drop(['sig_id','cp_type'],axis=1)\n        cp_time = pd.get_dummies(self.test_data.cp_time,drop_first = True)\n        \n    \n        self.test_data = pd.concat([cp_time,self.test_data.drop(['cp_time'],axis=1)],axis=1)\n        return self.test_data\n    \n    \n    def getFold(self,fold):\n        X_train = self.train_data[self.train_data.kfold!=fold].loc[:,self.get_test().columns]\n        X_val = self.train_data[self.train_data.kfold==fold].loc[:,self.get_test().columns]\n        \n        Y_train = self.train_data[self.train_data.kfold!=fold].loc[:,self.targets]\n        Y_val = self.train_data[self.train_data.kfold==fold].loc[:,self.targets]\n        \n        return X_train,X_val,Y_train,Y_val\n    \n    def get_train(self):\n        return self.train_data.loc[:,self.get_test().columns]\n    \n    def get_test(self):\n        return self.test_data\n    \n    def __del__(self):\n        del self.train_data\n        del self.test_data","960967eb":"# Class for building the model and implementing the score metric\nclass ModelBuilder:\n    def __init__(self,random_state=666):\n        self.random_state=random_state\n        \n    def SimpleNeuralNet(self,shape=None,learning_rate=0.001,output_shape=206):\n        \"\"\"shape=None, lr=0.001\"\"\"\n        model = tf.keras.models.Sequential([\n                L.InputLayer(input_shape=shape),\n                L.BatchNormalization(),\n                L.Dropout(0.5),\n                tfa.layers.WeightNormalization(L.Dense(256,kernel_initializer=\"he_normal\")),\n                L.BatchNormalization(),\n                L.Activation(tf.nn.leaky_relu),\n                L.Dropout(0.5),\n                tfa.layers.WeightNormalization(L.Dense(128,kernel_initializer=\"he_normal\")),\n                L.BatchNormalization(),\n                L.Activation(tf.nn.leaky_relu),\n                L.Dropout(0.3),\n                tfa.layers.WeightNormalization(L.Dense(output_shape,activation=\"sigmoid\",kernel_initializer=\"he_normal\"))\n            ])\n        \n        model.compile(optimizer= tfa.optimizers.AdamW(lr=learning_rate,weight_decay=1e-5, clipvalue=900),loss=\"binary_crossentropy\",metrics=[\"binary_crossentropy\"])\n        return model\n    \n    def transfer_weight(self,model_source,model_dest):\n        for i in range(len(model_source.layers[:-1])):\n            model_dest.layers[i].set_weights(model_source.layers[i].get_weights())\n        return model_dest\n    \n    def metric(self,train,predict):\n        metrics=[]\n        for col in range(train.shape[1]):\n            metrics.append(log_loss(train.iloc[:,col],predict[:,col],labels=[0,1]))\n        return np.mean(metrics)","63002086":"data_prepare = MoaDataPrepare(train_data,test_data)","d50cb117":"data_prepare.get_train()","119b0f62":"model_builder = ModelBuilder(random_state=606)","5f9ec119":"transfer_data = data_prepare.get_train()\ntransfer_data.loc[:,'sig_id'] = train_data.sig_id\ntransfer_data = transfer_data.merge(train_targets_nonscored,how='inner',on='sig_id')\ntransfer_data = transfer_data.drop('sig_id',axis=1)\nY_transfer = transfer_data.loc[:,train_targets_nonscored.columns[1:]]\ntransfer_data = transfer_data.loc[:,data_prepare.get_train().columns]","9e4be966":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(transfer_data,Y_transfer,test_size=0.2,random_state = 101)","31f33bc9":"model = model_builder.SimpleNeuralNet(shape = (x_train.shape[1],),output_shape = 402)","03371af3":"save_weight = tf.keras.callbacks.ModelCheckpoint('model.learned.hdf5',save_best_only=True,save_weights_only=True,monitor = 'val_loss',mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\nearly = EarlyStopping(monitor='val_loss',patience=5,mode='min')","fa1f547b":"model.fit(x_train,y_train,\n          epochs=30,\n          batch_size=128,\n          validation_data=(x_test,y_test),\n          callbacks=[early,save_weight,reduce_lr_loss]\n         )","d7b2f38c":"model.load_weights('model.learned.hdf5')","4202c24d":"ss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss.loc[:,'sig_id'] = test_data['sig_id'].values\nss.iloc[:,1:]=0","e5a98d01":"histories=[]\nscores = []\n\n    \nrandom.seed(100)\nfor fold in range(5):\n\n    x_train,x_val,y_train,y_val = data_prepare.getFold(fold)\n    \n    print(f'Fold: {fold}\\n')\n\n    tf.keras.backend.clear_session()\n    print('training with transfered weights')\n    model_fin = model_builder.SimpleNeuralNet(shape=(x_train.shape[1],),learning_rate=0.001,output_shape=206)\n    model_fin = model_builder.transfer_weight(model,model_fin)\n    for layer in model_fin.layers:\n        layer.trainable=True\n\n    checkpoint_path = f'best_model_{fold}.hdf5'\n\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n\n    checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                  mode = 'min')\n\n    early = EarlyStopping(monitor='val_loss',patience=5,mode='min')\n\n    history=model_fin.fit(x_train.values,\n              y_train.values,\n              validation_data=(x_val.values,y_val.values),\n              epochs=50, batch_size=128,\n              callbacks=[early,reduce_lr_loss,checkpt]\n             )\n    print('')\n    histories.append(history)\n\n    model_fin= tf.keras.models.load_model(checkpoint_path, custom_objects={'leaky_relu': tf.nn.leaky_relu})\n    score= model_builder.metric(y_val,model_fin.predict(x_val).astype(float))\n    scores.append(score)\n    print(f'Validation metric: {score}')\n    test_predict = model_fin.predict(data_prepare.get_test().values)\n\n    ss.loc[:, y_train.columns] += test_predict\n    print('')\n","70c99e2d":"ss.loc[:, train_targets_scored.columns[1:]] \/= 5","e4881601":"ss.loc[test_data[test_data.cp_type==\"ctl_vehicle\"].index,train_targets_scored.drop('sig_id',axis=1).columns]=0","8daa70d4":"np.mean(scores)","fdd26c02":"import matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nplt.title('training_curve')\nfor h in histories:\n    plt.plot(h.history['val_loss'],color='red',label='val')\n    plt.plot(h.history['loss'],color=\"green\",label='train')\nplt.legend()","b088da19":"ss.to_csv('.\/submission.csv',index=False)","fe742d52":"## Note:\n* I have used multilabel stratified Kfold from the iterstrat library.\n> * The resaon that the code for the same is not visible is because I have already set up my cross validation scheme on my local machine.\n> * The data was split into folds and the respective folds were listed against the sample. Therefore the train_data that I am importing is not exactly the train_features.csv file. It is a concatenation of the features and target values along with a column for kfold.\n> * In case someone wants to fork this notebook please use MultilabelStratifiedKfold to split the data for training. \n* In my notebook I have used objects of two classes, that I wrote myself, for preparing the data and for building the model. Please pardon me if those code sections are not readable or understandable.","0ad05ab1":"# Train Main Model","3162c5bf":"# Prepare a model for transfer learning","34e9edee":"## Classes\n* The following classes are to prepare the data and build the model respectively.\n* The class MoaDataPrepare might not be understandable. Therefore feel free to skip reading it.\n* The structure of my model may be found in the class ModelBuilder","dcf9e599":"# Key Points\n* This notebook uses the method of transfer learning\n> * A model was trained on the training set using the non-scored targets.\n> * The weights of this model were transferred onto the model that trains on the scored tragets.\n> * This has a significant effect on the score.\n* The model is a very simple one\n> * It uses two dense layers and an output layer, each with weight normalization.\n> * Batch normalization layers as well as dropout layers are used.\n> * According to many researches, the order of batch normalization, activation and dropout is significant to model performance.\n* The cell viability features were found to have high correlation amongst them. Therefore PCA was used on these features to retain 98% of the variance.\n* About the Data:\n> * A comprehensive exploratory analysis can be found in one of my notebooks [here](https:\/\/www.kaggle.com\/bibhash123\/the-moa-challenge-an-analysis-of-the-data).\n> * The categrical feature cp_dose was label encoded and cp_time was one hot encoded.\n> * Only the samples with cp_type trt_cp were used for training. Thus the cp_type column was dropped.\n> * After the prediction on the test set the target values for samples with cp_type ctl_vehicle were set to 0.\n* Scope for improvement.\n> * The model is only a very simple one with a very shallow feed forward network.\n> * Certain anlysis and discussion imply that sequential models may also be used for better performance.\n> * Most of the high scores in the leaderboard have been reported to be achieved by ensembles or blended models."}}