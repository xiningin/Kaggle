{"cell_type":{"dc6198be":"code","a365bcb4":"code","5932b024":"code","70a1c2b9":"code","300446a1":"code","2902839b":"code","577d7768":"code","4e9eea8c":"code","62a75559":"code","d8ae2e27":"code","1e64e332":"code","d8540fe0":"code","5db42f8d":"code","07404f34":"code","0b571199":"code","4024ecae":"code","eb3488ea":"code","26f6514f":"code","3b183c9f":"code","e6c23201":"code","ee8bcb97":"code","6e59640e":"code","a5ac33cf":"code","ae4a8579":"code","d89bb5a3":"code","1edc582c":"code","5d4f4146":"code","37a04d37":"code","f2ef9c16":"code","14c16576":"code","987ef057":"code","7799e854":"code","7745f091":"code","b2c4fe6c":"code","c16bf2d4":"code","96ce3f79":"code","eece4092":"code","8ccfff87":"code","b3246d8c":"code","66781d5c":"code","588ab499":"code","699f14b9":"code","606c574d":"code","30b1f4a8":"code","bb735349":"code","70937306":"code","c94befaf":"code","c236ea56":"code","83a27541":"code","03cd8a2a":"code","82b96851":"code","71c93b7a":"code","8a8371b2":"code","bbaafc6d":"code","e7a188d9":"code","d48c25d5":"code","b3289471":"code","f2d30665":"code","1d23ed92":"markdown","34d8e7a9":"markdown","1c8fc5c2":"markdown","105d1046":"markdown","8fd19983":"markdown","f8d765ac":"markdown","288b5746":"markdown","5b4221d2":"markdown","4fd3b60b":"markdown","169a8888":"markdown","d3415b42":"markdown","a5d4fb42":"markdown","49378a38":"markdown","6b1026a6":"markdown","5206143c":"markdown","0b37003f":"markdown","8cfefb53":"markdown","adb452cc":"markdown","4cff7de0":"markdown","e61400a2":"markdown","68945d68":"markdown","d426b0f0":"markdown","b0e3eaa3":"markdown","f44af22b":"markdown","90c77df4":"markdown","a304b304":"markdown","690dbec7":"markdown","27d722ba":"markdown","d3e0e07f":"markdown","9d553789":"markdown","8ff1aaff":"markdown","ae993e06":"markdown","f89f7a58":"markdown","bf28d2c0":"markdown","f3145a84":"markdown","97c23139":"markdown","1a59d9a0":"markdown","1955abef":"markdown","15059aef":"markdown","7ba4350c":"markdown","03f9495f":"markdown","c3144a4f":"markdown","267647fa":"markdown","352de7c2":"markdown","190ea59e":"markdown","cc209c37":"markdown","da41fbbd":"markdown","5c64ca83":"markdown","afca0c4a":"markdown","681b2a2e":"markdown","121aec12":"markdown","5d18cd5c":"markdown","8adaaa11":"markdown","2373c39e":"markdown","52dd817e":"markdown","1bce344d":"markdown","a25494a9":"markdown","ed39658e":"markdown","26668419":"markdown","c82a2fce":"markdown","9287d677":"markdown","ab36435c":"markdown","d42b73f7":"markdown","ee58b0f2":"markdown","1dbe9fb2":"markdown","b17447bb":"markdown","bc605b33":"markdown","f7658ed7":"markdown","d01ab262":"markdown","57b7bc1f":"markdown","af28bd52":"markdown"},"source":{"dc6198be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a365bcb4":"import pandas as pd\nimport numpy as np\n\nimport altair as alt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as matplotlib\nfrom scipy.stats import loguniform\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score, precision_score, f1_score,roc_auc_score, accuracy_score\n\nfrom sklearn.ensemble import RandomForestRegressor, VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\nalt.data_transformers.disable_max_rows()\nsns.set_context('notebook')\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('Set2')","5932b024":"#Author: Dennis Trimarchi\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)\n","70a1c2b9":"df_train = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv')\ndf_train.head()","300446a1":"df_train.shape","2902839b":"df_test = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv')\n\ndf_test.head()","577d7768":"df_test.shape","4e9eea8c":"df_train.info()","62a75559":"df_train = df_train.drop(columns = ['Id'])","d8ae2e27":"cat_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']","1e64e332":"sns.set(font_scale= 4)\nfig, axes = plt.subplots(2, 3, figsize=(35,25), sharey=False)\nfig.suptitle('Distribution for all numerical variables', size = 40)\nfig.tight_layout(h_pad = 2, w_pad = 2 )\n\nfor i, num_var in enumerate(num_vars):\n    ax = sns.histplot(df_train[num_var], ax = axes[i\/\/3, i%3], kde = 1)\n    \nplt.show()","d8540fe0":"sns.set(font_scale= 5)\nfig, axes = plt.subplots(3, 3, figsize=(70,60), sharey=False)\nfig.tight_layout(h_pad = 8, w_pad = 2 )\n\nfor i, cat_var in enumerate(cat_vars):\n    \n    ax = sns.countplot(x = cat_var, hue= 'income', data=df_train, ax = axes[i\/\/3, i%3])\n    ax.tick_params(axis='x', labelrotation=90)\n\nplt.show()","5db42f8d":"df_train.education.value_counts()","07404f34":"dict_order = {'Preschool':0, '1st-4th':1, '5th-6th':2, '7th-8th':3, '9th':4, '10th': 5,\n              '11th': 6, '12th':7, 'HS-grad':8, 'Prof-school':9, 'Assoc-acdm':10, 'Assoc-voc':11,\n              'Some-college':12,  'Bachelors':13, 'Masters':14,  'Doctorate':15}\n\ndf_train['education'] = df_train['education'].apply(lambda x: dict_order[x])\ndf_test['education'] = df_test['education'].apply(lambda x: dict_order[x])","0b571199":"cat_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']","4024ecae":"df_train['marital.status'].value_counts()","eb3488ea":"dict_assemble_variables_marital = {'Separated': 'Divorced', 'Married-AF-spouse':'Other',\n                           'Married-spouse-absent': 'Other', 'Widowed': 'Other' }\n\ndef preprocessing_assemble_variables(x, dict_assemble_variables):\n    if x in dict_assemble_variables:\n        return dict_assemble_variables[x]\n    return x\n\ndf_train['marital.status'] = df_train['marital.status'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_marital))\ndf_test['marital.status'] = df_test['marital.status'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_marital))\n\ndf_train['marital.status'].value_counts()","26f6514f":"df_train.race.value_counts()","3b183c9f":"dict_assemble_variables_race = {'Amer-Indian-Eskimo': 'Other'}\n\ndf_train['race'] = df_train['race'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_race))\ndf_test['race'] = df_test['race'].apply(lambda x: preprocessing_assemble_variables(x,dict_assemble_variables_race))\n\ndf_train['race'].value_counts()","e6c23201":"df_train['native.country'].value_counts()","ee8bcb97":"def preprocess_country(x):\n    if x in ['United-States','?','Mexico']:\n        return x\n    return 'Other'\n\n\ndf_train['native.country'] = df_train['native.country'].apply(lambda x: preprocess_country(x))\ndf_test['native.country'] = df_test['native.country'].apply(lambda x: preprocess_country(x))\n\ndf_train['native.country'].value_counts()","6e59640e":"sns.set_context('notebook')\nsns.pairplot(df_train, hue = 'income')","a5ac33cf":"fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\nsns.heatmap(df_train[num_vars].corr(), annot = True)","ae4a8579":"df_train = df_train.drop(columns = ['education'])\ndf_test = df_test.drop(columns = ['education'])","d89bb5a3":"X_train, X_val, y_train, y_val = train_test_split(df_train.drop('income',axis=1),\n                                                  df_train['income'], train_size=0.8, random_state=42\n                                                 )","1edc582c":"cat_vars_complete = [i for i in df_train.columns if df_train.dtypes[i] == 'object']\nnum_vars_complete = [i for i in df_train.columns if df_train.dtypes[i] == 'float64' or df_train.dtypes[i] == 'int64']\n\nignore_variables = ['fnlwgt','education']\ncat_vars = [element for element in cat_vars_complete if (element != 'income' and element not in ignore_variables)]\nspecific_vars = []\nnum_vars = [element for element in num_vars_complete if (element not in specific_vars + ignore_variables)]\n\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n#     ('polynomial_features',PolynomialFeatures(interaction_only=True)),\n#     ('pca', PCA())\n])\n\nspecific_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('power_transform', PowerTransformer(method='yeo-johnson')),\n#     ('polynomial_features',PolynomialFeatures(interaction_only=False)),\n\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n#     ('truncated_svd', TruncatedSVD())\n#     ('sparse_pca', sparse_pca),\n#     ('to_dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n    ])\n\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_vars),\n        ('specific', specific_transformer, specific_vars),\n        ('cat', categorical_transformer, cat_vars),\n    ])","5d4f4146":"lg_model = LogisticRegression()\n\nparam_dist = dict()\nparam_dist['solver'] = ['newton-cg', 'liblinear']\nparam_dist['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nparam_dist['C'] = loguniform(1e-5, 100)\n\nlg_random_search = RandomizedSearchCV(lg_model, param_distributions=param_dist)\n\n\n\nmodel_lg = Pipeline(steps = [('preprocessor', preprocessor),\n                            ('model', lg_random_search\n                            )\n                         ])","37a04d37":"model_lg.fit(X_train, y_train)\ny_pred = model_lg.predict(X_val)","f2ef9c16":"model_lg['model'].best_params_","14c16576":"model_lg['model'].best_score_","987ef057":"cf_matrix = confusion_matrix(y_val, y_pred, labels=[\"<=50K\", \">50K\"])\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['<=50K', '>50K']\nmake_confusion_matrix(cf_matrix, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='binary')","7799e854":"dict_map = {'<=50K' : 0, '>50K' : 1}\ny_val_binary = pd.Series(y_val).apply(lambda x: dict_map[x])\ny_pred_binary = pd.Series(y_pred).apply(lambda x: dict_map[x])\n\ndict_results = {'model':['Logistic Regression'], 'accuracy_score':[accuracy_score(y_val_binary, y_pred_binary)], 'recall_score':[recall_score(y_val_binary, y_pred_binary)],\n                'precision_score': [precision_score(y_val_binary, y_pred_binary)], 'f1_score': [f1_score(y_val_binary, y_pred_binary)],\n                'roc_auc_score':[roc_auc_score(y_val_binary, y_pred_binary)]}\n\ndf_results_lg = pd.DataFrame(dict_results)\ndf_results_lg","7745f091":"rf_classifier = RandomForestClassifier()\n\nparam_dist_rf = {\"max_depth\": [None, 1],\n              \"max_features\": ['auto','sqrt'],\n              \"min_samples_split\": [2,3],\n              \"min_samples_leaf\": [3, 4],\n              \"n_estimators\": [400]}\n\nrf_random_search = RandomizedSearchCV(rf_classifier, param_distributions=param_dist_rf)\n\n\n\nmodel_rf = Pipeline(steps = [('preprocessor', preprocessor),\n                            ('model', rf_random_search\n                            )\n                         ])","b2c4fe6c":"model_rf.fit(X_train, y_train)\ny_pred = model_rf.predict(X_val)","c16bf2d4":"model_rf['model'].best_params_","96ce3f79":"model_rf['model'].best_score_","eece4092":"cf_matrix = confusion_matrix(y_val, y_pred, labels=[\"<=50K\", \">50K\"])\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['<=50K', '>50K']\nmake_confusion_matrix(cf_matrix, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='binary')","8ccfff87":"dict_map = {'<=50K' : 0, '>50K' : 1}\ny_val_binary = pd.Series(y_val).apply(lambda x: dict_map[x])\ny_pred_binary = pd.Series(y_pred).apply(lambda x: dict_map[x])\n\ndict_results = {'model':['Random Forest'], 'accuracy_score':[accuracy_score(y_val_binary, y_pred_binary)], 'recall_score':[recall_score(y_val_binary, y_pred_binary)],\n                'precision_score': [precision_score(y_val_binary, y_pred_binary)], 'f1_score': [f1_score(y_val_binary, y_pred_binary)],\n                'roc_auc_score':[roc_auc_score(y_val_binary, y_pred_binary)]}\n\ndf_results_rf = pd.DataFrame(dict_results)\ndf_results_rf","b3246d8c":"mlp = MLPClassifier()\nparam_dist = {\n    'hidden_layer_sizes': [(25,25), (50,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['adam'],\n    'alpha': [0.01, 0.05],\n    'learning_rate': ['constant'],\n}\n\nmlp_random_search = RandomizedSearchCV(mlp, param_distributions=param_dist)\n\n\n\nmodel_mlp = Pipeline(steps = [('preprocessor', preprocessor),\n                            ('model', mlp_random_search\n                            )\n                         ])","66781d5c":"model_mlp.fit(X_train, y_train)\ny_pred = model_mlp.predict(X_val)","588ab499":"model_mlp['model'].best_params_","699f14b9":"model_mlp['model'].best_score_","606c574d":"cf_matrix = confusion_matrix(y_val, y_pred, labels=[\"<=50K\", \">50K\"])\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['<=50K', '>50K']\nmake_confusion_matrix(cf_matrix, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='binary')","30b1f4a8":"dict_map = {'<=50K' : 0, '>50K' : 1}\ny_val_binary = pd.Series(y_val).apply(lambda x: dict_map[x])\ny_pred_binary = pd.Series(y_pred).apply(lambda x: dict_map[x])\n\ndict_results = {'model':['MLP'],'accuracy_score':[accuracy_score(y_val_binary, y_pred_binary)], 'recall_score':[recall_score(y_val_binary, y_pred_binary)],\n                'precision_score': [precision_score(y_val_binary, y_pred_binary)], 'f1_score': [f1_score(y_val_binary, y_pred_binary)],\n                'roc_auc_score':[roc_auc_score(y_val_binary, y_pred_binary)]}\n\ndf_results_mlp = pd.DataFrame(dict_results)\ndf_results_mlp","bb735349":"xgb = XGBClassifier()\n\nparam_dist_xgb = {'n_estimators': [300],\n               'learning_rate': np.linspace(1e-3, 1),\n               'max_depth': np.arange(1, 20),\n               'reg_alpha': loguniform(1e-14, 1e1),\n               'reg_lambda': loguniform(1e-14, 1e1),}\n\nxgb_random_search = RandomizedSearchCV(xgb, param_distributions=param_dist_xgb)\n\n\n\nmodel_xgb = Pipeline(steps = [('preprocessor', preprocessor),\n                            ('model', xgb_random_search\n                            )\n                         ])","70937306":"model_xgb.fit(X_train, y_train)\ny_pred = model_xgb.predict(X_val)","c94befaf":"model_xgb['model'].best_params_","c236ea56":"model_xgb['model'].best_score_","83a27541":"cf_matrix = confusion_matrix(y_val, y_pred, labels=[\"<=50K\", \">50K\"])\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['<=50K', '>50K']\nmake_confusion_matrix(cf_matrix, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='binary')","03cd8a2a":"dict_map = {'<=50K' : 0, '>50K' : 1}\ny_val_binary = pd.Series(y_val).apply(lambda x: dict_map[x])\ny_pred_binary = pd.Series(y_pred).apply(lambda x: dict_map[x])\n\ndict_results = {'model':['XGBoost'], 'accuracy_score':[accuracy_score(y_val_binary, y_pred_binary)], 'recall_score':[recall_score(y_val_binary, y_pred_binary)],\n                'precision_score': [precision_score(y_val_binary, y_pred_binary)], 'f1_score': [f1_score(y_val_binary, y_pred_binary)],\n                'roc_auc_score':[roc_auc_score(y_val_binary, y_pred_binary)]}\n\ndf_results_xgb = pd.DataFrame(dict_results)\ndf_results_xgb","82b96851":"final_results = pd.concat([df_results_lg, df_results_rf ,df_results_mlp, df_results_xgb])\nfinal_results","71c93b7a":"model = model_xgb","8a8371b2":"model.fit(df_train.drop(columns = ['income']),df_train['income'])","bbaafc6d":"model['model'].best_params_","e7a188d9":"model['model'].best_score_","d48c25d5":"y_pred = model.predict(df_test.drop(columns = ['Id']))","b3289471":"final_pred = pd.DataFrame(columns = [\"Id\",\"income\"])\n\nfinal_pred.Id = df_test.Id\nfinal_pred.income = y_pred","f2d30665":"final_pred.to_csv(\"submission.csv\", index=False)","1d23ed92":"In this section, we will try to understand if the variables are correlated or not, and try to see what variables are more important to predict our target variable. ","34d8e7a9":"The next model we will use is a `Random Forest`. This model is extremly useful and normally give great results. \n\nRandom Forests are based on decision trees, which consist of really simple trees that decides the best rules on the parameters in order to classify our data as well as it can. Therefore, it partitions the feature space in hypercubes. \n\nRandom Forest provides two main improvements over simple decision trees [3]: \n\n1. It uses an ensemble technique calls bagging, which **reduces the variance** of the model by constructing multiple trees and averaging the results. In bagging, the way this is done is by using a technique called bootstrap, which basically means we take multiple samples from the training set to train different tree models. The final prediction here would be:\n\n$$\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x)$$\n\n   i.e. an average of the prediction obtained by the decision trees trained on each bootstrapped training set. As mentioned above, this method is called bagging and is extremly useful to reduce the variance of models.\n    \n2. The second improvement is a small tweak that **decorrelates trees**. To do that, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. Here, $m \\approx \\sqrt{p}$. The idea here is to decorrelate the trees by having trees that are different between them, because, like that, the average operation done in bagging is more useful in reducing the variance. \n\nOf course, all of this is implemented in `sklearn` so we can easily instantiate the `RandomForestClassifier` class and use it. Again, we will try to optimize the parameters of the random forest model by using `RandomizedSearchCV`. ","1c8fc5c2":"Now that everything is ready, we call finally fit our model and predict what we think are the labels for the `X_val` data. ","105d1046":"After building the preprocessor Pipeline, we can finally move into our models.","8fd19983":"### 2.3.1 Logistic Regression","f8d765ac":"After doing some iterations and try some different combination of parameters in our `Randomized Search`, we finally get that the **best parameters** we have for our problem are:","288b5746":"# 1. Exploring the data","5b4221d2":"We can thus conclude that our worst metric, overall, is the Recall, since we usually miss about 35%-40% of the true labels. Moreover, we can conclude that, overall, `XGBoost` is our best model, since it is the best in basically every metric. Therefore, let's use it as our final model to export our results.  ","4fd3b60b":"#### 2.3.3.2 Model evaluation ","169a8888":"Here, we will use `pandas` to read the training data and test data. ","d3415b42":"Let's recompute our cat_vars and num_vars to take that into account. ","a5d4fb42":"After doing some iterations and try some different combination of parameters in our `Randomized Search`, we finally get that the **best parameters** we have for our problem are:","49378a38":"# 2. Developing our models","6b1026a6":"## 1.5 Understanding the correlation between the variables","5206143c":"- `occupation`: we also see some camouflaged null values with the '?' symbol. Again, we might not want to not consider them because they may contain some information. Otherwise, this feature seems pretty useful and don't need a lot of pre processing. \n\n- `relationship`: we see that this variable seems to be a combination of the marital.status variable and sex variable. Therefore, we might want to drop some of them in order to decrease the dimension of the problem. For instance, we see that Husbands tend to have bigger income, which we could also conclude by looking at the sex variable + the marital.status variable. \n\n- `race`: we see that most people with high income are white. However, there are not many Ameri-Indian-Eskimo, so we may want to put them together with the other group. \n\nLet's do that: ","0b37003f":"## 1.3 Separating the types of variables","8cfefb53":"We can separate the different types of variables by the following code. ","adb452cc":"In this section we will train our models and evaluate their performance. For this specific challenge, we are going to explore 4 different models. We will use here a `RandomizedSearchCV`, which will basically try some random possible combinations of hyperparameters on the dictionary `param_dist`, doing cross validations each time. By doing that, our final model will have the best possible combination (between the tested combinations) that reduces our CV Errors. Moreover, since `RandomizedSearchCV` is initialized with `refit = True`, i.e. once it finds the best estimator, it retrains it on the whole training set, which is great. \n\nFor the evaluation part we will see our results comparing `y_val` and `y_pred`. To do that, we will plot a confusion matrix, that will be a lot more useful than just knowing the accuracy. We will also have the values for precision, recall and the F1 Score, which are really important metrics. \n\nWe recall here the meaning of this metrics: \n\n$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} = \\text{the % got right}$$\n\n$$Precision = \\frac{TP}{TP + FP} = \\text{% of instances classified as Positive that were right}$$\n\n$$Recall = \\frac{TP}{TP + FN} = \\text{% of Positive instances that we actually captured}$$\n\n$$F1_{score} = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = \\text{harmonic mean of Precision and Recall}$$\n\nwhere `T` means true, `F` means False, saying if our classification corresponded to the reality or not and `P` means positive and `N` means negative, regarding the classification of our model. Therefore, a `FP`, for instance, is something our model thought was positive, but the real label was negative. Here, our `positive = '>50K'` of income.  \n\nAnother important metric we will use is the `ROC-AUC`, that is usefull to evaluate the metrics in different thresholds. It's basically the area under the ROC curve, that is curve that represents the True Positive Rate and False Positive Rate while varying the thresholds. More details can be found in [3].\n\nNow let's move into exploring different models we saw in the course in order to get the best possible performance we can. We will explore here 4 models: \n\n1. Logistic Regression \n2. Random Forest Classifier\n3. Support Vector Machine Classifier \n4. XGBoost Classifier\n","4cff7de0":"Below, we can see the types of the variables we are working with. Moreover, we see that we do not have any missing values (null values), which is great. ","e61400a2":"Let's summarize our results on the following dataframe:","68945d68":"### 2.3.2 Random Forest","d426b0f0":"We can conclude a lot of different interesting things. Summing up, we can conclude that: \n\n- `workclass`: there are actually some camouflaged null values here, with the '?'. However, we may not remove this data, since it might contain some information, i.e. there might be a pattern in the people we don't know the workclassk. We also notice that most people with high income have workclass = Private. \n\n- `education`: besides being a categorical variable, we may want to ordenate them, since the degree of education naturally follows a sequence. This may help us when transforming this variables into numbers, since we won't need to use One Hot Enconding, which would increase the dimension of our problem by a lot. Moreover, we see that people with lower levels of education don't usually have a big income, which matches our intuition.\n\nIn order to do that, we will need to order the variables manually. Let's try to do that: ","b0e3eaa3":"# 3. Exporting final model","f44af22b":"## 1.4 Understanding the distribution of the variables and preprocessing the variables","90c77df4":"#### 2.3.1.2 Model evaluation","a304b304":"## 2.4 Comparing our models\nLet's now compare our models and choose the one that performs the best. To do that, we will use several metrics that were calculated above, while we were running the models. The metrics used here are: \n1. Accuracy Score\n2. Recall \n3. Precision \n4. F1 Score\n5. ROC-AUC Score\n\nAll these metrics are really important on the classification task and each one has a specific meaning that was explained on `section 2.3`. Let's now compare the final results. ","690dbec7":"As we can see, we have `32560 data points for the training set` and `16280 for the test set`. Moreover, the test set does not contain the target variable, so in order to train and test our model we will need to use the training data set. ","27d722ba":"Let's check the best CV score of our model: ","d3e0e07f":"Now that everything is ready, we call finally fit our model and predict what we think are the labels for the `X_val` data. ","9d553789":"Let's now begin to explore different types of models in order to solve our classification problem. \n\nThe first model we will explore is the `Logistic Regression`. The Logistic Regression is an adaptation of the Linear Regression and its intuition is really simple. For instance, we could try to model the $\\mathbb{P}(>50k \\mid X)$ using a simple linear regression, but the problem with that is that we would have values that are bigger than 1 and values that are smaller than 0. This is, of course, not reasonable [3], since we cannot have negative probabilities or probabilities bigger than 1. To avoid this problem, we can use the logistic function and model the probability as follows: \n\n\n$$ \\mathbb{P}( >50k \\mid X) = \\frac{e^{\\beta_0 + \\beta^T X}}{1+ e^{\\beta_0 + \\beta^T X}}$$\n\nThe logistic funcion produces an S-shaped function, which is always between 0 and 1, being thus great to model probabilities. To fit the model, we use a model called `maximum likelihood`, that is better described in [1]. \n\nOf course, all of that is already implemented on `sklearn`, so we can just use it out of the box. Moreover, we will try to use the logistic regression with some different parameters. For instance, we will change the solver and the regularization parameters, that should be useful to guarantee the model will not overfit. To iterate on these paramenters we will use `RandomizedSearchCV` that, as explained above, will randomly try parameters out of the distribution of parameters we give the model. ","8ff1aaff":"## 2.2 Making our pipeline","ae993e06":"## 0.2 Importing function to plot confusion matrix","f89f7a58":"Let's summarize our results on the following dataframe:","bf28d2c0":"- `marital.status`: here, we also may try to put some variables together, such as divorced and separated. Moreover, we notice most people with more than 50k of income are actually in the category Married-civ-spouse. Furthermore, we see that relationship is kind of a less noisy version of this variable, so we may want to not use it afterwards, in order to decrease the dimension of the problem. \n\nLet's pre-treat this variable, by putting separated and divorced together and by putting less common categories in a common \"Other\" category. ","f3145a84":"#### 2.3.2.1 Model training ","97c23139":"#### 2.3.4.1 Model training ","1a59d9a0":"## 0.3 Reading data from csv","1955abef":"In order to build our model, we will use the `Pipeline` function from sklearn, which makes our life much easier. Moreover, we will separate our variables in four types of variables: \n\n- `ignore_variables`: variables we will not use, since they are not really useful to our final model and are basically just fitting some noise;\n- `cat_vars`: categorical variables that are not in the ignore_variables list. \n- `specific_vars`: list of variables that have high skewness where it might be worth trying some different preprocessing techniques, such as the PowerTransformer. \n- `num_vars`: numerical variables that are not in the ignore_variables, neither on the specific_vars list. \n\n\nSeparating the variables like that make it possible for us to treat the variables differently. For instance, for the numerical variables, we want to scale the features, since KNN highly depends on the distances, so if the variables have different scales, it won't work well. On the other hand, for categorical variables, we will use the one hot encoder, for instance. \n\nAfter some iteration and testing (using for instance the Backward stepwise feature selection method), we decide that we want to ignore the variables `fnlwgt` and `education`. Moreover, our specific_transformer was not working particularly well for this problem, so we might just say that we will not put any variables on the specific variable features. \n","15059aef":"### 2.3.4 XGBoost\n\nThe last model we will build is the `XGBoost classifier`. The XGBoost is a huge star on the Kaggle world, since it has been the winner of many challenges. It's based on the boosting, that is a technique of bulding trees _sequentially_, i.e. each tree is growing using information from previously grown trees. Howeverm XGBoost makes several improvements over classical boosting techniques (such as the Adaboost we saw in class). Some of these improvements are cited below [4]: \n\n- Clever penalization of trees\n- A proportional shrinking of leaf nodes\n- Newton Boosting\n- Extra randomization parameter\n- Implementation on single, distributed systems and out-of-core computation\n- Automatic Feature selection\n\nTherefore, it is an amazing tool that is avaible to everyone out there on the `xgboost` library. It also integrates well with the `sklearn` environment, so we call still use `RandomizedSearchCV` to do our hyperparameter tunning. ","7ba4350c":"Let's check the best CV score of our model: ","03f9495f":"The first thing we can do is drop the variable `Id`, since there is no reason for it to influence the income of someone and we would just be fitting some noise, by putting it in our classifier.","c3144a4f":"## 0.1 Importing packages ","267647fa":"#### 2.3.3.1 Model training ","352de7c2":"Let's summarize our results on the following dataframe:","190ea59e":"## 2.3 Training and Evaluating the models ","cc209c37":"# 0. Setup code\nIn this section, we will import some packages, set up our data visualization environment and read our data from the CSVs. ","da41fbbd":"Let's summarize our results on the following dataframe:","5c64ca83":"After doing some iterations and try some different combination of parameters in our `Randomized Search`, we finally get that the **best parameters** we have for our problem are:","afca0c4a":"## 1.1 Exploring types of variables\/missing values","681b2a2e":"We notice that many of the distributions are quite skewed, so we might want to try to do some pre processing steps that fit well this type of that. Moreover, `capital.gain` and `capital.loss` seem to be quite concentrated around 0. ","121aec12":"### 2.3.3 Multi layer Perceptron \n\nA `multilayer perceptron (MLP)` is a class of feedforward artificial neural network. It consists of a neural network that is usually built using some layers that combine linear operation with non-linear operations in order to create a complex model that is quite flexible. It's inspired on the human brain and is the foundation of deep learning, which is a Machine Learning field that is on its prime. We will not go into further details in this notebook but an in-depth explanation can be found on [1]. The neural networks have a thon of parameters, so sometimes choosing the best network structure, learning rates and activation layers can be hard and based on experimentation. \n\nAgain, this is implemented in `sklearn` so we can easily instantiate the `MLPClassifier` class and use it. Again, we will try to optimize the parameters of the neural network by using `RandomizedSearchCV`. ","5d18cd5c":"Let's check the best CV score of our model: ","8adaaa11":"#### 2.3.1.1 Model training ","2373c39e":"#### 2.3.4.2 Model evaluation ","52dd817e":"[1] Hastie, Trevor, Trevor Hastie, Robert Tibshirani, and J H. Friedman. **The Elements of Statistical Learning: Data Mining, Inference, and Prediction**. New York: Springer, 2001.\n\n[2] Larry Wasserman. **All of statistics: a concise course in statistical inference**. Springer Science & Business Media,2013.\n\n[3] Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. **An Introduction to Statistical Learning : with Applications in R**. New York :Springer, 2013.\n\n[4] \u201cXGBoost.\u201d Wikipedia, the free encyclopedia. Avaible on : https:\/\/en.wikipedia.org\/wiki\/XGBoost","1bce344d":"Before diving into our modelling, let's review why the KNN used in `PMR3508 - KNN for Dataset Adult` is limited. Firstly, we know [1] that: \n\n$$ \\hat{G}(x) = \\mathcal{G}_k \\text{ if } \\mathbb{P}(\\mathcal{G}_k \\mid X = x) = \\max_{g\\in\\mathcal{G}}\\,\\mathbb{P}(g \\mid X = x) $$\n\nThis classifier is called the _`Bayes Classifier`_ and is basically saying that we will classify the most probable classifier, given the data. This is the classifier that minimizes the Expected Prediction Error and its error rate is called the _`Bayes rate`_. Here the problem is a little bit simpler, since we only have two classes. \n\nWhat is interesting to note is that the KNN classifier is actually trying to directly approximate this solution: the probability is estimated by training-samples proportion and the conditional probability in a point is relaxed to a conditional probability on a neighborhood of a point. Therefore, it seems that, when having a big training set, we would be able to approximate the theorical optimal classifier. \n\nHowever, the problem here is that our intuition might not work well in high dimensions. That is what is called the _`curse of dimensionality`_. For instance, if we want to take 1% of all the data in a unit hyper cube, we need an edge of 0.63 (since $0.63^{10} \\approx 0.01$). This is counterintuitive but it is why it is not sufficient to use only KNN Classifiers, i.e. in high dimensions our KNN Classifier breaks. \n\nTherefore, it is useful to use different model, such as tree based models, that does not necessarily directly approximate our `Bayes Classifier`, but that in practice works pretty well. We will explore **4 different models** here, in order to try to get the best performance. ","a25494a9":"After doing some iterations and try some different combination of parameters in our `Randomized Search`, we finally get that the **best parameters** we have for our problem are:","ed39658e":"#### 2.3.2.2 Model evaluation ","26668419":"# 4. References ","c82a2fce":"Let's check the best CV score of our model: ","9287d677":"- `sex`: males tend to have higher income than females. Moreover, we have more data from males than females. \n\n- `native.country`: we see that most the data we have come from the USA. We also have some '?' variables, which we may keep, since they are quite representative. Also, we seem to have a significat amount of data for Mexico. However, we can put the rest of the countries together in a 'Other' category in order to decrease the dimension of the problem.  \n\nLet's do that:","ab36435c":"## 2.1 Splitting the data","d42b73f7":"# PMR3508 - Classification with the Adult dataset\n\n\n#### Author: Caio Iglesias\n\n> This notebook is a sequel to the last notebook \"PMR3508 - KNN for Dataset Adult\", that showed some EDA and a model using the KNN. This time, we will keep our EDA the same, but try more complex models in order to improve our performance.  \n\nThe goal of this notebook is to predict if the income of people is bigger than 50k or smaller than that, based on features such as their age, their education, their country, etc. To do that, we will use the Adult Dataset from the `UCI Machine Lerning Repository`, that can be found here: https:\/\/archive.ics.uci.edu\/ml\/index.php . Moreover, the goal here is not necessarily to obtain the best possible performance (here measured in accuracy) but to develop a solid framework that solves this problem, based on the theory learned on the course. We will use mostly `sklearn` to develop the Pipelines of our model, `pandas` to manipulate the data and `matplotlib` and `seaborn` for data visualization.","ee58b0f2":"## 1.2 Excluding Id column","1dbe9fb2":"Now that everything is ready, we call finally fit our model and predict what we think are the labels for the `X_val` data. ","b17447bb":"Now let's take a look at our categorical variables and how they are related to our target variable income. ","bc605b33":"We notice that we have `15 features and 1 target variable (income)`. We also see that we have a lot of categorical data, that we will need to treat them carefully in order to not harm the performance of our KNN model (more on that on section 2). Let's take a look at the shape of our train and test data sets. ","f7658ed7":"The first thing we want to do is to split the training data in a train and validation data set. Here, we won't call it a test data set, because the actual test data set is the one on Kaggle that we are going to test at the really end. Here, we will use the `X_train` and `y_train` to train and optimize our hyperparameters, using cross validation. Then, we will use `X_val` and `y_val` to test our optimized model and evaluate metrics such as precision, recall and the F1 factor. These metrics are extremly useful in a classification problem and we would not be able to estimate them if we did not do this split, since we do not have access to the full Kaggle test dataset here. \n\nWe will also set a random_state, just to be sure we will get the same split every time we run the code.","d01ab262":"Here, we will try to understand the distribution of our numerical variables. In order to do that, we will use the function `sns.histplot` from the seaborn package, here imported as _sns_. This function is really useful, since, besides plotting an histogram of the data, it also plots the `Kernel Density Estimation` or KDE, which is smoother and converges faster to the true density than the histograms. The KDE non-parametric statistic method and is really useful in practice. \n\nA kernel [2] is basically a smooth function K, such that $K(X) \\ge 0$ , $\\int K(x) dx = 1$, $\\int x K(x) dx = 0$ and $\\sigma_K^2 \\equiv \\int x^2 K(x) dx > 0$. The kernel used in the seaborn package is the Gaussian or Normal kernel, which is written as $K(x) = (2\\pi)^{-1\/2}e^{-x^2\/2}$. Then, the Kernel Density Estimator itself is written as: \n\n$$\\hat{f}(x) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K(\\frac{x-X_i}{h}) $$\n\nwhere $h$ is a positive number called the bandwidth. There are a lot of theories on how one may choose $h$, but this is out of the escpope of this notebook. Let's now take a look at how our data is distributed. ","57b7bc1f":"We see that the `education` variable we created seems to be completely correlated with the `education.num` variable, so we may want to drop one of them, since a correlation between variables could harm our model. (less probable in KNN but in general this is true). Let's drop education, since it is the one we created and is probably less trustworthy. Moreover, `hours.per.week` and `fnlwgt` seem not to be really important to our problem. So we might consider not putting that on our final model. ","af28bd52":"Now that everything is ready, we call finally fit our model and predict what we think are the labels for the `X_val` data. "}}