{"cell_type":{"fddc5f95":"code","2a77ff0e":"code","3a48ef19":"code","33a372b0":"code","ddf43393":"code","d1ee7255":"code","5bda3a49":"code","1c427aa3":"code","920591de":"code","ad1ffe87":"code","c9b7d0f3":"code","7b6bdf10":"code","f50264cd":"code","9d2b5d17":"markdown","d2399239":"markdown","95858ab0":"markdown","3b20aed8":"markdown","f20f327d":"markdown","2eb72d81":"markdown","9a81f938":"markdown"},"source":{"fddc5f95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2a77ff0e":"# Read the data\ndf_train = pd.read_csv('..\/input\/learn-together\/train.csv')\ndf_test = pd.read_csv('..\/input\/learn-together\/test.csv')","3a48ef19":"df_train.head()","33a372b0":"\n\nprint('Train size: ',df_train.shape)\nprint('Test size: ',df_test.shape)","ddf43393":"#why is there only 55 columns in Test - whats different\n\ntrain_cols = df_train.columns\ntest_cols = df_test.columns\n\nmissing_cols = train_cols.difference(test_cols)\nprint('Missing cols in df_test are -', missing_cols)","d1ee7255":"# What are the columns \n\ndf_train.columns\n\n# looking deeper\n\ndf_train.info()","5bda3a49":"# Create target object and call it y\ny = df_train.Cover_Type\n\n#Create X for the Features\n#features = ['Elevation','Aspect','Slope']\n\nfeatures = df_test.columns.values.tolist()\nprint(features)\n\nX = df_train[features]","1c427aa3":"# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Specify Model\nforest_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\nforest_model.fit(train_X, train_y)","920591de":"# Make validation predictions and calculate mean absolute error\nval_predictions = forest_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))","ad1ffe87":"# Using best value for max_leaf_nodes\nforest_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\nforest_model.fit(train_X, train_y)\nval_predictions = forest_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))","c9b7d0f3":"# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","7b6bdf10":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor(random_state=1)\n\n# fit rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(train_X, train_y)\n","f50264cd":"\n# read test data file using pandas\ntest_data = df_test\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\ntest_X  = test_data[features]\n\n# make predictions which we will submit. \ntest_preds = forest_model.predict(test_X)\n\n# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('submission.csv', index=False)","9d2b5d17":"# Make Predictions\nRead the file of \"test\" data. And apply your model to make predictions","d2399239":"## Understand what the data looks like.\n\n* Whats the shape\n    Training is 15120 Rows by 56 Columns\n    Test is 565892 by 55 Columns - \n    > Missing column is **Cover Type** which is what we are tying to predict\n* Are there any missing values ","95858ab0":"Build a simple Decision Tree Model\n\n*Based on Kaggle Machine learning exercise*","3b20aed8":"# The challenge:\n![](https:\/\/www.fs.usda.gov\/Internet\/FSE_MEDIA\/fseprd587687.jpg)\n\n\n\nThe leaderboard will be finalized on ** October 31, 2019.\n\nIn this competition you\u2019ll **predict what types of trees there are in an area based on various geographic features.**\n\n*The competition datasets comes from a study conducted in four wilderness areas within the beautiful Roosevelt National Forest of northern Colorado. These areas represent forests with very little human disturbances \u2013 the existing forest cover types there are more a result of ecological processes rather than forest management practices.*\n\nThe data is in raw form and contains categorical data such as wilderness areas and soil type.\n\nSubmissions are **evaluated on categorization accuracy.\n\nThat's just what fraction of predictions did you get right. You will want to use Classifier models like RandomForestClassifier rather than Regression models. With classifier models, the predict method will tell you which category is most likely.\n\nSubmission File\n\nFor each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n\nID,TARGET\n2,0\n5,3\n6,2\netc.","f20f327d":"# Creating a Model For the Competition\n\nBuild a Random Forest model and train it on all of **X** and **y**.  ","2eb72d81":"### What can we learn outside the data set from other sources.\n\nI know its a data problem but I like a bit of context, a quick Google on \"Jock A. Blackard and Colorado State University\" pulls up https:\/\/archive.ics.uci.edu\/ml\/datasets\/covertype\n\nThis looks like it could be the source of the dataset and has the following information.\n\n### Data Set Information:\n\nPredicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types). \n\nThis study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. \n\nSome background information for these four wilderness areas: \n    Neota (area 2) probably has the highest mean elevational value of the 4 wilderness areas.\n    Rawah (area 1) and Comanche Peak (area 3) would have a lower mean elevational value,\n    Cache la Poudre (area 4) would have the lowest mean elevational value. \n\nAs for primary major tree species in these areas,\n    Neota would have spruce\/fir (type 1),\n    >while Rawah and Comanche Peak would probably have lodgepole pine (type 2) as their primary species, followed by spruce\/fir and aspen (type 5).\n    >Cache la Poudre would tend to have Ponderosa pine (type 3), Douglas-fir (type 6), and cottonwood\/willow (type 4). \n\nThe Rawah and Comanche Peak areas would tend to be more typical of the overall dataset than either the Neota or Cache la Poudre, due to their assortment of tree species and range of predictive variable values (elevation, etc.) Cache la Poudre would probably be more unique than the others, due to its relatively low elevation range and species composition.\n\n\n### Attribute Information:\n\nGiven is the attribute name, attribute type, the measurement unit and a brief description. The forest cover type is the classification problem. The order of this listing corresponds to the order of numerals along the rows of the database. \n\nName \/ Data Type \/ Measurement \/ Description \n\nElevation \/ quantitative \/meters \/ Elevation in meters \nAspect \/ quantitative \/ azimuth \/ Aspect in degrees azimuth \nSlope \/ quantitative \/ degrees \/ Slope in degrees \nHorizontal_Distance_To_Hydrology \/ quantitative \/ meters \/ Horz Dist to nearest surface water features \nVertical_Distance_To_Hydrology \/ quantitative \/ meters \/ Vert Dist to nearest surface water features \nHorizontal_Distance_To_Roadways \/ quantitative \/ meters \/ Horz Dist to nearest roadway \nHillshade_9am \/ quantitative \/ 0 to 255 index \/ Hillshade index at 9am, summer solstice \nHillshade_Noon \/ quantitative \/ 0 to 255 index \/ Hillshade index at noon, summer soltice \nHillshade_3pm \/ quantitative \/ 0 to 255 index \/ Hillshade index at 3pm, summer solstice \nHorizontal_Distance_To_Fire_Points \/ quantitative \/ meters \/ Horz Dist to nearest wildfire ignition points \nWilderness_Area (4 binary columns) \/ qualitative \/ 0 (absence) or 1 (presence) \/ Wilderness area designation \nSoil_Type (40 binary columns) \/ qualitative \/ 0 (absence) or 1 (presence) \/ Soil Type designation \nCover_Type (7 types) \/ integer \/ 1 to 7 \/ Forest Cover Type designation","9a81f938":"## Preface\n\n*This is my first real go at this so I thourght I would jump in and also try and learn how to make the notebooks readable from the outset... so here goes.*"}}