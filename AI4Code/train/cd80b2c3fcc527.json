{"cell_type":{"6a60750e":"code","cb2a3e88":"code","5b071288":"code","c3e67f4a":"code","0fe0c643":"code","c908ab18":"markdown","0d1ee556":"markdown"},"source":{"6a60750e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import Parallel, delayed\nimport lightgbm as lgb\nfrom scipy import stats\n\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\ntqdm.pandas()","cb2a3e88":"test = pd.read_csv('..\/input\/freesound-audio-tagging-2019\/sample_submission.csv')\n\nlabel_columns = list( test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\n\nprint(test.shape)","5b071288":"X     = np.load( '..\/input\/freesoundpreproc1\/LGB-train-1.npy' )\nXtest = np.load( '..\/input\/freesoundpreproc1\/LGB-test-1.npy' )\nY     = np.load( '..\/input\/freesoundpreproc1\/LGB-target.npy' )\n\nX.shape, Xtest.shape, Y.shape","c3e67f4a":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=69)\n\nparams = {'num_leaves': 15,\n         'min_data_in_leaf': 200, \n         'objective':'binary',\n         \"metric\": 'auc',\n         'max_depth': -1,\n         'learning_rate': 0.05,\n         \"boosting\": \"gbdt\",\n         \"bagging_fraction\": 0.85,\n         \"bagging_freq\": 1,\n         \"feature_fraction\": 0.20,\n         \"bagging_seed\": 42,\n         \"verbosity\": -1,\n         \"nthread\": -1,\n         \"random_state\": 69}\n\nPREDTRAIN = np.zeros( (X.shape[0],80) )\nPREDTEST  = np.zeros( (Xtest.shape[0],80) )\nfor f in range(len(label_columns)):\n    y = Y[:,f] #target label\n    oof      = np.zeros( X.shape[0] )\n    oof_test = np.zeros( Xtest.shape[0] )\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n        model = lgb.LGBMClassifier(**params, n_estimators = 20000)\n        model.fit(X[trn_idx,:], \n                  y[trn_idx], \n                  eval_set=[(X[val_idx,:], y[val_idx])], \n                  eval_metric='auc',\n                  verbose=0, \n                  early_stopping_rounds=25)\n        oof[val_idx] = model.predict_proba(X[val_idx,:], num_iteration=model.best_iteration_)[:,1]\n        oof_test    += model.predict_proba(Xtest       , num_iteration=model.best_iteration_)[:,1]\/n_fold\n\n    PREDTRAIN[:,f] = oof    \n    PREDTEST [:,f] = oof_test\n    \n    print( f, str(roc_auc_score( y, oof ))[:6], label_columns[f] )\n    \nprint( 'Competition Metric Lwlrap cv:', calculate_overall_lwlrap_sklearn( Y, PREDTRAIN ) )","0fe0c643":"test[label_columns] = PREDTEST\ntest.to_csv('submission.csv', index=False)\ntest.head()","c908ab18":"Load preprocessed train and test","0d1ee556":"> This experiment uses Lightgbm to fit statistical features extracted from the raw files.\n>\n> As you may know, Deep Learning\/CNN is the right way to deal with this kind of challenge.\n>\n> But the challenge in this Kernel is to use pure statistics and GBDT to build good solutions.\n>\n> As extracting takes a lot of time, I preprocessed train and test locally and just uploaded files.\n>\n> Giba\n"}}