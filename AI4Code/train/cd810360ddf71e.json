{"cell_type":{"d84390d2":"code","4cb10853":"code","908547bd":"code","c0b1189a":"code","2bbb63e8":"code","658f0989":"code","f88d095c":"code","e8cd0d70":"code","fe6f7a04":"code","e901b044":"code","b6e5bbe9":"code","17ef4c50":"code","94032688":"code","2110851b":"code","cc9b2193":"code","06d6d9a1":"code","d7da9d9f":"code","bd852c4e":"code","dda7c7d3":"code","0b306f2d":"markdown","05b9833a":"markdown","bc9bfea4":"markdown","cf8067f7":"markdown","3623bce6":"markdown","7896c031":"markdown","1e9893d3":"markdown","f6aea505":"markdown","b941f411":"markdown","cd10b4b9":"markdown","2d59e1e2":"markdown","292caf0f":"markdown","eba3908d":"markdown"},"source":{"d84390d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4cb10853":"## load libraries\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nfrom collections import Counter \nimport plotly.graph_objs as go\nfrom plotly import tools\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \nimport plotly \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\ninit_notebook_mode(connected=True)\n\n## load data and remove (students, not-employed)\ndf = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\", low_memory = False)\ndf = df[~df['Q5'].isin([\"Student\", \"Not employed\"])]","908547bd":"## function to identify value counts of a column responses presented in multiple other columns\ndef summate_columns(col, xaxis_def=None):\n    xaxis, yaxis1 = [], []\n    for i in range(1, 20):\n        if col+'_Part_'+str(i) not in df:\n            break\n        \n        doc = dict(df[col+'_Part_'+str(i)].value_counts())\n        if len(doc) > 0:\n            key = list(doc.keys())[0]\n        else:\n            continue \n        xaxis.append(key)\n        \n        if key in doc:\n            yaxis1.append(doc[key])\n        else:\n            yaxis1.append(0)\n            \n    if xaxis_def != None:\n        xaxis = xaxis_def\n    \n    xaxis = xaxis[::-1]\n    xaxis = [_.split(\"(\")[0]+\"    \" for _ in xaxis]\n    yaxis1 = [x*100\/len(df) for x in yaxis1][::-1]\n\n    temp_df = pd.DataFrame()\n    temp_df['xaxis'] = xaxis\n    temp_df['yaxis'] = yaxis1\n    temp_df = temp_df.sort_values(\"yaxis\")\n    temp_df['colors'] = ['#fad46b']*len(temp_df)\n    colors = np.array(temp_df['colors'])\n    colors[-3], colors[-2], colors[-1] = \"#97c6e8\", \"#97c6e8\", \"#97c6e8\"\n    colors[-4] = \"red\"\n    \n    annotations = []\n    for yd, xd in zip(xaxis, yaxis1):\n        annotations.append(dict(xref='x', yref='y',\n                            x=xd+2, y=yd,\n                            text=str(round(xd, 1)) + '%',\n                            font=dict(family='Arial', size=14, color='black'),\n                            showarrow=False))\n    \n    fig = go.Figure(data=[go.Bar(name='', y=temp_df['xaxis'], x=temp_df['yaxis'], orientation='h', marker=dict(color=colors, opacity=0.6))], \n                    layout = go.Layout(title=\"Sources of Learning Data Science\", plot_bgcolor='#fff', paper_bgcolor='#fff', margin=dict(l=100), \n                                       width=900, height=500, legend=dict(orientation=\"h\", x=0.1, y=1.1)))\n    fig.update_layout(barmode='group')\n    \n    annotations += [go.layout.Annotation(x=40, y=9, xref=\"x\", yref=\"y\", text=\"(Most Popular)\", showarrow=False, arrowhead=7, ax=0, ay=-40)]\n    \n    fig.update_layout(annotations=annotations)\n    fig.update_xaxes(showgrid=False, zeroline=False, title=\"% of respondents\")\n    fig.update_yaxes(showgrid=False, zeroline=False)\n    fig.add_shape(go.layout.Shape(type=\"rect\", xref=\"x\", yref=\"paper\", x0=22, x1=48.6, y0=0.75, y1=1, fillcolor=\"gray\", opacity=0.2, layer=\"below\", line_width=0))\n    fig.show()\n    \nsummate_columns(\"Q13\")","c0b1189a":"d = {\n        'Gain More Knowledge': 69.7,\n        'Better Job Roles': 34.5,\n        'Personal Goal': 26.2,\n        'Career Change': 41.8,\n        'Better Salary': 43.5,\n}\n\nxx = [\"Personal Goal\", \"Better Salary\", \"Gain More Knowledge\", \"Better Job Roles\", \"Career Change\"]\nxx = [_ + \"<br>(\" +str(d[_])+ \"%)\" for _ in xx]\nyy = [\"\"]*len(d)\nzz = [13.7, 63, 100, 40, 56]\ncc = ['red', 'green', 'purple', 'orange', \"blue\"] \n\ntrace1 = go.Scatter(x = xx, y = [\"\"]*len(d), mode='markers', name=\"\", marker=dict(color=cc, opacity=0.4, size = zz))\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=100), title='Reasons for pursuing University Degrees',\n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                   showlegend=False)\n\nfig = go.Figure(data=[trace1], layout=layout)\niplot(fig)","2bbb63e8":"university_data = \"\"\"CMU Tepper\t67575\t24\nUC Berkeley\t66150\t20\nGeorgia Tech\t9900\t23\nIllinois-Urbana\t19200\t17.5\nSouth California\t55886\t24\nWisconsin\t30600\t17.5\nBoston Metropolitan\t34400\t24\nJohns Hopkins\t53740\t17.5\nPennsylvania State\t29250\t17.5\nNorthwestern SPS\t53148\t18\nRutgers\t23136\t24\nUCD Dublin\t7575\t36\nTexas A&M\t65000\t24\nArizona State\t39622\t15\nNortheastern\t34200\t24\nOhio\t35250\t20\nRice\t129000\t24\nIndiana Bloomington\t15172\t18\nNotre Dame\t48000\t21\nIIT Chicago\t30985\t16\nSyracuse\t54000\t24\nCalifornia Riverside\t24990\t13\nIowa State\t24000\t21\nOregon State\t23445\t17.5\nMissouri\t34000\t24\nCUNY SPS\t13200\t17.5\nAmerican\t54186\t15\nOklahoma\t26103\t14\nColorado\t39450\t24\nOklahoma State\t33990\t24\nBaker College\t17100\t17.5\nBay Path\t29340\t24\nBellevue\t20700\t17.5\nBlanchardstown\t2320\t24\nCapella\t33360\t12\nCentral Connecticut\t21681\t17.5\nColorado Technical\t28080\t24\nDePaul\t43160\t17.5\nDS TechInstitute\t7900\t9\nDakota State\t13320\t10\nElmhurst College\t25350\t24\nFull Sail\t28008\t12\nLa Salle\t26250\t20\nLewis\t26235\t24\nMaryville St. Louis\t22950\t17.5\nPhoenix\t29316\t17.5\nRegis\t25560\t17.5\nSaint Mary's College\t31946\t24\nSouth Dakota State\t13320\t12\nSaint Joseph's\t17520\t24\nSouthern Methodist\t57084\t28\nSouthern New Hampshire\t22572\t15\nSlippery Rock\t16269\t10\nAlabama Huntsville\t21810\t12\nMaryland College\t24984\t17.5\nVillanova\t43400\t24\nWest Virginia\t30690\t17.5\nNorthcentral\t30530\t23\nEdinburgh Napier\t9060\t33\nDrexel\t56925\t24\nMerrimack College\t28320\t16\nVarna Free\t5391\t12\nJohnson & Wales\t23744\t24\nKent State\t21524\t12\"\"\"\n\nuniversity_data = university_data.split(\"\\n\")\nudf = pd.DataFrame()\nudf['name'] = [_.split(\"\t\")[0] for _ in university_data]\nudf['tution'] = [float(_.split(\"\t\")[1]) for _ in university_data]\nudf['months'] = [float(_.split(\"\t\")[2]) for _ in university_data]\n\nudf1 = udf[udf['months'] > 6]\nudf1 = udf1[udf1['months'] < 25]\nudf1 = udf1[udf1['tution'] < 100000]\nudf1['name1'] = udf1.apply(lambda x : \"\" if x['months'] in [9, 12, 17.5, 24] else x['name'], axis = 1)\n\nfig = go.Figure([go.Bar(x=udf['name'], y=udf['tution'], orientation=\"v\", name=\"Tution Fee\", marker=dict(color=\"orange\", opacity=0.5), ),\n                 go.Scatter(x=udf['name'], name=\"Median Household Income\", y=[59000]*len(udf), mode=\"lines\", marker=dict(color=\"red\"), line=dict(dash='dash')) ])\nfig.update_layout(title=\"Tution Fee : University Degrees in Data Science\", plot_bgcolor='#fff', paper_bgcolor='#fff', legend=dict(orientation=\"h\", x=0.1, y=1.1), \n                 xaxis = dict(tickangle = 45), height=500)\nfig.show()\n\nfig = go.Figure([go.Bar(x=udf['name'], y=udf['months'], orientation=\"v\", marker=dict(color=\"#3498d5\", opacity=0.5, line=dict(color=\"#3498d5\")), name=\"Duration\"),\n                 go.Scatter(x=udf['name'], y=[12]*len(udf), mode=\"lines\", marker=dict(color=\"red\"), line=dict(dash='dash'), name=\"1 Yr\"), \n                 go.Scatter(x=udf['name'], y=[18]*len(udf), mode=\"lines\", marker=dict(color=\"blue\"), line=dict(dash='dash'), name=\"1.5 Yr\"), \n                 go.Scatter(x=udf['name'], y=[24]*len(udf), mode=\"lines\", marker=dict(color=\"orange\"), line=dict(dash='dash'), name=\"2 Yr\"), \n                ])\nfig.update_layout(title=\"Duration : University Degrees in Data Science\", plot_bgcolor='#fff', paper_bgcolor='#fff', legend=dict(orientation=\"h\", x=0.1, y=1.1),\n                   xaxis = dict(tickangle = 45), height=400)\nfig.show()","658f0989":"key1 = \"University Courses (resulting in a university degree)\"\ndf1 = df[df['Q13_Part_10'] == key1]\ndf2 = df[df['Q13_Part_10'] != key1]\n\nnations = [\"United States of America\", \"Canada\", \"United Kingdom of Great Britain and Northern Ireland\", \"Brazil\", \"Russia\", \"Germany\", \"Spain\", \"France\",  \"India\", \"Japan\", \"China\", \"Other\"]\nnation_map = {\"United States of America\" : \"USA\", \"United Kingdom of Great Britain and Northern Ireland\" : \"UK\"}\nplt.figure(figsize=(12,12))\n\nvals = []\nfor j in range(len(nations)):\n    country = nations[j]\n    country_df = df[df['Q3'] == country]\n    ddf1 = country_df[country_df['Q13_Part_10'] == key1]\n    ddf2 = country_df[country_df['Q13_Part_10'] != key1]\n    plt.subplot(4, 4, j+1)\n    \n    if j < 4:\n        colors = [\"#ff8ce0\", \"#60cfe6\"]\n    elif j < 8:\n        colors = [\"#ff8ce0\", \"#89e8a2\"]\n    else:\n        colors = [\"#ff8ce0\", \"#827ec4\"]\n    \n    vals.append(len(ddf1) \/ (len(ddf1) + len(ddf2)))    \n    plt.pie([len(ddf1), len(ddf2)],\n            labels=[\"With Degree\", \"No Degree\"],\n            autopct=\"%1.0f%%\", \n            colors=colors,\n            wedgeprops={\"linewidth\":5,\"edgecolor\":\"white\"})\n    if country in nation_map:\n        country = nation_map[country]\n    plt.title(r\"$\\bf{\" + country + \"}$\")","f88d095c":"col_yes = \"#fad46b\"\ncol_no = \"#97c6e8\"\n\ndef compute_stats(df, col, xaxis=None):\n    agg_df = df[col].value_counts().to_frame()\n    total = sum(agg_df[col])\n    agg_df['percent'] = agg_df[col].apply(lambda x : 100*x \/ total)\n    agg_df = agg_df.reset_index().rename(columns = {col: \"count\", 'index' : col})\n    agg_doc = {}\n    if xaxis != None:\n        for _ in xaxis:\n            try:\n                agg_doc[_] = agg_df[agg_df[col] == _]['percent'].iloc(0)[0]\n            except:\n                agg_doc[_] = 0\n    return agg_doc\n\ndef plot_ver_bars(c, ht=500, annot = True):\n    dxf1 = df1[df1['Q5'].isin([\"Data Scientist\"])]\n    dxf2 = df2[df2['Q5'].isin([\"Data Scientist\"])]\n\n    count_df1 = dxf1[dxf1[\"Q3\"].isin([c])]\n    count_df2 = dxf2[dxf2[\"Q3\"].isin([c])]\n    col = \"Q10\"\n    xaxis = [\"$0-999\", \"1,000-1,999\", \"2,000-2,999\", \"3,000-3,999\", \"4,000-4,999\", \"5,000-7,499\", \"7,500-9,999\", \"10,000-14,999\", \"15,000-19,999\", \"20,000-24,999\", \"25,000-29,999\", \"30,000-39,999\", \"40,000-49,999\", \"50,000-59,999\", \"60,000-69,999\", \"70,000-79,999\", \"80,000-89,999\", \"90,000-99,999\", \"100,000-124,999\", \"125,000-149,999\", \"150,000-199,999\", \"200,000-249,999\", \"250,000-299,999\", \"300,000-500,000\", \"> $500,000\"]\n    t1_doc = compute_stats(count_df1, col, xaxis=xaxis)\n    t2_doc = compute_stats(count_df2, col, xaxis=xaxis)\n\n    ## plot the bar chart\n    xaxis1 = [\"$0K\", \"1 - 1K\", \"2 - 2K\", \"3 - 3K\", \"4 - 4K\", \"5 - 7.5K\", \"7.5 - 10K\", \"10 - 15K\", \"15 - 20K\", \"20 - 25K\", \"25 - 30K\", \"30 - 40K\", \"40 - 50K\", \"50 - 60K\", \"60 - 70K\", \"70 - 80K\", \"80 - 90K\", \"90 - 100K\", \"100 - 125K\", \"125 - 150K\", \"150 - 200K\", \"200 - 250K\", \"250 - 300K\", \"300 - 500K\", \"> $500K\"]\n    fig = go.Figure(data=[\n                go.Bar(name='Without University Degree', x=xaxis1[7:], y=[t2_doc[_] for _ in xaxis][7:], marker=dict(color=col_no, opacity=0.8)),\n                go.Bar(name='With University Degree', x=xaxis1[7:], y=[t1_doc[_] for _ in xaxis][7:], marker=dict(color=col_yes, opacity=0.8))])\n    fig.update_layout(barmode='group', title=\"Data Scientists in <b>\"+c+\"<\/b>\", \n                      yaxis = dict(title=\"% of respondents\"), xaxis = dict(title=\"US$\"), height=ht,\n                      legend=dict(orientation=\"h\", x=0.1, y=1.1), plot_bgcolor='#fff', paper_bgcolor='#fff')\n                                 \n    fig.update_xaxes(showgrid=False, zeroline=False)\n    fig.update_yaxes(showgrid=False, zeroline=False)\n    if annot == True:\n        fig.update_layout(annotations=[go.layout.Annotation(x=15.5, y=13, xref=\"x\", yref=\"y\", \n                                                       text=\"More % of Individuals <br> Without University Degrees <br>Earning > $150K\", \n                                                       showarrow=True, arrowhead=7, ax=0, ay=-40), \n                                 go.layout.Annotation(x=11, y=24, xref=\"x\", yref=\"y\", \n                                                       text=\"With University Degrees <br> Earning More\", \n                                                       showarrow=True, arrowhead=7, ax=0, ay=-40)])\n        fig.add_shape(go.layout.Shape(\n                    type=\"rect\",\n                    xref=\"x\",\n                    yref=\"paper\",\n                    x0=12.5,\n                    x1=17.5,\n                    y0=0.03,\n                    y1=1,\n                    fillcolor=\"red\",\n                    opacity=0.1,\n                    layer=\"below\",\n                    line_width=0))\n        fig.add_shape(go.layout.Shape(\n                    type=\"rect\",\n                    xref=\"x\",\n                    yref=\"paper\",\n                    x0=10.5,\n                    x1=12.4,\n                    y0=0.5,\n                    y1=1,\n                    fillcolor=\"green\",\n                    opacity=0.1,\n                    layer=\"below\",\n                    line_width=0))\n    \n    \n    fig.show()\n    return count_df1, count_df2\n\nnations = [\"United States of America\", \"Germany\"]\nc1, c2 = plot_ver_bars(nations[0])","e8cd0d70":"salaries = [\"100,000-124,999\", \"125,000-149,999\", \"150,000-199,999\", \"200,000-249,999\", \"300,000-500,000\"]\ngroup1 = c1[c1[\"Q10\"].isin(salaries)]\ngroup2 = c2[c2[\"Q10\"].isin(salaries)]\n\nvc1 = group1['Q15'].value_counts().to_frame().reset_index()\nvc1['percent'] = vc1['Q15'].apply(lambda x : 100*x \/ sum(vc1['Q15']))\n\nvc2 = group2['Q15'].value_counts().to_frame().reset_index()\nvc2['percent'] = vc2['Q15'].apply(lambda x : 100*x \/ sum(vc2['Q15']))\n\norder = [\"< 1 years\", \"1-2 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\ndoc1, doc2 = {}, {}\nfor _, r in vc1.iterrows():\n    doc1[r['index']] = r['percent']\nfor _, r in vc2.iterrows():\n    doc2[r['index']] = r['percent']\n\ntrace1 = go.Bar(x=order, y=[doc1[_] for _ in order], name=\"\", orientation=\"v\", marker=dict(color=col_yes, opacity=0.8))\ntrace2 = go.Bar(x=order, y=[doc2[_] for _ in order], name=\"\", orientation=\"v\", marker=dict(color=col_no, opacity=0.8))\n\nvc1 = group1['Q1'].value_counts().to_frame().reset_index()\nvc1['percent'] = vc1['Q1'].apply(lambda x : 100*x \/ sum(vc1['Q1']))\n\nvc2 = group2['Q1'].value_counts().to_frame().reset_index()\nvc2['percent'] = vc2['Q1'].apply(lambda x : 100*x \/ sum(vc2['Q1']))\n\norder = ['22-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59']\ndoc1, doc2 = {}, {}\nfor _, r in vc1.iterrows():\n    doc1[r['index']] = r['percent']\nfor _, r in vc2.iterrows():\n    doc2[r['index']] = r['percent']\n\ntrace3 = go.Scatter(x=order, y=[doc1[_] for _ in order], name=\"\", mode=\"lines+markers\", marker=dict(color=col_yes, opacity=0.8))\ntrace4 = go.Scatter(x=order, y=[doc2[_] for _ in order], name=\"\", mode=\"lines+markers\", marker=dict(color=col_no, opacity=0.8))\n\n\n### ML Exp \n\nvc1 = group1['Q23'].value_counts().to_frame().reset_index()\nvc1['percent'] = vc1['Q23'].apply(lambda x : 100*x \/ sum(vc1['Q23']))\n\nvc2 = group2['Q23'].value_counts().to_frame().reset_index()\nvc2['percent'] = vc2['Q23'].apply(lambda x : 100*x \/ sum(vc2['Q23']))\n\norder = ['< 1 years', '1-2 years', '2-3 years', '3-4 years', '4-5 years', '5-10 years', '10-15 years', '15-20 years', '20+ years']\ndoc1, doc2 = {}, {}\nfor _, r in vc1.iterrows():\n    doc1[r['index']] = r['percent']\nfor _, r in vc2.iterrows():\n    doc2[r['index']] = r['percent']\n\ntrace5 = go.Bar(x=order, y=[doc1[_] if _ in doc1 else 0 for _ in order], name=\"With University Degree\", orientation=\"v\", marker=dict(color=col_yes, opacity=0.8))\ntrace6 = go.Bar(x=order, y=[doc2[_] if _ in doc1 else 0 for _ in order], name=\"Without University Degree\", orientation=\"v\", marker=dict(color=col_no, opacity=0.8))\n\nfig = make_subplots(rows=2, cols=2, specs=[[{}, {}],\n           [{\"colspan\": 2}, None]], subplot_titles=(\"Machine Learning Experience (In Years)\", \"Coding Experience (In Years)\", \"Age : Data Scientists earning >150K USD\"))\nfig.add_trace(trace6, 1, 1)\nfig.add_trace(trace5, 1, 1)\nfig.add_trace(trace2, 1, 2)\nfig.add_trace(trace1, 1, 2)\nfig.add_trace(trace4, 2, 1)\nfig.add_trace(trace3, 2, 1)\nfig.update_layout(height=800, title=\"Data Scientists in USA earning >150K USD : Key Characteristics\", plot_bgcolor='#fff', paper_bgcolor='#fff',\n                  yaxis = dict(title=\"% of respondents\"), legend=dict(orientation=\"h\", x=0.3, y=1.2))\nfig.show()","fe6f7a04":"_, _ = plot_ver_bars(\"Germany\", ht=300, annot = False)\n_, _ = plot_ver_bars(\"Canada\", ht=300, annot = False)","e901b044":"pd.options.mode.chained_assignment = None  # default='warn'\ndef diff_workroles(col, xaxis_def=None, get=False, sum_choices = False):\n    xaxis, yaxis1, yaxis2 = [], [], []\n    for i in range(1, 20):\n        if col+'_Part_'+str(i) not in df1:\n            break\n            \n        c1 = df1[df1[\"Q3\"].isin([\"United States of America\"])]\n        c2 = df2[df2[\"Q3\"].isin([\"United States of America\"])]\n\n        c1 = c1[c1['Q5'].isin([\"Data Scientist\"])]\n        c2 = c2[c2['Q5'].isin([\"Data Scientist\"])]\n        \n        doc1 = dict(c1[col+'_Part_'+str(i)].value_counts())\n        doc2 = dict(c2[col+'_Part_'+str(i)].value_counts())\n\n        if len(doc1) > 0:\n            key = list(doc1.keys())[0]\n        elif len(doc2) > 0:\n            key = list(doc2.keys())[0]\n        else:\n            continue \n        xaxis.append(key)\n        \n        if key in doc1:\n            yaxis1.append(doc1[key])\n        else:\n            yaxis1.append(0)\n        if key in doc2:\n            yaxis2.append(doc2[key])\n        else:\n            yaxis2.append(0)\n            \n    if xaxis_def != None:\n        xaxis = xaxis_def\n    \n    \n    xaxis = xaxis[::-1]\n    \n    ln_c1 = len(c1)\n    ln_c2 = len(c2)\n    if sum_choices == True:\n        ln_c1 = sum(yaxis1)\n        ln_c2 = sum(yaxis2)\n    \n    yaxis1 = [x*100\/ln_c1 for x in yaxis1][::-1]\n    yaxis2 = [x*100\/ln_c2 for x in yaxis2][::-1]\n    if get == True:\n        return xaxis, yaxis1, yaxis2\n    \n    tra1 = go.Bar(name='With Degree', y=xaxis, x=yaxis1, orientation='h', width=0.3, marker=dict(color=col_yes, opacity=0.8))\n    tra2 = go.Bar(name='Without Degree', y=xaxis, x=yaxis2, orientation='h', width=0.3, marker=dict(color=col_no, opacity=0.8))\n    return tra1, tra2\n\nxaxis = [\"Data Analysis \/ Exploration    \",\n         \"Build\/Run Data Infrastructure    \",\n         \"Build Machine Learning Prototypes    \",\n         \"Build Machine Learning Services    \",\n         \"Experimentation and Improvements    \",\n         \"Research \/ SOTA Models    \",\n         \"None of these activities    \",\n         \"Other\"]\n\nx, y1, y2 = diff_workroles(\"Q9\", xaxis, get=True)\ntrace1 = go.Bar(name='With Degree', y=x[2:], x=y1[2:], orientation='h', width=0.3, marker=dict(color=col_yes, opacity=0.8))\ntrace2 = go.Bar(name='Without Degree', y=x[2:], x=y2[2:], orientation='h', width=0.3, marker=dict(color=col_no, opacity=0.8))\n\nx, y1, y2 = diff_workroles(\"Q9\", xaxis, sum_choices = True, get=True)\ntrace5 = go.Bar(name='', y=x[2:], x=y1[2:], orientation='h', width=0.3, marker=dict(color=col_yes, opacity=0.5))\ntrace6 = go.Bar(name='', y=x[2:], x=y2[2:], orientation='h', width=0.3, marker=dict(color=col_no, opacity=0.5))\n\n\ndef summate(row, cols):\n    count = 0\n    for c in cols:\n        if str(row[c]) == \"nan\":\n            continue\n        count += 1\n    return count\n\ndef skill_variety(col, df):\n    dfx = df[df[\"Q3\"].isin([\"United States of America\"])]\n\n    cols = []\n    for i in range(1, 20):\n        if col+'_Part_'+str(i) not in df:\n            break\n        doc = dict(dfx[col+'_Part_'+str(i)].value_counts())\n        if any(_ in doc for _ in [\"None of these activities are an important part of my role at work\", \"Other\"]):\n            continue\n        cols.append(col+'_Part_'+str(i))            \n    dfx[col+'_sum'] = dfx.apply(lambda x : summate(x, cols), axis=1)\n    return dfx\n\norder = [0,1,2,3,4,5,6]\ndoc1, doc2 = {}, {}\nc1 = skill_variety(\"Q9\", df1)\nvc1 = c1['Q9_sum'].value_counts().to_frame().reset_index()\nvc1['percent'] = vc1['Q9_sum'].apply(lambda x : 100*x \/ len(c1))\nfor _, r in vc1.iterrows():\n    doc1[r['index']] = r['percent']\n\nc2 = skill_variety(\"Q9\", df2)\nvc2 = c2['Q9_sum'].value_counts().to_frame().reset_index()\nvc2['percent'] = vc2['Q9_sum'].apply(lambda x : 100*x \/ len(c2))\nfor _, r in vc2.iterrows():\n    doc2[r['index']] = r['percent']\n\ntrace3 = go.Scatter(x=order, y=[doc1[_] for _ in order], name=\"With University Degrees\", mode=\"markers+lines\", marker=dict(color=col_yes, opacity=0.8))\ntrace4 = go.Scatter(x=order, y=[doc2[_] for _ in order], name=\"Without University Degrees\", mode=\"markers+lines\", marker=dict(color=col_no, opacity=0.8))\n\nfig = make_subplots(rows=1, cols=2, \n                    subplot_titles=(\"A: % of respondents with different responsibilities\", \n                                    \"B: % of times a responsibility is selected\"))\nfig.add_trace(trace1, 1, 1)\nfig.add_trace(trace2, 1, 1)\nfig.add_trace(trace5, 1, 2)\nfig.add_trace(trace6, 1, 2)\nfig.update_layout(title=\"\", plot_bgcolor='#fff', paper_bgcolor='#fff',\n                  xaxis1 = dict(title=\"% of respondents\"), \n                  xaxis2 = dict(title=\"% this responsibility is choosen\"), yaxis2 = dict(showticklabels=False),\n                  legend=dict(orientation=\"h\", x=0.1, y=1.2))\nfig.show()","b6e5bbe9":"fig = go.Figure([trace3, trace4])\nfig.update_layout(title=\"Number of responsibilities Data Scientists do\", plot_bgcolor='#fff', paper_bgcolor='#fff',\n                  yaxis = dict(title=\"% of respondents\", showgrid=False),xaxis = dict(title=\"Number of job responsibilities\",showgrid=False), legend=dict(orientation=\"h\", x=0.3, y=1.2))\nfig.update_layout(annotations=[go.layout.Annotation(x=1, y=13, xref=\"x\", yref=\"y\", \n                                               text=\"Specialists : Involved in <br> only one activity\", \n                                               showarrow=True, arrowhead=3, ax=0, ay=-60), \n                         go.layout.Annotation(x=6, y=10, xref=\"x\", yref=\"y\", \n                                               text=\"Generalists : Involved in <br> every activity\", \n                                               showarrow=True, arrowhead=3, ax=0, ay=-60)])\nfig.add_shape(go.layout.Shape(type=\"circle\", xref=\"x\", yref=\"paper\", x0=0.8, x1=1.23, y0=0.1, y1=0.3, fillcolor=\"PaleTurquoise\", opacity=0.7, layer='below', line_color=\"#34eb71\", line_width=2))\nfig.add_shape(go.layout.Shape(type=\"circle\", xref=\"x\", yref=\"paper\", x0=6.2, x1=5.8, y0=0.01, y1=0.2, fillcolor=\"PaleTurquoise\", opacity=0.7, layer='below', line_color=\"#34eb71\", line_width=2))\nfig.show()","17ef4c50":"def generate_bubb_diff(col, title):\n    # x, y1, y2 = diff_workroles(\"Q9\", xaxis, sum_choices = True, get=True)\n    x, y1, y2 = diff_workroles(col, get = True, sum_choices = True)\n    \n    x = [_.split(\"(\")[0].strip() for _ in x]\n    x = x[2:]\n    y1 = y1[2:]\n    y2 = y2[2:]\n\n    ss1 = y1 + y2 + [np.abs(a-b) for a,b in zip(y1, y2)]\n    ss1 = [str(round(_, 1))+\"%\" for _ in y1+y2] + [round(a-b, 2) for a,b in zip(y1, y2)]\n    \n    diffs1, diffs2 = [], []\n    for i in range(len(y1)):\n        d = y1[i] - y2[i]\n        if d > 0:\n            diffs1.append(2.5+d)\n            diffs2.append(3)\n        else:\n            diffs2.append(2.5-d)\n            diffs1.append(3)\n    ss = [_*diffs1[i] for i, _ in enumerate(y1)]\n    ss += [_*diffs2[i] for i, _ in enumerate(y2)]\n    ss += [np.abs(a-b)*10 for a,b in zip(y1, y2)]\n    \n    trace1 = go.Scatter(x = [\"With\"]*len(x)+[\"Without\"]*len(x)+[\"Difference\"]*len(x), y = x+x+x, mode='markers+text', textposition=\"middle right\", text=ss1, name=\"\", \n                        marker=dict(color=[col_yes]*len(x)+[col_no]*len(x), opacity=0.8, size = ss))\n    layout = go.Layout(barmode='stack', margin=dict(l=200), height=900, title=title,\n                       legend = dict(orientation=\"h\", x=0.1, y=1.15), plot_bgcolor='#fff', paper_bgcolor='#fff', \n                       showlegend=False)\n\n    fig = go.Figure(data=[trace1], layout=layout)\n    iplot(fig)\n    \ngenerate_bubb_diff(\"Q24\", title = \"Machine Learning Models Used\")\ngenerate_bubb_diff(\"Q28\", \"Tools \/ Techniques used\")","94032688":"c1 = skill_variety(\"Q24\", df1)\nc2 = skill_variety(\"Q24\", df2)\n\nvc1 = c1['Q24_sum'].value_counts().to_frame().reset_index()\nvc2 = c2['Q24_sum'].value_counts().to_frame().reset_index()\n\nvc1['percent'] = vc1['Q24_sum'].apply(lambda x : 100*x \/ len(c1))\nvc2['percent'] = vc2['Q24_sum'].apply(lambda x : 100*x \/ len(c2))\n\nvc1 = vc1.sort_values(\"index\")[1:]\nvc2 = vc2.sort_values(\"index\")[1:]\n\ntr1 = go.Scatter(x=vc1['index'], y=vc1['percent'], name='with',fill='tozeroy', marker=dict(color=col_yes),showlegend=False, opacity=0.8)\ntr2 = go.Scatter(x=vc2['index'], y=vc2['percent'], name='without',fill='tozeroy', marker=dict(color=col_no),showlegend=False, opacity=0.8)\n\nc1 = skill_variety(\"Q28\", df1)\nc2 = skill_variety(\"Q28\", df2)\n\nvc1 = c1['Q28_sum'].value_counts().to_frame().reset_index()\nvc2 = c2['Q28_sum'].value_counts().to_frame().reset_index()\n\nvc1['percent'] = vc1['Q28_sum'].apply(lambda x : 100*x \/ len(c1))\nvc2['percent'] = vc2['Q28_sum'].apply(lambda x : 100*x \/ len(c2))\n\nvc1 = vc1.sort_values(\"index\")[1:]\nvc2 = vc2.sort_values(\"index\")[1:]\n\ntr3 = go.Scatter(x=vc1['index'], y=vc1['percent'], name='with',fill='tozeroy', marker=dict(color=col_yes),showlegend=False, opacity=0.8)\ntr4 = go.Scatter(x=vc2['index'], y=vc2['percent'], name='without',fill='tozeroy', marker=dict(color=col_no),showlegend=False, opacity=0.8)\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"# of ML Techniques\", \"# of ML Algorithms\"))\nfig.add_trace(tr1, 1, 1)\nfig.add_trace(tr2, 1, 1)\nfig.add_trace(tr3, 1, 2)\nfig.add_trace(tr4, 1, 2)\n\nfig.update_layout(height=400, showlegend = False, plot_bgcolor='#fff', paper_bgcolor='#fff',)\nfig.show()","2110851b":"generate_bubb_diff(\"Q17\", \"Notebooks Used\")\ngenerate_bubb_diff(\"Q16\", \"Editors \/ Tools Used\")","cc9b2193":"c1 = skill_variety(\"Q17\", df1)\nc2 = skill_variety(\"Q17\", df2)\n\nvc1 = c1['Q17_sum'].value_counts().to_frame().reset_index()\nvc2 = c2['Q17_sum'].value_counts().to_frame().reset_index()\n\nvc1['percent'] = vc1['Q17_sum'].apply(lambda x : 100*x \/ len(c1))\nvc2['percent'] = vc2['Q17_sum'].apply(lambda x : 100*x \/ len(c2))\n\nvc1 = vc1.sort_values(\"index\")[1:]\nvc2 = vc2.sort_values(\"index\")[1:]\n\ntr1 = go.Scatter(x=vc1['index'], y=vc1['percent'], name='with',fill='tozeroy', marker=dict(color=col_yes),showlegend=False, opacity=0.8)\ntr2 = go.Scatter(x=vc2['index'], y=vc2['percent'], name='without',fill='tozeroy', marker=dict(color=col_no),showlegend=False, opacity=0.8)\n\nc1 = skill_variety(\"Q16\", df1)\nc2 = skill_variety(\"Q16\", df2)\n\nvc1 = c1['Q16_sum'].value_counts().to_frame().reset_index()\nvc2 = c2['Q16_sum'].value_counts().to_frame().reset_index()\n\nvc1['percent'] = vc1['Q16_sum'].apply(lambda x : 100*x \/ len(c1))\nvc2['percent'] = vc2['Q16_sum'].apply(lambda x : 100*x \/ len(c2))\n\nvc1 = vc1.sort_values(\"index\")[1:]\nvc2 = vc2.sort_values(\"index\")[1:]\n\ntr3 = go.Scatter(x=vc1['index'], y=vc1['percent'], name='with',fill='tozeroy', marker=dict(color=col_yes),showlegend=False, opacity=0.8)\ntr4 = go.Scatter(x=vc2['index'], y=vc2['percent'], name='without',fill='tozeroy', marker=dict(color=col_no),showlegend=False, opacity=0.8)\n    \nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"# of Cloud Notebooks\", \"# of Platforms\"))\nfig.add_trace(tr1, 1, 1)\nfig.add_trace(tr2, 1, 1)\nfig.add_trace(tr3, 1, 2)\nfig.add_trace(tr4, 1, 2)\n\nfig.update_layout(height=400, showlegend = False, plot_bgcolor='#fff', paper_bgcolor='#fff',)\nfig.show()","06d6d9a1":"from IPython.core.display import display, HTML, Javascript\nfrom collections import Counter \nimport IPython.display\nimport json\n\nnaming_doc = {\n              'Q25': 'ML Tools',\n              'Q29': 'Cloud Computing Platforms',\n              'Q30': 'Cloud Computing Products',\n              'Q31': 'BigData \/ Analytics Products',\n              'Q32': 'ML Products',\n              'Q33': 'Automated ML Products',\n              'Q34': 'Relational Database'}\n\nresults = {'name' : 'flare', \"children\" : [], \"size\":\"\"}\nfor col in naming_doc:\n    x, y1, y2 = diff_workroles(col, get = True)\n    \n    resp = {'name' : naming_doc[col], \"children\" : [], \"size\": \"\"}\n    for k,v in zip(x, y1):\n        if \"none\" in k.lower():\n            continue\n        k = str(k.strip()).split(\"(\")[0]\n        k = \"\\n\".join(k.split())\n        doc = {'name' : k, \"size\": round(v, 1)}\n        resp['children'].append(doc)\n\n    results['children'].append(resp)\n\nwith open('output.json', 'w') as outfile:  \n    json.dump(results, outfile)\n\nhtmlt1 = \"\"\"<!DOCTYPE html><meta charset=\"utf-8\"><style>.node {cursor: pointer;}.node:hover {stroke: #fff;stroke-width: 1.5px;}.node--leaf {fill: white;}\n.label {font: 11px \"Helvetica Neue\", Helvetica, Arial, sans-serif;text-anchor: middle;text-shadow: 0 1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff, 0 -1px 0 #fff;}\n.label,.node--root,.node--leaf {pointer-events: none;}<\/style><svg id=\"two\" width=\"760\" height=\"760\"><\/svg>\n\"\"\"\njs_t1=\"\"\"require.config({\n    paths: {\n        d3: \"https:\/\/d3js.org\/d3.v4.min\"\n     }\n });\nrequire([\"d3\"], function(d3) {\nvar svg = d3.select(\"#two\"),\n    margin = 20,\n    diameter = +svg.attr(\"width\"),\n    g = svg.append(\"g\").attr(\"transform\", \"translate(\" + diameter \/ 2 + \",\" + diameter \/ 2 + \")\"),\n    color = d3.scaleQuantize()\n    .domain([-2,2])\n    .range([\"#fad46b\", \"#fad46b\", \"#fad46b\", \"#fad46b\"]),\n    \n    pack = d3.pack().size([diameter - margin, diameter - margin]).padding(2);\nd3.json(\"output.json\", function(t, n) {\nif (t) throw t;\nvar r, e = n = d3.hierarchy(n).sum(function(t) {\n        return t.size\n    }).sort(function(t, n) {\n        return n.value - t.value\n    }),\n    a = pack(n).descendants(),\n    i = g.selectAll(\"circle\").data(a).enter().append(\"circle\").attr(\"class\", function(t) {\n        return t.parent ? t.children ? \"node\" : \"node node--leaf\" : \"node node--root\"\n    }).style(\"fill\", function(t) {\n        return t.children ? color(t.depth) : null\n    }).on(\"click\", function(t) {\n        e !== t && (l(t), d3.event.stopPropagation())\n    }),\n    o = (g.selectAll(\"text\").data(a).enter().append(\"text\").attr(\"class\", \"label\").style(\"fill-opacity\", function(t) {\n        return t.parent === n ? 1 : 0\n    }).style(\"display\", function(t) {\n        return t.parent === n ? \"inline\" : \"none\"\n    }).text(function(t) {\n        return t.data.name + \": \" + t.data.size + \"%\"\n    }), g.selectAll(\"circle,text\"));\n\nfunction l(t) {\n    e = t, d3.transition().duration(d3.event.altKey ? 7500 : 750).tween(\"zoom\", function(t) {\n        var n = d3.interpolateZoom(r, [e.x, e.y, 2 * e.r + margin]);\n        return function(t) {\n            c(n(t))\n        }\n    }).selectAll(\"text\").filter(function(t) {\n        return t.parent === e || \"inline\" === this.style.display\n    }).style(\"fill-opacity\", function(t) {\n        return t.parent === e ? 1 : 0\n    }).on(\"start\", function(t) {\n        t.parent === e && (this.style.display = \"inline\")\n    }).on(\"end\", function(t) {\n        t.parent !== e && (this.style.display = \"none\")\n    })\n}\n\nfunction c(n) {\n    var e = diameter \/ n[2];\n    r = n, o.attr(\"transform\", function(t) {\n        return \"translate(\" + (t.x - n[0]) * e + \",\" + (t.y - n[1]) * e + \")\"\n    }), i.attr(\"r\", function(t) {\n        return t.r * e\n    })\n}\nsvg.style(\"background\", color(-1)).on(\"click\", function() {\n    l(n)\n}), c([n.x, n.y, 2 * n.r + margin])\n});\n});\"\"\"\n\nh = display(HTML(\"<i>Note: The following chart is interactive, Click on the Clusters to view more details<\/i>\"))\nh = display(HTML(htmlt1))\nj = IPython.display.Javascript(js_t1)\nIPython.display.display_javascript(j)","d7da9d9f":"results = {'name' : 'flare', \"children\" : [], \"size\":\"\"}\nfor col in naming_doc:\n    x, y1, y2 = diff_workroles(col, get = True)\n    resp = {'name' : naming_doc[col], \"children\" : [], \"size\": \"\"}\n    for k,v in zip(x, y2):\n        if \"none\" in k.lower():\n            continue\n        k = str(k.strip()).split(\"(\")[0]\n        k = \"\\n\".join(k.split())\n        doc = {'name' : k, \"size\": round(v, 1)}\n        resp['children'].append(doc)\n    results['children'].append(resp)\n\nwith open('output1.json', 'w') as outfile:  \n    json.dump(results, outfile)\n\nhtmlt1 = \"\"\"<svg id=\"three\" width=\"760\" height=\"760\"><\/svg>\"\"\"\njs_t1=\"\"\"require([\"d3\"], function(d3) {\nvar svg1 = d3.select(\"#three\"),\n    margin = 20,\n    diameter = +svg1.attr(\"width\"),\n    g = svg1.append(\"g\").attr(\"transform\", \"translate(\" + diameter \/ 2 + \",\" + diameter \/ 2 + \")\"),\n    color = d3.scaleQuantize()\n    .domain([-2,2])\n    .range([\"#97c6e8\", \"#97c6e8\", \"#97c6e8\", \"#97c6e8\"]),\n    \n    pack = d3.pack().size([diameter - margin, diameter - margin]).padding(2);\n\n\nd3.json(\"output1.json\", function(t, n) {\nif (t) throw t;\nvar r, e = n = d3.hierarchy(n).sum(function(t) {\n        return t.size\n    }).sort(function(t, n) {\n        return n.value - t.value\n    }),\n    a = pack(n).descendants(),\n    i = g.selectAll(\"circle\").data(a).enter().append(\"circle\").attr(\"class\", function(t) {\n        return t.parent ? t.children ? \"node\" : \"node node--leaf\" : \"node node--root\"\n    }).style(\"fill\", function(t) {\n        return t.children ? color(t.depth) : null\n    }).on(\"click\", function(t) {\n        e !== t && (l(t), d3.event.stopPropagation())\n    }),\n    o = (g.selectAll(\"text\").data(a).enter().append(\"text\").attr(\"class\", \"label\").style(\"fill-opacity\", function(t) {\n        return t.parent === n ? 1 : 0\n    }).style(\"display\", function(t) {\n        return t.parent === n ? \"inline\" : \"none\"\n    }).text(function(t) {\n        return t.data.name + \": \" + t.data.size + \"%\"\n    }), g.selectAll(\"circle,text\"));\n\nfunction l(t) {\n    e = t, d3.transition().duration(d3.event.altKey ? 7500 : 750).tween(\"zoom\", function(t) {\n        var n = d3.interpolateZoom(r, [e.x, e.y, 2 * e.r + margin]);\n        return function(t) {\n            c(n(t))\n        }\n    }).selectAll(\"text\").filter(function(t) {\n        return t.parent === e || \"inline\" === this.style.display\n    }).style(\"fill-opacity\", function(t) {\n        return t.parent === e ? 1 : 0\n    }).on(\"start\", function(t) {\n        t.parent === e && (this.style.display = \"inline\")\n    }).on(\"end\", function(t) {\n        t.parent !== e && (this.style.display = \"none\")\n    })\n}\n\nfunction c(n) {\n    var e = diameter \/ n[2];\n    r = n, o.attr(\"transform\", function(t) {\n        return \"translate(\" + (t.x - n[0]) * e + \",\" + (t.y - n[1]) * e + \")\"\n    }), i.attr(\"r\", function(t) {\n        return t.r * e\n    })\n}\nsvg1.style(\"background\", color(-1)).on(\"click\", function() {\n    l(n)\n}), c([n.x, n.y, 2 * n.r + margin])\n});\n});\"\"\"\n\nh = display(HTML(\"<i>Note: The following chart is interactive, Click on the Clusters to view more details<\/i>\"))\nh = display(HTML(htmlt1))\nj = IPython.display.Javascript(js_t1)\nIPython.display.display_javascript(j)","bd852c4e":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import accuracy_score\nimport eli5\nnp.random.seed(0)\n\ncols = list(df.columns)\ncols = [c for c in cols if \"TEXT\" not in c][1:]\ntempdf = df[cols][1:].fillna(0)\n\n## prepare the data\nsingle_vars, multi_vars = [], []\nfor c in tempdf.columns:\n    if len(tempdf[c].value_counts()) == 2:\n        tempdf[c] = tempdf[c].apply(lambda x : 1 if x != 0 else x)\n        single_vars.append(c)\n    else:\n        multi_vars.append (c)\none_hot_df = pd.get_dummies(tempdf[multi_vars])\none_hot_df = pd.concat([one_hot_df, tempdf[single_vars]], axis=1)\n\n## train a simple RF\nignore_cols = [\"Q13_Part_10\"]#\"Q11\", \"Q13\", \"Q10\", \"Q5\", \"Q4\", \"Q1\", \"Q2\", \"Q3\"]\nfeatures = []\nfor x in one_hot_df.columns:\n    if any(_ in x for _ in ignore_cols):\n        continue\n    features.append(x)\n\nX = one_hot_df[features]\ny = one_hot_df[\"Q13_Part_10\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 2)\nmodel = RandomForestClassifier(n_estimators=50, random_state=0).fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint (accuracy_score(y_pred, y_test))","dda7c7d3":"pfs = [\"University Courses\", \"Kaggle Learn\"]\nroles = [\"Data Analysis\/Exploration\", \"Build Infrastructures\", \"Build ML Services\", \"Build ML Prototypes\", \"Experimentation\", \"Research\"]\nds = [\"Data Scientist\", \"Data Analyst\", \"Business Analyst\"]\nvals = [0.5657851779301338, 0.3480248122755468, 0.4975514201762977, 0.3382304929807378, 0.3813254978778975, 0.25334639242572643]\nvals += [0.5408446362515413, 0.32336621454993836, 0.46824907521578296, 0.3153514180024661 ,0.3594327990135635, 0.21393341553637485]\n# vals += [0.5001363512407963, 0.30160894464139626, 0.38396509408235613, 0.27188437414780475, 0.2877011180801745, 0.18271066266703026]\nvals = [100*_ for _ in vals]\n\nfig = go.Figure(data=[go.Sankey(\n    node = dict(\n      pad = 50,\n      thickness = 10,\n      line = dict(color = \"blue\", width = 0.1),\n      label =  pfs +  roles + ds,\n      color = \"blue\"\n    ),\n    link = dict(\n      source = [0, 1]*len(roles) + [2, 3,4,5,6,7]*len(ds),\n      target = [2]*len(pfs) + [3]*len(pfs) + [4]*len(pfs) + [5]*len(pfs) + [6]*len(pfs) + [7]*len(pfs) + [8]*len(pfs)\n                + [9]*len(roles) + [10]*len(roles) + [11]*len(roles),\n      value = vals\n  ))])\n\nfig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\nfig.show()","0b306f2d":"The chart shows that there is a considerable number of individuals in each bracket of the machine learning experience, coding experience, and age. The charts are not skewed in a particular bracket.\nWith more number of years in experience for coding and machine learning (greater than 5), We see that relatively a higher percentage of people are there earning more than 150K without university degrees.\nMore number of individuals who are aged less than 34 years and with a university degree earn greater than 150K. Good to see that even more percentage of young individuals, aged 22-24 also get higher compensation in the USA.\nThese trends are also similar in other countries. According to many links, top countries for higher education university degrees are the USA, Germany, Canadas etc.\n\n","05b9833a":"In most of the countries, more than 1\/4th of the respondents completed their university degrees to becoming a data scientist. In the United States, about 27% of the individuals who were part of this survey completed their university degrees.\nCountries with the highest proportion of data scientists with university degrees are 'Tunisia', 'Austria', 'New Zealand' and 'Greece' with over 40% of the individuals completing university degrees.\nOn the other hand, countries like 'Japan', 'Nigeria', 'Belarus', and 'Algeria' shows a lower number (less than 10%) of individuals completing university degrees.\nAmong the genders, female respondents have a higher number for completing university degrees than male respondents. 23% of the female respondents completed their university degrees and only 20% of the male respondents completed those degrees.\n3. Are there a Significant Differences - With or Without University Degrees ?\nAccording to Forbes, Most people with data science job titles don\u2019t have these new degrees. I also looked at the profiles of a few data scientist in my Linkedin Network and observed that not all of them have data science degrees. People tend to take different paths - some have degrees in business, economics, maths etc, while some have specialed data science degrees, and some have no university degree. But all of them are working in good organizations with good job roles. In the next section, let's look at the key insights from the Survey Data Analysis. The focus of the analysis is to compare the two groups - individuals who completed university degree vs those without for becoming a data scientist, and measure the key differences (if any).\n\nMainly, We will look at three perspectives: Are there fairly equal percentage of individuals from two groups\n\nWith every compensation bracket.\nFor each type of job role or activity.\nFor each type of activity they do on the daily basis.\n3.1 Compensation","bc9bfea4":"2019 was an important year for me, Not only I got engaged this year but also I completed my higher education degree. After working for several years in the industry I decided to take a short break, go back to academics and pursue higher education. No doubt, it was one of the best learning experiences I had best but there was also a huge investment of time and money. Many people contact me regularly asking about my experience and viewpoint about such degrees. They often ask questions like - *whether is it worth spending huge chunks of money for such degrees?* Well, there is no fixed answer for these type of questions because every individual will have a different viewpoint and their opinion might be biased. The best way to answer this question is to make use of data and analyse the cohorts of people who are well settled in the industry (example - data scientists). Then compare the difference in their roles, position, responsibilities, and annual compensation. This type of analysis can provide many interesting insights and help in looking at the broader view of the problem. In this notebook, I decided to take a stab at this scenario and share my experience along with the analysis of Kaggle's annual data science survey data.\n\n\nSource: Upslash\nThere is one set of people who wants to pursue higher education degrees due to their passion and interest. For these people, it makes sense to get enrol in the relevant university courses and pursue their passion. On the other hand, there is another set of people, who wants to get these degrees to get a specific job title, or specific job roles that help them make more money. For this group, I consider that university courses are not just the only way to achieve these objectives.\n\nThe most obvious example is in the field of **Data Science**. In recent years, higher education degrees are sought as one of the pathways to become Data Scientist. It is not astonishing that Data Scientist is one of the fastest-growing job titles across the globe. The demand for skilled data scientists is increasing and this has given the universities an option to attract students and make huge amounts of money. Several universities have started dedicated higher education degrees specialized in data science and analytics. Those want to become a data scientist or to switch from another profession to data science profession are now strongly considering these university degrees as the only pathway.\n\nBut these university courses are not easy to get in and affordable for everyone. These degrees don\u2019t come for free, tuition fees can be exorbitant and can range anything from USD 30,000 to USD 100,000. And that doesn\u2019t include the actual cost of living. Many consider applying for student loans but they add a huge lump sum to the existing mountain of debts. A common myth is that the earning potential for those with postgraduate qualifications is higher but of course, there is no guarantee that one will get a stable job at the end of it. Additionally, Pursuing a university\u2019s higher degree takes anything from one to three years, depending on different factors. This can seem like a long time, especially when the fellow peers getting started on their careers, while one is still studying.\n\nThe question of interest here is - does one need to get that expensive higher education degree, do they create a difference from those who do not have university degrees?. Some resources online also suggest that one can get the depth of knowledge, variety of skills and learn something new. But again, is it possible to get the same skills, same profile, or even better compensation without such degrees?.\n\n\n\nKaggle conducted their Annual Data Science survey and it was full of interesting questions. Participants of this survey were asked different questions about their demographics, profiles, companies, what they use etc. I analysed this data intending to dig deeper into the profiles of people who completed the university degrees and those who did not. The focus of the story in this notebook is to identify if the working data scientists with official higher education degrees differ significantly from the other group. The analysis and storyline are segmented according to different factors.\n\nNote - For the analysis, I removed the respondents who were \"students\" and who was \"not employed\". The two groups were selected based on the respondent's choice if they completed the university degrees or not.\n\nContents\n\n1. Sources of Learning Data Science\n    - Why People Choose Higher Education Degrees\n    - The Academic Landscape : Masters in Data Science Degrees\n2. Proportion of Individuals with University Degrees\n3. Are there a Significant Differences - With and Without University Degrees ?\n    3.1 Compensation\n        - Key Characteristics : Data Scientists earning > USD 150K\n    3.2 Job Roles\n    3.3 Job Personas\n        - Other Tools : Usage and Comparison\n4. Key Features that differentiate two groups\n\n1. Sources of Learning Data Science\nData science skills are difficult to obtain, but nowadays there are many online and offline which teaches them in detail. While some prefer online courses such as Coursera or Udacity, some prefer to go to universities for a year or two-year long dedicated courses. Let's look at what are the most popular sources of learning data science among the respondents of the kaggle survey. In this question, one participant could have chosen multiple choices, hence the x-axis represents \"percentage\" of respondents who selected a particular choice.","cf8067f7":"Other Tools : Usage and Comparison\nThe following graph summarizes the usage of other tools, techniques, databases, platforms, and frameworks used by people who completed their university degrees as comapred to ones who did not.\n\nUsage Patterns of Individuals with University Degrees:","3623bce6":"Usage Patterns of Individuals without University Degrees:","7896c031":"Time and money are the two biggest investments associated with university degrees. The graph shows that the tuition fee for most of these courses is not cheap and the duration can range from anywhere 1 to 3 years depending upon specialization, location, and university type.\n\nThe university courses are of two types: Generic courses and Specializations. Generic courses are typically very comprehensive, they cover all parts of data science but they are not very deep and detailed. These type of courses are good for those who want to get acquainted with main elements of this field. The specializations, on the other hand, aim to cover every possible detail of one particular area. They are generally very deep. For both types of courses, the investment of money and time are always higher as compared to the alternative free ones. Additionally, these courses are never meant to teach everything and do the spoon-feeding. They are more like the guided paths, and it is mostly the self-effort along that path which makes the students learn. If guided correctly, even through the non-degree courses (such as the ones on coursera or kaggle learn etc), one may also get the same outcomes.\n\nHead of Data Science from Restaurant Technologies, Inc. shared, \"No single Masters Program could cover all the disciplines needed in significant depth for one to be an expert in all these areas. Selecting an area or two or three and having depth and expertise in those is common. Many companies do not have just a \"Data Scientist\" but teams comprised of experts from the different disciplines.\"\n\n2. Proportion of Individuals with University Degrees\nLet's look at what per cent of individuals completed their university degrees to become a data scientist across different countries. Respondents were asked about their country in one of the questions.\n\n","1e9893d3":"3.2 Job Roles\nNext we look at the job roles of individuals. Does it make a difference in terms of what kind of activity the data scientists do on the daily basis if they are coming with a university degree as compared to without.","f6aea505":"4. Identifying Key Traits\n\nLet's look at the what are the key characterisics of individuals who prefer to go to university. In this particular task, we aim to identify what are the most important signals strongly related to university degree holders. To identify these signals, we can treat this task as a predictive modelling problem. First we need to prepare the dataset and create the training and testing sets. We will train a simple random forest classifier in which the features are weather the individual used a particular tool \/ platform \/ framework \/ technique etc. and the target will be a binary variable - weather they completed university degree or not. These features capture information about their demographics, their roles, their companies etc and all of them are binary. Once the model is trained, the most important features of the model sorted accoriding to their importance will be used to identify key signals. I have used the machine learning interpretability technique called - Permutation Importance technique which is generally used in machine learning explainability to explain the models.","b941f411":"About 44% of the respondents selected \"Coursera\" as the primary source of learning data science. Coursera is the popular online learning platform which provides both free courses and paid specializations.\nCoursera and its founder Andrew NG have made very significant contributions to the data science revolution. Back in 2012, Andrew NG released the very popular Machine Learning course which became the first choice for many to learn data science. In 2017, Deeplearning.ai was launched and it became very popular data science specialization. These courses and many others from well known academic names on the online platform makes Coursera as the primary choice among the data science enthusiasts.\nThe search results for Coursera data science page says that there are 1054 courses about Data Science as of Dec 2019.\n\nKaggle Learn was selected by almost one-fourth of the respondents. Kaggle team launched these courses somewhere around early 2017. They are composed of notebook style materials which not only focusses on teaching the concepts but also the programming part as well.\nThen there are other sources such as Udemy, Udacity, edX, fast.ai etc. These platforms als provide online materials and courses to learn data science.\n\nThen there is a group of individuals who prefer to go to a university to pursue higher education degrees. Among the survey participants, about one-fifth of the participants had completed university degrees. Accoding to multiple sources ( Masters-And-More, Uniplaces, CareerAdditct, MyBaggage ) different individuals have many different reasons to choose university courses over online courses. The most common are:\n\nPersonal Goal\nBetter Salary\nGain More Knowledge\nBetter Job Roles\nCareer Change\nDesigning My Own Survey : Why People Choose Higher Education Degrees\nMy personal reason for engaging in a higher education degree was majorly driven by passion and interest, it was a personal goal of mine to get another degree after the bachelors. But I was curious to know why other individuals decide to pursue a higher education degree. I created my own survey to know the student's choices this question and shared it in several groups associated with National University of Singapore. I managed to get about 120 responses from different people for this survey.\n\nLink of the Survey: Survey\nLink of the Responses: Responses\n\nFollowing is the response distribution of the Question.","cd10b4b9":"The above plot shows that more percentage of people who completed university degrees are involved in multiple responsibilities.\nBased on the number of job responsibilities, a data scientist can be classified into two categories - Generalist and Specialist. Generalist are the individuals involved in all parts of life cycle of a data science project (ie. the points on the extereme right). Specialists are the individuals who are focussed on hardly 1 or 2 job responsibilities, ie. (points in the left).\nWe see that slightly more percentage of generalists are there having university degrees. They are they people who are involved in every step - doing analysis, experimentation, building services, and also research.\n3.3 Job Personas - What type of tools\/techniques are used ?\nAnother important point in the set of common beliefs is that the job profile of those who completed their university degrees is very different from those who did not. Mainly the differences are considered in terms of tools, technologies, techniques used on day to day basis have some differences. Let's analyse from the kaggle survey data and identify are there really major differences in what type of tools and techniques are used by people with or without university degrees.","2d59e1e2":"Most of the people decided to pursue higher education degrees to gain more knowledge. This was the primary reason for about 70% of the individuals who were part of this survey. Every two out of five people decided to pursue university degrees to get better salaries or to change their professions. Only about one-fourth of individuals had their own personal goal to go for a university degree.\nIf we just look at the number of individuals who selected \"gain more knowledge\", the same but important question arises again: \"Is it worth spending a huge amount of money to gain knowledge that one can get from free sources?\" Arent' the free online sources good enough to gain more knowledge. Or, Can't these sources provide enough skills and knowledge to get that better salary, better job roles, or provide a pathway for a career change.\nThe interesting fact to note that most of the online courses on Coursera, Udemy etc. are also from the same universities or the same professors. Many of them are free as well. So if \"gaining more knowledge\" is the only goal then it is worth considering these free courses.\n\nThe Academic Landscape : Masters in Data Science Degrees\nWhatever be the debate but one point is definitely clear, Universities across the globe do benefit a lot from this increased interest in higher education degrees. Many universities have now started specialized masters degree programs in analytics, data science, business analytics etc. The following chart shows some of the popular master's degree programmes from US universities along with their tuition fee and duration. Some of them are provided online, but the same are also provided on campus.\n\nSource of Data : https:\/\/www.kdnuggets.com\/2019\/04\/best-masters-data-science-analytics-online.html","292caf0f":"The plot shows the percentage of respondents from the United States of America in each compensation bracket. Looking at every bucket, it is clear that there are no significant differences between the two groups. Approximately they differ by a few per cent (less than 5). However, a few sections in this chart are very interesting.\n\nA common belief about higher education degrees is that one get higher compensation. The chart shows that there is a large percentage of individuals *without a university degree also earning more than 100K USD. Even without university degrees, if individuals manage to obtain the right skills and the right direction, one can also grab high compensation.\nSlight differences are only observed when the compensation is less than 125K. The area is shown in the green section in the chart. For this range, there is a higher percentage of individuals having university degrees.\nInteresting to note that there are more percentage of respondents without university degrees than those who have who are earning in the range of $150k-300K. This area is shown in the red section. It will be further interesting to specifically look into this cohort where data scientists earn >150K USD.\nKey Characteristics : Data Scientists earning > $150K\nThe following graph shows the key characteristics: age distribution, coding experience (in years) etc. for Data Scientist earning more than 150K USD.","eba3908d":"Plot A shows what percentage of respondents selected a particular activity which people do in their day to day activities. Plot B shows, out of all the selected choices by all the respondents, what percentage is a particular responsibility is selected. Both of these plots capture different information. The first one shows which are the most common activities people do in their data science project and the difference between a degree and non-degree holders. While the second plot shows, for a particular group (degree, non-degree holders), which activity has more importance. For example - If responsibility is rarely selected, then in the overall percentage will not be higher. If responsibility is selected most of the times, then its overall percentage will also be higher.\n\nPlot A shows that a higher percentage of individuals with a university degree are involved in different tasks. The biggest differences are observed for data scientists who do \"Data Analysis or Exploration\", and performing \"Research and developing State of the art models\".\nThe percentage of non-degree holders is always lesser than the counterpart, however, the differences are not extreme. There is still a significant percentage of people who are involved in similar responsibilities as the university holders. Hence, it will be wrong to say that only university holders do a particular type of activities or have specialized activities.\nPlot B on the right shows a higher percentage of people with university degrees selected being involved in Research work, Data Analysis, and Building\/Running Infrastructures. While a slightly more percentage of people without university degrees selected being involved in Experimentation and Building Machine Learning services.\nA fairly equal percentage is observed for people who build Machine Learning service despite their degrees. This selection shows that there is no difference."}}