{"cell_type":{"03a0fc56":"code","6fc8b18a":"code","9a01b596":"code","d23ff2ea":"code","69aa516f":"code","42254707":"code","81421a17":"code","41401f57":"code","d9d0b6b8":"code","087e2c26":"code","9547f5b5":"code","2c85a4bd":"code","bf9e20c3":"code","03d524fc":"code","27e61c43":"code","d24450c8":"code","423fb35c":"code","edc1fab3":"code","d76fc31d":"code","bba6f02a":"code","7da1a55a":"code","da84a943":"code","8252c6d4":"code","2159a578":"code","d5dfbe0a":"code","42536ddd":"code","247a7aa6":"code","5b641105":"code","6836a399":"code","5b2fb7c2":"code","e68827e7":"code","ed096a23":"code","bedf66dc":"code","540def40":"code","52e6ddda":"code","bd9f3ad6":"code","2129618c":"code","2d59f08f":"code","6b11717f":"code","d4aac78f":"code","ded9f26b":"code","52e8501f":"code","7312d683":"code","22002b8c":"code","81b280f8":"code","1ea96ac5":"code","388e8e18":"code","e8a20690":"code","a6980028":"code","cf146884":"code","2e264076":"code","87b4fe2e":"code","67c4f643":"markdown","0be4d43b":"markdown","6a32265d":"markdown","5181c968":"markdown","dc5f129e":"markdown","5c168fdf":"markdown","9983c5be":"markdown","3ef9a6b7":"markdown","b6187a55":"markdown","917031dd":"markdown","3a9c10e8":"markdown","8303cbbd":"markdown","4363f8da":"markdown","a81b33e6":"markdown","de183464":"markdown","4189715c":"markdown","9dff2d20":"markdown","3d85078f":"markdown","430eb457":"markdown","77a2a2e5":"markdown","171a4a1f":"markdown","138b1f66":"markdown","dfe42d8d":"markdown","3ebf17e5":"markdown","a5c3a9a4":"markdown","751ceecf":"markdown","b805d6b3":"markdown","aa0ab5c0":"markdown","441b49dc":"markdown","f8654f1c":"markdown","11862540":"markdown","3d2c987a":"markdown"},"source":{"03a0fc56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6fc8b18a":"## for data\nimport pandas as pd\nimport collections\nimport json\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wordcloud\n## for text processing\nimport re\nimport nltk\n \n## for sentiment\nfrom textblob import TextBlob\n## for ner\nimport spacy\n## for vectorizer\nfrom sklearn import feature_extraction, manifold\n","9a01b596":"## for data\nimport json\nimport pandas as pd\nimport numpy as np\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for processing\nimport re\nimport nltk\n## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n## for explainer\nfrom lime import lime_text\n## for word embedding\nimport gensim\nimport gensim.downloader as gensim_api\n## for deep learning\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom tensorflow.keras import backend as K\n## for bert language model\nimport transformers","d23ff2ea":"lst_dics = []\nwith open('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', mode='r', errors='ignore') as json_file:\n    for dic in json_file:\n        lst_dics.append( json.loads(dic) )\n## print the first one\nlst_dics[0]","69aa516f":"dtf = pd.DataFrame(lst_dics)\ndtf.head()","42254707":"## filter categories\ndtf = dtf[ dtf[\"category\"].isin(['ENTERTAINMENT','POLITICS','TECH']) ][[\"category\",\"headline\"]]\n## rename columns\ndtf = dtf.rename(columns={\"category\":\"y\", \"headline\":\"text\"})\n## print 5 random rows\ndtf.sample(5)","81421a17":"print(dtf.shape)\ndtf.columns.values","41401f57":"dtf.info()","d9d0b6b8":"dtf.y.unique() ","087e2c26":"plt.figure(figsize=(12,6))\nsns.countplot(x='y',data=dtf)","9547f5b5":"fig, ax = plt.subplots()\nfig.suptitle(\"y\", fontsize=12)\ndtf[\"y\"].reset_index().groupby(\"y\").count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","2c85a4bd":"s = pd.Series(dtf.text[0:10], dtype=\"string\")\n# Concatenating a single Series into a string\nt=s.str.cat(sep=',')\n# Import the word cloud function  \n \nfrom wordcloud import WordCloud, STOPWORDS\n# Create and generate a word cloud image \nmy_cloud =  WordCloud(background_color='white', stopwords=STOPWORDS).generate(t)\n\n# Display the generated wordcloud image\nplt.imshow(my_cloud, interpolation='bilinear') \nplt.axis(\"off\")\n# Don't forget to show the final image\nplt.show()","bf9e20c3":"import spacy\n## call model\nner = spacy.load(\"en_core_web_lg\")\n## tag text\ntxt = dtf[\"text\"].iloc[0]\ndoc = ner(txt)\n## display result\nspacy.displacy.render(doc, style=\"ent\")","03d524fc":"## tag text and exctract tags into a list\ndtf[\"tags\"] = dtf[\"text\"].apply(lambda x: [(tag.text, tag.label_) \n                                for tag in ner(x).ents] )","27e61c43":"dtf[\"tags\"]","d24450c8":"import collections\n## utils function to count the element of a list\ndef utils_lst_count(lst):\n    dic_counter = collections.Counter()\n    for x in lst:\n        dic_counter[x] += 1\n    dic_counter = collections.OrderedDict( \n                     sorted(dic_counter.items(), \n                     key=lambda x: x[1], reverse=True))\n    lst_count = [ {key:value} for key,value in dic_counter.items() ]\n    return lst_count\n\n## count tags\ndtf[\"tags\"] = dtf[\"tags\"].apply(lambda x: utils_lst_count(x))\n\n## utils function create new column for each tag category\ndef utils_ner_features(lst_dics_tuples, tag):\n    if len(lst_dics_tuples) > 0:\n        tag_type = []\n        for dic_tuples in lst_dics_tuples:\n            for tuple in dic_tuples:\n                type, n = tuple[1], dic_tuples[tuple]\n                tag_type = tag_type + [type]*n\n                dic_counter = collections.Counter()\n                for x in tag_type:\n                    dic_counter[x] += 1\n        return dic_counter[tag]\n    else:\n        return 0\n\n## extract features\ntags_set = []\nfor lst in dtf[\"tags\"].tolist():\n     for dic in lst:\n        for k in dic.keys():\n            tags_set.append(k[1])\ntags_set = list(set(tags_set))\nfor feature in tags_set:\n     dtf[\"tags_\"+feature] = dtf[\"tags\"].apply(lambda x: \n                             utils_ner_features(x, feature))\n\n## print result\ndtf.head()","423fb35c":"#list(tags_list[2].values())","edc1fab3":"y = \"ENTERTAINMENT\"\ntags_list = dtf[dtf[\"y\"]==y][\"tags\"].sum()\nmap_lst = list(map(lambda x: list(x.keys())[0], tags_list))\ndtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])\ndtf_tags[\"count\"] = 1\ndtf_tags = dtf_tags.groupby(['type',  \n                'tag']).count().reset_index().sort_values(\"count\", \n                 ascending=False)\nfig, ax = plt.subplots()\nfig.suptitle(\"Top frequent tags\", fontsize=12)\nsns.barplot(x=\"count\", y=\"tag\", hue=\"type\", \n            data=dtf_tags.iloc[:20,:], dodge=False, ax=ax)\nax.grid(axis=\"x\")\nplt.show()","d76fc31d":"## predict wit NER\ntxt = dtf[\"text\"].iloc[0]\nentities = ner(txt).ents\n## tag text\ntagged_txt = txt\nfor tag in entities:\n    tagged_txt = re.sub(tag.text, \"_\".join(tag.text.split()), \n                        tagged_txt) \n## show result\nprint(tagged_txt)","bba6f02a":"'''\nPreprocess a string.\n:parameter\n    :param text: string - name of column containing text\n    :param lst_stopwords: list - list of stopwords to remove\n    :param flg_stemm: bool - whether stemming is to be applied\n    :param flg_lemm: bool - whether lemmitisation is to be applied\n:return\n    cleaned text\n'''\ndef utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","7da1a55a":"lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n","da84a943":"txt = dtf[\"text\"].iloc[0]\n","8252c6d4":"print(\"--- original ---\")\nprint(txt)\nprint(\"--- cleaning ---\")\ntxt = re.sub(r'[^\\w\\s]', '', str(txt).lower().strip())\nprint(txt)\nprint(\"--- tokenization ---\")\ntxt = txt.split()\nprint(txt)","2159a578":"print(\"--- stemming ---\")\nps = nltk.stem.porter.PorterStemmer()\nprint([ps.stem(word) for word in txt])\nprint(\"--- lemmatisation ---\")\nlem = nltk.stem.wordnet.WordNetLemmatizer()\nprint([lem.lemmatize(word) for word in txt])","d5dfbe0a":"dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))","42536ddd":"y = \"POLITICS\"\ncorpus = dtf[dtf[\"y\"]==y][\"text_clean\"]\nlst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\nfig, ax = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(\"Most frequent words\", fontsize=15)\n    \n## unigrams\ndic_words_freq = nltk.FreqDist(lst_tokens)\ndtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n                       columns=[\"Word\",\"Freq\"])\ndtf_uni.set_index(\"Word\").iloc[:20,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n                  legend=False).grid(axis='x')\nax[0].set(ylabel=None)\n    \n## bigrams\ndic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\ndtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n                      columns=[\"Word\",\"Freq\"])\ndtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n                   string for string in x) )\ndtf_bi.set_index(\"Word\").iloc[:20,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n                  legend=False).grid(axis='x')\nax[1].set(ylabel=None)\nplt.show()","247a7aa6":"lst_words = [\"box office\", \"republican\", \"apple\"]\n## count\nlst_grams = [len(word.split(\" \")) for word in lst_words]\nvectorizer = feature_extraction.text.CountVectorizer(\n                 vocabulary=lst_words, \n                 ngram_range=(min(lst_grams),max(lst_grams)))\ndtf_X = pd.DataFrame(vectorizer.fit_transform(dtf[\"text_clean\"]).todense(), columns=lst_words)\n## add the new features as columns\ndtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)\ndtf.head()","5b641105":"dtf['word_count'] = dtf[\"text\"].apply(lambda x: len(str(x).split(\" \")))\ndtf['char_count'] = dtf[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndtf['sentence_count'] = dtf[\"text\"].apply(lambda x: len(str(x).split(\".\")))\ndtf['avg_word_length'] = dtf['char_count'] \/ dtf['word_count']\ndtf['avg_sentence_lenght'] = dtf['word_count'] \/ dtf['sentence_count']\ndtf.head()","6836a399":"x, y = \"char_count\", \"y\"\nfig, ax = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(x, fontsize=12)\nfor i in dtf[y].unique():\n    sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, \n                 bins=10, hist_kws={\"alpha\":0.8}, \n                 axlabel=\"histogram\", ax=ax[0])\n    sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, \n                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n                 ax=ax[1])\nax[0].grid(True)\nax[0].legend(dtf[y].unique())\nax[1].grid(True)\nplt.show()","5b2fb7c2":"dtf[\"sentiment\"] = dtf['text'].apply(lambda x:  TextBlob(x).sentiment.polarity)\ndtf.head()","e68827e7":"print(dtf[\"text\"].iloc[0], \" --> \", dtf[\"sentiment\"].iloc[0])","ed096a23":"x, y = \"sentiment\", \"y\"\nfig, ax = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(x, fontsize=12)\nfor i in dtf[y].unique():\n    sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False, \n                 bins=10, hist_kws={\"alpha\":0.8}, \n                 axlabel=\"histogram\", ax=ax[0])\n    sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True, \n                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n                 ax=ax[1])\nax[0].grid(True)\nax[0].legend(dtf[y].unique())\nax[1].grid(True)\nplt.show()","bedf66dc":"nlp = gensim_api.load(\"glove-wiki-gigaword-300\")","540def40":"word = \"love\"\nnlp[word]","52e6ddda":"nlp[word].shape","bd9f3ad6":"## find closest vectors\nlabels, X, x, y = [], [], [], []\nfor t in nlp.most_similar(word, topn=20):\n    X.append(nlp[t[0]])\n    labels.append(t[0])\n## reduce dimensions\npca = manifold.TSNE(perplexity=40, n_components=2, init='pca')\nnew_values = pca.fit_transform(X)\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n## plot\nfig = plt.figure()\nfor i in range(len(x)):\n    plt.scatter(x[i], y[i], c=\"black\")\n    plt.annotate(labels[i], xy=(x[i],y[i]), xytext=(5,2), \n               textcoords='offset points', ha='right', va='bottom')\n## add center\nplt.scatter(x=0, y=0, c=\"red\")\nplt.annotate(word, xy=(0,0), xytext=(5,2), textcoords='offset points', ha='right', va='bottom')","2129618c":"y = \"TECH\"\ncorpus = dtf[dtf[\"y\"]==y][\"text_clean\"]\n\n## pre-process corpus\nlst_corpus = []\nfor string in corpus:\n    lst_words = string.split()\n    lst_grams = [\" \".join(lst_words[i:i + 2]) for i in range(0, \n                     len(lst_words), 2)]\n    lst_corpus.append(lst_grams)\n## map words to an id\nid2word = gensim.corpora.Dictionary(lst_corpus)\n## create dictionary word:freq\ndic_corpus = [id2word.doc2bow(word) for word in lst_corpus] \n## train LDA\nlda_model = gensim.models.ldamodel.LdaModel(corpus=dic_corpus, id2word=id2word, num_topics=3, random_state=123, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n   \n## output\nlst_dics = []\nfor i in range(0,3):\n    lst_tuples = lda_model.get_topic_terms(i)\n    for tupla in lst_tuples:\n        lst_dics.append({\"topic\":i, \"id\":tupla[0], \n                         \"word\":id2word[tupla[0]], \n                         \"weight\":tupla[1]})\ndtf_topics = pd.DataFrame(lst_dics, \n                         columns=['topic','id','word','weight'])\n    \n## plot\nfig, ax = plt.subplots()\nsns.barplot(y=\"word\", x=\"weight\", hue=\"topic\", data=dtf_topics, dodge=False, ax=ax).set_title('Main Topics')\nax.set(ylabel=\"\", xlabel=\"Word Importance\")\nplt.show()","2d59f08f":"## split dataset\ndtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)\n## get target\ny_train = dtf_train[\"y\"].values\ny_test = dtf_test[\"y\"].values","6b11717f":"## Count (classic BoW)\n#vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n\n## Tf-Idf (advanced variant of BoW)\nvectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))","d4aac78f":"corpus = dtf_train[\"text_clean\"]\nvectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","ded9f26b":"X_train.shape","52e8501f":"word = \"new york\"\ndic_vocabulary[word]","7312d683":"## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n\n","22002b8c":"from sklearn import feature_selection\ny = dtf_train[\"y\"]\nX_names = vectorizer.get_feature_names()\np_value_limit = 0.95\ndtf_features = pd.DataFrame()\nfor cat in np.unique(y):\n    chi2, p = feature_selection.chi2(X_train, y==cat)\n    dtf_features = dtf_features.append(pd.DataFrame(\n                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n                    ascending=[True,False])\n    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\nX_names = dtf_features[\"feature\"].unique().tolist()","81b280f8":"dtf_features.head()","1ea96ac5":"for cat in np.unique(y):\n   print(\"# {}:\".format(cat))\n   print(\"  . selected features:\",\n         len(dtf_features[dtf_features[\"y\"]==cat]))\n   print(\"  . top features:\", \",\".join(\ndtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n   print(\" \")","388e8e18":"vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\nvectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","e8a20690":"X_train.shape","a6980028":"classifier = naive_bayes.MultinomialNB()","cf146884":"## pipeline\nmodel = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", classifier)])\n## train classifier\nmodel[\"classifier\"].fit(X_train, y_train)\n## test\nX_test = dtf_test[\"text_clean\"].values\npredicted = model.predict(X_test)\npredicted_prob = model.predict_proba(X_test)","2e264076":"from  sklearn import metrics \nclasses = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    \n## Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted)\nauc = metrics.roc_auc_score(y_test, predicted_prob, \n                            multi_class=\"ovr\")\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Auc:\", round(auc,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted))\n    \n## Plot confusion matrix\ncm = metrics.confusion_matrix(y_test, predicted)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\n## Plot roc\nfor i in range(len(classes)):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n                           predicted_prob[:,i])\n    ax[0].plot(fpr, tpr, lw=3, \n              label='{0} (area={1:0.2f})'.format(classes[i], \n                              metrics.auc(fpr, tpr))\n               )\nax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\nax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n          xlabel='False Positive Rate', \n          ylabel=\"True Positive Rate (Recall)\", \n          title=\"Receiver operating characteristic\")\nax[0].legend(loc=\"lower right\")\nax[0].grid(True)\n    \n## Plot precision-recall curve\nfor i in range(len(classes)):\n    precision, recall, thresholds = metrics.precision_recall_curve(\n                 y_test_array[:,i], predicted_prob[:,i])\n    ax[1].plot(recall, precision, lw=3, \n               label='{0} (area={1:0.2f})'.format(classes[i], \n                                  metrics.auc(recall, precision))\n              )\nax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n          ylabel=\"Precision\", title=\"Precision-Recall curve\")\nax[1].legend(loc=\"best\")\nax[1].grid(True)\nplt.show()","87b4fe2e":"## select observation\ni = 1\ntxt_instance = dtf_test[\"text\"].iloc[i]\n## check true value and predicted value\nprint(\"True:\", y_test[i], \"--> Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=\n             np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, \n             model.predict_proba, num_features=5)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","67c4f643":"Most of the headlines have a neutral sentiment, except for Politics news that is skewed on the negative tail, and Tech news that has a spike on the positive tail.","0be4d43b":"If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.\nIn order to drop some columns and reduce the matrix dimensionality, we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\ntreat each category as binary (for example, the \u201cTech\u201d category is 1 for the Tech news and 0 for the others);\nperform a Chi-Square test to determine whether a feature and the (binary) target are independent;\nkeep only the features with a certain p-value from the Chi-Square test.","6a32265d":"This article come form :https:\/\/towardsdatascience.com\/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794","5181c968":"# Length Analysis\nIt\u2019s important to have a look at the length of the text because it\u2019s an easy calculation that can give a lot of insights. Maybe, for instance, we are lucky enough to discover that one category is systematically longer than another and the length would simply be the only feature needed to build the model. Unfortunately, this won\u2019t be the case as news headlines have similar lengths, but it\u2019s worth a try.\nThere are several length measures for text data. I will give some examples:\nword count: counts the number of tokens in the text (separated by a space)\ncharacter count: sum the number of characters of each token\nsentence count: count the number of sentences (separated by a period)\naverage word length: sum of words length divided by the number of words (character count\/word count)\naverage sentence length: sum of sentences length divided by the number of sentences (word count\/sentence count)","dc5f129e":"I\u2019m going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step.","5c168fdf":"Moving forward with another useful application of NER: do you remember when we removed stop words losing the word \u201cWill\u201d from the name of \u201cWill Smith\u201d? An interesting solution to that problem would be replacing \u201cWill Smith\u201d with \u201cWill_Smith\u201d, so that it won\u2019t be affected by stop words removal. Since going through all the texts in the dataset to change names would be impossible, let\u2019s use SpaCy for that. As we know, SpaCy can recognize a person name, therefore we can use it for name detection and then modify the string.","9983c5be":"It\u2019s time to train a machine learning model and test it. I recommend using a Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes\u2019 Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.","3ef9a6b7":"# Named-Entity Recognition\nNER (Named-entity recognition) is the process to tag named entities mentioned in unstructured text with pre-defined categories such as person names, organizations, locations, time expressions, quantities, etc.\nTraining a NER model is really time-consuming because it requires a pretty rich dataset. Luckily there is someone who already did this job for us. One of the best open source NER tools is SpaCy. It provides different NLP models that are able to recognize several categories of entities.\n\n\nI will give an example using the SpaCy model en_core_web_lg (the large model for English trained on web data) on our usual headline (raw text, not preprocessed)","b6187a55":"The dataset is imbalanced: the proportion of Tech news is really small compared to the others, this will make for models to recognize Tech news rather tough.\nBefore explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set.","917031dd":" # Word Frequency\nSo far we\u2019ve seen how to do feature engineering by analyzing and processing the whole text. Now we are going to look at the importance of single words by computing the n-grams frequency. An n-gram is a contiguous sequence of n items from a given sample of text. When the n-gram has the size of 1 is referred to as a unigram (size of 2 is a bigram).\nFor example, the phrase \u201cI like this article\u201d can be decomposed in:\n4 unigrams: \u201cI\u201d, \u201clike\u201d, \u201cthis\u201d, \u201carticle\u201d\n3 bigrams: \u201cI like\u201d, \u201clike this\u201d, \u201cthis article\u201d\nI will show how to calculate unigrams and bigrams frequency taking the sample of Politics news.","3a9c10e8":"#  Sentiment Analysis\nSentiment analysis is the representation of subjective emotions of text data through numbers or classes. Calculating sentiment is one of the toughest tasks of NLP as natural language is full of ambiguity. For example, the phrase \u201cThis is so bad that it\u2019s good\u201d has more than one interpretation. A model could assign a positive signal to the word \u201cgood\u201d and a negative one to the word \u201cbad\u201d, resulting in a neutral sentiment. That happens because the context is unknown.\nThe best approach would be training your own sentiment model that fits your data properly. When there is no enough time or data for that, one can use pre-trained models, like Textblob and Vader. Textblob, built on top of NLTK, is one of the most popular, it can assign polarity to words and estimate the sentiment of the whole text as an average. On the other hand, Vader (Valence aware dictionary and sentiment reasoner) is a rule-based model that works particularly well on social media data.\nI am going to add a sentiment feature with Textblob:","8303cbbd":"The BoW model got 85% of the test set right (Accuracy is 0.85), but struggles to recognize Tech news (only 252 predicted correctly).\nLet\u2019s try to understand why the model classifies news with a certain category and assess the explainability of these predictions. The lime package can help us to build an explainer. To give an illustration, I will take a random observation from the test set and see what the model predicts and why.","4363f8da":"# Models :\n# Word Vectors\nRecently, the NLP field has developed new linguistic models that rely on a neural network architecture instead of more traditional n-gram models. These new techniques are a set of language modelling and feature learning techniques where words are transformed into vectors of real numbers, hence they are called word embeddings.\nWord embedding models map a certain word to a vector by building a probability distribution of what tokens would appear before and after the selected word. These models have quickly become popular because, once you have real numbers instead of strings, you can perform calculations. For example, to find words of the same context, one can simply calculate the vectors distance.\nThere are several Python libraries that work with this kind of model. SpaCy is one, but since we have already used it, I will talk about another famous package: Gensim. An open-source library for unsupervised topic modeling and natural language processing that uses modern statistical machine learning. Using Gensim, I will load a pre-trained GloVe model. GloVe (Global Vectors) is an unsupervised learning algorithm for obtaining vector representations for words of size 300.","a81b33e6":"If you are interested in a deeper text analysis and preprocessing, you can check this article.https:\/\/towardsdatascience.com\/text-analysis-feature-engineering-with-nlp-502d6ea9225d\nWith this in mind, I am going to partition the dataset into training set (70%) and test set (30%) in order to evaluate the models performance.","de183464":"The original dataset contains over 30 categories, but for the purposes of this tutorial, I will work with a subset of 3: Entertainment, Politics, and Tech.\n# EDA \nhttps:\/\/towardsdatascience.com\/text-analysis-feature-engineering-with-nlp-502d6ea9225d","4189715c":"We can use this object to map words to vectors:","9dff2d20":"We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:\nAccuracy: the fraction of predictions the model got right.\nConfusion Matrix: a summary table that breaks down the number of correct and incorrect predictions by each class.\nROC: a plot that illustrates the true positive rate against the false positive rate at various threshold settings. The area under the curve (AUC) indicates the probability that the classifier will rank a randomly chosen positive observation higher than a randomly chosen negative one.\nPrecision: the fraction of relevant instances among the retrieved instances.\nRecall: the fraction of the total amount of relevant instances that were actually retrieved.","3d85078f":"Is there a pattern between categories and sentiment?","430eb457":"The feature matrix X_train has a shape of 34,265 (Number of documents in training) x 10,000 (Length of vocabulary) and it\u2019s pretty sparse\nIn order to know the position of a certain word, we can look it up in the vocabulary:","77a2a2e5":"That\u2019s pretty cool, but how can we turn this into a useful feature? This is what I\u2019m going to do:\nrun the NER model on every text observation in the dataset, like I did in the previous example.\nFor each news headline, I shall put all the recognized entities into a new column (named \u201ctags\u201d) along with the number of times that same entity appears in the text. In the example, would be\n{ (\u2018Will Smith\u2019, \u2018PERSON\u2019):1,\n(\u2018Diplo\u2019, \u2018PERSON\u2019):1,\n(\u2018Nicky Jam\u2019, \u2018PERSON\u2019):1,\n(\u201cThe 2018 World Cup\u2019s\u201d, \u2018EVENT\u2019):1 }\nThen I will create a new column for each tag category (Person, Org, Event, \u2026) and count the number of found entities of each one. In the example above, the features would be\ntags_PERSON = 3\ntags_EVENT = 1","171a4a1f":"#  Bag-of-Words\nThe Bag-of-Words model is simple: it builds a vocabulary from a corpus of documents and counts how many times the words appear in each document.\nAs you can imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the feature matrix will be a huge sparse matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming\/lemmatization) aimed to reduce the dimensionality problem.\nTerms frequency is not necessarily the best representation for text. In fact, you can find in the corpus common words with the highest frequency but little predictive power over the target variable. To address this problem there is an advanced variant of the Bag-of-Words that, instead of simple counting, uses the term frequency\u2013inverse document frequency (or Tf\u2013Idf). Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.\nLet\u2019s start with the Feature Engineering, the process to create features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. \u201cnew\u201d and \u201cyork\u201d) and bigrams (i.e. \u201cnew york\u201d). I will provide the code for the classic count vectorizer as well:","138b1f66":"# Text Preprocessing\nData preprocessing is the phase of preparing raw data to make it suitable for a machine learning model. \n\nFor NLP, that includes text cleaning, stopwords removal, stemming and lemmatization.\n\nText cleaning steps vary according to the type of data and the required task. Generally, the string is converted to lowercase and punctuation is removed before text gets tokenized. Tokenization is the process of splitting a string into a list of strings (or \u201ctokens\u201d).\nI will put all those preprocessing steps into a single function and apply it to the whole dataset\n\nhttps:\/\/srinivas-yeeda.medium.com\/preprocessing-for-natural-language-processing-498df071ab6e","dfe42d8d":"Trying to capture the content of 6 years in only 3 topics may be a bit hard, but as we can see, everything regarding Apple Inc. ended up in the same topic.","3ebf17e5":"I showed how to detect the language the data is in, and how to preprocess and clean text. Then I explained different measures of length, did sentiment analysis with Textblob, and we used SpaCy for named-entity recognition. Finally, I explained the differences between traditional word frequency approaches with Scikit-learn and modern language models using Gensim.\nNow you know pretty much all the NLP basics to start working with text data.","a5c3a9a4":"We can refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary.","751ceecf":"Now let\u2019s see what are the closest word vectors or, to put in another way, the words that mostly appear in similar contexts. In order to plot the vectors in a two-dimensional space, I need to reduce the dimensions from 300 to 2. I am going to do that with t-distributed Stochastic Neighbor Embedding from Scikit-learn. t-SNE is a tool to visualize high-dimensional data that converts similarities between data points to joint probabilities.","b805d6b3":"In order to go deeper into the analysis, we need to unpack the column \u201ctags\u201d we created in the previous code. Let\u2019s plot the most frequent tags for one of the headline categories:","aa0ab5c0":"I reduced the number of features from 10,000 to 3,152 by keeping the most statistically relevant ones. Let\u2019s print some:","441b49dc":"# Topic Modeling\nThe Genism package is specialized in topic modeling. A topic model is a type of statistical model for discovering the abstract \u201ctopics\u201d that occur in a collection of documents.\nI will show how to extract topics using LDA (Latent Dirichlet Allocation): a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. Basically, documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.\nLet\u2019s see what topics we can extract from Tech news. I need to specify the number of topics the model has to cluster, I am going to try with 3:","f8654f1c":"Now I will use the vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix.","11862540":"# Summary\n\nIn this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with Word2Vec), and the cutting edge Language models (with BERT).\nNLP (Natural Language Processing) is the field of artificial intelligence that studies the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP is often applied for classifying text data. Text classification is the problem of assigning categories to text data according to its content.\n\nThere are different techniques to extract information from raw text data and use it to train a classification model. This tutorial compares the old school approach of Bag-of-Words (used with a simple machine learning algorithm), the popular Word Embedding model (used with a deep learning neural network), and the state of the art Language models (used with transfer learning from attention-based transformers) that have completely revolutionized the NLP landscape.\n\nI will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example (link to the full code below).\n\nI will use the \u201cNews category dataset\u201d in which you are provided with news headlines from the year 2012 to 2018 obtained from HuffPost and you are asked to classify them with the right category, therefore this is a multiclass classification problem (link below)\n\nIn particular, I will go through:\nSetup: import packages, read data, Preprocessing, Partitioning.\nBag-of-Words: Feature Engineering & Feature Selection & Machine Learning with scikit-learn, Testing & Evaluation, Explainability with lime.\nWord Embedding: Fitting a Word2Vec with gensim, Feature Engineering & Deep Learning with tensorflow\/keras, Testing & Evaluation, Explainability with the Attention mechanism.\nLanguage Models: Feature Engineering with transformers, Transfer Learning from pre-trained BERT with transformers and tensorflow\/keras, Testing & Evaluation.\n# Setup\nFirst of all, I need to import the following libraries:","3d2c987a":"If there are n-grams that appear only in one category (i.e \u201cRepublican\u201d in Politics news), those can become new features. A more laborious approach would be to vectorize the whole corpus and use all the words as features (Bag of Words approach).\nNow I\u2019m going to show you how to add word frequency as a feature in your dataframe. We just need the CountVectorizer from Scikit-learn, one of the most popular libraries for machine learning in Python. A vectorizer converts a collection of text documents to a matrix of token counts. I shall give an example using 3 n-grams: \u201cbox office\u201d (frequent in Entertainment), \u201crepublican\u201d (frequent in Politics), \u201capple\u201d (frequent in Tech)."}}