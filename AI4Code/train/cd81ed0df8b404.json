{"cell_type":{"009764fe":"code","ba2a9197":"code","399fb788":"code","3c4533e4":"code","cae62c8d":"code","ce603ac0":"code","8ea5dd34":"code","74090512":"code","b1371a32":"code","9cd17128":"code","73a47635":"code","0cb2194b":"code","e7f8117b":"code","fffde422":"code","9a573c0a":"code","d77f8a4d":"code","3343cfbc":"code","96357443":"code","5d3f1126":"code","be9e4fe3":"code","74333fd8":"code","e6a674ba":"code","4790aaa7":"code","566297e3":"code","d1ab0254":"code","cde4349f":"code","336b2837":"code","c9aab97f":"code","2cdf2e2b":"code","2831bcad":"code","b3cf266e":"code","df2a3dad":"code","f7220227":"code","bea1c902":"code","e5f0c57b":"code","666b9e1d":"code","024a3391":"code","45db6796":"code","61417d3c":"code","75d52b40":"code","28ccb466":"code","7fc14f09":"code","d51055af":"code","d456554a":"code","96ddc7c5":"code","ec8765a7":"code","19ea489b":"code","3bdfe70b":"code","53045006":"code","6344fbbf":"code","9de6fed7":"markdown","ababbcd2":"markdown","1e0e9a42":"markdown","79f102e1":"markdown","f48dbf28":"markdown","d59a8f41":"markdown","e67dea1c":"markdown","b5999f0b":"markdown","56a7e645":"markdown","bbd92c68":"markdown","9033b730":"markdown","b1cc72e4":"markdown","fd7465ca":"markdown","7368cdec":"markdown","e334345a":"markdown","34be5e60":"markdown","137debab":"markdown","dfd5f90a":"markdown","dd98f245":"markdown","451139a4":"markdown","421c2388":"markdown","03bc0ef4":"markdown","05b4d43e":"markdown","cb7ab539":"markdown","32c367de":"markdown"},"source":{"009764fe":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nimport xgboost\n\nimport os\nimport itertools\n\nsns.set_style('whitegrid')\nsns.set_palette(\"Set2\")\n\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\nsns.palplot(sns.color_palette())","ba2a9197":"\"\"\" Load in the data and examine the head. \"\"\"\n\ndata = '..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv'\ndf = pd.read_csv(data)\ndf.head(5)","399fb788":"df.columns","3c4533e4":"df.shape","cae62c8d":"df.info()","ce603ac0":"\"\"\" Transpose the describe to make it easier to read. \"\"\"\ndf.describe().T","8ea5dd34":"\"\"\" Drop the two constant columns. \"\"\"\n\ndf.drop(['EmployeeCount','EmployeeNumber'],axis=1,inplace=True)","74090512":"\"\"\" Investigate categorical columns that seem irrelevent. \"\"\"\n\ndf.Over18.value_counts()","b1371a32":"\"\"\" Drop Over18 as it is also a constant. \"\"\"\n\ndf.drop('Over18',axis=1,inplace=True)","9cd17128":"\"\"\" Alter the Education column to show categorical variables. This will later be dummied for modelling. \"\"\"\n\ndf.replace({'Education':{1:'Below_college', 2:'College', 3:'Bachelor', 4:'Masters', 5:'Doctor'}}, inplace=True)","73a47635":"\"\"\" Iterate through the dataframes columns and covert to lowercase for QoL. \"\"\"\nlow_col = []\nfor i in df.columns:\n    i = i.lower()\n    low_col.append(i)\ndf.columns = low_col","0cb2194b":"df.columns","e7f8117b":"# \"\"\" Save this dataframe in its current state before dummying for EDA purposes. \"\"\"\n\n#df.to_csv('.\/Dataset\/Clean_nodummy.csv')","fffde422":"\"\"\" Apply Pandas get_dummies function to the dataframe to transform the categorical variables to numeric.\n    This will create additional columns in your dataframe.\"\"\"\n\ndf_dum = pd.get_dummies(df)","9a573c0a":"\"\"\" Take a look at the shape before and after dummying to see how many new columns have been created. \"\"\"\n\nprint(df.shape)\nprint(df_dum.shape)","d77f8a4d":"df_dum.columns","3343cfbc":"\"\"\" get_dummies has a drop_first parameter, which removes the first column created for each categorical variable\n    which acts as a default, however we do not want this to be done for all of our columns, since this is a relatively\n    small dataset, i will do this manually.\"\"\"\n\ndf_dum.drop(['gender_Female','overtime_No','attrition_No'],axis=1,inplace=True)","96357443":"\"\"\" Due to the presence of capital letters in the variables, once again iterate through and convert all the lowercase \"\"\"\nlow_col = []\nfor i in df_dum.columns:\n    i = i.lower()\n    low_col.append(i)\ndf_dum.columns = low_col","5d3f1126":"df_dum.columns","be9e4fe3":"#\"\"\" Export this dummied dataframe to be used in the modelling phase of this project. \"\"\"\n#df_dum.to_csv('.\/Dataset\/IBM_dummied.csv')","74333fd8":"\"\"\" Read in the Undummied CSV. \"\"\"\n\ndf = pd.read_csv('..\/input\/ibm-data\/Clean_nodummy.csv', index_col=0)","e6a674ba":"\"\"\" Lets have a look at the target variable in this case. \"\"\"\n\ndf.attrition.head()","4790aaa7":"\"\"\" At the moment, this column is categorical. For use in visualisations etc. this will need to be numeric,\n    so lets convert Yes and No, into binary, or 1's and 0's\"\"\"\n\ndf.loc[df.attrition == 'Yes', 'attrition'] = 1\ndf.loc[df.attrition == 'No', 'attrition'] = 0","566297e3":"\"\"\" Next, take a look at the value_counts to see the state of the target. \"\"\"\ndf.attrition.value_counts()","d1ab0254":"\"\"\" Lets plot the target variable to make it easier to visualise. \"\"\"\n\n\"\"\" Set the figure size. \"\"\"\nplt.figure(figsize=(6,4))\n\n\"\"\" We will use a countplot here from the seaborn library. \"\"\"\nfig = sns.countplot(df.attrition)\n\n\"\"\" sns.despine allows the customisation of the borders in the plot just for aesthetic purposes. \"\"\"\nsns.despine(left=True)\n\n\"\"\" Configure the axes labels. \"\"\"\nfig.set_xlabel('Attrition', fontsize=16)\nplt.xticks(fontsize=16)\nfig.set_ylabel('Count', fontsize=16, rotation=0)\nfig.yaxis.labelpad = 30\nplt.yticks(fontsize=16)\n\n\"\"\" Add annotations on the plot to show the actual count values on each of the columns.\"\"\"\nfor p in fig.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    fig.annotate('{:}'.format(p.get_height()), (x.mean(), y-150), ha='center', va='bottom', fontsize=16, color='white')\nplt.show()","cde4349f":"\"\"\" Show the percentage breakdown of the target column. This also shows us the Baseline accuracy for our models. (83.9%) \"\"\"\n\nprint('Percentage breakdown of Attrition')\nprint('-'*33)\nround(df.attrition.value_counts(normalize=True)*100,2)","336b2837":"\"\"\" Plot the distribution of Age. \"\"\"\n\nplt.figure(figsize=(12,6))\nfig = sns.distplot(df.age,kde=False, bins=10, hist_kws=dict(alpha=1))\nsns.despine(left=True)\nfig.set_xlabel('Age',fontsize=20)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nfig.yaxis.labelpad = 35\nplt.show()","c9aab97f":"\"\"\" Plot the distribution of Age where attrition is true and false. \"\"\"\n\nplt.figure(figsize=(12,8))\n\n\"\"\" Adjusting the bin size can alter the look of your graph, worth testing different sizes to see various plots. \"\"\"\nfig = sns.distplot(df[df['attrition'] == 0]['age'], label='Non Attrition', kde=0, bins=10)\nsns.distplot(df[df['attrition'] == 1]['age'], label='Attrition', kde=0, bins=10)\n\nsns.despine(left=1)\n\n\"\"\" Removes the vertical gridlines. \"\"\"\nfig.grid(axis='x')\n\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Density',fontsize=15, rotation=0)\nfig.yaxis.labelpad = 30\nplt.title('Distribution of Age',fontsize=20);\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nfig.yaxis.labelpad = 35\n\n\"\"\" Control the size and positioning of the legend. \"\"\"\nplt.legend(fontsize='x-large', bbox_to_anchor=(0.03, 0.95), loc=2, borderaxespad=0., frameon=1)\nplt.show()","2cdf2e2b":"plt.figure(figsize=(12,8))\nfig = sns.countplot(x='gender', hue='attrition', data=df)\nsns.despine(left=True)\nfig.set_xlabel('Gender', fontsize=20)\nplt.xticks(fontsize=20)\nfig.set_ylabel('Count', fontsize=20, rotation=0)\nfig.yaxis.labelpad = 30\nplt.yticks(fontsize=20)\nfor p in fig.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    fig.annotate('{:}'.format(p.get_height()), (x.mean(), y-50), ha='center', va='bottom', fontsize=20, color='white')\nplt.legend(labels =['Non Attrition','Attrition'],fontsize='x-large', bbox_to_anchor=(0.03, 0.9), loc=2, borderaxespad=0., frameon=0)\nplt.show()\n\nprint('Female Attrition percentage & count')\nprint('-'*35)\nprint(round(df[df.gender == 'Female'].attrition.value_counts(normalize=True)*100,2))\nprint(df[df.gender == 'Female'].attrition.value_counts())\nprint('_'*35)\nprint(''*35)\nprint('Male Attrition percentage & count')\nprint('-'*35)\nprint(round(df[df.gender == 'Male'].attrition.value_counts(normalize=True)*100,2))\nprint(df[df.gender == 'Male'].attrition.value_counts())\nprint('_'*35)","2831bcad":"plt.figure(figsize=(14,8))\nfig = sns.distplot(df[df['attrition'] == 0]['monthlyincome'], label='Non Attrition', kde=0, bins=10)\nsns.distplot(df[df['attrition'] == 1]['monthlyincome'], label='Attrition', kde=0, bins=10)\nsns.despine(left=1)\nfig.grid(axis='x')\nplt.xlabel('Monthly Income',fontsize=18)\nplt.ylabel('Density',fontsize=18, rotation=0)\nfig.yaxis.labelpad = 30\nplt.title('Distribution of Monthly Income',fontsize=20);\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nfig.yaxis.labelpad = 35\nplt.legend(fontsize='x-large', bbox_to_anchor=(0.4, 0.94), loc=2, borderaxespad=0., frameon=0)\nplt.show()\nprint('Average Monthly Income:',df.monthlyincome.mean())\nprint('Average Monthly Income for Males:',df[df.gender == 'Male']['monthlyincome'].mean())\nprint('Average Monthly Income for Females:',df[df.gender == 'Female']['monthlyincome'].mean())","b3cf266e":"income = df.groupby(by='jobrole').mean().monthlyincome\ninc = pd.DataFrame(income)\ninc = inc.sort_values(by='monthlyincome')","df2a3dad":"job_atr = df[df['attrition'] == 1]['jobrole']\njob_atr_val = job_atr.value_counts()\njob_atr_df = pd.DataFrame(job_atr_val)","f7220227":"plt.figure(figsize=(12,4))\nfig = sns.barplot(y=inc.index, x='monthlyincome', data=inc,\n                  palette=sns.color_palette(\"Greens\", n_colors=len(inc.index)))\nfig.set_title('AVG monthly income per Job role',fontsize=18)\nfig.set_xlabel('Average monthly income', fontsize=18)\nfig.set_ylabel('Job role', fontsize=18, position=(0,1), rotation=0)\nfig.yaxis.labelpad= -120\nplt.xticks(fontsize=16, rotation=45)\nplt.yticks(fontsize=16)\nplt.show()\n\nplt.figure(figsize=(12,4))\nfig = sns.barplot(y=job_atr_df.index, x='jobrole', data=job_atr_df, \n                  palette=sns.color_palette(\"Greens_r\", n_colors=len(job_atr_df.index)))\nfig.set_title('Attrition count per Job role',fontsize=18)\nfig.set_xlabel('Attrition count', fontsize=18)\nfig.set_ylabel('Job role', fontsize=18, position=(0,1), rotation=0)\nfig.yaxis.labelpad= -120\nplt.xticks(fontsize=16, rotation=0)\nplt.yticks(fontsize=16)\nplt.show()","bea1c902":"edu_sal = df.groupby('education').mean().monthlyincome\nedu_sal_df=pd.DataFrame(edu_sal)\nedu_sal_df = edu_sal_df.sort_values('monthlyincome', ascending=False)","e5f0c57b":"plt.figure(figsize=(12,4))\nfig = sns.barplot(y=edu_sal_df.index, x='monthlyincome', data=edu_sal_df, \n                  palette=sns.color_palette(\"Greens_r\", n_colors=len(job_atr_df.index)))\nfig.set_title('Monthly income per Education level',fontsize=18)\nfig.set_xlabel('Average Monthly income', fontsize=18)\nfig.set_ylabel('Education level', fontsize=18, position=(0,1), rotation=0)\nfig.yaxis.labelpad= -50\nplt.xticks(fontsize=16, rotation=0)\nplt.yticks(fontsize=16)\nplt.show()","666b9e1d":"plt.figure(figsize=(12,8))\nfig = sns.countplot(x='overtime', hue='attrition', data=df)\nsns.despine(left=True)\nfig.set_xlabel('Overtime', fontsize=20)\nplt.xticks(fontsize=20)\nfig.set_ylabel('Count', fontsize=20, rotation=0)\nfig.yaxis.labelpad = 30\nplt.yticks(fontsize=20)\nfor p in fig.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    fig.annotate('{:}'.format(p.get_height()), (x.mean(), y-50), ha='center', va='bottom', fontsize=20, color='white')\nplt.legend(labels =['Non Attrition','Attrition'],fontsize='x-large', bbox_to_anchor=(0.03, 0.9), loc=2, borderaxespad=0., frameon=0)\nplt.show()","024a3391":"\"\"\" Start by loading in the dummied dataset we created earlier for modelling. \"\"\"\ndata = '..\/input\/ibm-data\/IBM_dummied.csv'\ndf = pd.read_csv(data, index_col=0)\n\n\"\"\" Convert all to float. \"\"\"\ndf = df.astype(float)\n\ndf.shape","45db6796":"df.rename({'attrition_yes':'attrition'}, axis=1, inplace=True)","61417d3c":"df.head()","75d52b40":"\"\"\" Seperate dataframe into the target and features. \"\"\"\nX = df.drop('attrition', axis=1)\ny = df.attrition","28ccb466":"\"\"\" Split the dataframe into the train and test groups. The split size can be specified, for this i am\n    setting aside 20% for the testing data.\"\"\"\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7fc14f09":"\"\"\" Find our baseline accuracy, using value_counts and taking the dominating class since this is a binary target.\n    Our baseline accuracy is 83.9%\"\"\"\ny.value_counts(normalize=True)","d51055af":"\"\"\" Defining the models i am going to use into a list. \"\"\"\nclassifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB()]\n    \n\"\"\" Logging for visual comparison. \"\"\" \n\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n\n\"\"\" Iterate through each classification model stated above, fitting the model to the train data and finally\n    printing the accuracy and log loss of each model. \"\"\"\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","d456554a":"\"\"\" From this, we can then sort by accuracy and log loss to effectively visualise our results. \"\"\"\n\nlog1 = log.sort_values(by='Accuracy',ascending=False)\nlog2 = log.sort_values(by='Log Loss')","96ddc7c5":"plt.figure(figsize=(12,4))\nfig = sns.barplot(x='Accuracy', y='Classifier', data=log1, palette=sns.color_palette(\"Blues_r\", n_colors=len('classifier')))\nplt.xlabel('Accuracy %', fontsize=18)\nplt.ylabel('Classifier Model',fontsize=18, position=(0,1),rotation=0)\nfig.yaxis.labelpad= -125\nfig.set_xticks(ticks=[0,10,20,30,40,50,60,70,80,90])\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=16)\nplt.title('Classifier Accuracy', fontsize=18)\nplt.axvline(83.8, 0,1, lw=4, color='red')\nplt.annotate(s='Baseline:83.8%', xy=(75,-0.5), fontsize=16, color='black')\nplt.show()\n\nplt.figure(figsize=(12,4))\nfig = sns.barplot(x='Log Loss', y='Classifier', data=log2, palette=sns.color_palette(\"Blues\", n_colors=len('Classifier')))\nplt.xlabel('Log Loss', fontsize=18)\nplt.ylabel('Classifier Model',fontsize=18, position=(0,1),rotation=0)\nfig.yaxis.labelpad= -125\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=16)\nplt.title('Classifier Log Loss', fontsize=18)\nplt.show()","ec8765a7":"\"\"\" Create the parameter grid that will be supplied and applied to the model on each iteration. \"\"\"\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty':['l1','l2'] }\n\nfrom sklearn.model_selection import GridSearchCV\n\nclf = GridSearchCV(LogisticRegression(), param_grid)\nGridSearchCV(cv=None,\n             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n               dual=False, fit_intercept=True, penalty='l1', tol=0.0001),\n             param_grid=param_grid)","19ea489b":"clf.fit(X_train, y_train)\nclf.param_grid","3bdfe70b":"\"\"\" Create a new DataFrame containing the results from the gridsearch, with the C param and penalty associated. \"\"\"\n\nresults = pd.DataFrame(clf.cv_results_)\nfinal = results[['param_C','param_penalty','mean_test_score']].sort_values('mean_test_score')\nfinal","53045006":"\"\"\" Using a catplot, show the average model score for each version of the model. \"\"\"\n\nsns.catplot(y='mean_test_score', x='param_C', hue='param_penalty', data=final, kind='bar', aspect=15\/8.27)\nplt.axhline(0.83, 0,1, lw=4, color='red')\nplt.title('GridsearchCV scores',fontsize=18)\nplt.xlabel('C Parameter', fontsize=16)\nplt.ylabel('Average model score', fontsize=16)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","6344fbbf":"\"\"\" Print out the best parameters for the model, along with the final score utilizing these parameters. \"\"\"\nprint(clf.best_params_)\nprint(\"=\"*30)\nprint(clf.best_estimator_)\nprint(\"=\"*30)\nprint(clf.best_score_)\nprint(\"=\"*30)\ny_predict = clf.predict(X_test)\naccuracy = accuracy_score(y_test,y_predict)\nprint('Accuracy of the best classifier after CV is %.2f%%' % (accuracy*100))","9de6fed7":"In this stage of the project, the aim is to train a model on a subset of the data (training data) that successfully predicts our target variable of a different subset (test data) more accurately than baseline. By utilizing numerous models, it is possible to assess each model used and then fine tune the best performing to output the best possibly accuracy for this dataset.\n\nAs this is a relatively small dataset, i will be using a wide variety of classification models as the computation time will not be very large. however, this strategy would not suit bigger, more complex datasets as running through multiple models may take an excessive amount of time, and in the real world, cost a business unnecessarily.","ababbcd2":"### view dataframe info\n    - check for missing values\n    - examine the different datatypes","1e0e9a42":"Now that we have a model that sufficiently predicts above baseline, we can then apply further techniques to tune this model through various penalties and parameters. This can also include feature selection depending on the model. For this Logisitic Regression model, I will be using GridsearchCV to find the optimal C parameter and penalty to use to get the best accruacy for this dataset.","79f102e1":"#### insights gained from this:\n- columns such as education that are numerical that actually represent categorical variables may need further investigation.\n- EmployeeCount is a constant column with all values being 1.\n- performance rating whilst being on a scale of 1-4, only has 3s and 4s.\n- Standard Hours is a constant column with all values being 80 hours.","f48dbf28":"### EDA Summary\nFrom this brief EDA of the dataset there are numerous things that stand out in relation to attrition.\n##### Class imbalance\nClass imbalance in the target variable can be problematic when it comes to machine learning, and in relation to this dataset our focus is the minority class. Class imbalance creates a high baseline accuracy for our models to improve on. Methods to reduce this imbalance may be needed such as under\/oversampling.\n###### Attrition & Age\nAttrition seems to be more prevalent in the early career stages, most notebly between the ages of 20 and 30. Whilst there is records of attrition at almost every age grouping, it would be beneficial to retain these younger employees and develop them within the business.\n###### Attrition & monthly income\nWhile it isn't groundbreaking that the employees getting paid more are less likely to quit their jobs, exploring this area did uncover certain job roles where despite their pay level, attrition was abnormally higher than other roles. This relationship between income and retention could also be worth exploring in relation to preventing attrition, by offering incentives in the form of pay increases, how much of an effect, if any, would this have on the employees attrition chance.\n###### Attrition & overtime\nEmployees that worked overtime had a much higher rate of attrition than their colleagues that didnt have to. By increasing the total number of hours worked and making workers stay late\/arrive early the chance of them leaving increases. Again this provides another potential area of focus to improve employee retention, by minimising overtime hours, employee satisfaction could increase and the chance of attrition could go down for this specific group of employees.","d59a8f41":"# 2. EDA","e67dea1c":"# Predicting attrition with classification models\n### Dataset provided by IBM\n##### Matt Warr - Data analyst\n\n##### The aim of this notebook is to effectively predict attrition in a business. This will be a step by step process from data collection and cleaning, to model selection and tuning to produce the most accurate score, focusing on effectively representing my skills learnt during the 12 week Data science immersive course at General Assembly (Sydney). ","b5999f0b":"In this stage of the project, the main objective is to look further into the dataset and try and find insights and potential trends relating to the target variable in order to have a better understanding when it comes to selecting features for the modelling stage of the project.\n\nBeing able to thoroughly explore a dataset and present unique insights can set you apart from other data analysts\/scientists. Most semi-capable analysts can find the obvious trends\/relationships in data, it takes a keener eye for detail and following your gut instinct to delve deeper and connect the dots to provide those hidden relationships that can potentially make you stand above the rest.\n\n(This may not always be the case, some datasets are an open book, unfortunately theres not always that secret insight that will jump out at you, but its worth looking, even for a little while just in case.)","56a7e645":"### Initial results\nAfter training all the different classification models and visualising their accuracy and log loss, multiple models managed to predict more accurately than baseline. However, from here we can now choose the best model from this and further tweak this model through model tuning to see if we can improve the score any more.\n\nThe logistic regression model stands out at the moment as our best model to improve on. This makes sense as it is the most common and often most accurate when presenting with a binary target.","bbd92c68":"## Do any Cleaning needed\n- As this dataset is from Kaggle, dataset is relatively clean, any cleaning done is mainly to improve quality of life (QoL) for coding.","9033b730":"# 3. Implementing Machine Learning","b1cc72e4":"From this graph, we can see that attrition is present across all the age ranges.\n\nHowever, for some age groups, attrition is much more prevalent. This is especially true in the early 20s age group where it is almost equal.\n\nAttrition seems to be at its lowest in the mid 40s, and steadily increases as it gets closer the retirement age.","fd7465ca":"# 5. Conclusion","7368cdec":"Showing the average monthly income per job role on top of the attrition count per job role unveils an interesting relationship with the highest paid jobs averaging the lowest attrition and vice versa. Jobs such as Sales executive however go against this trend with a higher average income whilst recording the second highest attrition rate out of all the job roles.","e334345a":"## 1. Data Cleaning & Preparation","34be5e60":"### Examine columns","137debab":"## Import all packages needed.","dfd5f90a":"### Target column - Attrition\n    -Binary target currently Yes\/no","dd98f245":"# 4. Model tuning","451139a4":"##### final model\nThrough the use of many different classification models, and tuning the best performing, my final model was predicting with a 90.14% accuracy.\n\nWhile this accuracy sounds high enough to deem it a success, looking back on this project, nothing was done to address the class imbalance, which may have affected our model. \n\nThe other problem with this project is the fact that it is a fictional dataset, meaning that it is not applicable to real world situations, And even if it were, I dont beleive that it could be implemented into a different business as its scope is set purely to the one business. The same steps could be taken and a model could be generated for a different business, however the results may not be the same due to the differing factors present.\n\nThis project was much more of a exploration of what i have learnt whilst studying at General Assembly, Sydney. A showcase of the broad array of subjects covered in the 12 week Data science immersive course. \n\nAny feedback would be appreciated, and if you would like to get in contact with me, i am always happy to discuss all things data science!\nMy Linkedin & the Github to this notebook is linked below.\n\n#### Linkedin : https:\/\/www.linkedin.com\/in\/matt-warr\/\n\n#### github : https:\/\/github.com\/mjw236\/IBM_Attrition\n\n","421c2388":"The bulk of any data science work is usually the cleaning and preparation of your dataset. This stage can and should take up most of your time throughout a machine learning project. Luckily for me, this dataset was sourced from kaggle and comes in a relatively clean state.\n\nIn the real world however, most of the time, Data will be presented to you in all sorts of forms and conditions. Being able to effectively clean, combine and prepare data will set you up for success whenever you are striving to build a machine learning model.\n\nThere is a reason why it was drilled into us from the start of the course, that 80% of your workload will likely be cleaning, and its best to embrace this rather than fight it. A clean dataset is the holy grail of Data Science.","03bc0ef4":"The first thing we can gather from this is that there is some definite class imbalance to this target variable, With non-attrition outweighing attrition heavily. \n\nThis tells us that there may potentially be a need for over\/under sampling.\n\nIt also means that the baselines for our models will be quite high to begin with as with a binary classification problem, the baseline is determined by the percentage of the dominating class.","05b4d43e":"### Check dataset shape","cb7ab539":"This graph shows a normal distribution with a slightly positive skew as indicated by the longer tail going off toward the right hand side.\n\nLets look into the Age feature in more detail in relation to the target variable.","32c367de":"### Use the describe function to examine the dataframe further\n    - Look for outstanding numbers or consistency\n    - This will only show the numeric columns (may need to examine categorical variables further individually)"}}