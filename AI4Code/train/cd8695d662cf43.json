{"cell_type":{"54f109ef":"code","052eab15":"code","629f5825":"code","64dbeb3f":"code","ec1997ea":"code","50b0ecd6":"code","bb6a981c":"code","bb6f50eb":"code","c64f50a3":"code","6f52e34d":"code","4206e77b":"code","92982620":"code","a66bacae":"code","2d0b1cbc":"code","430ee197":"code","b66afc5f":"code","c05a0971":"code","f75b7054":"code","7feb7bf7":"code","9258618c":"code","c2e01d0e":"code","b9b94deb":"code","0c8a744a":"code","0acc20ad":"code","3fece4a6":"code","a0b49bb2":"code","7d90247a":"code","5e6f65c5":"code","3d499b84":"code","df3b4c7f":"code","ef9fd19f":"code","cce8019e":"code","786e2a36":"code","88f5365b":"code","4fc658b0":"code","480e72a1":"code","185a6e7e":"code","3d90398e":"code","683311a4":"code","bf01370b":"code","345831f0":"code","e6136391":"code","123e9ac6":"code","96e6260d":"code","4e8cf163":"code","a3a32ce7":"code","72398f4c":"code","a68d78c2":"code","39854b3f":"code","53eeb680":"code","3313af2b":"markdown","a072550f":"markdown","85548c0b":"markdown","2e4c1603":"markdown","a0f98417":"markdown","f474e628":"markdown","9cf02b04":"markdown","9b6574bb":"markdown","3008e4ce":"markdown","7df55491":"markdown","6397f9ca":"markdown","09884e80":"markdown","1165cb02":"markdown","1137f74d":"markdown","bf7a856b":"markdown","d1d07ad8":"markdown","e6da17dc":"markdown","9dece506":"markdown"},"source":{"54f109ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/fruits\/fruits-360_dataset\/fruits-360\/\"))\n\n# Any results you write to the current directory are saved as output.","052eab15":"import os\nimport gc\nimport psutil\nimport pandas as pd\nimport numpy  as np","629f5825":"import matplotlib.pyplot as plt ","64dbeb3f":"import os\nPATH = \"..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Training\/\"\nPATH1 = \"..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Test\/\"","ec1997ea":"#Uncomment to check tha files in the directory\n##os.listdir(PATH1)","50b0ecd6":"data_dir_list = os.listdir(PATH)\ndata_dir_list1 = os.listdir(PATH1)","bb6a981c":"img_rows=64\nimg_cols=64\nnum_channel=3 \n\nnum_epoch=3\nbatch_size=100\n","bb6f50eb":"img_data_list=[]\nclasses_names_list=[]\ntraining_label = []","c64f50a3":"process = psutil.Process(os.getpid())\nprint('start', process.memory_info().rss)","6f52e34d":"import cv2\n\nfor dataset in data_dir_list:\n    classes_names_list.append(dataset) \n    #print ('Loading images from {} folder\\n'.format(dataset)) \n    img_list=os.listdir(PATH+'\/'+ dataset)\n    img_path = dataset\n    for img in img_list:\n        input_img=cv2.imread(PATH + '\/'+ dataset + '\/'+ img )\n        input_img_resize=cv2.resize(input_img,(img_rows, img_cols))\n        img_data_list.append(input_img_resize)\n        training_label.append(img_path)","4206e77b":"img_data_list1=[]\nclasses_names_list1=[]\nTest_label = []","92982620":"import cv2\n\nfor dataset in data_dir_list1:\n    classes_names_list1.append(dataset) \n    #print ('Loading images from {} folder\\n'.format(dataset)) \n    img_list1=os.listdir(PATH1+'\/'+ dataset)\n    img_path1 = dataset\n    for img in img_list1:\n        input_img1=cv2.imread(PATH1 + '\/'+ dataset + '\/'+ img )\n        input_img_resize1=cv2.resize(input_img1,(img_rows, img_cols))\n        img_data_list1.append(input_img_resize1)\n        Test_label.append(img_path1)","a66bacae":"num_classes = len(classes_names_list)\nprint(num_classes)\n","2d0b1cbc":"training_label = np.array(training_label)\nTest_label = np.array(Test_label)","430ee197":"import numpy as np\nimg_data = np.array(img_data_list)\nimg_data = img_data.astype('float32')\nimg_data \/= 255","b66afc5f":"img_data1 = np.array(img_data_list1)\nimg_data1 = img_data1.astype('float32')\nimg_data1 \/= 255","c05a0971":"del img_data_list1\ndel img_data_list","f75b7054":"gc.collect()","7feb7bf7":"print (img_data.shape)","9258618c":"plt.imshow(img_data[1])","c2e01d0e":"label_to_id = {v : k for k, v in enumerate(np.unique(training_label))}\nid_to_label = {v : k for k, v in label_to_id.items()}\nlabel_to_id1 = {v : k for k, v in enumerate(np.unique(Test_label))}\nid_to_label1 = {v : k for k, v in label_to_id1.items()}","b9b94deb":"## Uncomment to check the list of labels\n#id_to_label1","0c8a744a":"num_of_samples = img_data.shape[0]\ninput_shape = img_data[0].shape\nTest_num_of_samples = img_data1.shape[0]\nTest_input_shape = img_data1[0].shape","0acc20ad":"classes = np.ones((num_of_samples,), dtype='int64')\nTest_classes = np.ones((Test_num_of_samples,), dtype='int64')\nclasses = np.array([label_to_id[i] for i in training_label])\nTest_classes = np.array([label_to_id1[i] for i in Test_label])","3fece4a6":"len(Test_label)","a0b49bb2":"len(training_label)","7d90247a":"from keras.utils import to_categorical\n\nclasses = to_categorical(classes, num_classes)\nTest_classes = to_categorical(Test_classes, num_classes)","5e6f65c5":"from sklearn.utils import shuffle\n\nX, Y = shuffle(img_data, classes, random_state=2)\nX_test,Y_test = shuffle(img_data1,Test_classes,random_state=4)","3d499b84":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=2)","df3b4c7f":"X_train.shape","ef9fd19f":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,Activation,BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D","cce8019e":"model = Sequential()\n\nmodel.add(Conv2D(16, (3, 3), input_shape=input_shape,kernel_initializer=\"glorot_uniform\" ))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(16, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(32, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(32, (3, 3),kernel_initializer='glorot_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\n\n\nmodel.add(Conv2D(64, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(64, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax',kernel_initializer=\"glorot_uniform\"))","786e2a36":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])","88f5365b":"model.summary()","4fc658b0":"history = model.fit(X_train, y_train,validation_data=(X_val, y_val), epochs=100, batch_size=128, verbose=0)","480e72a1":"import matplotlib.pyplot as plt\n\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","185a6e7e":"model.evaluate(X_test,Y_test)","3d90398e":"model = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), input_shape=input_shape,kernel_initializer=\"glorot_uniform\" ))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(64, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(128, (3, 3),kernel_initializer='glorot_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Conv2D(256, (3, 3),kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(600,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(300,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(150,kernel_initializer=\"glorot_uniform\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax',kernel_initializer=\"glorot_uniform\"))","683311a4":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\nmodel.summary()","bf01370b":"history = model.fit(X_train, y_train,validation_data=(X_val, y_val), epochs=5, batch_size=128, verbose=1)","345831f0":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e6136391":"model.evaluate(X_test,Y_test)","123e9ac6":"from IPython.display import Image\nImage(filename='..\/input\/cnn-png\/CNN.png')","96e6260d":"from keras import models\nfrom keras.callbacks import ModelCheckpoint\nimport glob\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport imageio as im","4e8cf163":"layer_outputs = [layer.output for layer in model.layers[:20]] # Extracts the outputs of the top 12 layers","a3a32ce7":"activation_model = models.Model(inputs=model.input, outputs=layer_outputs) ","72398f4c":"import numpy as np\nimg_tensor = np.array(img_data[:15])\nimg_tensor = img_tensor.astype('float32')\nimg_tensor \/= 255","a68d78c2":"print(img_tensor.shape)","39854b3f":"activations = activation_model.predict(img_tensor)","53eeb680":"layer_names = []\nfor layer in model.layers[:20]:\n    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n    \nimages_per_row = 16\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","3313af2b":"### Assigning the labels to data","a072550f":"### Train, Test and Validation Split","85548c0b":"### What is Convolution?\n\n![alt text](https:\/\/leonardoaraujosantos.gitbooks.io\/artificial-inteligence\/content\/more_images\/Convolution_schematic.gif \"Logo Title Text 1\")\n\n![alt text](http:\/\/xrds.acm.org\/blog\/wp-content\/uploads\/2016\/06\/Figure_2.png \"Logo Title Text 1\")\n\n- A convolution is an orderly procedure where two sources of information are intertwined.\n\n- A kernel (also called a filter) is a smaller-sized matrix in comparison to the input dimensions of the image, that consists of real valued entries.\n\n- Kernels are then convolved with the input volume to obtain so-called \u2018activation maps\u2019 (also called feature maps).  \n- Activation maps indicate \u2018activated\u2019 regions, i.e. regions where features specific to the kernel have been detected in the input. \n\n- The real values of the kernel matrix change with each learning iteration over the training set, indicating that the network is learning to identify which regions are of significance for extracting features from the data.\n\n- We compute the dot product between the kernel and the input matrix. -The convolved value obtained by summing the resultant terms from the dot product forms a single entry in the activation matrix. \n\n- The patch selection is then slided (towards the right, or downwards when the boundary of the matrix is reached) by a certain amount called the \u2018stride\u2019 value, and the process is repeated till the entire input image has been processed. - The process is carried out for all colour channels.\n\n- instead of connecting each neuron to all possible pixels, we specify a 2 dimensional region called the \u2018receptive field[14]\u2019 (say of size 5\u00d75 units) extending to the entire depth of the input (5x5x3 for a 3 colour channel input), within which the encompassed pixels are fully connected to the neural network\u2019s input layer. It\u2019s over these small regions that the network layer cross-sections (each consisting of several neurons (called \u2018depth columns\u2019)) operate and produce the activation map. (reduces computational complexity)\n![alt text](http:\/\/i.imgur.com\/g4hRI6Z.png \"Logo Title Text 1\")\n![alt text](http:\/\/i.imgur.com\/tpQvMps.jpg \"Logo Title Text 1\")\n![alt text](http:\/\/i.imgur.com\/oyXkhHi.jpg \"Logo Title Text 1\")\n\n### Max Pooling\n\n![alt text](http:\/\/xrds.acm.org\/blog\/wp-content\/uploads\/2016\/06\/Figure_6.png \"Logo Title Text 1\")\n\n- Pooling reducing the spatial dimensions (Width x Height) of the Input Volume for the next Convolutional Layer. It does not affect the depth dimension of the Volume.  \n- The transformation is either performed by taking the maximum value from the values observable in the window (called \u2018max pooling\u2019), or by taking the average of the values. Max pooling has been favoured over others due to its better performance characteristics.\n- also called downsampling\n","2e4c1603":"![alt text](http:\/\/xrds.acm.org\/blog\/wp-content\/uploads\/2016\/06\/Figure1.png \"Logo Title Text 1\")\n\n- Every image is a matrix of pixel values. \n- The range of values that can be encoded in each pixel depends upon its bit size. \n- Most commonly, we have 8 bit or 1 Byte-sized pixels. Thus the possible range of values a single pixel can represent is [0, 255]. \n- However, with coloured images, particularly RGB (Red, Green, Blue)-based images, the presence of separate colour channels (3 in the case of RGB images) introduces an additional \u2018depth\u2019 field to the data, making the input 3-dimensional. \n- Hence, for a given RGB image of size, say 255\u00d7255 (Width x Height) pixels, we\u2019ll have 3 matrices associated with each image, one for each of the colour channels. \n- Thus the image in it\u2019s entirety, constitutes a 3-dimensional structure called the Input Volume (255x255x3).","a0f98417":"### Creating 95 different catagorical fetures to reprosent 95 classes, as keras will accept the data only if it is in this format.","f474e628":"### Read the images and store them in the list","9cf02b04":"### Get the number of classes of images","9b6574bb":"## Convolution Neural Network ","3008e4ce":"### Loding the data from the file","7df55491":"### Image Pre-Processing","6397f9ca":"### Similar Flow of my model\n* This below image showes the architecture of VGG Net","09884e80":"* In order to Normalise this data we need the values in Float.So, we are converting them.\n* Dividing the data with 255 will normalize the pixel intensity values.","1165cb02":"### Required variables declaration and initialization","1137f74d":"### Here it comes, this network works well.\n* Once I have realised the mistakes done in my previous model.\n* I have rebuild it by making changes to overcome those issues mentioned above.","bf7a856b":"#### Previous CNN is no good because of some mistakes\n* I have used a shallow but deep Convolution network which is not good for large data.But Works fine with small datasets.\n* Used Dropouts between the Convolution layes which sometimes drops the importent wights. Do not use dropouts between Convolution Layers.\n* Build a dense network which is vary shallow, because of which if could not capture enough information.\n* Final Dense layer was having only 16 nodes where as my output has 95 class. This is one of the main reason that my previous network Fail.","d1d07ad8":"### Second CNN ","e6da17dc":"### First CNN","9dece506":"### Checking the image after normalizing. "}}