{"cell_type":{"a301c8f6":"code","a54e77c6":"code","76db1fae":"code","8e0f412c":"code","fa17205d":"code","3ce5c74d":"code","4b7371a3":"code","e616b557":"code","c3488fb8":"code","c5b731c2":"code","a695644b":"code","f2d7811d":"code","c5e34b90":"code","8de282f1":"code","316223f2":"code","a0075faa":"code","ac755585":"code","dec32a7a":"code","98f7559a":"code","f09e805b":"code","95d4c5d3":"code","cb3f4320":"code","0fce25f7":"code","ffa958e2":"code","eab74e18":"code","5f4e567a":"code","51fab36d":"code","0106f704":"code","35560915":"markdown","4ce93952":"markdown","a0f85e77":"markdown","b70d7814":"markdown","9b2b836c":"markdown","b01096a6":"markdown","c370382a":"markdown","a5e42cfb":"markdown","b0384cba":"markdown","f6f7ceb3":"markdown","a24db2de":"markdown"},"source":{"a301c8f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a54e77c6":"df = pd.read_csv('..\/input\/BreadBasket_DMS.csv')","76db1fae":"df.head(10)","8e0f412c":"print(\"Length of the dataset: %d\\nNumber of different items: %d\" % (len(df), len(df.Item.unique())))\n","fa17205d":"print(\"Number of different transactions: %d\" % df['Transaction'].max())","3ce5c74d":"nTransactions = df.groupby(['Transaction'])['Item'].count().reset_index()\nnTransactions.columns = ['Transaction', 'nItems']","4b7371a3":"#Checking...\nnTransactions.head()\n","e616b557":"print(\"Number of Items per transaction in average: %.2f\" % nTransactions['nItems'].mean())","c3488fb8":"nTransactions['nItems'].describe()","c5b731c2":"#Transforming original dataset. First, adding the new column \"number of Items, nItems\" to the original. Just joining by Transaction .\n# tt stands for \"transformed Transactions\" :o)\n#On the other hand, we build 'times' dataset for time analysis\n\ntt = nTransactions.merge(df, on='Transaction')\ntimes = tt.drop_duplicates(subset='Transaction', keep='first')[['Transaction', 'nItems', 'Date', 'Time']]\ntimes.head()","a695644b":"timeCount = times.groupby('Time').count()['Date'].reset_index()\ntimeCount.head()\n#Ok, let's group by a more generic hour. What about hour and decimal of minutes? ;)\n","f2d7811d":"MIN_UNIT_INDEX=4\nHOUR_UNIT_INDEX=2\ntimes['hourMin'] = times['Time'].apply(lambda x: str(x)[:HOUR_UNIT_INDEX]+\"x\")\ntimes.head()","c5e34b90":"timeCount = times.groupby('hourMin').count()['Date'].reset_index()\ntimeCount.columns=['hourMin', 'countTransactions']\n\ntimeCount.head(10)","8de282f1":"len(timeCount)","316223f2":"timeCount.describe()","a0075faa":"timeCount.plot.bar(x='hourMin', y='countTransactions', rot=30, figsize=(15,10))","ac755585":"times['dayWeek'] = pd.to_datetime(times['Date']).dt.weekday_name","dec32a7a":"times.head()","98f7559a":"timesDay = times.groupby('dayWeek').count()['Transaction'].reset_index()\ntimesDay.columns = ['dayWeek', 'nTransactions']\n","f09e805b":"timesDay.plot.bar(x='dayWeek', y='nTransactions', rot=30, figsize=(15,10))","95d4c5d3":"times = times[['Transaction', 'nItems', 'hourMin', 'dayWeek']]\ndfplus = df.merge(times, on='Transaction')","cb3f4320":"dfplus.head()","0fce25f7":"res = dfplus.groupby(['dayWeek', 'Item']).count()['nItems'].reset_index()","ffa958e2":"res","eab74e18":"mostPerDay = res.groupby('dayWeek').agg(['min', 'max'])","5f4e567a":"mostPerDay","51fab36d":"totalItems = res.groupby('Item').sum().sort_values(by='nItems', ascending=False)","0106f704":"# Top 10 ;P\ntotalItems[:10]","35560915":"**RRC**: Now, same analysis by day of the week. ","4ce93952":"## **Read file**","a0f85e77":"**RRC**: Cool! Let's calculate top of the items selled","b70d7814":"**RRC**: Done! Ok, but let's dig a bit more... ","9b2b836c":"## Some conclusions\n\n* The top Item is Coffee (of course!)\n* But the max items selled per day are the Vegan mincepie and the Victorian Sponge\n* Would be nice to see how the sellings of this items evolve on time. ","b01096a6":"\n**RRC**: Ok, first ideas.\n* Let's see the distribution of the transactions along the time. First, hour of the day. Then we will calculate the day of the with corresponding to the date and see what do we have. \n* Evaluate which items are most demanded. \n* Evaluate those Items in time. ","c370382a":"## Exploratory analysis","a5e42cfb":"**RRC**: Now, let's join this dataframe with the original in order to get this info at line level","b0384cba":"**RRC**: Whoa! 12 is the max. And nice quartiles as well... Such a great information to work with.  ","f6f7ceb3":"**RRC observation**: Huh, interesting. A dataframe line is a transaction or a part of it. So, same transaction number implies is part of the same payment. See line 0 (Transaction=1) vs lines 1 and 2 (Transaction=2)","a24db2de":"**RRC observation**: Ok, almost 10k different transaction over a total of 21k lines of dataset. Expecting ~2 items per transaction as average, right? Let's confirm. "}}