{"cell_type":{"a33440de":"code","8f55e954":"code","60a6522e":"code","f50954d6":"code","bf33c1da":"code","c552f1d2":"code","eba627fa":"code","cc1009c5":"code","374a4884":"code","063133cb":"code","a16e55f1":"code","0e55dc59":"code","8890c664":"code","47ba3e14":"code","0f5070f0":"code","4f26da53":"code","a6653f96":"code","f5c30972":"code","2027a089":"code","f7c549ff":"code","0a68b588":"code","09c7c394":"code","ad70205d":"code","17eb520a":"code","f7226922":"code","0bd4ad4f":"code","a0060b70":"code","924a4a81":"code","9653961d":"code","5639a4cc":"code","d5a46c81":"code","21620c03":"code","4eba86ca":"code","31d1af83":"code","909be954":"code","bef0b91e":"code","bee774bb":"code","3fef1b51":"code","c5dc7c6b":"code","e847021a":"code","b1a51a25":"code","62d88ec0":"code","b46cdaf1":"code","9fa07a66":"code","7924eef9":"code","550012ca":"code","d89c9cc5":"code","5bb62b3c":"code","49db6bf2":"code","5a8177ec":"code","862b2ee8":"code","85446f0d":"code","3ec111dc":"code","b95f20b1":"code","8abc9c63":"markdown","7283f1a3":"markdown","64346d85":"markdown","ac9f2cda":"markdown","ba64d67f":"markdown","638a5302":"markdown","3d68cc70":"markdown","0fc6622a":"markdown","f00ea868":"markdown","3ce4578c":"markdown","930ce42d":"markdown","291abb98":"markdown","c0c607a7":"markdown","7baead87":"markdown","a21970df":"markdown","0fc89e41":"markdown","1888e934":"markdown"},"source":{"a33440de":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom subprocess import check_output","8f55e954":"# this part use functions from kernel CORD-19-EDA,parse JSON and generate clean csv\n# https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","60a6522e":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","f50954d6":"all_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\nfile = all_files[0]\nprint(\"Dictionary keys:\", file.keys())","bf33c1da":"cleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)\nlen(cleaned_files)","c552f1d2":"col_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head(5)","eba627fa":"#clean_df.to_csv('biorxiv_clean.csv', index=False)\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\n#pmc_df.to_csv('clean_pmc.csv', index=False)\n#pmc_df.head()\ncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)\n#comm_df.to_csv('clean_comm_use.csv', index=False)\n#comm_df.head()\nnoncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)\n#noncomm_df.to_csv('clean_noncomm_use.csv', index=False)\n#noncomm_df.head()\n","cc1009c5":"#check the number of json files from each directory\nprint(len(clean_df))\nprint(len(pmc_df))\nprint(len(comm_df))\nprint(len(noncomm_df))","374a4884":"bigdata1 = pd.concat([clean_df, pmc_df], ignore_index=True, sort =False)\nbigdata2 = pd.concat([comm_df, noncomm_df], ignore_index=True, sort =False)\nfinal_df = pd.concat([bigdata1, bigdata2], ignore_index=True, sort =False)\nprint(len(final_df))\n#final_df.to_csv('alljson.csv', index=False)\ndel clean_df,pmc_df,comm_df,noncomm_df,bigdata1,bigdata2","063133cb":"#final_df.columns","a16e55f1":"#final_df['paper_id'][:5]","0e55dc59":"meta_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv') # \n \nnRow, nCol = meta_df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","8890c664":"meta_df.head(5)","47ba3e14":"meta_df.columns","0f5070f0":"meta_df['sha'].isna().sum()  # null id number","4f26da53":"meta_df1=meta_df[meta_df['sha'].notnull()]\nprint(len(meta_df1))","a6653f96":"\"\"\"\n# If we want to merge the two data frames, meta_df1 is from meta data, and final_df is from json and has text info\n# but running on Kaggle encountered the memory issue, so this part is only if your machine has enough memory and \n# finding out how many paper id from json overlaps with metadata sha, so we can merge them together\nprint(len(list(set(meta_df1['sha'])&set(final_df['paper_id']))))\n#merge two dataframes\npd_merge_all= pd.merge(meta_df1, final_df, how='inner',left_on='sha', right_on='paper_id')\nprint(len(pd_merge_all))\n# remove some duplicate columns\npd_merge_all=pd_merge_all[['cord_uid', 'sha', 'source_x', 'title_x', 'doi', 'pmcid', 'pubmed_id',\n       'license', 'abstract_x', 'publish_time', 'authors_x', 'journal',\n       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_pdf_parse',\n       'has_pmc_xml_parse', 'full_text_file', 'url', 'affiliations', 'text', 'bibliography',\n       'raw_authors', 'raw_bibliography']]\npd_merge_all=pd_merge_all.dropna(subset=['sha', 'text'])  # drop if any of these two columns have nan\n\n\"\"\"","f5c30972":"# since we didn't use the above step, so use meta data only\npd_merge_all=meta_df1","2027a089":"pd_merge_all.head()","f7c549ff":"import re\nimport nltk\nimport string","0a68b588":"# remove nan titles\nprint(len(pd_merge_all))\npd_merge_all=pd_merge_all[pd_merge_all['title']!='nan']\n# remove nan abstracts\nprint(len(pd_merge_all))\npd_merge_all=pd_merge_all[pd_merge_all['abstract']!='nan']\nprint(len(pd_merge_all))","09c7c394":"# data cleaning\ndef clean_dfonecol(new_df,col):\n    new_df=new_df.replace(np.nan,'',regex = True)\n    new_df = new_df[pd.notnull(new_df[col])] \n    #print(len(new_df))\n    # lower case\n    new_df[col] = new_df[col].apply(lambda x: x.lower()) \n    #punctuation\n    new_df[col] = new_df[col].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n    return new_df\n\n# below is the data with full text info, for now, dropped\n#pd_merge_all=clean_dfonecol(pd_merge_all,'title_x')\n#print(len(pd_merge_all))\n#pd_merge_all=clean_dfonecol(pd_merge_all,'abstract_x')\n#print(len(pd_merge_all))\n#pd_merge_all=clean_dfonecol(pd_merge_all,'text')    # too much text to process\n#print(len(pd_merge_all))","ad70205d":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop = stopwords.words('english')\n    \ndef remv_stopwords(new_df,col):\n# remove stopwords\n    \n    new_df[col]= new_df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    new_df[col]= new_df[col].str.findall('\\w{2,}').str.join(' ')\n    return new_df\n\n#pd_merge_all=remv_stopwords(pd_merge_all,'abstract_x')\n#pd_merge_all=remv_stopwords(pd_merge_all,'text') # takes too much memory, so not runnning it","17eb520a":"# only process abstract from meta_df1\n#print(len(meta_df1))\nmeta_df1=clean_dfonecol(pd_merge_all,'abstract')\nprint(len(meta_df1))","f7226922":"#print(len(meta_df1))\nmeta_df1=remv_stopwords(meta_df1,'abstract')\nprint(len(meta_df1))","0bd4ad4f":"meta_df1.reset_index(inplace=True)","a0060b70":"# put all abstract in one list, sentence by sentence\ntext1=[]\nfor i in range(len(meta_df1)):\n    text1.append(meta_df1['abstract'].loc[i])\n\n# put all, by words\ntext2=[]  \nfor i in range(len(meta_df1)):\n    text2.append(meta_df1['abstract'].loc[i].split())","924a4a81":"import gensim\nfrom gensim import corpora\nfrom pprint import pprint\n# Create dictionary by using all the words\ndictionary = corpora.Dictionary(text2)","9653961d":"# Get information about the dictionary\nprint(dictionary)","5639a4cc":"#print(dictionary.token2id[) ","d5a46c81":"# Create the Corpus\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in text2]\n#pprint(mycorpus[:10])   # this will show the token and its frequency in the text ","21620c03":"# create word count\nword_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n#pprint(word_counts[:100]) ","4eba86ca":"#import gensim.downloader as api\n#dataset = api.load(\"text8\")","31d1af83":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","909be954":"show_wordcloud(text2)","bef0b91e":"# Use gensim summarization \nfrom gensim.summarization import summarize, keywords\n#from pprint import pprint","bee774bb":"# Important keywords from the paragraph\nprint(text1[100])  # one abstract example\n# the keyword from this paragraph\nprint(keywords(text1[100]))  ","3fef1b51":"# Summarize the paragraph\n# need the comma to distinguish the sentences, just one example below\ntext1examp=\"outbreak pneumonia originating wuhan china generated 24500 confirmed cases including 492 deaths. february \\\n2020 virus 2019ncov spread elsewhere china, 24 countries including south korea, thailand, japan, usa. fortunately limited \\\nhumantohuman transmission outside china. assess risk sustained transmission whenever coronavirus arrives countries. data \\\ndescribing times symptom onset hospitalisation. 47 patients infected. early current outbreak used generate estimate \\\nprobability. imported case followed sustained humantohuman transmission assumptions. imported case representative  \\\npatients china. 2019ncov similarly transmissible sars. coronavirus probability imported case followed sustained  \\\nhumantohuman transmission. 041 credible interval. however mean time symptom onset hospitalisation halved.  \\\nintense surveillance probability imported case leads sustained transmission. 0012 credible interval. 0099 emphasises. \\\nimportance current surveillance efforts. countries around world ensure ongoing outbreak become global pandemic.\"\npprint(summarize(text1examp, word_count=20)) # this will be good for full text summarize","c5dc7c6b":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_documents, preprocess_string\n# tockenize abstract\nmeta_df_tokens = meta_df1.abstract.fillna('').apply(preprocess_string) ","e847021a":"!pip install rank_bm25","b1a51a25":"from rank_bm25 import BM25Okapi\n\n# tockenize abstract\n#pd_merge_abstract_tokens = pd_merge_all.abstract_x.fillna('').apply(preprocess_string)  # tokenize each abstract to word\n# tockenize the text, this is too memory consuming to be run on kaggle machine\n#pd_merge_text_tokens = pd_merge_all.text.fillna('').apply(preprocess_string)  # tokenize each textto word","62d88ec0":"meta_df_tokens[:100]","b46cdaf1":"from nltk.corpus import wordnet\nnltk.download('wordnet')\ndef find_syn_ant(word):\n    synonyms = []\n    antonyms = []\n\n    for syn in wordnet.synsets(word):\n        for l in syn.lemmas():\n            synonyms.append(l.name())\n            if l.antonyms():\n                antonyms.append(l.antonyms()[0].name())\n\n    #print(set(synonyms))\n    #print(set(antonyms))\n    return set(synonyms)\n\nprint(find_syn_ant(\"smoking\"))","9fa07a66":"bm25_index = BM25Okapi(meta_df_tokens.tolist())\n\ndef search(search_string, num_results=10):  # can change the num_results to top 50 or more\n    search_tokens = preprocess_string(search_string)\n    scores = bm25_index.get_scores(search_tokens)\n    top_indexes = np.argsort(scores)[::-1][:num_results]\n    return top_indexes","7924eef9":"# example: now show the abstract of the top index\nmeta_df1.loc[search('novel coronavirus treatment')][['abstract', 'publish_time']]","550012ca":"# Now break the above questions to the following search strings\nstring1='smoking, pre-existing pulmonary disease'\nstring2= 'Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities'\nstring3= 'Neonates and pregnant women'\nstring4=  'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences'\nstring5= 'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors'\nstring6=  'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups'\nstring7=  'Susceptibility of populations'\nstring8= 'Public health mitigation measures that could be effective for control'","d89c9cc5":"meta_df1.loc[search(string1)][['abstract', 'publish_time']]","5bb62b3c":"meta_df1.loc[search(string2)][['abstract', 'publish_time']]","49db6bf2":"meta_df1.loc[search(string3)][['abstract', 'publish_time']]","5a8177ec":"meta_df1.loc[search(string4)][['abstract', 'publish_time']]","862b2ee8":"meta_df1.loc[search(string5)][['abstract', 'publish_time']]","85446f0d":"meta_df1.loc[search(string6)][['abstract', 'publish_time']]","3ec111dc":"meta_df1.loc[search(string7)][['abstract', 'publish_time']]","b95f20b1":"meta_df1.loc[search(string8)][['abstract', 'publish_time']]","8abc9c63":"### Part 2: **Loading the metadata**","7283f1a3":"#### The above search is fast, but the search is only within the abstracts from meta data. If we have more memory, we can use the combined abstract and text data and search from text directly. It may significantly improve the query relevance score","64346d85":"The bm25 rank feature implements the Okapi BM25 ranking function used to estimate the relevance of a text document given a search query. It is a pure text ranking feature which operates over an indexed string field. The feature is very cheap to compute, about 3-4 times faster than nativeRank, while still providing a good rank score quality wise. It is a good candidate to use in a first phase ranking function when ranking text documents.","ac9f2cda":"#### It shows that abstracts  has 112323 unique tokens from 29500 papers, this number will change with added papers","ba64d67f":"Create a BM25Okapi index from the tokens. Implement a search function that returns the top 10 results from the search. Note that in search wer are asking the index to return the dataframe indexes of the tokens most similar to the search string.","638a5302":"The bm25 feature calculates a score for how good a query with terms q1,...,qn\n matches an indexed string field t in a document D. The score is calculated as follows:\n![image.png](attachment:image.png)\nwhere f(qi,D) is qi's term frequency in the document D, |D| is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. k1 and b are free parameters, usually chosen, in absence of an advanced optimization. IDF(qi) is the IDF (inverse document frequency) weight of the query term qi. It is usually computed as:","3d68cc70":"### If the above library cannot be installed, please refer to this link to see the finished results I ran outside Kaggle \n\nhttps:\/\/github.com\/zhaotxtina\/COVID-19_Kaggle_Shell_team\/blob\/master\/cord-19-eda-BM25_query.pdf","0fc6622a":"###  Part 1: **Load all json and convert to dataframe**\nThis part can be run in Kaggle. However when doing the tokenizer, the kernel stopped due to memory limit reached.\nSo it's shown below to demonstrate how to do it. But some parts will be commented out so they are not excuted for the submission","f00ea868":"### Part 3: Merge two dataframes, metadata and all json text","3ce4578c":"### Part 5:  **Search Use BM25**","930ce42d":"## This section shows how to use both the text from four json folders and meta data to do a query to search for the information we need","291abb98":"##### Since text column will take a lot of memory, will only perform on abstract for now","c0c607a7":"###  Part 4   NLP processing ","7baead87":"![image.png](attachment:image.png)\nwhere N is the total number of documents in the collection, and n(qi) is the number of documents containing qi.","a21970df":"  #### Normalization of text\nThe following was only done to metadata abstract text, it can be easily done to alljson dataframe text column, but due to memory limit, only show the process to abstract ","0fc89e41":"### Text processing","1888e934":"### Below are the ones we need to check related to different risk factors:\n\n- Data on potential risks factors\n        - Smoking, pre-existing pulmonary disease\n        - Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n        - Neonates and pregnant women\n        - Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n- Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n- Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n- Susceptibility of populations\n- Public health mitigation measures that could be effective for control"}}