{"cell_type":{"c42387d2":"code","9ec7207b":"code","a265533d":"code","6bd3c3df":"code","d34128df":"code","aeed7221":"code","e2f05a98":"code","ad258425":"code","d239fde3":"code","8aa34486":"code","c4abc099":"code","ac1407c8":"code","cd0c2c02":"code","a2d5bf6a":"code","2256df81":"code","906f692a":"code","26f95dfd":"markdown","df426b75":"markdown","4c8a3a51":"markdown","d1cbd0db":"markdown","5b8ad4d0":"markdown","e87fa063":"markdown","7a4369bb":"markdown","e1cf9649":"markdown","5a06db66":"markdown","00b08114":"markdown"},"source":{"c42387d2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf","9ec7207b":"pd.options.display.max_columns = None\ntrain = pd.read_csv('..\/input\/iitgai-summer-course-capstone-challenge\/train.csv', index_col='Id')\nprint('Training data shape: ', train.shape)\ntrain.head()","a265533d":"train.info()","6bd3c3df":"test = pd.read_csv('..\/input\/iitgai-summer-course-capstone-challenge\/test.csv', index_col='Id')\nprint('Testing data shape: ', test.shape)\ntest.head()","d34128df":"submission = pd.read_csv('..\/input\/iitgai-summer-course-capstone-challenge\/sample_submission.csv', index_col='Id')\nsubmission.head()","aeed7221":"target = train['targets']\ntrain_cols = [col for col in train.columns if col not in ['targets']]\nX = train[train_cols]","e2f05a98":"X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3)","ad258425":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d239fde3":"# initializing the model\nmodel = tf.keras.Sequential()\n\n# adding the first layer - with 16 inputs and 4 outputs\nmodel.add(tf.keras.layers.Dense(4, input_shape=(16,)))\n\n# adding the second layer - with 4 inputs(need not specify) and 1 output target\nmodel.add(tf.keras.layers.Dense(1))","8aa34486":"# let us see the summary\nmodel.summary()","c4abc099":"# get the mean squared loss\nlos = tf.keras.losses.MeanSquaredError()\n\n# get the mean squared metric\nmetric = tf.keras.metrics.MeanSquaredError()\n\n# compile your words - which is just the technical word for \"getting ready!\"\nmodel.compile(optimizer = 'sgd',  # we will use the Stochastic Gradient Descent optimizer\n             loss = los,\n             metrics = [metric])","ac1407c8":"# batch size specifies, how many samples you want to train at a time. \n# let us train for 5 epochs now\n\n# tensorflow keras also returns the history of the training for us, to examine later\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=5)","cd0c2c02":"plt.title(\"Mean squared error\")\nplt.plot(history.history['mean_squared_error'])\nplt.show()","a2d5bf6a":"plt.title(\"Loss\")\nplt.plot(history.history['loss'])\nplt.show()","2256df81":"# training predictions\ntpred = model.predict(X_train)\ntscore = mean_squared_error(y_train, tpred)\n\n# validation predictions\nvpred = model.predict(X_test)\nvscore = mean_squared_error(y_test, vpred)\n\nprint(f'Training score   - {tscore:.3f} \\nValidation score - {vscore:.3f}')","906f692a":"# get the data and transform\nfinal_test = test.values\nfinal_test = scaler.transform(final_test)\n\n# predict\npreds = model.predict(final_test)\n\n# put it in submission file\nsubmission['targets'] = preds\n\n# save the file\nsubmission.to_csv(\"submission.csv\")","26f95dfd":"Predict on test and create the submission!","df426b75":"Lets check our training","4c8a3a51":"Drop Target column from your training data","d1cbd0db":"This Starter Notebook will guide you on how to make your first Kaggle Submission, from loading data to making predictions using the most basic model. You can use the \"Copy and Edit\" button in the upper right of the page to create your own copy of this notebook and experiment on your own.\n\nWe start off by importing all the relevant libraries.","5b8ad4d0":"# Reading the Data","e87fa063":"# Baseline model\n\nLet us build a small neural network to predict the values","7a4369bb":"Let the Training begin!","e1cf9649":"We standardize our data, i.e rescale the data so that the mean of observed values is 0 and the standard deviation is 1. Standardisation is an important pre processing step since it helps the algorithm converge faster and make our model more robust.","5a06db66":"There are 73 Parameters for us to train! <br>\nDon't worry tensorflow will take care of all of it! but hope you remember how this all works ;)","00b08114":"Use Train Test Split to test the performance of your model. This helps you get a better idea if your model is overfitting on the training data or not."}}