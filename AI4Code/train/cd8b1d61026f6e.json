{"cell_type":{"6fb50aef":"code","ebd79697":"code","445934fb":"code","e974c768":"code","9c67bad2":"code","3bbed1a0":"code","d4196af8":"code","f6ff865d":"code","31011c10":"code","95f32ed6":"code","68d9383f":"code","fa525917":"code","95d418e0":"code","7c9e98d7":"code","9eab3e76":"code","8bb4dbe6":"code","b6ae278a":"code","b9dced2a":"code","3cfc2b10":"markdown","9f466b5f":"markdown","87fa25d3":"markdown","72223543":"markdown","1704f33a":"markdown","ac964a17":"markdown","6f7f7cb9":"markdown","58922f21":"markdown","8cd53c82":"markdown","bd2fe394":"markdown"},"source":{"6fb50aef":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport string\nimport tokenizers\nimport transformers\n\nimport torch\nimport torch.nn as nn\n\nfrom sklearn import model_selection\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","ebd79697":"MAX_LEN = 192\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\nROBERTA_PATH = \"..\/input\/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\nDEVICE = 'cuda'\nTRAIN_PATH = '..\/input\/tweet-sentiment-extraction\/train.csv'\nFOLDS = 5","445934fb":"train_df = pd.read_csv(TRAIN_PATH)\ntrain_df['kfolds'] = -1\nkf = model_selection.StratifiedKFold(n_splits = FOLDS, shuffle = False, random_state = 10)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X = train_df, y=train_df.sentiment.values)):\n    print(len(train_idx), len(val_idx))\n    train_df.loc[val_idx, 'kfolds'] = fold\ntrain_df","e974c768":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","9c67bad2":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # Let's take pretrained weights for our model\n        config = transformers.RobertaConfig.from_pretrained(\n            '..\/input\/roberta-base\/config.json', output_hidden_states=True)    \n        self.roberta = transformers.RobertaModel.from_pretrained(\n            '..\/input\/roberta-base\/pytorch_model.bin', config=config)\n        self.drop_out = nn.Dropout(0.3)\n        # The final layer will have two output features for start and end indexes.\n        self.fc = nn.Linear(768, 2)\n#         nn.init.normal_(self.l0.weight, std = 0.02)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n        \n    def forward(self, ids, mask):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask = mask\n        )\n        out = torch.stack([out[-1], out[-2], out[-3], out[-4]])\n        out = torch.mean(out, 0)\n        out = self.drop_out(out)\n        logits = self.fc(out)\n        start_logits, end_logits = logits.split(1, dim = -1)\n        \n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        return start_logits, end_logits","3bbed1a0":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    '''\n    tweet - Tweet from which we have to perform sentiment extraction.\n    selected_text - Expected output of sentiment extraction\n    sentiment - sentiment to extract (positive, negative or neutral)\n    tokenizer - tokenizer to be used for creating tokens\n    max_len - max length of tweet.\n    '''\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n    \n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n    \n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind + len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n            \n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        char_targets[idx0: idx1 + 1] = [1]*(idx1 + 1 - idx0)\n\n    tok_tweets = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweets.ids\n    tweet_offsets = tok_tweets.offsets\n    \n    target_idx = []\n    \n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n            \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0,0)]\n    targets_start += 4\n    targets_end += 4\n    \n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n        mask = mask + [0] * padding_length\n        token_type_ids += [0] * padding_length\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","d4196af8":"df_train = train_df[train_df.kfolds == 1].reset_index(drop = True)\nprint('Tweet: ' + df_train.iloc[0]['text'])\nprint('Sentiment: ' + df_train.iloc[0]['sentiment'])\nprint('Expected Output: ' + df_train.iloc[0]['selected_text'])\noutput = process_data(df_train.iloc[0]['text'], df_train.iloc[0]['selected_text'], df_train.iloc[0]['sentiment'],TOKENIZER, MAX_LEN)\n\nprint('Tokens: ')\nprint(output['ids'])\nprint('Token Types:')\nprint(output['token_type_ids'])\nprint('Mask:')\nprint(output['mask'])\nprint('Offsets:')\nprint(output['offsets'])","f6ff865d":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item],\n            self.selected_text[item],\n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        \n        return {\n            'ids': torch.tensor(data['ids'], dtype = torch.long),\n            'mask': torch.tensor(data['mask'], dtype = torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype = torch.long),\n            'targets_start': torch.tensor(data['targets_start'], dtype = torch.long),\n            'targets_end': torch.tensor(data['targets_end'], dtype = torch.long),\n            'orig_tweet': data['orig_tweet'],\n            'orig_selected': data['orig_selected'],\n            'sentiment': data['sentiment'],\n            'offsets': torch.tensor(data['offsets'], dtype = torch.long)\n        }","31011c10":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output","95f32ed6":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","68d9383f":"\ndef train(data_loader, model, optimizer, device, scheduler = None):\n    model.train()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total = len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        token_type_ids = d[\"token_type_ids\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        sentiment = d[\"sentiment\"]\n        offsets = d[\"offsets\"]\n        \n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n        \n        model.zero_grad()\n        predicted_start, predicted_end = model(ids, mask)\n        loss = loss_fn(predicted_start, predicted_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        \n        predicted_start = torch.softmax(predicted_start, dim = 1).cpu().detach().numpy()\n        predicted_end = torch.softmax(predicted_end, dim = 1).cpu().detach().numpy()\n        \n        jaccard_scores = []\n        \n        for i, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[i]\n            tweet_sentiment = sentiment[i]\n            jaccard_score, _ = calculate_jaccard_score(\n                                original_tweet = tweet,\n                                target_string = selected_tweet,\n                                sentiment_val = tweet_sentiment,\n                                idx_start = np.argmax(predicted_start[i, :]),\n                                idx_end = np.argmax(predicted_end[i, :]),\n                                offsets = offsets[i]\n                                )\n            jaccard_scores.append(jaccard_score)\n            \n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            \n            tk0.set_postfix(loss = losses.avg, jaccard = jaccards.avg)\n        ","fa525917":"def eval_fn(data_loader, model, device, scheduler = None):\n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total = len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            sentiment = d[\"sentiment\"]\n            offsets = d[\"offsets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            predicted_start, predicted_end = model(ids, mask)\n            \n            loss = loss_fn(predicted_start, predicted_end, targets_start, targets_end)\n            \n            predicted_start = torch.softmax(predicted_start, dim = 1).cpu().detach().numpy()\n            predicted_end = torch.softmax(predicted_end, dim = 1).cpu().detach().numpy()\n            \n            jaccard_scores = []\n\n            for i, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[i]\n                tweet_sentiment = sentiment[i]\n                jaccard_score, _ = calculate_jaccard_score(\n                                    original_tweet = tweet,\n                                    target_string = selected_tweet,\n                                    sentiment_val = tweet_sentiment,\n                                    idx_start = np.argmax(predicted_start[i, :]),\n                                    idx_end = np.argmax(predicted_end[i, :]),\n                                    offsets = offsets[i]\n                                    )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n\n            tk0.set_postfix(loss = losses.avg, jaccard = jaccards.avg)\n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg","95d418e0":"def main(fold):\n    df_train = train_df[train_df.kfolds != fold].reset_index(drop = True)\n    df_val = train_df[train_df.kfolds == fold].reset_index(drop = True)\n    train_dataset = TweetDataset(\n                    tweet = df_train.text.values,\n                    sentiment = df_train.sentiment.values,\n                    selected_text = df_train.selected_text.values)\n    train_dataloader = torch.utils.data.DataLoader(\n                            train_dataset,\n                            batch_size = TRAIN_BATCH_SIZE,\n                            num_workers = 0\n                        )\n    valid_dataset = TweetDataset(\n                    tweet = df_val.text.values,\n                    sentiment = df_val.sentiment.values,\n                    selected_text = df_val.selected_text.values)\n    valid_dataloader = torch.utils.data.DataLoader(\n                            valid_dataset,\n                            batch_size = VALID_BATCH_SIZE,\n                            num_workers = 0\n                        )\n    \n    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(conf=model_config)\n    model.to(DEVICE)\n    \n    num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=2, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    for epoch in range(EPOCHS):\n        train(train_dataloader, model, optimizer, DEVICE, scheduler=scheduler)\n        jaccard = eval_fn(valid_dataloader, model, DEVICE)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n    ","7c9e98d7":"main(0)","9eab3e76":"main(1)","8bb4dbe6":"main(2)","b6ae278a":"main(3)","b9dced2a":"main(4)","3cfc2b10":"## Data","9f466b5f":"> ## Imports","87fa25d3":"## Training","72223543":"## Data Preprocessing","1704f33a":"## Driver Code","ac964a17":"## Utilities","6f7f7cb9":"## Model","58922f21":"## Config","8cd53c82":"### Let's look at the output of our processed data.","bd2fe394":"## Evaluation"}}