{"cell_type":{"77aa958f":"code","06f5b93c":"code","d1b56815":"code","74187e3a":"code","e2212bd2":"code","75e49128":"code","e15071b0":"code","5e6e1932":"code","a6428926":"code","a9040c58":"code","456f98b9":"code","c9535df1":"code","02b1fc0a":"code","6b95064a":"code","a2c595df":"code","94f5b618":"code","e7e1d039":"code","d72c089b":"code","e76b0e0e":"code","3e74df45":"code","1a5fd175":"code","67d38998":"code","541986e0":"code","d06c4c25":"code","30d69b06":"code","176f4087":"code","7c63b76f":"code","594ccb9d":"code","d058a513":"code","f41bc03e":"code","53a05284":"code","6a633c54":"code","a4a01672":"code","c9684e1a":"code","65b822f3":"code","61a58e49":"code","f6d01d7d":"code","6f1487b5":"code","14dae6d0":"code","80089be2":"code","2d17654a":"code","8f620d7c":"code","b738e8e6":"code","51eb6a02":"code","6e56c5ce":"code","f7637c34":"code","5821ffe4":"code","da8077d0":"code","6fe08f79":"code","e728b49f":"code","7982f62f":"code","3392ccbd":"code","b3e468fe":"markdown","cc225f04":"markdown","d07e1a53":"markdown","bb16b404":"markdown","6f4fb58f":"markdown","4855dc2f":"markdown","f479bde3":"markdown","9f1fa359":"markdown"},"source":{"77aa958f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","06f5b93c":"data = pd.read_csv('..\/input\/IBM_Employee.csv')\ndata.head()","d1b56815":"data.columns","74187e3a":"# #Converting certain features to categorical form\ncategorical_features = ['Attrition', 'BusinessTravel','Department','Education', 'EducationField',\n                        'EnvironmentSatisfaction', 'Gender','JobInvolvement', 'JobLevel', 'JobRole',\n                        'JobSatisfaction','MaritalStatus']\ndata[categorical_features] = data[categorical_features].astype('category')\ndata.info()","e2212bd2":"def categorical_eda(df):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"\n    print(\"To check: Unique count of non-numeric data\")\n    print(df.select_dtypes(include=['category']).nunique())\n    # Plot count distribution of categorical data\n    \n    for col in df.select_dtypes(include='category').columns:\n        if df[col].nunique() < 20:\n            fig = sns.catplot(x=col,hue='Attrition', kind=\"count\", data=df)\n            fig.set_xticklabels(rotation=90)\n            plt.show()\n        \n        \ncategorical_eda(data)","75e49128":"data.columns","e15071b0":"# #Converting certain features to categorical form\nNumerical_features = ['MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating',\n       'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n       'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']\ndata[Numerical_features] = data[Numerical_features].astype('int')\ndata[Numerical_features]","5e6e1932":"\ndef Numerical_eda(df):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"\n    print(\"To check: Unique count of numeric data\")\n    print(df.select_dtypes(include=['int64']).nunique())\n    # Plot count distribution of categorical data\n    \n    for col in df.select_dtypes(include='int64').columns:\n        if df[col].nunique() < 20:\n            g = sns.FacetGrid(data, col='Attrition')\n            g = g.map(sns.distplot, col)\n#             fig.set_xticklabels(rotation=90)\n            plt.show()\n        \n        \nNumerical_eda(data)\n\n\n# g = sns.FacetGrid(data, col='Attrition')\n# g = g.map(sns.distplot, \"WorkLifeBalance\")","a6428926":"def Numerical_eda(df):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"\n    print(\"To check: Unique count of numeric data\")\n    print(df.select_dtypes(include=['int32']).nunique())\n    # Plot count distribution of categorical data\n    \n    for col in df.select_dtypes(include='int32').columns:\n        if df[col].nunique() < 20:\n            g = sns.FacetGrid(data, col='Attrition')\n            g = g.map(sns.distplot, col)\n#             fig.set_xticklabels(rotation=90)\n            plt.show()\nNumerical_eda(data)","a9040c58":"data.info()","456f98b9":"# Explore Parch Attrition vs DistanceFromHome\ng  = sns.factorplot(y=\"DistanceFromHome\",x=\"Attrition\",hue=\"Department\",data=data,kind=\"bar\", size = 6 , palette = \"muted\")","c9535df1":"# Explore Parch Attrition vs DailyRate\ng  = sns.factorplot(y=\"DailyRate\",x=\"Attrition\",data=data,kind=\"bar\", size = 6 , palette = \"muted\")","02b1fc0a":"# Explore Parch Attrition vs HourlyRate\ng  = sns.factorplot(y=\"HourlyRate\",x=\"Attrition\",data=data,kind=\"bar\", hue=\"Gender\",size = 6 , palette = \"muted\")","6b95064a":"# Explore Parch Attrition vs MonthlyIncome\ng  = sns.factorplot(y=\"MonthlyIncome\",x=\"Attrition\",data=data,kind=\"bar\", size = 6 , palette = \"muted\")","a2c595df":"# Explore Parch Attrition vs MonthlyRate\ng  = sns.factorplot(y=\"MonthlyRate\",x=\"Attrition\",data=data,kind=\"bar\", size = 6 , palette = \"muted\")","94f5b618":"data.info()","e7e1d039":"data.describe(include=['object', 'bool'])\ndata['OverTime'] = data['OverTime'].map({'Yes':1, 'No':0}).astype('int')\ng = sns.barplot(x=\"OverTime\",y=\"Attrition\",data=data)","d72c089b":"g = sns.barplot(y=\"PerformanceRating\",x=\"Attrition\",hue=\"Gender\",data=data)","e76b0e0e":"g = sns.barplot(x=\"Attrition\",y=\"TotalWorkingYears\",hue=\"Gender\",data=data)","3e74df45":"# YearsInCurrentRole\ng = sns.barplot(x=\"Attrition\",y=\"YearsInCurrentRole\",hue=\"Gender\",data=data)","1a5fd175":"g = sns.barplot(x=\"Attrition\",y=\"YearsSinceLastPromotion\",hue=\"BusinessTravel\",data=data)","67d38998":"data.isna().sum()","541986e0":"data=data.drop(['EmployeeCount','EmployeeNumber','HourlyRate','Over18','PerformanceRating','StandardHours'],axis=1)","d06c4c25":"data.head()","30d69b06":"data.info()","176f4087":"plt.figure(figsize=(20,20))\ng = sns.heatmap(data.corr(),cmap=\"BrBG\",annot=True)","7c63b76f":"data.head()","594ccb9d":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()","d058a513":"# Here applying label encoder to categorical attribute by using column key name.\ndata['Attrition']=labelencoder.fit_transform(data['Attrition'])\ndata['Attrition'].unique()","f41bc03e":"data.columns","53a05284":"data['BusinessTravel']=labelencoder.fit_transform(data['BusinessTravel'])\ndata['Department']=labelencoder.fit_transform(data['Department'])\ndata['EducationField']=labelencoder.fit_transform(data['EducationField'])\ndata['Gender']=labelencoder.fit_transform(data['Gender'])\ndata['JobRole']=labelencoder.fit_transform(data['JobRole'])\ndata['MaritalStatus']=labelencoder.fit_transform(data['MaritalStatus'])\ndata['OverTime']=labelencoder.fit_transform(data['OverTime'])","6a633c54":"data.head()","a4a01672":"data.info()","c9684e1a":"data['Education'] = data['Education'].astype('int')\ndata['EnvironmentSatisfaction'] = data['EnvironmentSatisfaction'].astype('int')\ndata['JobInvolvement'] = data['JobInvolvement'].astype('int')\ndata['JobLevel'] = data['JobLevel'].astype('int')\ndata['JobSatisfaction'] = data['JobSatisfaction'].astype('int')","65b822f3":"data.info()","61a58e49":"y=data['Attrition'].values\nx=data.drop(['Attrition'],axis=1).values\nprint(X_train.shape)\nprint(y_train.shape)","f6d01d7d":"# dataset split.\ntrain_size=0.80\ntest_size=0.20\nseed=5\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)","6f1487b5":"n_neighbors=5\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# keeping all models in one list\nmodels=[]\nmodels.append(('LogisticRegression',LogisticRegression()))\nmodels.append(('knn',KNeighborsClassifier(n_neighbors=n_neighbors)))\nmodels.append(('SVC',SVC()))\nmodels.append((\"decision_tree\",DecisionTreeClassifier()))\nmodels.append(('Naive Bayes',GaussianNB()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnames=[]\npredictions=[]\nerror='accuracy'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,X_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\\\n    \n# # Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","14dae6d0":"# Spot Checking and Comparing Algorithms With StandardScaler Scaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import StandardScaler\npipelines=[]\npipelines.append(('scaled Logisitic Regression',Pipeline([('scaler',StandardScaler()),('LogisticRegression',LogisticRegression())])))\npipelines.append(('scaled KNN',Pipeline([('scaler',StandardScaler()),('KNN',KNeighborsClassifier(n_neighbors=n_neighbors))])))\npipelines.append(('scaled SVC',Pipeline([('scaler',StandardScaler()),('SVC',SVC())])))\npipelines.append(('scaled DecisionTree',Pipeline([('scaler',StandardScaler()),('decision',DecisionTreeClassifier())])))\npipelines.append(('scaled naive bayes',Pipeline([('scaler',StandardScaler()),('scaled Naive Bayes',GaussianNB())])))\n\n# # Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,X_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# # Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","80089be2":"# Ensemble and Boosting algorithm to improve performance\n\n#Ensemble\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Bagging methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',StandardScaler()),('AB',AdaBoostClassifier())])))\nensembles.append(('scaledGBC',Pipeline([('scale',StandardScaler()),('GBc',GradientBoostingClassifier())])))\nensembles.append(('scaledRFC',Pipeline([('scale',StandardScaler()),('rf',RandomForestClassifier(n_estimators=10))])))\nensembles.append(('scaledETC',Pipeline([('scale',StandardScaler()),('ETC',ExtraTreesClassifier(n_estimators=10))])))\n\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,X_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","2d17654a":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4,\n                           n_informative=2, n_redundant=0,\n                          random_state=0, shuffle=False)\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, y_train)\nAdaBoostClassifier(n_estimators=100, random_state=0)\nclf.feature_importances_\nclf.score(X_test,y_test)","8f620d7c":"import xgboost\nclassifier=xgboost.XGBClassifier()\n\nclassifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.5, gamma=0.4, learning_rate=0.1,\n       max_delta_step=0, max_depth=6, min_child_weight=7, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)\n\nfrom sklearn.model_selection import cross_val_score\nscore=cross_val_score(classifier,X_train, y_train,cv=10)\n\nscore\nscore.mean()","b738e8e6":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","51eb6a02":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout","6e56c5ce":"# Initialising the ANN\nclassifier = Sequential()","f7637c34":"X_train.shape","5821ffe4":"# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'he_uniform',activation='relu',input_dim = 28))","da8077d0":"# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'he_uniform',activation='relu'))\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'glorot_uniform', activation = 'sigmoid'))","6fe08f79":"# Compiling the ANN\nclassifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])","e728b49f":"# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 10, nb_epoch = 100)","7982f62f":"y_pred=classifier.predict(X_test)\ny_pred=(y_pred>0.5)\ny_pred\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test, y_pred))","3392ccbd":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b3e468fe":"## Modelling","cc225f04":"## Data Visualization","d07e1a53":"## Data Cleaning","bb16b404":"so from above analysis it can be seen that EmployeeCount,EmployeeNumber,HourlyRate,Over18,PerformanceRating,StandardHours is not adding any value for diffrentiation, hence we can ignore this columns","6f4fb58f":"## Neural Network ","4855dc2f":"--------XGBOOST------------","f479bde3":"----------ADABOOST------------","9f1fa359":"## Using LabelEncode library"}}