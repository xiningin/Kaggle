{"cell_type":{"614c3320":"code","04195ccf":"code","ef06c4f1":"code","7b42a6fd":"code","0f6e9936":"code","d146e134":"code","be04253a":"code","beaf1207":"code","552dae93":"code","15980c97":"code","f69e28fb":"code","e41f1091":"code","71e931dc":"code","24a83fe0":"code","f56d741b":"code","0350fc34":"code","458afadc":"code","c74601ae":"code","bfbd0fc2":"code","078bcf32":"code","cf1383d7":"code","2a9b317b":"code","e68e7e7a":"code","514f1454":"code","6ccef0a3":"code","8cd9d576":"code","e08e5739":"markdown","b9ab4afd":"markdown","c5f053dc":"markdown","8b6cdb78":"markdown","71536784":"markdown","2587eaa5":"markdown","74e01525":"markdown","67c6187a":"markdown","07234212":"markdown","81d0db7a":"markdown","7d2896a7":"markdown"},"source":{"614c3320":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\n\n\n#sklearn model\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.filterwarnings('ignore')","04195ccf":"# reduce memory\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","ef06c4f1":"train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)","7b42a6fd":"train.head()","0f6e9936":"FEATURES = [col for col in train.columns if col not in ['id', 'target']]","d146e134":"x_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\n\nx_test = test.drop('id', axis=1)","be04253a":"# x_train = x_train.iloc[:10000, :10]\n# y_train = y_train.iloc[:10000]\n# x_test = x_test.iloc[:10000, :10]\n\n# FEATURES = FEATURES[:10]\n\ndel train, test\ngc.collect()","beaf1207":"print('x_train shape: ', x_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('\\r')\nprint('x_train data null count: ', x_train.isnull().sum().sum())\nprint('y_train data null count: ', y_train.isnull().sum().sum())\nprint('x_test data null count: ', x_test.isnull().sum().sum())","552dae93":"x_train.info()","15980c97":"x_test.info()","f69e28fb":"x_train.describe().T","e41f1091":"x_test.describe().T","71e931dc":"#nothing to do","24a83fe0":"df = pd.concat([x_train[FEATURES], x_test[FEATURES]], axis=0)\ncon_feature = [col for col in FEATURES if df[col].nunique() > 2]\ncat_feature = [col for col in FEATURES if df[col].nunique() <= 2]\n\ndel df\ngc.collect()\n\nprint('con feature len: ', len(con_feature))\nprint('cat feature len: ', len(cat_feature))\nplt.pie([len(con_feature), len(cat_feature)], labels=['Continue', 'Categorate'], autopct='%1.1f%%')","f56d741b":"# target visualization\nplt.pie(y_train.value_counts(), labels=['One', 'Zero'], autopct='%1.1f%%')\nplt.axis('equal') ","0350fc34":"#feature visualization\nncols = 3\nnrows = x_train.shape[1] \/\/ ncols + (x_train.shape[1] % ncols != 0)\nfig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5*nrows))\n\nfor row in range(nrows):\n    for col in range(ncols):\n        index = row * ncols + col\n        if index >= x_train.shape[1] :\n            break\n        sns.kdeplot(x_train.iloc[:, index], ax=axes[row, col])\n        sns.kdeplot(x_test.iloc[:, index], ax=axes[row, col])","458afadc":"fig, ax = plt.subplots(1, 1, figsize=(16 , 16))\ncorr = x_train.sample(10000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1, vmax=0.1, vmin=-0.1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .85}, mask=mask ) \n\nax.set_title('Correlation heatmap: Numerical features', fontsize=24, y= 1.05);\n","c74601ae":"x_train[\"mean\"] = x_train[FEATURES].mean(axis=1)\nx_train[\"std\"] = x_train[FEATURES].std(axis=1)\nx_train[\"min\"] = x_train[FEATURES].min(axis=1)\nx_train[\"max\"] = x_train[FEATURES].max(axis=1)\n\nx_test[\"mean\"] = x_test[FEATURES].mean(axis=1)\nx_test[\"std\"] = x_test[FEATURES].std(axis=1)\nx_test[\"min\"] = x_test[FEATURES].min(axis=1)\nx_test[\"max\"] = x_test[FEATURES].max(axis=1)\n\nx_train.drop('f101', axis=1, inplace=True)\nx_test.drop('f101', axis=1, inplace=True)","bfbd0fc2":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\ny_train = y_train.values\ngc.collect()","078bcf32":"def objective(trial):\n\n    param_grid = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n              'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n              'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.1),\n              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n              'max_depth': trial.suggest_int('max_depth', 2, 20),\n              'booster': 'gbtree',\n              'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n              'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n              'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n              'random_state': 42,\n                 }\n\n    x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=50)\n    xgb_model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor',\n                            eval_metric=['logloss'])\n\n    xgb_model.fit(x_train_, y_train_, verbose=False)\n    y_pred = xgb_model.predict_proba(x_val)[:, 1]\n    return roc_auc_score(y_val, y_pred)","cf1383d7":"train_time = 1 * 30 * 60 # h * m * s\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='XGBClassifier')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","2a9b317b":"xgb_params = trial.params\n# xgb_params = {}\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\nxgb_params['use_label_encoder'] = False,","e68e7e7a":"from sklearn.model_selection import KFold\n\nn_split = 10\nkfold = KFold(n_split)\n\nval_pred = np.zeros(y_train.shape)\ny_test = np.zeros((x_test.shape[0],))\n\nfor i, (train_index, val_index) in enumerate(kfold.split(x_train)):\n    # train model\n    print(\"fold {} training\".format(i))\n    model = XGBClassifier(**xgb_params, eval_metric=['logloss'])\n    model.fit(x_train[train_index], y_train[train_index])\n    \n    # predict val and test\n    val_pred[val_index] = model.predict_proba(x_train[val_index])[:, 1]\n    vla_score = roc_auc_score(y_train[val_index], val_pred[val_index])\n    print(\"fold {} validation auc score {}\".format(i, vla_score))\n    \n    y_test += model.predict_proba(x_test)[:, 1] \/ n_split\n    ","514f1454":"# evaluate validation score    \nprint(\"val auc score :\", roc_auc_score(y_train, val_pred))","6ccef0a3":"sub_mission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub_mission.target = y_test\nsub_mission.to_csv('submission.csv', index=False)","8cd9d576":"sns.kdeplot(y_test)","e08e5739":"### data visualization","b9ab4afd":"### linear regression","c5f053dc":"## Read Data","8b6cdb78":"### data shape","71536784":"## Train Model","2587eaa5":"### data cleanning","74e01525":"## Import Lib","67c6187a":"### feature engineering","07234212":"## Validation Score","81d0db7a":"## Submission","7d2896a7":"## EDA"}}