{"cell_type":{"7d89099c":"code","511da1ce":"code","63799525":"code","8f762994":"code","b66d3803":"code","0d9dc8f6":"code","da075c79":"code","a89e5f34":"code","eb651234":"code","bf66e3ed":"code","48ec5fe7":"code","65511ae2":"code","297e8d74":"code","37c55c80":"code","9a4dbc38":"code","daeb315b":"code","beb613bd":"code","e5d8003d":"code","57b97279":"code","c07674d3":"code","98500ca9":"code","727e15d2":"code","2b521e93":"code","bb372c46":"code","ee87f720":"markdown","8007beeb":"markdown","652c1b2b":"markdown","5aaf5d4d":"markdown","d5638d95":"markdown","b9521c60":"markdown","af5947d7":"markdown","2b69277a":"markdown","8a105e35":"markdown","7b79f4eb":"markdown","42a4c738":"markdown","7c6b1250":"markdown","8c2443c0":"markdown","b3c345a7":"markdown","d45c327b":"markdown","4de0f8a9":"markdown"},"source":{"7d89099c":"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport random\nimport gc\n\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModel\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport logging","511da1ce":"print(tf.__version__)","63799525":"config = {\n    'seed' : 42,\n    'model': '..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2',\n    'group': 'XLMRSquad2',\n    'batch_size': 4,\n    'max_length': 384,\n    'doc_stride': 128,\n    'device' : 'TPU',\n    'epochs' : 5,\n    'n_best_scores': 20,\n    'max_answer_length': 50,\n    \n    'use_transfer_learning' : False,\n    'use_extra_data': True,\n    'num_folds' : 8,\n    \n    'lr' : 5e-6,\n    'scheduler' : None,\n    'use_wandb': True,\n    'wandb_mode' : 'online',\n    \n    'use_dropout': False,\n    'dropout_value':0.1\n}","8f762994":"def seed_everything(seed = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \n# Creating a logger \ud83d\udcc3\ndef init_logger(log_file:str ='training.log'):\n    \n    # Specify the format \n    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n    \n    # Create a StreamHandler Instance\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.DEBUG)\n    stream_handler.setFormatter(formatter)\n    \n    # Create a FileHandler Instance\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(formatter)\n    \n    # Create a logging.Logger Instance\n    logger = logging.getLogger('Chaii-XLMR-TPU-MultiFold')\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\n\nLOGGER = init_logger()\nLOGGER.info(\"Logger Initialized\")\n\nseed_everything(config['seed'])\nLOGGER.info(\"Seed Setting done\")\n\n","b66d3803":"## To make this work with TPU\n\ndef get_device(device):\n    if device == 'TPU':\n        try: # detect TPUs\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n            strategy = tf.distribute.TPUStrategy(tpu)\n        except ValueError: # detect GPUs\n            print('Cannot initialize TPU')\n    else:\n        strategy = tf.distribute.MirroredStrategy() \n        \n\n    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    return strategy\n\nstrategy= get_device(config['device'])\nconfig['batch_size'] = config['batch_size'] * strategy.num_replicas_in_sync\n\nLOGGER.info(\"Effective batch size is \" + str(config['batch_size']))","0d9dc8f6":"if config['use_wandb']:\n    import wandb\n    from wandb.keras import WandbCallback\n    from kaggle_secrets import UserSecretsClient\n\n    if config['wandb_mode'] == 'offline':\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n        key='X'*40\n        wandb.login(key=key)\n    else:\n        user_secrets = UserSecretsClient()\n        wandb_api = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=wandb_api)\n\n    run = wandb.init(project='chaii', \n                     group =config['group'], \n                     job_type='train-tl',\n                     config = config)\n\n    LOGGER.info(\"Wandb is initialized\")","da075c79":"df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', encoding='utf-8')\ndf.head()","a89e5f34":"df.language.value_counts(normalize=True)*100","eb651234":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nclass ChaiiDataset:\n    def __init__(self, max_length, stride, tokenizer):\n        self.max_length = max_length\n        self.doc_stride = stride\n        self.pad_on_right = tokenizer.padding_side == \"right\"\n        self.tokenizer = tokenizer\n    \n\n    def run_tokenizer(self, data):\n        tokenized_data = self.tokenizer(\n        list(data[\"question\" if self.pad_on_right else \"context\"].values),\n        list(data[\"context\" if self.pad_on_right else \"question\"].values),\n        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n        max_length=self.max_length,\n        stride=self.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        )\n\n        return tokenized_data\n\n    def prepare_train_features(self, train_data):\n        tokenized_train_data = self.run_tokenizer(train_data)\n\n        sample_mapping = tokenized_train_data['overflow_to_sample_mapping']\n        offset_mapping = tokenized_train_data['offset_mapping']\n\n        tokenized_train_data[\"start_positions\"] = []\n        tokenized_train_data[\"end_positions\"] = []\n\n\n        for i, offsets in tqdm(enumerate(offset_mapping)):\n            input_ids = tokenized_train_data[\"input_ids\"][i]\n            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n\n            \n            sequence_ids = tokenized_train_data.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = list(train_data[\"answer_text\"].values)[sample_index]\n            \n            if len(train_data[\"answer_start\"]) == 0:\n                tokenized_train_data[\"start_positions\"].append(cls_index)\n                tokenized_train_data[\"end_positions\"].append(cls_index)\n            else:\n                start_char = list(train_data[\"answer_start\"].values)[sample_index]\n                end_char = start_char + len(answers)\n\n\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n                    token_start_index += 1\n\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n                    token_end_index -= 1\n\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_train_data[\"start_positions\"].append(cls_index)\n                    tokenized_train_data[\"end_positions\"].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_train_data[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_train_data[\"end_positions\"].append(token_end_index + 1)\n\n        return tokenized_train_data\n\n\n    def prepare_eval_features(self, eval_data):\n        tokenized_eval_data = self.run_tokenizer(eval_data)\n        sample_mapping = tokenized_eval_data.pop(\"overflow_to_sample_mapping\")\n\n        tokenized_eval_data[\"example_id\"] = []\n\n        for i in tqdm(range(len(tokenized_eval_data[\"input_ids\"]))):\n            sequence_ids = tokenized_eval_data.sequence_ids(i)\n            context_index = 1 if self.pad_on_right else 0\n\n            sample_index = sample_mapping[i]\n            tokenized_eval_data[\"example_id\"].append(eval_data[\"id\"].values[sample_index])\n\n            tokenized_eval_data[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(tokenized_eval_data[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_eval_data\n\n    def prepare_tf_data_pipeline(self, data, batch_size = 16, type='train'):\n        \n        if type=='train' or type=='valid':\n            tokenized_data = self.prepare_train_features(data)\n        else:\n            tokenized_data = self.prepare_eval_features(data)\n\n        input_ids = tokenized_data['input_ids']\n        attention_mask = tokenized_data['attention_mask']\n\n        def map_func(X_ids,  X_att, labels_s, labels_e):\n            return {'input_ids':X_ids,  'attention_mask': X_att}, {'start_positions': labels_s, 'end_positions': labels_e}\n\n        def map_func_eval(X_ids, X_att):\n            return {'input_ids':X_ids,  'attention_mask': X_att}\n\n        \n        if type=='train':\n            start_positions = tokenized_data['start_positions']\n            end_positions =  tokenized_data['end_positions']\n\n            dataset_train_raw = tf.data.Dataset.from_tensor_slices((input_ids,  attention_mask, start_positions,  end_positions))\n            dataset_train = dataset_train_raw.map(map_func).shuffle(1024).batch(batch_size).prefetch(buffer_size=AUTOTUNE) \n            return dataset_train\n        \n        if type=='valid':\n            start_positions = tokenized_data['start_positions']\n            end_positions =  tokenized_data['end_positions']\n\n            dataset_valid_raw = tf.data.Dataset.from_tensor_slices((input_ids,  attention_mask, start_positions,  end_positions))\n            dataset_valid = dataset_valid_raw.map(map_func).batch(batch_size).prefetch(buffer_size=AUTOTUNE) \n            return dataset_valid\n\n\n        if type == 'eval':\n            dataset_eval_raw = tf.data.Dataset.from_tensor_slices((input_ids,  attention_mask))\n            dataset_eval = dataset_eval_raw.map(map_func_eval).batch(batch_size)\n            return dataset_eval, tokenized_data       \n","bf66e3ed":"tokenizer = AutoTokenizer.from_pretrained(config['model'])\nprint(tokenizer)\nLOGGER.info('Tokenizer loaded')","48ec5fe7":"        \ndef postprocess(raw_dataframe, features, pred_start, pred_end):\n    postprocessed_predictions = {}\n    dict_local = {}\n\n    for index, item in enumerate(features['example_id']):\n        if item not in dict_local:\n            dict_local[item] = [index]\n        else:\n            dict_local[item].append(index)\n\n\n    for key, value in dict_local.items():\n        valid_answers = []\n        min_null_score = None\n        context = raw_dataframe[raw_dataframe['id'] == key]['context'].values[0]\n\n        for indx in value:\n            start = pred_start[indx]\n            end = pred_end[indx]\n\n            offset_mapping = features[\"offset_mapping\"][indx]\n            cls_index = features[\"input_ids\"][indx].index(tokenizer.cls_token_id)\n            feature_null_score = start[cls_index] + end[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start)[-1 : -config['n_best_scores'] - 1 : -1].tolist()\n            end_indexes = np.argsort(end)[-1 : -config['n_best_scores'] - 1 : -1].tolist()\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > config['max_answer_length']:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    local_context = features[\"input_ids\"][indx]\n                    valid_answers.append(\n                        {\n                            \"score\": start[start_index] + end[end_index],\n                            \"text\": context[start_char: end_char],\n                            \"token\": tokenizer.decode(local_context[start_char: end_char])\n                        }\n                    )\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        postprocessed_predictions[key] = best_answer['text']\n\n    return postprocessed_predictions\n","65511ae2":"dataset = ChaiiDataset(config['max_length'], config['doc_stride'], tokenizer)","297e8d74":"\ndef get_keras_model():\n    with strategy.scope():\n        bert = TFAutoModel.from_pretrained(config['model'], from_pt=True)\n\n        input_ids = layers.Input(shape=(config['max_length'],),  name='input_ids', dtype=tf.int32)\n        attention_mask = layers.Input(shape=(config['max_length'],), name='attention_mask', dtype=tf.int32)\n        embedding = bert(input_ids,  attention_mask=attention_mask)[0]\n\n        \n        if config['use_dropout']:\n            x1 = tf.keras.layers.Dropout(config['dropout_value'])(embedding)\n            x1 = tf.keras.layers.Conv1D(1,1)(x1)\n            \n            x2 = tf.keras.layers.Dropout(config['dropout_value'])(embedding)\n            x2 = tf.keras.layers.Conv1D(1,1)(x2)\n        else:\n            x1 = tf.keras.layers.Conv1D(1,1)(embedding)\n            x2 = tf.keras.layers.Conv1D(1,1)(embedding)\n\n        \n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax', name='start_positions')(x1)\n\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax', name='end_positions')(x2)\n\n        model = keras.Model(inputs=[input_ids, attention_mask],\n                            outputs=[x1, x2])\n        \n    \n    if config['scheduler']=='cosine_decay':\n        decay_steps = 600\n#         lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n#         initial_learning_rate = config['lr'], decay_steps=decay_steps)\n        #optimizer = tf.keras.optimizers.Adam(lr=decayed_learning_rate(config['lr'], decay_steps))\n    else:\n        optimizer = tf.keras.optimizers.Adam(lr=config['lr'])\n    \n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer)\n    \n    return model","37c55c80":"if config['use_transfer_learning']:\n    for layer in model.layers:\n        if 'tfxlm_roberta_model' in layer.name:\n            layer.trainable = False\n","9a4dbc38":"def scheduler(epoch, lr):\n    if epoch < 2:\n        return lr\n    else:\n        return lr * 0.125\n\ndef get_callbacks(fold):\n    bm = tf.keras.callbacks.ModelCheckpoint(f'best_model_fold_{fold}.h5',verbose=1, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)\n    scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    callbacks = [bm, scheduler] \n    return callbacks\n\n\ndef get_jaccard(str1, str2):\n        a = set(str1.split()) \n        b = set(str2.split())\n        if (len(a)==0) & (len(b)==0): return 0.5\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    \n    \ndef get_jaccard_score(y_true, y_pred):\n    score=0.0\n\n    for gt, pred in zip(y_true, y_pred):\n        score += get_jaccard(gt, pred)\n    score = score \/ len(y_pred)\n    return score","daeb315b":"df[\"fold\"] = -1\noof_start = np.zeros((len(df), config['max_length']))\nskf = StratifiedKFold(n_splits=config['num_folds'],shuffle=True,random_state=config['seed'])\n\nfor indx, (train_index, test_index) in enumerate(skf.split(X = df, y = df['language'])):\n    df.loc[test_index, 'fold'] = indx\n\nLOGGER.info(\"Data divided into folds\")","beb613bd":"%%time\ndf_test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ntest_dataset, test_tokenized = dataset.prepare_tf_data_pipeline(df_test, type='eval', batch_size=config['batch_size'])\nLOGGER.info(\"Test dataset is loaded.\")","e5d8003d":"df_extra_data = pd.read_csv('..\/input\/chaiiexternalextradata\/mlqa_hindi.csv')\ndf_extra_data_1 = pd.read_csv('..\/input\/chaiiexternalextradata\/xquad.csv')\nLOGGER.info(\"Extra dataset is loaded\")","57b97279":"pred_list = {}\njaccard_list = []","c07674d3":"global_model = get_keras_model()\nglobal_model.save_weights(\"base_model.h5\")\nLOGGER.info(\"Global model loaded\")","98500ca9":"for idx in range(config['num_folds']):\n    K.clear_session()\n    print(f\"############ Fold: {idx} ############\")\n    LOGGER.debug(\"Fold \"+str(idx))\n    df_train = df[df['fold'] != idx]\n    df_valid = df[df['fold'] == idx]\n    \n    if config['use_extra_data']:\n        df_train = pd.concat([df_train, df_extra_data])\n        df_train = pd.concat([df_train, df_extra_data_1])\n    \n    print(f\"### Preparing Data for Fold: {idx}\")\n    LOGGER.debug(\"Data Prep Started for Fold \"+str(idx))\n    \n    train_dataset = dataset.prepare_tf_data_pipeline(df_train , batch_size = config['batch_size'])\n    valid_dataset_for_epoch = dataset.prepare_tf_data_pipeline(df_valid, type='valid', batch_size=config['batch_size'])    \n    \n    callbacks = get_callbacks(idx)\n    if config['use_wandb'] and config['device']!='TPU':\n        callbacks.append( WandbCallback(save_model=False) )\n    \n    model = global_model\n    model.load_weights('base_model.h5')\n\n    print(f\"### Traning Data Prep done for Fold: {idx}\")\n    LOGGER.debug(\"Data Prep done for Fold \"+str(idx))\n\n    \n    print(f\"### Model training for Fold: {idx}\")\n    LOGGER.debug(\"Model training for Fold \"+str(idx))\n\n    history = model.fit(train_dataset, epochs=config['epochs'], callbacks=callbacks, verbose = 1, validation_data = valid_dataset_for_epoch)\n    model.load_weights(f'best_model_fold_{idx}.h5')\n    \n    print(f\"### Predicting on Valid set for Fold: {idx}\")\n    valid_dataset, valid_tokenized = dataset.prepare_tf_data_pipeline(df_valid, type='eval',  batch_size=config['batch_size'])\n    pred_valid_start, pred_valid_end = model.predict(valid_dataset, verbose = 1, workers=4)\n    valid_preds = postprocess(df_valid.reset_index(drop=True), valid_tokenized, pred_valid_start, pred_valid_end)\n    \n    y_valid_gt = []\n    y_valid_pred = []\n    for id, answer in valid_preds.items():\n        y_valid_gt.append(df_valid[df_valid['id'] == id]['answer_text'].iloc[0])\n        y_valid_pred.append(answer)\n        \n    \n     \n    jaccard_fold = get_jaccard_score(y_valid_gt, y_valid_pred)\n    jaccard_list.append(jaccard_fold)\n    \n    print(f\"### Jaccard for Fold {idx}: \", jaccard_fold)\n    \n    \n    pred_test_start, pred_test_end = model.predict(test_dataset, verbose = 1, workers=4)\n    pred_list['fold_'+str(idx)] = {'start': pred_test_start, 'end': pred_test_end }\n    del model\n    del valid_dataset\n    del valid_tokenized\n    del train_dataset\n\n    gc.collect()","727e15d2":"mean_jaccard = np.mean(jaccard_list)\nprint(f\"Mean jaccard is {mean_jaccard}\")\n","2b521e93":"start_ = []\nend_ = []\nfor i in range(0,config['num_folds']):\n    start_.append(pred_list[f'fold_{i}']['start'])\n    end_.append(pred_list[f'fold_{i}']['end'])\n\npred_test_start = np.mean(start_, axis=0)                           \npred_test_end = np.mean(end_, axis=0)    ","bb372c46":"test_preds = postprocess(df_test, test_tokenized, pred_test_start, pred_test_end)\npred_test_df = pd.DataFrame(test_preds.keys(), test_preds.values()).reset_index()\npred_test_df.columns = ['PredictionString', 'id']\npred_test_df = pred_test_df[['id', 'PredictionString']]\npred_test_df.to_csv('submission.csv', index=False)","ee87f720":"## Saving Predictions","8007beeb":"<a id=\"section8\"> <\/a>\n## Create Model","652c1b2b":"<a id=\"section4\"><\/a>\n## Setting Tokenizer","5aaf5d4d":"<a id=\"section10\"> <\/a>\n## Multi fold Training","d5638d95":"<a id=\"section9\"> <\/a>\n## Define Folds","b9521c60":"### Start Training","af5947d7":"<a id=\"section1\"><\/a>\n## Imports","2b69277a":"<a id=\"section6\"> <\/a>\n## Postprocessing Pipeline","8a105e35":"## Stack\n\n1. Pandas for Data Preprocessing\n\n2. tf.data from Tensorflow for Data Pipelines\n\n3. Hugging Face for pretrained model and tokenizer\n\n4. XLM Roberta Squad2 as the pretrained model\n\n5. Keras for Model training\n\n","7b79f4eb":"# Table of Contents\n\n1. [Imports](#section1)\n2. [Configuration](#section2)\n3. [Define Data Pipelines](#section3)\n4. [Setting Tokenizer](#section4)\n6. [Postprocessing Pipeline](#section6)\n8. [Creating Model](#section8)\n9. [Define folds](#section9)\n10. [Multi Fold Training](#section10)","42a4c738":"### Freezing Embedding layer","7c6b1250":"**If you learnt something from this kernel please don't forget to upvote**\n\nMy other kernels:\n\nhttps:\/\/www.kaggle.com\/harveenchadha\/chaii-muril-hf-keras-wb","8c2443c0":"## XLM Roberta Squad2\n\nIn this notebook I try to train model using Roberta finetuned model (Finetuned on Squad 2 dataset) combined with Hugging Face and Keras. The idea is to freeze the embeddings and train only the final layer parameters to create a baseline.\n\n\nI will be doing very less of EDA as other fellow kagglers have done an excellent job in explaining the dataset.\n\n**I will also be using extra dataset provided by other Kagglers\"\n\n**Note**: I am not using the Datasets library by HF because I love to work directly with pandas and tf.data so there are a couple of changes in the preprocesssing script that everyone has been using from HF notebook.","b3c345a7":"<a id=\"section3\"> <\/a>\n## Define Data Pipelines","d45c327b":"### Read Test data","4de0f8a9":"<a id=\"section2\"><\/a>\n## Configuration"}}