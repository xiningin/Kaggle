{"cell_type":{"bc3f2043":"code","db3a0ffb":"code","797fa195":"code","209eeb4b":"code","b3fad514":"code","beba5f8f":"code","8a7ac765":"code","37d2a530":"code","22855187":"code","71a9c4b3":"code","b6bf90b0":"code","283334ef":"code","1e320830":"code","b9488577":"code","21917b15":"code","54866c72":"code","bb5de244":"code","0c1f9274":"code","356e2aa3":"code","3c338d23":"code","da4b1a71":"code","0a621067":"code","b0782ed3":"code","1bd9182f":"code","05d76889":"code","e6bf97c8":"code","58fa274f":"code","4d411797":"code","f883d9c8":"code","f7d1264c":"code","89e1e997":"code","a670be91":"code","318e9741":"code","3c261b1f":"code","e1348a96":"code","dc5f7cbd":"code","1e0a0d61":"code","059eccfa":"code","eb63434b":"markdown","41244a17":"markdown","c0749e57":"markdown","8d99eda4":"markdown","e3ec3c75":"markdown","4d2dc95f":"markdown","f20c0064":"markdown","0c64ef1b":"markdown","b6584075":"markdown","829746c4":"markdown","a0ffa429":"markdown","c11eab9c":"markdown","0a300897":"markdown","a5e433fc":"markdown","2b6488be":"markdown","9e410a42":"markdown","45b39e63":"markdown","93849e37":"markdown","eb8397e2":"markdown","10f0146d":"markdown","a3438889":"markdown"},"source":{"bc3f2043":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import binary_crossentropy\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline","db3a0ffb":"df = pd.read_csv('creditcard.csv')","797fa195":"df.head()","209eeb4b":"df.info()","b3fad514":"df.shape","beba5f8f":"# Printing unique values present \ndf.nunique()","8a7ac765":"\ndf.describe().T","37d2a530":"# check for null values\ndf.isnull().sum()","22855187":"# Ploting the graph for transaction class distribution\n\ncount_classes = pd.value_counts(df['Class'], sort = True)\n\ncount_classes.plot(kind = 'bar', rot=0)\n\nplt.title(\"Transaction Class Distribution\")\n\nLabels = ['Legitimate','Fraud']\n\nplt.xticks(range(2), Labels)\n\nplt.xlabel(\"Class\")\n\nplt.ylabel(\"Frequency\")","71a9c4b3":"frauds = df.loc[df['Class'] == 1]\nlegitimate = df.loc[df['Class'] == 0]\nprint(\"We have\", len(frauds), \"fraud data points and\", len(legitimate), \"regular data points\")","b6bf90b0":"# amount of money used in different transaction classes\nfrauds.Amount.describe()","283334ef":"legitimate.Amount.describe()","1e320830":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(frauds.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(legitimate.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","b9488577":"## Correlation\n\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","21917b15":"df['Class'].value_counts()","54866c72":"x = df.drop('Class',axis=1)\ny = df['Class']","bb5de244":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.33,random_state = 0 ,stratify = y)","0c1f9274":"x_train.shape","356e2aa3":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","3c338d23":"y_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","da4b1a71":"x_test.shape","0a621067":"x_train.shape,x_test.shape","b0782ed3":"activation_function = 'relu'\nloss = 'binary_crossentropy'\nhidden_units_layer_1 = 34\nhidden_units_layer_2 = 36\noutput_units = 1\nbatch_size = 5\nepochs = 15\nlearning_rate = 0.001","1bd9182f":"model = Sequential([\n    Dense(hidden_units_layer_1, input_dim=30, activation=activation_function),\n    Dense(hidden_units_layer_2, activation=activation_function),\n    Dense(output_units, activation='sigmoid')\n])\nmodel.summary()","05d76889":"optimizer = Adam(lr = 0.001)","e6bf97c8":"model.compile(optimizer, loss= loss, metrics=['accuracy'])\n","58fa274f":"history = model.fit(x_train, y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=2)\n","4d411797":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nlegend = ['Train','Validation']","f883d9c8":"plt.plot(accuracy)\nplt.plot(val_accuracy)\nplt.title(\"Training v\/s Validation accuracy\")\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(legend);","f7d1264c":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nlegend = ['Training_Loss','Validation_loss']","89e1e997":"plt.plot(loss)\nplt.plot(val_loss)\nplt.title(\"Training Loss v\/s Validation loss\")\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.legend(legend);","a670be91":"lss,acc =  model.evaluate(x_test, y_test, verbose=0)\nprint(\"Loss: {} and Accuracy : {}\".format(lss,acc))\n","318e9741":"y_predicted = model.predict(x_test)\n","3c261b1f":"def plot_confusion_matrix(cm,classes,normalize=False,\n                          title='confusion_matrix',cmap=plt.cm.Blues):\n    plt.imshow(cm,interpolation='nearest',cmap= cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,classes,rotation=30)\n    plt.yticks(tick_marks,classes)\n    if normalize:\n        cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print('Normalized Confusion Matrix')\n    else:\n        print('confusion  matrix without normalization')\n    print(cm)\n    \n    thresh = cm.max()\/2\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,cm[i,j],\n                 horizontalalignment='center',\n                 color='white' if cm[i,j]>thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('true label')\n\n    plt.xlabel('predicted label')","e1348a96":"predictions = model.predict_classes(x_test,batch_size=100,verbose=0)\n","dc5f7cbd":"cm = confusion_matrix(y_test, predictions)\n","1e0a0d61":"import itertools\ncm_plot_labels = ['Legitimate', 'Fraudlent']\n_ = plot_confusion_matrix(cm,cm_plot_labels,title='Confusion_matrix')\n","059eccfa":"# Do not commit\n#model.save('creditcard_fraud_detection.h5')","eb63434b":"#### Training Accuracy vs Validation Accuracy","41244a17":"<a id=\"explore\"><\/a>\n### 3. Exploratory Analysis","c0749e57":"#### Import the required libraries","8d99eda4":"#### Set hyperparameters","e3ec3c75":"#### Model Accuracy","4d2dc95f":"#### Training Loss vs Validation Loss","f20c0064":"# Credit Card Fraud Detection\n\n## Contents\n\n### [1. Introduction](#intro)\n\n### [2. Data Preparation](#data)\n   * **Import the required libraries**\n   * **Download and unzip the dataset**\n   * **Split the dataset**\n   \n### [3. Exploratory Analysis](#explore)\n\n### [4. Model Architecture](#cnn)\n   * **Set hyperparameters**\n   * **Define the model**\n   * **Set optimizer** \n   * **Compile model**\n   * **Train model**\n\n### [5. Model Evaluation](#eval)\n   * **Training Accuracy vs Validation Accuracy**\n   * **Training Loss vs Validation Loss**\n   * **Model Accuracy**\n   * **Observations**\n\n### [6. Prediction](#predict)\n\n### [7. Save Model to Disk](#save)\n  ","0c64ef1b":"#### Observations:\n 99% is a good accuracy this is because previously I have used standard scaler for test and train data.The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. The test accuracy is also less it is about 0.007493834....","b6584075":"<a id=\"data\"><\/a>\n### 2. Data Preparation","829746c4":"#### Compile model","a0ffa429":"### Split the data","c11eab9c":"<a id=\"intro\"><\/a>\n### 1. Introduction","0a300897":"#### Set optimizer ","a5e433fc":"<a id=\"save\"><\/a>\n### 7. Save Model to Disk","2b6488be":"<a id=\"predict\"><\/a>\n### 6. Prediction","9e410a42":"<a id=\"eval\"><\/a>\n### 5. Model Evaluation","45b39e63":"#### Train model","93849e37":"#### About the dataset\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n#### Problem statement\nClassify the transactions as **fraud (1)** and **legitimate (0)**.\n\n#### Dataset link: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\/","eb8397e2":"#### Download and unzip the dataset","10f0146d":"#### Define the model","a3438889":"<a id=\"cnn\"><\/a>\n### 4. Model Architecture"}}