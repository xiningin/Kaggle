{"cell_type":{"9ea3da3d":"code","48c58900":"code","b4f51d36":"code","e683a260":"code","dfbec317":"code","d6f21b1c":"code","136fc23f":"code","7a08260e":"code","9da0224b":"code","76992090":"code","fb7391b9":"code","eb12781d":"code","a0f70154":"code","3d5b3ec8":"code","4104d350":"code","659e13a5":"markdown"},"source":{"9ea3da3d":"from fastai.tabular.all import *\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fastai.callback import *\nfrom tqdm.notebook import tqdm\nimport sys\nsys.path.append('\/kaggle\/input\/iterative-stratification\/iterative-stratification-master')\n#print(sys.path)\n#!ls ..\/input\/iterative-stratification\/iterative-stratification-master\n#from iterstrat import ml_stratifiers\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#from ml_stratifiers import MultilabelStratifiedKFold\nimport copy\nfrom torch.distributions.beta import Beta\nfrom sklearn.preprocessing import QuantileTransformer","48c58900":"pd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\nKAGGLE = True\nTRAIN = True\nINFERENCE = True\nPATH = '..\/input\/lish-moa\/' if KAGGLE else None\n\nprint(PATH)\ntest_features = pd.read_csv(PATH + 'test_features.csv')\ntrain_features = pd.read_csv(PATH  + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(PATH + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(PATH + 'train_targets_nonscored.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ndrug_ids = pd.read_csv(PATH + 'train_drug.csv')","b4f51d36":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","e683a260":"genes_cols = [col for col in train_features.columns if col.startswith('g-')]\ncells_cols = [col for col in train_features.columns if col.startswith('c-')]\n","dfbec317":"train_and_test_genes_features = pd.concat([train_features[genes_cols],test_features[genes_cols]])\ntrain_and_test_cell_features = pd.concat([train_features[cells_cols],test_features[cells_cols]])\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nn_comp_genes = 600\npca_genes = PCA(n_components=n_comp_genes)\npca_names_genes = ['pca_genes_' + str(i) for i in range(300)]#range(n_comp_genes)]\npca_genes.fit(train_and_test_genes_features)\nprint(pca_genes.explained_variance_ratio_[:300].sum())\nn_comp_cells = 60\npca_cells = PCA(n_components=n_comp_cells)\npca_names_cells = ['pca_cells_' + str(i) for i in range(n_comp_cells)]\npca_cells.fit(train_and_test_cell_features)\nprint(pca_cells.explained_variance_ratio_.sum())\n\n#np.sum(pca.explained_variance_ratio_),pca.explained_variance_ratio_\ntrain_pca_features_genes = pca_genes.transform(train_features[genes_cols])[:, :300]\ntest_pca_features_genes = pca_genes.transform(test_features[genes_cols])[:,:300]\ntrain_features[pca_names_genes] = train_pca_features_genes\ntest_features[pca_names_genes] = test_pca_features_genes\n\ntrain_pca_features_cells= pca_cells.transform(train_features[cells_cols])\ntest_pca_features_cells = pca_cells.transform(test_features[cells_cols])\ntrain_features[pca_names_cells] = train_pca_features_cells\ntest_features[pca_names_cells] = test_pca_features_cells\n\n#tmp_df = pd.DataFrame(columns = pca_names)\n#tmp_df[pca_names] = train_pca_features\n#train_features.info()","d6f21b1c":"target_scored_cols = train_targets_scored.columns.tolist()[1:]\ntarget_nonscored_cols = train_targets_nonscored.columns.tolist()[1:]\ntrain_df = train_features.merge(train_targets_scored,on='sig_id',how='left').merge(\n           train_targets_nonscored,on='sig_id',how='left').merge(\n           drug_ids,on='sig_id',how='left')\n \ndf = train_df.sample(frac=1.,random_state=2020)\n\ndf['kfold_scored'] = -1\nkf = MultilabelStratifiedKFold(n_splits=5)\ny = df[target_scored_cols + ['drug_id']].values\nfor fold, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold_scored'] = fold\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold    \ndf['kfold_nonscored'] = -1\nkf = MultilabelStratifiedKFold(n_splits=5)\n#kf = StratifiedKFold(n_splits=5)\ny = df[target_nonscored_cols + target_scored_cols + ['drug_id']].values\nfor fold, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold_nonscored'] = fold\n    \n#???df.to_csv    ","136fc23f":"test_sig_ids = test_features[test_features['cp_type'] == 'ctl_vehicle']['sig_id'].values\n#len(target_cols)\ncat_names =  ['cp_type', 'cp_time', 'cp_dose']# + ['n']\ncont_names = [c for c in train_features.columns if c not in cat_names \n               and c != 'sig_id'\n               and c != 'drug_id']  #+ pca_names#+ ['n']","7a08260e":"all_features = pd.concat([train_features[cont_names],test_features[cont_names]])\nmean  = all_features.mean().values\nstd = all_features.std().values","9da0224b":"from torch.nn import BCELoss\n\n\nclass LabelSmoothingCrossEntropyEgm(nn.Module):\n    y1 = None\n    lam = None\n    def __init__(self, smoothing:float=0.005, reduction='mean'):\n        super().__init__()\n        self.smoothing,self.reduction = smoothing,reduction\n        self.f = BCELoss(reduction = 'none')\n        \n    \n    def forward(self, output, target):\n        \n        c = output.size()[-1]\n        target=target.float()\n        with torch.no_grad():\n            target = target * (1.0 - self.smoothing) + 0.08 * self.smoothing\n        bce_loss = self.f(output, target)\n              \n        if (self.y1 == None):\n            loss = bce_loss.mean()\n        \n            return loss\n        else:\n            target1=self.y1.float()\n            with torch.no_grad():\n                 target1 = target1 * (1.0 - self.smoothing) + 0.08 * self.smoothing\n            bce_loss1 = self.f(output, target1)\n            all_loss = torch.lerp(bce_loss, bce_loss1, self.lam)\n            return all_loss.mean()\n    \n    \n #0.001 0.05   \n #0.005 0.08\nclass ChangeLoss(Callback):\n    _order = 90 #Runs after normalization and cuda\n    \n    valid_loss = BCELossFlat()\n    train_loss = LabelSmoothingCrossEntropyEgm()\n    \n    def before_batch(self, **kwargs):\n        val_condition = (self.learn.dls[1] == self.learn.dl)\n        \n        if (val_condition):\n            self.learn.loss_func = self.valid_loss\n        else:\n            self.learn.loss_func = self.train_loss\nclass NormalizeCallback(Callback):\n    def before_batch(self, **kwargs):\n         \n        \n        a_cat, a_cont = self.learn.xb\n        x_cont = a_cont.cpu()\n        #x_cont[:, -360:] = (a_cont[:,-360:].cpu() - pca_mean)\/pca_std\n        x_cont = (a_cont.cpu() - mean)\/std\n        x_cont = x_cont.float()\n        x_cont = x_cont.to(a_cont.device)\n        \n        self.learn.xb = (a_cat, x_cont)\na, b = None,None\nclass CatMixUp(Callback):\n    #run_after,run_valid = [Normalize],False\n    run_before = [Normalize]\n    def __init__(self, alpha=0.4): self.distrib = Beta(tensor(alpha), tensor(alpha))\n    \n    def before_batch(self, **kwargs):\n        ret_condition = (self.learn.dls[0] != self.learn.dl)\n        \n        if (ret_condition): #if not train do nothing\n            return \n        #global a,b\n        a=self.learn.xb \n        b, =self.learn.yb\n        #print(3\/0)\n        lam = self.distrib.sample((self.y.size(0),)).unsqueeze(-1)\n\n        a_cat, a_cont = a\n        x_cat = a_cat.detach().clone()\n        x_cont = a_cont.detach().clone()\n        y=b.detach().clone()\n\n        codes = x_cat[:,0]*16 + x_cat[:,1]*4 +x_cat[:,2]# categories common code\n        uniq_codes = torch.unique(codes)\n        for code in uniq_codes:\n            indexes = (codes==code).nonzero().view(-1) # at which indexes\n            ind_perm = torch.randperm(len(indexes))\n            x_cont[indexes] = x_cont[indexes[ind_perm]]\n            y[indexes] = y[indexes[ind_perm]]\n        \n        x_cont = x_cont.to(a_cont.device)\n        y=y.to(a_cont.device)\n        lam=lam.to(a_cont.device)\n        out_cont = torch.lerp(a_cont, x_cont, lam) \n        #####772+100+300+60\n        out_cont[:,772+100:772+100+300]=torch.tensor(pca_genes.transform(out_cont[:,:772].tolist())[:,:300])\n        out_cont[:,-60:]=torch.tensor(pca_cells.transform(out_cont[:, 772:772+100].tolist()))\n        #####\n        \n        \n        \n        out_y = torch.lerp(b.float(), y.float(), lam)\n        out_cont = out_cont.to(a_cont.device)\n        out_y = out_y.to(b.device)\n        self.learn.loss_func.lam = lam\n        self.learn.loss_func.y1 = y\n        self.learn.xb = (a_cat, out_cont)\n        self.learn.yb = (out_y,)        \n\nseeds = [42, 7, 9, 13, 37, 11, 5, 29, 31, 37, 41, 53]          \nBCE_LOSS = BCELoss(reduction='mean')#????","76992090":"def get_data(fold, target_names, \n             procs = [Categorify, FillMissing],\n             cat_names = cat_names,\n             cont_names = cont_names):\n    val_idx = df[df.kfold_nonscored==fold].index\n    dls = TabularDataLoaders.from_df(df, path=PATH, \n                                        y_names=target_names,\n                                        cat_names = cat_names,\n                                        cont_names = cont_names,\n                                        procs = procs,#, Normalize],\n                                        valid_idx=val_idx,                                        \n                                        bs=64)\n    return dls\ndef get_cbs(do_mixup):\n    ncb = NormalizeCallback()\n    ch_loss_cb = ChangeLoss()    \n    if do_mixup:\n        return  [ch_loss_cb,CatMixUp(),ncb]\n        #return  [ch_loss_cb,ncb] <- when I talked about scores I've changed exactly this line....\n    return [ncb, ch_loss_cb]\n\ntest_scores = []\nresults = []\n\ndef do_train_and_inf(num_iters=1, do_train=True, do_inference=False, cbs=get_cbs(False), \n                     lr=9e-3, epochs=5, target_names=target_scored_cols, \n                     use_pretr=False, pretr_model=False, file_name = 'something'):\n    global test_scores\n    global results\n    model = None\n    for ind in tqdm(range(num_iters)):\n        seed_everything(seeds[ind])\n        i = ind % 5 \n        dls = get_data(i, target_names = target_names) # Data\n        model_dir = '\/kaggle\/working\/' if TRAIN else '\/kaggle\/input\/fastai-egm'\n        config = tabular_config(ps=0.2)\n        learn = tabular_learner(dls , y_range=(0,1), \n                                layers = [1024, 512, 512, 256],                                \n                                loss_func = LabelSmoothingCrossEntropyEgm(),\n                                config=config,\n                                model_dir=model_dir,\n                            cbs=cbs\n                           ) # Model\n        model = learn.model\n    \n        if (use_pretr):\n            print(\"Will change model\")\n            remember = learn.model.layers[-2]    \n            learn.model = copy.deepcopy(pretr_model)#.load_state_dict(torch.load('\/kaggle\/working\/pretrained'))\n            learn.model.layers[-2] = remember\n\n        name = file_name + str(ind)\n    \n        cb = SaveModelCallback(monitor='valid_loss',fname=name ,mode='min') # Callbacks    \n        if (do_train):\n           \n            learn.fit_one_cycle(epochs, lr=slice(lr\/(2.6**4),lr), cbs=cb) # Training\n            results = results + [learn.recorder.loss.value.item()]\n            \n        if (do_inference):\n            learn.load(name) # Load best model\n                \n            test_dl = learn.dls.test_dl(test_features)#learn.dls.valid#learn.valid_dllearn.dls.test_dl(test_features)\n            sub = learn.get_preds(dl=test_dl) # prediction\n            test_scores.append(sub[0].numpy())\n    \n    #if TRAIN:\n        #learn.export('\/kaggle\/working\/'+name+'.pkl') # export model\n    \n    return model\n    \n    ","fb7391b9":"model = do_train_and_inf(num_iters=1, do_train = True, do_inference=False, \n                         cbs = get_cbs(False), lr = 9e-3, epochs = 5, \n                        target_names = target_nonscored_cols + target_scored_cols,\n                        use_pretr = False, pretr_model = None,\n                        file_name = 'pretrain_')\n","eb12781d":"do_train_and_inf(num_iters=10, do_train = True, do_inference=True, \n                cbs = get_cbs(True), lr = 9e-3, epochs = 10, \n                target_names = target_scored_cols,\n                use_pretr = True, pretr_model = model,\n                file_name = 'pretrain_')\ntest_sc = np.array(test_scores)","a0f70154":"for r in results:\n    print(str(r).replace('.',','))","3d5b3ec8":"avg_prds = test_sc.mean(axis=0)\nsubmission = sample_submission.copy()\nsubmission[target_scored_cols] = avg_prds\nsubmission.loc[submission['sig_id'].isin(test_features.loc[test_features['cp_type'] =='ctl_vehicle', 'sig_id']), train_targets_scored.columns[1:]] = 0\n#submission['atp-sensitive_potassium_channel_antagonist'] = 0\n#submission['erbb2_inhibitor'] = 0\nsubmission.to_csv('submission.csv', index=False)","4104d350":"'''\n#per=100\/train_targets_scored.ne(0).sum(axis=1)[1:].sum()\n#plt.hist(train_targets_scored.ne(0).sum(axis=1)[1:].values)\n\n#np.sort(train_targets_scored.ne(0).sum(axis=1)[1:].values), 100\/per\n#all_labels = train_targets_scored.ne(0).sum(axis=1)\n#l136 = train_targets_scored[target_cols[136]]\n#l163 = train_targets_scored[target_cols[163]]\n#l136_163 = l136 + l163\n#plt.hist(all_labels)\n#plt.hist(all_labels - l136_163)\n#plt.hist(all_labels)\n#plt.hist(train_targets_scored.loc[train_targets_scored[target_cols[163]] !=0].ne(0).sum(axis=1).values)\n#print(np.sort(train_targets_scored.ne(0).sum().values)[:-1])\n#plt.hist(np.sort(train_targets_scored.ne(0).sum().values)[:-1])\n#plt.hist(train_targets_nonscored.ne(0).sum(axis=1))\ntrain_targets_nonscored = pd.read_csv(PATH + 'train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv(PATH + 'train_targets_scored.csv')\ntrain_targets_nonscored['sum'] = train_targets_nonscored[train_targets_nonscored.columns[1:]].ne(0).sum(axis=1)\nids1 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==1]['sig_id']\nids2 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==2]['sig_id']\nids3 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==3]['sig_id']\nids4 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==4]['sig_id']\nids5 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==5]['sig_id']\nids0 = train_targets_nonscored.loc[train_targets_nonscored['sum'] ==0]['sig_id']\nids6 = train_targets_nonscored.loc[train_targets_nonscored['sum'] >5]['sig_id']\n#print(np.sort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids3)].ne(0).sum()[1:-1].values)[-15:-1])\nprint(np.argsort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids3)].ne(0).sum()[1:-1].values)[-15:-1])\n#print(np.sort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids2)].ne(0).sum()[1:-1].values)[-25:-1])\nprint(np.argsort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids2)].ne(0).sum()[1:-1].values)[-25:-1])\n#print(np.sort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids1)].ne(0).sum()[1:-1].values)[-25:-1])\nprint(np.argsort(train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids1)].ne(0).sum()[1:-1].values)[-25:-1])\n\nlen(ids0),len(ids1),len(ids2),len(ids3),len(ids4),len(ids5),len(ids6),\n#np.sort(train_targets_nonscored['sum'].values)\n#np.sort(train_targets_nonscored.loc[train_targets_nonscored['sum'] ==2].ne(0).sum()[1:-1].values)\n#plt.hist(train_targets_nonscored['sum'].values)\n#v = train_targets_nonscored.loc[train_targets_nonscored['sig_id'].isin(ids1)].ne(0).sum()[1:-1].values\n#np.sort(v), np.argsort(v)\n#train_targets_nonscored.columns[403]'''","659e13a5":"In this kernel I've tried to implement **mixup augmentation** (like saw in images). \n\nThis aproach gave me significant boost.\n(Private: 0.01653 -> 0.01637, Public: 0.01883-> 0.1871).\nSo in absolute values these are not big scores, however, it could be used as part of a solution).\n\nMixup for images had been described in (https:\/\/arxiv.org\/pdf\/1710.09412.pdf). \nIn short, we produce new row by mixing 2 existing: lambda * row1 + (1- lambda)* row2, where lambda is from Beta distribution (see wikipedia, and especiall this magic one https:\/\/en.wikipedia.org\/wiki\/Beta_distribution#\/media\/File:PDF_of_the_Beta_distribution.gif). In our case alfa equals beta. So most of the time, we take lambda close to zero, and mixing from time to time...\n\nIt could be done in each batch (I use fastai, so did that in callback). Then it should be supported by loss function...(I used loss smoothing, so even just gives good result. However, lambda * smoothed_loss(output_from_row1) + (1- lambda)*smoothed_loss(output_from_row2) was better to me.\nThen... we have categories, so I mixed inside each category (I made from all categories some code).\nThen... tricky part is pca...we could mixup pca features pca(lin_comb) != lin_comb(pca). So we need for lin_comb do pca anew.\nThen... In my experiments, mixup is better when normalization is before mixing, in pca - when normilization is after pca. I mean (x- mean)\/std by normalization\n\nAlso, mixup increases training time (more data).\n\nHope, that'll be useful for someone....\n"}}