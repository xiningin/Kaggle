{"cell_type":{"ec4f9aba":"code","6470da0e":"code","3952ab30":"code","c7609520":"code","7241bff0":"code","08552dd3":"code","f654eb65":"code","5774a68f":"code","1a32652f":"code","a334b323":"code","2694dc10":"code","1ec264f5":"code","302fc0fd":"code","3746fa68":"code","9fce6793":"code","de680bad":"code","2cfa7b6d":"code","7687f5dd":"code","352df1a1":"code","cd3e38ac":"code","b1b9f279":"code","f888be59":"code","0158a1f5":"code","97a3b50c":"code","b27acfb0":"code","d3507345":"code","460d81ef":"code","bc88378c":"code","6ed6f4b4":"code","6da4ac2f":"code","b4f4b55a":"code","60668415":"code","ec7484e4":"code","e729e191":"code","5d963888":"code","774d4d41":"code","d4a99718":"code","7a515dd4":"code","9d6e6739":"code","a3b613b5":"code","0ab91a22":"code","3d52372b":"code","3b572cd1":"code","058f03a7":"code","7d7a4657":"code","0de2aa1e":"code","84b44763":"code","c9f6b509":"code","bc27319d":"code","c52e9808":"code","af9da13c":"code","c8ce9ca2":"markdown","e056b8a2":"markdown","3fa2142d":"markdown","8f545d3c":"markdown","210216a3":"markdown","66888cd8":"markdown","339d90cb":"markdown","42e96d4e":"markdown","51c2efa0":"markdown","1239f3c8":"markdown","90e8e7ce":"markdown","cb9d2825":"markdown","33fb49dc":"markdown","c5163555":"markdown","1f77cdea":"markdown","4f09517d":"markdown","ef953e17":"markdown","4d71dc6c":"markdown","b108900e":"markdown","057835c4":"markdown","580257a4":"markdown","4041c68c":"markdown","f5c5e188":"markdown","7514d1e1":"markdown","718e3ea6":"markdown","610bc7fa":"markdown","e2d52724":"markdown"},"source":{"ec4f9aba":"#Trying to follow the concept of these 2 article, and more\n#https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n#https:\/\/www.kaggle.com\/mohamedtimor\/titanic-dataset\n\nimport numpy as np\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nimport warnings\nwarnings.filterwarnings('ignore')","6470da0e":"#get data\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\n#merge train & test data to perform data exploration and transformation \ndf = pd.concat([train_df, test_df] , sort = True)","3952ab30":"#Data Exploration\/Analysis\ndf.info()","c7609520":"df.describe()","7241bff0":"df.head(3)","08552dd3":"#check in details what data actuall missing\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent_1 = df.isnull().sum()\/df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\ndisplay(missing_data.head(5))\nprint(df.shape)","f654eb65":"# Check the correlation for the current numeric feature set.\nprint(df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr())\nsns.heatmap(df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr(), annot=True, fmt = \".2f\", cmap = \"coolwarm\")","5774a68f":"df.columns.values","1a32652f":"#Train set evaluate\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","a334b323":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","2694dc10":"sns.barplot(x='Pclass', y='Survived', data=train_df)","1ec264f5":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","302fc0fd":"train_df['relatives'] = train_df['SibSp'] + train_df['Parch']\naxes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","3746fa68":"titles = set()\nfor name in df['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nprint(titles)","9fce6793":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Dona\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}","de680bad":"#sur checking\nsur = set()\nfor name in df['Name']:\n    sur.add(name.split(' ')[0].replace(' ','').replace(',','').lower())\nprint(sur)","2cfa7b6d":"def get_titles():\n    # we extract the title from each name\n    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # a map of more aggregated title\n    # we map each title\n    df['Title'] = df.Title.map(Title_Dictionary)\n    return df\n\ndf = get_titles()\n\ndf['Title'].value_counts()","7687f5dd":"def get_sur():\n    df['sur'] = df['Name'].map(lambda name:name.split(' ')[0].replace(' ','').replace(',','').lower())\n    return df\n\ndf = get_sur()\ndf['sur'].value_counts()","352df1a1":"print(\"Train- Missing Age\")\nprint(train_df.iloc[:891].Age.isnull().sum())\n\nprint(\"Test- Missing Age\")\nprint(test_df.iloc[:891].Age.isnull().sum())","cd3e38ac":"grouped_train = df.groupby(['Sex','Pclass','Title'])\ngrouped_median_train = grouped_train.median()\ngrouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\ngrouped_median_train","b1b9f279":"def fill_age(row):\n    condition = (\n        (grouped_median_train['Sex'] == row['Sex']) & \n        (grouped_median_train['Title'] == row['Title']) & \n        (grouped_median_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_train[condition]['Age'].values[0]\n\n\ndef process_age():\n    global df\n    # a function that fills the missing values of the Age variable\n    df['Age'] = df.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n    return df\n\ndf = process_age()","f888be59":"#make dummies for title \ndef process_names():\n    global df\n    # we clean the Name variable\n    df.drop('Name', axis=1, inplace=True)\n    \n    # encoding in dummy variable\n    titles_dummies = pd.get_dummies(df['Title'], prefix='Title')\n    df = pd.concat([df, titles_dummies], axis=1)\n    \n    # removing the title variable\n    df.drop('Title', axis=1, inplace=True)\n    \n    return df\n\ndf = process_names()\ndf.sample(5)","0158a1f5":"#make dummies for sur \ndef process_sur():\n    global df\n    \n    # encoding in dummy variable\n    sur_dummies = pd.get_dummies(df['sur'], prefix='sur')\n    df = pd.concat([df, sur_dummies], axis=1)\n    \n    # removing the title variable\n    df.drop('sur', axis=1, inplace=True)\n    \n    return df\n\ndf = process_sur()\ndf.sample(5)","97a3b50c":"print(\"Train- Missing Cabin\")\nprint(train_df.iloc[:891].Cabin.isnull().sum())\n\nprint(\"Test- Missing Cabin\")\nprint(test_df.iloc[:891].Cabin.isnull().sum())","b27acfb0":"train_cabin, test_cabin = set(), set()\n\nfor c in df.iloc[:891]['Cabin']:\n    try:\n        train_cabin.add(c[0])\n    except:\n        train_cabin.add('U') # replaces NaN values with U (for Unknow)\n        \nfor c in df.iloc[891:]['Cabin']:\n    try:\n        test_cabin.add(c[0])\n    except:\n        test_cabin.add('U') \n\nprint(\"train_cabin -> \" +str(train_cabin))\n\nprint(\"test_cabin -> \" +str(test_cabin))","d3507345":"#don't have any cabin letter in the test set that is not present in the train set.","460d81ef":"def process_cabin():\n    global df    \n    # replacing missing cabins with U (for Uknown)\n    df.Cabin.fillna('U', inplace=True)\n    \n    # mapping each Cabin value with the cabin letter\n    df['Cabin'] = df['Cabin'].map(lambda c: c[0])\n    \n    # dummy encoding ...\n    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')    \n    df = pd.concat([df, cabin_dummies], axis=1)\n\n    df.drop('Cabin', axis=1, inplace=True)\n    return df\n\ndf = process_cabin()\ndf.head()","bc88378c":"df['Embarked'].describe()","6ed6f4b4":"common_value = 'S'\ndf['Embarked'] = df['Embarked'].fillna(common_value)","6da4ac2f":"def process_embarked():\n    global df\n    # two missing embarked values - filling them with the most frequent one in the train  set(S)\n    df.Embarked.fillna('S', inplace=True)\n    # dummy encoding \n    embarked_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n    df = pd.concat([df, embarked_dummies], axis=1)\n    df.drop('Embarked', axis=1, inplace=True)\n    return df\n\ndf = process_embarked()","b4f4b55a":"df['Fare'] = df['Fare'].fillna(df.Fare.mean())\ndf['Fare'] = df['Fare'].astype(int)","60668415":"def process_sex():\n    global df\n    # mapping string values to numerical one \n    df['Sex'] = df['Sex'].map({'male':1, 'female':0})\n    return df\n\ndf = process_sex()","ec7484e4":"def process_pclass():\n    \n    global df\n    # encoding into 3 categories:\n    pclass_dummies = pd.get_dummies(df['Pclass'], prefix=\"Pclass\")\n    \n    # adding dummy variable\n    df = pd.concat([df, pclass_dummies],axis=1)\n    \n    # removing \"Pclass\"\n    df.drop('Pclass',axis=1,inplace=True)\n    return df\n\ncombined = process_pclass()","e729e191":"def cleanTicket(ticket):\n    ticket = ticket.replace('.', '')\n    ticket = ticket.replace('\/', '')\n    ticket = ticket.split()\n    ticket = map(lambda t : t.strip(), ticket)\n    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n    if len(ticket) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\ntickets = set()\nfor t in df['Ticket']:\n    tickets.add(cleanTicket(t))\n    \nprint(len(tickets))","5d963888":"def process_ticket():\n    \n    global df\n    \n    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n    def cleanTicket(ticket):\n        ticket = ticket.replace('.','')\n        ticket = ticket.replace('\/','')\n        ticket = ticket.split()\n        ticket = map(lambda t : t.strip(), ticket)\n        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n        \n        if len(ticket) > 0:\n            return ticket[0]\n        else: \n            return 'XXX'\n    \n\n    # Extracting dummy variables from tickets:\n\n    df['Ticket'] = df['Ticket'].map(cleanTicket)\n    tickets_dummies = pd.get_dummies(df['Ticket'], prefix='Ticket')\n    df = pd.concat([df, tickets_dummies], axis=1)\n    df.drop('Ticket', inplace=True, axis=1)\n    return df\n\ndf = process_ticket()","774d4d41":"df.sample(5)","d4a99718":"def process_family():\n    \n    global df\n    # introducing a new feature : the size of families (including the passenger)\n    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n\n    # introducing other features based on the family size\n    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    return df\n\ndf = process_family()","7a515dd4":"def process_parch():\n    global df\n    \n    # dummy encoding \n    parch_dummies = pd.get_dummies(df['Parch'], prefix='Parch')\n    df = pd.concat([df, parch_dummies], axis=1)\n    df.drop('Parch', axis=1, inplace=True)\n    return df\n\ndf = process_parch()\n\ndef process_SibSp():\n    global df\n    \n    # dummy encoding \n    SibSp_dummies = pd.get_dummies(df['SibSp'], prefix='SibSp')\n    df = pd.concat([df, SibSp_dummies], axis=1)\n    df.drop('SibSp', axis=1, inplace=True)\n    return df\n\ndf = process_SibSp()","9d6e6739":"df['Age'] = df['Age'].astype(int)\ndf.loc[ df['Age'] <= 11, 'Age'] = 0\ndf.loc[(df['Age'] > 11) & (df['Age'] <= 18), 'Age'] = 1\ndf.loc[(df['Age'] > 18) & (df['Age'] <= 22), 'Age'] = 2\ndf.loc[(df['Age'] > 22) & (df['Age'] <= 27), 'Age'] = 3\ndf.loc[(df['Age'] > 27) & (df['Age'] <= 33), 'Age'] = 4\ndf.loc[(df['Age'] > 33) & (df['Age'] <= 40), 'Age'] = 5\ndf.loc[(df['Age'] > 40) & (df['Age'] <= 66), 'Age'] = 6\ndf.loc[ df['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ndf['Age'].value_counts()","a3b613b5":"def process_age():\n    global df\n    \n    # dummy encoding \n    embarked_dummies = pd.get_dummies(df['Age'], prefix='Age')\n    df = pd.concat([df, embarked_dummies], axis=1)\n    df.drop('Age', axis=1, inplace=True)\n    return df\n\ndf = process_age()\ndf.sample(3)","0ab91a22":"df['Fare'].describe()","3d52372b":"df.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\ndf.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\ndf.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\ndf.loc[(df['Fare'] > 31) & (df['Fare'] <= 99), 'Fare']   = 3\ndf.loc[(df['Fare'] > 99) & (df['Fare'] <= 250), 'Fare']   = 4\ndf.loc[ df['Fare'] > 250, 'Fare'] = 5\ndf['Fare'] = df['Fare'].astype(int)","3b572cd1":"def process_fare():\n    global df\n    \n    # dummy encoding \n    embarked_dummies = pd.get_dummies(df['Fare'], prefix='Fare')\n    df = pd.concat([df, embarked_dummies], axis=1)\n    df.drop('Fare', axis=1, inplace=True)\n    return df\n\ndf = process_fare()\ndf.sample(3)","058f03a7":"print(df.sample(3))\nprint(df.shape)","7d7a4657":"df.drop(['PassengerId'], 1, inplace=True)\nfeature_train_df = df.iloc[:891].copy()\ndf.drop(['Survived'], 1, inplace=True)","0de2aa1e":"def recover_train_test_target():\n    global df\n    \n    passengerId = test_df['PassengerId']\n    targets = pd.read_csv('..\/input\/train.csv', usecols=['Survived'])['Survived'].values\n    train = df.iloc[:891]\n    test = df.iloc[891:]\n    \n    return train, test, targets,passengerId\n\ntrain, test, targets, passengerId= recover_train_test_target()","84b44763":"param = ['Sex',\n 'Title_Master',\n 'Title_Miss',\n 'Title_Mr',\n 'Title_Mrs',\n 'Title_Officer',\n 'Title_Royalty',\n 'sur_abbott',\n 'sur_ahlin',\n 'sur_aks',\n 'sur_albimona',\n 'sur_ali',\n 'sur_allison',\n 'sur_andersen-jensen',\n 'sur_anderson',\n 'sur_andrew',\n 'sur_andrews',\n 'sur_angle',\n 'sur_appleton',\n 'sur_arnold-franchi',\n 'sur_artagaveytia',\n 'sur_asplund',\n 'sur_attalah',\n 'sur_ayoub',\n 'sur_backstrom',\n 'sur_baclini',\n 'sur_bailey',\n 'sur_banfield',\n 'sur_barah',\n 'sur_barbara',\n 'sur_barber',\n 'sur_barkworth',\n 'sur_bateman',\n 'sur_baumann',\n 'sur_baxter',\n 'sur_beane',\n 'sur_becker',\n 'sur_beckwith',\n 'sur_beesley',\n 'sur_behr',\n 'sur_berriman',\n 'sur_bidois',\n 'sur_bing',\n 'sur_bishop',\n 'sur_bjornstrom-steffansson',\n 'sur_blackwell',\n 'sur_blank',\n 'sur_bonnell',\n 'sur_boulos',\n 'sur_bourke',\n 'sur_bracken',\n 'sur_bradley',\n 'sur_braund',\n 'sur_brewe',\n 'sur_brown',\n 'sur_bryhl',\n 'sur_buss',\n 'sur_butler',\n 'sur_butt',\n 'sur_byles',\n 'sur_bystrom',\n 'sur_cacic',\n 'sur_cairns',\n 'sur_calderhead',\n 'sur_caldwell',\n 'sur_calic',\n 'sur_cameron',\n 'sur_campbell',\n 'sur_canavan',\n 'sur_caram',\n 'sur_carbines',\n 'sur_cardeza',\n 'sur_carlsson',\n 'sur_carr',\n 'sur_carrau',\n 'sur_cavendish',\n 'sur_chaffee',\n 'sur_chambers',\n 'sur_chapman',\n 'sur_chip',\n 'sur_christy',\n 'sur_clarke',\n 'sur_cleaver',\n 'sur_clifford',\n 'sur_cohen',\n 'sur_coleff',\n 'sur_collander',\n 'sur_colley',\n 'sur_connolly',\n 'sur_coutts',\n 'sur_crosby',\n 'sur_cunningham',\n 'sur_dahl',\n 'sur_dahlberg',\n 'sur_daly',\n 'sur_danbom',\n 'sur_daniel',\n 'sur_davidson',\n 'sur_davis',\n 'sur_davison',\n 'sur_de',\n 'sur_dean',\n 'sur_del',\n 'sur_devaney',\n 'sur_dick',\n 'sur_dodge',\n 'sur_doling',\n 'sur_dorking',\n 'sur_douglas',\n 'sur_dowdell',\n 'sur_downton',\n 'sur_drew',\n 'sur_duff',\n 'sur_duran',\n 'sur_eitemiller',\n 'sur_elias',\n 'sur_emanuel',\n 'sur_fahlstrom',\n 'sur_farthing',\n 'sur_faunthorpe',\n 'sur_flynn',\n 'sur_foo',\n 'sur_ford',\n 'sur_foreman',\n 'sur_fortune',\n 'sur_fox',\n 'sur_frauenthal',\n 'sur_frolicher-stehli',\n 'sur_frost',\n 'sur_fry',\n 'sur_funk',\n 'sur_futrelle',\n 'sur_fynney',\n 'sur_gale',\n 'sur_garside',\n 'sur_gaskell',\n 'sur_gavey',\n 'sur_gee',\n 'sur_giglio',\n 'sur_giles',\n 'sur_gill',\n 'sur_gillespie',\n 'sur_gilnagh',\n 'sur_givard',\n 'sur_glynn',\n 'sur_goldenberg',\n 'sur_goldschmidt',\n 'sur_goldsmith',\n 'sur_goodwin',\n 'sur_greenfield',\n 'sur_guggenheim',\n 'sur_gustafsson',\n 'sur_haas',\n 'sur_hagland',\n 'sur_hakkarainen',\n 'sur_hale',\n 'sur_hamalainen',\n 'sur_hansen',\n 'sur_harder',\n 'sur_harknett',\n 'sur_harper',\n 'sur_harrington',\n 'sur_harris',\n 'sur_harrison',\n 'sur_hart',\n 'sur_hassab',\n 'sur_hassan',\n 'sur_hawksford',\n 'sur_hays',\n 'sur_healy',\n 'sur_hedman',\n 'sur_hegarty',\n 'sur_heikkinen',\n 'sur_heininen',\n 'sur_henry',\n 'sur_herman',\n 'sur_hewlett',\n 'sur_hickman',\n 'sur_hirvonen',\n 'sur_hocking',\n 'sur_hold',\n 'sur_homer',\n 'sur_honkanen',\n 'sur_hood',\n 'sur_hosono',\n 'sur_hoyt',\n 'sur_hunt',\n 'sur_ilett',\n 'sur_ilmakangas',\n 'sur_isham',\n 'sur_jalsevac',\n 'sur_jansson',\n 'sur_jarvis',\n 'sur_jenkin',\n 'sur_jensen',\n 'sur_jermyn',\n 'sur_johannesen-bratthammer',\n 'sur_johansson',\n 'sur_johnson',\n 'sur_johnston',\n 'sur_jonsson',\n 'sur_jussila',\n 'sur_kallio',\n 'sur_karun',\n 'sur_kelly',\n 'sur_kent',\n 'sur_kimball',\n 'sur_kink-heilmann',\n 'sur_kirkland',\n 'sur_klaber',\n 'sur_knight',\n 'sur_kvillner',\n 'sur_lahtinen',\n 'sur_laitinen',\n 'sur_lam',\n 'sur_landergren',\n 'sur_lang',\n 'sur_larsson',\n 'sur_leader',\n 'sur_leeni',\n 'sur_lefebre',\n 'sur_lehmann',\n 'sur_leitch',\n 'sur_lemore',\n 'sur_lesurer',\n 'sur_levy',\n 'sur_lewy',\n 'sur_leyson',\n 'sur_lindahl',\n 'sur_lindblom',\n 'sur_lindqvist',\n 'sur_lobb',\n 'sur_long',\n 'sur_louch',\n 'sur_lulic',\n 'sur_mack',\n 'sur_madigan',\n 'sur_madsen',\n 'sur_mamee',\n 'sur_mangan',\n 'sur_mannion',\n 'sur_marechal',\n 'sur_marvin',\n 'sur_masselmani',\n 'sur_matthews',\n 'sur_mccarthy',\n 'sur_mccormack',\n 'sur_mccoy',\n 'sur_mcdermott',\n 'sur_mcevoy',\n 'sur_mcgough',\n 'sur_mcgovern',\n 'sur_mcgowan',\n 'sur_mckane',\n 'sur_meanwell',\n 'sur_meek',\n 'sur_mellinger',\n 'sur_mellors',\n 'sur_meyer',\n 'sur_millet',\n 'sur_minahan',\n 'sur_mockler',\n 'sur_moen',\n 'sur_molson',\n 'sur_montvila',\n 'sur_moor',\n 'sur_moraweck',\n 'sur_morley',\n 'sur_moss',\n 'sur_moubarek',\n 'sur_moussa',\n 'sur_mudd',\n 'sur_mullens',\n 'sur_murphy',\n 'sur_najib',\n 'sur_nakid',\n 'sur_nasser',\n 'sur_natsch',\n 'sur_newell',\n 'sur_nicholls',\n 'sur_nicholson',\n 'sur_nicola-yarred',\n 'sur_nilsson',\n 'sur_niskanen',\n 'sur_norman',\n 'sur_nye',\n 'sur_nysten',\n \"sur_o'driscoll\",\n \"sur_o'dwyer\",\n \"sur_o'leary\",\n \"sur_o'sullivan\",\n 'sur_ohman',\n 'sur_olsen',\n 'sur_olsson',\n 'sur_oreskovic',\n 'sur_osman',\n 'sur_ostby',\n 'sur_otter',\n 'sur_padro',\n 'sur_pain',\n 'sur_palsson',\n 'sur_panula',\n 'sur_parkes',\n 'sur_parr',\n 'sur_parrish',\n 'sur_partner',\n 'sur_pears',\n 'sur_penasco',\n 'sur_pengelly',\n 'sur_pernot',\n 'sur_persson',\n 'sur_peter',\n 'sur_peters',\n 'sur_petranec',\n 'sur_petroff',\n 'sur_pettersson',\n 'sur_peuchen',\n 'sur_phillips',\n 'sur_pickard',\n 'sur_pinsky',\n 'sur_ponesell',\n 'sur_porter',\n 'sur_quick',\n 'sur_reeves',\n 'sur_reuchlin',\n 'sur_reynaldo',\n 'sur_rice',\n 'sur_richard',\n 'sur_richards',\n 'sur_ridsdale',\n 'sur_ringhini',\n 'sur_robbins',\n 'sur_robins',\n 'sur_roebling',\n 'sur_romaine',\n 'sur_rood',\n 'sur_rosblom',\n 'sur_ross',\n 'sur_rothes',\n 'sur_rothschild',\n 'sur_rugg',\n 'sur_ryan',\n 'sur_ryerson',\n 'sur_saad',\n 'sur_saalfeld',\n 'sur_sage',\n 'sur_salkjelsvik',\n 'sur_sandstrom',\n 'sur_sedgwick',\n 'sur_seward',\n 'sur_sharp',\n 'sur_sheerlinck',\n 'sur_shelley',\n 'sur_silven',\n 'sur_silverthorne',\n 'sur_silvey',\n 'sur_simonius-blumer',\n 'sur_sinkkonen',\n 'sur_sjoblom',\n 'sur_skoog',\n 'sur_slayter',\n 'sur_slemen',\n 'sur_sloper',\n 'sur_smart',\n 'sur_smith',\n 'sur_sobey',\n 'sur_soholt',\n 'sur_stahelin-maeglin',\n 'sur_stanley',\n 'sur_stead',\n 'sur_stewart',\n 'sur_strandberg',\n 'sur_stranden',\n 'sur_strom',\n 'sur_sunderland',\n 'sur_sundman',\n 'sur_sutton',\n 'sur_taussig',\n 'sur_taylor',\n 'sur_thayer',\n 'sur_thomas',\n 'sur_thorne',\n 'sur_tobin',\n 'sur_toomey',\n 'sur_tornquist',\n 'sur_touma',\n 'sur_troupiansky',\n 'sur_trout',\n 'sur_troutt',\n 'sur_turja',\n 'sur_turkula',\n 'sur_turpin',\n 'sur_uruchurtu',\n 'sur_van',\n 'sur_vander',\n 'sur_vestrom',\n 'sur_walker',\n 'sur_watson',\n 'sur_watt',\n 'sur_weir',\n 'sur_weisz',\n 'sur_wells',\n 'sur_west',\n 'sur_white',\n 'sur_wick',\n 'sur_widener',\n 'sur_wilhelms',\n 'sur_williams',\n 'sur_williams-lambert',\n 'sur_woolner',\n 'sur_wright',\n 'sur_yasbeck',\n 'sur_yrois',\n 'sur_zabour',\n 'Cabin_B',\n 'Cabin_D',\n 'Cabin_E',\n 'Cabin_F',\n 'Cabin_G',\n 'Cabin_T',\n 'Cabin_U',\n 'Embarked_C',\n 'Embarked_Q',\n 'Embarked_S',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'Ticket_A4',\n 'Ticket_A5',\n 'Ticket_C',\n 'Ticket_CA',\n 'Ticket_CASOTON',\n 'Ticket_FC',\n 'Ticket_FCC',\n 'Ticket_LINE',\n 'Ticket_PC',\n 'Ticket_PP',\n 'Ticket_SC',\n 'Ticket_SCOW',\n 'Ticket_SCParis',\n 'Ticket_SOC',\n 'Ticket_SOP',\n 'Ticket_SOPP',\n 'Ticket_STONO',\n 'Ticket_SWPP',\n 'Ticket_WC',\n 'Ticket_WEP',\n 'Ticket_XXX',\n 'FamilySize',\n 'Singleton',\n 'SmallFamily',\n 'LargeFamily',\n 'Parch_0',\n 'Parch_1',\n 'Parch_2',\n 'Parch_3',\n 'Parch_4',\n 'Parch_5',\n 'Parch_6',\n 'SibSp_0',\n 'SibSp_1',\n 'SibSp_2',\n 'SibSp_3',\n 'SibSp_4',\n 'SibSp_5',\n 'SibSp_8',\n 'Age_0',\n 'Age_1',\n 'Age_2',\n 'Age_4',\n 'Age_5',\n 'Age_6',\n 'Fare_0',\n 'Fare_1',\n 'Fare_2',\n 'Fare_3',\n 'Fare_4',\n 'Fare_5']\n\ntrain = train[param]\ntest = test[param]","c9f6b509":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\ntrain_scale= pd.DataFrame(ss.fit_transform(train), columns=test.columns)\ntest_scale = pd.DataFrame(ss.fit_transform(test), columns=test.columns)","bc27319d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\ndef svc_param_selection(X, y, nfolds):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    kernel =[\"linear\",\"rbf\"]\n    param_grid = {'C': Cs, 'gamma' : gammas,'kernel':kernel}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\nparam = svc_param_selection(train_scale,targets,10)\ndisplay(param)\n\n#take times to run, use back local given param","c52e9808":"from sklearn import svm\nfrom sklearn.svm import SVC\n\nsvc = SVC(C=10,gamma=0.001,kernel='rbf')\nsvc.fit(train_scale, targets)\npred = svc.predict(test_scale).astype(int)\n\nprint(round(svc.score(train_scale, targets) * 100, 2))","af9da13c":"#Save result into df, csv\npassengerId = test_df['PassengerId']\nnew_df = pd.DataFrame(columns=['PassengerId','Survived'])\nnew_df['PassengerId'] = passengerId\nnew_df['Survived'] = pred\n\nnew_df.to_csv(\"submition.csv\", index = False)","c8ce9ca2":"# Done data preprocessing\n","e056b8a2":"## Feature selection - done in local computer with 10methods, select the best","3fa2142d":"# Data Preprocessing\n","8f545d3c":"# 3. Embarked, Pclass and Sex:","210216a3":"# 5. SibSp and Parch:","66888cd8":"# 1. Age and Sex:\n","339d90cb":"# Ticket:\ncheck how many variance of ticket in dataset","42e96d4e":"# Missing Data: - Age (263), Cabin (1014), Embarked (2), Fare(1)\n","51c2efa0":"# Data Exploration","1239f3c8":"# Embarked: (2)","90e8e7ce":"# dummies for sur name","cb9d2825":"# Sur name","33fb49dc":"# Sex:","c5163555":"# Pclass","1f77cdea":"# Cabin: (1014)\n\n### First thought, we have to delete the \u2018Cabin\u2019 variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the letter refers to the deck. Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G.","4f09517d":"## Title","ef953e17":"# Family:\nFamilySize : the total number of relatives including the passenger (him\/her)self.\n\nSigleton : a boolean variable that describes families of size = 1 \\n\n\nSmallFamily : a boolean variable that describes families of 2 <= size <= 4 \\n\n\nLargeFamily : a boolean variable that describes families of 5 < size","4d71dc6c":"# ~Missing value settled~\n","b108900e":"# Building Machine Learning Models","057835c4":"# 7. Sur name - this quite interesting, does name affect your destiny??","580257a4":"# Fare:","4041c68c":"# Creating Categories:\n\n## Age:","f5c5e188":"# 4. Pclass:\n","7514d1e1":"## Age: (263)","718e3ea6":"# 6. Title","610bc7fa":"# Fare: (1)","e2d52724":"### dummies for title"}}