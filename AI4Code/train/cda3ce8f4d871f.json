{"cell_type":{"3ac563d6":"code","d787d031":"code","20b0a2ae":"code","77fd58e0":"code","d1172978":"code","e10989af":"code","b2c36bc5":"code","1de28b0d":"code","b1d212f9":"code","badfd6dc":"code","963f5dbf":"code","b7a420e1":"code","6178914a":"code","26941b4b":"code","1469b405":"code","91c2caea":"code","c808151d":"code","217e3613":"code","95c2165b":"code","4dbaa368":"code","75486214":"code","fc3ad109":"code","3e5e3fff":"code","1153dca0":"code","2db2eece":"code","2b43ab78":"markdown","2b7203c5":"markdown","4d26a447":"markdown","3a7659b5":"markdown","3320db46":"markdown","6ea0ca41":"markdown","7812c6f1":"markdown","1c2c03de":"markdown","40e5f144":"markdown","2d044c75":"markdown","3a0d1fde":"markdown","c6d25cff":"markdown","8bfeb411":"markdown","59d024e4":"markdown","7c142f04":"markdown","f3f08902":"markdown","969986e0":"markdown","d4a9ca65":"markdown","8a615713":"markdown","9563d8f0":"markdown"},"source":{"3ac563d6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\nimport pandas as pd\nfrom wordcloud import WordCloud as wc\nimport matplotlib.pyplot as plt\nimport os\n","d787d031":"violations = pd.read_csv('..\/input\/violations.csv')\n","20b0a2ae":"violations.head()","77fd58e0":"viol_desc = violations.violation_description\nviol_str = viol_desc.str.cat(sep = ' ')","d1172978":"stop = set(stopwords.words('english'))\nlist_of_words = [i.lower() for i in wordpunct_tokenize(viol_str) if i.lower() not in stop and i.isalpha()]\nlist_of_words[:15]","e10989af":"wordfreqdist = nltk.FreqDist(list_of_words)\nmostcommon = wordfreqdist.most_common(30)\nprint(mostcommon)","b2c36bc5":"plt.barh(range(len(mostcommon)),[val[1] for val in mostcommon], align='center')\nplt.yticks(range(len(mostcommon)), [val[0] for val in mostcommon])\nplt.show()","1de28b0d":"wc1 = wc().generate(' '.join(list_of_words))\n \nplt.imshow(wc1)\nplt.axis(\"off\")\nplt.show()","b1d212f9":"inspections = pd.read_csv('..\/input\/inspections_train.csv')","badfd6dc":"inspections.head()","963f5dbf":"combinedDF = pd.merge(violations,inspections[['camis','passed']], right_on = 'camis', left_on = 'camis')","b7a420e1":"combinedDF.head()","6178914a":"passedDF = combinedDF[combinedDF['passed'] == 1]\nfailedDF = combinedDF[combinedDF['passed'] == 0]\n","26941b4b":"viol_desc_passed = passedDF.violation_description\nviol_desc_failed = failedDF.violation_description","1469b405":"viol_str_passed = viol_desc_passed.str.cat(sep = ' ')\nviol_str_failed = viol_desc_failed.str.cat(sep = ' ')\n\n\nlist_of_words_passed = [i.lower() for i in wordpunct_tokenize(viol_str_passed) if i.lower() not in stop and i.isalpha()]\nlist_of_words_failed = [i.lower() for i in wordpunct_tokenize(viol_str_failed) if i.lower() not in stop and i.isalpha()]\n","91c2caea":"wordfreqdistpassed = nltk.FreqDist(list_of_words_passed)\nmostcommonpassed = wordfreqdistpassed.most_common(30)\nprint(mostcommonpassed)","c808151d":"plt.barh(range(len(mostcommonpassed)),[val[1] for val in mostcommonpassed], align='center')\nplt.yticks(range(len(mostcommonpassed)), [val[0] for val in mostcommonpassed])\nplt.show()","217e3613":"wordfreqdistfailed = nltk.FreqDist(list_of_words_failed)\nmostcommonfailed = wordfreqdistfailed.most_common(30)\nprint(mostcommonfailed)","95c2165b":"plt.barh(range(len(mostcommonfailed)),[val[1] for val in mostcommonfailed], align='center')\nplt.yticks(range(len(mostcommonfailed)), [val[0] for val in mostcommonfailed])\nplt.show()","4dbaa368":"wcpassed = wc().generate(' '.join(list_of_words_passed))\n \nplt.imshow(wcpassed)\nplt.axis(\"off\")\nplt.show()","75486214":"wcfailed = wc().generate(' '.join(list_of_words_failed))\n \nplt.imshow(wcfailed)\nplt.axis(\"off\")\nplt.show()\n ","fc3ad109":"failedwordlen = len(list_of_words_failed)\nworddictfailed = dict(wordfreqdistfailed)\nworddictfailednormalized = {k: float(v) \/ failedwordlen for k, v in worddictfailed.items()}\nworddictfailednormalized\n","3e5e3fff":"passedwordlen = len(list_of_words_passed)\nworddictpassed = dict(wordfreqdistpassed)\nworddictpassednormalized = {k: float(v) \/ passedwordlen for k, v in worddictpassed.items()}\nworddictpassednormalized","1153dca0":"\nworddictrelative = {k: worddictfailednormalized[k] - worddictpassednormalized[k] \n                    for k in worddictfailednormalized if k in worddictpassednormalized}\n\nworddictrelative","2db2eece":"wcrel = wc().generate_from_frequencies(worddictrelative)\n\nplt.imshow(wcrel)\nplt.axis(\"off\")\nplt.show()\n ","2b43ab78":"## kernel 4: Feature Exploration and Word Clouds\n#### Can we find any useful features in the violation description field?\n\n\n\nAs a first step, we're going to import some useful tools and load the data.","2b7203c5":"Now that we have a combined DataFrame let's build out our tokens for passed and failed descriptions separately.","4d26a447":"First we concatonate all of the violation descriptions into one long string. ","3a7659b5":"Now let's have some fun with the visualization! \n\nWe will show these words as a word cloud that emphasizes the most common words in our datset.","3320db46":"Now we generate wordclouds for both of these to see if we can detect differences.","6ea0ca41":"We next plot a histogram of these words to get a visual representation of the frequency.","7812c6f1":"Looking at the charts and wordclouds we do see some words stand out as occuring in failed more often than passed inspections. Flies and vermin are two of the most common and seem like they should lead to failures!\n\nHow can we make these differences more obvious?\n\nOne approach is to create a wordcloud based on the relative frequency of words in the failed vs. passed inspections. A naive approach to this is to first convert the frequency of each word in the failed (or passed) set to a % of total words. As an example if vermin appears 100 out of 1,000 words we convert the frequency to 10%. We can then take the difference in % occurence between the failed and the passed sets of words. If vermin appears in 10% of failed words and 2% of passed words then we set the relative occurence to 8%. We can then build a wordcloud using this relative frequency.\n\nCan you think of any issues with calculating relative frequencies in this manner? How can you improve upon this method?","1c2c03de":"Cool visualization but how can this give us insight into which inspections lead to passes and failures? \n\nTo determine this we will need to compare word clouds from the passed and the failed inspections.\n\nLet's first pull in the inspections dataset and join it to the descriptions dataset.","40e5f144":"We then tokenize this string to extract each word in the violations. Tokenization takes the long string and breaks it on every space into multiple words. We additionally will remove stop words. Stop words are common words in the language you are using that are unlikely to help in your feature engineering. Some examples are \"the\", \"a\", \"and\".","2d044c75":"First we will pull in the violations data","3a0d1fde":"> > ","c6d25cff":"First we normalize the frequency for the failed words.","8bfeb411":"Now let's see the most common words in the passed corpus","59d024e4":"Now we see some words popping out that make sense for failures: Flies, Vermin, Mice.\n\nIt seems like the violation description field could be useful, but how can we leverage these findings for our final prediction? Are there any issues with looking at the violation descriptions as independent words? Might there be a better way to do this?\n\n","7c142f04":"Let's now do the same for the failed corpus.","f3f08902":"To explore the data more we will first see what are the most common words in our corpus.","969986e0":"Then we normalize the frequency for the passed words.","d4a9ca65":"Now let's create our relative frequency dictionary.","8a615713":"Let's take a look at the word cloud.","9563d8f0":"Let's take a look at this dataset. We will focus our exploration on the violation_description field"}}