{"cell_type":{"0598dc7e":"code","3b162a6a":"code","a4919e67":"code","247176e7":"code","29fcd241":"code","83e947bf":"code","f82fa5e3":"code","34f2b64c":"code","7b0d6ba9":"code","096edd8d":"code","c70eeefb":"code","ce969831":"code","00f56c70":"code","229bce4e":"code","e9ace3c6":"code","4edb9e75":"code","0fb5043c":"code","442648c6":"code","0b5bff00":"code","763f39ad":"code","8921ad09":"code","cc6e63e5":"code","57cea731":"code","2982b8e7":"markdown"},"source":{"0598dc7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b162a6a":"import torch\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F","a4919e67":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nx_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain.head()","247176e7":"\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntrainX_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\ntrainY_numpy = train.label.values\n\n# train test split. Size of train data is 80% and size of test data is 20%. \ntrainX ,testX ,trainY ,testY = train_test_split(trainX_numpy,trainY_numpy,test_size = 0.2) \n\ntrainX = torch.from_numpy(trainX)\ntrainY = torch.from_numpy(trainY).type(torch.LongTensor) # data type is long\n\ntrainX = trainX.view(-1,1,28,28)\n\n# create feature and targets tensor for test set.\ntestX = torch.from_numpy(testX)\ntestY = torch.from_numpy(testY).type(torch.LongTensor) # data type is long\n\ntestX = testX.view(-1,1,28,28)\n\n","29fcd241":"print(trainX.shape,trainY.shape,testX.shape,testY.shape)\n\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","83e947bf":"# Pytorch train and test sets\ntrain = TensorDataset(trainX,trainY)\ntest = TensorDataset(testX,testY)\n\n# data loader\nbatch_size = 128\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\nvalid_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","f82fa5e3":"fig = plt.figure(figsize = (8,8))\nimg = images[1]\nimg = img.squeeze(0)\nwidth, height = img.shape\nax = fig.add_subplot(111)\nax.imshow(img, cmap='gray')\nthresh = img.max()\/2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(float(img[x][y]),2) if img[x][y] !=0 else 0\n        ax.annotate(str(val), xy=(y,x),\n                horizontalalignment='center',\n                verticalalignment='center', size=8,\n                color='white' if img[x][y]<thresh else 'black')\n","34f2b64c":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# helper conv function\ndef conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n    \n    layers.append(conv_layer)\n\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    return nn.Sequential(*layers)","7b0d6ba9":"\n\n# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # convolutional layer\n        #28*28*1\n        self.conv1 = conv(1,16,kernel_size=3,stride=1,padding=1,batch_norm=True)\n        # max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        #14*14*16\n        self.conv2 = conv(16,32,kernel_size=3,stride=1,padding=1,batch_norm=True)\n        \n        #7*7*64\n        self.conv3 = conv(32,64,kernel_size=3,stride=1,padding=1,batch_norm=True)\n        \n        #3*3*128\n        self.fc1 = nn.Linear(64*3*3,256)\n        self.fc2 = nn.Linear(256,10)\n        \n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1,64*3*3)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# create a complete CNN\nmodel = Net()\nprint(model)\n\n# move tensors to GPU if CUDA is available\nif train_on_gpu:\n    model.cuda()","096edd8d":"import torch.optim as optim\n\n# specify loss function\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\noptimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)","c70eeefb":"# number of epochs to train the model\nn_epochs = 500 # you may increase this number to train a final model\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data.float())\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data.float())\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.dataset)\n    valid_loss = valid_loss\/len(valid_loader.dataset)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_mnist.pt')\n        valid_loss_min = valid_loss","ce969831":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","00f56c70":"plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');","229bce4e":"PATH = '.\/model_mnist.pt'\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()","e9ace3c6":"BATCH_SIZE = 128\ndef evaluate(model):\n    correct = 0\n    \n    for data,target in valid_loader:\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        data = data.float()\n        output = model(data)\n        predicted = torch.max(output,1)[1]\n        correct += (predicted == target).sum()\n    print(\"Test accuracy:{:.3f} \".format( float(correct) \/ (len(valid_loader)*BATCH_SIZE)))\nevaluate(model)","4edb9e75":"sub_test_x = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nsub_test_x.head()","0fb5043c":"# create feature and targets tensor for test set.\nsub_test_x = sub_test_x.loc[:,:].values\/255 # normalization\nsub_test_x = torch.from_numpy(sub_test_x)\nsub_test_x = sub_test_x.view(-1,1,28,28)\nprint(len(sub_test_x),sub_test_x.shape)","442648c6":"batch_size = 128\ntest_loader = DataLoader(sub_test_x, batch_size = batch_size, shuffle = False)","0b5bff00":"dataiter = iter(test_loader)\ntest_images = dataiter.next()\nprint(type(test_images))\nprint(test_images.shape)\n","763f39ad":"plt.imshow(test_images[0].numpy().squeeze(), cmap='Greys_r');","8921ad09":"def Y_predict(model):\n    correct = 0\n    sub_test_y = []\n    for data in test_loader:\n        if train_on_gpu:\n            data = data.cuda()\n        data = data.float()\n        output = model(data)\n        predicted = torch.max(output,1)[1]\n        for value in predicted:\n            cpu_value = value.cpu()\n            cpu_value = cpu_value.numpy()\n            sub_test_y.append(cpu_value.tolist())\n\n    \n    return sub_test_y","cc6e63e5":"sub_test_y = Y_predict(model)\n","57cea731":"\nmy_submission = pd.DataFrame({'ImageId': list(range(1,len(sub_test_y)+1)), 'Label':sub_test_y})\nmy_submission.to_csv('Final_submission1.csv', index=False)","2982b8e7":"<H2>A Simple Aprroach to Mnist Handwritten digits using Pytorch<\/H2>\nWe will be using CNN for this problem since it is best classifying image with Spatial invariance<br>\nPlease upvote this kernel if you like this simple approach."}}