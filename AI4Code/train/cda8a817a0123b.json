{"cell_type":{"61e8e541":"code","27520d69":"code","a7eb4e46":"code","09ba9f7d":"code","fe8d8fc3":"code","6f8d276e":"code","cd001de6":"code","5ec96c8c":"code","8dbb4ca3":"code","cbd061f0":"code","6f28af48":"code","fa0d2739":"code","247ae898":"code","7d8d65bf":"code","768e9f2e":"code","abbc774a":"code","0a9d6f2a":"code","b2f73c2a":"code","1f7feeee":"code","f77b403b":"code","3eb7d4f1":"code","2465235b":"code","1b889ba5":"code","c382ab66":"code","72809de3":"code","547ef58d":"code","bdcdbf2f":"code","9665055f":"code","6dc8fbfe":"code","9970dc5a":"code","1b95804a":"code","07c8a707":"code","14708f46":"code","9896549c":"code","b5d8e6d7":"code","f0b7a0cb":"code","0516f1ea":"code","df404276":"code","e39e0072":"markdown","0283fb1f":"markdown","a4f961a3":"markdown","d0fd4f24":"markdown","922a7e3a":"markdown","56bd8f55":"markdown","49ad9623":"markdown","64a9a3e8":"markdown","5b1f6b50":"markdown","3992aaa6":"markdown","d20c95a3":"markdown","43da8f41":"markdown","b0b9f568":"markdown","9e5a9d2a":"markdown","e4882501":"markdown","f3d48133":"markdown","6417477a":"markdown","5c07861d":"markdown","42a20e21":"markdown","7ff0f1aa":"markdown","468132e0":"markdown","95e0504a":"markdown","87f823c0":"markdown","af4ad9b7":"markdown","0302f132":"markdown","6af69804":"markdown","523325b9":"markdown","4b356e65":"markdown","54299a6a":"markdown","f13ec059":"markdown","f030efef":"markdown","47d9cde4":"markdown","b474b2fb":"markdown","d86c8a44":"markdown","bc1ce37e":"markdown","3d55c494":"markdown","ecab301a":"markdown","0cdf32b0":"markdown","332784aa":"markdown","7ac74899":"markdown","fd5e8ee6":"markdown"},"source":{"61e8e541":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\nimport gc\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","27520d69":"# List files available\nprint(os.listdir(\"..\/input\/\"))","a7eb4e46":"# Training data\napp_train = pd.read_csv('..\/input\/application_train.csv')\napp_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Training data shape: {}, Testing data shape: {}'.format( app_train.shape,app_test.shape))","09ba9f7d":"df = app_train\ndf = df.append(app_test)\ndf.describe()","fe8d8fc3":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","6f8d276e":"# NaN values for DAYS_EMPLOYED: 365.243 -> nan\ndf['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)","cd001de6":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","5ec96c8c":"print (df['CODE_GENDER'].value_counts())\n\n# Remove the rows with XNA value in CODE_GENDER\nprint ('\\nSize Before {}'.format(df.shape))\ndf = df[df['CODE_GENDER'] != 'XNA']\nprint ('Size After {}'.format(df.shape))","8dbb4ca3":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n# Iterate through the columns\nfor col in df:\n    if df[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df[col].unique())) <= 2:\n            le.fit(df[col])\n            df[col] = le.transform(df[col])\n            le_count += 1\nprint ('{} variable are label encoded'.format(le_count))\n\n\n# One Hot Encoding \ncategorical_columns = [col for col in df.columns if df[col].dtype == 'object']\ndf = pd.get_dummies(df, columns= categorical_columns, dummy_na= True)\nprint ('The shape of dataset after One hot encoding: {}'.format(df.shape))","cbd061f0":"df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\ndf['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\ndf['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\ndf['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\ndf['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']","6f28af48":"app_train['TARGET'].value_counts()","fa0d2739":"app_train['TARGET'].astype(int).plot.hist();","247ae898":"del app_test, app_train\ngc.collect()","7d8d65bf":"# function to obtain Categorical Features\ndef _get_categorical_features(df):\n    feats = [col for col in list(df.columns) if df[col].dtype == 'object']\n    return feats\n\n# function to factorize categorical features\ndef _factorize_categoricals(df, cats):\n    for col in cats:\n        df[col], _ = pd.factorize(df[col])\n    return df \n\n# function to create dummy variables of categorical features\ndef _get_dummies(df, cats):\n    for col in cats:\n        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n    return df ","768e9f2e":"# # factorize the categorical features from train and test data\n# df_cats = _get_categorical_features(df)\n# df = _factorize_categoricals(df, df_cats)","abbc774a":"bureau = pd.read_csv('..\/input\/bureau.csv', nrows = None)\nbb = pd.read_csv('..\/input\/bureau_balance.csv', nrows = None)","0a9d6f2a":"bereau_cats = _get_categorical_features(bureau)\nbb_cats = _get_categorical_features(bb)\n\nbureau = _get_dummies(bureau,bereau_cats)\nbb = _get_dummies(bb,bb_cats)","b2f73c2a":"# Average Values for all bureau features \nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\nbureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\ndf = df.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')\n#df.head()\ndel bb, bureau_avg\ngc.collect()","1f7feeee":"prev = pd.read_csv('..\/input\/previous_application.csv', nrows = None)","f77b403b":"prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)","3eb7d4f1":"prev_app_cats = _get_categorical_features(prev)\nprev = _get_dummies(prev, prev_app_cats)","2465235b":"## count the number of previous applications for a given ID\nprev_apps_count = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(prev_apps_count['SK_ID_PREV'])\n\n## Average values for all other features in previous applications\nprev_apps_avg = prev.groupby('SK_ID_CURR').mean()\nprev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\ndf = df.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n## Garbage Collection\ndel prev, prev_apps_avg\ngc.collect()","1b889ba5":"pos = pd.read_csv('..\/input\/POS_CASH_balance.csv', nrows = None)","c382ab66":"pos_cats = _get_categorical_features(pos)\npos = _get_dummies(pos, pos_cats)","72809de3":"### count the number of pos cash for a given ID\npos_count = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\npos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(pos_count['SK_ID_PREV'])\n\n## Average Values for all other variables in pos cash\npos_avg = pos.groupby('SK_ID_CURR').mean()\ndf = df.merge(right=pos_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n\ndel pos, pos_avg\ngc.collect()","547ef58d":"ins = pd.read_csv('..\/input\/installments_payments.csv', nrows = None)","bdcdbf2f":"ins_cats = _get_categorical_features(ins)\nins = _get_dummies(ins, ins_cats)","9665055f":"# Percentage and difference paid in each installment (amount paid and installment value)\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n\n# Days past due and days before due (no negative values)\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)","6dc8fbfe":"## count the number of previous installments\ncnt_inst = ins[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nins['SK_ID_PREV'] = ins['SK_ID_CURR'].map(cnt_inst['SK_ID_PREV'])\n\n## Average values for all other variables in installments payments\navg_inst = ins.groupby('SK_ID_CURR').mean()\navg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\ndf = df.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n\ndel ins, avg_inst\ngc.collect()","9970dc5a":"cc = pd.read_csv('..\/input\/credit_card_balance.csv', nrows = None)","1b95804a":"ccbal_cats = _get_categorical_features(cc)\ncredit_card_balance = _get_dummies(cc, ccbal_cats)","07c8a707":"### count the number of previous applications for a given ID\nnb_prevs = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncredit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n\n### average of all other columns \navg_cc_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\navg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\ndf = df.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n\ndel cc, avg_cc_bal\ngc.collect()","14708f46":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold","9896549c":"# Divide in training\/validation and test data\ntrain_df = df[df['TARGET'].notnull()]\ntest_df = df[df['TARGET'].isnull()]\nprint(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))","b5d8e6d7":"folds = KFold(n_splits=2, shuffle=True, random_state=1001)","f0b7a0cb":"# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\nfeature_importance_df = pd.DataFrame()\nfeats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]","0516f1ea":"# Iterate through each fold\nfor train_indices, valid_indices in folds.split(train_df[feats],train_df['TARGET']):\n        \n    # Training data for the fold\n    train_features, train_labels = train_df[feats].iloc[train_indices], train_df['TARGET'].iloc[train_indices]\n    # Validation data for the fold\n    valid_features, valid_labels = train_df[feats].iloc[valid_indices], train_df['TARGET'].iloc[valid_indices]\n        \n    # Create the model\n    model = LGBMClassifier(n_estimators=5000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n    \n        \n    # Train the model\n    model.fit(train_features, train_labels, eval_metric = 'auc',\n                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                eval_names = ['valid', 'train'],\n                early_stopping_rounds = 100, verbose = 200)\n    \n    oof_preds[valid_indices] = model.predict_proba(valid_features, num_iteration=model.best_iteration_)[:, 1]\n    sub_preds += model.predict_proba(test_df[feats], num_iteration=model.best_iteration_)[:, 1] \/ folds.n_splits\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feats\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    del model, train_features, train_labels, valid_features, valid_labels\n    gc.collect()\n\nprint('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))","df404276":"test_df['TARGET'] = sub_preds\ntest_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index= False)","e39e0072":"## 4.4 Label Encoder and One Hot Encoding\n\n**Label Encoder **: As you might know by now, we can\u2019t have text in our data if we\u2019re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.\n\nAnd to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.\n\nSuppose, we have a feature State which has 3 category i.e India , France, China . So, Label Encoder will categorize them as 0, 1, 2.\n\n**One Hot Encoding** : One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. \n\nIf we applied OHE on say Gender column which has category male,female. OHE will create two new column Gender_male, Gender_female and store the value 0 and 1 according to the main category value.","0283fb1f":"## 4.2 Removing Anamolies \/ Outlier\nIn statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.\n\n\n","a4f961a3":"365243 days is somewhat around 1000 years, which is impossible.\n\nSo, its best for us to replace it by NaN.","d0fd4f24":"## 4.5 Add some feature variables\nSince i don't consider myself as a credit expert. I have used this features from [this](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features\/code#L261) script by Aguiar.\n\n- DAYS_EMPLOYED_PERC: the percentage of the days employed relative to the client's age.\n- INCOME_CREDIT_PERC: the percentage of the credit amount relative to a client's income.\n- INCOME_PER_PERSON : the percentage of income per person.\n- ANNUITY_INCOME_PERC: the percentage of the loan annuity relative to a client's income.\n- PAYMENT_RATE : the percentage of rate of payment annually.","922a7e3a":"## Handling Categorical Features","56bd8f55":"# Home Credit Solution \n## This Kernal is made for beginners learning purpose.\n### Feel free to fork and use it. \n### You will learn how to handle large dataset without being confused.\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSkr6pXKnRSPy5tYRwUBMr0nZwjAzDVAzVFAigd6bauuwvHoQTf)\n","49ad9623":"If you wish not to use GC then better watch your RAM. It is likely to over-exceed and Kernel error may come up.","64a9a3e8":"## 6.3 Feature Engineering - Previous Application","5b1f6b50":"## 6.2 One Hot Encoding","3992aaa6":"## 8.2 Adding some new features","d20c95a3":"## 10.1 Prepare Final Train and Test data","43da8f41":"# 5. Exploration of Bureau and Bureau_data","b0b9f568":"# 9. Exploration of Credit Card","9e5a9d2a":"## Target Column Distribution \nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.","e4882501":"# 1. Introduction\nIn this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan.","f3d48133":"## 6.1 Handling Outliers","6417477a":"## 7.1 One Hot Encoding","5c07861d":"# 6. Exploration of Previous Application","42a20e21":"**Thanks for taking time to go through the kernel. **\n\n**Show your appreciation by upvoting or commenting about your feedbacks. **","7ff0f1aa":"## 4.1 Merge both train and test dataset","468132e0":"# 4. Exploration of Application Train\/Test Data.","95e0504a":"## 4.3 Categorical Variables ","87f823c0":"We can see there are some anamolies in  `DAYS_EMPLOYED`.\n\n`DAYS_EMPLOYED` at max seems to have very large positive value.\n","af4ad9b7":"# 8. Exploration of Installment Payments","0302f132":"# 3. Retrieving the Data","6af69804":"## 8.3 Feature Engineering - Installment Payments","523325b9":"`CODE_GENDER` has unknown value 'XNA'. Its better to remove these 4 rows.","4b356e65":"# 2. Importing requires packages","54299a6a":"# 10. LightGBM\n\n- It is a gradient boosting framework that uses **tree based learning algorithm**.\n- It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.\n-  It is prefixed as \u2018Light\u2019 because of its** high speed**.\n- It can handle the **large size of data** and takes **lower memory to run.**\n- This is popular because it focuses on **accuracy of results.**\n-  LGBM also supports **GPU learning.**\n\n**Why not to use Light GBM?**\n\n- It is **not advisable** to use LGBM on **small datasets**.\n- Light GBM is sensitive to overfitting and can easily **overfit small data**.\n\nThe only complicated thing is **parameter tuning.** Light GBM covers more than 100 parameters but don\u2019t worry, you don\u2019t need to learn all.\n\nLets learn about some of the parameters we used in our model :\n\n- **n_estimators** : number of boosting iterations.\n- **objective** : This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.\n\n    - regression: for regression\n    - binary: for binary classification\n    - multiclass: for multiclass classification problem\n   \n- **learning_rate** : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003\u2026\n- **reg_alpha** : L1 regularization\n-  **reg_lambda** : L2 regularization\n","f13ec059":"## Garbage Collection\nPython\u2019s memory allocation and deallocation method is automatic. The user does not have to preallocate or deallocate memory similar to using dynamic memory allocation in languages such as C or C++.\n\nHere we are using **Manual Garbage Collection**\n\nInvoking the garbage collector manually during the execution of a program can be a good idea on how to handle memory being consumed by reference cycles.\nThe garbage collection can be invoked manually in the following way:","f030efef":"## 8.1 One Hot Encoding","47d9cde4":"There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.","b474b2fb":"# 7. Exploration of POS Cash Balance","d86c8a44":"## 9.1 One Hot Encoding","bc1ce37e":"## 7.2 Feature Engineering - POS Cash Balance","3d55c494":"## 5.2 Feature Engineering - Bureau Data","ecab301a":"## 10.3 Fitting the model and Predicting","0cdf32b0":"## 9.2 Feature Engineering - Credit Card","332784aa":"## 10.2 Cross Validation Model\n**K-Fold Cross Validation**: To reduce variability, in most machine learning methods multiple rounds of cross-validation are performed using different partitions, and the validation are averaged at the end. This method is known as k-fold cross validation.\n\nIncrease the `n_splits` to make better prediction. But it may increase the time of processing.","7ac74899":"## 10.5 Submission","fd5e8ee6":"## 5.1 One Hot Encoding"}}