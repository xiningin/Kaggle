{"cell_type":{"b4998e8a":"code","5772c8d3":"code","64915dd1":"code","94d7bc23":"code","f8c43420":"code","66ce1772":"code","6f127b87":"code","0c9826b1":"code","8a3b3ecc":"code","3de3447e":"code","829ba6e5":"code","0dd278ea":"code","2122cf45":"code","a2053d6e":"code","535f4c5d":"code","2918202f":"code","dcba0850":"code","0c028b8a":"code","498973f4":"code","7ab04924":"code","7e29b49f":"code","e234f72e":"code","5cff9cc0":"code","b25f1602":"code","03c9f807":"code","1f82f2e3":"code","eb1ca026":"code","10ce38be":"code","c5aad7d4":"code","09feb66a":"code","ed4a180b":"code","089221cf":"markdown"},"source":{"b4998e8a":"image_size = 512\nbatch_size = 32\nnum_workers = 4","5772c8d3":"import pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') ","64915dd1":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","94d7bc23":"transforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])","f8c43420":"class RANZCRDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.astype(np.float32)\n        img = img.transpose(2,0,1)\n        \n        if self.mode == 'test':\n            return torch.tensor(img).float()\n        else:\n            return torch.tensor(img).float(), torch.tensor(row.PatientID).float()","66ce1772":"class ArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s=10, m=0):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs","6f127b87":"class MetricLearningModel(nn.Module):\n\n    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=False):\n        super(MetricLearningModel, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.channel_size = channel_size\n        self.out_feature = out_feature\n        self.in_features = self.backbone.classifier.in_features\n        self.margin = ArcModule(in_features=self.channel_size, out_features = self.out_feature)\n        self.bn1 = nn.BatchNorm2d(self.in_features)\n        self.dropout = nn.Dropout2d(dropout, inplace=True)\n        self.fc1 = nn.Linear(self.in_features * 16 * 16 , self.channel_size)\n        self.bn2 = nn.BatchNorm1d(self.channel_size)\n        \n    def forward(self, x, labels=None):\n        features = self.backbone.features(x)\n        features = self.bn1(features)\n        features = self.dropout(features)\n        features = features.view(features.size(0), -1)\n        features = self.fc1(features)\n        features = self.bn2(features)\n        features = F.normalize(features)\n        if labels is not None:\n            return self.margin(features, labels)\n        return features\n","0c9826b1":"model = MetricLearningModel(image_size, 30805)\nmodel.load_state_dict(torch.load('..\/input\/feature-extractor\/dense121_feature_extractor.pth', map_location='cuda:0'))\nmodel.to(device);","8a3b3ecc":"!mkdir external","3de3447e":"!cp ..\/input\/bimcv-all-images-512-scale\/scale_512\/* external","829ba6e5":"!unzip -q -d . ..\/input\/ricord-eda\/train.zip","0dd278ea":"!cp .\/train\/* external","2122cf45":"len(os.listdir('.\/external'))","a2053d6e":"files = glob.glob('.\/external\/*')\ndf = pd.DataFrame(files)\ndf.columns = ['file_path']\ndf['StudyInstanceUID'] = df['file_path'].str.split('\/', expand=True)[2]\ndataset_ext = RANZCRDataset(df, 'test', transform=transforms_valid)\next_loader = torch.utils.data.DataLoader(dataset_ext, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","535f4c5d":"def generate_test_features(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images) in enumerate(bar):\n\n            images = images.to(device)\n\n            features = model(images)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","2918202f":"FEAS = generate_test_features(ext_loader)\nFEAS = torch.tensor(FEAS).cuda()","dcba0850":"!rm -r train","0c028b8a":"!mkdir train\n!tar -xf ..\/input\/siim-covid-19-convert-to-jpg-256px\/train.tar.gz -C train","498973f4":"trn_files = glob.glob('.\/train\/*')\ntrn_df = pd.DataFrame(trn_files)\ntrn_df.columns = ['file_path']\ntrn_df['StudyInstanceUID'] = trn_df['file_path'].str.split('\/', expand=True)[2]\ndataset_trn = RANZCRDataset(trn_df, 'test', transform=transforms_valid)\ntrn_loader = torch.utils.data.DataLoader(dataset_trn, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","7ab04924":"FEAS_trn = generate_test_features(trn_loader)\nFEAS_trn = torch.tensor(FEAS_trn).cuda()","7e29b49f":"# chestx_df = pd.read_csv('..\/input\/data\/Data_Entry_2017.csv')\n# chestx_df['file_path'] = sorted(glob.glob('..\/input\/data\/images_*\/*\/*'))","e234f72e":"# chestx_features = np.load('..\/input\/chest-x-features\/chest_x_features.npy')\n# chestx_features = torch.tensor(chestx_features).cuda()\n","5cff9cc0":"product = (FEAS@FEAS_trn.T)\n# test['chest_x_image'] = chestx_df.loc[idx.detach().cpu()]['Image Index'].values\n# test['chest_x_file_path'] = chestx_df.loc[idx.detach().cpu()]['file_path'].values","b25f1602":"sim_df = pd.DataFrame(product.cpu().numpy(), index=df['StudyInstanceUID'], columns=trn_df['StudyInstanceUID'])","03c9f807":"ext_prefix = '.\/external\/'\ntrn_prefix = '.\/train\/'","1f82f2e3":"(sim_df.max(1) < 0.90).sum()","eb1ca026":"se = sim_df[(sim_df.max(1) > 0.95) & (sim_df.max(1) < 0.97)].idxmax(1).sample(1)\nx = se.index[0]\ny = se.values[0]\n\nimg1 = cv2.imread(ext_prefix+x)\nimg2 = cv2.imread(trn_prefix+y)\n\nf, ax = plt.subplots(1, 2, figsize=(16, 8))\nax[0].imshow(img1)\nax[1].imshow(img2)","10ce38be":"!rm -r train","c5aad7d4":"sim_df.to_pickle('sim.pkl')","09feb66a":"!zip -r external.zip external","ed4a180b":"!rm -r external","089221cf":".\/external.\/external.\/external.\/external## The train features"}}