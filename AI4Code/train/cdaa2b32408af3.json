{"cell_type":{"f0e96f1c":"code","c5afbfa4":"code","9ee6a89a":"code","ab9be8b1":"code","f8bc7eb1":"code","c3865eb7":"code","7f18c5b9":"code","e705677f":"code","dcf4bbd8":"code","d83f8ef9":"code","89ff2891":"code","d01e12e4":"code","48072419":"code","d9a78c44":"code","9074e248":"code","74fc2331":"code","4bf9b6b8":"code","6303490b":"code","68f1a0a7":"code","3fa80ece":"code","007c1117":"code","0d0664ba":"code","42bc134e":"code","7090192a":"code","6faa3346":"code","dd2af05f":"code","74b42702":"code","68e1f126":"code","4e38cf1e":"code","47bf80be":"code","92bc4021":"code","7363a541":"code","58f85a6d":"code","47c0e0d5":"code","fe2ec6b5":"code","35061526":"code","030a2a22":"markdown","93cdc6df":"markdown","2dc6d35b":"markdown","d0ab9e1c":"markdown","d488e58b":"markdown","28269369":"markdown"},"source":{"f0e96f1c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split as tts\n\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5afbfa4":"data = pd.read_csv('..\/input\/adult-census-income\/adult.csv')","9ee6a89a":"data","ab9be8b1":"data.isna().sum()","f8bc7eb1":"data.isin(['?']).sum()","c3865eb7":"data","7f18c5b9":"data['workclass'].unique()","e705677f":"data['occupation'].unique()","dcf4bbd8":"data['native.country'].unique()","d83f8ef9":"data = data.replace('?', np.NaN)","89ff2891":"data.isna().sum()","d01e12e4":"data.head()","48072419":"data.info()","d9a78c44":"data.columns","9074e248":"categorical_features = []\n\nfor column in data.columns:\n    if data[column].dtypes == 'object':\n        categorical_features.append(column)","74fc2331":"categorical_features","4bf9b6b8":"data.drop('education', axis = 1, inplace = True)","6303490b":"categorical_features.remove('income')","68f1a0a7":"categorical_features.remove('education')","3fa80ece":"categorical_features","007c1117":"def get_uniques(df, columns):\n    uniques = dict()\n    \n    for column in columns:\n        uniques[column] = list(df[column].unique())\n        \n    return uniques","0d0664ba":"get_uniques(data, categorical_features)","42bc134e":"binary_features = ['sex']\nnominal_features = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'native.country']","7090192a":"def binary_encode(df, columns):\n    label_encoder = LabelEncoder()\n    for column in columns:\n        df[column] = label_encoder.fit_transform(df[column])\n    return df\n\ndef onehot_encode(df, columns):\n    for column in columns:\n        dummies = pd.get_dummies(df[column])\n        df = pd.concat([df, dummies], axis = 1)\n        df.drop(column, axis = 1, inplace = True)\n    return df","6faa3346":"data = binary_encode(data, binary_features)\ndata = onehot_encode(data, nominal_features)","dd2af05f":"(data.dtypes == 'object').sum()","74b42702":"y = data['income']\nX = data.drop('income', axis = 1)","68e1f126":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\ny_mappings = {index : label for index, label in enumerate(label_encoder.classes_)}\ny_mappings","4e38cf1e":"y","47bf80be":"scaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)","92bc4021":"X","7363a541":"X_train, X_test, y_train, y_test = tts(X, y, train_size = 0.8)","58f85a6d":"inputs = tf.keras.Input(shape = (88, ))\nx = tf.keras.layers.Dense(512, activation = 'relu')(inputs)\nx = tf.keras.layers.Dense(256, activation = 'relu')(x)\noutputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n\nmetrics = [\n    tf.keras.metrics.BinaryAccuracy(name = 'acc'),\n    tf.keras.metrics.AUC(name = 'auc')\n]\n\nmodel.compile(\n    optimizer = optimizer,\n    loss = 'binary_crossentropy',\n    metrics = metrics\n)\n\nbatch_size = 32\nepochs = 25\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split = 0.2,\n    batch_size = batch_size,\n    epochs = epochs\n)","47c0e0d5":"plt.figure(figsize = (14, 10))\nepochs_range = range(1, epochs + 1)\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(epochs_range, train_loss, label = 'Training loss')\nplt.plot(epochs_range, val_loss, label = 'Validation loss')\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","fe2ec6b5":"model.evaluate(X_test, y_test)","35061526":"y.sum() \/ len(y)","030a2a22":"# Import Libraries \ud83d\udcd6","93cdc6df":"# Load dataset \ud83d\udcbe","2dc6d35b":"# Data Cleaning \ud83e\uddf9","d0ab9e1c":"# Data encoding \ud83d\udc68\u200d\ud83d\udcbb","d488e58b":"# Scores \ud83d\udcc8","28269369":"# Model training \ud83d\udee0\ufe0f"}}