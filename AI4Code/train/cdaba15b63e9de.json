{"cell_type":{"621fdc55":"code","deba4d6d":"code","3611aee5":"code","07b5b7bf":"code","5528478d":"markdown","e93dedb5":"markdown","52ea27d1":"markdown"},"source":{"621fdc55":"def to_categorical(sequences, categories):\n    cat_sequences = []\n    for s in sequences:\n        cats = []\n        for item in s:\n            cats.append(np.zeros(categories))\n            cats[-1][item] = 1.0\n        cat_sequences.append(cats)\n    return np.array(cat_sequences)\n\ndef logits_to_tokens(sequences, index):\n    token_sequences = []\n    for categorical_sequence in sequences:\n        token_sequence = []\n        for categorical in categorical_sequence:\n            token_sequence.append(index[np.argmax(categorical)])\n\n        token_sequences.append(token_sequence)\n\n    return token_sequences\n\ndef get_words(sentences):\n    words = set([])\n    for sentence in sentences:\n        for word in sentence:\n            words.add(word)\n    return words\n\n\ndef get_tags(sentences_tags):\n    tags = set([])\n    for tag in sentences_tags:\n        for t in tag:\n            tags.add(t)\n    return tags\n\ndef get_train_sentences_x(train_sentences, word2index):\n    train_sentences_x = []\n    for sentence in train_sentences:\n        sentence_index = []\n        for word in sentence:\n            try:\n                sentence_index.append(word2index[word])\n            except KeyError:\n                sentence_index.append(word2index['-OOV-'])\n\n        train_sentences_x.append(sentence_index)\n    return train_sentences_x\n\n\ndef get_test_sentences_x(test_sentences, word2index):\n    test_sentences_x = []\n    for sentence in test_sentences:\n        sentence_index = []\n        for word in sentence:\n            try:\n                sentence_index.append(word2index[word])\n            except KeyError:\n                sentence_index.append(word2index['-OOV-'])\n        test_sentences_x.append(sentence_index)\n    return test_sentences_x\n\ndef get_train_tags_y(train_tags, tag2index):\n    train_tags_y = []\n    for tags in train_tags:\n        train_tags_y.append([tag2index[t] for t in tags])\n    return train_tags_y\n\n\ndef get_test_tags_y(test_tags, tag2index):\n    test_tags_y = []\n    for tags in test_tags:\n        test_tags_y.append([tag2index[t] for t in tags])\n    return test_tags_y","deba4d6d":"import ast\nimport codecs\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.layers import Dense, InputLayer, Embedding, Activation, LSTM\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n\ntagged_sentences = codecs.open(\"..\/input\/urduner\/ner.txt\", encoding=\"utf-8\").readlines()\n\nprint(tagged_sentences[0])\nprint(\"Tagged sentences: \", len(tagged_sentences))\n\nsentences, sentence_tags = [], []\nfor tagged_sentence in tagged_sentences:\n    sentence, tags = zip(*ast.literal_eval(tagged_sentence))\n    sentences.append(np.array(sentence))\n    sentence_tags.append(np.array(tags))\n\n(train_sentences,\n test_sentences,\n train_tags,\n test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n\nwords = get_words(sentences)\ntags = get_tags(sentence_tags)\n\nword2index = {w: i + 2 for i, w in enumerate(list(words))}\nword2index['-PAD-'] = 0\nword2index['-OOV-'] = 1\n\ntag2index = {t: i + 1 for i, t in enumerate(list(tags))}\ntag2index['-PAD-'] = 0\n\ntrain_sentences_x = get_train_sentences_x(train_sentences, word2index)\ntest_sentences_x = get_test_sentences_x(test_sentences, word2index)\n\ntrain_tags_y = get_train_tags_y(train_tags, tag2index)\ntest_tags_y = get_test_tags_y(test_tags, tag2index)\n\nMAX_LENGTH = len(max(train_sentences_x, key=len))\n# MAX_LENGTH = 181\n\ntrain_sentences_x = pad_sequences(train_sentences_x, maxlen=MAX_LENGTH, padding='post')\ntest_sentences_x = pad_sequences(test_sentences_x, maxlen=MAX_LENGTH, padding='post')\ntrain_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\ntest_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(MAX_LENGTH,)))\nmodel.add(Embedding(len(word2index), 128))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dense(len(tag2index)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(0.001),\n              metrics=['accuracy'])\nhistory = model.fit(train_sentences_x, to_categorical(train_tags_y, len(tag2index)), batch_size=32, epochs=10,\n                    validation_split=0.2).history\n# model.save(\"..\/models\/lstm_ner.h5\")\nmodel.summary()\n\nscores = model.evaluate(test_sentences_x, to_categorical(test_tags_y, len(tag2index)))\nprint(f\"{model.metrics_names[1]}: {scores[1] * 100}\")  # acc: 98.39311069478103\nprint(test_sentences[0])\nprint(test_tags[0])\ntest_samples = [\n    test_sentences[0],\n]\n\ntest_samples_x = []\nfor sentence in test_samples:\n    sentence_index = []\n    for word in sentence:\n        try:\n            sentence_index.append(word2index[word])\n        except KeyError:\n            sentence_index.append(word2index['-OOV-'])\n    test_samples_x.append(sentence_index)\n\ntest_samples_X = pad_sequences(test_samples_x, maxlen=MAX_LENGTH, padding='post')\n\npredictions = model.predict(test_samples_X)\nprint(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n","3611aee5":"plt.plot(history['accuracy'])\nplt.plot(history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='lower right')\n# plt.savefig(\"..\/results\/accuracy_lstm_ner.png\")\n\n","07b5b7bf":"plt.clf()\n\n# Plot training & validation loss values\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\n# plt.savefig(\"..\/results\/loss_lstm_ner.png\")","5528478d":"Utility functions","e93dedb5":"Plot it","52ea27d1":"accuracy"}}