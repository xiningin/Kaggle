{"cell_type":{"7dd59544":"code","9c8f823b":"code","4f3d69fd":"code","b96d6b45":"code","ffac1bc0":"code","cf1c3b99":"code","462d3d1d":"code","c65388d3":"code","7a4662df":"code","321206a6":"code","68b66326":"code","cab6dbed":"code","0e2ef7a7":"code","6b823f75":"code","652a17e1":"code","dd840ffb":"code","67d0e95f":"code","03a29ea5":"markdown"},"source":{"7dd59544":"%pip install requests\n%pip install elasticsearch\n%pip install tqdm","9c8f823b":"%%script bash\nwget https:\/\/artifacts.elastic.co\/downloads\/elasticsearch\/elasticsearch-7.9.3-linux-x86_64.tar.gz\nwget https:\/\/artifacts.elastic.co\/downloads\/elasticsearch\/elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512\nshasum -a 512 -c elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512 \ntar -xzf elasticsearch-7.9.3-linux-x86_64.tar.gz\nrm elasticsearch-7.9.3-linux-x86_64.tar.gz\nrm elasticsearch-7.9.3-linux-x86_64.tar.gz.sha512","4f3d69fd":"!useradd elasticuser\n!chown -R elasticuser elasticsearch-7.9.3","b96d6b45":"%%script bash --bg --out script_out\nsu elasticuser -c .\/elasticsearch-7.9.3\/bin\/elasticsearch &","ffac1bc0":"def save_picke(file_path,obj):\n    with open(file_path, 'wb') as handle:\n        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef load_pickle(file_path):\n    with open(file_path, 'rb') as handle:\n        obj = pickle.load(handle)\n    return obj\n\ndef finished(n=1):\n    for i in range(n):\n        playsound('assets\/bell.wav')\n        time.sleep(1.5)\n\ndef read_file(file,start_line = 0,n=20,encoding = None):\n    lines = []\n    read_lines = 0\n    with open(file,'r',encoding=encoding) as f:\n        for i,line in enumerate(f):\n            if not start_line or i  >= start_line:\n                lines.append(line)\n                read_lines +=1\n                if n and read_lines > n:\n                    break\n    return lines\n\ndef download_file(target_path,url,override=False):\n    local_filename = url.split('\/')[-1]\n    # NOTE the stream=True parameter below\n    file_downloaded = False\n    file_path = os.path.join(target_path,local_filename)\n    byte_pos = 0\n    if not override and os.path.exists(file_path):\n        print(f'\\tFile {file_path} already exists, skipping...')\n        return file_path\n    try:\n        os.remove(file_path)\n    except OSError:\n        pass\n    print(f'Getting file from {url}')\n    while not file_downloaded:\n        resume_header = {f'Range': 'bytes=%d-' % byte_pos}\n        try:\n            with requests.get(url, headers=resume_header, stream=True,  verify=False, allow_redirects=True) as r:\n            #with requests.get(url, stream=True) as r:\n                r.raise_for_status()\n                for chunk in  r.iter_content(chunk_size=8192):\n                    with open(file_path, 'ab') as f:\n                        # If you have chunk encoded response uncomment if\n                        # and set chunk_size parameter to None.\n                        #if chunk: \n                        f.write(chunk)\n                        byte_pos += 1\n                file_downloaded = True\n        except:\n            print('An error occured while downloading. Retrying...')\n    return local_filename\n\ndef clear_indices(excluded_indices= []):\n    for index in  [index for index  in es.indices.stats()['indices'].keys() if index not in excluded_indices]:\n        es.indices.delete(index)\n        \ndef create_index(es,index_name,body,overwrite = False):\n    indices = es.indices.stats()['indices'].keys()\n    if index_name in  indices:\n        if overwrite:\n            print(f'overwriting index {index_name}')\n            es.indices.delete(index_name)\n        else:\n            print(f'Index {index_name} already exists')\n    else:\n        es.indices.create(index_name,body=body)\n        \ndef extract_gz_files(file_path,override=False,n=8,max_n=None):\n    x_file_out_path = file_path.replace('.gz','')\n    if override:\n        try:\n            os.remove(x_file_out_path)\n        except OSError:\n            pass\n    if os.path.exists(x_file_out_path):\n        print(f'\\tFile {x_file_out_path} already exists, skipping...')\n    else:\n        print(f'\\tExtracting file {file_path}')\n        gz_file = gzip.GzipFile(file_path, 'rb')\n        n_i = 0\n        while True:\n            chunk = gz_file.read(n)\n            n += len(chunk)\n            if chunk == b'' or (max_n and n_i > max_n):\n                break\n            x_file_out = open(x_file_out_path, 'ab')\n            x_file_out.write(chunk)\n            x_file_out.close()\n        gz_file.close()\n        print(f'\\t\\tExtracted {x_file_out_path}!')\n    return x_file_out_path\n\ndef get_next_from_gz(file_path,pos = 0,n=8,stop_char='\\n'):\n    chunk = ''\n    with gzip.GzipFile(file_path, 'rb') as gz_file:\n        gz_file.seek(pos)\n        chunk = gz_file.read(n)\n        if stop_char:\n            stop_char_at = chunk.find(ord(stop_char))\n            if stop_char_at > -1:\n                chunk = chunk[:stop_char_at +1]\n            else:\n                pos = pos + len(chunk)\n                chunk += get_next_from_gz(file_path,pos = pos,n=n,stop_char=stop_char)\n    return chunk\n\ndef get_next_from_gz_file(gz_file,pos = 0,n=8,stop_char='\\n'):\n    chunk = ''\n    gz_file.seek(pos)\n    chunk = gz_file.read(n)\n    if stop_char:\n        stop_char_at = chunk.find(stop_char)\n        if stop_char_at > -1:\n            chunk = chunk[:stop_char_at]\n        else:\n            pos = pos + len(chunk)\n            chunk += get_next_from_gz_file(gz_file,pos = pos,n=n,stop_char=stop_char)\n    return chunk\n\n    \n  \n    \ndef extract_document(doc_str):\n    keys = ['id','url','title','body']\n    document = {}\n    doc_id = None\n    doc_meta = doc_str.split('\\t')\n    for i in range(len(doc_meta)):\n        key = keys[i]\n        if key == 'id':\n            doc_id = doc_meta[i]\n        elif key == 'body':\n            meta = doc_meta[i]\n            # Used to remove initial double quote and ending pattern [ \"\\n] per document (\") \n            document[key] = doc_meta[i][1:-3]\n        else:    \n            document[key] = doc_meta[i]\n    return doc_id,document\n\n\ndef process_corpus(file_path,n=None,encoding=None):\n        lines_read = 0\n        continue_at_line = 0\n        finished_no_error = False\n        while not finished_no_error:\n            print(f'Continuing from line {continue_at_line}')\n            with open(file_path,'r',encoding=encoding) as f:\n                try:\n                    for i, line in enumerate(f):\n                        if i < continue_at_line:\n                            continue\n                        if n and lines_read >= n:\n                            finished_no_error = True\n                            break\n                        doc_id, doc = extract_document(line)\n                        lines_read += 1\n                        print(f\"\\rProcessing document no: {lines_read} [{doc_id}...]\", end=\"\")\n                        for es in ES_INSTANCES:\n                            es.index(index=INDEX_NAME, id=doc_id, body=doc)\n                        \n                        continue_at_line = i\n                        finished_no_error = True\n                except:\n                    print(f'An error ocurred while parsing processing the document {lines_read} {doc_id} {sys.exc_info()[0]}')","cf1c3b99":"import gzip\nimport math\nimport numpy as np\nimport os\nimport pytest\nimport pickle\nimport random\nimport requests\nimport tarfile\nimport time\nimport timeit\nimport sys\n\nfrom collections import Counter\nfrom collections import defaultdict\nfrom elasticsearch import Elasticsearch\n#from playsound import playsound\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","462d3d1d":"\nurls = [\n'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-docs.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-docs-lookup.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-doctrain-queries.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-docdev-queries.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-docdev-top100.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/msmarco-docdev-qrels.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/docleaderboard-queries.tsv.gz'\n,'https:\/\/msmarco.blob.core.windows.net\/msmarcoranking\/docleaderboard-top100.tsv.gz'\n]\n\nsource_path = 'MS-MARCO'\n\nif not os.path.isdir(source_path):\n        os.mkdir(source_path)\n\n\ngzfiles = []\nfor url in urls:\n    gzfile = download_file(source_path,url,override=False)\n    gzfiles.append(gzfile)","c65388d3":"FIELDS = ['url','title', 'body']\nINDEX_NAME = 'ms-marco'\nbody = {\n    'mappings': {\n            'properties': {\n                'url': {\n                    'type': 'text',\n                    'term_vector': 'yes',\n                    'analyzer': 'english'\n                },\n                'title': {\n                    'type': 'text',\n                    'term_vector': 'yes',\n                    'analyzer': 'english'\n                },\n                'body': {\n                    'type': 'text',\n                    'term_vector': 'yes',\n                    'analyzer': 'english'\n                }\n            }\n        }\n    }\n","7a4662df":"overwrite = False # DO NOT CHANGE THIS FLAG!!!\nuser = 'elastic'\npassword = 'IfKREtTr7fCqMYTD8NKE4yBi'\nremote_url = f'https:\/\/{user}:{password}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n\nREMOTE_ES = Elasticsearch(hosts=remote_url)\n\ncreate_index(REMOTE_ES,INDEX_NAME,body,overwrite = overwrite)\nprint(REMOTE_ES.info())","321206a6":"files = [\n    'MS-MARCO\/msmarco-docdev-top100.gz'\n    ,'MS-MARCO\/docleaderboard-top100.tsv.gz'\n    ,'MS-MARCO\/msmarco-docdev-qrels.tsv.gz'\n]\n\nfor i in range(len(files)):\n    \n    files[i] = extract_gz_files(files[i])\n    \n    \ndocs_needed = set()\nfor file in files:\n    \n    with open(file, 'r',encoding='UTF-8') as f:\n        for line in f:\n            doc = line.split()[2]\n            docs_needed.add(doc)\nprint(len(docs_needed))","68b66326":"with open('MS-MARCO\/required_doc_ids','w') as f:\n    for doc_id in docs_needed:\n        f.write(doc_id + '\\n')","cab6dbed":"#os.remove('.\/MS-MARCO\/msmarco-docs.tsv.gz')\ndump = gzip.GzipFile('MS-MARCO\/required_docs.tsv.gz','wb')\nwith  gzip.GzipFile('MS-MARCO\/msmarco-docs.tsv.gz', 'rb') as gz_file:\n    for raw_line in gz_file:\n        line = raw_line.decode('UTF-8')\n        doc_id = line[:line.find('\\t')]\n        if doc_id in docs_needed:\n            docs_needed.remove(doc_id)\n            dump.write(raw_line)\ndump.close()\n","0e2ef7a7":"os.remove('.\/MS-MARCO\/msmarco-docs.tsv.gz')","6b823f75":"processed_indexes = []\ncontinue_indexing = False\ndoc_id_cache = 'D1966040'\n#Indexing: 27262 D3095996...\nwith gzip.GzipFile('MS-MARCO\/required_docs.tsv.gz','rb') as dump:\n    for line in dump:\n        line = line.decode('UTF-8')\n        doc = extract_document(line)\n        if doc[0] == doc_id_cache:\n            continue_indexing = True\n        \n        if continue_indexing:\n            REMOTE_ES.index(index=INDEX_NAME, id=doc[0], body=doc[1])\n            print(f\"\\rIndexing: {len(processed_indexes)} {doc[0]}...\", end=\"\")\n        processed_indexes.append(doc[0])\n#Indexing: 100076 D1966040...        ","652a17e1":"es = Elasticsearch()\nes.info()\ncreate_index(es,INDEX_NAME,body,overwrite = overwrite)\n\nprocessed_indexes = []\ncontinue_indexing = False\ndoc_id_cache = None\n#Indexing: 27262 D3095996...\nwith gzip.GzipFile('MS-MARCO\/required_docs.tsv.gz','rb') as dump:\n    for line in dump:\n        line = line.decode('UTF-8')\n        doc = extract_document(line)\n        if not doc_id_cache or  doc[0] == doc_id_cache:\n            continue_indexing = True\n        \n        if continue_indexing:\n            es.index(index=INDEX_NAME, id=doc[0], body=doc[1])\n            print(f\"\\rIndexing: {len(processed_indexes)} {doc[0]}...\", end=\"\")\n        processed_indexes.append(doc[0])\n        doc_id_cache = doc_id\n#Indexing: 100076 D1966040...        ","dd840ffb":"#killall \u2013 Kill a process by name","67d0e95f":"! ls MS-MARCO\n#rm MS-MARCO\/required_docs.tsv.gz","03a29ea5":"## Execute these shell commands to install and run elasticsearch locally"}}