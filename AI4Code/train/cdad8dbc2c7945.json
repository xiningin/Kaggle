{"cell_type":{"0166ee95":"code","2869d2b7":"code","5a0067d1":"code","51611610":"code","39d7f10f":"code","8dfb4375":"code","9c6575c0":"code","eb7d309c":"code","0a5de798":"code","57270506":"code","b5bff973":"code","887794a4":"code","b4fc4078":"code","448d2709":"code","f5c42785":"code","d3b31c4f":"code","de8c9762":"code","5325e4dc":"code","88bd05e8":"code","876c32a7":"code","1f011ca7":"code","1b48bd8b":"code","fe04b014":"code","7a2106ac":"code","993e8e74":"code","a60f4c12":"code","ade22ad7":"code","b3472c6d":"code","4d4475b6":"code","69a92505":"code","ff5f705a":"code","b9b615e1":"code","d0099e37":"code","8601b110":"code","c17084af":"code","7357c463":"code","1dc727fe":"code","ab096c0a":"code","d9ff8c1d":"code","f756b1e3":"code","6f06b73f":"code","36635746":"code","a5cdd060":"code","5d405004":"code","6cbf4722":"code","074469d4":"code","560add0a":"code","e576c44d":"code","0b353afd":"code","fbc95990":"code","19a9f2ac":"code","acbabcc9":"code","a7fb5799":"code","82283e8a":"code","35a8d329":"code","d83af7f8":"code","f33e6cfc":"code","9e185b90":"code","5cf9805e":"code","30981e0c":"code","ef495948":"code","d387b5de":"code","9a374119":"code","6c3448f1":"code","93df9db5":"code","fdc40c6d":"code","d3a17a97":"code","5eede17b":"code","33247480":"code","6417f805":"code","c89310eb":"code","af36baf8":"code","f5a04efa":"code","10855cf0":"code","93d81d4e":"code","790c19ab":"code","1da97d9c":"code","298fbfd6":"markdown","422cd6b6":"markdown","48f32e3b":"markdown","7d55f227":"markdown","62d7427c":"markdown","6e336c7c":"markdown","708d0bfa":"markdown","fd5f0f78":"markdown","4c8162a1":"markdown","a80fba40":"markdown","625b2d39":"markdown","a98c5b17":"markdown","88a47635":"markdown","5e0fe5fa":"markdown","4aafe760":"markdown","961d587e":"markdown","fee0c560":"markdown","5325ee6e":"markdown","8d2cfbec":"markdown","2e597329":"markdown","0aad554f":"markdown","07da6735":"markdown","e210f42b":"markdown","b996c832":"markdown","16346d52":"markdown","5f6d7000":"markdown","d378c88d":"markdown","ad4618c1":"markdown","1d9124f3":"markdown","98088053":"markdown","9213f4c2":"markdown","7d27038f":"markdown","cf8fef38":"markdown","e895434d":"markdown","387ee69a":"markdown","980e180c":"markdown","b75738bc":"markdown","48d3e5ac":"markdown"},"source":{"0166ee95":"# Image\nfrom PIL import Image\n\n# Python Collectino\nfrom collections import Counter\n\n# FOR Loop Verbose\nfrom tqdm import tqdm\n\n# System\nimport os\n\n# String\nimport string\n\n# Natural Language Processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\n# Date and Time\nimport datetime\n\n# Dataframe\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# Numerical Data\nimport numpy as np\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.ensemble import GradientBoostingClassifier","2869d2b7":"data_path = '\/usr\/share\/nltk_data'\nprint(data_path)\nif not os.path.exists(data_path):\n    nltk.download()\nnltk.data.path.append(data_path)","5a0067d1":"pd.options.display.max_rows = 499\npd.options.display.max_columns = 499\npd.options.mode.chained_assignment = None","51611610":"%matplotlib inline","39d7f10f":"raw = pd.read_csv('\/kaggle\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv', encoding='utf-8')\nraw","8dfb4375":"raw.info()","9c6575c0":"raw.describe(include='all')","eb7d309c":"raw.isna().sum()","0a5de798":"cleaned = raw.copy()","57270506":"cleaned.title = cleaned.title.fillna('null')","b5bff973":"cleaned[cleaned.title == 'null']","887794a4":"columns = ['author_flair_text', 'removed_by', 'total_awards_received', 'awarders']\ncleaned = cleaned.drop(columns, axis=1)\ncleaned","b4fc4078":"cleaned.isna().sum()","448d2709":"def utc_to_datetime(data):\n    data['year'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).year)\n    data['month'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).month)\n    data['day'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).day)    \n    data['hour'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).hour)\n    data['minute'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).minute)\n    data['second'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).second)    ","f5c42785":"utc_to_datetime(cleaned)\ncleaned","d3b31c4f":"cleaned['original_content'] = cleaned['title'].str.contains('[OC]').astype(int)\ncleaned","de8c9762":"def get_wordnet_tag(tag):\n    if tag == 'ADJ':\n        return 'j'\n    elif tag == 'VERB':\n        return 'v'\n    elif tag == 'NOUN':\n        return 'n'\n    elif tag == 'ADV':\n        return 'r'\n    else:\n        return 'n'","5325e4dc":"def lemmatize_text(title):\n    stop = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    words = list()\n    title = word_tokenize(title)\n    for word, tag in pos_tag(title):\n        tag = get_wordnet_tag(tag)\n        word = lemmatizer.lemmatize(word, tag)\n        if word not in stop:\n            words.append(word)\n    \n    return ' '.join(words)        ","88bd05e8":"def clean_text(dataset):\n    \n    tqdm.pandas()\n    \n    dataset['title_cleaned'] = dataset['title'].str.lower()\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'\\[oc\\]', ' ')\n    pattern_link = r'https?:\/\/[^\\s]+|www\\.[^\\s]+|[^\\s]+\\.com[^\\s]*|[^\\s]+\\.org[^\\s]*|[^\\s]+\\.html[^\\s]*'\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(pattern_link, ' link ')\n    \n    pattern_punctuation = r'[' + string.punctuation + '\u2019]'\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(pattern_punctuation, '')\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r' [\\d]+ |^[\\d]+ | [\\d]+$', ' ')\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'[^\\w\\d\\s]+', ' ')\n    dataset['title_cleaned'] = dataset['title_cleaned'].progress_apply(lambda title: lemmatize_text(title))\n    \n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'\\s[\\s]+', ' ')","876c32a7":"clean_text(cleaned)\ncleaned","1f011ca7":"cleaned['over_18'] = cleaned['over_18'].apply(lambda x: int(x))\ncleaned","1b48bd8b":"def boxplot(data, feature, base):\n    assert base in ['over_18', 'original_content']\n    \n    plt.figure(figsize=(30, 12))\n    sns.boxplot(x=base, y=feature, data=data)\n    plt.show()","fe04b014":"cleaned['score'].value_counts()","7a2106ac":"boxplot(cleaned, 'score', 'over_18')","993e8e74":"boxplot(cleaned, 'score', 'original_content')","a60f4c12":"cleaned['num_comments'].value_counts()","ade22ad7":"boxplot(cleaned, 'num_comments', 'over_18')","b3472c6d":"boxplot(cleaned, 'num_comments', 'original_content')","4d4475b6":"def countplot(data, by='year'):\n    assert by in ['year', 'month', 'day']\n    data_copy = data.copy()\n    data_copy['year'] = data_copy['year'].astype(str)\n    data_copy['month'] = data_copy['month'].astype(str)\n    data_copy['day'] = data_copy['day'].astype(str)\n\n    plt.figure(figsize=(30, 10))\n    if by == 'year':\n        stat = data_copy['year'].value_counts()        \n        sns.countplot(by, data=data_copy)\n        plt.xlabel(by)\n    elif by == 'month':\n        data_copy['month'] = data_copy['year'] + '\/' + data_copy['month']\n        stat = data_copy['month'].value_counts()        \n        sns.countplot(by, data=data_copy)\n        plt.xlabel(by)\n        plt.xticks(rotation=45)\n    elif by == 'day':\n        data_copy['day'] = data_copy['year'] + '\/' + data_copy['month'] + '\/' + data_copy['day']\n        stat = data_copy['day'].value_counts()        \n        sns.countplot(by, data=data_copy)            \n        \n    plt.ylabel('count')\n    plt.title('Count by Year\/Month\/Day Recent to Old')\n    plt.show()\n    \n    return stat\n    ","69a92505":"countplot(cleaned, 'year')","ff5f705a":"countplot(cleaned, 'month')","b9b615e1":"countplot(cleaned, 'day')","d0099e37":"def wordcloud(dataset, min_freq=1):\n    bow = list()\n    for title in tqdm(dataset['title_cleaned']):\n        bow += word_tokenize(title)\n    \n    word_freq = dict()\n    counter = Counter(bow)\n    for word, freq in counter.items():\n        if freq >= min_freq:\n            word_freq[word] = freq\n    \n#     reddit_mask = np.array(Image.open('\/kaggle\/working\/reddit_icon.png'))    \n    \n    wc = WordCloud(width=800, height=800, background_color='white') #, mask=reddit_mask)\n    wc = wc.generate_from_frequencies(word_freq)\n    \n    plt.figure(figsize=(12, 12))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n\n    \n    return counter, word_freq","8601b110":"counter, word_freq = wordcloud(cleaned)","c17084af":"def wordcloud_by_date(dataset, year=None, month=None, day=None):\n    dataset_cp = dataset.copy()\n    \n    if year:\n        dataset_cp = dataset_cp[dataset_cp['year'] == year]\n    if month:\n        dataset_cp = dataset_cp[dataset_cp['month'] == month]\n    if day:\n        dataset_cp = dataset_cp[dataset_cp['day'] == day]\n    \n    return wordcloud(dataset_cp)","7357c463":"counter_2020, word_freq_2020 = wordcloud_by_date(cleaned, year=2020)","1dc727fe":"counter_20170401, word_freq_20170401 = wordcloud_by_date(cleaned, year=2017, month=4, day=1)","ab096c0a":"counter_20170402, word_freq_20170402 = wordcloud_by_date(cleaned, year=2017, month=4, day=2)","d9ff8c1d":"counter_201704, word_freq_201704 = wordcloud_by_date(cleaned, year=2017, month=4)","f756b1e3":"counter_04, word_freq_04 = wordcloud_by_date(cleaned, month=4)","6f06b73f":"def count_vectorize(dataset):\n    \n    vectorizer = CountVectorizer()\n    \n    documents = list()\n    for title in tqdm(dataset['title_cleaned']):\n        documents.append(title)\n    document_vector = vectorizer.fit_transform(documents)\n    return vectorizer, document_vector","36635746":"cv, cv_encoded = count_vectorize(cleaned)","a5cdd060":"cv_encoded.shape","5d405004":"for i, j in zip(cv_encoded.nonzero()[0][:30], cv_encoded.nonzero()[1][:30]):\n    print('({:4}, {:8}({:15})) -> {}'.format(i, j, cv.get_feature_names()[j], cv_encoded[i, j]))","6cbf4722":"def tfidf_vectorize(dataset):\n    vectorizer = TfidfVectorizer()\n    \n    documents = list()\n    for title in tqdm(dataset['title_cleaned']):\n        documents.append(title)\n    document_vector = vectorizer.fit_transform(documents)\n    return vectorizer, document_vector","074469d4":"tfidf, tfidf_encoded = tfidf_vectorize(cleaned)","560add0a":"tfidf_encoded.shape","e576c44d":"for i, j in zip(tfidf_encoded.nonzero()[0][:30], tfidf_encoded.nonzero()[1][:30]):\n    print('({:4}, {:8}({:15})) -> {}'.format(i, j, tfidf.get_feature_names()[j], tfidf_encoded[i, j]))","0b353afd":"oc = cleaned[cleaned['original_content'] == 1]\nnoc = cleaned[cleaned['original_content'] == 0]","fbc95990":"def scatter(data, x):\n    oc = data[data['original_content'] == 1]\n    noc = data[data['original_content'] == 0]\n\n    plt.figure(figsize=(10, 10))\n    plt.scatter(x[oc.index, 0], x[oc.index, 1], color='red', label='Original Content')\n    plt.scatter(x[noc.index, 0], x[noc.index, 1], color='blue', label='Not Original Content')\n    plt.legend()\n    plt.title('Sample 2-Dimension Features')\n    plt.show()","19a9f2ac":"def svd(encoded, dimension=50):\n    svd = TruncatedSVD(n_components=dimension, n_iter=10, random_state=2020)\n    reduced = svd.fit_transform(encoded)\n    return svd, reduced","acbabcc9":"svd50, svd50_reduced = svd(tfidf_encoded)","a7fb5799":"svd50_reduced.shape","82283e8a":"scatter(cleaned, svd50_reduced)","35a8d329":"def tsne(encoded, dimension=2):\n    tsne = TSNE(n_components=dimension, verbose=5, random_state=2020, n_jobs=4)\n    reduced = tsne.fit_transform(encoded)\n    return tsne, reduced","d83af7f8":"# tsne2, tsne2_reduced = tsne(svd50_reduced)","f33e6cfc":"# tsne2_reduced.shape","9e185b90":"# scatter(cleaned, tsne2_reduced)","5cf9805e":"X = svd50_reduced\nY = np.array(cleaned['original_content'])","30981e0c":"X.shape","ef495948":"Y.shape","d387b5de":"x_train, x_test, y_train, y_test = train_test_split(\n    X, Y,\n    test_size=0.2,\n    stratify=Y\n)","9a374119":"x_train.shape","6c3448f1":"x_test.shape","93df9db5":"def gradient_boosting_model(x_train, y_train, x_test, y_test):\n    model = GradientBoostingClassifier(learning_rate=0.01, n_estimators=100, random_state=2020, verbose=1)\n    scores = cross_validate(model, x_train, y_train, scoring='accuracy', cv=2, return_train_score=True, verbose=1)\n\n    model.fit(x_train, y_train)\n    acc = accuracy_score(model.predict(x_test), y_test)\n    \n    return model, scores, acc","fdc40c6d":"gb_model, gb_scores, gb_acc = gradient_boosting_model(x_train, y_train, x_test, y_test)","d3a17a97":"print('Validation Accuracies: {}'.format(gb_scores['test_score']))\nprint('Test Accuracy: {}'.format(gb_acc))","5eede17b":"def lda(encoded, n_topic=10):\n    lda = LatentDirichletAllocation(n_components=n_topic, verbose=1, random_state=2020)\n    lda.fit(encoded)\n    return lda","33247480":"lda10 = lda(tfidf_encoded, n_topic=10)","6417f805":"for idx, topic in enumerate(lda10.components_):\n    words = [tfidf.get_feature_names()[topic_id] for topic_id in topic.argsort()[::-1][:10]]\n    print('Topic {:2d} -> {}'.format(idx, words))","c89310eb":"topic_df = cleaned.copy()\nlength = cleaned['title_cleaned'].shape[0]\nfor idx, title in tqdm(enumerate(cleaned['title_cleaned'])):\n    encoded = tfidf_encoded[idx]\n    topics = lda10.transform(encoded)\n    topic = topics.argsort()[0][::-1][0]\n\n    topic_df.loc[idx, 'topic'] = topic\n    topic_df.loc[idx, 'topic_value'] = topics[0][topic]\n\n    if idx % 30000 == 0 or idx == length - 1:\n        print('Topic {:2d}({:.6f}) {:}'.format(topic, topics[0][topic], title))\ntopic_df","af36baf8":"topic_df['topic'] = topic_df['topic'].astype(int)","f5a04efa":"plt.figure(figsize=(12, 12))\nsns.countplot(topic_df['topic'])\nplt.xlabel('Topic')\nplt.title('Topic Counter')\nplt.show()","10855cf0":"svd2, svd2_reduced = svd(tfidf_encoded, dimension=2)","93d81d4e":"svd2_reduced.shape","790c19ab":"scatter(cleaned, svd2_reduced)","1da97d9c":"plt.figure(figsize=(12, 12))\nfor topic in sorted(topic_df['topic'].unique()):\n    index = topic_df[topic_df['topic'] == topic].index\n    sns.scatterplot(x=svd2_reduced[index, 0], y=svd2_reduced[index, 1], label=str(topic), s=100)\nplt.legend()\nplt.title('Topic Distribution in 2-d representation')\nplt.show()","298fbfd6":"## 5.2. Time Analysis<a id=\"5.2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","422cd6b6":"Is it possible to discriminate Original Content vs Non Original Content by title? -> About 60% Accuracy, may need model tuning","48f32e3b":"## 4.4. Text Cleaning<a id=\"4.4\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","7d55f227":"## 4.1. Missing Value<a id=\"4.1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","62d7427c":"## 4.5. Data Type Conversion<a id=\"4.5\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","6e336c7c":"# 1. Problem Description <a id=\"1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","708d0bfa":"## 5.3. Wordcloud Text Analysis<a id=\"5.3\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","fd5f0f78":"1. [Problem Description](#1)\n2. [Data Description](#2)\n3. [Environment Setting](#3)\n    1. [Import Library](#3.1)\n    2. [Load Dataset](#3.2)\n4. [Data Preprocessing](#4)\n    1. [Missing Value](#4.1)\n    2. [Time Management](#4.2)\n    3. [OC(Original Content)](#4.3)\n    4. [Text Cleaning](#4.4)\n5. [Exploratory Data Analysis(EDA)](#5)\n    1. [Distribution of Numerical Value](#5.1)\n    2. [Time Analysis](#5.2)\n    3. [Wordcloud Text Analysis](#5.3)\n6. [Word Embedding](#6)\n    1. [Count Vectorizer](#6.1)\n    2. [TF-IDF Vectorizer](#6.2)\n7. [Data Modeling](#7)\n    1. [Dimension Reduction](#7.1)\n    2. [Classification](#7.2)\n    3. [Topic Modeling](#7.3)\n\n<hr>","4c8162a1":"## 3.2. Load Dataset<a id=\"3.2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","a80fba40":"## 4.3. OC(Original Content)<a id=\"4.3\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","625b2d39":"**title**<br>\nimpute title to 'null'","a98c5b17":"**author_flair_text, removed_by, total_awards_received, awarders**<br>\ndrop","88a47635":"> **Kaggle Data**<br>\n> \n> [Reddit - Data is Beautiful](https:\/\/www.kaggle.com\/unanimad\/dataisbeautiful)<br>\n\n> **Questions**<br>\n> \n> [tqdm: Using progress bar in pandas apply function](https:\/\/stackoverflow.com\/questions\/18603270\/progress-indicator-during-pandas-operations)<br>\n> [nltk: pos_tag + lemmatiza](https:\/\/stackoverflow.com\/questions\/15586721\/wordnet-lemmatization-and-pos-tagging-in-python)<br>\n> [wordcloud: how to draw wordcloud](https:\/\/lovit.github.io\/nlp\/2018\/04\/17\/word_cloud\/)<br>\n> [sci-kit learn: count vectorizer get feature name](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)<br>","5e0fe5fa":"> What happened on April 1st, 2nd? -> DATAIRL(DATA In Real Life)\n\n> coronavirus is overwhelming","4aafe760":"# 7. Data Modeling<a id=\"7\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","961d587e":"## What is about..","fee0c560":"**Truncated SVD(for Sparse Data)**","5325ee6e":"## 6.2. TF-IDF Vectorizer<a id=\"6.2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","8d2cfbec":"Reddit Data is Beautiful - from [Kaggle](https:\/\/www.kaggle.com\/unanimad\/dataisbeautiful)\n> **About**<br>\n> \n> Data is Beautiful, r\/dataisbeautiful, is a place for visual representations of data: Graphs, charts, maps, etc. DataIsBeautiful is for visualizations that effectively convey information. Aesthetics are an important part of information visualization, but pretty pictures are not the aim of this subreddit.\n\n> **Content**<br>\n> \n> This dataset contains a couple of fields with the information based on Reddit post submission, such:\n> Fields:\n> * id\n> * title\n> * score\n> * author\n> * authorfalirtext\n> * removed_by\n> * totalawardsreceived\n> * awarders\n> * created_utc\n> * full_link\n> * num_commnets\n> * over_18\n\n> **Method**<br>\n> \n> The data was extracted using the PushShift API for Reddit. Thanks Watchful1 for show me this API.","2e597329":"## 7.3. Topic Modeling<a id=\"7.3\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","0aad554f":"## 4.2. Time Management<a id=\"4.2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","07da6735":"# 4. Data Preprocessing<a id=\"4\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","e210f42b":"# 5. Exploratory Data Analysis(EDA)<a id=\"5\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","b996c832":"## Table of Contents.. <a id=\"top\"><\/a>","16346d52":"## Thanks to.. <a id=\"top\"><\/a>","5f6d7000":"## 7.2. Classification<a id=\"7.2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","d378c88d":"# 6. Word Embedding<a id=\"6\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","ad4618c1":"<center><h1>Reddit Data Analysis(EDA)-v1<\/h1><\/center>\n<hr>","1d9124f3":"Using Reddit Dataset, do Exploratory Data Analysis. Question and Validation using Data Visualization.","98088053":"Explore the Reddit Dataset.","9213f4c2":"> What happened on April 1st, 2nd?","7d27038f":"## 7.1. Dimension Reduction<a id=\"7.1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","cf8fef38":"# 3. Environment Setting<a id=\"3\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","e895434d":"## 5.1. Distribution of Numerical Value<a id=\"5.1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","387ee69a":"# 2. Data Description <a id=\"2\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","980e180c":"## 3.1. Import Library<a id=\"3.1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","b75738bc":"## 6.1. Count Vectorizer<a id=\"6.1\"><\/a>\n<p style=\"text-align:right;\"><a href=\"#top\">\ud83d\udd1d top<\/a><\/p>","48d3e5ac":"**T-SNE**<br>\nTooooooo Many Times needed"}}