{"cell_type":{"09412893":"code","6db699ee":"code","1032cb71":"code","197db5a1":"code","d6db20a1":"code","1fbd1e47":"code","2b913a0e":"code","c08153a4":"code","9812161f":"code","0b5d1347":"code","d6446882":"code","11b8e99d":"code","ea341543":"code","ebc0e227":"code","ce6c8355":"code","539706e0":"code","8225c105":"code","edce3714":"code","87dbe58e":"code","53dd1d6c":"code","e08d1df1":"code","77b6d9cc":"code","d5f36538":"code","2166f0e4":"code","62456d2b":"code","e2d8710d":"code","733c3835":"code","61b0a0cb":"code","54f8ecd0":"code","8f1e6853":"code","84cc1c7f":"code","4bb27822":"code","c60bcbfc":"code","6baaa0ea":"code","c6589119":"code","731b97c1":"code","b67e53c6":"code","2b5c4435":"code","7db89dd8":"code","fbc6e0a9":"code","3b086288":"code","04284566":"code","0986f64c":"code","fb8499fe":"code","f87dd58e":"code","7fea7732":"code","df3a3340":"code","b061ae98":"markdown","83828c11":"markdown","3fc648f0":"markdown","38ca8e85":"markdown","8c2480ec":"markdown","8803e3a0":"markdown","12964b6a":"markdown","3e3f34b8":"markdown","e7699c43":"markdown","e061dcc4":"markdown","4af066e8":"markdown","53bd82eb":"markdown","b98a659d":"markdown","7df0a257":"markdown","43715fae":"markdown","ade82d58":"markdown","9200144d":"markdown","59210a2a":"markdown","3ee88242":"markdown","01123f99":"markdown","e5fd5a29":"markdown","ff093823":"markdown"},"source":{"09412893":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics","6db699ee":"import h2o","1032cb71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","197db5a1":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","d6db20a1":"mushrooms_report = ProfileReport(df)","1fbd1e47":"mushrooms_report","2b913a0e":"# we can delete *veil-type*\ndf = df.drop('veil-type', axis=1)\ndf.shape","c08153a4":"fig = px.histogram(df, x='class', color='class', color_discrete_sequence=px.colors.qualitative.Plotly[3:])\nfig.show()","9812161f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(df['class'])","0b5d1347":"model_df = df.copy()\nmodel_df['class'] = y\nmodel_df.head()","d6446882":"from h2o.estimators import H2OXGBoostEstimator","11b8e99d":"h2o.init()","ea341543":"hf = h2o.H2OFrame(model_df)","ebc0e227":"hf['class'] = hf['class'].asfactor()\npredictors = hf.drop('class').columns\nresponse = 'class'","ce6c8355":"# Split into train and test\ntrain, valid = hf.split_frame(ratios=[.8], seed=1234)","539706e0":"# Build and train the model:\nmushroom_xgb = H2OXGBoostEstimator(booster='dart',\n                                  normalize_type=\"tree\",\n                                  seed=1234,\n                                  nfolds=5,\n                                  )","8225c105":"# Train the model\nmushroom_xgb.train(x=predictors,\n                  y=response,\n                  training_frame=train,\n                  validation_frame=valid)","edce3714":"mushroom_xgb.metric(metric='accuracy', valid=True)","87dbe58e":"mushroom_xgb.model_performance(valid=True)","53dd1d6c":"mushroom_xgb.varimp_plot(num_of_features=10)","e08d1df1":"mushroom_xgb.shap_summary_plot(valid)","77b6d9cc":"from h2o.automl import H2OAutoML","d5f36538":"aml = H2OAutoML(\n    max_models=20,\n    max_runtime_secs=300,\n    seed=1\n)","2166f0e4":"# Train the model\naml.train(x=predictors,\n        y=response,\n        training_frame=train,\n        validation_frame=valid\n)","62456d2b":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","e2d8710d":"lb = lb.as_data_frame()\nlb['model_type'] = lb['model_id'].apply(lambda x: x.split('_')[0])\nfig = px.bar(\n    lb, \n    x='model_id',\n    y='auc',\n    color='model_type'\n)\nfig.update_yaxes(range=[0.999, 1])\nfig.show()\n","733c3835":"print('The model performance in Accuracy: {}'.format(aml.leader.accuracy(valid=True)))\nprint('The model performance in AUC: {}'.format(aml.leader.auc(valid=True)))","61b0a0cb":"aml.leader.varimp_plot(num_of_features=10)","54f8ecd0":"# Label encoding y - dependent variable\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(df['class'])","8f1e6853":"# One hot encoding independent variable x\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\nx = ohe.fit_transform(df.drop('class', axis=1)).toarray()\nx.shape","84cc1c7f":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(x)","4bb27822":"covariance = pca.get_covariance()\nexplained_variance = pca.explained_variance_","c60bcbfc":"fig = go.Figure(go.Bar(\n    x=list(range(30)), \n    y=explained_variance[:30],\n))\nfig.update_layout(\n    xaxis_title='Principal components',\n    yaxis_title='Explained variance ratio',\n    legend_title='individual explained variance',\n)\nfig.show()","6baaa0ea":"pca = PCA(n_components=9)\nx_pca = pca.fit_transform(x)","c6589119":"x_pca.shape","731b97c1":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size = 0.2, random_state=1234)","b67e53c6":"# Create a list for the accuracy of all the models we are going to see\nacscore = []","2b5c4435":"# Training the Logistic Regression Model on the Training set\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression(random_state=1234)\nclassifier.fit(x_train, y_train)","7db89dd8":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nacscore.append(accuracy)\nprint(accuracy)","fbc6e0a9":"# Training the Naive Bayes Classification model\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","3b086288":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nacscore.append(accuracy)\nprint(accuracy)","04284566":"# Training the XGBoost Classification model\nfrom xgboost import XGBClassifier\n\nclassifier = XGBClassifier()\nclassifier.fit(x_train,y_train)","0986f64c":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nacscore.append(accuracy)\nprint(accuracy)","fb8499fe":"# Training the Random Forest Classification model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Lets use a standard 60 n_erstimatoprs\nclassifier = RandomForestClassifier(criterion='entropy', random_state=1234, n_estimators=60)\nclassifier.fit(x_train, y_train)","f87dd58e":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nacscore.append(accuracy)\nprint(accuracy)","7fea7732":"models_names = ['LogisticRegression', 'NaiveBayes', 'XGBoost', 'RandomForest']","df3a3340":"fig = go.Figure(go.Bar(\n    x=models_names,\n    y=acscore,\n))\nfig.show()","b061ae98":"![h2o-automl-logo2.jpg](attachment:h2o-automl-logo2.jpg)","83828c11":"As we can see, all the models have really high AUC. **H2O AutoML is awesome**","3fc648f0":"![one-hot-encoding.png](attachment:one-hot-encoding.png)","38ca8e85":"![boosting-trees.png](attachment:boosting-trees.png)","8c2480ec":"![logistic_regression.png](attachment:logistic_regression.png)","8803e3a0":"## XGBoost","12964b6a":"# Conclusions\n\nAs we saw, this a very easy to solve problem and we  solved it with really high accuracy and AUC.\n\n**The best models are XGBoost as we can see in the state of the art**.\n\nWe have checked that **H2O has and awsome performance and really fast**. Also, you can check the Sparkling Water parallelization for bigger datasets: [H2O original documentation](https:\/\/www.h2o.ai\/blog\/how-sparkling-water-brings-h2o-to-spark\/)","3e3f34b8":"# Data Analysis and Visualization","e7699c43":"# Classification Models Comparisson\n\nNow lets try to use the core classification methods. With label encoder and tipical PCA.","e061dcc4":"## Early data conclusiones:\n\nAll the variables are categorical so we will have to manage it.\n\n- **veil-type**:  is a constant variable so we can delete it\n- **odor**: is highly correlated with the class so we should take care\n- **correlation**: there is high correlation between *veil-color*, *gill-attachment* and *stalk-color-above-ring*","4af066e8":"![pca.jpeg](attachment:pca.jpeg)","53bd82eb":"As we can see, only the top (maybe 9 variables?) are the most importante. So we can just hold those ones.","b98a659d":"## Logistic Regression","7df0a257":"# H2O Classification\n\nIn this section we will train different models and compare the performance.","43715fae":"# AutoML\n\nIn this section we will try H2O AutoML and watch the best model performance.\n\nRemember that all models are trined using cross-validation.","ade82d58":"## XGBoost","9200144d":"## Principal Component Analysis\n\nNow we have 116 binary variables so we can try to some feature selection process.","59210a2a":"![naive-bayes-probability-conditioned.jpg](attachment:naive-bayes-probability-conditioned.jpg)","3ee88242":"![mushrooms.jpg](attachment:mushrooms.jpg)","01123f99":"As we can see, both classes are equally distributed and balanced.","e5fd5a29":"### Variable importance\n\nHere we can see wich variables are more important for the model and have higher relevance in the class.\n\n**As we saw at the bigining, *odor* is the highest correlated variable with the class**","ff093823":"## Naive Bayes"}}