{"cell_type":{"2970e341":"code","cb83af35":"code","56a0a084":"code","2acee9e0":"code","087360eb":"code","85959d6d":"code","a5e99f2e":"code","88d1cc6b":"code","83ba5491":"code","e7f80c74":"code","236d7f91":"code","9d0c8ed8":"code","4b279c23":"code","53c33988":"code","64dcb9e0":"code","9df177e7":"code","4f8f4c59":"code","99e40e02":"code","ac093b7a":"code","47f51e5c":"code","44fd3115":"code","44480640":"code","60fbf509":"code","d433e7a9":"code","6efec3d9":"code","b27fe219":"code","34cda87a":"code","244f5a7b":"code","d73aa1dd":"code","04ce64e1":"code","29049a18":"code","47c10bdb":"code","763e6ab7":"code","18a64d61":"code","7d7215a5":"code","84b7de8d":"code","34442d66":"code","f466eaba":"code","dfbfb003":"code","9e9f3a61":"code","373307ce":"code","dd4e1728":"code","95b4f3d4":"code","a7602194":"code","b99ed417":"code","c3c8c0f7":"code","4e9dfec7":"code","88dfc956":"code","a2d9c10a":"code","c7bb8158":"code","7c64cb76":"code","81a4f2ec":"code","ad4df494":"code","206bb4e8":"code","06bedf99":"code","30e32af1":"code","96d815dc":"code","b58028dc":"code","af50c60e":"code","7293ec31":"code","50713cbc":"code","c8ec634f":"code","108ed49b":"code","93965089":"code","f919e836":"code","0ed83153":"code","36eb0a95":"code","cd27bf5d":"code","4e2e9a98":"code","9834d337":"markdown","ecfeede8":"markdown","a70edd13":"markdown","86774ad6":"markdown","fe84fd71":"markdown","bdff8394":"markdown","9130f6d4":"markdown","6dd09a95":"markdown","0f757cf0":"markdown","dec78f50":"markdown","cca75fde":"markdown","99a88d8b":"markdown","0f227396":"markdown","3da88767":"markdown","9edec80a":"markdown","6c994257":"markdown","0e4b98dc":"markdown","1b001f89":"markdown","ac992b16":"markdown","00764a24":"markdown","945e3ff3":"markdown","942d4a15":"markdown","dc0c5aae":"markdown","cbdc6e0e":"markdown","0b6824c8":"markdown","8df8efb2":"markdown","032e268d":"markdown","a46cd6ac":"markdown","86764e02":"markdown","c8dc884b":"markdown","1bb71b05":"markdown","ad073634":"markdown","40d324be":"markdown","7f4cc69e":"markdown","7cacaec0":"markdown","4e350b43":"markdown","7340a871":"markdown","a9764f33":"markdown","6537df2e":"markdown","33886c1b":"markdown","c8efbd66":"markdown","74814ad5":"markdown","cd6c2de0":"markdown","89a37e4b":"markdown","e204539a":"markdown","a883d136":"markdown","4e61ce12":"markdown","4aaa41b5":"markdown","66704707":"markdown","e26bd9aa":"markdown","7da006bc":"markdown","9da4f704":"markdown","baa36873":"markdown","88bdea10":"markdown"},"source":{"2970e341":"!pip install chart_studio","cb83af35":"!pip install plotly==3.10.0","56a0a084":"import numpy as np \nimport pandas as pd \n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\n# Standard plotly imports\n#import plotly.plotly as py\n#import plotly.graph_objs as go\n#import plotly.tools as tls\n#from plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\n#import plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\n#init_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2acee9e0":"folder_path = '..\/input\/ieee-fraud-detection\/'\nind = 'TransactionID'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","087360eb":"train_transaction.head()","85959d6d":"train_identity.head()","a5e99f2e":"test_transaction.head()","88d1cc6b":"train_identity.shape, train_transaction.shape","83ba5491":"print('train_transaction has shape',train_transaction.shape)\nprint('train_identity has shape',train_identity.shape)\nprint('test_transaction has shape',test_transaction.shape)\nprint('test_identity has shape',test_identity.shape)","e7f80c74":"train_transaction['TransactionDT']","236d7f91":"fig = plt.figure(figsize = (12,8))\nax = fig.gca()\n\nplt.hist(train['TransactionDT'], label='train',bins=100);\nplt.hist(test['TransactionDT'], label='test',bins=100);\nplt.legend();\nplt.title('Distribution of Transaction Dates');\n\n   ","9d0c8ed8":"del train_identity,train_transaction,test_identity,test_transaction\ngc.collect()","4b279c23":"def red_mem_usage(df, verbose=True):\n    num = ['int16','int32','int64','float16','float32','float64']\n    start_mem = df.memory_usage(deep = True).sum()\/1024**2\n    for col in df.columns:\n        col_types = df[col].dtypes\n        if col_types in num:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_types)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem=df.memory_usage(deep = True).sum()\/1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n    \n    ","53c33988":"red_mem_usage(train)\nred_mem_usage(test)","64dcb9e0":"def analize(df):\n    analysis = pd.DataFrame(df.dtypes,columns=['d_types'])\n    analysis = analysis.reset_index()\n    analysis = analysis.rename(columns={\"index\": \"Col_name\"})\n    analysis['Missing_values'] = df.isnull().sum().values\n    analysis['Unique_values'] = df.nunique().values\n    return analysis","9df177e7":"def overview(df):\n    for col, values in df.iteritems():\n        num_uniques = values.nunique()\n        print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n        print (values.unique())\n        print ('\\n')\n    return","4f8f4c59":"train[['TransactionAmt']].describe()","99e40e02":"train[['TransactionAmt']].astype('float32').describe()","ac093b7a":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Amount Distributions', fontsize=18)\n\nplt.subplot(221)\nd = sns.distplot(train['TransactionAmt'])\nd.set_title(\"Transaction Amount Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)\n\n\nplt.subplot(222)\nd1 = sns.distplot(train[train['TransactionAmt'] <= 1000]['TransactionAmt'])\nd1.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\nd1.set_xlabel(\"\")\nd1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(223)\nl = sns.distplot(np.log(train['TransactionAmt']),color='r')\nl.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\nl.set_xlabel(\"\")\nl.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(224)\nl1 = sns.distplot(np.log(train[train['TransactionAmt']<=1000]['TransactionAmt']),color='r')\nl1.set_title(\"Transaction Amount (Log) Distribuition <= 1000\", fontsize=18)\nl1.set_xlabel(\"\")\nl1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n","47f51e5c":"\nplt.figure(figsize=(16,12))\nplt.suptitle('Transaction Amount Distributions, isFraud==1', fontsize=18)\n\nplt.subplot(221)\nd = sns.distplot(train[train['isFraud']==1]['TransactionAmt'])\nd.set_title(\"Transaction Amount Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\nd = sns.distplot(np.log(train[train['isFraud']==1]['TransactionAmt']),color='r')\nd.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)\n\n","44fd3115":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Amount Distributions, isFraud==0', fontsize=18)\n\nplt.subplot(221)\nd = sns.distplot(train[train['isFraud']==0]['TransactionAmt'],color='b')\nd.set_title(\"Transaction Amount Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\nd = sns.distplot(np.log(train[train['isFraud']==0]['TransactionAmt']),color='r')\nd.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)","44480640":"train[train['isFraud']==0]['TransactionAmt'].values.mean()","60fbf509":"train[train['isFraud']==1]['TransactionAmt'].values.mean()","d433e7a9":"train[train['isFraud']==0]['isFraud'].count()\/train['isFraud'].count()","6efec3d9":"analize(train[['dist1','dist2']])","b27fe219":"print(\"{0:.2f}\".format((352271\/590541)*100),'% of missing values in dist1 column and',\"{0:.2f}\".format((552913\/590541)*100),'% of missing values in dist 2 column')","34cda87a":"plt.figure(figsize=(16,12))\nplt.suptitle('dist1 and dist2',fontsize=18)\n\na=train[['dist1']].dropna(axis=0)\nb=train[['dist2']].dropna(axis=0)\n\nplt.subplot(221)\nd = sns.distplot(a,color='b')\nd.set_title(\"dist1 Distribuition\", fontsize=18)\nd.set_xlabel(\"\")\nd.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\nd2 = sns.distplot(b,color='r')\nd2.set_title(\"dist2 Distribution\", fontsize=18)\nd2.set_xlabel(\"\")\nd2.set_ylabel(\"Probability\", fontsize=15)\n","244f5a7b":"analize(train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])","d73aa1dd":"overview(train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])","04ce64e1":"train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']].astype('float32').describe()","29049a18":"analize(train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']])","47c10bdb":"train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']][:20]","763e6ab7":"train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']].astype('float32').describe()","18a64d61":"pd.set_option('display.max_columns',400)\nv_col = [c for c in train if c[0] == 'V']\ntrain[v_col].head()","7d7215a5":"id_col = ['id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11']\ntrain[id_col].astype('float32').describe()","84b7de8d":"analize(train[id_col])","34442d66":"plt.figure(figsize=(35, 12))\nfeatures = list(train[id_col])\nuniques = [len(train[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","f466eaba":"test.columns","dfbfb003":"analize(test)","9e9f3a61":"train.head()","373307ce":"for i in range(39):\n    if i<9:\n        test=test.rename(columns={\"id-0\"+str(i+1): \"id_0\"+str(i+1)})\n    test=test.rename(columns={\"id-\"+str(i+1): \"id_\"+str(i+1)})\ntest.head()","dd4e1728":"plt.figure(figsize=(35, 12))\nfeatures = list(test[id_col])\nuniques = [len(test[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","95b4f3d4":"l=[]\nfor i in range(12,39):\n    l.append('id_'+str(i))\ntrain[l].head()","a7602194":"n=[i for i in l if train[i].dtype=='float16']\ntrain[n]=train[n].astype('float32')\ntrain[n].describe()\n","b99ed417":"c = [k for k in l if train[k].dtype=='object']\ntrain[c].describe()","c3c8c0f7":"analize(train[l])","4e9dfec7":"overview(train[l])","88dfc956":"for i in n:\n    try:\n        train.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        test.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        plt.show()\n    except TypeError:\n        pass","a2d9c10a":"cols = ['TransactionDT'] + n\nplt.figure(figsize=(15,15))\nsns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\nplt.title('ID')\nplt.show()","c7bb8158":"enc_c = ['id_12','id_15','id_16','id_27','id_28','id_29','id_34','id_35','id_36','id_37','id_38']\nnenc_c=[k for k in c if k not in enc_c]\ndc = {'Unknown':-1,'NotFound':0,'Found':1,'New':2,'F':0,'T':1,'match_status:2':2, 'match_status:1':1, 'match_status:0':0, 'match_status:-1':-1}\nfor i in enc_c:\n    train[i]=train[i].map(dc)","7c64cb76":"cols = ['TransactionDT'] + enc_c\nplt.figure(figsize=(15,15))\nsns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\nplt.title('ID')\nplt.show()","81a4f2ec":"for i in nenc_c:\n    plt.figure(figsize=(80,30))\n\n    train[i]=train[i].fillna('Missing')\n    features = list(train[i].unique()[:20])\n    #if you want to see 10 most frequent values \n    #features = train['DeviceInfo'].value_counts()[:10].index.tolist()\n    uniques = [(train[i]==col).sum() for col in features]\n    sns.set(font_scale=2)\n    ax = sns.barplot(features,uniques, log=True)\n    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n    for p, uniq in zip(ax.patches, uniques):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","ad4df494":"overview(train[['ProductCD']])","206bb4e8":"plt.figure(figsize=(30,15))\ni='ProductCD'\n#train[i]=train[i].fillna('Missing')\nfeatures = list(train[i].unique())\nuniques = [(train[i]==col).sum() for col in features]\nsns.set(font_scale=1)\nax = sns.barplot(features,uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","06bedf99":"overview(train[['DeviceType']])","30e32af1":"plt.figure(figsize=(30,15))\ni='DeviceType'\ntrain[i]=train[i].fillna('Missing')\nfeatures = list(train[i].unique())\nuniques = [(train[i]==col).sum() for col in features]\nsns.set(font_scale=1)\nax = sns.barplot(features,uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","96d815dc":"overview(train[['DeviceInfo']])","b58028dc":"plt.figure(figsize=(30,15))\ni='DeviceInfo'\ntrain[i]=train[i].fillna('Missing')\nfeatures = train['DeviceInfo'].value_counts()[:10].index.tolist()\nuniques = [(train[i]==col).sum() for col in features]\nsns.set(font_scale=1)\nax = sns.barplot(features,uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","af50c60e":"train['DeviceInfo'].value_counts()[:10].index.tolist()","7293ec31":"new = ['card1','card2','card3','card4','card5','card6','M1','M2','M3','M4','M5','M6','M7','M8','M9']\nfor i in new:\n    plt.figure(figsize=(25,10))\n\n    train[i]=train[i].fillna('Missing')\n    features = list(train[i].unique()[:20])\n    #if you want to see 10 most frequent values \n    #features = train['DeviceInfo'].value_counts()[:10].index.tolist()\n    uniques = [(train[i]==col).sum() for col in features]\n    sns.set(font_scale=2)\n    ax = sns.barplot(features,uniques, log=True)\n    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n    for p, uniq in zip(ax.patches, uniques):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","50713cbc":"overview(train[['addr1','addr2']])","c8ec634f":"analize(train[['addr1','addr2']])","108ed49b":"train[['addr1','addr2']].describe()","93965089":"(train['addr2']==87).sum()","f919e836":"analize(train[['P_emaildomain','R_emaildomain']])","0ed83153":"overview(train[['P_emaildomain','R_emaildomain']])","36eb0a95":"for i in ['P_emaildomain','R_emaildomain']:\n    plt.figure(figsize=(25,10))\n\n    train[i]=train[i].fillna('Missing')\n    #features = list(train[i].unique()[:20])\n    #if you want to see 10 most frequent values \n    features = train[i].value_counts()[:10].index.tolist()\n    uniques = [(train[i]==col).sum() for col in features]\n    sns.set(font_scale=2)\n    ax = sns.barplot(features,uniques, log=True)\n    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' most frequent email adresses')\n    for p, uniq in zip(ax.patches, uniques):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","cd27bf5d":"for i in ['P_emaildomain','R_emaildomain']:\n    plt.figure(figsize=(25,10))\n\n    train[i]=train[i].fillna('Missing')\n    #features = list(train[i].unique()[:20])\n    #if you want to see 10 most frequent values \n    features = train[i].value_counts()[-10:].index.tolist()\n    uniques = [(train[i]==col).sum() for col in features]\n    sns.set(font_scale=2)\n    ax = sns.barplot(features,uniques, log=True)\n    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' least frequent email adresses')\n    for p, uniq in zip(ax.patches, uniques):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","4e2e9a98":"for i in ['P_emaildomain','R_emaildomain']:\n    plt.figure(figsize=(25,10))\n\n    #train[i]=train[i].fillna('Missing')\n    #features = list(train[i].unique()[:20])\n    #if you want to see 10 most frequent values \n    features = (train[train.iloc[:]['addr2']== 87]['P_emaildomain']).value_counts(sort=True)[:10].index.tolist()\n    uniques = [(train[i]==col).sum() for col in features]\n    sns.set(font_scale=2)\n    ax = sns.barplot(features,uniques, log=True)\n    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' with addr2==87')\n    for p, uniq in zip(ax.patches, uniques):\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 10,\n                uniq,\n                ha=\"center\") ","9834d337":"### DeviceType,DeviceInfo and ProductCD","ecfeede8":"## Numerical Data Analysis","a70edd13":"> Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\u201cFor example, how many times the payment card associated with a IP and email or address appeared in 24 hours time range, etc.\u201d\n\"All Vesta features were derived as numerical. some of them are count of orders within a clustering, a time-period or condition, so the value is finite and has ordering (or ranking). I wouldn't recommend to treat any of them as categorical. If any of them resulted in binary by chance, it maybe worth trying.\"","86774ad6":"It is hard to find out what exactly these features mean. ","fe84fd71":"### dist1, dist2","bdff8394":"> card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.","9130f6d4":"> ProductCD: product code, the product for each transaction\n\u201cProduct isn't necessary to be a real 'product' (like one item to be added to the shopping cart). It could be any kind of service.\u201d","6dd09a95":"10 most frequently used emails","0f757cf0":"## EDA","dec78f50":"10 least frequently used emails","cca75fde":"In discussion described as follows:","99a88d8b":"# Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n**Files**\n\n- train_{transaction, identity}.csv - the training set\n- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n- sample_submission.csv - a sample submission file in the correct format\n","0f227396":"### V1-V339","3da88767":"Let's inspect Transaction amounts first and see if transaction amount has anything to do with fraud. My assumption is that bigger the transaction amount the bigger the chance there is that the transaction is a fraud, but on the other hand maybe we can make a different argument. Intuitively speaking, we can think that maybe someone would make a small fraud because of the fear of getting caught. Data will say more.","9edec80a":"### card1-card6 and M1-M9","6c994257":"Data is splitted in two data sets called Identity and Transaction. Each of these two sets has a column named TransactionID. We merged our two data sets on that column.","0e4b98dc":"Both in test set and training set column with the highest number of unique values is id_02. We have less missing values in test set.","1b001f89":"I find this 4 features important because I think they will play big role in our predictive model. Why do I think that? Because I find them very interesting and think that we can extract some key insights from them about some fraud \"patterns\" and maybe connect them to some other features.","ac992b16":"A lot of data missing.","00764a24":"A lot of ones and Nan's.","945e3ff3":">addr: address; \n>\u201cboth addresses are for purchaser: addr1 as billing region\naddr2 as billing country\u201d\"\n\n> P_ and (R_) emaildomain: purchaser and recipient email domain; \u201c certain transactions don't need recipient, so Remaildomain is null.\u201d","942d4a15":"> D1-D15: timedelta, such as days between previous transaction, etc.","dc0c5aae":"As we can see from the code above, data is imbalanced. There is 96.5% data samples with no fraud and only 3.5% with fraud. ","cbdc6e0e":"### C1-C14","0b6824c8":"Important\n> **The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n[The timespan of the dataset is 1 year ?\n](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100071#latest-577632) by Suchith**\n\n```\nTrain: min = 86400 max = 15811131\nTest: min = 18403224 max = 34214345\n```\n\nThe difference train.min() and test.max() is ```x = 34214345 - 86400 = 34127945``` but we don't know is it in seconds,minutes or hours.\n\n```\nTime span of the total dataset is 394.9993634259259 days\nTime span of Train dataset is  181.99920138888888 days\nTime span of Test dataset is  182.99908564814814 days\nThe gap between train and test is 30.00107638888889 days\n```\n\nIf it is in seconds then dataset timespan will be ```x\/(3600*24*365) = 1.0821``` years which seems reasonable to me. So if the **transactionDT** is in **seconds** then\n\n```\nTime span of the total dataset is 394.9993634259259 days\nTime span of Train dataset is  181.99920138888888 days\nTime span of Test dataset is  182.99908564814814 days\nThe gap between train and test is 30.00107638888889 days\n```\n","8df8efb2":"### Transaction amount ","032e268d":"If the assumption is right that we got data for 394 days, more precisely for a year and 30 days in between, then we can see spikes in the same part of the year.\nWith help of discussions, it would seem that data set starts wit 01.12. and ends with 31.12.","a46cd6ac":"### Quick overview of a whole data set","86764e02":"We can see some repetative behaviour (first month resembles the last one).","c8dc884b":"We can clearly see some patterns (in most cases):\n* When D1==0 D2==Nan and when D1==x, x>0, D2==x\n* Same thing with D3 and D5, but happens less often\n   ","1bb71b05":"> C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n\u201cCan you please give more examples of counts in the variables C1-15? Would these be like counts of phone numbers, email addresses, names associated with the user? I can't think of 15.\nYour guess is good, plus like device, ipaddr, billingaddr, etc. Also these are for both purchaser and recipient, which doubles the number.\u201d","ad073634":"# **Fraud Detection**","40d324be":"Some of the features have a lot of unique values (look more like numerical data). ","7f4cc69e":"As we can see the result of a describe method wasn't exactly as we would liked it to be because there is no mean and std results. That happened because dtype of a column is float16. So we do this:","7cacaec0":"We don't know what exactly this 2 numerical features are. They could be distances between billing address, zip code, IP address, phone area...","4e350b43":"A lot of missing values. Is there any reason behind?","7340a871":"Still a lot of missing data but as expected windows is most frequently used. ","a9764f33":"The thing that is bugging me is why are there so many negative values, because by the description of the Vesta company this features seem all to be positive. Let try to analize some general stuff and aybe come to this later.","6537df2e":"Reducing the memory of numerics","33886c1b":"So as we can see we have 332 billing regions and 74 billing countries.","c8efbd66":"Let's inspect our data set! First thing that we are going to do is inspect how many missing and unique values are there.","74814ad5":"I find this very interesting. Could it be that value 87 == USA? ","cd6c2de0":"encode all categorical values","89a37e4b":"### addr1, addr2, P_emaildomain and R_emaildomain","e204539a":"The real meaning behind D and C features is masked and it is hard to find a real meaning of each feature. We can only take a guess for some of the feature's meaning.","a883d136":"> \u201cid01 to id11 are numerical features for identity, which is collected by Vesta and security partners such as device rating, ip_domain rating, proxy rating, etc. Also it recorded behavioral fingerprint like account login times\/failed to login times, how long an account stayed on the page, etc. All of these are not able to elaborate due to security partner T&C. I hope you could get basic meaning of these features, and by mentioning them as numerical\/categorical, you won't deal with them inappropriately.\u201d\n\n","4e61ce12":"> M1-M9: match, such as names on card and address, etc.","4aaa41b5":"### D1-D15","66704707":"Unfortunately, it doesn't tell me anything special. Second interesting method some used in this competition was looking at the decimal places of the transaction amount and then looking at those mails. Why is this a smart approach? Because they saw that rows that had 3 or more decimal places were linked to non-USA emails due to the exchange of the currencies.","e26bd9aa":"## Categorical features","7da006bc":"### id1-id11","9da4f704":"As we can see **test dataset** and **train dataset** have similar number of samples. We will deal with a lot of columns in transaction dataset that have many unknown values and many features that we dont know what they are.","baa36873":"From the kernel:\n\n[EDA what's behind D features?](https:\/\/www.kaggle.com\/akasyanama13\/eda-what-s-behind-d-features)\n\nWe can see that D3 feature indicates days from the previous transaction.","88bdea10":"Let's start analyzing categorical features. We will start with id categorical features."}}