{"cell_type":{"1f711942":"code","1356eb55":"code","cdba405d":"code","93171f02":"code","ea08169a":"code","f519b26c":"code","15992349":"code","17a40f88":"code","f5b7e201":"code","1cae0dd6":"code","5027d572":"code","d349c96e":"code","15ee652c":"code","5f7d166e":"code","ac43c5dc":"code","898886f2":"code","1ba34acd":"code","bfecca81":"code","4cbd0970":"code","c7f92929":"markdown","e5517d8a":"markdown","6bb115e9":"markdown","b1c6b5d3":"markdown"},"source":{"1f711942":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1356eb55":"from sklearn.datasets import load_boston\nimport seaborn as sns\nfrom sklearn import linear_model\n\nboston = load_boston()\ndataset=pd.DataFrame(boston.data, columns=boston.feature_names)\ndataset['target']=boston.target","cdba405d":"sns.pairplot(dataset)\nplt.show()","93171f02":"sns.pairplot(dataset[['target', 'LSTAT', \n                     'INDUS','NOX']])","ea08169a":"g = sns.PairGrid(dataset[['target', 'LSTAT', \n                     'INDUS','NOX']])\ng = g.map_upper(plt.scatter)\ng = g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng = g.map_diag(sns.kdeplot, lw=3, legend=False)","f519b26c":"corr=dataset.corr()\ncorr[corr==1]=np.nan\n\nfor i in range(len(corr)):\n    for j in range(len(corr)):\n        if i<=j:\n            corr.iloc[i,j]=np.nan\n            \n            \nplt.figure(figsize=(20,10))\n\nsns.heatmap(corr,linewidths=3, annot=True, cmap='coolwarm')\nplt.show()","15992349":"from sklearn.preprocessing import StandardScaler\nX=dataset.iloc[:,1:-1]\nobservations = len(dataset)\nvariables = dataset.columns\nstandarddization=StandardScaler()\nXst = standarddization.fit_transform(X)\noriginal_means=standarddization.mean_\noriginal_stds=standarddization.var_**0.5\nXst = np.column_stack((Xst, np.ones(observations)))\ny = dataset['target']","17a40f88":"import random\ndef random_w( p ):\n    return np.array([np.random.normal() for j in range(p)])\ndef hypothesis(X,w):\n    return np.dot(X,w)\ndef loss(X,w,y):\n    return hypothesis(X,w) - y\ndef squared_loss(X,w,y):\n    return loss(X,w,y)**2\ndef gradient(X,w,y):\n    gradients = list()\n    n = float(len( y ))\n    for j in range(len(w)):\n        gradients.append(np.sum(loss(X,w,y) * X[:,j]) \/ n)\n    return gradients\ndef update(X,w,y, alpha=0.01):\n    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\ndef optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n    w = random_w(X.shape[1])\n    path = list()\n    for k in range(iterations):\n        SSL = np.sum(squared_loss(X,w,y))\n        new_w = update(X,w,y, alpha=alpha)\n        new_SSL = np.sum(squared_loss(X,new_w,y))\n        w = new_w\n        if k>=5 and (new_SSL - SSL <= eta and \\\n        new_SSL - SSL >= -eta):\n            path.append(new_SSL)\n            return w, path\n        if k % (iterations \/ 20) == 0:\n            path.append(new_SSL)\n    return w, path\n\nalpha = 0.02\nw, path = optimize(Xst, y, alpha, eta = 10**-12, \\\niterations = 20000)\nprint (\"These are our final standardized coefficients: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))","f5b7e201":"unstandardized_betas = w[:-1]\/original_stds\nunstandardized_bias=w[-1]-np.sum(original_means\/original_stds*w[:-1])\nprint(unstandardized_betas)\nprint(unstandardized_bias)","1cae0dd6":"linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)","5027d572":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nstandardization = StandardScaler()\nStand_Coeff = make_pipeline(standardization, linear_regression)\n\nlinear_regression.fit(X,y)\n\nfor coef, var in sorted(zip(map(abs,linear_regression.coef_), dataset.columns), reverse=True):\n    print(\"%6.3f %s\" %(coef,var))","d349c96e":"from sklearn.metrics import r2_score\nlinear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n\ndef r2_est(X,y):\n    return r2_score(y, linear_regression.fit(X,y).predict(X))\n\nprint('Baseline: ', r2_est(X,y))","15ee652c":"from sklearn.preprocessing import PolynomialFeatures\ncreateinteractions=PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)","5f7d166e":"\nXi=createinteractions.fit_transform(X)\nXi=X\nXi['interaction']=X['RM']*X['LSTAT']","ac43c5dc":"print('R2 of a model with RM*LSTAT interaction: %0.3f' %r2_est(Xi,y))","898886f2":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n\nlinear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\ncreate_cubic = PolynomialFeatures(degree=3, interaction_only=False)\ncreate_highdegree = PolynomialFeatures(degree=7, interaction_only=False, include_bias=False)\n\nlinear_predictor = make_pipeline(linear_regression)\ncubic_predictor=make_pipeline(create_cubic, linear_regression)","1ba34acd":"predictor = 'LSTAT'\nx=dataset.LSTAT.values.reshape((len(dataset.LSTAT.values),1))\n\nxt=np.arange(0,50,0.1).reshape((500,1))\nx_range = [dataset[predictor].min(), dataset[predictor].max()] \ny_range = [dataset['target'].min(), dataset['target'].max()] \n\nscatter=dataset.plot(kind='scatter', x=predictor, y='target', xlim=x_range, ylim=y_range)\nregression_line=scatter.plot(xt, linear_predictor.fit(x,y).predict(xt), color='red', linewidth=3)\n","bfecca81":"scatter=dataset.plot(kind='scatter', x=predictor, y='target', xlim=x_range, ylim=y_range)\nregression_line=scatter.plot(xt, cubic_predictor.fit(x,y).predict(xt), color='red', linewidth=3)","4cbd0970":"i=0\nplt.style.use('ggplot')\nfig=plt.figure(figsize=(20,15))\nfor d in [1,2,3,5,15]:\n    i+=1\n    create_pynomial = PolynomialFeatures(degree=d, interaction_only=False, include_bias=False)\n    poly = make_pipeline(create_pynomial, StandardScaler(), linear_regression)\n    model = poly.fit(x,y)\n    print('R2 degree - %2i polynomial :%0.3f' %(d, r2_score(y, model.predict(x))))\n    plt.subplot(2,3,i)\n    plt.scatter(x,y, color='gray')\n    plt.plot(xt, model.fit(x,y).predict(xt), linewidth=3)\n    plt.xlim(x_range)\n    plt.ylim(y_range)\n    plt.title('R2 degree - %2i polynomial :%0.3f' %(d, r2_score(y, model.predict(x))))\n    \nplt.show()","c7f92929":"## **UNSTANDARD COEFFICIENTS**","e5517d8a":"## **Revisiting Gradient Descent**","6bb115e9":"## Working with standardized coefficients","b1c6b5d3":"## The Coefficient of Determination (R2)"}}