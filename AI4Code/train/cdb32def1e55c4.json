{"cell_type":{"e7b063de":"code","cc9178d2":"code","70719621":"code","b35c3457":"code","37b6b54f":"code","820afcf8":"code","68a3afb9":"code","565b19e4":"code","0fdc12c3":"code","31098ded":"code","f4bd023e":"code","fad6a37f":"code","30d177d5":"code","7544254c":"code","04917671":"code","9e914876":"code","fd7e9053":"code","16f2be65":"code","0104f7f2":"code","923d311c":"code","ea86a377":"code","4324f20e":"code","ecefcb6b":"code","084b6aa5":"code","a1b9d8a7":"code","9780a60a":"code","32cbfe14":"code","6dabc50e":"code","883c0b48":"code","4224543b":"code","de9ec964":"code","f6652e30":"code","c36e5acc":"code","0f4120d2":"markdown","198513ee":"markdown","8bb70c7e":"markdown","01cd44c4":"markdown","994e39a5":"markdown","21afea2c":"markdown","076cdb8d":"markdown","e550554c":"markdown","6e7f749b":"markdown","de5ca2c6":"markdown","284726ca":"markdown","95a720da":"markdown","83715a03":"markdown","c739d2d0":"markdown","0291399e":"markdown","3c8c83c2":"markdown","54168f93":"markdown","40e3a0d0":"markdown","945b6b1c":"markdown","bf5d8c04":"markdown","242656f0":"markdown","811fba23":"markdown","afc0e7e3":"markdown","46bdb564":"markdown","8754abf5":"markdown","3e8a2bbd":"markdown","a0b361a0":"markdown","36d9e387":"markdown","2bc1b08a":"markdown","546663b3":"markdown","56423370":"markdown","f29e7b6d":"markdown","9a6ce03d":"markdown","1b0f1c3e":"markdown","0817290a":"markdown","92f0a259":"markdown"},"source":{"e7b063de":"import numpy as np  \nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt","cc9178d2":"DATASET_COLUMNS=['target','ids','date','flag','user','text']\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding=DATASET_ENCODING,names=DATASET_COLUMNS, skiprows=795000, nrows = 10000)","70719621":"#dataset=dataset.sample(n=100000)","b35c3457":"dataset.head()","37b6b54f":"dataset.shape","820afcf8":"dataset.info()","68a3afb9":"dataset.isnull().sum()","565b19e4":"import seaborn as sns\nsns.countplot(x='target', data=dataset)","0fdc12c3":"data=dataset[['text','target']]","31098ded":"data['target'].unique()","f4bd023e":"data['target'] = data['target'].replace(4,1)","fad6a37f":"data['target'].unique()","30d177d5":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer","7544254c":"nltk.download('wordnet')","04917671":"from collections import defaultdict","9e914876":"text = []\nfor i in range(0, int(data.shape[0])):\n    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    lm = nltk.WordNetLemmatizer()\n    review = [lm.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    text.append(review)","fd7e9053":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000)","16f2be65":"X = cv.fit_transform(text).toarray()","0104f7f2":"X.shape","923d311c":"Y = data.target","ea86a377":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.05, random_state = 0)","4324f20e":"from sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import accuracy_score\ndef model_Evaluate(model, x_test,  y_test):\n    y_pred = model.predict(x_test)\n    print(accuracy_score(y_test, y_pred))","ecefcb6b":"from sklearn.linear_model import LogisticRegression\nclassifier1 = LogisticRegression(random_state = 0)\nclassifier1.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier1, X_test, Y_test)","084b6aa5":"from sklearn.neighbors import KNeighborsClassifier\nclassifier2 = KNeighborsClassifier(n_neighbors = 50, metric = \"minkowski\", p=2)\nclassifier2.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier2, X_test, Y_test)","a1b9d8a7":"from sklearn.svm import SVC\nclassifier3 = SVC(kernel = \"linear\", random_state = 0)\nclassifier3.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier3, X_test, Y_test)","9780a60a":"from sklearn.svm import SVC\nclassifier4 = SVC(kernel = 'rbf', random_state = 0)\nclassifier4.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier4,X_test, Y_test)","32cbfe14":"from sklearn.tree import DecisionTreeClassifier\nclassifier5 = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\nclassifier5.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier5, X_test, Y_test)","6dabc50e":"from sklearn.ensemble import RandomForestClassifier\nclassifier6 = RandomForestClassifier(n_estimators = 200, criterion = \"entropy\", random_state = 0)\nclassifier6.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier6, X_test, Y_test)","883c0b48":"from sklearn.naive_bayes import GaussianNB\nclassifier7 = GaussianNB()\nclassifier7.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier7, X_test, Y_test)","4224543b":"from xgboost import XGBClassifier\nclassifier8 = XGBClassifier()\nclassifier8.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier8, X_test, Y_test)","de9ec964":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=40, activation='relu'))\nann.add(tf.keras.layers.Dense(units=20, activation='relu'))\nann.add(tf.keras.layers.Dense(units=10, activation='relu'))\nann.add(tf.keras.layers.Dense(units=5, activation='relu'))\nann.add(tf.keras.layers.Dense(units=2, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nann.fit(X_train, Y_train, batch_size = 32, epochs = 100, validation_split=0.2)","f6652e30":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)","c36e5acc":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_test , y_pred)","0f4120d2":"**Project Pipeline**\n\nThe various steps involved in the Machine Learning Pipeline are :\n\nImport Necessary Libraries\n\nRead and Load the Dataset\n\nExploratory Data Analysis\n\nData Visualization of Target Variables\n\nData Preprocessing\n\nSplitting our data into Train and Test Subset\n\nFunction For Model Evaluation\n\nModel Building\n\nConclusion\n","198513ee":"*5.8: Fit the CountVectorizer and transform the data*","8bb70c7e":"# **Step-2: Read and Load the Dataset**","01cd44c4":"**Logistic Regression**","994e39a5":"**Artificial Neural Network**","21afea2c":"Upon evaluating all the models we can conclude that the **Logistic Regression** is the best model for the above-given dataset.","076cdb8d":"**K-Nearest Neighbors**","e550554c":"*5.6: Cleaning the tweet text*","6e7f749b":"# **Step-4: Data Visualization of Target Variables**","de5ca2c6":"# **Step-5: Data Preprocessing**","284726ca":"*3.1: Five top records of data*","95a720da":"# **Step-3: Exploratory Data Analysis**","83715a03":"**Decision Tree Classifier**","c739d2d0":"**XG-Boost** ","0291399e":"*Step 5.5 : Importing necessary Dependencies*","3c8c83c2":"*5.2: Print unique values of target*","54168f93":"*3.4 : Checking for missing values*","40e3a0d0":"# **Step-6: Splitting our dataset into Train and Test Subset**","945b6b1c":"**Random Forest Classifier**","bf5d8c04":"*5.4: Print unique values of target*","242656f0":"# **Step-7: Function For Model Evaluation**","811fba23":"# Conclusion","afc0e7e3":"*K-fold cross Validation*","46bdb564":"**Support Vector Machines (S.V.M.)**","8754abf5":"**Kernel S.V.M.**","3e8a2bbd":"*5.7: Creating bag of words model \/ Getting tokenization of tweet text*","a0b361a0":"**Naive Bayes**","36d9e387":"# **Twitter Sentiment** **Analysis** ","2bc1b08a":"**Models used :**\n\n  Logistic Regression\n\n  K-Nearest Neighbors\n\n  Support Vector Machines (S.V.M.)\n\n  Kernel S.V.M.\n\n  Decision Tree Classifier\n\n  Random Forest Classifier\n\n  Naive Bayes\n\n  Artificial Neural Network\n\n  XG-Boost","546663b3":"*3.3: Data information*","56423370":"*We will train various models and compare their accuracies to get the model.*","f29e7b6d":"*5.1: Selecting the text and Target column for our further analysis*","9a6ce03d":"# **Step-1: Import Necessary Libraries**","1b0f1c3e":"*3.2: Shape of data*","0817290a":"*5.3: Replacing the values to ease understanding. (Assigning 1 to Positive sentiment 4)*","92f0a259":"# **Step-8: Model Building**"}}