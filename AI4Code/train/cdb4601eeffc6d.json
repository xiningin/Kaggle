{"cell_type":{"fb484be4":"code","90feb32b":"code","8b8b35e6":"code","a182e292":"code","b2d4db9e":"code","6e25deee":"code","e63c2d79":"code","73c9f8af":"code","821d7168":"code","e319e7c6":"code","e2e717c4":"code","db128fc9":"code","578e42f6":"code","65fdda49":"code","730692d5":"code","1dbc7224":"code","79587b60":"code","fc2fdc50":"code","08fa8950":"code","5925aa02":"code","4295c379":"code","dbd99d24":"code","01e3d70d":"code","5eb12059":"code","5b5e6966":"code","cbefce47":"code","ebb8bdc2":"code","77fdd674":"code","beacd4a0":"code","d04246d6":"code","700cdf4d":"code","3c08750e":"code","12e9153c":"code","1506c6b3":"code","5f3e65ab":"code","96151ce9":"code","63572f1a":"markdown","0ac8cc17":"markdown","b41f18f0":"markdown","fbb8d164":"markdown","3a83622e":"markdown","55a4647a":"markdown","37409f9a":"markdown","05c97e12":"markdown","63ae61fa":"markdown","bf188768":"markdown","b578fd7f":"markdown","7df82545":"markdown","31a606cd":"markdown","7506f1ef":"markdown","8fcffa02":"markdown","bf7aade9":"markdown","1065b84f":"markdown","e6be5f57":"markdown","a7cab73e":"markdown","4e2b9de8":"markdown","3151af73":"markdown","d57c3a44":"markdown","464e1eef":"markdown","d1880836":"markdown","74fa0a89":"markdown","7738a16c":"markdown","87c17d18":"markdown","c344ce00":"markdown","abe9857c":"markdown","6eb1019e":"markdown","dd8c5eca":"markdown","a908c0d9":"markdown","a443ae9e":"markdown","938cf2c8":"markdown","6259eed8":"markdown","93c5b24c":"markdown","74791538":"markdown","eaa3f154":"markdown","ed7fb257":"markdown","6c39fb88":"markdown","7a4fc90b":"markdown","287b6ce5":"markdown","d3d7c4c0":"markdown","8e7b5afb":"markdown","24f9616f":"markdown","99cb9c50":"markdown","bd872226":"markdown","667e28f7":"markdown","adec1d38":"markdown","859b8cc6":"markdown","1f9c64b6":"markdown","d4b528f5":"markdown","b67c83a7":"markdown","2374db84":"markdown","1feb400f":"markdown","bfd0ca2d":"markdown","5e3df286":"markdown","c4dac9d6":"markdown","641aba55":"markdown","d65343cd":"markdown","f82c3da2":"markdown","f6308d41":"markdown","0a64f760":"markdown","9a2d2d02":"markdown","be1ba552":"markdown","9e073406":"markdown","d17e1d47":"markdown","bcf52d7a":"markdown","182c642f":"markdown","12977b0d":"markdown","aafcf604":"markdown","06d54530":"markdown","65390755":"markdown","675aa30c":"markdown","0f9a87b0":"markdown","ebd18b80":"markdown","44f15b6e":"markdown","433adec8":"markdown","f13ecac7":"markdown","90be3baf":"markdown","e9e7e2d8":"markdown","e463d80f":"markdown","28906c34":"markdown","a54c8a5c":"markdown"},"source":{"fb484be4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","90feb32b":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(data.shape)\ndata.head()","8b8b35e6":"data.isnull().sum()","a182e292":"data.Cabin.unique()","b2d4db9e":"n_data = data.drop(columns=[\"Cabin\"])\nn_data.isnull().sum()","6e25deee":"#Mean on train data\nmean = (n_data[\"Age\"].mean()*n_data.shape[0])\/n_data.shape[0]\nn_data[\"Age\"] = n_data[\"Age\"].fillna(int(float(mean)))\nn_data.isnull().sum()","e63c2d79":"n_data[\"Embarked\"] = n_data[\"Embarked\"].fillna(str(n_data[\"Embarked\"].mode()))\nn_data.isnull().sum()","73c9f8af":"n_data = n_data.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"])\nn_data.head()","821d7168":"sn_data = n_data[n_data.Survived == 1]\nsn_data.head()","e319e7c6":"plt.hist(n_data.Sex, bins=n_data.Sex.unique().size*2 - 1, color=\"r\")\nplt.hist(sn_data.Sex, bins=n_data.Sex.unique().size + 1)\nplt.show()","e2e717c4":"plt.hist(n_data.Pclass, bins=n_data.Pclass.unique().size*2 -1, color=\"r\")\nplt.hist(sn_data.Pclass, bins=sn_data.Pclass.unique().size*2 -1)\nplt.show()","db128fc9":"plt.subplots(figsize=(17, 4))\nplt.subplot(1, 3, 1)\nsns.boxplot(n_data.Age, color=\"r\")\nplt.subplot(1, 3, 2)\nsns.boxplot(sn_data.Age)\nplt.subplot(1, 3, 3)\nsns.distplot(n_data.Age, color=\"r\")\nsns.distplot(sn_data.Age)\nplt.show()","578e42f6":"plt.subplots(figsize=(17, 4))\nplt.subplot(1, 3, 1)\nsns.boxplot(n_data.SibSp, color=\"r\")\nplt.subplot(1, 3, 2)\nsns.boxplot(sn_data.SibSp)\nplt.subplot(1, 3, 3)\nplt.hist(n_data.SibSp, bins=20, color=\"r\")\nplt.hist(sn_data.SibSp, bins=10)\nplt.show()","65fdda49":"plt.subplots(figsize=(17, 4))\nplt.subplot(1, 3, 1)\nsns.boxplot(n_data.Parch, color=\"r\")\nplt.subplot(1, 3, 2)\nsns.boxplot(sn_data.Parch)\nplt.subplot(1, 3, 3)\nplt.hist(n_data.Parch, bins=17, color=\"r\")\nplt.hist(sn_data.Parch, bins=7*2)\nplt.show()","730692d5":"plt.subplots(figsize=(17, 4))\nplt.subplot(1, 2, 1)\nsns.distplot(n_data.Fare, color=\"r\")\nsns.distplot(sn_data.Fare)\nplt.subplot(2, 2, 2)\nsns.boxplot(sn_data.Fare)\nplt.subplot(2, 2, 4)\nsns.boxplot(n_data.Fare, color=\"r\")\nplt.show()","1dbc7224":"plt.hist(n_data.Embarked, bins=15, color=\"r\")\nplt.hist(sn_data.Embarked, bins=15)\nplt.show","79587b60":"n_data_d = pd.get_dummies(n_data)","fc2fdc50":"n_data_d","08fa8950":"n_data_d = n_data_d.drop(columns=[\"Embarked_0    S\\ndtype: object\"])\nn_data_d.head()","5925aa02":"corr_m = n_data_d.corr()\nplt.subplots(figsize=(12, 8))\nsns.heatmap(corr_m, annot=True, square=True)\nplt.show()","4295c379":"high_corr = corr_m.nlargest(12, 'Survived')['Survived'].drop(['Survived'])\nhigh_corr","dbd99d24":"n_data_d.Age = np.log1p(n_data_d.Age)","01e3d70d":"sns.distplot(n_data_d.Age)\nplt.show()","5eb12059":"n_data_d.Fare = np.log1p(n_data_d.Fare)","5b5e6966":"sns.distplot(n_data_d.Fare)\nplt.show()","cbefce47":"from sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split, RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve, make_scorer\nimport tensorflow.keras as keras\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.models import Sequential\nimport tensorflow as tf\nfrom lightgbm import LGBMClassifier","ebb8bdc2":"print(\"Survived = {} \\nDead = {}\".format(n_data_d[n_data_d.Survived == 1].shape[0], n_data_d[n_data_d.Survived == 0].shape[0]))","77fdd674":"n_data_d.head()","beacd4a0":"#X is what we will use to predict, and y is what we should predict\nX, y = n_data_d.drop(columns=[\"Survived\"]), n_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","d04246d6":"roc_auc = make_scorer(roc_auc_score, higher_is_better=True)\n\nrf = RandomForestClassifier(bootstrap=True, n_estimators=700, criterion='entropy')\nrf.fit(X_train, y_train)\n\nprint(\"Best score on train data: {:.4f}\".format(roc_auc_score(rf.predict(X_train), y_train)))\nprint(\"Best score on test data: {}\\n\".format(roc_auc_score(rf.predict(X_test), y_test)))","700cdf4d":"for i in np.arange(len(rf.feature_importances_)):\n    print(\"{} : {:.4f}\".format(X_train.columns[i], rf.feature_importances_[i]))","3c08750e":"params={'n_neighbors' : range(1, 20), 'leaf_size' : range(1, 50)}\n\nknn_grid = GridSearchCV(KNeighborsClassifier(), params, scoring='roc_auc')\nknn_grid.fit(X_train, y_train)\n\nprint(\"Best GridSearchCV params: {}\".format(knn_grid.best_params_))\nprint(\"Best score on train data: {:.4f}\".format(knn_grid.best_score_))\nprint(\"Best score on test data: {}\\n\".format(roc_auc_score(knn_grid.predict(X_test), y_test)))","12e9153c":"params = {'alpha' : [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000], 'normalize' : [True, False], 'random_state' : [0, 50, 100, 150, 200]}\n\nr_grid = GridSearchCV(RidgeClassifier(), params, scoring='roc_auc')\nr_grid.fit(X_train, y_train)\n\nprint(\"Best GridSearchCV params: {}\".format(r_grid.best_params_))\nprint(\"Best score on train data: {:.4f}\".format(r_grid.best_score_))\nprint(\"Best score on test data: {}\\n\".format(roc_auc_score(r_grid.predict(X_test), y_test)))","1506c6b3":"for i in np.arange(len(r_grid.best_estimator_.coef_[0])):\n    print(\"{} : {}\".format(X_train.columns[i], r_grid.best_estimator_.coef_[0][i]))","5f3e65ab":"params = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'], 'C' : [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]}\n\nlog_reg_grid = GridSearchCV(LogisticRegression(), params, scoring='roc_auc')\nlog_reg_grid.fit(X_train, y_train)\n\nprint(\"Best GridSearchCV params: {}\".format(r_grid.best_params_))\nprint(\"Best score on train data: {:.4f}\".format(log_reg_grid.best_score_))\nprint(\"Best score on test data: {}\\n\".format(roc_auc_score(log_reg_grid.predict(X_test), y_test)))","96151ce9":"nn = Sequential()\n\nnn.add(Dense(10, kernel_initializer=keras.initializers.glorot_uniform, activation='tanh'))\nnn.add(Dense(16, kernel_initializer=keras.initializers.he_normal, activation='elu'))\nnn.add(Dropout(0.3))\nnn.add(Dense(32, kernel_initializer=keras.initializers.glorot_uniform, activation='tanh'))\nnn.add(Dropout(0.3))\nnn.add(Dense(4, kernel_initializer=keras.initializers.glorot_uniform, activation='tanh'))\nnn.add(Flatten())\nnn.add(Dense(1, activation='sigmoid'))\n\nnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.keras.metrics.AUC(curve='ROC'))\nnn.fit(X_train, y_train, epochs=500, verbose=0)\nscores = nn.evaluate(X_train, y_train, verbose=0)\nprint(\"\\nAccuracy on train data : {}\".format(scores[1]))\nscores = nn.evaluate(X_test, y_test, verbose=0)\nnn_pred = np.where(nn.predict(X_test) > 0.5, 1, 0)\nprint(\"Accuracy on test data : {}\".format(roc_auc_score(nn_pred, y_test)))","63572f1a":"First of all, let's check if we have any missing data.","0ac8cc17":"What can be seen, is the fact that sometimes cabin numbers contain even letters, so we won't be able to fill missing values with mean or median, for example. And, as we found out before, the missing values are the giant part of all the values for this column. So, we can actually drop it.","b41f18f0":"### Features visualizing and analysis","fbb8d164":"Let's use log to make Age and Fare columns distribution more similar to normal.","3a83622e":"Let's take a look at survived passengers. (Blue histogram)","55a4647a":"### LogisticRegression","37409f9a":"As we can see, Survive correlates mostly on Sex_female, Sex_male, Pclass and Fare.","05c97e12":"This plot don't really give us usefull information.","63ae61fa":"As can be seen from the results, the most important features are : Pclass, Age, Sex and Fare.\n","bf188768":"Let's visualize SibSp column (number of siblings\/spouses aboard the Titanic).","b578fd7f":"Now, let's check if any features are correlating between each other.","7df82545":"We will use RandomForestClassifier. bootstrap is True, as the size of data isn't really big. We will use ROC-AUC metrics to score our models. The advantage of ROC-AUC is, that it doesn't depend on data's balance.","31a606cd":"What can be seen from the plot, is that the biggest part of passengers are the 3rd class, which was the cheapest. The amount of 1st and 2nd class passengers is almost the same. (Red histogram)","7506f1ef":"And let's fill test data Fare missing value with mean.","8fcffa02":"Looks like neural networks' scores are pretty fine, but still worse then KNeighborsClassifier's","bf7aade9":"As we can see LogisticRegression makes better results, than RandomForest and RidgeClassifier, but it is still worse than KNeighbors.","1065b84f":"Seems like Fare, Age and Sex have the highest importance for our model.","e6be5f57":"What about Age column, we can simply fill it with the rounded mean.","a7cab73e":"#### Fare","4e2b9de8":"### KNeighborsClassifier","3151af73":"People with SibSp > 4 didn't survive at all. (Blue boxplot)","d57c3a44":"#### Age","464e1eef":"### Distribution","d1880836":"### Result","74fa0a89":"#### SibSp","7738a16c":"### Missing data","87c17d18":"Our classes aren't really well-balanced, so the accuracy metric won't show us the real prediction accuracy. That's why we will use AUC-ROC metrics.","c344ce00":"for col in n_data:\n    print(col, n_data[col].dtypes)","abe9857c":"Let's visualize Pclass column.","6eb1019e":"### Data encoding","dd8c5eca":"As we can see there are only several passengers, who are older than 70. (Red boxplot)","a908c0d9":"Let's visualize Parch column (number of parents \/ children aboard the Titanic).","a443ae9e":"What about survived passengers, the situation here is much different, as the 2\/3 of all the survived passengers are women. We will use this fact later. (Blue histogram)","938cf2c8":"### Correlation Matrix","6259eed8":"This operation can help us, because models can make better prediction, when data is normaly distributed.","93c5b24c":"As can be seen from the plot, there were much more men on the board. (Red histogram)","74791538":"#### Parch","eaa3f154":"#### Sex","ed7fb257":"Situation changes dramatically. As we can see, the highest amount of passengers are from the 1st. ","6c39fb88":"#### Pclass","7a4fc90b":"### RidgeClassifier","287b6ce5":"These plots don't really give us usefull information.","d3d7c4c0":"### RandomForestClssifier","8e7b5afb":"What can be seen from the boxplot, is that the amount of people with SibSp >= 3 is really small. (Red boxplot)","24f9616f":"Seems like all the data left is numeric and categorical. Let's check if it is true.","99cb9c50":"We have got only several people with Parch > 0 (Red\/first boxplot)","bd872226":"We will also create new dataframe with only survived passengers data, to compare it with the data of both survived and dead passengers later.","667e28f7":"What can be seen from the distplot, is that more children survived, than died. 20-40 year old adults were also more likely to survive. But for the people, who are older than 60, chances to survive were really small.\nFor other people there is no obvious age-related dependencies.","adec1d38":"Moreover, survivors are:                                                                                                 almost 60% of 1st class passengers;\nthe half of the 2nd class passengers;\nsmall part of 3ed class passengers.","859b8cc6":"Finding out which features are important.","1f9c64b6":"We will use GridSearchCV to find the best parameters for KNeighborsClassifier.","d4b528f5":"Now, let's also drop PassengerId, Name and Ticker columns, as they won't make any sense on our future predictions.","b67c83a7":"#### Embarked","2374db84":"Let's visualize Fare column","1feb400f":"There are only 2 values missing in Embarked column for train data. Let's fill them with the mode.","bfd0ca2d":"Only one person from the age 65-80 survived. (Blue boxplot)","5e3df286":"Let's check Sex column plot. From now, blue bars will display survived passengers data and red bars will display all the passengers.","c4dac9d6":"We will use GridSearchCV to find the best parameters for LogisticRegression.","641aba55":"First of all, as our task is to classify, we should check, if both Survived classes are balanced.","d65343cd":"There are any obvious dependencies on histogram, so we will move on.","f82c3da2":"Let's check out if it worked fine.","f6308d41":"To encode Sex and Embarked columns we will use OneHotEncoder.","0a64f760":"Let's visualize Embarked - the last column.","9a2d2d02":"Import all models and functions","be1ba552":"Let's take a look at our features and theirs distribution. We will visualize it on a plot.","9e073406":"We will use GridSearchCV to find the best parameters for Ridge.","d17e1d47":"There are any obvious dependencies on histogram, so we will move on.","bcf52d7a":"That's fine, we have dealt with missing data!","182c642f":"The best acuracy is given by KNeighborsClassifier. ","12977b0d":"We have got only 3 embarked values : Q, S and C. Let's drop Embarked_0    S\\ndtype: object column as it doesn't make any sense.","aafcf604":"People with 4 and 6 Parch didn't survive at all. (Blue boxplot)","06d54530":"## Importing libraries and data","65390755":"Almost the same features are important for both RandomForestClassifier and RidgeClassifier, but RidgeClassifier also has Pclass feature as important one.","675aa30c":"Let's also take a look at important features.","0f9a87b0":"The last model will be neural network. We will use keras to build it.","ebd18b80":"Let's visualize Age column.","44f15b6e":"Seems like the scores are actually better than RandomForestClassifier's","433adec8":"### Neural Network","f13ecac7":"Now, before training our models, we should split data, to be able to find the best hyperparameters for our models.","90be3baf":"## Exploratory Data Analysis","e9e7e2d8":"That means, that 1rd class passengers were more likely to survive.","e463d80f":"### Splitting data","28906c34":"# Model choosing and training","a54c8a5c":"687 of 891 Cabin are missing. Let's check all the unique values of this column."}}