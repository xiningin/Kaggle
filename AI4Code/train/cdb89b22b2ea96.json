{"cell_type":{"bcb52222":"code","123c9340":"code","6da946e3":"code","e715354d":"code","ca866bac":"code","96f14457":"code","4a371231":"code","71a0fdee":"code","bac21278":"code","e1d23d7e":"code","fc58e0be":"code","4e9c7903":"code","9cfbbcbf":"code","df6d3326":"code","3939d331":"code","7fe1bc54":"code","2608fc10":"code","35eb1f33":"code","a0591650":"code","0b3d69b0":"code","7d47b384":"code","6bd78eb2":"code","771fd22d":"code","67e1c17b":"code","dfbefb5f":"code","a65f641d":"code","587b4a0f":"code","d2e13b7a":"code","1af46250":"markdown","137f3f40":"markdown","d91a5c7e":"markdown","73dd38ca":"markdown","a9b2ea44":"markdown","1b2e081a":"markdown","0d71786d":"markdown","21b79a55":"markdown","f8496cb1":"markdown","5626d77c":"markdown","cff19f0d":"markdown","3298174a":"markdown","4cdeb337":"markdown","ef69f23c":"markdown","3dc20a20":"markdown","361c83a3":"markdown","a93d2716":"markdown","258297e3":"markdown","357547c3":"markdown","05d8e37b":"markdown","a3bb6cfd":"markdown","641d5f9b":"markdown"},"source":{"bcb52222":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","123c9340":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","6da946e3":"train['set'], test['set'] = 'train', 'test'\ncombined = pd.concat([train, test])","e715354d":"combined.isnull().sum()","ca866bac":"# combined = combined.drop(['Cabin', 'Embarked'], axis=1)","96f14457":"pclass = combined.loc[combined.Fare.isnull(), 'Pclass'].values[0]\nmedian_fare = combined.loc[combined.Pclass== pclass, 'Fare'].median()\ncombined.loc[combined.Fare.isnull(), 'Fare'] = median_fare","4a371231":"# Select everything before the . as title\ncombined['Title'] = combined['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ncombined['Title'].unique()","71a0fdee":"title_reduction = {'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', \n                   'Master': 'Master', 'Don': 'Mr', 'Rev': 'Rev',\n                   'Dr': 'Dr', 'Mme': 'Miss', 'Ms': 'Miss',\n                   'Major': 'Mr', 'Lady': 'Mrs', 'Sir': 'Mr',\n                   'Mlle': 'Miss', 'Col': 'Mr', 'Capt': 'Mr',\n                   'Countess': 'Mrs','Jonkheer': 'Mr',\n                   'Dona': 'Mrs'}\ncombined['Title'] = combined['Title'].map(title_reduction)\ncombined['Title'].unique()","bac21278":"for title, age in combined.groupby('Title')['Age'].median().iteritems():\n    print(title, age)\n    combined.loc[(combined['Title']==title) & (combined['Age'].isnull()), 'Age'] = age","e1d23d7e":"combined.isnull().sum()","fc58e0be":"def other_family_members_survived(dataset, label='family_survival'):\n    \"\"\"\n    Check if other family members survived\n      -> 0 other did not survive\n      -> 1 at least one other family member survived\n      -> 0.5 unknown if other members survived or person was alone\n    \n    Parameters\n    ----------\n    dataset : DataFrame\n      The sub-dataframe containing the family\n    \"\"\"\n    ds = dataset.copy()\n    if len(dataset) == 1:\n        ds[label] = 0.5\n        return ds\n    result = []\n    for ix, row in dataset.iterrows():\n        survived_fraction = dataset.drop(ix)['Survived'].mean()\n        if np.isnan(survived_fraction):\n            result.append(0.5)\n        elif survived_fraction == 0:\n            result.append(0)\n        else:\n            result.append(1)\n    ds[label] = result\n    return ds","4e9c7903":"combined['surname'] = combined['Name'].apply(lambda x: x.split(\",\")[0])\ncombined = combined.groupby(['surname', 'Fare']).apply(other_family_members_survived).reset_index(drop=True)","9cfbbcbf":"combined = combined.groupby(['Ticket']).apply(lambda x: other_family_members_survived(x, label='family_survival_ticket')).reset_index(drop=True)\ncombined.loc[combined['family_survival'] == 0.5, 'family_survival'] = combined.loc[combined['family_survival'] == 0.5, 'family_survival_ticket']","df6d3326":"combined['family_size'] = combined['Parch'] + combined['SibSp']","3939d331":"combined['Sex'] = LabelEncoder().fit_transform(combined['Sex'])","7fe1bc54":"combined.loc[:, 'Age'] = pd.qcut(combined['Age'], 4, labels=False)\ncombined.loc[:, 'Fare'] = pd.qcut(combined['Fare'], 5, labels=False)","2608fc10":"selected = ['Pclass', 'Sex', 'Age', 'Fare', 'family_size', 'family_survival']\nscaler  = StandardScaler()\nscaler.fit(combined[selected])\ncombined[selected] = scaler.transform(combined[selected])","35eb1f33":"combined.to_parquet('titanic_family_survivabillity.parquet', index=False)","a0591650":"train = combined.loc[combined['set'] == 'train'].drop('set', axis=1).reset_index(drop=True)\ntest = combined.loc[combined['set'] == 'test'].drop(['set', 'Survived'], axis=1).reset_index(drop=True)","0b3d69b0":"def euclidean_distance(vector1, vector2):\n    return np.sqrt(np.sum((vector1 - vector2)**2))\n\n# test function\nvec1 = np.array([3, 0])\nvec2 = np.array([0, 4])\n\n# this is the 3:4:5 triangle and therefore, it should return 5 (Long live Pythagoras)\neuclidean_distance(vec1, vec2)","7d47b384":"# A first implementation\ndef get_nearest_neighbor(vector, dataset, number_of_neighbors=1, ignore_cols=['Survived']):\n    distances = []\n    for ix, row in dataset.loc[:, ~dataset.columns.isin(ignore_cols)].iterrows():\n        distance = euclidean_distance(row, vector)\n        distances.append((distance, ix))\n    indices = [x[1] for x in sorted(distances, key=lambda x: x[0])]\n    neighbors = dataset.loc[indices[:number_of_neighbors]]\n    return neighbors\n\n# Another implementation using Pandas\ndef get_nearest_neighbor(vector, dataset, number_of_vectors=1, ignore_cols=['Survived'], not_count_duplicates=False):\n    ds = dataset.copy()\n    ds['distance'] = ds.loc[:, ~ds.columns.isin(ignore_cols)].apply(\n        lambda x: euclidean_distance(x, vector), axis=1)\n    if not_count_duplicates:\n        distances = sorted(ds.distance.unique())[:number_of_vectors]\n        return ds.loc[ds.distance <= max(distances)].drop('distance', axis=1)\n    return ds.sort_values('distance', ascending=True).head(number_of_vectors).drop('distance', axis=1)\n        \n# test function\ndataset = pd.DataFrame([\n    {'a': 1, 'b': 1, 'Survived': 1},\n    {'a': 2, 'b': 2, 'Survived': 1},\n    {'a': 3, 'b': 3, 'Survived': 0},\n    {'a': 4, 'b': 4, 'Survived': 0},\n    {'a': 5, 'b': 5, 'Survived': 0},\n])\nvector = pd.Series({'a': 2.5, 'b': 2.5})\n\n# should be (2,2) and (3,3) (if keeping track of duplicates)\nget_nearest_neighbor(vector, dataset)","6bd78eb2":"def predict(vector, dataset, number_of_neighbors=1, y='Survived'):\n    neighbors = get_nearest_neighbor(vector, dataset, number_of_neighbors)\n    return round(neighbors[y].mean())\n\n# test function\nprint(predict(vector, dataset))\nprint(predict(pd.Series({'a': 4.5, 'b': 4.5}), dataset))","771fd22d":"def predict_dataset(dataset, number_of_neighbors=1):\n    ds = dataset.copy()\n    def predict_row(vector, dataset):\n        subset = dataset.loc[~(dataset.index==vector.name)]\n        if vector.name % 100 == 0:\n            print(vector.name)\n        return int(predict(vector, subset, number_of_neighbors))\n\n    ds['predicted'] = ds.loc[:, ds.columns.isin(selected)].apply(\n        lambda x: predict_row(x, ds), axis=1)\n    \n    return ds\n\nds = predict_dataset(train, number_of_neighbors=10)\n\nprint('Accuracy:', sum(ds['Survived'] == ds['predicted']) \/ len(ds))\n","67e1c17b":"def predict_testset(test_dataset, train_dataset, number_of_neighbors=1):\n    ds = test_dataset.copy()\n    select = selected + ['Survived']\n    \n    def predict_row(vector, dataset):\n        if vector.name % 100 == 0:\n            print(vector.name)\n        return int(predict(vector, dataset[select], number_of_neighbors))\n\n    ds['Survived'] = ds.loc[:, ds.columns.isin(selected)].apply(\n        lambda x: predict_row(x, train_dataset), axis=1)\n    \n    return ds","dfbefb5f":"final_test = predict_testset(test, train, number_of_neighbors=10)","a65f641d":"result = final_test[['PassengerId', 'Survived']].copy()","587b4a0f":"result","d2e13b7a":"result.to_csv('results.csv', index=False)","1af46250":"Using our distance function we will now find the closest match in our dataset, when providing a vector.","137f3f40":"KNN needs numerical features therefore, we will convert them to numbers. In a general sense, binary categorical data can work. For larger categorical groups, it only makes sense when the numerical values itself have meaning. For example, for class levels, the difference between first class and third class actually mean something. On the other hand, if we would convert Embarked to a number, there is no meaning in the difference between embarked 1 and embarked 2.","d91a5c7e":"#### Only one Fare price is missing. fill it with the median of his Pclass:","73dd38ca":"We find a accuracy of 83.5% on the training set. Lets now make our test set predictions.","a9b2ea44":"# Titanic: k-nearest-neighbor (KNN) from scratch [0.81339]\nIn This notebook we write our own k-nearest-neighbor (KNN) from scratch in Python and apply it to predict the survivability on the famous Titanic dataset.\n\nWhile the score also comes from the feature engineering, it is still amazing that using such a simple algorithm we can get such a high score.\n\n### Content\n1. Data preparation\n2. Create additional features\n3. Algorithm creation\n4. Test algorithm\n5. Create test predictions","1b2e081a":"# 1. Data preparation","0d71786d":"To test, we select on row from the set and use KNN to find the best candidate. Of course, we will remove that row from the dataset, or we would find a distance of zero as the identical row is in there.","21b79a55":"#### We will not be using Embarked and Cabin for this experiment.","f8496cb1":"## 5. create prediction for testset","5626d77c":"Missing data on families can also be extracted from Tickets. Same ticket orders have the same ticket number.","cff19f0d":"Now that we have a function to list the nearest neighbors, we need to select the most dominant class of those neighbors to select as our prediction.","3298174a":"#### Select only coluns we will use and scale them","4cdeb337":"# 3. Algorithm\nKNN works by finding, as the name suggests, the nearest neighbor. It assumes that classes share similar properties. To compare to points, we have to see them as vectors. Each feature adds to a dimension. Would we only have a single feature it would be the numeric distance between the two points. With two or more features, we need to do something smarter. To create a single number to express the similarity (or distance) we need to agegrate all the features (or dimensions). One way is to use the Euclidean distance, which is the root of the sum of squared distances between all features. There are many other ways to agegrate the feature distances, all with their strengths and weaknesses. For this example, we will use the Euclidean distance.","ef69f23c":"# 2. Create additional features","3dc20a20":"An interesting theory I saw here on Kaggle is that there are groups of people, who would help each other. This might affect the survivability.","361c83a3":"### Missing values\n#### First step is to fill missing values and drop columns we will not be using.","a93d2716":"#### Missing ages\nTo fill in the missing ages, we can do something more clever then just take the overal median age. The names contain titles of which some are linked to their age. Master is a younger boy (in general). Lets take the median of each age group.","258297e3":"First bin the Fare and Age. The groups are choosen arbitrarily:","357547c3":"Get family size from Parch and Sibsp","05d8e37b":"First lets import the data and generate one set to process all data at once","a3bb6cfd":"#### Now we split the sets back to train\/test","641d5f9b":"# 4. Test algorithm on Titanic dataset"}}