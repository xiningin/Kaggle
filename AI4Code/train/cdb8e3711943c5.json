{"cell_type":{"2b5e570c":"code","df14e275":"code","4b835338":"code","0fd8f7f4":"code","52d6dfe5":"code","74baf501":"code","a54a3239":"code","0f4c7da8":"code","49e4532b":"markdown","8408022f":"markdown","06bab49f":"markdown","a0289577":"markdown","a99bbb8d":"markdown","9ff0a433":"markdown","580fbf8e":"markdown","2d41aa51":"markdown","1e6954d0":"markdown","08b44c2e":"markdown","ebef1865":"markdown","288125f0":"markdown","46143cb5":"markdown","960b7346":"markdown","106bd8fa":"markdown"},"source":{"2b5e570c":"import numpy as np, pandas as pd, os\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    \n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('QDA scores CV =',round(auc,5))","df14e275":"# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n        preds[test2.index] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n       \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","4b835338":"from sklearn import svm, neighbors, linear_model, neural_network\nfrom sklearn.svm import NuSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.covariance import ShrunkCovariance\nfrom sklearn.mixture import GaussianMixture\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\noof_QDA = np.zeros(len(train))\npreds_QDA = np.zeros(len(test))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nfor i in tqdm_notebook(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=25, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof_QDA[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_QDA[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof_QDA)\nprint('QDA scores CV =',round(auc,8))\nsub_QDA = pd.read_csv('..\/input\/sample_submission.csv')\nsub_QDA['target'] = preds_QDA\nsub_QDA.to_csv('submission_QDA.csv',index=False)\noof_preds_QDA = train[['id', 'target']].copy()\noof_preds_QDA['target'] = oof_QDA\noof_preds_QDA.to_csv('oof_preds_QDA.csv', index = False)","0fd8f7f4":"test['target'] = preds_QDA\noof_QDA_PL = np.zeros(len(train))\npreds_QDA_PL = np.zeros(len(test))\npreds_QDA_PL_label = np.zeros(len(test))\nn = 240\nfor k in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k].sort_values(by = 'target')\n    idx2 = test2.index\n    # ADD PSEUDO LABELED DATA\n    #Use Private as Pseudo Label to see LB\n    test2p = pd.concat([test2[: n], test2[-n: ]], axis = 0)\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    # STRATIFIED K FOLD\n    preds_QDA_PL_label_slice = preds_QDA_PL_label[list(idx2[: n]) + list(idx2[-n: ])]\n    skf = StratifiedKFold(n_splits=25, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        test_index4 = test_index[ test_index>=len(train3) ]\n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_QDA_PL[idx1[test_index3]] = clf.predict_proba(train3p[test_index3])[:,1]\n        if(len(test_index4) > 0):\n            preds_QDA_PL_label_slice[test_index4 - len(train3)] = clf.predict_proba(train3p[test_index4])[:, 1]\n        preds_QDA_PL[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    preds_QDA_PL_label[list(idx2[: n]) + list(idx2[-n: ])] = preds_QDA_PL_label_slice\npreds_QDA_PL[preds_QDA_PL_label != 0] = preds_QDA_PL_label[preds_QDA_PL_label != 0]\n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof_QDA_PL)\nprint('Pseudo Labeled QDA scores CV =',round(auc,8))\nsub_QDA_PL = pd.read_csv('..\/input\/sample_submission.csv')\nsub_QDA_PL['target'] = preds_QDA_PL\nsub_QDA_PL.to_csv('submission_QDA_PL.csv',index=False)\noof_preds_QDA_PL = train[['id', 'target']].copy()\noof_preds_QDA_PL['target'] = oof_QDA_PL\noof_preds_QDA_PL.to_csv('oof_preds_QDA_PL.csv', index = False)","52d6dfe5":"def get_mean_cov(x,y):\n    model = ShrunkCovariance()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps\noof_GMM = np.zeros(len(train)) \npreds_GMM = np.zeros(len(test))\nfor i in tqdm_notebook(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=25, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        # MODEL AND PREDICT WITH QDA\n        ms, ps = get_mean_cov(train3[train_index,:],train2.loc[train_index]['target'].values)\n        gm = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001,reg_covar=0.001, max_iter=100, n_init=1,means_init=ms, precisions_init=ps)\n        gm.fit(np.concatenate([train3[train_index,:],test3],axis = 0))\n        oof_GMM[idx1[test_index]] = gm.predict_proba(train3[test_index,:])[:,0]\n        preds_GMM[idx2] += gm.predict_proba(test3)[:,0] \/ skf.n_splits\nauc = roc_auc_score(train['target'],oof_GMM)\nprint('GMM scores CV =',round(auc,8))\n\nsub_GMM = pd.read_csv('..\/input\/sample_submission.csv')\nsub_GMM['target'] = preds_GMM\nsub_GMM.to_csv('submission_GMM.csv', index = False)\noof_preds_GMM = train[['id', 'target']].copy()\noof_preds_GMM['target'] = oof_GMM\noof_preds_GMM.to_csv('oof_preds_GMM.csv', index = False)","74baf501":"test['target'] = preds_QDA\noof_NuSVC = np.zeros(len(train)) \npreds_NuSVC = np.zeros(len(test))\npreds_NuSVC_label = np.zeros(len(test))\nn = 220\nfor i in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i].sort_values(by = 'target')\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    pca = PCA(svd_solver='full',n_components='mle')\n    scaler = StandardScaler()\n    data2 = scaler.fit_transform(pca.fit_transform(data[cols]))\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n    #Pseudo_Label\n#Use Private as Pseudo Label to see LB\n    test2p = pd.concat([test2[: n], test2[-n: ]], axis = 0)\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0\n    train2p = pd.concat([train2, test2p], axis = 0)\n    train2p.reset_index(drop=True,inplace=True)\n    test5 = scaler.transform(pca.transform(test2p[cols]))\n    train3p = np.concatenate([train3, test5])\n    \n    # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n    preds_NuSVC_label_slice = preds_NuSVC_label[list(idx2[: n]) + list(idx2[-n: ])]\n    skf = StratifiedKFold(n_splits=25, random_state=42, shuffle = True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[test_index < len(train3)]\n        test_index4 = test_index[test_index >= len(train3)]\n        clf = NuSVC(probability=True, kernel='poly', degree=4, gamma='auto', random_state=4, nu=0.75, coef0=0.053)\n        clf.fit(train3p[train_index],train2p.loc[train_index]['target'])\n        \n        oof_NuSVC[idx1[test_index3]] = clf.predict_proba(train3p[test_index3])[:,1]\n        if(len(test_index4) > 0):\n            preds_NuSVC_label_slice[test_index4 - len(train3)] = clf.predict_proba(train3p[test_index4])[:, 1]\n        preds_NuSVC[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    preds_NuSVC_label[list(idx2[: n]) + list(idx2[-n: ])] = preds_NuSVC_label_slice\npreds_NuSVC[preds_NuSVC_label != 0] = preds_NuSVC_label[preds_NuSVC_label != 0]\n\nauc = roc_auc_score(train['target'],oof_NuSVC)\nprint('Pseudo Labeled NuSVC scores CV =',round(auc,8))\nsub_NuSVC = pd.read_csv('..\/input\/sample_submission.csv')\nsub_NuSVC['target'] = preds_NuSVC\nsub_NuSVC.to_csv('submission_NuSVC.csv', index = False)\noof_preds_NuSVC = train[['id', 'target']].copy()\noof_preds_NuSVC['target'] = oof_NuSVC\noof_preds_NuSVC.to_csv('oof_preds_NuSVC.csv', index = False)","a54a3239":"test[\"target\"] = preds_QDA\noof_LS = np.zeros(len(train)) \npreds_LS = np.zeros(len(test))\npreds_LS_label = np.zeros(len(test))\nn = 230\nfor k in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k].sort_values(by = 'target')\n    idx2 = test2.index\n    # ADD PSEUDO LABELED DATA\n    test2p = pd.concat([test2[: n], test2[-n: ]], axis = 0)\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    # STRATIFIED K FOLD\n    preds_LS_label_slice = preds_LS_label[list(idx2[: n]) + list(idx2[-n: ])]\n    skf = StratifiedKFold(n_splits = 25, random_state = 42, shuffle = True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        test_index4 = test_index[ test_index>=len(train3) ]\n        # MODEL AND PREDICT WITH QDA\n        clf = LabelSpreading(gamma = 0.0125, kernel = 'rbf', max_iter = 10,alpha = 0.4,tol = 0.001)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_LS[idx1[test_index3]] = clf.predict_proba(train3p[test_index3])[:,1]\n        if(len(test_index4) > 0):\n            preds_LS_label_slice[test_index4 - len(train3)] = clf.predict_proba(train3p[test_index4])[:, 1]\n        preds_LS[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    preds_LS_label[list(idx2[: n]) + list(idx2[-n: ])] = preds_LS_label_slice\npreds_LS[preds_LS_label != 0] = preds_LS_label[preds_LS_label != 0]\n\nauc = roc_auc_score(train['target'],oof_LS)\nprint('Pseudo Labeled LS scores CV =',round(auc,8))\nsub_LS = pd.read_csv('..\/input\/sample_submission.csv')\nsub_LS['target'] = preds_LS\nsub_LS.to_csv('submission_LS.csv', index = False)\noof_preds_LS = train[['id', 'target']].copy()\noof_preds_LS['target'] = oof_LS\noof_preds_LS.to_csv('oof_preds_LS.csv', index = False)","0f4c7da8":"oof_preds = pd.concat([oof_preds_QDA_PL['target'], oof_preds_NuSVC['target'], oof_preds_GMM['target'], oof_preds_LS['target']], axis = 1)\nsub_preds = pd.concat([sub_QDA_PL['target'], sub_NuSVC['target'], sub_GMM['target'], sub_LS['target']], axis = 1)\n\noof_stacking = np.zeros(len(train)) \npreds_stacking = np.zeros(len(test))\nskf = StratifiedKFold(n_splits=25, random_state=42, shuffle = True)\nfor train_index, test_index in skf.split(oof_preds, train['target']):\n    lrr = linear_model.LogisticRegression()\n    lrr.fit(oof_preds.loc[train_index], train.loc[train_index, 'target'])\n    oof_stacking[test_index] = lrr.predict_proba(oof_preds.loc[test_index,:])[:,1]\n    preds_stacking += lrr.predict_proba(sub_preds)[:,1] \/ skf.n_splits\nauc = roc_auc_score(train['target'],oof_stacking)\nprint('Stacking scores CV =',round(auc,8))\n\nsub_stacking = pd.read_csv('..\/input\/sample_submission.csv')\nsub_stacking['target'] = preds_stacking\nsub_stacking.to_csv('submission_stacking.csv', index = False)\noof_preds_stacking = train[['id', 'target']].copy()\noof_preds_stacking['target'] = oof_stacking\noof_preds_stacking.to_csv('oof_preds_stacking.csv', index = False)","49e4532b":"Stacking is working with pseudo labeling now.\n\nIn the end, thanks for Chirs, Roman, Vlad, Dieter, Bojan and other contributors for this competition. Thank you all for reading here and enduring my poor English V\u25cf\u1d25\u25cfV.","8408022f":"## Stacking","06bab49f":"## Second model (QDA)","a0289577":"## Second model (Label Spreading)","a99bbb8d":"## First model (QDA)","9ff0a433":"## Second model (QDA)","580fbf8e":"# Original method\nLet's take a look at the original method. The steps of pseudo labeling are as follows: build first model and add pseudo label to the second model. Just like the code from [Chris' kernel](https:\/\/www.kaggle.com\/cdeotte\/pseudo-labeling-qda-0-969).","2d41aa51":"The game is over and people are talking about ways to reach 0.974+, but I still remember my efforts on stacking before I discovered the right use of GMM as a magic. So, this kernel talks about my exploration in stacking and pseudo labeling.\n\nIn the middle of the competition, Roman introduced pseudo labeling and Chris explained it later, which leaded people to 0.969+. However, 0.970 is an unbreakable barrier for most people because stacking is hard to work after using pseudo labeling. I thought for days and found a breakthrough.","1e6954d0":"# Why stacking doesn't work?\nThe key point is that **stacking improves CV, but LB does not**. We use train set to calculate CV and test set to calculate LB, so there must be some differences between train set and test set.\n\nThen I found that **all public kernels I see about pseudo labeling were training part of the test set and predicting themself! ** ","08b44c2e":"In the step 2, the second model(still QDA) uses train3p and train2p['target'] to fit, after that uses test3 to predict. Obviously, there must be some points in train3p and test3 at the same time. Considering that train2p ['target'] comes from the first model's result, the consequence of doing so is that the prediction of these overlapping points in the second model will be very close to those of the first model.\n\nJust like, first model tells the second model that the target of these A B C D test points are 1 1 0 0. Then Whatever the second model is, it will predict the same points A B C D to similar results 0.99 0.99 0.01 0.01. Then stacking doesn't work because of the similarity of the predictions from different second models.","ebef1865":"## First model (QDA)","288125f0":"## Second model (GMM)","46143cb5":"# New method\nIn fact, here's what we want pseudo labeling to do. First model should tell the second model that the target of A B C points are 1 1 0, then the second model uses this additional information to predict D and only uses D's predicted results as submission. It's much like n-fold cross validation.\n\nUsing the new method, the predictions of different second models are no longer similar, and stacking is beginning to work. Here's my code, the key is the change of second model in prediction.","960b7346":"Then build more second models for stacking.","106bd8fa":"## Second model (NuSVC)"}}