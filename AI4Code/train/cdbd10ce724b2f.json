{"cell_type":{"0cfb7549":"code","948862a2":"code","1b228571":"code","ba2d2540":"code","b0188f0d":"code","b3672972":"code","2cb06f0e":"code","e07f20b8":"code","db2bc652":"code","098f3374":"code","d67fe52b":"code","0f3c8ad3":"code","a4bb6217":"code","7b0d6ea1":"code","a1ef856f":"code","1091862c":"code","5f6abecf":"markdown"},"source":{"0cfb7549":"# Imports\nimport os\nimport pickle\nimport time\n\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import (\n    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, \n    StackingClassifier\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import (\n    roc_auc_score, accuracy_score, f1_score,\n    precision_score, recall_score,\n    mean_absolute_error, mean_squared_error,r2_score,\n    log_loss\n)\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","948862a2":"# Utilities\ncombine = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n\n\nclass Timer():\n    \"\"\"Callable that maintains a list of timestamps.\n\n    Attributes:\n        times: A list of times the class instance was called, including\n            initialization.\n        total_time: Total accumulated time.\n    \"\"\"\n    def __init__(self):\n        self.times = [time.perf_counter()]\n        self.total_time = 0.0\n\n    def __call__(self, include_in_total: bool = True):\n        self.times.append(time.perf_counter())\n        delta_t = self.times[-1] - self.times[-2]\n        if include_in_total:\n            self.total_time += delta_t\n        return delta_t\n\n\nclass LowCountGroupingTransformer(BaseEstimator):\n    def __init__(self, min_obs=10, grouped_value='<OTHER>'):\n        super().__init__()\n        self.min_obs = min_obs\n        self.grouped_value = grouped_value\n        self.is_fitted = False\n    \n    def _fit_column(self, x):\n        # THis is a pickling problem\n        value_map = defaultdict(lambda: self.grouped_value)\n        values = x.value_counts()\n        for x in values.index[values >= self.min_obs]:\n            value_map[x] = x\n        return value_map\n    \n    def _transform_column(self, x, i):\n        return x.map(self.value_maps[i], na_action='ignore')\n    \n    def fit(self, X, y=None):\n        self.value_maps = [None] * X.shape[1]\n        self.num_values = [None] * X.shape[1]\n        for i, col in enumerate(X.columns):\n            self.value_maps[i] = self._fit_column(X[col])\n            self.num_values[i] = len(self.value_maps[i])\n        self.is_fitted = True\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        for i, col in enumerate(X.columns):\n            X.loc[:, col] = self._transform_column(X[col], i)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)\n\n\nclass LabelEncoder(BaseEstimator):\n    def __init__(self):\n        super().__init__()\n        self.is_fitted = False\n    \n    def _fit_column(self, x):\n        value_map = defaultdict(int)\n        values = x.value_counts()\n        for idx, x in enumerate(values.index):\n            value_map[x] = idx\n        return value_map\n    \n    def _transform_column(self, x, i):\n        return x.map(self.value_maps[i], na_action='ignore').fillna(0)\n    \n    def fit(self, X, y=None):\n        self.value_maps = [None] * X.shape[1]\n        self.max_values = [None] * X.shape[1]\n        X = X.copy()\n        X = pd.DataFrame(X)\n        for i, col in enumerate(X.columns):\n            self.value_maps[i] = self._fit_column(X[col])\n            self.max_values[i] = X[col].max()\n        self.is_fitted = True\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        X = pd.DataFrame(X)\n        for i, col in enumerate(X.columns):\n            X.loc[:, col] = self._transform_column(X[col], i)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X)\n        return self.transform(X)\n\n\ndef calc_metrics_binary(probs, targets, threshold=0.5):\n    pred_class = (probs > threshold).astype(np.int32)\n    return {\n        'n': len(probs),\n        'auc': roc_auc_score(y_score=probs, y_true=targets),\n        'accuracy': accuracy_score(y_pred=pred_class, y_true=targets),\n        'precision': precision_score(y_pred=pred_class, y_true=targets),\n        'recall': recall_score(y_pred=pred_class, y_true=targets),\n        'f1': f1_score(y_pred=pred_class, y_true=targets),\n        'log_loss': log_loss(y_pred=probs, y_true=targets)\n    }","1b228571":"# Initialize data\ndata_in_base_path = '\/kaggle\/input\/titanic'\nsplits = ('train', 'test')\ndata = {\n    split: pd.read_csv(f'{data_in_base_path}\/{split}.csv')\n    for split in splits\n}\ntest_cols = data['test'].columns\nsummaries = {\n    'nunique': lambda x: x.nunique(),\n    'dtype': lambda x: x.dtypes,\n    'missing': lambda x: x.isnull().sum(),\n}\npd.DataFrame({\n    f'{smry}_{split}': summaries[smry](data[split][test_cols])\n    for split in splits for smry in summaries.keys()\n})","ba2d2540":"# Create separate numeric and categorical versions of maybe-categorical columns\n# Convert all categorical columns to strin\ncategorical_col_names = ['Pclass', 'Sex', 'Embarked']\nmaybe_cat_col_names = []  # 'Age', 'SibSp', 'Parch'\nnumeric_col_names = ['Fare', 'Age', 'SibSp', 'Parch']\nunused_col_names = ['PassengerId', 'Name', 'Ticket', 'Cabin']\nlabel_col_name = 'Survived'\n\nfor split in splits:\n    for col_name in maybe_cat_col_names:\n        data[split][col_name + '_cat'] = data[split][col_name].astype(str)\n    for col_name in categorical_col_names:\n        data[split][col_name] = data[split][col_name].astype(str)\n\nnumeric_col_names = numeric_col_names + maybe_cat_col_names\ncategorical_col_names = categorical_col_names + [x + '_cat' for x in maybe_cat_col_names]","b0188f0d":"# Drop unused columns and train\/val split\nx_train = data['train'][numeric_col_names + categorical_col_names]\ny_train = data['train'][label_col_name]\nx_test = data['test'][numeric_col_names + categorical_col_names]\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=23131)\nx_train_full = data['train'][numeric_col_names + categorical_col_names]\ny_train_full = data['train'][label_col_name]\nprint(x_train.shape)\nprint(x_train_full.shape)\ndisplay(x_train.head())\ndisplay(x_train_full.head())","b3672972":"numeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n])\nlabel_encoder = Pipeline([\n    ('grouper', LowCountGroupingTransformer(min_obs=10, grouped_value='<UNK>')),\n    ('imputer', SimpleImputer(strategy='constant', fill_value='<UNK>')), \n    ('encoder', LabelEncoder()),\n])\npreprocessor_label_encoder = ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric_col_names),\n    ('categorical', label_encoder, categorical_col_names),\n])\nmodels = {\n#     'random_forest_entropy': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', RandomForestClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [3, 6, 10],\n#             'classifier__criterion': ['entropy'],\n#             'classifier__n_estimators': [500],\n#         },\n#         cv=3,\n#     ),\n#     'random_forest_gini': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', RandomForestClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [3, 6, 10],\n#             'classifier__criterion': ['gini'],\n#             'classifier__n_estimators': [500],\n#         },\n#         cv=3,\n#     ),\n#     'extra_trees_gini': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', ExtraTreesClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [6, 10, 15],\n#             'classifier__criterion': ['gini'],\n#             'classifier__n_estimators': [500],\n#         },\n#         cv=3,\n#     ),\n#     'extra_trees_entropy': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', ExtraTreesClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [6, 10, 15],\n#             'classifier__criterion': ['entropy'],\n#             'classifier__n_estimators': [500],\n#         },\n#         cv=3,\n#     ),\n#     'xgboost': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', XGBClassifier(verbosity=0, silent=True, use_label_encoder=False)),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [6, 10, 15],\n#             'classifier__learning_rate': [0.1, 0.3, 0.5],\n#         },\n#         cv=3,\n#     ),\n#     'gbm': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', GradientBoostingClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [2, 3, 6, 10],\n#         },\n#         cv=3,\n#     ),\n#     'lightgbm': GridSearchCV(\n#         Pipeline([\n#             ('preprocessor', preprocessor_label_encoder),\n#             ('classifier', LGBMClassifier()),\n#         ]),\n#         param_grid={\n#             'classifier__max_depth': [-1, 3, 6, 10],\n#             'classifier__learning_rate': [0.05, 0.1, 0.3],\n#             'classifier__n_estimators': [500]\n#         },\n#         cv=5,\n#     ),\n}","2cb06f0e":"# timer = Timer()\n# for model_id, model in models.items():\n#     models[model_id] = model.fit(x_train, y_train).best_estimator_\n#     print(f'{model_id} fit in {timer():.3f} seconds')","e07f20b8":"# results = []\n# for model_id, model in models.items():\n#     print(model_id)\n#     results.append(combine(\n#         {'model_id': model_id}, \n#         calc_metrics_binary(model.predict_proba(x_val)[:, 1], y_val)\n#     ))\n#     print(model['classifier'].get_params())\n\n# results = pd.DataFrame(results)\n# display(results)","db2bc652":"# catboost_params = CatBoostClassifier(\n#     verbose=False, cat_features=categorical_col_names, \n# ).grid_search(\n#     param_grid={\n#         'learning_rate': [0.075, 0.1, 0.125],\n#         'depth': [3, 4, 6],\n#         'l2_leaf_reg': [3, 5, 7]\n#     }, \n#     X=x_train, y=y_train, verbose=False\n# )\n\n# models['catboost'] = Pipeline([('classifier', CatBoostClassifier(\n#     verbose=False, cat_features=categorical_col_names, \n#     **catboost_params['params']\n# ))]).fit(x_train, y_train)","098f3374":"hyperparams = {\n    'catboost': {'learning_rate': 0.1, 'depth': 4, 'l2_leaf_reg': 5, 'verbose': False, 'cat_features': ['Pclass', 'Sex', 'Embarked']},\n    'random_forest_entropy': {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False},\n    'random_forest_gini': {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False},\n    'extra_trees_gini': {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False},\n    'extra_trees_entropy': {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False},\n    'xgboost': {'objective': 'binary:logistic', 'use_label_encoder': False, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'enable_categorical': False, 'gamma': 0, 'gpu_id': -1, 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 4, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': 0, 'silent': True},\n    'gbm': {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False},\n    'lightgbm': {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.05, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0},\n}\nmodels = {\n    'catboost': Pipeline([\n        ('classifier', CatBoostClassifier(**hyperparams['catboost'])),\n    ]),\n    'random_forest_entropy': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', RandomForestClassifier(**hyperparams['random_forest_entropy'])),\n    ]),\n    'random_forest_gini': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', RandomForestClassifier(**hyperparams['random_forest_gini'])),\n    ]),\n    'extra_trees_entropy': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', ExtraTreesClassifier(**hyperparams['extra_trees_entropy'])),\n    ]),\n    'extra_trees_gini': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', ExtraTreesClassifier(**hyperparams['extra_trees_gini'])),\n    ]),\n    'xgboost': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', XGBClassifier(**hyperparams['xgboost'])),\n    ]),\n    'gbm': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', GradientBoostingClassifier(**hyperparams['gbm'])),\n    ]),\n    'lightgbm': Pipeline([\n        ('preprocessor', preprocessor_label_encoder),\n        ('classifier', LGBMClassifier(**hyperparams['lightgbm'])),\n    ]),\n}","d67fe52b":"timer = Timer()\nresults = []\nfor model_id, model in models.items():\n    models[model_id] = model.fit(x_train, y_train)\n    print(f'{model_id} fit in {timer():.3f} seconds')\n    results.append(combine(\n        {'model_id': model_id}, \n        calc_metrics_binary(model.predict_proba(x_val)[:, 1], y_val)\n    ))\n\nresults = pd.DataFrame(results)\ndisplay(results)","0f3c8ad3":"model_stacked = StackingClassifier(\n    estimators=list(models.items()), \n    final_estimator=LogisticRegressionCV(Cs=50), \n    cv=3\n).fit(x_train, y_train)\nprint(model_stacked.final_estimator_.coef_)\nprint(model_stacked.final_estimator_.intercept_)\nprint(model_stacked.final_estimator_.C_)","a4bb6217":"models['stacked'] = model_stacked\nresults = pd.DataFrame([\n    combine(\n        {'model_id': model_id, 'split': 'val'}, \n        calc_metrics_binary(model.predict_proba(x_val)[:, 1], y_val)\n    )\n    for model_id, model in models.items()\n])\ndisplay(results)\n\nresults = pd.DataFrame([\n    combine(\n        {'model_id': model_id, 'split': 'train'}, \n        calc_metrics_binary(model.predict_proba(x_train)[:, 1], y_train)\n    )\n    for model_id, model in models.items()\n])\ndisplay(results)\n\n# with open('model.pickle', 'wb') as fout:\n#     pickle.dump(model_stacked_full, fout)","7b0d6ea1":"# from sklearn.calibration import CalibratedClassifierCV\n# model_stacked_calibrated = CalibratedClassifierCV(\n#     base_estimator=model_stacked, \n#     cv=3\n# ).fit(x_train, y_train)","a1ef856f":"model_stacked_full = model_stacked.fit(x_train_full, y_train_full)\nprint(model_stacked_full.final_estimator_.coef_)\nprint(model_stacked_full.final_estimator_.intercept_)\nprint(model_stacked_full.final_estimator_.C_)\n\noutput = pd.DataFrame({\n    'PassengerId': data['test']['PassengerId'], \n    'Survived': model_stacked_full.predict(x_test)\n})\n\noutput.to_csv('submission5.csv', index=False)\ndisplay(output)","1091862c":"print(data['train']['Survived'].value_counts())\nprint(output['Survived'].value_counts())","5f6abecf":"# Titanic + Kaggle Notebook Scratch\n\n### TODOs\n\n* Probability calibration\n* Feature engineering\n* Other models: Decision trees, Linear, KNN, NN\n    * How would we deal with something like a tabnet model in the stacking?\n* 2nd layer of stacking\n* Other types of top-layer models\n* Pseudo-labels"}}