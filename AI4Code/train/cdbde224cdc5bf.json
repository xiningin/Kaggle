{"cell_type":{"24a20514":"code","6fe830d3":"code","e3ea42b9":"code","ed8157ff":"code","6258e56e":"code","f863e653":"code","a1617de0":"code","aa0239df":"code","4af4af56":"code","3039ddff":"code","e93c24c2":"code","713118ee":"code","35bb1a72":"code","c97d9da6":"code","9c13b1ef":"code","d56427c4":"code","3daf3b5f":"code","ba71f51c":"code","feffb04a":"code","acc67384":"code","b365ff81":"code","fec3cb66":"code","5f92f795":"code","2252818a":"code","4b71fdeb":"code","aa626102":"code","6f15e353":"code","b57182e0":"code","b417d471":"code","835aaf85":"code","66e444fb":"code","6448ba0f":"code","b0d75cd3":"code","2b20ff74":"code","92b376ad":"code","09689c89":"code","08989155":"code","1c186541":"code","f744483e":"code","a2385c8b":"code","5e75e2b5":"code","0831678b":"code","b7bd7156":"code","76c0dbde":"code","80d82f34":"code","f0b51308":"code","2805f690":"markdown","213709b0":"markdown","22f3975f":"markdown","8b6e3578":"markdown"},"source":{"24a20514":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom tqdm.notebook import tqdm\nimport copy\nimport itertools\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport torch.optim as optim\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6fe830d3":"# IN_PATH = '\/kaggle\/input\/dow-jones-index-data-set\/dow_jones_index.data'\n# df = pd.read_csv(IN_PATH)\n# for col in ['open', 'high', 'low', 'close', 'next_weeks_open', 'next_weeks_close']:\n#     df[col] = df[col].apply(lambda w : float(w.replace('$', '')))\n    \n# df['date'] = pd.to_datetime(df['date'], format='%m\/%d\/%Y')\n\n# ibm_index = df[df['stock'] == 'IBM']\n\n# plt.figure(figsize=(14,9))\n# sns.lineplot(x=ibm_index['date'], y=ibm_index['open'])","e3ea42b9":"IN_PATH = '\/kaggle\/input\/dow-jones-20162021\/DJIA.csv'\ndf = pd.read_csv(IN_PATH)\ndf.rename( {'DATE' : 'date', 'DJIA' : 'y' }, axis=1, inplace=True )\n\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\ndf['y'] = df['y'].apply( lambda w : float(w) if w != '.' else np.nan )\n\ndf.head()","ed8157ff":"# fill NaN\nneighbors_smoothed = (df['y'].shift(1) + df['y'].shift(-1)) \/ 2\nind = pd.isna( df['y'] )\ndf.loc[ind, 'y'] = neighbors_smoothed[ind]","6258e56e":"plt.figure(figsize=(10, 7))\nt_df = df #df[df['date'].apply(lambda w : w.year >= 2019)]\nrolling_mean = t_df['y'].rolling(50, min_periods=1).mean()\nsns.lineplot( x = t_df['date'], y = t_df['y'] )\n\nplt.title('Daily Dow Jones Average', fontsize=20)\n#plt.xlabel('xlabel', fontsize=18)\n#plt.ylabel('ylabel', fontsize=16)\n# sns.lineplot( x = t_df['date'], y = rolling_mean )\nplt.savefig('index.png')\nplt.show()\n\n\nplt.figure(figsize=(14, 7))\nsns.lineplot( x = t_df['date'], y = t_df['y'] - rolling_mean )","f863e653":"train_size = int(df.shape[0] * 0.6)\ntest_size = int(df.shape[0] * 0.2)\nval_size = df.shape[0] - train_size - test_size\ntrain_size, val_size, test_size","a1617de0":"train_df = df.iloc[:train_size]\nval_df = df.iloc[train_size:train_size + val_size]\ntest_df = df.iloc[train_size + val_size:]","aa0239df":"class TimeChecker:\n    def __init__(self):\n        self.start_time = None\n        \n    def start(self):\n        self.start_time = datetime.now()\n        return self.start_time\n    \n    def end(self, mode='seconds'):\n        end_time = datetime.now()\n        delta = (end_time - self.start_time).seconds\n        self.start_time = end_time\n        \n        if mode == 'seconds':\n            return delta\n        if mode == 'minutes':\n            return delta \/ 60\n        \n        return delta","4af4af56":"from pathlib import Path\nfrom logging import getLogger, Formatter, FileHandler, StreamHandler, INFO, DEBUG\nfrom datetime import datetime\n\nVERSION = str(datetime.now())\n\ndef create_logger():\n    log_file = (\"log_{}.log\".format( VERSION ))\n\n    # logger\n    logger_ = getLogger(VERSION)\n    logger_.setLevel(DEBUG)\n\n    # formatter\n    fmr = Formatter(\"[%(levelname)s] %(asctime)s||\\t%(message)s\")\n\n    # file handler\n    fh = FileHandler(log_file)\n    fh.setLevel(DEBUG)\n    fh.setFormatter(fmr)\n\n    # stream handler\n    ch = StreamHandler()\n    ch.setLevel(INFO)\n    ch.setFormatter(fmr)\n\n    logger_.addHandler(fh)\n    logger_.addHandler(ch)\n\n\ndef get_logger(exp_version):\n    return getLogger(exp_version)\n\n\ncreate_logger()\nget_logger(VERSION).info(\"Start session...\")","3039ddff":"def mape(y_true, y_pred):\n    return np.absolute((y_true - y_pred) \/ y_true).mean()\n\ndef mae(y_true, y_pred):\n    return np.absolute(y_true - y_pred).mean()\n\ndef rmse(y_true, y_pred):\n    return np.sqrt( ((y_true - y_pred) ** 2).mean() )","e93c24c2":"def validate_with_point(df, Model, n_splits=3):\n    tss = TimeSeriesSplit(n_splits=n_splits)\n    mape_scores, mae_scores, rmse_scores = [], [], []\n    \n    for train_index, test_index in tqdm(tss.split(df)):\n        df_train, df_val = df.iloc[train_index], df.iloc[test_index]\n        \n        history = df_val['y'].values.tolist()\n        y_pred = []\n        y_true = df_val['y']\n        for t in y_true:\n            model = Model()\n            model = model.fit(df_train)\n            pred_val = model.predict_point(df_val)\n            history.append(t)\n            y_pred.append(pred_val)\n            \n            \n        \n        mape_score = mape(y_true, y_pred)\n        mae_score = mae(y_true, y_pred)\n        rmse_score = rmse(y_true, y_pred)\n\n        mape_scores.append(mape_score)\n        mae_scores.append(mae_score)\n        rmse_scores.append(rmse_score)\n        \n    return mape_scores, mae_scores, rmse_scores","713118ee":"GLOBAL_ORDER = (5, 1, 0)\nclass Model1:\n    def __init__(self):\n        self.model = None\n    def fit(self, df_train):\n        self.model = ARIMA(df_train['y'], order=GLOBAL_ORDER).fit()\n        return self\n    def predict(self, df_test):\n        return self.model.forecast().values[0]","35bb1a72":"p = d = q = range(0, 2) # 3 \u0434\u043e\u043b\u0433\u043e\ns = [66, 130, 261] # [5, 22, 66] # [5, 22, 66, 130, 261] \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0434\u043e\u043b\u0433\u043e\npdq = [[1,1,1]] #list(itertools.product(p, d, q))\nseasonal_pdq = list(itertools.product([0], [1], [1], s)) #list(itertools.product(p, d, q, s))","c97d9da6":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nsaved_results = []\n\nfor param in tqdm(pdq):\n    for param_seasonal in tqdm(seasonal_pdq):\n        try:\n            #mod = SARIMAX(train_df['y'],\n            mod = SARIMAX(df['y'],\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit(disp=False)\n            \n            saved_results.append( (results.aic, param, param_seasonal) )\n            \n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            print('error')\n            #raise\n            continue","9c13b1ef":"sorted(saved_results)[0:15]\n# TODO: (1, 1, 1), (0, 1, 1, x)), x in [...] enforce_stationarity=False,\n                                           # enforce_invertibility=False","d56427c4":"mod = SARIMAX(train_df['y'],\n                                            order=(1, 1, 1),\n                                            seasonal_order=(0, 1, 1, 66),\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\nresults.plot_diagnostics(figsize=(15, 12))\nplt.show()","3daf3b5f":"import torch \nfrom torch import nn\nfrom torch.nn import LSTM, GRU\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler","ba71f51c":"input_dim = 100\n\ndef generate_time_lags(df, n_lags):\n    df_n = df.copy()\n    for n in range(1, n_lags + 1):\n        df_n[f\"lag{n}\"] = df_n[\"y\"].shift(n)\n    df_n = df_n.iloc[n_lags:]\n    return df_n\n    \n\ndf_generated = generate_time_lags(df, input_dim)\ndf_generated.drop(['date'], axis=1, inplace=True)\ndf_generated","feffb04a":"def feature_label_split(df, target_col):\n    y = df[[target_col]]\n    X = df.drop(columns=[target_col])\n    return X, y\n\ndef train_val_test_split(df, target_col, test_ratio):\n    val_ratio = test_ratio \/ (1 - test_ratio)\n    X, y = feature_label_split(df, target_col)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df_generated, 'y', 0.2)","acc67384":"def get_scaler(scaler):\n    scalers = {\n        \"minmax\": MinMaxScaler,\n        \"standard\": StandardScaler,\n        \"maxabs\": MaxAbsScaler,\n        \"robust\": RobustScaler,\n    }\n    return scalers.get(scaler.lower())()\n    \nscaler = get_scaler('minmax')","b365ff81":"X_train_arr = scaler.fit_transform(X_train)\nX_val_arr = scaler.transform(X_val)\nX_test_arr = scaler.transform(X_test)\n\ny_train_arr = scaler.fit_transform(y_train)\ny_val_arr = scaler.transform(y_val)\ny_test_arr = scaler.transform(y_test)","fec3cb66":"from torch.utils.data import TensorDataset, DataLoader\n\nbatch_size = 64\n\ntrain_features = torch.Tensor(X_train_arr)\ntrain_targets = torch.Tensor(y_train_arr)\nval_features = torch.Tensor(X_val_arr)\nval_targets = torch.Tensor(y_val_arr)\ntest_features = torch.Tensor(X_test_arr)\ntest_targets = torch.Tensor(y_test_arr)\n\ntrain = TensorDataset(train_features, train_targets)\nval = TensorDataset(val_features, val_targets)\ntest = TensorDataset(test_features, test_targets)\n\ntrain_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\nval_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\ntest_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\ntest_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)","5f92f795":"class RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n        super(RNNModel, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        \n        self.rnn = nn.RNN(\n            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n\n        out, h0 = self.rnn(x, h0.detach())\n        out = out[:, -1, :]\n        out = self.fc(out)\n        \n        return out","2252818a":"class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n        super(LSTMModel, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.lstm = nn.LSTM(\n            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n\n        out, (hn, cn) = self.lstm(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.fc(out)\n\n        return out\n    \n    \nclass LSTMModelX(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n        super(LSTMModelX, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.lstm = nn.LSTM(\n            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n        )\n        \n        self.hidden_linear_size = 128\n        self.lin_h = nn.Linear(hidden_dim, self.hidden_linear_size)\n        self.fc = nn.Linear(self.hidden_linear_size, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n\n        out, (hn, cn) = self.lstm(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.lin_h(out)\n        out = self.fc(out)\n\n        return out","4b71fdeb":"class GRUModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n        super(GRUModel, self).__init__()\n\n        self.layer_dim = layer_dim\n        self.hidden_dim = hidden_dim\n        self.gru = nn.GRU(\n            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        out, _ = self.gru(x, h0.detach())\n        out = out[:, -1, :]\n        out = self.fc(out)\n\n        return out","aa626102":"def get_model(model, model_params):\n    models = {\n        \"rnn\": RNNModel,\n        \"lstm\": LSTMModel,\n        \"gru\": GRUModel,\n        \"xlstm\": LSTMModelX,\n    }\n    return models.get(model.lower())(**model_params)","6f15e353":"class Optimization:\n    def __init__(self, model, loss_fn, optimizer):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.train_losses = []\n        self.val_losses = []\n    \n    def train_step(self, x, y):\n        self.model.train()\n\n        yhat = self.model(x)\n        loss = self.loss_fn(y, yhat)\n        loss.backward()\n\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        return loss.item()\n    \n    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n        #model_path = f'models\/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n\n        for epoch in range(1, n_epochs + 1):\n            batch_losses = []\n            for x_batch, y_batch in train_loader:\n                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n                y_batch = y_batch.to(device)\n                loss = self.train_step(x_batch, y_batch)\n                batch_losses.append(loss)\n            training_loss = np.mean(batch_losses)\n            self.train_losses.append(training_loss)\n\n            with torch.no_grad():\n                batch_val_losses = []\n                for x_val, y_val in val_loader:\n                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n                    y_val = y_val.to(device)\n                    self.model.eval()\n                    yhat = self.model(x_val)\n                    val_loss = self.loss_fn(y_val, yhat).item()\n                    batch_val_losses.append(val_loss)\n                validation_loss = np.mean(batch_val_losses)\n                self.val_losses.append(validation_loss)\n\n            if (epoch <= 10) | (epoch % 50 == 0):\n                print(\n                    f\"[{epoch}\/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n                )\n\n        #torch.save(self.model.state_dict(), model_path)\n        \n    \n    def evaluate(self, test_loader, batch_size=1, n_features=1):\n        with torch.no_grad():\n            predictions = []\n            values = []\n            for x_test, y_test in test_loader:\n                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n                y_test = y_test.to(device)\n                self.model.eval()\n                yhat = self.model(x_test)\n                predictions.append(yhat.to(device).detach().cpu().numpy())\n                values.append(y_test.to(device).detach().cpu().numpy())\n\n        return predictions, values\n    \n    \n    def plot_losses(self):\n        plt.plot(self.train_losses, label=\"Training loss\")\n        plt.plot(self.val_losses, label=\"Validation loss\")\n        plt.legend()\n        plt.title(\"Losses\")\n        plt.show()\n        plt.close()","b57182e0":"input_dim = len(X_train.columns)\noutput_dim = 1\nhidden_dim = 64\nlayer_dim = 1\nbatch_size = 64\ndropout = 0.5\nn_epochs = 3000\nlearning_rate = 1e-5\nweight_decay = 1e-6\ndevice = 'cuda'\n\nmodel_params = {'input_dim': input_dim,\n                'hidden_dim' : hidden_dim,\n                'layer_dim' : layer_dim,\n                'output_dim' : output_dim,\n                'dropout_prob' : dropout}\n\nmodel = get_model('xlstm', model_params).to(device)\n#model = get_model('rnn', model_params).to(device)\n\nloss_fn = nn.MSELoss(reduction=\"mean\")\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\nopt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\nopt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\nopt.plot_losses()\n\npredictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n\n\ndef inverse_transform(scaler, df, columns):\n    for col in columns:\n        df[col] = scaler.inverse_transform(df[col])\n    return df\n\n\ndef format_predictions(predictions, values, df_test, scaler):\n    vals = np.concatenate(values, axis=0).ravel()\n    preds = np.concatenate(predictions, axis=0).ravel()\n    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds}, index=df_test.head(len(vals)).index)\n    df_result = df_result.sort_index()\n    df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n    return df_result\n\n\ndf_result = format_predictions(predictions, values, X_test, scaler)\ndf_result.plot()","b417d471":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndef calculate_metrics(df):\n    return {'mae' : mean_absolute_error(df.value, df.prediction),\n            'rmse' : mean_squared_error(df.value, df.prediction) ** 0.5,\n            'r2' : r2_score(df.value, df.prediction),\n           'mape' : mape(df.value, df.prediction)}\n\nresult_metrics = calculate_metrics(df_result)\nresult_metrics","835aaf85":"lstm + linear\n\ninput_dim = len(X_train.columns)\noutput_dim = 1\nhidden_dim = 64\nlayer_dim = 1\nbatch_size = 64\ndropout = 0.3\nn_epochs = 3000\nlearning_rate = 1e-5\nweight_decay = 1e-6\ndevice = 'cuda'\n\n{'mae': 1051.4524,\n 'rmse': 1200.708072347313,\n 'r2': 0.6287840717016756,\n 'mape': 0.031079119071364403}","66e444fb":"lstm\n\ninput_dim = len(X_train.columns)\noutput_dim = 1\nhidden_dim = 64\nlayer_dim = 1\nbatch_size = 64\ndropout = 0.2\nn_epochs = 3000\nlearning_rate = 1e-5\nweight_decay = 1e-6\ndevice = 'cuda'\n\n{'mae': 1334.4255,\n 'rmse': 1463.0793724196922,\n 'r2': 0.4488274813919264,\n 'mape': 0.03963728994131088}","6448ba0f":"GRU \n\ninput_dim = len(X_train.columns)\noutput_dim = 1\nhidden_dim = 64\nlayer_dim = 1\nbatch_size = 64\ndropout = 0.2\nn_epochs = 3000\nlearning_rate = 1e-5\nweight_decay = 1e-6\ndevice = 'cuda'\n\n{'mae': 1412.4103,\n 'rmse': 1559.0283833208425,\n 'r2': 0.37416508438553353,\n 'mape': 0.04189912974834442}","b0d75cd3":"rnn\n\n{'mae': 937.9477,\n 'rmse': 1026.7855180124036,\n 'r2': 0.7285366112371796,\n 'mape': 0.027957651764154434}\n\ninput_dim = len(X_train.columns)\noutput_dim = 1\nhidden_dim = 64\nlayer_dim = 1\nbatch_size = 64\ndropout = 0.2\nn_epochs = 3000\nlearning_rate = 1e-5\nweight_decay = 1e-6\ndevice = 'cuda'","2b20ff74":"# arima (0.056451196560597376, 1288.1876497425512, 1461.318052681035)\n# sarimax (5, 1, 10) best  0.03907596261813451 947.5825418772287\n# exp [3.91138090e-02 9.49529989e+02 1.17399010e+03]\n# prophet (0.04395567122708405, 1080.3012223441665, 1335.7179819426449)","92b376ad":"def validate_with_interval(df, Model, n_splits=3):\n    tss = TimeSeriesSplit(n_splits=n_splits)\n    mape_scores, mae_scores, rmse_scores = [], [], []\n    \n    for train_index, test_index in tqdm(tss.split(df)):\n        df_train, df_val = df.iloc[train_index], df.iloc[test_index]\n        y_true = df_val['y']\n\n        model = Model()\n        model = model.fit(df_train)\n        y_pred = model.predict(df_val)\n        \n        mape_score = mape(y_true, y_pred)\n        mae_score = mae(y_true, y_pred)\n        rmse_score = rmse(y_true, y_pred)\n\n        mape_scores.append(mape_score)\n        mae_scores.append(mae_score)\n        rmse_scores.append(rmse_score)\n        \n    return mape_scores, mae_scores, rmse_scores\n\n\nGLOBAL_ORDER = (5, 1, 0)\nclass Model11:\n    def __init__(self):\n        self.model = None\n    def fit(self, df_train):\n        self.model = SARIMAX(train_df['y']).fit(disp=False)\n        return self\n    def predict_point(self, df_test):\n        return self.model.forecast().values[0]\n    def predict(self, df_test):\n        return self.model.get_forecast(steps=df_test.shape[0]).predicted_mean.values","09689c89":"#mape_scores, mae_scores, rmse_scores = validate_with_interval(train_df, Model11, 3)\nmape_scores, mae_scores, rmse_scores = validate_with_interval(df, Model11, 3)\nnp.mean(mape_scores), np.mean(mae_scores), np.mean(rmse_scores)\n# arima (0.056451196560597376, 1288.1876497425512, 1461.318052681035)","08989155":"m1_list, m2_list, m3_list = [], [], []\nfor p, q, r in itertools.product([5, 10, 20], [0, 1, 2], [0, 10, 100]):\n    GLOBAL_ORDER = (p, q, r)\n    try:\n        mape_scores, mae_scores, rmse_scores = validate(train_df, Model1)\n        m1, m2, m3 = np.mean(mape_scores), np.mean(mae_scores), np.mean(rmse_scores)\n        m1_list.append( (m1, (p, q, r)) )\n        m2_list.append( (m2, (p, q, r)) )\n        m3_list.append( (m3, (p, q, r)) )\n        print(GLOBAL_ORDER, m1, 'mape')\n    except:\n        print(GLOBAL_ORDER, 'error')","1c186541":"print(sorted(m1_list))\nprint(sorted(m2_list))\nprint(sorted(m3_list))\n# (5, 1, 10) best  0.03907596261813451 947.5825418772287","f744483e":"(5, 1, 10) 0.03907596261813451 mape","a2385c8b":"def get_metrics(train_df, Model):\n    try:\n        mape_scores, mae_scores, rmse_scores = validate(train_df, Model)\n        m1, m2, m3 = np.mean(mape_scores), np.mean(mae_scores), np.mean(rmse_scores)\n        return m1, m2, m3\n    except:\n        print('error')","5e75e2b5":"# ['add', 'mul'], [True, False], ['add', 'mul'], [5, 22, 66]\n\nGLOBAL_ORDER = {}\nclass Model2:\n    def __init__(self):\n        self.model = None\n    def fit(self, df_train):\n        self.model = ExponentialSmoothing(df_train['y'], trend='add', damped=True,\n                     seasonal='add', seasonal_periods=5).fit()\n        #self.model = ARIMA(df_train['y'], order=GLOBAL_ORDER).fit()\n        return self\n    def predict_point(self, df_test):\n        return self.model.forecast().values[0]","0831678b":"#validate_with_point(train_df, Model2)\nvalidate_with_point(df, Model2)","b7bd7156":"mape_scores, mae_scores, rmse_scores = validate_with_point(df, Model2)\n#mape_scores, mae_scores, rmse_scores = validate_with_point(train_df, Model2)\nnp.mean(mape_scores), np.mean(mae_scores), np.mean(rmse_scores)\n# [3.91138090e-02 9.49529989e+02 1.17399010e+03]","76c0dbde":"GLOBAL_ORDER = {}\nclass Model3:\n    def __init__(self):\n        self.model = Prophet(n_changepoints=50, seasonality_prior_scale=20, \n                             changepoint_prior_scale=0.9)\n        \n    def fit(self, df_train):\n        t = df_train.copy()\n        t.rename({'date' : 'ds'}, axis=1, inplace=True)\n        self.model = self.model.fit(t, verbose=0)\n        return self\n    \n    def predict_point(self, df_test):\n        t = df_test[['date']].copy()\n        t = t.iloc[[0]]\n        t.rename({'date' : 'ds'}, axis=1, inplace=True)\n        return self.model.predict(t)['yhat'][0]\n    def predict(self, df_test):\n        t = df_test[['date']].copy()\n        t.rename({'date' : 'ds'}, axis=1, inplace=True)\n        pred = self.model.predict(t)['yhat'].values\n        #print('PRED:', pred)\n        return pred","80d82f34":"mape_scores, mae_scores, rmse_scores = validate_with_interval(df, Model3, 3)\n#mape_scores, mae_scores, rmse_scores = validate_with_interval(train_df, Model3, 3)\nnp.mean(mape_scores), np.mean(mae_scores), np.mean(rmse_scores)\n# (0.04395567122708405, 1080.3012223441665, 1335.7179819426449)","f0b51308":"(n_changepoints=50, seasonality_prior_scale=20, \n                             changepoint_prior_scale=0.9)\n# (0.04395567122708405, 1080.3012223441665, 1335.7179819426449)","2805f690":"## Train-val-test","213709b0":"## utils","22f3975f":"## ","8b6e3578":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/dow+jones+index\n\n\u0414\u0430\u043d\u043d\u044b\u0435:\nhttps:\/\/fred.stlouisfed.org\/series\/DJIA\n\nCPI: https:\/\/datahub.io\/core\/cpi-us#readme"}}