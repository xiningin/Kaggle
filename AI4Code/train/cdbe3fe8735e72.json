{"cell_type":{"b321d10b":"code","e58c916a":"code","dd1718e7":"code","ce65ba56":"code","dfb7278d":"code","ccdabd04":"code","9cd5a4fd":"code","35cddbe0":"code","b8e5e7e7":"code","fd62d631":"code","698ab8f3":"code","155c0d60":"code","abdfe317":"code","75a7e108":"code","91e3c761":"code","c71e0f45":"code","05b96362":"code","56b317fc":"code","f74318cf":"code","44d6a592":"code","44db867e":"code","e91de965":"code","6ab1dd24":"code","863f2326":"code","ed22943c":"code","491641f8":"code","d51e0a18":"code","06cb5a4e":"code","459cda6b":"code","1233ad6a":"code","d18daf69":"code","44e7c12b":"code","873f57f0":"code","df4478b7":"code","0aa8c37b":"code","b1e2a5a9":"markdown","de69a447":"markdown","ed62c6bd":"markdown","71d6af9b":"markdown","08de5517":"markdown","99e3917b":"markdown","bede1d98":"markdown","aa19bb38":"markdown","378e2f38":"markdown","eb97b17b":"markdown"},"source":{"b321d10b":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15, 10)","e58c916a":"!ls ..\/input","dd1718e7":"train_df = pd.read_csv('..\/input\/train-balanced-sarcasm.csv')","ce65ba56":"train_df.head()","dfb7278d":"train_df.info()","ccdabd04":"train_df.dropna(subset=['comment'], inplace=True)","9cd5a4fd":"train_df.label.value_counts()","35cddbe0":"train_comments, valid_comments, y_train, y_valid = \\\ntrain_test_split(train_df['comment'], train_df['label'], random_state=17)","b8e5e7e7":"subreddits_to_plot = train_df.subreddit.value_counts().head(30).index","fd62d631":"plot = sns.countplot(x='subreddit', data=train_df[train_df.subreddit.isin(subreddits_to_plot)], hue='label')\n_ = plot.set_xticklabels(plot.get_xticklabels(), rotation=90)","698ab8f3":"time_label_data = train_df.groupby(['date']).label.value_counts(normalize=True)","155c0d60":"plot = sns.lineplot(data=time_label_data.loc[:, 1])\nfor item in plot.get_xticklabels():\n    item.set_rotation(90)","abdfe317":"# get new features from existing\ntrain_df['year'] = train_df.date.apply(lambda x: x.split('-')[0])\ntrain_df['month'] = train_df.date.apply(lambda x: x.split('-')[1])","75a7e108":"seasonal_time_year = train_df.groupby(['year', 'month']).label.value_counts(normalize=True).loc[:,:,1]\\\n.reset_index(level=[0, 1])\nseasonal_time_year.month = seasonal_time_year.month.astype(int)\nseasonal_time_year.head()","91e3c761":"seasonal_time_year.loc[(seasonal_time_year.month >= 3) & (seasonal_time_year.month <= 5), 'season'] = 'spring'\nseasonal_time_year.loc[(seasonal_time_year.month >= 6) & (seasonal_time_year.month <= 8), 'season'] = 'summer'\nseasonal_time_year.loc[(seasonal_time_year.month >= 9) & (seasonal_time_year.month <= 11), 'season'] = 'autumn'\nseasonal_time_year.loc[(seasonal_time_year.month >= 12) | (seasonal_time_year.month <= 2), 'season'] = 'winter'","c71e0f45":"sns.factorplot(x='year', y='label', hue='season', data=seasonal_time_year, kind='bar')","05b96362":"train_df.created_utc = pd.to_datetime(train_df.created_utc)\ntrain_df['hour_created'] = train_df.created_utc.dt.hour\ntrain_df.head()","56b317fc":"train_df.month = train_df.month.astype(int)\ntrain_df.year = train_df.year.astype(int)\nsns.heatmap(train_df.corr())","f74318cf":"sns.distplot(train_df.loc[train_df['label'] == 1, 'hour_created'], bins=24)","44d6a592":"train_df['day_of_week_created'] = train_df.created_utc.dt.dayofweek\nsns.distplot(train_df.loc[train_df['label'] == 1, 'day_of_week_created'], bins=7)","44db867e":"len(train_df.subreddit.unique()) # there 14876 unique subreddits","e91de965":"train_df.head()","6ab1dd24":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\nvectorized_comments = vectorizer.fit_transform(train_df['comment'])\nvectorized_comments.shape","863f2326":"X_train, y_train = train_df.loc[:,['hour_created']], train_df.loc[:,'label']","ed22943c":"from sklearn.preprocessing import LabelEncoder","491641f8":"X_train.head()","d51e0a18":"from scipy.sparse import hstack","06cb5a4e":"X_train, X_test, y_train, y_test = \\\ntrain_test_split(hstack([X_train, vectorized_comments]), y_train, random_state=17)","459cda6b":"logit = LogisticRegression(random_state=17, n_jobs=-1, verbose=True, solver='lbfgs')\nparameters = {'C' : np.logspace(-2, 2, 5)}","1233ad6a":"clf = GridSearchCV(logit, parameters, cv=5)\nclf.fit(X_train, y_train)","d18daf69":"logit.fit(X_train, y_train)","44e7c12b":"accuracy_score(y_test, clf.predict(X_test))","873f57f0":"accuracy_score(y_test, logit.predict(X_test))","df4478b7":"import eli5\neli5.show_weights(estimator=clf)","0aa8c37b":"clf.best_params_","b1e2a5a9":"## First, let's make an exploratory data analysis to detect some important features","de69a447":"**I see no trends in seasonal data**","ed62c6bd":"**Create 4 new seasonal features: autumn, winter, spring and summer**","71d6af9b":"**The distribution is quite weird, but we clearly can see the tendency for sarcasm post to appear after 11 am and \nbefore 0 am**","08de5517":"**The trend is monotone, except for the weekends, where the amount of sarcastic comments decreases. Maybe, this happens, because people are usually less exhaused on weekends**","99e3917b":"**It can be clearly seen, that the only numerical feature, with which label has correlation is amount of downvotes**","bede1d98":"Let's do the same to parent comments","aa19bb38":"**The general trend is that the amount of sarcasm in trands is decreasing**  \n### Let's try to check if it's seasonal data","378e2f38":"Now, let's have a look at some text features","eb97b17b":"**We can clearly see, that the percentage of sarcastic comments differs from subreddit to subreddit**"}}