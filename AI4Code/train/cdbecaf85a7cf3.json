{"cell_type":{"693efb86":"code","204d7618":"code","71035f26":"code","752d1364":"code","43f374ce":"code","99970b9a":"code","140908c3":"code","61869428":"code","97848b28":"code","3a6c501f":"code","46cfeb77":"code","8c21383d":"code","f4c3a15c":"code","2d296d97":"code","ac2df107":"code","ac628697":"code","b5f51f9e":"code","c9fa59aa":"code","344c7857":"code","9d36486d":"code","b40a124d":"code","d69c3805":"code","3be1907e":"code","ef8bd7b2":"code","c5a4aa73":"code","a132b804":"code","0a280c92":"code","c0a0e00c":"code","d2b0b9c6":"code","97dcf4d1":"code","6f9735d4":"code","a49633d2":"code","47439b60":"code","eeccd8f5":"code","a1995722":"code","5f46b477":"code","27d76702":"code","2270c212":"code","0ee779ab":"code","bb8916d4":"code","bec12406":"code","43588974":"code","bb5994d9":"code","fb9fb100":"code","fd2c0324":"code","2725e57e":"code","f0eed8bd":"code","7ed1601d":"code","19a5ab40":"code","e98771b6":"code","2c5ab16d":"code","54623f83":"code","6b47bc74":"code","5156747b":"code","0d56576e":"code","69920da3":"code","23806156":"code","d4bbbfac":"markdown","b5b30bdb":"markdown","8950427e":"markdown","573b7da9":"markdown","79244f2e":"markdown","f74caffa":"markdown","f5b9a6f6":"markdown","5d77da0c":"markdown","0663e0ab":"markdown","38f2a122":"markdown"},"source":{"693efb86":"text = \"\"\"Latanya Sweeney and Nick Diakopoulos pioneered the study of misbehavior in Google systems (Sweeney, 2013; Diakopoulos, 2013; Diakopoulos, 2016). Their work exposed instances of algorithmic defa- mation in Google searches and ads. Diakopoulos discussed a canonical example of such algorithmic defamation in which search engine auto- completion routines, fed a steady diet of historical user queries, learn to make incorrect defamatory or bigoted associations about people or groups of people.10 Sweeney showed that such learned negative associa- tions affect Google\u2019s targeted ads. In her example, just searching for certain types of names led to advertising for criminal justice services, such as bail bonds or criminal record checking. Diakopoulos\u2019s exam- ples included consistently defamatory associations for searches related to transgender issues.\nStudies like Sweeney\u2019s and Diakopoulos\u2019s are archetypes in the growing field of data and algorithmic journalism. More news and research articles chronicle the many missteps of the algorithms that affect different parts of our lives, online and off. IBM\u2019s Jeopardy- winning AI, Watson, famously had to have its excessive swearing habit corrected after its learning algorithms ingested some unsavory data (Madrigal, 2013). There have also been reports on the effects of Waze\u2019s traffic routing algorithms on urban traffic patterns (Bradley, 2015). One revealing book describes the quirks of the data and algorithms underlying the popular OkCupid dating service (Rudder, 2014). More recently, former Facebook contractors revealed that Facebook\u2019s news feed trend algorithm was actually the result of subjective input from a human panel (Tufekci, 2016).\"\"\"","204d7618":"# We can split the text-chunk into something like sentences.\nsplit_text = text.split('.')\nprint(split_text)","71035f26":"# print out the first sentence\n\nsentence_1 = split_text[0]\nprint(sentence_1)","752d1364":"# Let's create tokens from this sentence\ntokens_sentence_1 = [word for word in sentence_1.split(' ')]\nprint(tokens_sentence_1)","43f374ce":"# Let's lowercase all these tokens and clean up the \\n (new line command)\n\n\ntokens_sentence_1_lower = [word.lower().strip() for word in sentence_1.split(' ')]\nprint('### OUTPUT1 ###')\nprint(tokens_sentence_1_lower)\nprint('\\n')\n    \n# Also we will replace \"()\" as well as make sure that only words lend in our list\ntokens_sentence_1_lower = [word.replace('(','').replace(')','') for word in tokens_sentence_1_lower if word.isalpha()]\n\nprint('### OUTPUT2 ###')\nprint(tokens_sentence_1_lower)","99970b9a":"# Removing stopwords\n\nstopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n                'won', \"won't\", 'wouldn', \"wouldn't\"]","140908c3":"tokens_sentence_1_clean = [word for word in tokens_sentence_1_lower if word not in stopwords_en]\nprint(tokens_sentence_1_clean)","61869428":"# Tokenizing sentences\nfrom nltk.tokenize import sent_tokenize\n\n# Tokenizing words\nfrom nltk.tokenize import word_tokenize\n\n# Tokenizing Tweets!\nfrom nltk.tokenize import TweetTokenizer","97848b28":"# Let's get our stences.\n# Note that the full-stops at the end of each sentence are still there\nsentences = sent_tokenize(text)\nprint(sentences)","3a6c501f":"# Use word_tokenize to tokenize the first sentence: tokenized_sent\ntokenized_sent = word_tokenize(sentences[0])\n\n# Make a set of unique tokens in the entire text: unique_tokens\nunique_tokens = set(word_tokenize(text))\nprint(unique_tokens)","46cfeb77":"# The preprocessing is donw for you\n\nimport pandas as pd\n\ntrump_tweets = pd.read_json('https:\/\/cdn.rawgit.com\/SDS-AAU\/M2-2018\/2cbfe741\/input\/trump_twitter.json')\ntrump_tweets.set_index(pd.to_datetime(trump_tweets['created_at']), inplace=True)","8c21383d":"trump_tweets","f4c3a15c":"# time-indexing let's us perform neat things such as resampling\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\ntrump_tweets.resample('W').favorite_count.mean().plot()\ntrump_tweets.resample('W').retweet_count.mean().plot()","2d296d97":"# Here we extract the tweets for year 2016. Can you also extract those for 2017 and 2018\ntrump_tweets_2016 = trump_tweets[trump_tweets.index.year == 2016]","ac2df107":"# A quick lock at the created Dataframe\ntrump_tweets_2016.info()","ac628697":"# Can you find out the right column for the actual tweets?\ntweets = trump_tweets_2016['text']","b5f51f9e":"# We can use the tweet tokenizer to parse these tweets:\n\ntknzr = TweetTokenizer()\n\n# parse the tweets\ntweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\n\n# print out the 10 first tweets in parsed\nprint(tweets_tokenized[:10])","c9fa59aa":"# Get out all hashtags using loops\n\n# create an empty list: hashtags\nhashtags = []\n\n# Filter hashtags\nfor tweet in tweets_tokenized:\n    hashtags.extend([word for word in tweet if word.startswith('#')])\n    \n# Print out the first 20 hashtags to check\nprint(hashtags[:20])","344c7857":"# Let't import the counter function\nfrom collections import Counter","9d36486d":"# Count all hashtags\nhashtags_counter = Counter(hashtags)\n\n# create an object and print out the most common 10 hashtags\nmost_common_10 = hashtags_counter.most_common(10)\nprint(most_common_10)","b40a124d":"# Let's define a (a bit clunky but easy to read) function \n# that picks out the top10 hashtags for 1 year (performing the above)\n\ndef pick_top_10(year):\n    tweet_df = trump_tweets[trump_tweets.index.year == year]\n    tweets = tweet_df['text']\n    tweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\n    hashtags = []\n    for tweet in tweets_tokenized:\n        hashtags.extend([word for word in tweet if word.startswith('#')])\n        hashtags_counter = Counter(hashtags)\n    return dict(hashtags_counter.most_common(10))","d69c3805":"years = [2015,2016,2017,2018]\n\n\ntop_10 = []\n\nfor year in years:\n    top_10.append(pick_top_10(year))","3be1907e":"top_10","ef8bd7b2":"# Importing stopwords\nfrom nltk.corpus import stopwords\nstopwords_en = stopwords.words('english')\n\n# Let's import a lemmatizer from NLTK and try how it works\nfrom nltk.stem import WordNetLemmatizer\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","c5a4aa73":"# We already imported the data above and can use it right away\n\n# Tokenize each tweet\ntrump_tweets['tokenized'] = trump_tweets['text'].map(lambda t: tknzr.tokenize(t))","a132b804":"# lowecase, strip and ensure we only include words\ntrump_tweets['tokenized'] = trump_tweets['tokenized'].map(\n    lambda t: [word.lower().strip() for word in t if word.isalpha()])","0a280c92":"# lemmarize and remove stopwords\ntrump_tweets['tokenized'] = trump_tweets['tokenized'].map(\n    lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t \n               if word not in stopwords_en])","c0a0e00c":"# Quick check\ntrump_tweets['tokenized'][:10]","d2b0b9c6":"# We start by importing and initializing a Gensim Dictionary. \n# The dictionary will be used to map between words and IDs\n\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(trump_tweets['tokenized'])","97dcf4d1":"# And this is how you can map back and forth\n# Select the id for \"hillary\": hillary_id\nhillary_id = dictionary.token2id.get('hillary')\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(hillary_id))","6f9735d4":"# Create a Corpus: corpus\n# We use a list comprehension to transform our abstracts into BoWs\ncorpus = [dictionary.doc2bow(tweet) for tweet in trump_tweets['tokenized']]","a49633d2":"# Import the TfidfModel from Gensim\nfrom gensim.models.tfidfmodel import TfidfModel\n\n# Create and fit a new TfidfModel using the corpus: tfidf\ntfidf = TfidfModel(corpus)\n\n# Now we can transform the whole corpus\ntfidf_corpus = tfidf[corpus]","47439b60":"# Just like before, we import the model\nfrom gensim.models.lsimodel import LsiModel\n\n# Fit a lsi model: lsi using the tfidf transformed corpus as input\nlsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)","eeccd8f5":"# Inspect the first 10 topics\nlsi.show_topics(num_topics=10)","a1995722":"# Create a transformed corpus using the lsi model from the tfidf corpus: lsi_corpus\nlsi_corpus = lsi[tfidf_corpus]","5f46b477":"# Load the MatrixSimilarity\nfrom gensim.similarities import MatrixSimilarity\n\n# Create the document-topic-matrix\ndocument_topic_matrix = MatrixSimilarity(lsi_corpus)\ndocument_topic_matrix = document_topic_matrix.index","27d76702":"pd.DataFrame(document_topic_matrix.dot(document_topic_matrix.T))","2270c212":"# Let's identify some clusters in our corpus\n\n# We import KMeans form the Sklearn library\nfrom sklearn.cluster import KMeans\n\n# Instatiate a model with 4 clusters\nkmeans = KMeans(n_clusters = 10)\n\n# And fit it on our document-topic-matrix\nkmeans.fit(document_topic_matrix)","0ee779ab":"# Let's annotate our abstracts with the assigned cluster number\ntrump_tweets['cluster'] = kmeans.labels_","bb8916d4":"# We can try to visualize our documents using TSNE - \n# an approach for visualizing high-dimensional data\n\n# Import the module first\n#from sklearn.manifold import TSNE\n\n#faster as TSNE\nimport umap\n\nvisualization = umap.UMAP().fit_transform(document_topic_matrix)\n\n# And instantiate\n#tsne = TSNE()\n\n\n# Let's try to boil down the 100 dimensions into 2\n#visualization = tsne.fit_transform(document_topic_matrix)","bec12406":"# Import the plotting library\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","43588974":"# Plot the trump_tweet map\nplt.figure(figsize=(15,15))\nsns.scatterplot(visualization[:,0],visualization[:,1], \n           data = trump_tweets, palette='RdBu', \n           hue=trump_tweets.cluster.values, legend='full')","bb5994d9":"# Collectiong\n\n# Select a cluster e.g. 1\nCluster = 6\n\n# Create an empty cluster token list: cluster_tweets\ncluster_tweets = []\n\n# Create a loop which iterates over all tokenized tweets in the \n# trump_tweets dataframe and extends the created list with them\nfor x in trump_tweets[trump_tweets['cluster'] == Cluster]['tokenized']:\n    cluster_tweets.extend(x)","fb9fb100":"# Transfortm the selected tweets using the tfidf model\n\ntweets_in_cluster_tfidf = tfidf[dictionary.doc2bow(cluster_tweets)]","fd2c0324":"# Sort the weights from highest to lowest: sorted_tfidf_weights\n# this has been done for you\ntweets_in_cluster_tfidf = sorted(tweets_in_cluster_tfidf, key=lambda w: w[1], reverse=True)\n\n# Print the top 10 weighted words\nfor term_id, weight in tweets_in_cluster_tfidf[:10]:\n    print(dictionary.get(term_id), weight)","2725e57e":"# Tiny not too pretty graph\ntrump_tweets.groupby('cluster').resample('M')['cluster'].count().unstack().T.plot()","f0eed8bd":"trump_tweets['is_retweet'] = trump_tweets['is_retweet'].astype(bool)","7ed1601d":"y = trump_tweets[trump_tweets['is_retweet'] == False].favorite_count.values","19a5ab40":"trump_tweets['indexing'] = range(len(trump_tweets))","e98771b6":"x_selector = trump_tweets[trump_tweets['is_retweet'] == False]['indexing']","2c5ab16d":"X = document_topic_matrix[x_selector,:]","54623f83":"# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)","6b47bc74":"# Let's fit a simple linear regression model\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\n\nregressor.fit(X_train, y_train)","5156747b":"# Let's fit a simple linear regression model\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nregressor = GradientBoostingRegressor()\n\nregressor.fit(X_train, y_train)","0d56576e":"regressor.score(X_test, y_test)","69920da3":"y_pred = regressor.predict(X_test)","23806156":"sns.regplot(y_test, y_pred)","d4bbbfac":"### Exercise 5:  Explore the content of the created clusters","b5b30bdb":"### Exercise 3: 3 Years of Donald T. on Twitter\n\nIn this exercise you will explore Trump's hashtag use in the years 2016 - 2018\nThe data is obtained from: https:\/\/github.com\/bpb27\/trump_tweet_data_archive\/\nIt will be loaded into a Pandas Dataframe (~Table), which will be time-indexed.\nThe tweets for 2016 are extracted. Your job is to tokenize the tweets using the tweet-tokenizer and to select all used hashtags into one long list.\nThen use a counter to find the most common hashtags for the different years.","8950427e":"Note that the object is a list of tupels where the first value is always the hashtag and the second the number how often it appeared in the course of a year.\n\n**Now let's bring everything down into compact functional form and rerun it for all years**","573b7da9":"# Natural Language Processing (NLP) in Python\n## An introduction to key concepts and techniques\n\nExercise notebook.\n\n- basic string manipulation\n- tokens and tokenization + some preprocessing\n- the Bag-of-Words model\n- topic modeling (and its close relation to dimensionality reduction \/ unsupervised machine learning)\n- text classification","79244f2e":"Sure, one could do so much more to pre-process. We could try to identify bi-grams, remove prepositions, verbs etc. But already this brings us rather far.\n\nNow we will dive into Gensim further transform our abstracts using more advanced techniques.","f74caffa":"### Exercise 2: Using advanced tokenizers from NLTK","f5b9a6f6":"### Exercise 4: Going beyond hashtags: Mr. Trump continued\n\n![trump](https:\/\/media1.tenor.com\/images\/9cdcdb12c0c4c6895f61c614273588f0\/tenor.gif?itemid=4814164)\n\nLet's ask python to read all Trump tweets and perhaps we can find some patterns and clusters using Gensim, TF-IDF and topic modelling","5d77da0c":"At this point, our corpus is a document-topic matrix. in corpus-format. We can create a full matrix using the built in MatrixSimilarity function (which is actually used for similarity-queries)","0663e0ab":"### Bonus: Predicting popular tweets","38f2a122":"### Exercise 1: Basic string manimulation"}}