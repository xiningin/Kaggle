{"cell_type":{"1091fc36":"code","09674f84":"code","88899295":"code","b1acd8ed":"code","8f84f61c":"code","8deac287":"code","e20327ac":"code","51cdeaaa":"code","4148e94a":"code","50928c19":"code","cc7842ec":"code","c57a8e62":"code","c2d9c183":"code","7d5b3ae3":"code","1e75f804":"code","b193fa26":"code","89edbb8a":"code","2a029a7c":"code","9d2a37cb":"code","da5ee830":"code","a133303f":"code","56cac3ab":"code","f3c89026":"code","4a0752cc":"code","efbaf404":"code","d03c21c2":"code","766e1e5b":"code","a393090c":"code","d1460798":"code","a95b38c5":"markdown","ccdb0c24":"markdown","768e6a8c":"markdown","90093dcc":"markdown","c05f8725":"markdown","0cd81ed5":"markdown","7b6a1f5c":"markdown","d9c29fa4":"markdown","231b14b2":"markdown","a05b92e6":"markdown","bfeb48ee":"markdown","79447806":"markdown","fca41f98":"markdown","c592e174":"markdown","42caf227":"markdown"},"source":{"1091fc36":"import numpy as np\nimport pandas as pd\nimport spacy\nimport re\nfrom collections import Counter\nimport string\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","09674f84":"train_file = \"..\/input\/commonlitreadabilityprize\/train.csv\"","88899295":"# Loading the data\ndata = pd.read_csv(train_file)\nprint(data.shape)\ndata.head()","b1acd8ed":"# Tokenization\ntok = spacy.load('en')\ndef tokenize (text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(nopunct)]","8f84f61c":"# Count number of occurences of each word\ncounts = Counter()\nfor text in list(data['excerpt']):\n    counts.update(tokenize(text))","8deac287":"# Deleting infrequent words\nprint(\"num_words before:\",len(counts.keys()))\nfor word in list(counts):\n    if counts[word] < 2:\n        del counts[word]\nprint(\"num_words after:\",len(counts.keys()))","e20327ac":"# Creating vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","51cdeaaa":"def encode_sentence(text, vocab2index, N=200):\n    tokenized = tokenize(text)\n    encoded = np.zeros(N, dtype=int)\n    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n    length = min(N, len(enc1))\n    encoded[:length] = enc1[:length]\n    return encoded, length","4148e94a":"data['encoded'] = data['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\ndata.head()","50928c19":"class CommonLitReadabiltyDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]","cc7842ec":"X = list(data['encoded'])\ny = list(data['target'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)\n\ntrain_ds = CommonLitReadabiltyDataset(X_train, y_train)\nvalid_ds = CommonLitReadabiltyDataset(X_valid, y_valid)","c57a8e62":"def train_model_regr(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y, l in train_dl:\n            x = x.long()\n            y = y.float()\n            y_pred = model(x, l)\n            optimizer.zero_grad()\n            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss = validation_metrics_regr(model, val_dl)\n        if i % 5 == 1:\n            print(\"train mse %.3f val rmse %.3f\" % (sum_loss\/total, val_loss))\n\ndef validation_metrics_regr (model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, y, l in valid_dl:\n        x = x.long()\n        y = y.float()\n        y_hat = model(x, l)\n        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss\/total","c2d9c183":"batch_size = 64\nvocab_size = len(words)\nembedding_dim = 300\nhidden_dim = 200\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds, batch_size=batch_size)","7d5b3ae3":"class LSTM_regr(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.4)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])","1e75f804":"model =  LSTM_regr(vocab_size, embedding_dim, hidden_dim)","b193fa26":"train_model_regr(model, epochs=30, lr=0.005)","89edbb8a":"glove_embedding_vectors_text_file = \"..\/input\/embeddings-glove-crawl-torch-cached\/crawl-300d-2M.vec\"\nglove_embedding_vectors_pt_file = \"..\/input\/embeddings-glove-crawl-torch-cached\/crawl-300d-2M.vec.pt\"","2a029a7c":"# We can load the vectors using our custom functions\ndef load_glove_vectors(glove_file= glove_embedding_vectors_text_file):\n    \"\"\"Load the glove word vectors\"\"\"\n    word_vectors = {}\n    with open(glove_file) as f:\n        for line in f:\n            split = line.split()\n            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n    return word_vectors\n\ndef get_emb_matrix(pretrained, word_counts, emb_size = 300):\n    \"\"\" Creates embedding matrix from word vectors\"\"\"\n    vocab_size = len(word_counts) + 2\n    vocab_to_idx = {}\n    vocab = [\"\", \"UNK\"]\n    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n    vocab_to_idx[\"UNK\"] = 1\n    i = 2\n    for word in word_counts:\n        if word in word_vecs:\n            W[i] = word_vecs[word]\n        else:\n            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n        vocab_to_idx[word] = i\n        vocab.append(word)\n        i += 1   \n    return W, np.array(vocab), vocab_to_idx\n\nword_vecs = load_glove_vectors()\npretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)","9d2a37cb":"# Or we can do it directly \nitos, stoi, pretrained_weights, embedding_dim = torch.load(glove_embedding_vectors_pt_file)\nvocab_size = pretrained_weights.size(0)\npretrained_weights = pretrained_weights.numpy()","da5ee830":"class LSTM_reg_glove_vecs(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n        self.embeddings.weight.requires_grad = False ## Freeze embeddings\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.4)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])","a133303f":"model_glove = LSTM_reg_glove_vecs(vocab_size, embedding_dim, hidden_dim, pretrained_weights)","56cac3ab":"train_model_regr(model_glove, epochs=30, lr=0.005)","f3c89026":"class PositionAwareAttention(nn.Module):\n    \n    def __init__(self, input_size, attn_size):\n        super().__init__()\n        self.input_size  = input_size\n        self.wx = nn.Conv1d(input_size, attn_size, 1, bias=True)  # from input to attention matrix\n        self.wh = nn.Conv1d(input_size, attn_size, 1, bias=False) # from hidden to attention matrix\n        self.wt = nn.Conv1d(attn_size, 1, 1, bias=True)           # from attention matrix to score\n        \n    def forward(self, x, h):\n        x = x.permute(1,2,0) # features last\n        wx = self.wx(x)\n        wh = self.wh(h.permute(1,0,2).contiguous().view(-1,self.input_size,1))\n        score = self.wt(torch.tanh(wx + wh))\n        score = F.softmax(score, dim=2)\n        out = torch.bmm(score, x.permute(0,2,1)).squeeze()\n        \n        return out","4a0752cc":"class RecNN(nn.Module):\n    def __init__(self, embs_dim, hidden_size, glove_weights, layers=1, atten_features = 24, \n                 dropout=0., bidirectional=False):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.num_layers = layers\n        self.emb_dim = embs_dim\n        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.emb.weight.data.copy_(torch.from_numpy(glove_weights)) # load pretrained vectors\n        self.emb.weight.requires_grad = False # make embedding non trainable\n        \n        self.lstm = nn.LSTM(self.emb_dim, self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.gru = nn.GRU(self.emb_dim, self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        self.pregru = nn.Conv1d(self.emb_dim, self.emb_dim, 1, bias=True)\n        self.atten = PositionAwareAttention(hidden_size*(bidirectional+1), atten_features)\n        \n        self.out = nn.Linear(2* hidden_size*(bidirectional+1), 32)\n        self.last = nn.Linear(32, 1)\n    def forward(self, x, l):\n        \n        embs = self.emb(x)\n        \n        lstm, (h1, c) = self.lstm(embs)\n        gru = F.relu(self.pregru(embs.permute(1,2,0)), inplace=True).permute(2,0,1)\n        \n        gru, h2 = self.gru(gru, h1)\n        lstm = lstm + gru\n        \n        x_max, _ = lstm.max(dim=0, keepdim=False) \n        x_atten = self.atten(lstm, h1+h2)\n        out = self.out(torch.cat([x_max, x_atten],dim = 1))\n        out = self.last(F.relu(out)).squeeze()\n        return out","efbaf404":"model_LSTM_attention = RecNN(embedding_dim, hidden_dim, pretrained_weights, dropout=0.4)","d03c21c2":"train_model_regr(model_LSTM_attention, epochs=30, lr=0.005)","766e1e5b":"checkpoint_path = \".\/LSTM_regr_model_with_glove.pth\"\ntorch.save(model_glove, checkpoint_path)","a393090c":"# Load in the best model\nmodel = torch.load(checkpoint_path)\nmodel.eval()\n\n# Read the test excerpts\ntest_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nprint(test_data.head())\n\n# Apply the same encoding as the train texts\ntest_data['encoded'] = test_data['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\nidx, excerpts_test = test_data['id'], test_data['encoded']\n\nX_test = [excerpts_test[i][0] for i in range(len(test_data))]\nl_test = [excerpts_test[i][1] for i in range(len(test_data))]\nX_test = torch.LongTensor(X_test)","d1460798":"# Apply the trained model\ny_hat = model(X_test, l_test)\ntest_target = y_hat.reshape(-1).tolist()\n\nmy_submission = pd.DataFrame({'id': idx, 'target': test_target})\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.head()","a95b38c5":"# Inference & Test","ccdb0c24":"# LSTM + attention model","768e6a8c":"# LSTM regression model","90093dcc":"# Necessary libraries","c05f8725":"# Introduction to LSTM","0cd81ed5":"# PyTorch Dataset","7b6a1f5c":"# Create Submission File","d9c29fa4":"You can see that using the glove embedding vectors we achieve a better rmse value than using random embedding vectors. You can tweak some parameters in order to get much better performance.","231b14b2":"What sets language models apart from conventional neural networks is their dependency on context. Conventional feed-forward networks assume inputs to be independent of one another. For NLP, we need a mechanism to be able to use sequential information from previous inputs to determine the current output. Recurrent Neural Networks (RNNs) tackle this problem by having loops, allowing information to persist through the network.\n\n![image.png](attachment:896a386f-e304-4129-810a-ef19d562b6f8.png)\n\nLong Short Term Memory networks (LSTM) are a special kind of RNN, which are capable of learning long-term dependencies. They do so by maintaining an internal memory state called the \u201ccell state\u201d and have regulators called \u201cgates\u201d to control the flow of information inside each LSTM unit.\nfor more information about those models you can visit this link:\nhttps:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\nSo in this notebook we'll perform a regression using LSTM model (using PyTorch) applied on the excerpt texts of our dataset and explore the results","a05b92e6":"# Load Glove vectors","bfeb48ee":"# Preprocessing","79447806":"# Dataset","fca41f98":"If you find the notebook helpful and interesting please upvote it ! ","c592e174":"# PyTorch training loop","42caf227":"# LSTM Model with Golve vectors"}}