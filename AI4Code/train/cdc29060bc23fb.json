{"cell_type":{"e33d98d1":"code","276de884":"code","00916a9b":"code","b9fbcf9b":"code","86d9e919":"code","6d312fb5":"code","841e1989":"code","80a38d5d":"code","88daffd5":"code","67e68f1c":"code","51cf443e":"code","ba792941":"code","57f786cf":"code","b9e4286f":"code","35b56d58":"code","2012cdcf":"code","01e35386":"code","0d8f38ef":"code","40742336":"code","a5adec19":"code","d37c6c50":"code","7f5e9c69":"code","1c260be6":"code","0d91c4d6":"code","bac1d01f":"code","7476f7cc":"code","a3950537":"code","402a9640":"code","08f4646b":"code","cb039f2e":"code","f97c7ce5":"code","2b063cf5":"code","3c4dfdf2":"code","f98d542d":"code","6178b4ed":"markdown","34fa19e6":"markdown","9e6a0df8":"markdown","d3abf65e":"markdown","82f3f06e":"markdown","d08bcfad":"markdown","3f2d7d3e":"markdown","7940191b":"markdown","3f42c4e6":"markdown","396d98b2":"markdown","7e59d441":"markdown","f284cb59":"markdown","dec3f591":"markdown","ac000948":"markdown","e56741b4":"markdown","6ee3e303":"markdown","f77205b0":"markdown","5b0eb26b":"markdown","f03343c5":"markdown","f7406752":"markdown","df969d74":"markdown","cb36424f":"markdown","4097f1d1":"markdown","3e165155":"markdown"},"source":{"e33d98d1":"from absl import logging\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Add,Concatenate,Conv2D,Input,Lambda,LeakyReLU,MaxPool2D,UpSampling2D,ZeroPadding2D,BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.losses import binary_crossentropy, sparse_categorical_crossentropy\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,TensorBoard\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nfrom IPython.display import display\nfrom seaborn import color_palette\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport tensorflow as tf\nimport time\nfrom tqdm import tqdm\nimport datetime\n# from my_yolo import *","276de884":"images = os.listdir(\"..\/input\/malaria-bounding-boxes\/malaria\/images\") # read the images data where all images are present\ntrain = pd.read_json(\"..\/input\/malaria-bounding-boxes\/malaria\/training.json\") # read the train json file where info about all images\n# are given including bounding box information.\ntest = pd.read_json(\"..\/input\/malaria-bounding-boxes\/malaria\/test.json\") # read the test json file.","00916a9b":"# here we have store train and test image in different list\ntrain_image = [] # train images stored here\ntest_image = [] # test images stored here\nfor df in train['image']:\n    train_image.append(df['pathname'].split('\/')[-1])\nfor df in test['image']:\n    test_image.append(df['pathname'].split('\/')[-1])\nprint(\"Train image length is -> \",len(train_image))\nprint(\"Train image :\",train_image[:5])\nprint(\"Test image length is -> \",len(test_image))\nprint(\"Test image :\",test_image[:5])\n\nclass_dict = {'red blood cell':0,'trophozoite': 1, 'schizont': 2, 'difficult': 3, 'ring': 4,\n              'leukocyte': 5, 'gametocyte': 6}\n\n#class_dict = {'uninfected': 0, 'infected': 1}\n# we have defined the dictionary for labels or classes.","b9fbcf9b":"\nimg_info = [] # containg information of bounding box\nfor row in range(train.shape[0]):\n    path = '..\/input\/malaria-bounding-boxes\/malaria' + train.iloc[row]['image']['pathname']\n    for info in train.iloc[row]['objects']:\n        category = info['category'] #  category here refer to label \n        bounding_box = info['bounding_box'] # containing image info like r_min,r_max,c_min,c_max\n        box_dim = bounding_box['maximum']['r'],bounding_box['maximum']['c'],bounding_box['minimum']['r'],bounding_box['minimum']['c'],category,path\n        img_info.append(box_dim)\nfor row in range(test.shape[0]):\n    path = '..\/input\/malaria-bounding-boxes\/malaria' + test.iloc[row]['image']['pathname']\n    for info in test.iloc[row]['objects']:\n        category = info['category'] #  category here refer to label \n        bounding_box = info['bounding_box'] # containing image info like r_min,r_max,c_min,c_max\n        box_dim = bounding_box['maximum']['r'],bounding_box['maximum']['c'],bounding_box['minimum']['r'],bounding_box['minimum']['c'],category,path\n        img_info.append(box_dim)\n# storing each images info in train_img_info list\n\nimport csv\nheaders = [ 'max_r', 'max_c', 'min_r','min_c', 'category','img_path']\nwith open('all_data.csv', 'w') as f:\n        wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n        wr.writerow(headers) \n        wr.writerows(img_info)\n# the above code snippet is taking each data and writing it into train csv file.\n\n\n# after seperating out each cell data in images we have got 80113 cells info.\n# here max_r,min_r,max_c,min_c are dimension for each cell which will help us out while cropping that cell in an image.\n# so each image contain various malaria cells including all 6 types of cells.\n# As we will see in further analysis that RBC has outnumbered other cell categories \n\ndf = pd.read_csv(\"all_data.csv\")\n\nprint(\"This malaria image data contains rows {} and columns {}\".format(df.shape[0],df.shape[1]))\ndf['label'] = df['category'].map(class_dict) \n\ndata = df\n","86d9e919":"# this is a class distribution \nfig,ax = plt.subplots(figsize=(24,12))\ntemp = data['category'].value_counts().values\nsns.barplot(x = data['category'].value_counts().index.tolist(),y = data['category'].value_counts().values.tolist())\nfor i,text in enumerate(data['category'].value_counts(normalize = True).items()):\n    ax.annotate(\"{:.2f}%\".format(text[1]*100),xy = (i-0.10,temp[i]+1000),fontsize = 20)\nplt.xlabel(\"Malaria Cells Classes\",fontsize = 20)\nplt.ylabel(\"Frequency\",fontsize = 20)\nplt.suptitle(\"Malaria Imbalanced Class Distribution\",fontsize = 24)\nplt.show()\n","6d312fb5":"plt.figure(figsize = (24,10))\ncount=1\nfor row in list(np.random.choice(train.shape[0],6)): # taking 6 random index inputs and displaying the image\n    path = '..\/input\/malaria-bounding-boxes\/malaria' + train.iloc[row]['image']['pathname']\n    im = Image.open(path)\n    plt.subplot(2,3,count)\n    plt.imshow(im)\n    plt.grid(False)\n    count+=1\n","841e1989":"from matplotlib import patches\nfig = plt.figure(figsize = (24,8))\n\nax = fig.add_axes([0,0,1,1])\nrow = np.random.choice(train.shape[0],1)[0]\npath = '..\/input\/malaria-bounding-boxes\/malaria' + train.iloc[row]['image']['pathname']\nimage = plt.imread(path)\nplt.imshow(image)\n\nfor _,row in data[data.img_path==path].iterrows():\n    xmin = row.min_c # x1\n    xmax = row.max_c # x2\n    ymin = row.min_r # y1\n    ymax = row.max_r # y2\n\n    l = xmax-xmin # x2-x1\n    b = ymax-ymin # y2-y1\n\n    if row.category=='red blood cell':\n        color='r' # bounding box color for RBC\n        ax.annotate('rbc',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='trophozoite':\n        color='g' # bounding box color for trophozoite\n        ax.annotate('trophozoite',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='difficult':\n        color='b'# bounding box color for difficult\n        ax.annotate('difficult',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='ring':\n        color='r' # bounding box color for ring\n        ax.annotate('ring',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='schizont':\n        color='r' # bounding box color for schizont\n        ax.annotate('schizont',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='gametocyte':\n        color='o' # bounding box color for gametocyte\n        ax.annotate('gametocyte',xy=(xmax-40,ymin+20)) # bonding box dimension\n    elif row.category=='leukocyte':\n        color='b' # bounding box color for Leukocyte\n        ax.annotate('leukocyte',xy=(xmax-40,ymin+20)) # bonding box dimension\n\n    rect = patches.Rectangle((xmin,ymin), l, b, edgecolor = color, facecolor = 'none')\n    ax.add_patch(rect)\n\n\n# rbc and leukocyte are uninfected cells\n# and other four classes trophozoite,difficult,ring,schizont are infected cells.\n","80a38d5d":"yolo_anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61), (62, 45),\n                         (59, 119), (116, 90), (156, 198), (373, 326)],\n                        np.float32) \/ 416\nyolo_anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n\nsize = 416 # size of resize image\nbatch_size = 8\nyolo_max_boxes = 223 # maximum yolo boxes predicted per image i.e output shape: (batch_size,223,4)\n\n# here \"yolo_max_boxes\" means information of bounding box dimension of number of object contains in an image \n# It's just like padding if we have image containing 73 objects then rest(223-73 = 150) will be padded with [0,0,0,0]\n# There are 4 index in a list i.e xcent,ycent,width,height\n\n\nyolo_iou_threshold = 0.5 # IOU threshold score \nyolo_score_threshold = 0.4 # objectness threshold score\nlearning_rate = 1e-4 # learning rate\nnum_classes = 7 # num of category in our dataset\nepochs = 100 # epochs run to fine tune our model","88daffd5":"## we are going to use functional API, as sequential API can be somewhat messy for such architecture implementation.\n\n# below is simple implementation of each convolutional Layer in Darknet. We have created\n# one function for simple convolutional layer and one function for residual connection\n\ndef DarknetConv(x, filters, size, strides=1, batch_norm=True):\n    if strides == 1: # if stride is 1 then we apply 'same' padding\n        padding = 'same' # i.e which output same shape as input if we input = (416,416) then output will also be (416,416)\n    else:\n        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding \n        padding = 'valid' \n    x = Conv2D(filters=filters, kernel_size=size,\n               strides=strides, padding=padding,\n               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)\n    if batch_norm:\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.1)(x) # by default alpha is 0.3\n    return x\n\n# below is residual connection layer function\ndef DarknetResidual(x, filters):\n    prev = x # storing input in prev variable \n    x = DarknetConv(x, filters \/\/ 2, 1) \n    x = DarknetConv(x, filters, 3)\n    x = Add()([prev, x]) # residual connection\n    return x\n\n# this is our real Darknetblock function calling above 2 fucntions \ndef DarknetBlock(x, filters, blocks):\n    x = DarknetConv(x, filters, 3, strides=2) \n    for _ in range(blocks):\n        x = DarknetResidual(x, filters)\n    return x\n\n# the below x_36,x_61 and x are outputs which we will use in next function for upsampling and detecting objects.\n# x : used for detecting large objects i.e having grid_size of (13,13)\n# x_61 : used for detecting medium size objects i.e having grid_size of (26,26)\n# x_36 : used for detecting small objects i.e having grid_size of (52,52)\n\ndef Darknet(name=None):\n    x = inputs = Input([None, None, 3])\n    x = DarknetConv(x, 32, 3)\n    x = DarknetBlock(x, 64, 1)\n    x = DarknetBlock(x, 128, 2)  # skip connection\n    x = x_36 = DarknetBlock(x, 256, 8)  # skip connection\n    x = x_61 = DarknetBlock(x, 512, 8)\n    x = DarknetBlock(x, 1024, 4) # last layer detecting bounding box dimension (tx,ty,bx,by)\n    return tf.keras.Model(inputs, (x_36, x_61, x), name=name)","67e68f1c":"def YoloConv(filters, name=None):\n    def yolo_conv(x_in):\n        if isinstance(x_in, tuple):\n            inputs = Input(x_in[0].shape[1:]), Input(x_in[1].shape[1:])\n            x, x_skip = inputs\n\n            # concat with skip connection\n            x = DarknetConv(x, filters, 1)\n            # upsampling of a layer\n            x = UpSampling2D(2)(x)\n            # concatenation of skip connection result and last output result\n            x = Concatenate()([x, x_skip])\n        else:\n            x = inputs = Input(x_in.shape[1:])\n        \n        x = DarknetConv(x, filters, 1)\n        x = DarknetConv(x, filters * 2, 3)\n        x = DarknetConv(x, filters, 1)\n        x = DarknetConv(x, filters * 2, 3)\n        x = DarknetConv(x, filters, 1)\n        return Model(inputs, x, name=name)(x_in)\n    return yolo_conv","51cf443e":"# below function is yolov3 output i.e last 3 convolution layer which will predict anchors \n# objectness score, tx,ty,tw,th which we will use in some operation to learn xcent,ycent and width,height.\n# which is clearly mention in https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf paper.\n\n\ndef YoloOutput(filters, anchors, classes, name=None):\n    def yolo_output(x_in):\n        x = inputs = Input(x_in.shape[1:]) # this is an input shape excluded with batch size.\n        x = DarknetConv(x, filters * 2, 3) # Darkconv is a fn implemented above which is internally calling\n        \n        # DarknetBlock, which internally calling DarknetConv,DarknetResidual fn.\n        # It's a simple nested functions which we have learned in python or c++.\n\n        x = DarknetConv(x, anchors * (classes + 5), 1, batch_norm=False)\n        x = Lambda(lambda x: tf.reshape(x, (-1, tf.shape(x)[1], tf.shape(x)[2],\n                                            anchors, classes + 5)))(x)\n        # x is reshaped into (None, grid_size, grid_size, anchors, (x,y,w,h,objectness score,..classes))\n        return tf.keras.Model(inputs, x, name=name)(x_in)\n    return yolo_output\n\n\n# for more help in understanding how yolov3 works go to https:\/\/pylessons.com\/YOLOv3-introduction\/\n\n# this fn helps in predicting for each of predicted yolo boxes objectness scores, x,y,w,h.\n# Remember last layer is a sigmoid layer and not a softmax layer.\n\ndef yolo_boxes(pred, anchors, classes):\n    # pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...classes))\n    grid_size = tf.shape(pred)[1]\n    box_xy, box_wh, objectness, class_probs = tf.split(\n        pred, (2, 2, 1, classes), axis=-1)\n    # objectness : it's means whether there is any object in a predicted box\n    # class_probs : it's a probability of a class given object is there i.e P(Pc|object)\n\n    box_xy = tf.sigmoid(box_xy) \n    objectness = tf.sigmoid(objectness)\n    class_probs = tf.sigmoid(class_probs)\n    pred_box = tf.concat((box_xy, box_wh), axis=-1)  # original xywh for loss\n\n    # !!! grid[x][y] == (y, x)\n    grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n\n    # Below is the operation of predicting true dimension in an image\n    # bx=\u03c3(tx)+cx\n    # by=\u03c3(ty)+cy\n    # bw=pw.e^tw\n    # bh=ph.e^th\n\n    box_xy = (box_xy + tf.cast(grid, tf.float32)) \/ \\\n        tf.cast(grid_size, tf.float32)\n    box_wh = tf.exp(box_wh) * anchors\n\n    box_x1y1 = box_xy - box_wh \/ 2 \n    box_x2y2 = box_xy + box_wh \/ 2\n    bbox = tf.concat([box_x1y1, box_x2y2], axis=-1)\n\n    return bbox, objectness, class_probs, pred_box\n\n# below nms function will suppressed those bounding box result which is not effective on the basis of\n# yolo_maximum_threshold(IOU) and objectness score.\n\ndef yolo_nms(outputs, anchors, masks, classes):\n    # boxes, confidence scores(objectness scores), class probabilities\n\n    b, c, t = [], [], []\n\n    # iterating through each outputs predicted by model\n    for o in outputs:\n        b.append(tf.reshape(o[0], (tf.shape(o[0])[0], -1, tf.shape(o[0])[-1])))\n        c.append(tf.reshape(o[1], (tf.shape(o[1])[0], -1, tf.shape(o[1])[-1])))\n        t.append(tf.reshape(o[2], (tf.shape(o[2])[0], -1, tf.shape(o[2])[-1])))\n\n    bbox = tf.concat(b, axis=1)\n    confidence = tf.concat(c, axis=1)\n    class_probs = tf.concat(t, axis=1)\n\n    scores = confidence * class_probs # this is P(Pc|objectness score) value\n\n    # this is beautifull implementation of nms code\n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/image\/combined_non_max_suppression\n\n    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n        scores=tf.reshape(\n            scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),\n        max_output_size_per_class = yolo_max_boxes, # here it is 223, define above \n        max_total_size = yolo_max_boxes,\n        iou_threshold = yolo_iou_threshold, # threshold for filtering the boxes\n        score_threshold = yolo_score_threshold # threshold for objectness score below which we ignore that bounding box\n    )\n\n    return boxes, scores, classes, valid_detections\n\n# this is main function for our backbone or darknet archietcture which calls above implemented functions\n# \ndef YoloV3(size=None, channels=3, anchors=yolo_anchors,\n           masks=yolo_anchor_masks, classes=80, training=False):\n    x = inputs = Input([size, size, channels], name='input') # input of an image\n\n    x_36, x_61, x = Darknet(name='yolo_darknet')(x) # backbone networks 3 outputs w.r.t to each grid size\n    # till here darknet network\n\n    # from below it's a Feature Pyramind Network with lateral connections\n    # for 13*13 grid size output\n    x = YoloConv(512, name='yolo_conv_0')(x) \n    output_0 = YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n    \n    # for 26*26 grid size output\n    x = YoloConv(256, name='yolo_conv_1')((x, x_61))\n    output_1 = YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n\n    # for 52*52 grid size output\n    x = YoloConv(128, name='yolo_conv_2')((x, x_36))\n    output_2 = YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n\n    if training:\n        return Model(inputs, (output_0, output_1, output_2), name='yolov3')\n\n    # for 13*13 grid size output\n    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n                     name='yolo_boxes_0')(output_0)\n    # for 26*26 grid size output\n    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n                     name='yolo_boxes_1')(output_1)\n    # for 52*52 grid size output\n    boxes_2 = Lambda(lambda x: yolo_boxes(x, anchors[masks[2]], classes),\n                     name='yolo_boxes_2')(output_2)\n    #  after combining boxes from various scales we have total 10,647 boxes which is too large\n    # so to remove invalid boxes we use non_maximum_suppression \n\n    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n                     name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n\n    return Model(inputs, outputs, name='yolov3')\n","ba792941":"# below is a yolo loss function (both categorical loss function and mean square error)\n# we have calculated each individual loss and summed at last.\ndef YoloLoss(anchors, classes=80, ignore_thresh=0.5):\n    def yolo_loss(y_true, y_pred):\n        # 1. transform all pred outputs\n        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n            y_pred, anchors, classes)\n        pred_xy = pred_xywh[..., 0:2]\n        pred_wh = pred_xywh[..., 2:4]\n\n        # 2. transform all true outputs\n        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n        \n        true_box, true_obj, true_class_idx = tf.split(\n            y_true, (4, 1, 1), axis=-1) \n\n        # the above split function split (x1,y1,x2...cls) into (x1,y1),(x2,y2),(obj),(cls)\n        # the 4,1,1 is a length at which it split\n\n        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) \/ 2 # finding center (Xcen,Ycen)\n        true_wh = true_box[..., 2:4] - true_box[..., 0:2] # width and height\n\n        # give higher weights to small boxes\n        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n\n        # 3. inverting the pred box equations\n        grid_size = tf.shape(y_true)[1]\n        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n        true_xy = true_xy * tf.cast(grid_size, tf.float32) - \\\n            tf.cast(grid, tf.float32) # this code snippet giving us at which point each cell is starting and ending\n            # in resize image of 416 * 416 \n            # suppose there 13*13 = 169 cells , so every cell we will have starting and ending point\n        true_wh = tf.math.log(true_wh \/ anchors) \n        # YOLO doesn\u2019t predict the absolute coordinates of the bounding box\u2019s center\n        true_wh = tf.where(tf.math.is_inf(true_wh),\n                           tf.zeros_like(true_wh), true_wh)\n\n        # 4. calculate all masks\n        obj_mask = tf.squeeze(true_obj, -1)\n        \n        # ignore false positive when iou is over threshold\n        best_iou = tf.map_fn(\n            lambda x: tf.reduce_max(broadcast_iou(x[0], tf.boolean_mask(\n                x[1], tf.cast(x[2], tf.bool))), axis=-1),\n            (pred_box, true_box, obj_mask),\n            tf.float32)\n        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n\n        # 5. calculate all losses\n        xy_loss = obj_mask * box_loss_scale * \\\n            tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n        wh_loss = obj_mask * box_loss_scale * \\\n            tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n        \n        obj_loss = binary_crossentropy(true_obj, pred_obj)\n        # obj_loss = obj_mask * obj_loss + \\\n        #     (1 - obj_mask) * ignore_mask * obj_loss\n\n        # below we implemented focal loss so that all the false negative weights can be down weight\n        # and our model can learn from actual loss.\n        \n        # https:\/\/leimao.github.io\/blog\/Focal-Loss-Explained\/\n        \n        alpha = 0.85 # focal loss hyperparameter\n        conf_focal = tf.pow(obj_mask-tf.squeeze(tf.sigmoid(pred_obj),-1),2)\n        obj_loss = conf_focal*((1-alpha)*obj_mask*obj_loss + alpha*(1-obj_mask)*ignore_mask*obj_loss)  # batch * grid * grid * anchors_per_scale\n\n        # TODO: use binary_crossentropy instead\n        class_loss = obj_mask * sparse_categorical_crossentropy(\n            true_class_idx, pred_class)\n\n        # 6. sum over (batch, gridx, gridy, anchors) => (batch, 1)\n        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n\n        return xy_loss + wh_loss + obj_loss + class_loss\n    return yolo_loss","57f786cf":"YOLOV3_LAYER_LIST = ['yolo_darknet','yolo_conv_0','yolo_output_0','yolo_conv_1',\n                     'yolo_output_1','yolo_conv_2','yolo_output_2',]\n\n# Below function will help in load darknet weights which is already saved.\n# it can be yolov3 weights or fine tune model weights\n\ndef load_darknet_weights(model, weights_file, tiny=False):\n    wf = open(weights_file, 'rb') # reading weights file\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    layers = YOLOV3_LAYER_LIST\n\n    # iterating through all layers define in above yolov3_layers_list\n    for layer_name in layers:\n        # for eg if there is one layer darknet then there is many sub layers inside it's network\n        sub_model = model.get_layer(layer_name)\n        for i, layer in enumerate(sub_model.layers):\n            if not layer.name.startswith('conv2d'): \n                continue\n            batch_norm = None\n            if i + 1 < len(sub_model.layers) and \\\n                    sub_model.layers[i + 1].name.startswith('batch_norm'):\n                batch_norm = sub_model.layers[i + 1]\n\n            logging.info(\"{}\/{} {}\".format(\n                sub_model.name, layer.name, 'bn' if batch_norm else 'bias'))\n\n            filters = layer.filters\n            size = layer.kernel_size[0]\n            in_dim = layer.input_shape[-1]\n\n            if batch_norm is None:\n                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n            else:\n                # darknet [beta, gamma, mean, variance]\n                bn_weights = np.fromfile(\n                    wf, dtype=np.float32, count=4 * filters)\n                # tf [gamma, beta, mean, variance]\n                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n\n            # darknet shape (out_dim, in_dim, height, width)\n            conv_shape = (filters, in_dim, size, size)\n            conv_weights = np.fromfile(\n                wf, dtype=np.float32, count=np.product(conv_shape))\n            # tf shape (height, width, in_dim, out_dim)\n            conv_weights = conv_weights.reshape(\n                conv_shape).transpose([2, 3, 1, 0])\n\n            if batch_norm is None:\n                layer.set_weights([conv_weights, conv_bias])\n            else:\n                layer.set_weights([conv_weights])\n                batch_norm.set_weights(bn_weights)\n\n    assert len(wf.read()) == 0, 'failed to read all data'\n    wf.close()\n\n# below function will calculate Intersection over Union ratio which help in above nms function.\n# it's implemented in such a way which is self-explanatory.\n\ndef broadcast_iou(box_1, box_2):\n    # box_1: (..., (x1, y1, x2, y2))\n    # box_2: (N, (x1, y1, x2, y2))\n\n    # broadcast boxes\n    box_1 = tf.expand_dims(box_1, -2)\n    box_2 = tf.expand_dims(box_2, 0)\n    # new_shape: (..., N, (x1, y1, x2, y2))\n    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_1), tf.shape(box_2))\n    box_1 = tf.broadcast_to(box_1, new_shape) # it will change the shape of box into new_shape given\n    box_2 = tf.broadcast_to(box_2, new_shape)\n\n    # in below code we are finding intersection box width and height through which we will find intersection area.\n    # and this we are finding all boxes \n    int_w = tf.maximum(tf.minimum(box_1[..., 2], box_2[..., 2]) -\n                       tf.maximum(box_1[..., 0], box_2[..., 0]), 0) \n    int_h = tf.maximum(tf.minimum(box_1[..., 3], box_2[..., 3]) -\n                       tf.maximum(box_1[..., 1], box_2[..., 1]), 0) \n    int_area = int_w * int_h  # area of intersection\n    box_1_area = (box_1[..., 2] - box_1[..., 0]) * \\\n        (box_1[..., 3] - box_1[..., 1]) # this box_1_area contains all boxes area predicted in an image\n    box_2_area = (box_2[..., 2] - box_2[..., 0]) * \\\n        (box_2[..., 3] - box_2[..., 1]) # this box2_area is our ground truth box area\n\n    # Formula: Union(A,B) = A + B - Inter(A,B)\n    return int_area \/ (box_1_area + box_2_area - int_area)\n\n# below function will help in comparing the results when we visualize it after we have pre-trained our model.\n\ndef draw_outputs(img, outputs, class_names,no_rbc = True):\n    boxes, objectness, classes, nums = outputs # predicted outputs\n    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]\n    wh = np.flip(img.shape[0:2])\n    # iterate through each valid predictions\n    for i in range(nums):\n        if no_rbc:\n            if classes[i]==0:\n                continue \n        x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32)) # \n        x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2) # it will create a rectangle box around object.\n        img = cv2.putText(img, '{} {:.4f}'.format(\n            class_names[int(classes[i])], objectness[i]),\n            x1y1, cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2) # help it displaying text of \n            # predicted class with objectness score*class_probabilities\n    return img\n\n# below this function helps in freeze the layers i.e making it non-trainable\n# here in our case we pre-training darknet layer.\n\ndef freeze_all(model, frozen=True):\n    model.trainable = not frozen\n    if isinstance(model, tf.keras.Model):\n        for l in model.layers:\n            freeze_all(l, frozen)","b9e4286f":"%%time\n# below function parse the created csv file and structured into yolo acceptable format\n# data : dataframe which was created above\n# class_dict: dictionary containing classes with index(created above)\n# size : Input size of each image (here by default is 416)\n# path : path to images data folder\n\ndef parse_dataset(data,class_dict,size,image,path,yolo_max_boxes,count=0):\n    X = []\n    Y = []\n    # iterating over all images in a train dataset\n    for img in tqdm(image):\n        x_train = Image.open(path+img) # reading image\n        width,height = x_train.size # storing actual width and height so that we can later scale it\n        x_train = x_train.resize((size,size)) # resizing \n        x_train = np.array(x_train)\n        temp_data = []\n        # ierating over dataset having info about objects in an image\n        for _,row in data[data['img_path']==path+img].iterrows():\n            xmin = row.min_c\/width\n            xmax = row.max_c\/width\n            ymin = row.min_r\/height\n            ymax = row.max_r\/height\n            cls = class_dict[row.category]\n            temp_data.append([xmin,ymin,xmax,ymax,cls])\n        temp_data = temp_data+[[0,0,0,0,0]]*(yolo_max_boxes-len(temp_data)) # it's like padding \n        #return(temp)\n        Y.append(temp_data)\n        X.append(x_train)\n    return(np.array(X),np.stack(np.array(Y)))\n\n# transforming each image and normalizing it in range [0,1]\ndef transform_images(x,size):\n    x = tf.image.resize(x,(size,size))\n    x = x\/255.0\n    return(x)","35b56d58":"#https:\/\/www.tensorflow.org\/guide\/function \n# https:\/\/towardsdatascience.com\/tensorflow-2-0-tf-function-and-autograph-af2b974cf4f7 \n\n\n@tf.function\ndef transform_targets_for_output(y_true, grid_size, anchor_idxs):\n    # y_true: (N, boxes, (x1, y1, x2, y2, class, best_anchor))\n    N = tf.shape(y_true)[0]\n\n    # y_true_out: (N, grid, grid, anchors, [x, y, w, h, obj, class])\n    y_true_out = tf.zeros(\n        (N, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6))\n\n    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n\n    indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n    idx = 0\n\n    # below iteration change the values and update it to the format which acceptable by yolov3.\n    for i in tf.range(N):\n        for j in tf.range(tf.shape(y_true)[1]):\n            if tf.equal(y_true[i][j][2], 0): \n                continue\n            anchor_eq = tf.equal(\n                anchor_idxs, tf.cast(y_true[i][j][5], tf.int32))\n\n            if tf.reduce_any(anchor_eq):\n                box = y_true[i][j][0:4] #(x1,y1,x2,y2)\n                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) \/ 2 # ((x1+x2)\/2,(y1+y2)\/2)\n\n                # which is (Xcenter,Ycenter)\n\n                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n                grid_xy = tf.cast(box_xy \/\/ (1\/grid_size), tf.int32) # multiplying it by grid_size\n\n                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n                indexes = indexes.write(\n                    idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]]) \n                updates = updates.write(\n                    idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]])\n                idx += 1\n\n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/tensor_scatter_nd_update\n    # below function helps in updating passed in y_true_out by updating values at passed in indexes\n    # with updates values.\n    return tf.tensor_scatter_nd_update(\n        y_true_out, indexes.stack(), updates.stack())\n","2012cdcf":"def transform_targets(y_train, anchors, anchor_masks, size):\n    y_outs = []\n    grid_size = size \/\/ 32 # suppose we input 416 size then grid size is 416\/\/32 = 13\n\n    # calculate anchor index for true boxes\n    anchors = tf.cast(anchors, tf.float32) # casting every anchors to float\n    anchor_area = anchors[..., 0] * anchors[..., 1] # calculating the area of anchors\n    box_wh = y_train[..., 2:4] - y_train[..., 0:2] # here we are peforming xmax-xmin,ymax-ymin using vectors\n    box_wh = tf.tile(tf.expand_dims(box_wh, -2),\n                     (1, 1, tf.shape(anchors)[0], 1))\n    box_area = box_wh[..., 0] * box_wh[..., 1] # these are our Ground Truth Box Area\n    intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * \\\n        tf.minimum(box_wh[..., 1], anchors[..., 1]) # here we tring to get IOU area \n    iou = intersection \/ (box_area + anchor_area - intersection) # simple operation of Intersection\/Union\n    anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32) # storing those anchor index which has highest IOU number\n    anchor_idx = tf.expand_dims(anchor_idx, axis=-1)\n\n    y_train = tf.concat([y_train, anchor_idx], axis=-1)\n\n    for anchor_idxs in anchor_masks:\n        y_outs.append(transform_targets_for_output(\n            y_train, grid_size, anchor_idxs))\n        grid_size *= 2 # here we are calling the above function for 13*13 grid then, 26*26 grid then, 52*52 grid.\n\n    return tuple(y_outs)","01e35386":"%%time\n# below we are loading or converting yolov3 weights in a format which we have defined above.\nyolo = YoloV3(classes=80)\nyolo.summary()\n#load_darknet_weights(yolo,\"\/content\/drive\/My Drive\/yolov3.weights\", False)\n#yolo.save_weights(\"\/content\/drive\/My Drive\/yolov3_checkpoint\/yolov3.tf\")\nyolo.load_weights(\"..\/input\/yolov3-tf-pretrained\/yolov3.tf\")","0d8f38ef":"x,y= parse_dataset(\n   df,class_dict,size,train_image[:],'..\/input\/malaria-bounding-boxes\/malaria\/images\/',223) \n# df = data\nx = x.astype(np.float32)\ny = y.astype(np.float32)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x,y))","40742336":"# buffer_size helps to randomized the data for training\ntrain_dataset = train_dataset.shuffle(buffer_size=64)\n# this is a batch_size that we are using to train our model.\ntrain_dataset = train_dataset.batch(8)\n# after taking batch of images we map them to transform into particular scale and particular format\n# which is acceptable by yolov3 for training. \ntrain_dataset = train_dataset.map(lambda x, y: (transform_images(x, size),\n                                  transform_targets(y, yolo_anchors, yolo_anchor_masks, size)))\n# prefetch try to spped up thinks by prefetching next batch of images and performing operation on it.\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","a5adec19":"# parsing valid dataset \nx,y= parse_dataset(df,class_dict,size,test_image[:],'..\/input\/malaria-bounding-boxes\/malaria\/images\/',223) \nx = x.astype(np.float32)\ny = y.astype(np.float32)\nval_dataset = tf.data.Dataset.from_tensor_slices((x,y))","d37c6c50":"val_dataset = val_dataset.shuffle(buffer_size=16)\nval_dataset = val_dataset.batch(8)\nval_dataset = val_dataset.map(\n    lambda x, y: (transform_images(x, size),\n    transform_targets(y, yolo_anchors, yolo_anchor_masks, size)))\n\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","7f5e9c69":"# initializing the network\nmodel = YoloV3(size, training=True, classes=num_classes)\nanchors = yolo_anchors\nanchor_masks = yolo_anchor_masks","1c260be6":"%%time\n\nmodel_pretrained = YoloV3(size, training=True, classes=80)\nmodel_pretrained.load_weights(\"..\/input\/yolov3-tf-pretrained\/yolov3.tf\")\n # first loading yolov3 weights in out pretrained model\n# getting weights of yolo_darknet as we are going to fine tuning we are freezing all layers in the\n# darknet. To make it non-trainable.\nmodel.get_layer('yolo_darknet').set_weights(\nmodel_pretrained.get_layer('yolo_darknet').get_weights())\nfreeze_all(model.get_layer('yolo_darknet'))","0d91c4d6":" # we are using graph mode of tensorflow so that we can use our own Gradient Tape\noptimizer = tf.keras.optimizers.Adam(lr=1e-4) # Adam optimizers\nloss = [YoloLoss(anchors[mask], classes=num_classes) # customized yolo loss define above in utils.\n            for mask in anchor_masks]","bac1d01f":"import datetime\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n# train and testd log directory for tensorboard where we going to write ouw own custom summary.\n\ntrain_log_dir = '416_logs\/gradient_tape\/' + current_time + '\/train' # train dir path\ntest_log_dir = '416_logs\/gradient_tape\/' + current_time + '\/test' # test dir path\n#all_log_dir = 'logs\/gradient_tape\/' + current_time + '\/all'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir) # train writer\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir) # test writer\n#all_summary_writer = tf.summary.create_file_writer(all_log_dir)","7476f7cc":"# Eager mode is great for debugging\n# Non eager graph mode is recommended for real training\n\navg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\navg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n\n# below is a checpoint manager which will help in recording our loss of last 3 checkpoints\n# If suppose your training stop for some reason it can reload latest checkpoint and start from there\n# this really helps in synching of checpoints and tensorboard\n# step variable below helps in recording steps\n\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model = model)\nmanager = tf.train.CheckpointManager(ckpt, '416_checkpoints\/yolov3_train\/tf_ckpts', max_to_keep=3)\nckpt.restore(manager.latest_checkpoint)\n\nif manager.latest_checkpoint:\n  # if there is a checkpoint in a file it will restore it.\n    print(\"Restored from {}\".format(manager.latest_checkpoint))\n    start = ckpt.step.numpy() \nelse:\n    print(\"Initializing from scratch.\")\n    start = 0\n\n  \nfor epoch in range(start, epochs+1):\n    \n    # training dataset\n    # if epoch%22==0 and epoch!=0:\n    #   lr = float(input(\"Input Learning Rate\"))\n    #   optimizer = tf.keras.optimizers.Adam(lr=lr)\n    for batch, (images, labels) in enumerate(train_dataset):\n        \n        # we are using tf.GradientTape to record every values at each step and later use it to\n        # calculate our gradient and losses.\n        # https:\/\/stackoverflow.com\/questions\/53953099\/what-is-the-purpose-of-the-tensorflow-gradient-tape\n        with tf.GradientTape() as tape:\n            outputs = model(images, training=True)\n            regularization_loss = tf.reduce_sum(model.losses)\n            pred_loss = []\n            for output, label, loss_fn in zip(outputs, labels, loss):\n                pred_loss.append(loss_fn(label, output))\n            total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n        # calculating grads over trainable parameters\n        grads = tape.gradient(total_loss, model.trainable_variables) # calculating loss after each batch \n        optimizer.apply_gradients(\n            zip(grads, model.trainable_variables)) # then appliying gradient optimization on the loss to fine tune the weights \n\n        # writing summary to train file(for tensorboard)\n\n        with train_summary_writer.as_default():\n            tf.summary.scalar('avg_loss', total_loss.numpy(), step=epoch)\n        # to update avg loss after each batch.\n        avg_loss.update_state(total_loss)\n    \n    # testing datasets\n    for batch, (images, labels) in enumerate(val_dataset):\n        outputs = model(images)\n        regularization_loss = tf.reduce_sum(model.losses)\n        pred_loss = []\n        for output, label, loss_fn in zip(outputs, labels, loss):\n            pred_loss.append(loss_fn(label, output))\n        total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n\n        # writing summary to test file(for tensorboard)\n\n        with test_summary_writer.as_default():\n            tf.summary.scalar('avg_val_loss', total_loss.numpy(), step=epoch)\n        avg_val_loss.update_state(total_loss)\n\n    # print result \n    print(\"{}, train: {}, val: {}\".format(\n        epoch,\n        avg_loss.result().numpy(),\n        avg_val_loss.result().numpy()))\n    \n    ckpt.step.assign_add(1)\n    if int(ckpt.step) % 5 == 0:\n        save_path = manager.save()\n        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n\n    avg_loss.reset_states()\n    avg_val_loss.reset_states()","a3950537":"# running tensorboard \n# https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_in_notebooks\n%load_ext tensorboard\n%tensorboard --logdir 416_logs","402a9640":"# below function draws grounf truth boxes\nclass_names = {j:i for i,j in class_dict.items()}\ndef draw_gt_outputs(path, data, class_names,no_rbc = True):\n    img = plt.imread(\"malaria\/images\/\"+path)\n    wh = np.flip(img.shape[0:2])\n    # iterate through each valid predictions\n    nums,classes = [],[]\n    for _,row in data[data.img_path==\"malaria\/images\/\"+path].iterrows():\n        xmin = row.min_c # x1\n        xmax = row.max_c # x2\n        ymin = row.min_r # y1\n        ymax = row.max_r # y2\n        nums.append([xmin,ymin,xmax,ymax])\n        classes.append(row.label)\n    nums = np.array(nums)\n    for i in range(nums.shape[0]):\n        if no_rbc:\n            if classes[i]==0:\n                continue \n        x1y1 = tuple((np.array(nums[i][0:2])).astype(np.int32)) # \n        x2y2 = tuple((np.array(nums[i][2:4])).astype(np.int32))\n        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2) # it will create a rectangle box around object.\n        img = cv2.putText(img, '{}'.format(\n            class_names[int(classes[i])]),\n            x1y1, cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2) # help it displaying text of \n            # predicted class with objectness score*class_probabilities\n    return img","08f4646b":"\ndef predict(data,indx,weights_path = '\/content\/drive\/My Drive\/320_checkpoints\/yolov3_train\/tf_ckpts',visualize = False,no_rbc = True):\n  # data : train_image or test_image\n  \n    yolo = YoloV3(classes=num_classes)\n    ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model = yolo)\n    manager = tf.train.CheckpointManager(ckpt,weights_path, max_to_keep=3)\n    ckpt.restore(weights_path + '\/ckpt-17')\n\n    #yolo.load_weights(weights_path).expect_partial()\n\n    class_names = list(class_dict.keys())\n\n    img_raw = tf.image.decode_image(\n    open('malaria\/images\/'+data[indx], 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, size)\n    img1 = img[0][:]\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names,no_rbc)\n\n    if visualize:\n    plt.suptitle(\"Ground Truth vs Predicted\",fontsize = 20)\n    plt.figure(figsize=(24,8))\n    plt.subplot(1,2,1)\n    plt.title(\"Image Name (Predicted) {}\".format(data[indx]))\n    gt_img = draw_gt_outputs(data[indx],df,class_names,no_rbc)\n    plt.imshow(gt_img)\n    plt.subplot(1,2,2)\n    plt.title(\"Image Name (Predicted) {}\".format(data[indx]))\n    plt.imshow(img)\n    # return(img)\n\n\n    else:\n    return( [boxes, scores, classes, nums])\n    # cv2.imwrite(\"res.jpg\", img)","cb039f2e":"from IPython.display import Image\nImage(\"..\/input\/malaria-image\/download (1).png\")","f97c7ce5":"Image(\"..\/input\/malaria-image\/download (2).png\")","2b063cf5":"Image(\"..\/input\/malaria-image\/download (3).png\")","3c4dfdf2":"Image(\"..\/input\/malaria-image\/download (4).png\")","f98d542d":"Image(\"..\/input\/malaria-image\/download (5).png\")","6178b4ed":"## Evaluation Metric\n\nIn our case we have consider Mean Average Precision Score as a model evaluation metrics.\nGeneral idea of how to calculate MAP Score can be get at https:\/\/tarangshah.com\/blog\/2018-01-27\/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models\/.\n\nMetrics Intuition:\n1. Here we calculate avg precision for each individual classes or categories then calculate mean of all class precision.\nhttps:\/\/towardsdatascience.com\/evaluating-performance-of-an-object-detection-model-137a349c517b","34fa19e6":"### Utils Function","9e6a0df8":"#### Tensorboard","d3abf65e":"### Loading Yolov3 weights into Model","82f3f06e":"#### Data Exploration","d08bcfad":"## So, This concludes my work. Thank you for reading it............ I hope you enjoyed it.......... pls do upvote....","3f2d7d3e":"### Yolov3 Model","7940191b":"#### Loading File","3f42c4e6":"#### Initializing Important Variables","396d98b2":"![image.png](attachment:image.png)","7e59d441":"#### Model Training Started","f284cb59":"#### Load Modules","dec3f591":"### Results or Testing","ac000948":"### Data Loader And Transformation","e56741b4":"## Refrences\nSome nice articles and blogs that we will help in understanding the concept behind yolov3.\n1. https:\/\/michhar.github.io\/learning-from-learning-yolov3\/#transfer-learning\n2. This one is brilliant blog with explanation and code(pytorch) https:\/\/blog.paperspace.com\/how-to-implement-a-yolo-object-detector-in-pytorch\/.\n3.From where I have taken refrence of this code and modify as per my needs https:\/\/pylessons.com\/YOLOv3-introduction\/.\n4. Dataset can be loaded from https:\/\/www.kaggle.com\/kmader\/malaria-bounding-boxes.","6ee3e303":"#### Conclusion\n1. Dataset is highly **imbalaced data** with 96% of RBC cells and other 4% for 5 categories.\n2. When using categorical **cross entropy loss** for negative samples i.e Bounding box for which there is no object is to high and loss for positive samples is too low for which our optimizers tries to lower negative samples loss as it's higher.Therefore, our model wasn't working well at detections.\n3. So to overcome the above hurdle we have tried **focal loss** which try to down weight **negative sample loss** and hence improve our model performace.\n4. If there is more data for all categories(except RBC) our model could have performed much better.\n5. For this model I have taken alpha parameter of **focal loss 0.85**, I have also tried 0.70,0.75,0.80 but at last 0.85 suited best for this model.\n","f77205b0":"* I haven't trained it from beginning as it will take time to get the model ready.\n* I have trained my model for 100 epochs, and after every 5 steps it keep saving the best 3 checkpoints.\n* It also stores the steps you ran before getting disconneted due to any issue and start from where it has stopped.","5b0eb26b":"#### Validataion Dataset","f03343c5":"## Problem Statement\n\n1. Given the image data which contains cells of 6 different classes of which 2 classes are of uninfected cells and 4 classes of infected cells. We have to build a model which can localize each object cells in an image with bounding box and detect the class of cells with high confidence score and with high mean average precision.\n\n","f7406752":"## Information About Data\n\nImages are in .png or .jpg format. There are 3 sets of images consisting of 1364 images (~80,000 cells) with different researchers having prepared each one: from Brazil (Stefanie Lopes), from Southeast Asia (Benoit Malleret), and time course (Gabriel Rangel). Blood smears were stained with Giemsa reagent.\n\nLabels\nThe data consists of two classes of uninfected cells (RBCs and leukocytes) and four classes of infected cells (gametocytes, rings, trophozoites, and schizonts). Annotators were permitted to mark some cells as difficult if not clearly in one of the cell classes. The data had a heavy imbalance towards uninfected RBCs versus uninfected leukocytes and infected cells, making up over 95% of all cells.\n\nA class label and set of bounding box coordinates were given for each cell. For all data sets, infected cells were given a class label by Stefanie Lopes, malaria researcher at the Dr. Heitor Vieira Dourado Tropical Medicine Foundation hospital, indicating stage of development or marked as difficult.\n","df969d74":"### Below displayed is a result of my YoloV3 Model","cb36424f":"## Malaria Bounding Box\n\nMalaria is a deadly, infectious, mosquito-borne disease caused by Plasmodium parasites that are transmitted by the bites of infected female Anopheles mosquitoes. There are five parasites that cause malaria, but two types\u2014P. falciparum and P. vivax\u2014cause the majority of the cases.\n\nMalaria is a disease caused by Plasmodium parasites that remains a major threat in global health, affecting 200 million people and causing 400,000 deaths a year. The main species of malaria that affect humans are Plasmodium falciparum and Plasmodium vivax.\n\nFor malaria as well as other microbial infections, manual inspection of thick and thin blood smears by trained microscopists remains the gold standard for parasite detection and stage determination because of its low reagent and instrument cost and high flexibility. Despite manual inspection being extremely low throughput and susceptible to human bias, automatic counting software remains largely unused because of the wide range of variations in brightfield microscopy images. However, a robust automatic counting and cell classification solution would provide enormous benefits due to faster and more accurate quantitative results without human variability; researchers and medical professionals could better characterize stage-specific drug targets and better quantify patient reactions to drugs.\n\n","4097f1d1":"#### Calculating Loss using Gradient Tape","3e165155":"### Yolo Output,Loss Function and Non-Maximum Suppression."}}