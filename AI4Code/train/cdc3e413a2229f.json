{"cell_type":{"bae18402":"code","4829abea":"code","5821d9b5":"code","9b302f18":"code","c693c0aa":"code","bf7ab02c":"code","2777fe3a":"code","d30e9971":"code","74f748d2":"code","881f8531":"code","1b9eb4cd":"code","21fdaf4b":"code","36ce1507":"code","074365e0":"code","d77c1691":"code","aab0d9cf":"markdown","7836abca":"markdown","44d3bba1":"markdown","20702c17":"markdown","53243cc1":"markdown","10e69d81":"markdown","e9c2ebb5":"markdown","90c864e0":"markdown","c96bdfd1":"markdown"},"source":{"bae18402":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4829abea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom PIL import Image\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\nfrom string import punctuation\nimport nltk\nfrom nltk.corpus import stopwords\n\n!pip install pymystem3\nfrom pymystem3 import Mystem\n\n!pip install pymorphy2\nimport pymorphy2\n\n\n%matplotlib inline","5821d9b5":"df = pd.read_csv('\/kaggle\/input\/economic-news-of-cyprus-ru\/VesKipLinks.csv', index_col = 'Unnamed: 0')","9b302f18":"print(f'Size {df.shape}')\ndf = df.drop('link', axis=1)\ndf = df.drop('hits', axis=1)\ndf.sample(2)","c693c0aa":"mystem = Mystem() \nnltk.download(\"stopwords\")\nstopwords = stopwords.words(\"russian\") + stopwords.words(\"english\") # some articles contain english quotes\/parts\n\ndef preprocess_text(text):\n    '''lemmatizes text'''\n    text = str(text)\n    words = mystem.lemmatize(text.lower())\n    words = [word for word in words if word not in stopwords\\\n              and word != \" \" \\\n              and word.strip() not in punctuation]\n    \n    text = \" \".join(words)\n    \n    return text","bf7ab02c":"df['lemmatized_content'] = df.content.apply(preprocess_text)","2777fe3a":"def replacer (phrase):\n    '''replaces hand-picked items'''\n    new_phrase = ''\n    for word in phrase.split(\" \"):\n        new_word = word\n        if new_word.lower().replace(' ', '') in ['\u043a\u0438\u043f\u0440\u0430', '\u043a\u0438\u043f\u0440 \u043a\u0438\u043f\u0440' '\u043a\u0438\u043f\u0440\u0435', '\u043a\u0438\u043f\u0440\u0443', '\u043a\u0438\u043f\u0440\u043e\u043c', '\u043a\u0438\u043f\u0440','cyprus','\u043a\u0438\u043f\u0440\u0441\u043a\u0438\u0439','\u043a\u0438\u043f\u0440a']:\n            new_word = '\u043a\u0438\u043f\u0440'\n        if new_word.lower().replace(' ', '') in ['\u0431\u0430\u043d\u043a\u0430', 'bank']:\n            new_word = '\u0431\u0430\u043d\u043a'\n        if new_word.lower().replace(' ', '') in ['\u043d\u043e\u0432\u043e\u0435', 'new']:\n            new_word = '\u043d\u043e\u0432\u044b\u0439'\n        if new_word.lower().replace(' ', '') in ['\u0440\u0430\u0441\u0442\u0438\u0442\u044c', '\u0432\u044b\u0440\u0430\u0441\u0442\u0430\u0442\u044c', '\u0432\u044b\u0440\u0430\u0441\u0442\u0438\u0442\u044c']:\n            new_word = '\u0440\u0430\u0441\u0442\u0438'\n        new_phrase += new_word + ' '\n    return new_phrase","d30e9971":"df['lemmatized_content'] = df.lemmatized_content.apply(replacer)","74f748d2":"def date_process(t):\n    '''extracts year'''\n    try:\n        return(int(t[-10:-6]))\n    except:\n        pass\n    \ndf.date = df.date.apply(date_process)","881f8531":"df.date.unique()","1b9eb4cd":"morph = pymorphy2.MorphAnalyzer()\n\nlist_1=['PREP','CONJ', 'PRCL', 'INTJ','NPRO']\ndef drop_grafems (phrase,list=list_1):\n    new_phrase = ''\n    for word in phrase.split(\" \"):\n        new_word = morph.parse(word)[0]\n        if not new_word.tag.POS in list:\n            new_phrase += new_word.normal_form + ' '\n    return new_phrase \n\n\n\nlist_2=['NOUN', 'VERB', 'ADJF']\ndef filter_grafems (phrase,list=list_2):\n    new_phrase = ''\n    for word in phrase.split(\" \"):\n        new_word = morph.parse(word)[0]\n        if new_word.tag.POS in list:\n            new_phrase += new_word.normal_form + ' '\n    return new_phrase \n\n\n\n'''\n\u0413\u0440\u0430\u043c\u043c\u0435\u043c\u0430\t\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435\t\u041f\u0440\u0438\u043c\u0435\u0440\u044b\n* NOUN\t\u0438\u043c\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435\t\u0445\u043e\u043c\u044f\u043a\n* ADJF\t\u0438\u043c\u044f \u043f\u0440\u0438\u043b\u0430\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435 (\u043f\u043e\u043b\u043d\u043e\u0435)\t\u0445\u043e\u0440\u043e\u0448\u0438\u0439\n* ADJS\t\u0438\u043c\u044f \u043f\u0440\u0438\u043b\u0430\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435 (\u043a\u0440\u0430\u0442\u043a\u043e\u0435)\t\u0445\u043e\u0440\u043e\u0448\n* COMP\t\u043a\u043e\u043c\u043f\u0430\u0440\u0430\u0442\u0438\u0432\t\u043b\u0443\u0447\u0448\u0435, \u043f\u043e\u043b\u0443\u0447\u0448\u0435, \u0432\u044b\u0448\u0435\n* VERB\t\u0433\u043b\u0430\u0433\u043e\u043b (\u043b\u0438\u0447\u043d\u0430\u044f \u0444\u043e\u0440\u043c\u0430)\t\u0433\u043e\u0432\u043e\u0440\u044e, \u0433\u043e\u0432\u043e\u0440\u0438\u0442, \u0433\u043e\u0432\u043e\u0440\u0438\u043b\n* INFN\t\u0433\u043b\u0430\u0433\u043e\u043b (\u0438\u043d\u0444\u0438\u043d\u0438\u0442\u0438\u0432)\t\u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u0441\u043a\u0430\u0437\u0430\u0442\u044c\n* PRTF\t\u043f\u0440\u0438\u0447\u0430\u0441\u0442\u0438\u0435 (\u043f\u043e\u043b\u043d\u043e\u0435)\t\u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0432\u0448\u0438\u0439, \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u0430\u044f\n* PRTS\t\u043f\u0440\u0438\u0447\u0430\u0441\u0442\u0438\u0435 (\u043a\u0440\u0430\u0442\u043a\u043e\u0435)\t\u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u0430\n* GRND\t\u0434\u0435\u0435\u043f\u0440\u0438\u0447\u0430\u0441\u0442\u0438\u0435\t\u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0432, \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044f\n* NUMR\t\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435\t\u0442\u0440\u0438, \u043f\u044f\u0442\u044c\u0434\u0435\u0441\u044f\u0442\n* ADVB\t\u043d\u0430\u0440\u0435\u0447\u0438\u0435\t\u043a\u0440\u0443\u0442\u043e\n* NPRO\t\u043c\u0435\u0441\u0442\u043e\u0438\u043c\u0435\u043d\u0438\u0435-\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435\t\u043e\u043d\n* PRED\t\u043f\u0440\u0435\u0434\u0438\u043a\u0430\u0442\u0438\u0432\t\u043d\u0435\u043a\u043e\u0433\u0434\u0430\n* PREP\t\u043f\u0440\u0435\u0434\u043b\u043e\u0433\t\u0432\n* CONJ\t\u0441\u043e\u044e\u0437\t\u0438\n* PRCL\t\u0447\u0430\u0441\u0442\u0438\u0446\u0430\t\u0431\u044b, \u0436\u0435, \u043b\u0438\u0448\u044c\n* INTJ\t\u043c\u0435\u0436\u0434\u043e\u043c\u0435\u0442\u0438\u0435\t\u043e\u0439\n'''\npass","21fdaf4b":"df['lemmatized_content_filtered'] = df.lemmatized_content.apply(filter_grafems)","36ce1507":"#?WordCloud\n# run above line to get doctring","074365e0":"mask = np.array(Image.open( \"..\/input\/pictures\/cyprus.jpg\"))\n\ndef wordcloud_draw(data, color = 'black'):\n    '''draws a chart'''\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()])\n    wordcloud = WordCloud(stopwords=stopwords,\n                      background_color=color, \n                          mask = mask,\n                      width=640,\n                      height=480,\n                          max_words=350,\n                          colormap=\"nipy_spectral\",\n                          scale=10,\n                          ).generate(cleaned_word)\n    wordcloud.to_file('N.png') # saving chart to file - just in case\n    plt.figure(1,figsize=(25, 25))\n    plt.imshow(wordcloud,  cmap=plt.cm.gray, interpolation=\"bicubic\")\n    plt.margins(x=0, y=0)\n\n    plt.axis('off')\n    plt.show()","d77c1691":"wordcloud_draw(df.lemmatized_content_filtered,'white')","aab0d9cf":"And finally - WordCloud","7836abca":"In order to analyse text I will normalise it by means of lemmatization it in order to reduce variations of the word forms fown to base words.  As well I will drop out from analysys meaningless words (those are imported from nltk)","44d3bba1":"Conversion of dates to year only","20702c17":"Unnecessary data is dropped:","53243cc1":"Couple more functions to filter words by type (werb \/ conjunction \/ etc)","10e69d81":"I have prepared database with news articles and headlines of Cyprus Russian language news for last 5 years. All further work will be with this database","e9c2ebb5":"Some further tailor made adjustment (upon analysys of chart):","90c864e0":"In this Notebook I will work with data from Russian news in Cyprus in order to lemmatize it and than prepare a nice word cloud with news.\nLibraries required for natural language processing will be used.","c96bdfd1":"WordCloud is a technique to show which words are the most frequent among the given text. "}}