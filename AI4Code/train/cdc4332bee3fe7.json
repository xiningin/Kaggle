{"cell_type":{"c40dfbd2":"code","88eb499d":"code","1cb2ca0b":"code","9c609fc2":"code","f6c7a435":"code","5e70329f":"code","71e89269":"code","0d32e8f2":"code","4b667b49":"code","32b336db":"code","d13a5cf1":"code","2d584a92":"code","18d75988":"code","b25a8c72":"code","943c4e65":"code","a9d36ba4":"code","2d774e13":"code","6717f1ed":"code","42c1e11d":"code","cbf8ee5d":"code","073deb2d":"code","2de5ce68":"markdown","55d06a6a":"markdown","85e1b34d":"markdown","da83243c":"markdown","a2afa129":"markdown","00073b44":"markdown","8de5ced9":"markdown","bc2e1112":"markdown","d162705b":"markdown","2d00cf8f":"markdown","055b4bc0":"markdown","4680fca7":"markdown","e79bed66":"markdown","06e1e6a9":"markdown","29631ebf":"markdown","0597fc7c":"markdown","0342dec3":"markdown","de80ed22":"markdown","db34141b":"markdown","8edc6933":"markdown","a1abae82":"markdown","2e79f64b":"markdown","4b50f4a6":"markdown","3713074b":"markdown","4e94c70c":"markdown","87f492e0":"markdown","fe1eb1ca":"markdown","2dd6ab64":"markdown","18cbdd0c":"markdown"},"source":{"c40dfbd2":"pip install pypng","88eb499d":" \n# Import statements\n \nimport tensorflow as tf\nimport numpy as np\nimport png\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\n","1cb2ca0b":"print(tf.__version__)","9c609fc2":"\n# Unpickle the data\n\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nqmnist = unpickle(\"..\/input\/qmnist-the-extended-mnist-dataset-120k-images\/MNIST-120k\")\n\ndata = qmnist['data']\nlabels = qmnist['labels']\n","f6c7a435":"\n# Understand the dataset\n\nprint(\"Shape of the data array: \", data.shape)\nprint(\"Shape of labels array: \", labels.shape)\n\nlabels = labels.flatten()   # Convert labels to a 1-D array\nprint(\"Reshaped labels array: \", labels.shape)\n","5e70329f":"# Convert the images to PNG and save them in directories\n\nn = data.shape[0]                                                        # Total number of images\n\nif not os.path.exists('dataset'):\n    os.mkdir('dataset')                                                  # Make a parent folder to hold all the classes\n\nfor i in range(0,n):\n  x = data[i]\n  y = str(labels[i])\n  name = str(i)\n\n  if not os.path.exists('dataset\/{}'.format(y)):\n    os.mkdir('dataset\/{}'.format(y))\n  \n  png.from_array(x, mode=\"L\").save(\"dataset\/{}\/{}.png\".format(y,name))   # Convert np array to png and save it to respective folder\n","71e89269":" \n# Split dataset into Train, Validation & Test sets\n\nparent_folder = '.\/dataset'\nsize = 75   # InceptionV3 requires minimum image size of (75,75)\nbs = 32     # Batch size\n\n\n# No Data Augmentation is performed\n\ntrain_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)\ntrain_data = train_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='training')\n\nvalidation_data_gen = ImageDataGenerator(validation_split=0.15, preprocessing_function=preprocess_input)\nvalidation_data = validation_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', batch_size=bs, seed=42, subset='validation')\n\ntest_data_gen = ImageDataGenerator(validation_split=0.10, preprocessing_function=preprocess_input)\ntest_data = test_data_gen.flow_from_directory(parent_folder, class_mode='categorical', target_size=(size,size), color_mode='rgb', subset='validation', shuffle=False)\n ","0d32e8f2":"\n# Visualize the first image of every class in the training set\n\nlabels = list(train_data.class_indices.keys())\n\nfig = plt.figure(figsize=(23,25))\nfor i in range(0,train_data.num_classes):\n  img = [x for x in train_data.filepaths if labels[i] in x.split('\/')[-2]][0]\n  fig.add_subplot(6,6,i+1)\n  plt.title(labels[i])\n  plt.imshow(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB))\n  ","4b667b49":"\n# Assign essential variables\n\nshape = train_data.image_shape\nprint(\"Shape of images in train set: \", shape)                               # Shape of train images (height,width,channels)\nk = train_data.num_classes\nprint(\"Total number of classes: \", k)                                        # Total number of labels\/classes\ntrain_samples = train_data.samples\nprint(\"Total number of images in the train set: \", train_samples)            # Total number of images in train set\nvalidation_samples = validation_data.samples\nprint(\"Total number of images in the validation set: \", validation_samples)  # total number of images in validation set\n","32b336db":"\n# Build the model\n\ninput = Input(shape=shape)\n\nbasemodel = InceptionV3(include_top=False, weights='imagenet', input_shape=shape, pooling='avg')\nbasemodel.trainable = False\n\nx = basemodel(input)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.2)(x)\noutput = Dense(k, activation='softmax')(x)\n\nmodel = Model(input,output)\n","d13a5cf1":"\n# Compile the model\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","2d584a92":" \n# Initialize callbacks\n \nstop = EarlyStopping(monitor='val_loss', patience=4, mode='min', restore_best_weights=True)                                             # Stops training early to prevent overfitting\ncheckpoint = ModelCheckpoint(filepath='loss_{val_loss:.4f}-epoch_{epoch:02d}-weights.hdf5', monitor='val_loss', mode='min', save_best_only=True)   # Saves the model for every best val_loss\n","18d75988":"\n# Model Summary\n\nmodel.summary()\n","b25a8c72":"\n# Train the model\n\nep = 50                      # Number of epochs\nspe = train_samples\/bs       # Steps per epoch\nvs = validation_samples\/bs   # Validation steps\n \nr = model.fit(train_data, validation_data=validation_data, steps_per_epoch=spe, validation_steps=vs, epochs=ep, callbacks=[stop,checkpoint])\n","943c4e65":" \n# Evaluate the model\n \nmodel.evaluate(validation_data)\n","a9d36ba4":"\n# Plot training history\n \nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\nplt.plot(r.history['accuracy'], label='accuracy')\nplt.plot(r.history['val_accuracy'], label='val_accuracy')\nplt.legend()\nplt.show()\n","2d774e13":"\n# Visualize the first image of every class in the training set\n\nlabels = list(test_data.class_indices.keys())\n\nfig = plt.figure(figsize=(23,25))\nfor i in range(0,test_data.num_classes):\n  img = [x for x in test_data.filepaths if labels[i] in x.split('\/')[-2]][0]\n  fig.add_subplot(6,6,i+1)\n  plt.title(labels[i])\n  plt.imshow(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB))\n  ","6717f1ed":"\n# Predictions on the test data\n\npred = model.predict(test_data).argmax(axis=1)\nlabels = list(train_data.class_indices.keys())   # class_indices method returns a dictionary with keys as string classes and values as integer indices\n","42c1e11d":"\n# Get the F1 score on test data prediction\n\nprint(classification_report(test_data.classes, pred))\n","cbf8ee5d":"\n# Plot confusion matrix\n\ncm = confusion_matrix(test_data.classes, pred)\nplt.figure(figsize=(15,10))\nsns.heatmap(cm, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, cmap=\"BuPu\")\nplt.title('Confusion Matrix')\nplt.show()\n","073deb2d":"\n# Visualize random predictions on the test data\n\nrand = np.random.randint(low=0, high=test_data.samples, size=6)\n\nfig = plt.figure(figsize=(20,20))\ni=1\nfor n in rand:\n  true_index = test_data.classes[n]\n  predicted_index = pred[n]\n  img = cv2.imread(test_data.filepaths[n])\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  fig.add_subplot(3,3,i)\n  plt.title('True label={}  Predicted label={}'.format(labels[true_index], labels[predicted_index]))\n  plt.imshow(img)\n  i += 1\n","2de5ce68":"[Return to Table of Contents](#contents)\n    <a id='4.3'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Compile the model<\/h1>","55d06a6a":"[Return to Table of Contents](#contents)\n    <a id='6.4'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Plot confusion matrix<\/h1>\n","85e1b34d":"[Return to Table of Contents](#contents)\n    <a id='4'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>Creating and Training the Model<\/center><\/b><\/h1>","da83243c":"[Return to Table of Contents](#contents)\n    <a id='3.4'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Split dataset into Train, Validation & Test sets<\/h1>","a2afa129":"[Return to Table of Contents](#contents)\n    <a id='6.1'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Visualize the first image of every class in the test set<\/h1>","00073b44":"[Return to Table of Contents](#contents)\n    <a id='3.3'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Convert the images to PNG and save them in directories<\/h1>\n","8de5ced9":"[Return to Table of Contents](#contents)\n    <a id='6'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>Testing the model + Visual analysis<\/b><\/center><\/h1>\n","bc2e1112":"<a id='intro'><\/a>\n<h1 style=\"font-family:verdana; font-size:35px\"> <center><u>Transfer Learning with Tensorflow API<\/u><\/center><\/h1><br>\n<div style=\"width:100%;text-align: center;\"><img align=middle src=\"http:\/\/media5.datahacker.rs\/2019\/12\/tf_keras_mnist_1-1-1024x458.png\" alt=\"TF_MNIST\" style=\"height:300px;margin-top:3rem;\"><\/div>\n<center><cite>Image from <a href=\"http:\/\/media5.datahacker.rs\/2019\/12\/tf_keras_mnist_1-1-1024x458.png\">media5.datahacker.rs (NOT SECURE)<\/a><\/cite><\/center>","d162705b":"[Return to Table of Contents](#contents)\n    <a id='4.7'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Evaluate the model<\/h1>\n","2d00cf8f":"[Return to Table of Contents](#contents)\n    <a id='3.1'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Unpickle the data<\/h1>","055b4bc0":"<div style=\"font-size:15px; font-family:verdana;\">\nHi there,<br><br>\n\n<p>This is my take on implementing transfer learning technique with the with the Tensorflow API. I'm extremely new to deep learrning in general and have tried my best to perfect the code as I have learned it.<br>\nThat being said, there will definitely be quite a few imperfections and flaws in the code. Please feel free to point out any errors and\/or corrections that could improve the quality of this notebook.\n<\/p>\n<p>Cheers and have a great day !<\/p>\n<p><i>PS: This is my first time formatting a notebook. Please don't laugh <\/i>\ud83d\ude01<\/p>\n<\/div>","4680fca7":"[Return to Table of Contents](#contents)\n    <a id='4.5'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Model Summary<\/h1>\n","e79bed66":"[Return to Table of Contents](#contents)\n    <a id='2'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>Import Statements<\/b><\/center><\/h1>","06e1e6a9":"[Return to Table of Contents](#contents)\n    <a id='3.2'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Understand the dataset<\/h1>","29631ebf":"\n<a id='1'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>Download and install PyPNG to convert numpy array to images<\/b><\/center><\/h1>\n\n> <i>PyPNG Github link : [Click here](https:\/\/github.com\/drj11\/pypng)<\/i><br>\n> <i>PyPNG Official documentation : [Click here](https:\/\/pypng.readthedocs.io\/en\/latest\/)<\/i>","0597fc7c":"[Return to Table of Contents](#contents)\n    <a id='3'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>Dealing with the Data<\/b><\/center><\/h1>\n","0342dec3":"[Return to Table of Contents](#contents)\n    <a id='5'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Plot training history<\/h1>","de80ed22":"[Return to Table of Contents](#contents)\n    <a id='6.3'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Classification report on test data<\/h1>\n","db34141b":"[Return to Table of Contents](#contents)\n    <a id='4.6'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Train the model<\/h1>\n","8edc6933":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc <i>Please note : I have converted all the Numpy array data to PNG format files and saved them in sub-directories with their respective class names as it is easier for me to work with and reduces the load on RAM.<\/i><\/div>","a1abae82":"[Return to Table of Contents](#contents)\n    <a id='4.2'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Build the model<\/h1>","2e79f64b":"<div style=\"font-size:15px; font-family:verdana;\">\n<center>Dataset used : <a href=\"https:\/\/www.kaggle.com\/fedesoriano\/qmnist-the-extended-mnist-dataset-120k-images\">QMNIST Extended Dataset by fedesoriant<\/a><\/center>\n\n<center>Deep learning library used : <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\">Tensorflow with Keras<\/a><br><\/center>\n\n<center>Pretrained model used for Transfer Learning : <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/inception_v3\/InceptionV3\">InceptionV3<\/a><br><\/center>\n<\/div>","4b50f4a6":"[Return to Table of Contents](#contents)\n<div style=\"width:100%;text-align: center;\"><img align=middle src=\"https:\/\/linguaholic.com\/linguablog\/wp-content\/uploads\/2021\/03\/A-Huge-Thank-You-720x405.jpeg.webp\" alt=\"Thanks\" style=\"height:300px;\"><\/div>\n\n<center><cite>Image from <a href=\"https:\/\/linguaholic.com\/linguablog\/a-huge-thank-you\/\">this blog<\/a> by MARCEL ISELI<\/cite><center>","3713074b":"<html>\n    <a id='contents'><\/a>\n    <h1 style=\"font-family:verdana;font-size:30px\">\n        <center><b>\ud83d\udcdc Table of Contents \ud83d\udcdc<\/b><\/center><\/h1>\n<\/html>\n\n* [Download and install PyPNG](#1)\n* [Import statements](#2)\n* [Dealing with the data](#3)\n    * [Unpickle the data](#3.1)\n    * [Understand the dataset](#3.2)\n    * [Convert array to png and save in directories](#3.3)\n    * [Split dataset into Train, Validation and Test sets](#3.4)\n    * [Visualize the first image of every class in the training set](#3.5)\n* [Creating and training the Model](#4)\n    * [Assign essential variables](#4.1)\n    * [Build the model](#4.2)\n    * [Compile the Model](#4.3)\n    * [Initialize callbacks](#4.4)\n    * [Model Summary](#4.5)\n    * [Train the model](#4.6)\n    * [Evaluate the model](#4.7)\n* [Plot training history](#5)\n* [Testing the model + Visual analysis](#6)\n* [ Visualize the first image of every class in the test set](#6.1)\n    * [Make predictions on the Test data](#6.2)\n    * [Classification report on Test data](#6.3)\n    * [Plot confustion matrix](#6.4)\n    * [Visualize random predictions on test data](#6.5)","4e94c70c":"[Return to Table of Contents](#contents)\n    <a id='6.2'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Predictions on the test data<\/h1>\n","87f492e0":"[Return to Table of Contents](#contents)\n    <a id='4.4'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Initialize callbacks<\/h1>","fe1eb1ca":"[Return to Table of Contents](#contents)\n    <a id='6.5'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Visualize random predictions on the test data<\/h1>","2dd6ab64":"[Return to Table of Contents](#contents)\n    <a id='3.5'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Visualize the first image of every class in the training set<\/h1>","18cbdd0c":"[Return to Table of Contents](#contents)\n    <a id='4.1'><\/a>\n    <h1 style=\"font-family:verdana\">\n        Assign essential variables<\/h1>"}}