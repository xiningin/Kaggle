{"cell_type":{"13a97295":"code","24380d77":"code","0a1611f0":"code","14b20e3e":"code","6f87889c":"code","f55814b0":"code","f60691b3":"code","200589a0":"code","d68e8d62":"code","2d131a9e":"code","524e2ada":"code","a8930bab":"code","11796b4c":"code","93f780f1":"code","a6bb9764":"code","bf593fe5":"code","e1f175a7":"code","f0965f66":"code","fc1d4710":"code","bada7e39":"code","6a794c60":"code","2b4b8ad9":"code","a8479317":"markdown","3090b963":"markdown","f3439d22":"markdown","891e9f17":"markdown","ec5273a6":"markdown","fc83e539":"markdown","78ac737a":"markdown","9c18295e":"markdown","8dbab8ef":"markdown","8221f1be":"markdown","eb8766da":"markdown","2d1b88eb":"markdown","712962a7":"markdown","37474f83":"markdown"},"source":{"13a97295":"import os\nimport gc\nimport cv2\nimport cuml\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom numba import cuda\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier","24380d77":"!ls ..\/input\/ranzcr-clip-catheter-line-classification","0a1611f0":"train = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train.csv')\ntest = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/sample_submission.csv')\nprint(train.shape)\nprint(test.shape)\ntrain.head(10)","14b20e3e":"train.mean()","6f87889c":"img = cv2.imread('..\/input\/ranzcr-clip-catheter-line-classification\/train\/'+train.StudyInstanceUID.values[0]+'.jpg')\nplt.imshow(img)","f55814b0":"import ast\n\nannot = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train_annotations.csv')\nprint(annot.shape)\nannot.head()","f60691b3":"RES = np.zeros( (512,512) )\nfor i in tqdm(range(annot.shape[0])):\n    img = cv2.imread('..\/input\/ranzcr-clip-catheter-line-classification\/train\/'+annot.StudyInstanceUID.values[i]+'.jpg')\n    img[:] = 0\n    data = eval(annot.data.values[i])\n    for i in range(len(data)-1):\n        img = cv2.line(img, (data[i][1],data[i][0]), (data[i+1][1],data[i+1][0]), (255,255,255), 20 )\n    img = cv2.resize(img,(512,512))\n    RES += img[:,:,0]\n    \nRES \/= annot.shape[0]","200589a0":"plt.imshow(np.clip(RES,0,1))","d68e8d62":"mask = RES.copy()\nmask[mask>0.5] = 1.\nmask[mask<1] = 0\nmask = mask.astype(np.uint8)\nmask = np.stack( (mask,mask,mask), 2 )\n\ndel RES\ngc.collect()\nplt.imshow(mask)","2d131a9e":"import keras\nfrom keras.applications.mobilenet import preprocess_input\n\ndir(keras.applications)","524e2ada":"!ls ..\/input\/keras-pretrained-models\/","a8930bab":"import keras\nfrom keras.applications.mobilenet import preprocess_input\n\n# Instantiate model\nbase = keras.applications.Xception( weights=None,  include_top=True)\n\n# Load pretrained imagenet weights\nbase.load_weights('..\/input\/keras-pretrained-models\/Xception_Top_ImageNet.h5')\nbase.trainable = False\n\nmodel = keras.Model(inputs=base.input, outputs=base.get_layer('avg_pool').output)","11796b4c":"train_path = '..\/input\/ranzcr-clip-catheter-line-classification\/train\/'\n\nemb_train = np.zeros( (train.shape[0],2048), dtype=np.float32 )\nfor n, filename in tqdm(enumerate(train.StudyInstanceUID.values), total=train.shape[0]):\n    img = cv2.imread(train_path+filename+'.jpg')\n    img = cv2.resize(img,(512,512))\n    img *= mask\n    img = preprocess_input(img)[np.newaxis]\n    emb_train[n] = model.predict(img)[0]\n    \ngc.collect()","93f780f1":"test_path = '..\/input\/ranzcr-clip-catheter-line-classification\/test\/'\n\nemb_test = np.zeros( (test.shape[0],2048), dtype=np.float32 )\nfor n, filename in tqdm(enumerate(test.StudyInstanceUID.values), total=test.shape[0]):\n    img = cv2.imread(test_path+filename+'.jpg')\n    img = cv2.resize(img,(512,512))\n    img *= mask\n    img = preprocess_input(img)[np.newaxis]\n    emb_test[n] = model.predict(img)[0]\n    \ngc.collect()","a6bb9764":"del model\ngc.collect()\nkeras.backend.clear_session() \ngc.collect()","bf593fe5":"cuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","e1f175a7":"train.head()\ntargets = train.columns[1:-1]\nprint(targets)","f0965f66":"train_index = np.where( (np.arange(emb_train.shape[0])%20)!=7 )[0]\nvalid_index = np.where( (np.arange(emb_train.shape[0])%20)==7 )[0]\nlen(train_index), len(valid_index)","fc1d4710":"ytarget = train[targets].values[valid_index]\nypred = np.zeros( (len(valid_index), len(targets)) )\n\nfor n, target in tqdm(enumerate(targets), total=len(targets)):\n    \n    rf = cuml.ensemble.RandomForestClassifier(n_estimators=250, max_features=500, n_bins=16, output_type='numpy')\n    \n    rf.fit( emb_train[train_index], train[target].values[train_index] )\n    \n    ypred[:,n] = rf.predict_proba(emb_train[valid_index])[:,1]\n    test[target] = rf.predict_proba(emb_test)[:,1]\n    \n    print(n, roc_auc_score( ytarget[:,n], ypred[:,n] ), target )\n    \n    del rf\n    gc.collect()\n    \nprint( 'Final AUC:', roc_auc_score( ytarget.flatten(), ypred.flatten() ) )","bada7e39":"test.head()","6a794c60":"test.mean()","2b4b8ad9":"test.to_csv('submission.csv', index=False)","a8479317":"# Process average of cateter position to be used as a mask.","3090b963":"# Delete model and release memory","f3439d22":"# Check test predictions distribution","891e9f17":"# Split train and valid set: 95%\/5%","ec5273a6":"# I found this trick to clear all Keras allocated memory in GPU.","fc83e539":"# Check labels names","78ac737a":"# Submit","9c18295e":"# Lets extract features from the images using transfer learning from pretrained Imagenet models.","8dbab8ef":"# Load train and test as DataFrames","8221f1be":"# Fit each label and predict test using the embeddings features","eb8766da":"# Extract features from test images","2d1b88eb":"# Check first image in train","712962a7":"# Check distribution of labels in train","37474f83":"# Inefficient, but easy to understand for loop to extract features from train images"}}