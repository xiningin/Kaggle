{"cell_type":{"eb6f5cfe":"code","022b6b81":"code","08a18659":"code","d0ad0e9c":"code","1af624f8":"code","1f88a725":"code","d16ce313":"code","c18d2ed1":"code","da2e31cf":"code","7eb1322c":"code","9e0bde4f":"code","a454648e":"code","cf03cc9a":"code","6de0433f":"code","10c20246":"code","23daacf8":"code","5d624d4b":"code","0de1a9b5":"code","118f6d37":"code","038eea00":"code","db6fb63c":"code","a2ec2d66":"code","95579bea":"code","454d036d":"code","ab4cc2a2":"code","c69ebef2":"code","650214f3":"code","2457d672":"markdown","bcb03409":"markdown","87771dee":"markdown","422d748d":"markdown","9366e901":"markdown","6e7b44aa":"markdown","bdb2bb6e":"markdown","830aab0f":"markdown","81af9597":"markdown","4be73c2c":"markdown","8ac58f4d":"markdown","a2fd348f":"markdown","6831b0f4":"markdown","fa7ba9b8":"markdown","bede5a4c":"markdown","21a84130":"markdown","1b243fef":"markdown","d2080dbe":"markdown","0cd54e96":"markdown","ea533c57":"markdown","5ffe60fe":"markdown"},"source":{"eb6f5cfe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom scipy.io import wavfile\nimport os.path\nimport IPython.display\nimport seaborn as sns\nimport librosa\nimport librosa.display\nimport soundfile\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras import utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten, BatchNormalization\nfrom keras import optimizers\n\nimport warnings","022b6b81":"image_dir = Path('..\/input\/speech-emotion-recognition-en\/Crema')","08a18659":"filepaths = list(image_dir.glob(r'**\/*.wav'))","d0ad0e9c":"labels = list(map(lambda x: os.path.split(x)[1].split('_')[2], filepaths))","1af624f8":"set(labels)","1f88a725":"filepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\naudio_df = pd.concat([filepaths, labels], axis=1)\naudio_df","d16ce313":"sns.set(rc={'figure.figsize':(12,8)})\nsns.set_style('darkgrid')\nsns.histplot(labels, color='#4FAEB0')","c18d2ed1":"audio_arrays = []\n\nfor i in audio_df['Filepath']:\n    x, sr = librosa.load(i, sr=44100)\n    audio_arrays.append(x)\n    \naudio_df['Arrays'] = audio_arrays","da2e31cf":"audio_df","7eb1322c":"angfile = audio_df[audio_df['Label'] == 'ANG']['Filepath']\nangarray = audio_df[audio_df['Label'] == 'ANG']['Arrays']\n\nlibrosa.display.waveplot(angarray.iloc[0], color='#C00808')\nIPython.display.Audio(angfile.iloc[0])","9e0bde4f":"disfile = audio_df[audio_df['Label'] == 'DIS']['Filepath']\ndisarray = audio_df[audio_df['Label'] == 'DIS']['Arrays']\n\nlibrosa.display.waveplot(disarray.iloc[0], color='#804E2D')\nIPython.display.Audio(disfile.iloc[0])","a454648e":"feafile = audio_df[audio_df['Label'] == 'FEA']['Filepath']\nfeaarray = audio_df[audio_df['Label'] == 'FEA']['Arrays']\n\nlibrosa.display.waveplot(feaarray.iloc[0], color='#7D55AA')\nIPython.display.Audio(feafile.iloc[0])","cf03cc9a":"hapfile = audio_df[audio_df['Label'] == 'HAP']['Filepath']\nhaparray = audio_df[audio_df['Label'] == 'HAP']['Arrays']\n\nlibrosa.display.waveplot(haparray.iloc[0], color='#F19C0E')\nIPython.display.Audio(hapfile.iloc[0])","6de0433f":"neufile = audio_df[audio_df['Label'] == 'NEU']['Filepath']\nneuarray = audio_df[audio_df['Label'] == 'NEU']['Arrays']\n\nlibrosa.display.waveplot(neuarray.iloc[0], color='#4CB847')\nIPython.display.Audio(neufile.iloc[0])","10c20246":"sadfile = audio_df[audio_df['Label'] == 'SAD']['Filepath']\nsadarray = audio_df[audio_df['Label'] == 'SAD']['Arrays']\n\nlibrosa.display.waveplot(sadarray.iloc[0], color='#478FB8')\nIPython.display.Audio(sadfile.iloc[0])","23daacf8":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","5d624d4b":"def extract_features(data):\n    # Zero Crossing Rate\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr))\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr, n_fft=200).T, axis=0)\n    result = np.hstack((result, chroma_stft))     \n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sr, n_fft=200).T, axis=0)\n    result = np.hstack((result, mfcc))\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sr, n_fft=200).T, axis=0)\n    result = np.hstack((result, mel))\n    \n    # Tonnetz\n    tonnetz = np.mean(librosa.feature.tonnetz(y=data, sr=sr).T, axis=0);\n    result = np.hstack((result, tonnetz));\n    \n    return result","0de1a9b5":"def get_features(data):\n    result = []\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result.append(res1)\n    \n    # with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result.append(res2)\n    \n    # with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sr)\n    res3 = extract_features(data_stretch_pitch)\n    result.append(res3)\n    \n    return result","118f6d37":"warnings.filterwarnings('ignore')\n\nx = []\ny = []\nfor i in range(len(audio_df)):\n    feature=get_features(audio_df['Arrays'].iloc[i]);\n    for j in feature:\n        x.append(j)\n        y.append(audio_df['Label'].iloc[i])","038eea00":"le = LabelEncoder()\ny = utils.to_categorical(le.fit_transform(y))\ny","db6fb63c":"x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size=0.1)","a2ec2d66":"print((x_train.shape, y_train.shape, x_test.shape, y_test.shape))","95579bea":"x_train = np.expand_dims(x_train,axis=2)\nx_test = np.expand_dims(x_test,axis=2)","454d036d":"print((x_train.shape, y_train.shape, x_test.shape, y_test.shape))","ab4cc2a2":"model = Sequential()\nmodel.add(Conv1D(128, 3, activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D((1)))\nmodel.add(Conv1D(256, 3, activation='relu'))\nmodel.add(MaxPooling1D((1)))\nmodel.add(Conv1D(512, 3, activation='relu'))\nmodel.add(MaxPooling1D((1)))\nmodel.add(Conv1D(1024, 3, activation='relu'))\nmodel.add(MaxPooling1D((1)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n             optimizer=optimizers.RMSprop(lr=0.0005),\n             metrics=['accuracy'])\n\nmodel.summary()","c69ebef2":"history = model.fit(x_train, y_train,\n                    epochs=50,\n                    batch_size=128,\n                    validation_data=(x_test, y_test))","650214f3":"y_pred = model.predict(x_test)\nmatrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nax = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap = 'rocket_r', xticklabels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness'], yticklabels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness'])","2457d672":"## <h2><span class=\"label label-default\" style=\"background-color:#C00808;\">\ud83d\udca5 ANGER \ud83d\udca5<\/span><\/h2>","bcb03409":"# <h1 id='model'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">7. Creating Model<\/span><\/h1>","87771dee":"# <h1 id='load'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">2. Loading Data<\/span><\/h1>","422d748d":"# <h1 id='traintest'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">6. Train Test Split<\/span><\/h1>","9366e901":"# <center>\ud83d\ude00\ud83d\ude10\ud83d\ude2d Speech Emotion Detection \ud83d\ude2d\ud83d\ude10\ud83d\ude00<\/center>","6e7b44aa":"## <h2><span class=\"label label-default\" style=\"background-color:#F19C0E;\"> \ud83c\udf08 HAPPINESS \ud83c\udf08 <\/span><\/h2>","bdb2bb6e":"Same number of files for all emotions except neutral, but the difference is not significant enough to require the use of oversampling.","830aab0f":"Honestly, I would even find it difficult to define the emotions in each of these clips, so it will be interesting to see if we can create a model capable of doing it!","81af9597":"# <h1 id='eval'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">8. Evaluating Model<\/span><\/h1>","4be73c2c":"# <h1 id='visual'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">3. Visualising Data<\/span><\/h1>","8ac58f4d":"This is my first attempt at audio classification on Kaggle. I am using the popular dataset Crema from <a href=\"https:\/\/www.kaggle.com\/dmitrybabko\/speech-emotion-recognition-en\" style=\"color:#4FAEB0\"> Speech Emotion Recognition (en)<\/a> which contains 7,442 original clips from 91 actors - 48 male and 43 female of a wide range of ages, races and ethnicities.\n\nThe actors spoke from a selection of 12 sentences, each presented using one of six emotions (anger, disgust, fear, happiness, neutral and sadness).","a2fd348f":"These correspond to the emotions:\n\n* ANG: Anger \ud83d\ude21\n* DIS: Disgust \ud83e\udd22\n* FEA: Fear \ud83d\ude31\n* HAP: Happiness \ud83d\ude00\n* NEU: Neutral \ud83d\ude10\n* SAD: Sadness \ud83d\ude2d","6831b0f4":"## <h2><span class=\"label label-default\" style=\"background-color:#478FB8;\"> \u2614 SADNESS \u2614 <\/span><\/h2>","fa7ba9b8":"![image.png](attachment:image.png)","bede5a4c":"## <h2><span class=\"label label-default\" style=\"background-color:#7D55AA;\"> \ud83d\udc7b FEAR \ud83d\udc7b <\/span><\/h2>","21a84130":"# <h1 id='aug'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">4. Data Augmentation<\/span><\/h1>","1b243fef":"# <h1 id='features'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">5. Extracting Features<\/span><\/h1>","d2080dbe":"## <h2><span class=\"label label-default\" style=\"background-color:#4CB847;\"> \ud83d\ude10 NEUTRAL \ud83d\ude10 <\/span><\/h2>","0cd54e96":"# <h1 id= 'imports'><span class=\"label label-default\" style=\"background-color:#4FAEB0; font-size:20px\">1. Imports<\/span><\/h1>","ea533c57":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">Table of Contents<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#imports\" role=\"tab\" aria-controls=\"profile\" style=\"color:#8F908E\">Imports<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#load\" role=\"tab\" aria-controls=\"messages\" style=\"color:#8F908E\">Loading Data<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#visual\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\">Visualising Data<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#aug\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\">Data Augmentation<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#features\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\">Extracting Features<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">5<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#traintest\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\">Train Test Split<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">6<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#model\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\"> Creating Model<span class=\"badge badge-primary badge-pill\" style=\"background-color:#4FAEB0; border-color:#4FAEB0\">7<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eval\" role=\"tab\" aria-controls=\"settings\" style=\"color:#8F908E\">Evaluating Model<span class=\"badge badge-primary badge-pill\"  style=\"background-color:#4FAEB0; border-color:#4FAEB0\">8<\/span><\/a>  ","5ffe60fe":"## <h2><span class=\"label label-default\" style=\"background-color:#804E2D;\"> \ud83e\udd22 DISGUST \ud83e\udd22 <\/span><\/h2>"}}