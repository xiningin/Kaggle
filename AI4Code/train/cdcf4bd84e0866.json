{"cell_type":{"26665ebc":"code","cec948bf":"code","76c68152":"code","93cbcbba":"code","31ac9827":"code","8e87d0f9":"code","c1b70b3d":"code","d6fb5179":"code","82dc92d3":"code","93601d2e":"code","0aedc190":"code","b4f86cd7":"code","44063cff":"code","511270da":"code","aa56b435":"code","bfbfd254":"code","eda46d08":"code","ce9e3e11":"code","cb407097":"markdown","285fd269":"markdown","962d57c4":"markdown","9fcbae56":"markdown","e578fffc":"markdown","848ae47c":"markdown","1b04b7e0":"markdown","d311c0e4":"markdown","eac997ea":"markdown","7b6f37ad":"markdown","53af628f":"markdown","dc3a77f3":"markdown","12dda16d":"markdown","e011807d":"markdown","a69279f7":"markdown","74dc4018":"markdown","143b699d":"markdown","6dd17955":"markdown","40758366":"markdown","205271e9":"markdown","c1d68db2":"markdown","243ec676":"markdown","bf5bea2f":"markdown","c1cac41c":"markdown"},"source":{"26665ebc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","cec948bf":"# Load files\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ndisplay(test.head(15))\n\n# A brief statistical summary of the training dataset\ndisplay(train.describe())","76c68152":"# Analyze the data shape\ntrain_shape = train.shape\ntest_shape = test.shape\nall_data = pd.concat([train,test], sort=False)\nprint(\"Train set shape: \", train_shape)\nprint(\"Test set shape: \", test_shape)\n\n# Female\/Male survival ratio is an obvious candidate to key feature\nfemale_survival = len(train[(train.Sex=='female') & (train.Survived==1)])\/len(train[(train.Sex=='female')])\nmale_survival = len(train[(train.Sex=='male') & (train.Survived==1)])\/len(train[(train.Sex=='male')])\nprint(\"Percentage of females that survived: \", female_survival)\nprint(\"Percentage of males that survived: \", male_survival)\n\n# Now we plot some plots about survival rates\/counts\nfig, axes = plt.subplots(1, 2, figsize=(12,5))\naxes[0].bar(['male', 'female'], [male_survival,female_survival], alpha = 0.5)\naxes[0].set_xlabel('Sex')\naxes[0].set_ylabel(\"Survival fraction\")\naxes[0].set_title('Survival percentage')\naxes[0].set_ylim(0,1)\n\nsns.countplot(x='Sex', hue='Survived', ax=axes[1], data=train, palette='Set3')\naxes[1].set_title('Survival count')","93cbcbba":"train.drop('PassengerId', axis=1).hist(bins=20, figsize=(10, 8), alpha=0.5)\nplt.tight_layout()","31ac9827":"fig, axes = plt.subplots(1, 3, figsize=(15,5))\n\nsns.distplot(train[train['Sex']=='male']['Age'].dropna(), kde=False, color='blue', bins=30, ax=axes[0])\naxes[0].set_title('Age male distribution')\naxes[0].set_xlim(0, 80)\n\nsns.distplot(train[train['Sex']=='female']['Age'].dropna(), kde=False, color='orange', bins=30, ax=axes[1])\naxes[1].set_title('Age female distribution')\naxes[1].set_xlim(0, 80)\n\nsns.kdeplot(train[train['Sex']=='male']['Age'].dropna(), color='blue', ax=axes[2])\nsns.kdeplot(train[train['Sex']=='female']['Age'].dropna(), color='orange', ax=axes[2])\naxes[2].set_title('Continuous distribution by sex')","8e87d0f9":"fig, axes = plt.subplots(1, 3, figsize=(15,5))\n\nsns.countplot(x='Pclass', data=train, ax=axes[0], palette='Set1')\naxes[0].set_title('Pclass count')\n\nsns.barplot(x='Pclass', y='Pclass', hue='Sex', data=train, ax=axes[1], palette='Set3', estimator=lambda x: len(x) \/ len(train) * 100)\naxes[1].set(ylabel=\"Percent\")\naxes[1].set_title('Pclass by Sex')\n\nsns.barplot(x='Pclass', y='Pclass', hue='Survived', data=train, ax=axes[2], palette='Set2', estimator=lambda x: len(x) \/ len(train) * 100)\naxes[2].set_title('Pclass survival')\naxes[2].set_ylabel('')","c1b70b3d":"fig, axes = plt.subplots(1, 3, figsize=(15,5))\n\nsns.countplot(x='Embarked', data=train, ax=axes[0], palette='Set1')\naxes[0].set_title('Embarked port count')\n\nsns.countplot(x='Embarked', hue='Sex', data=train, ax=axes[1],  palette='Set3')\naxes[1].set_title('Sex distribution for each Embarked port')\naxes[1].set_ylabel('')\n\nsns.countplot(x='Embarked', hue='Survived', data=train, ax=axes[2], palette='Set2')\naxes[2].set_title('Survival rate by Embarked port')\naxes[2].set_ylabel('')","d6fb5179":"# Correlation matrix of relevant features\nprint(\"Correlations with Survived:\")\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = all_data[:len(train)].join(pd.get_dummies(all_data[:len(train)][features]), rsuffix=\"_dummies\", sort=False)\ncorr = X.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\nHighest_corr = corr.nlargest(30, 'Survived')['Survived']\nprint(Highest_corr)\n\nprint(\"Data Exploration: Completed\")","82dc92d3":"# Look for missing data \nmissings = {col:all_data[col].isnull().sum() for col in all_data.columns if all_data[col].isnull().sum()>0}\nprint(\"The following features contain missing values: \", missings)\n\n## FARE ##\n# Passenger with pass_id = 1044 has no Fare informed. He was 3rd class, let's replace his fare with the avg of 3rd class.\nall_data_sorted_fare = all_data\nall_data_sorted_fare.sort_values(by=['Fare'])\n#print(all_data_sorted_fare)\nall_data.set_value(all_data[all_data['PassengerId']==1044].index, 'Fare', all_data[all_data['Pclass']==3].Fare.mean())\n\n## EMBARKED ##\n# For the passengers without Embarked data, we will use the mode\nall_data['Embarked'].fillna(all_data['Embarked'].mode()[0], axis=0, inplace=True)\n\n## AGE ##\n# 263 passengers have no age informed. To fill these values, we need some feature engineering (next step)\n\n## CABIN ## \n# Most cabin values are missing. To fill these values, we need some feature engineering (next step)\n\nprint(\"Missing cleaning: Completed\")","93601d2e":"## CABIN ##\n# The letter of the cabin number is related to the docker of the ship (hypothesis). \nall_data['Docker_num'] = [cab[:1] if pd.notnull(cab) else \"Unknown\" for cab in all_data['Cabin']]\nall_data['Has_cabin_informed'] = [1 if pd.notnull(cab) else 0 for cab in all_data['Cabin']]\n\n\n## TITLE NAME ##\n# Mr, Mrs, Miss, Master, etc, are indicative of person status. Hence, it's interesting to extract titles feature\nall_data['Title'] = [re.search('\\,(.*)\\.', name).group(1) for name in all_data['Name']]\nall_data.set_value(all_data['PassengerId']==514, 'Title', ' Mrs')\ndictionary_of_titles = {\n    \" Capt\": \"Crew\",\n    \" Col\": \"Crew\",\n    \" Major\": \"Crew\",\n    \" Dr\": \"Crew\",\n    \" Rev\": \"Crew\",\n    \" Jonkheer\": \"VIP\",\n    \" Don\": \"VIP\",\n    \" Dona\": \"VIP\",\n    \" Sir\" : \"VIP\",\n    \" the Countess\":\"VIP\",\n    \" Lady\" : \"VIP\",\n    \" Mme\": \"Mrs\",\n    \" Ms\": \"Miss\",\n    \" Mrs\" : \"Mrs\",\n    \" Mlle\": \"Miss\",\n    \" Miss\" : \"Miss\",\n    \" Mr\" : \"Mr\",\n    \" Master\" : \"Master\"\n}\nall_data['Title'] = all_data.Title.map(dictionary_of_titles)\n\n\n## AGE ##\n# Function to fill missing age depending on title\ndef fill_missing_age_title(all_data):\n    # Average age for each title:\n    class_age_female_miss = all_data[(all_data['Title']=='Miss')]['Age'].dropna().mean()\n    class_age_female_mrs = all_data[(all_data['Title']=='Mrs')]['Age'].dropna().mean()\n    class_age_male_master = all_data[(all_data['Title']=='Master')]['Age'].dropna().mean()\n    class_age_male_mr = all_data[(all_data['Title']=='Mr')]['Age'].dropna().mean()\n    #class_age_male_crew = all_data[(all_data['Title']=='Crew')]['Age'].dropna().mean()\n    #class_age_male_vip = all_data[(all_data['Title']=='VIP')]['Age'].dropna().mean()\n    \n    # We fill missing age from the average age of the same Title\n    all_data[all_data['Age'].isnull()==True].head(5)\n    all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='Miss')].index, 'Age', class_age_female_miss)\n    all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='Mrs')].index, 'Age', class_age_female_mrs)\n    all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='Master')].index, 'Age', class_age_male_master)\n    all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='Mr')].index, 'Age', class_age_male_mr)\n    #all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='Crew')].index, 'Age', class_age_male_crew)\n    #all_data.set_value(all_data[(all_data['Age'].isnull()==True) & (all_data['Title']=='VIP')].index, 'Age', class_age_male_vip)\n    \n    return all_data\nall_data = fill_missing_age_title(all_data)\n        \n    \n## AGE CATEGORY ##\ndef age_cat(age):\n    if age <= 15:\n        return 1\n    if age <= 35:\n        return 2\n    if age <= 55:\n        return 3\n    if age > 55:\n        return 4\n    else:\n        return 0\nall_data['Age_cat'] = all_data['Age'].apply(age_cat)\n    \n    \n## FARE CATEGORY ##\ndef fare_cat(fare):\n    if fare < 15:\n        return 1\n    if fare < 35:\n        return 2\n    if fare < 100:\n        return 3\n    if fare > 100:\n        return 4\n    else:\n        return 0    \nall_data['Fare_cat'] = all_data['Age'].apply(fare_cat)\n\n\n## RELATIVES ##\n# Compute number of relatives in the ship\nall_data['Relatives'] = all_data['SibSp'] + all_data['Parch']\n\n\n## FAMILIES ##\n# Compute families based on surname and fare\ndef fill_families(all_data):\n    all_data['Last_Name'] = all_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n    # Random chance of surviving, 50%\n    default_survival_chance = 0.5\n    all_data['Family_Survival'] = default_survival_chance\n\n    # Group data by last name and fare - looking for families\n    for grp, grp_df in all_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                               'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n\n        # If not equal to 1, a family is found \n        # Then work out survival chance depending on whether or not that family member survived\n        if (len(grp_df) != 1):\n            for ind, row in grp_df.iterrows():\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n\n    all_data['IsAlone'] = 0\n    all_data.loc[all_data['Relatives'] == 0, 'IsAlone'] = 1 \n    return all_data\nall_data = fill_families(all_data)\n\n# Let's take a look to our new features\nall_data.head(5) ","0aedc190":"## FARE SKEWNESS ##\n# Skewness and Kurtosis analysis for Fare. Apply log transform if skew is too high (see graph below)\nsns.distplot(all_data['Fare'].dropna())\nplt.ylabel('Frequency')\nplt.title('Fare distribution')\n#print(\"Skewness: %f\" % all_data['Fare'].dropna().skew())\n#print(\"Kurtosis: %f\" % all_data['Fare'].dropna().kurt())\nall_data['Fare']=all_data['Fare'].apply(lambda x: np.log(x))","b4f86cd7":"## FARE SKEWNESS ##\n# Skewness and Kurtosis analysis for Age. Apply log transform if skew is too high\nsns.distplot(all_data['Age'].dropna())\nplt.ylabel('Frequency')\nplt.title('Age distribution')\n#print(\"Skewness: %f\" % all_data['Fare'].dropna().skew())\n#print(\"Kurtosis: %f\" % all_data['Fare'].dropna().kurt())\nall_data['Age']=all_data['Age'].apply(lambda x: np.log(x))\n                \nprint(\"Feature engineering: Completed\")","44063cff":"# Split again train\/test\nX = all_data[:len(train)]\nX_test_full = all_data[len(train):]\n\n# Split target variable\ny = X.Survived\nX.drop('Survived', axis=1, inplace=True)\nprint(len(all_data), len(X), len(X_test_full))\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.95, test_size=0.05, random_state=0)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype not in ['int64', 'float64']]\nprint(\"Low cardinality columns: \", low_cardinality_cols)\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\nprint(\"Numeric columns: \", numeric_cols)\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","511270da":"def xgb_optimize(X_train, y_train):\n    xgb1 = xgb()\n    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n                  'learning_rate': [.005, .004, .003, .002, .0009, 0.008], \n                  'max_depth': [4, 5, 6, 7],\n                  'min_child_weight': [4, 5, 6],\n                  'silent': [1],\n                  'subsample': [0.5],\n                  'colsample_bytree': [0.7],\n                  'n_estimators': [1000, 2500, 5000, 7500]}\n\n    xgb_grid = GridSearchCV(xgb1,\n                            parameters,\n                            cv = 3,\n                            n_jobs = 5,\n                            verbose=True)\n\n    xgb_grid.fit(X_train, y_train.astype(int))\n\n    print(xgb_grid.best_score_)\n    print(xgb_grid.best_params_)\n    \n# Uncomment the call to the xgb_optimize function to perform a (very) time consuming grid search \n# xgb_optimize(X_train, y_train)","aa56b435":"# Define model with best MAE\nmodel = xgb(colsample_bytree=0.7, learning_rate=0.0009, max_depth=6, min_child_weight=5, n_estimators=2500, \n                     nthread=1, silent=1, subsample=0.7, random_state=0, \n                     early_stopping_rounds = 10, eval_set=[(X_valid, y_valid)], verbose=False)\n\n# Train and test the model\n\nprint(\"Let's the training begin. Plase wait.\")\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('model', model)])\nmy_pipeline.fit(X_train, y_train.astype(int))\n\nprint(\"Training finished! Now let's predict test values.\")\n\npreds_test = my_pipeline.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': X_test.index+892, # We didn't preserved the indexes of rows, so we need to correct it manually. Not elegant at all but it works\n                       'Survived': preds_test.astype(int)})\noutput.to_csv('submission.csv', index=False)\n\nprint(\"Everything finished correctly!\")","bfbfd254":"# Cross validation accuracy for 3 folds\nscores = cross_val_score(my_pipeline, X_train, y_train,\n                              cv=5,\n                              scoring='accuracy')\nprint(scores)","eda46d08":"for pass_id in [956,981,1053,1086,1088,1199,1284,1309]:\n    output.set_value(output['PassengerId']==pass_id, 'Survived', 1)\n\nfor pass_id in [910,925,929,1024,1032,1080,1172,1176,1257,1259]:\n    output.set_value(output['PassengerId']==pass_id, 'Survived', 0)\n\n# Analysis of particular parentship\/Sex casuistics that should lead to predictable outputs\n#output.set_value(output[output[output['PassengerId'].isin([956,981,1053,1086,1088,1199,1284,1309])], 'Survived', [1,1,1,1,1,1,1,1]])\n#output.set_value(output[output[output['PassengerId'].isin([956,981,1053,1086,1088,1199,1284,1309])], 'Survived', [0,0,0,0,0,0,0,0]])\noutput[output['PassengerId'].isin([956,981,1053,1086,1088,1199,1284,1309])]\noutput[output['PassengerId'].isin([910,925,929,1024,1032,1080,1172,1176,1257,1259])]\n\nall_data[all_data['PassengerId']==864] # Had 14.5 years, algorithm predicted 21.8\nall_data[all_data['PassengerId']==29] # Had 22.7 years, algorithm predicted 21.8\noutput[output['PassengerId']==1298] # OK\noutput[output['PassengerId']==1301] # OK\noutput[output['PassengerId']==1300] # OK\noutput[output['PassengerId']==893] # Fail (47y, has Sib\/Sp=1, 3rd class)\nall_data[all_data['PassengerId'].isin(output[output['Survived']==0]['PassengerId'])]","ce9e3e11":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': X_test.index+892,\n                       'Survived': preds_test.astype(int)})\noutput.to_csv('submission.csv', index=False)","cb407097":"# 2. Clean missing data <a id=\"section2\"><\/a>\n\nIn real life, almost all datasets you find have tons of missing data. To decide how to process missings, either by directly dropping a column with a lot of nulls or by rhardcoding certain values, may be critical for our final machine learning performance. \n\nIn this section, we will cover only two features: \n* Fare (1 missing) \n* Embarked (2 missings)\n\nThe other two missing columns (Age -263 missings- and Cabin -1014 missings-) will be tackled in the next step (feature engineering), since they require more advanced techniques or custom features to deal with their missings.","285fd269":"**Observations**:\n1. As commented in section 2.1, there is almost the same number of 3rd class passengers than 1st and 2nd together\n2. The difference between class passengers is larger in males than in females\n3. Indeed, class type is fundamental for survival rate. The higher the class, the more likely you are to survive","962d57c4":"**Observations**: \n1. Most people embarked in the first port, Southampton\n2. The distribution of male\/females are similar for all 3 ports\n3. However, survival rates are strongly dependent on the port. People embarked in Cherbourg had better chances to survive than to die","9fcbae56":"## 1.2. Age distribution <a id=\"section12\"><\/a>\n\nBack in the 1900's, there was a big seggregation by sex in many aspects. Let's verify if this is true for our dataset:","e578fffc":"**Observations**:\n1. Age follows more or less normal distribution (for such a small dataset, <1000 passengers).\n2. Fare distribution follows the expected exponentially decay. Typically wealth distributions are exponential.\n3. There is almost the same number of 3rd class passengers than 1st and 2nd together.","848ae47c":"# 3. Feature engineering <a id=\"section3\"><\/a>\n\nData exploration gave us some insight about how datasets are structured and which features could be more relevant for our model. Missing values from Fare and Embarked have been replaced, but but not those from Age and Cabin. Moreover, there's still room for improvement in our dataset, and in this step we will try to squeeze out all information available.\n\nA brief summary of what we will do in this step:\n* **Docker_num**. From the Cabin column.\n* **Has_cabin_informed** (flag). From the Cabin column.\n* **Title**. Specified in the passenger's name (i.e. Kelly, **Mr**. James). We will group some title as VIPS and Crew\n* **Age missings**. Replace null values with the mean of the people with the same Title.\n* **Age_cat**. Age groups may help to diferentiate even more people, adding a non-continuous dimension of the age\n* **Fare_cat**. Same as Age, create bins of fare paid for the ticket\n* **Relatives**. Compute the number of relatives, SibSp + Parch\n* **Family_survival**. Compute families of passengers from last_name and fare information\n* **Is_Alone** (flag). If Relatives=0, then Is_Alone=1\n* **Skewness analysis**. Analyze Fare and Age distribution to see if they are highly non-normal. Apply corrections in this case, log(x)","1b04b7e0":"# 5. Parameter optimization of the XGB model <a id=\"section5\"><\/a>\n\nIn order to get the best of the possible scores, it's recommended some fine tunning of our model. I chose XGBoost as my machine learning model, and in this step we will perform a grid search of parameters to find the combination with best score over 3 cross validation folds.","d311c0e4":"Let's now analyze the size of our train and test datasets, as well as an initial survey on male\/female survival rate:","eac997ea":"## 3.1. Skewness analysis <a id=\"section31\"><\/a>\n\nSometimes, numeric distributions do not follow a normal distribution but they are skewed (either positive or negative). This is important for several reasons, you can see a good thread for the House pricing competition in [here](https:\/\/stats.stackexchange.com\/questions\/299154\/the-benefit-of-unskewing-skewed-data). To sum up let's say that skewness is import due to two main facts:\n* Certain features should be interpreted (and computed) in a multiplicative way. For example, the relative  difference between 10\u20ac and 100\u20ac is much higher than between 10.010\u20ac and 10.100\u20ac.\n* A number of machine learning algorithms give better predictions for normally distributed data\n\nHence, it's good to check skewness in potential features, and eventually transform them if necessary (typically with a log transform, but there are many options). For us, both Fare and Age are candidates.","7b6f37ad":"**Observations**:\n\n* There's a huge difference in the survival rate between males and females. The reason for this is the traditional behavior tendency summarized by *\"women and children first\"*, so that men were prone to stay out from rescue boats. Hence, the gender feature (Sex) will be critical for our prediction algorithm. ","53af628f":"## 1.5. Correlation matrix <a id=\"section15\"><\/a>\n\nCreate a correlation matrix to detect dependencies between features. Print explicitly the correlation values for all features with respect to the target (Survived).","dc3a77f3":"Feature types observed from the displayed data:\n* **Categorical**: Survived, Sex, and Embarked. \n* **Numeric**: Pclass, Age, Fare, SibSp and Parch.\n* **Mixed**: Cabin","12dda16d":"It can be concluded that both the sex of the passenger and the fare paid for the ticket are the most important features for the survival of a person.","e011807d":"## 1.3. Passenger class distribution (Pclass) <a id=\"section13\"><\/a>\n\nThere are 3 main passenger classes in the dataset, which is a clear candidate of key feature for the survival rate. One would expect that higher class passengers survive more often than lower ones. ","a69279f7":"**Observations**:\n1. There's a reasonable difference between age distribution shapes. Female distr. is roughly gaussian, while males distr. is more skewed\n2. Females tend to be younger than males\n3. Males age distribution has a longer tail, since there are older men passengers (>60 years)","74dc4018":"And that's it, we have our final submission. Hope you enjoyed the kernel and see you around!","143b699d":"# 1. Exploratory data analysis (EDA) <a id=\"section1\"><\/a>\n\nProceed to read all files provided by Titanic's Kaggle competition, and print a subset to see how data is structured.","6dd17955":"## 1.1. Variable distributions <a id=\"section11\"><\/a>\nHow other variables are distributed? This analysis allows to find  outliers and key behaviors,","40758366":"# 7. Ad-hoc rules (optional) <a id=\"section7\"><\/a>\n\nThe following code block is completely optional. Here we modify by hand some passengers that have particular cases: \n* Any mother whose daughters\/sons died, is also dead\n* Any child whose  mother and sisters\/brothers lived, is alive\n\nThis procedure is irrespective of our previous predictions, but it seems a logical rule to follow. However, we can verify that it only modifies one of our predictions.\nFinally, we perform an analysis of how well our missing replacements worked, by checking some passengers' real ages.","205271e9":"## 1.4. Embarkation port <a id=\"section14\"><\/a>\n\nThere are 3 informed embarcations: S = Southampton, C = Cherbourg and Q = Queenstown. This could be related to where passengers were located in the ship, or the order of the cabins assigned. ","c1d68db2":"Cross validation scores are pretty high, thanks to our extensive feature engineering and the XGB parameter optimization.","243ec676":"# **Titanic comp: EDA, feature engineering & XGBoost** \n\nThe aim of this kernel is to predict which passengers from the historical Titanic catastrophe survived based on raw personal data. \nThe kernel may be useful as a reference for any data scientist wishing to review a complete walkthrough through all the steps involved in any machine learning algorithm.\n\n**TABLE OF CONTENTS**\n\n1. [Exploratory data analysis (EDA)](#section1)\n2. [Cleaning of missing data](#section2)\n3. [Feature engineering](#section3)\n4. [Preparation of data](#section4)\n5. [Parameter optimization of the XGB model](#section5)\n6. [Final prediction and submit](#section6)\n7. [Ad-hoc rules (optional)](#section7)","bf5bea2f":"# 4. Preparation of data <a id=\"section4\"><\/a>\n\n\nFinally, let's proceed to split data, extract the target column and encode cateogorical data (One Hot).\n\nData split:\n* Train (95% of the original train)\n* Validation (5% of the original train)\n* Test (original test passengers)","c1cac41c":"# 6. Prediction and submit <a id=\"section6\"><\/a>\n\nFinally, we know the set of parameters that optimize our score, and there is only pending the definitive prediction. \nYou will se that I used a very trivial Pipeline, but this is just for best practices purposes."}}