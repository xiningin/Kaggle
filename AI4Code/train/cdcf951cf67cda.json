{"cell_type":{"efcb8d57":"code","180911be":"code","f69e94c4":"code","8fdd2b6b":"code","ce4de120":"code","09478702":"code","1b8be56f":"code","befacaec":"code","c1418d28":"code","48192063":"code","80e2bf45":"code","888bdc2c":"code","a715eb38":"code","bbd7e2f6":"code","af5b4682":"code","e09c2b3e":"code","fde84928":"code","8b282a93":"code","b4ef44ee":"code","8c73829c":"code","a965a639":"code","5efe9d2d":"code","1155d6bd":"code","45d510ab":"code","005cbd9c":"code","558cd031":"code","73f2d39a":"code","97690062":"code","9cb26372":"code","e90a8460":"code","5a7176cb":"code","a15f7b15":"code","a5a4e920":"code","a787011b":"code","e7f21771":"code","664f24c8":"code","ed2ab4a2":"code","6cb61b06":"code","f2da6736":"code","f1d5508d":"code","a84ebd41":"code","9eb79172":"code","1ddd2818":"code","cb723685":"code","c7f1435f":"code","85713a4f":"code","64b9855e":"code","708e6459":"code","9de227c6":"code","139db890":"code","12c3fedf":"code","b894c5a3":"code","775e3518":"code","99ad641b":"code","9fd3f476":"code","ac2e533b":"code","7aa1ca57":"code","5680fceb":"markdown","6fa994b4":"markdown","4e1096bc":"markdown","79502816":"markdown","c73febec":"markdown","2a9e7901":"markdown","f53ff54f":"markdown","571dad4e":"markdown","3781c0b1":"markdown","93f55bcf":"markdown","e5cf8e6e":"markdown","8f77fbba":"markdown","cce8b835":"markdown","d8b9bab7":"markdown","d68ebf61":"markdown","f6530de9":"markdown","bcf84616":"markdown","15d6c119":"markdown","c7881097":"markdown","b3c2608d":"markdown","31e211f7":"markdown","96db7b38":"markdown","a4aedce1":"markdown","bce9069b":"markdown","0bd00d09":"markdown","89e45100":"markdown","5c1833c7":"markdown","f6f3f835":"markdown","3c57db9d":"markdown","c05160e6":"markdown","c610a9ce":"markdown","ae998610":"markdown","2e23cb13":"markdown","09ab47db":"markdown","997c8c9b":"markdown","f394cb3a":"markdown","bb7cb7e1":"markdown","a4bc8dfb":"markdown","17ff07a2":"markdown","442c9531":"markdown","e8fdfda2":"markdown"},"source":{"efcb8d57":"## all imports\nfrom urllib.request import urlopen\nfrom IPython.display import HTML\nimport numpy as np\n#import urllib2\nimport bs4 #this is beautiful soup\nimport time\nimport operator\nimport socket\n#import cPickle\nimport re # regular expressions\n\nfrom pandas import Series\nimport pandas as pd\nfrom pandas import DataFrame\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_context(\"talk\")\nsns.set_style(\"white\")","180911be":"\n# pass in column names for each CSV\nuser_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n\nusers = pd.read_csv(\n    'http:\/\/files.grouplens.org\/datasets\/movielens\/ml-100k\/u.user', \n    sep='|', names=user_cols)\n\nusers.head()","f69e94c4":"ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv(\n    'http:\/\/files.grouplens.org\/datasets\/movielens\/ml-100k\/u.data', \n    sep='\\t', names=ratings_cols)\n\nratings.head()","8fdd2b6b":"# the movies file contains columns indicating the movie's genres\n# let's only load the first five columns of the file with usecols\nmovie_cols = ['movie_id', 'title', 'release_date', \n            'video_release_date', 'imdb_url']\n\nmovies = pd.read_csv('http:\/\/files.grouplens.org\/datasets\/movielens\/ml-100k\/u.item', sep='|', \n                     names=movie_cols, usecols=range(5), encoding = \"ISO-8859-1\")\nmovies.head()","ce4de120":"print(movies.dtypes)\nprint (movies.describe())\n# *** Why only those two columns? ***","09478702":"A = users.head()\nB = users['occupation'].head()\n\ncolumns_you_want = ['occupation', 'sex'] \nD = users[columns_you_want].head()\n\nprint (A, \"\\n========\")\nprint (B, \"\\n========\")\nprint (D)","1b8be56f":"oldUsers = users[users['age'] > 25]\noldUsers.head()","befacaec":"# users aged 40 AND male\n# your code here\n","c1418d28":"users[(users.age == 40) & (users.sex == \"M\")].head(3)","48192063":"users[(users[\"age\"] == 40) & (users[\"sex\"] == \"M\")].head(3)","80e2bf45":"# but what if i want to see only age and sex\n# your code here\ncolumns_you_want_2 = users[['age', 'sex']]\nprint(columns_you_want_2.head())","888bdc2c":"#solution_cont\nolder = columns_you_want_2[(columns_you_want_2.age == 40) & (columns_you_want_2['sex'] == 'M')]\nolder.head()","a715eb38":"## users who are female and programmers\n# your code here\n\n## show statistic summary or compute mean\n# your code here\n","bbd7e2f6":"users['occupation'].unique()","af5b4682":"females = users[(users.sex == \"F\") & (users.occupation == \"programmer\")].head(5)\nfemales","e09c2b3e":"#If we want only those column (sex, occupation)\ncolumns_you_want = users[['sex', 'occupation']]\nprint(columns_you_want.head())","fde84928":"#solution_cont\nocc = columns_you_want[(columns_you_want['sex'] == 'F') & (columns_you_want['occupation'] == 'programmer')]\nocc.head()","8b282a93":"# a smarter way\ncolumns_you_want_better = females[['sex', 'occupation']]\ncolumns_you_want_better","b4ef44ee":"print (ratings.head())\nprint(\"=======\")","8c73829c":"\n## split data\n#grouped_data = ratings.groupby('user_id')\ngrouped_data = ratings['movie_id'].groupby(ratings['user_id'])\n#print(grouped_data.head(5))\n\n## count and combine\nratings_per_user = grouped_data.count()\n\nratings_per_user.head(5)","a965a639":"#Other method\nratings.set_index([\"user_id\", \"movie_id\"]).count(level=\"user_id\").head()","5efe9d2d":"ratings.count()","1155d6bd":"## split data\n\n# your code here\ngrouped_data_1 = ratings['rating'].groupby(ratings['movie_id'])\ngrouped_data_1.head(2)","45d510ab":"## average and combine\n# your code here\\\naverage_ratings = grouped_data.mean()\nprint (\"Average ratings:\")\nprint (average_ratings.head())","005cbd9c":"# get the maximum rating\n# your code here\nmaximum_rating = average_ratings.max()\nmaximum_rating","558cd031":"# get movie ids with that rating\n# your code here\ngood_movie_ids = average_ratings[average_ratings == maximum_rating].index\ngood_movie_ids","73f2d39a":"print (\"Good movie ids:\")\nprint #your code here\nprint (good_movie_ids)\nprint (\"===============\\n=============\")\nprint (\"Best movie titles\")\nprint # your code here\nprint (movies[movies.movie_id.isin(good_movie_ids)].title)","97690062":"# get number of ratings per movie\n# your code here\nhow_many_ratings = grouped_data.count()\nprint (\"Number of ratings per movie\")\nprint # your code here\nprint (how_many_ratings[average_ratings == maximum_rating])","9cb26372":"average_ratings = grouped_data.apply(lambda f: f.mean())\naverage_ratings.head()","e90a8460":"# get the average rating per user\n# your code here\n","5a7176cb":"grouped_data = ratings['rating'].groupby(ratings.user_id)\naverage_ratings = grouped_data.mean()\naverage_ratings.head()","a15f7b15":"# list all occupations and if they are male or female dominant\n# your code here","a5a4e920":"grouped_data = users['sex'].groupby(users['occupation'])\nmale_dominant_occupations = grouped_data.apply(lambda f: \n                                               sum(f == 'M') > sum(f == 'F'))\nprint (male_dominant_occupations)\nprint ('\\n')","a787011b":"print ('number of male users: ')\nprint (sum(users['sex'] == 'M'))\n\nprint ('number of female users: ')\nprint (sum(users['sex'] == 'F'))","e7f21771":"\nhtmlString = \"\"\"<!DOCTYPE html>\n<html>\n  <head>\n    <title>This is a title<\/title>\n  <\/head>\n  <body>\n    <h2> Test <\/h2>\n    <p>Hello world!<\/p>\n  <\/body>\n<\/html>\"\"\"\n\nhtmlOutput = HTML(htmlString)\nhtmlOutput","664f24c8":"import requests as req","ed2ab4a2":"url = 'http:\/\/www.crummy.com\/software\/BeautifulSoup'\nsource = req.get(url)\nprint (source.status_code) #To check that a request is successful, use r.raise_for_status() or check r.status_code is what you expect.","6cb61b06":"print(source.headers)","f2da6736":"print(source.headers[\"Content-Type\"])","f1d5508d":"print(source.text[:480])","a84ebd41":"params = {\"query\": \"python download url content\", \"source\":\"chrome\"}\nsource2 = req.get(\"http:\/\/www.google.com\/search\", params=params)\nprint(source2.status_code)","9eb79172":"re.findall(r\"Soup\", source.text)","1ddd2818":"soup = re.search(r\"Soup\", source.text)\nprint(soup)","cb723685":"## get bs4 object\nsoup = bs4.BeautifulSoup(source.text)","c7f1435f":"## compare the two print statements\nprint (soup)\n#print soup.prettify()","85713a4f":"## show how to find all a tags\nsoup.findAll('a')\n\n## ***Why does this not work? ***\n#soup.findAll('Soup')","64b9855e":"## get attribute value from an element:\n## find tag: this only returns the first occurrence, not all tags in the string\nfirst_tag = soup.find('a')\nprint(first_tag)\n","708e6459":"## get attribute `href`\nprint(first_tag.get('href'))\n","9de227c6":"\n## get all links in the page\nlink_list = [l.get('href') for l in soup.findAll('a')]\nlink_list\n\n# or\n# link_list = []\n# for l in soup.findAll('a'):\n#     link_list.append(l.get('href'))\n# link_list","139db890":"# So, to find all the soup. We search within the tags\n\nlink_list = [l.get('Soup') for l in soup.findAll('html')]\nlink_list","12c3fedf":"# Get your own at https:\/\/github.com\/settings\/tokens\/new\ntoken = \"\" \nresponse = req.get(\"https:\/\/api.github.com\/user\", params={\"access_token\":token})\n\n#print(response.status_code)\nprint(response.headers[\"Content-Type\"])\nprint(response.json().keys())","b894c5a3":"response = req.get(\"https:\/\/api.github.com\/user\", auth=(\"kennydukor@gmail.com\", \"github_password\"))\nprint(response.status_code)\nprint(response.headers[\"Content-Type\"])\nprint(response.json())\nprint(response.json().keys())","775e3518":"print(response.content)","99ad641b":"import json\nprint(json.loads(response.content))","9fd3f476":"data = {\"a\":[1,2,3,{\"b\":2.1}], 'c':4}\njson.dumps(data)","ac2e533b":"#json.dumps(response)","7aa1ca57":"a = {'a': 1, 'b':2}\ns = json.dumps(a)\na2 = json.loads(s)\n\n## a is a dictionary\nprint (a)\n## vs s is a string containing a in JSON encoding\nprint (s)\n## reading back the keys are now in unicode\nprint (a2)","5680fceb":"Lets find the diligent users","6fa994b4":"# Filtering Data\n\nSelect users older than 25","4e1096bc":"We have already seen that we can use the `response.json()` call to convert this to a Python dictionary, but more common is to use the `json` library in the Python standard library: documentation page [here](https:\/\/docs.python.org\/3\/library\/json.html).  To convert our GitHub response to a Python dictionary manually, we can use the `json.loads()` (load string) function like the following.","79502816":"Notice that Python code, unlike JSON, can include single quotes to denote strings, but converting it to JSON will replace it with double quotes.  Finally, if you try to dump an object that includes types not representable by JSON, it will throw an error.","c73febec":"# Selecting data\n\n- DataFrame => group of Series with shared index\n- single DataFrame column => Series","2a9e7901":"# Split-apply-combine\n\n- splitting the data into groups based on some criteria\n- applying a function to each group independently\n- combining the results into a data structure","f53ff54f":"The token element there (that is an example that was linked to my account, which I have since deleted, you can get your own token for your account at https:\/\/github.com\/settings\/tokens\/new) identifies your account, and because this is a REST API there is no \"login\" procedure, you just simply include this token with each call to identify yourself. The call here is just a standard HTTP request: it requests the URL https:\/\/api.github.com\/user passing our token as the parameter access_token. The response looks similar to our above response, except if we look closer we see that the \"Content-Type\" in the HTTP header is \"application\/json\". In these cases, the requests library has a nice function, response.json(), which will convert the returned data into a Python dictionary (I'm just showing the keys of the dictionary here).","571dad4e":"# Scraping with Python\n\n- different useful libraries:\n- - urllib\n- - beautifulsoup\n- - pattern\n- - soupy\n- - LXML","3781c0b1":"\n# Read the ratings","93f55bcf":"#### What went wrong?\n`read_csv` takes an encoding option to deal with files in different formats. I mostly use `read_csv('file', encoding = \"ISO-8859-1\")`, or alternatively `encoding = \"utf-8\"` for reading, and generally` utf-8 for to_csv.`\n\nYou can also use one of several alias options like 'latin' instead of `'ISO-8859-1' `(see python docs, also for numerous other encodings you may encounter).\n\nSee relevant Pandas documentation, python docs examples on csv files, and plenty of related questions here on SO.\n\nTo detect the encoding (assuming the file contains non-ascii characters), you can use enca (see man page) or file -i (linux) or file -I (osx) (see man page).**bold text**","e5cf8e6e":"Besides the HTTP GET command, there are other common HTTP commands (POST, PUT, DELETE) which can also be called by the corresponding function in the library.","8f77fbba":"# Get information about data","cce8b835":"# References","d8b9bab7":"## Important Note\n\nThere's one very important point here, which may be obvious to you if you've spend substantial time doing any kind of software development, but if most of your experience with programming is via class exercises, it may not be completely apparent, so I emphasize it here. You will see code samples like this throughout the course, in the slides and in these notes. It's important not to take this to mean that you should memorize these precise function calls, or even do anything other than just scan over them briefly. As a data scientist, you'll be dealing with hundreds of different libraries and APIs, and trying to commit them all to memory is not useful. Instead, what you need to develop is the ability to quickly find a library and function call that you need to accomplish some task. For example, even if you know nothing about the in this case, you want to download the content of some URL. You can type into Google something like \"Python download url content\" (I just picked this precise phrasing randomly, feel free to try some variants on this). The first result for my search is a Stack Overflow page: How do I download a file over HTTP using Python?. While the first response actually lists the urllib2 package (this was the more common library at one point, but the requests library provides a simpler interface that does things like automatically encode parameters to urls and other niceties), the requests library home page is a few responses down. And once you find the home page for that library, the very first example on the page shows how to use it for simple calls like the one above. You can look through documentation here, but like above, if you have a question about the requests library, you can likely use good for a direct answer there too. For instance, if you want to learn to use the POST command, you can Google something like \"python requests library post command\" and the searches will either bring you straight to the relevant requests documentation or to a Stack Overflow page.","d68ebf61":"Quiz :\n\n- Is the word 'Alice' mentioned on the beautiful soup homepage?\n- How often does the word 'Soup' occur on the site?\n- - hint: use .count() \n- At what index occurs the substring 'alien video games' ?\n- - hint: use .find() \n","f6530de9":"You probably have seen URLS like this before\n\nhttps:\/\/www.google.com\/search?q=python+download+url+content&source=chrome\nThe https:\/\/www.google.com\/search string is the URL, and everything after the ? are parameters; each parameter is of the form \"parameter=value\" and are separated by ampersands &. If you've seen URLS before you've noticed that a lot of content needs to be encoded in these parameters, such as spaces replaces with the code \"%20\" (the Google url above can also handle the \"+\" character, but \"%20\" is the actual encoding of a space). Fortunately, requests handles all of this for you. You can simply pass all the parameters as a Python dictionary.","bcf84616":"### RESTful APIs\n\nWhile parsing data in HTML (the format returned by these web queries) is sometimes a necessity, and we'll discuss it further before, HTML is meant as a format for displaying pages visually, not as the most efficient manner for encoding data.  Fortunately, a fair number of web-based data services you will use in practice employ something called REST (Representational State Transfer, but no one uses this term) APIs.  We won't go into detail about REST APIs, but there are a few main feature that are important for our purposes:\n\n1. You call REST APIs using standard HTTP commands: GET, POST, DELETE, PUT.  You will probably see GET and POST used most frequently.\n2. REST servers don't store state.  This means that each time you issue a request, you need to include all relevant information like your account key, etc.\n3. REST calls will usually return information in a nice format, typically JSON (more on this later).  The `requests` library will automatically parse it to return a Python dictionary with the relevant data.\n\nLet's see how to issue a REST request using the same method as before.  We'll here query my GitHub account to get information.  More info about GitHub's REST API is available at their [Developer Site](https:\/\/developer.github.com\/v3\/).","15d6c119":"- by Healey and Ramaswamy\n\n- http:\/\/www.csc.ncsu.edu\/faculty\/healey\/tweet_viz\/tweet_app\/\n\nType a keyword into the input field, then click the Query button. Recent tweets that contain your keyword are pulled from Twitter and visualized in the Sentiment tab as circles. Hover your mouse over a tweet or click on it to see its text.","c7881097":"![alt text](https:\/\/camo.githubusercontent.com\/aee227c701091e56cfe6479ccc8f6a757c0d6c94\/687474703a2f2f7777772e6373632e6e6373752e6564752f666163756c74792f6865616c65792f74776565745f76697a2f666967732f74776565742d76697a2d65782e706e67)","b3c2608d":"But the best movie rating should also be dependent on the number of people that rated the movie","31e211f7":"# HTML\n\n- HyperText Markup Language\n- standard for creating webpages\n- HTML tags\n- - have angle brackets\n- - typically come in pairs\n\n\nThis is an example for a minimal webpage defined in HTML tags. The root tag is '< html>' and then you have the < head> tag. This part of the page typically includes the title of the page and might also have other meta information like the author or keywords that are important for search engines. The < body> tag marks the actual content of the page. You can play around with the < h2> tag trying different header levels. They range from 1 to 6.","96db7b38":"### JSON data\n\nAlthough originally built as a data format specific to the Javascript language, JSON (Javascript Object Notation) is another extremely common way to share data.  We've already seen in it with the GitHub API example above, but very briefly, JSON allows for storing a few different data types:\n\n- Numbers: e.g. `1.0`, either integers or floating point, but typically always parsed as floating point\n- Booleans: `true` or `false` (or `null`)\n- Strings: `\"string\"` characters enclosed in double quotes (the `\"` character then needs to be escaped as `\\\"`)\n- Arrays (lists): `[item1, item2, item3]` list of items, where item is any of the described data types\n- Objects (dictionaries): `{\"key1\":item1, \"key2\":item2}`, where the keys are strings and item is again any data type\n\nNote that lists and dictionaries can be nested within each other, so that, for instance\n\n    {\"key1\":[1.0, 2.0, {\"key2\":\"test\"}], \"key3\":false}\n\nwould be a valid JSON object.\n\nLet's look at the full JSON returned by the GitHub API above:","a4aedce1":"# Some other examples","bce9069b":"# Pulling Data from an already exising table\n\nWe will be exploring the movielens data using pandas\n\nhttp:\/\/grouplens.org\/datasets\/movielens\/ \n\nExample inspired by Greg Reda: http:\/\/www.gregreda.com\/2013\/10\/26\/using-pandas-on-the-movielens-dataset\/","0bd00d09":"CMU datascience course (data College and scraping) http:\/\/www.datasciencecourse.org\/lectures\/\n\n\nHarvard datascience course (Web scraping) http:\/\/cs109.github.io\/2015\/pages\/videos.html","89e45100":"# Quiz:\n\n- show users aged 40 and male\n\n- show the mean age of female programmers","5c1833c7":"so in summary","f6f3f835":"# API registrations","3c57db9d":"If you would like to run all the examples in this notebook, you need to register for the following APIs:\n\n- Github\n\nhttps:\/\/github.com\/settings\/tokens\/new\n\n- Twitter\n\nhttps:\/\/apps.twitter.com\/app\/new\n\n- Twitter Instructions\n\nhttps:\/\/twittercommunity.com\/t\/how-to-get-my-api-key\/7033","c05160e6":"\n# Data about the movies","c610a9ce":"\n\n- by Justin Blinder\n- http:\/\/projects.justinblinder.com\/We-Read-We-Tweet\n\u201cWe Read, We Tweet\u201d geographically visualizes the dissemination of New York Times articles through Twitter. Each line connects the location of a tweet to the contextual location of the New York Times article it referenced. The lines are generated in a sequence based on the time in which a tweet occurs. The project explores digital news distribution in a temporal and spatial context through the social space of Twitter.","ae998610":"This code issues an \"HTTP GET\" request to load the content of the paper, and returns it in the response object. The status_code field contains the \"200\" code, which indicates a successful query, and the headers field contains meta-information about the page (in this case, you could see, for instance, that despite the URL, we're actually hosting this page on github). If you want to see the actual content of the page, you can use the response.content or response.text fields, as below.","2e23cb13":"# Quiz\n\n- get the average rating per user\n- advanced: list all occupations and if they are male or female dominant","09ab47db":"# Passing a Function","997c8c9b":"\n# Useful Tags\n\n- heading < h1>< \/h1> ... < h6>< \/h6>\n- paragraph < p>< \/p>\n- line break < br>\n- link with attribute\n\n< a href=\"http:\/\/www.example.com\/\">An example link< \/a>","f394cb3a":"If you have the data as a file (i.e., as a file descriptor opened with the Python `open()` command), you can use the `json.load()` function instead.  To convert a Python dictionary to a JSON object, you'll use the `json.dumps()` command, such as the following.","bb7cb7e1":"General advice about programming\n\n- You will find nearly everything on google\n- Try: length of a list in python\n- A programmer is someone who can turn stack overflow snippets into running code\n- Use tab completion\n- Make your variable names meaningful","a4bc8dfb":"### Authentication\n\nMost APIs will use an authentication procedure that is more involved than this example above.  The standard here for a while was called \"Basic Authentication\", and can be used via the `requests` library by simply passing the login and password as the `auth` argument to the relevant calls, as below. ","17ff07a2":"# Read the user data","442c9531":"# Python data scraping\n\n- Why scrape the web?\n- - vast source of information\n- - automate tasks\n- - keep up with sites\n- - fun!","e8fdfda2":"# Quiz\n\n- get the average rating per movie\n- advanced: get the movie titles with the highest average rating"}}