{"cell_type":{"1d3c71eb":"code","cc3f3769":"code","2e16d8fb":"code","541c3eb6":"code","f04d995a":"code","b78d2b5b":"code","b4d6dba3":"code","55726d04":"code","f27d4bae":"code","d200992b":"code","b9c6c35c":"code","4f825be8":"code","c1e7ace3":"code","3723cdeb":"code","1f93caf5":"code","3d216a4e":"code","bac5bd6b":"code","7917a96e":"code","c9428ec4":"code","28fa1aee":"code","a3271988":"code","c2af2197":"code","809b4127":"code","ed02a6cf":"code","40747740":"code","b82bb303":"code","a7239c39":"code","ef15869a":"code","05047aca":"code","8a8c2eaa":"code","8e2f5963":"code","033939b6":"code","3fddc5f2":"code","9f6cd1fb":"code","3ed58038":"code","48ab851c":"code","60a72b36":"code","6d1669bd":"code","075f6be2":"code","7a0657a4":"code","1369bbcd":"code","b5657161":"code","7368fdb0":"code","49da64af":"code","7beb228d":"code","013c62f4":"code","bccad811":"code","9af992fc":"markdown","038b4e71":"markdown","18b73204":"markdown","b3c13690":"markdown","66ed8d52":"markdown","38073dbf":"markdown","bc332308":"markdown","2c40c339":"markdown","8c07a0c6":"markdown","36784a1b":"markdown","a2fd5377":"markdown","15acd33a":"markdown","d2e94a5f":"markdown","a19c914d":"markdown","379dc615":"markdown","7526dd71":"markdown","5407363a":"markdown","c5abad2b":"markdown","56e0e11e":"markdown","6187666f":"markdown","a4b25218":"markdown","99c58f32":"markdown","70e45827":"markdown","207cb7fd":"markdown","88b62e1a":"markdown","355896f2":"markdown","16a0d33e":"markdown","d00edf16":"markdown","1ece83a2":"markdown","b72931db":"markdown","6fa4df0a":"markdown","4a4cd5ad":"markdown","3a670438":"markdown","ace1212b":"markdown","bf02f8a9":"markdown","50d1d6e3":"markdown","bff6a5e4":"markdown","8c500c1f":"markdown","e9849fb6":"markdown","ddca6f3c":"markdown","b4c075e8":"markdown","d3642993":"markdown","a0f46858":"markdown","09dc7826":"markdown","9be99eca":"markdown","02f5f26d":"markdown","86dd9307":"markdown","f4f668b5":"markdown","93ac9e1f":"markdown","9281be2e":"markdown","56f7bf50":"markdown","4a411224":"markdown","4d6303b4":"markdown","096d82b8":"markdown","a7dd6423":"markdown","d0f98c8c":"markdown"},"source":{"1d3c71eb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.simplefilter(action='ignore')","cc3f3769":"df = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv')\ndf.head()","2e16d8fb":"df.shape","541c3eb6":"df.duplicated().any()","f04d995a":"df = df.drop_duplicates()","b78d2b5b":"df.shape","b4d6dba3":"df.info()","55726d04":"df.drop(['torque'],axis=1, inplace = True)\ndf.head()","f27d4bae":"df.isnull().any()","d200992b":"df.isnull().sum() \/ df.shape[0] * 100","b9c6c35c":"df.dropna(axis=0, inplace=True)\ndf.isnull().any()","4f825be8":"df.shape","c1e7ace3":"df['age'] = 2021 - df['year']\ndf.drop(['year'],axis = 1,inplace = True)\ndf['owner'] = df['owner'].replace({'First Owner': 1, 'Second Owner': 2, 'Third Owner': 3})\ndf.head()","3723cdeb":"df['mileage'] = df['mileage'].str.strip('kmpl').str.strip('km\/kg')\ndf['engine'] = df['engine'].str.strip('CC')\ndf['max_power'] = df['max_power'].str.strip('bhp').str.strip()\ndf.head()","1f93caf5":"df['mileage'] = pd.to_numeric(df['mileage'])\ndf['engine'] = pd.to_numeric(df['engine'])\ndf['max_power'] = pd.to_numeric(df['max_power'])","3d216a4e":"df['seats'] = df['seats'].astype(str)","bac5bd6b":"fig = make_subplots(rows=3, cols=2,subplot_titles=(\"Selling Price in Rupee\", \"Total KM Driven\", \"Fuel Efficiency in KM per litre\",\n                                                   \"Engine CC\", \"Brake Horse Power(BHP)\", \"Age of Car\",\"Number of Seats\"))\n\nfig.add_trace(\n    go.Histogram(x=df['selling_price'], name=\"Rupee\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n   go.Histogram(x=df['km_driven'], name=\"KM\"),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Histogram(x=df['mileage'], name=\"KM\/L\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df['engine'], name=\"CC\"),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Histogram(x=df['max_power'], name=\"BHP\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df['age'], name=\"Years\"),\n    row=3, col=2\n)\n\n\n\nfig.update_layout(height=1400, width=800, title_text=\"Distribution of numerical data\")\nfig.show()","7917a96e":"fig = make_subplots(rows=3, cols=2,subplot_titles=(\"Selling Price in Rupee\", \"Total KM Driven\", \"Fuel Efficiency in KM per litre\",\n                                                   \"Engine CC\", \"Brake Horse Power(BHP)\", \"Age of Car\",\"Number of Seats\"))\n\nfig.add_trace(\n    go.Box(x=df['selling_price'], name=\"Rupee\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n   go.Box(x=df['km_driven'], name=\"KM\"),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Box(x=df['mileage'], name=\"KM\/L\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Box(x=df['engine'], name=\"CC\"),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Box(x=df['max_power'], name=\"BHP\"),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Box(x=df['age'], name=\"Years\"),\n    row=3, col=2\n)\n\n\n\nfig.update_layout(height=1400, width=800, title_text=\"Distribution of numerical data\")\nfig.show()\n\n\n# data = [trace1]\n# layout = go.Layout(go.Layout(title=\"Total km driven\"))\n# fig = go.Figure(data, layout=layout)\n# fig.show()","c9428ec4":"count_fuel = df['fuel'].value_counts().reset_index()\ncount_fuel = count_fuel.rename(columns = {'index':'fuel','fuel':'count'})\n\ncount_seller = df['seller_type'].value_counts().reset_index()\ncount_seller = count_seller.rename(columns = {'index':'seller_type','seller_type':'count'})\n\ncount_transmission = df['transmission'].value_counts().reset_index()\ncount_transmission = count_transmission.rename(columns = {'index':'transmission','transmission':'count'})\n\ncount_owner = df['owner'].value_counts().reset_index()\ncount_owner = count_owner.rename(columns = {'index':'owner','owner':'count'})\n\ncount_seats = df['seats'].value_counts().reset_index()\ncount_seats = count_seats.rename(columns = {'index':'seats','seats':'count'})","28fa1aee":"fig = make_subplots(rows=3, cols=2,subplot_titles=(\"Fuel Type\", \"Seller Type\", \"Transmission Type\",\n                                                   \"Number of Owners\", \"Number of Seats\"))\n\nfig.add_trace(\n    go.Bar(y=count_fuel['count'], x=count_fuel['fuel'], name=\"Fuel type\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Bar(y=count_seller['count'], x=count_seller['seller_type'], name=\"Seller type\"),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Bar(y=count_transmission['count'], x=count_transmission['transmission'], name=\"Transmission\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Bar(y=count_owner['count'], x=count_owner['owner'], name=\"Number of owners\"),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Bar(y=count_seats['count'], x=count_seats['seats'], name=\"Number of seats\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1000, width=800, title_text=\"Distribution of categorical data\")\nfig.show()\n\n\n# data = [trace1]\n# layout = go.Layout(title=\"Fuel type\",xaxis_title='fuel',yaxis_title='count', height=700, legend=dict(x=0.1, y=1.1))\n# fig = go.Figure(data, layout=layout)\n# fig.show()","a3271988":"sns.heatmap(df.corr(), annot=True, cmap=\"RdBu\")\nplt.show()","c2af2197":"sns.pairplot(df)","809b4127":"# Make a copy of the data for modelling\ndf_model = df.copy()\n\n# Create the 'brand' column by splitting the 'name' column\ndf_model['brand'] = df_model['name'].str.split(' ').str.get(0)\ndf_model.drop(['name'],axis=1,inplace=True)\n\n# Filter the outlier and log-transform the target variable('selling_price')\ndf_model = df_model[df_model['selling_price'] < 2500000]\ndf_model['selling_price'] = np.log(df_model['selling_price'])\n\n# Filter the outlier in 'km_driven' feature\ndf_model = df_model[df_model['km_driven'] < 300000]\n\n# Filter the unwanted rows in 'fuel' feature\ndf_model = df_model[~df_model['fuel'].isin(['CNG','LPG'])]\n\n# Filter the outliers in 'mileage' feature\ndf_model = df_model[(df_model['mileage'] > 5) & (df_model['mileage'] < 35)]\n\n# Filter the outlier in 'max_power' feature and log-transform the data.\ndf_model = df_model[df_model['max_power'] < 300]\ndf_model['max_power'] = np.log(df_model['max_power'])\n\n# Log-transform the 'age' feature data.\ndf_model['age'] = np.log(df_model['age'])\n\n\n# Show the first five records of the feature engineered DataFrame.\ndf_model.head()\n","ed02a6cf":"df_model = pd.get_dummies(data = df_model, drop_first=True)\ndf_model.head()","40747740":"X = df_model.drop(['selling_price'],axis=1)\ny = df_model['selling_price']","b82bb303":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","a7239c39":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_var = ['km_driven', 'mileage', 'engine', 'max_power', 'age']\nX_train[num_var] = scaler.fit_transform(X_train[num_var])\nX_test[num_var] = scaler.transform(X_test[num_var])","ef15869a":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nselect = RFE(RandomForestRegressor(n_estimators=100, random_state=42),\n                 n_features_to_select=40)\nselect.fit(X_train, y_train)\nX_train_rfe= select.transform(X_train)\nX_test_rfe= select.transform(X_test)","05047aca":"from sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\nr2_train_scores = []\nr2_test_scores = []\ncv_mean = []\n\ndef car_price_prediction_model(model):\n    model.fit(X_train, y_train)\n    \n    #R2 score of training set\n    y_train_pred = model.predict(X_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    r2_train_scores.append(round(r2_train,2))\n    \n    #R2 score of test set\n    y_test_pred = model.predict(X_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    r2_test_scores.append(round(r2_test,2))\n    \n    # CV score of training set\n    cv_training = cross_val_score(model, X_train, y_train, cv=5)\n    cv_mean_training = cv_training.mean()\n    cv_mean.append(round(cv_mean_training,2))\n    \n    \n    \n    # Printing each score\n    print(\"Training set R2 scores: \",round(r2_train,2))\n    print(\"Test set R2 scores: \",round(r2_test,2))\n    print(\"Training cross validation score: \", cv_training)\n    print(\"Training cross validation mean score: \",round(cv_mean_training,2))\n    \n    \n    fig, ax = plt.subplots(1,2,figsize = (10,4))\n    ax[0].set_title('Residual Plot of Train samples')\n    sns.distplot((y_train-y_train_pred),hist = False,ax = ax[0])\n    ax[0].set_xlabel('y_pred')\n    \n    # Y_test vs Y_train scatter plot\n    ax[1].set_title('y_test vs y_pred_test')\n    ax[1].scatter(x = y_test, y = y_test_pred)\n    ax[1].set_xlabel('y_test')\n    ax[1].set_ylabel('y_pred_test')\n    \n    plt.show()","8a8c2eaa":"from sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\nr2_train_scores_rfe = []\nr2_test_scores_rfe = []\ncv_mean_rfe = []\n\ndef car_price_prediction_model_rfe(model):\n    model.fit(X_train_rfe, y_train)\n    \n    \n    #R2 score of RFE training set\n    y_train_pred_rfe = model.predict(X_train_rfe)\n    r2_train_rfe = r2_score(y_train, y_train_pred_rfe)\n    r2_train_scores_rfe.append(round(r2_train_rfe,2))\n    \n    #R2 score of RFE test set\n    y_test_pred_rfe = model.predict(X_test_rfe)\n    r2_test_rfe = r2_score(y_test, y_test_pred_rfe)\n    r2_test_scores_rfe.append(round(r2_test_rfe,2))\n\n    # CV score of RFE training set\n    cv_training_rfe = cross_val_score(model, X_train_rfe, y_train, cv=5)\n    cv_mean_training_rfe = cv_training_rfe.mean()\n    cv_mean_rfe.append(round(cv_mean_training_rfe,2))\n    \n    # Printing each score\n    print(\"Training set R2 scores: \",round(r2_train_rfe,2))\n    print(\"Test set R2 scores: \",round(r2_test_rfe,2))\n    print(\"Training cross validation score: \", cv_training_rfe)\n    print(\"Training cross validation mean score: \",round(cv_mean_training_rfe,2))\n    \n    fig, ax = plt.subplots(1,2,figsize = (10,4))\n    ax[0].set_title('Residual Plot of RFE-Train samples')\n    sns.distplot((y_train-y_train_pred_rfe),hist = False,ax = ax[0])\n    ax[0].set_xlabel('residual')\n    \n    # Y_test vs Y_train scatter plot\n    ax[1].set_title('y_test vs y_pred_test_rfe')\n    ax[1].scatter(x = y_test, y = y_test_pred_rfe)\n    ax[1].set_xlabel('y_test')\n    ax[1].set_ylabel('y_pred_test_rfe')\n    \n    plt.show()","8e2f5963":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\ncar_price_prediction_model(lm)","033939b6":"car_price_prediction_model_rfe(lm)","3fddc5f2":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrg = Ridge()\nalpha = np.logspace(-3,3,num=14)\nrg_rs = RandomizedSearchCV(estimator=rg, param_distributions=dict(alpha=alpha))\ncar_price_prediction_model(rg_rs)","9f6cd1fb":"car_price_prediction_model_rfe(rg_rs)","3ed58038":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\n\nls = Lasso()\nalpha = np.logspace(-3,3,num=14)\nls_rs = RandomizedSearchCV(estimator=ls, param_distributions=dict(alpha=alpha))\ncar_price_prediction_model(ls_rs)","48ab851c":"car_price_prediction_model_rfe(ls_rs)","60a72b36":"from xgboost import XGBRegressor\nxg = XGBRegressor(verbosity= 0)\n\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\nbase_score=[0.25,0.5,0.75,1]\n\n\nparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }\n\nxg_rs = RandomizedSearchCV(estimator=xg, param_distributions=parameter_grid)\n            \n","6d1669bd":"car_price_prediction_model(xg_rs)","075f6be2":"car_price_prediction_model_rfe(xg_rs)","7a0657a4":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestRegressor()\n\n# Number of trees in Random forest\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\nrf_rs = RandomizedSearchCV(estimator = rf, param_distributions = param_grid)","1369bbcd":"car_price_prediction_model(rf_rs)","b5657161":"car_price_prediction_model_rfe(rf_rs)","7368fdb0":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngb = GradientBoostingRegressor()\n\n# Rate at which correcting is being made\nlearning_rate = [0.001, 0.01, 0.1, 0.2]\n# Number of trees in Gradient boosting\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"learning_rate\":learning_rate,\n              \"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\ngb_rs = RandomizedSearchCV(estimator = gb, param_distributions = param_grid)","49da64af":"car_price_prediction_model(gb_rs)","7beb228d":"car_price_prediction_model_rfe(gb_rs)","013c62f4":"algo = [\"LinearRegression(OLS)\",\"LinearRegression(Ridge)\",\"LinearRegression(Lasso)\",\n        \"ExtremeGradientBoostingRegressor\",\"RandomForestRegressor\",\"GradientBoostingRegressor\"]\n\nmodel_eval = pd.DataFrame({'Model': algo,'R Squared(Train)': r2_train_scores,'R Squared(Test)': r2_test_scores,\n                           'CV score mean(Train)': cv_mean})\ndisplay(model_eval)","bccad811":"model_eval_RFE = pd.DataFrame({'Model': algo,'R Squared(Train)': r2_train_scores_rfe,\n                                'R Squared(Test)': r2_test_scores_rfe,'CV score mean(Train)': cv_mean_rfe})\ndisplay(model_eval_RFE)","9af992fc":"#### 8. Main function to fit all regression model based on the RFE-dataset, check r2_score,check cross-validation score, plot residual plot and plot scatterplot of y_test_prediction_rfe vs y_test","038b4e71":"## Conclusion","18b73204":"## Regression Modelling and Evaluation","b3c13690":"- __We can look at the selling_price row to see the correlation coefficient of each numerical feature to the target variable. And from it, we can tell the relationship between the features and the target variable. Higher coefficient represents stronger relationship between the two variables(regardless of the sign).__\n- __Check for multicollinearity among the feature variable, as a rule of thumb, coefficient <+-0.8 is acceptable.__","66ed8d52":"#### RFE Version","38073dbf":"#### 3. Cleaning the data by removing the strings on the datas","bc332308":"#### 3. Plotting bar graphs to show the distribution of all the categorical data","2c40c339":"#### 5. Scaling the numerical data.","8c07a0c6":"#### 7. Main function to fit all regression model, check r2_score,check cross-validation score, plot residual plot and plot scatterplot of y_test_prediction vs y_test","36784a1b":"#### 2. Plotting the scatterplots for each numerical variable to visualize their relationships","a2fd5377":"#### 1. Lineaer Regression(Ordinary Least Square)","15acd33a":"#### 3. Assigning the feature variables and the target variable","d2e94a5f":"### Bivariate\/Multivariate Analysis","a19c914d":"#### 2. Plotting boxplots to explore all the numerical data","379dc615":"#### RFE Version","7526dd71":"#### RFE Version","5407363a":"##  Get to know the dataset and the features\n__Thoughts on each feature and hypotheses of their effect:__\n- selling_price will be our target variable since we are predicting the price of vehicles.\n- name: Name and model of vehicle, I think the brand of the car will be somewhat valuable, since most of us do care about our car's brand, don't we?\n- year: The year when the vehicle is bought, I assume it will tell us more information if we convert it to another feature called 'age' by deducting the 'year' by year of now(2021). I also assume older vehicle should be cheaper.\n- km_driven: This can indirectly tell us the condition of the vehicle, vehicle which travelled a longer distance tends to mean the vehicle is older and hence the selling price will be lower.\n- fuel: Diesel vs petrol should make a difference since the price of the fuel and the engine type are not the same.\n- seller_type: I assume selling price of vehicle for 'Individual seller' should be lower since 'Dealer' often charge commission or service fee or any form of fees.\n- transmission: I assume 'Manual' car should be cheaper than 'Automatic' vehicle, as of my experience.\n- owner: This specify the number of owners the vehicle had before, I assume the more owners the vehicle had before, the cheaper the vehicle will be.\n- mileage: This is the fuel efficiency metric, I assume higher mileage vehicle should be higher in selling_price.\n- engine: The Cubic Capacity(CC) of engine, I assume higher CC vehicle should be higher in selling_price.\n- max_power: The Brake Horse Power(BHP) of the vehicle, I assume higher BHP should be higher in selling_price.\n- torque: The torque of the vehicle, for modelling purpose, this does not contain much information since they are rated at different rpm, so I will drop this feature.\n- seats: Seats can possibly represent the size of the vehicle, I assume the vehicle with more seats will be higher in selling price.","c5abad2b":"#### 1. Plotting the correlation coefficient heatmap to visualize the relationship of each numericle variable","56e0e11e":"#### 4. Splitting the dataset into training set(for modelling) and test set(for evaluation)","6187666f":"#### 1. Plotting histogram to visualize the distribution of all the  numerical data","a4b25218":"#### 2. Read the data into a pandas DataFrame and look at the first five rows of data","99c58f32":"- Extreme Gradient Boosting Regressor is the model I will choose since it has the highest CV score(91%) which mean it generalize better than other models.\n- Linear model is also a great model choice if we have computational power constraint since the non-linear model are quite computational expensive.\n- The automatic feature selection(RFE) did not make significant improvement on all of the models. Hence we do not need it unless computational time is of concern.\n","70e45827":"#### RFE Version","207cb7fd":"#### 1. Feature Engineering","88b62e1a":"#### 7. Basic information of the columns","355896f2":"#### 5. Remove duplicated data","16a0d33e":"#### 3. Linear Regression(Lasso)","d00edf16":"#### 2. One-hot encoding to represent the categorical data for regression modelling","1ece83a2":"## Model Evaluation and Conclusion","b72931db":"- __From the box plots above, we can see all of them contain some sort of outliers. We will decide later during the feature engineering phase to decide which outlier to be removed.__","6fa4df0a":"   # Vehicle price prediction using several regression models with step by step data cleaning, exploration, feature selection and engineering, data modelling, model evaluation and finally conclusion.","4a4cd5ad":"#### 6. Gradient Boosting Regressor","3a670438":"## Feature Selection, Feature Engineering and Data Preparation for Modelling","ace1212b":"#### 4. Check for duplicated data","bf02f8a9":"#### 9. Check for missing values","50d1d6e3":"#### 6. Check the size of the data with no duplicated records","bff6a5e4":"#### 4. Extreme Gradient Boosting Regressor","8c500c1f":"#### 12. Check the size of dataset after removing the missing values to make sure only small number of samples are deleted","e9849fb6":"#### 1. Import the modules used for the analysis","ddca6f3c":"#### RFE Version","b4c075e8":"#### RFE Version","d3642993":"### Univariate Analysis","a0f46858":"#### 4. Converting the data into float format since they are numerical data(continuous data)","09dc7826":"#### 3. Chech the size of original dataset","9be99eca":"## Let's do some exploratory analysis(EDA)","02f5f26d":"#### 5. Converting the datatype of 'seats' to string object since it is a categorical data","86dd9307":"#### 2. Linear Regression(Ridge)","f4f668b5":"- __From the histograms above, we can see the 'selling_price', 'km_driven', 'max_power' and 'age' plots look like a positively-skewed distribution, while 'mileage' look somewhat like a normal curve and 'engine' does not look like to follow a certain distribution.__","93ac9e1f":"#### 5. Random Forest Regressor","9281be2e":"#### 10. Missing values in percentage of the total samples","56f7bf50":"__Now, we look at each feature and decide which features we would like to include in training our model. Selecting features that represent the data well is crucial in building a model that generalize well(which is our ultimate goal). Some of the features can be engineered(Feature Engineering) so that they represent the data better and hence  a better model can be built.For example, the original 'year' column tells us which year the car is bought, if we slightly engineer the data by deducting the 'year' column from now(2021 as of writing this), we can get the age of the car which is an important factor when we are considering to buy an used car. You can think of features as the food for a growing baby(model)- the more nutritious food(better features) you feed the baby, the smarter(better prediction) the baby becomes. So, feature selection and feature engineering are as important as choosing a good model to fit your data.__\n- name: We can obtain the brand of the vehicle and discard the rest of the information. I will remove this column and add a 'brand' column for the vehicle brand.\n- selling_price(in Rupee): From the histogram, we can see the target variable is positively skewed, hence, I am going to use log-transformation on this data to make it behave more like a normal distribution to improve the linear models for regression. As we can see from the boxplot, this target variable is very noisy. I am going to remove all the rows for (selling price > 2.5M Rupee).\n- km_driven: As we can see from the boxplot, this feature contains many outliers. So I decided to remove all the outliers(>300k km).\n- fuel: I will remove the CNG and LPG fuelled car since their 'mileage' metric is measured in 'km\/kg' while the petrol and diesel vechiles 'mileage' are measured in 'km\/l'. Also, CNG and LPG fuelled car are of very small sample size.\n- mileage: There are some outliers, I will remove vehicles whose 'mileage' is < 5 and > 35.\n- max_power: There are some outliers, I will remove vehicles with BHP > 300. I will also do a log-transformation on the positively-skewed data for the better performance in linear models.\n- age: As we can see from the histogram, 'age' is positively-skewed, I am going to do a log-transformation on it to make it behave more like a normal distribution for the performance of linear models.","4a411224":"## Data Cleaning","4d6303b4":"#### 8. Dropping the 'torque' column","096d82b8":"#### 11. Since we have more than 6000 samples which is decent, I decided to remove the rows with missing values since they only constitute about 3% of the data","a7dd6423":"#### 1. Adding the 'age' feature which is a better feature for modelling purpose and removing the 'year' column\n#### 2. Replacing the string in 'owner' with numerical representation for better illustration","d0f98c8c":"#### 6. Automatic feature selection using Recursive Feature Elimination(RFE). I have just learnt about RFE and I am implementing RFE as an extra experiment to check the effectiveness of RFE on model performance."}}