{"cell_type":{"13f306bb":"code","439566f9":"code","92c2076d":"code","ac150fd0":"code","63ebe75d":"code","f699c175":"code","148adac0":"code","dfc9c157":"code","b1f71dad":"code","10d0502a":"code","10ab2473":"code","603f59f0":"code","5443cea6":"code","0b6026a8":"code","91d1dd77":"code","8c21e23e":"code","b326fe97":"code","5b4d14fb":"code","238ff22a":"code","042ffc54":"code","4c0ceb5c":"code","ec917c78":"code","24e2ca6d":"code","b1af6b9f":"code","6b4f6f56":"code","efbe7d80":"code","8efe358a":"markdown","66a72e3e":"markdown","c1ae3e28":"markdown","e0b6346f":"markdown","470be7b8":"markdown","8c0dc802":"markdown","3434db15":"markdown","83703720":"markdown","258f98e6":"markdown","d0a51566":"markdown","5b8a709a":"markdown","80e07fdf":"markdown","f5741eb0":"markdown","57b73dcb":"markdown","e92ed6e3":"markdown","59494e20":"markdown","78393c39":"markdown","ac11ab05":"markdown","cdd34da1":"markdown","06b5768a":"markdown","b85a9d78":"markdown","bc5512bb":"markdown","b7de047e":"markdown","c5140531":"markdown","e95a721c":"markdown"},"source":{"13f306bb":"from lightgbm import LGBMRegressor\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.svm import LinearSVR","439566f9":"df = pd.read_csv('..\/input\/renfe.csv', index_col=0)\n\nprint(df.shape)\ndf.head()","92c2076d":"for col in ['insert_date', 'start_date', 'end_date']:\n    date_col = pd.to_datetime(df[col])\n    df[col + '_hour'] = date_col.dt.hour\n    df[col + '_minute'] = date_col.dt.minute\n    df[col + '_second'] = date_col.dt.second\n    df[col + '_weekday'] = date_col.dt.weekday_name\n    df[col + '_day'] = date_col.dt.day\n    df[col + '_month'] = date_col.dt.month\n    df[col + '_year'] = date_col.dt.year\n    \n    del df[col]","ac150fd0":"df.head()","63ebe75d":"df.isnull().sum()","f699c175":"df.dropna(inplace=True)","148adac0":"for col in df.columns:\n    print(col, \":\", df[col].unique().shape[0])","dfc9c157":"columns_to_drop = [col for col in df.columns if df[col].unique().shape[0] == 1]\ndf.drop(columns=columns_to_drop, inplace=True)","b1f71dad":"df.head()","10d0502a":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","10ab2473":"df.drop(columns=['end_date_day', 'end_date_month'], inplace=True)","603f59f0":"price_freq = df['price'].value_counts()\nprice_freq.head()","5443cea6":"price_freq.tail()","0b6026a8":"X_df = df.drop(columns='price')\ny = df['price'].values","91d1dd77":"encoder = OneHotEncoder()\nX = encoder.fit_transform(X_df.values)\nX","8c21e23e":"for category in encoder.categories_:\n    print(category[:5])","b326fe97":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=2019\n)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","5b4d14fb":"%%time\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","238ff22a":"train_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\n\nprint(\"Train Score:\", train_score)\nprint(\"Test Score:\", test_score)","042ffc54":"def compute_mse(model, X, y_true, name):\n    y_pred = model.predict(X)\n    mse = mean_squared_error(y_true, y_pred)\n    print(f'Mean Squared Error for {name}: {mse}')\n    \ncompute_mse(model, X_train, y_train, 'training set')\ncompute_mse(model, X_test, y_test, 'test set')","4c0ceb5c":"y_train.mean()","ec917c78":"def build_evaluate_fn(X_train, y_train, X_test, y_test):\n    def evaluate(model):\n        train_score = model.score(X_train, y_train)\n        test_score = model.score(X_test, y_test)\n\n        print(\"Train Score:\", train_score)\n        print(\"Test Score:\", test_score)\n        print()\n        \n        compute_mse(model, X_train, y_train, 'training set')\n        compute_mse(model, X_test, y_test, 'test set')\n    \n    return evaluate\n\nevaluate = build_evaluate_fn(X_train, y_train, X_test, y_test)","24e2ca6d":"%%time\nsvm = LinearSVR()\nsvm.fit(X_train, y_train);","b1af6b9f":"evaluate(svm)","6b4f6f56":"%%time\ngbr = LGBMRegressor(n_estimators=1000)\ngbr.fit(X_train, y_train)","efbe7d80":"evaluate(gbr)","8efe358a":"# Conclusion\n\nWe went through a simple workflow for preprocessing the dataset, then encoding and splitting it into training and test set. We then tested 3 different algorithms, i.e. a Linear Regression, an SVM, and Gradient Boosting. We can observe that gradient boosting, in this case, is not only faster, but significantly more accurate.","66a72e3e":"# Introduction\n\nThe goal of this kernel is to go through the workflow of a simple ML task: predicting the price of a train ticket based on all the information surrounding it. We will cover:\n\n* **Preprocessing**: How to clean parse dates, remove missing values, handle correlations, encoding categorical data, and splitting the dataset.\n* **Linear Regression**: A simple baseline model\n* **Linear SVM**: A slightly more sophisticated model that has been used extensively in the 90's.\n* **Light GBM**: An advanced model often used in Kaggle competitions, based on ensembling.","c1ae3e28":"## Removing Missing Values\n\nLet's take a look at all the columns with missing values, and decide whether to remove any row based on missing values.","e0b6346f":"Although price is categorical, there's an important imbalance in the dataset. Prices such as \\$76.30 is extremely frequent, whereas $68.97 appears only once. This is likely because the former is a standard price, and the latter is a one-time discounted price.\n\nIt is therefore wise to try to predict a numerical price rather than a making it a classification problem. We can now split the data into `X` and `y`","470be7b8":"## Finding unique columns\n\nIf a certain column only contains one category of value, then we will drop it. This will also tell us either we have continuous or categorical data.","8c0dc802":"## Aside: Defining an evaluation function","3434db15":"## One Hot Encoding\n\nWe will need to process the categorical data to be ready for input. The usual way to do that is to use `pd.get_dummies` or `sklearn.preprocessing.OneHotEncoder`. The former is more polyvalent, but the latter lets you output a sparse matrix instead of a regular numpy array. This is good for saving memory.","83703720":"The MSE is pretty high, considering that the mean is only:","258f98e6":"Let's take a look at the categories learned by our encoder.","d0a51566":"We see that all the data is categorical in this case. We can one-hot-encode them afterwards.\n\nAlso, it seems like there is only one year in the dataset. We can safely drop that column.","5b8a709a":"Instead of repeating ourselves, we will build a simple function called `evaluate`, which will print the score and MSE of our models on both the training and test sets.","80e07fdf":"# Gradient Boosting\n\nWe will now try to use a gradient boosting machine, which is a method that combines a collection of trees (i.e. an ensemble method) to make a well-supported prediction. The wikipedia page on Gradient Boosting offers a really nice [informal introduction](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting#Informal_introduction) to the algorithm. We will be using the [LightGBM API](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html#lightgbm.LGBMRegressor), an efficient and lightweight implementation of the [original algorithm](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting#Gradient_tree_boosting). It is described in [this paper](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf).\n\nThe number of trees used depends on many different factors, including dimensionality and size of dataset. It is usually a good idea to determine the optimal number of trees through Cross-Validation. We will go with 1000 trees, but feel free to try a different number.","f5741eb0":"## Splitting into Train and Test Set","57b73dcb":"# Preprocessing","e92ed6e3":"The only highly correlated feature we can observe is the between the start and end date (both day and month). We can drop off one of each.","59494e20":"# Linear Regression\n\nWe will start with a linear regression, perhaps the simplest algorithm you can use for predicting a numerical value. We will use the [scikit-learn implementation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html).","78393c39":"## Examining Price\n\nLet's take a closer at price, since it seems to be numerical, but with a certain number of categories.","ac11ab05":"# SVM\n\nNext, we will try out a linear SVM, which is a popular algorithm [invented in the 60s by Vladimir Vapnik, and refined in the 90s by Corinna Cortes and again Vladimir Vapnik](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine). Although we will be using the linear models, there are multiple types of SVM that exists, which is explained in detail in the [scikit-learn docs](https:\/\/scikit-learn.org\/stable\/modules\/svm.html). Here are some simple examples retrieved from the docs:\n\n![image](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_0011.png)\n\n\nWe will be using the LinearSVR model, which is [documented here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVR.html).","cdd34da1":"This teaches us to not trust a single evaluation metric! Therefore, there is still some room for improvement.","06b5768a":"## Observing correlation\n\nIf two columns are highly correlated, we can safely drop them.","b85a9d78":"## Expand Dates\n\nSince the date is given in a string format, we will have to expand it into different columns: year, month, day, and day of the week.****","bc5512bb":"Since we have so many data points, we will only use 10% of the data as test set.","b7de047e":"## Load Data","c5140531":"The missing values are pretty consistent and pretty isolated (only 300k samples out of 2M). Since we are doing price prediction, we can simply drop them.","e95a721c":"What is the `model.score` for a Linear Regression? According to the [sklearn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression):\n\n> The coefficient R^2 is defined as (1 - u\/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n\nIn other words,\n\n$$\nSS_{tot} = \\sum_i (y_i - \\bar{y})^2 \\\\\nSS_{res} = \\sum_i (y_i - f_i)^2 \\\\\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n$$\n\nwhere $\\bar{y}$ is the mean, $y_i$ is the true value for $i$, and $f_i$ is the predicted value for $i$, where $i$ is a data point.\n\n*But how can we interpret our results?* According to this formula, the variance from the true value of our model is about 8x smaller than the variance of the true value from the mean. This is not bad, but let's take a look at the an actual metric that is related to variance, i.e. the MSE:"}}