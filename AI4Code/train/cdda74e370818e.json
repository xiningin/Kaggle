{"cell_type":{"b178116e":"code","ed8083cb":"code","40bc2cb0":"code","3957184d":"code","4bb8c8e7":"code","56c19c21":"code","063d22d6":"code","a60a8419":"code","5b491a57":"code","58c9e1a7":"code","806170d0":"code","557df8e5":"code","e7a64a75":"code","e84f64a4":"code","bfabe732":"code","26fe446d":"code","6ca3391f":"code","309375ac":"code","aee3dcfd":"code","7f219ae5":"code","866e4b26":"code","48c9a123":"code","de08a756":"code","b7298f07":"code","467a1d4c":"code","6a34a959":"code","ce8fcb39":"code","00285967":"code","bda1dd94":"code","e5ea73e0":"code","6b6d7da6":"code","9e593117":"code","3d8839c5":"code","a82fd57c":"code","c73654d5":"code","623b9875":"code","91920b7b":"code","d35341ba":"code","d37bb69e":"code","a5b85b60":"code","24136790":"code","41748775":"code","b1796dd2":"code","15388c22":"code","58f5689e":"code","7d26a34d":"code","cd98a55b":"code","1a3af632":"code","4f054135":"code","06b0356d":"code","1f133151":"code","fe98a7b5":"code","c015fd43":"code","6aef43c7":"code","a9dd56d4":"code","f99380e5":"code","37b39297":"code","1ceaa81c":"code","9d4cbccb":"code","2555478d":"code","2069eb8f":"code","46c8ca47":"code","266ae0f9":"code","73834b4c":"code","81bb47cd":"code","52017396":"code","2830b19a":"code","6cabbf39":"code","35b3bf36":"code","d71ef7c3":"code","1316bc3e":"code","20982fd7":"code","f5068ecb":"code","41c4a54c":"code","c72f255d":"code","8cb49c7d":"code","ca78b729":"code","6db65baf":"code","d4128ee0":"code","b74dc784":"code","a1f7f870":"code","31449e93":"code","fd27da7d":"code","e06e4bd8":"code","80c85183":"code","95440c16":"code","813dc0fd":"code","c8bedfe8":"code","5d7d31f9":"code","ac4036ea":"code","f17d1f36":"code","29317905":"code","d54151a2":"code","0c87ce7b":"code","9ad85aef":"markdown","b4a0ed30":"markdown","e3289c37":"markdown","5d515f36":"markdown","fc4c0de5":"markdown","691128ed":"markdown","a659931c":"markdown","8d8af0a7":"markdown","e5569ec4":"markdown","b501d0ed":"markdown","60a69b0b":"markdown","5453b8f1":"markdown","ef4c15cc":"markdown","d4063d6c":"markdown","a570228a":"markdown","cb1e244a":"markdown","f99ff78d":"markdown","d963f546":"markdown","3bbd4dd4":"markdown","ff8fe70d":"markdown","2156e1e8":"markdown","947ed843":"markdown","d2be480c":"markdown","29ffed4e":"markdown","6c94dde6":"markdown","2b52649c":"markdown","2398ecd2":"markdown","9c3a83ff":"markdown","77135f1c":"markdown","093ecc17":"markdown","a5d3939f":"markdown","c8a34793":"markdown","02196c46":"markdown","345fd635":"markdown","b0b48b4b":"markdown","1455dfea":"markdown","fbdc9d7b":"markdown","cec852d9":"markdown","c592bea5":"markdown","992854c5":"markdown","d1f3fa93":"markdown","b07597f7":"markdown"},"source":{"b178116e":"# Import libraries\nfrom datetime import datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom catboost import CatBoostRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set infinite max rows since data has 89 Columns\n# Showing all rows makes interpreting data easier\n# pd.set_option('display.max_rows', None)\n# pd.set_option('display.max_colwidth', None)","ed8083cb":"# Check input files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","40bc2cb0":"# Load data\ndf = pd.read_csv('\/kaggle\/input\/fifa19\/data.csv')\ndf.info()","3957184d":"# Player position raking columns\npos_col = ['LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']\n\n# Some rows contain no data on position rating\ndf[pos_col].isna().sum()","4bb8c8e7":"# Rows that have no position ratings are the same rows as each other\ndf[df[pos_col].isna().all(axis=1)][pos_col]","56c19c21":"gk_pos_count  = df[df[pos_col[1:]].isna().all(axis=1)][df.Position == 'GK'].shape[0]\nnan_pos_count = df[df[pos_col[1:]].isna().all(axis=1)][df.Position.isna()].shape[0]\n\nprint('GK  :', gk_pos_count)\nprint('NaN :', nan_pos_count)","063d22d6":"# Value related columns\nval_col = ['Club', 'Value', 'Wage']\n\n# Some rows contains no data on club, value, and wage\n# Value and wage will not be NaN, but be valued at 0\nbad_club  = df['Club'].isna()\nbad_value = df['Value'].str.match('\u20ac0')\nbad_wage  = df['Wage'].str.match('\u20ac0')\n\npd.DataFrame(data={'Club': bad_club, 'Value': bad_value, 'Wage': bad_wage}).sum()","a60a8419":"df[df['Club'].isna()][val_col]","5b491a57":"# Rows with bad position rating data will be dropped\n# Position rating is a row we use for some preprocessing\ndf = df.dropna(subset=pos_col)\n\n# Rows with bad Club, Value, and Wage will be dropped\n# Players without club is assumed to not play in this game\n# Value and wage is 0, being an anomaly\ndf = df.dropna(subset=val_col)\n\ndf.shape","58c9e1a7":"# Create copy of dataframe for cleaner processing\ndata = df.iloc[:]","806170d0":"# Split Work Rate column into AWR and DWR\ndata = data.drop(['Work Rate'], axis=1, errors='ignore')\ndata['AWR'] = df['Work Rate'].apply(lambda x: x.split('\/')[0].strip())\ndata['DWR'] = df['Work Rate'].apply(lambda x: x.split('\/')[1].strip())\n\ndata[['AWR', 'DWR']].head()","557df8e5":"# Parse Date as unix\n# Accepted format is `<abbr_month> <date>, <year>`\n# example `Jan 1, 2000`\ndef date2unix(d):\n    d = datetime.strptime(d, '%b %d, %Y')\n    return int(d.timestamp())","e7a64a75":"# Parse joined value to unix timestamp\ndata = data.drop(['Joined'], axis=1, errors='ignore')\ndata['joined_val'] = df['Joined'].fillna(\"Jan 1, 1970\").apply(date2unix)\n\ndata[['joined_val']].head()","e84f64a4":"# Height represented in typical feet'inch notation\n# Convert to inches, since units does not matter\ndef height2int(h):\n    feet, inch = h.split(\"'\")\n    return int(feet) * 12 + int(inch)\n\n# Weight represented in lbs\n# Truncate lbs suffix\ndef weight2int(w):\n    return int(w[:-3])","bfabe732":"# Parse height to numeric data\ndata = data.drop(['Height'], axis=1, errors='ignore')\ndata['height_val'] = df['Height'].fillna(\"0'0\").apply(height2int)\n\n# Parse weight to numeric data\ndata = data.drop(['Weight'], axis=1, errors='ignore')\ndata['weight_val'] = df['Weight'].fillna('0lbs').apply(weight2int)\n\ndata[['height_val', 'weight_val']].head()","26fe446d":"# Ratings are represented with x+y\n# x is the base rating\n# y is the modifier when the player is in a good mood\n# For the purpose of this kernel we can use the base rating\ndef rating2int(r):\n    base, _ = r.split('+')\n    return int(base)","6ca3391f":"# Transform position rating values to integer\nfor col in pos_col:\n    data = data.drop([col], axis=1, errors='ignore')\n    data[col] = df[col].fillna('0+0').apply(rating2int)\n\ndata[pos_col].head()","309375ac":"# Conversion function from currency to int\n# Currency syntaxin this dataset is represented as such\n# \u20ac1M, \u20ac10K, \u20ac0, etc\ndef money2int(m):\n    # Cut out Euro sign prefix\n    if m.startswith('\u20ac'):\n        m = m[1:]\n    \n    # Get multiplier suffix for thousands and millions\n    multi = 1\n    if m.endswith('K'):\n        multi = 1000\n        m = m[:-1]\n    if m.endswith('M'):\n        multi = 1000000\n        m = m[:-1]\n    \n    val = float(m) * multi\n    return int(val)","aee3dcfd":"# Transform currency columns to integer\ndata = data.drop(['Value', 'Wage', 'Release Clause'], axis=1, errors='ignore')\ndata['value_val']          = df['Value'].fillna('\u20ac0').apply(money2int)\ndata['wage_val']           = df['Wage'].fillna('\u20ac0').apply(money2int)\ndata['release_clause_val'] = df['Release Clause'].fillna('\u20ac0').apply(money2int)\n\ndata[['value_val', 'wage_val', 'release_clause_val']].head()","7f219ae5":"# Show correlation to justify dropping a significant column\ndata[['value_val', 'wage_val', 'release_clause_val']].corr()","866e4b26":"# Delete Release Clause\ndata = data.drop(['release_clause_val'], axis=1, errors='ignore')","48c9a123":"# Ignore image fields\ndata = data.drop(['Photo', 'Flag', 'Club Logo'], axis=1, errors='ignore')\n\n# Ignore identitier fields with high cardinality\ndata = data.drop(['Unnamed: 0', 'ID', 'Name'], axis=1, errors='ignore')\n\n# Ignore irrelevant data\ndata = data.drop(['Real Face', 'Jersey Number', 'Loaned From', 'Contract Valid Until'], axis=1, errors='ignore')","de08a756":"data.info()","b7298f07":"# Create copy of dataset to preserve original\ndata_naive = data.iloc[:]","467a1d4c":"# Get one hot encoding for naive categorical attributes\nnaive_attr = ['Nationality', 'Club', 'Preferred Foot', 'Body Type', 'Position', 'AWR', 'DWR']\nnaive_dummies = pd.get_dummies(data[naive_attr])\n\nnaive_dummies.head()","6a34a959":"# Append body type encoding to dataset\ndata_naive = data_naive.drop(naive_attr, axis=1, errors='ignore')\ndata_naive = data_naive.drop(naive_dummies.columns, axis=1, errors='ignore')\ndata_naive = data_naive.join(naive_dummies)\n\ndata_naive.head()","ce8fcb39":"data_naive.info()","00285967":"# Split data into attributes and targets\nX  = data_naive.drop(['value_val', 'wage_val'], axis=1)\ny1 = data_naive['value_val']\ny2 = data_naive['wage_val']\n\nX1_train, X1_test , y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=420)\nX2_train, X2_test , y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=420)\n\nX2_train.info()","bda1dd94":"# Perform linear regression on player Value\nlin_reg1 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg1.fit(X1_train, y1_train)\nlin_pred1_train = lin_reg1.predict(X1_train)\nlin_pred1_test = lin_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE LINEAR REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nlin_coef = pd.Series(lin_reg1.coef_, index=X1_train.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, lin_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, lin_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, lin_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, lin_pred1_test))","e5ea73e0":"# Perform linear regression on player Wage\nlin_reg2 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg2.fit(X2_train, y2_train)\nlin_pred2_train = lin_reg2.predict(X2_train)\nlin_pred2_test = lin_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE LINEAR REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nlin_coef = pd.Series(lin_reg2.coef_, index=X2_train.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, lin_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, lin_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, lin_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, lin_pred2_test))","6b6d7da6":"# Find best alpha for ridge regression on player Value\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X1_train, y1_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","9e593117":"# Perform ridge regression on player Value\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg1 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg1.fit(X1_train, y1_train)\nridge_pred1_train = ridge_reg1.predict(X1_train)\nridge_pred1_test = ridge_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE RIDGE REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nridge_coef = pd.Series(ridge_reg1.coef_, index=X1_train.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, ridge_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, ridge_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, ridge_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, ridge_pred1_test))","3d8839c5":"# Find best alpha for ridge regression on player Wage\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X2_train, y2_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","a82fd57c":"# Perform ridge regression on player Wage\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg2 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg2.fit(X2_train, y2_train)\nridge_pred2_train = ridge_reg2.predict(X2_train)\nridge_pred2_test = ridge_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE RIDGE REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nridge_coef = pd.Series(ridge_reg2.coef_, index=X1_train.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, ridge_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, ridge_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, ridge_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, ridge_pred2_test))","c73654d5":"# Perform random forest regression on player Value\nforest_reg1 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg1.fit(X1_train, y1_train)\nforest_pred1_train = forest_reg1.predict(X1_train)\nforest_pred1_test = forest_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nforest_imp = pd.Series(forest_reg1.feature_importances_, index=X1_train.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, forest_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, forest_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, forest_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, forest_pred1_test))","623b9875":"# Perform random forest regression on player Wage\nforest_reg2 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg2.fit(X2_train, y2_train)\nforest_pred2_train = forest_reg2.predict(X2_train)\nforest_pred2_test = forest_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('NAIVE RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nforest_imp = pd.Series(forest_reg2.feature_importances_, index=X2_train.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, forest_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, forest_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, forest_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, forest_pred2_test))","91920b7b":"fig, axs = plt.subplots(2, 3, figsize=(15,7))\nfig.suptitle('Naive Regression Coeficients\/Importance')\n\naxs[0,0].plot(np.sort(np.abs(lin_reg1.coef_)))\naxs[0,0].set_yscale('log')\naxs[0,0].set_title('LinReg Player Value')\n\naxs[0,1].plot(np.sort(np.abs(ridge_reg1.coef_)))\naxs[0,1].set_yscale('log')\naxs[0,1].set_title('RidgeReg Player Value')\n\naxs[0,2].plot(np.sort(forest_reg1.feature_importances_))\naxs[0,2].set_yscale('log')\naxs[0,2].set_title('ForestReg Player Value')\n\naxs[1,0].plot(np.sort(np.abs(lin_reg2.coef_)))\naxs[1,0].set_yscale('log')\naxs[1,0].set_title('LinReg Player Wage')\n\naxs[1,1].plot(np.sort(np.abs(ridge_reg2.coef_)))\naxs[1,1].set_yscale('log')\naxs[1,1].set_title('RidgeReg Player Wage')\n\naxs[1,2].plot(np.sort(forest_reg2.feature_importances_))\naxs[1,2].set_yscale('log')\naxs[1,2].set_title('ForestReg Player Wage')\n\nplt.show()","d35341ba":"# Create copy of dataset to preserve original\ndata_clean = data.iloc[:]","d37bb69e":"# Show unique values in Preferred foot\ndata['Preferred Foot'].unique()","a5b85b60":"# Assign value to left and right preferred foot\ndata_clean = data_clean.drop(['Preferred Foot'], axis=1, errors='ignore')\ndata_clean['pref_foot_val'] = data['Preferred Foot'].apply(lambda x: 0 if x == 'Left' else 1)\n\ndata_clean[['pref_foot_val']].head()","24136790":"# List unique values for Body Type\ndata['Body Type'].unique()","41748775":"# List all unique values in Body Type column\ndata['Body Type'].value_counts()","b1796dd2":"# Transform the abnormal unique values into their proper values\ndef rebodytype(b):\n    if b == 'C. Ronaldo':\n        return 'Lean'\n    if b == 'Messi':\n        return 'Normal'\n    if b == 'Shaqiri':\n        return 'Normal'\n    if b == 'Neymar':\n        return 'Normal'\n    if b == 'Akinfenwa':\n        return 'Stocky'\n    if b == 'PLAYER_BODY_TYPE_25':\n        return 'Lean'\n\n    return b\n\n# Clean body type values\ndata['Body Type'].apply(rebodytype).value_counts()","15388c22":"# List unique values for AWR\ndata['AWR'].unique()","58f5689e":"# List unique values for DWR\ndata['DWR'].unique()","7d26a34d":"# Assigns a numeric value to list of ordered ordinal data\ndef ordinal2int(ordinal):\n    return lambda x: ordinal.index(x) + 1\n\n# Clean convert ordinal values to integer\ndata_clean = data_clean.drop(['Body Type', 'AWR', 'DWR'], axis=1, errors='ignore')\ndata_clean['body_type_val'] = data['Body Type'].apply(rebodytype).apply(ordinal2int(['Lean', 'Normal', 'Stocky']))\ndata_clean['awr_val'] = data['AWR'].apply(ordinal2int(['Low', 'Medium', 'High']))\ndata_clean['dwr_val'] = data['DWR'].apply(ordinal2int(['Low', 'Medium', 'High']))\n\ndata_clean[['body_type_val', 'awr_val', 'dwr_val']].head()","cd98a55b":"# Use value of player's specialized position\n# This is more than likely his most valued attribute\ndata_clean['pos_rating'] = data.lookup(data.index, data['Position'])\n\n# Simple function to get n maximum values\ndef nmax(arr, n=1):\n    arr.sort()\n    return sum(arr[-n:])\n\n# Use sum of 3 best positions\n# Player's performance will center on several positions, not all position\ndata_clean['flex_rating'] = np.array([nmax(d, 3) for d in data[pos_col].values])\n\n# Drop original fields position rating fields\ndata_clean = data_clean.drop(pos_col, axis=1, errors='ignore')\n\ndata_clean[['pos_rating', 'flex_rating']].head()","1a3af632":"# # Get one hot encoding for position column\n# one_hot_attr = ['Position']\n# one_hot_dummies = pd.get_dummies(data[one_hot_attr])\n\n# one_hot_dummies.head()","4f054135":"# # Append body type encoding to dataset\n# data_clean = data_clean.drop(one_hot_attr, axis=1, errors='ignore')\n# data_clean = data_clean.drop(one_hot_dummies.columns, axis=1, errors='ignore')\n# data_clean = data_clean.join(one_hot_dummies)\n\n# data_clean.head()","06b0356d":"# Split data into attributes and targets\nX  = data_clean.drop(['value_val', 'wage_val'], axis=1)\ny1 = data_clean['value_val']\ny2 = data_clean['wage_val']\n\nX1_train_prep, X1_test_prep, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=69)\nX2_train_prep, X2_test_prep, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=69)\n\nX1_train_prep.info()","1f133151":"# Create encoding for Club with mean encoding from training data\nclub_value_enc = X1_train_prep.join(y1_train).groupby('Club').mean()[['value_val']]\nclub_value_enc.columns = ['club_val']\n\n# Create encoding for Nationality with mean encoding from training data\nnat_value_enc = X1_train_prep.join(y1_train).groupby('Nationality').mean()[['value_val']]\nnat_value_enc.columns = ['nationality_val']\n\n# Create encoding for Position with mean encoding from training data\npos_value_enc = X1_train_prep.join(y1_train).groupby('Position').mean()[['value_val']]\npos_value_enc.columns = ['position_val']\n\nvalue_mean = y1_train.mean()","fe98a7b5":"# Encode training dataset with Club and Nationality mean encoding\nX1_train = X1_train_prep.iloc[:]\nX1_train = X1_train.join(X1_train.merge(club_value_enc, on='Club', right_index=True)['club_val'])\nX1_train = X1_train.join(X1_train.merge(nat_value_enc, on='Nationality', right_index=True)['nationality_val'])\nX1_train = X1_train.join(X1_train.merge(pos_value_enc, on='Position', right_index=True)['position_val'])\n\n# Encode test dataset with Club and Nationality mean encoding\nX1_test = X1_test_prep.iloc[:]\nX1_test = X1_test.join(X1_test.merge(club_value_enc, on='Club', right_index=True)['club_val'])\nX1_test = X1_test.join(X1_test.merge(nat_value_enc, on='Nationality', right_index=True)['nationality_val'])\nX1_test = X1_test.join(X1_test.merge(pos_value_enc, on='Position', right_index=True)['position_val'])\n\n# Fill out data not found in training dataset with training mean\nX1_test['club_val'] = X1_test['club_val'].fillna(value_mean)\nX1_test['nationality_val'] = X1_test['nationality_val'].fillna(value_mean)\nX1_test['position_val'] = X1_test['position_val'].fillna(value_mean)\n\n# Drop unencoded columns\nX1_train = X1_train.drop(['Club', 'Nationality', 'Position'], axis=1, errors='ignore')\nX1_test = X1_test.drop(['Club', 'Nationality', 'Position'], axis=1, errors='ignore')\n\nX1_train[['club_val', 'nationality_val', 'position_val']].head()","c015fd43":"# Create encoding for Club with mean encoding from training data\nclub_value_enc = X2_train_prep.join(y2_train).groupby('Club').mean()[['wage_val']]\nclub_value_enc.columns = ['club_val']\n\n# Create encoding for Nationality with mean encoding from training data\nnat_value_enc = X2_train_prep.join(y2_train).groupby('Nationality').mean()[['wage_val']]\nnat_value_enc.columns = ['nationality_val']\n\n# Create encoding for Position with mean encoding from training data\npos_value_enc = X2_train_prep.join(y2_train).groupby('Position').mean()[['wage_val']]\npos_value_enc.columns = ['position_val']\n\nvalue_mean = y2_train.mean()","6aef43c7":"# Encode training dataset with Club and Nationality mean encoding\nX2_train = X2_train_prep.iloc[:]\nX2_train = X2_train.join(X2_train.merge(club_value_enc, on='Club', right_index=True)['club_val'])\nX2_train = X2_train.join(X2_train.merge(nat_value_enc, on='Nationality', right_index=True)['nationality_val'])\nX2_train = X2_train.join(X2_train.merge(pos_value_enc, on='Position', right_index=True)['position_val'])\n\n# Encode test dataset with Club and Nationality mean encoding\nX2_test = X2_test_prep.iloc[:]\nX2_test = X2_test.join(X2_test.merge(club_value_enc, on='Club', right_index=True)['club_val'])\nX2_test = X2_test.join(X2_test.merge(nat_value_enc, on='Nationality', right_index=True)['nationality_val'])\nX2_test = X2_test.join(X2_test.merge(pos_value_enc, on='Position', right_index=True)['position_val'])\n\n# Fill out data not found in training dataset with training mean\nX2_test['club_val'] = X2_test['club_val'].fillna(value_mean)\nX2_test['nationality_val'] = X2_test['nationality_val'].fillna(value_mean)\nX2_test['position_val'] = X2_test['position_val'].fillna(value_mean)\n\n# Drop unencoded columns\nX2_train = X2_train.drop(['Club', 'Nationality', 'Position'], axis=1, errors='ignore')\nX2_test = X2_test.drop(['Club', 'Nationality', 'Position'], axis=1, errors='ignore')\n\nX2_train[['club_val', 'nationality_val', 'position_val']].head()","a9dd56d4":"# Perform linear regression on player Value\nlin_reg1 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg1.fit(X1_train, y1_train)\nlin_pred1_train = lin_reg1.predict(X1_train)\nlin_pred1_test = lin_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED LINEAR REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nlin_coef = pd.Series(lin_reg1.coef_, index=X1_train.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, lin_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, lin_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, lin_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, lin_pred1_test))","f99380e5":"# Perform linear regression on player Wage\nlin_reg2 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg2.fit(X2_train, y2_train)\nlin_pred2_train = lin_reg2.predict(X2_train)\nlin_pred2_test = lin_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED LINEAR REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nlin_coef = pd.Series(lin_reg2.coef_, index=X2_train.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, lin_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, lin_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, lin_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, lin_pred2_test))","37b39297":"# Find best alpha for ridge regression on player Value\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X1_train, y1_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","1ceaa81c":"# Perform ridge regression on player Value\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg1 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg1.fit(X1_train, y1_train)\nridge_pred1_train = ridge_reg1.predict(X1_train)\nridge_pred1_test = ridge_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED RIDGE REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nridge_coef = pd.Series(ridge_reg1.coef_, index=X1_train.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, ridge_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, ridge_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, ridge_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, ridge_pred1_test))","9d4cbccb":"# Find best alpha for ridge regression on player Wage\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X2_train, y2_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","2555478d":"# Perform ridge regression on player Wage\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg2 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg2.fit(X2_train, y2_train)\nridge_pred2_train = ridge_reg2.predict(X2_train)\nridge_pred2_test = ridge_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED RIDGE REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nridge_coef = pd.Series(ridge_reg2.coef_, index=X2_train.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, ridge_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, ridge_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, ridge_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, ridge_pred2_test))","2069eb8f":"# Perform random forest regression on player Value\nforest_reg1 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg1.fit(X1_train, y1_train)\nforest_pred1_train = forest_reg1.predict(X1_train)\nforest_pred1_test = forest_reg1.predict(X1_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nforest_imp = pd.Series(forest_reg1.feature_importances_, index=X1_train.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, forest_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, forest_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, forest_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, forest_pred1_test))","46c8ca47":"# Perform random forest regression on player Wage\nforest_reg2 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg2.fit(X2_train, y2_train)\nforest_pred2_train = forest_reg2.predict(X2_train)\nforest_pred2_test = forest_reg2.predict(X2_test)\nrun_time = time.time() - run_time\n\nprint('PREPROCESSED RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nforest_imp = pd.Series(forest_reg2.feature_importances_, index=X2_train.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, forest_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, forest_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, forest_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, forest_pred2_test))","266ae0f9":"fig, axs = plt.subplots(2, 3, figsize=(15,7))\nfig.suptitle('Pre-processed Regression Coeficients\/Importance')\n\naxs[0,0].plot(np.sort(np.abs(lin_reg1.coef_)))\naxs[0,0].set_yscale('log')\naxs[0,0].set_title('LinReg Player Value')\n\naxs[0,1].plot(np.sort(np.abs(ridge_reg1.coef_)))\naxs[0,1].set_yscale('log')\naxs[0,1].set_title('RidgeReg Player Value')\n\naxs[0,2].plot(np.sort(forest_reg1.feature_importances_))\naxs[0,2].set_yscale('log')\naxs[0,2].set_title('ForestReg Player Value')\n\naxs[1,0].plot(np.sort(np.abs(lin_reg2.coef_)))\naxs[1,0].set_yscale('log')\naxs[1,0].set_title('LinReg Player Wage')\n\naxs[1,1].plot(np.sort(np.abs(ridge_reg2.coef_)))\naxs[1,1].set_yscale('log')\naxs[1,1].set_title('RidgeReg Player Wage')\n\naxs[1,2].plot(np.sort(forest_reg2.feature_importances_))\naxs[1,2].set_yscale('log')\naxs[1,2].set_title('ForestReg Player Wage')\n\nplt.show()","73834b4c":"X1_train.shape","81bb47cd":"# Sanity check whether PCA is worth it\npca = PCA().fit(X1_train)\npd.DataFrame(pca.transform(X1_train)).shape","52017396":"# Get covariance matrix\nc = X1_train.cov().to_numpy()\neigval, eigvec = np.linalg.eig(c)\n\n# Get sorted eigen values and vectors\nidx = eigval.argsort()[::-1]\neigval = eigval[idx]\neigvec = eigvec[:,idx]\n\neigval","2830b19a":"# Plot out eigen values to see where to cut off\nplt.plot(eigval)\nplt.yscale('log')\nplt.show()","6cabbf39":"# Cut off eigen vector at 52 attributes since there doesn't seem to be a good cutoff point\neigmat = eigvec[:,:52]\neigmat","35b3bf36":"# Generate test and train dataset transformed with PCA\nX1_train_pca = X1_train.dot(eigmat)\nX1_test_pca = X1_test.dot(eigmat)\n\nX1_train_pca.head()","d71ef7c3":"# Check reconstruction error\nX1_pca_recon = X1_train_pca.dot(eigmat.transpose()).to_numpy()\nX1_original = X1_train.to_numpy()\n((X1_pca_recon - X1_original)**2).mean()","1316bc3e":"def x1_pca_recon_err(n):\n    eigmat = eigvec[:,:n]\n\n    X1_train_pca = X1_train.dot(eigmat)\n    X1_pca_recon = X1_train_pca.dot(eigmat.transpose()).to_numpy()\n    X1_original = X1_train.to_numpy()\n    return ((X1_pca_recon - X1_original)**2).mean()\n\nfor i in range(53, 33, -1):\n    print('attributes:',  i, '        reconstruction err:', x1_pca_recon_err(i))","20982fd7":"# Get covariance matrix\nc = X2_train.cov().to_numpy()\neigval, eigvec = np.linalg.eig(c)\n\n# Get sorted eigen values and vectors\nidx = eigval.argsort()[::-1]\neigval = eigval[idx]\neigvec = eigvec[:,idx]\n\neigval","f5068ecb":"def x2_pca_recon_err(n):\n    eigmat = eigvec[:,:n]\n\n    X2_train_pca = X2_train.dot(eigmat)\n    X2_pca_recon = X2_train_pca.dot(eigmat.transpose()).to_numpy()\n    X2_original = X2_train.to_numpy()\n    return ((X2_pca_recon - X2_original)**2).mean()\n\nfor i in range(53, 33, -1):\n    print('attributes:',  i, '        reconstruction err:', x2_pca_recon_err(i))","41c4a54c":"# Cut off eigen vector at 52 attributes since there doesn't seem to be a good cutoff point\neigmat = eigvec[:,:52]\n\n# Generate test and train dataset transformed with PCA\nX2_train_pca = X2_train.dot(eigmat)\nX2_test_pca = X2_test.dot(eigmat)\n\nX2_train_pca.head()","c72f255d":"# Perform linear regression on player Value\nlin_reg1 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg1.fit(X1_train_pca, y1_train)\nlin_pred1_train = lin_reg1.predict(X1_train_pca)\nlin_pred1_test = lin_reg1.predict(X1_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA LINEAR REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nlin_coef = pd.Series(lin_reg1.coef_, index=X1_train_pca.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, lin_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, lin_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, lin_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, lin_pred1_test))","8cb49c7d":"# Perform linear regression on player Wage\nlin_reg2 = LinearRegression(normalize=True)\n\nrun_time = time.time()\nlin_reg2.fit(X2_train_pca, y2_train)\nlin_pred2_train = lin_reg2.predict(X2_train_pca)\nlin_pred2_test = lin_reg2.predict(X2_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA LINEAR REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nlin_coef = pd.Series(lin_reg2.coef_, index=X2_train_pca.columns)\nprint('Most significant Coef:')\nprint(lin_coef.reindex(lin_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, lin_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, lin_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, lin_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, lin_pred2_test))","ca78b729":"# Find best alpha for ridge regression on player Value\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X1_train_pca, y1_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","6db65baf":"# Perform ridge regression on player Value\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg1 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg1.fit(X1_train_pca, y1_train)\nridge_pred1_train = ridge_reg1.predict(X1_train_pca)\nridge_pred1_test = ridge_reg1.predict(X1_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA RIDGE REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nridge_coef = pd.Series(ridge_reg1.coef_, index=X1_train_pca.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, ridge_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, ridge_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, ridge_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, ridge_pred1_test))","d4128ee0":"# Find best alpha for ridge regression on player Wage\nalphas = 10**np.linspace(-5,5,100)\nparams = {'alpha': alphas}\n\nregressor = GridSearchCV(Ridge(), params, scoring='neg_mean_squared_error', cv=5)\n\nrun_time = time.time()\nregressor.fit(X2_train_pca, y2_train)\nrun_time = time.time() - run_time\n\nprint('RUNTIME :', run_time * 1000, 'ms')\nprint('Lambda  :', regressor.best_params_)\nprint('MSE     :', regressor.best_score_)","b74dc784":"# Perform ridge regression on player Wage\nbest_alpha = regressor.best_params_['alpha']\n\nridge_reg2 = Ridge(alpha=best_alpha, normalize=True)\n\nrun_time = time.time()\nridge_reg2.fit(X2_train_pca, y2_train)\nridge_pred2_train = ridge_reg2.predict(X2_train_pca)\nridge_pred2_test = ridge_reg2.predict(X2_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA RIDGE REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nridge_coef = pd.Series(ridge_reg2.coef_, index=X1_train_pca.columns)\nprint('Most significant Coef:')\nprint(ridge_coef.reindex(ridge_coef.abs().sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, ridge_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, ridge_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, ridge_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, ridge_pred2_test))","a1f7f870":"# Perform random forest regression on player Value\nforest_reg1 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg1.fit(X1_train_pca, y1_train)\nforest_pred1_train = forest_reg1.predict(X1_train_pca)\nforest_pred1_test = forest_reg1.predict(X1_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nforest_imp = pd.Series(forest_reg1.feature_importances_, index=X1_train_pca.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, forest_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, forest_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, forest_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, forest_pred1_test))","31449e93":"# Perform random forest regression on player Wage\nforest_reg2 = RandomForestRegressor(max_depth=8, random_state=42)\n\nrun_time = time.time()\nforest_reg2.fit(X2_train_pca, y2_train)\nforest_pred2_train = forest_reg2.predict(X2_train_pca)\nforest_pred2_test = forest_reg2.predict(X2_test_pca)\nrun_time = time.time() - run_time\n\nprint('PCA RANDOM FOREST REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nforest_imp = pd.Series(forest_reg2.feature_importances_, index=X2_train_pca.columns)\nprint('Most important Features:')\nprint(forest_imp.reindex(forest_imp.sort_values(ascending=False).index).head(10))\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, forest_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, forest_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, forest_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, forest_pred2_test))","fd27da7d":"fig, axs = plt.subplots(2, 3, figsize=(15,7))\nfig.suptitle('PCA Regression Coeficients\/Importance')\n\naxs[0,0].plot(np.sort(np.abs(lin_reg1.coef_)))\naxs[0,0].set_yscale('log')\naxs[0,0].set_title('LinReg Player Value')\n\naxs[0,1].plot(np.sort(np.abs(ridge_reg1.coef_)))\naxs[0,1].set_yscale('log')\naxs[0,1].set_title('RidgeReg Player Value')\n\naxs[0,2].plot(np.sort(forest_reg1.feature_importances_))\naxs[0,2].set_yscale('log')\naxs[0,2].set_title('ForestReg Player Value')\n\naxs[1,0].plot(np.sort(np.abs(lin_reg2.coef_)))\naxs[1,0].set_yscale('log')\naxs[1,0].set_title('LinReg Player Wage')\n\naxs[1,1].plot(np.sort(np.abs(ridge_reg2.coef_)))\naxs[1,1].set_yscale('log')\naxs[1,1].set_title('RidgeReg Player Wage')\n\naxs[1,2].plot(np.sort(forest_reg2.feature_importances_))\naxs[1,2].set_yscale('log')\naxs[1,2].set_title('ForestReg Player Wage')\n\nplt.show()","e06e4bd8":"X1_train.nunique().sum()","80c85183":"X1_train_pca.nunique().sum()","95440c16":"# Split data into attributes and targets\nX  = data.drop(['value_val', 'wage_val'], axis=1)\ny1 = data['value_val']\ny2 = data['wage_val']\n\nX1_train, X1_test , y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)\nX2_train, X2_test , y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n\nX2_train.info()","813dc0fd":"# Perform slope boost regression with prebuilt model on player Value\ncat_features = [1, 4, 6, 10, 11, 46, 47]\ncatboost_reg1 = CatBoostRegressor(iterations=200,\n                                 learning_rate=0.1,\n                                 depth=8)\n\nrun_time = time.time()\ncatboost_reg1.fit(X1_train, y1_train, cat_features, verbose=False)\ncatboost_pred1_train = catboost_reg1.predict(X1_train)\ncatboost_pred1_test = catboost_reg1.predict(X1_test)\nrun_time = time.time() - run_time\nprint()\n\nprint('CLEAN CATBOOST REGRESSION')\nprint('PREDICTING PLAYER VALUE')\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y1_train, catboost_pred1_train))\nprint('R2 Score TEST   :', r2_score(y1_test, catboost_pred1_test))\nprint('MSE Score TRAIN :', mean_squared_error(y1_train, catboost_pred1_train))\nprint('MSE Score TEST  :', mean_squared_error(y1_test, catboost_pred1_test))","c8bedfe8":"# Perform slope boost regression with prebuilt model on player Wage\ncat_features = [1, 4, 6, 10, 11, 46, 47]\ncatboost_reg2 = CatBoostRegressor(iterations=200,\n                                 learning_rate=0.1,\n                                 depth=8)\n\nrun_time = time.time()\ncatboost_reg2.fit(X2_train, y2_train, cat_features, verbose=False)\ncatboost_pred2_train = catboost_reg2.predict(X2_train)\ncatboost_pred2_test = catboost_reg2.predict(X2_test)\nrun_time = time.time() - run_time\nprint()\n\nprint('CLEAN CATBOOST REGRESSION')\nprint('PREDICTING PLAYER WAGE')\nprint()\n\nprint('RUNTIME         :', run_time * 1000, 'ms')\nprint('R2 Score TRAIN  :', r2_score(y2_train, catboost_pred2_train))\nprint('R2 Score TEST   :', r2_score(y2_test, catboost_pred2_test))\nprint('MSE Score TRAIN :', mean_squared_error(y2_train, catboost_pred2_train))\nprint('MSE Score TEST  :', mean_squared_error(y2_test, catboost_pred2_test))","5d7d31f9":"# Previous runtime results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_RUNTIME' : [\n            1395.0705528259277,\n            43.75290870666504,\n            31.097888946533203,\n            None,\n        ],\n        'RIDGEREG_RUNTIME' : [\n            481.5702438354492,\n            30.005216598510742,\n            26.578664779663086,\n            None,\n        ],\n        'FORESTREG_RUNTIME' : [\n            49870.77331542969,\n            14616.775751113892,\n            37502.03585624695,\n            None,\n        ],\n        'CATBOOST_RUNTIME' : [\n            None,\n            None,\n            None,\n            5879.0154457092285,\n        ],\n    }\n)","ac4036ea":"# Previous R2 results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_R2_TRAIN' : [\n            0.7193842683071934,\n            0.6811962636493403,\n            0.6764538606726269,\n            None,\n        ],\n        'LINREG_R2_TEST' : [\n            -2.007368207784571e+24,\n            0.6908466302454932,\n            0.687966745331998,\n            None,\n        ],\n        'RIDGEREG_R2_TRAIN' : [\n            0.747300305671266,\n            0.4362093255187618,\n            0.08442516570869185,\n            None,\n        ],\n        'RIDGEREG_R2_TEST' : [\n            0.6922009795979305,\n            0.45298678097485645,\n            0.0845167641060749,\n            None,\n        ],\n        'FORESTREG_R2_TRAIN' : [\n            0.9958668452361841,\n            0.9952358572702557,\n            0.9230677146733887,\n            None,\n        ],\n        'FORESTREG_R2_TEST' : [\n            0.968032534257517,\n            0.983041207042161,\n            0.753495776123223,\n            None,\n        ],\n        'CATBOOST_R2_TRAIN' : [\n            None,\n            None,\n            None,\n            0.9990151544521757,\n        ],\n        'CATBOOST_R2_TEST' : [\n            None,\n            None,\n            None,\n            0.9839465566697981,\n        ],\n    }\n)","f17d1f36":"# Previous MSE results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_MSE_TRAIN' : [\n            9190175287380.48,\n            10255749711879.357,\n            10408310339050.133,\n            None,\n        ],\n        'LINREG_MSE_TEST' : [\n            6.766706098142399e+37,\n            11130832118237.354,\n            11234520185805.291,\n            None,\n        ],\n        'RIDGEREG_MSE_TRAIN' : [\n            8275924061487.902,\n            18136851573820.21,\n            29453564285265.76,\n            None,\n        ],\n        'RIDGEREG_MSE_TEST' : [\n            10375702376275.074,\n            19694795215269.402,\n            32961278131588.008,\n            None,\n        ],\n        'FORESTREG_MSE_TRAIN' : [\n            135360966900.18025,\n            153259983672.79407,\n            2474871443177.7456,\n            None,\n        ],\n        'FORESTREG_MSE_TEST' : [\n            1077602228345.3073,\n            610588451588.1108,\n            8875197234911.596,\n            None,\n        ],\n        'CATBOOST_MSE_TRAIN' : [\n            None,\n            None,\n            None,\n            30815427200.89645,\n        ],\n        'CATBOOST_MSE_TEST' : [\n            None,\n            None,\n            None,\n            634936722810.1262,\n        ],\n    }\n)","29317905":"# Previous runtime results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_RUNTIME' : [\n            1298.584222793579,\n            30.440807342529297,\n            29.247760772705078,\n            None,\n        ],\n        'RIDGEREG_RUNTIME' : [\n            559.8382949829102,\n            28.866291046142578,\n            27.935028076171875,\n            None,\n        ],\n        'FORESTREG_RUNTIME' : [\n            48230.66830635071,\n            14755.709409713745,\n            38085.474729537964,\n            None,\n        ],\n        'CATBOOST_RUNTIME' : [\n            None,\n            None,\n            None,\n            6367.7990436553955,\n        ],\n    }\n)","d54151a2":"# Previous R2 results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_R2_TRAIN' : [\n            0.7961566934117595,\n            0.7291388329945518,\n            0.7272799630194423,\n            None,\n        ],\n        'LINREG_R2_TEST' : [\n            -1.4937935438398293e+24,\n            0.7591790115899942,\n            0.7574900343482656,\n            None,\n        ],\n        'RIDGEREG_R2_TRAIN' : [\n            0.786210504976484,\n            0.5373330779735297,\n            0.3780338691031372,\n            None,\n        ],\n        'RIDGEREG_R2_TEST' : [\n            0.7574142187993411,\n            0.5301747053026842,\n            0.3676874489739226,\n            None,\n        ],\n        'FORESTREG_R2_TRAIN' : [\n            0.8979487218767636,\n            0.9753495018510526,\n            0.9593433749381992,\n            None,\n        ],\n        'FORESTREG_R2_TEST' : [\n            0.8266111288857395,\n            0.9470141207397708,\n            0.8938425415401196,\n            None,\n        ],\n        'CATBOOST_R2_TRAIN' : [\n            None,\n            None,\n            None,\n            0.9664667862347623,\n        ],\n        'CATBOOST_R2_TEST' : [\n            None,\n            None,\n            None,\n            0.9047771928538145,\n        ],\n    }\n)","0c87ce7b":"# Previous MSE results\nprint(\"DISCLAIMER: These results are taken from a previous run, and thus may not fully reflect the results of the current run\")\n\npd.DataFrame(\n    index=['Naive', 'Preprocess', 'PCA', 'Catboost'],\n    data={\n        'LINREG_MSE_TRAIN' : [\n            103397445.66153847,\n            129496441.17487168,\n            130385151.24374124,\n            None,\n        ],\n        'LINREG_MSE_TEST' : [\n            8.150553267490191e+32,\n            159286113.64080223,\n            160403253.06722423,\n            None,\n        ],\n        'RIDGEREG_MSE_TRAIN' : [\n            108442548.66486241,\n            221197156.1451429,\n            297356765.3602706,\n            None,\n        ],\n        'RIDGEREG_MSE_TEST' : [\n            132361552.8909861,\n            310756324.7563297,\n            418230194.6530032,\n            None,\n        ],\n        'FORESTREG_MSE_TRAIN' : [\n            51764473.70799574,\n            11785195.414934598,\n            19437589.795138095,\n            None,\n        ],\n        'FORESTREG_MSE_TEST' : [\n            94605793.13886109,\n            35046425.317520335,\n            70215678.05905038,\n            None,\n        ],\n        'CATBOOST_MSE_TRAIN' : [\n            None,\n            None,\n            None,\n            16338483.178480953,\n        ],\n        'CATBOOST_MSE_TEST' : [\n            None,\n            None,\n            None,\n            59566791.78955186,\n        ],\n    }\n)","9ad85aef":"### Collapse position ranking\n\nSince position rating contains 26 columns, we can reduce the number of columns by applying a little bit of logic. In football, a player will typically specialize in a small number of positions. Thus, a player will not be judged based on their total rating, but on the position they specialize in, and some other roles for flexibility. Knowing this, to reduce the number of attributes, we can do the following\n- Add column for the position rating the player specializes in\n- Add column for the sum of top 3 position rating the player has (showing overall performance and flexibility)\n- Drop all original rating columns","b4a0ed30":"By iterating over multiple values for the PCA cutoff, we decided to use 52 attributes seems to be the only compromise that is acceptable, since any less attributes causes the reconstruction error to go up significantly.","e3289c37":"According to sklearn's PCA, it seems like it does not recommend reducing the number of attributes we should use. However, we will continue on and find out whether PCA can improve our R2 score and runtime","5d515f36":"### Preferred Foot\n\nAssign 0\/1 value to preferred foot, since data is split to be either **Left** or **Right**, a simple if will suffice","fc4c0de5":"Upon closer look at the data above, we can conclude that\n- Rows with bad data in position columns can be grouped together\n- Row with bad club and value related columns can be grouped together\n\nRows with no data in regards to position rating can technically be used, since there are other fields that can be used as identifier. However, it is common knowledge that player's position and their relative skill in their position can influence their valuation for transfers and wages. Thus, rows without data in these fields will be ignored for the purposes of this kernel.\n\nAs mentioned in the note above, the players with no position rating are mostly Goal keepers. Because of this, values and wages for goal keepers will not be taken into account in this analysis. To evaluate the values and wages for Goal keepers, a separate data preparation and cleanup should be used, which is beyond the scope of this kernel.\n\nRows with no data in **Club** column can be assumed to mean that the player did not participate in the season, and did not play. Knowing this, since this kernel explores player's expected wage and value, if great players have 0 wage just because they don't play this season, this might become outliers and should be dropped.\n\nOne thing to point out is an exception in the **Value** column. There are 11 active players with clubs and wages that are valued at \u20ac0. According to what I can find, player value is derived from transfers between clubs. If the player values are 0, we will assume that those players have been in the club that are currently in, and have not been bought\/transferred, leaving them with 0 value. Since this is something that naturally happens, players with 0 value will still be processed normally.\n\nWith this observation, out of 18207 rows, 2281 will be dropped.","691128ed":"#### Dropping Release Clause column\n\nAs stated in the cleanup steps stated above, Value and Release Clause are very highly correlated to one another, since the buyout of a player will be based on their Value. Because the target of this kernel is to predict Value and Wage, using Release Clause as an attribute will cause the other attributes to have little to no effect, and thus will be dropped in this preprocess step","a659931c":"### Height and Weight\n\nPreparing height and weight data is very simple, by representing height in inches, and weight in lbs. We do not need SI units for these fields since exact values doesn't matter. This step just boils to splitting the strings, and multiplying accordingly","8d8af0a7":"#### Run time\n\nLooking at the results above, some observations can be made. The most obvious of which is from the runtime. Looking at the runtime table, the naive data encoding takes significantly more time to complete than other preparation methods. This roughly follows the number of attributes each preparation method produces\n\n```\n  Naive      : 926 ->   1.4s -> 0.481s\n  Preprocess : 53  -> 0.043s ->  0.03s\n  PCA        : 52  -> 0.031s -> 0.026s\n```\n\nThere is exception in the run time against this trend, which is the PCA random forest run time. As explained above, our hypothesis is that because random forest works by getting the GINI values for each potential internal node, and since PCA data preparation creates attributes which have continuous values, the number of unique values skyrocketed, causing potential checks for GINI values to increase, leading to significantly slower run times.\n\nOverall, the run times for the Naive data preparation is significantly slower than the manually processed and PCA decomposed data, which are comparable to each other. The run time for the catboost gradient boosting model seems to sit somewhere in the middle. Since this is a pre-built model which calculates the optimal configurations for the categorical data, it has its own overhead when processing the dataset fed, which contains 76 attributes, 7 of which are categorical data.","e5569ec4":"### Drop excluded attributes\n\nFor the target this kernel is trying to achieve, some attributes have too much variance in them, or is otherwise not logically relevant (or have some other special case) to the Value and Wage of players. In this case, these attributes will simply be dropped, so the dataset can be cleaner","b501d0ed":"By using PCA, Linear regression methods seems to have their runtime reduced very little, along with the accuracy. In my opinion, this trade off would not be worth it since R2 score should be higher priority of what to aim for.\n\nEven worse, using random forest regression on this new dataset has significantly worse runtime and results. Our current hypothesis is that the original data has relatively low cardinality. Because of this, a random forest regressor would take not too much effort when calculating when to create an internal node. After doing through PCA decomposition, the all values become continuous, and have very high cardinality. This causes the random forest to work much slower when deciding an internal node, and will generate worse internal nodes because of the tree depth limit.","60a69b0b":"## Analysis\n\nIn this kernel, we looked at multiple ways to prepare data and regression methods, the results from which varies wildly. Each data preparation method is tested with each regression method, with the exception of the `catboost` library, which is prepared and tested separately. We also looked at the results using a player's `Value` and `Wage`, which are somewhat related to each other, to have a more general view of how these methods differ.","5453b8f1":"### Currency\n\nData preparation for currency is pretty straightforward, since all currency is written in a consistent syntax (\u20ac1M). The only feature to be careful of is the suffix, either being **M** (million), **K** (thousand), or none.","ef4c15cc":"### Work Rate\n\nWork Rate attribute consists of 2 information, the player's attack work rate, and the player's defense work rate. Since this data is combined into 1 string, we will split them into their own columns AWR and DWR","d4063d6c":"## Data preprocessing\n\nLooking at the results from naive one-hot encoding, which has over 900 attributes, the results from linear regression and ridge regression does not look too good, while the result from random forest regression seems very promising. However, looking at the run time, having 900+ attributes seems to cause the regression to take a lot of time.\n\nBy doing some small preprocessing, we can hopefully achieve better results with faster runtimes by reducing the number of attributes to process. To do this, we will avoid doing one hot encoding wherever possible. Thus, some preprocessing with these columns will be done\n\n- Assign 0\/1 value to boolean-like data\n  - Preferred Foot\n- Assign values to ordinal data\n  - Body Type\n  - AWR\n  - DWR\n- Collapse Position rankings into 2 columns\n- Use mean encoding to assign values to nominal data with large cardinality (on training data)\n  - Nationality\n  - Club\n  - Position","a570228a":"## Determining a player's expected Value and Wage\n\nIn this kernel, we perform regression to find the expected Value and Wage for a player given other attributes.","cb1e244a":"## Optional PCA\n\nJust as a test, we will also try to perform PCA on our pre-processed data. Since reducing the number of attributes seems to significantly speed up the regression process, we may be able to improve this even further with PCA to reduce the attributes","f99ff78d":"### Testing the model\n\nAfter the data preparation is finished, we will try different methods to compare the results of using different algorithms. In this kernel, the metrics we will be focusing on are the `Run Time`, `R2 Score`, and `Mean Square Error` of the models. In analyzing these models, in truth, `R2 Score` should not be used to gauge the performance of said models, since their value can be volatile. However, since the dataset we are using is not normalized, purely using `MSE` would be confusing, since there is no static range to which the result can be compared to. The `R2 Score` is expected to have a value between 0 and 1, which will be a sufficient litmus test on whether the models are performant.\n\nWe will compare different algorithms to see their pros and cons, and what kind of data is compatible with them. The models we will be focusing on here will be as follows\n\n- OLS regression (linear regression)\n- Ridge regression\n- Random forest regression\n- CatBoostRegressor (Gradient boosting regression with encoding model)","d963f546":"## Deleting dirty rows\n\nIn the data provided, there is number of rows with empty columns. The number of bad data is quite significant, over 10% of the total data available. However, This still leaves over 15000 rows to process, which is fine.\n\nThe columns with particularly significant data loss is the position ratings, clubs, and value\/wage columns. Taking a look at those data reveals these results.","3bbd4dd4":"#### Observation on Body Type unique values\n\nLooking at the body type values, there seems to be some suspicious values. Some values seems to be names of real players\n- Messi\n- C. Ronaldo\n- Neymar\n- Shaqiri\n- Akinfenwa","ff8fe70d":"### Mean encoding for Club, Nationality, and Position\n\nSince we want to replicate real use case scenario, and assume that test dataset is unknown by our model, this encoding has to be done after the test\/train split is done. To achieve this, we will have to rejoin the attributes and target from our training dataset, then encode them with the mean values for the Club Nationality, and Position columns","2156e1e8":"## Data cleaning\n\nBefore going on to processing this dataset, some columns needs to be dropped, cleaned up, and transformed. Steps to be taken are as follows\n\n- Categorical data with way too high cardinality will be dropped\n  - Unnamed: 0\n  - ID\n  - Name\n- Data assumed to be irrelevant will also be dropped\n  - Real Face\n  - Jersey Number (Specifically for this column, usually players prefer certain numbers, and higher valued players get to choose their number. Although this will have some correlation with value and wages, I prefer not to use this data since this will most likely be determined outside of wage\/value)\n  - Loaned From\n  - Contract Valid Until\n  - Release Clause (This column is the value for which the player is bought out by another club. This data has high correlation with value and wage, which will be shown below. Since this data is basically another representation of a player's value, this column will be ignored)\n- Image data will not be processed in this kernel\n  - Photo\n  - Flag\n  - Club Logo\n- Transform data representation to numeric (currency, height, rating, etc)\n  - Value\n  - Wage\n  - Joined\n  - Height\n  - Weight\n  - Position ranking columns (LS, ST, RS, etc)\n- Split Work Rate columns into Attack Work Rate (AWS) and Defense Work Rate (DWR)\n  - Work Rate","947ed843":"## Conclusion\n\nBelieve in catboost Gradient Boosting Regression","d2be480c":"### Preparation\n\nInitially, the dataset we use has a lot of bad and dirty data, which cannot be directly used as inputs for our regression models. Before any processing steps can be done, there are some rows with missing data that needs to be dropped, and columns with ambiguous string data that needs to be represented as numerical data, details of which can be seen in the steps above.\n\nAfter the data has been cleaned, next step is to prepare all the remaining data to be directly used by the regression models. The cleaned data have been prepared in 4 different ways, which will be reviewed a little in this section\n\n#### Naive one-hot encoding\n\nAfter data clean up, we are left with 7 categorical data. This naive step will encode all the categorical data with one-hot encoding, which is simple enough, which leaves the dataset with 926 attributes to perform regression on.\n\n#### Manual pre-processing\n\nSince a naive one-hot encoding produces way too many attributes, an alternative method we can use is to manually process the data to reduce the number of attributes. This will be achieved by using mean encoding and applying a little bit of logic to reduce the number of attributes in the dataset.\n\n#### PCA\n\nAfter the manual process to reduce the dataset, we will see if we can use PCA to further minimize the dimensionality of our dataset. Doing this may cause some loss of accuracy, but hopefully can improve the run time when creating the model.\n\n#### Catboost library\n\nCatboost is a library made to perform data analytics on data with categorical attributes. Since that is the case, we can use the raw cleaned data, and feed it to the CatBoostRegressor. The library will then decide on how to encode the categorical data, then use gradient boosting to perform regression on the dataset.","29ffed4e":"Upon closer inspection, it appears that there are these values appear to be unique in the dataset. Looking back at the previous regression result, having these unique values may have affected the regression results heavily, with abnormally large coefficients even though logically, this attribute should not have as much of an impact.\n- `Body Type_Neymar` `4.978180e+07`\n- `Body Type_Messi` `3.829972e+07`\n- `Body Type_PLAYER_BODY_TYPE_25` `1.565128e+07`\n- `Body Type_Lean` `-2.629127e+07`\n- `Body Type_Normal` `-2.626753e+07`\n- `Body Type_Stocky` `-2.622343e+07`\n- `Body Type_Akinfenwa` `-2.495027e+07`\n\nTo handle this abnormality, since these abnormalities mostly have the player's name as their values, a quick google search lets us assign the their proper body types. After that, the attribute will contain only contain 3 ordinal unique values (Lean, Normal, or Stocky) and can be assigned values as usual","6c94dde6":"### Perform PCA on for Wage related dataset\n\nSame steps as above are repeated for X2_train and X2_test","2b52649c":"### Ordinal Data\n\nWith ordinal data, rather than using one-hot encoding, which will add a lot of additional attributes, we can assign them values which will reduce their dimensionality, though not by much, since these columns don't have too many unique values to begin with.","2398ecd2":"## Naive One-hot encoding\n\nFor the first step of data analysis, we will try to perform regression on the previously prepared data. However, as seen in the dataframe info, there are still some columns which are represented as strings. In this stage, we will naively encode these data with one-hot encoding just to see the results without any preprocessing.\n\nThese are the attributes that need to be encoded\n- Nationality\n- Club\n- Preferred Foot\n- Body Type\n- Position\n- AWR\n- DWR","9c3a83ff":"## Regression on naive one-hot data\n\nUsing the data above, we will use the different algorithms above to perform regression to predict a given player's Value and Wage. Algorithms used will be as follows\n- OLS Regression\n- Ridge Regression\n- Random Forest Regression","77135f1c":"## Regression on manually pre-processed data\n\nUsing the data above, we will use the different algorithms above to perform regression to predict a given player's Value and Wage. Algorithms used will be as follows\n- OLS Regression\n- Ridge Regression\n- Random Forest Regression","093ecc17":"### Results of regression towards player Wage\n\nResults of the different methods of regression to find a player's `Wage` is copied from the results above, and hardcoded into dataframes for easier evaluation","a5d3939f":"After the above encoding step, our data is finally ready for the first iteration of analysis. Although the data used is certainly not ideal, having over 900 columns, this will serve as the base line of how we can improve our model by properly preparing\/preprocessing our data.","c8a34793":"### Position Rating\n\nCleanup for position rating is very simple. Since it contains the base value and modifier based on mood, we can ignore the mood since this is usually not considered when detemining a player's value. A simple split and truncate will do.","02196c46":"### Results of regression towards player Value\n\nResults of the different methods of regression to find a player's `Value` is copied from the results above, and hardcoded into dataframes for easier evaluation","345fd635":"## Pre-built slope boosting regressor library\n\nAs an extra check, we decided to use a pre-built regressor library with their own model of how to encode and pre-process attributes. To use this, we just use the cleaned data, which has not been encoded or pre-processed by us in any way.","b0b48b4b":"## Regression model planning\n\nIn this kernel, we we compare several different regression methods to see which one might work best for the data we have. In this kernel, the target of the regression will be to predict the values for `value_val` and `wage_val` for a given player. To achieve this, we will be comparing several different Algorithms to see which one will work best.\n\nHere, we will be comparing these algorithms\n- OLS Regression\n- Ridge Regression\n- Random Forest Regression\n- Catboost\n\nAlong with the different algorithms, we will also try out several different data preparation methods\n- Naive one-hot encoding\n- Manual preprocessing (author insights)\n- PCA (using manual preprocessing data)\n- Catboost","1455dfea":"These results are similar to the regression towards player Value, only the values are shifted a bit. Not much to add from having this information. This is to be expected, since a player's Value and Wage are highly correlated (0.85).","fbdc9d7b":"## Regression on PCA decomposed attributes\n\nTo evaluate the result of the PCA, the same regression tests will be done on this new dataset as the tests run on the manually cleaned dataset.","cec852d9":"So far, the PCA does not seem to have any significant reconstruction error. However, this PCA was only able to reduce the number of attributes by 1. This does not seem like it will affect the result by much. We decided to iterate over different values for the cut off to see whether a better compromise can be reached between the reconstruction error and number of attributes.","c592bea5":"#### Accuracy\n\nAccording to the results we got, the most notable result we can observe occurs in the Linear regression of the naive data. Due to the data having over 900 attributes and training data only containing around 12000 rows, the model produced is highly overfit to the training dataset. As a result, the MSE is on the test dataset is extremely high. Looking at the most significant coeficients, the model seems to highly prioritize the Nationality of the player, which logically should not be such a high factor. Without any bias to balance this, overfitting is bound to happen. To compensate for this, using ridge regression, even with a small alpha (1e-2), proved to have a significant impact in reducing overfitting.\n\n- Using the naive dataset, the result using the linear regression dataset is unusable, while using the ridge regression is acceptable.\n- Using the manually preprocessed dataset, the result using linear regression and ridge regression were similar, but worse than the naive dataset. One possibility for that is that since this dataset was processed manually, some amount of information was lost. However, since the data was handled manually, we managed to keep the number of attributes very low, at 53. With this, the linear regression worked much better since the number of attributes were much lower than the number of rows, the data would have a much lower chance to be overfit. This observation is aligned with the ridge regression having a worse MSE.\n- The PCA model does not give any interesting results. The overall MSE shows similar trends to the manual preprocessed dataset. This is to be expected since the PCS dataset is derived from the previous method, only reducing the number attributes by one.\n- One note to add about the PCA data preparation. Previously, in this kernel, we tried to use the naive encoded dataset as the basis for PCA, but trying to decompose that dataset produced some complex numbers, which could not be processed by our algorithms of choice\n\nA peculiar note to take is from the random forest regressor. This regressor consistently generates overfit models, but the results even after overfit are still better overall than the linear and ridge regression models. In general, random forest regression is known to be more accurate than linear and ridge reggresion since the separator does not have to be continuous. However, we suspect the maximum depth of 8 to be too large for a random forest regressor with these amounts of attributes, which lead to the highly overfit models.\n\nLastly, with the CatBoostRegressor, the model they have built seems to encode the data much better than any methods used in this kernel. The MSE of the catboost model is at least an order of magnitude better than our current methods, but their model was still overfit. To negate this, we could further use hyperparameter tweaking to find better parameters to run this model.\n\nKey takeaways:\n\n- Naive encoding with 900+ attributes can easily lead to overfit data with linear regression\n- The result of the manual pre-processing is an underfit predictor since some information was lost in the manual encoding step\n- PCA should be done with a dataset with more redundant information, so the reduction can be effective\n- Some hyperparameter tweaking should be done with random forest regressor and CatBoostRegressor to generate a better fit model","992854c5":"### Note\n\nAn observation can be made that most players without position rating data are Goal keepers. Outside of that, 60 of them have no position, which I currently have no explaination for.","d1f3fa93":"### [Deprecated] Position\n\nThis attribute is a nominal data with 26 unique values. Since this column's cardinality is not too large, we can use one hot encoding to encode this column","b07597f7":"### Joined\n\nThe Joined column is the date when the player joined, which is represented with with a syntax that looks like this `Jan 1, 1970`. To parse this column into a numeric value, a method we will use is to parse this into python datetime, then transform it into unix time."}}