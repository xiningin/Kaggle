{"cell_type":{"81f31b57":"code","9518fbf7":"code","c58407b0":"code","14b7d2f0":"code","b3809cd5":"code","13d93fb7":"code","a7b2939b":"code","d307b38f":"code","701ec7d4":"code","a4b119e7":"code","bcc0ff86":"code","bab000e7":"code","93d7da5c":"code","f35fad3d":"code","845e6293":"code","4505621c":"code","078701c0":"code","b7cb3476":"code","67b8e878":"code","8e3216b7":"code","f4cb46c7":"code","0c8c9f41":"code","0098a3be":"code","a0bb7617":"code","2bfd12bb":"code","97f4f09b":"code","daac8556":"code","6952e4e9":"code","03f09699":"code","2e06c37e":"code","facd6e2b":"code","d8a1454a":"code","5607cdc7":"code","19e0dedf":"code","111f8ed9":"code","eaeac6d0":"code","0e7830f0":"code","5b33bd0b":"code","b76de420":"code","d4d4dfea":"code","ad9b9650":"code","370d351a":"code","6b6d8285":"code","ae81cd54":"code","6ece4eee":"code","10e3271c":"code","544805b6":"code","44cbf542":"code","450ca0b8":"code","406bb93b":"code","15d8ecf5":"code","f1b7b470":"code","484eda92":"code","95c0e2fb":"code","42dfae4b":"code","565b1ca6":"code","2c52cefa":"code","dee7335e":"code","a64e89bf":"code","fd340476":"code","5fdaabc8":"code","16117605":"code","84169349":"code","bed26e87":"code","9d8b8f8b":"code","b151f8a1":"code","165c7955":"code","53286f8c":"code","d97bba30":"code","0d6e529f":"code","348e66f2":"code","217c7911":"code","0372c049":"code","b6ad5ea5":"code","6596efae":"markdown","eb1727c9":"markdown","e8a9466c":"markdown","bc2b9511":"markdown","5d77b4fd":"markdown","8be5ed29":"markdown","449d6dc6":"markdown","25d20705":"markdown","20eb8ed2":"markdown","0b0aa637":"markdown","d4b32a9c":"markdown","a3a2e49c":"markdown","dca2fdce":"markdown","2359cace":"markdown","75a82696":"markdown","7c2f514a":"markdown","78e18986":"markdown","e0669402":"markdown","9d4a46fc":"markdown","88f83a27":"markdown","059ea846":"markdown","c237c03e":"markdown","4dd04c72":"markdown","9419a499":"markdown","4f3e1fe3":"markdown","dd751b58":"markdown","91be6e66":"markdown","481fb3fb":"markdown","2af42cfe":"markdown","180fa319":"markdown","bbdf91f3":"markdown","397a8ca6":"markdown","858d1a49":"markdown","cbf4832c":"markdown","3de55266":"markdown","60a5da84":"markdown","133b4e3e":"markdown","88b38eea":"markdown","153c390e":"markdown","16984dba":"markdown","72935c27":"markdown","80d9be4a":"markdown","c5b4314d":"markdown"},"source":{"81f31b57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9518fbf7":"# Read train.csv into dataframe train\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","c58407b0":"# Print first 5 rows of the df\ntrain.head()","14b7d2f0":"# Get the Data statistics\nprint(\"Number of rows:\",train.shape[0])\nprint(\"Number of columns:\",train.shape[1])\nprint(\"Column Names:\",train.columns.values.tolist())","b3809cd5":"# Check if data has any null values\nprint(\"Columns with Null Values:\",train.isnull().any())","13d93fb7":"# Check if data has any missing values\nprint(\"Columns with Missing Values:\",train.columns[train.isnull().any()].tolist())","a7b2939b":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","d307b38f":"# Get Toxic Comments\ntoxic_comments=train[train['toxic']==1]['comment_text']\n# Get Severe Toxic Comments\nseveretoxic_comments=train[train['severe_toxic']==1]['comment_text']\n# Get Obscene Comments\nobscene_comments=train[train['obscene']==1]['comment_text']\n# Get Threat Comments\nthreat_comments=train[train['threat']==1]['comment_text']\n# Get Insult Comments\ninsult_comments=train[train['insult']==1]['comment_text']\n# Get Identity Hate Comments\nidentityhate_comments=train[train['identity_hate']==1]['comment_text']","701ec7d4":"print(\"TOXIC COMMENTS:\\n\".format(),toxic_comments[:5])\nprint(\"\\nSEVERE TOXIC COMMENTS:\\n\".format(),severetoxic_comments[:5])\nprint(\"\\nOBSCENE COMMENTS:\\n\".format(),obscene_comments[:5])\nprint(\"\\nTHREAT COMMENTS:\\n\".format(),threat_comments[:5])\nprint(\"\\nINSULT COMMENTS:\\n\".format(),insult_comments[:5])\nprint(\"\\nIDENTITY HATE COMMENTS:\\n\".format(),identityhate_comments[:5])","a4b119e7":"# Toxicity category columns\nlabel_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# Assigned to count all clean comments\nvalue_counts_0 = []\n# Assigned to count all offensive comments\nvalue_counts_1 = []\n\nfor col in label_columns:\n  value_counts_0.append(train[col].value_counts()[0])\n  value_counts_1.append(train[col].value_counts()[1])\n\nprint(\"Number of clean comments:\",value_counts_0)\nprint(\"Number of offensive comments:\",value_counts_1)","bcc0ff86":"def plot_toxicity(labels,count,ylabel,xlabel,subtitle):\n    fig = plt.figure(figsize = (10, 5)) \n\n    # creating the bar plot \n    plt.bar(labels,count, color=['#422680','#341671','#280659','#660F56','#AE2D67','#F54952'],  width = 0.5) \n\n    plt.xlabel(xlabel,fontweight ='bold',fontname='Monsterrat') \n    plt.ylabel(ylabel,fontweight ='bold',fontname='Monsterrat') \n    plt.title(subtitle,fontweight ='bold',fontname='Comic Sans MS') \n\n    for x,y in zip(labels,count):\n\n        label = \"{:}\".format(y)\n\n        plt.annotate(label, # this is the text\n                     (x,y), # this is the point to label\n                     textcoords=\"offset points\", # how to position the text\n                     xytext=(0,3), # distance from text to points (x,y)\n                     ha='center') # horizontal alignment can be left, right or c\n    \n    plt.show() \n\nplot_toxicity(label_columns,value_counts_0,\"No. of occurences\",\"Type of toxicity\",\"Clean Comments\")\nplot_toxicity(label_columns,value_counts_1,\"No. of occurences\",\"Type of toxicity\",\"Toxicity Distribution\")","bab000e7":"#Heat map to show the correlation\ntemp_df=train.iloc[:,2:]\ncorr=temp_df.corr()\nplt.figure(figsize=(10,5))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","93d7da5c":"# Example of comments that are toxic and obscene\ntoxicnobscene_comments=train[(train['toxic']==1) & train['obscene']==1]['comment_text']\nprint(\"TOXIC AND OBSCENE COMMENTS:\\n\".format(),toxicnobscene_comments[:5])\n","f35fad3d":"# Example of comments that are toxic and severe_toxic\ntoxicnsevere_comments=train[(train['toxic']==1) & train['severe_toxic']==1]['comment_text']\nprint(\"TOXIC AND SEVERE TOXIC COMMENTS:\\n\".format(),toxicnsevere_comments[:5])","845e6293":"# Example of comments that fall under all catgeories\nallcat_comments=train[(train['toxic']==1) \n                    & (train['severe_toxic']==1)\n                    & (train['obscene']==1)\n                    & (train['threat']==1)\n                    & (train['insult']==1)\n                    & (train['identity_hate']==1)]['comment_text']\nprint(\"Comments in all categories:\\n\".format(),allcat_comments[:5])\nprint(\"\\nCount of comments in all categories:\",allcat_comments.count())","4505621c":"# Summing up all the counts for toxic columns\nx=train.iloc[:,2:].sum()\n# Calculate sum of rows\nrowsums=train.iloc[:,2:].sum(axis=1)\n# marking comments without any tags as \"clean\"\n# creates a new column with bool value marking as clean or offensive\ntrain['clean']=(rowsums==0)\n#count number of clean entries\ntrain['clean'].sum()\n\nprint(\"Total comments:\",len(train))\nprint(\"Total clean comments:\",train['clean'].sum())\nprint(\"Total offensive comments:\",x.sum())","078701c0":"train.head()","b7cb3476":"# Count of all clean comments\ncount_clean=train[train['clean']==True]\n# Count of all offensive comments\ncount_offensive=train[train['clean']==False]","67b8e878":"#Analyse the count of words in each segment\n#Function for checking word length\ndef cal_len(data):\n    return len(data)\n\n#Create generic plotter with Seaborn\ndef plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='#422680')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='#F54952')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\ncount_clean_words=count_clean['comment_text'].str.split().apply(lambda z:cal_len(z))\ncount_offensive_words=count_offensive['comment_text'].str.split().apply(lambda z:cal_len(z))\nprint(\"Clean Words:\" + str(count_clean_words))\nprint(\"Offensive Words:\" + str(count_offensive_words))\nplot_count(count_clean_words,count_offensive_words,\"Clean Words\",\"Offensive words\",\"Comments Word Analysis\")","8e3216b7":"#Count Punctuations\/Stopwords\/Codes and other semantic datatypes\n#We will be using the \"generic_plotter\" function.\n\ncount_clean_punctuations=count_clean['comment_text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\ncount_offensive_punctuations=count_offensive['comment_text'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\nplot_count(count_clean_punctuations,count_offensive_punctuations,\"Clean Comments Punctuations\",\"Offensive Comments Punctuations\",\"Comments Word Punctuation Analysis\")","f4cb46c7":"#Analyse Stopwords\n\ndef plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='#422680')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='#F54952')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\nstops=set(stopwords.words('english'))\ncount_clean_stops=count_clean['comment_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ncount_offensive_stops=count_offensive['comment_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\nplot_count_1(count_clean_stops,count_offensive_stops,\"Clean Comments Stopwords\",\"Offensive Comments Stopwords\",\"Comments Stopwords Analysis\")","0c8c9f41":"## Checking number of Urls\ncount_clean_urls=count_clean['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ncount_offensive_urls=count_offensive['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\nplot_count_1(count_clean_stops,count_offensive_stops,\"Clean Comments URLs\",\"Offensive Comments URLs\",\"Comments URLs Analysis\")","0098a3be":"#WordCloud Visualizations\n#Method for creating wordclouds\nfrom PIL import Image\ndef display_cloud(data,label):\n    plt.subplots(figsize=(10,10))\n    text = data[data[label]==1][\"comment_text\"].tolist()\n    wc = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=1800,\n                          height=800,\n                         )\n    wc.generate(' '.join(text))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.title(label,fontsize=36)\n    plt.show()\n    \n","a0bb7617":"display_cloud(train,\"toxic\")","2bfd12bb":"display_cloud(train,\"severe_toxic\")","97f4f09b":"display_cloud(train,\"obscene\")","daac8556":"display_cloud(train,\"threat\")","6952e4e9":"display_cloud(train,\"insult\")","03f09699":"display_cloud(train,\"identity_hate\")","2e06c37e":"#Simplified counter function\ndef create_corpus(word):\n    corpus=[]\n    \n    for x in train[train['clean']==word]['comment_text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ndef plotword(corpus):\n    counter=Counter(corpus)\n    most=counter.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:50]:\n        if (word not in stops) :\n            x.append(word)\n            y.append(count)\n    return sns.barplot(x=y,y=x,palette=\"rocket\")","facd6e2b":"# Top words for offensive comments\ncorpus=create_corpus(False)\nplotword(corpus)\n","d8a1454a":"# Top words for clean comments\ncorpus=create_corpus(True)\nplotword(corpus)","5607cdc7":"#Gram analysis on Training set- Bigram and Trigram\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            #print(len(tokens))\n            if (len(tokens) <=30):\n                freq_dict[tokens]+=1\n    return freq_dict\n\n# Horizontal bar chart\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    #print(freq_df.head())\n    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n    #plt.show()\n    trace=horizontal_bar_chart(freq_df[:20],'#660F56')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one):\n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of clean comments\", \n                                          \"Frequent words of offensive comments\"])\n    fig.append_trace(trace_zero, 1, 1)\n    fig.append_trace(trace_ones, 1, 2)\n    fig['layout'].update(height=1200, width=900, title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n    \n    \ntrain_df_zero=count_clean['comment_text']\ntrain_df_ones=count_offensive['comment_text']\n","19e0dedf":"#print(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\n","111f8ed9":"#print(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","eaeac6d0":"%%time\nimport re\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[\\.,!?;:]+')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ntrain['comment_text']=train['comment_text'].apply(lambda z: remove_punctuations(z))\ntrain['comment_text']=train['comment_text'].apply(lambda z: remove_html(z))\ntrain['comment_text']=train['comment_text'].apply(lambda z: remove_url(z))\ntrain['comment_text']=train['comment_text'].apply(lambda z: remove_emoji(z))","0e7830f0":"def remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", data)\n    data = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", data)\n    data = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"don\u00e5\u00abt\", \"do not\", data)  \n    return data\n\n\ntrain['comment_text']=train['comment_text'].apply(lambda z: remove_abb(z))","5b33bd0b":"# Count of clean and offensive comments\ncount_good=train[train['clean']==True]\ncount_bad=train[train['clean']==False]","b76de420":"#Apply Gram Analysis\ntrain_df_zero=count_bad['comment_text']\ntrain_df_ones=count_good['comment_text']\n\nprint(train_df_zero)\n","d4d4dfea":"print(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","ad9b9650":"display_cloud(train,\"toxic\")","370d351a":"display_cloud(train,\"severe_toxic\")","6b6d8285":"display_cloud(train,\"obscene\")","ae81cd54":"display_cloud(train,\"threat\")\n","6ece4eee":"display_cloud(train,\"insult\")","10e3271c":"display_cloud(train,\"identity_hate\")","544805b6":"%%time\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\ndef lemma_traincorpus(text):\n    lemm_words = \" \".join([lemm.lemmatize(word) for word in text.split()])    \n    return lemm_words\n                        \n\n\ntrain['comment_text']=train['comment_text'].apply(lambda z: lemma_traincorpus(z))","44cbf542":"#check a sample from the lemmatized dataset\ntrain['comment_text'][5:10]","450ca0b8":"#pd.set_option('display.max_colwidth', None)","406bb93b":"#print(train[train['id']== '001e89eb3f0b0915']['comment_text'])","15d8ecf5":"%%time\ntfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\ntrain_tfidf=tfidf_vect.fit_transform(train['comment_text'].values.tolist())\ntrain_tfidf.shape","f1b7b470":"## Outputs from the TF-IDF transformed data\nprint(train_tfidf)","484eda92":"%%time\ntrain_li=[]\nfor i in range(len(train)):\n    if (train['clean'][i]==False):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain['Binary']=train_li\ntrain.head()","95c0e2fb":"#Count Vectorization\nimport matplotlib\nimport matplotlib.pyplot as plt\ndef vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv\n","42dfae4b":"#Tfidf vectorization from sklearn\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf,tfidfv","565b1ca6":"def dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['#422680','#F54952']\n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label)       \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='#F54952',label='Offensive Comment')\n        color_orange=mpatches.Patch(color='#422680',label='Clean Comment')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['#422680','#F54952']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='#F54952',label='Offensive Comment')\n        color_orange=mpatches.Patch(color='#422680',label='Clean Comment')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['#422680','#F54952']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='#F54952',label='Offensive Comment')\n        color_orange=mpatches.Patch(color='#422680',label='Clean Comment')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show()\n","2c52cefa":"train_data=train       \ndata_vect=train_data['comment_text'].values\ndata_vect_good=count_good['comment_text'].values\ntarget_vect=train_data['Binary'].values\ntarget_data_vect_good=train[train['clean']==True]['Binary'].values\ndata_vect_bad=count_bad['comment_text'].values\ntarget_data_vect_bad=train[train['clean']==False]['Binary'].values\ntrain_data_cv,cv= vectorize(data_vect)\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\nfalse_review_train_data_cv,cv=vectorize(data_vect_bad)","dee7335e":"dimen_reduc_plot(train_data_cv,target_vect,1)","a64e89bf":"dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,1)","fd340476":"dimen_reduc_plot(false_review_train_data_cv,target_data_vect_bad,1)","5fdaabc8":"# using TFIDF\ntrain_data_cv,cv= tfidf(data_vect)","16117605":"dimen_reduc_plot(train_data_cv,target_vect,1)","84169349":"#TSNE visualization on first 1000 samples\ntrain_data=train[:1000]       \ndata_vect=train_data['comment_text'].values\ndata_vect_good=count_good['comment_text'].values\ntarget_vect=train_data['Binary'].values\ntarget_data_vect_good=train[train['clean']==True]['Binary'].values\ndata_vect_bad=count_bad['comment_text'].values\ntarget_data_vect_bad=train[train['clean']==False]['Binary'].values\ntrain_data_cv,cv= vectorize(data_vect)\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\ndimen_reduc_plot(train_data_cv,target_vect,3)","bed26e87":"check_df=list(train['comment_text'].str.split())","9d8b8f8b":"%%time\n## Load word2vec algorithm from gensim\nfrom gensim.models import Word2Vec,KeyedVectors\n\nmodel=Word2Vec(check_df,min_count=1)\nword_li=list(model.wv.vocab)\n","b151f8a1":"print(model)\nprint(model['edits'])","165c7955":"#View the Embedding Word Vector\nplt.plot(model['edits'])\nplt.show()","53286f8c":"##save the modeled words produced from Word2Vec\nmodel.save('word2vec_model.bin')\nloaded_model=KeyedVectors.load('word2vec_model.bin')\nprint(loaded_model)","d97bba30":"# Check similarity with another word with cosine similarity\ndistance=model.similarity('edits','suggestion')\nprint(distance)","0d6e529f":"# PCA transform in 2D for visualization of embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=loaded_model[loaded_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(loaded_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","348e66f2":"model.wv.most_similar(\"edits\")","217c7911":"from gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_file='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nprint(glove_loaded)","0372c049":"glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nplt.plot(glove_model['edits'])\nplt.plot(glove_model['suggestion'])\nplt.show()","b6ad5ea5":"# PCA transform in 2D for visualization of glove embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\ntransformation_model=glove_model[glove_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(glove_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","6596efae":"The most important phase is now done! Cleaning and Preprocessing of data is the most crucial step. If unclean data is used for further analysis, the expected result would be far from reality. \n","eb1727c9":"# Gram Statistic\n\nGrams analysis is crucial to undersatnd what is the context, the neighbors of the word and how that can be leverged to see what needs to emitted and what needs to be transformed.\n","e8a9466c":"# Vectorization\n\nThis is to convert our text data to the world of numbers so that the machine can better understand. \n\nVectorization deals with non-sematic analysis,\n* Count Vectorization\n* TF-IDF\n","bc2b9511":"Let us know see how many clean(0) and offensive(1) comments are available in each category of toxicity","5d77b4fd":"# Steps to arrive at our goal, (will be updated accordingly as we progress)\n\n1. Load Data\n2. Understand the data\n3. Import Libraries \n4. Statistical Analysis I\n5. Statistical Analysis II\n6. Cleaning the data\n7. Transformation\n8. Vectorization and Embeddings\n9. Converstion to Binary numbers","8be5ed29":"# Semantic Embeddings\n\n'Semantic' means meaning\/related to. And our context here is to look at words that are similar. To enable the machine to capture words with their meaning like we humans do. Hence, we look at embeddings that are semantic and that can be static or dynamic embeddings. \n\nWord Embeddings: These are vector space transformations of the words present in the corpus. When converted to vectors, several metrics can be applied like finding similarity, distance measurement between the vectors, numerical transforms of the vectors. With word vectors, we can specify semantic similarity between different words or collection of words.","449d6dc6":"2) With Negative Sampling: This includes the follwing steps,\n\n* An (integer) input of a target word and a real or negative context word\n* An embedding layer lookup (i.e. looking up the integer index of the word in the embedding matrix to get the word vector)\n* The application of a dot product operation\n* The output sigmoid layer scale of 0 and 1\n* Find error = target - sigmoid_scores\n* Update model parameters and train\n\n![image.png](attachment:image.png)","25d20705":"# Statistical Analysis I\n\nIts time for us to look at the data more closely. Lets see some numbers and graphs to know more!","20eb8ed2":"# Another way to look at the data is by marking all clean comments and offensive comments. The comments with any of the label columns as 1 will be tagged as offensive and the rows with all 0 will be marked clean. \n\nP.S: Now, let us not get confused with \"clean data\" and \"clean comments\" :)  Well, clean comments are comments with no toxicity and clean data is what we get after cleaning the whole data for punctuations, stop words, etc! ","0b0aa637":"To help us achieve dimensionality reduction, useful and powerful techniques are available,\n* PCA \u2013 Principal Component Analysis \u2013 technique uses Eigen vectors to find the small matrices\n* SVD \u2013 Singular Value Decomposition \u2013 Linear dimensionality reduction technique usually on Sparse matrices \n* TSNE \u2013 t-distributed Stochastic Neighbor Embedding \u2013 technique that uses non-convex optimization along with gradient descent and minimizes the distance between datapoints.\n\nIt is advised to use PCA for Dense Data and SVD for Sparse data. TSNE is limited in usage as it takes more memory and time.\n","d4b32a9c":"[Jay](http:\/\/jalammar.github.io\/illustrated-word2vec\/)'s blog is a great resource!\n\nCo learning Lounge's youtube [video](https:\/\/www.youtube.com\/watch?v=UTknlpS1bFU&t=334s) by Abhilash is of great help!","a3a2e49c":"# Stopwords analysis","dca2fdce":"# Inference from Analysis - II\n\nThe gram analysis typically shows the bag of words that are together and the frequency. \n\nAlso shows some cleaning is required in terms of punctuations and characters.","2359cace":"# Check for URLs","75a82696":"# **Inference From Analysis -I**\n\nThe following can be inferred from the data:\n\n* The dataset is not balanced with respect to clean and offensive comments.\n* There are more clean comments in comparison with offensive comments\n* The length of comments has no influence on whether it is offensive or not!\n* The dataset contains redundant words and html syntaxes.\n* Punctuations\/stopwords are more in clean comments in comparison with offensive comments\n\nThe above analysis could also be done on toxic columns. The goal here is to understand the presence of data that needs to be cleaned. Same goes with the further analysis.\n","7c2f514a":"![Capture7.PNG](attachment:Capture7.PNG)","78e18986":"There is yet two ways to do this, \n\n1) **Softmax function**: This includes the following steps,\n* Build vocabulary from training documents.\n* Represent the word as one-hot vector or encoding technique as Huffman \n* Calculate the probability distribution of the input word\n* Get the log likelihood of the words\n* Derive the hidden layer weight matrix\n* Output layer will be a softmax regression classifier which produces output between 0 and 1\n\n\n![image.png](attachment:image.png)","e0669402":"# Converting the data to Binary numbers","9d4a46fc":"# Transforming Data\n\nNow that we have cleaned the data, there could be some transformations applied to the data to knock off some redundencies. \nLemmatization is used in order to get the root word of words. Stemming takes a back seat here as it simply cuts the word to a meaning word.","88f83a27":"![image.png](attachment:image.png)","059ea846":"# This marks the end of Data Preparation!","c237c03e":"# Load the dataset\n\nThe problem statement comes wth four csv files, \n* train\n* test\n* test_labels \n* sample_submission\n\ntrain is the dataset that will be seen with a microscope to establish what the data is, the balance in the data, toxicity distribution, etc. Exploring the data will set us in a path to pin point the pain point!\n\nLater, the train data is enhanced and analyzed to get a model.\n\nOnce we have the model ready, we treat the same on the test data to ensure we have achieved our goal!\n\nLets bring the real food(data) to the table!","4dd04c72":"It is seen from the heat map above that a comment can fall into multiple categories. For example, it can be just toxic or\/and obscene. ","9419a499":"![Capture1.PNG](attachment:Capture1.PNG)","4f3e1fe3":"# Data Cleaning\n\nFinaly, we arrive at cleaning the data.\n\nThis process would involve, \n* Removing puctuations \n* Removing HTML tags\n* Removing URL data\n* Removing emojis\n* Removing Abbrevations\n","dd751b58":"![Capture2.PNG](attachment:Capture2.PNG)","91be6e66":"![image.png](attachment:image.png)","481fb3fb":"# **Data Overview:**\n      \nNow that we have seen how the data looks, Here is a high level overview of the data. The dataset has comments that are rated for toxicity. \n\nThe comments are tagged in the following categories, \n* toxic\n* severe_toxic\n* obscene\n* threat \n* insult\n* identity_hate\n\nWe also know, \n* 0 says the comment is not offensive and 1 says the comment is offensive\n* We have 159571 records in the training set\n* the are no nulls or missing values in the dataset \n\nDoes this mean we have a clean dataset? Let's find out in the following sections!","2af42cfe":"# Counter","180fa319":"# Word Length in each segment","bbdf91f3":"![image.png](attachment:image.png)\n","397a8ca6":"The above plot shows the vectors of each word with the word \"edits\".","858d1a49":"# Static Word Embeddings\n\nUnsupervised technique with supervised tasks, takes a corpus of words and produces word embeddings as output. \n\n\n# Word2Vec Methodology\n\nTransforming words with its context to vectors that the machine can understand has two steps,\n1. Mapping of a high dimensional one-hot style representation of words to a lower dimensional vector.\n2. Mainting the meaning and context.\n\nTwo different architectures\/approaches to achieve this are,","cbf4832c":"![Capture3.PNG](attachment:Capture3.PNG)","3de55266":"# The Toxic Comments Problem\n\n# **Introduction**\n\nInternet is the new punching bag for some folks where they put out their profanity best and shower all the harsh, offensive and ill-themed words out there for everyone to read. \n\nThis spreads a whole new level of negativity and impacts the people on the other side!\n\nHere is an approach to identity,analyze and eliminate the toxic comments in order to keep the sanity of the medium. \n\nCredits to Abhilash Majemdar's [notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop) \n\nThanks to [Co-learning Lounge](https:\/\/www.kaggle.com\/colearninglounge) and Yogesh Kothiya.","60a5da84":"# Import Libraries\n\nIn order to get deeper into the data, we need libraries that enable us with various functionalities to make it easier for us.\n\n","133b4e3e":"![image.png](attachment:image.png)","88b38eea":"![Capture6.PNG](attachment:Capture6.PNG)","153c390e":"**Sample data of all the categories of toxicity**","16984dba":"![Capture.PNG](attachment:Capture.PNG)","72935c27":"# Wordcloud","80d9be4a":"# Visualization of Vectors\n\nThough human eye is super powerful, there is only 2 and 3 dimension that can be perceived well and understood. A dataset can be recognised with n number of measures or dimensions. Making sense of each one visually is unimaginable. Hence, Reduction comes into picture! Resons why we need reduction is listed below. ","c5b4314d":"# Count punctuations "}}