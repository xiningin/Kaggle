{"cell_type":{"83bb015f":"code","61c8ce86":"code","7ba0a6e9":"code","6865441b":"code","063dc789":"code","7406b525":"code","68c38976":"code","2524e260":"code","36ae8f41":"code","0982d8b8":"code","abbb9880":"code","56a5500e":"code","3d065b25":"code","918b1deb":"code","c94ae034":"code","4c9cff4a":"code","a25f1546":"code","573cde53":"code","084b1828":"code","f1c103e4":"code","f84d4edc":"code","e9cac34e":"code","94343499":"code","3dd5da32":"code","3bf7f9e5":"code","1a8e88f8":"code","3fe8ea5d":"code","bcbed102":"code","5bee146d":"code","f2317374":"markdown","6ad63180":"markdown","e6139f00":"markdown","29fbbc3f":"markdown","89312e90":"markdown","5222c173":"markdown"},"source":{"83bb015f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61c8ce86":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","7ba0a6e9":"import tokenization\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for text processing\nimport re\nimport nltk \n## for ner\nimport spacy\n\nimport sys\nimport random\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom spacy.util import minibatch\nfrom tensorflow import math\n\n#keras Bert\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","6865441b":"random_seed = 0\ndata = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nprint(data.shape)\ndata.head()","063dc789":"#number of missing values\nprint(data.isnull().sum())\n\n#number of unique values\ndisplay(data.nunique())","7406b525":"fig, ax = plt.subplots()\nfig.suptitle('target distribution', fontsize=12)\ndata['target'].reset_index().groupby('target').count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","68c38976":"def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## Remove URLs from a sample string\n    text = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", text)\n    \n    ## Remove the html in sample text\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    text =  re.sub(html, \"\", text)\n        \n     ## Remove non-ASCII characters\n    l = len(text)\n    t = text\n    text= re.sub(r'[^\\x00-\\x7f]',r'', text)\n        \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n\n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    \n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","2524e260":"lst_stopwords = nltk.corpus.stopwords.words(\"english\")\ndata[\"text_clean\"] = data[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\ndata.head()","36ae8f41":"def clean_location(data):\n    # convert to lower case\n    data['location'] = data['location'].str.lower()\n    # remove trailing white spaces\n    data['location'] = data['location'].str.strip()\n    # remove comma\n    data['location'] = data['location'].apply(lambda x: re.sub(r',(?s).*$', r'', x) if str(x) != str(np.nan) else np.nan)\n    return data","0982d8b8":"data = clean_location(data)","abbb9880":"# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio >= 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match","56a5500e":"locations = data.groupby('location').location.count().sort_values(ascending=False)\nlocations = locations[locations.values > 3]\nlocations = locations.index\n#apply function with the most common locations\nfor loc in locations:\n    replace_matches_in_column(df=data, column='location', string_to_match=loc)","3d065b25":"#manualy fusion some of them\ndef rename_location(data):\n    data.loc[(data.location == 'united states'), 'location'] = 'usa'\n    data.loc[(data.location == 'united states of america'), 'location'] = 'usa'\n    data.loc[(data.location == 'us'), 'location'] = 'usa'\n    data.loc[(data.location == 'u.s.'), 'location'] = 'usa'\n    data.loc[(data.location == 'u.s.a'), 'location'] = 'usa'\n    data.loc[(data.location == 'u.s.a.'), 'location'] = 'usa'\n    data.loc[(data.location == 'new york'), 'location'] = 'nyc'\n    data.loc[(data.location == 'ny'), 'location'] = 'nyc'\n    data.loc[(data.location == 'new york city'), 'location'] = 'nyc'\n    data.loc[(data.location == 'uk'), 'location'] = 'england'\n    data.loc[(data.location == 'uk.'), 'location'] = 'england'\n    data.loc[(data.location == 'united kingdom'), 'location'] = 'england'\n    data.loc[(data.location == '?? | pittsburgh'), 'location'] = 'pittsburgh'\n    data.loc[(data.location == '??????'), 'location'] = np.nan\n    data.loc[(data.location == '?????'), 'location'] = np.nan\n    data.loc[(data.location == '??'), 'location'] = np.nan\n    data.loc[(data.location == 'world'), 'location'] = np.nan\n    data.loc[(data.location == 'world wide'), 'location'] = np.nan\n    data.loc[(data.location == 'worldwide'), 'location'] = np.nan\n    data.loc[(data.location == 'everywhere'), 'location'] = np.nan\n    data.loc[(data.location == 'everywhere..'), 'location'] = np.nan\n    data.loc[(data.location == 'earth'), 'location'] = np.nan\n    data.loc[(data.location == 'happily married with 2 kids'), 'location'] = np.nan\n    data.loc[(data.location == 'road to the billionaires club'), 'location'] = np.nan\n    data.loc[(data.location == 'pedophile hunting ground'), 'location'] = np.nan\n    data.loc[(data.location == 'does it really matter!'), 'location'] = np.nan\n    data.loc[(data.location == 'planet earth'), 'location'] = np.nan\n    data.loc[(data.location == 'in the word of god'), 'location'] = np.nan\n    data.loc[(data.location == 'mad as hell'), 'location'] = np.nan\n    data.loc[(data.location == 'global'), 'location'] = np.nan\n    data.loc[(data.location == '#global'), 'location'] = np.nan\n    data.loc[(data.location == '304'), 'location'] = np.nan\n    data.loc[(data.location == ''), 'location'] = np.nan\n    return data","918b1deb":"data = rename_location(data)","c94ae034":"locations = data.groupby('location').location.count().sort_values(ascending=False)\nlocations = locations[locations.values <= 3]\nlocations = locations.index\n\n#set None to locations seen less than 3 times\nfor loc in locations:\n    data.loc[(data.location == loc), 'location'] = np.nan\ndata.groupby('location').location.count().sort_values(ascending=False)","4c9cff4a":"df = data.groupby('location').location.count().sort_values(ascending=False)\nd = pd.DataFrame({'Disaster': [data.loc[(data.location == k) & (data.target == 1)].count()[0] for k, v in df.items()],\n                    'NotDisaster': [v for k, v in df.items()]},\n                     index=[k for k, v in df.items()])\n\n#target distribution by localisations\nd[:16].plot.barh(figsize=(15,3), stacked=True)\nd[16:].plot.barh(figsize=(15,25), stacked=True)\nplt.show()","a25f1546":"#format sequence : XXTWT text_clean XXLOC location XXKEY keyword TWTXX\ndef concatenate(data):\n    data['sequence'] = data['text_clean'].map(str) + ' XXLOC ' + data['location'].map(str) \\\n                    + ' XXKEY ' + data['keyword'].map(str) + ' TWTXX'\n    return data\ndata = concatenate(data)\nprint(data.sequence[0])\nprint(data.sequence[98])\nprint(data.sequence[100])","573cde53":"#preprocessing test.csv\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n#text\ntest[\"text_clean\"] = test[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n#location\ntest = clean_location(test)\n\nlocations = test.groupby('location').location.count().sort_values(ascending=False)\nlocations = locations.index\n#apply replace_matches_in_column in test\nfor loc in locations:\n    replace_matches_in_column(df=test, column='location', string_to_match=loc)\nlocations = data.groupby('location').location.count().sort_values(ascending=False)\nlocations = locations.index\n#set test locations same as data locations\nfor loc in locations:\n    replace_matches_in_column(df=test, column='location', string_to_match=loc)\n\ntest = rename_location(test)\n#create sequences\ntest = concatenate(test)","084b1828":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","f1c103e4":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","f84d4edc":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","e9cac34e":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","94343499":"#max token\nprint(data['sequence'].map(lambda s: len(tokenizer.tokenize(s))).max())\n#max token\nprint(test['sequence'].map(lambda s: len(tokenizer.tokenize(s))).max())","3dd5da32":"train_input = bert_encode(data.sequence.values, tokenizer, max_len=65)\ntest_input = bert_encode(test.sequence.values, tokenizer, max_len=65)\ntrain_labels = data.target.values","3bf7f9e5":"model = build_model(bert_layer, max_len=65)\nmodel.summary()","1a8e88f8":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.15,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=8\n)","3fe8ea5d":"#training history\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(train_history.history['loss'], label=\"Training loss\")\nax[0].plot(train_history.history['val_loss'], label=\"validation loss\",)\nlegend = ax[0].legend(loc='best')\n\nax[1].plot(train_history.history['accuracy'], label=\"Training accuracy\")\nax[1].plot(train_history.history['val_accuracy'], label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best')","bcbed102":"#predict with test.csv\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)","5bee146d":"#make submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","f2317374":"# Make Submission","6ad63180":"# Keras Bert using TFHub","e6139f00":"from 3341 unique locations to 166","29fbbc3f":"# Concatenate data","89312e90":"# Location preprocessing","5222c173":"# Text Preprocessing"}}