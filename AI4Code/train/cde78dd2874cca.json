{"cell_type":{"b6e269be":"code","48d7f113":"code","8009e5eb":"code","15b73a97":"code","8b4f93c9":"code","4705eb76":"code","461e9edb":"code","9fbe2e1e":"code","4be0740a":"code","5add2a8a":"code","9e61cf8d":"code","63a60222":"code","44c749c6":"code","5b2d43c9":"code","3a4bda14":"code","95aedadc":"code","5fa0d668":"code","524f2a7b":"code","bf891d9d":"code","680adb3c":"code","33120251":"code","2d2064df":"code","8629f93f":"markdown","dbb3316f":"markdown","3d440ba6":"markdown"},"source":{"b6e269be":"#Kutuphaneler\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom tensorflow.python.keras.utils import np_utils\n\n#Veri Setimizi Projeye Dahil Edelim\ndataset = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","48d7f113":"#Veri setinin ilk 5 verisi yazdiralim.\ndataset.head()","8009e5eb":"dataset = dataset.drop('Id', 1) ##Id s\u00fctununu e\u011fitim s\u0131ras\u0131nda bizim bir i\u015fimize yaramayacak. O y\u00fczden onu silebiliriz.","15b73a97":"dataset.head() #Id s\u00fctununu silebildik mi kontrol edelim.","8b4f93c9":"#Her t\u00fcrden ka\u00e7 tane veri oldu\u011funa bakal\u0131m.\ndataset[\"Species\"].value_counts()","4705eb76":"#Birde Grafik olarak bakal\u0131m.\nsns.countplot(x='Species',data=dataset)","461e9edb":"#Alt Yaprak Uzunlu\u011funa g\u00f6re da\u011f\u0131l\u0131m\u0131n grafi\u011fini inceleyelim.\nsns.FacetGrid(dataset, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n   .add_legend()\n\nplt.text(x=4.2, y=4.7, s='Alt Yaprak Uzunlu\u011fu ve Genisli\u011fi', fontsize=16, weight='bold')\nplt.show()","9fbe2e1e":"#Ta\u00e7 Yaprak Uzunlu\u011funa g\u00f6re da\u011f\u0131l\u0131m\u0131n grafi\u011fini inceleyelim.\n\nax = sns.FacetGrid(dataset, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\") \\\n   .add_legend()\n\nplt.text(x=1, y=2.8, s='Ta\u00e7 Yaprak Uzunlu\u011fu ve Genisli\u011fi', fontsize=16, weight='bold')\nplt.show()\n","4be0740a":"#iloc int bir de\u011fere eri\u015fmemizi sa\u011flar. \u0130lk 4 verimiz ba\u011f\u0131ms\u0131z de\u011fi\u015fken, 5. verimiz ise hedef de\u011fi\u015fkenimiz, yani ba\u011f\u0131ml\u0131 de\u011fi\u015fken. Bunlar\u0131 ay\u0131ral\u0131m.(Dizilerin 0'dan ba\u015flad\u0131\u011f\u0131n\u0131 unutmayal\u0131m...)\nX = dataset.iloc[:,0:4].values\n#Hedef nitelik 4.indiste\ny = dataset.iloc[:,4].values\n\n#print(X)\n#print(y)\n\n#Deneyerek ay\u0131rma i\u015flemini yap\u0131p yapmad\u0131\u011f\u0131m\u0131z\u0131 kontrol edebiliriz.","5add2a8a":"#Hedef degiskenimiz kategorik halde onuda say\u0131sal hale \u00e7evirmeliyiz. C\u00fcnk\u00fc Ysa'lar\u0131 sadece say\u0131sal de\u011ferler ile \u00e7al\u0131\u015fabilir. Bunun i\u00e7in  Label Encoder kullanaca\u011f\u0131z ve verimizi say\u0131sal hale d\u00f6n\u00fc\u015ft\u00fcrece\u011fiz.\nfrom sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\ny = le.fit_transform(y)\n\n#Kategorik degi\u015fkenden g\u00f6lge degiskeni yaratal\u0131m.\ny = np_utils.to_categorical(y)\n\n#X'teki verilerimiz zaten say\u0131sal oldu\u011fu i\u00e7in onlara LabelEncoder i\u015flemi uygulamayaca\u011f\u0131z...\n","9e61cf8d":"#Hedef de\u011fi\u015fkenlerimizi kategorik hale getirip getiremedi\u011fimizi kontrol edelim.\n\n#print(y) ile bakabiliriz.\n\n#\u0130ris Setosa = [1. 0. 0.]\n#\u0130ris Versicolor = [0. 1. 0.]\n#\u0130ris Virginica =  [0. 0. 1.]","63a60222":"#Veri setimizi e\u011fitim ve test olarak ay\u0131ral\u0131m.(train_test_split kullanaca\u011f\u0131z)\nfrom sklearn.model_selection import train_test_split\n#Verilerin %20 sini test icin ayirdik. Geriye kalan %80'lik k\u0131sm\u0131da e\u011fitim verimiz olacak. RandomState'i verileri hangi indisten ba\u015flayarak ay\u0131rma i\u015flemi yapacak gibi d\u00fc\u015f\u00fcnebiliriz.E\u011fer bu \u015fekilde kullan\u0131rsan\u0131z sizdeki e\u011fitim ve test verileri ile benim e\u011fitim ve test verilerim ayn\u0131 olacakt\u0131r.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n#Veri setimizi istedi\u011fimiz \u015fekilde ay\u0131rabildik mi bir bakal\u0131m.\nprint(\"X_Train Veri Say\u0131s\u0131:\",X_train.shape[0])\nprint(\"y_Train Veri Say\u0131s\u0131:\",y_train.shape[0])\nprint(\"X_Test Veri Say\u0131s\u0131:\",X_test.shape[0])\nprint(\"y_Test Veri Say\u0131s\u0131:\",y_test.shape[0])\n","44c749c6":"#Normalizasyon\n#Ba\u015far\u0131 oran\u0131n\u0131 artt\u0131rmak i\u00e7in verilerin normalize edilmi\u015f hallerini kullanmam\u0131z gerekiyor. Bu i\u015fleme \"Feature Scaling\" yani \u00d6zellik \u00d6l\u00e7eklendirme denir.\n#Mesela MinMax Scaler kullanarak normalizasyon i\u015flemi ger\u00e7ekle\u015ftirilirken t\u00fcm verilerimizi 0 - 1 aral\u0131\u011f\u0131na \u00e7ekmi\u015f olaca\u011f\u0131z. Veri Setimizideki en k\u00fc\u00e7\u00fck say\u0131sal de\u011fer 0'\u0131 en b\u00fcy\u00fck say\u0131sal de\u011fer ise 1'i temsil edecek. Di\u011fer de\u011ferler ise bu aral\u0131kta yer alacaklar.\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\n#MinMax veya Standart Scaler kullanabiliriz.\n#scaler = MinMaxScaler() \nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n#print(X_train) #Verilerimizin normalize edilmi\u015f hallerine bir g\u00f6z atal\u0131m.\n","5b2d43c9":"#Gerekli k\u00fct\u00fcphaneleri ekleyeyim. \nimport keras\nfrom keras.models import Sequential \nfrom keras.layers import Dense\n\n#Ysa'yi baslatalim\n#Gizli Katman ka\u00e7 tane eklemeliyiz ? \n#Kesin bir kural olmamakla birlikte genel olarak \"(Girdi Say\u0131s\u0131 + \u00c7\u0131kts\u0131 Say\u0131s\u0131) \/ 2\" \u015feklinde kullan\u0131l\u0131yor. Yani gizli katman say\u0131m\u0131z : (4 + 3) \/ 2 = 4 (Yuvarl\u0131yoruz)\nmodel = Sequential()\nmodel.add(Dense(4 ,input_dim=4, activation='relu')) #input_dim giri\u015f katman\u0131m\u0131z\u0131 ifade ediyor. 4 ise gizli katmandaki n\u00f6ron say\u0131s\u0131n\u0131.\nmodel.add(Dense(3, activation = 'softmax')) #\u00c7\u0131kt\u0131 Katman\u0131 Ekleyelim. Hedef de\u011fi\u015fkenimiz 3 farkli de\u011fer alabildi\u011fi, \u00e7\u0131kt\u0131 katmanm\u0131z 3 n\u00f6rondan olu\u015facakt\u0131r.Aktivasyon fonksiyonu olarak softmax kullanal\u0131m.\n","3a4bda14":"#Ysa'nin Derlenmesi\n#optimizer: Stochastic Gradient Descenti g\u00f6steren \"adam\".\n#loss: SGD\u2019nin optimizasyonu i\u00e7in kullan\u0131lacak loss fonk. Tahmin y ile ger\u00e7ek ye de\u011feri aras\u0131n\u0131 hesaplay\u0131p en optimal de\u011feri SGD\u2019ye buldurur\n#Hedef de\u011fi\u015fken say\u0131m\u0131z 2'den fazla oldu\u011fu i\u00e7in categorical_crossentropy kullaniyoruz. E\u011fer 2 s\u0131n\u0131f olsayd\u0131 binary_crossentropy kullanacakt\u0131k.\n\nmodel.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])","95aedadc":"#\u00d6\u011frenilmesi gereken parametre say\u0131s\u0131 nas\u0131l hesaplan\u0131yor ona bakal\u0131m.\n#Toplam \u00d6\u011frenilmesi Gereken Parametre Say\u0131s\u0131 = [Giri\u015fKatman\u0131 x GizliKatman] + [GizliKatman x \u00c7\u0131k\u0131\u015fKatman\u0131] + N\u00f6ron Say\u0131s\u0131 (Giri\u015f Katman\u0131 Hari\u00e7)\n#Toplam \u00d6\u011frenilmesi Gereken Parametre Say\u0131s\u0131 = [4 x 4] + [ 4 x 3] + 4 + 3 = 35\nmodel.summary() #Modelimizin \u00d6zetini G\u00f6sterir","5fa0d668":"#Modelimizi olu\u015fturduk, \u015fimdi e\u011fitim zaman\u0131 !\n#Epouch :E\u011fitim s\u0131ras\u0131nda t\u00fcm e\u011fitim verilerinin a\u011fa ka\u00e7 tur g\u00f6sterilece\u011finin say\u0131s\u0131d\u0131r.\n#Batch Size :Modelin ayn\u0131 anda e\u011fitece\u011fi veri say\u0131s\u0131d\u0131r.\nhistory = model.fit(X_train, y_train, batch_size=6, epochs=100)","524f2a7b":"#Modelimiz e\u011fitim s\u00fcrecini tamamlad\u0131. \u015eimdi ise daha \u00f6nceden ay\u0131rd\u0131\u011f\u0131m\u0131z %20'lik test verisi ile modelimizi test edelim.\nscores = model.evaluate(X_test, y_test)\nprint(\"\\nAccuracy( Do\u011fruluk ): %\",scores[1]*100) ","bf891d9d":"plt.figure()\nplt.title('Model Ba\u015far\u0131s\u0131')\nplt.ylabel('Do\u011fruluk')\nplt.xlabel('Tur Say\u0131s\u0131')\nplt.plot(model.history.history[\"accuracy\"],label=\"E\u011fitim Do\u011fruluk\")\nplt.plot(model.history.history[\"loss\"],label=\"E\u011fitim Loss\")\nplt.legend()\nplt.show()","680adb3c":"#Ba\u015far\u0131y\u0131 yukar\u0131daki \u015fekilde h\u0131zl\u0131ca bulabiliriz ama birde Karma\u015f\u0131kl\u0131k Matrisi(Confusion Matrix) \u00e7izdirmek istersek bu \u015fekilde yapal\u0131m.\n\ny_pred = model.predict(X_test)\ny_test_class = np.argmax(y_test,axis=1) #Daha \u00f6nce hedef de\u011fi\u015fkenlerimizi [0,0,1], [0,1,0], [1,0,0] bu hale getirmi\u015ftik. Karma\u015f\u0131kl\u0131k Matrisinde kullanmak i\u00e7in 0,1,2 haline getirmemiz gerekiyor.\ny_pred_class = np.argmax(y_pred,axis=1) \n\nprint(\"Test Verileri:  \",y_test_class,\"\\nYSA Tahminleri: \",y_pred_class ) #Test verilerimiz ile tahminleri burada kar\u015f\u0131la\u015ft\u0131rabiliriz.","33120251":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test_class, y_pred_class)) # Precision(Kesinlik) , Recall(Hassasiyet), F1-Score(F1-De\u011ferlendirme)\ncm = confusion_matrix(y_test_class, y_pred_class)\nprint(cm)","2d2064df":"#Heatmap'le karma\u015f\u0131kl\u0131k matrisimizi(cm) g\u00f6rsel hale getirelim.\ndf_cm = pd.DataFrame(cm)\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Greens\") #Farkl\u0131 renklerde g\u00f6rmek i\u00e7in cmap'i \"BuPu\",\"Blues\",\"YlGnBu\",\"Greens\" yapabiliriz.","8629f93f":"\n\n![iris%20cicegi%20yaprak%20turleri.png](attachment:iris%20cicegi%20yaprak%20turleri.png)","dbb3316f":"![ysa_iris.png](attachment:ysa_iris.png)","3d440ba6":"\n**Yapay sinir a\u011flar\u0131 ** kullanarak \u0130ris \u00c7ice\u011fi ile bir **s\u0131n\u0131fland\u0131rma** (classification) i\u015flemi ger\u00e7ekle\u015ftirece\u011fiz.\n**4** nitelikten olu\u015fan, **3** kategoriye ayr\u0131lm\u0131\u015f toplamda **150** tane verimiz var.  Bu nitelikler alt yaprak uzunlu\u011fu, alt yaprak geni\u015fli\u011fi, ta\u00e7 yaprak uzunlu\u011fu,ta\u00e7 yaprak geni\u015fli\u011fidir. Biz de bu 4 niteli\u011fi kullanarak iris \u00e7ice\u011finin hangi kategoride oldu\u011fu tahmin etmeye \u00e7al\u0131\u015faca\u011f\u0131z.\n"}}