{"cell_type":{"76929cd0":"code","e6e1a730":"code","2d90be7c":"code","85624a18":"code","081c0d6a":"code","25c959ff":"code","17965b27":"code","5ce52062":"code","e2f9e3a7":"code","de0d723e":"code","d8605ac8":"code","2a8733b6":"code","bfe9d9cc":"code","45c1bd2d":"code","bececec3":"code","e84d6a79":"code","a2ed05d9":"code","c7e8df09":"code","2e58c2f9":"code","7db8770b":"code","b8c81cb8":"code","a58f0800":"code","1ec3d46f":"code","1ddcc266":"code","01dd7939":"code","c75de1b8":"code","89cd9766":"code","c56b9113":"code","a75bcf05":"code","59c378a3":"code","99355faf":"code","e63c00f7":"code","9089069d":"code","580fbb9b":"code","94bb6005":"code","b772acef":"code","b9cd57be":"code","d55aa3cf":"code","e2c523a2":"code","e546f86e":"code","1033f2d7":"code","4a755c88":"code","2c1a473e":"code","a1be4f47":"code","b92fc2db":"code","ce6bbc9d":"code","0d5d7b7e":"code","78bf5ef8":"code","b2fddcf3":"code","5ab2101a":"code","51b01775":"code","e40c0d4c":"code","21f01593":"code","c951a38b":"code","bc7bc865":"markdown","3ec1c3b9":"markdown","f6d01c3a":"markdown","a73d7b29":"markdown","12239a75":"markdown","86988a44":"markdown","8c0e1fca":"markdown","c5a3cc72":"markdown","2fcb92d2":"markdown","d32e50f1":"markdown","e7a07001":"markdown"},"source":{"76929cd0":"import pandas as pd\nimport numpy as np\nimport string\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='whitegrid', palette='muted',\n        rc={'figure.figsize': (15,10)})","e6e1a730":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2d90be7c":"train.head()","85624a18":"test.head()","081c0d6a":"print(train.shape, test.shape)","25c959ff":"pres = {'deKlerk': 0,\n        'Mandela': 1,\n        'Mbeki': 2,\n        'Motlanthe': 3,\n        'Zuma': 4,\n        'Ramaphosa': 5}\n\ntrain.replace({'president': pres}, inplace=True)","17965b27":"# speech number: intro lines\nstarts = {\n    0: 1,\n    1: 1,\n    2: 1,\n    3: 12,\n    4: 12,\n    5: 5,\n    6: 1,\n    7: 1,\n    8: 8,\n    9: 9,\n    10: 12,\n    11: 14,\n    12: 14,\n    13: 15,\n    14: 15,\n    15: 15,\n    16: 15,\n    17: 15,\n    18: 15,\n    19: 15,\n    20: 20,\n    21: 1,\n    22: 15,\n    23: 20,\n    24: 20,\n    25: 15,\n    26: 15,\n    27: 20,\n    28: 20,\n    29: 15,\n    30: 18\n}","5ce52062":"def divide_on(df, char):\n    \n    # iterate over text column of DataFrame, splitting at each occurrence of char\n\n    sentences = []\n    # let's split the data into senteces\n    for i, row in df.iterrows():\n        \n        # skip the intro lines of the speech\n        for sentence in row['text'].split(char)[starts[i]:]:\n            sentences.append([row['president'], sentence])\n\n    df = pd.DataFrame(sentences, columns=['president', 'text'])\n    \n    return df[df['text'] != '']","e2f9e3a7":"train = divide_on(train, '.')","de0d723e":"train.head(5)","d8605ac8":"train.shape","2a8733b6":"train['president'].value_counts()","bfe9d9cc":"# proportion of total\ntrain['president'].value_counts()\/train.shape[0]","45c1bd2d":"train['sentence'] = None\ntest['president'] = None\n\ndf = pd.concat([train, test], axis=0, sort=False)","bececec3":"# reorder columns\ndf = df[['sentence', 'text', 'president']]","e84d6a79":"df.tail()","a2ed05d9":"def fixup(text):\n    \n    # remove punctuation\n    text = ''.join([char for char in text if char == '-' or char not in string.punctuation])\n    # remove special characters\n    text = text.replace(r'^[*-]', '')\n    # remove numbers\n    text = ''.join([char for char in text if not char.isdigit()])\n    # lowercase\n    text = text.lower()\n    \n    # remove hanging whitespace\n    text = \" \".join(text.split())\n    \n    return text\n\n\ndf['text'] = df['text'].apply(fixup)","c7e8df09":"df.head(5)","2e58c2f9":"# get length of sentence as variable\ndf['length'] = df['text'].apply(len)","7db8770b":"# what are our longest sentences?\ndf.sort_values(by='length', ascending=False).head(10)","b8c81cb8":"df.loc[3930][1]","a58f0800":"# what are our shortest sentences?\ndf.sort_values(by='length').head(5)","1ec3d46f":"# let's check the shortest sentences in our test set\ndf[pd.isnull(df['president'])].sort_values(by='length').head()","1ddcc266":"# sentences with just a few characters are of no use to us\ndf = df[df['length']>10]","01dd7939":"# what are our shortest sentences now?\ndf.sort_values(by='length').head(5)","c75de1b8":"df['president'].value_counts()","89cd9766":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer","c56b9113":"tfidf = TfidfVectorizer(strip_accents='unicode', ngram_range=(1,3), stop_words='english', min_df=6)\nX = tfidf.fit_transform(df['text']).todense()\nX.shape","a75bcf05":"tfidf.get_feature_names()","59c378a3":"X = pd.DataFrame(data=X, columns=tfidf.get_feature_names())","99355faf":"df = df.drop(columns=['text', 'length'], axis=1)","e63c00f7":"df.head()","9089069d":"X = pd.DataFrame(np.hstack((df, X)))","580fbb9b":"X.shape","94bb6005":"X.columns = ['sentence_id', 'president_id'] + tfidf.get_feature_names()","b772acef":"X.head()","b9cd57be":"X.shape","d55aa3cf":"train = X[pd.isnull(X['sentence_id'])]\ntest = X[pd.notnull(X['sentence_id'])]","e2c523a2":"train.shape","e546f86e":"X_train = train.drop(['sentence_id', 'president_id'], axis=1)\nX_test = test.drop(['sentence_id', 'president_id'], axis=1)","1033f2d7":"def one_hot_encode(label):\n    \n    # initialize zero array\n    vec = [0, 0, 0, 0, 0, 0]\n    \n    # set index of array corresponding to label = 1\n    vec[label] = 1\n    \n    return vec\n\n# save encoded labels as target for model\ny_train = np.vstack(row for row in train['president_id'].apply(one_hot_encode).values)","4a755c88":"y_train[600]","2c1a473e":"print('Train size:', X_train.shape)\nprint('Test size:', X_test.shape)","a1be4f47":"def create_model(lyrs=[X_train.shape[1], 1028, 512, 256], act='relu', opt='Adam', dr=0.25):\n    \n    model = Sequential()\n    \n    # create first hidden layer\n    model.add(Dense(lyrs[0], input_dim=X_train.shape[1], activation=act))\n    \n    # create additional hidden layers\n    for i in range(1,len(lyrs)):\n        model.add(Dense(lyrs[i], activation=act))\n    \n    # add dropout, default is none\n    model.add(Dropout(dr))\n    \n    # create output layer\n    model.add(Dense(6, activation='softmax'))  # output layer\n    \n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model","b92fc2db":"model = create_model()\nprint(model.summary())","ce6bbc9d":"# train model on full train set, with 80\/20 CV split\ntraining = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\nval_acc = np.mean(training.history['val_acc'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","0d5d7b7e":"# summarize history for accuracy\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","78bf5ef8":"predictions = model.predict(X_test)","b2fddcf3":"pred_lbls = []\nfor pred in predictions:\n    pred = list(pred)\n    max_value = max(pred)\n    max_index = pred.index(max_value)\n    pred_lbls.append(max_index)\n\npredictions = np.array(pred_lbls)","5ab2101a":"predictions.shape","51b01775":"test['president_id'] = predictions","e40c0d4c":"test['president_id'].value_counts()","21f01593":"submission = test[['sentence_id','president_id']]\nsubmission.columns = ['sentence', 'president']\nsubmission.to_csv('rnn_1.csv', index=False)","c951a38b":"submission.president.value_counts()","bc7bc865":"### Split Lines On End Characters\n\nTaking for example the sentence above, there are some lines which contain multiple sentences. The original data was split into rows at every new line character, but we will need to further split the data at every full stop, question mark and exclamation mark.   \n\nEach speach has a different number of introductory lines that will not be useful training data.","3ec1c3b9":"### Sentence Length","f6d01c3a":"# Language model","a73d7b29":"### Encode president names","12239a75":"### Number of sentences per president","86988a44":"There are a few sentences which contain more than 500 characters. Although somewhat long-winded, they are all technically still single sentences.   \n\nLet's take a look at the other end of the spectrum:","8c0e1fca":"### Load speech data","c5a3cc72":"### Combine train and test sets\nWe want to do all of our pre-processing at the same time, so we will need to combine the two data sets before doing any NLP.","2fcb92d2":"### Clean text","d32e50f1":"# Pre-Processing","e7a07001":"After splitting the data into individual sentences, we now have more observations for each president."}}