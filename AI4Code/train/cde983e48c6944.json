{"cell_type":{"8588d314":"code","214b44ae":"code","c13b3228":"code","8b2ffbfc":"code","3b5227e6":"code","448940b8":"code","959c4cee":"code","6ca567d3":"code","99c86846":"code","0bdbfb88":"code","033c45d7":"code","b8a7bf82":"markdown","e63b02e7":"markdown","a40ede83":"markdown","58557f5c":"markdown","3ba5adff":"markdown","569f97e1":"markdown","450f0448":"markdown","9e1c0b37":"markdown"},"source":{"8588d314":"import pickle\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm_notebook\n\nfrom skopt import forest_minimize\nfrom skopt.plots import plot_convergence\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\n\nimport xgboost\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None","214b44ae":"inp_path = '..\/input\/porto-seguro-data-challenge\/'\nmetadata = pd.read_csv(inp_path + 'metadata.csv')\ntest = pd.read_csv(inp_path + 'test.csv')\ntrain = pd.read_csv(inp_path + 'train.csv')\ntrain = train.sample(frac=1, random_state=0).reset_index(drop=True)  # shuffling dataset\nsub = pd.read_csv(inp_path + 'submission_sample.csv')","c13b3228":"def _gen_strat_folds(df, tgt_name, n_splits=5, shuffle=True, random_state=0):\n    \"\"\" Creates Folds Inplace \"\"\"\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n    for fold, (train_index, valid_index) in enumerate(skf.split(df.drop(columns=tgt_name), df[tgt_name])):\n        df.loc[df[df.index.isin(valid_index)].index, 'fold'] = fold\n\n_gen_strat_folds(train, 'y')","8b2ffbfc":"scores = []\nxgb = XGBClassifier(\n        n_jobs=-1,\n        eval_metric='auc',\n        random_state=0,\n    )\n\n# always use a well thought cross validation to take decisions\nfor fold in tqdm_notebook(range(5), desc=\"Cross validation progress\"):\n    train_data = train[train.fold != fold].copy()\n    test_data = train[train.fold == fold].copy()\n    X_train = train_data.drop(columns=['fold', 'y']).values\n    X_test = test_data.drop(columns=['fold', 'y']).values\n    y_train = train_data['y'].values\n    y_test = test_data['y'].values\n\n    fit_params = {\n        'early_stopping_rounds': 100,\n        'eval_metric' : 'auc',\n        'eval_set' : [(X_test, y_test)],\n        'verbose': False,\n    }\n\n    xgb.fit(X_train, y_train, **fit_params)\n    p = xgb.predict_proba(X_test)[:, -1]\n    scores.append(metrics.roc_auc_score(y_test, p))\n    \navg_score = np.mean(scores)\nprint(f\"Baseline auc = {round(avg_score, 4)}\")","3b5227e6":"def train_func(params):\n    \"\"\" training funcion that will be optimised \"\"\"\n    learning_rate = params[0]\n    max_depth = params[1]\n    min_child_weight=params[2]\n    subsample = params[3]\n    colsample_bynode = params[4]\n    num_parallel_tree = params[5]\n    print(f\"Testing parameters: {params}\")\n    \n    scores = []\n    for fold in range(5):\n        train_data = train[train.fold != fold].copy()\n        test_data = train[train.fold == fold].copy()\n        X_train = train_data.drop(columns=['fold', 'y']).values\n        X_test = test_data.drop(columns=['fold', 'y']).values\n        y_train = train_data['y'].values\n        y_test = test_data['y'].values\n\n        \n        fit_params = {\n            'early_stopping_rounds': 100,\n            'eval_metric' : 'auc',\n            'eval_set': [(X_test, y_test)],\n            'verbose': False,\n        }\n\n        xgb = XGBClassifier(\n            n_jobs=-1,\n            eval_metric='auc',\n            random_state=0,\n            n_estimators=100,  # you should tune n_estimators aswell\n            learning_rate=learning_rate,\n            max_depth=max_depth,\n            min_child_weight=min_child_weight,\n            subsample=subsample,\n            colsample_bynode=colsample_bynode,\n            num_parallel_tree=num_parallel_tree\n        )\n        \n        xgb.fit(X_train, y_train, **fit_params)\n        p = xgb.predict_proba(X_test)[:, -1]\n        scores.append(metrics.roc_auc_score(y_test, p))\n    avg_score = np.mean(scores)\n    return -avg_score  # metric that will be minimized, since a bigger auc is better, we negate it\n\nspace = [\n    (1e-3, 9e-1, 'log-uniform'),  # learning_rate\n    (3, 30),  # max_depth\n    (0.01, 20.0, 'log-uniform'),  # min_child_weight\n    (0.2, 1.0),  # subsample\n    (0.2, 1.0),  # colsample_bynode\n    [1, 2, 3],  # num_parallel_tree\n]\n\nres_gp = forest_minimize(train_func, space, base_estimator='RF', random_state=0, verbose=1, n_calls=50, n_random_starts=10)\nxgb_best_params = res_gp.x\nprint(f\"Best found params: {xgb_best_params}\") \nplot_convergence(res_gp);","448940b8":"# setting the best found LR as base\ndef xgb_learning_rate_decay_power_099(boosting_round, num_boost_round):\n    base_learning_rate = xgb_best_params[0]\n    lr = base_learning_rate  * np.power(.99, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_decay_power_0995(boosting_round, num_boost_round):\n    base_learning_rate = xgb_best_params[0]\n    lr = base_learning_rate  * np.power(.995, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_decay_power_095(boosting_round, num_boost_round):\n    base_learning_rate = xgb_best_params[0]\n    lr = base_learning_rate  * np.power(.95, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\n# testing some default values\ndef xgb_learning_rate_010_decay_power_099(boosting_round, num_boost_round):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_010_decay_power_0995(boosting_round, num_boost_round):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_010_decay_power_095(boosting_round, num_boost_round):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.95, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_050_decay_power_099(boosting_round, num_boost_round):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_050_decay_power_0995(boosting_round, num_boost_round):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.995, boosting_round)\n    return lr if lr > 1e-3 else 1e-3\n\ndef xgb_learning_rate_050_decay_power_095(boosting_round, num_boost_round):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.95, boosting_round)\n    return lr if lr > 1e-3 else 1e-3","959c4cee":"# could have been a part of the hyper parameter search, but here I will be tuning it only for the best model \nlearning_rate = xgb_best_params[0]\nmax_depth = xgb_best_params[1]\nmin_child_weight = xgb_best_params[2]\nsubsample = xgb_best_params[3]\ncolsample_bynode = xgb_best_params[4]\nnum_parallel_tree = xgb_best_params[5]\n\nposs_cbs = {\n    0: [xgboost.callback.reset_learning_rate(xgb_learning_rate_decay_power_099)],\n    1: [xgboost.callback.reset_learning_rate(xgb_learning_rate_decay_power_0995)],\n    2: [xgboost.callback.reset_learning_rate(xgb_learning_rate_decay_power_095)],\n    3: [xgboost.callback.reset_learning_rate(xgb_learning_rate_010_decay_power_099)],\n    4: [xgboost.callback.reset_learning_rate(xgb_learning_rate_010_decay_power_0995)],\n    5: [xgboost.callback.reset_learning_rate(xgb_learning_rate_010_decay_power_095)],\n    6: [xgboost.callback.reset_learning_rate(xgb_learning_rate_050_decay_power_099)],\n    7: [xgboost.callback.reset_learning_rate(xgb_learning_rate_050_decay_power_0995)],\n    8: [xgboost.callback.reset_learning_rate(xgb_learning_rate_050_decay_power_095)]\n}\n\n# trying different lr decay callbacks\nbest_cb = None\nbest_score = 0\nfor cb in poss_cbs:\n    scores = []\n    for fold in range(5):\n        train_data = train[train.fold != fold].copy()\n        test_data = train[train.fold == fold].copy()\n        X_train = train_data.drop(columns=['fold', 'y']).values\n        X_test = test_data.drop(columns=['fold', 'y']).values\n        y_train = train_data['y'].values\n        y_test = test_data['y'].values\n\n        fit_params = {\n            'early_stopping_rounds': 1000,\n            'eval_metric' : 'auc',\n            'eval_set': [(X_test, y_test)],\n            'callbacks': poss_cbs[cb],\n            'verbose': False,\n        }\n\n        xgb = XGBClassifier(\n            n_jobs=-1,\n            eval_metric='auc',\n            random_state=0,\n            n_estimators=100,\n            learning_rate=learning_rate,\n            max_depth=max_depth,\n            min_child_weight=min_child_weight,\n            subsample=subsample,\n            colsample_bynode=colsample_bynode,\n            num_parallel_tree=num_parallel_tree\n        )\n\n        xgb.fit(X_train, y_train, **fit_params)\n        p = xgb.predict_proba(X_test)[:, -1]\n        scores.append(metrics.roc_auc_score(y_test, p))\n    avg_score = np.mean(scores)\n    print(f\"Callback: {cb}, auc = {round(avg_score, 4)}\")\n    if avg_score > best_score:\n        best_score = avg_score\n        best_cb = cb","6ca567d3":"cross_val_probas = []\nground_truths = []\nfor fold in range(5):\n    train_data = train[train.fold != fold].copy()\n    test_data = train[train.fold == fold].copy()\n    X_train = train_data.drop(columns=['fold', 'y']).values\n    X_test = test_data.drop(columns=['fold', 'y']).values\n    y_train = train_data['y'].values\n    y_test = test_data['y'].values\n\n    fit_params = {\n        'early_stopping_rounds': 1000,\n        'eval_metric' : 'auc',\n        'eval_set': [(X_test, y_test)],\n        'callbacks': poss_cbs[best_cb],\n        'verbose': False,\n    }\n\n    xgb = XGBClassifier(\n        n_jobs=-1,\n        eval_metric='auc',\n        random_state=0,\n        n_estimators=100,\n        learning_rate=learning_rate,\n        max_depth=max_depth,\n        min_child_weight=min_child_weight,\n        subsample=subsample,\n        colsample_bynode=colsample_bynode,\n        num_parallel_tree=num_parallel_tree\n    )\n\n    xgb.fit(X_train, y_train, **fit_params)\n    p = xgb.predict_proba(X_test)[:, -1]\n    cross_val_probas += p.tolist()\n    ground_truths += y_test.tolist()","99c86846":"best_th = 0\nf1_max = 0\nfor th in np.linspace(0.1, 0.9, num=1000):\n    f1 = metrics.f1_score(np.asarray(ground_truths), (np.asarray(cross_val_probas) >= th).astype(int))\n    if f1_max < f1:\n        f1_max = f1\n        best_th = th\nprint(f\"MAX F1 SCORE: {f1_max}, THRESH HOLD: {best_th}\")","0bdbfb88":"train_data = train.copy()\ntest_data = test.copy()\nX_train = train_data.drop(columns=['fold', 'y']).values\nX_test = test_data.values\ny_train = train_data['y'].values\n\nfit_params = {\n    'eval_metric' : 'auc',\n    'callbacks': poss_cbs[best_cb],\n    'verbose': False,\n}\n\nxgb = XGBClassifier(\n    n_jobs=-1,\n    eval_metric='auc',\n    random_state=0,\n    n_estimators=100,\n    learning_rate=learning_rate,\n    max_depth=max_depth,\n    min_child_weight=min_child_weight,\n    subsample=subsample,\n    colsample_bynode=colsample_bynode,\n    num_parallel_tree=num_parallel_tree\n)\n\nxgb.fit(X_train, y_train, **fit_params)\np = xgb.predict_proba(X_test)[:, -1]\npreds = p >= best_th","033c45d7":"sub['predicted'] = preds\nsub.to_csv('submission.csv', index=False)\nsub.head(10)","b8a7bf82":"### Building Cross Validation (Stratified)","e63b02e7":"### Reading data from Porto Seguro Data Challenge 2021","a40ede83":"## Finding the best threshold","58557f5c":"## Tree Based Optimisation (RF)","3ba5adff":"# Final model and submission","569f97e1":"### XGBoost Classifier Baseline","450f0448":"# XGBoost - Tree Based Optimisation + LR Decay  \n#### In this notebook I won't get in to the details of the dataset(dealing with missing data, preprocessing and so on), since my objective here is to share a hyper parameter tuning technique and to demonstrate on how to apply learning rate decay on XGBoost training.  \n#### If it does help you in any way, please upvote!  \nFeel free to connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/gabriel-tardochi-salles-a1653a193\/).","9e1c0b37":"## Applying learning rate decay"}}