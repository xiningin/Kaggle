{"cell_type":{"5bf7546d":"code","cbcef8d8":"code","45d71214":"code","61c13f4e":"code","cbac316c":"code","fc022b11":"code","b64be414":"code","84358ec4":"code","54f24fe0":"code","a07e09fb":"code","1e657be2":"code","22076814":"code","3a28f147":"code","9b3f36a7":"code","65aeb8ad":"code","19731a7e":"code","cc8e252a":"code","006a2fb0":"code","57ec3396":"code","3571eada":"code","20c47403":"code","309973d0":"code","9554f944":"code","3f17974c":"code","60329da6":"code","f8311175":"code","42916417":"code","65221622":"code","3be741f7":"code","8d1a2908":"code","1d05e3fb":"code","a78621ec":"markdown","ef90bdc5":"markdown","03894709":"markdown","c2dc8350":"markdown","5769aefd":"markdown","08792820":"markdown","889e3c4b":"markdown","6f018007":"markdown","89ff7f3f":"markdown","d25a9ac7":"markdown","6dc5b18e":"markdown","e7be5a94":"markdown","f4f67832":"markdown","ca9ac3df":"markdown","cca70ebe":"markdown","a86dff40":"markdown","de37fda4":"markdown","82c81e3e":"markdown","a871b6f7":"markdown","6435c85b":"markdown","7017414e":"markdown","3357a1cb":"markdown","b3936daf":"markdown","fee4068d":"markdown","9e692b79":"markdown"},"source":{"5bf7546d":"import numpy as np\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\ntqdm.pandas()\npd.set_option(\"display.precision\", 2)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import os for system interaction and garbage collector\nimport os\nimport gc\n\n# import textblog for text processing\nfrom textblob import TextBlob\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n# import gensim for statistical semantical analysis and structure\nimport gensim\n\nfrom sklearn.model_selection import KFold\n\nfrom keras.layers import *\nfrom keras.initializers import *\nfrom keras.constraints import *\nfrom keras.regularizers import *\nfrom keras.activations import *\nfrom keras.optimizers import *\nimport keras.backend as K\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom IPython.display import SVG\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cbcef8d8":"import zipfile\n\n# unzip file to specified path\ndef import_zipped_data(file, output_path):\n    with zipfile.ZipFile(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/\"+file+\".zip\",\"r\") as z:\n        z.extractall(\"\/kaggle\/input\")\n        \ndatasets = ['train.csv', 'test.csv', 'test_labels.csv']\n\nkaggle_home = '\/kaggle\/input'\nfor dataset in datasets:\n    import_zipped_data(dataset, output_path = kaggle_home)","45d71214":"test_df = pd.read_csv('\/kaggle\/input\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/train.csv')","61c13f4e":"train_df.head()","cbac316c":"# col with input text\nTEXT = 'comment_text'","fc022b11":"non_toxic = len(train_df[train_df['toxic'] == 0])\ntoxic = len(train_df[train_df['toxic'] == 1])\nprint(f'There are {non_toxic} non-toxic comments, representing a {round((non_toxic\/len(train_df)*100))}% of the total {len(train_df)} samples collected.')\nprint(f'Only {toxic} - about a {round((toxic\/len(train_df))*100)}% - are toxic comments.')","b64be414":"plt.barh(['non_toxic', 'toxic'], [non_toxic, toxic], color = 'r', alpha = 0.5)\nplt.title('Toxicity distribution')\nplt.show()","84358ec4":"labels = ['obscene', 'threat', 'insult', 'identity_hate']\nclass_cnt = {}\nfor label in labels:\n    # count number of samples per toxicity type\n    class_cnt[label] = len(train_df[train_df[label] == 1])\n    \n# sort dict from bigger to lower key value\nclass_cnt = {k: v for k, v in sorted(class_cnt.items(), key = lambda item: item[1], reverse = True)}","54f24fe0":"plt.bar(*zip(*class_cnt.items()), color = 'r', alpha = 0.5)\nplt.title('Toxicity type distribution')\nplt.show()","a07e09fb":"print(f'The percentage respect to toxic comments of each toxicity subtype are:')\nfor label in labels:\n    print(f'>> {label} comments: {round((class_cnt[label]\/toxic)*100)}%')","1e657be2":"print(f'The percentage respect to toxic comments of each toxicity subtype are:')\nfor label in labels:\n    print(f'>> {label} comments: {round((class_cnt[label]\/len(train_df))*100)}%')","22076814":"labels = ['toxic', 'severe_toxic']\nclass_cnt = {}\nfor label in labels:\n    # count number of samples per toxicity type\n    class_cnt[label] = len(train_df[train_df[label] == 1])\n    \n# sort dict from bigger to lower key value\nclass_cnt = {k: v for k, v in sorted(class_cnt.items(), key = lambda item: item[1], reverse = True)}","3a28f147":"plt.bar(*zip(*class_cnt.items()), color = 'r', alpha = 0.5)\nplt.title('Toxicity level distribution')\nplt.show()","9b3f36a7":"# compute character length of comments\nlengths = train_df[TEXT].apply(len)\nlengths_df = lengths.to_frame()\n# print basic metrics\nlengths.mean(), lengths.std(), lengths.min(), lengths.max()","65aeb8ad":"lengths = train_df[TEXT].apply(len)\nlengths_df = lengths.to_frame()\nsns.boxplot(x=lengths_df, color='r')\nplt.title('Boxplot of characters per sentence')\nplt.show()","19731a7e":"Q1, Q3 = lengths_df.quantile(0.25), lengths_df.quantile(0.75)\nIQR = Q3 - Q1\nIQR","cc8e252a":"legths_df_iqr = lengths_df[lengths_df[TEXT] < int(round(IQR))]\nsns.boxplot(x=legths_df_iqr, color='r')\nplt.title('Boxplot of characters per sentence without IQR Outliers')\nplt.show()","006a2fb0":"lengths = train_df[TEXT].apply(len)\ntrain_df['lengths'] = lengths\nlengths = train_df.loc[train_df['lengths']<1125]['lengths']\nsns.distplot(lengths, color='r')\nplt.title('Number of characters per sentence')\nplt.show()","57ec3396":"words = train_df[TEXT].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain_df['words'] = words\nwords = train_df.loc[train_df['words']<200]['words']\nsns.distplot(words, color='r')\nplt.title('Number of words per sentence')\nplt.show()","3571eada":"avg_word_len = train_df[TEXT].apply(lambda x: 1.0*len(''.join(x.split()))\/(len(x) - len(''.join(x.split())) + 1))\ntrain_df['avg_word_len'] = avg_word_len\navg_word_len = train_df.loc[train_df['avg_word_len']<10]['avg_word_len']\nsns.distplot(avg_word_len, color='b')\nplt.title('Average word length')\nplt.show()","20c47403":"# take a small sample of training dataset to speed up sentiment analysis\ntiny_train_df = train_df.sample(n=10000)","309973d0":"import matplotlib.patches as mpatches\nnon_toxic_0 = tiny_train_df.loc[(tiny_train_df.toxic<0.5) & (tiny_train_df.words<200)]['words']\ntoxic_1 = tiny_train_df.loc[(tiny_train_df.toxic>0.5) & (tiny_train_df.words<200)]['words']\nsns.distplot(non_toxic_0, color='green')\nsns.distplot(toxic_1, color='red')\nred_patch = mpatches.Patch(color='red', label='Toxic')\ngreen_patch = mpatches.Patch(color='green', label='Non-Toxic')\nplt.legend(handles=[red_patch, green_patch])\nplt.title('Toxicity per word number in text')\nplt.show()","9554f944":"# take a small sample of training dataset to speed up sentiment analysis\ntiny_train_df = train_df.sample(n=10000)","3f17974c":"sia = SentimentIntensityAnalyzer()\nnon_toxic_0 = tiny_train_df.loc[tiny_train_df.toxic<0.5][TEXT].apply(lambda x: sia.polarity_scores(x))\ntoxic_1 = tiny_train_df.loc[tiny_train_df.toxic>0.5][TEXT].apply(lambda x: sia.polarity_scores(x))","60329da6":"sns.distplot([polarity['neg'] for polarity in non_toxic_0], color='green')\nsns.distplot([polarity['neg'] for polarity in toxic_1], color='red')\nred_patch = mpatches.Patch(color='red', label='Toxic')\ngreen_patch = mpatches.Patch(color='green', label='Non-Toxic')\nplt.legend(handles=[red_patch, green_patch])\nplt.title('Distribution of negativity in comments')\nplt.show()","f8311175":"sns.distplot([polarity['pos'] for polarity in non_toxic_0], color='green')\nsns.distplot([polarity['pos'] for polarity in toxic_1], color='red')\nred_patch = mpatches.Patch(color='red', label='Toxic')\ngreen_patch = mpatches.Patch(color='green', label='Non-Toxic')\nplt.legend(handles=[red_patch, green_patch])\nplt.title('Distribution of positivity in comments')\nplt.show()","42916417":"sns.distplot([polarity['neu'] for polarity in non_toxic_0], color='green')\nsns.distplot([polarity['neu'] for polarity in toxic_1], color='red')\nred_patch = mpatches.Patch(color='red', label='Toxic')\ngreen_patch = mpatches.Patch(color='green', label='Non-Toxic')\nplt.legend(handles=[red_patch, green_patch])\nplt.title('Distribution of neutrality in comments')\nplt.show()","65221622":"sns.distplot([polarity['compound'] for polarity in non_toxic_0], color='green')\nsns.distplot([polarity['compound'] for polarity in toxic_1], color='red')\nred_patch = mpatches.Patch(color='red', label='Toxic')\ngreen_patch = mpatches.Patch(color='green', label='Non-Toxic')\nplt.legend(handles=[red_patch, green_patch])\nplt.title('Distribution of complexity in comments')\nplt.show()","3be741f7":"from wordcloud import WordCloud\n\ndef class_wordcloud(dataframe, label, max_words):\n    # data preprocessing: concatenate all reviews per class\n    text = \" \".join(x for x in dataframe[dataframe[label]==1].comment_text)\n\n    # create and generate a word cloud image\n    wordcloud = WordCloud(max_words=max_words, background_color=\"white\", collocations=False).generate(text)\n\n    # display the generated image\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f\"Most popular {max_words} words in class {label}\")\n    plt.show()","8d1a2908":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor label in labels:\n    class_wordcloud(train_df, label, 50)","1d05e3fb":"# data preprocessing: concatenate all reviews per class\ntext = \" \".join(x for x in train_df[train_df['toxic']==0].comment_text)\n\n# create and generate a word cloud image\nwordcloud = WordCloud(max_words=50, background_color=\"white\", collocations=False).generate(text)\n\n# display the generated image\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(f\"Most popular 50 words for non-toxic comments\")\nplt.show()","a78621ec":"# 2. Exploratory Analysis <a><\/a>\n## 2.1 Class Distribution\nLet's visualize the number of toxic and non-toxic comments contained in the dataset:","ef90bdc5":"Interestingly, there is an greater proportion of toxic comments that are shorter, which suggests a possible correlation between these two metrics.","03894709":"The toxic comments distribution has a higher mean than the non-toxic, which shows that such comments generally tend to be more negative on average. ","c2dc8350":"Words per sentence follows a distribution very similar to that of the number of characters.","5769aefd":"### Sentiment - compoundness \/ complexity of comment)","08792820":"The non-toxic comments distribution clearly has a higher mean than the toxic distribution and higher neutrality on average. This may be due to toxic comments expressing more extreme emotions - like hate or anger.","889e3c4b":"Use IQR to filter out the outliers:","6f018007":"### Number of words per sentence","89ff7f3f":"## 2.3 Word Cloud","d25a9ac7":"# Table of Contents\n1. [Introduction](#1)\n2. [Exploratory Analysis](#2)\n - 2.1 [Class Distribution](#3)\n - 2.2 [Token Size Distribution](#4)\n - 2.3 [Sentiment Analysis](#5)\n - 2.4 [Word Cloud](#6)","6dc5b18e":"It looks like obscene and insult toxicity types are almost on pair. On the other hand, identity hate appears in a much lower percentage, and threat comments comprise only 3% of the toxic comments and less than a 1% of the total dataset. This fact may lead to poor classification performance in regards to these classes. On the model evaluation stage, we'll pay attention to the ratios of False Negatives and False Positives for the minority subgroups.\n\nLastly, let's take a look at the distribution of comments tagged as severly toxic:","e7be5a94":"# 1. Introduction <a><\/a>\nIn order to understand the training data and address any possible shortcomings that may affect the models' performance beforehand, a detailed exploratory analysis has been performed. A basic EDA for text classification contains:\n\n* **Data types and format**\n* **Basic statistics** - average number of words per sequence, median, max and min\n* **Size of the dataset and number of classes**\n* **Number of samples per class** - useful to check potential class imbalance\n* **Number of words per sample** - helps understand if context capabilities may be required i.e. for long sentences meaning may fade or become more complex with increased size\n* **Distribution of words per class** - helps unveil which words\/tokens are informative to class membership\n\nOther more complex visualizations may help us improve our understanding of the data, such as:\n* **Sentiment analysis** - visualization of positivity\/negativity\/complexity\/etc. of the text data per class\n* **Word cloud** - visualization of top N most common tokens per class\n\nFirst, import required libraries and datasets:","f4f67832":"## 2.2 Token Size Distribution\nDepending on the format of the input sequence tokens, different models may be better suitted for the classification tasks. Google recommends computing the word\/sequence ration in order to define the complexity of the model, as detailed [here](https:\/\/developers.google.com\/machine-learning\/guides\/text-classification\/step-2-5). \n\nFirst, we'll obtain the number of characters per input sequence.\n\n### Number of characters per sentence","ca9ac3df":"There is a clear **class inbalance**, as only about 10% of the comments are toxic. It is reasonable to assume that, given homogeneous sampling, there will always be a bigger proportion non-toxic comments.\n\nNow we'll explore the toxicity subtypes. Are they imbalanced as well?","cca70ebe":"## 2.2 Sentiment Analysis","a86dff40":"Average word length follows a simple bell-shaped normal distribution with a mean of around 4.5\n### Word size per label (toxic\/non-toxic)","de37fda4":"Interentingly, the distribution of complexity in toxic comments is skewed towards the left. This matches with the idea that toxic comments are less gramatically complex, with many of the consisting in short, aggregsive threats or insults.\n\nOn the other hand, non-toxic comments may be written with a more constructive tone, often trying to express an opinion or make a point, thus using more complex structures.","82c81e3e":"### Average Word Length","a871b6f7":"In contrast, the 50 most popular words for comments that are not labeled as toxic are:","6435c85b":"Both distributions are very similar. This shows that there is no significant difference in terms of positivity levels between toxic and non-toxic comments.","7017414e":"They comprise less that 10% of all toxic comments. Since severely toxic comments are sometimes also labeled as toxic, the discrimination may be more difficult.","3357a1cb":"### Sentiment - negativity","b3936daf":"### Sentiment - neutrality","fee4068d":"### Sentiment - positivity","9e692b79":"Most input texts are below the 400 character mark."}}