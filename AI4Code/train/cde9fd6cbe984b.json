{"cell_type":{"d29eb69c":"code","cc892a76":"code","580fe6d6":"code","26473406":"code","eaa2ff26":"code","bfccc501":"code","ffe897d9":"code","9b8524a7":"code","e72b6412":"code","a667c771":"code","77a8af28":"code","241fa97e":"code","765215ef":"markdown","317b2265":"markdown","ada9cbec":"markdown"},"source":{"d29eb69c":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","cc892a76":"# Input data\n\nx_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]","580fe6d6":"w = torch.tensor([1.0], requires_grad=True)","26473406":"def forward(x):\n    return x*w","eaa2ff26":"def loss(y_pred,y):\n    return (y_pred-y)**2","bfccc501":"# Training loop\n\nprint('Predict (before training)', 4, forward(4).item())\n\n# Training loop\n\nfor epoch in range(10):\n#     l_sum=0\n    for x_val, y_val in zip(x_data, y_data):\n        y_pred = forward(x_val) # Forward pass\n        l = loss(y_pred,y_val) # Loss\n        l.backward() # Backpropagation\n        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n        w.data = w.data - 0.01 * w.grad.item()\n\n        # Manually zero the gradients after updating weights\n        w.grad.data.zero_()\n        \n    print(f\"Epoch: {epoch} | Loss: {l.item()} | w: {w.item()}\")\n#     w_list.append(w)\n#     mse_list.append(l_sum\/3)\n    \n    \nprint('Predict (After training)', '4 hours', forward(4).item())    ","ffe897d9":"x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\ny_data = [1.0, 6.0, 15.0, 28,45]","9b8524a7":"plt.plot(x_data,y_data)\nplt.show()","e72b6412":"# Initialize w2 and w1 with randon values\n\nw_1 = torch.tensor([1.0], requires_grad=True)\nw_2 = torch.tensor([1.0], requires_grad=True)","a667c771":"# Quadratic forward pass based on the function above. Taking b as zero for now\n\ndef quad_forward(x):\n    return w_1*(x**2)+w_2*x","77a8af28":"# Loss fucntion as per the defination above\n\ndef loss(y_pred,y):\n    return (y_pred-y)**2","241fa97e":"# Training loop\n\nprint('Predict (before training)', 6, quad_forward(6).item())\n\nfor epoch in range(100):\n    for x_val, y_val in zip(x_data, y_data):\n        y_pred = quad_forward(x_val)\n        l = loss(y_pred, y_val)\n        l.backward()\n        print(\"\\tgrad: \", x_val, y_val, w_1.grad.item(), w_1.grad.item())\n        w_1.data = w_1.data - 0.0012*w_1.grad.item()\n        w_2.data = w_2.data - 0.0012*w_2.grad.item()\n        \n        # Manually zero the gradients after updating weights\n        w_1.grad.data.zero_()\n        w_2.grad.data.zero_()\n        \n    print(f\"Epoch: {epoch} | Loss: {l.item()} | w1: {w_1.item()} | w2: {w_2.item()}\")\n\n\nprint('Predict (After training)', 6, quad_forward(6).item())","765215ef":"### Session 04:\n\n**Backpropagation**: Algorithm to caculate gradient  for all the weights in the network with several weights. \n\n* It uses the `Chain Rule` to calcuate the gradient for multiple nodes at the same time. \n* In pytorch this is implemented using a `variable` data type and `loss.backward()` method to get the gradients","317b2265":"#### Using backward propagation for quadratic model\n\n$\\hat{y} = x^2*w_{2} + x*w_{1} + b$\n\n$loss = (\\hat{y}-y)^2$\n\n* Using Dummy values of x and y\n\n`x = 1,2,3,4,5`\n`y = 1,6,15,28,45`","ada9cbec":"#### Using .backward method for linear regression\n\n$y = x * w$\n\n$loss =(\\hat{y}-y)^2$"}}