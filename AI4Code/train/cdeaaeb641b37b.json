{"cell_type":{"05a805e6":"code","ce65d11c":"code","b71aefd0":"code","29e71e51":"code","f44075be":"code","7a290071":"code","4c611915":"code","e1356500":"code","9c29c7d4":"code","5f999d13":"code","b2a0575f":"code","2013014f":"code","e70274f8":"code","6424e6e2":"code","7fc6918c":"code","53578cb1":"code","2f093adc":"code","8b88cbea":"code","026f724c":"code","95975011":"code","f341e2a5":"code","39f95ce5":"code","3d3ad613":"markdown","ed499e37":"markdown","20d9dea2":"markdown","3bcdb13f":"markdown","cc366182":"markdown","37517517":"markdown","e652de97":"markdown","acd7bb37":"markdown","461b7edf":"markdown"},"source":{"05a805e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce65d11c":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","b71aefd0":"df.head()","29e71e51":"df.info()","f44075be":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr())","7a290071":"df.corr()['quality'].sort_values(ascending=False)","4c611915":"df.quality.unique() #gives the actual number of labels","e1356500":"sns.countplot(df.quality) #to see how the output labels are distributed","9c29c7d4":"df.hist(figsize=(20,15),bins=50)","5f999d13":"import numpy as np\ndf['chlorides'] = df['chlorides'].apply(lambda x : np.log(x))\ndf['free sulfur dioxide'] = df['free sulfur dioxide'].apply(lambda x : np.log(x))\ndf['total sulfur dioxide'] = df['total sulfur dioxide'].apply(lambda x : np.log(x))\ndf['sulphates'] = df['sulphates'].apply(lambda x : np.log(x))","b2a0575f":"df.hist(bins=50,figsize=(20,15))","2013014f":"df.quality.value_counts()","e70274f8":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy = {5: 5000, 6: 5000, 7: 5000, 4: 5000, 8: 5000, 3: 5000})\nX_test_os,y_test_os = sm.fit_resample(df.drop(['quality','residual sugar'],axis=1),df['quality'])","6424e6e2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_test_os,y_test_os, test_size=0.3, random_state=42)","7fc6918c":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1,leaf_size=10,p=1,metric='manhattan')\nknn.fit(X_train,y_train)","53578cb1":"pred = knn.predict(X_test)","2f093adc":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(pred,y_test))\nprint(classification_report(pred,y_test))","8b88cbea":"from sklearn.model_selection import GridSearchCV","026f724c":"params = [{'weights' : ['uniform', 'distance'],'leaf_size' :[10,20,30],'n_neighbors':[1,10,20,30],\n           'p':[1,2,3]}]\nknn_ = KNeighborsClassifier()\ngrid_search = GridSearchCV(knn_,params,cv=3,n_jobs=100,scoring='f1')\ngrid_search.fit(X_train,y_train)","95975011":"grid_search.best_params_","f341e2a5":"grid_search.best_estimator_","39f95ce5":"knn.effective_metric_","3d3ad613":"Let us apply log transformation on features that are more concentrated to the left and make them uniform<br>\ninstead standard scaling can also be used","ed499e37":"Oversample the quality levels 3,4,7,8 along with the majority class to a sufficiently large value(5000)","20d9dea2":"Looks like the dataset has no null values in any of the features, labels.<br>\nTherefore imputation is not necessary.<br>\nLet us now find out the correlations between the features and labels.<br>","3bcdb13f":"Let us do some EDA on the dataset","cc366182":"The above plot shows that the output lables are not uniformly distributed.<br>\nHence we have to perform some sampling on the input , which we will get to after we see how the data<br>\nis distributed","37517517":"The above correlation helps us understand the parameters that increase the quality(parameters>0) and<br>\nthe parameters that decrease the quality(parameters<0)<br>\nfor residual sugar correlation is almost zero and we can drop it","e652de97":"Before plotting the distribution plots, let us find out the nature of the label(quality)","acd7bb37":"Now lets do some machine learning with knn<br>\nActual knn parameters found after hyper parameter tuning for best accuracy,scroll below to find the same.","461b7edf":"since there are a lot of input quantities let us print the corr values"}}