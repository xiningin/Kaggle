{"cell_type":{"86bbd33c":"code","3f75a3b1":"code","6238e7d4":"code","b661ef0e":"code","feba1531":"code","97a848bf":"code","392a69fe":"code","da299fa7":"code","72953ef7":"code","2115f8f5":"code","55418105":"code","d99021c3":"code","c040e955":"code","9bd3e697":"code","f6cb6fbd":"code","23b618c7":"code","52b6e1ac":"code","cca19e34":"code","e0dbadc9":"code","2f4bcde6":"code","b3ca13f0":"code","aa962ff1":"code","d71b2303":"code","359d93ad":"code","99e30b04":"code","f7b462fa":"code","fa1f4f27":"code","6d13bb2e":"code","f61a4d5c":"code","d50b856e":"code","a535351d":"code","ebfaa64f":"markdown","eb12ddf8":"markdown","27a16afd":"markdown","66d7b6a9":"markdown","44d14983":"markdown","313f60f1":"markdown","ec01ca80":"markdown","170cf761":"markdown"},"source":{"86bbd33c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\n","3f75a3b1":"tf.config.list_physical_devices('GPU')","6238e7d4":"from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples = [150, 120, 90, 100], \n                  n_features = 2, \n                  centers = [(6, 40), (4, 50), (6, 55), (4, 65)], \n                  cluster_std = 3.3, \n                  random_state = 1123)\n\n\nprint(f'The shape of X is: {X.shape}')\nprint(f'The shape of y is: {y.shape}')\n\n","b661ef0e":"cdict = {0: 'blue', 1: 'red', 2: 'green', 3: 'black'}\nplt.figure(figsize = (6, 6))\n\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], c = cdict[i], label = i, alpha = 0.4, marker = '*')\n\n\nplt.legend() ","feba1531":"y[np.where(y == 2)] = 1\ny[np.where(y == 3)] = 0\ny","97a848bf":"cdict = {0:'red', 1:'blue'}\nplt.figure(figsize = (6, 6))\nfor i in np.unique(y):\n    indices = np.where(y == i)\n    plt.scatter(x = X[indices, 0], y = X[indices, 1], marker = '*', alpha = .4, label = i)\n\nplt.legend()","392a69fe":"from tensorflow import keras\nfrom keras.layers import Dense\nfrom keras import Sequential","da299fa7":"n_features = X.shape[1]\nn_features","72953ef7":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(X)\n\n","2115f8f5":"# Deep Network Function\ndef deep_network(X, y, activation_function = 'sigmoid', opt = keras.optimizers.SGD(learning_rate = 0.3), epochs = 250, batch_size = 32, mean_weight_init = 0, sd_weight_init = 0.01):\n    model = Sequential()\n    weight_initializer = tf.keras.initializers.RandomNormal(mean = mean_weight_init, stddev = sd_weight_init)\n    model.add(Dense(4, activation = activation_function, input_dim = n_features, kernel_initializer = weight_initializer))\n    model.add(Dense(4, activation = activation_function, kernel_initializer = weight_initializer))\n    model.add(Dense(4, activation = activation_function, kernel_initializer = weight_initializer))\n    model.add(Dense(4, activation = activation_function, kernel_initializer = weight_initializer))\n    model.add(Dense(1, activation = 'sigmoid', kernel_initializer = weight_initializer))\n    \n    \n    # config_model\n    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    #Train The model\n    history = model.fit(X, y, epochs = epochs, batch_size = batch_size, validation_split = 0.2, verbose = 0)\n    return(history)","55418105":"model1 = deep_network(scaled_features, y, opt = keras.optimizers.SGD(learning_rate = 0.3), activation_function = 'sigmoid', epochs = 1000, batch_size = 16)\n","d99021c3":"# Loss vs Epochs for Model 1:\nplt.figure(figsize = (8, 6))\nplt.plot(model1.history['loss'],label = 'train')\nplt.plot(model1.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model 1', size = 25)\nplt.grid()\n","c040e955":"# Accuracy - Epochs for Model 1:\nplt.figure(figsize = (8, 6))\nplt.plot(model1.history['accuracy'], label = 'Train')\nplt.plot(model1.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 1', size = 25)\nplt.grid()\n","9bd3e697":"model2 = deep_network(scaled_features, y, opt = keras.optimizers.SGD(learning_rate = 0.3), activation_function = 'relu', epochs = 1000, batch_size = 16)","f6cb6fbd":"# Loss vs Epochs for Model 2:\nplt.figure(figsize = (8, 6))\nplt.plot(model2.history['loss'],label = 'train')\nplt.plot(model2.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model2', size = 25)\nplt.grid()\n","23b618c7":"# Accuracy - Epochs for Model 2:\nplt.figure(figsize = (8, 6))\nplt.plot(model2.history['accuracy'], label = 'Train')\nplt.plot(model2.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 2', size = 25)\nplt.grid()\n","52b6e1ac":"model3 = deep_network(scaled_features, y, opt = keras.optimizers.SGD(learning_rate = 0.1), activation_function = 'relu', epochs = 1000, batch_size = 16)","cca19e34":"# Loss vs Epochs for Model 3:\nplt.figure(figsize = (8, 6))\nplt.plot(model3.history['loss'],label = 'train')\nplt.plot(model3.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model3', size = 25)\nplt.grid()\n","e0dbadc9":"# Accuracy - Epochs for Model 3:\nplt.figure(figsize = (8, 6))\nplt.plot(model3.history['accuracy'], label = 'Train')\nplt.plot(model3.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 3', size = 25)\nplt.grid()\n","2f4bcde6":"# Xavier initialization: sigma_2 = 1 \/ N (N = the number of input neurons to a particular layer) - for Sigmoid and tanh\n# He initialization: sigma_2 = 2 \/ N (N = the number of input neurons to a particular layer) - for ReLU\nmodel4 = deep_network(scaled_features, y, opt = keras.optimizers.SGD(learning_rate = 0.1), activation_function = 'relu', epochs = 100, batch_size = 16, mean_weight_init = 0, sd_weight_init = np.sqrt(2 \/ 4))","b3ca13f0":"# Loss vs Epochs for Model 4:\nplt.figure(figsize = (8, 6))\nplt.plot(model4.history['loss'],label = 'train')\nplt.plot(model4.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model4', size = 25)\nplt.grid()\n","aa962ff1":"# Accuracy - Epochs for Model 4:\nplt.figure(figsize = (8, 6))\nplt.plot(model4.history['accuracy'], label = 'Train')\nplt.plot(model4.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 4', size = 25)\nplt.grid()\n","d71b2303":"# Xavier initialization: sigma_2 = 1 \/ N (N = the number of input neurons to a particular layer) - for Sigmoid and tanh\n# He initialization: sigma_2 = 2 \/ N (N = the number of input neurons to a particular layer) - for ReLU\nmodel5 = deep_network(scaled_features, y, opt = keras.optimizers.SGD(learning_rate = 0.1), activation_function = 'tanh', epochs = 600, batch_size = 16, mean_weight_init = 0, sd_weight_init = np.sqrt(1 \/ 4))","359d93ad":"# Loss vs Epochs for Model 5:\nplt.figure(figsize = (8, 6))\nplt.plot(model5.history['loss'],label = 'train')\nplt.plot(model5.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model5', size = 25)\nplt.grid()\n","99e30b04":"# Accuracy - Epochs for Model 5:\nplt.figure(figsize = (8, 6))\nplt.plot(model5.history['accuracy'], label = 'Train')\nplt.plot(model5.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 5', size = 25)\nplt.grid()\n","f7b462fa":"# Xavier initialization: sigma_2 = 1 \/ N (N = the number of input neurons to a particular layer) - for Sigmoid and tanh\n# He initialization: sigma_2 = 2 \/ N (N = the number of input neurons to a particular layer) - for ReLU\nmodel6 = deep_network(scaled_features, y, opt = 'adam', activation_function = 'relu', epochs = 150, batch_size = 16, mean_weight_init = 0, sd_weight_init = np.sqrt(2 \/ 4))","fa1f4f27":"# Loss vs Epochs for Model 6:\nplt.figure(figsize = (8, 6))\nplt.plot(model6.history['loss'],label = 'train')\nplt.plot(model6.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model6', size = 25)\nplt.grid()\n","6d13bb2e":"# Accuracy - Epochs for Model 6:\nplt.figure(figsize = (8, 6))\nplt.plot(model6.history['accuracy'], label = 'Train')\nplt.plot(model6.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 6', size = 25)\nplt.grid()\n","f61a4d5c":"# Xavier initialization: sigma_2 = 1 \/ N (N = the number of input neurons to a particular layer) - for Sigmoid and tanh\n# He initialization: sigma_2 = 2 \/ N (N = the number of input neurons to a particular layer) - for ReLU\nmodel7 = deep_network(X, y, opt = 'adam', activation_function = 'relu', epochs = 150, batch_size = 16, mean_weight_init = 0, sd_weight_init = np.sqrt(2 \/ 4))","d50b856e":"# Loss vs Epochs for Model 6:\nplt.figure(figsize = (8, 6))\nplt.plot(model7.history['loss'],label = 'train')\nplt.plot(model7.history['val_loss'], alpha = 0.7, label = 'validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss vs Epochs for Model7', size = 25)\nplt.grid()\n","a535351d":"# Accuracy - Epochs for Model 6:\nplt.figure(figsize = (8, 6))\nplt.plot(model7.history['accuracy'], label = 'Train')\nplt.plot(model7.history['val_accuracy'], label = 'Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy - Epochs for Model 7', size = 25)\nplt.grid()\n","ebfaa64f":"# Model6: \n- **Activation_function Hidden layers : <span style=\"color:red\">relu<\/span>.**\n- **Optimization: <span style=\"color:red\">Adam<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">150<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**\n- **Use Weight Initializtion**","eb12ddf8":"# Model1: \n- **Activation_function Hidden layers : <span style=\"color:red\">sigmoid<\/span>.**\n- **Optimization : <span style=\"color:red\">SGD<\/span>**\n- **Learning_Rate : <span style=\"color:red\">0.3<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">1000<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**\n","27a16afd":"# Model3: \n- **Activation_function Hidden layers : <span style=\"color:red\">relu<\/span>.**\n- **Optimization: <span style=\"color:red\">SGD<\/span>**\n- **Learning_Rate : <span style=\"color:red\">0.1<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">1000<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**","66d7b6a9":"# Model7: \n- **Activation_function Hidden layers : <span style=\"color:red\">relu<\/span>.**\n- **Optimization: <span style=\"color:red\">Adam<\/span>**\n- **Features : <span style=\"color:red\">Unscaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">150<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**\n- **Use Weight Initializtion**","44d14983":"# Model5: \n- **Activation_function Hidden layers : <span style=\"color:red\">tanh<\/span>.**\n- **Optimization: <span style=\"color:red\">SGD<\/span>**\n- **Learning_Rate : <span style=\"color:red\">0.1<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">600<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**\n- **Use Weight Initializtion**","313f60f1":"### In the part one we consider a shallow Network, But here we want to use the deep Network","ec01ca80":"# Model4: \n- **Activation_function Hidden layers : <span style=\"color:red\">relu<\/span>.**\n- **Optimization: <span style=\"color:red\">SGD<\/span>**\n- **Learning_Rate : <span style=\"color:red\">0.1<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">100<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**\n- **Use Weight Initializtion**","170cf761":"# Model2: \n- **Activation_function Hidden layers : <span style=\"color:red\">relu<\/span>.**\n- **Optimization: <span style=\"color:red\">SGD<\/span>**\n- **Learning_Rate : <span style=\"color:red\">0.3<\/span>**\n- **Features : <span style=\"color:red\">Scaled Features<\/span>**\n- **Epochs : <span style=\"color:red\">1000<\/span>**\n- **Batch_size : <span style=\"color:red\">16<\/span>**"}}