{"cell_type":{"d361e3b2":"code","c20d6a4a":"code","a371cd5e":"code","d0bf432d":"code","ef1c11be":"code","c6abf45b":"code","362cae9a":"code","5b06517b":"code","21e6afdf":"code","3f4e59da":"code","228c9a3a":"code","44fac68d":"code","a5e03b87":"code","f33bf7ad":"code","5fbf2cc5":"code","f785bc67":"code","2f46df24":"code","491025c8":"code","2db25fd8":"code","cc1874bb":"code","4b33d081":"code","de5326bc":"code","730f4689":"code","1299d25d":"code","d8f9eed9":"code","bf40b3a2":"code","8b72c4ad":"code","41f6835d":"markdown","292f4a56":"markdown","48f9af3e":"markdown","8cea4dfa":"markdown","28c273a3":"markdown","3a4ac999":"markdown","1629b8c2":"markdown","95d15be0":"markdown","8db1b025":"markdown","8f75948d":"markdown","236e73a3":"markdown","3620e312":"markdown","caead7c8":"markdown","e670468c":"markdown","400cd786":"markdown","469b3dae":"markdown","d816b59a":"markdown","5c5ae977":"markdown"},"source":{"d361e3b2":"import numpy as np\nimport pandas as pd\nimport re \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset\n\n","c20d6a4a":"data = pd.concat([pd.read_csv('..\/input\/tweets-with-sarcasm-and-irony\/train.csv'),\n                  pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/test.csv\")],axis=0)\n","a371cd5e":"data.info()","d0bf432d":"data.head()","ef1c11be":"np.unique(list(data[\"class\"]))","c6abf45b":"data.dropna(inplace=True)\ndata.info()","362cae9a":"class Vectorizer():\n    def __init__(self,clean_pattern=None,max_features=None,stop_words=None):\n        self.clean_pattern = clean_pattern\n        self.max_features = max_features\n        self.stopwords = stop_words\n        self.tfidf = TfidfVectorizer(stop_words=self.stopwords,max_features=self.max_features)\n        self.builded = False\n        \n    \n    def _clean_texts(self,texts):\n        \n        cleaned = []\n        for text in texts:\n            if self.clean_pattern is not None:\n                text = re.sub(self.clean_pattern,\" \",text)\n            \n            text = text.lower().strip()\n            cleaned.append(text)\n        \n        return cleaned\n    \n    \n    def _set_tfidf(self,cleaned_texts):\n        self.tfidf.fit(cleaned_texts)\n    \n    def build_vectorizer(self,texts):\n        cleaned_texts = self._clean_texts(texts)\n        self._set_tfidf(cleaned_texts)\n        self.builded = True\n        \n    def vectorizeTexts(self,texts):\n        if self.builded:\n            cleaned_texts = self._clean_texts(texts)\n            return self.tfidf.transform(cleaned_texts)\n        \n        else:\n            raise Exception(\"Vectorizer is not builded.\")\n            \n            ","5b06517b":"x = list(data[\"tweets\"])\ny = list(data[\"class\"])","21e6afdf":"vectorizer = Vectorizer(\"[^a-zA-Z0-9]\",max_features=7000,stop_words=\"english\");","3f4e59da":"vectorizer.build_vectorizer(x)","228c9a3a":"vectorized_x = vectorizer.vectorizeTexts(x).toarray()\n","44fac68d":"vectorized_x.shape","a5e03b87":"label_map = {\n    \"figurative\":0,\n    \"sarcasm\":1,\n    \"irony\":2,\n    \"regular\":3\n}\n","f33bf7ad":"y_encoded = []\nfor y_sample in y:\n    y_encoded.append(label_map[y_sample])\n    \ny_encoded = np.asarray(y_encoded)","5fbf2cc5":"y_encoded.shape","f785bc67":"class TweetDataset(Dataset):\n    \n    def __init__(self,x_vectorized,y_encoded):\n        self.x_vectorized = x_vectorized\n        self.y_encoded = y_encoded\n        \n    \n    def __len__(self):\n        return len(self.x_vectorized)\n    \n    \n    def __getitem__(self,index):\n        return self.x_vectorized[index],self.y_encoded[index]\n    \n    ","2f46df24":"dataset = TweetDataset(vectorized_x,y_encoded)\nprint(\"Length of our dataset is\",len(dataset))\n\nprint(dataset[2])","491025c8":"# We've splitted our indices as train and test to use them in subset samplers.\ntrain_indices,test_indices = train_test_split(list(range(0,len(dataset))),test_size=0.25,random_state=42)","2db25fd8":"print(len(train_indices))\nprint(len(test_indices))","cc1874bb":"train_sampler = SubsetRandomSampler(train_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n","4b33d081":"BATCH_SIZE = 128\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n                                                sampler=test_sampler)","de5326bc":"class DenseNetwork(nn.Module):\n    \n    def __init__(self):\n        super(DenseNetwork,self).__init__()\n        self.fc1 = nn.Linear(7000,1024)\n        self.drop1 = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(1024,256)\n        self.drop2 = nn.Dropout(0.4)\n        self.prediction = nn.Linear(256,4)\n        \n    def forward(self,x):\n        \n        x = F.relu(self.fc1(x.to(torch.float)))\n        x = self.drop1(x)\n        x = F.relu(self.fc2(x))\n        x = self.drop2(x)\n        x = F.log_softmax(self.prediction(x),dim=1)\n        \n        return x","730f4689":"device = torch.device(\"cuda\")\ndevice","1299d25d":"model = DenseNetwork().to(device)\n","d8f9eed9":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(model.parameters(),lr=1e-3)\n","bf40b3a2":"EPOCHS = 6\nTRAIN_LOSSES = []\nTRAIN_ACCURACIES = []\n\nfor epoch in range(1,EPOCHS+1):\n    epoch_loss = 0.0\n    epoch_true = 0\n    epoch_total = 0\n    for data_,target_ in train_loader:\n        data_ = data_.to(device)\n        target_ = target_.to(device)\n        \n        # Cleaning optimizer cache.\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(data_)\n        \n        # Computing loss & backward propagation\n        loss = criterion(outputs,target_)\n        loss.backward()\n        \n        # Applying gradients\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        _,pred = torch.max(outputs,dim=1)\n        epoch_true = epoch_true + torch.sum(pred == target_).item()\n        \n        epoch_total += target_.size(0)\n        \n    TRAIN_LOSSES.append(epoch_loss)\n    TRAIN_ACCURACIES.append(100 * epoch_true \/ epoch_total)\n    \n    print(f\"Epoch {epoch}\/{EPOCHS} finished: train_loss = {epoch_loss}, train_accuracy = {TRAIN_ACCURACIES[epoch-1]}\")\n    \n        \n        ","8b72c4ad":"test_true = 0\ntest_total = len(test_sampler)\ntest_loss = 0.0\nwith torch.no_grad():\n    for data_,target_ in validation_loader:\n        data_,target_ = data_.to(device),target_.to(device)\n        \n        outputs = model(data_)\n        \n        loss = criterion(outputs,target_).item()\n        \n        _,pred = torch.max(outputs,dim=1)\n        \n        test_true += torch.sum(pred==target_).item()\n        test_loss += loss\n        \n\nprint(f\"Validation finished: Accuracy = {round(100 * test_true \/ test_total,2)}%, Loss = {test_loss}\")","41f6835d":"* And let's create an object from this class and make our dataset cleaned and vectorized","292f4a56":"* And our small and lovely neural network is ready, before creating a model object let's define our device (gpu)","48f9af3e":"# Introduction\nHello community, welcome to this kernel. In this kernel we're going to discover how to classify texts using TD-IDF vectorization and fully connected Pytorch models.\n\n**We'll use Torch based solutions and play the game by its rules :)**\n\nIn this kernel I did not wanna use RNNs and Word Embeddings because they're way harder so we'll see them in the next kernel.\n\nSo let's start!","8cea4dfa":"# Conclusion\n\nHey! We've finished this kernel and discovered how to use Pytorch and TF-IDF together. Validation accuracy might seems bad, but it's because of our data processing. If we would process it better it'd be better.\n\nIf you have a question about this, please ask me in the comment section of this kernel and also mention me because I generally can't see them if you don't mention me.\n\nHave a good day\/night and if you liked this kernel, please upvote to support and motivate me :)","28c273a3":"* You know, if test set has labels it's pretty useless to split them before making it ready to use. So I concatenated them.","3a4ac999":"# Step 2: Building Neural Network Architecture\n\nIn this section we're gonna create a custom network class which will be inherited from nn.Module and after creating our simple neural network we'll create loss function and optimizer.","1629b8c2":"* It seems like we have nan values, let's remove them.","95d15be0":"# Step 3: Training The Neural Network\nOur model and dataset is ready, so in this section we're gonna train our model.","8db1b025":"# Step 4: Testing Model\nWe've trained our model and it's time to test our model using our test set.","8f75948d":"* It's really easy to implement a custom dataset, let's create an object and test it.","236e73a3":"* And now it's time for creating the model.","3620e312":"* And we're ready to create our custom Dataset object by inheriting it.","caead7c8":"* Now it's okay. Let's write a class which will clean and vectorize our texts.","e670468c":"* And now we'll declare our criterion (loss) and optimizer.","400cd786":"* You know, to get random samples we need a random subset sampler, now we'll prepare it.","469b3dae":"* Our dataset and samplers are ready, we can create our data loader objects and start to model our artifical neural network.","d816b59a":"* And now everything is okay with texts, let's encode the classes.","5c5ae977":"# Step 1: Preparing Dataset\nIn this step we're going to read our dataset, create our dataset class and prepare our dataloaders. "}}