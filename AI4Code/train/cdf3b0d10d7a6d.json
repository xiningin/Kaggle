{"cell_type":{"bfe8b16e":"code","7da17eb2":"code","f022d3ce":"code","15e7f4b8":"code","6a5da10f":"code","2116c88b":"code","4e4a334a":"code","c8faffc6":"code","1fe6327c":"code","030fe369":"code","92e04875":"code","f095598d":"code","a040482c":"code","270bff52":"code","d98fb9ad":"code","44bf62e3":"code","cc64b8ce":"code","e27a2855":"code","4ebd6542":"code","8a98ab88":"code","3e507eba":"code","7d26e76e":"code","1862c883":"code","21841aa8":"code","49ccf95e":"code","7da2b313":"code","3c9e2092":"code","d619fbf0":"code","af0373a4":"code","f0522691":"code","54b35a00":"code","96d564f2":"code","c0640a5a":"code","5579614b":"code","c189ec8d":"code","66fda4b5":"code","9109b363":"code","37a79a27":"code","14d4952f":"code","88106a8f":"code","7fb8cdfa":"code","8695e0ec":"code","2791ef68":"code","e9321190":"code","fa3d30ab":"code","274f6260":"code","98e3c547":"code","8a385f74":"code","8daf83f9":"code","c4abd306":"code","f13711bc":"code","d7485af0":"code","5add581e":"code","f4a521d1":"code","dc0f04af":"code","85d9a78d":"code","59cc2896":"code","f9f1b1d5":"code","c915405c":"code","18d03fb0":"code","52a41f18":"code","de164824":"code","6d06ddd8":"code","2f185010":"code","d7d77fb9":"code","9c639f5b":"code","40deb5fe":"code","643f3b7a":"code","c3ddbf71":"code","5ce08f81":"code","07039b06":"code","08d1ef30":"code","07b026e8":"markdown","c7c679c7":"markdown","b417286a":"markdown","db30020d":"markdown","02e82683":"markdown","54537089":"markdown","f310cde9":"markdown","48da1636":"markdown","1cc4a859":"markdown","bf7b1e1a":"markdown","9833979b":"markdown","206674d0":"markdown","50941ccc":"markdown","a8996eee":"markdown","be3ebdbc":"markdown","4bce591d":"markdown","e69aaa59":"markdown","e94e5b11":"markdown"},"source":{"bfe8b16e":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.corpus import movie_reviews\nimport matplotlib.pyplot as plt","7da17eb2":"words = movie_reviews.words()\nwords","f022d3ce":"len(words)","15e7f4b8":"cat = movie_reviews.categories()","6a5da10f":"cat","2116c88b":"nltk.FreqDist(movie_reviews.words())","4e4a334a":"field_id = movie_reviews.fileids()\nfield_id[:5]","c8faffc6":"first_review = movie_reviews.words(field_id[0])\nfirst_review[:20]","1fe6327c":"len(first_review)","030fe369":"from nltk.corpus import stopwords","92e04875":"stop_words = set(stopwords.words('english'))","f095598d":"first_review_without_stop_words = []\nfor x in first_review:\n    if x not in stop_words:\n        first_review_without_stop_words.append(x)\n        \nfirst_review_without_stop_words","a040482c":"edit_distance = nltk.edit_distance('happy','sappy')\nedit_distance","270bff52":"from nltk.corpus import wordnet\nfrom itertools import product","d98fb9ad":"allsyns1 = set(ss  for ss in wordnet.synsets('happy'))\nallsyns2 = set(ss  for ss in wordnet.synsets('glad'))\nbest = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\nprint(best)","44bf62e3":"allsyns1 = set(ss  for ss in wordnet.synsets('happy'))\nallsyns2 = set(ss  for ss in wordnet.synsets('sappy'))\nbest = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\nprint(best)","cc64b8ce":"from nltk import TweetTokenizer","e27a2855":"tweets = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","4ebd6542":"tweets.head()","8a98ab88":"tweets_text = tweets['text']","3e507eba":"tweets_text","7d26e76e":"first_tweet = tweets_text[0]","1862c883":"tokenizer = TweetTokenizer()\ntokenized_first_tweet = tokenizer.tokenize(first_tweet)","21841aa8":"tokenized_first_tweet","49ccf95e":"pip install ftfy","7da2b313":"pip install bs4","3c9e2092":"import ftfy\nfrom bs4 import BeautifulSoup\nimport re\nimport string","d619fbf0":"def remove_punctuation(text):\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text","af0373a4":"text = 'Hello!! How are you?'\ncleaned_text = remove_punctuation(text)\ncleaned_text","f0522691":"def remove_extra_whitespace(text):\n    text = ' '.join(text.split())\n    return text","54b35a00":"text = 'Hello   How  are    you?'\ncleaned_text = remove_extra_whitespace(text)\ncleaned_text","96d564f2":"pip install textblob","c0640a5a":"from textblob import TextBlob\ndef correct_spelling(text):\n    corrected_text = []\n    word_list = text.split()\n    for word in word_list:\n        corrected_word = TextBlob(word)\n        corrected_text.append(str(corrected_word.correct()))\n    correct_text = ' '.join(corrected_text)\n    return correct_text\n        \n        ","5579614b":"text = 'Hello How are yhu?'\ncorrected_text = correct_spelling(text)\ncorrected_text","c189ec8d":"from nltk.stem import WordNetLemmatizer\n \nlemmatizer = WordNetLemmatizer()\n \nprint(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\nprint(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n \n# a denotes adjective in \"pos\"\nprint(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))","66fda4b5":"data = pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv',encoding='latin-1',names =['sentiment','text'])","9109b363":"data","37a79a27":"data['sentiment'].value_counts()","14d4952f":"import collections\nfrom sklearn.feature_extraction.text import CountVectorizer","88106a8f":"CountVec = CountVectorizer(ngram_range=(1,1),  stop_words='english')","7fb8cdfa":"Count_data = CountVec.fit_transform(tweets_text)","8695e0ec":"cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\nprint(cv_dataframe)","2791ef68":"from collections import defaultdict\npositiveValues=defaultdict(int)\nnegativeValues=defaultdict(int)\nneutralValues=defaultdict(int)","e9321190":"def generate_N_grams(text,ngram=1):\n    words=[word for word in text.split(\" \") if word not in set(stopwords.words('english'))]  \n    #print(\"Sentence after removing stopwords:\",words)\n    temp=zip(*[words[i:] for i in range(0,ngram)])\n    ans=[' '.join(ngram) for ngram in temp]\n    return ans","fa3d30ab":"generate_N_grams(\"The sun rises in the east\",2)","274f6260":"for text in data[data.sentiment==\"positive\"].text:\n    for word in generate_N_grams(text,2):\n        positiveValues[word]+=1 ","98e3c547":"for text in data[data.sentiment==\"negative\"].text:\n    for word in generate_N_grams(text,2):\n        negativeValues[word]+=1","8a385f74":"for text in data[data.sentiment==\"neutral\"].text:\n    for word in generate_N_grams(text,2):\n        neutralValues[word]+=1","8daf83f9":"df_positive=pd.DataFrame(sorted(positiveValues.items(),key=lambda x:x[1],reverse=True))\ndf_negative=pd.DataFrame(sorted(negativeValues.items(),key=lambda x:x[1],reverse=True))\ndf_neutral=pd.DataFrame(sorted(neutralValues.items(),key=lambda x:x[1],reverse=True))","c4abd306":"pd1=df_positive[0][:10]\npd2=df_positive[1][:10]","f13711bc":"plt.figure(1,figsize=(16,4))\nplt.bar(pd1,pd2, color ='green',\n        width = 0.4)\nplt.xlabel(\"Words in positive dataframe\")\nplt.ylabel(\"Count\")\nplt.title(\"Top 10 words in positive dataframe-BIGRAM ANALYSIS\")\nplt.savefig(\"positive-unigram.png\")\nplt.show()","d7485af0":"ned1=df_negative[0][:10]\nned2=df_negative[1][:10]","5add581e":"plt.figure(1,figsize=(16,4))\nplt.bar(ned1,ned2, color ='red',\n        width = 0.4)\nplt.xlabel(\"Words in negative dataframe\")\nplt.ylabel(\"Count\")\nplt.title(\"Top 10 words in negative dataframe-BIGRAM ANALYSIS\")\nplt.savefig(\"negative-unigram.png\")\nplt.show()","f4a521d1":"from sklearn.feature_extraction.text import TfidfVectorizer","dc0f04af":"tfidf = TfidfVectorizer()","85d9a78d":"corpus = ['This is First Sentence', 'This is Second', ' I guess This will be third', \n          'Stop it man not funny, this is fourth sentence like this']","59cc2896":"result = tfidf.fit_transform(corpus)","f9f1b1d5":"print('\\nidf values:')\nfor ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n    print(ele1, ':', ele2)","c915405c":"import re","18d03fb0":"def tokenize(text):\n    # obtains tokens with a least 1 alphabet\n    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n    return pattern.findall(text.lower())\n\ndef mapping(tokens):\n    word_to_id = dict()\n    id_to_word = dict()\n\n    for i, token in enumerate(set(tokens)):\n        word_to_id[token] = i\n        id_to_word[i] = token\n\n    return word_to_id, id_to_word","52a41f18":"def generate_training_data(tokens, word_to_id, window_size):\n    N = len(tokens)\n    X, Y = [], []\n\n    for i in range(N):\n        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n                   list(range(i + 1, min(N, i + window_size + 1)))\n        for j in nbr_inds:\n            X.append(word_to_id[tokens[i]])\n            Y.append(word_to_id[tokens[j]])\n            \n    X = np.array(X)\n    X = np.expand_dims(X, axis=0)\n    Y = np.array(Y)\n    Y = np.expand_dims(Y, axis=0)\n\n    return X, Y","de164824":"doc = '''Hi, I am Sourabh Yadav. \n     I am a student of Netaji Subhas University of Technology.\n     I am Studying Computer Engineering and This subject is NLP.\n    '''\ntokens = tokenize(doc)\nword_to_id, id_to_word = mapping(tokens)\nX, Y = generate_training_data(tokens, word_to_id, 3)\nvocab_size = len(id_to_word)\nm = Y.shape[1]\n# turn Y into one hot encoding\nY_one_hot = np.zeros((vocab_size, m))\nY_one_hot[Y.flatten(), np.arange(m)] = 1\nY_one_hot","6d06ddd8":"from sklearn.model_selection import train_test_split\nimport pprint, time\n \n\nnltk.download('treebank')\nnltk.download('universal_tagset')\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\nprint(nltk_data[:2])","2f185010":"train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)","d7d77fb9":"train_tagged_words = [ tup for sent in train_set for tup in sent ]","9c639f5b":"tags = {tag for word,tag in train_tagged_words}\nprint(len(tags))\nprint(tags)\n\nvocab = {word for word,tag in train_tagged_words}","40deb5fe":"def word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n \n     \n    return (count_w_given_tag, count_tag)","643f3b7a":"def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1)","c3ddbf71":"tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\nfor i, t1 in enumerate(list(tags)):\n    for j, t2 in enumerate(list(tags)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","5ce08f81":"tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\ndisplay(tags_df)","07039b06":"def Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n     \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                 \n\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n             \n        pmax = max(p)\n\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))","08d1ef30":"test_sent=\"Hi, How are you?\"\npred_tags= Viterbi(test_sent.split())\nprint(pred_tags)","07b026e8":"# Remove Extra White Space","c7c679c7":"# Tweets Tokenization","b417286a":"## Using Wordnet ","db30020d":"## TF-IDF","02e82683":"## Word2Vec","54537089":"# Stop Word Removal","f310cde9":"# Remove Punctuation","48da1636":"# Spelling Correction","1cc4a859":"# Categories in movie review corpus","bf7b1e1a":"## N Grams","9833979b":"## Using Levenshtein distance","206674d0":"# Similarities between words","50941ccc":"# Individual Reviews","a8996eee":"# Words In movie review","be3ebdbc":"## Bag of words","4bce591d":"# Vectorization","e69aaa59":"# Frequency Distribution of words","e94e5b11":"# POS Tagging"}}