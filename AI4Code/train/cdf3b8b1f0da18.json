{"cell_type":{"4d9173fe":"code","72bb7051":"code","a284be89":"code","4b0a0d60":"code","c6a25e10":"code","d21a1df1":"code","b6de2f4a":"code","4a5d1c91":"code","8a2dc5fb":"code","fb49bd78":"code","9c571d21":"code","8ee5f008":"code","122ab78d":"code","88100108":"code","fc9a52df":"code","4be48fef":"code","eb929d04":"code","ddf388e6":"code","6e506bf0":"code","c107415d":"code","0de8a93b":"code","b4dddaa7":"code","bdd8d6f9":"code","d57a00d2":"code","5b2cff22":"code","5687338d":"code","1f766f43":"code","a17fc901":"code","75a96cb1":"code","3a86e74e":"code","9770596f":"code","251c805d":"code","b92539cb":"code","59efefd4":"code","8e0eb469":"code","9d8dadc8":"code","ccf0008b":"code","2e6c56bb":"code","26fdf4bc":"code","1ba8a2fe":"code","6eacdf2a":"code","25af31ee":"code","6ab93b5b":"code","3772d623":"code","60dffe40":"code","5824d080":"code","68fe4f61":"code","1f5c5b1f":"code","5a098eea":"code","38115efc":"code","204bf978":"code","85085ebb":"code","6025a709":"code","7519e59d":"code","94db4b10":"code","2ad9ed21":"code","a390a33a":"code","cfd49ded":"code","da6253ce":"code","85d8f71a":"code","c8d0db35":"code","8422ab59":"code","a63b5c79":"code","18cb43e3":"code","0bfd0083":"code","f2c249c9":"code","f8e5085c":"code","1e655e01":"code","6459ada5":"code","bd1949e9":"code","800a7ad2":"code","440833e7":"code","a4bc662e":"code","c7f7554f":"code","ac393c1d":"code","9d09b427":"code","d5092848":"code","8f9c7146":"code","3e636e54":"code","3946cd02":"code","52910f37":"code","a5a8e02e":"code","e26c35a4":"code","6526f9e8":"code","84991bb4":"code","66c9a991":"markdown"},"source":{"4d9173fe":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","72bb7051":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a284be89":"# Importing Breast Cancer datasets\nBCdata = pd.read_csv('..\/input\/breastcancerdata.csv')\nBCdata.head(5).transpose()","4b0a0d60":"# Let's check the dimensions of the dataframe\nprint(BCdata.shape)\n# Let's see the type of each column\nprint(BCdata.info())","c6a25e10":"# summarising number of missing values in each column\nBCdata.isnull().sum()","d21a1df1":"# summarising number of missing values in each row\nBCdata.isnull().sum(axis=1)","b6de2f4a":"#checking for redundant duplicate rows\nprint(sum(BCdata.duplicated()))\n#Dropping Duplicate Rows\nBCdata.drop_duplicates(keep=False,inplace=True)\nprint(sum(BCdata.duplicated()))","4a5d1c91":"#dropping columns having null value \"Unnamed:32\"\nBCdata.drop(['Unnamed: 32'], axis = 1, inplace = True)","8a2dc5fb":"# let's look at the outliers for numeric features in dataframe\nBCdata.describe(percentiles=[.25,.5,.75,.90,.95,.99]).transpose()","fb49bd78":"# correlation matrix\ncor = BCdata.corr()\ncor","9c571d21":"# Plotting correlations on a heatmap post outlier treatment\n# figure size\nplt.figure(figsize=(20,15))\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()","8ee5f008":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","122ab78d":"#pairplots for numerical data frames\nplt.figure(figsize=(20,12))\nsns.pairplot(BCdata)\nplt.show()","88100108":"# List of binary variables with M\/B values using map converting these to 1\/0\nvarlist =  ['diagnosis']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'M': 1, 'B': 0})\n\n# Applying the function to the leads score list\nBCdata[varlist] = BCdata[varlist].apply(binary_map)","fc9a52df":"from sklearn.model_selection import train_test_split\n# Putting feature variables to X by first dropping y (Attrition) from HRdata\nX = BCdata.drop(['diagnosis'], axis=1)\n# Putting response variable to y\ny = BCdata['diagnosis']\nprint(y.head())","4be48fef":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","eb929d04":"X.columns","ddf388e6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']] = scaler.fit_transform(X_train[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']])\n\n#verifying the scaled data in X_train dataframe\nX_train.describe()","6e506bf0":"### Before we build the Logistic regression model, we need to know how much percent of Diagnosis as Malign is seen in the original data\n### Calculating the Diagnosis Rate\nDiagnosisRate = round((sum(BCdata['diagnosis'])\/len(BCdata['diagnosis'].index))*100,2)\nDiagnosisRate","c107415d":"import statsmodels.api as sm\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","0de8a93b":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,10)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, y_train)","b4dddaa7":"rfe.support_\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","bdd8d6f9":"col = X_train.columns[rfe.support_]","d57a00d2":"X_train.columns[~rfe.support_]","5b2cff22":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","5687338d":"#### Check for the VIF values of the feature variables.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","1f766f43":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a17fc901":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('radius_worst',1)\ncol","75a96cb1":"#,'perimeter_mean','perimeter_worst','area_mean','area_worst','radius_se','perimeter_se'","3a86e74e":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","9770596f":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","251c805d":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('perimeter_worst',1)\ncol","b92539cb":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","59efefd4":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8e0eb469":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('area_se',1)\ncol","9d8dadc8":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","ccf0008b":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2e6c56bb":"##Removing all features showing high value in VIF exceeding value of 5, as this indicates high multi collinearity\ncol = col.drop('concave points_worst',1)\ncol","26fdf4bc":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","1ba8a2fe":"## VIF AGAIN\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6eacdf2a":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","25af31ee":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","6ab93b5b":"y_train_pred_final = pd.DataFrame({'Diagnosis':y_train.values, 'Diagnosis_Probability':y_train_pred})\ny_train_pred_final['PatientID'] = y_train.index\ny_train_pred_final.head()","3772d623":"y_train_pred_final['predicted'] = y_train_pred_final.Diagnosis_Probability.map(lambda x: 1 if x > 0.8 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","60dffe40":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.predicted )\nprint(confusion)","5824d080":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted))","68fe4f61":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","1f5c5b1f":"# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity is:\")\nTP \/ float(TP+FN)","5a098eea":"# Let us calculate specificity\nprint(\"Specificity is:\")\nTN \/ float(TN+FP)","38115efc":"# Calculate false postive rate - predicting Conversion when customer does not Convert\nprint(\"False Positive Rate is:\")\nprint(FP\/ float(TN+FP))","204bf978":"# positive predictive value \nprint(\"Positive Predictive value is:\")\nprint (TP \/ float(TP+FP))","85085ebb":"# Negative predictive value\nprint(\"Negative Predictive value is:\")\nprint (TN \/ float(TN+ FN))","6025a709":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","7519e59d":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability, drop_intermediate = False )","94db4b10":"draw_roc(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability)","2ad9ed21":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Diagnosis_Probability.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","a390a33a":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","cfd49ded":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","da6253ce":"y_train_pred_final['final_predicted'] = y_train_pred_final.Diagnosis_Probability.map( lambda x: 1 if x > 0.35 else 0)\ny_train_pred_final.head()","85d8f71a":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Diagnosis, y_train_pred_final.final_predicted)","c8d0db35":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Diagnosis, y_train_pred_final.final_predicted )\nconfusion2","8422ab59":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a63b5c79":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","18cb43e3":"# Let us calculate specificity - High specificity indicates the model can identify those who will not have attrition will have a negative test result.\nTN \/ float(TN+FP)","0bfd0083":"# Calculate false postive rate - predicting Attrition when Employee is not Attrition\nprint(FP\/ float(TN+FP))","f2c249c9":"# Positive predictive value \nprint (TP \/ float(TP+FP))","f8e5085c":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","1e655e01":"#Precision\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","6459ada5":"#Recall\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","bd1949e9":"## Using sklearn to calculate above\nfrom sklearn.metrics import precision_score, recall_score\nprecision_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted)","800a7ad2":"recall_score(y_train_pred_final.Diagnosis, y_train_pred_final.predicted)","440833e7":"from sklearn.metrics import precision_recall_curve\ny_train_pred_final.Diagnosis, y_train_pred_final.predicted\np, r, thresholds = precision_recall_curve(y_train_pred_final.Diagnosis, y_train_pred_final.Diagnosis_Probability)","a4bc662e":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","c7f7554f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_test[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']] = scaler.fit_transform(X_test[['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']])\n\nX_test = X_test[col]\nX_test.head()","ac393c1d":"X_test.columns","9d09b427":"X_test_sm = sm.add_constant(X_test)\n# Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","d5092848":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n# Let's see the head\ny_pred_1.head()","8f9c7146":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","3e636e54":"# Putting LeadID to index\ny_test_df['PatientID'] = y_test_df.index","3946cd02":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","52910f37":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","a5a8e02e":"y_pred_final.head()","e26c35a4":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Diagnosis_Probability'})","6526f9e8":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['PatientID','diagnosis','Diagnosis_Probability'], axis=1)","84991bb4":"# Let's see the head of y_pred_final\ny_pred_final","66c9a991":"### Breast Cancer Detection from Tissue Cell Diagnostics"}}