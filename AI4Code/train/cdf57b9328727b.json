{"cell_type":{"19ad81b6":"code","5e875624":"code","34bf7370":"code","b2e88b60":"code","aaf5035d":"code","fe7ad3c3":"code","a0f1cb92":"code","d1c6898d":"code","16fcb7ec":"code","1f1072bf":"code","a199a5dc":"code","f6e7cf6b":"code","83c481fa":"code","5aa3a52a":"code","03216e82":"code","a6c1bca4":"code","f9458cfe":"code","4b276671":"code","ad835317":"code","5f6a682e":"code","9949d8d2":"markdown","21f7c856":"markdown","5ec358fc":"markdown","1e7d4b81":"markdown","4e6a8a51":"markdown","9ec3096b":"markdown","4e627107":"markdown","664f39c9":"markdown","9c8a5503":"markdown","c0e8e2f0":"markdown","ad69050e":"markdown","3b213bb3":"markdown","5e3ccd2d":"markdown"},"source":{"19ad81b6":"import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","5e875624":"X1, y1 = make_blobs(n_samples=200, centers=2,random_state=0, cluster_std=0.60)\ny1 = np.where(y1 <= 0, -1, 1)\nprint(\"First five rows and col values \\nX1 : \\n\",X1[:5], \" \\n y1 :\\n\",y1[:5])\nplt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='winter', alpha=.5)\nplt.title(\"Dataset 1\")\nplt.show()","34bf7370":"X2, y2 = make_moons(n_samples=200, noise=.05)\ny2 = np.where(y2 <= 0, -1, 1)\nprint(\"First five rows and col values \\nX2 : \\n\",X2[:5], \" \\n y2 :\\n\",y1[:5])\nplt.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap='winter', alpha=.5)\nplt.title(\"Dataset 2\")\nplt.show()","b2e88b60":"X3, y3 = noisy_circles = make_circles(n_samples=200, factor=.5, noise=.05)\ny3 = np.where(y3 <= 0, -1, 1)\nprint(\"First five rows and col values \\nX1 : \\n\",X3[:5], \" \\n y3 :\\n\",y1[:5])\nplt.scatter(X3[:, 0], X3[:, 1], c=y3, s=50, cmap='winter', alpha=.5)\nplt.title(\"Dataset 3\")\nplt.show()","aaf5035d":"class SVM_soft_margin:\n\n    def __init__(self, alpha = 0.001, lambda_ = 0.01, n_iterations = 1000):\n        self.alpha = alpha # learning rate\n        self.lambda_ = lambda_ # tradeoff\n        self.n_iterations = n_iterations # number of iterations\n        self.w = None # weights or slopes\n        self.b = None # intercept\n\n\n    def fit(self, X, y):\n        \n        n_samples, n_features = X.shape        \n        self.w = np.zeros(n_features) # initalizing with 0\n        self.b = 0 # initializewith 0\n        \n        for iteration in range(self.n_iterations):\n            for i, Xi in enumerate(X):\n                # yixiw-b\u22651\n                if y[i] * (np.dot(Xi, self.w) - self.b) >= 1 : \n                    self.w -= self.alpha * (2 * self.lambda_ * self.w) # w = w + \u03b1* (2\u03bbw - yixi)\n                else:\n                    self.w -= self.alpha * (2 * self.lambda_ * self.w - np.dot(Xi, y[i])) # w = w + \u03b1* (2\u03bbw - yixi)\n                    self.b -= self.alpha * y[i] # b = b - \u03b1* (yi)\n        return self.w, self.b\n\n\n    def predict(self, X):\n        pred = np.dot(X, self.w) - self.b \n        result = [1 if val > 0 else -1 for val in pred] # returning in the form of -1 and 1\n        return result","fe7ad3c3":"def get_hyperplane(x, w, b, offset):\n        return (-w[0] * x + b + offset) \/ w[1]","a0f1cb92":"def plot_svm(X, y, w, b, title ='Plot for linear SVM'):    \n\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    plt.scatter(X[:,0], X[:,1], marker='o',c=y)\n\n    x0_1 = np.amin(X[:,0])\n    x0_2 = np.amax(X[:,0])\n\n    x1_1 = get_hyperplane(x0_1, w, b, 0)\n    x1_2 = get_hyperplane(x0_2, w, b, 0)\n\n    x1_1_m = get_hyperplane(x0_1, w, b, -1)\n    x1_2_m = get_hyperplane(x0_2, w, b, -1)\n\n    x1_1_p = get_hyperplane(x0_1, w, b, 1)\n    x1_2_p = get_hyperplane(x0_2, w, b, 1)\n\n    ax.plot([x0_1, x0_2],[x1_1, x1_2], 'y--')\n    ax.plot([x0_1, x0_2],[x1_1_m, x1_2_m], 'k')\n    ax.plot([x0_1, x0_2],[x1_1_p, x1_2_p], 'k')\n\n    x1_min = np.amin(X[:,1])\n    x1_max = np.amax(X[:,1])\n    ax.set_ylim([x1_min-3,x1_max+3])\n    \n    plt.title(title)\n    plt.show()","d1c6898d":"svm1 = SVM_soft_margin()\nw1,b1 = svm1.fit(X1,y1)\nprint(\"For dataset 1, score:\" ,accuracy_score(svm1.predict(X1),y1))\nplot_svm(X1, y1, w1, b1, title= 'Linear SVM for dataset 1')","16fcb7ec":"svm2 = SVM_soft_margin()\nw2,b2 = svm2.fit(X2,y2)\nprint(\"For dataset 2, score:\" ,accuracy_score(svm2.predict(X2),y2))\nplot_svm(X2, y2, w2, b2, title= 'Linear SVM for dataset 2')","1f1072bf":"svm3 = SVM_soft_margin()\nw3,b3 = svm3.fit(X3,y3)\nprint(\"For dataset 3, score:\" ,accuracy_score(svm3.predict(X3),y3))\nplot_svm(X3, y3, w3, b3, title= 'Linear SVM for dataset 3')","a199a5dc":"# implementing SVM using Stochastic Gradient Descent and Pegasos algorithms\n\ndef SVM_SGD(X,Y,lambda_,epoches):\n    \n    X = np.c_[np.ones(X.shape[0]), X] # adding feature containing 1\n    w = np.zeros(X.shape[1]) # initializing w with 0s\n    \n    # converting arrays to matrix\n    X = np.matrix(X)\n    Y = np.matrix(Y) \n    Y = Y.T\n    w = np.matrix(w)\n\n    for t in range(1,epoches+1):\n        \n        # choosing random points\n        random_pos = random.randrange(0, X.shape[0]-1)\n        x = X[random_pos,:]\n        y = Y[random_pos,:]\n    \n        # computing eta and finding class\n        eta = 1\/(lambda_*t)\n        cls = y * (w*x.T)\n\n        if cls < 1:\n            w = (1 - eta*lambda_)*w + eta*y*x\n        elif cls >= 1:\n            w = (1 - eta*lambda_)*w\n\n    pred = w*X.T\n    # converting matrix back to array\n    pred = np.array(pred)\n    result = [1 if val > 0 else -1 for val in pred[0]] # converting predictions to -1 and 1\n    return np.array(w)[0], result\n        ","f6e7cf6b":"# performance for 1st dataset\nw1,result1 = SVM_SGD(X1,y1,lambda_ = 0.01,epoches = 1000)\nprint(\"For dataset 1, score: \", accuracy_score(result1,y1))\n\nw2,result2 = SVM_SGD(X2,y2,lambda_ = 0.01,epoches = 1000)\nprint(\"For dataset 2, score: \", accuracy_score(result2,y2))\n\nw3,result3 = SVM_SGD(X3,y3,lambda_ = 0.01,epoches = 1000)\nprint(\"For dataset 3, score: \", accuracy_score(result3,y3))","83c481fa":"class SVM_Dual:\n\n    def __init__(self, kernel='poly', degree=2, sigma=0.1, epoches=1000, learning_rate= 0.001):\n        self.alpha = None\n        self.b = 0\n        self.degree = degree\n        self.c = 1\n        self.C = 1\n        self.sigma = sigma\n        self.epoches = epoches\n        self.learning_rate = learning_rate\n\n        if kernel == 'poly':\n            self.kernel = self.polynomial_kernal # for polynomial kernal\n        elif kernel == 'rbf':\n            self.kernel =  self.gaussian_kernal # for guassian\n\n    def polynomial_kernal(self,X,Z):\n        return (self.c + X.dot(Z.T))**self.degree #(c + X.y)^degree\n        \n    def gaussian_kernal(self, X,Z):\n        return np.exp(-(1 \/ self.sigma ** 2) * np.linalg.norm(X[:, np.newaxis] - Z[np.newaxis, :], axis=2) ** 2) #e ^-(1\/ \u03c32) ||X-y|| ^2\n    \n    def train(self,X,y):\n        self.X = X\n        self.y = y\n        self.alpha = np.random.random(X.shape[0])\n        self.b = 0\n        self.ones = np.ones(X.shape[0]) \n\n        y_mul_kernal = np.outer(y, y) * self.kernel(X, X) # yi yj K(xi, xj)\n\n        for i in range(self.epoches):\n            gradient = self.ones - y_mul_kernal.dot(self.alpha) # 1 \u2013 yk \u2211 \u03b1j yj K(xj, xk)\n\n            self.alpha += self.learning_rate * gradient # \u03b1 = \u03b1 + \u03b7*(1 \u2013 yk \u2211 \u03b1j yj K(xj, xk)) to maximize\n            self.alpha[self.alpha > self.C] = self.C # 0<\u03b1<C\n            self.alpha[self.alpha < 0] = 0 # 0<\u03b1<C\n\n            loss = np.sum(self.alpha) - 0.5 * np.sum(np.outer(self.alpha, self.alpha) * y_mul_kernal) # \u2211\u03b1i \u2013 (1\/2) \u2211i \u2211j \u03b1i \u03b1j yi yj K(xi, xj)\n            \n        alpha_index = np.where((self.alpha) > 0 & (self.alpha < self.C))[0]\n        \n        # for intercept b, we will only consider \u03b1 which are 0<\u03b1<C \n        b_list = []        \n        for index in alpha_index:\n            b_list.append(y[index] - (self.alpha * y).dot(self.kernel(X, X[index])))\n\n        self.b = np.mean(b_list) # avgC\u2264\u03b1i\u22640{ yi \u2013 \u2211\u03b1jyj K(xj, xi) }\n            \n    def predict(self, X):\n        return np.sign(self.decision_function(X))\n    \n    def score(self, X, y):\n        y_hat = self.predict(X)\n        return np.mean(y == y_hat)\n    \n    def decision_function(self, X):\n        return (self.alpha * self.y).dot(self.kernel(self.X, X)) + self.b\n\n    # https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_separating_hyperplane.html\n    def plot(self, title='Plot for non linear SVM'):\n        plt.scatter(self.X[:, 0], self.X[:, 1], c=self.y, s=50, cmap='winter', alpha=.5)\n        ax = plt.gca()\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        xx = np.linspace(xlim[0], xlim[1], 50)\n        yy = np.linspace(ylim[0], ylim[1], 50)\n        YY, XX = np.meshgrid(yy, xx)\n        xy = np.vstack([XX.ravel(), YY.ravel()]).T\n        Z = self.decision_function(xy).reshape(XX.shape)\n        ax.contour(XX, YY, Z, levels=[-1, 0, 1],linestyles=['--', '-', '--'])\n        plt.title(title)\n        plt.show()","5aa3a52a":"#dataset 1, using gaussian\nsvm_dual1 = SVM_Dual(kernel = 'rbf')\nsvm_dual1.train(X1,y1)\nprint(\"Accuracy: \", svm_dual1.score(X1,y1))\nsvm_dual1.plot('Non linear SVM plot for Dataset 1 usign rbf')","03216e82":"#dataset 2, using gaussian\nsvm_dual2 = SVM_Dual(kernel = 'rbf')\nsvm_dual2.train(X2,y2)\nprint(\"Accuracy: \", svm_dual2.score(X2,y2))\nsvm_dual2.plot('Non linear SVM plot for Dataset 2 usign rbf')","a6c1bca4":"# using polynomial with degree 2, because its circular\nsvm_dual3 = SVM_Dual(kernel='poly', degree=2)\nsvm_dual3.train(X3,y3)\nprint(svm_dual3.score(X3,y3))\nsvm_dual3.plot('Non linear SVM plot for Dataset 3 usign polynomial with degree 2')","f9458cfe":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_separating_hyperplane.html\n\n# defining a function to plot decision boundary according to the svm model\ndef plot(X, y, svm, title='SVM plot'):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n    # plot the decision function\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = svm.decision_function(xy).reshape(XX.shape)\n\n    # plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    # plot support vectors\n    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=100,\n               linewidth=1, facecolors='none', edgecolors='k')\n    plt.title(title)\n    plt.show()","4b276671":"# for dataset 1\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0) # creating testing and training set\n\nsvm1 = SVC(kernel='linear') # using linear kernal as our margin is a line\nsvm1.fit(X_train, y_train) # training the model\n\n# accurcy print\nprint(\"Test accuracy\", accuracy_score(svm1.predict(X_test), y_test))\nprint(\"Train accuracy\", accuracy_score(svm1.predict(X_train), y_train))\n\nplot(X1, y1, svm1, title='SVM plot for dataset 1 using linear kernel')","ad835317":"# for dataset 2\nX_train, X_test, y_train, y_test = train_test_split(X2, y2, random_state=0) # creating testing and training set\n\nsvm2 = SVC(kernel='rbf')  # using rbf kernal as our margin is non linear\nsvm2.fit(X_train, y_train) # training the model\n\n# accurcy print\nprint(\"Test accuracy\", accuracy_score(svm2.predict(X_test), y_test))\nprint(\"Train accuracy\", accuracy_score(svm2.predict(X_train), y_train))\nplot(X2, y2, svm2, title='SVM plot for dataset 2 using rbf kernel')","5f6a682e":"# for dataset 3\nX_train, X_test, y_train, y_test = train_test_split(X3, y3, random_state=0) # creating testing and training set\n\nsvm3 = SVC(kernel='poly', degree=2) # using poly with degree 2 as our margin is circular\nsvm3.fit(X_train, y_train)  # training the model\n\n# accurcy print\nprint(\"Test accuracy\", accuracy_score(svm3.predict(X_test), y_test))\nprint(\"Train accuracy\", accuracy_score(svm3.predict(X_train), y_train))\nplot(X3, y3, svm3, title='SVM plot for dataset 3 using polynomial kernel')","9949d8d2":"<br><br>\n<hr>\n<br><br>\n<center><h2>SVM for Non-Linear Dataset using Kernal<\/h2><center>","21f7c856":"<hr>\n<br>\n<br>\n<center><h1> Creating SVM models <\/h1><\/center>\n<br><br><br>\n\n## Without kernals\n\nFor a n-dimensional feature, SVM creates an n-1 dimensional hyperplane to seperate the classes.\nAny hyperplane can be represented as:\n\n<mark> w<sup>T<\/sup>X -b = 0 <\/mark>\n\nFor example, if we have a dataset which can be represented by a line, hyperplane would be a point. In our case we have 2 independent and 1 dependent feature. Thus, our hyperplane is a line with the quation: \n\n<mark> y = w<sub>1<\/sub>X<sub>1<\/sub> + w<sub>2<\/sub>X<sub>2<\/sub> - b <\/mark>\n\nwhere, w = [ w<sub>1<\/sub>, w<sub>2<\/sub> ]\n\nWe will proceed with the assumption that m<sub>1<\/sub>, m<sub>2<\/sub> and b are 0s, and we will update their value in accordane with learning rate. \n\n### Hard margin\n\nIf the dataset can be linearly sperated like our first dataset and partially in the second, we can create two parallel hyperplane(lines here) for each classes and contain each data points under these hyperplanes and our goal would be to maximize the distance between these two parallel hyperplane. The region between these two plane is called **margin**. The equations for both hyperplane is:\n\n<mark>\nw<sup>T<\/sup>X -b = -1 and,<br> \nw<sup>T<\/sup>X -b = 1 \n<\/mark>\n\nThe distance between them is 2\/||w|| and to maximize the distance, ||w|| should be minimum. \n\nTo prevent any data point falling inside margin we add the restriction,\n\n<mark>\ny<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b) >= 1 \n<\/mark>\n\nwhere y<sub>i<\/sub> = ith row in the target\nand X<sub>i<\/sub> = ith row in the X\n\n\n### Soft margin\n\nIf the dataset in non-linearly seprable (dataset 2 and 3, **note**: if in dataset 1, one or more points are in wrong classes, then it is also non linear), we can use hinge loss for loss function: <br>\n\n<mark> max(0, 1-y<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b)), <\/mark>\n\nIf the datapoint has class = 1, then the loss will be 0, otherwise it will be the distance between the margin and the datapoint.\n\nand our goal is to minimize\n\n<mark>Loss = (1\/n) \u03a3 max(0, 1-y<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b)) + \u03bb||w||<sup>2<\/sup> <\/mark>\n\nwhere \u03bb is a tradeoff between the margin size and x<sub>i<\/sub> being on the correct side of margon. If \u03bb is too low, the equation becomes hard margin. \n\n<h4>Updating weights<\/h4>\n\nLet's define by how we are updating weights by differntiating both terms in the loss with w<sub>k<\/sub>:\n\nFirst term:\n\n<mark>\ud835\udeffmax(0, 1-y<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b ))\/\ud835\udeffw<sub>k<\/sub> = { 0, if y<sub>i<\/sub>x<sub>i<\/sub>w -b\u22651 else -y<sub>i<\/sub>x<sub>i<\/sub> }<\/mark>\n\nSecond term:\n\n<mark>\ud835\udeff(\u03bb||w||<sup>2<\/sup>)\/\ud835\udeffw<sub>k<\/sub> = 2\u03bbw<\/mark>\n\nIf y<sub>i<\/sub>x<sub>i<\/sub>w-b\u22651\n\n<mark>w = w - \u03b1* 2\u03bbw<\/mark>\n\nelse,\n\n<mark>w = w + \u03b1* (2\u03bbw - y<sub>i<\/sub>x<sub>i<\/sub>)<\/mark>\n\n<h4>Updating intercept<\/h4>\n\nDifferentiaite loss by b\n\nFirst term:\n\n<mark>\ud835\udeffmax(0, 1-y<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b ))\/\ud835\udeffb = { 0, if y<sub>i<\/sub>x<sub>i<\/sub>w -b\u22651 else -y<sub>i<\/sub> }<\/mark>\n\nSecond term:\n\n<mark>\ud835\udeff(\u03bb||w||<sup>2<\/sup>)\/\ud835\udeffb = 0<\/mark>\n\n\nIf y<sub>i<\/sub>x<sub>i<\/sub>w-b\u22651\n\n<mark>b = b + \u03b1*0<\/mark>\n\nelse,\n\n<mark> b = b - \u03b1* (y<sub>i<\/sub>) <\/mark>\n\n\n**Note**:\n\n<h4>Slack Variable<\/h4>\n\nSometimes we have to allow few points to be inside margin. Slack veriable defines how much we can violate the margin which means how many points can be inside the margin.\n\n<mark>\ny<sub>i<\/sub>(w<sup>T<\/sup>X<sub>i<\/sub> -b) >= 1 - \u03be<sub>i<\/sub>\n<\/mark>\n\n\n<br>\n<br>\n\n**References**: \n\n1. [Support-vector machine, wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine) \n2. [Support Vector Machine \u2014 Introduction to Machine Learning Algorithms, Medium](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47). \n3. [A Support Vector Machine in just a few Lines of Python Code](https:\/\/maviccprp.github.io\/a-support-vector-machine-in-just-a-few-lines-of-python-code\/)\n\nPlease go through these if have facing any difficuly in understanding the implementation.\n","5ec358fc":"## Working with the pre-defined SVM Library","1e7d4b81":"![j](https:\/\/miro.medium.com\/max\/700\/1*R1GhsOV0UuMbnTwx8A47Zw.jpeg)","4e6a8a51":"## Thank you","9ec3096b":"## Creating dataset","4e627107":"**Note** : I am using accuracy_score as we are dealing with classification problems.\nr2_score is used for regression analysis (used in the Linear regression notebook)","664f39c9":"To use SVM for Non-Linear dataset, we have to develop a dual SVM. The maths behind it is a bit complicated so please refer the links below in the order:\n\n1. [Support Vector Machines for Beginners \u2013 Duality Problem](http:\/\/www.adeveloperdiary.com\/data-science\/machine-learning\/support-vector-machines-for-beginners-duality-problem\/)\n2. [Support Vector Machines for Beginners \u2013 Kernel SVM](http:\/\/www.adeveloperdiary.com\/data-science\/machine-learning\/support-vector-machines-for-beginners-kernel-svm\/)\n3. [Support Vector Machines for Beginners \u2013 Training Algorithms](http:\/\/www.adeveloperdiary.com\/data-science\/machine-learning\/support-vector-machines-for-beginners-training-algorithms\/)\n\n<h4>Loss<\/h4>\nThe Dual Lagrangian loss function which we are trying to maximize is:\n\n<mark>L<sub>dual<\/sub> = \u2211\u03b1<sub>i<\/sub> \u2013 (1\/2) \u2211<sub>i<\/sub> \u2211<sub>j<\/sub> \u03b1<sub>i<\/sub> \u03b1<sub>j<\/sub> y<sub>i<\/sub> y<sub>j<\/sub> K(x<sub>i<\/sub>, x<sub>j<\/sub>) <\/mark>\n\n<h4>Gradient<\/h4>\n\nDifferentiating the loss wrt \u03b1<sub>k<\/sub> , using kth term for Gradient Ascent:\n\n<mark>\u03b4L<sub>dual<\/sub>\/\u03b4\u03b1<sub>k<\/sub> = 1 \u2013 y<sub>k<\/sub> \u2211 \u03b1<sub>j<\/sub> y<sub>j<\/sub> K(x<sub>j<\/sub>, x<sub>k<\/sub>)<\/mark>\n\nwhere, <br>\n\nK(x<sub>i<\/sub>, x<sub>j<\/sub>) is our Kernal function which could be linear, polynomial or gaussian(rbf).\n\n<h4>Updates<\/h4>\n\n\u03b1 = \u03b1 + \u03b7*(gradient)\n\nwhere \u03b7 = learning rate\n\nAfter training, calculate intercept b:\n\n<mark>b = avg<sub>C\u2264\u03b1i\u22640<\/sub>{ y<sub>i<\/sub> \u2013 \u2211\u03b1<sub>j<\/sub>y<sub>j<\/sub> K(x<sub>j<\/sub>, x<sub>i<\/sub>) }<\/mark>\n\n<h4>Prediction <\/h4>\n\nFor \u03b1>0 :\n\n<mark>y^ = sign( \u2211 \u03b1<sub>i<\/sub>y<sub>i<\/sub> k(x<sub>i<\/sub>, x<sub>i<\/sub>)+b)<\/mark>\n\n1. Polynomial = (c + X.y)<sup>degree<\/sup>\n\n2. Gaussian = e <sup> -(1\/ \u03c3<sup>2<\/sup>) ||X-y|| <sup>2<\/sup> <\/sup> ","9c8a5503":"Creating three types of datasets using make_blobs ([docmentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py)). \n\n1. Using make_blobs\n2. Using make_moons\n3. Using make_circles","c0e8e2f0":"<hr>\n<br><br>\n<center><h2> SVM using PEGASOS and Stochastic Gradient Descent <\/h2><\/center>","ad69050e":"The above equation shows Pegasos algorithm, which is used with Stochastic Gradient Descent.\nFor implementing Stochastic Gradient Descent, we will append one more feature containing 1 to X and we will remove b. Thus, our equation <br>\n\ny = w<sub>1<\/sub>X<sub>1<\/sub> + w<sub>2<\/sub>X<sub>2<\/sub> - b <br>\n\nwill change to<br>\n\ny = w<sub>0<\/sub>X<sub>0<\/sub> + w<sub>1<\/sub>X<sub>1<\/sub> + w<sub>2<\/sub>X<sub>2<\/sub> <br>\n\nbut as x<sub>0<\/sub> = 1, thus w<sub>0<\/sub> = -b\n\n<br>\n\n**References**\n1. [svm-from-scratch-step-by-step-in-python](https:\/\/fordcombs.medium.com\/svm-from-scratch-step-by-step-in-python-f1e2d5b9c5be)\n\n","3b213bb3":"## Further reading\n\n\n1. [Support Vector Machine \u2014 Introduction to Machine Learning Algorithms, Towards Data Science](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n\n2. [SVM from scratch: step by step in Python, Medium](https:\/\/fordcombs.medium.com\/svm-from-scratch-step-by-step-in-python-f1e2d5b9c5be)\n\n3. [Pegasos algorithm for One-class Support Vector Machine, Researchgate](https:\/\/www.researchgate.net\/publication\/262425529_Pegasos_Algorithm_for_One-Class_Support_Vector_Machine)\n\n4. [Support Vector Machines - THE MATH YOU SHOULD KNOW, Code Emporium, Youtube](https:\/\/youtu.be\/05VABNfa1ds)\n\n5. [Math behind SVM (Support Vector Machine), Medium](https:\/\/medium.com\/@ankitnitjsr13\/math-behind-support-vector-machine-svm-5e7376d0ee4d)\n\n6. [Math Behind SVM(Kernel Trick), Medium](https:\/\/medium.com\/@ankitnitjsr13\/math-behind-svm-kernel-trick-5a82aa04ab04)\n\n7. [The Mathematics Behind Support Vector Machine Algorithm (SVM), Analyticsvidhya](https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/the-mathematics-behind-svm\/)\n\n8. [Kernel Support Vector Machines from scratch, Medium](https:\/\/towardsdatascience.com\/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76)","5e3ccd2d":"Let's check the algorithm for all 3 datasets"}}