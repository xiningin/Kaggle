{"cell_type":{"5d7f71ca":"code","48fa107c":"code","708d2500":"code","720044cb":"code","284dccf7":"code","92a21f2f":"code","5633329a":"code","8c0f3461":"code","a54355f0":"code","39b0a1ad":"code","9a74ee92":"code","6a79bb9f":"code","d3679db1":"code","a19f369f":"code","9ea52c5c":"markdown","990c403d":"markdown","7df95152":"markdown","e1cf1fa1":"markdown"},"source":{"5d7f71ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom fastai.metrics import accuracy, KappaScore\nfrom fastai.vision import *\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48fa107c":"train_df = pd.read_csv('\/kaggle\/input\/louisiana-flood-2016\/train.csv')\nvalid_df = pd.read_csv('\/kaggle\/input\/louisiana-flood-2016\/test.csv')","708d2500":"train_df.head(), valid_df.head()","720044cb":"train_df['Image ID'] = train_df['Image ID'].apply(lambda x: os.path.join('train', x))\nvalid_df['Image ID'] = valid_df['Image ID'].apply(lambda x: os.path.join('test', x))","284dccf7":"train_df.head(), valid_df.head()","92a21f2f":"df = pd.concat([train_df, valid_df])\ndf.shape, df.head()","5633329a":"df = df[df['Normal']==0]\ndf.shape, df.head()","8c0f3461":"data_dir = Path('..\/input\/louisiana-flood-2016\/')","a54355f0":"data = ImageDataBunch.from_df(data_dir, df, valid_pct=0.2, \n                              label_col='Flooded', ds_tfms=None,\n                              size=224).normalize(imagenet_stats)","39b0a1ad":"data.classes","9a74ee92":"kappa = KappaScore()\nkappa.weights = \"quadratic\"","6a79bb9f":"learn = cnn_learner(data, models.resnet34, metrics=[accuracy, kappa], model_dir=Path('\/kaggle\/working\/'))","d3679db1":"learn.fit_one_cycle(8)\nlearn.recorder.plot_losses()","a19f369f":"learn.show_results()","9ea52c5c":"We dont have many data for performing deeplearning. Even though the model was able to give high accuracy, there is a chance that it is overfitting.\nMay be we can use augmentation for generating more samples.","990c403d":"The apporach in this notebook is to ignore before\/after satellite images and perform binary classification on images containing flooded and not flooded regions.","7df95152":"For getting a benchmark accuracy lets combine train and valid data.","e1cf1fa1":"Remove the images taken before\/after the flood."}}