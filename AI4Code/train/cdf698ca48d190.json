{"cell_type":{"4fca775b":"code","62a5d669":"code","5e7de3b1":"code","96ef3df0":"code","b59612e5":"code","ef3b6cb3":"code","9d4bf701":"code","646cc3d6":"code","5658c566":"code","dec21869":"code","1f4fe814":"code","edc1e21a":"code","2c534d4e":"code","ed1d52bb":"code","7f2fd0d5":"code","05f7b0ba":"code","0faf7c0d":"code","d031823a":"code","99b7c64c":"code","b988e917":"code","51aacd4c":"code","d3b22a5c":"markdown","27bb3478":"markdown","9adb5647":"markdown","e88c43a9":"markdown","8842fbe6":"markdown","31a215b8":"markdown","04173ea1":"markdown","dafa5a19":"markdown","4f403725":"markdown","4d750e2e":"markdown","355c5f03":"markdown","c1fc45ae":"markdown","021aac91":"markdown","26ef1c1e":"markdown","5daf0ffa":"markdown","d01220a8":"markdown","81aa6cf9":"markdown","fb63a203":"markdown","407d0fb0":"markdown","4856bf88":"markdown","c50c6969":"markdown","dc647b3d":"markdown","8d688520":"markdown","36744cad":"markdown","c1d298e0":"markdown","38a9cab4":"markdown","9459dc25":"markdown","fb847d90":"markdown"},"source":{"4fca775b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","62a5d669":"\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport eli5\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer,accuracy_score\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp,get_dataset,info_plots\nfrom sklearn.feature_selection import RFECV\nfrom scipy.stats import norm, skew\nfrom scipy.optimize import curve_fit\nimport shap","5e7de3b1":"data=pd.read_csv(\"..\/input\/heart.csv\")","96ef3df0":"data.sample(5)","b59612e5":"print(data.shape)","ef3b6cb3":"data.info()","9d4bf701":"continuous=['age','trestbps','chol','thalach','oldpeak']\n\nf,ax=plt.subplots(3,2,figsize=(10,10))\nfor i,feature in enumerate(continuous):\n    (mu,sigma)=norm.fit(data[feature])\n    alpha=skew(data[feature])\n    sns.distplot(data[feature],fit=norm,ax=ax[i\/\/2][i%2])\n    ax[i\/\/2][i%2].set_title('Distribution of {}'.format(feature))\n    ax[i\/\/2][i%2].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, $\\\\alpha=$ {:.2f}'.format(mu,sigma,alpha)],loc='best')\n    ax[i\/\/2][i%2].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","646cc3d6":"bins=[28,40,50,60,80]\nnames=['Young Adult','Adult','Adult 2','Old']\ndata['age']=pd.cut(data['age'],bins=bins,labels=names)\nage_map={'Young Adult':'0','Adult':'1','Adult 2':'2','Old':'3'}\ndata['age']=data['age'].map(age_map)\ndata['age']=data['age'].astype('int64')\ndata['age'].value_counts().plot.bar()\nplt.xlabel('Age Category')\nplt.ylabel('Frequency')\nplt.show()","5658c566":"f,ax=plt.subplots(2,2,figsize=(10,10))\ncontinuous.remove('age')\nfor i,feature in enumerate(continuous):\n    df=data.groupby('age')[feature].mean().reset_index()\n    sns.lineplot(data=df,y=df[feature],x=df['age'],ax=ax[i\/\/2][i%2])\n    ax[i\/\/2][i%2].set_title('{}'.format(feature))\n    \nplt.tight_layout()\nplt.show()","dec21869":"for feature in continuous:\n    sns.boxplot(x='target',y=feature,data=data)\n    plt.show()","1f4fe814":"f,ax=plt.subplots(5,2,figsize=(12,12))\n\nfor i,feature in enumerate(['age','sex','cp','fbs','restecg','exang','slope','ca','thal']):\n    sns.countplot(x=feature,data=data,hue='target',ax=ax[i\/\/2,i%2])\n    ax[i\/\/2,i%2].set_title('Distribution of target wrt {}'.format(feature))\n    ax[i\/\/2,i%2].legend(loc='best')\n\nplt.tight_layout()\nplt.show()","edc1e21a":"X=data.drop('target',axis=1)\nY=data['target']\n\ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,random_state=1,test_size=0.2)\n\nss=StandardScaler()\n\ntrain_X[continuous]=ss.fit_transform(train_X[continuous])\ntest_X[continuous]=ss.transform(test_X[continuous])\n    ","2c534d4e":"my_model=xgb.XGBClassifier(n_estimators=100).fit(train_X,train_y)\n\nfeat_imp=pd.DataFrame({'importances':my_model.feature_importances_})\nfeat_imp['features']=train_X.columns\nfeat_imp=feat_imp.sort_values(by='importances',ascending=False)\n\n\nfeat_imp=feat_imp.set_index('features')\nfeat_imp.plot.barh(title='Feature Importances',figsize=(10,10))\nplt.xlabel('Feature Importance Score')\nplt.show()","ed1d52bb":"perm=PermutationImportance(my_model,random_state=1).fit(test_X,test_y)\neli5.show_weights(perm,feature_names=test_X.columns.tolist())","7f2fd0d5":"corr=data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr,annot=True)\nplt.show()\n","05f7b0ba":"for features in test_X.columns:\n    partial_graph=pdp.pdp_isolate(model=my_model,dataset=test_X,model_features=test_X.columns,feature=features)\n    pdp.pdp_plot(partial_graph,features)\n    plt.show()","0faf7c0d":"explainer=shap.TreeExplainer(my_model)\nshap_values=explainer.shap_values(test_X)\nshap.summary_plot(shap_values,test_X)","d031823a":"pred=my_model.predict(test_X)\n\nprint('Accuracy:',accuracy_score(test_y,pred))","99b7c64c":"rfc=RandomForestClassifier(n_estimators=100)\nacc=make_scorer(accuracy_score)\nrfecv=RFECV(rfc,cv=5,scoring=acc,step=1)\nrfecv=rfecv.fit(train_X,train_y)\n\nprint('Optimal number of features:',rfecv.n_features_)\nprint('\\nBest features:',train_X.columns[rfecv.support_])","b988e917":"train_X1=rfecv.transform(train_X)\ntest_X1=rfecv.transform(test_X)\n\nrfc_1=RandomForestClassifier(n_estimators=100,random_state=2).fit(train_X1,train_y)\npred_1=rfc_1.predict(test_X1)\nprint('Accuracy:',accuracy_score(test_y,pred_1))","51aacd4c":"plt.plot(range(1,len(rfecv.grid_scores_)+1),rfecv.grid_scores_)\nplt.xlabel('Number of features')\nplt.ylabel('Grid Scores')\nplt.show()","d3b22a5c":"**Correlation Matrix**\n","27bb3478":"In each plot, we see how the probability of having some target value (0 or 1) changes as the feature under construction increases its value. ","9adb5647":"**1.Importing Modules**","e88c43a9":"We see that except for distribution of exercise induced ST depression (oldpeak), all other continuous features have normal distribution to good extent","8842fbe6":"**2.Importing data and taking overview of it**","31a215b8":"Making the continuous features normally distributed. ","04173ea1":"In case of age vs target, proportion of people suffering from heart disease in that particular age group decreases with age. Again, this goes against my intuition ! In case of sex vs target, greater proportion of females have heart disease than males. Greater chest pain correlates with higher probability of having heart disease. ","dafa5a19":"**4. How do the continuous features vary with age?**\n","4f403725":"We can see that rest blood pressure (trestbps) ,cholesterol (chol), max heart rate (thalach) and exercise induced ST depression (oldpeak) have continuous forms while all other features have discrete form.","4d750e2e":"1 in target refers to presence of heart disease. We see that trestbps,chol and oldpeak of people suffering from heart disease is lesser than 0 target people while thalach is more than 0 target people. Intuitionally, I expected trestbps, chol and oldpeak of 1 target people to be higher than 0 target people. I am not an expert in medical field, so maybe I was wrong.","355c5f03":"**8. Feature Importance**","c1fc45ae":"As expected, we see that trestbps, chol and oldpeak increase with age, while thalach (max heart rate achieved) decreases with age.","021aac91":"There is no substantial correlation amongst the features tmeselves. ","26ef1c1e":"We see that there are no null values. ","5daf0ffa":"**5. How do the continuous features vary with target ?**","d01220a8":"**How Grid Score varies with number of features? **","81aa6cf9":"**SHAP values**","fb63a203":"**6. How do the discrete features vary with target ?**\n","407d0fb0":"Thus we see that if the decisions were taken by the classifier by ignoring chest pain, then large number of them go haywire. So it has the most feature importance. ","4856bf88":"**9. Permutation Importance**","c50c6969":"We see how much impact cp,ca, oldpeak have on probability of having either target values. Negative impact refers that probability of having 0 target value increases while positive impact refers to probability of having heart disease increases.","dc647b3d":"I transformed the age feature in bins as per intuition. This gives us an idea of how many people are young while how many are aged.. though it is subjective xD\n\nWe see that there are more number of aged people than younger people","8d688520":"**3. How is the distribution of continuous features?**","36744cad":"We see that the best fit is the one with 12 features. The fbs feature is removed. Even in case of permutation importance, permuting the values of fbs had no effect on prediction. Even in feature importances, fbs has least importance.","c1d298e0":"If the values of thal are shuffled in test_X while keeping all other features as it is, and then prediction was done then it has most impact than the shuffling of some other feature. ","38a9cab4":"**Recursive Feature Elimination with Cross Validation**","9459dc25":"7. Splitting and Preprocessing","fb847d90":"**Partial Dependence Plots of all features**"}}