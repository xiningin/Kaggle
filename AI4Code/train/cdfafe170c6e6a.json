{"cell_type":{"21f6814b":"code","623bdc3e":"code","a7b1bf45":"code","c103690c":"code","f56e03a1":"code","4de7ffbe":"code","734f11a6":"code","d43c2509":"code","53fe656b":"code","a0c8ec5e":"code","bd9ecb93":"code","e99ec442":"code","b918706a":"code","1d64968e":"code","3af9c3e4":"code","f56e8dd1":"code","5ce1b6bd":"code","062fabb2":"code","d0c69646":"code","be44001d":"code","97a4712e":"code","1fd2c596":"code","27eecf19":"code","e3e14144":"code","f4f0ff08":"code","869699e6":"code","153680e0":"code","0bbd6494":"code","8651063c":"markdown","f4038d9f":"markdown","8eab2956":"markdown","760e3b34":"markdown","3819d5f9":"markdown","5d2220a1":"markdown","86f03a78":"markdown","5de05010":"markdown","afcfcea3":"markdown","42ef3d6a":"markdown","8a15a643":"markdown","94bb2506":"markdown","5fa6bb42":"markdown","3b95a963":"markdown","5e254bce":"markdown","eb89b74e":"markdown","924be44c":"markdown","8dd0c9dd":"markdown","4016c19d":"markdown","b52eef15":"markdown","3ab6c5a2":"markdown","b02add15":"markdown","bce20d2a":"markdown","b6b51979":"markdown","37bc61df":"markdown","584dfd7e":"markdown"},"source":{"21f6814b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","623bdc3e":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas import DataFrame\nfrom sklearn.svm import SVC\n# Set seed for reproducibility\nSEED = 123","a7b1bf45":"dataset = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","c103690c":"dataset.info()","f56e03a1":"dataset.head()","4de7ffbe":"dataset.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","734f11a6":"dataset['diagnosis'] = dataset['diagnosis'].map({'M':1, 'B':0})","d43c2509":"dataset.describe()","53fe656b":"dataset['diagnosis'].value_counts()","a0c8ec5e":"sns.countplot(x='diagnosis', data=dataset)\nplt.title('Breast Cancer Diagnosis')\nplt.show()","bd9ecb93":"sns.scatterplot(x = 'area_mean', y = 'radius_mean', hue = 'diagnosis', data = dataset)\nplt.show()","e99ec442":"sns.scatterplot(x = 'radius_worst', y = 'radius_mean', hue = 'diagnosis', data = dataset)\nplt.show()","b918706a":"plt.figure(figsize=(20,10)) \nsns.heatmap(dataset.corr(), annot=True)\nplt.show()","1d64968e":"new_dataset = dataset.drop(['perimeter_mean', 'area_mean', \n                            'radius_worst', 'perimeter_worst', 'area_worst',\n                           'perimeter_se', 'area_se', 'texture_worst',\n                           'concave points_worst', 'concavity_mean', 'compactness_worst'], axis=1)","3af9c3e4":"plt.figure(figsize=(20,10)) \nsns.heatmap(new_dataset.corr(), annot=True)\nplt.show()","f56e8dd1":"X = new_dataset.drop(['diagnosis'], axis=1)\ny = new_dataset['diagnosis']","5ce1b6bd":"X.head()","062fabb2":"# Split dataset into 70% train, 30% test\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=SEED)","d0c69646":"# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n\n# transform training data\nX_train_norm = norm.transform(X_train)\n\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)","be44001d":"DataFrame(X_train_norm).describe()","97a4712e":"# fit scaler on training data\nstdscale = StandardScaler().fit(X_train)\n\n# transform training data\nX_train_std = stdscale.transform(X_train)\n\n# transform testing dataabs\nX_test_std = stdscale.transform(X_test)","1fd2c596":"DataFrame(X_train_std).describe()","27eecf19":"# Instantiate individual classifiers\nlr = LogisticRegression(max_iter = 500, n_jobs=-1, random_state=SEED)\nknn = KNN()\ndt = DecisionTreeClassifier(random_state=SEED)\nsvc = SVC(kernel='rbf', probability = True, random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)\n\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n('K Nearest Neighbours', knn),\n('SVM', svc),\n('Random Forest Classifier', rf),\n('Decision Tree', dt)]              ","e3e14144":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","f4f0ff08":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_norm, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_norm)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","869699e6":"# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train_std, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test_std)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","153680e0":"cm = confusion_matrix(y_test, lr.predict(X_test_std))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","0bbd6494":"cm = confusion_matrix(y_test, svc.predict(X_test_std))\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","8651063c":"From the above correlation plot, we can see there are many features which are highly correlated and might not be useful in our model. \nFor example, radius_mean is highly correlated with perimeter_mean, area_mean, radius_worst, perimeter_worst and area_worst. So we can drop them and use only radius_mean.","f4038d9f":"From above info., we can say that\n1. Different features have different mean values, standard deviation, min and max values. Some machine learning models don't perform well if the values are not standardized or normalized. So we will see later how this impacts.\n2. We really don't understand these features and their meaning properly. We might not need to.\n3. We don't know if all the features will be useful in determing the class label. There might be which features which are highly correlated and we don't need them. We will find that out.","8eab2956":"### Creating a test set and a training set\n\nSince this data set is not ordered, we will to do a simple 70:30 split to create a training data set and a test data set.","760e3b34":"From the new_dataset heatmap, we can observe that we have removed most of the highly correlated features. ","3819d5f9":"We create a new_dataset dataframe, so that we can compare it later with the original feature dataset if there is a difference in performance between the model predictions using these datasets. ","5d2220a1":"From the above accuracy scores, we can observe the following:\n1. DecisionTree and RandomForestClassifier are insensitive to feature scaling.\n2. LinearRegression, KNN and SVM are sensitive to feature scaling.\n3. SVM and LogisticRegression models gives us the highest accuracy.\n\nPlease find the reason behind this from the this article, which explains it very nicely\n\n[feature-scaling-machine-learning-normalization-standardization](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/)","86f03a78":"From above, we can see that after standardizing the data all the columns have standard deviation of 1.","5de05010":"First start by analyzing the breast cancer data. Checking how many features and rows of data we have.","afcfcea3":"Most of the times, our dataset will contain features highly varying in magnitudes, units and range. \nBut since, most of the machine learning algorithms use Eucledian distance between two data points in their computations. \nWe need to bring all features to the same level of magnitudes. This can be achieved by scaling. \nThis means that you\u2019re transforming your data so that it fits within a specific scale, like 0\u2013100 or 0\u20131.","42ef3d6a":"From above, we can observe that\n1. There are 33 cols and 569 data rows.\n2. A feature\/col name **Unnamed: 32**, which contain all null values. So we can drop it. \n3. The **id** column are all uniques values, which won't be of any use.\n4. **diagnosis** col is our class label.","8a15a643":"### Models prediction without any normalization or standardization","94bb2506":"### Models prediction with Standardized data","5fa6bb42":"# Model Selection","3b95a963":"# Data Visualization","5e254bce":"Lets find out the distribution of Malignant and Benign data.","eb89b74e":"# Feature Scaling","924be44c":"# Data Analysis","8dd0c9dd":"### Models prediction with Normalized data","4016c19d":"From the class label values M and B, we know (M) Malignant means a person being diagnoed with Cancer and (B) Benign means a person not being diagnosed with Cancer. We will map these values to 1 and 0 respectively.","b52eef15":"# Introduction\nIn this notebook, I tried to focus on finding the Best Machine Learning (ML) model for Breast Cancer Dataset.","3ab6c5a2":"From below plot we can say that as the **area_mean** and **radius_mean** values increase their is a higher chance a female being diagnosed with Cancer.","b02add15":"### Standardize the data","bce20d2a":"From above, we can see that after normalizing the data all the columns have min and max values between 0 and 1 respectively.","b6b51979":"### Normalize the data","37bc61df":"From the above confusion matrix, we can observe that both svc and lr models predict only 1 value incorrect. ","584dfd7e":"Similarly, from below plot we can say that as the **radius_worst** and **radius_mean** values increase their is a higher chance a female being diagnosed with **Cancer**."}}