{"cell_type":{"3fb6b9e0":"code","764daac2":"code","cf6f83f2":"code","8647da31":"code","b48d092b":"code","0b8b0c95":"code","c451f064":"code","e07df383":"code","dff5c1d5":"code","f939107b":"code","6ac3ea06":"code","fc67f151":"code","2da78d26":"code","279600bc":"code","64cb971d":"code","31a4add6":"markdown","221831d8":"markdown","9c3a3930":"markdown","630e9084":"markdown","7839d876":"markdown","2c8400ef":"markdown","f5eabfc6":"markdown","73d39a16":"markdown","9966542e":"markdown","22d93d29":"markdown"},"source":{"3fb6b9e0":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","764daac2":"import pandas as pd\nmatrix = pd.read_csv(\"..\/input\/mean-encodings-predict-future-sales\/mean_encodings.csv\")\n","cf6f83f2":"def downcastColumns(df):\n    for column in df.columns.values:\n        if df[column].dtype == 'float64' or df[column].dtype == 'int64':\n            df[column] = df[column].astype('float32')","8647da31":"downcastColumns(matrix)","b48d092b":"def expandingMeanEncodings(train,train_new,column,target):\n    cumsum = train.groupby(column)[target].cumsum() - train[target]\n    cumcnt = train.groupby(column).cumcount()\n\n    train_new[column+'_target_enc'] = cumsum \/ cumcnt\n    train_new[column+'_target_enc'].fillna(train[target].mean(), inplace=True) ","0b8b0c95":"def calculateMapMeanEncodings(train,val,train_new,val_new,column,target):\n    target_mean = train.groupby(column)[target].mean()\n    train_new[column+'_target_enc'] = train_new[column].map(target_mean)\n    val_new[column+'_target_enc'] = val_new[column].map(target_mean)\n    val_new[column+'_target_enc'].fillna(train[target].mean(), inplace=True) \n","c451f064":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nimport statistics, gc, copy\nimport numpy as np\nscores = []\n\n#initilize variables\ntarget_col = 'item_cnt_month'\nfeatures_to_consider = ['item_id','item_category_id','type_code','city_code','subtype_code','shop_id']\n\nfor cur_block_num in tqdm([32,33]):\n    \n    print(cur_block_num,\"current block number\")\n    #split data into train and val\n    train = matrix[matrix['date_block_num'] <cur_block_num]\n    val = matrix[matrix['date_block_num'] == cur_block_num]\n    train_new = copy.deepcopy(train)\n    val_new = copy.deepcopy(val)\n    \n    \n    \n    for feature in features_to_consider:\n        #calculate mean encodings on train and map to val and train\n        calculateMapMeanEncodings(train,val,train_new,val_new, feature,target_col)\n    \n        #expanding mean encodings on train\n        expandingMeanEncodings(train,train_new,feature,target_col)\n        \n    \n    #validate model\n    _X_train = train_new[[column for column in train_new.columns.values if column not in ['date_block_num','type_code','item_category_id','item_id','city_code',\n                                                                                          'subtype_code','shop_id','item_cnt_month']]]\n    _X_val = val_new[[column for column in train_new.columns.values if column not in ['type_code','item_category_id','item_id','city_code','subtype_code',\n                                                                                      'shop_id','item_cnt_month','date_block_num']]]\n    _y_train = train_new[target_col].values\n    _y_val = val_new[target_col].values\n    \n    \n    model = XGBRegressor(\n        max_depth=12,\n        n_estimators=25,\n        min_child_weight=40, \n        colsample_bytree=0.5, \n        subsample=0.7, \n        eta=0.1,    \n        n_jobs = -1,\n    seed=42,tree_method=\"approx\")\n\n    model.fit(_X_train, _y_train, eval_metric=\"rmse\", eval_set=[(_X_train, _y_train), (_X_val, _y_val)], verbose=True, \n            early_stopping_rounds = 10)\n    val_pred_ = model.predict(_X_val)\n    train_pred_ = model.predict(_X_train)\n    print('Validation root mean square for XGBoost is %f' % mean_squared_error(_y_val, val_pred_,squared=False))\n    print('Train root mean square for XGBoost is %f' % mean_squared_error(_y_train, train_pred_,squared=False))\n    scores.append(mean_squared_error(_y_val, val_pred_,squared=False))\n    \n    \n    del train, val, train_new, val_new, _X_train, _X_val, _y_train, _y_val\n    gc.collect()\n    \nprint(\"mean score for cross validation is %f \" % statistics.mean(scores))\nprint(\"worst of scores for cross validation is %f \" % max(scores))\nprint(\"best of scores for cross validation is %f \" % min(scores))\n\n","e07df383":"def expandingMeanEncodings(train,column,target,new_column):\n    cumsum = train.groupby(column)[target].cumsum() - train[target]\n    cumcnt = train.groupby(column).cumcount()\n\n    train[new_column] = cumsum \/ cumcnt\n    train[new_column].fillna(train[target].mean(), inplace=True) ","dff5c1d5":"def calculateMapMeanEncodings(train,val,column,target,new_column):\n    target_mean = train.groupby(column)[target].mean()\n    train[new_column] = train[column].map(target_mean)\n    val[new_column] = val[column].map(target_mean)\n    val[new_column].fillna(train[target].mean(), inplace=True) \n","f939107b":"import sys\ndef kFoldMeanEncodings(train,column,target,new_column):\n    kf = KFold(n_splits = 5, shuffle = False)\n    train[new_column] = train[target].mean()\n    for tr_ind, val_ind in kf.split(train):\n        X_tr = train.iloc[tr_ind].copy()\n        X_val = train.iloc[val_ind].copy()\n        means = X_val[column].map(X_tr.groupby(column)[target].mean())\n        X_val[new_column] = means\n        train.iloc[val_ind] = X_val\n        del X_tr,X_val\n        gc.collect()\n    train[new_column].fillna(train[target].mean(), inplace = True)\n        ","6ac3ea06":"def looMeanEncodings(train,column,target,new_column):\n    target_sum = train.groupby(column)[target].sum()\n    target_count = train.groupby(column)[target].count()\n    train[column + target+'_sum'] = train[column].map(target_sum)\n    train[column + target+'_count'] = train[column].map(target_count)\n\n    train[new_column] = (train[column + target+'_sum'] - train[target]) \/ (train[column + target+'_count'] - 1)\n    train[new_column].fillna(train[target].mean(), inplace = True)\n","fc67f151":"def smoothMeanEncodings(train,column,target,new_column,alpha):\n    target_mean = train.groupby(column)[target].mean()\n    target_count = train.groupby(column)[target].count()\n\n    train[column + target+'_mean'] = train[column].map(target_mean)\n    train[column + target+'_count'] = train[column].map(target_count)\n\n    train[new_column] = (train[column + target+'_mean'] *  train[column + target+'_count'] + train[target].mean() * alpha) \/ (alpha + train[column + target+'_count'])\n    train[new_column].fillna(train[target].mean(), inplace=True)\n","2da78d26":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom xgboost import plot_importance\nfrom tqdm import tqdm\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport statistics, gc, sys\nimport numpy as np\nscores = {}\n\n#initilize variables\ntarget_col = 'item_cnt_month'\nfeatures_to_consider = ['item_id','item_category_id','type_code','city_code','subtype_code','shop_id']\ncorrcoefs = pd.DataFrame(columns = ['Cor'])\n\nfor cur_block_num in tqdm([33]):\n    start_time = time.time()\n    print('---Start applying mean-encodings---')\n    #split data into train and val\n    train = matrix[matrix['date_block_num'] <cur_block_num].copy(deep=True)\n    val = matrix[matrix['date_block_num'] == cur_block_num].copy(deep=True)\n    \n    \n    for feature in features_to_consider:\n        #calculate mean encodings on train and map to val and train\n        calculateMapMeanEncodings(train,val,feature,target_col,feature+'_expanding_target_enc')\n        #1. expanding mean encodings on train\n        expandingMeanEncodings(train,feature,target_col,feature+'_expanding_target_enc')\n        corrcoefs.loc[feature+'_expanding_target_enc'] = np.corrcoef(train[target_col],train[feature+'_expanding_target_enc'])[0][1]\n        \n    \n        #calculate mean encodings on train and map to val and train\n        calculateMapMeanEncodings(train,val, feature,target_col,feature+'_kfold_target_enc')\n        #2. expanding mean encodings on train\n        kFoldMeanEncodings(train,feature,target_col,feature+'_kfold_target_enc')\n        corrcoefs.loc[feature+'_kfold_target_enc'] = np.corrcoef(train[target_col],train[feature+'_kfold_target_enc'])[0][1]\n        \n        #calculate mean encodings on train and map to val and train\n        calculateMapMeanEncodings(train,val, feature,target_col,feature+'_loo_target_enc')\n        #3. leave one out mean encodings on train\n        looMeanEncodings(train,feature,target_col,feature+'_loo_target_enc')\n        corrcoefs.loc[feature+'_loo_target_enc'] = np.corrcoef(train[target_col],train[feature+'_loo_target_enc'])[0][1]\n        \n        #calculate mean encodings on train and map to val and train\n        calculateMapMeanEncodings(train,val, feature,target_col,feature+'_smooth_target_enc')\n        #4. smooth mean encodings on train\n        smoothMeanEncodings(train,feature,target_col,feature+'_smooth_target_enc',100)\n        corrcoefs.loc[feature+'_smooth_target_enc'] = np.corrcoef(train[target_col], train[feature+'_smooth_target_enc'])[0][1]\n    print('---Finish mean-encodings, elapsed time in min: %0.2f---'%((time.time() - start_time)\/60))\n    for method in ['kfold','expanding','loo','smooth']:\n        print('---Start modelling using %s mean encoding---'%(method))\n        start_time = time.time()\n        #validate model\n        _X_train = train[[column for column in train.columns.values if method in column and column not in ['date_block_num','type_code','item_category_id',\n                                                                                                           'item_id','city_code','subtype_code','shop_id',\n                                                                                                           'item_cnt_month']]]\n        _X_val = val[[column for column in train.columns.values if method in column and column not in ['type_code','item_category_id','item_id','city_code',\n                                                                                                       'subtype_code','shop_id','item_cnt_month','date_block_num']]]\n        _y_train = train[target_col].values\n        _y_val = val[target_col].values\n\n\n        model = XGBRegressor(\n            max_depth=12,\n            n_estimators=25,\n            min_child_weight=40, \n            colsample_bytree=0.5, \n            subsample=0.7, \n            eta=0.1,    \n            n_jobs = -1,\n        seed=42,tree_method=\"approx\")\n\n        model.fit(_X_train, _y_train, eval_metric=\"rmse\", eval_set=[(_X_train, _y_train), (_X_val, _y_val)], verbose=True)\n        val_pred_ = model.predict(_X_val)\n        train_pred_ = model.predict(_X_train)\n        scores[method] = {'train': model.evals_result()['validation_0']['rmse'], \n                          'test': model.evals_result()['validation_1']['rmse']}        \n        del _X_train, _X_val, _y_train, _y_val\n        gc.collect()\n        print('---Finish modelling using %s, elapsed time in min: %0.2f---'%(method,(time.time() - start_time)\/60))\n    \n    del train, val\n    gc.collect()\n    \n\n\n","279600bc":"import matplotlib.pyplot as plt\n\n\nepochs = 25\nx_axis = range(0, epochs)\n\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, scores['kfold']['train'], label='k_fold train error')\nax.plot(x_axis, scores['expanding']['train'], label='expanding train error')\nax.plot(x_axis, scores['loo']['train'], label='loo train error')\nax.plot(x_axis, scores['smooth']['train'], label='smooth train error')\nax.legend(loc=0, prop={'size': 8})\nplt.ylabel('RMSE')\nplt.xlabel('Number of Epochs')\nplt.title('XGBoost Performance Change')\nplt.show()\n\nx_axis = range(0, epochs)\n\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, scores['kfold']['test'], label='k_fold test error')\nax.plot(x_axis, scores['expanding']['test'], label='expanding test error')\nax.plot(x_axis, scores['loo']['test'], label='loo test error')\nax.plot(x_axis, scores['smooth']['test'], label='smooth test error')\nax.legend(loc=0, prop={'size': 8})\nplt.ylabel('RMSE')\nplt.xlabel('Number of Epochs')\nplt.title('XGBoost Performance Change')\nplt.show()","64cb971d":"corrcoefs","31a4add6":"Correlation values show the correlation between the target and the respective features. The higher the value, a stronger linear relationship exists between them. For example, it may be the best option to use expanding mean encodings for item_id, as it has the highes correlation value.","221831d8":"## Navigation\n1. [Motivation](#motivation)\n2. [Load dataset](#load-dataset) \n3. [Apply mean encodings](#apply-mean-encodings)\n4. [Comparison of different regularization techniques](#different-regularization-techniques)\n","9c3a3930":"## 2.Load dataset\n<a id='load-dataset'><\/a>","630e9084":"## 1.Motivation\n<a id='motivation'><\/a>","7839d876":"## 3.Apply mean encodings\n<a id='apply-mean-encodings'><\/a>","2c8400ef":"## 4.Comparison of different regularization techniques\n<a id='different-regularization-techniques'><\/a>","f5eabfc6":"todo: Apply same operation above without any early stopping so that every method has the same number of epochs","73d39a16":"### Some remarks\n- It should be noted that mean encoding methods are compared based on the constructed features of the respective methods. It is very likely that a model with a better performace would have been built using best performed feature of different methods.\n- The performance of the built model could be subject to change, when different parameters of the base model are used.\n","9966542e":"Expanding mean encoding works very well, followed by smoothing and k fold methods. ","22d93d29":"Encoding categorical features with mean target value is a popular and useful encoding methodology for especially tree-based models. The method calculates mean target value for each categorical feature (regression tasks) or calculates the likelihood of a point to belong to a class (classification tasks).\n\nThis methodology is very similar to label encoding in a way that both assigns labels to categorical feautres. While these labels are random in label encoding, they are correlated with target in mean encoding, which helps machine learning models to use these features more efficiently.\n\nIn this study, I am going to implement different types of regularization techniques used in mean encodings and show the performance differences between them. "}}