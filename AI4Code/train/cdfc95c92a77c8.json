{"cell_type":{"8314b9c7":"code","ebd224d3":"code","753ca5d6":"code","248db168":"code","bcf7932a":"code","82b5e519":"code","f890ebfe":"code","bb340854":"code","dea053e3":"code","d77c4c08":"code","61bb8789":"code","f7c5b119":"code","3b0480c0":"code","cdd2c490":"code","edd0eb43":"code","502ac2b1":"code","c5d21b13":"code","54470211":"code","efcd1b54":"code","adf5b1ec":"code","d8ab25ee":"code","f29f47c2":"code","82f527aa":"code","f930923a":"code","0c6f5dcd":"code","fe31d512":"code","b5718499":"code","d8beef9f":"code","d74a91ae":"code","0a0fec87":"code","d7fbe33f":"code","41dbbf2b":"code","ad42e29b":"code","a6300d68":"code","d78a42b7":"code","4a0631cb":"code","2a430448":"markdown","2c8a0bc1":"markdown","1bb55108":"markdown","451da796":"markdown","a352b421":"markdown","6a62bd0f":"markdown","9fc2dd00":"markdown","50bad8f9":"markdown","e4c21a58":"markdown","df9fe9c0":"markdown","3476b645":"markdown","d4dfcb80":"markdown","70946b8c":"markdown","9fa7a016":"markdown","c1d8312b":"markdown","59791b41":"markdown","8dfb146b":"markdown","d46c281d":"markdown","569ad776":"markdown","85ad11d2":"markdown","698a028a":"markdown","fc10cd2a":"markdown","94a33133":"markdown","193f168e":"markdown","dd02e93a":"markdown","888665e1":"markdown","1405da20":"markdown","ecaed83e":"markdown","63e596c5":"markdown","ef13640a":"markdown","ce8ccec7":"markdown","4c5bab61":"markdown","563d3cf2":"markdown"},"source":{"8314b9c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","ebd224d3":"email_data = pd.read_csv('..\/input\/spam.csv', encoding = 'latin-1' )","753ca5d6":"email_data.head(20)","248db168":"email_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis  = 1, inplace = True)","bcf7932a":"email_data.head(10)","82b5e519":"email_data = email_data.rename(columns = {\"v1\": \"label\", \"v2\": \"text\"})","f890ebfe":"email_data.head()","bb340854":"sns.countplot(email_data['label'], label = \"Count of the Labels\")","dea053e3":"email_data['length'] = email_data['text'].apply(len)","d77c4c08":"email_data.head()","61bb8789":"email_data['length'].hist(bins = 50, color = 'g')","f7c5b119":"#Let's import the needed libraries\nimport string\nfrom nltk.corpus import stopwords","3b0480c0":"def clean_review(review):\n    remove_punctuation = [word for word in review if word not in string.punctuation]\n    join_characters = ''.join(remove_punctuation)\n    remove_stopwords = [word for word in join_characters.split() if word.lower() not in stopwords.words('english')]\n    cleaned_review = remove_stopwords\n    return cleaned_review","cdd2c490":"#The original data\nemail_data['text'][4]","edd0eb43":"#Applying the cleaning function\nclean_review(email_data['text'][4])","502ac2b1":"from sklearn.feature_extraction.text import CountVectorizer\n#Define the clean_review function as an argument of the CountVectorizer.\ncount_vectorizer = CountVectorizer(analyzer = clean_review)\n#Fit the countvectorizer to the dataset\nemail_countvec = count_vectorizer.fit_transform(email_data['text'])","c5d21b13":"print(count_vectorizer.get_feature_names())","54470211":"email_countvec.shape","efcd1b54":"#Let's have a quick look at the data once again\nemail_data.head()","adf5b1ec":"#Using LabelEncoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nemail_data['label'] = le.fit_transform(email_data['label'])","d8ab25ee":"#Assign X and y\nX = email_countvec\ny = email_data['label']","f29f47c2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","82f527aa":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB() #Using the default parameters\nclassifier.fit(X_train, y_train)","f930923a":"#Making predictions to the Test Set\ny_predictions = classifier.predict(X_test)","0c6f5dcd":"from sklearn.metrics import confusion_matrix, classification_report\n\ncm = confusion_matrix(y_test, y_predictions)\n\n#Taking a look at the Confusion Matrix with a Heatmap\nsns.heatmap(cm, annot=True)","fe31d512":"print(classification_report(y_test, y_predictions))","b5718499":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfid = TfidfVectorizer()\ntfidvec = tfid.fit_transform(email_data['text'])","d8beef9f":"tfidvec.shape","d74a91ae":"print(tfid.get_feature_names())","0a0fec87":"print(tfidvec[:,:])","d7fbe33f":"#Assigning X2 and y2\nX2 = tfidvec\ny2 = email_data['label']","41dbbf2b":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 0.25)","ad42e29b":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier2 = MultinomialNB() #Using the default parameters\nclassifier2.fit(X_train2, y_train2)","a6300d68":"#Making predictions to the Test Set\ny_predictions2 = classifier2.predict(X_test2)","d78a42b7":"from sklearn.metrics import confusion_matrix, classification_report\n\ncm2 = confusion_matrix(y_test2, y_predictions2)\n\n#Taking a look at the Confusion Matrix with a Heatmap\nsns.heatmap(cm2, annot=True)","4a0631cb":"print(classification_report(y_test2, y_predictions2))","2a430448":"Let's see if the function that we define is working.","2c8a0bc1":"Before Initializing and Training the model. We will first split the data into training and testing data.","1bb55108":"Since we will be using the label column as our dependent variable. We need to encode it into binary variables.","451da796":"# Using TF-IDF","a352b421":"# Applying Count Vectorizer in the Dataset","6a62bd0f":"We can see that the dataset is not balanced. There are more data that are classified as ham other than spam. ","9fc2dd00":"We will be using Naive Bayes Classifier as our model.","50bad8f9":"## Problem Statement","e4c21a58":"Before the data can be fed to a Machine Learning model,it is required that we clean the data first for the model to understand and process each review well.","df9fe9c0":"Taking a look at the distribution of the text lengths","3476b645":"## Visualizing the Dataset","d4dfcb80":"# Classification of Text Messages","70946b8c":"Let's compute the length of the reviews to add as a new column.","9fa7a016":"Let's drop the columns with no values.","c1d8312b":"Here's the shape of the new Vectorized Data.","59791b41":"We will be applying count vectorizer to the dataset to convert the reviews into a matrix of token counts.","8dfb146b":"After training the Model we will proceed into evaluating its performance. Using confusion matrix and the classification report","d46c281d":"We will define a function that will remove all the punctuations and all the common words.","569ad776":"From the histogram, we can see that most of the reviews are about have less than 50 words.","85ad11d2":"The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.","698a028a":"### Splitting the Data into Training and Testing","fc10cd2a":"Taking a look at the Feature names","94a33133":"## Training the Classification Model","193f168e":"## Importing the Libraries and the Dataset","dd02e93a":"## Data Preprocessing \/ Data Cleaning\n","888665e1":"We will use the function that we created later. ","1405da20":"We will be using 75% of the data for training and 25% for testing","ecaed83e":"## Evaluating the Model","63e596c5":"We will be using this dataset to build a machine learning model to classify if a message is ham (Legitimate) or spam. We will be using some NLP techniques to preprocess the data and use seaborn to do some visualization.","ef13640a":"## Model Training","ce8ccec7":"From only 3 columns, we can see now that every word in the whole dataset have its own column.","4c5bab61":"### We can see that by using Term Frequency - Inverse Document Frequency, we can improve the performance of our model.","563d3cf2":"## The model actually produces a pretty good results with just using some of the basic techniques in NLP."}}