{"cell_type":{"61c37cc4":"code","2350df0c":"code","b8347f3f":"code","c8e43cf8":"code","383e7c2c":"code","11cf931b":"code","58d7e701":"code","6b0cff83":"code","800ce900":"code","88b18400":"code","7a1b5ee0":"code","d95579bd":"code","daa2c54e":"code","e79fb5a2":"code","841e0ec5":"code","d4d0135c":"code","24578bd6":"code","688c9510":"code","e4de5348":"code","83503eaa":"code","e31dfd63":"code","d8cb396e":"code","5ffe9071":"code","40c0d1df":"code","bc1daf19":"code","89d06f88":"code","aa62f424":"code","67427187":"code","1c6a3ecb":"code","5b78c935":"code","70ffdd3a":"code","36adbb3d":"code","46fd5401":"code","d2558d36":"code","cef9e162":"code","02cc146b":"code","41a1566e":"code","5c543772":"code","f391441f":"code","fafa3ff4":"code","b4f45205":"code","9f207646":"code","2194c05b":"code","cffd3b39":"code","62d0026e":"code","1c3366f1":"code","4ba53663":"code","30f06590":"code","33ac7713":"code","f18a2bc9":"code","813e591e":"code","b6e33789":"code","a1fa60e8":"code","1dcc0b69":"code","c8ff35bb":"code","50b2f9f7":"code","a3e6f642":"code","aac9ded8":"code","f7dc6637":"code","1ac47fb2":"code","370515a2":"code","09d3b03a":"code","77f29261":"code","24e0e471":"code","1964c8f0":"code","64a5558f":"code","58a05b7b":"code","07d18f39":"code","9f23d3fe":"code","abeb8ad1":"code","d5888012":"code","4c59f6a5":"code","6f983641":"code","09123b3e":"code","2ffe1c8a":"code","9934ae77":"code","34416007":"code","d7d38adc":"code","a908bdb1":"code","9ccbe457":"code","e239db38":"code","13ffb09a":"code","c301c9cb":"code","73b38c3e":"code","6d90cf44":"code","4f933547":"code","1c936e8a":"code","34693a17":"code","524d3a2e":"code","91347888":"code","439966ce":"code","791a941f":"code","128bbe3d":"code","5c738269":"code","feb2c8d9":"code","5aadc305":"code","617c8c2c":"code","79548aec":"code","d7fc8475":"code","ec8e2018":"code","e95a991b":"code","dcb51fe2":"code","6a9907f1":"code","edc11c17":"code","236bdfcc":"code","868c02fc":"code","a3f96061":"code","985f5601":"code","20ced55c":"code","f3e22df5":"code","7b91e28a":"code","6ffaaecb":"code","e3378ee2":"code","af32148a":"code","59deca2e":"code","14b3a210":"code","49be151a":"code","99eaa29f":"code","d0cfabc6":"code","7e22e5e4":"code","72a9dc82":"code","1925e70f":"code","f878a185":"code","bd6ba68b":"code","cea0b290":"code","99f1963f":"code","4e3c02ab":"code","6e52de2c":"code","0a01735b":"code","e255f6be":"code","f07eae02":"code","2925418c":"code","ff3f73f8":"code","f7a58bc9":"code","66cec870":"code","48132a82":"code","57ab1ad0":"code","697f28f6":"code","ef9c0edd":"code","31792e14":"code","0955788a":"code","d5dd50ab":"code","c36fdc23":"code","54bf6064":"code","0b472b08":"code","13dfb3e2":"code","f10eef97":"markdown","f5037822":"markdown","51fdb4db":"markdown","150be978":"markdown","39d71360":"markdown","244f3abe":"markdown","31fbb812":"markdown","484d12bc":"markdown","bf7a748a":"markdown","9763626d":"markdown","47f28fc6":"markdown","e0e4518a":"markdown","75cc815a":"markdown","467e264d":"markdown","8fd3be4f":"markdown","06512470":"markdown","3456dd14":"markdown","8bae3b69":"markdown","4bce586b":"markdown","2063d708":"markdown","208eccde":"markdown","73772607":"markdown","bd672f3a":"markdown"},"source":{"61c37cc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2350df0c":"#Data Source\n#https:\/\/www.kaggle.com\/nehalbirla\/vehicle-dataset-from-cardekho\/?select=Car+details+v3.csv\n'''\n## Task Details\nThe Objective - deeply analyze the dataset, by experimenting with a variety of ML models and technics\n--- Main Tasks ---\n\n1. Cleaning and organizing the data\n2. Get an initial understanding of relations in the data \n3. Clustering \u2013 for categorizing data, set \u201cstrong\u201d features\n4. Regression \u2013 seeking the best model to determine \u201cselling price\u201d  \n''' ","b8347f3f":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.cluster import MiniBatchKMeans\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")","c8e43cf8":"import os\nos.getcwd()","383e7c2c":"df = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv').drop('torque',axis = 1 ).dropna(how = 'all')\n\ndf.drop_duplicates(subset = ['name','year','selling_price', 'km_driven', 'seller_type','mileage'], keep = 'first')\ndf.sample(4)","11cf931b":"df.owner.unique()\n# sum(df.owner.str.contains('First | Second | third', case = False))\ndf['owner'].replace({'First Owner': 1,'Second Owner':2, 'Third Owner':3,'Fourth & Above Owner':4, 'Test Drive Car':6}, inplace = True)","58d7e701":"df.owner.unique()","6b0cff83":"# set the car age feature \n\nimport datetime as dt\ndt.datetime.today().year\ndf['car_age'] = dt.datetime.today().year - df.year \ndf['car_age']\ndf.drop('year', axis = 1 ) ","800ce900":"df.shape","88b18400":"df.info()","7a1b5ee0":"# set features as numeric\ndf['max_power'] = df['max_power'].str.strip('bhp')\ndf['max_power'] = df['max_power'].str.replace(\" \", \"\")\ndf['max_power'].sort_values(ascending = False).head()#.astype('float64')\ndf['max_power'] = pd.to_numeric(df['max_power'],errors='coerce')\n\ndf['engine'] = df['engine'].str.replace(\"CC\", \"\")\ndf['engine'] = pd.to_numeric(df['engine'], errors= 'coerce')\ndf['mileage'] = df['mileage'].str.replace(\"kmpl\", \"\")\ndf['mileage'] = pd.to_numeric(df['mileage'], errors= 'coerce')\ndf['mileage'].sample(5)\n","d95579bd":"# Change name to brand\ndf['brand'] = df.name.str.split(' ').str[0]\ndf.drop(['year','name'], axis =1, inplace = True)","daa2c54e":"# drop nulls\ndf.dropna(inplace = True)\n# df.isnull().any()\ndf.isnull().sum()","e79fb5a2":"df.info()","841e0ec5":"# Shuffling the data\ndf.sample(frac = 1).reset_index(drop = True)","d4d0135c":"df.describe()","24578bd6":"import seaborn as sns\nfig, ax = plt.subplots(figsize = (10,5))\nsns.heatmap(df.corr(), annot = True )","688c9510":"plt.boxplot(df[['max_power','car_age', 'engine']])\nplt.show()\ndf[['max_power','car_age', 'engine']].describe()","e4de5348":"fig, ax  = plt.subplots(figsize = (10,5)) \nplt.scatter(df.selling_price, df.max_power)\nplt.ylabel('Max Power')\nplt.xlabel('Selling Price')\nplt.show()","83503eaa":"print('seller_type: ',df.seller_type.unique())\nprint('transmission: ',df.transmission.unique())\nprint('fuel: ',df.fuel.unique())","e31dfd63":"from sklearn import preprocessing as pre\ndf.transmission = pre.LabelEncoder().fit_transform(df.transmission)\ndf.fuel = pre.LabelEncoder().fit_transform(df.fuel)\n","d8cb396e":"print('transmission: ',df.transmission.unique())\nprint('fuel: ',df.fuel.unique())","5ffe9071":"# One Hot Encoding - (for better nameing...)\n\ndf_temp = pd.get_dummies(df.seller_type)\n\nprint(df_temp.iloc[:,0:2].shape  , df.shape)\ndf  = pd.concat([df,df_temp.iloc[:,0:2]], axis = 1)\nprint(df.shape)\n\ndel df_temp\ndf.drop(['seller_type'],axis = 1, inplace= True)","40c0d1df":"df  = pd.get_dummies(df, columns= ['brand'])\ndf.shape","bc1daf19":"from sklearn.metrics import silhouette_score\nmodel = KMeans(n_clusters=3,max_iter=1000, algorithm = 'full', tol=0.0001 ).fit(df)\nmodel.labels_","89d06f88":"# The silhouette_score gives the average value for all the samples.\n# This gives a perspective into the density and separation of the formed\n# clusters\nsilhouette_avg = silhouette_score(df,model.labels_)\nprint(\"For n_clusters =\", model.n_clusters,\n      \"The average silhouette_score is :\", silhouette_avg)","aa62f424":"df_num_only = df.iloc[:,np.r_[0:2,5:9]]\ndf_full_feature = df\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf_std = sc.fit_transform(df_full_feature)\ndf_std_num = sc.fit_transform(df_num_only)\nprint('std values , all features ')\nprint(df_std)\nprint('\\n std values , only numeric features ')\nprint(df_std_num)","67427187":"from sklearn.model_selection import ParameterGrid","1c6a3ecb":"best_score = -1\nn_Kmeans =list(range(3,11,1))\nn_Kmeans","5b78c935":"best_score = -1\nbest_no_Kmeans = 0\ninter_no = 0\nfor i in n_Kmeans:\n    for j in [200, 1000, 100000]:\n        model = KMeans(n_clusters=i,max_iter= j , algorithm = 'full', tol=0.00001 ).fit(df_std_num)\n        silhouette_avg = silhouette_score(df_std_num,model.labels_)\n        print(\"For n_clusters =\", model.n_clusters,'inter_no: ',model.max_iter,\"The average silhouette_score is :\", silhouette_avg)\n        if silhouette_avg > best_score:\n            best_score =  silhouette_avg\n            best_no_Kmeans = model.n_clusters\n            inter_no = model.max_iter","70ffdd3a":"print(\"Best n_clusters =\", best_no_Kmeans,'Max_iter: ',model.max_iter,\" | Best silhouette_score is :\", best_score)","36adbb3d":"model = KMeans(n_clusters =  best_no_Kmeans ,max_iter=1000).fit(df)\nKMeans_results = model.labels_","46fd5401":"len(KMeans_results)","d2558d36":"from sklearn.cluster import estimate_bandwidth\nestimate_bandwidth(df)","cef9e162":"model = MeanShift(bandwidth=estimate_bandwidth(df)).fit(df)","02cc146b":"model.labels_","41a1566e":"silhouette_score = silhouette_score(df, model.labels_)\nsilhouette_score","5c543772":"print('silhouette_score: ' , silhouette_score, ' | No. of clusters: ', len(set(model.labels_)))","f391441f":"len(set(model.labels_))","fafa3ff4":"from sklearn.cluster import MeanShift\nfrom sklearn.cluster import estimate_bandwidth\nmodel = MeanShift(bandwidth=estimate_bandwidth(df_std_num)).fit(df_std_num)","b4f45205":"from sklearn.metrics import silhouette_score\nsilhouette_score = silhouette_score(df_std_num, model.labels_)\nsilhouette_score","9f207646":"MeanShift_results = model.labels_","2194c05b":"print('silhouette_score: ' , silhouette_score, ' | No. of clusters: ', len(set(model.labels_)))","cffd3b39":"set(model.labels_)","62d0026e":"n_clusters =np.arange(3,10,1)\nn_clusters","1c3366f1":"from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import Birch\nbest_score = -1\nbest_no_clusters = 0\nfor i in n_clusters:\n    model = Birch(n_clusters= i, threshold= 0.3).fit(df_std_num)\n    print(i, model.labels_)\n    silhouette_avg = silhouette_score(df_std_num,model.labels_)\n    print(\"For n_clusters =\", model.n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n    if silhouette_avg > best_score:\n        best_score =  silhouette_avg\n        best_no_clusters = model.n_clusters","4ba53663":"set(model.labels_)","30f06590":"silhouette_score(df_std,model.labels_)","33ac7713":"print('Best No. of clusters: ', best_no_clusters,\" | Best silhouette_score is :\", best_score)","f18a2bc9":"model = Birch(n_clusters= best_no_clusters, threshold= 0.3).fit(df_std_num)\nBirch_results = model.labels_\nlen(Birch_results)","813e591e":"set(model.labels_)","b6e33789":"#export base table\ndf.to_csv('Car_processed.csv', index= False)","a1fa60e8":"df_cl = df.copy()","1dcc0b69":"# carry 'Birch' features to nest step (regression)\n\ndf_cl['Birch'] = Birch_results\n#df_cl['MeanShift'] = MeanShift_results\n# df_cl['DBSCAN'] = DBSCAN_results\n#df_cl['KMeans'] = KMeans_results\ndf_cl","c8ff35bb":"df_cl.pivot_table(columns= 'owner', index= 'Birch', values = 'selling_price', aggfunc='mean')","50b2f9f7":"#export table with Clusters assigned\n\ndf_cl.to_csv('Car_processed_cl.csv', index= False)","a3e6f642":"ls","aac9ded8":"del df","f7dc6637":"df = pd.read_csv('Car_processed_cl.csv')","1ac47fb2":"df.shape","370515a2":"df.columns","09d3b03a":"header_names = df.columns","77f29261":"df1 = df[['selling_price', 'km_driven','mileage', 'engine', 'max_power', 'seats', 'car_age','owner']]\ndf1.head()\ndf2= df.drop(['selling_price', 'km_driven','mileage', 'engine', 'max_power', 'seats', 'car_age','owner'], axis=1)\ndf2.head()","24e0e471":"names = df1.columns\ndf1.sample(3)","1964c8f0":"from sklearn.preprocessing import StandardScaler as sc\n# sc = StandardScaler()\ndf1 = sc().fit_transform(df1)\n\ndf1 = pd.DataFrame(df1)","64a5558f":"df1.columns = names\ndf1","58a05b7b":"df = pd.concat((df1,df2), axis = 1)\ndf.head()","07d18f39":"# The Birch classifier \n\ntemp = pd.concat( [df2[['Birch']],df1],axis =1)\n\nbirch0 = temp.loc[temp.Birch ==0,:].drop(['Birch','km_driven'], axis =1)\nbirch1 = temp.loc[temp.Birch ==1,:].drop(['Birch','km_driven'], axis =1)\nbirch2 = temp.loc[temp.Birch ==2,:].drop(['Birch','km_driven'], axis =1)\n\nfig = plt.figure(figsize =(12, 8))\n \n# Creating axes instance\n# ax = fig.add_axes([0, 0, 1, 1])\nax = fig.add_subplot(111)\n\n# Creating plot\nbp1 = ax.boxplot(birch0)\nbp2 = ax.boxplot(birch1) \nbp3 = ax.boxplot(birch2,patch_artist=True)\n\n# bp1 = box_plot(example_data1, 'red', 'tan')\n\nfor median in bp1['medians']:\n    median.set(color ='Blue',\n               linewidth = 4)\nfor median in bp2['medians']:\n    median.set(color ='Orange',\n               linewidth = 4)\nfor median in bp3['medians']:\n    median.set(color ='Red',\n           linewidth = 4)\nplt.title('Birch clusters - Blue:0 , Orange:1, Red:2')\n\n# # fill with colors\n# colors = ['pink', 'lightblue', 'lightgreen']\n# for bplot in (bp1, bp2,bp3):\n#     for patch, color in zip(bplot['boxes'], colors):\n#         patch.set_facecolor(color)\n\n\n# show plot\nplt.show(bp1)\nplt.show(bp2)\nplt.show(bp3)\n","9f23d3fe":"fig, ax  = plt.subplots(figsize = (10,5)) # set the frame\nplt.scatter(df.car_age,df.selling_price)\nplt.title('Selling Price Vs. Age')\nplt.ylabel('selling_price')","abeb8ad1":"fig, ax  = plt.subplots(figsize = (10,5)) # set the frame\nplt.scatter(df.seats,df.max_power)\n\nplt.ylabel('selling_price')","d5888012":"#! pip install fitter\nfrom fitter import Fitter, get_common_distributions, get_distributions\nsns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'selling_price', kind='hist', bins = 100, aspect= 2)\nplt.title('Selling Price Distribution')\nplt.show()","4c59f6a5":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'car_age', kind='hist', bins = 50, aspect= 2)\nplt.title('car_age Distribution')\nplt.show()","6f983641":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'km_driven', kind='hist', bins = 100, aspect= 2)\nplt.title('km_driven Distribution')\nplt.show()","09123b3e":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'mileage', kind='hist', bins = 100, aspect= 2)\nplt.title('mileage Distribution')\nplt.show()","2ffe1c8a":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'max_power', kind='hist', bins = 100, aspect= 2)\nplt.title('engine Distribution')\nplt.show()","9934ae77":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'seats', kind='hist', bins = 100, aspect= 2)\nplt.title('seats Distribution')\nplt.show()","34416007":"sns.set_context('paper', font_scale= 1.5)\nsns.displot(data=df, x = 'car_age', kind='hist', bins = 100, aspect= 2)\nplt.title('car_age Distribution')\nplt.show()","d7d38adc":"for i in range(0,3):\n    df  = df.sample(frac= 1).reset_index(drop = True) \n    print('reshuffle no.: ',i)","a908bdb1":"import seaborn as sns\nfig, ax = plt.subplots(figsize = (8,4))\nsns.heatmap(df1.corr(),vmin= -0.3, vmax=- 0.6, annot = True)","9ccbe457":"import seaborn as sns\nfig, ax = plt.subplots(figsize = (8,4))\nsns.heatmap(df1.corr(), annot = True)","e239db38":"from sklearn.model_selection import train_test_split\n\nx = df[['max_power']]\ny = df[['selling_price']]\n\nx_train,x_test,y_train, y_test = train_test_split(x,y, test_size= 0.3) ","13ffb09a":"x_train.sample(5)\nprint(x_train.size, x_test.size, x_train.size+ x_test.size)","c301c9cb":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression().fit(x_train, y_train)","73b38c3e":"linear_model.score(x_train, y_train)","6d90cf44":"y_pred = linear_model.predict(x_test)\ny_pred","4f933547":"from sklearn.metrics import r2_score , mean_squared_error\nprint('Linear reg - selling price ~ max_power (one feature)')\nprint('r2_score: ', r2_score(y_test,y_pred))\nprint('MSE:', mean_squared_error(y_test, y_pred))","1c936e8a":"plt.scatter(x_test,y_test)\nplt.plot(x_test, y_pred, color = 'red')\nplt.title('Linear regression with one feature - selling price ~ max_power')\n\nplt.show()","34693a17":"x = df[['engine']]\ny = df[['selling_price']]\n\nx_train,x_test,y_train, y_test = train_test_split(x,y, test_size= 0.3) ","524d3a2e":"linear_model = LinearRegression(normalize=True).fit(x_train,y_train)","91347888":"y_pred = linear_model.predict(x_test)","439966ce":"print('Linear reg - selling price ~ engine (one feature)')\nprint('r2_score: ', r2_score(y_test,y_pred))\nprint('MSE:', mean_squared_error(y_test, y_pred))","791a941f":"fig , ax = plt.subplots()\nplt.scatter(x_test,y_test)\nplt.plot(x_test,y_pred, color= 'red')\nplt.title('Linear regression with one feature - selling price ~ engine')\nplt.show()","128bbe3d":"df1.sample(7)","5c738269":"X = df1.drop('selling_price', axis = 1)\nY = df1['selling_price']\nX\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n\nlinear_model2 = LinearRegression(normalize = True).fit(x_train,y_train)\n\ny_pred = linear_model2.predict(x_test)","feb2c8d9":"coef  = pd.Series(linear_model2.coef_, x_train.columns)\nprint(coef)","5aadc305":"print('Linear reg - selling price ~ all Numeric Featurs')\nprint('r2_score: ', r2_score(y_test,y_pred))\nprint('MSE:', mean_squared_error(y_test, y_pred))","617c8c2c":"plt.figure(figsize = (10,5))\nplt.plot(y_pred[0:300], label = 'pred')\nplt.plot(list(y_test[0:300]), label = 'test')\nplt.ylabel('selling_price')\nplt.title('Linear regression - selling price ~ All Numeric (300)')\nplt.legend()\nplt.show()","79548aec":"X = df.drop('selling_price', axis = 1)\nY = df['selling_price']\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\nlinear_model3 = LinearRegression(normalize=True).fit(x_train,y_train)\ny_pred = linear_model3.predict(x_test)\n\nprint('Linear reg - selling price ~ all Featurs')\nprint('r2_score: ', r2_score(y_test,y_pred))\nprint('MSE:', mean_squared_error(y_test, y_pred))","d7fc8475":"plt.figure(figsize = (10,5))\nplt.plot(y_pred[0:300], label = 'pred')\nplt.plot(list(y_test[0:300]), label = 'test')\nplt.ylabel('selling_price')\n#plt.legend()\nplt.title('Linear regression - selling price ~ All Features (300)')\nplt.show()","ec8e2018":"from sklearn.linear_model import Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV","e95a991b":"b = df2.loc[:,('fuel','transmission','Dealer','Individual','Birch')]\na = df1.drop(['km_driven','seats'],axis =1)\ndf_n = pd.concat([a,b],axis =1)\ndel a,b\ndf_n.columns","dcb51fe2":"X = df_n.drop('selling_price', axis =1)\nY = df_n['selling_price']\nx_train, x_test, y_train, y_test = train_test_split(X, Y , test_size= 0.3)","6a9907f1":"lasso_model = Lasso(alpha= 0.9).fit(x_train, y_train)\ny_pred = lasso_model.predict(x_test)","edc11c17":"param = {'alpha' : [float(i) for i in np.arange(0.01,1,0.1)]}\nparam","236bdfcc":"grid_search = GridSearchCV(Lasso(), param, cv=10, return_train_score=True)\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_params_","868c02fc":"lasso_model = Lasso(alpha=grid_search.best_params_['alpha']).fit(x_train, y_train)","a3f96061":"y_pred = lasso_model.predict(x_test)\ny_pred","985f5601":"print('MSE_score: ', mean_squared_error(y_test, y_pred))","20ced55c":"result_dict = {}\n\nresult_dict['lasso_model_MSE'] = mean_squared_error(y_test, y_pred)\nresult_dict\n\nplt.plot(y_test.values, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.legend()\nplt.show()","f3e22df5":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\n\ngrid_search = GridSearchCV(Ridge(), param, cv=10, return_train_score=True)\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_params_","7b91e28a":"grid_search.best_params_['alpha']","6ffaaecb":"Ridge()\nridge_model = Ridge(alpha=grid_search.best_params_['alpha'],normalize=True, max_iter=100000).fit(x_train, y_train)\ny_pred = ridge_model.predict(x_test)\ny_pred","e3378ee2":"def get_results(model_name):\n    print('MSE_score: ', mean_squared_error(y_test, y_pred))\n    result_dict[model_name] = mean_squared_error(y_test, y_pred)\n    plt.plot(y_test.values, label='Actual')\n    plt.plot(y_pred, label='Predicted')\n    plt.legend()\n    plt.show()\n    for key,value in result_dict.items():\n        print(key,': ',value)","af32148a":"get_results('Ridge_model_MSE')","59deca2e":"param = {}\nparam = {'alpha' : [float(i) for i in np.arange(0.5,1.7,0.3)]}\nparam['max_iter'] = [j for j in range(300,140400,40000)]\nparam['l1_ratio'] = [k for k in np.arange(0.2,1,0.2)]\nparam","14b3a210":"grid_search = GridSearchCV(ElasticNet(), param, cv=10, return_train_score=True)\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_params_","49be151a":"grid_search.best_params_['l1_ratio']","99eaa29f":"elastic_net_model = ElasticNet(alpha=grid_search.best_params_['alpha'] \\\n                               , max_iter=grid_search.best_params_['max_iter'],\\\n                               l1_ratio=grid_search.best_params_['l1_ratio']).fit(x_train, y_train)\ny_pred = elastic_net_model.predict(x_test)\nget_results('elastic_net_model_MSE')","d0cfabc6":"from sklearn.neighbors import KNeighborsRegressor as knn\n\nparam = {}\nparam['n_neighbors'] = [i for i in range(2,22,2)]\nparam","7e22e5e4":"grid_search = GridSearchCV(knn(),param, cv =10, return_train_score= True)\ngrid_search.fit(x_train,y_train)\nprint(grid_search.best_score_)\ngrid_search.best_params_['n_neighbors']","72a9dc82":"knn_model = knn(n_neighbors= grid_search.best_params_['n_neighbors']).fit(x_train,y_train)\ny_pred = knn_model.predict(x_test)\nget_results('knn_model_MSE')","1925e70f":"from sklearn.tree import DecisionTreeRegressor\n\nparam = {}\nparam['max_depth'] = [i for i in range(1,5)]\nparam","f878a185":"grid_search = GridSearchCV(DecisionTreeRegressor(), param, cv=10)\ngrid_search.fit(x_train, y_train)\ngrid_search.best_params_ , grid_search.best_score_, grid_search.best_estimator_","bd6ba68b":"model_decision_tree = grid_search.best_estimator_.fit(x_train,y_train)\ny_pred = model_decision_tree.predict(x_test)\nget_results('decision_tree_model_MSE')","cea0b290":"from sklearn.svm import SVR\n\nparam = {}\nparam['epsilon'] = [i for i in np.arange(0.02,0.7,0.1)]\nparam['C'] = [j for j in np.arange(0.1,1.2,0.3)]          # C is the penalty\nparam","99f1963f":"grid_search = GridSearchCV(SVR(), param, cv=10, return_train_score=True)\ngrid_search.fit(x_train, y_train)\ngrid_search.best_params_ , grid_search.best_estimator_","4e3c02ab":"model_SVR = SVR(kernel='linear', epsilon=grid_search.best_params_['epsilon'],\\\n                C=grid_search.best_params_['C']).fit(x_train,y_train)\ny_pred = model_SVR.predict(x_test)\nget_results('Support_Vector_Reg_model_MSE')","6e52de2c":"from sklearn.linear_model import Lars\n\nparam = {}\nparam['n_nonzero_coefs'] = [i for i in range(1,11)]\nparam","0a01735b":"grid_search = GridSearchCV(Lars(), param, cv=10, return_train_score=True)\ngrid_search.fit(x_train, y_train)\ngrid_search.best_params_ , grid_search.best_estimator_","e255f6be":"model_LARS = grid_search.best_estimator_.fit(x_train,y_train)\ny_pred = model_LARS.predict(x_test)\nget_results('Least_Angel_Reg_model_MSE')","f07eae02":"from sklearn.ensemble import RandomForestRegressor\n\nrnd_forest_reg = RandomForestRegressor(n_estimators=600, max_leaf_nodes=12, n_jobs=-1).fit(x_train, y_train)\n\ny_pred = rnd_forest_reg.predict(x_test)\n\nget_results('Random_Forest_model_MSE')","2925418c":"rnd_forest_reg.feature_importances_","ff3f73f8":"importent_features  = pd.DataFrame(rnd_forest_reg.feature_importances_, x_test.columns)\nimportent_features.columns =['importances']\nimportent_features = importent_features.sort_values('importances', ascending= False)","f7a58bc9":"importent_features['feature'] = importent_features.index\nimportent_features = importent_features.reset_index()\nimportent_features","66cec870":"# fig, ax  = plt.subplots(figsize = (13,5)) # set the frame\n# plt.bar(imported_features.feature, imported_features.importances ,color ='maroon')\n# plt.title(\"importent_features\")\n# plt.show()","48132a82":"rt = result_dict.copy()\n# list(rt.items())\nrt = pd.DataFrame(list(rt.items()),columns = ['Model','MSE_Result'])\nrt['Model'] = rt['Model'].str.upper()\nrt = rt.sort_values('MSE_Result')\nrt","57ab1ad0":"fig, ax  = plt.subplots(figsize = (13,5)) # set the frame\nplt.bar(rt.Model, rt.MSE_Result ,color = (0.5,0.1,0.5,0.6))\nplt.title(\"Best MSE results\")\nfig.autofmt_xdate()\nplt.show()","697f28f6":"from statsmodels.api import OLS\nOLS(y_pred,x_test).fit().summary()","ef9c0edd":"nd_forest_reg = RandomForestRegressor(n_estimators=600, max_leaf_nodes=12, n_jobs=-1).fit(x_train, y_train)\ny_pred = rnd_forest_reg.predict(x_train)","31792e14":"y2 = y_train - y_pred\ny2[:5]","0955788a":"model_LARS = Lars(n_nonzero_coefs=9).fit(x_train,y_train)\ny_pred = model_LARS.predict(x_train)","d5dd50ab":"y3 = y2 - y_pred\ny3[:5]","c36fdc23":"model_decision_tree = DecisionTreeRegressor(max_depth=4).fit(x_train,y3)","54bf6064":"y_pred = sum(model.predict(x_test) for model in (rnd_forest_reg, model_LARS, model_decision_tree))\nget_results('Gardient_Boosting_3models_MSE')","0b472b08":"plt.figure(figsize = (15,5))\nplt.plot(y_pred[0:200], label = 'pred')\nplt.plot(list(y_test[0:200]), label = 'test')\nplt.ylabel('selling_price_norm')\n#plt.legend()\nplt.title('Gradient Boosting (200) > Random Forest > Least Angel Reg > Decision Tree')\nplt.show()","13dfb3e2":"# import shap\n# ex = shap.TreeExplainer(model_decision_tree)\n# shap_values = ex.shap_values(x_test)\n# shap.summary_plot(shap_values, x_test)\n# # shap_values","f10eef97":"#### KNN model","f5037822":"#### Linear Elastic Net","51fdb4db":"#### MeanShift clustering","150be978":"### K-mean - Basic","39d71360":"### RandomForestRegressor","244f3abe":"### Hyper Tuning - the clustering models\n#### standardize the data","31fbb812":"#### Decision Tree model","484d12bc":"### Linear regression with all mumeric features","bf7a748a":"#### Birch clustering","9763626d":"### Linear regression with one feature - selling price ~ max_power","47f28fc6":"#### SVR model (SVM) - Support Vector Regression","e0e4518a":"### Categorial data into no.","75cc815a":"#### K-mean","467e264d":"### **Hyper Parameters**\n#### Linear LASSO","8fd3be4f":"*Gradient Boosting turn to be the best* : running :Random Forest  >  Least Angel Reg  >   Decision Tree","06512470":"### Linear regression with one feature - selling price ~engine","3456dd14":"##   Gradient Boosting  \n#### Random Forest  >  Least Angel Reg  >   Decision Tree","8bae3b69":"### Some Descriptive Statistics","4bce586b":"### Linear regression with one feature - selling price ~ max_power","2063d708":"#### LARS model - Least Angel Regression\n###### LARS Regression (Least Angle Regression Squers) <br>equel to step wise regression , fit small data with alot of featurs<br>puts the most importent coeficients featurs first","208eccde":"### Linear regression with one feature - selling price ~engine","73772607":"### Shap - features testing","bd672f3a":"#### Linear RIDGE"}}