{"cell_type":{"454cc496":"code","9a22127d":"code","a31ebaea":"code","5761fbf9":"code","70b184b3":"code","05990962":"code","9e9c265c":"code","d4a755fb":"code","b17ebcbd":"code","d4e957ec":"code","ab6021d4":"code","d1efbbbd":"code","0721884c":"code","21a32ff3":"code","e1b168ec":"code","59092820":"code","5374bcc0":"code","3ab23e59":"code","5f403e53":"code","2c4bc4c4":"code","0e7bae23":"code","1a694582":"code","a7a0b53c":"code","f621f203":"code","3640c700":"code","4d1e64a4":"code","d175a747":"code","c0d6c88c":"code","d60df8a1":"code","bfc6dd0b":"code","cc6ca033":"code","192be631":"code","b4c40b73":"code","96463a09":"code","f597faca":"code","b1cc222a":"code","806f1dd6":"code","d3d15d78":"code","ca7b7371":"code","e6bd0233":"code","de12e865":"code","3c1059f4":"code","5bb839d7":"code","19d7c993":"code","cbf88f58":"code","55d87009":"code","756d3435":"code","fd167b57":"code","974c6a72":"code","3d61e3e8":"code","2a903a3f":"code","f55a755f":"code","3ddd9373":"code","c689e3f2":"code","eb326a84":"code","a9df8493":"code","19009fd0":"code","4a9649cc":"code","1a9bb217":"code","b49b2884":"code","cb4d84c0":"code","19df4816":"code","9106a000":"code","b8f4f5f9":"code","89a35bcc":"code","10f2c77f":"code","9b1be8f8":"code","bdf57150":"markdown","03bf130e":"markdown","78f02e4a":"markdown","9a330341":"markdown","aeaad4c7":"markdown","3a0d0369":"markdown","66942775":"markdown","4ad9a6b6":"markdown","20ef89a2":"markdown","fa31ca24":"markdown","2eddcac0":"markdown","61557500":"markdown","94c99fa2":"markdown","431f25a2":"markdown","c329c0a5":"markdown","0168b6fe":"markdown","649adb4b":"markdown","233a6ed3":"markdown","8959610a":"markdown","c9b49bd2":"markdown","cff09d74":"markdown"},"source":{"454cc496":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a22127d":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\ndf","a31ebaea":"f_na = [feature for feature in df.columns if df[feature].isna().sum()>0]\nf_na","5761fbf9":"f_num = [feature for feature in df.columns if df[feature].dtype!='object']\nf_num","70b184b3":"# Discrete Numerical Features\nf_dis = [feature for feature in f_num if len(df[feature].unique())<25]\nprint(f_dis)\n# Continuous Numerical Features\nf_cont = [feature for feature in f_num if feature not in f_dis]\nprint(f_cont)","05990962":"import matplotlib.pyplot as plt\nfor feature in f_cont:\n    plt.hist(df[feature], bins=50)\n    plt.title(feature)\n    plt.show()","9e9c265c":"for feature in f_cont:\n    data = df.copy()\n    data[feature] = np.log(data[feature])\n    plt.hist(data[feature], bins=50)\n    plt.title(feature)\n    plt.show()","d4a755fb":"import statsmodels.api as sm\nimport scipy.stats as stats\nimport pylab \nfig, axs = plt.subplots(len(f_cont), 2, figsize=(15,15))\n\nfor i in range(len(f_cont)):\n    data = df.copy()\n    data[f_cont[i]] = np.sqrt(data[f_cont[i]])\n    stats.probplot(df[f_cont[i]], dist=\"norm\", plot=axs[i,0])\n    axs[i,0].set_title(f_cont[i])\n    stats.probplot(data[f_cont[i]], dist=\"norm\", plot=axs[i,1])\n    axs[i,1].set_title(f_cont[i])\n    \n    #plt.title(feature)\nplt.show()","b17ebcbd":"f_cont[0]","d4e957ec":"df[f_dis]['children'].unique()","ab6021d4":"f_cat = [feature for feature in df.columns if df[feature].dtype=='O']\nf_cat","d1efbbbd":"for i in f_cat:\n    print(df[i].unique(),\"\\n\",df[i].value_counts())","0721884c":"import seaborn as sns\nsns.set(style=\"whitegrid\")\nfor i in f_num:\n    ax = sns.boxplot(x=df[i])\n    plt.show()","21a32ff3":"df.head()","e1b168ec":"df[f_num]","59092820":"from scipy import stats \nIQR=[]\nfor i in f_num:\n    IQR.append(stats.iqr(df[i], interpolation = 'midpoint'))\nIQR","5374bcc0":"limits = dict()\nj=0\nfor i in f_num:\n    Q1 = np.percentile(df[i], 25, interpolation = 'midpoint')  \n    Q3 = np.percentile(df[i], 75, interpolation = 'midpoint')  \n    #print(Q1, Q3)\n    limits[i] = [Q1-(1.5*IQR[j]), Q3+(1.5*IQR[j])]\n    j+=1\n\nlimits","3ab23e59":"df[df['bmi']>=47].index","5f403e53":"outliers = dict()\nfor i in f_num:\n    outliers[i]=list()\n    for x in df[i]:\n        if(x<limits[i][0] or x>limits[i][1]):\n            outliers[i].append(x)\n   \noutliers","2c4bc4c4":"df","0e7bae23":"\"\"\"\n\n# One Hot Encoding for 'sex' and 'smoker' columns\ndf_new = df.copy()\npd.get_dummies(df_new, columns=['sex','smoker'], prefix=['sex', 'smoker'])\n\n\n# Binary Encoding for 'sex' and 'smoker' columns\nimport category_encoders as ce\ndf_new = df.copy()\nencoder = ce.BinaryEncoder(cols=['sex','smoker'])\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\n\"\"\"","1a694582":"#df_new = df.drop(df[df['bmi']>=47].index, axis=0).reset_index(drop=True)\n\ndf_new = df.copy()","a7a0b53c":"\n\n\n\n\"\"\"\n# Backward Difference Encoding for 'region' column\nimport category_encoders as ce\nencoder = ce.BackwardDifferenceEncoder(cols=['region'])\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\"\"\"\n\n# One Hot Encoding for 'region' column\nimport category_encoders as ce\nimport pandas as pd\n\n#Create object for one-hot encoding\nencoder=ce.OneHotEncoder(cols=['region'],handle_unknown='return_nan',return_df=True,use_cat_names=True)\n\n#Original Data\nprint(df_new.head())\n#Fit and transform Data\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\n\n# Label Encoding for 'sex' and 'smoker' columns\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_new['sex'] = le.fit_transform(df_new['sex'])\ndf_new['smoker'] = le.fit_transform(df_new['smoker'])\nprint(list(le.classes_))\ndf_new.head()","f621f203":"#df_new.drop(['intercept'], axis=1, inplace=True)","3640c700":"df_new.head()","4d1e64a4":"df_new.shape","d175a747":"df_new['age'] = np.sqrt(df_new['age'])\ndf_new","c0d6c88c":"\"\"\"\n## StandardScaler\nfrom sklearn.preprocessing import StandardScaler\ndata = df_new.copy()\nscaler = StandardScaler()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n#print(scaler.mean_)\n#print(scaler.transform([[2, 2]]))\n\n## MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\ndata = df_new.copy()\nscaler = MinMaxScaler()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## MaxAbsScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## RobustScaler\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## QuantileTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nscaler = QuantileTransformer()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## PowerTransformer using yeo-johnson\nfrom sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='yeo-johnson')\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## PowerTransformer using box-cox\nfrom sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='box-cox')\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\"\"\"","d60df8a1":"data = df_new.copy()","bfc6dd0b":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(), annot=True)","cc6ca033":"X, y = data.iloc[:,:-1], data.iloc[:,-1]\nX","192be631":"y","b4c40b73":"\n\"\"\"\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n\nfrom sklearn.feature_selection import RFE\nselector = RFE(reg, n_features_to_select=5, step=1)\nselector = selector.fit(X, y)\nprint(selector.support_)\nprint(selector.ranking_)\n\nf_selected = X.columns[selector.support_]\nf_selected\n\n\"\"\"","96463a09":"# Feature Importance with Extra Trees Classifier\nfrom pandas import read_csv\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# feature extraction\nmodel = ExtraTreesRegressor(n_estimators=10)\nmodel.fit(X, y)\nprint(model.feature_importances_)\ntop_features = np.array(model.feature_importances_)\n\nindices = (-top_features).argsort()[:3]\nprint(indices)\nf_selected = X.columns[indices]\nprint(X.columns)\nprint(f_selected)","f597faca":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=128)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b1cc222a":"X_train = X_train[f_selected]\nX_test = X_test[f_selected]\nprint(X_train.shape)\nprint(X_test.shape)","806f1dd6":"X_train","d3d15d78":"X_test","ca7b7371":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nreg = LinearRegression()\nscore = cross_val_score(reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","e6bd0233":"from sklearn.linear_model import LinearRegression, Ridge\nridge_reg = Ridge(alpha=0.1)\n#ridge_reg.fit(X_train, y_train)\nscore = cross_val_score(ridge_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","de12e865":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nscore = cross_val_score(lasso_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","3c1059f4":"from sklearn.svm import SVR\nimport numpy as np\nsvr_reg = SVR(C=1.0, epsilon=0.2)\nscore = cross_val_score(svr_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","5bb839d7":"X_train[f_selected].shape","19d7c993":"from sklearn.linear_model import ElasticNet\n\nelasticnet_reg = ElasticNet(l1_ratio=0.8, random_state=0)\nscore = cross_val_score(elasticnet_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","cbf88f58":"from sklearn.ensemble import RandomForestRegressor\n\nrandomfor_reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\nscore = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","55d87009":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\ndecisiontree_reg = DecisionTreeRegressor(max_depth=4, random_state=0)\nscore = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","756d3435":"from sklearn.ensemble import AdaBoostRegressor\nadaboost_reg = AdaBoostRegressor(n_estimators=100, random_state=0)\nscore = cross_val_score(adaboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","fd167b57":"from sklearn.ensemble import GradientBoostingRegressor\n\ngradientboost_reg = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=0)\nscore = cross_val_score(gradientboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","974c6a72":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(n_estimators=100, max_features=3, max_samples=50, random_state=0)\nscore = cross_val_score(bagging_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","3d61e3e8":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', max_depth=4, learning_rate = 0.1, alpha = 0.1, n_estimators = 200)\nscore = cross_val_score(xgb_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","2a903a3f":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","f55a755f":"NN_model = Sequential()\nweight_init = 'random_normal'\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer=weight_init,input_dim = X_train[f_selected].shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer=weight_init,activation='relu'))\nNN_model.add(Dense(256, kernel_initializer=weight_init,activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer=weight_init,activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='Nadam', metrics=[\"mean_absolute_error\"])\nNN_model.summary()","3ddd9373":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","c689e3f2":"#!rm .\/*","eb326a84":"NN_model.fit(X_train[f_selected], y_train, epochs=500, batch_size=64, validation_split = 0.2, callbacks=callbacks_list)","a9df8493":"\nfrom sklearn.metrics import mean_absolute_error\ny_pred = NN_model.predict(X_train[f_selected])\ny_test_pred = NN_model.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))","19009fd0":"# from sklearn.ensemble import GradientBoostingRegressor\n\ngradientboost_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, max_features='sqrt', random_state=0)\ngradientboost_reg.fit(X_train[f_selected], y_train)\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = gradientboost_reg.predict(X_train[f_selected])\ny_test_pred = gradientboost_reg.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))\n#print(score.mean())","4a9649cc":"\"\"\"\n# Load wights file of the best model :\nwights_file = '.\/Weights-365--1753.49939.hdf5' # choose the best checkpoint \nNN_model.load_weights(wights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n\"\"\"","1a9bb217":"\"\"\"\ngradientboost_reg = GradientBoostingRegressor(random_state=0)\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\n\nsearch_space = {\n        \"max_depth\": Integer(2, 10),\n        \"max_features\": Categorical(['auto', 'sqrt','log2']), \n        \"min_samples_leaf\": Integer(2, 10),\n        \"min_samples_split\": Integer(2, 10),\n        \"n_estimators\": Integer(50, 300)\n    }\n\ndef on_step(optim_result):\n    score = forest_bayes_search.best_score_\n    print(\"best score: %s\" % score)\n    if score >= 0.98:\n        print('Interrupting!')\n        return True\n\nforest_bayes_search = BayesSearchCV(gradientboost_reg, search_space, n_iter=32, scoring=\"r2_score\", n_jobs=-1, cv=5)\n\nforest_bayes_search.fit(X[f_selected], y, callback=on_step) # callback=on_step will print score after each iteration\n\n# Just like in Scikit-Learn we can view the best parameters:\nforest_bayes_search.best_params_\n# And the best estimator:\nforest_bayes_search.best_estimator_\n# And the best score:\nforest_bayes_search.best_score_\n\n\"\"\"","b49b2884":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 300)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(2, 10)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in np.linspace(2, 10)]\nloss = ['ls', 'lad', 'huber', 'quantile']\nlearning_rate = [0.01, 0.03, 0.1, 0.3]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'loss' : loss,\n               'learning_rate' : learning_rate,\n               }","cb4d84c0":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\ngradientboost_reg = GradientBoostingRegressor(random_state=0)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\ngb_random = RandomizedSearchCV(estimator = gradientboost_reg, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n# Fit the random search model\ngb_random.fit(X_train[f_selected], y_train)","19df4816":"print(gb_random.best_params_)\nprint(gb_random.best_estimator_)","9106a000":"best_random = gb_random.best_estimator_\n\n#gradientboost_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, max_features='sqrt', random_state=0)\nbest_random.fit(X_train[f_selected], y_train)\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = best_random.predict(X_train[f_selected])\ny_test_pred = best_random.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))\n#print(score.mean())","b8f4f5f9":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndef black_box_function(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    \n    gradientboost_reg = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(gradientboost_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    \n    return adj_r2\n    \n# Number of trees in random forest\nn_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'n_estimators': n_estimators,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=black_box_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50,\n)\n\nprint(optimizer.max)\n\ndef black_box_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    \n    gradientboost_reg = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    #print(gradientboost_reg)\n    gradientboost_reg.fit(X_train[f_selected], y_train)\n\n    #from sklearn.metrics import mean_absolute_error\n    y_pred = gradientboost_reg.predict(X_train[f_selected])\n    y_test_pred = gradientboost_reg.predict(X_test[f_selected])\n    #mae_train = mean_absolute_error(y_train, y_pred)\n    #mae_test = mean_absolute_error(y_test, y_test_pred)\n    #print(\"MAE for train :\",mae_train)\n    #print(\"MAE for test :\",mae_test)\n\n    from sklearn.metrics import r2_score\n    #print(\"R2 for train :\",r2_score(y_train, y_pred))\n    #print(\"R2 for test :\",r2_score(y_test, y_test_pred))\n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)\/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    #print(gradientboost_reg)\n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)\/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \nparams = optimizer.max['params']\nn_estimators = params['n_estimators']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf'\n                         ]\nprint(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n\nblack_box_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf)","89a35bcc":"from sklearn.ensemble import RandomForestRegressor\n\nrandomfor_reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\nscore = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\ndecisiontree_reg = DecisionTreeRegressor(max_depth=4, random_state=0)\nscore = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\n\nfrom sklearn.ensemble import AdaBoostRegressor\nadaboost_reg = AdaBoostRegressor(n_estimators=100, random_state=0)\nscore = cross_val_score(adaboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\n\nbagging_reg = BaggingRegressor(n_estimators=100, max_features=3, max_samples=50, random_state=0)\nscore = cross_val_score(bagging_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\nimport xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', max_depth=4, learning_rate = 0.1, alpha = 0.1, n_estimators = 200)\nscore = cross_val_score(xgb_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","10f2c77f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score \n\ndef randomfor_reg_bayesian_opt_function(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    \n\n    randomfor_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    \n    return adj_r2\n\n# Number of trees in random forest\nn_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'n_estimators': n_estimators,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=randomfor_reg_bayesian_opt_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50\n)\n\n\nparams = optimizer.max['params']\nn_estimators = params['n_estimators']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf']\nprint(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n\ndef randomfor_reg_bayesian_opt_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    randomfor_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n\n    \n    randomfor_reg.fit(X_train[f_selected], y_train)\n\n    y_pred = randomfor_reg.predict(X_train[f_selected])\n    y_test_pred = randomfor_reg.predict(X_test[f_selected])\n\n    from sklearn.metrics import r2_score\n    \n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)\/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)\/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \nrandomfor_reg_bayesian_opt_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf)","9b1be8f8":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndef decisiontree_reg_bayesian_opt_function(criterion, max_depth, min_samples_split, min_samples_leaf):\n    \n    #n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    criteria = [\"mse\", \"friedman_mse\", \"mae\"]\n    criterion = criteria[int(criterion)-1]\n    \n\n    decisiontree_reg = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    \n    return adj_r2\n\n\ncriterion = (1,3.99)\n# Number of trees in random forest\n#n_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'criterion': criterion,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=decisiontree_reg_bayesian_opt_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50\n)\n\n\nparams = optimizer.max['params']\ncriterion = params['criterion']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf']\nprint(criterion, max_depth, min_samples_split, min_samples_leaf)\n\ndef decisiontree_reg_bayesian_opt_function_test(criterion, max_depth, min_samples_split, min_samples_leaf):\n    \n    #n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    criteria = [\"mse\", \"friedman_mse\", \"mae\"]\n    criterion = criteria[int(criterion)]\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    decisiontree_reg = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n\n    \n    decisiontree_reg.fit(X_train[f_selected], y_train)\n\n    y_pred = decisiontree_reg.predict(X_train[f_selected])\n    y_test_pred = decisiontree_reg.predict(X_test[f_selected])\n\n    from sklearn.metrics import r2_score\n    \n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)\/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)\/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \ndecisiontree_reg_bayesian_opt_function_test(criterion, max_depth, min_samples_split, min_samples_leaf)","bdf57150":"# Model Building","03bf130e":"### SQRT Transformation","78f02e4a":"### Removing Outliers","9a330341":"### 2) Bayesian Optimization","aeaad4c7":"### 1) Missing Values","3a0d0369":"# Feature Scaling","66942775":"# Feature Selection","4ad9a6b6":"### DecisionTreeRegressor","20ef89a2":"### GradientBoostingRegressor","fa31ca24":"### RandomForestRegressor","2eddcac0":"### 2) Numerical Features","61557500":"# Data Analysis","94c99fa2":"# Hyperparameter Optimization","431f25a2":"* Taking sqrt of 'age' feature will make it gaussian distributed","c329c0a5":"### 4) Outliers","0168b6fe":"### 1) RandomizedSearchCV","649adb4b":"* No missing values in features","233a6ed3":"# Train-Test Split","8959610a":"### Handling Categorical Features","c9b49bd2":"# Feature Engineering","cff09d74":"### 3) Categorical Features"}}