{"cell_type":{"9a133ca5":"code","7e118192":"code","e4a7145a":"code","691e4c22":"code","96a7968b":"code","267791b4":"code","ec973f0f":"code","e8b8d899":"code","91b0bcb1":"code","1aef3bdd":"code","a74e15d5":"code","5c4328cf":"code","e2451adc":"code","02cf4c17":"code","cb78a2a3":"code","725a7fd7":"code","de8078d6":"code","04399970":"code","c1c4b95b":"code","f277605a":"code","f3fa2e5f":"code","f8ebaf23":"code","6ea18942":"code","3011a8a4":"code","a74605b7":"code","20bc1998":"code","ab9efb3d":"code","a319898b":"code","f47c30b2":"code","1543c77d":"code","480e0766":"code","d50088bf":"code","4d46e61c":"code","66f29fd0":"code","d43d7ca3":"code","6087b4a0":"code","d872e942":"code","e29cf59b":"code","36600e7b":"code","2bf12973":"code","1624a015":"code","9297895a":"code","65e50138":"code","e55f6eda":"code","709a64bb":"code","a8395d22":"code","539f01d2":"markdown","8accd2f1":"markdown","75ffcb51":"markdown","5c085145":"markdown","2d6e6a82":"markdown","e385c55f":"markdown","03a0c1eb":"markdown","f46c1e47":"markdown","745aaf08":"markdown","62a887dc":"markdown","fc1dc10b":"markdown","7285c6ac":"markdown","b03f8f84":"markdown","0cbdd2ca":"markdown","60d92647":"markdown","5780f76a":"markdown","6cae2513":"markdown","474dadc5":"markdown","63fb35dd":"markdown","c1ab7407":"markdown","d337ac32":"markdown"},"source":{"9a133ca5":"# Import Libraries\nimport os\nimport jax\nimport optax\nimport flax\nimport warnings\nimport datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom jax import jit\nimport jax.numpy as jnp\nfrom itertools import chain\nfrom tqdm.notebook import tqdm\nfrom typing import Callable\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\nfrom datasets import load_dataset, load_metric ,Dataset,list_metrics,load_from_disk\nfrom transformers import AutoTokenizer,BertTokenizer,FlaxAutoModelForSequenceClassification,AutoConfig\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(\"ignore\")\nkey = jax.random.PRNGKey(0)\n","7e118192":"print('JAX is running on', jax.lib.xla_bridge.get_backend().platform)","e4a7145a":"# Set DEBUG_MODE = True while experimenting \nDEBUG_MODE=False","691e4c22":"#some configurations \nif 'TPU_NAME' in os.environ:\n    import requests\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475\/requestversion\/tpu_driver_nightly'\n        resp = requests.post(url)\n        TPU_DRIVER_MODE = 1\n    from jax.config import config\n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n    print('Registered TPU:', config.FLAGS.jax_backend_target)\nelse:\n    print('No TPU detected. Can be changed under \"Runtime\/Change runtime type\".')","96a7968b":"# check jax local devices \njax.local_devices()","267791b4":"# load dataset\ndata = load_dataset(\"csv\", data_files={'train': ['..\/input\/goodreads-books-reviews-290312\/goodreads_train.csv']})\nraw_test = load_dataset('csv', data_files={'test': ['..\/input\/goodreads-books-reviews-290312\/goodreads_test.csv']})\n\n#setting sample size\nsample_size=15000\nif DEBUG_MODE==True:\n    data=data[\"train\"][:sample_size]\n    data = Dataset.from_dict(data)\n    data = data.train_test_split(0.20)\nelse:  \n    data = data[\"train\"].train_test_split(0.20)","ec973f0f":"raw_test","e8b8d899":"data","91b0bcb1":"# reading for analysis\ntrain=pd.read_csv(\"..\/input\/goodreads-books-reviews-290312\/goodreads_train.csv\")","1aef3bdd":"train.head(2)","a74e15d5":"plt.figure(figsize=(8, 8))\nax = sns.countplot(x=\"rating\", data=train,palette=\"hls\")\nplt.title(\"Target(rating) distribution\", fontsize=20)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Rating\",fontsize=18)\nax.xaxis.label.set_color('red')\nax.yaxis.label.set_color('red')","5c4328cf":"for rating_score in range(0,6):\n    reviews=train[train[\"rating\"]==rating_score][\"review_text\"]\n    wordcloud = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate(reviews.values[0])  \n    plt.figure(figsize=(10,8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(\"Reviews with score {}\".format(str(rating_score)),fontsize=15)\n    plt.axis(\"off\")\n    plt.tight_layout()","e2451adc":"model_checkpoint = \"bert-base-cased\" \ntokenizer = BertTokenizer.from_pretrained(model_checkpoint,use_fast=True)","02cf4c17":"def preprocess_function(input_batch):\n    '''\n    This function will take batch of data and returns the tokenized processed data\n    INPUT - input batch from from original dataset\n    RETURNS preprocessed data\n    '''\n    texts = (input_batch[\"review_text\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    processed[\"labels\"] = input_batch[\"rating\"]\n    return processed","cb78a2a3":"if DEBUG_MODE==True:\n    tokenized_dataset = data.map(preprocess_function, batched=True, remove_columns=data[\"train\"].column_names)\nelse:\n    if not os.path.exists(\"..\/input\/goodreads-tokenized-data\/train_tokenized_dataset\"):\n        tokenized_dataset = data.map(preprocess_function, batched=True, remove_columns=data[\"train\"].column_names)\n    else:\n        print(\"training dataset is already tokenized\")\n        tokenized_dataset=load_from_disk(\"..\/input\/goodreads-tokenized-data\/train_tokenized_dataset\")","725a7fd7":"train_dataset = tokenized_dataset[\"train\"]\nvalidation_dataset = tokenized_dataset[\"test\"]","de8078d6":"metrics_list = list_metrics()\nmetrics_list","04399970":"metric = load_metric('f1')\nmetric","c1c4b95b":"# Parameters \nnum_labels = 6 # 0-5\nseed = 0\nnum_train_epochs = 3\nlearning_rate = 2e-5\nper_device_batch_size = 32\nweight_decay=1e-2","f277605a":"total_batch_size = per_device_batch_size * jax.local_device_count()\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)","f3fa2e5f":"config = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\nmodel = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)","f8ebaf23":"num_train_steps = len(train_dataset) \/\/ total_batch_size * num_train_epochs\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.1)","6ea18942":"class TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)","3011a8a4":"def decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)","a74605b7":"def adamw(weight_decay):\n    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay,mask=decay_mask_fn)","20bc1998":"adamw = adamw(weight_decay)","ab9efb3d":"@jit\ndef loss_function(logits, labels):\n    xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n    return jnp.mean(xentropy)\n \n@jit\ndef eval_function(logits):\n    return logits.argmax(-1)","a319898b":"# Instantiate a TrainState.\nstate = TrainState.create(\n    apply_fn=model.__call__,\n    params=model.params,\n    tx=adamw,\n    logits_function=eval_function,\n    loss_function=loss_function,\n)","f47c30b2":"# Define train step \ndef train_step(state, batch, dropout_rng):\n    # take targets\n    targets = batch.pop(\"labels\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n    \n    #define loss function which runs the forward pass \n    def loss_function(params):\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n        loss = state.loss_function(logits, targets)\n        return loss\n    \n    \n    grad_fn = jax.value_and_grad(loss_function) #differentiate the loss function\n    loss, grad = grad_fn(state.params) \n    grad = jax.lax.pmean(grad, \"batch\") #compute the mean gradient over all devices \n    new_state = state.apply_gradients(grads=grad) #applies the gradients to the weights.\n    metrics = jax.lax.pmean({'loss': loss, 'learning_rate': learning_rate_function(state.step)}, axis_name='batch')\n    \n    return new_state, metrics, new_dropout_rng","1543c77d":"parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,)) # parallelized training over all TPU devices","480e0766":"# Define evaluation step\ndef eval_step(state, batch):\n    logits = state.apply_fn(**batch, params=state.params, train=False)[0] #stack the model's forward pass with the logits function\n    return state.logits_function(logits)","d50088bf":"parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")","4d46e61c":"# Returns batch model input\n# 1. define random permutation \n# 2. randomized dataset is extracted and then it converted to a JAX array and sharded over all local TPU devices.\ndef train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) \/\/ batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n        yield batch","66f29fd0":"# similar to train data loader \ndef eval_data_loader(dataset, batch_size): \n    for i in range(len(dataset) \/\/ batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n        yield batch","d43d7ca3":"# replicate\/copy the weight parameters on each device, to passthem to our pmapped functions.\nstate = flax.jax_utils.replicate(state)","6087b4a0":"# generating a seeded PRNGKey for the dropout layers and dataset shuffling.\nrng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","d872e942":"%%time\n# Full training loop\nfor i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n\n    # train\n    with tqdm(total=len(train_dataset) \/\/ total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n        for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n            progress_bar_train.update(1)\n\n    # evaluate\n    with tqdm(total=len(validation_dataset) \/\/ total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n          for batch in eval_data_loader(validation_dataset, total_batch_size):\n                labels = batch.pop(\"labels\")\n                predictions = parallel_eval_step(state, batch)\n                metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n                progress_bar_eval.update(1)\n\n    eval_metric = metric.compute(average='macro')\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    eval_score = round(list(eval_metric.values())[0],3)\n    metric_name = list(eval_metric.keys())[0]\n\n    print(f\"{i+1}\/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")","e29cf59b":"# preprocess test dataset\ndef preprocess_test_set(input_batch):\n    '''\n    This function will take batch of data and returns the tokenized processed data\n    INPUT - input batch from from original dataset\n    RETURNS processed data\n    '''\n    texts = (input_batch[\"review_text\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    return processed","36600e7b":"# using tokenized dataset\ntokenized_test_dataset=None\nif not os.path.exists(\"..\/input\/goodreads-tokenized-data\/test_tokenized_dataset\"):\n    tokenized_test_dataset = raw_test.map(preprocess_test_set, batched=True, remove_columns=raw_test[\"test\"].column_names)\nelse:\n    print(\"training dataset is already tokenized\")\n    tokenized_test_dataset=load_from_disk(\"..\/input\/goodreads-tokenized-data\/test_tokenized_dataset\")\ntest_dataset = tokenized_test_dataset[\"test\"]","2bf12973":"test_dataset","1624a015":"# similar to train dataloader, it takes dataset and batch_size\ndef test_data_loader(dataset,batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n        yield batch\n    else:\n        for i in range(len(dataset) \/\/ batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n            batch = shard(batch)\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n        yield batch","9297895a":"%%time\n# Running on test dataset , storing predictions \npreds=[]\nfor batch in test_data_loader(test_dataset, total_batch_size):\n    predictions = parallel_eval_step(state, batch)\n    preds.append(predictions[0])    ","65e50138":"# convert it into numpy array\npredictions=[]\nfor pred in preds:\n    predictions.extend(np.array(pred))","e55f6eda":"# create final submission file \nsample = pd.read_csv('..\/input\/goodreads-books-reviews-290312\/goodreads_sample_submission.csv')\nsample[\"rating\"]=predictions","709a64bb":"sample.head(2)","a8395d22":"sample.to_csv('submission.csv',index=False)","539f01d2":"# References\n* https:\/\/flax.readthedocs.io\/en\/latest\/notebooks\/annotated_mnist.html\n* https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/text_classification_flax.ipynb#scrollTo=Mn1GdGpipfWK\n* https:\/\/www.kaggle.com\/heyytanay\/sentiment-clf-jax-flax-on-tpus-w-b\n* https:\/\/optax.readthedocs.io\/\n* https:\/\/flax.readthedocs.io\/\n* https:\/\/jax.readthedocs.io\/\n* https:\/\/github.com\/google\/flax\/tree\/main\/examples","8accd2f1":"# Preprocessing\n\n**bert-base-cased** - Pretrained model on English language using a masked language modeling (MLM) objective.\n\nlearn more about it here - https:\/\/huggingface.co\/bert-base-cased","75ffcb51":"Evaluating num of train steps and defining learning_rate_function using [optax](https:\/\/optax.readthedocs.io\/en\/latest\/) for jax. \n\nHere I am using **[cosine onecycle learning rate scheduler](https:\/\/optax.readthedocs.io\/en\/latest\/api.html#optax.cosine_onecycle_schedule)** from optax library ","5c085145":"Flax provides a class [flax.training.train_state.TrainState](https:\/\/flax.readthedocs.io\/en\/latest\/flax.training.html#train-state), which stores the model parameters, the loss function, the optimizer, and exposes an **apply_gradients** function to update the model's weight parameters.","2d6e6a82":"Now , computing the **softmax cross entropy** between sets of logits and labels using the [optax](https:\/\/optax.readthedocs.io\/en\/latest\/) library.","e385c55f":"# ","03a0c1eb":"# Conclusion \nSo we successfully predicted rating score from the review text and submission.csv is created which you can find in Outputs (\"\/kaggle\/working\"). You can submit your submission file to the competition.It can be found in sidebar of notebook editor, under **Competitions** by clicking on the \"**Submit**\" Button or you can go to competition and click on the **submit predictions**. \n\nNote that submission format should be same as described [here](https:\/\/www.kaggle.com\/c\/goodreads-books-reviews-290312\/overview\/evaluation)\n\n\n","f46c1e47":"> Majority of rating is from 3 to 5 ","745aaf08":"### Data loaders ","62a887dc":"# Test Generation","fc1dc10b":"\n# Predict Book Review Rating using Jax\/Flax ","7285c6ac":"I am using 'F1-score' metric as given in [Competition evaluation section](https:\/\/www.kaggle.com\/c\/goodreads-books-reviews-290312\/overview\/evaluation) ","b03f8f84":"# Introduction\n\nThis notebook is taking [goodreads competition dataset](https:\/\/www.kaggle.com\/c\/goodreads-books-reviews-290312\/overview\/description) consisting reviews from the Goodreads book review website, and a variety of attributes describing the items. and predicting review rating which ranges from 0 to 5 using [flax framework](https:\/\/flax.readthedocs.io\/en\/latest\/overview.html#flax).\n\n# About Dataset\n\nThe dataset contains more than **1.3M book reviews** about **25,475 books** and **18,892 users** , which is a review subset for spoiler detection, where each book\/user has at least one associated spoiler review.\n\n**Files** - \n* goodreads_train.csv - the training set\n* goodreads_test.csv - the test set\n* goodreads_sample_submission.csv - sample submission file\n\nHere are the columns of the dataset -\n\n* user_id - Id of user\n* book_id - Id of Book\n* review_id - Id of review\n* rating - rating from 0 to 5\n* review_text - review text\n* date_added - date added\n* date_updated - date updated\n* read_at - read at\n* started_at - started at\n* n_votes - no. of votes\n* n_comments - no. of comments\n\nNote - I also included tokenized training and testing data which can be use directly to save time.\n\nyou can access it here - https:\/\/www.kaggle.com\/yashvi\/goodreads-tokenized-data\n\n# How to use this notebook \nYou can **copy & edit** this notebook , which you can find on **top-right** corner of kaggle notebook viewer.\n","0cbdd2ca":"# Exploratory data analysis ","60d92647":"# Model Generation","5780f76a":"# Imports","6cae2513":"Here I already tokenized data and created tokenized dataset , so that i don't have to tokenize it again everytime.\n\nlink to the tokenized dataset - https:\/\/www.kaggle.com\/yashvi\/goodreads-tokenized-data ","474dadc5":"### Word clouds for different Rating score.\n","63fb35dd":"Below step is downloading the pretrained model,as we are doing **sentence classification** we can use **FlaxAutoModelForSequenceClassification** class.\n**from_pretrained** method will download and cache the model.","c1ab7407":"### Target distribution","d337ac32":"Here we are using Adam optimizer with weight decay, and again we are using [optax](https:\/\/optax.readthedocs.io\/en\/latest\/)  library.\n\nHere is the interesting article on adam optimizer - https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/"}}