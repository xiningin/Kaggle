{"cell_type":{"e306470b":"code","029ac892":"code","fb428575":"code","6cf60fbb":"code","1bff52e9":"code","ee748af1":"code","bbdce6fb":"code","98bed023":"code","f10c7928":"code","efff8483":"code","ec38df74":"code","5f9a5630":"code","8c21bcc6":"code","a942cc6b":"code","d9649d33":"code","73ebd17d":"code","6441826b":"code","ec4fa9b3":"code","366fd143":"code","975974e9":"code","41204e56":"code","016ca982":"code","b65e1ce9":"code","f01407ac":"code","d194c377":"code","8884b82b":"code","c926bd7c":"code","cb520903":"code","f9b0a69a":"code","49399b6b":"code","6120da6f":"code","ae87f194":"code","0dbe9366":"code","1a0b3762":"code","c654e1c3":"code","8da7de7a":"code","1041a376":"code","c37fe64b":"code","0c4f0329":"code","bde06347":"code","fe350c06":"code","62c7ccb2":"code","74c8f4f4":"code","1221194d":"code","b49181b5":"code","c7e78023":"code","3fc0a0fc":"code","018422cf":"code","504fbeee":"code","7306126f":"code","572ba7ba":"code","4d704789":"markdown","ea757a44":"markdown","82c9d42d":"markdown","c5476409":"markdown","7dd045cb":"markdown","4e4e7469":"markdown","5f617b9e":"markdown","95129b6b":"markdown","4ed63213":"markdown","49cb9175":"markdown","577bcd3a":"markdown","b1a667fe":"markdown","fe99f613":"markdown","b47bf81a":"markdown","206f6a71":"markdown","48fab262":"markdown","af5679a4":"markdown","71012b32":"markdown","245fff31":"markdown","c0bfc03b":"markdown","b81d1f51":"markdown","c90dd2be":"markdown","4cdd6984":"markdown","49287aad":"markdown"},"source":{"e306470b":"import torch","029ac892":"tensor1 = torch.Tensor([[1, 2, 3], \n                       [4, 5, 6]])\ntensor1","fb428575":"tensor2 = torch.Tensor([[7, 8, 9], \n                        [10, 11, 12]])\n\ntensor2","6cf60fbb":"tensor1.requires_grad","1bff52e9":"tensor2.requires_grad","ee748af1":"tensor1.requires_grad_()","bbdce6fb":"tensor1.requires_grad","98bed023":"tensor2.requires_grad","f10c7928":"print(tensor1.grad)","efff8483":"print(tensor1.grad_fn)","ec38df74":"# The output tensor also has the grad property used to store gradients. \noutput_tensor = tensor1 * tensor2","5f9a5630":"output_tensor.requires_grad","8c21bcc6":"print(output_tensor.grad)","a942cc6b":"print(output_tensor.grad_fn)","d9649d33":"print(tensor1.grad_fn)","73ebd17d":"print(tensor2.grad_fn)","6441826b":"output_tensor = (tensor1 * tensor2).mean()\nprint(output_tensor.grad_fn)","ec4fa9b3":"print(tensor1.grad)","366fd143":"output_tensor.backward()","975974e9":"print(tensor1.grad)","41204e56":"tensor1.grad.shape, tensor1.shape","016ca982":"print(tensor2.grad)","b65e1ce9":"print(output_tensor.grad)","f01407ac":"new_tensor = tensor1 * 3\nprint(new_tensor.requires_grad)","d194c377":"new_tensor","8884b82b":"with torch.no_grad():\n    \n    new_tensor = tensor1 * 3\n    \n    print('new_tensor = ', new_tensor)\n    \n    print('requires_grad for tensor = ', tensor1.requires_grad)\n    \n    print('requires_grad for tensor = ', tensor2.requires_grad)\n    \n    print('requires_grad for new_tensor = ', new_tensor.requires_grad)","c926bd7c":"def calculate(t):\n    return t * 2","cb520903":"@torch.no_grad()\ndef calculate_with_no_grad(t):\n    return t * 2","f9b0a69a":"result_tensor = calculate(tensor1)\n\nresult_tensor","49399b6b":"result_tensor.requires_grad","6120da6f":"result_tensor_no_grad = calculate_with_no_grad(tensor1)\n\nresult_tensor_no_grad","ae87f194":"result_tensor_no_grad.requires_grad","0dbe9366":"with torch.no_grad():\n    \n    new_tensor_no_grad = tensor1 * 3\n    \n    print('new_tensor_no_grad = ', new_tensor_no_grad)\n    \n    with torch.enable_grad():\n        \n        new_tensor_grad = tensor1 * 3\n    \n        print('new_tensor_grad = ', new_tensor_grad)","1a0b3762":"tensor_one = torch.tensor([[1.0, 2.0], \n                           [3.0, 4.0]], requires_grad=True)  \ntensor_one","c654e1c3":"tensor_two = torch.Tensor([[5, 6], \n                           [7, 8]])\ntensor_two","8da7de7a":"tensor_one.requires_grad","1041a376":"tensor_two.requires_grad_()","c37fe64b":"final_tensor = (tensor_one + tensor_two).mean()\nfinal_tensor","0c4f0329":"final_tensor.requires_grad","bde06347":"print(tensor_one.grad)","fe350c06":"print(tensor_two.grad)","62c7ccb2":"final_tensor.backward()","74c8f4f4":"print(tensor_one.grad)","1221194d":"print(tensor_two.grad)","b49181b5":"print(final_tensor.grad)","c7e78023":"detached_tensor = tensor_one.detach()\n\ndetached_tensor","3fc0a0fc":"tensor_one","018422cf":"mean_tensor = (tensor_one + detached_tensor).mean()\n\nmean_tensor.backward()","504fbeee":"tensor_one.grad","7306126f":"print(detached_tensor.grad)","572ba7ba":"#End of Code","4d704789":"#### The original tensor still does not have a gradient function","ea757a44":"#### In spite of setting a gradient function for the output, the gradients for the input tensor is still empty","82c9d42d":"#### The requires_grad property has been derived from the original tensor","c5476409":"#### The gradients are now available for the input tensor\n\nFuture calls to backward will accumulate gradients into this vector","7dd045cb":"#### Can explicitly enabled gradients within a no_grad() context\n\nThere is an equivalent @torch.enable_grad() as well","4e4e7469":"#### Turning off gradient calculations for tensors\nYou can also stops autograd from tracking history on newly created tensors with requires_grad=True by wrapping the code block in <br \/>\n<b>with torch.no_grad():<\/b>","5f617b9e":"#### Changing the operation for the output changes the gradient function\nThe gradient function only contains the last operation. Here, even though there is a multiplication as well as a mean, only the mean calculation is recorded as the gradient function","95129b6b":"#### But there is a gradient function\nThis is from the multiplication operation performed on the original tensor ","4ed63213":"#### Requires_grad property\n\nEvery tensor created in PyTorch will have the requires_grad property. When the value of this is **TRUE**, this means that PyTorch **will track computations for this tensor in the forward phase** when we use the computation graph to make predictions, and it will **calculate gradients for this tensor in the backward phase**\n\nWe dont need to specify requires_grad = False, since by default it flags it as False","49cb9175":"#### The .grad_fn property contains the gradient function\n\nThe computation graph within PyTorch is made up of **tensors and functions**. These together make up our directed acyclic computation graph. You can think of tensors as the nodes in this graph and functions are the transformations performed along edges. Every tensor has a grad function used to create that function.\n\nThis has not been set either.","577bcd3a":"Purpose: Understand how Pytorch tensors can be set up to track history using the Autograd library package","b1a667fe":"#### Detach tensors from the computation graph","fe99f613":"#### The requires_grad property propagates to other tensors\nHere the new_tensor is created from the original tensor and gets the original's value of requires_grad","b47bf81a":"#### The requires\\_grad\\_() function sets requires_grad to True\n\nIf you want to enable tracking history for a particular tensor so that gradients are calculated with respect to that tensor, you need to enable the **requires_grad flag, which you can do by calling requires_grad_**. This will update the requires_grad property for this tensor in place","206f6a71":"#### The gradient vector is the same shape as the original vector","48fab262":"### Result tensors get requires_grad properties from input tensors","af5679a4":"#### The .grad property stores all the gradients for the tensor\n\nWe have set up a tensor, but we haven't used it within a computation graph. We haven't performed a forward or a backward pass. This tensor hasn't been used in a computation. Hence there are no gradients yet.","71012b32":"#### enable the gradients for  two tensors","245fff31":"#### To calculate the gradients, we need to explicitly perform a backward propagation","c0bfc03b":"#### There are still no gradients\n\nThere are no gradients, we haven't made any backward pass. But you'll find that this output tensor will have a grad function","b81d1f51":"#### The requires_grad property defines whether to track operations on this tensor\nBy default, it is set to False","c90dd2be":"#### final tensor has gradients enabled as it derives from the tensors its made up of","4cdd6984":"#### Create a new output tensor from our original tensor\n\nWhen you create a tensor using an operation, the **requires_grad property for the resulting tensor is based on the input tensors that we use to create this output tensor**. Requires_grad is true because tensor1 had requires_grad = true and tensor1 was used to create this output tensor. ","49287aad":"#### Can turn off gradient calculations performed within a function"}}