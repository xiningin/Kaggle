{"cell_type":{"81587ff5":"code","a9412a25":"code","3844835f":"code","d8823f45":"code","76a4071a":"code","837a23a3":"code","6bb5e1e0":"code","2ab3b2ce":"code","6a36aa74":"code","379f97fd":"markdown","32dc258e":"markdown","0b3abd31":"markdown","2327cfa2":"markdown","baef99d7":"markdown","716a59ec":"markdown"},"source":{"81587ff5":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nnp.random.seed(123)\n%matplotlib inline","a9412a25":"from sklearn.base import TransformerMixin\n\n# This is just a quick and simple StandardScaler transformer which works with 3 dimensional inputs like we are giving our model\n# read more about StandardScaler here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nclass CustomStandardScalerForCnn(TransformerMixin):\n    def __init__(self, with_mean=True, with_std=True):\n        self.with_mean = with_mean\n        self.with_std = with_std\n        self.mean_ = None\n        self.std_ = None\n        \n    def fit(self, X, y=None):\n        if self.with_mean:\n            self.mean_ = X.mean()\n        else:\n            self.mean_ = 0\n            \n        if self.with_std:\n            self.std_ = X.std()\n        else:\n            self.std_ = 1\n        \n        return self\n    \n    def transform(self, X):\n        if self.mean_ and self.std_:\n            return (X - self.mean_) \/ self.std_\n        else:\n            raise(\"CustomStandardScalerForCnn is not fitted\")\n            \n    def inverse_transform(self, X):\n        if self.with_std:\n            X *= self.std_\n        if self.with_mean:\n            X += self.mean_\n        return X\n    \n        ","3844835f":"def VGG_inspired_build():\n    clf = keras.models.Sequential([\n        keras.layers.ZeroPadding2D((1,1), input_shape=(32, 32, 3)),\n        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n        keras.layers.ZeroPadding2D((1,1)),\n        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)), # stride=2\n        keras.layers.ZeroPadding2D((1,1)),\n        keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n        keras.layers.ZeroPadding2D((1,1)),\n        keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)), # stride=2\n        keras.layers.ZeroPadding2D((1,1)),\n        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n        keras.layers.ZeroPadding2D((1,1)),\n        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n        keras.layers.MaxPooling2D(pool_size=(2, 2)), # stride=2\n        keras.layers.Flatten(),\n        keras.layers.Dense(2048, activation='relu'),\n        keras.layers.Dropout(0.75),\n        keras.layers.Dense(2048, activation='relu'),\n        keras.layers.Dropout(0.75),\n        keras.layers.Dense(33, activation='softmax')\n    ])\n    \n    \n    clf.compile(optimizer=keras.optimizers.Adam(),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n    return clf","d8823f45":"def plot_data(img_array, labels_array, n_samples=5):\n    classes = set(labels_array)\n    fig, ax = plt.subplots(len(classes), n_samples)\n    \n    for label_idx, class_label in enumerate(classes):\n        data = img_array[labels_array == class_label]\n        samples_from_class = random.choices(data, k=n_samples)\n        for i, img in enumerate(samples_from_class):\n            ax[label_idx, i].imshow(img)\n            ax[label_idx, i].axis('off')","76a4071a":"letters1 = pd.read_csv(os.path.join('..', 'input', 'letters.csv'))\nletters1['source_folder'] = 'letters'\nletters2 = pd.read_csv(os.path.join('..', 'input', 'letters2.csv'))\nletters2['source_folder'] = 'letters2'\nletters3 = pd.read_csv(os.path.join('..', 'input', 'letters3.csv'))\nletters3['source_folder'] = 'letters3'\nletters = pd.concat([letters1, letters2, letters3], ignore_index=True)\n\nletters.head()","837a23a3":"%%time\n# This is not the most efficient way to load the data but it's ok for now (should only take 30 sec)\nimport cv2\nX = []\ny = []\nfor i, row in letters.iterrows():\n    source_folder = row['source_folder']\n    img_name = row['file']\n    img_arr = cv2.imread(os.path.join('..', 'input', source_folder, img_name))\n    if img_arr.shape == (32, 32, 3): # There are some photos in a weird size, I will not use them.\n        X.append(img_arr)\n        y.append(row['letter'])\n\nX = np.array(X)\ny = np.array(y)\nX.shape, y.shape","6bb5e1e0":"plot_data(X, y, n_samples=25)\nplt.subplots_adjust(right=3, top=3)","2ab3b2ce":"np.random.seed(123)\nmodel = Pipeline([\n    ('scaler', CustomStandardScalerForCnn()),\n    ('keras', keras.wrappers.scikit_learn.KerasClassifier(VGG_inspired_build,\n                                                          epochs=40,\n                                                          batch_size=128,\n                                                          validation_split=0.1,\n                                                          callbacks=[\n                                                              keras.callbacks.EarlyStopping(patience=5),\n                                                              keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                                                                factor=0.5, patience=3,\n                                                                                                verbose=1, min_lr=0)\n                                                                    ],             \n                                                          verbose=1))\n])\n\nprint(VGG_inspired_build().summary())\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\nmodel.fit(x_train, y_train)","6a36aa74":"print('score on train data:', model.score(x_train, y_train))\nprint('score on test data:', model.score(x_test, y_test))\ny_pred = model.predict(x_test)\nprint(metrics.classification_report(y_test, y_pred))","379f97fd":"##### Load the data","32dc258e":"##### Train the model","0b3abd31":"##### init some usful functions","2327cfa2":"##### Let's Have a look at our data","baef99d7":"There is still a lot to to do to improve this model like:\n* Reduce the overfitting (99% on train and 95% on test).\n* Some False Analysis to uncover the models mistakes and maybe even some wrong or bad training examples\n* etc.","716a59ec":"## How I got 95% accuracy on a test set\nThe CNN model I used was inspired by the VGG architecture.\n\nI also created a custom [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) to work with 3d arrays."}}