{"cell_type":{"c7b3631e":"code","3c7b5184":"code","72590ef6":"code","e9bb2407":"code","843b2724":"code","31e26b43":"code","c701fffd":"code","1d71d92a":"code","3e493f3c":"code","ebad40c8":"code","2a877f71":"code","002c7ef8":"code","9ab3ab6e":"markdown","30fbbae3":"markdown","6adeaf4a":"markdown","67851c74":"markdown","9c2bec91":"markdown","59181da9":"markdown","f1c9af68":"markdown","a11ceb27":"markdown","8686915f":"markdown","98215d76":"markdown","1bafac2f":"markdown","f3173284":"markdown","ec63fcf9":"markdown","8051cd1e":"markdown","e3aceb15":"markdown","8a48129a":"markdown","b739d1a0":"markdown","f7ffdf14":"markdown","c4a6cd8a":"markdown"},"source":{"c7b3631e":"'''\n.--------------------.\n| \ud83d\udc39 | \u2744\ufe0f | \u2744\ufe0f | \u2744\ufe0f | \n|- - - - -  - - - - -|\n| \u2744\ufe0f | \ud83d\udd73 | \u2744\ufe0f | \ud83d\udd73 |\n|- - - - -  - - - - -|\n| \u2744\ufe0f | \u2744\ufe0f | \u2744\ufe0f | \ud83d\udd73 |\n|- - - - -  - - - - -|\n| \ud83d\udd73 | \u2744\ufe0f | \u2744\ufe0f | \ud83e\udd5c |\n.--------------------.\n'''","3c7b5184":"'''\n     U   D   L   R\n   .---------------.\n0  | 0 | 0 | 0 | 0 | \n   | - - - - - - - |\n1  | 0 | 0 | 0 | 0 |\n   | - - - - - - - |\n2  | 0 | 0 | 0 | 0 |\n   | - - - - - - - |\n.. | . | . | . | . |\n   | - - - - - - - |\n15 | 0 | 0 | 0 | 0 |\n   .---------------.\n'''","72590ef6":"import gym\nimport mxnet as mx\nimport matplotlib.pyplot as plt\nimport random\nfrom time import time","e9bb2407":"EPISODES = 2000\nSTEPS = 100\nBANDIT_ARMS = 4\nBANDIT_TRAINING = True\nEPSILON = 1 # setting the agent for exploration only -> try to learn the environment\nALPHA = 0.8\nGAMMA = 0.95","843b2724":"env = gym.make('FrozenLake-v0')\nstates = env.observation_space.n\nactions = env.action_space.n","31e26b43":"# make a table with the same dimensions as the test environment & initialize it with zero Q-values\nQ = mx.nd.zeros(shape=(states, actions), ctx=mx.cpu(), dtype='float32')","c701fffd":"def run_epsilon_greedy_bandit(arm_rewards, arms, training, epsilon, episode):\n    # check if bandit is used for training\n    if training:\n        # -> start with the full exploration and slowly reduce it as this algorithm learns\n        epsilon = mx.nd.exp(data=mx.nd.array([-0.01 * episode]))\n    \n    # flip the coin (randomly select the number between 0 to 1)\n    random_number = random.random()\n\n    # select the arm\n    if random_number > epsilon:\n        # Exploit\n        # -> find arms with highest rewards\n        max_reward_arms = [i for i, e in enumerate(arm_rewards) if e == max(arm_rewards)]\n        # -> select the best arm\n        if len(max_reward_arms) == 1:\n            # get the best arm\n            arm = max_reward_arms[0]\n        else:\n            # randomly choose an arm from the best arms\n            arm = random.choice(max_reward_arms)\n    else:\n        # Explore\n        # -> randomly choose an arm\n        arm = random.randrange(arms)\n        \n    return arm","1d71d92a":"# log the training start\ntraining_start = time()\n\n# store the training progress of this algorithm for each episode\nepisode_rewards = []\nepisode_steps = []\n\n# solve the environment over certain amount of episodes\nfor episode in range(EPISODES):\n    # reset the environment, rewards, and steps for the new episode\n    s = env.reset()\n    episode_reward = 0\n    step = 0\n    \n    # find the solution over certain amount of attempts (steps in each episode)\n    while step < STEPS:\n        # select the action in the current state by running the multiarmed bandit\n        a = run_epsilon_greedy_bandit(Q[s, :], BANDIT_ARMS, BANDIT_TRAINING, EPSILON, episode)\n        \n        # enter the environment and get the experience from it by performing there an action\n        # -> get the observation (new state), reward, done (success\/failure), and information\n        observation, reward, done, info = env.step(a)\n\n        # update the Q-value for the current state and action\n        # -> calculate this Q-value using its previous value & the experience from the environment\n        Q[s, a] = Q[s, a] + ALPHA * (reward + GAMMA * mx.nd.max(Q[observation, :]) - Q[s, a])\n\n        # add the reward to others during this episode\n        episode_reward += reward\n        \n        # change the state to the observed state for the next iteration\n        s = observation\n\n        # check if the environment has been exited\n        if done:\n            # -> store the collected rewards & number of steps in this episode     \n            episode_rewards.append(episode_reward)\n            episode_steps.append(step)\n            # -> quit the episode\n            break\n\n        # continue looping\n        step += 1\n\n# log the training end\ntraining_end = time()","3e493f3c":"print(\"trained Q-values\", Q)","ebad40c8":"# show the success rate for solving the environment & elapsed training time\nsuccess_rate = round((sum(episode_rewards) \/ EPISODES) * 100, 2)\nelapsed_training_time = int(training_end - training_start)\nprint(\"\\nThis environment has been solved\", str(success_rate), \"% of times over\",  str(EPISODES), \"episodes within\", str(elapsed_training_time), \"seconds!\")\n\n# plot the rewards and number of steps over all training episodes\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(episode_rewards, '-g', label = 'reward')\nax1.set_yticks([0,1])\nax2 = ax1.twinx()\nax2.plot(episode_steps, '+r', label = 'step')\nax1.set_xlabel(\"episode\")\nax1.set_ylabel(\"reward\")\nax2.set_ylabel(\"step\")\nax1.legend(loc=2)\nax2.legend(loc=1)\nplt.title(\"Training Progress\")\nplt.show()","2a877f71":"GAME_EPISODES = 10\nBANDIT_TRAINING = False\nEPSILON = 0 # setting the agent for pure exploitation -> use only learned Q values from training","002c7ef8":"for episode in range(GAME_EPISODES):\n    s = env.reset()\n    step = 0\n    while step < STEPS:\n        # take the action that have the maximum expected future reward\n        a = run_epsilon_greedy_bandit(Q[s, :], BANDIT_ARMS, BANDIT_TRAINING, EPSILON, episode)\n        \n        # enter the environment and get the experience from it\n        observation, reward, done, info = env.step(a)\n\n        # change the state to observed state for the next iteration\n        s = observation\n\n        # check if the environment has been exited\n        if done:\n            # print results of each episode\n            print(\"\\n-----------------------------\")\n            print(step, \"steps in episode\", episode)\n            print(\"The last action & state was:\")\n            env.render()\n            print(\"-----------------------------\")\n            # quit the episode\n            break\n\n        # continue looping\n        step += 1","9ab3ab6e":"* the **learning rate (\u03b1) can take values between 0 to 1**, where value of one means an agent tries to learn everything (exploration by ignoring prior knowledge) & value of zero means an agent learns nothing (exploitation of prior knowledge)\n* the **discount factor (\u0263) can take values between 0 to 1**, where value of one means it is considering only long-term rewards (an agent is farsighted) & value of zero means it is considering only the current rewards (an agent is nearsighted)\n\n### Bandit \ud83d\udd2b \ud83e\udd20 \ud83d\udc34\nWe will create an agent that **chooses actions** as if it were a **multiarmed bandit**. It is a famous problem from probability theory that was used to address decisions of gamblers playing slot machines in casinos. Here, tt can help us to address the exploration-exploitation dilemma in RL. This bandit will choose actions from supplied Q-values for a given state.\n* the bandit will have **4 arms**, since we have 4 actions to choose at each state\n* we will use the **epsilon-greedy** strategy for the bandit, i.e. our agent will randomly choose actions by flipping a coin so as to either go explore or exploit the environment\n* the **epsilon can take values between 0 to 1**, where 0 means an agent will do no exploration whatsoever (100% exploitation) and 1 is all about exploration (0% exploitation)\n\n## 3) Code \ud83d\udcbb\nWe will **train** the Q-learning for certain amount of episodes in the environment and then try to **play** the game with learned Q-values by training.\n\n### training \ud83c\udfcb\ufe0f\u200d \n> _... let's train the algorithm to solve the environment_\n\n_... load the Python **packages**_","30fbbae3":"_... create a **table** for Q-values_","6adeaf4a":"# Q-Learning with Table \ud83e\udd16 \ud83c\udf9b \ud83c\udf7d\n> This is a demo of Q-learning at its simplest for solving Reinforcement Learning (RL) problems. The challenge here is a frozen lake that is being solved via **table** approach for Q-learning implementation. It serves as an example code for an [introduction story into RL on Medium](https:\/\/towardsdatascience.com\/lite-intro-into-reinforcement-learning-857ca5c924d9).\n\n<img src=\"https:\/\/raw.githubusercontent.com\/tomtx\/q-learning-demo\/master\/img\/table-wall-e.png\" alt=\"Drawing\" style=\"width: 400px;\"\/>\n\n## 1) Environment \ud83d\uddfa\nIt is a [frozen lake by Gym OpenAI](https:\/\/gym.openai.com\/envs\/FrozenLake-v0\/), in which an agent tries to find a safe path across a grid of slippery ice and water tiles to get the ultimate reward.","67851c74":"### playing \ud83c\udfae\n> _... let's play the game to see how our agent \ud83d\udc39 solves the environment with the knowledge obtained from training, i.e. with the trained Q-values_","9c2bec91":"<img src=\"https:\/\/raw.githubusercontent.com\/tomtx\/q-learning-demo\/master\/img\/conclusions-wall-e.png\" alt=\"Drawing\" style=\"width: 500px;\"\/>","59181da9":"_... load the test environment of **frozen lake**_ & get its possible number of _**states and actions**_","f1c9af68":"_... print the **final Q-values**_","a11ceb27":"<img src=\"https:\/\/raw.githubusercontent.com\/tomtx\/q-learning-demo\/master\/img\/algo-estimation.png\" alt=\"Drawing\" align=\"left\" style=\"width: 650px;\"\/>","8686915f":"## References\n* R. S. Sutton and A. G. Barto: \"Reinforcement Learning: An Introduction\", 2nd edition, A Bradford Book, 2017","98215d76":"The environment is depicted in the picture above as a 4x4 grid world with 16 possible states. There is a hamster that tries to munch on peanuts. Its **goal is to reach those peanuts**!\n* this little fellow can choose to go one of **4 directions (actions) on the ice at each grid cell (state)**, i.e. it can go either up, down, left, or right\n* it tries to **avoid going into water holes**, which results in sinking and therefore an unsuccessful attempt\n* however, it is not that easy since there is a chance that this little creature can **randomly slip on ice and go into different grid cells** than wanted\n* the **success or failure** for solving the frozen lake is defined by this hamster getting those peanuts or sinking into the water hole, respectively\n\n## 2) Algorithm \ud83c\udfb0\nIt is a classic Q-learning algorithm that goes through iteration steps to experience the environment. After certain amount of episodes, it ultimately finds the best state-action pairs Q(s,a) for the algorithm, the so called \"Q-values\". The Q-values represent how good it is for an agent (\ud83d\udc39) to be in state in terms of getting the long term reward (\ud83e\udd5c). This algorithm has various implementations depending on which policy is chosen for taking actions or how Q-values are processed. The picture below shows an overview on the implementation of this model-free algorithm from the Temporal Difference learning (TD) method [Sutton & Barto 2017].\n\n![title](https:\/\/raw.githubusercontent.com\/tomtx\/q-learning-demo\/master\/img\/algo-method.png)\n> _... let's see the implementation!_ \ud83d\udee0\n\n### Table \ud83c\udf7d\nThe Q-values are stored in the **Q-table** and are updated during training of the algorithm. This table has rows representing states (s) and columns representing possible actions (a). It is initialized with some values at start and gets populated with different values as the algorithm proceeds. It ultimately reaches some optimal values for an agent to choose (higher Q-values are better), once it has learned how to solve the environment. This means that Q-values are learned after a certain amount of episodes (training steps\/iterations) & can be then used to play the game (solve the environment) better than without any prior training.","1bafac2f":"_... let's **play the game**_","f3173284":"The Q-table for our case is shown in the drawing above.\n* it will have **4 columns** representing possible actions (U for up, D for down, L for left, R for right) & **16 rows** representing all states for this environment\n* the Q-table is **initialized with zeros** at start of training\n\n#### estimation \u23f3\nWe will use the equation below to estimate the Q-values for the table scenario of Q-learning. This equation is just rewritten from the earlier introduced definition of Q-learning. It somehow expresses the learning process by the following observation. The first term on the right-hand side, is the reduced learning rate (1\u200a-\u200a\u03b1) multiplied by the old Q-value (current knowledge). Then, this is added to the learning rate (\u03b1) multiplied by the learned value (new knowledge). The learned value is the last term in parenthesis on the right-hand side, that contains rewards (R) and discounted best actions for the new (observed) state.","ec63fcf9":"_... define the **epsilon-greedy bandit**_","8051cd1e":">_... the training progress of this algorithm can be seen from the figure above, where each episode finishes either with value 0 or 1 as a reward -> this means that our agent \ud83d\udc39 fell into the ice hole \ud83d\udd73 or found those peanuts \ud83e\udd5c, respectively_\n> * the **algorithm is able to solve the environment more often as it learns** and starts exploiting its own knowledge more with increasing episodes\n> * with increasing episodes, **our agent also survives longer in the environment** without falling into the ice hole and eventually reaches the reward... how efficiently it reaches the goal is questionable though, since the ice is slippery and not being dead under it is probably important too :)","e3aceb15":"_... run the **Q-learning** for the test environment_","8a48129a":"_... set the **hyperparameters** for the model_","b739d1a0":"_... set the **hyperparameters for the game**_","f7ffdf14":"## Conclusions \ud83e\udd14\nWe have built a simple table model for Q-learning solving an RL problem of a frozen lake. As seen from running the code, our model learns during training and then takes advantage of such prior experience when solving the environment during playing. You can tweak the hyperparameters for learning rate & discount factor, or even implement different bandit strategies for choosing actions to see if it learns better during training.","c4a6cd8a":"_... print the **training progress** of the algorithm_"}}