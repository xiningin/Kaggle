{"cell_type":{"72196541":"code","39616457":"code","3accc2d3":"code","6ba09b30":"code","eef56c77":"code","f6dba9c6":"code","72a4bf9e":"code","c835aa23":"code","c181368b":"code","3e566aca":"code","99cd98e7":"code","9596703f":"code","c9083258":"code","dbb87c6d":"code","a6100743":"code","08361ecf":"code","7995178d":"code","c2d89d43":"code","20b84504":"code","f5abde09":"code","d2c01828":"code","6a79f9d8":"code","f7adf100":"code","b6628861":"code","0ea1f309":"code","6d5d942d":"code","78298e16":"code","2b05e2f0":"code","57d9d602":"code","79a8b3eb":"code","4cdd4e76":"code","3dfdf422":"code","b49b784c":"code","00232f6b":"code","c60188c2":"code","e3ef5fde":"code","350ea6f2":"code","53bce1d1":"code","2c3403c8":"code","3bc4de9e":"code","37c708d0":"code","db4a7ace":"code","2dc7b0df":"code","52ed99d6":"code","bb4b91d7":"code","f349c20b":"code","f498a66b":"code","23f344b4":"markdown","6a2338e9":"markdown"},"source":{"72196541":"!pip install xgboost\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_iris\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score","39616457":"train= pd.read_csv('..\/input\/siim-isic-melanoma-classification\/train.csv')\ntest= pd.read_csv('..\/input\/siim-isic-melanoma-classification\/test.csv')\nsub= pd.read_csv('..\/input\/siim-isic-melanoma-classification\/sample_submission.csv')","3accc2d3":"train['sex'] = train['sex'].fillna('na')\ntrain['age_approx'] = train['age_approx'].fillna(0)\ntrain['anatom_site_general_challenge'] = train['anatom_site_general_challenge'].fillna('na')\n\ntest['sex'] = test['sex'].fillna('na')\ntest['age_approx'] = test['age_approx'].fillna(0)\ntest['anatom_site_general_challenge'] = test['anatom_site_general_challenge'].fillna('na')\ntrain['sex'] = train['sex'].astype(\"category\").cat.codes +1\ntrain['anatom_site_general_challenge'] = train['anatom_site_general_challenge'].astype(\"category\").cat.codes +1\ntrain.head()","6ba09b30":"test['sex'] = test['sex'].astype(\"category\").cat.codes +1\ntest['anatom_site_general_challenge'] = test['anatom_site_general_challenge'].astype(\"category\").cat.codes +1\ntest.head()","eef56c77":"x_train = train[['sex', 'age_approx','anatom_site_general_challenge']]\ny_train = train['target']\n\n\nx_test = test[['sex', 'age_approx','anatom_site_general_challenge']]\n# y_train = test['target']\n\n\ntrain_DMatrix = xgb.DMatrix(x_train, label= y_train)\ntest_DMatrix = xgb.DMatrix(x_test)","f6dba9c6":"clf = xgb.XGBClassifier(n_estimators=2600, \n                        max_depth=20, \n                        objective='multi:softprob',\n                        seed=0,  \n                        nthread=-1, \n                        learning_rate=0.15, \n                        num_class = 2, \n                        scale_pos_weight = (32542\/584))\n\nclf.fit(x_train, y_train)","72a4bf9e":"clf.predict_proba(x_test)[:,1]\n# clf.predict(x_test)\nsub.target = clf.predict_proba(x_test)[:,1]\nsub_tabular = sub.copy()","c835aa23":"#sub_tabular.to_csv('submission.csv', index = False)","c181368b":"##Ensemble for 0.9565\n# sub_new = pd.read_csv('..\/input\/siimisic\/sub-new.csv')\n# sub_mean = pd.read_csv('\/kaggle\/input\/siimisic\/submission_mean.csv')\n# sub.target = sub_mean.target *0.2 + sub_new.target *0.6 + sub_tabular.target *0.2\n# sub.head()","3e566aca":"sub_tabular","99cd98e7":"#ens_sub = pd.read_csv('..\/input\/actual-output\/submission.csv')\n#ens_sub.to_csv('ens_sub.csv', index = False)","9596703f":"sub['target'] = (ens_sub.target - sub_tabular.target*0.2)\/0.8\nsub.to_csv('efn_sub.csv', index = False)","c9083258":"'''from scipy import stats\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import f1_score\n\nclf_xgb = XGBClassifier(objective = 'binary:logistic')\nparam_dist = {'n_estimators': stats.randint(150, 500),\n              'learning_rate': stats.uniform(0.01, 0.07),\n              'subsample': stats.uniform(0.3, 0.7),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': stats.uniform(0.5, 0.45),\n              'min_child_weight': [1, 2, 3]\n             }\nclf = RandomizedSearchCV(clf_xgb, param_distributions = param_dist, n_iter = 25, scoring = 'f1', error_score = 0, verbose = 3, n_jobs = -1)\n\nnumFolds = 5\nfolds = KFold(n_splits = numFolds, shuffle = True)\n\nestimators = []\nresults = np.zeros(len(x_train))\nscore = 0.0\nfor train_index, test_index in folds.split(x_train):\n    X_train, X_test = x_train[train_index,:], x_train.iloc[test_index,:]\n    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()\n    clf.fit(X_train, y_train)\n\n    estimators.append(clf.best_estimator_)\n    results[test_index] = clf.predict(X_test)\n    score += f1_score(y_test, results[test_index])\nscore \/= numFolds'''","dbb87c6d":"from scipy import stats\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import f1_score\n\nclf_xgb = XGBClassifier(objective = 'multi:softprob')\nparam_dist = {'n_estimators': stats.randint(150, 500),\n              'learning_rate': stats.uniform(0.01, 0.07),\n              'subsample': stats.uniform(0.3, 0.7),\n               'num_class':  (2,3),\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'colsample_bytree': stats.uniform(0.5, 0.45),\n              'min_child_weight': [1, 2, 3]\n             }\nclf = RandomizedSearchCV(clf_xgb, param_distributions = param_dist, n_iter = 25, scoring = 'f1', error_score = 0, verbose = 3, n_jobs = -1)\n","a6100743":"clf.fit(x_train, y_train)","08361ecf":"clf.best_params_\n","7995178d":"clf1 = xgb.XGBClassifier(n_estimators=432, \n                        max_depth=7, \n                        objective='multi:softprob',\n                        seed=0,  \n                        nthread=-1, \n                        learning_rate=0.015573109334896062,\n                        num_class = 2, \n                        scale_pos_weight = (32542\/584))\n\nclf1.fit(x_train, y_train)","c2d89d43":"clf.predict_proba(x_test)[:,1]\n# clf.predict(x_test)\nsub.target = clf.predict_proba(x_test)[:,1]\nsub_tabular = sub.copy()","20b84504":"sub_tabular.to_csv('submission.csv', index=False)","f5abde09":"'''import numpy as np\nimport pandas as pd\nimport xgboost as xgb\n#from xgboost.sklearn import  XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n#from sklearn import cross_validation, metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.grid_search import GridSearchCV\n\n\ndef modelFit(alg, X, y, useTrainCV=True, cvFolds=5, early_stopping_rounds=50):\n    if useTrainCV:\n        #xgbParams = alg.get_xgb_params()\n        xgTrain = xgb.DMatrix(X, label=y)\n        cvresult = xgb.cv(xgbParams,\n                      xgTrain,\n                      num_boost_round=alg.get_params()['n_estimators'],\n                      nfold=cvFolds,\n                      stratified=True,\n                      metrics={'mlogloss'},\n                      early_stopping_rounds=early_stopping_rounds,\n                      seed=0,\n                      callbacks=[xgb.callback.print_evaluation(show_stdv=False),                                                               xgb.callback.early_stop(3)])\n\n        print(cvresult)\n        alg.set_params(n_estimators=cvresult.shape[0])\n\n    # Fit the algorithm\n    alg.fit(X, y, eval_metric='mlogloss')\n\n    # Predict\n    dtrainPredictions = alg.predict(X)\n    dtrainPredProb = alg.predict_proba(X)\n\n    # Print model report:\n    print(\"\\nModel Report\")\n    print(\"Classification report: \\n\")\n    print(classification_report(y_val, y_val_pred))\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrainPredictions))\n    print(\"Log Loss Score (Train): %f\" % metrics.log_loss(y, dtrainPredProb))\n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n\n\n# 1) Read training set\nprint('>> Read training set')\n#train = pd.read_csv(trainFile)\n\n# 2) Extract target attribute and convert to numeric\nprint('>> Preprocessing')\ny_train = y_train\n#train['OutcomeType'].values\nle_y = LabelEncoder()\ny_train = le_y.fit_transform(y_train)\n#train.drop('OutcomeType', axis=1, inplace=True)\n\n# 4) Extract features and target from training set\nX_train = x_train\n\n# 5) First classifier\nxgb = xgb(learning_rate =0.1,\n                    n_estimators=1000,\n                    max_depth=5,\n                    min_child_weight=1,\n                    gamma=0,\n                    subsample=0.8,\n                    colsample_bytree=0.8,\n                    scale_pos_weight=1,\n                    objective='multi:softprob',\n                    seed=27)\n\nmodelFit(xgb, X_train, y_train)'''","d2c01828":"import xgboost as xgb","6a79f9d8":"from sklearn.model_selection import train_test_split\nx_train1,  x_test1, y_train1, y_test1=train_test_split(x_train, y_train, test_size=0.3, random_state=3)","f7adf100":"x_train1","b6628861":"y_train1","0ea1f309":"dtrain = xgb.DMatrix(x_train1, label=y_train1)\ndtest = xgb.DMatrix(x_test1, label=y_test1)","6d5d942d":"from sklearn.metrics import mean_absolute_error\nimport numpy as np\n# \"Learn\" the mean from the training data\nmean_train = np.mean(y_train)\n# Get predictions on the test set\nbaseline_predictions = np.ones(y_test1.shape) * mean_train\n# Compute MAE\nmae_baseline = mean_absolute_error(y_test1, baseline_predictions)\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n#Baseline MAE is 11.31","78298e16":"params = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:linear',\n}","2b05e2f0":"params['eval_metric'] = \"mae\"","57d9d602":"num_boost_round = 999","79a8b3eb":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","4cdd4e76":"print(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","3dfdf422":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\ncv_results","b49b784c":"cv_results['test-mae-mean'].min()","00232f6b":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]","c60188c2":"# Define initial best params and MAE\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","e3ef5fde":"params['max_depth'] = 10\nparams['min_child_weight'] = 6","350ea6f2":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]","53bce1d1":"min_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","2c3403c8":"params['subsample'] = .8\nparams['colsample_bytree'] = 1.","3bc4de9e":"%time\n# This can take some time\u2026\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    %time cv_results = xgb.cv(params, dtrain,num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'],early_stopping_rounds=10)\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","37c708d0":"params['eta'] = .01","db4a7ace":"params\n{'colsample_bytree': 1.0,\n 'eta': 0.01,\n 'eval_metric': 'mae',\n 'max_depth': 10,\n 'min_child_weight': 6,\n 'objective': 'reg:linear',\n 'subsample': 0.8}","2dc7b0df":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","52ed99d6":"num_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","bb4b91d7":"mean_absolute_error(best_model.predict(dtest), y_test1)","f349c20b":"best_model.save_model(\"my_model.model\")","f498a66b":"loaded_model = xgb.Booster()\nloaded_model.load_model(\"my_model.model\")\n# And use it for predictions.\nloaded_model.predict(dtest)","23f344b4":"# xgboost cross validation","6a2338e9":"this notebook is edit of kernel \nhttps:\/\/www.kaggle.com\/ks2019\/siim-isic-notebook-0-9565-submission\nplease upvote him i only add hyperparametr tunning part "}}