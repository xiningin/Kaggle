{"cell_type":{"64da81a5":"code","d658c195":"code","148b61b8":"code","93a1bf86":"code","71452074":"code","82c87f23":"code","039d7db3":"code","3d28a1c1":"code","b78e18eb":"code","6690ac41":"markdown"},"source":{"64da81a5":"#import important packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#used to scale the data\nfrom sklearn.preprocessing import StandardScaler\n#kfold for the data\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n#Labelencoder for target\nfrom sklearn.preprocessing import LabelEncoder\n#classifiers used\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#used to check best classifier\nfrom sklearn.model_selection import cross_val_score","d658c195":"#use pandas to import data to a dataframe\ndata = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n#sort data by quality\ndata.head()","148b61b8":"#make the quality either good or bad by using 0 for bad and 1 for good\ndata['quality'] = data.quality.map({3:0,4:0,5:0,6:0,7:1,8:1})","93a1bf86":"#split the data and target from all the data\nX = data[data.columns[:11]]\ny = data[data.columns[11]]","71452074":"#make the data all float64\nX = X.astype('float64')\ny = LabelEncoder().fit_transform(y.astype('str'))","82c87f23":"#now scale the data to make all the data scaled between 0 to 1\ntrans_norm = StandardScaler()\nX = trans_norm.fit_transform(X)\nX= pd.DataFrame(X)","039d7db3":"# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","3d28a1c1":"# models to check for best results\ndef get_models():\n    models = dict()\n    models['ExT'] = ExtraTreesClassifier(max_features = 'log2', min_samples_leaf =1, n_estimators = 1000)\n    models['Rnd'] = RandomForestClassifier(max_features = 'sqrt', min_samples_leaf =1, n_estimators = 1000)\n    models['Gb'] = GradientBoostingClassifier(max_features = 'log2', min_samples_leaf =2, n_estimators = 1000)\n    return models","b78e18eb":"# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","6690ac41":"##ExtraTreesClassifier and RandomforestClassifier produces the best results with 0.916 mean score.\n\n##Thanks for view my notebook, please write a comment and upvote if you like.\n\n##All the best\n##Gmanik"}}