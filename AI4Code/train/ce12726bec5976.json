{"cell_type":{"9b13e80d":"code","9269b2d4":"code","66c5ab83":"code","b9338bea":"code","3ee0b669":"code","a93bee59":"code","daa810f8":"code","0dd6b9e5":"code","74c90d7d":"code","b3fb4a21":"code","de0e7dcd":"code","95fdb6f8":"code","21b4aae7":"code","ca0ed424":"code","e43f9fbd":"code","2d8f6619":"code","d228ddee":"code","723f6b66":"code","a7ef5059":"code","e5cd8931":"code","a3817ee7":"code","4e8dcbfa":"code","41498f1e":"code","34e0d746":"code","1c432e0d":"code","5eba25d0":"code","16eb5838":"code","868b7afb":"code","ae6b7685":"code","b8e4a700":"code","4291904c":"code","45ded217":"code","63b28070":"code","2afbe256":"code","bfdb34af":"code","02946345":"code","4666ed35":"code","f51a129b":"markdown","51f1a55d":"markdown","b1a6ec4c":"markdown","a05049d6":"markdown","7e84b79f":"markdown","9efb878a":"markdown","4a01c4d0":"markdown","913a2833":"markdown","0a3d281e":"markdown","f1bb6ba1":"markdown","80d19f5a":"markdown","8891e918":"markdown","18668c07":"markdown","8b8c0ef4":"markdown","95ff6319":"markdown"},"source":{"9b13e80d":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport cudf, cuml, cupy\nfrom textwrap import wrap\n\nimport gc\n\nimport itertools\nimport collections\nfrom collections import Counter\n\nimport re\nfrom wordcloud import WordCloud\n\nimport os\nprint(os.listdir('\/kaggle\/input\/shopee-product-matching\/'))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","9269b2d4":"base_dir = '\/kaggle\/input\/shopee-product-matching\/'","66c5ab83":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","b9338bea":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","3ee0b669":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","a93bee59":"print(f'Number of train images: {len(os.listdir(base_dir + \"train_images\/\"))}')\nprint(f'Number of test images: {len(os.listdir(base_dir + \"test_images\/\"))}')","daa810f8":"train['image_path'] = base_dir + 'train_images\/' + train['image']\ntest['image_path'] = base_dir + 'test_images\/' + test['image']","0dd6b9e5":"temp = train.groupby('label_group')['posting_id'].agg('unique').to_dict()\ntrain['target'] = train['label_group'].map(temp)\ntrain.head(2)","74c90d7d":"def get_f1score(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n \/ (len(row.target) + len(row[col]))\n    return f1score","b3fb4a21":"#To calculate F1 score - local\ntemp = train.groupby('image_phash')['posting_id'].agg('unique').to_dict()\ntrain['oof'] = train['image_phash'].map(temp)","de0e7dcd":"train['f1_base'] = train.apply(get_f1score('oof'), axis = 1)\nprint(f\"Train F1 Score: {round(train['f1_base'].mean(), 3)}\")","95fdb6f8":"#For submission test set will be replaced with bigger(70k) dataset\nif len(test) == 3:\n    df = train\n    img_dir = '..\/input\/shopee-product-matching\/train_images\/'\n    df_text = df[['title']]\n    print(df.shape)\nelse:\n    df = test\n    img_dir = '..\/input\/shopee-product-matching\/test_images\/'\n    df_text = df[['title']]\n    print(df.shape)","21b4aae7":"import string\n\ndef preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","ca0ed424":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","e43f9fbd":"from cuml.feature_extraction.text import TfidfVectorizer\n\ntfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","2d8f6619":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) \/ chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)","d228ddee":"df['preds_txt1'] = preds\ndf.head(2)","723f6b66":"print(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt1'), axis = 1).mean(), 3)}\")","a7ef5059":"import string\n\ndef preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        #x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","e5cd8931":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","a3817ee7":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","4e8dcbfa":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) \/ chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","41498f1e":"df['preds_txt2'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt2'), axis = 1).mean(), 3)}\")","34e0d746":"#from https:\/\/github.com\/stopwords-iso\/stopwords-id\nindo_stopwords = [\n                  \"ada\",\"adalah\",\"adanya\",\"adapun\",\"agak\",\"agaknya\",\"agar\",\"akan\",\"akankah\",\"akhir\",\"akhiri\",\n                  \"akhirnya\",\"aku\",\"akulah\",\"amat\",\"amatlah\",\"anda\",\"andalah\",\"antar\",\"antara\",\"antaranya\",\"apa\",\n                  \"apaan\",\"apabila\",\"apakah\",\"apalagi\",\"apatah\",\"artinya\",\"asal\",\"asalkan\",\"atas\",\"atau\",\"ataukah\",\n                  \"ataupun\",\"awal\",\"awalnya\",\"bagai\",\"bagaikan\",\"bagaimana\",\"bagaimanakah\",\"bagaimanapun\",\"bagi\",\n                  \"bagian\",\"bahkan\",\"bahwa\",\"bahwasanya\",\"baik\",\"bakal\",\"bakalan\",\"balik\",\"banyak\",\"bapak\",\"baru\",\n                  \"bawah\",\"beberapa\",\"begini\",\"beginian\",\"beginikah\",\"beginilah\",\"begitu\",\"begitukah\",\"begitulah\",\n                  \"begitupun\",\"bekerja\",\"belakang\",\"belakangan\",\"belum\",\"belumlah\",\"benar\",\"benarkah\",\"benarlah\",\n                  \"berada\",\"berakhir\",\"berakhirlah\",\"berakhirnya\",\"berapa\",\"berapakah\",\"berapalah\",\"berapapun\",\n                  \"berarti\",\"berawal\",\"berbagai\",\"berdatangan\",\"beri\",\"berikan\",\"berikut\",\"berikutnya\",\"berjumlah\",\n                  \"berkali-kali\",\"berkata\",\"berkehendak\",\"berkeinginan\",\"berkenaan\",\"berlainan\",\"berlalu\",\n                  \"berlangsung\",\"berlebihan\",\"bermacam\",\"bermacam-macam\",\"bermaksud\",\"bermula\",\"bersama\",\n                  \"bersama-sama\",\"bersiap\",\"bersiap-siap\",\"bertanya\",\"bertanya-tanya\",\"berturut\",\"berturut-turut\",\n                  \"bertutur\",\"berujar\",\"berupa\",\"besar\",\"betul\",\"betulkah\",\"biasa\",\"biasanya\",\"bila\",\"bilakah\",\n                  \"bisa\",\"bisakah\",\"boleh\",\"bolehkah\",\"bolehlah\",\"buat\",\"bukan\",\"bukankah\",\"bukanlah\",\"bukannya\",\n                  \"bulan\",\"bung\",\"cara\",\"caranya\",\"cukup\",\"cukupkah\",\"cukuplah\",\"cuma\",\"dahulu\",\"dalam\",\"dan\",\"dapat\",\n                  \"dari\",\"daripada\",\"datang\",\"dekat\",\"demi\",\"demikian\",\"demikianlah\",\"dengan\",\"depan\",\"di\",\"dia\",\n                  \"diakhiri\",\"diakhirinya\",\"dialah\",\"diantara\",\"diantaranya\",\"diberi\",\"diberikan\",\"diberikannya\",\n                  \"dibuat\",\"dibuatnya\",\"didapat\",\"didatangkan\",\"digunakan\",\"diibaratkan\",\"diibaratkannya\",\"diingat\",\n                  \"diingatkan\",\"diinginkan\",\"dijawab\",\"dijelaskan\",\"dijelaskannya\",\"dikarenakan\",\"dikatakan\",\n                  \"dikatakannya\",\"dikerjakan\",\"diketahui\",\"diketahuinya\",\"dikira\",\"dilakukan\",\"dilalui\",\"dilihat\",\n                  \"dimaksud\",\"dimaksudkan\",\"dimaksudkannya\",\"dimaksudnya\",\"diminta\",\"dimintai\",\"dimisalkan\",\"dimulai\",\n                  \"dimulailah\",\"dimulainya\",\"dimungkinkan\",\"dini\",\"dipastikan\",\"diperbuat\",\"diperbuatnya\",\n                  \"dipergunakan\",\"diperkirakan\",\"diperlihatkan\",\"diperlukan\",\"diperlukannya\",\"dipersoalkan\",\n                  \"dipertanyakan\",\"dipunyai\",\"diri\",\"dirinya\",\"disampaikan\",\"disebut\",\"disebutkan\",\"disebutkannya\",\n                  \"disini\",\"disinilah\",\"ditambahkan\",\"ditandaskan\",\"ditanya\",\"ditanyai\",\"ditanyakan\",\"ditegaskan\",\n                  \"ditujukan\",\"ditunjuk\",\"ditunjuki\",\"ditunjukkan\",\"ditunjukkannya\",\"ditunjuknya\",\"dituturkan\",\n                  \"dituturkannya\",\"diucapkan\",\"diucapkannya\",\"diungkapkan\",\"dong\",\"dua\",\"dulu\",\"empat\",\"enggak\",\n                  \"enggaknya\",\"entah\",\"entahlah\",\"guna\",\"gunakan\",\"hal\",\"hampir\",\"hanya\",\"hanyalah\",\"hari\",\"harus\",\n                  \"haruslah\",\"harusnya\",\"hendak\",\"hendaklah\",\"hendaknya\",\"hingga\",\"ia\",\"ialah\",\"ibarat\",\"ibaratkan\",\n                  \"ibaratnya\",\"ibu\",\"ikut\",\"ingat\",\"ingat-ingat\",\"ingin\",\"inginkah\",\"inginkan\",\"ini\",\"inikah\",\"inilah\",\n                  \"itu\",\"itukah\",\"itulah\",\"jadi\",\"jadilah\",\"jadinya\",\"jangan\",\"jangankan\",\"janganlah\",\"jauh\",\"jawab\",\n                  \"jawaban\",\"jawabnya\",\"jelas\",\"jelaskan\",\"jelaslah\",\"jelasnya\",\"jika\",\"jikalau\",\"juga\",\"jumlah\",\n                  \"jumlahnya\",\"justru\",\"kala\",\"kalau\",\"kalaulah\",\"kalaupun\",\"kalian\",\"kami\",\"kamilah\",\"kamu\",\n                  \"kamulah\",\"kan\",\"kapan\",\"kapankah\",\"kapanpun\",\"karena\",\"karenanya\",\"kasus\",\"kata\",\"katakan\",\n                  \"katakanlah\",\"katanya\",\"ke\",\"keadaan\",\"kebetulan\",\"kecil\",\"kedua\",\"keduanya\",\"keinginan\",\n                  \"kelamaan\",\"kelihatan\",\"kelihatannya\",\"kelima\",\"keluar\",\"kembali\",\"kemudian\",\"kemungkinan\",\n                  \"kemungkinannya\",\"kenapa\",\"kepada\",\"kepadanya\",\"kesampaian\",\"keseluruhan\",\"keseluruhannya\",\n                  \"keterlaluan\",\"ketika\",\"khususnya\",\"kini\",\"kinilah\",\"kira\",\"kira-kira\",\"kiranya\",\"kita\",\"kitalah\",\n                  \"kok\",\"kurang\",\"lagi\",\"lagian\",\"lah\",\"lain\",\"lainnya\",\"lalu\",\"lama\",\"lamanya\",\"lanjut\",\"lanjutnya\",\n                  \"lebih\",\"lewat\",\"lima\",\"luar\",\"macam\",\"maka\",\"makanya\",\"makin\",\"malah\",\"malahan\",\"mampu\",\"mampukah\",\n                  \"mana\",\"manakala\",\"manalagi\",\"masa\",\"masalah\",\"masalahnya\",\"masih\",\"masihkah\",\"masing\",\n                  \"masing-masing\",\"mau\",\"maupun\",\"melainkan\",\"melakukan\",\"melalui\",\"melihat\",\"melihatnya\",\"memang\",\n                  \"memastikan\",\"memberi\",\"memberikan\",\"membuat\",\"memerlukan\",\"memihak\",\"meminta\",\"memintakan\",\n                  \"memisalkan\",\"memperbuat\",\"mempergunakan\",\"memperkirakan\",\"memperlihatkan\",\"mempersiapkan\",\n                  \"mempersoalkan\",\"mempertanyakan\",\"mempunyai\",\"memulai\",\"memungkinkan\",\"menaiki\",\"menambahkan\",\n                  \"menandaskan\",\"menanti\",\"menanti-nanti\",\"menantikan\",\"menanya\",\"menanyai\",\"menanyakan\",\"mendapat\",\n                  \"mendapatkan\",\"mendatang\",\"mendatangi\",\"mendatangkan\",\"menegaskan\",\"mengakhiri\",\"mengapa\",\n                  \"mengatakan\",\"mengatakannya\",\"mengenai\",\"mengerjakan\",\"mengetahui\",\"menggunakan\",\"menghendaki\",\n                  \"mengibaratkan\",\"mengibaratkannya\",\"mengingat\",\"mengingatkan\",\"menginginkan\",\"mengira\",\"mengucapkan\",\n                  \"mengucapkannya\",\"mengungkapkan\",\"menjadi\",\"menjawab\",\"menjelaskan\",\"menuju\",\"menunjuk\",\"menunjuki\",\n                  \"menunjukkan\",\"menunjuknya\",\"menurut\",\"menuturkan\",\"menyampaikan\",\"menyangkut\",\"menyatakan\",\n                  \"menyebutkan\",\"menyeluruh\",\"menyiapkan\",\"merasa\",\"mereka\",\"merekalah\",\"merupakan\",\"meski\",\"meskipun\",\n                  \"meyakini\",\"meyakinkan\",\"minta\",\"mirip\",\"misal\",\"misalkan\",\"misalnya\",\"mula\",\"mulai\",\"mulailah\",\n                  \"mulanya\",\"mungkin\",\"mungkinkah\",\"nah\",\"naik\",\"namun\",\"nanti\",\"nantinya\",\"nyaris\",\"nyatanya\",\"oleh\",\n                  \"olehnya\",\"pada\",\"padahal\",\"padanya\",\"pak\",\"paling\",\"panjang\",\"pantas\",\"para\",\"pasti\",\"pastilah\",\n                  \"penting\",\"pentingnya\",\"per\",\"percuma\",\"perlu\",\"perlukah\",\"perlunya\",\"pernah\",\"persoalan\",\"pertama\",\n                  \"pertama-tama\",\"pertanyaan\",\"pertanyakan\",\"pihak\",\"pihaknya\",\"pukul\",\"pula\",\"pun\",\"punya\",\"rasa\",\n                  \"rasanya\",\"rata\",\"rupanya\",\"saat\",\"saatnya\",\"saja\",\"sajalah\",\"saling\",\"sama\",\"sama-sama\",\n                  \"sambil\",\"sampai\",\"sampai-sampai\",\"sampaikan\",\"sana\",\"sangat\",\"sangatlah\",\"satu\",\"saya\",\n                  \"sayalah\",\"se\",\"sebab\",\"sebabnya\",\"sebagai\",\"sebagaimana\",\"sebagainya\",\"sebagian\",\"sebaik\",\n                  \"sebaik-baiknya\",\"sebaiknya\",\"sebaliknya\",\"sebanyak\",\"sebegini\",\"sebegitu\",\"sebelum\",\n                  \"sebelumnya\",\"sebenarnya\",\"seberapa\",\"sebesar\",\"sebetulnya\",\"sebisanya\",\"sebuah\",\"sebut\",\n                  \"sebutlah\",\"sebutnya\",\"secara\",\"secukupnya\",\"sedang\",\"sedangkan\",\"sedemikian\",\"sedikit\",\n                  \"sedikitnya\",\"seenaknya\",\"segala\",\"segalanya\",\"segera\",\"seharusnya\",\"sehingga\",\"seingat\",\n                  \"sejak\",\"sejauh\",\"sejenak\",\"sejumlah\",\"sekadar\",\"sekadarnya\",\"sekali\",\"sekali-kali\",\"sekalian\",\n                  \"sekaligus\",\"sekalipun\",\"sekarang\",\"sekecil\",\"seketika\",\"sekiranya\",\"sekitar\",\"sekitarnya\",\n                  \"sekurang-kurangnya\",\"sekurangnya\",\"sela\",\"selagi\",\"selain\",\"selaku\",\"selalu\",\"selama\",\n                  \"selama-lamanya\",\"selamanya\",\"selanjutnya\",\"seluruh\",\"seluruhnya\",\"semacam\",\"semakin\",\n                  \"semampu\",\"semampunya\",\"semasa\",\"semasih\",\"semata\",\"semata-mata\",\"semaunya\",\"sementara\",\n                  \"semisal\",\"semisalnya\",\"sempat\",\"semua\",\"semuanya\",\"semula\",\"sendiri\",\"sendirian\",\"sendirinya\",\n                  \"seolah\",\"seolah-olah\",\"seorang\",\"sepanjang\",\"sepantasnya\",\"sepantasnyalah\",\"seperlunya\",\n                  \"seperti\",\"sepertinya\",\"sepihak\",\"sering\",\"seringnya\",\"serta\",\"serupa\",\"sesaat\",\"sesama\",\n                  \"sesampai\",\"sesegera\",\"sesekali\",\"seseorang\",\"sesuatu\",\"sesuatunya\",\"sesudah\",\"sesudahnya\",\n                  \"setelah\",\"setempat\",\"setengah\",\"seterusnya\",\"setiap\",\"setiba\",\"setibanya\",\"setidak-tidaknya\",\n                  \"setidaknya\",\"setinggi\",\"seusai\",\"sewaktu\",\"siap\",\"siapa\",\"siapakah\",\"siapapun\",\"sini\",\"sinilah\",\n                  \"soal\",\"soalnya\",\"suatu\",\"sudah\",\"sudahkah\",\"sudahlah\",\"supaya\",\"tadi\",\"tadinya\",\"tahu\",\"tahun\",\n                  \"tak\",\"tambah\",\"tambahnya\",\"tampak\",\"tampaknya\",\"tandas\",\"tandasnya\",\"tanpa\",\"tanya\",\"tanyakan\",\n                  \"tanyanya\",\"tapi\",\"tegas\",\"tegasnya\",\"telah\",\"tempat\",\"tengah\",\"tentang\",\"tentu\",\"tentulah\",\n                  \"tentunya\",\"tepat\",\"terakhir\",\"terasa\",\"terbanyak\",\"terdahulu\",\"terdapat\",\"terdiri\",\"terhadap\",\n                  \"terhadapnya\",\"teringat\",\"teringat-ingat\",\"terjadi\",\"terjadilah\",\"terjadinya\",\"terkira\",\n                  \"terlalu\",\"terlebih\",\"terlihat\",\"termasuk\",\"ternyata\",\"tersampaikan\",\"tersebut\",\"tersebutlah\",\n                  \"tertentu\",\"tertuju\",\"terus\",\"terutama\",\"tetap\",\"tetapi\",\"tiap\",\"tiba\",\"tiba-tiba\",\"tidak\",\n                  \"tidakkah\",\"tidaklah\",\"tiga\",\"tinggi\",\"toh\",\"tunjuk\",\"turut\",\"tutur\",\"tuturnya\",\"ucap\",\"ucapnya\",\n                  \"ujar\",\"ujarnya\",\"umum\",\"umumnya\",\"ungkap\",\"ungkapnya\",\"untuk\",\"usah\",\"usai\",\"waduh\",\"wah\",\"wahai\",\n                  \"waktu\",\"waktunya\",\"walau\",\"walaupun\",\"wong\",\"yaitu\",\"yakin\",\"yakni\",\"yang\"\n                 ]","1c432e0d":"def preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = ' '.join([word for word in x.split() if word not in indo_stopwords]) #stopwords\n        #x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","5eba25d0":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","16eb5838":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")","868b7afb":"%%time\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) \/ chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","ae6b7685":"df['preds_txt3'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt3'), axis = 1).mean(), 3)}\")","b8e4a700":"def preprocess(x):\n    try:\n        x = x.lower() #lower case\n        x = x.strip() #white space\n        x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #remove punctuations\n        x = ' '.join([word for word in x.split() if word not in indo_stopwords]) #stopwords\n        x = re.sub(r'[^a-z]', ' ', x) #Remove numbers and special characters\n    except:\n        None\n    return x","4291904c":"df_text['title_clean'] = df_text['title'].apply(lambda x: preprocess(x))\n#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title_clean']\ntitle_text.shape","45ded217":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")\n\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) \/ chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","63b28070":"df['preds_txt4'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt4'), axis = 1).mean(), 3)}\")","2afbe256":"#Convert to cudf to speed up\ntitle_text = cudf.DataFrame.from_pandas(df_text)['title']\ntitle_text.shape","bfdb34af":"tfid = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 25000)\n\ntext_embeddings = tfid.fit_transform(title_text).toarray()\nprint(f\"Title Text Embeddings shape: {text_embeddings.shape}\")\n\npreds = []\nchunk_size = 4096\n\nchunk_it = np.arange(np.ceil(len(df) \/ chunk_size))\n\nfor j in chunk_it: \n    a = int(j * chunk_size)\n    b = int((j + 1) * chunk_size)\n    b = min(b, len(df))\n    print('Processing chunk', a, 'to', b)\n    sim = cupy.matmul(text_embeddings, text_embeddings[a: b].T).T\n    for k in range(b - a):\n        idx = cupy.where(sim[k, ] > 0.75)[0]\n        post_ids = df.iloc[cupy.asnumpy(idx)]['posting_id'].values\n        preds.append(post_ids)\n\ndel text_embeddings\ngc.collect()","02946345":"df['preds_txt5'] = preds\nprint(f\"F1 Score for Text  : {round(df.apply(get_f1score('preds_txt5'), axis = 1).mean(), 3)}\")","4666ed35":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","f51a129b":"# Code Requirements\n\nSubmissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met:\n\n- CPU Notebook <= 9\n- GPU Notebook <= 2\n- Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named \"submission.csv\"","51f1a55d":"# 3. F1 Metric using custom Stopwords with numbers\/spl chars\n- Remove punctuations and remove malay stopwords","b1a6ec4c":"# 2. F1 Metric using Clean Text with numbers\/spl chars\n- Remove punctuations, lower case and strip white space","a05049d6":"# 5. F1 Metric title\n- No change","7e84b79f":"# Evaluation Metric\n\n__Submissions will be evaluated based on their mean F1 score.__","9efb878a":"In this notebook, I try to find the effect preprocessing of text has on the F1 score (considering only the title). I have tried:\n1. Using the title without any pre-processing\n2. Pre-process by removing punctuations, numbers and special characters\n3. Removing stopwords of the language used in the title\n4. Remove stopwords and numbers\/special characters","4a01c4d0":"__Shopee is the leading e-commerce platform in Southeast Asia and Taiwan.__","913a2833":"__Baseline F1 score using Image Phash provided__","0a3d281e":"# 4. F1 Metric using custom Stopwords\n- Remove punctuations numbers, special chars and indonesian stopwords","f1bb6ba1":"# Conclusion\n- Removing numbers and special characters results in higher F1 score (only Title)\n- Using custom stop words has a little effect","80d19f5a":"__Finding similar titles with Cosine Similarity__","8891e918":"# Competition Goal\n\n__In this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.__","18668c07":"# Find similar images using Text (title) embeddings","8b8c0ef4":"Many thanks to Chris Deotte @cdeotte for his great notebooks!!","95ff6319":"# 1. F1 Metric using Clean Text\n- Clean text of numbers, special characters, punctuations"}}