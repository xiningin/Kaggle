{"cell_type":{"2e50f50b":"code","84fc35b1":"code","1c7b7bf2":"code","50ef574b":"code","b6219ff0":"code","755dd007":"code","5cfcad55":"code","b09de726":"code","d6fe6076":"code","0d597297":"code","c41bce12":"code","938eb59e":"markdown","bd922cae":"markdown","49fcf8f7":"markdown","ae17e2a3":"markdown","2437fd38":"markdown","5106351a":"markdown"},"source":{"2e50f50b":"#------------------------------------\n# globals\n#------------------------------------\nEPOCHS           = 100\nVOCAB            = \"unicode\"# @[\"unicdoe\",\"grapheme\"]    \nclass ModelConfig:\n    num_layers       = 4               # number of encoder decoder layers\n    num_heads        = 8               # number of attention heads in MHA\n    d_model          = 128            # Embedding Dimension \n    dff              = 512             # Feed Forward netwrok Dimension\n    rate             = 0.1             # dropout rate \n    pe_max           = 50              # max positonal endocing \n    d_len            = 30             # data length\n    pad_value        = 0\n    start_value      = 1\n    end_value        = 2\n    inp_shape        = (30,)\n    tar_shape        = (30,)\n#-------------------------------\n# imports\n#-------------------------------\nimport os \nimport numpy as np\nimport json\nimport tensorflow as tf\nimport random\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom glob import glob\ntqdm.pandas()\n    \nconfig_json=\"..\/input\/dakshina-bnt-to-bn\/config.json\"\nwith open(config_json) as f:\n    config = json.load(f)\ninp_vocab=config[\"source_vocab\"]\ntgt_vocab=config[f\"{VOCAB[0]}_target_vocab\"]\n#-------------------------------\n# config update\n#-------------------------------\nModelConfig.inp_voclen=len(inp_vocab)\nModelConfig.tgt_voclen=len(tgt_vocab)\n\n#--------------\n# GCS\n#--------------\ndef get_tfrecs(_path):\n    gcs_pattern=os.path.join(_path,'*.tfrecord')\n    file_paths = tf.io.gfile.glob(gcs_pattern)\n    random.shuffle(file_paths)\n    return file_paths\n    \n\nGCS_PATH = KaggleDatasets().get_gcs_path('dakshina-bnt-to-bn')+\"\/tfrecords\/\"\ngp_train=GCS_PATH+\"train\/\"\ngp_eval =GCS_PATH+\"eval\/\"\ntrain_recs=get_tfrecs(gp_train)\neval_recs =get_tfrecs(gp_eval)\n# numbers\nnb_train  =int((len(train_recs)-1)*20480)\nnb_eval   =int((len(eval_recs)-1)*20480)\nprint(\"Train Data:\",nb_train,len(train_recs))\nprint(\"Eval Data:\",nb_eval,len(eval_recs))\n\n#----------------------------------------------------------\n# Detect hardware, return appropriate distribution strategy\n#----------------------------------------------------------\n# TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n#-------------------------------------\n# batching , strategy and steps\n#-------------------------------------\nif strategy.num_replicas_in_sync==1:\n    BATCH_SIZE = 32\nelse:\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\n# set    \nSTEPS_PER_EPOCH = nb_train\/\/BATCH_SIZE\nEVAL_STEPS      = nb_eval\/\/BATCH_SIZE\nprint(\"Steps:\",STEPS_PER_EPOCH)\nprint(\"Eval Steps:\",EVAL_STEPS)\nprint(\"Batch Size:\",BATCH_SIZE)\n#------------------------------\n# parsing tfrecords basic\n#------------------------------\ndef create_padding_mask(seq):\n    '''\n        creates padding mask: fixed pad value 0\n    '''\n    seq = tf.cast(tf.math.equal(seq,0), tf.float32)\n    # add extra dimensions to add the padding to the attention logits.\n    return seq[:,tf.newaxis, tf.newaxis, :] \n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    # (seq_len, seq_len)\n    return mask  \n    \n\ndef create_masks(inp,tar):\n    '''\n        create relevent masks:\n        args:\n            inp : source encoded\n            tar : target endcoded\n        returns:\n            mask     : Encoder padding mask\n                      * Used in the 2nd attention block in the decoder,\n                      * This padding mask is used to mask the encoder outputs.\n            comb_mask: look ahead mask\n                      * Used in the 1st attention block in the decoder.\n                      * It is used to pad and mask future tokens in the input received by the decoder.\n    '''\n    #mask\n    mask            = create_padding_mask(inp)\n    # lmask\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_mask        = create_padding_mask(tar)\n    combined_mask   = tf.maximum(dec_mask, look_ahead_mask)\n\n    return mask,combined_mask\n\n    \n    \n    \n\ndef data_input_fn(recs): \n    '''\n      This Function generates data from gcs\n      * The parser function should look similiar now because of datasetEDA\n    '''\n    def _parser(example):   \n        feature ={  \n                    'src'  : tf.io.FixedLenFeature([ModelConfig.d_len],tf.int64),\n                    'gtgt'  : tf.io.FixedLenFeature([ModelConfig.d_len],tf.int64),\n                    'utgt'  : tf.io.FixedLenFeature([ModelConfig.d_len],tf.int64)\n            \n\n        }    \n        parsed_example=tf.io.parse_single_example(example,feature)\n        # src\n        inp=parsed_example['src']\n        inp=tf.cast(inp, tf.int64)\n        if VOCAB==\"grapheme\": tgt=\"gtgt\"\n        else: tgt=\"utgt\"\n        # tar\n        tar=parsed_example[tgt]\n        tar=tf.cast(tar, tf.int64)\n        # mask\n        return inp,tar\n    # fixed code (for almost all tfrec training)\n    dataset = tf.data.TFRecordDataset(recs)\n    dataset = dataset.map(_parser)\n    dataset = dataset.shuffle(2048,reshuffle_each_iteration=True)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\n\ntrain_ds  =   data_input_fn(train_recs)\neval_ds   =   data_input_fn(eval_recs)\nfor x,y in train_ds.take(1):\n    print(\"---------------------------------------------------------------\")\n    print(\"source:\",x[0])\n    print(\"---------------------------------------------------------------\")\n    print(\"target:\",y[0])\n    print(\"---------------------------------------------------------------\")\n    print('Source Batch Shape:',x.shape)\n    print('Target Batch Shape:',y.shape)\n    \n    ","84fc35b1":"class Masking(tf.keras.layers.Layer):\n    def __init__(self,pad_value,size,**kwargs,):  \n        super().__init__(**kwargs)\n        self.pad_value = pad_value\n        self.size      = size\n        \n    def create_padding_mask(self,seq):\n        '''\n            creates padding mask: fixed pad value 0\n        '''\n        seq = tf.cast(tf.math.equal(seq,self.pad_value), tf.float32)\n        # add extra dimensions to add the padding to the attention logits.\n        return seq[:,tf.newaxis, tf.newaxis, :] \n\n    def create_look_ahead_mask(self,size):\n        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n        # (seq_len, seq_len)\n        return mask  \n\n\n    def call(self,inp,tar):\n        '''\n            create relevent masks:\n            args:\n                inp : source encoded\n                tar : target endcoded\n            returns:\n                mask     : Encoder padding mask\n                          * Used in the 2nd attention block in the decoder,\n                          * This padding mask is used to mask the encoder outputs.\n                comb_mask: look ahead mask\n                          * Used in the 1st attention block in the decoder.\n                          * It is used to pad and mask future tokens in the input received by the decoder.\n        '''\n        #mask\n        mask            = self.create_padding_mask(inp)\n        # lmask\n        look_ahead_mask = self.create_look_ahead_mask(self.size)\n        dec_mask        = self.create_padding_mask(tar)\n        combined_mask   = tf.maximum(dec_mask, look_ahead_mask)\n\n        return mask,combined_mask\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({'pad_value'   : self.pad_value,\n                       'size'        : self.size})\n        return config\n","1c7b7bf2":"import numpy as np\nclass PositionalEncoding(tf.keras.layers.Layer):\n    '''\n    tensorflow wrapper for positional encoding layer\n    args:\n      position  :   incoming sequence length\n      d_model   :   required embedding dim\n    '''\n    def __init__(self,position,d_model,use_scale,**kwargs,):  \n        super().__init__(**kwargs)\n        self.use_scale = use_scale\n        self.position  = position\n        self.d_model   = d_model\n        \n        \n    def call(self,x):\n        # pos encoding\n        pos=self.positional_encoding()\n        # input processing\n        seq_len = tf.shape(x)[1]\n        if self.use_scale:\n            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += pos[:, :seq_len, :]\n        return x \n    \n    def get_angles(self,pos, i):\n        angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(self.d_model))\n        return pos * angle_rates\n\n    def positional_encoding(self):\n        angle_rads = self.get_angles(np.arange(self.position)[:, np.newaxis],np.arange(self.d_model)[np.newaxis, :])\n        # apply sin to even indices in the array; 2i\n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        # apply cos to odd indices in the array; 2i+1\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n        pos_encoding = angle_rads[np.newaxis, ...]\n        return tf.cast(pos_encoding, dtype=tf.float32)\n\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'position'   : self.position,\n            'd_model'    : self.d_model,\n            'use_scale'  : self.use_scale\n        })\n        return config\n\n    \nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads,**kwargs,):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        assert d_model % num_heads == 0,\"Model Dimension Must be divideable by number of head provided\"\n        # attrs\n        self.num_heads   = num_heads\n        self.d_model     = d_model\n        self.depth       = self.d_model \/\/ self.num_heads\n        # ops\n        self.wq    = tf.keras.layers.Dense(d_model)\n        self.wk    = tf.keras.layers.Dense(d_model)\n        self.wv    = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        '''\n            * Split the last dimension into (num_heads, depth).\n            * Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        '''\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  ## (batch_size, seq_len, d_model)\n        k = self.wk(k)  ## (batch_size, seq_len, d_model)\n        v = self.wv(v)  ## (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  ## (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  ## (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  ## (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n        # (batch_size, seq_len_q, num_heads, depth)\n        scaled_attention                    = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n        # (batch_size, seq_len_q, d_model)\n        concat_attention                    = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  \n        # (batch_size, seq_len_q, d_model)\n        output                              = self.dense(concat_attention)  \n        return output, attention_weights\n    \n    def scaled_dot_product_attention(self,q, k, v, mask):\n        '''\n            Calculate the attention weights.\n\n            args:\n                q   : query shape == (..., seq_len_q, depth)\n                k   : key shape == (..., seq_len_k, depth)\n                v   : value shape == (..., seq_len_v, depth_v)\n                mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n            returns:\n                output, attention_weights\n            NOTES:\n            * q, k, v must have matching leading dimensions.\n            * k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n            * The mask has different shapes depending on its type(padding or look ahead) but it must be broadcastable for addition.\n\n        '''\n        ## (..., seq_len_q, seq_len_k)\n        matmul_qk = tf.matmul(q, k, transpose_b=True)  \n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n        # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n        ## (..., seq_len_q, seq_len_k)\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n        ## (..., seq_len_q, depth_v)\n        output = tf.matmul(attention_weights, v)  \n        return output, attention_weights\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n        })\n        return config\n","50ef574b":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  ## (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)                  ## (batch_size, seq_len, d_model)\n    ])\n\nclass EncoderBaseLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1,**kwargs,):\n        super(EncoderBaseLayer, self).__init__(**kwargs)\n        # attrs\n        self.num_heads = num_heads\n        self.d_model   = d_model\n        self.dff       = dff\n        self.rate      = rate\n        # ops\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x,mask,training=True):\n        ## op outs:(batch_size, input_seq_len, d_model)\n        attn_output, _ = self.mha(x, x, x, mask)  \n        attn_output    = self.dropout1(attn_output, training=training)\n        out1           = self.layernorm1(x + attn_output)  \n        \n        ffn_output     = self.ffn(out1)  \n        ffn_output     = self.dropout2(ffn_output, training=training)\n        out2           = self.layernorm2(out1 + ffn_output) \n        return out2\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n            'dff'      : self.dff,\n            'rate'     : self.rate\n        })\n        return config\n\n\nclass DecoderBaseLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1,**kwargs,):\n        super(DecoderBaseLayer, self).__init__(**kwargs,)\n        # attrs\n        self.num_heads = num_heads\n        self.d_model   = d_model\n        self.dff       = dff\n        self.rate      = rate\n        # ops\n        self.mha1 = MultiHeadAttention(self.d_model, self.num_heads)\n        self.mha2 = MultiHeadAttention(self.d_model, self.num_heads)\n        \n        self.ffn  = point_wise_feed_forward_network(self.d_model, self.dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(self.rate)\n        self.dropout2 = tf.keras.layers.Dropout(self.rate)\n        self.dropout3 = tf.keras.layers.Dropout(self.rate)\n\n    def call(self, x, enc,comb_mask,mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        ##op outs:(batch_size, target_seq_len, d_model)\n        attn1, attn_weights_block1 = self.mha1(x, x, x, comb_mask)  \n        attn1                      = self.dropout1(attn1)\n        out1                       = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc, enc, out1,mask)  \n        attn2                      = self.dropout2(attn2)\n        out2                       = self.layernorm2(attn2 + out1)  \n\n        ffn_output                 = self.ffn(out2)  \n        ffn_output                 = self.dropout3(ffn_output)\n        out3                       = self.layernorm3(ffn_output + out2)  \n\n        return out3, attn_weights_block1, attn_weights_block2\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'd_model'  : self.d_model,\n            'num_heads': self.num_heads,\n            'dff'      : self.dff,\n            'rate'     : self.rate\n        })\n        return config\n","b6219ff0":"def Encoder(inp,mask,cfg):\n    x   = tf.keras.layers.Embedding(cfg.inp_voclen,cfg.d_model,name=\"EncoderInputEncoding\")(inp)\n    x   = PositionalEncoding(cfg.pe_max,cfg.d_model,True,name=\"EncoderPositionalEncoding\")(x)\n    for i in range(cfg.num_layers):\n        x=EncoderBaseLayer(cfg.d_model, cfg.num_heads, cfg.dff, cfg.rate,name=f\"EncoderLayer_{i}\")(x,mask)\n    return x\n\ndef Decoder(tar,enc,mask,comb_mask,cfg):\n    x   = tf.keras.layers.Embedding(cfg.tgt_voclen,cfg.d_model,name=\"DecoderInputEncoding\")(tar)\n    x   = PositionalEncoding(cfg.pe_max,cfg.d_model,True,name=\"DecoderPositionalEncoding\")(x)\n    x   = tf.keras.layers.Dropout(cfg.rate)(x)\n    w_attn={}\n    for i in range(cfg.num_layers):\n        x,awb1,awb2=DecoderBaseLayer(cfg.d_model, cfg.num_heads, cfg.dff, cfg.rate,name=f\"DecoderLayer_{i}\")(x,enc,comb_mask,mask)\n        w_attn[f'decoder_layer{i+1}_block1'] = awb1\n        w_attn[f'decoder_layer{i+1}_block2'] = awb2\n    x=tf.keras.layers.Dense(cfg.tgt_voclen,name=\"logits\")(x)\n    return x,w_attn    \n\n\ndef net(cfg):\n    inp           = tf.keras.layers.Input(shape=cfg.inp_shape,name=\"input\")\n    tar           = tf.keras.layers.Input(shape=cfg.tar_shape,name=\"target\")\n    mask,comb_mask=Masking(pad_value=cfg.pad_value,size=cfg.d_len,name=\"masks\")(inp,tar)\n    enc= Encoder(inp,mask,cfg)\n    x,w_attn=Decoder(tar,enc,mask,comb_mask,cfg)\n    model=tf.keras.Model(inputs=[inp,tar],outputs=[x,w_attn],name=\"TransformerBaseNet\")\n    return model\n\ntransformer=net(ModelConfig)\ntransformer.summary()","755dd007":"import matplotlib.pyplot as plt\n%matplotlib inline \nd_model=ModelConfig.d_model\nwith strategy.scope():\n    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n        def __init__(self, d_model, warmup_steps=4000):\n            super(CustomSchedule, self).__init__()\n\n            self.d_model = d_model\n            self.d_model = tf.cast(self.d_model, tf.float32)\n\n            self.warmup_steps = warmup_steps\n\n        def __call__(self, step):\n            arg1 = tf.math.rsqrt(step)\n            arg2 = step * (self.warmup_steps ** -1.5)\n\n            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \n    learning_rate = CustomSchedule(d_model)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n\n    temp_learning_rate_schedule = CustomSchedule(d_model)\n\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    def loss_function(real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        loss_ = loss_object(real, pred)\n\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n\n        return tf.reduce_sum(loss_)\/tf.reduce_sum(mask)\n\n\n    def accuracy_function(real, pred):\n        accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        accuracies = tf.math.logical_and(mask, accuracies)\n\n        accuracies = tf.cast(accuracies, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracies)\/tf.reduce_sum(mask)\n","5cfcad55":"class Transformer(tf.keras.Model):\n    def __init__(self,cfg):\n        super(Transformer, self).__init__()\n        self.net    = net(cfg)\n        self.cfg    = cfg\n        \n    def compile(self,optimizer,loss_fn,acc):\n        super(Transformer, self).compile()\n        self.optimizer = optimizer\n        self.loss_fn   = loss_fn\n        self.acc       = acc\n        \n    def train_step(self, batch_data):\n        inp,tar=batch_data\n        with tf.GradientTape() as tape:\n            pred,_= self.net({\"input\":inp,\"target\":tar},training=True)\n            loss  = self.loss_fn(tar[:, 1:],pred[:,:-1,:])\n\n        gradients = tape.gradient(loss, self.net.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.net.trainable_variables))\n        return {\"loss\": loss,\n                \"acc\" : self.acc(tar[:, 1:],pred[:,:-1,:])}\n    \n    def test_step(self, batch_data):\n        inp,tar=batch_data\n        # label\n        label=tf.ones_like(tar,dtype=tf.int64)*self.cfg.start_value\n        preds=[]\n        for i in range(self.cfg.d_len):\n            pred,_ = self.net({\"input\":inp,\"target\":label},training=False)\n            pred   = pred[:,i, :]\n            preds.append(pred)\n            pred   =tf.nn.softmax(pred,axis=-1)\n            max_idx=tf.math.argmax(pred,axis=-1)\n\n            if i < self.cfg.d_len - 1:\n                label     = tf.unstack(label,axis=-1)\n                label[i+1]= tf.cast(max_idx,tf.int64)\n                label     = tf.stack(label,axis=-1)\n                label     = tf.cast(label,tf.int64)\n                \n        pred=tf.stack(preds,axis=1)\n        loss = self.loss_fn(tar[:, 1:],pred[:,:-1,:])\n\n        return {\"loss\": loss,\n                \"acc\" : self.acc(tar[:, 1:],pred[:,:-1,:])}","b09de726":"with strategy.scope():\n    model =  Transformer(ModelConfig)\n    model.compile(optimizer = optimizer,\n                  loss_fn   = loss_function,\n                  acc       = accuracy_function)","d6fe6076":"# early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=30, \n                                                  verbose=1, \n                                                  mode = 'auto') \n\nclass SaveBestModel(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.best = float('inf')\n\n    def on_epoch_end(self, epoch, logs=None):\n        metric_value = logs['val_loss']\n        if metric_value < self.best:\n            print(f\"Loss Improved epoch:{epoch} from {self.best} to {metric_value}\")\n            self.best = metric_value\n            self.model.net.save_weights(f\"model_{VOCAB}.h5\")\n            print(\"Saved Best Weights\")\n    def set_model(self, model):\n        self.model = model\n            \nmodel_save=SaveBestModel()\nmodel_save.set_model(model)\ncallbacks= [model_save,early_stopping]","0d597297":"history=model.fit(train_ds,\n                  epochs=EPOCHS,\n                  steps_per_epoch=STEPS_PER_EPOCH,\n                  verbose=1,\n                  validation_data=eval_ds,\n                  validation_steps=EVAL_STEPS,\n                  callbacks=callbacks)","c41bce12":"import pandas as pd\ncurves={}\nfor key in history.history.keys():\n    curves[key]=history.history[key]\ncurves=pd.DataFrame(curves)\ncurves.to_csv(f\"history_{VOCAB}.csv\",index=False)","938eb59e":"### Base Layers","bd922cae":"# APPROACH:2 IDEA:2 ","49fcf8f7":"### Composite Layers","ae17e2a3":"### Base Blocks","2437fd38":"# Model Params and callbacks","5106351a":"# Modeling\nBase Layers:\n* Multihead Attention\n* Positional Encoding\n\nComposite Layers\n* Encoder Base Layer\n* Decoder Base Layer\n\nBase Blocks:\n* Endoder \n* Decoder"}}