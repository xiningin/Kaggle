{"cell_type":{"390ad793":"code","f75f65bd":"code","2636735a":"code","f4812eb7":"code","5341bcd4":"code","a90f3535":"code","f11ab9aa":"code","2733bfe0":"code","c919ee29":"code","bbd4a5b1":"code","0325583e":"code","c67e8541":"code","cc3bf849":"code","907ebbf1":"code","da0b4b9f":"code","846dd37d":"code","2e14c190":"code","82d74ab4":"code","4e70f7f0":"code","b41b4e84":"code","3dc3c90a":"code","45fd6c82":"code","a87764e6":"code","7e883405":"code","0414c915":"code","c108d27d":"code","65ff2049":"code","d0741a9b":"code","e9ae8e00":"code","093a3c92":"code","2dc08a1b":"code","5e5e2225":"code","fb5bd383":"code","0ebdfa93":"code","b6e3d816":"code","6ca8da1f":"code","ec308ee6":"code","40ca243d":"code","049dbbb3":"code","423c3235":"code","54909a81":"code","b6ea1c62":"code","b9482683":"code","203d9b5e":"code","43c66ffc":"code","5da4590c":"code","424edf1c":"code","6c7ff36c":"code","bf15f3ad":"code","cb47a8db":"markdown","1c9fc378":"markdown","5748f907":"markdown","b75157fe":"markdown","672f9227":"markdown","770c4486":"markdown","ac69272b":"markdown","1e9cce9f":"markdown","c0e87d06":"markdown","cbc768e2":"markdown","67cc29d9":"markdown","e4894ecd":"markdown","b1f9d0da":"markdown","4c8db30d":"markdown","7aafa8c2":"markdown","9c955d2e":"markdown"},"source":{"390ad793":"%matplotlib inline\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.models import load_model\nfrom keras.layers import (\n    Dense,\n    Flatten,\n    Dropout,\n    Conv2D,\n    MaxPooling2D,\n    Activation,\n    BatchNormalization\n)\nfrom keras.utils import np_utils\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf","f75f65bd":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)","2636735a":"def plot_history(history):\n    history_dict = history.history\n    loss_values = history_dict['loss']\n    val_loss_values = history_dict['val_loss']\n    epochs = range(1, len(loss_values) + 1)\n\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax1.plot(epochs, loss_values, 'bo',\n             label='Training loss')\n    ax1.plot(epochs, val_loss_values, 'r',\n             label='Validation loss')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.set_xscale('log')\n\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n\n    ax2.plot(epochs, acc_values, 'bo',\n             label='Training acc')\n    ax2.plot(epochs, val_acc_values, 'r',\n             label='Validation acc')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_xscale('log')\n\n\n    plt.legend()\n    plt.show()","f4812eb7":"sample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","5341bcd4":"X_train = train.loc[:, train.columns!='label'].values.astype('uint8')\ny_train = train['label'].values\n\nX_train = X_train.reshape((X_train.shape[0],28,28))","a90f3535":"X_test = test.loc[:, test.columns!='label'].values.astype('uint8')\nX_test = X_test.reshape((X_test.shape[0],28,28))","f11ab9aa":"n = np.random.randint(X_train.shape[0])\nplt.imshow(Image.fromarray(X_train[n]))\nplt.show()\nprint(f'This is a {y_train[n]}')","2733bfe0":"X_train = X_train[:,:,:,None]\nX_test = X_test[:,:,:,None]","c919ee29":"X_train.shape","bbd4a5b1":"batch_size = 32\nnum_samples = X_train.shape[0]\nnum_classes = np.unique(y_train).shape[0]\nnum_epochs = 50\nimg_rows, img_cols = X_train[0,:,:,0].shape\nimg_channels = 1\nclasses = np.unique(y_train)","0325583e":"y_train = np_utils.to_categorical(y_train, num_classes)\n# y_test = np_utils.to_categorical(y_test, num_classes)","c67e8541":"X_train_norm = X_train.astype('float32')\nX_test_norm = X_test.astype('float32')\nX_train_norm \/= 255\nX_test_norm \/= 255","cc3bf849":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","907ebbf1":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=5,\n                                            verbose=1,\n                                            factor=0.2)\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","da0b4b9f":"history = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","846dd37d":"plot_history(history)","2e14c190":"! mkdir newer","82d74ab4":"model.save('newer\/simple.h5')","4e70f7f0":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","b41b4e84":"history1 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","3dc3c90a":"plot_history(history1)","45fd6c82":"model.save('newer\/simple_batch.h5')","a87764e6":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),\n                 activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","7e883405":"history2 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","0414c915":"plot_history(history2)","c108d27d":"model.save('newer\/32x64_64x128.h5')","65ff2049":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (5, 5), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","d0741a9b":"history3 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","e9ae8e00":"plot_history(history3)","093a3c92":"model.save('newer\/32x64x64.h5')","2dc08a1b":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","5e5e2225":"history4 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","fb5bd383":"plot_history(history4)","0ebdfa93":"model.save('newer\/64x128.h5')","b6e3d816":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(256, kernel_size=(3, 3),\n                 activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","6ca8da1f":"history5 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","ec308ee6":"plot_history(history5)","40ca243d":"model.save('newer\/64x128_256x512.h5')","049dbbb3":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(256, kernel_size=(3, 3),\n                 activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","423c3235":"history6 = model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=batch_size,\n    epochs=num_epochs,\n    validation_split=0.1,\n    shuffle=True,\n    callbacks=[learning_rate_reduction, es]\n)","54909a81":"plot_history(history6)","b6ea1c62":"model.save('newer\/64x128_256x512_diff_fcnn.h5')","b9482683":"labels = ['simple', 'simple+batch', '32+64_64+128',\n          '32+64+64', '64+128', '64+128_256+512', '64+128_256+512_diff_fcnn']\nplt.figure(figsize=(7.5,7.5))\nfor idx, h in enumerate([history, history1, history2,\n          history3, history4, history5, history6]):\n    history_dict = h.history\n    loss_values = history_dict['loss']\n    epochs = range(1, len(loss_values) + 1)\n    \n    plt.plot(epochs, loss_values, label=labels[idx],\n             linestyle='-', marker='o')\n\nplt.title('Training loss function')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(7.5,7.5))\nfor idx, h in enumerate([history, history1, history2,\n          history3, history4, history5, history6]):\n    history_dict = h.history\n    loss_values = history_dict['val_loss']\n    epochs = range(1, len(loss_values) + 1)\n    \n    plt.plot(epochs, loss_values, label=labels[idx],\n             linestyle='-', marker='o')\n\nplt.title('Validation loss function')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","203d9b5e":"labels = ['simple', 'simple+batch', '32+64_64+128',\n'32+64+64', '64+128', '64+128_256+512', '64+128_256+512_diff_fcnn']\nplt.figure(figsize=(7.5,7.5))\nfor idx, h in enumerate([history, history1, history2,\n          history3, history4, history5, history6]):\n    history_dict = h.history\n    acc_values = history_dict['accuracy']\n    epochs = range(1, len(acc_values) + 1)\n    \n    plt.plot(epochs, acc_values, label=labels[idx],\n             linestyle='-', marker='o')\n\nplt.title('Training accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(7.5,7.5))\nfor idx, h in enumerate([history, history1, history2,\n          history3, history4, history5, history6]):\n    history_dict = h.history\n    acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n    \n    plt.plot(epochs, acc_values, label=labels[idx],\n             linestyle='-', marker='o')\n\nplt.title('Validation accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","43c66ffc":"model = load_model('newer\/64x128_256x512_diff_fcnn.h5')","5da4590c":"pred = model.predict_classes(X_test_norm)","424edf1c":"sample_submission['Label'] = pred","6c7ff36c":"sample_submission.head()","bf15f3ad":"sample_submission.to_csv(\"submission.csv\", index=False)","cb47a8db":"Defining some parameters:","1c9fc378":"Reshaping the data to fit into the keras image format:\n(samples, rows, cols, channels)","5748f907":"Adding more layers doesn't seem to change anything. Adding more nodes...","b75157fe":"Standardizing the data:","672f9227":"### Adding more convolutional layers","770c4486":"### Adding batch normalization","ac69272b":"### Looks like the last variant was the best one","1e9cce9f":"### Plotting everything together","c0e87d06":"### Slightly changing the output layer configuration:","cbc768e2":"## Model","67cc29d9":"Another variant:","e4894ecd":"## Keras CNN study","b1f9d0da":"### Adding more nodes to existing layers:","4c8db30d":"## Import","7aafa8c2":"### More nodes & more layers:","9c955d2e":"### Defining some straightforward architecture"}}