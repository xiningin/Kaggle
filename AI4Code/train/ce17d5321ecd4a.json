{"cell_type":{"a38ad645":"code","7c5f23da":"code","83b928be":"code","80bfa342":"code","23561e80":"code","052eca50":"code","c02b901e":"code","4d85e3dc":"code","cee74d0e":"code","21e0f33b":"code","b158ff67":"code","e0a55308":"code","7a53d33e":"code","83f1ed12":"code","0a74d447":"code","58e95864":"markdown","1a8a648c":"markdown","9998b1ca":"markdown","9bd1eac9":"markdown","f393c0a2":"markdown","bbe08784":"markdown"},"source":{"a38ad645":"import gensim\nfrom gensim.models import Word2Vec\nimport numpy as np\nimport nltk\nimport itertools\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport scipy\nfrom scipy import spatial\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')","7c5f23da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83b928be":"# Let us consider we have 4 documents\nDoc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ]\nDoc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\nDoc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\nDoc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]","80bfa342":"query = 'cricket'","23561e80":"full_doc = Doc1+Doc2+Doc3+Doc4\nfull_doc","052eca50":"model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)","c02b901e":"def data_clean(text):\n    pattern = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(pattern,'',' '.join(text))\n    tokens = [token.strip() for token in text.split()]\n    filtered = [token for token in tokens if token.lower() not in stopword_list]\n    filtered = ' '.join(filtered)\n    return filtered","4d85e3dc":"data_clean(nltk.word_tokenize(full_doc[0]))","cee74d0e":"def embeddings(word):\n    print(word)\n    if word in model.key_to_index:\n        return model.get_vector(word)\n    else:\n        return np.zeros(300)","21e0f33b":"# Average vector for each document\nout_dict = {}\nfor sen in full_doc:\n    average_vector = (np.mean(np.array([embeddings(x) for x in data_clean(nltk.word_tokenize(sen)).split()]), axis=0))\n    d1 = {sen: (average_vector)}\n    out_dict.update(d1)","b158ff67":"out_dict","e0a55308":"def get_sim(query_embedding, average_vec):\n    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vec))]\n    return sim","7a53d33e":"# Ranking all docs based on similarity\ndef rankings(query):\n    query_words = (np.mean(np.array([embeddings(x) for x in nltk.word_tokenize(query.lower())], dtype=float), axis=0))\n    rank = []\n    for k, v in out_dict.items():\n        rank.append((k, get_sim(query_words, v)))\n    rank = sorted(rank, key=lambda t: t[1], reverse=True)\n    print(\"Ranked documents: \")\n    return rank","83f1ed12":"rankings(\"cricket\")","0a74d447":"rankings(\"driving\")","58e95864":"<h2>Creating the information retrieval model<\/h2>","1a8a648c":"Let us say we extracted all this textual information from 4 different documents. Now we want to retrieve the information that is most relevant to `cricket`. Let us see how to build such a model:","9998b1ca":"It\u2019s been proven that results will be good when queries are longer and\nthe result length is shorter. That\u2019s the reason we don\u2019t get great results in\nsearch engines when the search query has lesser number of words.","9bd1eac9":"<h1>Introduction<\/h1>\n\nThe task of information retrieval is of high importance in the world of NLP. The meaning of words or sentences depends on the exact words used as well as the meaning\/context. \n\nAn information retrieval system can allow us to search through large corpus of text and extract meaningful information based on the search text\/query.\n\nWe will be working with the `Word2Vec 300` embeddings provided by Google for this task.","f393c0a2":"<h2>Calculating similarity between query vector and document vector<\/h2>","bbe08784":"Therefore, through the given code above, we were able to create an information retrieval system that uses word embeddings to calculate the most similar document. This is the fundamental approach that can be used for many\napplications like the following:\n* Search engines\n* Document retrieval\n* Passage retrieval\n* Question and answer"}}