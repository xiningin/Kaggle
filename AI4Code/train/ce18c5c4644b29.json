{"cell_type":{"983f805c":"code","9b6ae2a8":"code","89b84e34":"code","cbe6857f":"code","469e0720":"code","8baa5b96":"code","ba4eb781":"code","e29b1e12":"code","a2a2bc32":"code","32cf763f":"code","3d58ca34":"code","674c755b":"code","ce91e64e":"code","f036f716":"code","3023b689":"code","62554407":"code","07094273":"code","95802eac":"code","8a10a3df":"code","a1212cd6":"code","06289b0c":"code","8cc02d8b":"code","f203b89d":"code","0d147cb9":"code","a03be448":"code","e240facd":"code","49bc1d64":"code","e263c3d0":"code","fd4d702e":"code","07a2b69f":"code","a15ddd16":"code","47e20242":"code","3d1854b7":"code","d6978d81":"code","db0e856d":"code","165db231":"markdown","52c3076f":"markdown","aaebf238":"markdown","0761c369":"markdown","c66756f0":"markdown","a280bfec":"markdown","8f89a693":"markdown","2a75f91a":"markdown","1649fc4f":"markdown","ebc1db04":"markdown","8d2ef375":"markdown","0398d67e":"markdown","bf2d81c7":"markdown","6f75c0a6":"markdown","b972aaf5":"markdown","ca501e31":"markdown","c9fd24ad":"markdown","230aeff6":"markdown","c2b36c7d":"markdown","9994e7e0":"markdown","feccacd8":"markdown","8470379a":"markdown","d8df5043":"markdown","5e78c0d6":"markdown","75a8df25":"markdown","c49dc287":"markdown","046e19e3":"markdown","89b026ac":"markdown"},"source":{"983f805c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b6ae2a8":"# Import helpful libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\n","89b84e34":"# Load the data, and separate the target\ntrain_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntrain_data = pd.read_csv(train_path)\ntest_data  = pd.read_csv(test_path)\ny = train_data.SalePrice\n","cbe6857f":"len(train_data.columns) == len(test_data.columns)","469e0720":"difference = [x for x in train_data.columns if x not in set(test_data.columns)]\ndifference","8baa5b96":"train_data.info()","ba4eb781":"Na_data_train = train_data.isnull().sum()\nNa_data_train = Na_data_train[Na_data_train>0]\nNa_data_train.sort_values(inplace=True)\nNa_data_train.plot.bar()\nNa_data_train\n","e29b1e12":"train_data = train_data.drop(columns=['PoolQC','MiscFeature', 'FireplaceQu','GarageYrBlt','Electrical','MasVnrType', 'MasVnrArea','LotFrontage'])\ntest_data = test_data.drop(columns=['PoolQC','MiscFeature', 'FireplaceQu','GarageYrBlt','Electrical','MasVnrType', 'MasVnrArea','LotFrontage'])\n\n#I'm dropping from both to keep it even. Otherwise I'll forget and have to search for way too long.","a2a2bc32":"#Training\ntrain_data['Alley'] = train_data['Alley'].fillna('None')\ntrain_data['Fence']=train_data['Fence'].fillna('None')\ntrain_data['GarageCond']=train_data['GarageCond'].fillna('No Garage')\ntrain_data['GarageType']=train_data['GarageType'].fillna('No Garage')\ntrain_data['GarageQual']=train_data['GarageQual'].fillna('No Garage')\ntrain_data['GarageFinish']=train_data['GarageFinish'].fillna('No Garage')\ntrain_data['BsmtQual']=train_data['BsmtQual'].fillna('No Bsmt')\ntrain_data['BsmtCond']=train_data['BsmtCond'].fillna('No Bsmt')\ntrain_data['BsmtFinType1']=train_data['BsmtFinType1'].fillna('No Bsmt')\ntrain_data['BsmtExposure']=train_data['BsmtExposure'].fillna('No Bsmt')\ntrain_data['BsmtFinType2']=train_data['BsmtFinType2'].fillna('No Bsmt')\n\n\n#Testing\ntest_data['Alley'] = test_data['Alley'].fillna('None')\ntest_data['Fence']=test_data['Fence'].fillna('None')\ntest_data['GarageCond']=test_data['GarageCond'].fillna('No Garage')\ntest_data['GarageType']=test_data['GarageType'].fillna('No Garage')\ntest_data['GarageQual']=test_data['GarageQual'].fillna('No Garage')\ntest_data['GarageFinish']=test_data['GarageFinish'].fillna('No Garage')\ntest_data['BsmtQual']=test_data['BsmtQual'].fillna('No Bsmt')\ntest_data['BsmtCond']=test_data['BsmtCond'].fillna('No Bsmt')\ntest_data['BsmtFinType1']=test_data['BsmtFinType1'].fillna('No Bsmt')\ntest_data['BsmtExposure']=test_data['BsmtExposure'].fillna('No Bsmt')\ntest_data['BsmtFinType2']=test_data['BsmtFinType2'].fillna('No Bsmt')","32cf763f":"#Testing to see if my datasets are free from Nas. Also re-running these to account for the changes.\nNa_data_train = train_data.isnull().sum()\nNa_data_train = Na_data_train[Na_data_train>0]\nNa_data_train.sort_values(inplace=True)\nprint('Data Training: \\n\\n',Na_data_train)\n\n\nNa_data_test = test_data.isnull().sum()\nNa_data_test = Na_data_test[Na_data_test>0]\nNa_data_test.sort_values(inplace=True)\n\nprint('Data Test: \\n\\n', Na_data_test)","3d58ca34":"\"\"\" Exterior1st     1\nExterior2nd     1\nBsmtFinSF1      1#\nBsmtFinSF2      1\nBsmtUnfSF       1\nTotalBsmtSF     1\nKitchenQual     1\nGarageCars      1\nGarageArea      1\nSaleType        1\nUtilities       2\nBsmtFullBath    2\nBsmtHalfBath    2\nFunctional      2\nMSZoning        4\n\n\nOnly a few of these are in my test\n\"\"\" \ntest_data['GarageArea']=test_data['GarageArea'].fillna(0)\ntest_data['GarageCars']=test_data['GarageCars'].fillna(0)\ntest_data['TotalBsmtSF']=test_data['TotalBsmtSF'].fillna(0)\ntest_data.info()","674c755b":"cat_train=train_data.select_dtypes('object'); cat_train.info()\ncat_test= test_data.select_dtypes('object');cat_test.info()\n\nquant_train = train_data.select_dtypes(exclude='object');quant_train.info()\nquant_test = test_data.select_dtypes(exclude='object');quant_test.info()","ce91e64e":"#MSSubClass, OverallQual,OverallCond, YearBuilt, YearRemodAdd, GarageYrBlt,MoSold, YrSold\ncat_train['MSSubClass']=quant_train.MSSubClass\ncat_train['OverallQual']=quant_train.OverallQual\ncat_train['OverallCond']=quant_train.OverallCond\ncat_train['YearBuilt']=quant_train.YearBuilt\ncat_train['YearRemodAdd']=quant_train.YearRemodAdd\ncat_train['MoSold']=quant_train.MoSold\ncat_train['YrSold']=quant_train.YrSold\n\nquant_train=quant_train.drop(columns=['MSSubClass', \n                                      'OverallQual',\n                                      'OverallCond', \n                                      'YearBuilt', \n                                      'YearRemodAdd',\n                                      'MoSold', \n                                      'YrSold',\n                                      'SalePrice'])\n\ncat_test['MSSubClass']=quant_test.MSSubClass\ncat_test['OverallQual']=quant_test.OverallQual\ncat_test['OverallCond']=quant_test.OverallCond\ncat_test['YearBuilt']=quant_test.YearBuilt\ncat_test['YearRemodAdd']=quant_test.YearRemodAdd\ncat_test['MoSold']=quant_test.MoSold\ncat_test['YrSold']=quant_test.YrSold\n\nquant_test=quant_test.drop(columns=['MSSubClass', \n                                      'OverallQual',\n                                      'OverallCond', \n                                      'YearBuilt', \n                                      'YearRemodAdd',\n                                      'MoSold', \n                                      'YrSold'])\n#Get rid of warning","f036f716":"quant_train.info()\ncat_train.info()\nquant_test.info()\ncat_test.info()","3023b689":"import seaborn as sns\nsns.histplot(train_data['SalePrice'])\nprint('Minimum Price: $', min(train_data['SalePrice']))","62554407":"import matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(train_data.corr())\ncorr= train_data.corr();corr","07094273":"#Pretty self explanatory, just getting an idea. Remember that two different forms of visualization are better than one.\n\n\ncorr.SalePrice.sort_values()","95802eac":"len(train_data.columns)","8a10a3df":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif_data = pd.DataFrame()\nvif_features = ['OverallQual','GrLivArea','GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\nvif_corr_dat = train_data[vif_features]\nvif_data[\"feature\"] = vif_corr_dat.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif_corr_dat.values, i)\n                          for i in range(len(vif_corr_dat.columns))]\nvif_data","a1212cd6":"cat_train.info()","06289b0c":"train_data.astype({'MoSold': 'object','YrSold':'object','YearRemodAdd':'object','YearBuilt':'object','MSSubClass':'object'}).dtypes\ntest_data.astype({'MoSold': 'object','YrSold':'object','YearRemodAdd':'object','YearBuilt':'object','MSSubClass':'object'}).dtypes\n","8cc02d8b":"train_data = pd.get_dummies(train_data)\ntest_data=pd.get_dummies(test_data)","f203b89d":"train_data['SalePrice']=np.log(train_data['SalePrice'])\nsns.histplot(train_data['SalePrice'])\n\n#Logged into normality\n","0d147cb9":"from scipy import stats\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)","a03be448":"X = train_data[vif_features]\ny = train_data.SalePrice\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\nlin_model = LinearRegression()\nlin_model.fit(train_X, train_y)\nlin_val_predictions = lin_model.predict(val_X)\nlin_val_mae = mean_absolute_error(np.exp(lin_val_predictions), np.exp(val_y))\nlin_val_mse= mean_squared_error(np.exp(lin_val_predictions), np.exp(val_y))\n\nprint('Validation MAE for Linear Regression without log norm: 23,332')\nprint(\"Validation MAE for Linear Regression Model: {:,.0f}\".format(lin_val_mae))\nprint('Validation RMSE for Linear Regression Model without log norm: 35,118')\nprint(\"Validation RMSE for Linear Regression Model: {:,.0f}\".format(np.sqrt(lin_val_mse)))","e240facd":"y_train_pred = lin_model.predict(train_X)\ny_train_pred","49bc1d64":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nlin_model_on_full_data = LinearRegression()\n\n# fit rf_model_on_full_data on all data from the training data\nlin_model_on_full_data.fit(X, y)\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\n\ntest_X = test_data[vif_features]\n\n# make predictions which we will submit. \ntest_preds_lin = lin_model_on_full_data.predict(test_X)","e263c3d0":"test_preds_lin = np.exp(test_preds_lin)\ntest_preds_lin","fd4d702e":"residual_train = (val_y-lin_val_predictions)\nfrom yellowbrick.regressor import ResidualsPlot\n\nvisualizer=ResidualsPlot(lin_model)\nvisualizer.score(val_X, val_y)\nvisualizer.show()    ","07a2b69f":"sns.scatterplot(x=y_train_pred,y=train_y)\nsns.scatterplot(x=lin_val_predictions, y=val_y)","a15ddd16":"from sklearn.linear_model import Ridge \nclf = Ridge(alpha=1.0)\nclf.fit(train_X, train_y)\nridge_val_predictions = clf.predict(val_X)\nridge_val_mae = mean_absolute_error(np.exp(ridge_val_predictions), np.exp(val_y))\nridge_val_mse= mean_squared_error(np.exp(ridge_val_predictions), np.exp(val_y))\nprint(\"Validation MAE for Ridge Regression Model: {:,.0f}\".format(ridge_val_mae))\nprint(\"Validation RMSE for Ridge Regression Model: {:,.0f}\".format(np.sqrt(ridge_val_mse)))\nridge_val_predictions","47e20242":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nridge_model_on_full_data = Ridge()\n\n# fit rf_model_on_full_data on all data from the training data\nridge_model_on_full_data.fit(X, y)\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\n\ntest_X = test_data[vif_features]\n\n# make predictions which we will submit. \ntest_preds_ridge = ridge_model_on_full_data.predict(test_X)\ntest_preds_ridge = np.exp(test_preds_ridge)\ntest_preds_ridge","3d1854b7":"# Removing log norm\ntrain_data['SalePrice']=np.exp(train_data['SalePrice'])\n\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr',\n            'TotRmsAbvGrd', 'OverallQual','GrLivArea','GarageCars', 'GarageArea','TotalBsmtSF']\n# Select columns corresponding to features, and preview the data\nX = train_data[features]\nX.head()\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Define a random forest model\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\nrf_val_mse= mean_squared_error(rf_val_predictions, val_y)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\nprint(\"Validation RMSE for Random Forest Model: {:,.0f}\".format(np.sqrt(rf_val_mse)))","d6978d81":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor(random_state=1)\n\n# fit rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(X, y)\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\ntest_X = test_data[features]\n\n# make predictions which we will submit. \ntest_preds_rf = rf_model_on_full_data.predict(test_X)","db0e856d":"output = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds_ridge})\noutput.to_csv('submission.csv', index=False)\n\n\n#Linear Regression Model w\/o log normalization = .67251\n#Linear Regression Model w\/ log normalization = .16785\n#Random Forest model = .16093","165db231":"Arguably, this is unnecessary. After all, I can create a specific cut-out of features (and I will) in order to strip from the dataframe. But I like to keep everything clean and even as I do my work. On the off chance that I get run over by some park'n'ride bus, then whoever comes after me can understand EXACTLY what I was doing. This is important and incredibly underrated. Especially in data science notebooks (which are often chaotic). It is important to recognize that not all data is necessarily important...especially in something like this where we have a lot of pretty high quality and multicollinear data.\n\nSpeaking of chaos...Go support the Balls of Chaos at https:\/\/www.youtube.com\/c\/JellesMarbleRuns","52c3076f":"### Lasso ","aaebf238":"For whatever reasons of mystical nonsense, I have some float64s sitting around and some ints in the other spot. I'm going to convert them to ints just to make it not a pain","0761c369":"Through here we can see that we have 3 float64, 35 int64, and 43 objects. My initial assumption will be that if something is either a float64 or an int64 that it is a quantitative variable, otherwise categorical.\n\nNote: As a reminder, 'ints' are whole numbers, 'floats' are decimals, and objects are a broad category that don't fit. They need to be double checked.","c66756f0":"### Linear Regression\n\nI'm only showing the log normalized version of this. Essentially, if you don't log normalize, you just remove all of the np.logs and np.exps around here.","a280bfec":"That is so incredibly painful to look at. VIF levels are really ridiculously high. Important to know. I'm going to leave it alone right now.\n\n\nAt this point, I'm going to get some dummy variables. Before that happens, I want to make sure that all my variables that need to be dummies...are dummifiable.","8f89a693":"Since I didn't set up the files, it is always good to do some checking to make sure the columns are accurate. In this and in most circumstances, you won't go by rows as those will be individual observations.","2a75f91a":"## Preprocessing\n\nFirst thing to note is that we are lucky enough to have a separation of training and test spreadsheets, we won't have to worry about splitting them (we'll worry about validation later).","1649fc4f":"# Modeling\n\n\nThe models I'll be trying\n\n\n1) Linear Regression\n\n2) Ridge Regression\n\n3) Lasso\n\n4) Random Forest\n\n5) GAMs\n\n6) XGBoost","ebc1db04":"Alright, this is one of those times where it is important to note two things:\n1) Note the number of observations you have in training (1460)\n\n2) Remember that you have a data dictionary and don't be afraid to use it.\n\nIn this circumstance, I'm effectively selecting between either \na) getting rid of certain categories or \nb) imputting them with other data. \n\nThis can be a bit of an arbitrary decision, so most of the time you have to make a judgment call.\n\nLet's go through the list.\n\n1) PoolQC is all about the condition of the pool. NAs technically mean that there is no pool, but I'm having a hard time believing that only 7 homes have pools. Remove it as the collinearity between this and the other pool category just isn't worth it.\n\n2) MiscFeature is a weird grab bag of nonsense. Even if it had more data I'd likely either delete it or separate it into different categories. Ignore it.\n\n3) Alley. NA describes no access. Change to 'None'\n\n4) Fence. NAs here describe no fence. Change to 'None'\n\n5) FireplaceQu. NA here describe no fireplace. Collinear with fireplace. Remove it.\n\n6) LotFrontage. Impute mean() from neighborhood.\n\n7) Garages. These all indicate 'No garage'. Indicate.\n\n8) Bsmt. Same as above.\n\n9) Masonry veneer...I mean, it won't be in our final model anyway. So it doesn't matter.\n\n10) Electrical. I'm just going to drop for the sake of it (I could remove this data point...but why even keep this? We have too much data","8d2ef375":"So, whatever I do with these NAs will likely be controversial (lots of people have a lot of opinions). Since we have so much data and because it isn't that big of deal, I'll just delete the observations. Note that I'm not deleting the columns. That is a very different thing and ruins a whole bunch of stuff you might want to do down the line. As per the mathematics, typically you can delete the observations. The columns need to stay consistent. I don't know if this will jive with the competition style, so I may have an issue there.","0398d67e":"The following two plots allow us to see how well things are matching. For a straight line linear regression, it doesn't make sense to try to get any better than this. Too much work for not enough payoff.","bf2d81c7":"## Exploration of Data Dictionary\n\nIn this circumstance, it is pretty obvious that we are going to be looking at housing prices and the typical factors that come into play when dealing with housing prices. Yet, it is important to get in the habit of at least glancing at your data dictionary before getting started. There are typically some unintuitively named categories of data which, if understood improperly, can really hurt the final outcome. I'll put a couple of the general bits of unintuitive or just poorly named categories of data here.\n\n* MSSubClass\n    * This little gem tells us both the style of the house AND the year it was built. I can already tell you that this\n    will be a complete mess in the graphs. Unfortunately, this not only mixes styles and years but also has categories\n    which combine ALL AGES. This creates a pretty big mess. In addition, this seems like unnecessary information as we     have HouseStyle & Bldg later on. All this gives us is a mess that combines age into relatively normal categories to\n    give us arbitrary 'classes.'\n* LotShape\n    * This deals with the shape of the lot from a bird's eye view. It goes from Regular ----> IR3 in descending order\n    of...strangeness. I would have to look at the lots to know what this means. Not entirely clear what separates IR2       from IR3 other than judgment. \n* LandSlope vs. LandContour\n    * These are awfully similar and not completely clarified in the data dictionary. Best thing to do here is likely to\n      remove one just to get rid of the collinearity. More data is not always better. I won't do that until I double-\n      check that there is, in fact, collinearity here.\n* Condition1 & Condition 2\n    * This is a weird one. Essentially, this is supposed to tell us whether or not the house is close to things like\n    main roads and railroads. They both discuss the same things and, if none of the things are present, I assume they\n    input 'normal'. Unfortunately, having this separated in two conditions makes it a pain to model. Not designed well.\n    If I need to use these conditions, I'll have to update the way they are organized at the moment.\n\n\nThose are the big ones. Thankfully, our problem here is too much information rather than too little. Always a better situation once you are able to get a handle on how the data is set up.","6f75c0a6":"### Random Forest","b972aaf5":"And of course, this makes sense. Now, this is a circumstance where you don't have the data because it is a competition. If people had the answers beforehand, they could overfit to the competition results. In a real world situation, you would have the data for this (unless your boss wanted to play a joke on you). Of course, in a real world situation, the whole setup would be different, so I'm not sure how relevant that comment is.","ca501e31":"It says false, now why would that be?","c9fd24ad":"### XGBoost","230aeff6":"After that, I can see what a random forest model would look like if I did it on the full model and fit","c2b36c7d":"Checking the minimum price allows me to know that I am going to be eligible for log transforms if I want to normalize the data.","9994e7e0":"Take a quick glance to make sure that the columns line up with what you expected them to be earlier and you are good to go. Note that these are not deep copies, so I can still do all of my data modulation from the main df_train dataframe. Any changes I make to one I want to make to the other. At this point I'll do one more little thing to the data and that'll be getting rid of any NAs in the data.\n\nAs I look through these copies, I notice that a couple of these aren't quite quantitative, I'll change those over.","feccacd8":"Because I know what my data looks like, I know that I do not want MoSold, YrSold, YearRemodAdd, YearBuilt, or MSSubClass to be sitting in any sort of ordered method.","8470379a":"Today I'm going to focus on leaning in pretty hard on the feature engineering and explanability side of the model. For data such as this, it is important to keep in mind that people want to know why a house is more expensive than another, they don't just want to know that it is. As for what style of model I will use, I can only say that it won't be a classification model. That isn't to say you couldn't do a classification model, if you were so inclined, you could attempt a model in which you group the homes into 'very low', 'low','medium', 'high', and 'extra high' on prices, but I won't be doing that today\n\n# Layout\n\nI'll walk through every single step of my thought process here. I'll try to keep it as clear as possible so that someone with a basic no code background could follow without much work. I will not load in all my libraries at the top as is typical, instead, I'll load them just before I use them the first time (not including the default libraries that kaggle inputs). That way you know exactly what libraries I'm using. If possible, I'll try to show different libraries and explain the differences and benefits between each one.\n\n* Exploration of Data Dictionary & highlighting my main goal with the dataset.\n* Preprocessing\n     * Checking to see if categorical & quantitative variables are correctly designated, if NAs exist and whether or\n       not to impute new data, delete them, or do something else if they do, checking for shape of data and doing \n       transforms if necessary, creating visualizations to confirm or check assumptions.\n* Model creation\n    * Creating some models that fit with the data I am working with.\n* Explanation\n    * Explaining why this data takes the form that it does. Why the significant variables seem to be significant, etc.","d8df5043":"The naive suggestion here is that, from looking at this list, the features of:\n\nOverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd are going to be the features with the highest relevance. I cut off at 50% for relevance, but that was arbitrary. I could lower it in the future.\n\nDigging a little bit deeper, it makes a certain amount of common sense that some of these (like the two Garage areas and the SF+GrLivArea numbers) will have some multicollinearity","5e78c0d6":"The easiest thing to do next is to create some categories for my data, specifically separating between categorical and quantitative, but how do I figure that out? I can painstakingly create an excel sheet and put everything in different categories,I can check for letters in the observations and move on from there, or I can check to see the type of object stored (this last one can be deceiving if your dataset isn't constructed well from the outset).","75a8df25":"# Plotting & Transformations of Exploratory Data\n\n\nIt is always useful to visualize this data as well. This will allow us a few insights (like normality and general correlation) that are easier to explain than the pure numbers (which we will also do, but now we'll have the visualizations to fall back on).\n\n\n* Corrplot\n* Shape of data","c49dc287":"### Ridge Regression","046e19e3":"This is...a lot. Luckily, SalePrice is on the edges (which makes this much easier to read). At a glance, this gives us the general impression that there are more significant features than insignificant ones. A decent problem to have.","89b026ac":"I can see that I have a lot of null values, so I'll check those in a more systematic way.\n\nNote: Always good to get two different representations of your data. You often want something that is visual and something that is numeric. You can mix them SOMETIMES, but usually it makes it too messy. Mostly, the quick visualization is useful for intuition, not rigor. Make it pretty and fast."}}