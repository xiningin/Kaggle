{"cell_type":{"c2ef8155":"code","0bd4de15":"code","0a432d3a":"code","b4c5ab58":"code","06c36a48":"code","8f807232":"code","83f7f26c":"code","6a8de217":"code","034f4c68":"code","e7160b7c":"code","2d793316":"code","61c8871c":"code","bb15ef97":"code","4292c180":"code","763bbeb0":"code","8ea31ac5":"markdown","857da784":"markdown","a4bb7947":"markdown","171b968f":"markdown","b9341a8c":"markdown"},"source":{"c2ef8155":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0bd4de15":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom pandas import DataFrame \nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","0a432d3a":"raw_df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\nraw_df = raw_df.drop('CUST_ID', axis = 1) \nraw_df.fillna(method ='ffill', inplace = True) \nraw_df.head(2)","b4c5ab58":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(raw_df) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","06c36a48":"gmm = GaussianMixture(n_components = 3) \ngmm.fit(X_principal)","8f807232":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = GaussianMixture(n_components = 3).fit_predict(X_principal), cmap =plt.cm.winter, alpha = 0.6) \nplt.show() ","83f7f26c":"def SelBest(arr:list, X:int)->list:\n    '''\n    returns the set of X configurations with shorter distance\n    '''\n    dx=np.argsort(arr)[:X]\n    return arr[dx]","6a8de217":"n_clusters=np.arange(2, 8)\nsils=[]\nsils_err=[]\niterations=20\nfor n in n_clusters:\n    tmp_sil=[]\n    for _ in range(iterations):\n        gmm=GaussianMixture(n, n_init=2).fit(X_principal) \n        labels=gmm.predict(X_principal)\n        sil=metrics.silhouette_score(X_principal, labels, metric='euclidean')\n        tmp_sil.append(sil)\n    val=np.mean(SelBest(np.array(tmp_sil), int(iterations\/5)))\n    err=np.std(tmp_sil)\n    sils.append(val)\n    sils_err.append(err)","034f4c68":"plt.errorbar(n_clusters, sils, yerr=sils_err)\nplt.title(\"Silhouette Scores\", fontsize=20)\nplt.xticks(n_clusters)\nplt.xlabel(\"N. of clusters\")\nplt.ylabel(\"Score\")\n","e7160b7c":"#Courtesy of https:\/\/stackoverflow.com\/questions\/26079881\/kl-divergence-of-two-gmms. Here the difference is that we take the squared root, so it's a proper metric\n\ndef gmm_js(gmm_p, gmm_q, n_samples=10**5):\n    X = gmm_p.sample(n_samples)[0]\n    log_p_X = gmm_p.score_samples(X)\n    log_q_X = gmm_q.score_samples(X)\n    log_mix_X = np.logaddexp(log_p_X, log_q_X)\n\n    Y = gmm_q.sample(n_samples)[0]\n    log_p_Y = gmm_p.score_samples(Y)\n    log_q_Y = gmm_q.score_samples(Y)\n    log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)\n\n    return np.sqrt((log_p_X.mean() - (log_mix_X.mean() - np.log(2))\n            + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) \/ 2)","2d793316":"n_clusters=np.arange(2, 8)\niterations=20\nresults=[]\nres_sigs=[]\nfor n in n_clusters:\n    dist=[]\n    \n    for iteration in range(iterations):\n        train, test=train_test_split(X_principal, test_size=0.5)\n        \n        gmm_train=GaussianMixture(n, n_init=2).fit(train) \n        gmm_test=GaussianMixture(n, n_init=2).fit(test) \n        dist.append(gmm_js(gmm_train, gmm_test))\n    selec=SelBest(np.array(dist), int(iterations\/5))\n    result=np.mean(selec)\n    res_sig=np.std(selec)\n    results.append(result)\n    res_sigs.append(res_sig)","61c8871c":"plt.errorbar(n_clusters, results, yerr=res_sigs)\nplt.title(\"Distance between Train and Test GMMs\", fontsize=20)\nplt.xticks(n_clusters)\nplt.xlabel(\"N. of clusters\")\nplt.ylabel(\"Distance\")\nplt.show()","bb15ef97":"n_clusters=np.arange(2, 8)\nbics=[]\nbics_err=[]\niterations=20\nfor n in n_clusters:\n    tmp_bic=[]\n    for _ in range(iterations):\n        gmm=GaussianMixture(n, n_init=2).fit(X_principal) \n        \n        tmp_bic.append(gmm.bic(X_principal))\n    val=np.mean(SelBest(np.array(tmp_bic), int(iterations\/5)))\n    err=np.std(tmp_bic)\n    bics.append(val)\n    bics_err.append(err)","4292c180":"plt.errorbar(n_clusters,bics, yerr=bics_err, label='BIC')\nplt.title(\"BIC Scores\", fontsize=20)\nplt.xticks(n_clusters)\nplt.xlabel(\"N. of clusters\")\nplt.ylabel(\"Score\")\nplt.legend()","763bbeb0":"plt.errorbar(n_clusters, np.gradient(bics), yerr=bics_err, label='BIC')\nplt.title(\"Gradient of BIC Scores\", fontsize=20)\nplt.xticks(n_clusters)\nplt.xlabel(\"N. of clusters\")\nplt.ylabel(\"grad(BIC)\")\nplt.legend()","8ea31ac5":"### <u>Number of components(k) selection<\/u>:\n\n#### <u>Silhouette score<\/u>\n\nSilhouette score checks how much the clusters are compact and well separated. The more the score is near to one, the better the clustering is. Read more [here](https:\/\/www.kaggle.com\/vipulgandhi\/kmeans-detailed-explanation)\n\nSince we already know that the fitting procedure is not deterministic, we run twenty fits for each number of clusters, then we consider the mean value and the standard deviation of the best five runs.","857da784":"#### <u>Distance between GMMs<\/u>\n\nHere we form two datasets, each with an half randomly choose amount of data. We will then check how much the GMMs trained on the two sets are similar, for each configuration.\n\nSince we are talking about distributions, the concept of similarity is embedded in the Jensen-Shannon (JS) metric. The lesser is the JS-distance between the two GMMs, the more the GMMs agree on how to fit the data.\n\nThe lower the distance, the better the cluster.","a4bb7947":"Following this criterion, the bigger the number of clusters, the better should be the model. Which means that the penalty BIC criteria gives to complex models do not save us from overfit.\n\nBut before screaming and trashing out this technique, we can notice two things. The first is that the curve is fairly smooth and monotone. The second is that the curve follows different slopes in different part of it. Starting from these two observations, the temptation to check where the BIC curve change slope is big. So let\u2019s check it!\n\nTechnically, we have to calculate the gradient of the BIC scores curve. Intuitively, the concept of gradient is simple: if two consecutive points have the same value, their gradient is zero. If they have different values, their gradient can be eighter negative, if the second point has a lower value, or positive otherwise. The magnitude of the gradient tells us how much the two values are different.\n","171b968f":"<b><u>Bayesian information criterion (BIC)<\/u><\/b>\n\nThis criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have. In order to avoid overfitting, this technique penalizes models with big number of clusters.","b9341a8c":"## <u> Gaussian Mixture Models Clustering Algorithm Explained <\/u>\n\nGaussian mixture models can be used to cluster unlabeled data in much the same way as k-means. There are, however, a couple of advantages to using Gaussian mixture models over k-means.\n\n- k-means does not account for variance(width of the bell shape curve). In two dimensions, variance\/ covariance determines the shape of the distribution.\n\n![0_tBFK650dBxOxqn2H.png](attachment:0_tBFK650dBxOxqn2H.png)\n\nk-means model places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster.\n\n![Screen%20Shot%202019-09-06%20at%2021.18.04.png](attachment:Screen%20Shot%202019-09-06%20at%2021.18.04.png)\n\nThis works fine for when data is circular. However, when data takes on different shape, we end up with something like this.\n\n![Screen%20Shot%202019-09-06%20at%2021.18.48.png](attachment:Screen%20Shot%202019-09-06%20at%2021.18.48.png)\n\nIn contrast, Gaussian mixture models can handle even very oblong clusters.\n\n![Screen%20Shot%202019-09-06%20at%2021.19.25.png](attachment:Screen%20Shot%202019-09-06%20at%2021.19.25.png)\n\n-  K-means performs hard classification whereas GMM performs soft classification, i.e. in k-means, data point is deterministically assigned to one and only one cluster, but in reality there may be overlapping between the cluster  GMM provide us the probabilities of the data point belonging to each of the possible clusters.\n\nIn Sklearn, $gmm.predict(X)$ the model assigns every data point to one of the clusters and $gmm.predict\\_proba(X)$ function return the probabilities that a data point belongs to each of the K clusters.\n\nSklearn's GaussianMixture also comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.\n\n### <u>Gaussian Mixture Models At A Glance<\/u>\n\nAs the name implies, a Gaussian mixture model involves the mixture (i.e. superposition) of multiple Gaussian distributions. Here rather than identifying clusters by \u201cnearest\u201d centroids, we fit a set of k gaussians to the data. And we estimate gaussian distribution parameters such as mean and Variance for each cluster and weight of a cluster. After learning the parameters for each data point we can calculate the probabilities of it belonging to each of the clusters.\n\n![1_lTv7e4Cdlp738X_WFZyZHA.png](attachment:1_lTv7e4Cdlp738X_WFZyZHA.png)\n\nEvery distribution is multiplied by a weight $\\pi$($\\pi_1 + \\pi_2 + \\pi_3 = 1$) to account for the fact that we do not have an equal number of samples from each category. In other words, we might only have included 1000 people from the red cluster class and 100,000 people from the green cluster class.\n\n### <u>Expectation Maximization<\/u>\n\n<b><u>Expectation<\/u><\/b>\n\nThe first step, known as the expectation step or $E$ step, consists of calculating the expectation of the component assignments $C_k$ for each data point $x_i \\in X$ given the model parameters $\\pi_k$ $\\mu_k$ and $\\sigma_k$.\n\n<b><u>Maximization<\/u><\/b>\n\nThe second step is known as the maximization step or $M$ step, which consists of maximizing the expectations calculated in the E step with respect to the model parameters. This step consists of updating the values $\\pi_k$, $\\mu_k$ and $\\sigma_k$.\n\nThe entire iterative process repeats until the algorithm converges, giving a maximum likelihood estimate. Intuitively, the algorithm works because knowing the component assignment $C_k$ for each $x_i$ makes solving for $\\pi_k$ $\\mu_k$ and $\\sigma_k$ easy, while knowing $\\pi_k$ $\\mu_k$  $\\sigma_k$ makes inferring $p(C_k|x_i)$ easy. The expectation step corresponds to the latter case while the maximization step corresponds to the former. Thus, by alternating between which values are assumed fixed, or known, maximum likelihood estimates of the non-fixed values can be calculated in an efficient manner.\n\n### <u>Algorithm<\/u>\n- Initialize the mean $\\mu_k$, the covariance matrix $\\Sigma_k$ and the mixing coefficients $\\pi_k$ by some random values(or other values).\n- Compute the $C_k$ values for all k.\n- Again Estimate all the parameters using the current \\C_k values.\n- Compute log-likelihood function.\n- Put some convergence criterion\n- If the log-likelihood value converges to some value (or if all the parameters converge to some values) then stop, else return to Step 2.\n\nThis algorithm only guarantee that we land to a local optimal point, but it do not guarantee that this local optima is also the global one. And so, if the algorithm starts from different initialization points, in general it lands into different configurations.\n\n\n### <u>Example<\/u>"}}