{"cell_type":{"c5ba8f18":"code","41a97af5":"code","a8fed71e":"code","0fa4b5d8":"code","db11fe4e":"code","bd1df27c":"code","d0b7215d":"code","23c90615":"code","d517cb6f":"code","4d592d4b":"code","68d4eaa7":"code","b7336f64":"code","1de5591f":"code","53c2eae1":"code","41dbbf9d":"code","841e7c59":"code","ff68da1f":"code","69a7b1d4":"code","0d884a2f":"code","77c803af":"code","62a70a66":"code","20f3f5c7":"code","f05edc05":"code","8a1baf9c":"code","e4b7b218":"code","57547344":"code","cd0c25f9":"code","28ab5105":"code","18424224":"markdown","ea5652e7":"markdown","75c1a196":"markdown","397f2c43":"markdown","f819a847":"markdown","12c16fcd":"markdown","7eb55aa4":"markdown","78bb6e18":"markdown","21847530":"markdown","fa4cedf0":"markdown","559d53a9":"markdown","73e3499c":"markdown","b0f5a6c0":"markdown","fdd2315c":"markdown","c46314ea":"markdown","c2e44553":"markdown","8e7e84d9":"markdown","5041fef1":"markdown","c6ddd69b":"markdown","9982b379":"markdown","fd8451f8":"markdown","a7c7b37b":"markdown","7beacc6f":"markdown","59099ba7":"markdown","b483941c":"markdown","e99fec29":"markdown","64568091":"markdown","1028a096":"markdown","1bf66ba1":"markdown","fb2cf376":"markdown","85adf677":"markdown"},"source":{"c5ba8f18":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport math\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\nfrom nltk.stem.snowball import SnowballStemmer\nimport re, string\nimport time\nfrom sklearn import preprocessing\nfrom sklearn.utils import resample\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom scipy.sparse import hstack\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, roc_curve, auc, roc_auc_score\nfrom sklearn.pipeline import Pipeline\n\n\n%matplotlib inline","41a97af5":"art_columns = ['articleID', 'articleWordCount', 'newDesk', 'typeOfMaterial', 'pubDate']\ncom_columns = ['articleID', 'createDate', 'approveDate', 'commentBody', 'recommendations', 'replyCount','editorsSelection']\n\ndf_art_jan17 = pd.read_csv('..\/input\/ArticlesJan2017.csv', usecols=art_columns)\ndf_com_jan17 = pd.read_csv('..\/input\/CommentsJan2017.csv', usecols=com_columns)\ndf_art_fev17 = pd.read_csv('..\/input\/ArticlesFeb2017.csv', usecols=art_columns)\ndf_com_fev17 = pd.read_csv('..\/input\/CommentsFeb2017.csv', usecols=com_columns)\ndf_art_mar17 = pd.read_csv('..\/input\/ArticlesMarch2017.csv', usecols=art_columns)\ndf_com_mar17 = pd.read_csv('..\/input\/CommentsMarch2017.csv', usecols=com_columns)\ndf_art_apr17 = pd.read_csv('..\/input\/ArticlesApril2017.csv', usecols=art_columns)\ndf_com_apr17 = pd.read_csv('..\/input\/CommentsApril2017.csv', usecols=com_columns)\ndf_art_may17 = pd.read_csv('..\/input\/ArticlesMay2017.csv', usecols=art_columns)\ndf_com_may17 = pd.read_csv('..\/input\/CommentsMay2017.csv', usecols=com_columns)\ndf_art_jan18 = pd.read_csv('..\/input\/ArticlesJan2018.csv', usecols=art_columns)\ndf_com_jan18 = pd.read_csv('..\/input\/CommentsJan2018.csv', usecols=com_columns)\ndf_art_fev18 = pd.read_csv('..\/input\/ArticlesFeb2018.csv', usecols=art_columns)\ndf_com_fev18 = pd.read_csv('..\/input\/CommentsFeb2018.csv', usecols=com_columns)\ndf_art_mar18 = pd.read_csv('..\/input\/ArticlesMarch2018.csv', usecols=art_columns)\ndf_com_mar18 = pd.read_csv('..\/input\/CommentsMarch2018.csv', usecols=com_columns)\ndf_art_apr18 = pd.read_csv('..\/input\/ArticlesApril2018.csv', usecols=art_columns)\ndf_com_apr18 = pd.read_csv('..\/input\/CommentsApril2018.csv', usecols=com_columns)\n\ncomments = [df_com_jan17, df_com_fev17, df_com_mar17, df_com_apr17, df_com_may17, df_com_jan18, df_com_fev18, df_com_mar18, df_com_apr18]\ndf_comments = pd.concat(comments)\narticles = [df_art_jan17, df_art_fev17, df_art_mar17, df_art_apr17, df_art_may17, df_art_jan18, df_art_fev18, df_art_mar18, df_art_apr18]\ndf_articles = pd.concat(articles)\n\ndf = pd.merge(df_articles, df_comments, on='articleID', how='inner') ","a8fed71e":"def remove_tags(text):\n    TAG_RE = re.compile(r'<[^>]+>')\n    return TAG_RE.sub('', text)\n\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]',' ',text)","0fa4b5d8":"df.commentBody = df.commentBody.apply(lambda x: remove_tags(x))\ndf.commentBody = df.commentBody.apply(lambda x: remove_punctuation(x))","db11fe4e":"df.commentBody = df.commentBody.apply(lambda x: str(x.encode('ascii', 'ignore')))","bd1df27c":"#Creating Name\/Entity\/Place dictionary\ndef NER(df):\n    continuous_chunk = []\n    for index, row in df.iterrows():\n        chunked = ne_chunk(pos_tag(word_tokenize(row['commentBody'])))\n        prev = None\n        current_chunk = []\n        for i in chunked:\n            if type(i) == Tree:\n                current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n            elif current_chunk:\n                named_entity = \" \".join(current_chunk)\n                if named_entity not in continuous_chunk:\n                    continuous_chunk.append(named_entity)\n                    current_chunk = []\n                else:\n                    continue\n    return set(continuous_chunk)\n\nstart_time = time.time()\n#Using only NER extracted from comments selected as \"Editor's Selection\" to reduce the dictionary\npeople_words = NER(df[df['editorsSelection'] == 1])\nprint(time.time() - start_time)","d0b7215d":"#Creating English Dictionary using NLTK package\nwords = set(nltk.corpus.words.words() + list(nltk.corpus.wordnet.words()) + list(people_words))","23c90615":"def clean_text(text):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w in words or w.lower() in words or (nltk.corpus.wordnet.morphy(w.lower()) is not None and nltk.corpus.wordnet.morphy(w.lower()).lower() in words))\n\nstart_time = time.time()\ndf.commentBody = df.commentBody.apply(lambda x: clean_text(x)) \nprint(time.time() - start_time)","d517cb6f":"df['commentApprovalLength'] = df.apply(lambda row: (dt.datetime.fromtimestamp(int(row['approveDate'])) - dt.datetime.fromtimestamp(int(row['createDate']))).total_seconds(), axis=1)\ndf['commentPubLength'] = df.apply(lambda row: (dt.datetime.fromtimestamp(int(row['approveDate'])) - dt.datetime.strptime(row['pubDate'], '%Y-%m-%d %H:%M:%S')).total_seconds(), axis=1)","4d592d4b":"df['commentWordCount'] = df.apply(lambda row: sum(Counter(row['commentBody'].split()).values()), axis=1)","68d4eaa7":"#Cleaning unused features\ndf.drop(columns=['approveDate', 'createDate', 'articleID', 'pubDate'], inplace=True)","b7336f64":"df = pd.get_dummies(df, columns=[\"newDesk\", \"typeOfMaterial\"])","1de5591f":"min_max_scaler = preprocessing.MinMaxScaler()\n#Array with numerical features\nnp_numbers = df[['articleWordCount', 'recommendations', 'replyCount', 'commentWordCount', 'commentApprovalLength', 'commentPubLength']].values.astype(float)\n\n#Normalizing\nnp_scaled = min_max_scaler.fit_transform(np_numbers)\ndf_normalized = pd.DataFrame(np_scaled)\n#Renaming columns\ndf_normalized.columns = ['articleWordCount', 'recommendations', 'replyCount', 'commentWordCount', 'commentApprovalLength', 'commentPubLength']\n\n#Joining new dataframe\ncols = [i for i in df.columns.values if i not in list(['articleWordCount', 'recommendations', 'replyCount', 'commentWordCount','commentApprovalLength','commentPubLength'])]\ndf_normalized = df_normalized.join(df[cols])","53c2eae1":"#Checking how the features are now\ndf_normalized.head(2)","41dbbf9d":"pd.value_counts(df_normalized['editorsSelection']).plot.pie(figsize = (5,5))\nprint((float(pd.value_counts(df_normalized['editorsSelection'])[1])\/float(pd.value_counts(df_normalized['editorsSelection']).sum())) * 100)","841e7c59":"'''\nReturns the balanced class using the percentage that is needed to maintain in the class with less occurrence.\nFor example, if we need the class 1 (minority) to represents 30% of the entire dataset, percent = 0.3. \nThe class 0 (majority) will have samples removed randomly until it represents 70% of the entire dataset.\n'''\ndef downSampling(sample, col_class, percent):\n    #Finding the majority and minority class\n    counts = sample[col_class].value_counts().to_dict()\n    max_label = max(counts.keys(), key=(lambda k: counts[k]))\n    min_label = min(counts.keys(), key=(lambda k: counts[k]))\n    #Separating class samples\n    sample_max = sample[sample[col_class] == max_label]\n    sample_min = sample[sample[col_class] == min_label]\n    #Finding the actual ratio between classes\n    actual_ratio = float(min(counts.values()))\/float(sum(counts.values()))\n    if(actual_ratio >= percent):\n        return sample\n    #Calculating the number of necessary samples to be excluded\n    desired_samples = int(float(min(counts.values()) - (percent * min(counts.values()))) \/ float(percent))\n    #Resampling dataset\n    sample_max_downsampled = resample(sample_max, replace=False, n_samples=desired_samples, random_state=100)\n    #Combining samples\n    sample_downsampled = pd.concat([sample_max_downsampled, sample_min])\n    return sample_downsampled\n\n#It was chosen the 1:4 (25%) ratio for downsampling\ndf_normalized_balanced = downSampling(df_normalized, 'editorsSelection', 0.25)\npd.value_counts(df_normalized_balanced['editorsSelection']).plot.pie(figsize = (5,5))\nprint((float(pd.value_counts(df_normalized_balanced['editorsSelection'])[1])\/float(pd.value_counts(df_normalized_balanced['editorsSelection']).sum())) * 100)","ff68da1f":"stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n    \nstemmed_count_vect = StemmedCountVectorizer(stop_words='english', decode_error='ignore')\ncounts = stemmed_count_vect.fit_transform(df_normalized_balanced['commentBody'].values.astype('U'))\n\nprint(counts.shape)","69a7b1d4":"tfidf_transformer = TfidfTransformer()\ntfidf = tfidf_transformer.fit_transform(counts)\n\nprint(tfidf.shape)","0d884a2f":"y = df_normalized_balanced[['editorsSelection']]\nX = df_normalized_balanced.drop(['editorsSelection', 'commentBody'], axis=1)","77c803af":"def get_metrics(y_test, predicted):\n    fpr, tpr, thresholds = roc_curve(y_test, predicted, pos_label=1)\n    roc = auc(fpr, tpr)\n    f1 = f1_score(y_test, predicted, average='binary')\n    ac = np.mean(predicted == y_test)\n    return roc, f1, ac","62a70a66":"def train_predict(clf, X_train, y_train, X_test):\n    clf.fit(X_train, y_train)\n    predicted = clf.predict(X_test)\n    return predicted\n\ndef results_train_test(clf, X, y):\n\n    X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=100)\n    X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=100)\n    X_train_60, X_test_40, y_train_60, y_test_40 = train_test_split(X, y, test_size=0.4, random_state=100)\n\n    pred_20 = train_predict(clf, X_train_80, y_train_80, X_test_20)\n    pred_30 = train_predict(clf, X_train_70, y_train_70, X_test_30)\n    pred_40 = train_predict(clf, X_train_60, y_train_60, X_test_40)\n\n    roc20, f120, acc20 = get_metrics(y_test_20, pred_20)\n    roc30, f130, acc30 = get_metrics(y_test_30, pred_30)\n    roc40, f140, acc40 = get_metrics(y_test_40, pred_40)\n    mean_roc = (roc20 + roc30 + roc40) \/ 3\n    mean_acc = (acc20 + acc30 + acc40) \/ 3\n    mean_f1 = (f120 + f130 + f140) \/ 3\n    print('Mean Accuracy: {0:0.2f}'.format(mean_acc))\n    print('Mean AUROC: {0:0.2f}'.format(mean_roc))\n    print('Mean F1-Score: {0:0.2f}'.format(mean_f1))","20f3f5c7":"cols = [i for i in df_normalized_balanced.columns.values if i not in list(['editorsSelection','commentBody', 'articleWordCount'])]\n\nfeatures_vector = hstack((tfidf,np.array(X['articleWordCount'])[:,None]))\nfor i in cols:\n    features_vector = hstack((features_vector,np.array(X[i])[:,None]))\nprint(features_vector.shape)","f05edc05":"#Creating SVM class with arbitrary parameters\nsvm_clf = linear_model.SGDClassifier(loss='hinge', max_iter=5, random_state=100)","8a1baf9c":"results_train_test(svm_clf, features_vector, y.values.ravel())","e4b7b218":"pipeline = Pipeline([\n    ('clf', SGDClassifier())\n])\n\nparameters = {\n    'clf__alpha': (0.0001, 0.00001, 0.000001, 0.0000001),\n    'clf__epsilon': (0.1, 0.01, 0.001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    'clf__max_iter': (5, 10, 15, 20),\n    'clf__class_weight': ('balanced', {0:.1, 1:.2}, {0:.1, 1:.3}, {0:.1, 1:.4})\n}\ngrid = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring = 'f1')","57547344":"#For GridSearch we will use the training\/testing ratio of 70%\/30%\nX_train, X_test, y_train, y_test = train_test_split(features_vector, y.values.ravel(), test_size=0.3, random_state=100)\ngrid.fit(X_train, y_train)","cd0c25f9":"print(\"Best Parameters:\")\nbest_parameters = grid.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","28ab5105":"optimal_svm_clf = linear_model.SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-06, epsilon=0.1, max_iter=20, class_weight={0: 0.1, 1: 0.2}, random_state=100)\nresults_train_test(optimal_svm_clf, features_vector, y.values.ravel())","18424224":"### NLP - Working with comments\n\nNow we use NLP techniques to transform the comments text into a feature vector, to use later in classification.\n\n#### Count Vectorizer e Stemming\n\nWe will use Count Vectorizer to transform text into a count vector with the words frequency. Also, we use Stemming to reduce words to the root form.","ea5652e7":"## Discussing results","75c1a196":"With the best parameters, it is possible to improve SVM performance.","397f2c43":"#### Quantifying categorical features\n\nThe categorical features should be transformed into numerical features to help the classifier to not misinterpret. It will be created new columns to represent the categories in numbers (0 or 1).","f819a847":"The classification algorithm chosen is SVM with linear kernel (**SGDClassifier**).\n\nThe chosen metrics to measure the model performance will be **Acurracy**, **AUROC**, and **F1-Score**.\n\nFor training and validation, the dataset will be split into three parts: **80%\/20%**, **70%\/30%** e **60%\/40%** (Training\/Testing). After that, we calculate the mean of the metrics.","12c16fcd":"#### SVM (first results)\n| |\tSVM (Raw)\t\t\t|\n|-|-----------------------|\n| Mean Accuracy\t\t\t| 0.80 |\n| Mean AUROC  | 0.61 |\n| Mean F1-Score\t\t\t| 0.36 |\n\nThe first results show us how hard is to the classifier to infer the class = 1 (\"Editor's Selection\") due to the class balance. Even reducing the unbalanced class from our first dataset, it is still hard to classify with not much knowledge of the parameters.\n\n#### SVM (Grid Search)\n| |\tSVM (Grid Search)\t\t\t|\n|-|-----------------------|\n| Mean Accuracy| 0.85 |\n| Mean AUROC  | 0.80 |\n| Mean F1-Score\t\t\t| 0.70 |\n\nThe results now are way better. Improving the f1-score in Grid Search helped the SVM to penalise misclassification of the class = 1. \n","7eb55aa4":"#### Future work suggestions\n\nThere are some ways to improve the model.\n\n- The ignored features from the beginning may be used to achieve new insights into the dataset.\n- Feature selection may help to improve the model generalization and avoid overfitting. Maybe XGBoost could be a good option to reduce dimensionality.\n- SVM may be improved using another kernel. Linear kernel is good in performance, but there are others kernels can generalize better.","78bb6e18":"**What we have so far**: cleaned and treated features, extra features and comment texts vectorized.\n\nNow we build our model.","21847530":"### Balancing classes\n\nLet's check how the targed feature **editorsSelection** is balanced in the sample.","fa4cedf0":"### Abstract\n\nThe following notebook has the goal to predict the \"Editor's Selection\" comments of the open articles from the [New York Times](https:\/\/www.nytimes.com\/) webpage.\n\nThese are the steps to achieve this goal:\n- **Import data**\n- **Pre-processing features**\n- **Processing comments texts**\n- **Modeling and testing**\n- **Discussing results**\n\nThe imported dataset contains data from January 2017 until May 2017 and from January 2018 until May 2018, related to articles and comments from NY Times. After importing, some features are pre-selected to be used in the model. The features are treated and some new features are created. The *editorsSelection* represents if the comment is an \"Editor's Selection\" or not, and will be the target.\n\nThe feature **commentBody** represents the comment itself. I separate it and process the text using NLP techniques. I do **Count Vectorization**, **Stemming** and **TF-IDF** to vectorize and prepare the comments for use in the model.\n\nFor classification, the **SVM** algorithm is the chosen one. It will be evaluated using three measures: **Accuracy**, **AUROC**, and **F1-Score**.\n\nAfter that, the SVM will be reparameterized with **Grid Search**.\n\nFinally, the results will be discussed and some future works will be recommended.\n\n>Some codes and ideas were inspired by [Aashita Kesarwani Predicting NYT's pick notebook](https:\/\/www.kaggle.com\/aashita\/predicting-nyt-s-pick\/notebook)","559d53a9":"Also, it is now possible to count the number of words of the treated commentBody feature creating a new column called **commentWordCount**.","73e3499c":"To optimize the SVM performance, we are going to use **Grid Search**.","b0f5a6c0":"#### Grid Search\n\nTo improve the SVM parameterization, we put some parameters to GridSearch finds the optimal configuration.","fdd2315c":"#### Removing wrong coding","c46314ea":"#### Removing tags and punctuation","c2e44553":"### Modeling and evaluating","8e7e84d9":"Firstly, let's separate the target feature from the rest.","5041fef1":"## Pre-processing data","c6ddd69b":"# Predicting New York Times \"Editor's Selection\" comments","9982b379":"## Importing data","fd8451f8":"It is notable the unbalanced dataset. The editorsSelection class = 1 represents **1.91%** of the entire dataset. To work with this class, it is necessary to downsample the editorsSelection class = 0.","a7c7b37b":"## Beginning","7beacc6f":"#### TF-IDF\nTo reduce the weight of the counting, it is needed to normalize frequency with TF-IDF.","59099ba7":"#### Normalizing numerical features\n\nThe numerical features should be normalized to avoid wrong weighted classification. The MinMaxScaler() will be used for that.","b483941c":"### Creating feature variations\n\nThe data features won't be useful in their original conditions. It is necessary to quantify them.\n\nIn this case, it will be created two new features: **commentApprovalLength**, which represents the waiting time for the comment to be approved; and **commentPubLength**, which is the time between the article publication and the comment publication.","e99fec29":"We can now test the **SVM**.","64568091":">It was chosen the value of 25% to compare to [Aashita Kesarwani Predicting NYT's pick notebook](https:\/\/www.kaggle.com\/aashita\/predicting-nyt-s-pick\/notebook)\n\nNow the editorsSelection class = 1 is balanced to represents **25%** of the dataset.","1028a096":"We now create the full feature vector, joining the TF-IDF output and the others features.\n\n>Hstack is used to avoid memory overflow when adding the TF-IDF features to dataframe. Also, we will work with sparse matrices from now on.","1bf66ba1":"#### Treating english words\n\nUsing Name Entity Recognition (NER) and some English dictionary, it is possible to maintain only English words in comments.","fb2cf376":"There are two kinds of datasets: Articles Dataset and Comments Dataset.\n\nThe Articles dataset contains more than 9000 articles and 16 features. It represents the articles headers, with information like date of publication (pubDate) and category (newDesk).\n\nThe Comments dataset contains more than 2 million comments and 34 features. This dataset contains the comment feature (commentBody) and some information about the comment, i.e. comment date of publication (approveDate). Also, it contains the target feature: **editorsSelection**.\n\nHere is the feature relation:\n\n|\tArticles Dataset\t\t|\tComments Dataset   |\n|-----------------------|---------------------------|\n|\tabstract\t\t\t|\tapproveDate\t\t\t\t|\n|\tarticleID\t\t\t|\tarticleID\t\t\t\t|\n|\tarticleWordCount\t|\tarticleWordCount\t\t|\n|\tbyline\t\t\t\t|\tcommentBody\t\t\t\t|\n|\twebURL\t\t\t\t|\tcommentID\t\t\t\t|\n|\ttypeOfMaterial\t\t|\tcommentSequence\t\t\t|\n|\tsource\t\t\t\t|\tcommentTitle\t\t\t|\n|\tsnippet\t\t\t\t|\tcommentType\t\t\t\t|\n|\tsectionName\t\t\t|\tcreateDate\t\t\t\t|\n|\tpubDate\t\t\t\t|\tdepth\t\t\t\t\t|\n|\tprintPage\t\t\t|\teditorsSelection\t\t|\n|\tdocumentType\t\t|\tinReplyTo\t\t\t\t|\n|\theadline\t\t\t|\tnewDesk\t\t\t\t\t|\n|\tkeywords\t\t\t|\tparentID\t\t\t\t|\n|\tmultimedia\t\t\t|\tparentUserDisplayName\t|\n|\tnewDesk\t\t\t\t|\tpermID\t\t\t\t\t|\n|\t\t\t\t\t\t|\tpicURL\t\t\t\t\t|\n|\t\t\t\t\t\t|\tprintPage\t\t\t\t|\n|\t\t\t\t\t\t|\trecommendations\t\t\t|\n|\t\t\t\t\t\t|\trecommendedFlag\t\t\t|\n|\t\t\t\t\t\t|\treplyCount\t\t\t\t|\n|\t\t\t\t\t\t|\treportAbuseFlag\t\t\t|\n|\t \t\t\t\t\t|\tsectionName\t\t\t\t|\n|\t\t\t\t\t\t|\tsharing\t\t\t\t\t|\n|\t\t\t\t\t\t|\tstatus\t\t\t\t\t|\n|\t\t\t\t\t\t|\ttimespeople\t\t\t\t|\n|\t\t\t\t\t\t|\ttrusted\t\t\t\t\t|\n|\t\t\t\t\t\t|\ttypeOfMaterial\t\t\t|\n|\t\t\t\t\t\t|\tupdateDate\t\t\t\t|\n|\t\t\t\t\t\t|\tuserDisplayName\t\t\t|\n|\t\t\t\t\t\t|\tuserID\t\t\t\t\t|\n|\t\t\t\t\t\t|\tuserLocation\t\t\t|\n|\t\t\t\t\t\t|\tuserTitle\t\t\t\t|\n|\t\t\t\t\t\t|\tuserURL\t\t\t\t\t|\n\nFor now, some features will be *excluded* from our study. Some will be removed due to their nature of non-relation with our problem.\n\nHere are the chosen features for now:\n\n|\tArticles festures\t\t|\tComments features\t|\n|---------------------------|---------------------------|\n|\tarticleID\t\t\t\t|\tarticleID\t\t\t\t|\n|\tarticleWordCount\t\t|\tcreateDate\t\t\t\t|\n|\tnewDesk\t\t\t\t\t|\tapproveDate\t\t\t\t|\n|\ttypeOfMaterial\t\t\t|\tcommentBody\t\t\t\t|\n|\tpubDate \t\t\t\t|\trecommendations\t\t\t|\n|\t     \t\t\t\t\t|\treplyCount\t\t\t\t|\n|\t\t\t\t\t\t\t|\teditorsSelection\t\t|","85adf677":"### Comments feature\n\nMost of the comments of NY Times naturally are written in English. However, since the comments are opened for anyone, consequently we expect to find errors and unsupported characters (from different languages). For example, HTML tags and Japanese characters.\n\nSince the nature of an \"Editor's Selection\" comment is to be well written and containing a relevant opinion about the subject, we must treat the inconveniences of the **commentBody** feature."}}