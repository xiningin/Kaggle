{"cell_type":{"e6d2ac6e":"code","1b3d644b":"code","39f5ed74":"code","0f905ffe":"code","24d7c115":"code","3d126ab4":"code","3150582b":"code","6e3afd9d":"code","5552862f":"code","764bd767":"code","e528cb68":"code","5f3c1052":"code","b484440d":"code","5f80ba0d":"code","a90aea8b":"code","7452c56f":"markdown","1ec9b4e0":"markdown","edb1b344":"markdown","0a6d76ff":"markdown","e8199a61":"markdown","8505b79a":"markdown","64affa89":"markdown","e9b4df7e":"markdown","c56d5751":"markdown","7b04065c":"markdown","0cca4570":"markdown"},"source":{"e6d2ac6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b3d644b":"data = '\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv'\nweather_df = pd.read_csv(data)\nweather_df.info()\nweather_df.dropna(subset = ['RainToday','RainTomorrow'],inplace = True)","39f5ed74":"plt.title('No. of Rows per Year')\nsns.countplot(x=pd.to_datetime(weather_df.Date).dt.year);","0f905ffe":"year = pd.to_datetime(weather_df.Date).dt.year\ntrain_df = weather_df[year < 2015]\nval_df = weather_df[year == 2015]\ntest_df = weather_df[year > 2015]","24d7c115":"input_cols = list(train_df.columns)[1:-1]\ntarget_cols = 'RainTomorrow'\n\ntrain_inputs = train_df[input_cols].copy()\ntrain_targets = train_df[target_cols].copy()\nval_inputs = val_df[input_cols].copy()\nval_targets = val_df[target_cols].copy()\ntest_inputs = test_df[input_cols].copy()\ntest_targets = test_df[target_cols].copy()\n\nnumerical_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = train_inputs.select_dtypes('object').columns.tolist()\n","3d126ab4":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy = 'mean').fit(weather_df[numerical_cols])\ntrain_inputs[numerical_cols] = imputer.transform(train_inputs[numerical_cols])\nval_inputs[numerical_cols] = imputer.transform(val_inputs[numerical_cols])\ntest_inputs[numerical_cols] = imputer.transform(test_inputs[numerical_cols])","3150582b":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(weather_df[numerical_cols])\ntrain_inputs[numerical_cols] = scaler.transform(train_inputs[numerical_cols])\nval_inputs[numerical_cols] = scaler.transform(val_inputs[numerical_cols])\ntest_inputs[numerical_cols] = scaler.transform(test_inputs[numerical_cols])","6e3afd9d":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\nencoder.fit(weather_df[categorical_cols].fillna('Unknown'))\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\ntrain_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols].fillna('Unknown'))\nval_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols].fillna('Unknown'))\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols].fillna('Unknown'))","5552862f":"\nX_train = train_inputs[numerical_cols+encoded_cols]\nV_train = val_inputs[numerical_cols+encoded_cols]\n\nmodel = DecisionTreeClassifier( random_state=1)\nmodel.fit(X_train, train_targets)\n","764bd767":"# accuracy score for training set\nmodel.score(X_train, train_targets)","e528cb68":"# accuracy score for valdation set\nmodel.score(V_train, val_targets)","5f3c1052":"from sklearn.tree import plot_tree\nplt.figure(figsize = (80,20))\nplot_tree(model, feature_names = X_train.columns, max_depth=2,filled=True)","b484440d":"model = DecisionTreeClassifier(max_depth = 11, random_state=1)\nmodel.fit(X_train, train_targets)\n# accuracy for training set\nmodel.score(X_train, train_targets)","5f80ba0d":"# accuracy for validation set\nmodel.score(V_train,val_targets)","a90aea8b":"model = DecisionTreeClassifier(max_leaf_nodes=128,random_state=1)\nmodel.fit(X_train,train_targets)\nmodel.score(V_train, val_targets)","7452c56f":"#### max_leaf_nodes","1ec9b4e0":"### 1.Defining the training,validation and test set","edb1b344":"### 5. One Hot Encoding categorical values","0a6d76ff":"### 3. Imputing missing values in numerical columns","e8199a61":"### 2.Creating input,target,numerical,categorical columns","8505b79a":"### Visualisation","64affa89":"### 4.Scaling values","e9b4df7e":"# Reading Rain Dataset using Pandas","c56d5751":"## Hyperparameter and Overfitting\n#### max_depth\n","7b04065c":"## Creating DECISION TREE CLASSFIER","0cca4570":"## Preprocessing the data"}}