{"cell_type":{"e0f91d82":"code","779c1cbc":"code","caf8b488":"code","3eb39884":"code","35a41c7b":"code","1f2e379a":"code","25f309dd":"code","ae9fcbbb":"code","10ea9c91":"code","dc15a02e":"code","c3ccd53c":"code","84c9b6ab":"code","8297e282":"code","fbc02b8d":"code","dd4f8973":"code","c57a7751":"code","906b1304":"code","079d3267":"code","13bf376d":"code","d28a7001":"code","c3a6bceb":"code","887f83bf":"code","f12b0e23":"code","62dcf62b":"code","37eda980":"code","d2363d0e":"code","3b99ae86":"code","7017f39f":"code","7689380a":"code","df072858":"code","94a9c967":"code","73abb310":"code","f005d480":"code","cd7de0ec":"code","75b28c0a":"code","9ccf8808":"code","9bb62aae":"code","00b4ae15":"code","4c5aa92e":"code","2383bba6":"code","f6fdf964":"code","57dd3e47":"code","5ea73e4b":"code","0f694662":"code","677edd94":"code","1973d83b":"code","cd3ddc2a":"code","97d50809":"code","4076997c":"code","52f95b3f":"code","ed8e848a":"code","7922c7e3":"code","8c5d4bf1":"code","4fcf6d65":"code","7b8f1ca1":"code","3cbbfa9c":"code","71658869":"markdown","c94771a0":"markdown"},"source":{"e0f91d82":"!pip install -q transformers\n!wget https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/huggingface_pytorch\/scibert_scivocab_uncased.tar\n!tar -xvf .\/scibert_scivocab_uncased.tar","779c1cbc":"!pip install -q langdetect\n!pip install -q googletrans\nfrom googletrans import Translator\nfrom langdetect import detect","caf8b488":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3eb39884":"import numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\nimport seaborn as sns\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport random \nfrom sklearn.metrics import f1_score,accuracy_score,roc_auc_score\n\n# fastai\nimport fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nimport transformers\nfrom transformers import *\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nimport re\n\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","35a41c7b":"train_data = pd.read_csv('\/kaggle\/input\/researchtopictags\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/researchtopictags\/test.csv')  ","1f2e379a":"print(train_data.shape)\ntrain_data.head()","25f309dd":"print(test_data.shape)\ntest_data.head()","ae9fcbbb":"train_data['combined_text'] = train_data['TITLE'] + \" <join> \" + train_data['ABSTRACT']\ntest_data['combined_text'] = test_data['TITLE'] + \" <join> \" + test_data['ABSTRACT']","10ea9c91":"topics = ['Computer Science','Physics','Mathematics', 'Statistics','Quantitative Biology','Quantitative Finance']","dc15a02e":"com_sc = train_data['Computer Science'].value_counts()[1]\nphy = train_data['Physics'].value_counts()[1]\nmat = train_data['Mathematics'].value_counts()[1]\nstats = train_data['Statistics'].value_counts()[1]\nbio = train_data['Quantitative Biology'].value_counts()[1]\nfin = train_data['Quantitative Finance'].value_counts()[1]\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\ncounts = [com_sc,phy,mat,stats,bio,fin]\nax.bar(topics,counts)\nplt.show()","c3ccd53c":"train_data['length'] = train_data['combined_text'].apply(lambda x : x.count(\" \") + 1)\nsns.distplot(train_data['length'])","84c9b6ab":"SciBertTokenizer = AutoTokenizer.from_pretrained('allenai\/scibert_scivocab_uncased')\n\nSciBertModel = AutoModel.from_pretrained('allenai\/scibert_scivocab_uncased')","8297e282":"MODEL_CLASSES = {'scibert' : (SciBertModel, SciBertTokenizer, PretrainedConfig.from_json_file('.\/scibert_scivocab_uncased\/config.json'))}","fbc02b8d":"# Parameters\n\nseed = 42\n\nuse_fp16 = False\n\nbs = 16\n\nthreshold = 0.4\n\nMAX_LEN = 320\n\nmodel_type = 'scibert'\n\npretrained_model_name = 'allenai\/scibert_scivocab_uncased'","dd4f8973":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","c57a7751":"def seed_all(seed_value):\n    \n    random.seed(seed_value) # Python\n    \n    np.random.seed(seed_value) # cpu vars\n    \n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        \n        torch.cuda.manual_seed(seed_value)\n        \n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        \n        torch.backends.cudnn.deterministic = True  #needed\n        \n        torch.backends.cudnn.benchmark = False","906b1304":"seed_all(seed)","079d3267":"class TransformersBaseTokenizer(BaseTokenizer):\n    \n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    \n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', max_len = MAX_LEN,**kwargs):\n        \n        self._pretrained_tokenizer = pretrained_tokenizer\n        \n        self.max_seq_len = max_len\n        \n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \n        \"\"\"Limits the maximum sequence length and add the special tokens\"\"\"\n        \n        CLS = self._pretrained_tokenizer.cls_token\n        \n        SEP = self._pretrained_tokenizer.sep_token\n        \n        tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n        \n        tokens = [CLS] + tokens + [SEP]\n        \n        return tokens","13bf376d":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","d28a7001":"class TransformersVocab(Vocab):\n    \n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        \n        super(TransformersVocab, self).__init__(itos = [])\n        \n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \n        \"Convert a list of tokens `t` to their ids.\"\n        \n        return self.tokenizer.convert_tokens_to_ids(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \n        \"Convert a list of `nums` to their tokens.\"\n        \n        nums = np.array(nums).tolist()\n        \n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        \n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        \n        self.itos = state['itos']\n        \n        self.tokenizer = state['tokenizer']\n        \n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","c3a6bceb":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","887f83bf":"pad_first = False\n\npad_idx = transformer_tokenizer.pad_token_id","f12b0e23":"data_classifier = (TextList.from_df(train_data, cols='combined_text', processor=transformer_processor)\n                         .split_by_rand_pct(0.3,seed=seed)\n                         .label_from_df(cols= topics)\n                         .add_test(test_data)\n                         .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","62dcf62b":"data_classifier.save('data_classifier.pkl')","37eda980":"data_classifier = load_data('','data_classifier.pkl',bs=bs)","d2363d0e":"print('[CLS] token :', transformer_tokenizer.cls_token)\n\nprint('[SEP] token :', transformer_tokenizer.sep_token)\n\nprint('[PAD] token :', transformer_tokenizer.pad_token)\n\ndata_classifier.show_batch()","3b99ae86":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\n\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\n\nprint('[PAD] id :', pad_idx)\n\ntest_one_batch = data_classifier.one_batch()[0]\n\nprint('Batch shape : ',test_one_batch.shape)\n\nprint(test_one_batch)","7017f39f":"# defining our model architecture \n\nclass CustomTransformerModel(nn.Module):\n    \n    def __init__(self, transformer_model: PreTrainedModel):\n        \n        super(CustomTransformerModel,self).__init__()\n        \n        self.transformer = transformer_model\n        \n        self.classifier = nn.Sequential(#nn.Linear(in_features=768, out_features=768, bias=True),\n                                        #nn.Dropout(p=0.1, inplace=False),\n                                        nn.Linear(in_features=768, out_features=6, bias=True))\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n   \n        outputs = self.transformer(input_ids,\n                                  attention_mask = attention_mask)\n        \n        pooled_output = outputs[1]\n        \n        logits = self.classifier(pooled_output)\n        \n        return logits","7689380a":"config = config_class.from_pretrained(pretrained_model_name)\n\nconfig.num_labels = 6\n\nconfig.use_bfloat16 = use_fp16\n\nprint(config)","df072858":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","94a9c967":"class MicroF1(Callback):\n\n    _order = -20 #is crucial - without it the custom columns will not be added - it tells the callback system to run this callback before the recorder system.\n\n    def __init__(self,learn,thresh,eps = 1e-15, sigmoid = True,**kwargs):\n        \n        self.learn = learn\n        \n        self.thresh = thresh\n        \n        self.eps = eps\n        \n        self.sigmoid = sigmoid\n\n    def on_train_begin(self, **kwargs): \n        \n        self.learn.recorder.add_metric_names(['MicroF1'])\n    \n    def on_epoch_begin(self, **kwargs):\n        \n        self.tp = 0\n        \n        self.total_pred = 0\n        \n        self.total_targ = 0\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        \n        pred, targ = ((last_output.sigmoid() if self.sigmoid else last_output) > self.thresh).byte(), last_target.byte()\n        \n        if torch.equal(torch.tensor(pred.shape),torch.tensor(targ.shape)):\n            \n            m = pred*targ\n            \n            self.tp += m.sum(0).float()\n            \n            self.total_pred += pred.sum(0).float()\n            \n            self.total_targ += targ.sum(0).float()\n    \n    def fbeta_score(self, precision, recall):\n        \n        return 2*(precision*recall)\/((precision + recall) + self.eps)\n\n    def on_epoch_end(self, last_metrics, **kwargs):\n        \n        self.total_pred += self.eps\n        \n        self.total_targ += self.eps\n        \n        precision, recall = self.tp.sum() \/ self.total_pred.sum(), self.tp.sum() \/ self.total_targ.sum()\n        \n        res = self.fbeta_score(precision, recall)        \n        \n        return add_metrics(last_metrics, res)","73abb310":"class AUCROC(Callback):\n    \n    _order = -20 \n    \n    def __init__(self, learn, **kwargs): \n        \n        self.learn = learn\n        \n        self.output, self.target = [], []\n        \n    def on_train_begin(self, **kwargs): \n        \n        self.learn.recorder.add_metric_names(topics)\n        \n    def on_epoch_begin(self, **kwargs): \n        \n        self.output, self.target = [], []\n    \n    def on_batch_end(self, last_target, last_output, train, **kwargs):\n        \n        if not train:\n            \n            self.output.append(last_output)\n            \n            self.target.append(last_target)\n                \n    def on_epoch_end(self, last_metrics, **kwargs):\n        \n        if len(self.output) > 0:\n            \n            output = torch.cat(self.output)\n            \n            target = torch.cat(self.target)\n            \n            preds = F.softmax(output, dim=1)\n            \n            metric = []\n\n            for i in range(0,target.shape[1]):\n                \n                \n                metric.append(roc_auc_score(target.cpu().numpy()[...,i], preds[...,i].cpu().numpy(),average='macro'))\n            \n            return add_metrics(last_metrics, metric)\n        \n        else:\n            \n            return","f005d480":"microF1 = partial(MicroF1,thresh = threshold) #metric\n\nCustomAdamW = partial(AdamW, correct_bias=False) #optimizer","cd7de0ec":"classifierModel = Learner(data_classifier, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  callback_fns = [microF1,AUCROC],\n                  loss_func = nn.BCEWithLogitsLoss()\n                 )\n\n# Show graph of learner stats and metrics after each epoch.\nclassifierModel.callbacks.append(ShowGraph(classifierModel))","75b28c0a":"print(classifierModel.model)","9ccf8808":"#n = len(classifierModel.model.transformer.base_model.encoder.layer)\/\/3\n\n#list_layers = [[classifierModel.model.transformer.base_model.embeddings],\n#               list(classifierModel.model.transformer.base_model.encoder.layer[:n]),\n#               list(classifierModel.model.transformer.base_model.encoder.layer[n+1:2*n]),\n#               list(classifierModel.model.transformer.base_model.encoder.layer[(2*n)+1:]),\n#               [classifierModel.model.transformer.base_model.pooler],\n#               classifierModel.model.classifier]","9bb62aae":"list_layers = [classifierModel.model.transformer.base_model.embeddings,\n              classifierModel.model.transformer.base_model.encoder.layer[0],\n              classifierModel.model.transformer.base_model.encoder.layer[1],\n              classifierModel.model.transformer.base_model.encoder.layer[2],\n              classifierModel.model.transformer.base_model.encoder.layer[3],\n              classifierModel.model.transformer.base_model.encoder.layer[4],\n              classifierModel.model.transformer.base_model.encoder.layer[5],\n              classifierModel.model.transformer.base_model.encoder.layer[6],\n              classifierModel.model.transformer.base_model.encoder.layer[7],\n              classifierModel.model.transformer.base_model.encoder.layer[8],\n              classifierModel.model.transformer.base_model.encoder.layer[9],\n              classifierModel.model.transformer.base_model.encoder.layer[10],\n              classifierModel.model.transformer.base_model.encoder.layer[11],\n              classifierModel.model.transformer.base_model.pooler]","00b4ae15":"classifierModel.split(list_layers)\n\nnum_groups = len(classifierModel.layer_groups)\n\nprint('Learner split in',num_groups,'groups')\n\nprint(classifierModel.layer_groups)\n","4c5aa92e":"classifierModel.save('untrain')","2383bba6":"seed_all(seed)\n\nclassifierModel.load('untrain');","f6fdf964":"classifierModel.freeze_to(-1)","57dd3e47":"classifierModel.summary()","5ea73e4b":"classifierModel.lr_find()","0f694662":"classifierModel.recorder.plot(skip_end=10,suggestion=True)","677edd94":"classifierModel.fit_one_cycle(5, max_lr = 3e-3 ,moms=(0.8,0.7))","1973d83b":"classifierModel.save('classifierModel1')","cd3ddc2a":"seed_all(seed)\n\nclassifierModel.load('classifierModel1');","97d50809":"for i in range(2,6):\n    \n    print('=' * 50, f\" Frozen Layer Group {i}\", '=' * 50)\n    \n    classifierModel.freeze_to(-i)\n    \n    classifierModel.fit_one_cycle(1,slice(1e-6,2e-6),moms=(0.8,0.7))\n    \n    print ('')","4076997c":"classifierModel.save('classifierModel2')","52f95b3f":"seed_all(seed)\n\nclassifierModel.load('classifierModel2');","ed8e848a":"classifierModel.unfreeze()","7922c7e3":"print('=' * 50, f\" All Layers Unfrozen \", '=' * 50)\nclassifierModel.fit_one_cycle(2, max_lr=slice(1e-6,2e-6), moms=(0.8, 0.9))","8c5d4bf1":"classifierModel.show_results()","4fcf6d65":"class_probs = classifierModel.get_preds(DatasetType.Test)[0]","7b8f1ca1":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = (class_probs > 0.4).byte().detach().cpu().numpy()\n    \n    sampler = [i for i in data_classifier.dl(ds_type).sampler]\n    \n    reverse_sampler = np.argsort(sampler)\n    \n    return preds[reverse_sampler, :]\n\npreds = get_preds_as_nparray(DatasetType.Test)","3cbbfa9c":"submission = pd.read_csv('\/kaggle\/input\/researchtopictags\/sample.csv')\n\nsubmission.iloc[:,1:] =  preds\n\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","71658869":"# References\n\nhttps:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/\n\nhttps:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/metrics.py#L318\n\nhttps:\/\/sgugger.github.io\/the-1cycle-policy.html","c94771a0":"## Learner\u00a0: Custom\u00a0Metric"}}