{"cell_type":{"a5ccf40c":"code","4e115e9b":"code","57f4f6e7":"code","3f24d83b":"code","ee3448b6":"code","f4503e2e":"code","0a4b6da2":"code","69feb694":"code","824384e6":"code","8660e158":"code","a8e99420":"code","1d0bcc91":"code","71e3cf1c":"code","91e377da":"code","dad258c5":"code","eee7052b":"code","af55f422":"code","2a22db5e":"code","34a7a1ee":"code","3b567c1f":"code","782a75d5":"markdown","6626ac52":"markdown","d0c9a47a":"markdown","6894e7ed":"markdown","e1bd951a":"markdown","18d9bcb1":"markdown","4d4566f3":"markdown","562e6b24":"markdown","33282104":"markdown","332a717d":"markdown","de28f937":"markdown","b68493d7":"markdown","d7e867fc":"markdown"},"source":{"a5ccf40c":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport ast\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom glob import glob\nfrom dask import bag\nimport cv2\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, DepthwiseConv2D, BatchNormalization, ZeroPadding2D, Lambda\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, AveragePooling2D , Layer\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.metrics import top_k_categorical_accuracy","4e115e9b":"import os\n# cwd = os.getcwd()\nfile_path = r\"..\/input\/quickdraw-doodle-recognition\/train_simplified\"\nfiles = os.listdir(file_path)\nword_category = [f.split(\".\")[0] for f in files]\nchunk_size = 100\n\n# for index, word in enumerate(word_category):\n#     df = pd.read_csv(os.path.join(file_path, str(word+\".csv\")))\n#     for k in range(chunk_size):\n#         filename = 'train_{}.csv'.format(k)\n#         df['file_index'] = index\n#         df['cv'] = (df.key_id \/\/ 10 ** 7) % chunk_size\n#         chunk = df[df.cv == k]\n#         chunk = chunk.drop(['key_id'], axis=1)\n\n# print(\"===Data shuffle Finished===\")","57f4f6e7":"# print(\"===Data Compression Starting===\")    \n# for k in tqdm(range(chunk_size)):\n#     filename = 'train_{}.csv'.format(k)\n#     if os.path.exists(filename):\n#         df = pd.read_csv(filename)\n#         df['rnd'] = np.random.rand(len(df))\n#         df = df.sort_values(by='rnd').drop('rnd', axis=1)\n#         df.to_csv(filename + '.gz', compression='gzip', index=False)\n#         os.remove(filename)\n# print(\"===Data Compression Done==\")","3f24d83b":"from PIL import ImageDraw, Image\ndef make_img(img_arr) :\n    image = Image.new(\"P\", (256,256), color=255)\n    image_draw = ImageDraw.Draw(image)\n    for stroke in img_arr:\n        for i in range(len(stroke[0])-1):\n            image_draw.line([stroke[0][i], \n                             stroke[1][i],\n                             stroke[0][i+1], \n                             stroke[1][i+1]],\n                            fill=0, width=5)\n    return image\n\n\ndef stroke_to_img(strokes): \n    img=np.zeros((256,256))\n    for each in ast.literal_eval(strokes):\n        for i in range(len(each[0])-1):\n            cv2.line(img,(each[0][i],each[1][i]),(each[0][i+1],each[1][i+1]),255,5)\n    img=cv2.resize(img,(32,32))\n    img=img\/255\n    return img","ee3448b6":"one_hot_encoding = np.eye(len(word_category))\ncategory_y_label = dict()\nindex = 0\nfor i in word_category:\n    category_y_label[i]=one_hot_encoding[index]\n    index+=1\n    \n# f = \"train_1.csv.gz\"\nf = os.path.join(file_path,\"horse.csv\")\ndf = pd.read_csv(f)\nX = []\nY = []\nnum=0\n# for i in df.values:\n#     if i[2]==True:   #recognized\uac00 True\uc77c\ub54c \n# #         x = make_img(ast.literal_eval(i[1]))\n# #         x = np.array(x.resize((64,64)))\n#         x = ast.literal_eval(i[1])\n#         X.append(x)\n#         Y.append(i[4])\n#         num+=1\n#         if n%1000 == 0:\n#             print(\"==={}\ubc88\uc9f8 \uc644\ub8cc\".format(num))\nfor i in df.values:\n    if i[3]==True:   #recognized\uac00 True\uc77c\ub54c \n#         x = make_img(ast.literal_eval(i[1]))\n#         x = np.array(x.resize((64,64)))\n        x = ast.literal_eval(i[1])\n        X.append(x)\n        Y.append(i[5])\n        num+=1\n        if num%10000 == 0:\n            print(\"==={}\ubc88\uc9f8 \uc644\ub8cc\".format(num))","f4503e2e":"X2 =[]\nn=0\nfor i in X:\n    x = make_img(i)\n    x = np.array(x.resize((64,64)))\n    X2.append(x)\n    if n%10000 == 0:\n        print(\"==={}\ubc88\uc9f8 \uc644\ub8cc==\".format(n))\n    n+=1\nX2 = np.array(X2)","0a4b6da2":"Y2 = []\nn = 0\nfor y in Y:\n    Y2.append(category_y_label[y])\n    if n%10000 == 0:\n        print(\"==={}\ubc88\uc9f8 \uc644\ub8cc==\".format(n))\n    n+=1\nY2 = np.array(Y2)","69feb694":"index = 340\n\nfor key,value in category_y_label.items():\n    if str(Y2[index])==str(value):\n        print(key)\n\nfor x,y in X[index]:\n    plt.plot(x, -np.array(y), lw=3)","824384e6":"## extra imports to set GPU options\nimport tensorflow as tf\nfrom keras import backend as k\n \n###################################\n# TensorFlow wizardry\nconfig = tf.compat.v1.ConfigProto()\n \n# Don't pre-allocate memory; allocate as-needed\nconfig.gpu_options.allow_growth = True\n \n# Only allow a total of half the GPU memory to be allocated\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\n \n# Create a session with the above options specified.\ntf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n# k.backend.set_session(tf.compat.v1.Session(config=config))\n# k.tensorflow_backend.set_session(tf.compat.v1.Session(config=config))\n###################################","8660e158":"train_grand=[]\nnum_class = 340\nper_class=2000","a8e99420":"class_paths = glob('\/kaggle\/input\/quickdraw-doodle-recognition\/train_simplified\/*.csv')\nfor i , c in enumerate(tqdm(class_paths[0:num_class])): \n    train=pd.read_csv(c,usecols=['drawing','recognized'],nrows=per_class*2)\n    train=train[train.recognized==True].head(per_class)\n    imagebag=bag.from_sequence(train.drawing.values).map(stroke_to_img)\n    train_array=np.array(imagebag.compute())\n    train_array=np.reshape(train_array,(per_class,-1))    \n    label_array=np.full((train.shape[0],1),i)\n    train_array=np.concatenate((label_array,train_array),axis=1)\n    train_grand.append(train_array)\ndel train_array\ndel label_array","1d0bcc91":"train_grand=np.array([train_grand.pop() for i in np.arange(num_class)]) \nheight = 32\nwidth = 32\ntrain_grand=train_grand.reshape((-1,(height*width+1))) \nprint(train_grand)","71e3cf1c":"specific = 0.1 \nsequence_length = 50\ncut = int(specific * train_grand.shape[0])\nprint(cut)\n\nnp.random.shuffle(train_grand)\ny_train, X_train = train_grand[cut: , 0], train_grand[cut: , 1:]\ny_val, X_val = train_grand[0:cut, 0], train_grand[0:cut, 1:]\n\ndel train_grand\n\nx_train=X_train.reshape(X_train.shape[0],height,width,1)\nx_val=X_val.reshape(X_val.shape[0],height,width,1)\n\nprint(y_train.shape, \"\\n\",\n      x_train.shape, \"\\n\",\n      y_val.shape, \"\\n\",\n      x_val.shape)","91e377da":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), strides=(1, 1), input_shape=(32, 32,1)))\nmodel.add(Activation('relu'))\n# model.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(32, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2),data_format=\"channels_last\"))\nmodel.add(Conv2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2),data_format=\"channels_last\"))\nmodel.add(Conv2D(128, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Conv2D(128, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_class))\nmodel.add(Activation('softmax'))\n# model.compile(RMSprop(lr=self.learningRate), 'MSE')\nmodel.summary()","dad258c5":"def top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)","eee7052b":"reduceLROnPlat=ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=3,\n                                 verbose=1,mode='auto',min_delta=0.005,\n                                 cooldown=5,min_lr=0.0001)\n\ncallbacks=[reduceLROnPlat]\n\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',\n              metrics=['accuracy',top_3_accuracy])\n\nhistory=model.fit(x=x_train,y=y_train,batch_size=32,epochs=200,\n                  validation_data=(x_val,y_val),callbacks=callbacks,verbose=1)","af55f422":"acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss= history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,label='Training acc')\nplt.plot(epochs,val_acc,label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs,loss,label='Training loss')\nplt.plot(epochs,val_loss,label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","2a22db5e":"#%% get test set\nttvlist = []\nreader = pd.read_csv('..\/input\/quickdraw-doodle-recognition\/test_simplified.csv', index_col=['key_id'],\n    chunksize=2048)\nfor chunk in tqdm(reader, total=55):\n    imagebag = bag.from_sequence(chunk.drawing.values).map(stroke_to_img)\n    testarray = np.array(imagebag.compute())\n    testarray = np.reshape(testarray, (testarray.shape[0], 32, 32, 1))\n    testpreds = model.predict(testarray, verbose=0)\n    ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n    ttvlist.append(ttvs)\n    \nttvarray = np.concatenate(ttvlist)","34a7a1ee":"numstonames={i : v[:-4].replace(' ','_') for i , v in enumerate(os.listdir(file_path))}\npreds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\npreds_df = preds_df.replace(numstonames)\npreds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n\nresult = pd.read_csv('..\/input\/quickdraw-doodle-recognition\/sample_submission.csv', index_col=['key_id'])\nresult['word'] = preds_df.words.values\nresult.to_csv('submission.csv')\nresult.head()","3b567c1f":"df2 = pd.read_csv('..\/input\/quickdraw-doodle-recognition\/test_simplified.csv', nrows=100)\ndf2['drawing'] = df2['drawing'].apply(ast.literal_eval)\nn = 10\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(16, 10))\nfor i, drawing in enumerate(df2['drawing']):\n    ax = axs[i \/\/ n, i % n]\n    for x, y in drawing:\n        ax.plot(x, -np.array(y), lw=3)\nplt.savefig(\"test_img.png\")\nplt.show();","782a75d5":"### IMAGE \uae30\ubc18 CNN \ud559\uc2b5","6626ac52":"### \ub370\uc774\ud130 \uadf8\ub9bc\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uae30","d0c9a47a":"1. \uac00\uc7a5 \uba3c\uc800 PIL \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 ImageDraw\ub97c \ud65c\uc6a9\ud558\uc5ec \uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\ub97c Height=64, Width=64, Channel=1 \uc758 array\ub85c \ubc14\uafc8\n2. One-hot encoding \uc2e4\ud589. \n3. shuffle\ub41c \ub370\uc774\ud130 \uc911 recognized\uac00 True\uc77c\ub54c, df['drawing']\uc744 ast.literal_eval \ud568\uc218\ub97c \ud65c\uc6a9\ud558\uc5ec string\uc774 \uc544\ub2cc \ubc30\uc5f4\ub85c \ubcc0\ud658\n4. 3\uc5d0\uc11c \ubcc0\ud658\ub41c \ub370\uc774\ud130\ub97c X\ub77c\ub294 \ub9ac\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \uc800\uc7a5\ud568 \n5. 4\uc5d0\uc11c \ubc1b\uc740 \ub370\uc774\ud130\ub97c (64,64)\ub85c reshape\ud558\uc5ec \uc0c8\ub85c\uc6b4 X2 \ubc30\uc5f4\uc5d0 \uc800\uc7a5\n6. \uc6d0\ud56b\ucf54\ub529\ud55c y label\uc744 Y2\ub77c\ub294 \ubc30\uc5f4\uc5d0 \uc800\uc7a5\ud568 \n\n***\uc6d0\ub798\ub294 train_0.csv.gz \ud30c\uc77c\ub85c 3~6 \ubc88\uc9f8 \ub2e8\uacc4\ub97c \uc2e4\ud589\ud558\uc9c0\ub9cc Kaggle\uc5d0\uc11c \ub370\uc774\ud130\ub294 read-only data \uc774\ubbc0\ub85c horse.csv\ub97c \ubd80\ub4dd\uc774\ud558\uac8c \uc0ac\uc6a9***","6894e7ed":"### GPU memory allocation\uc744 \uc904\uc5ec\uc8fc\ub294 \ucf54\ub4dc","e1bd951a":"### Data Preprocessing","18d9bcb1":"### Test Data Prediciton ","4d4566f3":"![test](https:\/\/user-images.githubusercontent.com\/43398106\/68266393-5769d400-0092-11ea-85b9-bc7b466d6799.gif)","562e6b24":"### \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n1. \ub370\uc774\ud130 \uaddc\ubaa8 \ud655\uc778<br>\n2. 320\uac1c \uc8fc\uc5b4\uc9c4 csv \ud30c\uc77c shuffle\ud558\uc5ec 100\uac1c\ub85c \ub098\ub220 \ud569\uce68. \ud30c\uc77c \uaddc\ubaa8\uac00 \ub108\ubb34 \ud06c\uae30 \ub54c\ubb38\uc5d0 csv.gz \ud30c\uc77c\ub85c \uc555\ucd95\ud574\uc11c \ud65c\uc6a9 <br>\n3. PIL \ub77c\uc774\ube0c\ub7ec\ub9ac \ud65c\uc6a9\ud574 64*64 \uc774\ubbf8\uc9c0\ub85c convert \ud558\uc5ec \uadf8\ub9bc\uc73c\ub85c \ub098\ud0c0\ub0c4.<br>\n4. one_hot_coding \uae30\ubc95 \uc774\uc6a9: 324\uac1c\uc758 y_label\uc744 np.eye(324)\ub97c \uc0ac\uc6a9\ud574\uc11c \uc6d0-\ud56b\uc778\ucf54\ub529\ud568. <br>\n","33282104":"### Easy Data Preprocessing\n--by Joo Kyung Song, <br>\n--Submit to \"Programmers, Winter Coding\" \n","332a717d":"### CNN \uad6c\ud604 -- keras \ub77c\uc774\ube0c\ub7ec\ub9ac \uc0ac\uc6a9 \n\n\ub124\ud2b8\uc6cc\ud06c \uad6c\uc131\uc740 \uc544\ub798\uc640 \uac19\uc74c <br>**** conv - relu - conv - relu - pool - <br> conv - relu - conv - relu - pool - <br> conv - relu - conv - relu - pool - <br>affine - relu - dropout - affine - dropout - softmax****<br>\n\noptimizer \uae30\ubc95\uc740 adamOptimizer \uc0ac\uc6a9, \uc624\ucc28\uacc4\uc0b0\ubc95\uc740 cross-entropy \uc0ac\uc6a9\ud568. \n","de28f937":"### test_simplified \uadf8\ub9bc\uc73c\ub85c \ud655\uc778\ud558\uae30","b68493d7":"1. ### Data Shuffle + Compression\n***\"beluga\" \ub2d8\uc758 Shuffle Csv\ub97c \ucc38\uace0\ud558\uc5ec \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4***\n\n320\uac1c\uc758 label\uc744 \uac00\uc9c4 {}.csv \ud30c\uc77c\uc744 100\uac1c\ub85c \ub098\ub204\uc5b4\uc11c \uc11c\ub85c shuffle. \n\ucc98\uc74c\uc5d0\ub294 sklearn library\uc758 shuffle\ud568\uc218\ub97c \uc4f0\ub824\uace0 \ud588\uc9c0\ub9cc 14GB\uc758 \ub370\uc774\ud130\ub97c \uc804\ubd80 \ub2f4\uc744 \ubc30\uc5f4\uc774 \ud544\uc694\ud588\uc74c.\n-> \uc544\uc608 \ub370\uc774\ud130 \uc21c\uc11c\ub300\ub85c \uac01 chunk_size\ub9cc\ud07c \ubf51\uc544\uc11c \ub530\ub85c csv\ud30c\uc77c\uc744 \ub9cc\ub4dc\ub294 \uac83\uc744 \uc120\ud0dd\ud568\n\n***Data Shuffle\uacfc Compression\uc740 Kaggle \ub0b4 RAM \uba54\ubaa8\ub9ac \ud560\ub2f9 \ubb38\uc81c \ub54c\ubb38\uc5d0 \uac01\uc8fc \ucc98\ub9ac\ud568***\n","d7e867fc":"### Required Libraries"}}