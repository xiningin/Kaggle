{"cell_type":{"9a71748b":"code","1a7dcc0e":"code","218dc854":"code","cb436c02":"code","1d3f8fd4":"code","8a688705":"code","5b3cc506":"code","998529b9":"code","511ab6f5":"code","592d846e":"code","65a82434":"code","3b1e14a2":"code","b6037bc6":"code","f951b6d5":"code","015a5146":"code","5d8ccda2":"code","eb9bc17a":"code","3e7b10c5":"markdown","150df2fb":"markdown","d4171e3b":"markdown","f811c9f5":"markdown","2a38d4e3":"markdown","bbb6bc25":"markdown","b707ff30":"markdown","3ca5dc54":"markdown","8567ddc8":"markdown","da82d196":"markdown","fcce5e38":"markdown","5dc8d291":"markdown","dd4af51b":"markdown","8bb7655c":"markdown","889ecbf0":"markdown","c1d2ac45":"markdown","1faf5f7c":"markdown","30f9fa31":"markdown","e2f6c189":"markdown","0bea6851":"markdown","7d8946b1":"markdown"},"source":{"9a71748b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras import models \nimport IPython.display\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a7dcc0e":"content='..\/input\/nstimgs\/eiffel-tower.jpg'\nstyle='..\/input\/nstimgs\/sn.jpg'\nstyle2='..\/input\/nstimgs\/716tsQsStCL._SL1000_.jpg'","218dc854":"vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\nvgg.trainable = False\nprint(vgg.summary())","cb436c02":"content_layers = ['block5_conv2'] \nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]","1d3f8fd4":"def visualize_layer_outputs(content_layers,style_layers,img_path):\n    max_dim = 512\n    arr=Image.open(img_path)\n    max_size = max(arr.size)\n    scale = max_dim\/max_size\n    arr = arr.resize((round(arr.size[0]*scale), round(arr.size[1]*scale)), Image.ANTIALIAS)\n    arr=np.asarray(arr)\n    arr=np.expand_dims(arr, axis=0)\n    arr=tf.keras.applications.vgg19.preprocess_input(arr)\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n    vgg.trainable = False\n    layer_outputs=[vgg.get_layer(l).output for l in style_layers+content_layers]\n    model=tf.keras.Model(vgg.input,layer_outputs)\n    outputs=model(arr)\n    return outputs","8a688705":"a=visualize_layer_outputs(content_layers,style_layers,content)\nfor j in range(0,5):\n    f,ax=plt.subplots(2,5,figsize=(30,8))\n    ax=ax.flatten()\n    for i in range(0,10):\n        ax[i].imshow(a[j][0,:,:,i])\n    plt.suptitle(style_layers[j],size=15)\n    plt.show()","5b3cc506":"plt.imshow(a[4][0,:,:,12])\nplt.show()\nplt.imshow(a[4][0,:,:,4])\nplt.show()","998529b9":"#Initializing model layers for style transfer\nclass style_transfer():\n    def __init__(self,style_layers,content_layers):\n        self.model=self.vgg_get_layers(style_layers+content_layers)\n        self.num_style_layers=len(style_layers)\n        self.opt=tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n        self.imgs=[]\n        self.style_target=[]\n        self.best_img=None\n  \n    def vgg_get_layers(self,layers): #Function to define the style transfer model\n        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n        vgg.trainable = False\n        layer_outputs=[vgg.get_layer(l).output for l in layers]\n        tr_model=tf.keras.Model(vgg.input,layer_outputs)\n        return tr_model\n\n    def img_to_arr(self,img_path):\n        max_dim = 512\n        arr=Image.open(img_path)\n        max_size = max(arr.size)\n        scale = max_dim\/max_size\n        arr = arr.resize((round(arr.size[0]*scale), round(arr.size[1]*scale)), Image.ANTIALIAS)\n        arr=np.asarray(arr)\n        arr=np.expand_dims(arr, axis=0)\n        arr=tf.keras.applications.vgg19.preprocess_input(arr)\n        return arr\n    \n    def gram_matrix(self,mat):\n        n=int(mat.shape[-1])\n        p=tf.reshape(mat,[-1,n])\n        nn = tf.shape(p)[0]\n        return tf.matmul(tf.transpose(p),p)\/ tf.cast(nn, tf.float32)\n\n    def content_loss(self,base):\n        return 0.5*tf.reduce_mean(tf.square(self.content_target[0]-base))\n\n    def style_loss(self,target,base):\n        h,w,c=base.get_shape().as_list()\n        gram_base=self.gram_matrix(base)\n        return tf.reduce_mean(tf.square(gram_base-target))#\/(4*(c**2)*(w*h)**2)\n\n    def transfer_loss(self,base):\n        s_loss=0\n        c_loss=0\n        outp=self.model(base)\n        style_base=outp[:self.num_style_layers]\n        content_base=outp[self.num_style_layers:]\n        for i in range(0,self.count_styles):\n            u=self.num_style_layers*i+4\n            l=self.num_style_layers*i\n            for j,(target,base) in enumerate(zip(self.style_gram_matrices[l:u+1],style_base)):\n                if self.style_layer_weights==None:\n                      s_loss+=self.style_img_wts[i]*(1\/self.num_style_layers)*self.style_loss(target,base[0]) #Combined loss from all style layers\n                else:\n                      s_loss+=self.style_img_wts[i]*self.style_layer_weights[j]*self.style_loss(target,base[0])\n        for base in (content_base):\n                c_loss+=self.content_loss(base[0])      \n        y_var=tf.abs(base[:,:-1,:,:]-base[:,1:,:,:])\n        x_var=tf.abs(base[:,:,:-1,:]-base[:,:,1:,:])\n        t_loss=self.style_loss_wt*s_loss + self.content_loss_wt*c_loss + tf.reduce_sum(x_var)+tf.reduce_sum(y_var)\n        return self.style_loss_wt*s_loss,self.content_loss_wt*c_loss,t_loss\n\n    def img_plot(self,x,iter,s_loss,c_loss,t_loss):\n        img=x.numpy()\n        img[:, :, 0] += 103.939\n        img[:, :, 1] += 116.779\n        img[:, :, 2] += 123.68\n        img = img[:, :, ::-1]\n        img = np.clip(img, 0, 255).astype('uint8')\n        f,ax=plt.subplots(figsize=(12,10))\n        IPython.display.clear_output(wait=True)\n        ax.matshow(img)\n        ax.set_title(f'Iteration={iter} \\n Style_loss={s_loss},Content_loss={c_loss},Total_loss={t_loss}')\n        plt.show()\n        return\n\n    def back_prop(self,base,iter):\n        vgg_std_pix = np.array([103.939, 116.779, 123.68])\n        pix_min = -vgg_std_pix\n        pix_max = 255 - vgg_std_pix \n        best_loss= float('inf')\n        for i in range(0,iter):\n            with tf.GradientTape() as g:\n                loss=self.transfer_loss(base)\n                grad=g.gradient(loss[2],base)\n                self.opt.apply_gradients([(grad,base)])\n                base.assign(tf.clip_by_value(base,pix_min,pix_max))\n\n            if loss[2]<best_loss:\n                best_loss=loss[2]\n                self.best_img=base[0]\n            self.img_plot(base[0],i,loss[0],loss[1],loss[2])\n\n    def __call__(self,content_img,style_img,content_wt,style_wt,iter,style_img_wts=None,style_layer_weights=None):\n        base_img=tf.Variable(self.img_to_arr(content_img))\n        self.style_loss_wt=style_wt\n        self.content_loss_wt=content_wt\n        self.content_target=self.model(self.img_to_arr(content_img))[self.num_style_layers:] #content target\n        self.count_styles=len(style_img)\n        self.style_layer_weights=style_layer_weights\n        if style_img_wts==None:\n            self.style_img_wts=[]\n            for i in range(0,len(style_img)):\n                self.style_img_wts.append(1\/len(style_img))\n        else:\n            self.style_img_wts=style_img_wts\n        for i in range(0,len(style_img)):\n            self.style_target.append(self.model(self.img_to_arr(style_img[i]))[:self.num_style_layers])#style target\n        self.style_gram_matrices=[]\n        for i in range(0,len(style_img)):\n            for style in self.style_target[i]:\n                self.style_gram_matrices.append(self.gram_matrix(style))\n        self.back_prop(base_img,iter)","511ab6f5":"def img_to_arr(img_path):\n        max_dim = 512\n        arr=Image.open(img_path)\n        max_size = max(arr.size)\n        scale = max_dim\/max_size\n        arr = arr.resize((round(arr.size[0]*scale), round(arr.size[1]*scale)), Image.ANTIALIAS)\n        arr=np.asarray(arr)\n        return arr","592d846e":"f,ax=plt.subplots(1,3,figsize=(20,8))\nax[0].imshow(img_to_arr(content))\nax[0].set_title('Content Image',size=15)\nax[1].imshow(img_to_arr(style))\nax[1].set_title('Style Image 1',size=15)\nax[2].imshow(img_to_arr(style2))\nax[2].set_title('Style Image 2',size=15)\nplt.show()","65a82434":"content_layers = ['block5_conv2'] \nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\nnst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\nnst(content,[style],100000,0.005,500)","3b1e14a2":"content_layers = ['block5_conv2'] \nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\nnst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\nnst(content,[style],100000,0.1,500)","b6037bc6":"content_layers = ['block5_conv2'] \nstyle_layers = ['block1_conv1'\n               ]\nnst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\nnst(content,[style],100000,0.005,500)","f951b6d5":"content_layers = ['block1_conv1'] \nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\nnst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\nnst(content,[style],100000,0.005,500)","015a5146":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\ninst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\ninst(content,[style],100000,0.005,500,style_img_wts=None,style_layer_weights=[0.4,0.3,0.2,0.05,0.05])","5d8ccda2":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\ninst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\ninst(content,[style],100000,0.005,500,style_img_wts=None,style_layer_weights=[0.05,0.05,0.2,0.3,0.4])","eb9bc17a":"content_layers = ['block5_conv2'] \nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1'\n               ]\ninst=style_transfer(style_layers,content_layers)\n#(content img path,style img path, content loss wt,style loss wt, iterations)\ninst(content,[style,style2],100000,0.005,500)","3e7b10c5":"This is the base case. We'll compare it with other results.","150df2fb":"Using just block_1_conv_1 for style loss does not result in a captivating recomposition. Perhaps, as syle consists of a variety of shades and spatial attributes, filters from multiple layers are necessary.","d4171e3b":"The Vgg-19 network has the following architecture:","f811c9f5":"# Experimenting with different hyperparameters","2a38d4e3":"Block1_conv1 captures global features from the target content image. This probably interferes with the global features from the style image,resulting in a horrid style transfer. This encourages the use of deeper layers to generate content information for NST. They focus solely on local details about prominent objects in the content image, enabling a smooth replication of the style image's theme.","bbb6bc25":"# Style Transfer with content loss wt=100000 and style loss=0.1","b707ff30":"# CNNs for style transfer\n\nTo carry style transfer, one needs to separate the contents and styles of the two images involved in the operation.This is done with the help of pre-trained CNNs. Each layer in a CNN, as you might know, learns to identify certain features of an image. The initial layers learn the high-level features, essentially the style, while the later layers learn more intricate details. Let's use the Vgg-19 network to visualize this.","3ca5dc54":"As the image progresses through the network, it becomes difficult to discern the outputs. The initial layers seem to identify the overall context of the image (global attributes). These layers appear to recognize edges and colors globally, and hence their outputs closely replicate the original image.Filter outputs in deeper layers, especially block_5_conv_2, are focused on specific locations and pull information about the shapes of different objects. The two filters below, as examples, are interested in the finer details of the Eiffel tower. In a classification problem, perhaps, these local features will help CNNs distinguish the Eiffel tower from any other building of a similar outline like electric towers.","8567ddc8":"# Stlye transfer using style from multiple images\nTo transfer style from multiple images, we'll also assign weights (fraction of 1 alloted to each image) to each style image to quantify the amount of information we want to retain from them. (By default, the weights are equal)","da82d196":"As expected, increasing the weight nudges the algorithm to capture more style. Hence, the recomposition has a better combination of blue and yellow.","fcce5e38":"We'll use the following images for carrying neural style transfer.","5dc8d291":"# Style transfer using multiple content layers","dd4af51b":"# Style Transfer with content loss wt=100000 and style loss=0.005","8bb7655c":"# NEURAL STYLE TRANSFER CLASS","889ecbf0":"Any image can be divided into two constituents, content and style. Content refers to the intricacies of different objects, while style is the overall theme. You can also interpret the two as local and global entities of an image. For instance, in the image below, the detailed design of the Eiffel tower is the content whereas, the background shades\u200a-\u200ablue sky and green surroundings\u200a-\u200aconstitute the style (theme).\n![image.png](attachment:c58bd4a0-c60d-493d-b491-8e4f33fabf04.png)\n\nThe idea of Neural Style Transfer(NST) is to take one image and recompose its content in the style of some other image.\n![image.png](attachment:cf062a7e-2d8a-47ba-b3d4-f3c5af787131.png)","c1d2ac45":"The above recomposition is not vastly different from the base case,however, it seems to me that the latter has an overall light shade. This is due to higher weight assigned to block1_conv_1, which according to the single style layer case, regenerates a white tone from the Starry night painting.","1faf5f7c":"It has a better color combination compared to the above case. Giving more weight to deeper layers seems to enhance style transfer. Gram matrices from these layers are perhaps computing the correlation between more dominant global features from Starry Night.","30f9fa31":"# Manually assigning weights to style layers [0.4,0.3,0.2,0.05,0.05]","e2f6c189":"# Manually assigning weights to style layers [0.05,0.05,0.2,0.3,0.4]","0bea6851":"# Style transfer with single layer for style loss","7d8946b1":"Each layer has at least 64 filters to learn image features. We'll use the output of the first 10 filters to see what different layers learn."}}