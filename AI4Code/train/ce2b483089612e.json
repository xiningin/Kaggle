{"cell_type":{"8688e1f3":"code","fa22e241":"code","d5c72a15":"code","67c0bc0e":"code","b8c6065b":"code","a30100d8":"code","4d37ad6a":"code","44d5cb0e":"code","7a0b5af7":"code","3e7e5c24":"code","23dd80af":"code","8b5e6a19":"code","f70b2831":"code","5ecc7817":"code","43755a96":"code","e3d4cf39":"code","001b6aa7":"code","c42d4cea":"code","4927feba":"code","7b7faf89":"code","b97b30f1":"code","3657f2d1":"code","49e97ad7":"code","b12f4b1d":"code","ad299f7a":"code","58b4a107":"code","ee6c3a7b":"code","257197af":"code","e156ef9f":"code","b96125e2":"code","c4336113":"code","cea89661":"code","8ac8f0af":"code","c54c8b88":"code","11205a06":"code","153b73e5":"code","50d09305":"code","bea09fb4":"code","b881262a":"code","2efd97d6":"code","a6e46c50":"code","13ef4317":"code","f188c0f5":"code","a65065d7":"code","13f94e09":"code","7dd64ed9":"code","2ef7302c":"code","1be358d3":"markdown","d41d806a":"markdown","184a2759":"markdown","c2315948":"markdown","437385b3":"markdown","3ea0d577":"markdown","ced6440f":"markdown","aa4121db":"markdown","855cadcb":"markdown","8438497e":"markdown","36a9ac8d":"markdown","9d939af6":"markdown","3b314d40":"markdown","60c9ca20":"markdown","1747e391":"markdown","47538315":"markdown","11e1f029":"markdown"},"source":{"8688e1f3":"!dpkg -i ..\/input\/python3gdcm\/build_1-1_amd64.deb\n!apt-get install -f","fa22e241":"!cp \/usr\/local\/lib\/gdcm.py \/opt\/conda\/lib\/python3.7\/site-packages\/.\n!cp \/usr\/local\/lib\/gdcmswig.py \/opt\/conda\/lib\/python3.7\/site-packages\/.\n!cp \/usr\/local\/lib\/_gdcmswig.so \/opt\/conda\/lib\/python3.7\/site-packages\/.\n!cp \/usr\/local\/lib\/libgdcm* \/opt\/conda\/lib\/python3.7\/site-packages\/.\n!ldconfig","d5c72a15":"import os\nimport cv2\nimport sys\nimport random\nimport pickle\nimport glob\nimport gc\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport gdcm\nimport pydicom\n\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import zoom\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\n\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","67c0bc0e":"sys.path.append('..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch-master\/')\nsys.path.append('..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/')\nsys.path.append('..\/input\/segmentation-models-pytorch\/')\nimport segmentation_models_pytorch as smp","b8c6065b":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)#set all gpus seed\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False#if input data type and channels' changes arent' large use it improve train efficient\n        torch.backends.cudnn.enabled = True\n    \nseed_everything(42)","a30100d8":"dicom_root_path = '..\/input\/osic-pulmonary-fibrosis-progression\/train\/'\nPatients_id = os.listdir(dicom_root_path)\nn_dicom_dict = {\"Patient\":[],\"n_dicom\":[],\"list_dicom\":[]}\n\nfor Patient_id in Patients_id:\n    dicom_id_path = glob.glob(dicom_root_path + Patient_id + \"\/*\")\n    n_dicom_dict[\"n_dicom\"].append(len(dicom_id_path))\n    n_dicom_dict[\"Patient\"].append(Patient_id)\n    list_dicom_id = sorted([int(i.split(\"\/\")[-1][:-4]) for i in dicom_id_path])\n    n_dicom_dict[\"list_dicom\"].append(list_dicom_id)\n\ndicom_pd = pd.DataFrame(n_dicom_dict)\ndicom_pd.head()","4d37ad6a":"print(f\"min dicom number is {min(dicom_pd['n_dicom'])}\\n\\\nmax dicom number is {max(dicom_pd['n_dicom'])}\")\n\nplt.hist(dicom_pd['n_dicom'], bins=20)\nplt.title('Number of dicom per patient');","44d5cb0e":"dicom_pd['height'],dicom_pd['width'] = -1,-1\nfor Patient_id in Patients_id:\n    dicom_id_path = glob.glob(dicom_root_path + Patient_id + \"\/*\")\n    for patient_dicom_id_path in dicom_id_path:\n        dicom = pydicom.dcmread(patient_dicom_id_path)\n        dicom_pd.loc[dicom_pd.Patient==Patient_id,'height'] = dicom.Rows\n        dicom_pd.loc[dicom_pd.Patient==Patient_id,'width'] = dicom.Columns\n        break","7a0b5af7":"reshape_dicom_pd = dicom_pd.loc[(dicom_pd.height!=512) | (dicom_pd.width!=512),:]\nreshape_dicom_pd = reshape_dicom_pd.reset_index(drop=True)\nreshape_dicom_pd.head()","3e7e5c24":"f, ax = plt.subplots(len(reshape_dicom_pd.head()),2, figsize=(15, 18))\nfor idx,patient_id in enumerate(reshape_dicom_pd.head()['Patient']):\n    paths = random.sample(glob.glob(dicom_root_path + patient_id + \"\/*\"),2)\n    dicom1 = pydicom.dcmread(paths[0])\n    dicom2 = pydicom.dcmread(paths[1])\n    ax[idx,0].set_title(f\"{patient_id}-{paths[0].split('\/')[-1][:-4]}-{reshape_dicom_pd.loc[idx,'height']}-{reshape_dicom_pd.loc[idx,'width']}\")\n    ax[idx,0].imshow(dicom1.pixel_array, cmap=plt.cm.bone)\n    ax[idx,1].set_title(f\"patient id is {patient_id}-{paths[1].split('\/')[-1][:-4]}-{reshape_dicom_pd.loc[idx,'height']}-{reshape_dicom_pd.loc[idx,'width']}\")\n    ax[idx,1].imshow(dicom2.pixel_array, cmap=plt.cm.bone)\nplt.show()","23dd80af":"crop_id = ['ID00240637202264138860065','ID00122637202216437668965','ID00086637202203494931510',\n            'ID00419637202311204720264','ID00014637202177757139317','ID00094637202205333947361',\n            'ID00067637202189903532242',]\nreshape_dicom_pd['resize_type'] = 'resize'\nreshape_dicom_pd.loc[reshape_dicom_pd.Patient.isin(crop_id),'resize_type'] = 'crop'","8b5e6a19":"dicom_pd['resize_type'] = 'no'\nfor idx,i in enumerate(reshape_dicom_pd['Patient']):\n    dicom_pd.loc[dicom_pd.Patient==i,'resize_type'] = reshape_dicom_pd.loc[idx,'resize_type']\ndicom_pd.head()","f70b2831":"train_pd = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ntemp_pd = pd.DataFrame(columns=train_pd.columns)\nfor i in range(len(dicom_pd)):\n    patient_pd = train_pd[train_pd.Patient==dicom_pd.iloc[i].Patient]\n    zeroweek = patient_pd['Weeks'].min()\n    #if sum(patient_pd.Weeks==zeroweek)>1:\n    #    print(pd.unique(patient_pd.Patient))\n    temp_pd = temp_pd.append(patient_pd[patient_pd.Weeks==zeroweek].iloc[0])\ndicom_pd = pd.merge(dicom_pd, temp_pd, on=['Patient'])\ndicom_pd.head()","5ecc7817":"dicom_pd[dicom_pd.resize_type!='no'].head()","43755a96":"def load_scan(path,resize_type='no'):\n    \"\"\"\n    Loads scans from a folder and into a list.\n    \n    Parameters: path (Folder path)\n    \n    Returns: slices (List of slices)\n    \"\"\"\n    slices = [pydicom.read_file(path + '\/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.InstanceNumber))\n    \n    try:\n        slice_thickness = abs(slices[-1].ImagePositionPatient[2] - slices[0].ImagePositionPatient[2])\/(len(slices))\n    except:\n        try:\n            slice_thickness = abs(slices[-1].SliceLocation - slices[0].SliceLocation)\/(len(slices))\n        except:\n            slice_thickness = slices[0].SliceThickness\n        \n    for s in slices:\n        s.SliceThickness = slice_thickness\n        if resize_type == 'resize':\n            s.PixelSpacing = s.PixelSpacing*(s.Rows\/512)  \n    return slices","e3d4cf39":"def transform_to_hu(slices):\n    \"\"\"\n    transform dicom.pixel_array to Hounsfield.\n    Parameters: list dicoms\n    Returns:numpy Hounsfield\n    \"\"\"\n    \n    images = np.stack([file.pixel_array for file in slices])\n    images = images.astype(np.int16)\n\n    # convert ouside pixel-values to air:\n    # I'm using <= -1000 to be sure that other defaults are captured as well\n    images[images <= -1000] = 0\n    \n    # convert to HU\n    for n in range(len(slices)):\n        \n        intercept = slices[n].RescaleIntercept\n        slope = slices[n].RescaleSlope\n        \n        if slope != 1:\n            images[n] = slope * images[n].astype(np.float64)\n            images[n] = images[n].astype(np.int16)\n            \n        images[n] += np.int16(intercept)\n    \n    return np.array(images, dtype=np.int16)","001b6aa7":"def crop_image(img: np.ndarray):\n    edge_pixel_value = img[0, 0]\n    mask = img != edge_pixel_value\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef resize_image(img: np.ndarray,reshape=(512,512)):\n    img = cv2.resize(img,(512,512))\n    return img\n\ndef preprocess_img(img,resize_type):\n    if resize_type == 'resize':\n        img = [resize_image(im) for im in img]\n    if resize_type == 'crop':\n        img = [crop_image(im) for im in img]\n        \n    return np.array(img, dtype=np.int64)","c42d4cea":"class Test_Generate(Dataset):\n    def __init__(self,imgs_dicom):\n        self.imgs_dicom = imgs_dicom\n    def __getitem__(self,index):\n        metainf = self.imgs_dicom[index]\n        slice_img = metainf.pixel_array\n        slice_img = (slice_img-slice_img.min())\/(slice_img.max()-slice_img.min())\n        slice_img = (slice_img*255).astype(np.uint8)\n            \n        if metainf.Rows!=512 or metainf.Columns!=512:\n            slice_img = cv2.resize(slice_img,(512,512))\n        \n        slice_img = slice_img[None,:,:]\n        slice_img = (slice_img\/255).astype(np.float32)\n        return slice_img\n        \n    def __len__(self):\n        return len(self.imgs_dicom)\n","4927feba":"device =  torch.device('cuda:0')\nmodel = smp.Unet('densenet121', classes=1, in_channels=1,activation='sigmoid',encoder_weights=None).to(device)\nmodel.load_state_dict(torch.load('..\/input\/2020osic\/best_lung_Unet_densenet121_my.pth'))\n\ndef Unet_mask(model: nn.Module,input_data: DataLoader):\n    model.eval()\n    outs = []\n    for idx, sample in enumerate(test_loader):\n        image = sample\n        image = image.to(device)\n        with torch.no_grad():\n            out = model(image)\n        out = out.cpu().data.numpy()\n        out = np.where(out>0.5,1,0)\n        out = np.squeeze(out,axis=1)\n        outs.append(out)\n\n    outs = np.concatenate(outs)\n    return outs","7b7faf89":"f, ax = plt.subplots(4,2, figsize=(14, 14))\n\nfor i in range(4):\n    path = os.path.join(dicom_root_path,dicom_pd.iloc[i].Patient)\n    patient_scans = load_scan(path)\n    \n    test_db = Test_Generate(patient_scans)\n    test_loader = DataLoader(test_db, batch_size=8, shuffle=False, num_workers=4)\n    \n    masks = Unet_mask(model,test_loader)\n    \n    \n    num_slices = len(masks)\n    patient_image = test_db[num_slices\/\/2][0]\n    patient_mask = masks[num_slices\/\/2]\n    \n    ax[i,0].set_title(f\"{dicom_pd.iloc[i].Patient}-{dicom_pd.iloc[i].FVC}\")\n    ax[i,0].imshow(patient_image,cmap='gray')\n    ax[i,1].imshow(patient_mask)\n    \nplt.show()\nplt.close()","b97b30f1":"thresh = [-1000,0]\nf, ax = plt.subplots(2,2, figsize=(18, 18))\nsampler = random.sample(range(len(dicom_pd)),4)\n\nfor i in range(4):\n    \n    path = os.path.join(dicom_root_path,dicom_pd.iloc[sampler[i]].Patient)\n    \n    patient_scans = load_scan(path)\n    \n    test_db = Test_Generate(patient_scans)\n    test_loader = DataLoader(test_db, batch_size=8, shuffle=False, num_workers=4)\n    \n    masks = Unet_mask(model,test_loader)\n    \n    patient_images = transform_to_hu(patient_scans)\n    patient_images = preprocess_img(patient_images,'resize')\n    \n    num_slices = len(patient_images)\n   \n    patient_images = masks*patient_images\n    patient_images_nonzero = patient_images[np.nonzero(patient_images)]\n   \n    \n    s_pixel = patient_images_nonzero.flatten()\n    s_pixel = s_pixel[np.where((s_pixel>thresh[0])&(s_pixel<thresh[1]))]\n    \n    ax[i\/\/2,i%2].set_title(f\"{dicom_pd.iloc[sampler[i]].Patient}-{dicom_pd.iloc[sampler[i]].FVC}\")\n    ax[i\/\/2,i%2].hist(s_pixel, bins=20)\n\nplt.show()","3657f2d1":"def caculate_lung_volume(patient_scans,patient_masks):\n    \"\"\"\n    caculate volume of lung from mask\n    Parameters: list dicom scans,list patient CT Mask\n    Returns: volume cm\u00b3\u3000(float)\n    \"\"\"\n    lung_volume = 0\n    for i in range(len(patient_masks)):\n        \n        pixel_spacing = patient_scans[i].PixelSpacing\n        slice_thickness = patient_scans[i].SliceThickness\n        lung_volume += np.count_nonzero(patient_masks[i])*pixel_spacing[0]*pixel_spacing[1]*slice_thickness\n        \n    return lung_volume*0.001","49e97ad7":"def caculate_histgram_statistical(patient_images,patient_masks,thresh = [-600,-250]):\n    \"\"\"\n    caculate hisgram kurthosis of lung hounsfield\n    Parameters: list patient CT image 512*512,thresh divide lung\n    Returns: histgram statistical characteristic(Mean,Skew,Kurthosis)\n    \"\"\"\n    statistical_characteristic = dict(Mean=0,Median=0,Skew=0,Kurthosis=0,HAA=0,midMean=0,\n                                      midSkew=0,midKurthosis=0,midMedian=0,midHAA=0)\n    num_slices = len(patient_images)\n   \n    patient_images = patient_masks*patient_images\n    patient_images_nonzero = patient_images[np.nonzero(patient_images)]\n    s_pixel = patient_images_nonzero.flatten()\n    haa_pixel = s_pixel[np.where((s_pixel>-1000)&(s_pixel<0))]\n    \n    #mid** is the largest CT slice of lung mask area\n    mid_index = np.argsort(np.sum(patient_masks,axis=(1,2)))[-1]\n    \n    mid_image = patient_images[mid_index]\n    mid_images_nonzero = mid_image[np.nonzero(mid_image)]\n    mid_pixel = mid_images_nonzero.flatten()\n    midhaa_pixel = mid_pixel[np.where((mid_pixel>thresh[0])&(mid_pixel<thresh[1]))]\n    \n    \n    statistical_characteristic['Mean'] = np.mean(s_pixel)\n    statistical_characteristic['Median'] = np.median(s_pixel)\n    statistical_characteristic['Skew'] = skew(s_pixel)\n    statistical_characteristic['Kurthosis'] = kurtosis(s_pixel)\n    statistical_characteristic['HAA'] = len(haa_pixel)\/len(s_pixel)\n    \n    statistical_characteristic['midMean'] = np.mean(mid_pixel)\n    statistical_characteristic['midMedian'] = np.median(mid_pixel)\n    statistical_characteristic['midSkew'] = skew(mid_pixel)\n    statistical_characteristic['midKurthosis'] = kurtosis(mid_pixel)\n    statistical_characteristic['midHAA'] = len(midhaa_pixel)\/len(mid_pixel)\n    \n    #the ratio of different Hounsfield value \n    for r in range(0,1000,100):\n        area_pixel = s_pixel[np.where((s_pixel>-r-100)&(s_pixel<-r))]\n        statistical_characteristic[f'pro_{r}'] = len(area_pixel)\/len(s_pixel)\n    return statistical_characteristic","b12f4b1d":"lung_stat_pd = pd.DataFrame(columns=['Patient','Volume','Mean','Median','Skew','Kurthosis','HAA',\n                                     'midMean','midMedian','midSkew','midKurthosis','midHAA',\n                                    'pro_0','pro_100','pro_200','pro_300','pro_400','pro_500',\n                                     'pro_600','pro_700','pro_800','pro_900'])\nhaa_thresh = [-600,-250]\nfor i in tqdm(range(len(dicom_pd))):\n    path = os.path.join(dicom_root_path,dicom_pd.iloc[i].Patient)\n    lung_stat_pd.loc[i,'Patient'] = dicom_pd.iloc[i].Patient\n    patient_scans = load_scan(path)\n    \n    test_db = Test_Generate(patient_scans)#,dicom_pd.iloc[i].resize_type)\n    test_loader = DataLoader(test_db, batch_size=8, shuffle=False, num_workers=4)\n    masks = Unet_mask(model,test_loader)\n    \n    \n    patient_images = transform_to_hu(patient_scans)\n    if patient_images[0].shape!=(512,512):\n        patient_images = preprocess_img(patient_images,'resize')\n    \n    lung_stat_pd.loc[i,'Volume'] = caculate_lung_volume(patient_scans,masks) \n    \n    statistical_characteristic = caculate_histgram_statistical(patient_images,masks,haa_thresh)\n    \n    lung_stat_pd.loc[i,'Mean'] = statistical_characteristic['Mean']\n    lung_stat_pd.loc[i,'Median'] = statistical_characteristic['Median']\n    lung_stat_pd.loc[i,'Skew'] = statistical_characteristic['Skew']\n    lung_stat_pd.loc[i,'Kurthosis'] = statistical_characteristic['Kurthosis']\n    lung_stat_pd.loc[i,'HAA'] = statistical_characteristic['HAA']\n    \n    lung_stat_pd.loc[i,'midMean'] = statistical_characteristic['midMean']\n    lung_stat_pd.loc[i,'midMedian'] = statistical_characteristic['midMedian']\n    lung_stat_pd.loc[i,'midSkew'] = statistical_characteristic['midSkew']\n    lung_stat_pd.loc[i,'midKurthosis'] = statistical_characteristic['midKurthosis']\n    lung_stat_pd.loc[i,'midHAA'] = statistical_characteristic['midHAA']\n    \n    lung_stat_pd.loc[i,'pro_0'] = statistical_characteristic['pro_0']\n    lung_stat_pd.loc[i,'pro_100'] = statistical_characteristic['pro_100']\n    lung_stat_pd.loc[i,'pro_200'] = statistical_characteristic['pro_200']\n    lung_stat_pd.loc[i,'pro_300'] = statistical_characteristic['pro_300']\n    lung_stat_pd.loc[i,'pro_400'] = statistical_characteristic['pro_400']\n    lung_stat_pd.loc[i,'pro_500'] = statistical_characteristic['pro_500']\n    lung_stat_pd.loc[i,'pro_600'] = statistical_characteristic['pro_600']\n    lung_stat_pd.loc[i,'pro_700'] = statistical_characteristic['pro_700']\n    lung_stat_pd.loc[i,'pro_800'] = statistical_characteristic['pro_800']\n    lung_stat_pd.loc[i,'pro_900'] = statistical_characteristic['pro_900']\n    \nlung_stat_pd.head()","ad299f7a":"lung_stat_pd.to_csv('.\/CT_21feature.csv',index=False)","58b4a107":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"\ndevice = torch.device('cuda')\n\ntr = pd.read_csv(f\"{ROOT}\/train.csv\")\ntr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n#feature_ct = pd.read_csv('..\/input\/2020osic\/CT_21feature.csv')\nfeature_ct = lung_stat_pd.copy()\ntr = tr.merge(feature_ct, on='Patient')\n\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")","ee6c3a7b":"ct_root_path = '..\/input\/osic-pulmonary-fibrosis-progression\/test\/'\n\nlung_stat_pd = pd.DataFrame(columns=['Patient','Volume','Mean','Median','Skew','Kurthosis','HAA',\n                                     'midMean','midMedian','midSkew','midKurthosis','midHAA',\n                                    'pro_0','pro_100','pro_200','pro_300','pro_400','pro_500',\n                                     'pro_600','pro_700','pro_800','pro_900'])\nhaa_thresh = [-600,-250]\nfor i,p in enumerate(tqdm(pd.unique(chunk['Patient']))):\n    \n    lung_stat_pd.loc[i,'Patient'] = p    \n    patient_scans = load_scan(ct_root_path + p)\n    test_db = Test_Generate(patient_scans)\n    test_loader = DataLoader(test_db, batch_size=8, shuffle=False, num_workers=4)\n    masks = Unet_mask(model,test_loader)\n    \n    patient_images = transform_to_hu(patient_scans)\n    if patient_images[0].shape!=(512,512):\n        patient_images = preprocess_img(patient_images,'resize')\n    \n    lung_stat_pd.loc[i,'Volume'] = caculate_lung_volume(patient_scans,masks)                           \n   \n    statistical_characteristic = caculate_histgram_statistical(patient_images,masks,haa_thresh)\n    \n    lung_stat_pd.loc[i,'Mean'] = statistical_characteristic['Mean']\n    lung_stat_pd.loc[i,'Median'] = statistical_characteristic['Median']\n    lung_stat_pd.loc[i,'Skew'] = statistical_characteristic['Skew']\n    lung_stat_pd.loc[i,'Kurthosis'] = statistical_characteristic['Kurthosis']\n    lung_stat_pd.loc[i,'HAA'] = statistical_characteristic['HAA']\n    \n    lung_stat_pd.loc[i,'midMean'] = statistical_characteristic['midMean']\n    lung_stat_pd.loc[i,'midMedian'] = statistical_characteristic['midMedian']\n    lung_stat_pd.loc[i,'midSkew'] = statistical_characteristic['midSkew']\n    lung_stat_pd.loc[i,'midKurthosis'] = statistical_characteristic['midKurthosis']\n    lung_stat_pd.loc[i,'midHAA'] = statistical_characteristic['midHAA']\n    \n    lung_stat_pd.loc[i,'pro_0'] = statistical_characteristic['pro_0']\n    lung_stat_pd.loc[i,'pro_100'] = statistical_characteristic['pro_100']\n    lung_stat_pd.loc[i,'pro_200'] = statistical_characteristic['pro_200']\n    lung_stat_pd.loc[i,'pro_300'] = statistical_characteristic['pro_300']\n    lung_stat_pd.loc[i,'pro_400'] = statistical_characteristic['pro_400']\n    lung_stat_pd.loc[i,'pro_500'] = statistical_characteristic['pro_500']\n    lung_stat_pd.loc[i,'pro_600'] = statistical_characteristic['pro_600']\n    lung_stat_pd.loc[i,'pro_700'] = statistical_characteristic['pro_700']\n    lung_stat_pd.loc[i,'pro_800'] = statistical_characteristic['pro_800']\n    lung_stat_pd.loc[i,'pro_900'] = statistical_characteristic['pro_900']\n\nlung_stat_pd.head()","257197af":"chunk = chunk.merge(lung_stat_pd)\n\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")\nsub.head()","e156ef9f":"tr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])","b96125e2":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n\nbase = (\n    data\n    .loc[data.Weeks == data.min_week][['Patient','FVC']]\n    .rename({'FVC': 'min_FVC'}, axis=1)\n    .groupby('Patient')\n    .first()\n    .reset_index()\n)\ndata = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base\n","c4336113":"data['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\n#data['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min())\ndata['week'] = data['base_week']# - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\n#data['percent'] = (data['min_Percent'] - data['min_Percent'].min() ) \/ ( data['min_Percent'].max() - data['min_Percent'].min())\n\ndata['volume'] = (data['Volume'] - data['Volume'].min() ) \/ ( data['Volume'].max() - data['Volume'].min())\ndata['mean'] = (data['Mean'] - data['Mean'].min()) \/ (data['Mean'].max() - data['Mean'].min())\ndata['skew'] = (data['Skew'] - data['Skew'].min())\/(data['Skew'].max() - data['Skew'].min())\ndata['median'] = (data['Median'] - data['Median'].min()) \/ (data['Median'].max() - data['Median'].min())\ndata['kurthosis'] = (data['Kurthosis'] - data['Kurthosis'].min())\/(data['Kurthosis'].max() - data['Kurthosis'].min())\ndata['haa'] = (data['HAA'] - data['HAA'].min())\/(data['HAA'].max() - data['HAA'].min())\n\n\ndata['midmedian'] = (data['midMedian'] - data['midMedian'].min() ) \/ ( data['midMedian'].max() - data['midMedian'].min())\ndata['midmean'] = (data['midMean'] - data['midMean'].min()) \/ (data['midMean'].max() - data['midMean'].min())\ndata['midskew'] = (data['midSkew'] - data['midSkew'].min())\/(data['midSkew'].max() - data['midSkew'].min())\ndata['midkurthosis'] = (data['midKurthosis'] - data['midKurthosis'].min())\/(data['midKurthosis'].max() - data['midKurthosis'].min())\ndata['midhaa'] = (data['midHAA'] - data['midHAA'].min())\/(data['midHAA'].max() - data['midHAA'].min())\n\ndata['pro_0'] = (data['pro_0']-data['pro_0'].min())\/(data['pro_0'].max() - data['pro_0'].min())\ndata['pro_100'] = (data['pro_100']-data['pro_100'].min())\/(data['pro_100'].max() - data['pro_100'].min())\ndata['pro_200'] = (data['pro_200']-data['pro_200'].min())\/(data['pro_200'].max() - data['pro_200'].min())\ndata['pro_300'] = (data['pro_300']-data['pro_300'].min())\/(data['pro_300'].max() - data['pro_300'].min())\ndata['pro_400'] = (data['pro_400']-data['pro_400'].min())\/(data['pro_400'].max() - data['pro_400'].min())\ndata['pro_500'] = (data['pro_500']-data['pro_500'].min())\/(data['pro_500'].max() - data['pro_500'].min())\ndata['pro_600'] = (data['pro_600']-data['pro_600'].min())\/(data['pro_600'].max() - data['pro_600'].min())\ndata['pro_700'] = (data['pro_700']-data['pro_700'].min())\/(data['pro_700'].max() - data['pro_700'].min())\ndata['pro_800'] = (data['pro_800']-data['pro_800'].min())\/(data['pro_800'].max() - data['pro_800'].min())\ndata['pro_900'] = (data['pro_900']-data['pro_900'].min())\/(data['pro_900'].max() - data['pro_900'].min())\n\ndata['res_fvc'] = data['min_FVC']-data['FVC']\n\n\ndata.loc[:,\"Sex\"] = pd.factorize(data.Sex)[0]\ndata.loc[:,\"SmokingStatus\"] = pd.factorize(data.SmokingStatus)[0]\ndata['Sex'] = (data['Sex'] - data['Sex'].min() ) \/ ( data['Sex'].max() - data['Sex'].min() )\ndata['SmokingStatus'] = (data['SmokingStatus'] - data['SmokingStatus'].min() ) \/ ( data['SmokingStatus'].max() - data['SmokingStatus'].min())","cea89661":"FE = ['SmokingStatus','Sex','age','week','volume','mean','skew','kurthosis','haa','median',\n     'midmean','midmedian','midskew','midkurthosis','midhaa',\n     'pro_0','pro_100','pro_200','pro_300','pro_400','pro_500',\n     'pro_600','pro_700','pro_800','pro_900']","8ac8f0af":"tr = data[data.WHERE=='train']\ntest = data[data.WHERE=='test']\nsub = data.loc[data.WHERE=='test']\ndel data\n\ntr.head()","c54c8b88":"NFOLD = 5\nkf = KFold(n_splits=NFOLD)\npd_patient = pd.DataFrame({\"Patient\":tr[\"Patient\"].unique()})\n\nfor idx, (tr_idx, val_idx) in enumerate(kf.split(pd_patient)):\n    pd_patient.loc[val_idx,\"fold\"] = idx\n\ntr = tr.merge(pd_patient, on='Patient', how='left')\ntr.head()","11205a06":"import torch.nn.functional as F\n\nclass MishFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_variables[0]\n        sigmoid = torch.sigmoid(x)\n        tanh_sp = torch.tanh(F.softplus(x)) \n        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return MishFunction.apply(x)\n\ndef to_Mish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Mish())\n        else:\n            to_Mish(child)","153b73e5":"C1, C2 = torch.tensor(70,dtype=torch.float),torch.tensor(1000,dtype=torch.float)\nC1, C2 = C1.to(device),C2.to(device)\n#=============================#\ndef score(y_true, y_pred):\n    y_true = y_true.to(torch.float)\n    y_pred = y_pred.to(torch.float)\n    \n    sigma = y_pred[:,2] - y_pred[:,0]\n    fvc_pred = y_pred[:,1]\n    sigma_clip = torch.max(sigma, C1)\n    delta = torch.abs(y_true[:,0] - fvc_pred)\n    delta = torch.min(delta, C2)\n    sq2 = torch.sqrt(torch.tensor(2, dtype=torch.float))\n    metric = (delta \/ sigma_clip)*sq2 + torch.log(sigma_clip* sq2)\n    return torch.mean(metric)\n\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    device = y_true.device\n    qs = [0.2, 0.50, 0.8]\n    q = torch.tensor(np.array([qs]), dtype=torch.float32)\n    q = q.to(device)\n    e = y_true - y_pred\n    v = torch.max(q*e, (q-1)*e)\n    return torch.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n#=================\nclass GaussianNoise(nn.Module):\n    \"\"\"Gaussian noise regularizer.\n\n    Args:\n        sigma (float, optional): relative standard deviation used to generate the\n            noise. Relative means that it will be multiplied by the magnitude of\n            the value your are adding the noise to. This means that sigma can be\n            the same regardless of the scale of the vector.\n        is_relative_detach (bool, optional): whether to detach the variable before\n            computing the scale of the noise. If `False` then the scale of the noise\n            won't be seen as a constant but something to optimize: this will bias the\n            network to generate vectors with smaller values.\n    \"\"\"\n    def __init__(self, sigma=0.1, is_relative_detach=True):\n        super().__init__()\n        self.sigma = sigma\n        self.is_relative_detach = is_relative_detach\n        self.register_buffer('noise', torch.tensor(0))\n\n    def forward(self, x):\n        if self.training and self.sigma != 0:\n            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n            sampled_noise = self.noise.expand(*x.size()).float().normal_() * scale\n            x = x + sampled_noise\n        return x\n    \n    \nclass make_model(nn.Module):\n    def __init__(self, in_ch, out_ch=3):\n        super(make_model, self).__init__()\n        self.gasus_nosie = GaussianNoise()\n        self.fc1 = nn.Sequential(\n            nn.Linear(in_ch, 256),\n            Mish()\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(256, 256),\n            Mish(),\n            nn.Dropout(0.5)\n        )\n        self.fc3_p1 = nn.Sequential(\n            nn.Linear(256, 128),\n            Mish(),\n            nn.Dropout(0.5)\n        )\n    \n        self.fc3_p2 = nn.Linear(128, out_ch)\n        \n        \n        \n    def forward(self, x):\n        x = self.gasus_nosie(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3_p1(x)\n        x = self.fc3_p2(x)\n        return x","50d09305":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","bea09fb4":"class Data_Generate(Dataset):\n    def __init__(self,data,label=None):\n        self.data = data\n        self.label = label\n        \n    def __getitem__(self,index):\n        z_ = self.data[index]\n        if self.label is not None:\n            y_ = self.label[index]\n            y_ = y_[None,]\n            return z_,y_\n        else:\n            return z_\n         \n    def __len__(self):\n        return len(self.data)","b881262a":"FE","2efd97d6":"%%time\ncnt = 0\nEPOCHS = 1000\ncriterion = mloss(0.3)\n#for tr_idx, val_idx in kf.split(z):\nfor fold in range(NFOLD):\n    list_train_loss,list_val_loss,list_train_score,list_val_score = [],[],[],[]\n    val_out = []\n    print(f\"FOLD {fold+1}\")\n    #==================load data kfold==========================#\n    tr_z = tr[FE][tr.fold!=fold].values.astype(np.float32)\n    tr_y = tr.res_fvc[tr.fold!=fold].values.astype(np.float32)\n    val_z = tr[FE][tr.fold==fold].values.astype(np.float32)\n    val_y = tr.res_fvc[tr.fold==fold].values.astype(np.float32)\n    base_tr = tr['min_FVC'][tr.fold!=fold].values.astype(np.float32)\n    base_val = tr['min_FVC'][tr.fold!=fold].values.astype(np.float32)\n    train_db = Data_Generate(tr_z,tr_y)\n    train_loader = DataLoader(train_db, batch_size=128, shuffle=True, num_workers=4)\n    val_db = Data_Generate(val_z,val_y)\n    val_loader = DataLoader(val_db, batch_size=128, shuffle=False, num_workers=4)\n    \n    #==================prepare model==========================#\n    tr_num_batch = len(train_loader)\n    val_num_batch = len(val_loader)\n    net = make_model(len(FE),3).to(device)\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001 ,weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-8, verbose=False)\n    early_stopping = EarlyStopping(patience=80,path=f'Osic-NN-fold_{fold}.pth',verbose=False)\n\n    for epoch in tqdm(range(EPOCHS)):\n        train_loss,train_score,val_loss,val_score = 0,0,0,0\n        #==================train ==========================#\n        net.train()\n        for idx, sample in enumerate(train_loader):\n            feature, label = sample\n          \n            feature, label = feature.to(device), label.to(device)\n            out = net(feature)\n            loss = criterion(label, out)\n            score_ = score(label ,out)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\/tr_num_batch\n            train_score += score_.item()\/tr_num_batch\n        list_train_loss.append(train_loss)\n        list_train_score.append(train_score)\n        #==================val ==========================#\n        net.eval()   \n        for idx, sample in enumerate(val_loader):\n            feature, label = sample\n            feature, label = feature.to(device), label.to(device)\n            with torch.no_grad():\n                out = net(feature)\n            #val_out.append(out.cpu().numpy())\n            loss = criterion(label, out)\n            score_ = score(label, out)\n            val_loss += loss.item()\/val_num_batch\n            val_score += score_.item()\/val_num_batch\n        list_val_loss.append(val_loss)\n        list_val_score.append(val_score)\n        early_stopping(val_score, net)\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        scheduler.step(val_loss)\n             \n    print(f\"train loss: {min(list_train_loss)}  train score: {min(list_train_score)}\\n \\\n          val loss: {min(list_val_loss)} val score: {min(list_val_score)}\\n \\\n          final lr: {optimizer.param_groups[0]['lr']}\"\n         )\n    ","a6e46c50":"test = pd.read_csv(f'{ROOT}\/test.csv')\nfor i in test.Patient:\n    sub.loc[sub.Patient==i,'week'] = sub[sub.Patient==i].Weeks.values-test[test.Patient==i].Weeks.values","13ef4317":"ze = (sub[FE].values).astype(np.float32)\nbase_eval = (sub['min_FVC'].values).astype(np.float32)\npe = np.zeros((ze.shape[0], 3))\n\ntest_db = Data_Generate(ze)\ntest_loader = DataLoader(test_db, batch_size=128, shuffle=False, num_workers=4)\n\nprint(\"predict test...\")\n\nfor k in range(NFOLD):\n\n    pred = []\n    base_fvc = sub.FVC.values.astype(np.float32)\n    net = make_model(len(FE)).to(device)\n    net.load_state_dict(torch.load(f\"Osic-NN-fold_{k}.pth\"))\n    net.eval()\n    for idx, sample in enumerate(test_loader):\n        data = sample\n        data = data.to(device)\n        with torch.no_grad():\n            out = net(data)\n        out = out.cpu().numpy()\n        pred.append(out)\n    pred = base_fvc[:,None] - np.concatenate(pred)\n    pe += pred \/ NFOLD","f188c0f5":"sub['FVC1'] = pe[:, 1]\nsub['Confidence1'] = abs(pe[:, 2] - pe[:, 0])\nsubm = sub[['Patient_Week','FVC1','Confidence1']].copy()\nsubm.rename(columns={'FVC1':'FVC','Confidence1':'Confidence'},inplace=True) \nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","a65065d7":"def score_np(fvc_true, fvc_pred, sigma):\n    sigma_clip = np.maximum(sigma, 70) \n    delta = np.abs(fvc_true - fvc_pred)\n    delta = np.minimum(delta, 1000)\n    sq2 = np.sqrt(2)\n    metric = (delta \/ sigma_clip)*sq2 + np.log(sigma_clip* sq2)\n    return np.mean(metric)\n\n\nscores,sigma,mean_FVC,true_y = [],[],[],[]\nbatch = 128\n#tr = new_tr\ntr['pred_FVC']=-1\ntr['Confidence']=-1\npreds = []\nfor fold in range(NFOLD):\n    pred = []\n    net = make_model(in_ch=len(FE)).to(device)\n    net.load_state_dict(torch.load(f\"Osic-NN-fold_{fold}.pth\"))\n    net.eval()\n    \n    tr_z = tr[FE][tr.fold==fold].values.astype(np.float32)\n    tr_y = tr.FVC[tr.fold==fold].values.astype(np.float32)\n    base_val = tr['min_FVC'][tr.fold==fold].values.astype(np.float32)\n    valid_db = Data_Generate(tr_z,tr_y)\n    valid_loader = DataLoader(valid_db, batch_size=batch, shuffle=False, num_workers=4)\n    \n    for idx, sample in enumerate(valid_loader):\n        feature, label = sample\n        feature, label = feature.to(device), label.to(device)\n        with torch.no_grad():\n            out = net(feature)\n        out = out.cpu().numpy() \n        pred.append(out)\n    pred = np.concatenate(pred)# + base_val[:,None]\n    preds.append(pred)\n    tr.loc[tr.fold==fold,'pred_FVC'] = base_val - pred[:,1]\n    tr.loc[tr.fold==fold,'pred_Confidence'] = abs(pred[:,2]-pred[:,0])","13f94e09":"all_mean,last_mean = [],[]\nfor i in tr.Patient.unique():\n    sigma = tr[tr.Patient==i].pred_Confidence\n    mean_FVC = tr[tr.Patient==i].pred_FVC\n    true_y = tr[tr.Patient==i].FVC\n    all_mean.append(score_np(np.array(true_y),np.array(mean_FVC),np.array(sigma)))\n    last_mean.append(score_np(np.array(true_y[-3:]),np.array(mean_FVC[-3:]),np.array(sigma[-3:])))\n    \nprint(f\"all predict fvc scores {np.mean(all_mean)},last 3 fvc scores {np.mean(last_mean)}\")","7dd64ed9":"######################xgboost##############################\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\npred_xgboost = []\nparas={\n    'booster':'gbtree',\n \n    'objective':'reg:squarederror',\n    'gamma':0.05,\n    'lambda':4,\n    'subsample':0.4,\n    'colsample_bytree':0.7,\n    'min_child_weight':1,\n    'eta':0.05,\n    'seed':0,\n    'nthread':4,\n    'eval_metric':'mae'\n}\n\nfor fold in range(NFOLD):\n        \n    print(f\"FOLD {fold+1}\")\n    #==================load data kfold==========================#\n    tr_z = tr[FE][tr.fold!=fold].values.astype(np.float32)\n    tr_y = tr.res_fvc[tr.fold!=fold].values.astype(np.float32)\n    val_z = tr[FE][tr.fold==fold].values.astype(np.float32)\n    val_y = tr.res_fvc[tr.fold==fold].values.astype(np.float32)\n    my_model = XGBRegressor(**paras,n_estimators=500)\n    dtrain = xgb.DMatrix(tr_z, label=tr_y)\n    dval = xgb.DMatrix(val_z, label=val_y)\n    evallist = [(dval, 'eval'), (dtrain, 'train')]\n    num_round = 10\n    my_model.fit(tr_z, tr_y, early_stopping_rounds=10, \n             eval_set=[(val_z, val_y)], verbose=False)\n    y_pred = my_model.predict(val_z)\n    pred_xgboost.append(y_pred)\n    print('The rmse of prediction is:', mean_absolute_error(val_y, y_pred))\n    \n\nfrom xgboost import plot_importance\nplot_importance(my_model)\nplt.rcParams['figure.figsize'] = [15, 15]\nplt.show()","2ef7302c":"[i for i in zip(FE,range(len(FE)))]","1be358d3":"ps:Some patients were tested FVC more than once a week","d41d806a":"## Top 10 feature is: week,age,volume,mean,skew,haa,midhaa,pro_0,pro_600,midmean","184a2759":"# Train","c2315948":"# Inference","437385b3":"version 1: use multiprocessing accelerate computing,fix bugs.\n\nversion 2: Unet instead of other tranditional segmentation methods.\n\nversion 3: You can use gdcm without internet\n\nversion 4: change caculate histogram\n\nversion 5: create submission\n\nversion 6: add 21 features and regression residual","3ea0d577":"# Top Features","ced6440f":"# Introduction\nI'm trying to figure out how to use CT images correctly,and get enough useful feature.\n\nNow I get **lung volume,Hounsfield histogram(Mean,Skew,Kurthosis)**.That may be helpful in the predicting FVC.\nMore potentially valuable features can be found in discussion:\n\nhttps:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/165727\n\nhttps:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/166123","aa4121db":" # Caculate feature and histogram of CT image","855cadcb":"# CT Image Preprocessing\n\nThe size of the images are not uniform, need to unify them to 512*512","8438497e":"![image.png](attachment:image.png)","36a9ac8d":"res_fvc is residual of between min_FVC and FVC","9d939af6":"Further with the analysis of the histogram: Mean, Skew, Kurthosis.\n\nMean - average value is higher if fibrous tisue is present - the more fibrous tissue, the higher is the average; don't forget, we are talking about attenuation values here, where air has -1000 and water 0, fibrous tissue up to 50-70 (when calcified then much more).\n\nSkew - normal lung is skewed to left (much more low attenuation values) whereas fibrous lung is skewed to the right (much more high attenuation values).\n\nKurthosis - the peak of the low attenuation pixels is much much lower (since we have more higher attenuating areas instead of it).\n\nread more:https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/165727","3b314d40":"# Evaluate CV","60c9ca20":"the model has been trained from [this](https:\/\/www.kaggle.com\/hfutybx\/unet-densenet121-lung-of-segmentation)","1747e391":"HAA means high-attenuation area.Percentage of lung voxels between -600 and -250 Hounsfield Units.More information [here](https:\/\/pubmed.ncbi.nlm.nih.gov\/27471206\/)","47538315":"# References:\n1. https:\/\/www.kaggle.com\/allunia\/pulmonary-fibrosis-dicom-preprocessing\n2. https:\/\/www.kaggle.com\/aadhavvignesh\/lung-segmentation-by-marker-controlled-watershed\n3. https:\/\/www.kaggle.com\/currypurin\/osic-image-shape-eda-and-preprocess\n4. https:\/\/kaggle.com\/kugane\/lazy-lung-cropping\/notebook\n5. https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter","11e1f029":"It assume that the week when the CT scan has been taken is week 0. So, positive week value in the CSV file indicates that this FVC has been recorded after the CT scan. Negative values mean that this FVC has been recorded before the scan. For example: a week value of -2 means that the corresponding FVC value has been recorded two weeks before the CT scan.from [this](http:\/\/https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/167229#930737) \n\nMost patients do not have FVC information where week is equal to 0 and  I only use min Week as a reference"}}