{"cell_type":{"bf110144":"code","53279094":"code","70de6217":"code","f09be907":"code","4c3eeb62":"code","6fc59d74":"code","48761475":"code","73ec4ae9":"code","505545e5":"code","1a87e1d7":"code","d877a8ef":"code","74b3dcaf":"code","d62edd4b":"code","b013310d":"code","c4f9b436":"code","199ebf6a":"code","77a7cfe3":"code","43d5ba87":"code","f06fbe23":"code","971e8340":"code","cac5e768":"code","1bf35c47":"code","e3db7733":"code","814ce1e1":"code","a93e4297":"code","b4b19ac3":"code","c77a7b21":"code","1a2583d2":"code","7dbc6a29":"code","2be6e86b":"code","f9bb9461":"markdown"},"source":{"bf110144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53279094":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","70de6217":"print(df.info())\nprint(df.describe())\nprint(df.head())","f09be907":"# One Hot Encoding  ever_married, smoking_status, Residence_type, work_type, gender\ndf  = pd.get_dummies(df, columns=[\"ever_married\", \"smoking_status\", \"Residence_type\", \"work_type\", \"gender\" ])","4c3eeb62":"df.isnull().sum()\n#only bmi values missing","6fc59d74":"# deal with missing bmis \n# what indicates\/influences the bmi?\ncorr = df.corr(\"pearson\")\nplt.figure(figsize=(20,20))\nsns.heatmap(corr ,annot=True,cmap=\"RdYlGn\")","48761475":"#sns.pairplot(df, hue=\"bmi\")","73ec4ae9":"names = list()\n\nfor index,element in corr[\"bmi\"].items(): \n    if element>0.2 or element < -0.2 :\n        names.append(index)\n\nnames","505545e5":"sns.displot(df, x=\"age\", bins=10)","1a87e1d7":"print(df[\"age\"].max())","d877a8ef":"#create age_class to fill missing bmis more accuratly \nfor i,e in df[\"age\"].items(): \n    if e <= 16.4:\n        df.at[i, \"age_class\"] = 1\n    if e > 16.4 and e <= 32.8: \n        df.at[i, \"age_class\"] = 2\n    if e > 32.8 and e <= 49.2: \n         df.at[i, \"age_class\"] = 3\n    if e > 49.2 and e <= 65.6:\n         df.at[i, \"age_class\"] = 4\n    if e > 65: \n         df.at[i, \"age_class\"] = 5","74b3dcaf":"#Preprocessing\n#test train split\ny = df[\"stroke\"]\nX = df.drop([\"stroke\"], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=46)\nscaler = StandardScaler()\n","d62edd4b":"#change class balance by oversampling\n#val train spilt\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n#                                                 test_size = 0.2,random_state=22)","b013310d":"#create median df after splitting in train\/test to prevent leakage of information \nmedian_df_train = X_train.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\nmedian_df_train = median_df_train.reset_index()\n\nmedian_df_test = X_test.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\nmedian_df_test = median_df_test.reset_index()\n\n#median_df_val = X_val.groupby([\"age_class\", \"ever_married_No\", \"smoking_status_Unknown\", \"work_type_Private\",\"work_type_children\"]).median()\n#median_df_val  = median_df_val.reset_index()\n","c4f9b436":"def get_value(row, df_median): \n    # return mean of cells that meet the rows conditions\n    condition = ((median_df_train[\"age_class\"] == row[\"age_class\"]) &\n                (median_df_train[\"ever_married_No\"] == row[\"ever_married_No\"]) &\n                (median_df_train[\"smoking_status_Unknown\"] == row[\"smoking_status_Unknown\"]) & \n                (median_df_train[\"work_type_Private\"] == row[\"work_type_Private\"]) &\n                (median_df_train[\"work_type_children\"] == row[\"work_type_children\"]))\n    return median_df_train[condition]['bmi'].values[0]\n\ndef fill_bmi(df, df_median): \n    bmis = list()\n    for index, row in df.iterrows():\n        if np.isnan(row[\"bmi\"]) : \n            row[\"bmi\"] = get_value(row, df_median)\n        bmis.append(row[\"bmi\"])\n    return bmis","199ebf6a":"#fill bmi in test\/train with corresponding medians\nX_train = X_train.copy()\nX_test = X_test.copy()\nX_train.loc[:,\"bmi\"] = fill_bmi(X_train, median_df_train )\nX_test.loc[:,\"bmi\"] = fill_bmi(X_test, median_df_test)\n#X_val.loc[:,\"bmi\"] = fill_bmi(X_val, median_df_val)","77a7cfe3":"#drop age_class \nX_train = X_train.drop(\"age_class\", axis = 1)\nX_test = X_test.drop(\"age_class\", axis = 1)\n#X_val = X_val.drop(\"age_class\", axis = 1)","43d5ba87":"#check for mulitcollinearity\nvif_data = pd.DataFrame()\nX_temp = sm.add_constant(X_train)\nvif_data[\"feature\"] = X_temp.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X_temp.values, i)\n                          for i in range(len(X_temp.columns))]\n\nprint(vif_data)\n#-> low mulitcollinearity -> try Logistic Regression","f06fbe23":"print(df.loc[df[\"stroke\"] ==1, \"id\"].count())\nprint(df.loc[df[\"stroke\"] ==0, \"id\"].count())   \n#imbalanced target","971e8340":"# standardize features to improve performance \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","cac5e768":"# use SMOTE to oversample class 1\nsm = SMOTE(random_state=42)\nx_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)","1bf35c47":"# Logistic Regression with oversampled class 1 \n# 1 (positive) is stroke, 0 (negative) no stroke \nlog_model_smote = LogisticRegression(max_iter=1000, solver = 'liblinear', random_state = 44)\nlog_model_smote.fit(x_train_smote, y_train_smote)","e3db7733":"# train results \nlog_smote_pred_train = log_model_smote.predict(x_train_smote)\nprint(classification_report(y_train_smote,log_smote_pred_train))\n\n\n# test results \nlog_smote_pred_test = log_model_smote.predict(X_test)\nprint(classification_report(y_test,log_smote_pred_test))\nprint(confusion_matrix(y_test, log_smote_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, log_smote_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","814ce1e1":"#Logistic Regression with Class_weigth \"balanced\"\nlog_model = LogisticRegression(max_iter = 10, class_weight = \"balanced\", solver = 'liblinear', random_state = 44)\nlog_model.fit(X_train, y_train)","a93e4297":"# train results \nlog_pred_train = log_model.predict(X_train)\nprint(classification_report(y_train,log_pred_train))\n\n\n# test results \nlog_pred_test = log_model.predict(X_test)\nprint(classification_report(y_test,log_pred_test))\nprint(confusion_matrix(y_test, log_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, log_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","b4b19ac3":"# knn \nknn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train_smote, y_train_smote)\n\n# train results \nknn_pred_train = knn_model.predict(x_train_smote)\nprint(classification_report(y_train_smote,knn_pred_train))\n\n\n# test results \nknn_pred_test = knn_model.predict(X_test)\nprint(classification_report(y_test,knn_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, knn_pred_test)\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","c77a7b21":"#random forest \nrndf_model = RandomForestClassifier(max_depth = 5, random_state = 45, class_weight='balanced_subsample')\nrndf_model.fit(X_train, y_train)\n\n# train results \nrndf_pred_train = rndf_model.predict(X_train)\nprint(classification_report(y_train,rndf_pred_train))\n\n\n# test results \nrndf_pred_test = rndf_model.predict(X_test)\nprint(classification_report(y_test,rndf_pred_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, rndf_pred_test)\nprint(confusion_matrix(y_test, rndf_pred_test))\nprint(y_test.value_counts())\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","1a2583d2":"# Create Pipe for Tuning of LogReg and RandomForest \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n","7dbc6a29":"logReg = LogisticRegression(random_state = 42)\n#rndf = RandomForestClassifier()\n#pipe = Pipeline([('logReg', logReg),('rndf', RandomForestClassifier())])\nparam_grid = [\n    {'penalty' : ['l1', 'l2'],\n    'class_weight' : ['balanced', {0:0.1, 1:0.8}, {0:0.1, 1:0.9}, {0:0.1, 1:0.3}],\n    'solver' : ['liblinear'],\n    'max_iter' : list(range(100,200))}]\n    #'rndf__n_estimators' : list(range(10,101,10)),\n    #'rndf__max_features' : list(range(6,10,5))}]\n\ngrid_search_log = GridSearchCV(logReg, param_grid=param_grid, cv = 5, verbose=True, scoring = 'recall')\nbest = grid_search_log.fit(X_train, y_train)\nprint(best)","2be6e86b":"best_logReg_train = grid_search_log.predict(X_train)\nbest_logReg_test = grid_search_log.predict(X_test)\n\nprint(classification_report(y_train , best_logReg_train))\nprint(classification_report(y_test,best_logReg_test))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, best_logReg_test)\nprint(confusion_matrix(y_test, best_logReg_test))\nprint(y_test.value_counts())\nprint(metrics.auc(fpr, tpr))\npyplot.plot(fpr, tpr, marker='.', label='log')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","f9bb9461":" **Feel free to leave comments! Every input is greatly appreciated!**"}}