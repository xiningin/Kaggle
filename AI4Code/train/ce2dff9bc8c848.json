{"cell_type":{"26f84ca2":"code","d66ca973":"code","cfb8d0f1":"code","e695f6f2":"code","9ab172a1":"code","8ad7cb17":"code","9e58d83f":"code","3402b275":"code","06a2e3ac":"code","a9b39412":"code","3797b5f7":"code","5e1d59ab":"code","90b5229c":"code","35418fb7":"code","5f03f79e":"code","2021697c":"code","1f974c29":"code","b99c4de1":"code","797f8d78":"code","7c06098f":"code","cf03a792":"code","51f726cd":"code","dc6477c1":"code","dbec2a57":"code","6c20de98":"code","c371d9f6":"code","fb1fa922":"code","4692df44":"code","3eaddbf3":"code","6bb488f7":"code","543c94be":"code","8bf49153":"code","8afd3e4e":"markdown","3a33f0b5":"markdown","b5dad6c1":"markdown","b0a28e25":"markdown"},"source":{"26f84ca2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d66ca973":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndata.head()","cfb8d0f1":"print(data.shape)\ndata.describe()","e695f6f2":"data.groupby('Pclass')['Survived'].mean()","9ab172a1":"data.groupby('Sex')['Survived'].mean()","8ad7cb17":"data['Age'].plot.hist(bins = 50)","9e58d83f":"baby_data = data.loc[data.Age <=3, :]\nbaby_data.head()","3402b275":"baby_data.describe()","06a2e3ac":"data.groupby('Pclass')['Fare'].mean()","a9b39412":"data.groupby('Pclass')['Fare'].std()","3797b5f7":"data.corr()","5e1d59ab":"import seaborn as sns\n\nsns.heatmap(data.corr())","90b5229c":"import matplotlib.pyplot as plt\n\ndata.plot(x='Age', y='Fare', style='o')","35418fb7":"# data.hist(by='Pclass', column='Fare', bins = 30, sharex=True)\ndata.hist(by='Pclass', column = 'Fare', bins=20)","5f03f79e":"data.loc[data.Fare>400, :]","2021697c":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint(train.shape, test.shape)\nprint(train.columns, '\\n', test.columns)\nprint(pd.DataFrame({'Train': train.isna().sum(), 'Test':test.isna().sum()}).dropna() )","1f974c29":"# An overview of statistics:\nprint(train.describe())\nprint(test.describe())","b99c4de1":"# Categorical Comparison:\ndef consistency(train, test, col):\n    train_weight = train[col].value_counts(normalize=True)\n    test_weight = test[col].value_counts(normalize=True)\n    compare_table = pd.DataFrame({'Train': train_weight, 'Test': test_weight}).fillna(0)\n    compare_table.index.name = col\n    active_weight = (compare_table.Train - compare_table.Test).abs().sum()\/2    \n#     print(compare_table.Train - compare_table.Test)\n    return active_weight, compare_table\n\ncol = 'Sex'\ncol_list = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\nfor col in col_list:\n    active, table = consistency(train, test, col)\n#     print(f'Active weight of {col}: {active:.4f}; \\n {table}')\n    print(f'Active weight of {col}: {active:.4f}')\n","797f8d78":"# Numerical Comparisom:","7c06098f":"# Adversarial Validation:\ntrain.drop('Survived', axis=1, inplace=True)\ntarget_col = 'Target'\ntrain[target_col]=0\ntest[target_col]=1\n\ncombined_df = pd.concat([train, test], axis=0, sort=False)\nprint(combined_df.shape)\ncombined_df.head()","cf03a792":"# Drop: PassengerId, Name, Ticket. (too many values)\n# One-hot: Sex, Cabin, Embarked\n\none_hot_col = ['Sex', 'Cabin', 'Embarked']\ndrop_col =['PassengerId', 'Name', 'Ticket']\n\nfor col in combined_df.columns:\n    print(f'Different values in {col}: {combined_df[col].nunique()}')\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder()\n\ncombined_df[one_hot_col] = combined_df[one_hot_col].fillna('Unknown')\n# print(combined_df.isna().sum())\ntemp = encoder.fit_transform(combined_df[one_hot_col])\ntemp\nencoder.get_feature_names()\nfor col in one_hot_col:\n    col_dummies =  pd.get_dummies(combined_df[col])\n    col_dummies.columns = col_dummies.columns.map(lambda s: s+'_'+col)\n    combined_df = pd.concat([combined_df.drop(col, axis=1), col_dummies], axis=1, sort=False)\n\ncombined_df.drop(drop_col, axis=1, inplace=True)\ncombined_df.shape","51f726cd":"import xgboost as xgb\n\nX = combined_df.drop(target_col, axis=1).values\ny = combined_df[target_col].values\ndtrain = xgb.DMatrix(X, label =y)\n\nparam = {'max_depth':2, 'eta':0.2, 'silent':1, 'objective':'binary:logistic'}\nnum_round = 3\nmodel = xgb.cv(param, dtrain, num_round, nfold = 10, metrics = 'auc', verbose_eval=True)\n# From the model, adversarial validation shows that the training data and testing data are very consistent. We can use usual k-fold validation as validation strategy.","dc6477c1":"combined_df.head()","dbec2a57":"def preprocessing(train_data,test_data=None, one_hot_col=None, drop_col=None, min_max_col=None, normal_col=None, inplace=False):\n    df = train_data.copy()\n#     if test_data:\n#         test_df = test_data.copy()\n#         test_df = test_df.drop(drop_col, axis=1)\n        \n    if drop_col:\n        df = df.drop(drop_col, axis=1)\n\n    if one_hot_col:\n        df[one_hot_col] = df[one_hot_col].fillna('Unknown')\n        for col in one_hot_col:\n            col_dummies =  pd.get_dummies(df[col])\n            col_dummies.columns = col_dummies.columns.map(lambda s: s+'_'+col)\n            df = pd.concat([df.drop(col, axis=1), col_dummies], axis=1, sort=False)\n    if min_max_col:\n        for col in min_max_col:\n            col_max = max(df[col])\n            col_min = min(df[col])\n            if col_max-col_min<1e8:\n                df[col]=0\n            else:\n                df[col] = (df[col]-col_min)\/(col_max-col_min)\n    if normal_col:\n        for col in normal_col:\n            col_mean = df[col].mean()\n            col_std = df[col].std()\n            if col_std<1e8:\n                df[col]=0\n            else:\n                df[col] = (df[col] - col_mean)\/(col_std)\n                \n    return df\n\ntrain.columns","6c20de98":"from math import log\n# ((train.Age-train.Age.mean())\/train.Age.std()).plot.hist()\ntrain_select = train.Fare[train.Fare<300]\ntrain_select.plot.hist(bins = 100)","c371d9f6":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\n\n# train = pd.read_csv('..\/input\/titanic\/train.csv')\n# test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# # From EDA:\n# one_hot_col = ['Sex', 'Cabin', 'Embarked']\n# drop_col =['PassengerId', 'Name', 'Ticket']\n# min_max_col = ['Age', 'Fare']\n\n# X, y = train.drop('Survived', axis=1), train['Survived']\n# X = preprocessing(X, one_hot_col = one_hot_col, drop_col=drop_col, min_max_col = min_max_col)\n# X['Age'].fillna(X.Age.mean(), inplace=True)\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)\n\n# nn_model = LogisticRegressionCV(cv = 5, verbose=1, max_iter=1e4)\n# nn_model.fit(X_train, y_train)\n# nn_model.score(X_test, y_test)","fb1fa922":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\n\n# train = pd.read_csv('..\/input\/titanic\/train.csv')\n# test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# # From EDA:\n# one_hot_col = ['Sex', 'Cabin', 'Embarked']\n# drop_col =['PassengerId', 'Name', 'Ticket']\n# min_max_col = ['Age', 'Fare']\n\n# X_combined = pd.concat([train.drop('Survived', axis=1), test], axis=0)\n# y_train = train['Survived']\n# X_combined = preprocessing(X_combined, one_hot_col = one_hot_col, drop_col=drop_col, min_max_col = min_max_col)\n# missing_col = ['Age', 'Fare']\n# for col in missing_col:\n#     X_combined[col].fillna(X_combined[col].mean(), inplace=True)\n\n# X_train = X_combined.iloc[:train.shape[0], :]\n# X_test =  X_combined.iloc[train.shape[0]:, :]\n\n# nn_model = LogisticRegressionCV(cv = 5, verbose=1, max_iter=1e4)\n# nn_model.fit(X_train, y_train)\n","4692df44":"# y_predict = nn_model.predict(X_test)\n# submission = pd.DataFrame({\n#         \"PassengerId\": test[\"PassengerId\"],\n#         \"Survived\": y_predict\n#     })\n# submission.to_csv('nn_submission.csv', index=False)\n\n# print(submission.shape)\n# submission.head()","3eaddbf3":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# From EDA:\none_hot_col = ['Sex', 'Cabin', 'Embarked']\ndrop_col =['PassengerId', 'Name', 'Ticket']\n\n# Preprocessing for training:\nX_combined = pd.concat([train.drop('Survived', axis=1), test], axis=0)\ny_train = train['Survived']\nX_combined = preprocessing(X_combined, one_hot_col = one_hot_col, drop_col=drop_col)\nX_train = X_combined.iloc[:train.shape[0], :]\nX_test =  X_combined.iloc[train.shape[0]:, :]\n\ndtrain = xgb.DMatrix(X_train, label =y_train.values)\nparam = {'max_depth':5, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}\nnum_round = 100\nmodel = xgb.train(param, dtrain, num_round)\n# model = xgb.cv(param, dtrain, num_round, nfold = 5, stratified=True,\n#                metrics = ['auc', 'error'], verbose_eval=True)\n","6bb488f7":"dtest = xgb.DMatrix(X_test)\ny_predict = model.predict(dtest).astype(int)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_predict\n    })\nsubmission.to_csv('xgb_submission.csv', index=False)\n\nprint(submission.shape)\nsubmission.head(10)","543c94be":"xgb.plot_importance(model, max_num_features=10)","8bf49153":"y_predict.astype(int)","8afd3e4e":"## Training Models:\nModel type we are going to train:\n* Neural Nets. (Simple form as Logistic)\n* Tree based. (Simple form as decision tree)\n* SVM","3a33f0b5":"## Before we start: Train-Test consistency\nProblem: \n* Is the training set similar to the test set? \n* Is the data imbalance distributed?\n","b5dad6c1":"## Tree models:\n","b0a28e25":"## Neural Nets:"}}