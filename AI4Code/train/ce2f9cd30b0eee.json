{"cell_type":{"7c5fdfbd":"code","c87ac82e":"code","f77e87d0":"code","1edbc60a":"code","f4cde43b":"code","e2d65d3b":"code","5690fdb6":"code","18306075":"code","6b215367":"code","29c6a336":"code","e14ef6d4":"code","dc914513":"code","524a523c":"code","7a43bc4d":"code","00fe9174":"code","992aff9e":"code","144fdaf5":"code","c4254244":"code","82fc112c":"code","319c3b60":"markdown"},"source":{"7c5fdfbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c87ac82e":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","f77e87d0":"####################### Train data #############################################\ntrain['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n\ntrain['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\ntrain.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) \/ 3 \n#Mean Distance to Fire and Water \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) \/ 2 \n\n####################### Test data #############################################\ntest['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\ntest['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\ntest.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) \/ 3 \n#Mean Distance to Fire and Water \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) \/ 2","1edbc60a":"train.head()","f4cde43b":"y = train.Cover_Type\nX = train.drop([\"Id\",\"Cover_Type\"],axis =1)\nX_test = test.iloc[:,1:]","e2d65d3b":"important = list(X.iloc[:,:10].columns) + list(X.iloc[:,-10:].columns)\nX_imp = X[important]\nX_nimp = X.drop(important,axis = 1)\ntest_imp = X_test[important]\ntest_nimp = X_test.drop(important,axis = 1)","5690fdb6":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_imp)","18306075":"X_imp = sc.transform(X_imp)\ntest_imp = sc.transform(test_imp)","6b215367":"# from sklearn import preprocessing\n# le = preprocessing.LabelEncoder()\n# y_train = le.fit_transform(y)","29c6a336":"import os\nimport sys\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model,Sequential\nfrom sklearn.metrics import roc_auc_score\nfrom keras.optimizers import Adam, RMSprop, Adagrad,Adadelta,SGD\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\nfrom sklearn.model_selection import StratifiedKFold","e14ef6d4":"VALIDATION_SPLIT = 0.10\nBATCH_SIZE = 64\nEPOCHS = 1000","dc914513":"# seed = 7\n# np.random.seed(seed)\n# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n# cvscores = []","524a523c":"input1 = Input(shape=(20, ))\ndense11 = Dense(40, )(input1)\ndense12 = Dense(20, )(dense11)\n\ninput2 = Input(shape=(44, ))\ndense21 = Dense(44, )(input2)\ndense22 = Dense(30, )(dense21)\n#dense22 = Dense(2, )(dense21)\n\nmerged = concatenate([dense12, dense22])\ndense1 = Dense(600, activation='relu')(merged)\ndrop = Dropout(0.1)(dense1)\noutput = Dense(20, activation='softmax')(drop)\n\nmodel = Model([input1,input2], output)    \n\nadam = Adam(lr = 0.001)\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer= \"Adagrad\",\n    metrics=['accuracy']\n)\nr = model.fit(\n    [X_imp,X_nimp],\n    y,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=VALIDATION_SPLIT,\n    verbose=0\n)\n","7a43bc4d":"plt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","00fe9174":"plt.plot(r.history['acc'], label='acc')\nplt.plot(r.history['val_acc'], label='val_acc')\nplt.legend()\nplt.show()","992aff9e":"print(model.summary())","144fdaf5":"pred = model.predict([test_imp,test_nimp])","c4254244":"#np.argmax(pred, axis=1)\nsamp_sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsamp_sub[\"Cover_Type\"] = np.argmax(pred, axis=1)","82fc112c":"samp_sub.to_csv(\"sub.csv\",index = False)","319c3b60":" I calculated feature importance,  Sparse data had about 20% importance. So I divided features into 2 parts"}}