{"cell_type":{"5643d73b":"code","e103a742":"code","a5d98e76":"code","a8e4ab99":"code","977f4318":"code","ae20755e":"code","e1bee9be":"code","b5765bc4":"code","2fb04dab":"code","0992009c":"code","9fd73422":"code","dd6345e2":"code","212cfa25":"code","758a07d5":"code","983f2efc":"code","583e4ab1":"code","b9b0dfe5":"code","da924a09":"code","53644989":"code","a3eef797":"code","46ff288f":"code","d5904fbc":"code","e60548a1":"code","e48fa953":"code","b432306d":"code","7fccc8b8":"code","976847f7":"code","583b3e1b":"code","9d8c2df8":"code","6c7993e8":"code","466d0b8c":"code","52d35b7c":"code","628821a1":"code","991e1da3":"code","eaf4b1a5":"code","3d799641":"code","7991128e":"markdown","d3a53170":"markdown"},"source":{"5643d73b":"!pip install --upgrade pip","e103a742":"!pip install spektral","a5d98e76":"import os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm.notebook import tqdm\nimport gc\ngc.enable()\nimport cv2\nfrom PIL import Image\nimport scipy.sparse as sp\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nfrom sklearn.utils import shuffle\nfrom sklearn.neighbors import kneighbors_graph\n\nimport seaborn as sns\nimport networkx as nx","a8e4ab99":"from spektral.layers.ops import sp_matrix_to_sp_tensor\nfrom spektral.layers import GCNConv, GlobalSumPool\nfrom spektral.data import Dataset, Graph\nfrom spektral.data.loaders import PackedBatchLoader\nfrom spektral.layers import GCNConv, GCSConv, GlobalAvgPool","977f4318":"# Detect hardware, return appropriate distribution strategy\ndef get_strategy():\n    gpu = \"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())     \n    except ValueError:\n        tpu = None\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        gpu = tf.config.list_physical_devices(\"GPU\")\n        if len(gpu) == 1:\n            print('Running on GPU ', gpu)\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    elif len(gpu) == 1:\n        strategy = tf.distribute.OneDeviceStrategy(device=\"\/gpu:0\")\n        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\":True})\n    else:\n        strategy = tf.distribute.get_strategy()\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    return strategy\n\n\nstrategy = get_strategy()","ae20755e":"LANGUAGE = 0  #0=Chinese handwritten digits, 1=Russian handwritten letters\nDEBUG    = False","e1bee9be":"if LANGUAGE == 0:\n    path = '..\/input\/chinese-mnist\/data\/data\/'\n    df = pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')\n    char_values = np.unique(df['character'].values)\n    MNIST_SIZE = 64\n    \nelse:\n    path = '..\/input\/russian-handwritten-letters\/all_letters_image\/all_letters_image\/'\n    df = pd.read_csv('..\/input\/russian-handwritten-letters\/all_letters_info.csv')    \n    df['label'] = df['label']-1  #label index start from 0\n    char_values = np.unique(df['letter'].values)\n    label_values = np.unique(df['label'].values)\n    MNIST_SIZE = 32","b5765bc4":"df.head()","2fb04dab":"idx_to_character = {i:c for i,c in enumerate(char_values)}\ncharacter_to_idx = {c:i for i,c in enumerate(char_values)}","0992009c":"# Parameters\nbatch_size = 128     #16 * strategy.num_replicas_in_sync  # Batch size\nepochs = 50         # Number of training epochs\npatience = epochs    # Patience for early stopping\nl2_reg = 5e-4        # Regularization rate for l2\n\nif DEBUG:\n    df = df[0:-1:15]  #debug\n    epochs = 20","9fd73422":"def decode_image(df):\n    features = []\n    labels = []\n    \n    if LANGUAGE ==0:\n    # index extracted: suite_id: 1, sample_id: 3, code: 4\n    # resulted file name: input_1_3_4.jpg        \n        for i in tqdm(range(len(df))):\n            image_path = path + 'input_' + str(df.iloc[i][0]) + \"_\" + str(df.iloc[i][1]) + \"_\" + str(df.iloc[i][2]) + \".jpg\" \n            image_arr = cv2.imread(image_path,0)    #flag 0=grayscale(64,64), -1=as is (64,64,3)\n            features.append(image_arr)\n            labels.append(character_to_idx[df.iloc[i]['character']])\n  \n    else:    \n        for i, file in enumerate(tqdm(df['file'].values)):\n            image_path = path + file\n            image_arr = cv2.imread(image_path,0)    #flag 0=grayscale(32,32), -1=as is (32,32,4)\n            features.append(cv2.resize(image_arr,(MNIST_SIZE, MNIST_SIZE)))\n            labels.append(character_to_idx[df.iloc[i]['letter']])\n                \n    features = np.array(features)\n    labels = np.array(labels)\n    features = features \/ 255.\n    return features, labels","dd6345e2":"features, labels = decode_image(df)\nn_out = len(char_values)\n# shuffle\nfeatures, labels = shuffle(features, labels, random_state=0)\n\nfeatures.shape, labels.shape, features.min(), features.max()","212cfa25":"for i in range(len(char_values)):\n    print(f'{i}:{char_values[i]}  ', end=' ')","758a07d5":"num_plt = 5   \nnum_fig = num_plt**2\n\nplt.figure(figsize=(12,10))\nfor i in range(num_fig):\n    plt.subplot(num_plt,num_plt,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(features[i], cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n    plt.title(labels[i])             #True label\n","983f2efc":"class MNIST_CH(Dataset):\n    \"\"\"\n    For efficiency, the adjacency matrix is stored in a special attribute of the\n    dataset and the Graphs only contain the node features.\n\n    **Arguments**\n    - `p_flip`: if >0, then edges are randomly flipped from 0 to 1 or vice versa\n    with that probability.\n    - `k`: number of neighbours of each node.\n    \"\"\"\n    \n    def __init__(self, features=features, labels=labels, p_flip=0., k=8, **kwargs):\n        self.a = None\n        self.features = features\n        self.labels = labels\n        self.k = k\n        self.p_flip = p_flip\n        super().__init__(**kwargs)\n    \n    def read(self):\n        self.a = _mnist_grid_graph(self.k)\n        self.a = _flip_random_edges(self.a, self.p_flip)\n        x = self.features.reshape(-1, MNIST_SIZE ** 2, 1)\n        y = self.labels          \n        return [Graph(x=x_, y=y_) for x_, y_ in zip(x, y)]\n\n    \ndef _grid_coordinates(side):\n    \"\"\"\n    Returns 2D coordinates for a square grid of equally spaced nodes.\n    :param side: int, the side of the grid (i.e., the grid has side * side nodes).\n    :return: np.array of shape (side * side, 2).\n    \"\"\"\n    M = side ** 2\n    x = np.linspace(0, 1, side, dtype=np.float32)\n    y = np.linspace(0, 1, side, dtype=np.float32)\n    xx, yy = np.meshgrid(x, y)\n    z = np.empty((M, 2), np.float32)\n    z[:, 0] = xx.reshape(M)\n    z[:, 1] = yy.reshape(M)\n    return z\n\n\ndef _get_adj_from_data(X, k, **kwargs):\n    \"\"\"\n    Computes adjacency matrix of a K-NN graph from the given data.\n    :param X: rank 1 np.array, the 2D coordinates of pixels on the grid.\n    :param kwargs: kwargs for sklearn.neighbors.kneighbors_graph (see docs\n    [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.kneighbors_graph.html)).\n    :return: scipy sparse matrix.\n    \"\"\"\n    A = kneighbors_graph(X, k, **kwargs).toarray()\n    A = sp.csr_matrix(np.maximum(A, A.T))\n\n    return A\n\n\ndef _mnist_grid_graph(k):\n    \"\"\"\n    Get the adjacency matrix for the KNN graph.\n    :param k: int, number of neighbours for each node;\n    :return:\n    \"\"\"\n    X = _grid_coordinates(MNIST_SIZE)\n    A = _get_adj_from_data(X, k, \n                           mode='connectivity', \n                           metric='euclidean', \n                           include_self=False)\n    return A\n\n\ndef _flip_random_edges(A, percent):\n    \"\"\"\n    Flips values of A randomly.\n    :param A: binary scipy sparse matrix.\n    :param percent: percent of the edges to flip.\n    :return: binary scipy sparse matrix.\n    \"\"\"\n    if not A.shape[0] == A.shape[1]:\n        raise ValueError('A must be a square matrix.')\n    \n    dtype = A.dtype\n    A = sp.lil_matrix(A).astype(np.bool)\n    \n    n_elem = A.shape[0] ** 2\n    n_elem_to_flip = round(percent * n_elem)\n    unique_idx = np.random.choice(n_elem, replace=False, size=n_elem_to_flip)\n    row_idx = unique_idx \/\/ A.shape[0]\n    col_idx = unique_idx % A.shape[0]\n    idxs = np.stack((row_idx, col_idx)).T\n    \n    for i in idxs:\n        i = tuple(i)\n        A[i] = np.logical_not(A[i])\n    A = A.tocsr().astype(dtype)\n    A.eliminate_zeros()\n    return A\n\n\ndef _grid_coordinates_from_img(in_img, threshold):\n    \"\"\"\n    Returns 2D coordinates for a square grid of equally spaced nodes.\n    :param side: int, the side of the grid (i.e., the grid has side * side nodes).\n    :return: np.array of shape (side * side, 2).\n    \"\"\"\n    x = np.linspace(0, 1, in_img.shape[0], dtype=np.float32)\n    y = np.linspace(0, 1, in_img.shape[1], dtype=np.float32)\n    xx, yy = np.meshgrid(x, y)\n    z = np.stack([\n        xx[in_img>threshold].ravel(),\n        yy[in_img>threshold].ravel(),\n        in_img[in_img>threshold].ravel(),\n                 ], -1)\n    z = z[np.argsort(-z[:, 2]), :] # sort by pixel value\n    return z\n\ndef _mnist_img_grid_graph(in_img, k, threshold=0.6):\n    \"\"\"\n    Get the adjacency matrix for the KNN graph.\n    :param k: int, number of neighbours for each node;\n    :return:\n    \"\"\"\n    X = _grid_coordinates_from_img(in_img, threshold=threshold)\n    \n    A = _get_adj_from_data(X[:, :2], k, mode='distance', \n                                        metric='euclidean', \n                                        include_self=False)\n    return A, X\n\n\n\nplt.rcParams[\"figure.figsize\"] = (6, 6)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\n\ndef draw_graph_mpl(g, pos=None, ax=None, layout_func=nx.drawing.layout.kamada_kawai_layout, draw_labels=True):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n    else:\n        fig = None\n    if pos is None:\n        pos = layout_func(g)\n    node_color = []\n    node_labels = {}\n    shift_pos = {}\n    for k in g:\n        node_color.append(g.nodes[k].get('color', 'green'))\n        node_labels[k] = g.nodes[k].get('label', k)\n        shift_pos[k] = [pos[k][0], pos[k][1]]\n    \n    edge_color = []\n    edge_width = []\n    for e in g.edges():\n        edge_color.append(g.edges[e].get('color', 'black'))\n        edge_width.append(g.edges[e].get('width', 0.5))\n    nx.draw_networkx_edges(g, pos, font_weight='bold', edge_color=edge_color, width=edge_width, alpha=0.5, ax=ax)\n    nx.draw_networkx_nodes(g, pos, node_color=node_color, node_shape='p', node_size=300, alpha=0.75, ax=ax)\n    if draw_labels:\n        nx.draw_networkx_labels(g, shift_pos, labels=node_labels, arrows=True, ax=ax)\n    ax.autoscale()\n    return fig, ax, pos  ","583e4ab1":"data = MNIST_CH(features, labels, k=8, p_flip=0 )\n\nadj = data.a\nadj_dense = GCNConv.preprocess(adj)\nadj = sp_matrix_to_sp_tensor(adj_dense)\n\nnp.random.seed(seed=2020)\ndata_tr  = data[:]\nloader_tr = PackedBatchLoader(data_tr, batch_size=batch_size, epochs=1)\n\nbatches = [b for b in loader_tr]\nx,y = batches[-1]\n\nx.shape, y.shape","b9b0dfe5":"adj, vec = _mnist_img_grid_graph(x[1].reshape(MNIST_SIZE, MNIST_SIZE), 8, threshold=0.0)\nprint('Label:', y[1], idx_to_character[y[1]])\n\n#visualize network in unraveled 2D array with threshold=0\nplt.matshow(adj.todense(), cmap='viridis')","da924a09":"G = nx.from_scipy_sparse_matrix(adj)\n\nfor k, (xval, yval, cval) in zip(G.nodes, vec):\n    G.nodes[k]['color'] = plt.cm.jet(cval)\nfor e in G.edges():\n    G.edges[e]['width'] = 0.1\/G.edges[e]['weight']\n    G.edges[e]['color'] = plt.cm.magma(0.025\/G.edges[e]['weight'])\n    \n#visualize network in X, Y coordinates with threshold=0\ndraw_graph_mpl(G, pos=vec[:, :2]);","53644989":"MNIST_SQ = MNIST_SIZE**2\ndata = MNIST_CH(features, labels, k=8, p_flip=15\/MNIST_SQ**2 )\n\nadj = data.a\nadj_dense = GCNConv.preprocess(adj)\nadj = sp_matrix_to_sp_tensor(adj_dense)\n\nnp.random.seed(seed=2020)\ndata_tr  = data[:]\nloader_tr = PackedBatchLoader(data_tr, batch_size=batch_size, epochs=1)\n\nbatches = [b for b in loader_tr]\nx,y = batches[-1]\n\ndata_tr, x.shape, y.shape","a3eef797":"adj, vec = _mnist_img_grid_graph(x[1].reshape(MNIST_SIZE, MNIST_SIZE), 8, threshold=0.1)\nprint('Label:', y[1], idx_to_character[y[1]])\n\n#visualize network in unraveled 2D array with p_flip, threshold\nplt.matshow(adj.todense(), cmap='viridis')","46ff288f":"G = nx.from_scipy_sparse_matrix(adj)\n\nfor k, (xval, yval, cval) in zip(G.nodes, vec):\n    G.nodes[k]['color'] = plt.cm.jet(cval)\nfor e in G.edges():\n    G.edges[e]['width'] = 0.1\/G.edges[e]['weight']\n    G.edges[e]['color'] = plt.cm.magma(0.025\/G.edges[e]['weight'])\n\n#visualize network in X, Y coordinates with p_flip, threshold\ndraw_graph_mpl(G, pos=vec[:, :2]);","d5904fbc":"# Load data\ndata = MNIST_CH(features, labels, k=8, p_flip=0 )","e60548a1":"# The adjacency matrix is stored as an attribute of the dataset.\n# Create filter for GCN and convert to sparse tensor.\nadj = data.a\nadj = GCNConv.preprocess(adj)\nadj = sp_matrix_to_sp_tensor(adj)\n\n#Train\/valid\/test split\np_split = int(0.15 * len(data))  #15 percent split\n\nnp.random.seed(seed=2020)\nnp.random.shuffle(data)\ndata_tr, data_va = data[:-p_split], data[-p_split:]\n\n#Train\/test split\ndata_tr, data_te = data[:-p_split], data[-p_split:]\n\nlen(data_tr), len(data_va), len(data_te)","e48fa953":"# Build model\nclass Net(Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.conv1 = GCNConv(32, activation='elu', kernel_regularizer=l2(l2_reg))\n        self.conv2 = GCNConv(32, activation='elu', kernel_regularizer=l2(l2_reg))\n        self.flatten = Flatten()\n        self.fc1 = Dense(512, activation='relu')\n        self.fc2 = Dense(n_out, activation='softmax')  # MNIST_CH hChinese=15, Russian=33 classes\n\n    def call(self, inputs):\n        x, a = inputs\n        x = self.conv1([x, a])\n        x = self.conv2([x, a])\n        output = self.flatten(x)\n        output = self.fc1(output)\n        output = self.fc2(output)\n\n        return output\n    \n# Build model\nclass Net_Russian(Model):\n#epcho  50\/50   | Train loss: 3.5015, acc: 0.0302 | Valid loss: 3.4991, acc: 0.0302 | Test loss: 3.4643, acc: 0.0356\n#GPU Total Epoch:50, time elapse:62.82459878921509\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.conv1 = GCNConv(32, activation='elu', kernel_regularizer=l2(l2_reg))\n        self.conv2 = GCNConv(32, activation='elu', kernel_regularizer=l2(l2_reg))\n        self.flatten = Flatten()\n        self.fc1 = Dense(512, activation='relu')\n        self.fc2 = Dense(512, activation='relu')\n        self.fc3 = Dense(n_out, activation='softmax')  # MNIST_CH Chinese=15, Russian=33 classes\n\n    def call(self, inputs):\n        x, a = inputs\n        x = self.conv1([x, a])\n        x = self.conv2([x, a])\n        output = self.flatten(x)\n        output = self.fc1(output)\n        output = self.fc2(output)\n        output = self.fc3(output)\n\n        return output    ","b432306d":"# Create model\nwith strategy.scope():\n    if LANGUAGE==0:\n        model = Net()\n    else:\n        model = Net_Russian()  #need further hyperparams tuning\n        \n    optimizer = Adam(lr=0.001)\n    loss_fn = SparseCategoricalCrossentropy()\n    acc_fn = SparseCategoricalAccuracy()","7fccc8b8":"# Training function\n@tf.function\ndef train_on_batch(inputs, target):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs, training=True)\n        loss = loss_fn(target, predictions) + sum(model.losses)\n        acc = acc_fn(target, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss, acc\n\n\n# Evaluation function\ndef evaluate(loader):\n    step = 0\n    results = []\n    for batch in loader:\n        step += 1\n        x, target = batch\n        predictions = model([x, adj], training=False)\n        loss = loss_fn(target, predictions)\n        acc = acc_fn(target, predictions)\n        results.append((loss, acc, len(target)))  # Keep track of batch size\n        if step == loader.steps_per_epoch:\n            results = np.array(results)\n            return np.average(results[:, :-1], 0, weights=results[:, -1])","976847f7":"# Setup training\nbest_val_loss = 99999\ncurrent_patience = patience\nstep = 0\n\n# We can use PackedBatchLoader because we only need to create batches of node\n# features with the same dimensions.\nloader_tr = PackedBatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\nloader_va = PackedBatchLoader(data_va, batch_size=batch_size)\nloader_te = PackedBatchLoader(data_te, batch_size=batch_size)","583b3e1b":"# Training loop\nresults_tr = []\nloss_history = []\n\nepoch_cnt = 0\nepoch_mod = 1\n\nif (100 < epochs < 1000):\n    epoch_mod=10\nelif (epochs > 1000):\n    epoch_mod=20\n    \nstart_time = time.time()\n\nfor batch in loader_tr:\n    \n    step += 1\n\n    # Training step\n    x, y = batch\n    loss, acc = train_on_batch([x, adj], y)\n    results_tr.append((loss, acc, len(y)))\n\n    if step == loader_tr.steps_per_epoch:\n        results_va = evaluate(loader_va)       #VALIDATE\n        if results_va[0] < best_val_loss:\n            best_val_loss = results_va[0]\n            current_patience = patience\n            results_te = evaluate(loader_te)   #TEST\n        else:\n            current_patience -= 1\n            if current_patience == 0:\n                print('Early stopping')\n                break\n        \n        # Print results\n        results_tr = np.array(results_tr)\n        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n        epoch_cnt +=1\n        if(epoch_cnt%epoch_mod == 0):  \n            print('epcho{:4}\/{:<4} | '\n                  'Train loss: {:.4f}, acc: {:.4f} | '\n                  'Valid loss: {:.4f}, acc: {:.4f} | '\n                  'Test loss: {:.4f}, acc: {:.4f}'\n                  .format(epochs, epoch_cnt, *results_tr, *results_va, *results_te))\n\n        loss_history.append((results_tr[0], results_tr[1], \n                             results_va[0], results_va[1],\n                             results_te[0], results_te[1]))\n        \n        # Reset epoch\n        results_tr = []\n        step = 0\n        \nloss_history = np.array(loss_history) \nend_time = time.time()\n\nprint(f'Total Epoch:{epoch_cnt}, time elapse:{end_time - start_time}')","9d8c2df8":"model.summary()","6c7993e8":"# Plots\nplt.figure(figsize=(15, 4))\n\nplt.subplot(121)\nplt.plot(loss_history[:, 0], label='Train loss')\nplt.plot(loss_history[:, 2], label='Val loss')\nplt.plot(loss_history[:, 4], label='Test loss')\nplt.legend()\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n\nplt.subplot(122)\nplt.plot(loss_history[:, 1], label='Train acc')\nplt.plot(loss_history[:, 3], label='Val acc')\nplt.plot(loss_history[:, 5], label='Test acc')\nplt.legend()\nplt.ylabel('Acc')\nplt.xlabel('Epoch')\n\nplt.show()","466d0b8c":"print('Validate model')\ny_pred=[]\ny_true=[]\n\nloader_va = PackedBatchLoader(data_va, batch_size=batch_size, epochs=1)\n#batches = [b for b in loader_va]\n#(x, a, e), y = batches[-1]\n\n#x,y = batches[-1]\nfor batch in loader_va:\n    x_va, y_va = batch   \n    p_va = model([x_va, adj], training=False)  #predict label per batch  \n    y_pred.append([np.argmax(g) for g in p_va])   \n    y_va = np.vstack(y_va)                     #True label per batch\n    y_true.append(y_va)","52d35b7c":"x.shape, y.shape, np.shape(y_true),np.shape(y_pred), y_va.shape, p_va.shape","628821a1":"#Last batch of Validate images\n\nval_images = np.squeeze(x_va)     #(batch,IMG_SIZE**2) <- (batch,MNIST_SIZE**2,1)\nval_images = np.reshape(val_images,[val_images.shape[0], MNIST_SIZE,-1])\ny_va = np.squeeze(y_va)\n\nnum_plt = int(np.sqrt(len(val_images)))\nnum_plt = np.amin([5,num_plt])\nnum_fig = num_plt**2\n\nplt.figure(figsize=(10,12))\nfor i in range(num_fig):\n    plt.subplot(num_plt,num_plt,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(val_images[i], cmap=plt.cm.binary)\n    plt.xlabel(np.argmax(p_va[i])) #Predict label\n    plt.title(y_va[i])             #True label \nplt.show()","991e1da3":"for i in range(len(char_values)):\n    print(f'{i}:{char_values[i]}  ', end=' ')","eaf4b1a5":"print('Testing model')\n\n#loader_te = PackedBatchLoader(data_te, batch_size=data_te.n_graphs, epochs=1)\nloader_te = PackedBatchLoader(data_te, batch_size=batch_size, epochs=1)\ny_pred=[]\ny_true=[]\nfor batch in loader_te:\n    x_te, y_te = batch   \n    p_te = model([x_te, adj], training=False)  #predict label per batch     \n    pp= np.vstack([np.argmax(g) for g in p_te])\n    y_pred.append(pp)\n    y_te = np.vstack(y_te)\n    y_true.append(y_te)\n\n\nlen(data_te)","3d799641":"#Plot last batch of Test images\n\ntest_images = np.squeeze(x_te)\ntest_images = np.reshape(test_images,[test_images.shape[0], MNIST_SIZE,-1])\ny_te = np.squeeze(y_te)\n\n#Plot Test images\nnum_plt = int(np.sqrt(len(test_images)))\nnum_plt = np.amin([5,num_plt])\nnum_fig = num_plt**2\n\nplt.figure(figsize=(10,12))\nfor i in range(num_fig):\n    plt.subplot(num_plt,num_plt,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(test_images[i], cmap=plt.cm.binary)\n    plt.xlabel(np.argmax(p_te[i])) #Predict label\n    plt.title(y_te[i])             #True label\nplt.show()\n","7991128e":"CHINESE Handwritten Digit\n\nGPU: epcho  50\/50   | Train loss: 0.0060, acc: 0.9581 | Valid loss: 0.3431, acc: 0.9583 | Test loss: 0.3431, acc: 0.9583\n#Total Epoch:50, time elapse:235.3305425643921\n\n8TPUs: epcho  50\/50   | Train loss: 0.0061, acc: 0.9579 | Valid loss: 0.3526, acc: 0.9582 | Test loss: 0.3502, acc: 0.9569\n#Total Epoch:50, time elapse:1778.945945739746","d3a53170":"To classify the handwritten Chinese digits or Russian letters using the intensity values as nodes and neighborhood relationships as edges.\n\n\nReferences This notebook was built using the following resources:\n\nhttps:\/\/github.com\/danielegrattarola\/spektral\/blob\/master\/spektral\/datasets\/mnist.py\n\nhttps:\/\/github.com\/danielegrattarola\/spektral\/blob\/master\/examples\/other\/graph_signal_classification_mnist.py\n\nhttps:\/\/www.kaggle.com\/kmader\/mnist-based-graphs\n\nhttps:\/\/www.kaggle.com\/alincijov\/schedulers-for-beginners-using-chinesse-mnist\n\nhttps:\/\/www.kaggle.com\/alincijov\/cnn-pytorch-russian-letters\/data"}}