{"cell_type":{"f2761966":"code","15dcb576":"code","e20cb1cb":"code","3003fc84":"code","358aa650":"code","a24f9d23":"code","f8baed24":"code","7d802244":"code","a064fe03":"code","a529f4f9":"code","35b0f818":"code","c201ff70":"code","72652ec8":"code","f097e192":"code","173dd80a":"code","ae4cc907":"code","386aab63":"code","d45a119d":"code","b8eb6afe":"code","7ba47126":"code","da314ca7":"code","bea902b4":"code","bae8fb07":"code","b0c06294":"code","f7555f31":"code","9751a048":"code","7d4e1417":"code","127ebb4b":"code","364c6469":"code","558f7e61":"code","321d9557":"code","72b6dce8":"markdown","f4df7a6e":"markdown","24326fec":"markdown","1cc2b0f2":"markdown","fe20022c":"markdown","be877f38":"markdown","6fb10832":"markdown","3ea08f6d":"markdown","c7fc104b":"markdown","e041c2c4":"markdown","93953e2c":"markdown","37d47ab0":"markdown","a238ce69":"markdown","2e8507d9":"markdown","247ba524":"markdown"},"source":{"f2761966":"# importning libraries\nimport pandas as pd\nimport numpy as np\n\n#data visualization library \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# models libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier","15dcb576":"# reading csv file\ntest_df = pd.read_csv('..\/input\/test.csv')\ntrain_df = pd.read_csv('..\/input\/train.csv')","e20cb1cb":"train_df.head()","3003fc84":"train_df.tail()","358aa650":"train_df.info()","a24f9d23":"%matplotlib inline \ntrain_df.hist(bins=50, figsize=(20,15))","f8baed24":"train_df.describe()","7d802244":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","a064fe03":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","a529f4f9":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","35b0f818":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","c201ff70":"common_value = 'S'\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","72652ec8":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain_df['not_alone'].value_counts()","f097e192":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","173dd80a":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","ae4cc907":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","386aab63":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head()","d45a119d":"train_df = train_df.drop(['PassengerId'], axis=1)","b8eb6afe":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\ntest_df.head()","7ba47126":"X_train.head()","da314ca7":"Y_train.head()","bea902b4":"X_test.head()","bae8fb07":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)","b0c06294":"logisticRegression = LogisticRegression()\nlogisticRegression.fit(X_train, Y_train)\nY_prediction = logisticRegression.predict(X_test)\nlogisticRegression.score(X_train, Y_train)","f7555f31":"xgBoost = XGBClassifier()\nxgBoost.fit(X_train, Y_train)\nY_prediction = xgBoost.predict(X_test)\nxgBoost.score(X_train, Y_train)","9751a048":"acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","7d4e1417":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)","127ebb4b":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","364c6469":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","558f7e61":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","321d9557":"submission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':Y_prediction})\nsubmission.to_csv('submission.csv', index=False)","72b6dce8":"----\n<a id='validatingModel'><\/a>\n## Validating Model","f4df7a6e":"----\n<a id='developingModel'><\/a>\n## Developing Model","24326fec":"![](https:\/\/i.imgur.com\/rE1OxtK.png)","1cc2b0f2":"# Titanic Analysis: Collide with destiny","fe20022c":"Same as `Sex` columns we can add numeric values to the `Embarked` columns, This column have 3 values, S, C and Q.","be877f38":"We have added numerica values to the `Sex` column, now, adding null values to the `Age` column.","6fb10832":"----\n<a href='conclusion'><\/a>\n## Conclusion\nHere I have implemented three ML algorithms and found the best model *random forest regression*.<br>\n\n#### Inspired from the [End to End Project](https:\/\/www.kaggle.com\/niklasdonges\/end-to-end-project-with-python) with Python and it's Medium article [Predicting the survival of titanic passengers](https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8).\n\nWhat will be in next version\n- Improvement accuracy of model\n- Explanatory analysis\n\nTo know more about me go to my website [https:\/\/krunal3kapadiya.app\/](https:\/\/krunal3kapadiya.app\/ )  <br>\nIf you like this kernel, don't forgot to **upvote** it.","3ea08f6d":"I have evaluated 3 models and it's score. You can see from above score that random forest classifier is the best model  out of 3 for the dataset.","c7fc104b":"Upto now we have done wrangling with dateset, Both `X_train` and `X_test` contains similar rows.","e041c2c4":"We have total, 2 columns float type, 5 columns have integer values and 5 columns have object values.","93953e2c":"----\n<a id='wranglingData'><\/a>\n## Wrangling Data","37d47ab0":"From above we can see that, Name, Sex, Ticket, Cabin, Embarked colunms have Object (String) values. ","a238ce69":"----\n<a id='overview'><\/a>\n## Overview\n\nThe data has been split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\n**The training set** should be used to build your machine learning models. For the training set, we provide the outcome (also known as the `ground truth`) for each passenger. Your model will be based on `features` like passengers\u2019 gender and class. You can also use feature engineering to create new [features](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/).\n\n**The test set** should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n> ### Variable Notes\n**pclass**: A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n**sibsp**: The dataset defines family relations in this way... <br>\nSibling = brother, sister, stepbrother, stepsister<br>\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n**parch**: The dataset defines family relations in this way...<br>\nParent = mother, father<br>\nChild = daughter, son, stepdaughter, stepson<br>\nSome children travelled only with a nanny, therefore parch=0 for them.","2e8507d9":"## Table of Contents\n- [Overview](#overview)\n- [Wrangling Data](#wranglingData)\n- [Developing Model](#developingModel)\n- [Validating Model](#validatingModel)\n- [Conclusion](#conclusion)\n> <B>NOTE<\/B>: This is my first Kaggle comptition kernel. Any feedback or suggestions will be warmly appreciated. ","247ba524":"From above we can see that Age column have 177 missing values, also gender do not have any numeric values. <br> Adding numerica values to the gender."}}