{"cell_type":{"82fd9f76":"code","515d051d":"code","9084c825":"code","cf4a8718":"code","a5c80841":"code","caa8b953":"code","7e9b5c22":"code","7296a49c":"code","f9d8cf16":"code","cb5db4d6":"code","6fa209dd":"code","44044b20":"code","09c44017":"code","4a25abda":"code","c3000c65":"markdown","450d76b8":"markdown","f34aa742":"markdown","66fddebd":"markdown","2f577be1":"markdown","84cef9d5":"markdown","b3837a0f":"markdown","57581394":"markdown"},"source":{"82fd9f76":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom abc import ABC, abstractmethod\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n\nimport time\n\nimport numpy as np\nfrom abc import ABC, abstractmethod\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# import warnings\n# warnings.filterwarnings('ignore')","515d051d":"train = pd.read_csv(\"\/kaggle\/input\/mnist-in-csv\/mnist_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/mnist-in-csv\/mnist_test.csv\")\n\ndef split_mnist_data(df):\n    df = df[(df.label==0) | (df.label==1)]\n    X, y = df.drop(\"label\", axis=1), df['label']\n    return X.values,y.values\n\nx_train, y_train = split_mnist_data(train)\nx_test, y_test = split_mnist_data(test)","9084c825":"ones_train = x_train[y_train == 1]\nzeros_train = x_train[y_train == 0]\nN=5\nfig, axes = plt.subplots(2, N, sharex=True, sharey=True) #create subplots\nfor i, ax in enumerate(axes.flatten()): \n    if i >= N: # if image is in the second row\n        ax.imshow(zeros_train[i].reshape(28,28))\n    else:\n        ax.imshow(ones_train[i].reshape(28,28))\n\nplt.suptitle(\"Drawing {} samples from data\".format(N)) #add super title\nplt.show()","cf4a8718":"class Classifier(ABC):\n\n    def score(self, X, y):\n        \"\"\"\n        :param X: X data matrix \n        :param y: response vector\n        :return: dictionary of different classification scores\n        \"\"\"\n        # predict classes\n        pred = self.predict(X)\n        \n        #Positive classified samples\n        P = (pred > 0).sum()\n        # Negative classified samples\n        N = (pred <= 0).sum()\n        \n        #True Negative\n        TN = ((pred <= 0) & (y <= 0)).sum()\n        FP = ((pred > 0) & (y <= 0)).sum()\n\n        # True Positive\n        TP = ((pred > 0) & (y > 0)).sum()\n        \n        # True Negative\n        FN = ((pred <= 0) & (y > 0)).sum()\n    \n        acc = (TP + TN) \/ (P + N) # Accuracy\n\n        return acc\n\n    @abstractmethod\n    def predict(self, X):\n        pass\n\n    @abstractmethod\n    def fit(self, X, y):\n        pass","a5c80841":"\nclass Perceptron(Classifier):\n\n    def __init__(self):\n        self.model = None\n        self.name = \"Perceptron\"\n\n    def fit(self, X, y_in):\n        \"\"\"\n        fit the perceptron w vector.\n        :param X: X data matrix\n        :param y_in: response input vector of 0 and 1\n        \"\"\"\n        \n        # convert negative labels from 0 to -1 \n        y = np.where(y_in > 0, 1, -1)\n        \n        # number of parameters, observations\n        p, m = X.shape[1] + 1, X.shape[0]\n        \n        # initialize w vector\n        self.model = np.zeros(p)\n\n        X = np.column_stack([np.ones(m), X])\n        \n        # while exist i s.t (y_i*<w, x_i>) <=0\n        while True:\n            z = (X @ self.model)\n            scores = y * z\n            if (scores <= 0).any():\n                # get the first i s.t. (y_i*<w, x_i>) <=0\n                i = np.argmax(scores <= 0)\n                # update perceptron\n                self.model += y[i] * X[i, :]\n            else:\n                break\n\n    def predict(self, X):\n        # number of parameters, observations\n        p, m = X.shape[1] + 1, X.shape[0]\n        X = np.column_stack([np.ones(m), X])\n        predictions =  np.sign(X @ self.model)\n        \n        y_hat = np.where(predictions > 0, 1, 0)\n        return y_hat\n","caa8b953":"class LDA(Classifier):\n\n    def __init__(self):\n        self.name = \"LDA\"\n        self.NEG, self.POS = 0, 1\n        self.pr_pos, self.pr_neg = None, None\n        self.mu_pos, self.mu_neg = None, None\n        self.cov_inv = None\n\n    def fit(self, X, y): \n        \"\"\"\n        calculate LDA parameters by the training data\n        :param X: X data matrix\n        :param y_in: response input vector\n        \"\"\"\n        pr_y = (y > 0).mean()\n        self.pr_pos, self.pr_neg = pr_y, 1 - pr_y\n       \n        self.mu_pos = X[y > 0,:].mean(axis=0)\n        self.mu_neg = X[ y <= 0,:].mean(axis=0)\n        self.cov_inv = np.linalg.pinv(np.cov(X.T))\n\n    def predict(self, X):\n        \"\"\"\n        classify by the LDA decision rule\n        :param X: X data matrix\n        return: y - predicted values vector\n        \"\"\"\n        \n        mu = self.mu_pos\n        \n        d1 = (X @ self.cov_inv @ mu) - 0.5 * (mu.T @ self.cov_inv @ mu) + np.log(self.pr_pos)\n        mu = self.mu_neg\n        d2 = (X @ self.cov_inv @ mu) - 0.5 * (mu.T @ self.cov_inv @ mu) + np.log(self.pr_neg)\n        y = []\n        for i in range(len(d1)):\n            if d1[i] > d2[i]:\n                y.append(self.POS)\n            else:\n                y.append(self.NEG)\n        return np.array(y)\n\n","7e9b5c22":"def rearrange_data(X):\n    \"\"\"\n    :param X: matrix from size m x N x N\n    :return: matrix from shape m x N^2\n    \"\"\"\n    m = X.shape[0]\n    return X.reshape(m, -1)\n\ndef draw_m_random_points(m, X, y):\n    \"\"\"\n    :param m: number of observations we draw\n    :param X: train matrix X from shape q x N x N\n    :param y: np vector from size q x 1    \n    :return: X_train_m - reshaped matrix (size m X N^2) of random choice from X\n             y_train_m - random choice from y (matched to X_train_m)\n    \"\"\"\n    #get random selection of indices without return\n    rand = np.random.choice(len(y), m, replace=False)\n    \n    #slice data by random indices\n    X_train_m, y_train_m = X[rand], y[rand]\n    \n    #check if we have at least one observation from each label\n    while (y_train_m == 0).sum() == 0 or (y_train_m == 1).sum() == 0:\n        rand = np.random.choice(len(y), 3, replace=False)\n        X_train_m, y_train_m = X[rand], y[rand]\n    \n    #rearranging X random data\n    X_train_m = rearrange_data(X_train_m)\n    \n    return X_train_m, y_train_m\n","7296a49c":"\n#create classifier objects\nlogistic, svm, tree, lda, perceptron = LogisticRegression(max_iter=2000), SVC(C=1e-3, kernel='linear'), DecisionTreeClassifier(max_depth=15),  LDA(), Perceptron()\n\n#create classifiers array\nclassifiers = [logistic, svm, tree, lda, perceptron]\n\n#create classifiers names array\nnames = [\"Logistic\", \"SVM\", \"TREE\", \"LDA\", \"Perceptron\"]\n\nX_test = rearrange_data(x_test)","f9d8cf16":"def classifier_score(c, X_train, y_train, m, X_test,y_test, name,ITER=50):\n    \"\"\"\n    This function compute the mean accuracy for a classifier c for ITER iterations\n    :param m: number of observations we draw on each iteration\n    :param X_train: Train matrix X\n    :param y_train: Training labels vector y\n    :param X_test: Test matrix X\n    :param y_test: Test labels vector y\n    :return: mean accuracy of the classifier\n    \"\"\"\n    acc_lst = []\n    fit_time_lst = []\n    score_time_lst = []\n    \n    for i in range(ITER):\n        X_train_m, y_train_m = draw_m_random_points(m, X_train,y_train)\n        s_fit = time.time()\n        c.fit(X_train_m,y_train_m)\n        end_fit = time.time()\n        s_score = time.time()\n        acc = c.score(X_test, y_test)\n        end_score = time.time()\n        fit_time = (end_fit-s_fit)\n        score_time = (end_score-s_score)   \n\n        acc_lst.append(acc)\n        fit_time_lst.append(fit_time)\n        score_time_lst.append(score_time)\n        try:\n            temp = round(np.mean(acc_lst),3)\n        except:\n            print(acc_lst)\n\n    \n    return round(np.mean(acc_lst),6), round(np.mean(fit_time_lst),3), round(np.mean(score_time_lst),3)","cb5db4d6":"# list of m sizes of training set\nM = [1000,2000,5000,12000]\n# create the final scores dict, later will be used to created dataframe.\nfinal_dict = {}\nfit_times = {}\nscore_times = {}\n","6fa209dd":"for m in M:\n    classifier_dict = {}\n    fit_time_dict = {}\n    score_time_dict = {}\n    for c in range(len(classifiers)):\n        score, fit_time, score_time = classifier_score(classifiers[c], x_train, y_train, m, X_test, y_test, name=names[c])\n        classifier_dict[names[c]] = score\n        fit_time_dict[names[c]] = fit_time\n        score_time_dict[names[c]] = score_time\n\n    final_dict[m] = classifier_dict\n    fit_times[m] = fit_time_dict\n    score_times[m] = score_time_dict","44044b20":"#create a dataframe of our results\ndf = pd.DataFrame(final_dict)\ndf_fit = pd.DataFrame(fit_times)\ndf_score = pd.DataFrame(score_times)\ndf_score['type'] = \"Prediction\"\ndf_fit['type'] = \"Training\"\n\n\ndf_times = pd.concat([df_fit, df_score]).reset_index().melt(id_vars=['index', 'type'])\n\n\n#melt dataframe for easy plotting\nmelted = df.reset_index().melt(id_vars=['index'])","09c44017":"\n(ggplot(melted) + geom_line(aes(x='variable',y='value',color='index', group='index')) \n + labs(x='Number of Train Observations', y='Accuracy', title='Accuracy vs. Train Set Size - Classification') \n + scale_color_discrete(name='Classifier')).draw();\n","4a25abda":"(ggplot(df_times) + geom_line(aes(x='variable',y='value',color='index', group='index')) + facet_grid(\"type~\", scales='free_y')\n + labs(x='Number of Train Observations', y='TIme (Seconds)', title='Time vs. Test Sample Size - Classification') \n + scale_color_discrete(name='Classifier') + theme_minimal() \n + theme(figure_size=(7,6), \n         axis_title_y = element_text(margin={'r': 30}),\n        strip_text_y = element_text(size = 14),\n        panel_spacing=.5)).draw();\n","c3000c65":"<a id=\"comp\"><\/a>\n# Classification and Comparison\n\nNow, we would like to try our our classifiers on real data, and compare them to other classifiers by the following:\n - Accuracy\n - Training Time\n - Prediction Time\n \n \nWe will compare our models to the following SKLearn Classifiers: Logistic Regression, SVM, and Decision Tree. Note that the parameters were chosen arbitrarily.\n\nWe will compare the models as following: We will run 50 iterations, while on each iteration we randomly draw m number of points, when $\\ m\\ \\in \\{300,1000,5000,12000\\}$. We will calculate the average score, training time and prediction over the 50 iterations, for each m samples.\n","450d76b8":"<a id=\"results\"><\/a>\n# Results\n\nWe can see that our classifers are doing not bad with !<br>\nIf any of you has any implementation comments, I would be happy if you'll comment them below.","f34aa742":"# Table of Contents\n\n1. [Data Preproccessing](#data-prep)\n2. [Presenting our data](#data-present)\n3. [Creating Classifiers Objects](#classifiers)\n    - [Perceptron](#percep)\n    - [Linear Discriminant Analysis](#lda)\n4. [Classification and Comparison](#comp) \n5. [Results](#results)","66fddebd":"<a id=\"data-present\"><\/a>\n# Creating Classifiers Objects\n\n\nNow, we will build our classifiers from scratch.\nOur Two classifiers will be LDA (Linear Discriminant Analysis) Classifier, and Perceptron. We will build later a Python Class For each one of them. To maintain the proper [object-oriented programming](https:\/\/en.wikipedia.org\/wiki\/Object-oriented_programming) paradigm, we would like to build a master class from which our classifiers will inherit. This class will also contain the scoring method, which is shared by the two classifiers built. \n\nAfter we classify our data, we can check how good our model is by different score methods. These methods calculation is based on the digits real class and predicted class.\n\nTrue Positive - an observation we classified correctly, and is positive (1)\nTrue Negative - an observation we classified correctly, and is negative (usually -1, or in our case, the digit 0)\n\nFalse Positive - an observation we classified **incorrectly** as positive, and it's real value is negative\nFalse Negative - an observation we classified **incorrectly** as negative, and it's real value is positive\n\nSo a rule thumb is: The first word of the terms above is True\\False if we were right\\wrong, and the second word refres to what our predicted value was.\n\nHere is a table that sums it up comfortably:<br>\n![](https:\/\/miro.medium.com\/max\/797\/0*JpiWBlOFqYTPa8Ta.png)\n[Source: kdnuggets.com\n](https:\/\/www.kdnuggets.com\/2019\/10\/5-classification-evaluation-metrics-every-data-scientist-must-know.html)\n\nThe scoring method we will use will be the Accuracy, by the following formula:\n$$Accuracy = \\dfrac{TP + TN}{P + N}$$\n\n\n\n","2f577be1":"<a id=\"data-present\"><\/a>\n# Presenting Our Data\n\nBefore we start, we would like to take a look on our data (which is splitted to two classes). We will print three examples for each label.","84cef9d5":"\n<a id=\"percep\"><\/a>\n## Perceptron\n\nThe [perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) is an algorithm for supervised learning of binary classifiers. The perceptron is a linear hypeplane `w` , that has an iterative learning rule. \nThe basic idea behind the perceptron is that the hyperplane is trying to seperate the to classes in dimension p (the number of our features).\n\n$\\exists\\ i$ such that $y_i\\langle{W,x_i}\\rangle\\leq0$:\nwhich means that the data point is in the \"wrong side\" of the perceptron. Note that this happens because we label our \"Positive\" labels as 1, and our \"Negative\" labels as -1.\n\n1. Input: Pairs of (X, y) while X is the data matrix and y is the response vector (in our case, X matrix is the digit image pixels vector, and y is the label: `0` or `1`), by the follwing term:\n\n2. Initialize w as vector of zeros.\n\n3. While $\\exists\\ i$ such that $y_i\\langle{W,x_i}\\rangle\\leq0$:\n\n    $w = w + y_ix_i$\n\nfinally: $return\\ w$\n\n\n\n\nFor further reading, I recommend this article from Towards Data Science, [Here.](https:\/\/towardsdatascience.com\/perceptron-learning-algorithm-d5db0deab975)\n\nNow, we can built our class based on the perceptron algorithm.","b3837a0f":"<a id=\"lda\"><\/a>\n## Linear Discriminant Analysis\n\nThe LDA algorithm (also called Fisher's Linear Discriminant) is another classification algorithm that we want to implement for our binary classification. It based on the idea that data with different labels were made from different distibutions.\n\nLDA approaches the problem by assuming that the conditional probability density functions $p(x|y=0)$ and $p(x|y=1)$ are both normally distributed with mean and covariance parameters $\\left({\\mu }_{0},\\Sigma\\right)$ and $\\left({\\mu }_{1},\\Sigma\\right)$, respectively. Under this assumptions of normallity and same covariance matrix, the Bayes optimal solution is to find which from which class there is a higher chance that the data came from, by the following decision rule:\n\n$$\\underset{y}{\\mathrm{argmax}}\\ \\delta_y(x) = x^{T}\\Sigma^{-1}\\mu_y - \\dfrac{1}{2}\\mu_y^T\\Sigma^{-1}\\mu_y + ln(\\pi_y)$$\n\nSo, the prediction lable (negative of positive, or in our case `0` or `1` by the digit labels) will be the same value that maximize $\\delta_y(x)$.\n\nFor further reading on LDA: [Stanford](https:\/\/web.stanford.edu\/class\/stats202\/content\/lec9.pdf)\n\nNow we can build our LDA class by the following formula.","57581394":"<a id=\"data-prep\"><\/a>\n# Data Preprocessing\n\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\nEach digit is a 28x28 pixels image, that can be represented as a 784x1 vector. this vector entries will be our explanatory variables, while the digit label will be our prediction label.\n\n\nIn this notebook, which is based on one of my excersizes in the Introduction to Machine Learning Course, we want to create classifiers that can predict weather a digit is `1` or `0`.\nAs first step, we will create our X matrix and y vector for the train and the test set."}}