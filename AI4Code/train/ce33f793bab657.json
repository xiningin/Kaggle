{"cell_type":{"5af78486":"code","4d8885ed":"code","6e79e7bc":"code","23187842":"code","9c5e564c":"code","547c0e8b":"code","bf9dffc4":"code","3dca12ae":"code","370ab8ac":"code","b2a5d9ea":"code","8d2037b2":"code","2724af66":"code","595a2140":"code","1ed95c45":"code","aaa1aaae":"code","50f25023":"code","e3066424":"code","33e56cd5":"code","b08e587d":"code","bf24e753":"code","46a3492b":"code","22298c4f":"code","38890011":"code","35be5430":"code","6fd18459":"code","572b724b":"code","e1eab9be":"code","ef81ca17":"code","81b892a0":"code","b0f61dc3":"code","ef6a62e2":"markdown","0a7b28a6":"markdown","6eb50967":"markdown","c2136bf8":"markdown","a63749dd":"markdown","2e7a8939":"markdown","2290b23e":"markdown","f2fb6bcb":"markdown","33c64c2e":"markdown","0c27bdc6":"markdown","c717822a":"markdown","d18aefec":"markdown","2ef9b456":"markdown","be552b09":"markdown","051b6626":"markdown","fbf0ad63":"markdown","73ee3421":"markdown","8b399de6":"markdown","f88ae81d":"markdown","141d0955":"markdown","2a131f5b":"markdown","3a788287":"markdown","772bad7f":"markdown","1f38e2f2":"markdown","96ea2f59":"markdown","5b0dcdf7":"markdown","48a995aa":"markdown","5117ca52":"markdown","ca7720e2":"markdown","08489ed2":"markdown","33b5985f":"markdown","e66f1297":"markdown","8c8a782e":"markdown","67cc7d81":"markdown","dca3dd23":"markdown","7b186730":"markdown","85880360":"markdown","f0e918fd":"markdown","2882d247":"markdown","3bbe4d2b":"markdown","df98820a":"markdown","72ab3aeb":"markdown","4d4c2f4c":"markdown","3f6f186e":"markdown","1504e999":"markdown","35df44f0":"markdown"},"source":{"5af78486":"import pandas as pd \n\ndata = pd.read_csv(\"..\/input\/betscsv\/bets.csv\")\n\n# The rows have duplicates because of home\/away, so remove every other row\ndata = data[data.site == \"home\"]\n\n# Adding points for both teams together\ndata[\"real_total\"] = data[\"points\"] + data[\"o:points\"]\n\n# Calculating the difference between the line and outcome \ndata[\"error\"] = data[\"real_total\"] - data[\"total\"]\n\n# Making the error the absolute value\ndata[\"error\"] = data.error.abs()\n\n# Calculating mean absolute error between the lines and outcome \nn = len(data)\ntotal_error = data.error.sum()\nmae_lines = total_error \/ n\n\nprint (\"MAE over last 10 years\",mae_lines)","4d8885ed":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport pandas as pd\nfrom sklearn import preprocessing \n\n# Read the data\nX_full = pd.read_csv(\"..\/input\/cleandata2csv\/cleandata2.csv\")\nX_full[\"GAME_DATE_EST\"] = pd.to_datetime(X_full[\"GAME_DATE_EST\"],infer_datetime_format=True)\nX_full = X_full.sort_values(by=[\"GAME_DATE_EST\"], ascending=False)\nX_full = X_full.reset_index()\n\n\n\n# Label \/ One-Hot encoding team IDs and season\nle = preprocessing.OneHotEncoder()\nle2 = preprocessing.LabelEncoder()\nohe_home = le.fit_transform(X_full[[\"HOME_TEAM_ID\"]]).toarray()\nohe_away = le.transform(X_full[[\"VISITOR_TEAM_ID\"]]).toarray()\nX_full[\"SEASON\"] = X_full[\"SEASON\"] - 2010\n\n# Drop old columns\nX_full = X_full.drop(columns =[\n    \"HOME_TEAM_ID\",\"VISITOR_TEAM_ID\", \"PTS_home\", \"PTS_away\",\"GAME_DATE_EST\",\"index\"], axis=1)","6e79e7bc":"# Testing features:\nX_full[\"diff\"] = X_full[\"avgpointtotal_home\"] - X_full[\"avgpointtotal_away\"]\n#X_full[\"diff\"] = X_full[\"diff\"]**2\n#X_full[\"diff\"] = X_full[\"diff\"].abs()\n#X_full[\"multi\"] = X_full.apply(lambda row: row.avgpointtotal_home \/ row.avgpointtotal_away, axis=1)\n\n#X_full[\"off_power_home\"] = X_full[\"point_average_last10\"] - X_full[\"away_point_againts_average_last10\"]\n#X_full[\"off_power_away\"] = X_full[\"away_point_average_last10\"] - X_full[\"point_againts_average_last10\"]\n#X_full[\"gap\"] = X_full[\"off_power_home\"].abs() + X_full[\"off_power_away\"].abs()","23187842":"X_full.head(5)","9c5e564c":"X_full.describe()","547c0e8b":"X_full.point_total.describe()","bf9dffc4":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(16, 12))\nsns.heatmap(X_full.corr(),\n            cmap=\"Blues\",annot=True, fmt='.2f', vmin=0);","3dca12ae":"ohe_home_df = pd.DataFrame(ohe_home)\nohe_away_df = pd.DataFrame(ohe_away, columns=list(\"abcdefghijklmnopqrstuvwxyzABCD\"))\nX_full = pd.concat([X_full,ohe_home_df], axis=1)\nX_full = pd.concat([X_full,ohe_away_df], axis=1)\nprint(X_full.shape)","370ab8ac":"X_full.head(5)","b2a5d9ea":"import matplotlib.pyplot as plt\n\nwith plt.style.context(\"ggplot\"):\n    plt.scatter(X_full.meanpointtotal, X_full.point_total, marker=\"o\", alpha=0.1, color='#9467bd')\n    plt.xlabel(\"Mean last 50 games\")\n    plt.ylabel(\"Total points (y)\")\n    plt.title(\"Total points (y) vs mean last 50 games \")\nfig1 = plt.figure();","8d2037b2":"with plt.style.context(\"ggplot\"):\n    plt.scatter(X_full[\"diff\"], X_full.point_total, marker=\"o\", alpha=0.1)\n    plt.xlabel(\"Difference\")\n    plt.ylabel(\"Total points (y)\")\n    plt.title(\"Total points (y) vs mean last 50 games \")\nfig2 = plt.figure();","2724af66":"with plt.style.context(\"ggplot\"):\n    plt.scatter(X_full[\"diff\"].abs(), X_full.point_total, marker=\"o\", alpha=0.1)\n    plt.xlabel(\"Difference\")\n    plt.ylabel(\"Total points (y)\")\n    plt.title(\"Total points (y) vs mean last 50 games \")\nfig3 = plt.figure();","595a2140":"#mean_df = X_full.copy()\n#mean_df[\"target\"] = y\nseason_avg = X_full.groupby(by=[\"SEASON\"]).mean()\n\nwith plt.style.context(\"ggplot\"):\n    plt.scatter(season_avg.index, season_avg.point_total, color = \"#d62728\" )\n    plt.xlabel(\"Season\")\n    plt.ylabel(\"Total points (y)\")\n    plt.title(\"Mean Total Points Per Season\")\nfig4 = plt.figure();","1ed95c45":"y = X_full[\"point_total\"]\nX_full = X_full.drop(columns=['point_total'])\n\ntest = len(X_full) - int(len(X_full)*0.9)\n\nX_train = X_full.iloc[test:]\nX_valid = X_full.iloc[:test]\ny_train = y.iloc[test:]\ny_valid = y.iloc[:test]\n\n","aaa1aaae":"from sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_absolute_error\n\nEN = ElasticNet().fit(X_train,y_train)\n\npred_EN = EN.predict(X_valid)\nmae_EN = mean_absolute_error(pred_EN, y_valid)\nprint (\"mae\",mae_EN)","50f25023":"from sklearn.linear_model import Lasso \n\nls = Lasso().fit(X_train, y_train)\n\npred_ls = ls.predict(X_valid)\nmae_ls = mean_absolute_error(pred_ls, y_valid)\nprint(\"mae\",mae_ls)","e3066424":"from sklearn.linear_model import Ridge\n\nrg = Ridge().fit(X_train, y_train)\n\npred_rg = rg.predict(X_valid)\nmae_rg = mean_absolute_error(pred_rg, y_valid)\nprint(\"mae\",mae_rg)","33e56cd5":"from sklearn.linear_model import TheilSenRegressor\n\nts = TheilSenRegressor().fit(X_train, y_train)\npred_ts = ts.predict(X_valid)\nmae_ts = mean_absolute_error(pred_ts,y_valid)\nprint(\"mae\",mae_ts)","b08e587d":"#lin reg \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import cross_val_score\n\nlin = LinearRegression(normalize=True, ).fit(\n    X_train,y_train\n)\npred_lin = lin.predict(X_valid)\nmae_lin = mean_absolute_error(pred_lin, y_valid)\nprint(\"mae\", mae_lin)","bf24e753":"from sklearn.neural_network import MLPRegressor\n\nregr = MLPRegressor(random_state=1, max_iter=1000).fit(\n    X_train, y_train\n)\n\npred_mlp = regr.predict(X_valid)\nmae_regr = mean_absolute_error(pred_mlp, y_valid)\nprint(\"mae\",mae_regr)","46a3492b":"import xgboost as xgb\n\nxg = xgb.XGBRegressor(\n    booster=\"gblinear\",\n    objective=\"reg:squarederror\",\n    base_score=\"1\",\n    eval_metric=\"cox-nloglik\"\n    ).fit(X_train,y_train)\npred_xg = xg.predict(X_valid)\nxg_regr = mean_absolute_error(pred_xg, y_valid)\n                         \nprint(\"mae\",xg_regr)\nprint(\"done\")","22298c4f":"error = []\nmean_last_season = season_avg.iloc[6,0]\nfor i in y_valid:\n    error.append(abs(mean_last_season - i))\nmae_mean = sum(error) \/ len(error)\n\nprint(mae_mean)\nprint(mean_last_season)","38890011":"from hpsklearn import HyperoptEstimator, any_regressor\nfrom hyperopt import tpe\n\nestim = HyperoptEstimator(regressor=any_regressor(\"svr\"))\n\nestim.fit(X_train.values, y_train.values)\n\nprint(estim.best_model())","35be5430":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_testing = scaler.fit_transform(X_train)\nX_testing_valid = scaler.transform(X_valid)","6fd18459":"xg1 = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n             colsample_bylevel=0.8886553695663921, colsample_bynode=1,\n             colsample_bytree=0.6068338278675369, gamma=0.0017889282555567038,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.0013963222902296055, max_delta_step=0, max_depth=7,\n             min_child_weight=19, monotone_constraints='()',\n             n_estimators=5400, n_jobs=0, num_parallel_tree=1,\n             objective='reg:linear', random_state=1,\n             reg_alpha=0.22448614695919908, reg_lambda=2.9482948529431052,\n             scale_pos_weight=1, seed=1, subsample=0.8571414026048771,\n             tree_method='exact', validate_parameters=1, verbosity=None).fit(X_testing,y_train)\n        \npred_xg1 = xg1.predict(X_testing_valid)\nxg1_regr = mean_absolute_error(pred_xg1, y_valid)\nprint(\"mae\",xg1_regr)","572b724b":"results = {\"Model\": [\"ElasticNet\",\"LinearRegression\",\n                     \"MLPRegression\",\"XGBoost\",\n                     \"Lasso\",\"Ridge\",\"TheilSen\",\n                     \"Betting lines\",\"Guessing the mean\",\n                     \"HyperoptEstimator\"],\n          \"MAE Score\": [mae_EN,mae_lin,\n                       mae_regr,xg_regr,mae_ls,\n                       mae_rg,mae_ts,mae_lines,\n                       mae_mean,xg1_regr]}\nresult_df = pd.DataFrame(data=results)\nresult_df = result_df.sort_values(by=[\"MAE Score\"])\nprint(result_df.to_string(index=False))","e1eab9be":"bttm = (1 - (xg1_regr \/ mae_mean))*100\nwttl = (1 - (mae_lines \/ xg1_regr))*100\nprint(\"The model is {:.0f}% better than guessing the mean\".format(bttm))\nprint(\"The model is {:.0f}% worse than the betting lines\".format(wttl))","ef81ca17":"import numpy as np\nwith plt.style.context(\"ggplot\"):\n    plt.scatter(range(len(pred_xg1)), pred_xg1, marker=\"o\", alpha=0.3,color = \"#0066ff\")\n    plt.xlabel(\"Games\")\n    plt.ylabel(\"Estimated total points (y)\")\n    plt.title(\"Best Model Predictions\")\nfig6 = plt.figure();","81b892a0":"with plt.style.context(\"ggplot\"):\n    plt.scatter(range(len(y_valid)), y_valid, marker=\"o\", alpha=0.3, color=\"#0066ff\")\n    plt.xlabel(\"Games\")\n    plt.ylabel(\"Total points (y)\")\n    plt.title(\"Real Outcomes\")\nfig7 = plt.figure();\n","b0f61dc3":"ls = []\nfor i in range(len(pred_ts)):\n    ls.append(pred_xg1[i] - y_valid.iloc[i])\nprint (\"Average Error of model:\",sum(ls))\nwith plt.style.context(\"ggplot\"):\n    plt.scatter(range(len(ls)), ls, marker=\"o\", alpha=0.3, color=\"#ff0000\")\n    plt.xlabel(\"Games\")\n    plt.ylabel(\"Prediction-Outcome Difference\")\n    plt.title(\"Error\")\nfig7 = plt.figure();","ef6a62e2":"A positive value means that the model on average is predicting higher than the real outcome. This can sometimes be insightful to see what the model is doing and adjust accordingly. Notice that the negative error values spread wider than the positive, this is because of the overtime games. ","0a7b28a6":"-------\n\n# Preprocessing & EDA <a name=\"pre\"> <\/a>","6eb50967":"The method for validation that I chose was doing a manual 90:10 train test split with no random shuffling. That is, when we are using all of the old data to train and the newest 10% to validate. This was chosen because it was the simplest way to achieve a good validation test for this type of model. Cross validation is not a good idea because then you would be training on data that you would not have access to if you were to use this model in practice. For example, you might be able to train on 2019 data and see that that is a high scoring season, then when predicting an early 2019 game you would get better results than you should. In theory the best method would be a leave future out validation method, because that is how the model would operate in practice. For simplicity I used 90:10. Which should yield decent enough results for the task. ","c2136bf8":"-----------\n<a name=\"results\"> <\/a>\n\n# Results ","a63749dd":"Here we can see the layout of our data so far. I will explain what some of the columns mean:  \n**point_average_last10** - is the average points scored for the home team in the last 10 games   \n**points_againts_average_last10** - is the average points that got scored against the home team in the past 10 games  \nThe next two columns are the same but for the visitor team.   \nThese columns should act as a way to identify strong defensive teams \/ strong offensive teams (which should hopefully indicate the outcome of the current game)  \n**cpg** - current games played (how many games they have completed this season) (_away is the same but for the visitor team)  \nlast 4 columns are the ones described earlier ([here](#here))\n","2e7a8939":"**Things I would have done differently if I were to do this again**:\n\n* Just pick one team, try to predict the total points of their games. Instead of doing every team and every match. Perhaps this would allow me to explore more features and focus on the specific trends of that team and matchups. \n\n* Identify games that went to overtime, and have the total points when the regular time ended. This could help reduce some overtime results  bias. \n","2290b23e":"There are some highly correlated features, these will be addressed later. The features with the highest correlation with the target are: season, meantotalpoints, and point_average_last10\/away_points_average_last10","f2fb6bcb":"```python\nrecorddata[\"SEASON_ID\"] = recorddata[\"SEASON_ID\"].apply(lambda x: x - 20000)\n```","33c64c2e":"On average the amount of points scored are going up over time. Season 1 refers to 2011, all the way to 2019 (9)","0c27bdc6":"Checking to see the correlation between all of the features. As expected there is a lot of correlation between the features. It is important to keep this in mind when selecting models\/parameters.","c717822a":"``` python\navgtotal_home = []\n\nfor ind in range(len(gamesdata)):\n    date = gamesdata[\"GAME_DATE_EST\"][ind]\n    hteam = gamesdata[\"HOME_TEAM_ID\"][ind]\n    \n    # When they are the home team \n    match_stats = gamesdata.loc[\n        (gamesdata.GAME_DATE_EST < date) & (gamesdata.HOME_TEAM_ID == hteam)\n    ]\n    \n    # Home team 25 home games \n    if not match_stats.empty:\n        prevgamescount = len(match_stats.index)  # How many games have been played\n    if match_stats.empty or prevgamescount < 25:\n        # If there are less than 25, then go back to the previous season and take the average\n        lastseas = gamesdata2017.loc[\n            (gamesdata2017.GAME_DATE_EST < date) & (gamesdata2017.HOME_TEAM_ID == hteam)\n        ]\n        lastseas10 = lastseas.iloc[:25]\n        avglast5home = lastseas.point_total.mean(axis=0)\n    else:\n        only5 = match_stats.iloc[:25]\n        avglast5home = only5.point_total.mean(axis=0)\n\n    avgtotal_home.append(avglast5home)\n    # This list will be added as a column to the dataframe \n```","d18aefec":"<a name=\"here\"> <\/a>\nThere were some more tricky features that I tried adding, for example I wanted to have a feature that was the point total sum of the last 10 home and 10 away games for each visitor and away team. The problem here is that at the beginning of the season 20 previous games don't exist, so the solution to this was to go back a season. So I had to make a separate dataframe including one year prior to obtain previous years data. After testing this feature I tried changing it to the previous 50 games (25:25, home:away) and this performed much better than only 20. Here is some of the code showing how I did this: ","2ef9b456":"```python\nmean = ((gamesdata.avgpointtotal_away + gamesdata.avgpointtotal_home) \/ 2)\ngamesdata[\"meanpointtotal\"] = mean \n```","be552b09":"```python\ngamesdata = pd.read_csv(\"data\/games.csv\")\n\npoint_total = gamesdata.PTS_home + gamesdata.PTS_away\ngamesdata[\"point_total\"] = point_total\n```","051b6626":"# NBA Sports Betting Model\n\n-----","fbf0ad63":"Let's take a look at some relationships between features and target:","73ee3421":"```python\nclass tableSpider(scrapy.Spider):\n    name= 'table'\n    \n    start_url =[\n        \"https:\/\/sportsdatabase.com\/nba\/query?output=default&sdql=date%2C+team%2C+site%2C+o%3Ateam%2C+total%2C++points%2C+o%3Apoints+%40season%3E2010&submit=++S+D+Q+L+%21++\"\n    ]\n\n    def parse(self, response):\n        for row in response.xpath('\/\/*[@class=\"dataTables_wrapper no-footer\"]\/\/tbody\/tr'):\n            if row.xpath('td[3]\/\/text()').extract_first() == \"home\":\n                yield {\n                    'date' : row.xpath('td[1]\/\/text()').extract_first(),\n                    'team' : row.xpath('td[2]\/\/text()').extract_first(),\n                    'site' : row.xpath('td[3]\/\/text()').extract_first(),\n                    'o:team' : row.xpath('td[4]\/\/text()').extract_first(),\n                    'total' : row.xpath('td[5]\/\/text()').extract_first(),\n                    'points' : row.xpath('td[6]\/\/text()').extract_first(),\n                    'o:points' : row.xpath('td[7]\/\/text()').extract_first(),\n                } \n```","8b399de6":"Once I had the average of the previous 50 games for both teams in the matchup. I then took the mean of those two numbers. The idea here is that you should be able to see based on past performance if the team often has high scoring games or not. Then if you take the mean of both teams you should get a good value for predicting if the game will be high or low scoring. This value, along with the season, ended up being the strongest features in my model:","f88ae81d":"<a name=\"explination\"> <\/a>\n##### What does Over\/Under mean? \n\nThe Over\/Under is a betting line offered where the betting website will provide a number which you must pick if the outcome will be over or under that given number. This betting model will be referring to the most common over\/under value, which is the total points (sum of both teams) scored at the end of the game. For example: if the  over\/under value is 225 and you think the outcome will be lower than this, you will take the under. Then after the game team A scores 110 and team B scores 105 (110 + 105 = 115) you have won the bet because 115 is under 225. \n\n--------","141d0955":"```python\ngamesdata = gamesdata.loc[gamesdata.SEASON >= 2011]\ngamesdata.sort_values(by=[\"GAME_DATE_EST\"])\ngamesdata = gamesdata.reset_index()\n```","2a131f5b":"One thing I notice here is that there is a lot of variance in the target variable. The standard deviation is ~22 and the range from Q1-Q3 is 30.","3a788287":"To see if our best model is predicting higher or lower than real outcome we can calculate the mean error without taking the absolute value: ","772bad7f":"There is a lot of noise and this isn't the strongest correlation. However, there is a clear trend between the total points scored and the average of both teams' last 50 games played. ","1f38e2f2":"The purpose of this notebook is to try to predict the [Over\/Under](#explination) of an NBA game given the teams previous outcomes. I do not expect to out predict the lines given by the betting websites. However, the main goal of this project is to improve my understanding of regression analysis and get some practice with common practices. If you have any advice or improvements that you would have made please leave a comment and let me know. I'm always happy to get feedback. \n\n**TLDR:**  \n\"They are both high scoring teams, this game is going to be high\" - The data from this notebook shows that this is not an accurate measurement to predict the total points of an upcoming game. There is a lot of variance when it comes to the total points scored in a match. Even the betting lines are not highly accurate. \n\n### TABLE OF CONTENTS:\n* [Data Collection & Cleaning](#clean)\n* [Data Scraping](#scrape)\n* [Preprocessing & Notebook start](#pre)\n* [Data Insights & Feature Engineering](#feat)\n* [Models & Validation](#model)\n* [Results](#results)\n\n<sub> I tried changing many features\/models\/methods throughout this project and left a lot of stuff out of this notebook to make it clean, but I will make notes describing most of the things I tried. Some of the other files with cleaning and trying different features can be found on my github, link above. <\/sub>\n\n-----","96ea2f59":"From here getting the mean absolute error of the betting lines was easy: ","5b0dcdf7":"People that helped: \n\n[Benjamin Retser](https:\/\/www.kaggle.com\/benjaminretser)\n","48a995aa":"Calculating the MAE for guessing the mean of last season every time. ","5117ca52":"After trying a lot of different features, adjusting how many games to go back when taking the mean; here are the most successful ones that I could get. I tried removing many of the features and trying different combinations. The best results are when I used more data and features. ","ca7720e2":"-----------\n\n# Feature Engineering <a name=\"feat\"> <\/a>","08489ed2":"<a name=\"clean\"> <\/a>\n# Data Collection & Cleaning :\n\n* Model data downloaded from Kaggle from [Nathan Lauga](https:\/\/www.kaggle.com\/nathanlauga\/nba-games). \n* Betting line history data scraped from [SDQL](https:\/\/sdql.com\/) using [scrapy](https:\/\/scrapy.org\/)\n\nSince the data provided was split up into a few different csv files, I had to do some matching and joining using pandas in order to add a few desired features like team records (wins, loses, gamesplayed). If you would like to see exactly what I did refer to my [github](https:\/\/github.com\/PerryGraham\/Betting-Model-NBA). I will only put some important snippets of the process in this notebook. \n\nOne very important thing I added to the original dataset was the total points scored in the game, which is the target variable of this model. ","33b5985f":"-----------\n\n# Data Scraping <a name=\"scrape\"> <\/a>\n\nI would like a baseline value to be able to compare to the accuracy of my model. So I decided to scrape previous betting line data and compare it to the outcome in order to calculate the mean absolute error of the betting line. I use the [scrapy](https:\/\/scrapy.org\/) library to do this. Here is the code for the web scraper: ","e66f1297":"Here I tried a few different operations to the average point total columns to see if I could get a useful feature from these. Turns out that the difference between the two happens to be a strong feature that helped get a bit better performance in my models. ","8c8a782e":"Then I restricted my data to start in 2011 -> present ","67cc7d81":"Thank you ","dca3dd23":"--------------\n\n# Models <a name=\"model\"> <\/a>","7b186730":"There is so much noise in the target variable that predicting the total points scored in an NBA game is not easy. Using past performances as an estimator for point total does not work very well. \"They scored 130 points last game, it's going to be high again\" - this data shows that this is clearly a flawed statement. Even the best models from the betting site have a mean absolute error of 13.40  points. \n\n##### Visualization of model predictions:","85880360":"Some columns had some input errors, for example the season years had an extra number in front (eg: 2016 was in the file as 22016). Simple fix, just subtract 20,000 from all the rows. ","f0e918fd":"#### The best performer is the **Hyperoptimized XGBoostRegressor**\n\nInterpretation of the models accuracy: ","2882d247":"**Regression model**\n\n> [By: Graham Pinsent](https:\/\/twitter.com\/GrahamPinsent)  \n> [Linkedin](https:\/\/www.linkedin.com\/in\/graham-pinsent\/)  \n> [GitHub](https:\/\/github.com\/PerryGraham)\n\n**Tools used:** Python, scrapy,  pandas, sklearn, xgboost, matplotlib, seaborn, numpy ","3bbe4d2b":"They look similar but the x-axis scale is much larger for the real outcomes graph. ","df98820a":"I decided to one-hot encode the team ids, this seems like the best way to handle the home and away teams. Although it adds a lot of columns to the dataframe. For the Season, I just scaled it down to start at 0. Then dropped the old columns, and the points scored per team because this is unknown untill the game is over, which would cause data leaks. ","72ab3aeb":"##### Visualization of actual outcomes:","4d4c2f4c":"An explanation of why the predictions look like this is because the first ~400 games are from the 2018 season and the rest is the 2019 season. ","3f6f186e":"Using [hpsklearn](https:\/\/github.com\/hyperopt\/hyperopt-sklearn\/tree\/master\/hpsklearn) library to try a collection of models with **hyperoptimization** of parameters: ","1504e999":"This graph shows that there is a slight trend towards a medium scoring game when there is a large difference between the two teams. But if the teams are closer in terms of their average last 50 games, then it could be a high or low scoring game. \n\nThis trend is a bit easier to see when you plot the absolute value of the difference. ","35df44f0":"Adding on the one-hot encoded columns: "}}