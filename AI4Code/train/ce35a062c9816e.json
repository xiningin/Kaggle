{"cell_type":{"a5416e4e":"code","fd50c9ac":"code","d7d0529c":"code","f41fc06d":"code","b28b7b3a":"code","f76bc8a8":"code","3cedbc00":"code","b17b2141":"code","2fe4f521":"code","ff530b36":"code","02c95bd1":"code","69816446":"code","48c4214f":"code","130aac90":"code","74bbe76d":"code","7588eaf6":"code","830b8b51":"code","0c947be6":"code","599969a1":"code","09225951":"code","f68204ab":"code","e337b21c":"code","03ea041a":"code","7442fb65":"code","e088639c":"code","ef0abea1":"code","b873c241":"code","38ab260c":"code","3918a4f8":"code","c6188b70":"code","c07df892":"code","d42619f8":"code","21732978":"code","ddc20de8":"code","5ca8ec0f":"code","7d41d241":"code","6ccfa23b":"code","114f1a39":"code","3133daa5":"code","d3decb90":"code","d13192f6":"code","9ad8481e":"markdown","515aabc1":"markdown","0a9787b4":"markdown","1a451050":"markdown","7318590f":"markdown","e55f15f9":"markdown","1215b312":"markdown","01bd3ab7":"markdown","5b8e2824":"markdown","ab98e158":"markdown","208dfaff":"markdown","d69aecb8":"markdown","ec50138e":"markdown","c0b6a665":"markdown","02822c48":"markdown","1a6dba33":"markdown","4c38983b":"markdown","d4ab3345":"markdown","6caf37af":"markdown"},"source":{"a5416e4e":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom fastai.vision import *\nfrom fastai.metrics import *\n\nimport os\npath = '..\/input'\nprint(os.listdir(path))\n","fd50c9ac":"class CustomImageList(ImageList):\n    def open(self, fn):\n        img = fn.reshape(28,28)\n        img = np.stack((img,)*3, axis=-1)\n        return Image(pil2tensor(img, dtype=np.float32))\n    \n    @classmethod\n    def from_csv_custom(cls, path:PathOrStr, csv_name:str, imgIdx:int=1, header:str='infer', **kwargs)->'ItemList': \n        df = pd.read_csv(Path(path)\/csv_name, header=header)\n        res = super().from_df(df, path=path, cols=0, **kwargs)\n        \n        res.items = df.iloc[:,imgIdx:].apply(lambda x: x.values \/ 255.0, axis=1).values\n        \n        return res\n    \n    @classmethod\n    def from_df_custom(cls, path:PathOrStr, df:DataFrame, imgIdx:int=1, header:str='infer', **kwargs)->'ItemList': \n        res = super().from_df(df, path=path, cols=0, **kwargs)\n        \n        res.items = df.iloc[:,imgIdx:].apply(lambda x: x.values \/ 255.0, axis=1).values\n        \n        return res","d7d0529c":"test = CustomImageList.from_csv_custom(path=path, csv_name='test.csv', imgIdx=0)","f41fc06d":"data = (CustomImageList.from_csv_custom(path=path, csv_name='train.csv', imgIdx=1)\n                .split_by_rand_pct(.2)\n                .label_from_df(cols='label')\n                .add_test(test, label=0)\n                .transform(get_transforms(do_flip=False))\n                .databunch(bs=128, num_workers=0)\n                .normalize(imagenet_stats))\n","b28b7b3a":"data.show_batch(rows=3, figsize=(5,5))","f76bc8a8":"learn = cnn_learner(data, models.resnet18, metrics=[accuracy], model_dir='\/kaggle\/working\/models')","3cedbc00":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","b17b2141":"learn.fit_one_cycle(4,max_lr=1e-2)","2fe4f521":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","ff530b36":"learn.fit_one_cycle(10,max_lr = slice(1e-6,1e-4))","02c95bd1":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","69816446":"# get the predictions\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\n# output to a file\nsubmission_df = pd.DataFrame({'ImageId': list(range(1,len(labels)+1)), 'Label': labels})\nsubmission_df.to_csv(f'submission_orig.csv', index=False)","48c4214f":"train_df = pd.read_csv(path+'\/train.csv')\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_df,test_size=0.2) # Here we will perform an 80%\/20% split of the dataset, with stratification to keep similar distribution in validation set","130aac90":"train_df['label'].hist(figsize = (10, 5))","74bbe76d":"proportions = pd.DataFrame({0: [0.5],\n                            1: [0.05],\n                            2: [0.1],\n                            3: [0.03],\n                            4: [0.03],\n                            5: [0.03],\n                            6: [0.03],\n                            7: [0.5],\n                            8: [0.5],\n                            9: [0.5],\n                           })","7588eaf6":"imbalanced_train_df = train_df.groupby('label').apply(lambda x: x.sample(frac=proportions[x.name]))","830b8b51":"imbalanced_train_df['label'].hist(figsize = (10, 5))","0c947be6":"df = pd.concat([imbalanced_train_df,val_df])","599969a1":"data = (CustomImageList.from_df_custom(df=df,path=path, imgIdx=1)\n                .split_by_idx(range(len(imbalanced_train_df)-1,len(df)))\n                .label_from_df(cols='label')\n                .add_test(test, label=0)\n                .transform(get_transforms(do_flip=False))\n                .databunch(bs=128, num_workers=0)\n                .normalize(imagenet_stats))","09225951":"data.show_batch(rows=3, figsize=(5,5))","f68204ab":"learn = cnn_learner(data, models.resnet18, metrics=[accuracy], model_dir='\/kaggle\/working\/models')","e337b21c":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","03ea041a":"learn.fit_one_cycle(4,max_lr=1e-2)","7442fb65":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","e088639c":"learn.fit_one_cycle(10,max_lr = slice(1e-6,5e-4))","ef0abea1":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","b873c241":"# get the predictions\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\n# output to a file\nsubmission_df = pd.DataFrame({'ImageId': list(range(1,len(labels)+1)), 'Label': labels})\nsubmission_df.to_csv(f'submission_imbalanced.csv', index=False)","38ab260c":"data.train_dl.dl.sampler","3918a4f8":"labels = []\nfor img,target in data.train_dl.dl:\n    labels.append(target)\nlabels = torch.cat(labels)\nplt.hist(labels)","c6188b70":"np.bincount([data.train_dl.dataset.y[i].data for i in range(len(data.train_dl.dataset))])","c07df892":"type(np.max(np.bincount([data.train_dl.dataset.y[i].data for i in range(len(data.train_dl.dataset))])))","d42619f8":"from torch.utils.data.sampler import WeightedRandomSampler\n\ntrain_labels = data.train_dl.dataset.y.items\n_, counts = np.unique(train_labels,return_counts=True)\nclass_weights = 1.\/counts\nweights = class_weights[train_labels]\nlabel_counts = np.bincount([learn.data.train_dl.dataset.y[i].data for i in range(len(learn.data.train_dl.dataset))])\ntotal_len_oversample = int(learn.data.c*np.max(label_counts))\ndata.train_dl.dl.batch_sampler = BatchSampler(WeightedRandomSampler(weights,total_len_oversample), data.train_dl.batch_size,False)","21732978":"labels = []\nfor img,target in data.train_dl:\n    labels.append(target)\nlabels = torch.cat(labels)\nplt.hist(labels)","ddc20de8":"class OverSamplingCallback(LearnerCallback):\n    def __init__(self,learn:Learner):\n        super().__init__(learn)\n        self.labels = self.learn.data.train_dl.dataset.y.items\n        _, counts = np.unique(self.labels,return_counts=True)\n        self.weights = torch.DoubleTensor((1\/counts)[self.labels])\n        self.label_counts = np.bincount([self.learn.data.train_dl.dataset.y[i].data for i in range(len(self.learn.data.train_dl.dataset))])\n        self.total_len_oversample = int(self.learn.data.c*np.max(self.label_counts))\n        \n    def on_train_begin(self, **kwargs):\n        self.learn.data.train_dl.dl.batch_sampler = BatchSampler(WeightedRandomSampler(weights,self.total_len_oversample), self.learn.data.train_dl.batch_size,False)","5ca8ec0f":"learn = cnn_learner(data, models.resnet18, metrics=[accuracy], callback_fns = [partial(OverSamplingCallback)], model_dir='\/kaggle\/working\/models')","7d41d241":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","6ccfa23b":"learn.fit_one_cycle(4,1e-2)","114f1a39":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","3133daa5":"learn.fit_one_cycle(10,5e-4)","d3decb90":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","d13192f6":"# get the predictions\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\n# output to a file\nsubmission_df = pd.DataFrame({'ImageId': list(range(1,len(labels)+1)), 'Label': labels})\nsubmission_df.to_csv(f'submission_oversampled.csv', index=False)","9ad8481e":"This is much better than the severely imbalanced case but still not as good as training on the original dataset. Let's create a submission.","515aabc1":"## Train model on imbalanced data","0a9787b4":"Let's create our DataBunch:","1a451050":"I hope this kernel demonstrated how to perform oversampling in the fastai library. This feature may be added to the `fastai` library if this is helpful to others.","7318590f":"Current distribution:","e55f15f9":"# Oversampling MNIST with Fastai\n\nThis kernel highlights the usefulness of oversampling for imbalanced datasets. I use fastai callbacks to oversample data during training. I will train on the full dataset, then an imbalanced dataset, and then an oversampled version of the imbalanced dataset.\n\nWe will see that the oversampled version will get improved performance (both on training set and on public leaderboard)","1215b312":"If we look at the submissions to the leaderboard, a previous run obtained these results:\n* Original - 0.99142\n* Imbalanced - 0.94700\n* Oversampled - 0.98557\n\nThis again demonstrates the usefulness of oversampling for imbalanced datasets.","01bd3ab7":"## Load data\n\nSince the data are represented as rows in a csv file, a custom `ImageList` is necessary to be able to properly open the data (adapted from [this](https:\/\/www.kaggle.com\/steventesta\/digit-recognizer-fast-ai-custom-databunch) kernel):","5b8e2824":"We can now create a callback which can be passed to the `Learner`.","ab98e158":"Let's create our DataBunch.","208dfaff":"New distribution:","d69aecb8":"## Train on imbalanced dataset with oversampling\n\nI will first show how we can use the Weighted Random Sampler in PyTorch to implement oversampling. We will then implement a callback for fastai that will perform oversampling of the dataset.","ec50138e":"# Creating imbalanced dataset\n\nNow let's create an imbalanced version of the MNIST dataset. The training dataset will be imbalanced and the validation dataset will be the same.","c0b6a665":"As we can see, we are able to get effectively perfect accuracy on the validation set. Let's create a submission of our model:","02822c48":"There is significantly less accuracy for the same set-up. Let's create a submission.","1a6dba33":"As you can see, the images are predominately zeroes, sevens, eights and nines.","4c38983b":"Currently the sampler is a random sampler:","d4ab3345":"## Train Original Model","6caf37af":"If we instead use a weighted random sampler with weights that are inverse of the counts of the labels, we can get a relatively balanced distribution."}}