{"cell_type":{"b9c1f56e":"code","ba9cfcc3":"code","0e2b2bd9":"code","053edf41":"code","f7d5af38":"code","e0edac96":"code","f85cf596":"code","75d5f8f0":"code","b19c76a1":"code","e524d5a9":"code","568e674d":"code","a2216f6f":"code","c3ab2161":"code","d47bf8ed":"code","2a23fc9e":"code","8919f267":"markdown","7c89a64d":"markdown","ee1a8b62":"markdown","7a0a1c70":"markdown","692b87fa":"markdown","5d737bdf":"markdown","7dc296e9":"markdown","ff7ab029":"markdown","de551c0b":"markdown","5fd1be25":"markdown","cafd4c10":"markdown"},"source":{"b9c1f56e":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n\nimport matplotlib.pyplot as plt","ba9cfcc3":"# Emotions Dataset\npath_to_emotion_train = '..\/input\/emotions-dataset-for-nlp\/train.txt'\npath_to_emotion_val = '..\/input\/emotions-dataset-for-nlp\/val.txt'\npath_to_emotion_test = '..\/input\/emotions-dataset-for-nlp\/test.txt'\n\n# GloVe\npath_to_glove_file = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'\n\n# save paths\npath_to_tokenizer = '.\/tokenizer.pickle'\npath_to_label_encoder = '.\/labelEncoder.pickle'\npath_to_model = '.\/emotion_recognition.h5'","0e2b2bd9":"str_punc = string.punctuation.replace(',', '').replace(\"'\",'')\n\ndef clean(text):\n    \"\"\"Text preprocessing function.\n\n    Args:\n        text (str): text to clean.\n\n    Returns:\n        str: clean text\n    \"\"\"\n    global str_punc\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    text = text.lower()\n    return text    ","053edf41":"# dataset = pd.read_csv('..\/input\/isear-emotion\/isear_dataset.csv')\ndf_train = pd.read_csv(path_to_emotion_train, names=['Text', 'Emotion'], sep=';')\ndf_val = pd.read_csv(path_to_emotion_val, names=['Text', 'Emotion'], sep=';')\ndf_test = pd.read_csv(path_to_emotion_test, names=['Text', 'Emotion'], sep=';')\n\nX_train = df_train['Text'].apply(clean)\ny_train = df_train['Emotion']\n\nX_test = df_test['Text'].apply(clean)\ny_test = df_test['Emotion']\n\nX_val = df_val['Text'].apply(clean)\ny_val = df_val['Emotion']\n\nX = pd.concat([X_train, X_test, X_val])\ny = pd.concat([y_train, y_test, y_val])\n\nprint('Train Length      :', X_train.shape[0])\nprint('Test Length       :', X_test.shape[0])\nprint('Validation Length :', X_val.shape[0])","f7d5af38":"# train, test, validation\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(13, 3))\n\nax1.bar(y_train.unique(), height=y_train.value_counts())\nax1.title.set_text(\"Training data - classes counts\")\n\nax2.bar(y_test.unique(), height=y_test.value_counts())\nax2.title.set_text((\"Testing data - classes counts\"))\n\nax3.bar(y_val.unique(), height=y_val.value_counts())\nax3.title.set_text((\"Validation data - classes counts\"))\n\nplt.tight_layout()\n\n# total values\nplt.figure(figsize=(15.83, 3))\nplt.bar(y.unique(), height=y.value_counts())\nplt.title((\"Total data - classes counts\"))\n\nplt.show()","e0edac96":"le = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)\ny_val = le.transform(y_val)\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)","f85cf596":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))\n\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nsequences_val = tokenizer.texts_to_sequences(X_val)\n\nX_train = pad_sequences(sequences_train, maxlen=256, truncating='pre')\nX_test = pad_sequences(sequences_test, maxlen=256, truncating='pre')\nX_val = pad_sequences(sequences_val, maxlen=256, truncating='pre')\n\nvocabSize = len(tokenizer.index_word) + 1\nprint(f\"Vocabulary size : {vocabSize}\")","75d5f8f0":"num_tokens = vocabSize\nembedding_dim = 200\nhits = 0\nmisses = 0\nembeddings_index = {}\n\n# read word vectors\nwith open(path_to_glove_file, encoding='utf-8') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n# assign word vectors to our dictionary\/vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros\n        # this includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","b19c76a1":"adam = Adam(learning_rate=0.005)\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = Sequential()\n    model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n    model.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n    model.add(Dense(6, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nmodel.summary()","e524d5a9":"# defining the early stopping function\ncallback = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=2,\n    restore_best_weights=True,\n)","568e674d":"# fit model\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_data=(X_test, y_test),\n    verbose=1,\n    batch_size=256,\n    epochs=25,\n    callbacks=[callback]\n)","a2216f6f":"model.evaluate(X_val, y_val, verbose=1)","c3ab2161":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(epochs, acc, 'r', label='Training accuracy')\nax1.plot(epochs, val_acc, 'b', label='Validation accuracy')\nax1.title.set_text('Training and validation accuracy')\nax1.legend()\n\nax2.plot(epochs, loss, 'r', label='Training Loss')\nax2.plot(epochs, val_loss, 'b', label='Validation Loss')\nax2.title.set_text('Training and validation loss')\nax2.legend()\n\nplt.show()","d47bf8ed":"sentences = [\n    \"He's over the moon about being accepted to the university\",\n    \"Your point on this certain matter made me outrageous, how can you say so? This is insane.\",\n    \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n    \"Merlin's beard harry, you can cast the Patronus charm! I'm amazed!\"\n]\n\nfor sentence in sentences:\n    print(sentence)\n\n    # clean\n    sentence = clean(sentence)\n\n    # tokenize\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=256, truncating='pre')\n\n    # finding result\n    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n    proba =  np.max(model.predict(sentence))\n\n    print(f\"{result} : {proba}\\n\\n\")","2a23fc9e":"# saving the model\nwith open(path_to_tokenizer, 'wb') as f:\n    pickle.dump(tokenizer, f)\n\nwith open(path_to_label_encoder, 'wb') as f:\n    pickle.dump(le, f)\n\nmodel.save(path_to_model)","8919f267":"## 4. Model Evaluation","7c89a64d":"## 5. Saving result","ee1a8b62":"# Emotion Recognition\n\n[Emotions Dataset](https:\/\/www.kaggle.com\/praveengovi\/emotions-dataset-for-nlp) has been used for the training.","7a0a1c70":"## 2. Read GloVE embeddings\n\n[GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.","692b87fa":"### 3.3 Visualizing history","5d737bdf":"## 1. Preprocessing","7dc296e9":"### 1.2 Encoding labels","ff7ab029":"### 1.1 Visualizing value counts of the labels","de551c0b":"### 1.3 Tokenize words","5fd1be25":"### 3.2 Training model","cafd4c10":"## 3. Neural Network\n\n### 3.1 Defining Neural Network Architecture"}}