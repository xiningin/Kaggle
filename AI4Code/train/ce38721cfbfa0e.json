{"cell_type":{"1632e633":"code","3bdb01bb":"code","d97bfea0":"code","5aba4639":"code","e84b7349":"code","3687ffa0":"code","bb855425":"code","1d7dd3b9":"code","faa6f360":"code","7019b5b6":"code","d2a829f3":"code","05dcd56a":"code","9a00d825":"code","83e6dfa5":"code","993e81d0":"code","875a2bd3":"code","2f785ad5":"code","7be179ed":"code","eead57c3":"code","822ebf21":"code","9e4569be":"code","f824d35d":"code","d094b4d6":"code","8c440db8":"code","331e6ee8":"markdown","099a664f":"markdown","b0edad29":"markdown","ff7d5fb7":"markdown","3d55e3e6":"markdown"},"source":{"1632e633":"#imports from scikit learn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import model_selection   \nfrom sklearn import metrics\n\n#imports from keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.layers import LSTM\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n#imports of time series models\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\n\n#utils\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nfrom matplotlib import pyplot\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import scatter_matrix\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport warnings","3bdb01bb":"#loading data\nstk_names = ['MSFT', 'IBM', 'GOOGL']\nccy_names = ['DEXJPUS', 'DEXUSUK']\nidx_names = ['SP500', 'DJIA', 'VIXCLS']\nstk_data = web.DataReader(stk_names, 'yahoo')\nccy_data = web.DataReader(ccy_names, 'fred')\nidx_data = web.DataReader(idx_names, 'fred')\n\n#print(np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(period).shift(-5))","d97bfea0":"#using a 5 day period for testing\nperiod = 5 #5 days\nY = np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(period).shift(-period)\nY.name = Y.name[-1]+'_pred'\nX1 = np.log(stk_data.loc[:, ('Adj Close', ('GOOGL', 'IBM'))]).diff(period)\nX1.columns = X1.columns.droplevel()\nX2 = np.log(ccy_data).diff(period)\nX3 = np.log(idx_data).diff(period)\nX4 = pd.concat([np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(i) \nfor i in [period, period*3,period*6, period*12]], axis=1).dropna()\nX4.columns = ['MSFT_DT', 'MSFT_3DT', 'MSFT_6DT', 'MSFT_12DT']\n\n#X1 = STK DATA, X2 = CURRENCY DATA, X3 = INDEX DATA, X$ = MICROSOFT PAST DATA\n#Y = MICROSOFT DATA FOR PREDICTION\nX = pd.concat([X1, X2, X3, X4], axis=1)\n\ndataset = pd.concat([Y, X], axis=1).dropna().iloc[::period, :]\nY = dataset.loc[:, Y.name]\nX = dataset.loc[:, X.columns]\n","5aba4639":"dataset.head()","e84b7349":"#plotting correlation matrix\ncorr = dataset.corr()\npyplot.figure(figsize=(15,15))\npyplot.title('Correlation Matrix')\nsns.heatmap(corr, vmax=1, square = True, annot = True, cmap='cubehelix')","3687ffa0":"#plotting scatterplot matrix\npyplot.figure(figsize=(15,15))\nscatter_matrix(dataset,figsize=(12,12))\npyplot.show()","bb855425":"#lstm seasonal\nres = sm.tsa.seasonal_decompose(Y,freq=52)\nfig = res.plot()\nfig.set_figheight(8)\nfig.set_figwidth(15)\npyplot.show()","1d7dd3b9":"val_size = 0.2\ntrain_size = int(len(X) * (1-val_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]","faa6f360":"#selecting models\nnum_folds = 10\nscoring = 'neg_mean_squared_error'\n\nmodels = []\n#regression and tree reg algo\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n#nn algo\nmodels.append(('MLP', MLPRegressor()))\n#ensemble models\n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))","7019b5b6":"#testing all algorithms for comparison\nnames = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n    ## k-fold analysis:\n    kfold = model_selection.KFold(n_splits=num_folds)\n    #converted mean squared error to positive. The lower the better\n    cv_results = -1* model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    kfold_results.append(cv_results)\n    # Full Training period\n    res = model.fit(X_train, Y_train)\n    train_result = metrics.mean_squared_error(res.predict(X_train), Y_train)\n    train_results.append(train_result)\n    # Test results\n    test_result = metrics.mean_squared_error(res.predict(X_test), Y_test)\n    test_results.append(test_result)\n","d2a829f3":"#geral error\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison: Kfold results')\npyplot.ylabel('Error')\nax = fig.add_subplot(111)\npyplot.boxplot(kfold_results)\nax.set_xticklabels(names)\nfig.set_size_inches(16,8)\npyplot.show()","05dcd56a":"#train and test errors\n# compare algorithms\ndef algorithm_comparison():\n    fig = pyplot.figure()\n    ind = np.arange(len(names)) # the x locations for the groups\n    width = 0.35 # the width of the bars\n    fig.suptitle('Algorithm Comparison')\n    ax = fig.add_subplot(111)\n    pyplot.bar(ind - width\/2, train_results, width=width, label='Train Error')\n    pyplot.bar(ind + width\/2, test_results, width=width, label='Test Error')\n    fig.set_size_inches(15,8)\n    pyplot.legend()\n    ax.set_xticks(ind)\n    ax.set_xticklabels(names)\n    pyplot.show()\nalgorithm_comparison()","9a00d825":"#using arima and lstm models\nX_train_ARIMA = X_train.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA','VIXCLS']]\nX_test_ARIMA = X_test.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA','VIXCLS']]\ntr_len = len(X_train_ARIMA)\nte_len = len(X_test_ARIMA)\nto_len = len (X)","83e6dfa5":"modelARIMA = ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[1,0,0])\nmodel_fit = modelARIMA.fit()","993e81d0":"error_Training_ARIMA = metrics.mean_squared_error(Y_train, model_fit.fittedvalues)\npredicted = model_fit.predict(start = tr_len -1 ,end = to_len -1,exog = X_test_ARIMA)[1:]\nerror_Test_ARIMA = metrics.mean_squared_error(Y_test,predicted)\nerror_Test_ARIMA","875a2bd3":"seq_len = 2 #Length of the seq for the LSTM\nY_train_LSTM, Y_test_LSTM = np.array(Y_train)[seq_len-1:], np.array(Y_test)\nX_train_LSTM = np.zeros((X_train.shape[0]+1-seq_len, seq_len, X_train.shape[1]))\nX_test_LSTM = np.zeros((X_test.shape[0], seq_len, X.shape[1]))\nfor i in range(seq_len):\n    X_train_LSTM[:, i, :] = np.array(X_train)[i:X_train.shape[0]+i+1-seq_len, :]\n    X_test_LSTM[:, i, :] = np.array(X)\\\n    [X_train.shape[0]+i-1:X.shape[0]+i+1-seq_len, :]","2f785ad5":"# LSTM Network\ndef create_LSTMmodel(learn_rate = 0.01, momentum=0):\n    # create model\n    model = Sequential()\n    model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],X_train_LSTM.shape[2])))\n    #More cells can be added if needed\n    model.add(Dense(1))\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss='mse', optimizer='adam')\n    return model\nLSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\nLSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM,validation_data=(X_test_LSTM, Y_test_LSTM),epochs=400, batch_size=72, verbose=0, shuffle=False)\n","7be179ed":"pyplot.plot(LSTMModel_fit.history['loss'], label='train', )\npyplot.plot(LSTMModel_fit.history['val_loss'], '--',label='test',)\npyplot.legend()\npyplot.show()","eead57c3":"error_Training_LSTM = metrics.mean_squared_error(Y_train_LSTM,LSTMModel.predict(X_train_LSTM))\npredicted = LSTMModel.predict(X_test_LSTM)\nerror_Test_LSTM = metrics.mean_squared_error(Y_test,predicted)","822ebf21":"#comparing time series and deep learning results\ntest_results.append(error_Test_ARIMA)\ntest_results.append(error_Test_LSTM)\ntrain_results.append(error_Training_ARIMA)\ntrain_results.append(error_Training_LSTM)\nnames.append(\"ARIMA\")\nnames.append(\"LSTM\")","9e4569be":"algorithm_comparison()","f824d35d":"def evaluate_arima_model(arima_order):\n    modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=arima_order)\n    model_fit = modelARIMA.fit()\n    error = mean_squared_error(Y_train, model_fit.fittedvalues)\n    return error\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.7f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))\n# evaluate parameters\np_values = [0, 1, 2]\nd_values = range(0, 2)\nq_values = range(0, 2)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)","d094b4d6":"# prepare model\nmodelARIMA_tuned = ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[2,0,1])\nmodel_fit_tuned = modelARIMA_tuned.fit()\n# estimate accuracy on validation set\npredicted_tuned = model_fit.predict(start = tr_len -1 ,end = to_len -1, exog = X_test_ARIMA)[1:]\nprint(metrics.mean_squared_error(Y_test,predicted_tuned))","8c440db8":"# plotting the actual data versus predicted data\npredicted_tuned.index = Y_test.index\npyplot.plot(np.exp(Y_test).cumprod(), 'r', label='actual',)\npyplot.ylabel('Logarithm of difference in share price per week')\npyplot.xlabel('Date')\n# plotting t, a separately\npyplot.plot(np.exp(predicted_tuned).cumprod(), 'b--', label='predicted')\npyplot.legend()\npyplot.rcParams[\"figure.figsize\"] = (8,5)\npyplot.grid()\npyplot.show()\n","331e6ee8":"## Data Exploration","099a664f":"## Imports","b0edad29":"## Testing and Comparing Models","ff7d5fb7":"## Loading\/Transforming Data","3d55e3e6":"## Objective\n#### Predict Microsoft (MSFT) Stock price change per week\n\n### Useful variables:\n\n* Technical Indicators\n* Correlated assets\n* Fundamental analysis (Performance reports and News)\n\n## Data used\n###  Stocks\n* IBM and GOOGL\n\n### Currency\n* USD\/JPY and GBP\/USD\n\n### Indices\n* S&P 500, Dow Jones and VIX"}}