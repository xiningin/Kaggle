{"cell_type":{"b9f123f9":"code","623c7661":"code","3a734cd8":"code","ed9b7724":"code","730e5fe1":"code","05685430":"code","9ce76a3c":"code","02b4109d":"code","911cbe3e":"code","8f670a1e":"code","4378939b":"code","58830c99":"code","961f0c23":"code","ad35a675":"code","c0ba9390":"code","ed734d70":"code","f47c0133":"code","96c28bb8":"markdown","7315ff71":"markdown","159389f8":"markdown","90b6b9da":"markdown","a53f02ad":"markdown","1b44fe53":"markdown"},"source":{"b9f123f9":"## This is a study of the data which found in Kaggle, Jan flight data in 2019 and 2020\n## Random Forest model used to try to predict the target flight will be delayed or not\n## By using the random forest also can find out the feature importance.\n\n## Finally, the random forest can achieve in 0.94 precision and 0.50 recall test data, \n## F1 around 0.66\n## Some importance found, for example, the previous flight depature time is most\n## important feature, and the current fligh time block is also important feature.\n\n\nimport pandas as pd\nimport os, re\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport copy\n\nrootpath = '\/kaggle\/input\/flight-delay-prediction'\ndatapath = os.path.join(rootpath)\nre_get_year = re.compile('Jan_(\\d{4})')\ndatafilelist = [ (re_get_year.findall(i)[0], i , pd.read_csv(os.path.join(datapath, i)).loc[:, :\"DISTANCE\"]  ) for i in os.listdir(datapath)]","623c7661":"def is_delay(df):\n    if ((df['ARR_DEL15'] == 1) or (df['DEP_DEL15'] == 1)) and ((df['CANCELLED']==0) & (df['DIVERTED']==0)):\n        return 1\n    else:\n        return 0","3a734cd8":"def get_statistic(df):\n    cancel_num = df.loc[df['CANCELLED']==1].shape[0]\n    divert_num = df.loc[df['DIVERTED']==1].shape[0]\n    total_num = df.loc[:, :].shape[0]\n    norm_num = total_num - cancel_num - divert_num\n    return { \n             'cancelled' : [cancel_num , cancel_num*1.\/total_num], \n             'diverted' : [divert_num , divert_num*1.\/total_num],\n             'norm_num' : [norm_num , norm_num*1.\/total_num]\n            }","ed9b7724":"def generate_value(unique_df, columns_name=None): \n    col_name = 'default' if columns_name is None else columns_name\n    tmp = {\n           'orig_v' : [], \n           'map_v' : []\n      }\n    for k,i in enumerate(unique_df):\n        tmp['orig_v'].append(i)\n        tmp['map_v'].append(k)\n    df_tmp = pd.DataFrame(tmp)\n    df_tmp.columns = [col_name , col_name +'_map'] \n    return df_tmp\n\n\ndef precision_recall(model, X, y_true):\n    test = pd.DataFrame({ 'predict':list(clf.predict(X)) , 'true' : list(y_true) } )\n    tp = test[ ((test['predict'] == 1) | (test['predict'] == 2)) & ((test['true']==1) | (test['true']==2) ) ].shape[0]\n    fp = test[ ((test['predict'] == 1) | (test['predict'] == 2)) & (test['true']==0)].shape[0]\n    tn = test[ (test['predict'] == 0) & (test['true']==0)].shape[0]\n    fn = test[ (test['predict'] == 0) & ((test['true']==1) | (test['true']==2) )].shape[0]\n    print(tp, fp, tn , fn)\n    precision = tp*1. \/ (tp+fp)\n    recall = tp*1. \/ (tp+fn)\n    acc = (tp+tn)*1. \/(tp+tn+fn+fp)\n    f1 = 2.*(precision * recall)\/(precision + recall)\n    return tp, fp, tn, fn , precision, recall, f1,acc\n\ndef metric_calc(model, X_train, Y_train, X_test, Y_test): \n    _,_,_,_, p_train, r_train, f_train, a_train = precision_recall(clf, X_train, Y_train)\n    _,_,_,_, p_test, r_test, f_test, a_test = precision_recall(clf, X_test, Y_test)\n    importance = list(model.feature_importances_)\n    importance = list(zip(list(X_train.columns), importance))\n    return p_train, r_train, f_train, a_train ,  p_test, r_test, f_test, a_test, importance","730e5fe1":"datafilelist[0][2]['is_delay']  = datafilelist[0][2].apply(lambda x: is_delay(x), axis=1)\ndatafilelist[1][2]['is_delay']  = datafilelist[1][2].apply(lambda x: is_delay(x), axis=1)\ndatafilelist[0][2]['Year'] = int(datafilelist[0][0])\ndatafilelist[1][2]['Year'] = int(datafilelist[1][0])\ndata = pd.concat( [datafilelist[0][2] , datafilelist[1][2]])\n\n\nop_map = generate_value(data['OP_CARRIER'].unique(), columns_name='op_carrier')\ntail_num_map = generate_value(data['TAIL_NUM'].unique(), columns_name='tail_num')\norigin_map = generate_value(data['ORIGIN'].unique(), columns_name='origin')\ndest_map = generate_value(data['DEST'].unique(), columns_name='dest')\ndep_time_blk_map = generate_value(data['DEP_TIME_BLK'].unique(), columns_name='dep_time_blk')\n\n\n\ndata_TMP = data.loc[: , [ 'Year' , 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'OP_CARRIER', 'TAIL_NUM', 'ORIGIN', 'DEST', 'DEP_TIME_BLK', 'DISTANCE','DEP_TIME', 'ARR_TIME', 'is_delay', 'DEP_DEL15', 'ARR_DEL15']]  \ndata_TMP = data_TMP.merge(op_map, how='left', left_on='OP_CARRIER' , right_on='op_carrier' ) \ndata_TMP = data_TMP.merge(tail_num_map, how='left' , left_on='TAIL_NUM' , right_on = 'tail_num')\ndata_TMP = data_TMP.merge(origin_map, how='left' , left_on='ORIGIN' , right_on = 'origin')\ndata_TMP = data_TMP.merge(dest_map, how='left' , left_on='DEST' , right_on = 'dest')\ndata_TMP = data_TMP.merge(dep_time_blk_map, how='left' , left_on='DEP_TIME_BLK' , right_on = 'dep_time_blk')\n\n\n\ndep_delay_predict_vector = ['Year' , 'DAY_OF_MONTH' , 'DAY_OF_WEEK' , 'op_carrier_map' , 'origin_map' , 'dest_map' , 'dep_time_blk_map' , 'DEP_TIME', 'DISTANCE', 'DEP_DEL15' ] \ndep_delay_predict_vector_rename = ['year' , 'daymonth' , 'dayweek' , 'opcarrier', 'origin' , 'dest' , 'deptimeblk', 'dep_time' ,'distance' , 'depdelay' ] \ndelay_predict_data = data_TMP.loc[ ~data_TMP['DEP_DEL15'].isnull(), dep_delay_predict_vector]\ndelay_predict_data.columns = dep_delay_predict_vector_rename","05685430":"\ndelay_predict_data = delay_predict_data.sort_values( ['origin', 'year', 'daymonth', 'dep_time'] ).reset_index().drop('index', axis=1)\ndelay_predict_data2 = copy.deepcopy(delay_predict_data)\ndelay_predict_data2=delay_predict_data2.groupby(\"origin\").shift(1)\ndelay_predict_data2.columns = ['p_' + i for i in delay_predict_data2.columns] \ndelay_predict_data2 = delay_predict_data2.merge(delay_predict_data, how='left' , left_index=True, right_index=True)\n\nX =  delay_predict_data2.loc[:, :'distance']\nY = delay_predict_data2.loc[:, 'depdelay'] \n\n# set all nul value into 0\nX.loc[X.p_year.isnull(), 'p_year'] = 0\nX.loc[X.p_daymonth.isnull(), 'p_daymonth'] = 0 \nX.loc[X.p_dayweek.isnull(), 'p_dayweek'] = 0\nX.loc[X.p_opcarrier.isnull(), 'p_opcarrier'] = 0\nX.loc[X.p_dest.isnull(), 'p_dest'] = 0\nX.loc[X.p_deptimeblk.isnull(), 'p_deptimeblk'] = 0\nX.loc[X.p_dep_time.isnull(), 'p_dep_time'] = 0\nX.loc[X.p_distance.isnull(), 'p_distance'] = 0\nX.loc[X.p_depdelay.isnull(), 'p_depdelay'] = 0\nX = X.drop(['p_year' , 'p_daymonth' , 'p_dayweek' , 'p_dest' , 'p_distance', 'dep_time'] , axis=1)\n\nprint('Number of feature : {}\\nNumber of data : {}'.format(X.shape[1], X.shape[0]))","9ce76a3c":"## Proccsing the split the data into testset(30%) and trainset (70%)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=25)\n\n\n## Processing multiple parameters testing and generate a dictinary to record the result\nresult = { \n            'idx' : [],\n            'est' :[],\n            'max_depth' :[], \n            'criterion' : [], \n            'class_weight' : [],\n            'train_precision' : [],\n            'train_recall' : [],\n            'train_f1' : [],\n            'train_acc' : [],\n            'test_precision' : [],\n            'test_recall' : [],\n            'test_f1' : [],\n            'test_acc' : [],\n            'importance': [], \n            'time' : [] \n         } \n\nnum_of_loop = { \n                'estimator' : [50] , \n                'max_depth' : [100],\n                'criterion' : ['entropy'], \n                'class_weight' : [{0:1, 1:10}] \n              } \n\n\nconfig_list = []\n\nfor est in num_of_loop['estimator']: \n    for max_depth in num_of_loop['max_depth']:\n        for criterion in num_of_loop['criterion']:\n            for class_weight in num_of_loop['class_weight']:\n                config_list.append( [est, max_depth, criterion, class_weight] )\n                \nconfig_list = list(enumerate(config_list))\nstart_index = 0\nconfig_list = config_list[start_index:]\n\n\nt=  tqdm(config_list, desc='Bar desc' , leave = True, position=0)\nfor config in t:\n    est = config[1][0]\n    max_depth = config[1][1]\n    criterion = config[1][2]\n    class_weight = config[1][3]\n    \n    clf = RandomForestClassifier(n_estimators=est, \n                                 max_depth=max_depth, \n                                 criterion=criterion, \n                                 random_state=14, \n                                 class_weight=class_weight,\n                                 n_jobs=3)\n    time_check = datetime.now()\n    clf.fit(X_train, Y_train)\n    p_train, r_train, f_train, a_train ,  p_test, r_test, f_test, a_test, importance = metric_calc(clf, X_train, Y_train, X_test, Y_test) \n    time_check = datetime.now() - time_check\n    result['idx'].append(config[0])\n    result['est'].append(est)\n    result['max_depth'].append(max_depth)\n    result['criterion'].append(criterion)\n    result['class_weight'].append(class_weight)\n    result['train_precision'].append(p_train)\n    result['train_recall'].append(r_train)\n    result['train_f1'].append(f_train)\n    result['train_acc'].append(a_train)\n    result['test_precision'].append(p_test)\n    result['test_recall'].append(r_test)\n    result['test_f1'].append(f_test)\n    result['test_acc'].append(a_test)\n    result['importance'].append(importance)\n    result['time'].append(time_check) \n    t.set_description('idx:{}, time:{}\\nestimator:{} , depth:{} , criterion:{}\\np_train:{}, r_train:{}\\np_test:{},r_test:{}\\n'.format(config[0], time_check, est, max_depth, criterion, p_train, r_train, p_test, r_test) )","02b4109d":"resultsample1_seed25 = pd.DataFrame(result)","911cbe3e":"feature_importance = resultsample1_seed25['importance'].values[0]\n_tmp = {'feature' : [] , 'importance' : []}\nfor i in feature_importance: \n    _tmp['feature'].append(i[0])\n    _tmp['importance'].append(i[1])\nfeature_importance = pd.DataFrame(_tmp)\nfeature_importance.sort_values('importance' , ascending=False, inplace=True)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(10, 8))\nplt.title('Feature vs Importance')\nplt.bar(feature_importance.feature, feature_importance.importance)\nplt.xticks(rotation=90)\nplt.ylabel('Importance(sklearn)', size=15)\nplt.xlabel('Feature', size=15)\nplt.show()","8f670a1e":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass Net(nn.Module): \n    def __init__(self, feature_size, hidden_dim=50, dropout = 0.2 ,leakslope=0.3): \n        super(Net, self).__init__()\n        self.leakslope = leakslope\n        self.fc1 = nn.Linear(feature_size, hidden_dim)\n        self.dropout1 = nn.Dropout(p=dropout)\n        self.batchnorm1 = nn.BatchNorm1d(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim\/\/3)\n        self.dropout2 = nn.Dropout(p=dropout)\n        self.batchnorm2 = nn.BatchNorm1d(hidden_dim\/\/3)\n        self.fc3 = nn.Linear(hidden_dim\/\/3, feature_size)\n        self.dropout3 = nn.Dropout(p=dropout)\n        self.batchnorm3 = nn.BatchNorm1d(feature_size)\n        self.fc4 = nn.Linear(feature_size, hidden_dim\/\/5)\n        self.dropout4 = nn.Dropout(p=dropout)\n        self.batchnorm4 = nn.BatchNorm1d(hidden_dim\/\/5)\n        self.fc_final = nn.Linear(hidden_dim\/\/5, 1)\n   \n    def activation(self, x):\n        act = torch.nn.LeakyReLU(self.leakslope)\n        return act(x)\n    \n    \n    def forward(self, _input): \n        residual = _input\n        x = self.fc1(_input)\n        x = self.activation(x)\n        x = self.batchnorm1(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.activation(x)\n        x = self.batchnorm2(x)\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        x = self.activation(x)\n        x = self.batchnorm3(x)\n        x = self.dropout3(x)\n        x += residual\n        x = self.fc4(x)\n        x = self.activation(x)\n        x = self.batchnorm4(x)\n        x = self.dropout4(x)\n        x = self.fc_final(x)\n        x = torch.sigmoid(x)\n        return x","4378939b":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=15)\n_X_train= X_train.to_numpy()\n_X_max = _X_train.max(axis=0)\n_X_min = _X_train.min(axis=0)\n_X_train = (_X_train - _X_min) \/ (_X_max - _X_min)\n_X_test= X_test.to_numpy()\n_X_max = _X_test.max(axis=0)\n_X_min = _X_train.min(axis=0)\n_X_test = (_X_test - _X_min) \/ (_X_max - _X_min)\ntrain_X = torch.tensor(_X_train).float()\ntest_X = torch.tensor(_X_test).float()\ntest_Y = torch.tensor(Y_test.to_numpy()).float()\ntrain_Y = torch.tensor(Y_train.to_numpy()).float()","58830c99":"# If can run around 150 epoch, the precision can achieve 90% and recall have achieve 0.5\n\nhidden_dim = 250\ndropout = 0.001\nleakslope = 0.1\nbatch_size = 1000\nepoch = 10 \nlr = 1e-1\nmomentum = (0.99, 0.99)\nshuffle = True\n\n\ntmp_Y = train_Y.reshape((-1,1))\ndataset = torch.cat((train_X, tmp_Y) , 1)\nnet = Net(feature_size=12, hidden_dim=hidden_dim, dropout = dropout, leakslope=leakslope)\n\n\nfrom torch.utils.data import DataLoader\nif torch.cuda.is_available():\n    net = net.to('cuda')\n\nloss_track = []  \nrecall_track = [] \nprecision_track = []\noptimizer = optim.Adam(net.parameters(), lr=lr, betas=momentum)\ncriterion = nn.BCELoss()\n\nif torch.cuda.is_available:\n    criterion.to('cuda')\n\nfor j in tqdm(range(epoch)):\n    dataloader = DataLoader(dataset, batch_size=batch_size ,shuffle=shuffle)\n    net.train()\n    for i in dataloader:\n        t_X = i[:,:-1]\n        t_Y = i[:,-1].reshape(-1, 1)\n            \n        if torch.cuda.is_available():\n            t_X = t_X.to('cuda')\n            t_Y = t_Y.to('cuda')\n        output = net(t_X)\n        optimizer.zero_grad()\n        loss = criterion(output, t_Y)\n        loss.backward()\n        optimizer.step()\n        loss_track.append(loss.to('cpu').tolist())\n    \n    net.eval()\n    predict_test =  net.forward(train_X).reshape(-1).to('cpu').tolist()\n    result = pd.DataFrame({ 'predict':predict_test , 'target': train_Y.reshape(-1)  } )\n    result['type'] = result.apply( lambda x: 'tn' if x['predict'] < 0.5 and x['target'] == 0 else 'fn' if x['predict'] < 0.5 and x['target'] == 1 else 'tp'if x['predict'] > 0.5 and x['target'] == 1 else 'fp',axis=1)\n    check = result\n    result = result.groupby('type').count()['predict']\n    try:\n        recall = result['tp'] *1.  \/ (result['tp']    + result['fn'])\n    except: \n        recall = 0\n    \n    try:\n        precision = result['tp'] *1.  \/ (result['tp']    + result['fp'])\n    except: \n        precision = 0 \n        \n    recall_track.append(recall)\n    precision_track.append(precision)","961f0c23":"net.eval()\npredict_test =  net.forward(train_X).reshape(-1).to('cpu').tolist()\nimport matplotlib.pyplot as plt \n%matplotlib inline\nplt.hist(predict_test)","ad35a675":"result = pd.DataFrame({ 'predict':predict_test , 'target': train_Y.reshape(-1).tolist()  } )\nresult['final_predict'] = result.apply( lambda x: 1 if x['predict'] > 0.5 else 0 , axis=1) \ntp = result[ (result['final_predict'] == 1) & (result['target'] == 1)].count()\npp = result[ (result['final_predict'] == 1) ].count()\nap = result[ (result['target'] == 1) ].count()\n\nfinal_loss = loss.item()\nprecision = (tp*1.\/pp)['target']\nrecall = (tp*1.\/ap)['target']\naccuracy = (result[result['target'] == result['final_predict'] ].count() *1.  \/ result.shape[0])['target']\n\nprint('precision : {} , recall: {} , accuracy:{} , loss:{}' .format(precision, recall, accuracy, final_loss))","c0ba9390":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(loss_track)\nplt.show()\n\nplt.plot(precision_track)\nplt.show()\n\n\nplt.plot(recall_track)\nplt.show()","ed734d70":"import numpy as np\ny = net.forward(test_X)\ny = y.tolist()\ny = np.array(y).reshape(-1).tolist()\ntt_Y = test_Y.tolist()","f47c0133":"result = pd.DataFrame({ 'predict':y , 'target': test_Y.reshape(-1).tolist()  } )\nresult['final_predict'] = result.apply( lambda x: 1 if x['predict'] > 0.5 else 0 , axis=1) \ntp = result[ (result['final_predict'] == 1) & (result['target'] == 1)].count()\npp = result[ (result['final_predict'] == 1) ].count()\nap = result[ (result['target'] == 1) ].count()\nprint('precision')\nprint(tp*1.\/pp)\nprint('\\n\\nrecall')\nprint(tp*1.\/ap)","96c28bb8":"# Processing: create the feature vector for models\n# Append the cloest flight information to the table finally the feature will become\n# vector = [previous flight inforamtion (depature time blk and delay or not)  , this flight information]","7315ff71":"# Function generate the unique value to investigate the data","159389f8":"    \n## Function to investigate how many canceled data and delay data","90b6b9da":"# **Neural Network Module**","a53f02ad":"\nProcessing the two years 2019 and 2020 data to a single pandas\nProcessing coding the category to numerical value\nProcessing merge the coded result to the pandas table\nProcessing: extract the wanted columns for flight delay prediction","1b44fe53":"## Function to elimintate the Cancelled and diverted data"}}