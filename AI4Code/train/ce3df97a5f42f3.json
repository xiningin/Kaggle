{"cell_type":{"363481da":"code","d8a7c216":"code","215d3153":"code","967c9106":"code","7ca8fea9":"code","1a246e60":"code","e7a5aec7":"code","4d956202":"code","540b4c8d":"code","e4bb1713":"code","f33500a8":"code","eeb430f4":"code","c7eedf6a":"code","14bc6aaf":"code","ce75de9c":"code","96c73a35":"code","004fb3f8":"code","336a7f39":"code","67eb814c":"code","139f180c":"code","d440f05a":"code","8f97b993":"code","a95e2845":"code","3e2c1be7":"code","21e036bd":"code","18da7a52":"code","de8604d1":"code","d4f64ff2":"code","63298d0c":"code","f36b3d2e":"code","48b0ad25":"code","8cfc2a79":"code","1ae1994e":"code","a3abe708":"code","5b051ee5":"code","c3982e33":"code","72f32994":"code","2c67349a":"code","668831cc":"code","f9a526d1":"code","e27c44df":"code","cc2ff5b2":"code","4ef41221":"code","2bb3613d":"code","4a8bce7b":"code","35e4bc5d":"code","55f94e25":"code","12c08f25":"code","936fb549":"code","7300a663":"code","9c726c8c":"code","b6f6e6da":"code","5f5643ef":"markdown","70aa5dff":"markdown","4d62c4cb":"markdown","82c65c12":"markdown","f9543b1f":"markdown","8b938e33":"markdown","3ee2004b":"markdown"},"source":{"363481da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8a7c216":"# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# scikit-learn is a rich ML library\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# library of XGBoost algorithm. You can find one in scikit-learn, too.\nimport xgboost as xgb\n\n# library for regex\nimport re\n\n# library for garbage collection\nimport gc\ngc.enable()\n\n# Let's ignore the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","215d3153":"%%time\n# TransactionID is the key column. We define it as the index column for ease of use.\ntr_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\nts_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')\ntr_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\nts_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\nprint(\"Data Loaded!\")","967c9106":"# Let's see the size of the tables\nprint(\"train transaction shape:\", tr_transaction.shape)\nprint(\"test transaction shape:\", ts_transaction.shape)\nprint(\"train identity shape:\", tr_identity.shape)\nprint(\"test identity shape:\", ts_identity.shape)","7ca8fea9":"# How is the data distributed among the classes?\nsns.countplot(tr_transaction['isFraud'], palette='Pastel1')","1a246e60":"# What portion of data in transaction table is missing?\nmissing_values_count = tr_transaction.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(tr_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (f'{round((total_missing\/total_cells) * 100, 2)}% of transaction data is missing!')","e7a5aec7":"# What portion of data in identity table is missing?\nmissing_values_count = tr_identity.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(tr_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (f'{round((total_missing\/total_cells) * 100, 2)}% of identity data is missing!')","4d956202":"# What portion of transactions have an identity record?\nprint(f'{round(np.sum(tr_transaction.index.isin(tr_identity.index.unique())) \/ len(tr_transaction) *100, 2)}% of transactions have identity.')","540b4c8d":"# What is the distribution of transaction's date-time?\nfig = px.histogram(tr_transaction, x='TransactionDT', color='isFraud', marginal='box')\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.6)\nfig.show()","e4bb1713":"# Let's plot it in logarithm scale for a better insight\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 1]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionDT, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 0]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","f33500a8":"# Do the date-times of transactions in train and test overlap?\nplt.hist(tr_transaction['TransactionDT'], label='train')\nplt.hist(ts_transaction['TransactionDT'], label='test')\nplt.legend()\nplt.title(\"Histogram of transaction datetime\")","eeb430f4":"# What is the distribution of transactions' amount?\nfig = px.histogram(tr_transaction, x='TransactionAmt', color='isFraud', marginal='box')\nfig.update_layout(barmode='overlay')\nfig.update_traces(opacity=0.6)\nfig.show()","c7eedf6a":"gc.collect()","14bc6aaf":"# Let's plot in logarithm scale for a better insight\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = tr_transaction.loc[tr_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","ce75de9c":"# M is a categorical feature. Let's explore it.\nfig, axes = plt.subplots(3, 3, figsize=(20, 20))\nfig.suptitle(\"Value Counts in M features\")\nfor i in range(3):\n    for j in range(3):\n        sns.countplot(data=tr_transaction, x=f'M{3*i+j+1}', hue='isFraud', ax=axes[i,j])","96c73a35":"# ProductCD is a categorical feature\nsns.countplot(data=tr_transaction, x=\"ProductCD\", hue='isFraud')","004fb3f8":"# How many unique values are there in each card feature in the train set?\nplt.figure(figsize=(35, 8))\nfeatures = [f'card{i}' for i in range(1, 7)]\nuniques = [len(tr_transaction[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","336a7f39":"# How many unique values are there in each card feature in the test set?\nplt.figure(figsize=(35, 8))\nfeatures = [f'card{i}' for i in range(1, 7)]\nuniques = [len(ts_transaction[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","67eb814c":"# Visualizing card4 categories\nsns.countplot(data=tr_transaction, x='card4', hue='isFraud')","139f180c":"# Visualizing card6 categories\nsns.countplot(data=tr_transaction, x='card6', hue='isFraud')","d440f05a":"# How many unique values are there in id features in the train set?\nplt.figure(figsize=(35, 8))\nfeatures = list(tr_identity.columns[0:38])\nuniques = [len(tr_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","8f97b993":"# How many unique values are there in id features in the test set?\nplt.figure(figsize=(35, 8))\nfeatures = list(ts_identity.columns[0:38])\nuniques = [len(ts_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='unique count', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","a95e2845":"tr_identity.head()","3e2c1be7":"# Are there categories that exist in the test set but not in the train?\nfor ft in features[11:]:\n    print(\"Feature:\", ft)\n    print(set(ts_identity[ft].unique()).difference(set(tr_identity[ft.replace('-', '_')].unique())))\n    print(\"*\"*40)","21e036bd":"tr_transaction['P_emaildomain'].unique()","18da7a52":"tr_transaction['R_emaildomain'].unique()","de8604d1":"tr_transaction['addr1'].unique()","d4f64ff2":"tr_transaction['addr2'].unique()","63298d0c":"# Visualizing DeviceType feature\nsns.countplot(data=tr_identity, x='DeviceType')","f36b3d2e":"tr_identity['DeviceInfo'].unique()","48b0ad25":"# id features are not named unifyingly in the train and test and should be fixed.\nts_identity.rename(columns={x: x.replace('-', '_') for x in ts_identity.columns[:38]}, inplace=True)","8cfc2a79":"# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage_numeric(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef reduce_mem_usage_cat(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type == object and col not in ['DeviceInfo', 'id_30', 'id_31']:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","1ae1994e":"# Function to drop some columns\ndef columns2drop(df):\n    drop_list = ['id_33', 'P_emaildomain', 'R_emaildomain', 'TransactionDT']\n    for col in df.columns:\n        if df[col].isnull().sum() \/ df.shape[0] > 0.9:\n            drop_list.append(col)\n    return drop_list","a3abe708":"# Function to split the email domains into separate columns\ndef split_email_domains(df):\n    df[['P_emaildomain1', 'P_emaildomain2', 'P_emaildomain3']] = df['P_emaildomain'].str.split('.', expand=True)\n    df[['R_emaildomain1', 'R_emaildomain2', 'R_emaildomain3']] = df['R_emaildomain'].str.split('.', expand=True)\n    for x in ['R', 'P']:\n        for i in range(1, 4):\n            df[f'{x}_emaildomain{i}'].fillna('', inplace=True)","5b051ee5":"# Function to generate a numerical column from id_33 which apparently contains string of dimensions\ndef split_id33(df):\n    name = \"id-33\"\n    if not 'id-33' in df.columns:\n        name = \"id_33\"\n    df[[\"height\", \"width\"]] = df[name].str.split('x', expand=True)\n    df['height'].fillna(0, inplace=True)\n    df['width'].fillna(-1, inplace=True)\n    df['aspect_ratio'] = df['height'].astype('uint16') \/ df['width'].astype('uint16')\n    df.drop(['height', 'width'], axis=1, inplace=True)","c3982e33":"# function to apply the preprocessing\ndef preprocessing(df, drop_list):\n    split_id33(df)\n    split_email_domains(df)\n    \n    df['Transaction_day_of_week'] = np.floor((df['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n    df['Transaction_hour'] = np.floor(df['TransactionDT'] \/ 3600) % 24\n    \n    df.drop(drop_list, axis=1, inplace=True)","72f32994":"# Functions to handle tfidf vectorization\ndef tokenizer(x):\n    return re.split(' ._-\/', x)\n\n\ndef tfidf_vectorizer(train_df, test_df, col):\n    train_df[col].fillna('Unknown', inplace=True)\n    test_df[col].fillna('Unknown', inplace=True)\n\n    tfidf = TfidfVectorizer(decode_error='replace', lowercase=True, strip_accents='ascii', analyzer='char_wb', tokenizer=tokenizer)\n    v = tfidf.fit_transform(train_df[col])\n    w = tfidf.transform(test_df[col])\n    \n    tr_tfidf = pd.DataFrame.sparse.from_spmatrix(v, index=train_df.index, columns=[f'{col}_{i}' for i in tfidf.vocabulary_])\n    ts_tfidf = pd.DataFrame.sparse.from_spmatrix(w, index=test_df.index, columns=[f'{col}_{i}' for i in tfidf.vocabulary_])\n    \n    for col in tr_tfidf.columns:\n        tr_tfidf[col] = tr_tfidf[col].values.to_dense().astype(np.float16)\n        ts_tfidf[col] = ts_tfidf[col].values.to_dense().astype(np.float16)\n\n    tr = pd.concat([train_df, tr_tfidf], axis=1)\n    del tr_tfidf, train_df\n    ts = pd.concat([test_df, ts_tfidf], axis=1)\n    del ts_tfidf, test_df\n    \n    gc.collect()\n    \n    return tr, ts","2c67349a":"# Join the two tables\ntrain = tr_transaction.merge(tr_identity, how='left', left_index=True, right_index=True)\ny_train = train['isFraud'].astype('uint8').copy()\ndel tr_transaction, tr_identity\n\ntest = ts_transaction.merge(ts_identity, how='left', left_index=True, right_index=True)\ndel ts_transaction, ts_identity\n\nprint(f\"Shape of train data: {train.shape}, Shape of test data: {test.shape}\")\n\ntrain.head()","668831cc":"gc.collect()","f9a526d1":"X_train = train.drop('isFraud', axis=1)\ndel train\ngc.collect()","e27c44df":"drop_list = columns2drop(X_train)\npreprocessing(X_train, drop_list)","cc2ff5b2":"preprocessing(test, drop_list)","4ef41221":"X_train = reduce_mem_usage_numeric(X_train)\ntest = reduce_mem_usage_numeric(test)","2bb3613d":"# Encoding labels in categorical features\nfor f in X_train.columns:\n    if f not in ['DeviceInfo', 'id_30', 'id_31'] and (X_train[f].dtype=='object' or test[f].dtype=='object'):\n        lbl = LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","4a8bce7b":"X_train, test = tfidf_vectorizer(X_train, test, 'DeviceInfo')\nX_train, test = tfidf_vectorizer(X_train, test, 'id_30')\nX_train, test = tfidf_vectorizer(X_train, test, 'id_31')","35e4bc5d":"X_train.drop(['DeviceInfo', 'id_30', 'id_31'], axis=1, inplace=True)\ntest.drop(['DeviceInfo', 'id_30', 'id_31'], axis=1, inplace=True)","55f94e25":"# Now all the features are numerical. Let's fill the missings with -1.\nX_train.fillna(-1, inplace=True)\ntest.fillna(-1, inplace=True)","12c08f25":"# Just to ensure no other category is left and if so, reduce its memory\nX_train = reduce_mem_usage_cat(X_train)\ntest = reduce_mem_usage_cat(test)","936fb549":"NFOLDS = 5\nkf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=1400)\n\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nscore = 0\n  \nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n    clf = xgb.XGBClassifier(  # For more info about the parameters, visit https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n        n_estimators=500,\n        max_depth=12,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        gamma = 0.2,\n        alpha = 5,\n        missing=-1,\n        tree_method='gpu_hist'\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n    y_preds += clf.predict_proba(test)[:,1] \/ NFOLDS\n    \n    # delete the excess memory\n    del X_tr, X_vl, y_tr, y_vl\n    gc.collect()\n    \n    \nprint(\"\\nMEAN AUC = {}\".format(score))\nprint(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof)))  # OOF stands for out-of-fold","7300a663":"# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean', ascending=False).head(25).plot(kind='bar', figsize=(30, 7))","9c726c8c":"# delete the excess memory\ndel clf, importance_df\ngc.collect()","b6f6e6da":"# Prepare for submission\nsub = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\nsub['isFraud'] = y_preds\nsub.to_csv('submission.csv')\nsub.head()","5f5643ef":"# Resources\n\n[Fraud complete EDA](https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\/notebook)\n\n[EDA for CIS Fraud Detection](https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection)\n\n[~Almost~ complete Feature Engineering IEEE data](https:\/\/www.kaggle.com\/kabure\/almost-complete-feature-engineering-ieee-data)\n\n[Extensive EDA and Modeling XGB Hyperopt](https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt)","70aa5dff":"# Feature Engineering","4d62c4cb":"# Build Model and Evaluate","82c65c12":"Let's take a look at unique values in some other categorical features","f9543b1f":"# Explore Data","8b938e33":"# Libraries","3ee2004b":"# Load Data"}}