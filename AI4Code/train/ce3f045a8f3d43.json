{"cell_type":{"a402220d":"code","d9ba8925":"code","f82d40d5":"code","5de65d7f":"code","04001faf":"code","26fc741c":"code","91829a2b":"code","19769993":"code","ef901ad8":"code","c822c460":"code","61abfcc7":"code","f861f310":"code","f213bc78":"code","bb52d928":"code","461a0e3c":"code","19feaa6c":"code","32b8b27b":"code","473eb69d":"code","de7cefd3":"code","1b11608c":"code","8664e7c2":"code","4e4b46cf":"code","589aee4c":"code","a6423f75":"code","7099ba61":"code","e04fc693":"code","90e4d72b":"code","e72edf5b":"code","548d8642":"code","dd6be84f":"code","50965c14":"code","4e05de9c":"code","0c4254a9":"code","e3625a62":"code","86acd421":"code","862452f3":"code","e6ed19de":"code","bf43650f":"code","9a28d0c7":"code","4133f4d6":"code","e2f37c26":"code","dd8bee29":"code","a1fceb64":"code","6693c260":"code","5867210f":"code","56352ed6":"code","38dafb35":"code","04b62491":"code","c8735e60":"code","09b2be88":"code","8cf99a73":"code","cd5a91d0":"code","c167887f":"code","23e3fedf":"code","01aec17a":"code","8d7a138f":"code","3745b2c4":"code","e925bc16":"code","7e53617d":"code","d2cb73fc":"code","0d63f977":"code","a8f972ba":"code","18af30e9":"code","e3828d3c":"code","eeb55ecb":"code","ac34e4e8":"code","26021bb5":"code","881c5556":"code","42f41d7a":"code","56d81eb9":"code","b40ccedf":"code","7e6167ee":"code","8034616f":"code","c12eee56":"code","33a36f5c":"code","42c30eb8":"code","0a74af93":"code","aef0c8d0":"code","bfafb714":"code","58fe86de":"code","a4ba7893":"code","ae0baa1f":"code","ec11ab82":"code","f5f6e0b1":"code","cc7d08cb":"code","b5590b10":"code","c4212a19":"code","583caeb8":"markdown","605bc719":"markdown","01f111f5":"markdown","6e0d92ea":"markdown","d2ef0347":"markdown","1776857e":"markdown","4f93efb7":"markdown","7ad8bee3":"markdown","efb73692":"markdown","d54d3171":"markdown","dc03991a":"markdown","ccdbc1a4":"markdown","f1c760c3":"markdown","a2cb7a2c":"markdown","dc3a098f":"markdown","17d6a92b":"markdown","7e366174":"markdown","a2ba4eda":"markdown","cccf1131":"markdown","628ab92c":"markdown","fb3c3c3d":"markdown","8d0344da":"markdown","b9e40924":"markdown","0a2fa26b":"markdown","cde02a27":"markdown","391c4843":"markdown","b9d40810":"markdown","5bb74552":"markdown","7999cf70":"markdown","5694d71c":"markdown","75b5bbc2":"markdown","eefb8ee4":"markdown","840c6ef8":"markdown","6520b0b3":"markdown","866f89b5":"markdown","70aa6055":"markdown","4e02753f":"markdown","975842cd":"markdown","072d202b":"markdown","9f98e983":"markdown","747e14c7":"markdown","d1153a85":"markdown","1ceaee3f":"markdown","5ec2059b":"markdown","fb110ddb":"markdown","e8bff1b1":"markdown","6206073b":"markdown","06614210":"markdown","e45023ba":"markdown","e90f25f7":"markdown","d9cf7812":"markdown","424f847e":"markdown"},"source":{"a402220d":"import pandas as pd\ndata_pd = pd.read_csv(\"..\/input\/b_depressed.csv\")\ndata_pd_original = pd.read_csv(\"..\/input\/b_depressed.csv\")","d9ba8925":"#Describing the datatypes:\ndata_pd.dtypes","f82d40d5":"data_pd.columns","5de65d7f":"data_pd.groupby('depressed').size()\n# People with depression: 1191\n# People with NO depression: 238","04001faf":"# Or if you like the percentages as follows: \nfp = data_pd.groupby('depressed').size()\/data_pd.shape[0]\nprint(fp)","26fc741c":"data_pd.mean(axis=0)","91829a2b":"import matplotlib.pyplot as plt\n\nplt.matshow(data_pd.corr())\nplt.show()\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data_pd.corr(), fignum=f.number)\nplt.xticks(range(data_pd.shape[1]), data_pd.columns, fontsize=14, rotation=45)\nplt.yticks(range(data_pd.shape[1]), data_pd.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('C',fontsize=16);","19769993":"hist = data_pd['Age'].hist()","ef901ad8":"#% matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.countplot(x='sex', data = data_pd) \n# Where woman is 1 and man is 0","c822c460":"sns.factorplot(x='depressed', col='Married', kind='count', data= data_pd)","61abfcc7":"#If we explore the next feature 'no_lasting_investment' there are some rows without values:\ndata_pd.no_lasting_investmen.isnull().sum()","f861f310":"data_pd['no_lasting_investmen'].fillna(data_pd['no_lasting_investmen'].median(), inplace=True)","f213bc78":"data_pd_original['no_lasting_investmen'].fillna(data_pd_original['no_lasting_investmen'].median(), inplace=True)","bb52d928":"data_pd.no_lasting_investmen.isnull().sum()","461a0e3c":"data_pd_original.no_lasting_investmen.isnull().sum()","19feaa6c":"data_pd['no_lasting_investmen'] = data_pd['no_lasting_investmen'].astype(int)","32b8b27b":"data_pd_original['no_lasting_investmen'] = data_pd_original['no_lasting_investmen'].astype(int)","473eb69d":"data_pd","de7cefd3":"columns_to_normalize = [ 'Age', 'Number_children','education_level', 'total_members', 'gained_asset', 'durable_asset',\n       'save_asset', 'living_expenses', 'other_expenses',\n       'incoming_agricultural', 'farm_expenses', \n       'lasting_investment', 'no_lasting_investmen']","1b11608c":"# We use Scipy library zscore to the normalization:\nfrom scipy import stats\n\nfor c in columns_to_normalize:\n    data_pd[c] = stats.zscore(data_pd[c])","8664e7c2":"#Print the data normalizaed \ndata_pd.head()","4e4b46cf":"# Drop By Name:\ndata_pd = data_pd.drop(['Survey_id', 'Ville_id'], axis=1)\ndata_pd.head(5)","589aee4c":"data_pd_original = data_pd_original.drop(['Survey_id', 'Ville_id'], axis=1)","a6423f75":"for c in columns_to_normalize:\n    data_pd[c] = pd.cut(data_pd[c], 5)\n\ndata_pd.head()","7099ba61":"columns_two_bins = ['sex', 'Married',\n       'incoming_own_farm', 'incoming_business','incoming_no_business', \n       'labor_primary', 'depressed']","e04fc693":"for c in columns_two_bins:\n  data_pd[c] = pd.cut(data_pd[c], 2)","90e4d72b":"data_pd.head()","e72edf5b":"total_data = pd.DataFrame()\nfor i in data_pd.columns:\n  data_pd[i] = pd.Categorical(data_pd[i])\n  data_frame_Dummies = pd.get_dummies(data_pd[i], prefix = i)\n  total_data = pd.concat([total_data, data_frame_Dummies], axis=1)","548d8642":"total_data.head()","dd6be84f":"depression_normalized = total_data","50965c14":"depression_normalized","4e05de9c":"#Compute the time that the Algorithm takes in calculates the rules for this excercise of Association:\nimport time\nfrom mlxtend.frequent_patterns import apriori\n\nstart_time = time.time()\nfrequent_apriori = apriori(total_data, min_support=0.65, use_colnames=True)\nprint(\"Execution time by Apriori Algorithm  %s\" % (time.time() - start_time))","0c4254a9":"from mlxtend.frequent_patterns import association_rules\n\nreglas_asociacion_dataset=association_rules(frequent_apriori, metric=\"confidence\", min_threshold=0.8)\ndisplay(reglas_asociacion_dataset.sort_values(by = ['support','confidence'], ascending = [False,False]))\nprint(\"Total reglas encontradas %s\" % (reglas_asociacion_dataset.shape[0]))","e3625a62":"X = data_pd_original.copy()","86acd421":"X.head()","862452f3":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nn_clusters = 1\nkm = KMeans( n_clusters=n_clusters)\nkm.fit(X)\ny = km.predict(X)\nX.shape","e6ed19de":"# It calculates the size of Cluster\npd.Series(y).value_counts()","bf43650f":"# List the centroids of the clusters\nkm.cluster_centers_","9a28d0c7":"data_pd_original.columns","4133f4d6":"# We decide to select some columns to separate the data and We can print the clusters very well created.\n\nX_filtered = X.filter(['save_asset','durable_asset', 'save_asset',\n                      'living_expenses','other_expenses','incoming_agricultural',\n                       'farm_expenses','lasting_investment','no_lasting_investmen']).values+15","e2f37c26":"n_clusters = 3\n\nkm = KMeans( n_clusters=n_clusters)\nkm.fit(X_filtered)\ny_filtered = km.predict(X_filtered)\n\nprint(X_filtered.shape)\nprint(y_filtered.shape)\n\n#Print the lists of the centroids\nkm.cluster_centers_","dd8bee29":"# print the groups of the centroids\n\nimport numpy as np\ncmap = plt.cm.plasma\n\ncmap((y_filtered*255.\/(n_clusters-1)).astype(int))\nfor i in np.unique(y_filtered):\n    cmap = plt.cm.bwr\n    col = cmap((i*255.\/(n_clusters-1)).astype(int))\n    Xr = X_filtered[y_filtered==i]\n    plt.scatter(Xr[:,0], Xr[:,1], color=col, label=\"cluster %d\"%i, alpha=.5)\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1],marker=\"x\", lw=5, s=200, color=\"black\")\nplt.legend()\n","a1fceb64":"Sum_of_squared_distances = []\n\n#It iterates\nK = range(2,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(X_filtered)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.show()","6693c260":"X_filtered_2 = X.filter(['Age', 'Number_children', 'education_level','total_members']).values+15\n\n\ndef plot_kmeans(n_clusters):\n    km = KMeans( n_clusters=n_clusters)\n    km.fit(X_filtered_2)\n    y_filtered_2 = km.predict(X_filtered_2)\n\n    # Dibuja los grupos con sus centroides\n    cmap = plt.cm.plasma\n\n    cmap((y_filtered*255.\/(n_clusters-1)).astype(int))\n    for i in np.unique(y_filtered_2):\n        cmap = plt.cm.bwr\n        col = cmap((i*255.\/(n_clusters-1)).astype(int))\n        Xr = X_filtered_2[y_filtered_2==i]\n        plt.scatter(Xr[:,0], Xr[:,1], color=col, label=\"cluster %d\"%i, alpha=.5)\n    plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1],marker=\"x\", lw=5, s=200, color=\"black\")\n    plt.legend()\n\nplot_kmeans(3)","5867210f":"plot_kmeans(8)","56352ed6":"Sum_of_squared_distances = []\n\n#Se itera \nK = range(2,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(X_filtered_2)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.show()","38dafb35":"from sklearn.metrics import silhouette_samples, silhouette_score","04b62491":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\nfrom sklearn.utils.multiclass import unique_labels\n\nX_filtered_3 = X.copy()\n\ny_true = X_filtered_3['depressed']\nclass_names = ['Depressed','No Depressed']\n\nkm = KMeans(n_clusters=2)\nkm = km.fit(X_filtered_2)\ny_pred = km.labels_\n\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)","c8735e60":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False, cmap=plt.cm.Blues):\n\n    title = 'Confusion Matrix'\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True Label',\n           xlabel='Predicted Label')\n\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"red\" if cm[i, j] > thresh else \"red\")\n    fig.tight_layout()\n    return ax\n\nnp.set_printoptions(precision=2)\nplot_confusion_matrix(y_true, y_pred, classes=class_names)\nplt.show()","09b2be88":"def compute_purity(confusion_matrix):\n    maximus = []\n    purity_score = 0\n    for i in range(0,confusion_matrix.shape[0]):\n        maximus.append(np.max(cm[i]))\n    purity_score = np.sum(maximus)\/np.sum(confusion_matrix)\n    return purity_score","8cf99a73":"purity_score= compute_purity(cm)\nprint(\"The purity calculated is: \", purity_score)","cd5a91d0":"#Evaluar la precisi\u00f3n:\nprecision = precision_score(y_true, y_pred, average='macro') \nrecall = recall_score(y_true, y_pred, average='macro')  \nf1score = f1_score(y_true, y_pred, average='macro')  \n\nprint(\"The Precision calculated was \", precision)\nprint(\"The Recall calculated was: \", recall)\nprint(\"The F1-Score calculated was: \", f1score)","c167887f":"X_copy = X.copy()\ny = X.depressed.values\nX = X.values","23e3fedf":"from sklearn.model_selection import train_test_split\n\n#Se parten los datos para usar 70 Entrenamiento y 30 test:\nX_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = train_test_split(X, y,\n                                                    test_size=.3)","01aec17a":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\nmodel = GaussianNB()\nmodel.fit(X_train_bayes, y_train_bayes)","8d7a138f":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, classification_report\ny_pred_bayes = model.predict(X_test_bayes)","3745b2c4":"cm = confusion_matrix(y_test_bayes, y_pred_bayes)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_bayes, digits=2))","e925bc16":"#Import the libraries required\nfrom sklearn.linear_model import LogisticRegression","7e53617d":"classifier = LogisticRegression(class_weight=\"balanced\", random_state=1, max_iter=1000, solver=\"liblinear\")\n\nclassifier.fit(X_train_bayes, y_train_bayes)\ny_pred_logistic = classifier.predict(X_test_bayes)\nscore = classifier.score(X_test_bayes, y_test_bayes)","d2cb73fc":"cm = confusion_matrix(y_test_bayes, y_pred_logistic)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_logistic, digits=2))","0d63f977":"from sklearn.ensemble import RandomForestClassifier","a8f972ba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","18af30e9":"rf = RandomForestClassifier()","e3828d3c":"rf.fit(X_train,y_train)","eeb55ecb":"y_pred = rf.predict(X_test)","ac34e4e8":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","26021bb5":"report = classification_report(y_test, y_pred)\nprint(report)","881c5556":"from keras.models import Sequential\nfrom keras.layers import Dense","42f41d7a":"model = Sequential() # Creation of the model\nmodel.add(Dense(units= 128, input_dim = 21, activation = 'relu')) # input_dim = Variables or attributes\nmodel.add(Dense(units = 64, activation='relu')) # First activation Layer\nmodel.add(Dense(units = 8, activation='relu')) # Second activation Layer\nmodel.add(Dense(units = 1, activation='sigmoid')) # Sigmoid activation function for binary classification","56d81eb9":"# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","b40ccedf":"y_pred_MLP = model.fit(X_train_bayes, y_train_bayes, \n                       validation_data=([X_test_bayes],[y_test_bayes]) ,epochs=150, batch_size=10)","7e6167ee":"_, accuracy = model.evaluate(X_train_bayes, y_train_bayes)\naccuracy","8034616f":"print(y_pred_MLP.history.keys())\n\n# summarize history for accuracy\nplt.plot(y_pred_MLP.history['accuracy'])\nplt.plot(y_pred_MLP.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c12eee56":"# summarize history for loss\nplt.plot(y_pred_MLP.history['loss'])\nplt.plot(y_pred_MLP.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","33a36f5c":"y_pred_NN = model.predict(X_test_bayes, batch_size=50, verbose=1)","42c30eb8":"y_pred_NN.shape","0a74af93":"y_pred_NN_bool = np.argmax(y_pred_NN, axis=1)","aef0c8d0":"cm = confusion_matrix(y_test_bayes, y_pred_NN_bool)\nprint(cm)\n# Print the precision and recall, among other metrics\nprint(classification_report(y_test_bayes, y_pred_NN_bool, digits=2))","bfafb714":"# Save depressed variable and after pup again,\ny_target = X_copy['depressed'].values","58fe86de":"print(y_target)","a4ba7893":"depression_normalized.values","ae0baa1f":"# To Apply the Random Forest Model Again:\nX = depression_normalized.values\ny = y_target","ec11ab82":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","f5f6e0b1":"from sklearn.model_selection import KFold\n\nindexes = []\nscores = []\ncv = KFold(n_splits=5, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index:\", train_index)\n    print(\"Test  Index:\", test_index)\n    X_train, X_test, y_train, y_test = X[train_index],X[test_index],y[train_index], y[test_index]\n    rf = RandomForestClassifier()\n    rf.fit(X_train,y_train)\n    scores.append(rf.score(X_test, y_test))","cc7d08cb":"folds = [0,1,2,3,4,5]","b5590b10":"print(scores)","c4212a19":"plt.plot(scores)\nplt.title('K fold - Cross Validation')\nplt.ylabel('Acuracy')\nplt.xlabel('Fold')\nplt.legend(['Training for each fold'], loc='upper left')\nplt.show()","583caeb8":"# 4. Modeling","605bc719":"Some conclusions about that, hurriedly We can say that to be married helps to not be depressed according with the before graphics !. But remember We dealing with unbalanced data, the majory of people (>80%) are no depressed.","01f111f5":"## 4.3.3. Multi-Layer Perceptron Neural Network Mode\n\nThe following guides We used to implement this MLP neural network:\n\n[1. tutorial-first-neural-network-python-keras](https:\/\/machinelearningmastery.com\/tutorial-first-neural-network-python-keras\/) <br>\n[2. build-multi-layer-perceptron-neural-network-models-keras](https:\/\/machinelearningmastery.com\/build-multi-layer-perceptron-neural-network-models-keras\/) <br>\n[3. basic-keras-neural-network-sequential-model](https:\/\/www.kdnuggets.com\/2018\/06\/basic-keras-neural-network-sequential-model.html)","6e0d92ea":"For example, according to correlation matrix, We can see a good relationship between \"labor_primary\" and \"incoming_own_farm\". Other good relationship is the following: \"incoming_salary\" and \"landing_investment\", make sense. Finally, We can analyze a histogram to some variable, for instance, the \"age\" variable.","d2ef0347":"4.3.1.1. Training the Model:","1776857e":"###  2.2. Correlation Matrix\nA very common way to describe the data is throught correlation matrix. this is a visual technique to show the relationship between variables od the dataset.\nThe correlation values use to be between -1 and +1. However, in the practice, on the whole the elements have positive correlations.","4f93efb7":"# Data Mining Project\n\nHi there, My name is Diego from Colombia and I want to share with you all my work done with this dataset.\nWe use the CRISP methodology when We apply some of strategies found in this. the main features applied are the following:\n> \n1. Business Understanting\n2. Data Understanding \n3. Data Preparation\n4. Modeling\n5. Evaluation","7ad8bee3":"1. We are going to proce to fill the missing values with the Median:","efb73692":"# 5. Evaluation","d54d3171":"Finally, it computes the Precision, Recall and F1 Score:","dc03991a":"### 4.2.2. Validation Metrics for Clustering\n\n#### 4.2.2.1. Internal Validation Metrics\n\n#### 4.2.2.1. Sum of Squared Errors SSE ","ccdbc1a4":"Applying a confidence of 90% and a minimum of support 65%, find the following rules:","f1c760c3":"Binning in Python and Pandas: We applied Bining or Discretization in the dimentions:\nThe size of binning for each variable is 5. It creates range of data, as follows:","a2cb7a2c":"![](http:\/\/)We have to remove the 'survey_id' and 'Ville_id' columns because this not represents a valid information to extract.","dc3a098f":"### 3.2. Cleaning data: Outliers Detection:\n\nThis dataset is clean, for the outliers values We applied the mean and the variance with the folowing umbral: media +-2 variance.\nThe variable wich had many outliers was the age. More ahead I'll show how to generate new discrete variables to take ranges and not values with a high difference between them. \n","17d6a92b":"The graphic before, shows the main ages where the the most frequent value is age = 30 with 400 times.","7e366174":"Evaluating the Model","a2ba4eda":"Training the model in Keras","cccf1131":"To discretize the binary data to avoid duplicate the quantity o columns","628ab92c":"# 4.3. Classification\nIs this section, We can analyse the supervised learning. Our dataset has a target class, the variable is \"depressed\". Some models have been applied to seek what is the best model to fit to our data. \n\n## 4.3.1. Bayesian Model","fb3c3c3d":"4.3.1.3. Model Validation with Confusion Matrix, Accuracy, Clasiffication error, Precision, Recall and f1-score","8d0344da":"Keras is a framework where you can create the architecture of the network in a very intuitive way, hidding the complex details. The following lines of code shows the Network implementation:","b9e40924":"4.3.1.2. To apply the new model to the Test set","0a2fa26b":"## 4.3.2. Logistic Regression Model","cde02a27":"The results are vey bad for our target class (\"No depressed\"). The accuracy of the model is 86%, but this imbalance dataset We have to check the F1-Score for the second class. In this case, We got 0% for this target class. ","391c4843":"Showing the graphics the curves of learning:","b9d40810":"# 4.2 Clustering\n\nSegmentation of people according to personal features and conditions of life. ","5bb74552":"### 3.1. Cleaning data: Missing values\n\nThis dataset is clean, for the missing values to the numerics fields, We opted to fill them computing the mean and median of all values of this variable and putting in each missing cell.\nThe columns where we applied this technique were the following: 'gained_asset', 'durable_asset', 'save_asset', 'living_expenses', 'other_expenses', 'incoming_agricultural','farm_expenses', and 'lasting_investment'","7999cf70":"### 2.1 Unbalanced classes","5694d71c":"#### 4.2.3.1.1. Purity","75b5bbc2":"### 4.2.3.1. External Metrical Validation\n\nThe validation is throuhg the Confusion Matrix","eefb8ee4":"#### 4.2.2.2. Silhouette Coefficient","840c6ef8":"## 1. Business Understanding\n\nThe dataset is involved into the analysis of depression. The data was consists as a study about the life conditions of people who live in rurales zones.\nBecause all the columns were not explicated in this challenge so We can\u00b4t understand them. We proceded to delete them or ignoring. Fhe final features or columns were the following:\n\n* Survey_id \n* Ville_id\n* sex\n* Age\n* Married\n* Number_children\n* education_level\n* total_members (in the family)\n* gained_asset\n* durable_asset\n* save_asset\n* living_expenses\n* other_expenses\n* incoming_salary\n* incoming_own_farm\n* incoming_business\n* incoming_no_business\n* incoming_agricultural\n* farm_expenses\n* labor_primary\n* lasting_investment\n* no_lasting_investmen\n* depressed: [ Zero: No depressed]  or [One: depressed] (Binary for target class)\n\nthe main objective is to show statistic analysis and some data mining techniques.\nThe dataset has 23 columns o dimensiones y un total de 1432 objetos o\nregistros.\n","6520b0b3":"# 4.1. Asociation Rules\n\nFor this task, it opted to do a normalization of data. The normalization used was z-score (Gaussian normalization). ","866f89b5":"To determine the number of K with SSE (Sum of Square Error)","70aa6055":"Partition of the dataset using 70% to training and 30% to Testing","4e02753f":"Printing the Training for each fold","975842cd":"Some more \"intuitive\" the Confusion Matrix","072d202b":"2. To comprobe that the data was filled","9f98e983":"In this chapter, I want to make in three sections: Asociation, Clustering and Classification. \nAsociation Rules are common ways to search dependencies between the variables doing some changes in support and confidence and find out or extract rules as market basket analysis. You can read mre about that [here](https:\/\/towardsdatascience.com\/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)\n\nWe propose some methods for clustering as K-Means, Hierachical clustering, K-Neighborhoods and some techniques to validation these algorithms like distance intra and extra cluster, similarity validation also. ","747e14c7":"### 4.3.3.1. Defining the model in Keras framework\n\n![](https:\/\/www.luisllamas.es\/wp-content\/uploads\/2019\/02\/tensorflowkeras.png)","d1153a85":"## 2. Data Understanding and Descriptive statistics","1ceaee3f":"K-Means with other Dimentions: \"Age, Number_children, education_level, total_members\"","5ec2059b":"## 3. Data Preparation","fb110ddb":"Elbow Graphic shows that the k estimated value for k is 8 aproximalety.","e8bff1b1":"## 5.1. K-fold Cross Validation","6206073b":"We define the optimizer as the estocastic gradient descent efficient algorithm (adam). \nCompiling the model:","06614210":" Obvisously, Not for all the columns make sense obtain the mean or median. However, We got the mean for all variables with mean function of pandas Data Frame.","e45023ba":"Now, We continue adding new Dummies variables","e90f25f7":"### 4.3.3.2. Normalization of the Dimensions\n\nThe normalization of the data made through z-score (Gaussian normalization).","d9cf7812":"### 4.2.1. K-Means Clustering\nIt executes the k-Means algorithm through a python implementation with the data without normalize. As input parameters, We specify 6 clusters (Number of K)","424f847e":"## 4.3.3. Random Forest Model"}}