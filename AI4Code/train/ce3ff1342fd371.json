{"cell_type":{"c7cb4c34":"code","68ca45be":"code","62157d6e":"code","27a9229d":"code","999ca37a":"code","76007bd8":"code","0fd02e7f":"code","97e00fe3":"code","e607c459":"code","f96ef926":"code","63df1165":"code","40d4f7f7":"code","75cdd641":"code","64946d24":"code","d1828e84":"code","0f0abd96":"code","9f12abae":"code","0e76eda7":"code","059d505d":"code","04e75f41":"code","07a28ad4":"code","a3ba2c8a":"markdown","cab838a9":"markdown","7e99f3f1":"markdown","60c5ad6c":"markdown","05ed8bf7":"markdown","23bbc763":"markdown"},"source":{"c7cb4c34":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","68ca45be":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","62157d6e":"pd.value_counts(data['Class']).plot.bar()\nplt.title('Fraud class histogram')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()","27a9229d":"print('No Frauds = ', data['Class'].value_counts()[0] , ' transactions' ,'--- with a percentage ' ,\n      round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds = ' ,data['Class'].value_counts()[1] , ' transactions' ,' --- with percentage  ', \n      round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\n","999ca37a":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)","76007bd8":"data.head()","0fd02e7f":"#Random shuffling for sample balancing\ndf = data.sample(frac = 1) ","97e00fe3":"# Count how many targets are 1 (fraud transactions)\nnum_one_targets = int(np.sum(df['Class']))\n\n# Setting a counter for targets that are 0 (non-fraud transactions)\nzero_targets_counter = 0\n\n# We want to create a \"balanced\" dataset, so we will have to remove some input\/target pairs.\n# Declaring a variable that will do that:\nindices_to_remove = []\n\n# Count the number of targets that are 0. \n# Once there are as many 0s as 1s, mark entries where the target is 0 then remove it.\nfor i in range(df.shape[0]):\n    if df['Class'][i] == 0:\n        zero_targets_counter += 1\n        if zero_targets_counter > num_one_targets:\n            indices_to_remove.append(i)\n            \n            \n# note that we are removing data with indices before shuffling \n\nbalanced_data = data.drop(data.index[indices_to_remove])\n\nprint(balanced_data['Class'].value_counts())","e607c459":"#Exploring Correlations\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='Spectral', ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \", fontsize=14)\n\n\nsub_sample_corr = balanced_data.corr()\nsns.heatmap(sub_sample_corr, cmap='Spectral',ax=ax2)\nax2.set_title('SubSample Correlation Matrix ', fontsize=14)\nplt.show()","f96ef926":"# Positive correlations (The higher the feature , the higher the probability to be a fraud transaction)\n# so we will remove the extreme outliers to improve the model accuracy for catching fraud transactions\n\nf, axes = plt.subplots( nrows=1 , ncols= 3, figsize=(20,4))\n\n\nsns.distplot(balanced_data['V11'], bins=15, ax=axes[0])\n\n\nsns.distplot(balanced_data['V4'], bins=15, ax=axes[1])\n\n\nsns.distplot(balanced_data['V2'], bins=15, ax=axes[2])\nplt.show()","63df1165":"f, axes = plt.subplots( nrows=1 , ncols= 3, figsize=(20,4))\n\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=balanced_data, palette=\"Set3\", ax=axes[0])\naxes[0].set_title('V11 vs Class positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=balanced_data, palette=\"Set3\", ax=axes[1])\naxes[1].set_title('V4 vs Class positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=balanced_data, palette=\"Set3\", ax=axes[2])\naxes[2].set_title('V2 vs Class positive Correlation')\nplt.show()","40d4f7f7":"# Removing outliers V11 Feature\nV11_fraud = balanced_data['V11'].values\nq25, q75 = np.percentile(V11_fraud, 25), np.percentile(V11_fraud, 75)\n\nV11_iqr = q75 - q25\nV11_cut_off = V11_iqr * 1.5\nV11_lower, V11_upper = q25 - V11_cut_off, q75 + V11_cut_off\n\nprint('V11 Lower: ', V11_lower)\nprint('V11 Upper: ', V11_upper)\noutliers = [x for x in V11_fraud if x < V11_lower or x > V11_upper]\n\nprint('V11 outliers:' , outliers)\nprint('Feature V11 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V11'] > V11_upper) | (balanced_data['V11'] < V11_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))","75cdd641":"# Removing outliers V4 Feature\nV4_fraud = balanced_data['V4'].values\nq25, q75 = np.percentile(V4_fraud, 25), np.percentile(V4_fraud, 75)\n\nV4_iqr = q75 - q25\nV4_cut_off = V4_iqr * 1.5\nV4_lower, V4_upper = q25 - V4_cut_off, q75 + V4_cut_off\n\nprint('V4 Lower: ', V4_lower)\nprint('V4 Upper: ', V4_upper)\noutliers = [x for x in V4_fraud if x < V4_lower or x > V4_upper]\n\nprint('V4 outliers:' , outliers)\nprint('Feature V4 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V4'] > V4_upper) | (balanced_data['V4'] < V4_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))\n","64946d24":"# Removing outliers V2 Feature\n### in this feature i have increased the cut off factor to 2.5 becasue we many outliers in this feature , if we removed all of them we might have the risk of losing too much information ###\nV2_fraud = balanced_data['V2'].values\nq25, q75 = np.percentile(V2_fraud, 25), np.percentile(V2_fraud, 75)\n\nV2_iqr = q75 - q25\nV2_cut_off = V2_iqr * 2.5\nV2_lower, V2_upper = q25 - V2_cut_off, q75 + V2_cut_off\n\nprint('V2 Lower: ', V2_lower)\nprint('V2 Upper: ', V2_upper)\noutliers = [x for x in V2_fraud if x < V2_lower or x > V2_upper]\n\nprint('V2 outliers:' , outliers)\nprint('Feature V2 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V2'] > V2_upper) | (balanced_data['V2'] < V2_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))\n","d1828e84":"print('No Frauds = ', balanced_data['Class'].value_counts()[0] , ' transactions' ,'--- with a percentage ' ,\n      round(balanced_data['Class'].value_counts()[0]\/len(balanced_data) * 100,2), '% of the dataset')\nprint('Frauds = ' ,balanced_data['Class'].value_counts()[1] , ' transactions' ,' --- with percentage  ', \n      round(balanced_data['Class'].value_counts()[1]\/len(balanced_data) * 100,2), '% of the dataset')\n","0f0abd96":"# balancing the dataset again after removing outliers\nnum_one_targets = int(np.sum(balanced_data['Class']))\nzero_targets_counter = 0\nindices_to_remove = []\n\nfor i in range(balanced_data.shape[0]):\n    if balanced_data['Class'].iloc[i] == 0:\n        zero_targets_counter += 1\n        if zero_targets_counter > num_one_targets:\n            indices_to_remove.append(i)\n            \nbalanced_data = balanced_data.drop(balanced_data.index[indices_to_remove])\n\nprint('No Frauds', round(balanced_data['Class'].value_counts()[0]\/len(balanced_data) * 100,2), '% of the dataset')\nprint('Frauds', round(balanced_data['Class'].value_counts()[1]\/len(balanced_data) * 100,2), '% of the dataset')\n","9f12abae":"dataset = balanced_data\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","0e76eda7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","059d505d":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression( random_state = 0)\nclassifier.fit(X_train, y_train)","04e75f41":"from sklearn.metrics import confusion_matrix, accuracy_score , classification_report\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\n\n\ny_actual = [\"non fraud\",\" fraud\"]\ny_predicted = [\"non fraud\",\" fraud\"]\n\ndf_cm = pd.DataFrame(cm, columns=(y_predicted), index = (y_actual))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm,linewidths=3, cmap=\"Set3\", annot=True)\nax1.set_title(\"confusion matrix\", fontsize=14)\nplt.show()\n\n","07a28ad4":"## Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","a3ba2c8a":"# Scaling\n","cab838a9":"# ****using random shuffling for making balanced data****","7e99f3f1":"# - features distributions are gaussian like , with some outliers specially for feature V2\n# - using interquartile range method to remove extreme outliers.\n# - you can see illustration below\n","60c5ad6c":"![image.png](attachment:image.png)","05ed8bf7":"## Exploring the data","23bbc763":"# **data balancing**"}}