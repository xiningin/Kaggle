{"cell_type":{"4c8aceba":"code","97af89e0":"code","bda96aac":"code","52779f94":"code","47675a3c":"code","86fe4166":"code","41725068":"code","11756a91":"code","a49ecad4":"code","efa5a475":"code","b1e280cf":"code","b2aa65fa":"code","cf6a81c4":"code","5ae38123":"code","da5d4cd2":"code","26469696":"code","dd735944":"code","ffd2073c":"code","6e9e20be":"code","308ad678":"code","8a32bc3b":"code","7d50b38a":"code","bcf0288b":"code","6688b92c":"code","380cfe83":"code","7be501b6":"code","f705bf75":"code","6f413fa8":"code","e413a671":"markdown","bde9361e":"markdown"},"source":{"4c8aceba":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport soundfile as sf\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt # to support librosa display\nimport seaborn as sns\nimport IPython.display as ipd # for playing audio\nfrom collections import Counter\nfrom tqdm import tqdm\nimport gc\nimport os\nimport sys\nimport shutil\nimport random\n\nfrom joblib import Parallel, delayed\nfrom functools import partial\n\nimport librosa # audio proccessing\nimport librosa.display # cool audio visuals\nfrom librosa import feature as lf\n\n\n\nfrom skimage.io import imread\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nfrom collections import Counter\nimport glob\nimport pickle\n\n\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, GaussianNoise, Reshape, Lambda, Concatenate\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Activation, LSTM, GRU, Bidirectional\nfrom tensorflow.keras.activations import relu, softmax\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.layers.experimental.preprocessing import Resizing, Normalization\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.models import load_model\n\nnp.random.seed(42)\ntf.random.set_seed(100)","97af89e0":"data_tp=pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_tp.csv')\ndata_fp=pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_fp.csv')","bda96aac":"data_tp.head(10)","52779f94":"data_fp.head(10)","47675a3c":"\ntrain = \"..\/input\/output-files\/MFCC\/train\"\ntest = \"..\/input\/output-files\/MFCC\/test\"\ntrain_files = os.listdir(train)\ntest_files = os.listdir(test) \noutput_dir = \".\/\"","86fe4166":"os.listdir(\"\/kaggle\")","41725068":"len(os.listdir(\"\/kaggle\/input\/output-files\/MFCC\/test\"))","11756a91":"len(os.listdir(\"\/kaggle\/input\/output-files\/MFCC\/train\"))","a49ecad4":"all_arrays_train = []\nfor npfile in train_files:\n    all_arrays_train.append(np.load(os.path.join(train, npfile)))\nall_arrays_train = np.array(all_arrays_train)\nnp.save('train_all.npy',all_arrays_train)","efa5a475":"train_mat= np.load('train_all.npy')\ntrain_mat.ndim \n\n","b1e280cf":"train_mat.shape","b2aa65fa":"all_arrays_test = []\nfor npfile in test_files:\n    all_arrays_test.append(np.load(os.path.join(test, npfile)))\nall_arrays_train = np.array(all_arrays_test)\nnp.save('test_all.npy',all_arrays_test)","cf6a81c4":"test_mat=np.load('test_all.npy')\ntest_mat.ndim ","5ae38123":"test_mat.shape","da5d4cd2":"df1 = data_tp.iloc[:, 0:2]\nrec= data_tp['recording_id'].values\ny_labels = data_tp['species_id'].values\ny = to_categorical(y_labels, num_classes=24)\nprint (data_tp.shape, y.shape)\ndf1\n","26469696":"X=np.empty((0,76, 143, 1),int)\nlabels=[]\nfor i,j,k in df1.itertuples():\n    data = np.load('..\/input\/output-files\/MFCC\/train\/'+j+'.npy')\n    data = np.expand_dims(data,axis=0)\n    X=np.append(X,data,axis=0)\n    labels.append(k)\n    \n \n\nlabels=np.array(labels)\nlabels=labels.astype(int)\n\nprint(labels.shape)\nY=np.zeros((labels.size,24))\n\nY[np.arange(labels.size),labels] = 1\n\n\nprint(X.shape)\nprint(Y.shape)","dd735944":"class_weight = compute_class_weight('balanced',np.arange(24), y_labels)\nclass_weights = dict(zip(np.arange(24), class_weight))\nclass_weights","ffd2073c":"x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2,\n                                                      stratify = y_labels, shuffle = True,\n                                                      random_state = 42)\n#x_train = x_train[..., np.newaxis]\n#x_valid = x_valid[..., np.newaxis]\nprint (x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)","6e9e20be":"\ndef create_model(input_shape = (76,143,1)):\n  model = Sequential()\n  model.add(Input(shape = input_shape))\n  model.add(Resizing(200,200))\n  \n  model.add(Conv2D(32, (3,3), kernel_initializer = 'he_normal', padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  \n\n\n  model.add(Conv2D(64, kernel_size = (3,3),  padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n  model.add(Conv2D(128, kernel_size = (3,3), padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n  model.add(Conv2D(256, kernel_size = (3,3), padding = 'same'))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n\n  model.add(Flatten())\n\n  model.add(Dense(512, kernel_initializer = 'he_normal'))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(Dropout(0.3))\n\n  model.add(Dense(24, activation = 'softmax'))\n  model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['acc'])\n  model.summary()\n  return model","308ad678":"\nmodel = create_model()","8a32bc3b":"\nmodel_checkpoint = os.path.join(train,'models','tp_model.hdf5')\n\ncp = ModelCheckpoint(model_checkpoint, monitor='val_loss', verbose=1, save_best_only=True)\n\nlr = ReduceLROnPlateau(\n      monitor='val_loss', factor=0.3, patience=10, verbose=0,\n      mode='auto'\n)\n\ncallbacks = [cp, lr]\nhistory = model.fit(x_train, y_train, validation_data = (x_valid, y_valid),\n                    epochs = 50, batch_size = 32, callbacks = callbacks, \n                    class_weight = class_weights)","7d50b38a":"def plot_history(history):\n    fig,axs = plt.subplots(1,2, figsize = (15,6))\n    axs[0].plot(history.history['acc'])\n    axs[0].plot(history.history['val_acc'])\n    axs[0].set_title('model accuracy')\n    axs[0].set_ylabel('accuracy')\n    axs[0].set_xlabel('epoch')\n    axs[0].legend(['train', 'val'], loc='upper left')\n    \n    axs[1].plot(history.history['loss'])\n    axs[1].plot(history.history['val_loss'])\n    axs[1].set_title('model loss')\n    axs[1].set_ylabel('loss')\n    axs[1].set_xlabel('epoch')\n    axs[1].legend(['train', 'val'], loc='upper right')\n    \n    plt.show()","bcf0288b":"plot_history(history)","6688b92c":"model.evaluate(x_valid, y_valid)","380cfe83":"y_pred = np.argmax(model.predict(x_valid),axis = 1)\ny_true = np.argmax(y_valid, axis = 1)\nprint (classification_report(y_true,y_pred ))","7be501b6":"#Input shape (76,143,1) if first extracting feature by conv else (76,143) if first layer is RNN layer\ndef create_model(input_shape = (76,143)):\n  model = Sequential()\n  model.add(Input(input_shape))\n  model.add(Normalization())\n  \n  model.add(Bidirectional(LSTM(90, kernel_initializer = 'he_normal', return_sequences= True)))\n  model.add(BatchNormalization())\n\n\n  model.add(Lambda(lambda x:tf.expand_dims(x, axis = -1)))\n  \n  model.add(Resizing(200,200))\n  \n  model.add(Conv2D(64, (3,3), kernel_initializer = 'he_normal', padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  \n\n\n  model.add(Conv2D(64, kernel_size = (3,3), kernel_initializer = 'he_normal', padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n  model.add(Conv2D(128, kernel_size = (3,3), kernel_initializer = 'he_normal', padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n\n  model.add(Conv2D(256, kernel_size = (3,3), kernel_initializer = 'he_normal', padding = 'same'))\n  #model.add(GaussianNoise(0.3))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPool2D((2,2)))\n  model.add(Dropout(0.2))\n\n  '''model.add(Conv2D(256, (1,1)))\n  model.add(Conv2D(32,(1,1)))\n  model.add(Conv2D(1,(1,1)))\n  \n  model.add(Reshape((12,12)))\n\n  model.add(Bidirectional(LSTM(units = 12, return_sequences=True)))\n  #model.add(LSTM(12))\n  '''\n  model.add(Flatten())\n\n  model.add(Dense(512, kernel_initializer='he_normal'))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(Dropout(0.3))\n\n  model.add(Dense(24, kernel_initializer = 'he_normal', activation = 'softmax'))\n  model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['acc'])\n  model.summary()\n  return model","f705bf75":"\nmodel = create_model()","6f413fa8":"model_checkpoint = os.path.join(train,'models','tp_model.hdf5')\n\ncp = ModelCheckpoint(model_checkpoint, monitor='val_loss', verbose=1, save_best_only=True)\n\nlr = ReduceLROnPlateau(\n      monitor='val_loss', factor=0.3, patience=10, verbose=0,\n      mode='auto'\n)\n\ncallbacks = [cp, lr]\n\nhistory = model.fit(x_train, y_train, validation_data = (x_valid, y_valid),\n                    epochs = 70, batch_size = 32, callbacks = callbacks, \n                    class_weight = class_weights)","e413a671":"# CNN Model only","bde9361e":"# Model with both CNN and LSTM"}}