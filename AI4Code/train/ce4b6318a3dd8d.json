{"cell_type":{"b84dc4f1":"code","886d88f9":"code","8f5e92c4":"code","1a8357e7":"code","f4916a15":"code","d50d3e71":"code","c93c76aa":"code","bf8361bc":"code","bf4cf52e":"code","29bee900":"code","111a9157":"code","df18efd7":"code","615e57a0":"code","7ab9ffad":"code","0006cb5c":"code","4f367728":"code","d851f8d3":"code","6b78849e":"code","37094ba1":"code","c24f1741":"code","96b38ee9":"code","34547597":"code","56309b10":"code","53c21874":"code","90caf4df":"code","48a6aa66":"code","3f9f0833":"code","585335e8":"code","42500fed":"code","55e10223":"code","01753cc2":"code","5cd29fab":"code","6392a0f9":"code","27e463a5":"code","1f4b0c8f":"code","7471036f":"code","4aba42ee":"code","3351d5e7":"code","185daa38":"code","321f2bf3":"code","a80434ce":"code","071dacfe":"code","849a49fd":"code","12dc046d":"code","5ba6dbb7":"code","b46de4a5":"code","6e47bb25":"code","e7d58d65":"code","6f78b9d6":"code","ccf5b65a":"code","8e3ef849":"code","e21f1cdb":"code","cfc23677":"code","6b95d417":"code","646bbe68":"code","df58c8cd":"code","c42165b9":"code","5f9ee940":"code","964e6827":"code","217b5f25":"code","3d95179e":"code","8f2d7c49":"code","3c27b42c":"code","90a19051":"code","425951f7":"code","f7c24e49":"code","977da4e1":"code","7e7b11a3":"code","2b4323de":"code","a87865f0":"code","8d9f8a5b":"code","94e8394b":"code","73e7a6dc":"code","81d23e5c":"code","0fe0c441":"code","47db1a18":"code","b6c483a8":"code","d579a174":"code","9555cd6f":"code","2c1d3cc2":"code","504c17ed":"code","64059170":"code","1b8a73ac":"code","5a6f28ec":"code","f409015e":"code","e5fc62f5":"code","a2d7d1ac":"code","da81fc08":"code","89d031bc":"code","f6e92eb7":"code","d2b80c99":"code","25566e38":"code","bae48c38":"code","8b0dd1bb":"code","2500f3a2":"markdown","47e76545":"markdown","5a2a1600":"markdown","2002a4c4":"markdown","eb363f80":"markdown","fbac1827":"markdown","4a43bd50":"markdown","2de151c0":"markdown","6ad828d5":"markdown","59f86072":"markdown","5dc84a39":"markdown","88b0e0e2":"markdown","292e5088":"markdown","811fb49a":"markdown","fc8e1b26":"markdown","14683954":"markdown","f6e496ce":"markdown","a72c145e":"markdown","26062934":"markdown","43fca566":"markdown","f1c9bdca":"markdown","762ef310":"markdown","23a631dc":"markdown","2388f216":"markdown","049e8d1a":"markdown","bffba61c":"markdown","725060ff":"markdown","87305080":"markdown","67309f20":"markdown","6315e718":"markdown","d4d51aba":"markdown","a593073c":"markdown","5dc82cdc":"markdown","55f12d89":"markdown","f5f1be7c":"markdown","935af797":"markdown","c367c323":"markdown","659a1670":"markdown","17415204":"markdown","f7def046":"markdown","f8a59acc":"markdown","51adca30":"markdown","2c185b52":"markdown","4b76f6e7":"markdown","1e81a50b":"markdown","f331d876":"markdown","91136c22":"markdown","ff657586":"markdown","95fefd97":"markdown","c0ff0ed8":"markdown","024ea4a0":"markdown","4a9d6c67":"markdown","5c392014":"markdown","3fec4722":"markdown","98541f54":"markdown","fe3d0b23":"markdown","05365d2c":"markdown","a3eb18b1":"markdown","dc8af252":"markdown","97f00ca9":"markdown","1adf888b":"markdown","d4594805":"markdown","2a4bab4a":"markdown","4fbb101a":"markdown","d27a0f33":"markdown","5f6a499f":"markdown","e11948f3":"markdown","084a5b9d":"markdown","0c2c17c2":"markdown","22bd3dd8":"markdown","09d4963a":"markdown","ca65a146":"markdown"},"source":{"b84dc4f1":"\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\nimport math \nfrom sklearn.metrics import r2_score\nnp.set_printoptions(precision=4)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(font_scale=1.5)","886d88f9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8f5e92c4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1a8357e7":"train.info()","f4916a15":"test.info()","d50d3e71":"train.head()\n","c93c76aa":"test.head()","bf8361bc":"train.dtypes","bf4cf52e":"test.dtypes","29bee900":"##\nprint(\"Statistics of the target variable (SalePrice) :\")\nprint(\"--------------------------------\")\nprint(train['SalePrice'].describe())\nprint(\"Skewness: %f\" % (train['SalePrice']).skew())","111a9157":"AC= train['SalePrice']\nmean=AC.mean()\nmedian=AC.median()\nmode=AC.mode()\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])","df18efd7":"AC.plot(kind='density', figsize=(10, 4), linewidth=2, color='r')\n        #')\nplt.xlabel(\"Sale Price\", labelpad=15)\nplt.ylabel(\"frequency\", labelpad=15)\nplt.title(\"The Sale Price Distrbution\", y=1.012, fontsize=22);\nmeasurements = [mean, median, mode[0] ]\nnames = [\"mode\", \"median\", \"mean\"]\ncolors = ['g', 'b', 'y']\nfor measurement, name, color in zip(measurements, names, colors):\n    plt.axvline(x=measurement, linestyle='--', linewidth=2.5, label='{0} at {1}'.format(name, measurement), c=color)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2 );\n","615e57a0":"test.describe()","7ab9ffad":"train.describe()","0006cb5c":"#collect the numerical value to use it in the plot below.\nnum_var = [f for f in train.columns if train.dtypes[f] != 'object']\nnum_var.remove('SalePrice')\nnum_var.remove('Id')\n# cato_var = [f for f in train.columns if train.dtypes[f] == 'object']\n","4f367728":"train55= train[num_var]\ntest55 = test[num_var]\n","d851f8d3":"def var_standardized(var):\n    var_stand = (var-var.mean())\/var.std()\n    return var_stand\n\ntrain_stand = var_standardized(train55)\nfig,ax = plt.subplots(figsize=(25,20))\n\nsns.boxplot(data=train_stand, orient='h', fliersize=2, linewidth=3, notch=True,\n                 saturation=0.5, ax=ax )\n\nax.set_title('All variables boxplot\\n')\nplt.show()","6b78849e":"def var_standardized(var):\n    var_stand = (var-var.mean())\/var.std()\n    return var_stand\n\ntest_stand = var_standardized(test55)\nfig,ax = plt.subplots(figsize=(25,20))\n\nsns.boxplot(data=test_stand, orient='h', fliersize=2, linewidth=3, notch=True,\n                 saturation=0.5, ax=ax)\n\nax.set_title('All variables boxplot\\n')\nplt.show()","37094ba1":"vn = pd.melt(train, value_vars=num_var)\nfig = sns.FacetGrid(vn, col=\"variable\",  col_wrap=5, sharex=False, sharey=False) # we using FacetGrid \nfig = fig.map(sns.distplot, \"value\")\nplt.show()\n","c24f1741":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice' , color='y')\nplt.title('SalePrice vs GrLivArea ', fontsize=24)\nplt.show()","96b38ee9":"plt.figure(figsize=(18,8));\n# OverallQual , GarageArea ,GrLivArea , TotalBsmtSF , YearBuilt , Fireplaces , FullBath ,MasVnrArea , YearRemodAdd , OpenPorchSF , BsmtFinSF1 , WoodDeckSF , LotFrontage\nsns.scatterplot(data=train, y='SalePrice', x='TotalBsmtSF', color='r')\nplt.title('TotalBsmtSF vs SalePrice ', fontsize=24)     \nplt.show()","34547597":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, x='GrLivArea', y='LotFrontage' , color='y')\nplt.title('GrLivArea vs LotFrontage ', fontsize=24)\nplt.show()","56309b10":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, y='TotalBsmtSF', x='BsmtFinSF1' ,  color='r')\nplt.title('TotalBsmtSF vs BsmtFinSF1 ', fontsize=24)\nplt.show()","53c21874":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, y='SalePrice', x='WoodDeckSF' , color='y')\nplt.title('SalePrice vs WoodDeckSF ', fontsize=24)\nplt.show()","90caf4df":"ax = plt.figure(figsize=(18,8));\nax=sns.barplot(data=train, y='SalePrice', x=\"Neighborhood\" )\nax.set_xticklabels(train[\"Neighborhood\"].unique().astype(str), rotation='vertical')#this line to control the labels in x axs\nplt.title('The Relation SalePrice vs Neighborhood ', fontsize=24)\nplt.show()","48a6aa66":"plt.figure(figsize=(18,8));\nsns.barplot(data=train, y='SalePrice', x=\"Foundation\" ,   palette='Set3')\nplt.title('The Relation SalePrice vs Foundation ', fontsize=24)\nplt.show() ","3f9f0833":"plt.figure(figsize=(18,8));\nsns.barplot(data=train, y='SalePrice', x=\"KitchenQual\", palette='Set3')\nplt.title('The Relation SalePrice vs KitchenQual ', fontsize=24)\n# sns.histplot(data=train, x='SaleCondition',hue=\"KitchenQual\", multiple=\"dodge\", shrink=.8 ,  )\nplt.show() ","585335e8":"plt.figure(figsize=(18,8));\nsns.histplot(data=train, x='CentralAir',multiple=\"dodge\", shrink=.8  , hue='HeatingQC' ,palette='Set3')\nplt.title('The Relation CentralAir vs HeatingQC ', fontsize=24)\nplt.show()","42500fed":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, y='SalePrice', x='YearBuilt',s=100,hue='KitchenQual')\nplt.title('The Relation YearBuilt vs SalePrice ', fontsize=24)\nplt.show()","55e10223":"plt.figure(figsize=(16,8)) # optional\nsns.violinplot(x=\"MSZoning\", y=\"SalePrice\", data=train , palette='Set3');\nplt.title('The Relation YearBuilt vs MSZoning ', fontsize=24);","01753cc2":"ax=plt.figure(figsize=(18,8))\nax=sns.scatterplot(data=train, y='SalePrice', x='YearBuilt',s=100,hue='HouseStyle' , markers =True)\nax.legend(bbox_to_anchor=(1,1),loc='upper left')#this line to change position of huebox\nplt.title('The Relation SalePrice vs YearBuilt ', fontsize=24);\nplt.show()","5cd29fab":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, y='SalePrice', x='GarageArea',s=100  , hue='GarageType' , palette='viridis')\nplt.title('The Relation SalePrice vs GarageArea ', fontsize=24);\nplt.show()","6392a0f9":"plt.figure(figsize=(18,8));\nsns.scatterplot(data=train, y='SalePrice', x='LotArea',s=100  , hue='LotConfig')\nplt.title('The Relation SalePrice vs LotArea ', fontsize=24);\nplt.show()","27e463a5":"sns.catplot(x=\"Utilities\", y=\"SalePrice\", data=train)\nplt.title('The Relation SalePrice vs Utilities', fontsize=24);\nplt.show()","1f4b0c8f":"train_copy = train.copy()\n\nmask = np.zeros_like(train_copy.corr(),dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n## Heatmap visualization\nplt.figure(figsize=(30,20))\nsns.heatmap(train_copy.corr(),\n            annot=True,\n            fmt=\".3f\",\n            annot_kws = {\"size\":10},\n            cmap=sns.cubehelix_palette(),\n            mask=mask)","7471036f":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","4aba42ee":"train.isna().sum()","3351d5e7":"test.isna().sum()","185daa38":"#Dealing with Missing Values of 'Alley' Feature\n#in documntaion N\/A value means No alley access so, \n#it dose not mean a misiing value we replace it to 0 and \"Grvl\" = 1 ,  \"Pave\"= 2\n#on train:\ntitle_mapping = {\"Grvl\": 1, \"Pave\": 2 , np.nan: 0} \ntrain['Alley'] = train['Alley'].map(title_mapping)\n#on test:\ntitle_mapping = {\"Grvl\": 1, \"Pave\": 2 , np.nan: 0} \ntest['Alley'] = test['Alley'].map(title_mapping)\n#Dealing with Missing Values of 'LotFrontage' Feature\n#on train:\ntrain.LotFrontage.replace(np.nan , train.LotFrontage.mean() , inplace=True)\n#on test:\ntest.LotFrontage.replace(np.nan , test.LotFrontage.mean() , inplace=True)\n#Dealing with Missing Values of 'MasVnrType' Feature\n#on train:\ntrain.MasVnrType.replace(np.nan , 'None'  , inplace=True)# None it's a value in MasVnrType we set type of Veneer is not provided\n#on test:\ntest.MasVnrType.replace(np.nan , 'None'  , inplace=True)# highest value \n#Dealing with Missing Values of 'MasVnrArea' Feature\n#on train:\ntrain.MasVnrArea.replace(np.nan , 0.0  , inplace=True)\n#on test:\ntest.MasVnrArea.replace(np.nan , 0.0  , inplace=True)\n#Dealing with Missing Values of 'BsmtQual' Feature\n#on train:\ntrain.BsmtQual.replace(np.nan , 'NB'  , inplace=True)#NB = NO Basement \n#on test:\ntest.BsmtQual.replace(np.nan , 'NB'  , inplace=True)#NB = NO Basement \n#Dealing with Missing Values of 'BsmtCond' Feature\n#on train:\ntrain.BsmtCond.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#on test:\ntest.BsmtCond.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#Dealing with Missing Values of 'BsmtExposure' Feature\n#on train:\ntrain.BsmtExposure.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#on test:\ntest.BsmtExposure.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#Dealing with Missing Values of 'BsmtFinType1' Feature\n#on train:\ntrain.BsmtFinType1.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#on test:\ntest.BsmtFinType1.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#Dealing with Missing Values of 'BsmtFinType2' Feature\n#on train:\ntrain.BsmtFinType2.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement \n#on test:\ntest.BsmtFinType2.replace(np.nan , 'NB'  , inplace=True) #NB = NO Basement   \n#Dealing with Missing Values of 'Electrical' Feature\n#on train:\ntrain.Electrical.replace(np.nan , 'SBrkr'  , inplace=True) #the highest value\n#on test:\ntest.Electrical.replace(np.nan , 'SBrkr'  , inplace=True) \n#Dealing with Missing Values of 'FireplaceQu' Feature\n#on train:\ntrain.FireplaceQu.replace(np.nan , 'NF'  , inplace=True)   #NF No Fireplace\n#on test:\ntest.FireplaceQu.replace(np.nan , 'NF'  , inplace=True)   #NF No Fireplace\n#Dealing with Missing Values of 'GarageType' Feature\n#on train:\ntrain.GarageType.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#on test:\ntest.GarageType.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#Dealing with Missing Values of 'GarageYrBlt' Feature\n#since the house has graage type = NG (means No Garage) we set GarageYrBlt to 0\n#on train:\nfor i , item in train.GarageType.iteritems() :\n    if item  == 'NG':#NG No Garage  \n         train.GarageYrBlt.iloc[i]=0\n        \n#on test:\nfor i , item in test.GarageType.iteritems():\n    if item  == 'NG':#NG No Garage  \n         test.GarageYrBlt.iloc[i] = 0\n    else:\n        test.GarageYrBlt.replace(np.nan ,  test.GarageYrBlt.mean() , inplace=True)     \n        \n        \n#Dealing with Missing Values of 'GarageFinish' Feature\n#on train:\ntrain.GarageFinish.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#on test:\ntest.GarageFinish.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#Dealing with Missing Values of 'GarageQual' Feature\n#on train:\ntrain.GarageQual.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#on test:\ntest.GarageQual.replace(np.nan , 'NG', inplace=True) #NG No Garage\n#Dealing with Missing Values of 'GarageCond' Feature\n#on train:\ntrain.GarageCond.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#on test:\ntest.GarageCond.replace(np.nan , 'NG'  , inplace=True) #NG No Garage\n#Dealing with Missing Values of 'PoolQC' Feature\n#on train:\ntrain.PoolQC.replace(np.nan , 'NP'  , inplace=True) #NP = No Pool\n#on test:\ntest.PoolQC.replace(np.nan , 'NP'  , inplace=True) #NP = No Pool\n#Dealing with Missing Values of 'Fence' Feature\n#on train:\ntrain.Fence.replace(np.nan , 'NF'  , inplace=True) #NF = No Fence\n#on test:\ntest.Fence.replace(np.nan , 'NF'  , inplace=True) #NF = No Fence\n#Dealing with Missing Values of 'MiscFeature' Feature\n#on train:\ntrain.MiscFeature.replace(np.nan , 'None'  , inplace=True) \n#on test:\ntest.MiscFeature.replace(np.nan , 'None'  , inplace=True) \n#Dealing with Missing Values of 'MSZoning' Feature\n#just on test:\ntest.MSZoning.replace(np.nan , 'RL'  , inplace=True) #the highest value \n#Dealing with Missing Values of 'Utilities' Feature\n#just on test:\ntest.Utilities.replace(np.nan , test.Utilities.mode()[0]  , inplace=True) \n#Dealing with Missing Values of 'Exterior1st , Exterior2nd' Features\n#just on test:\ntest.Exterior1st.replace(np.nan , test.Utilities.mode()[0]  , inplace=True) \ntest.Exterior2nd.replace(np.nan , test.Exterior2nd.mode()[0]  , inplace=True) \n#Dealing with Missing Values of 'BsmtFinSF1 , BsmtFinType2' Features\n#just on test in \"BsmtFinSF1\" :\nfor i , item in test.BsmtFinType1.iteritems() :\n    if item  == 'NB':#NB = NO Basement  \n        if test.BsmtFinSF1.iloc[i]==np.nan:\n             test.BsmtFinSF1.iloc[i]= 0\n        else:\n             test.BsmtFinSF1.iloc[i]=test.BsmtFinSF1.mean() \n        \n#just on test in \"BsmtFinSF2\" :\nfor i , item in test.BsmtFinType2.iteritems() :\n    if item  == 'NB':#NB = NO Basement  \n        if test.BsmtFinSF2.iloc[i]==np.nan:\n             test.BsmtFinSF2.iloc[i]= 0\n        else:\n             test.BsmtFinSF2.iloc[i]=test.BsmtFinSF2.mean() \n","321f2bf3":"\n#Dealing with Missing Values of Features in test data:\n#just on test:\ntest.BsmtUnfSF.replace(np.nan , test.BsmtUnfSF.mean() , inplace=True) \n# test.BsmtUnfSF.replace(np.nan , test.TotalBsmtSF.mean() , inplace=True) \ntest.BsmtFullBath.replace(np.nan , test.BsmtFullBath.mean() , inplace=True)  # check if there is a basement to enhance our model \ntest.TotalBsmtSF.replace(np.nan , test.TotalBsmtSF.mean() , inplace=True) # check if there is a basement to enhance our model \ntest.BsmtHalfBath.replace(np.nan , 0 , inplace=True) # we set nan values to 0 beacuse there is no basement \ntest.KitchenQual.replace(np.nan , test.KitchenQual.mode()[0]  , inplace=True)\ntest.Functional.replace(np.nan , test.Functional.mode()[0]  , inplace=True) \ntest.GarageCars.replace(np.nan , test.GarageCars.mean()  , inplace=True) \ntest.GarageArea.replace(np.nan , test.GarageArea.mean()  , inplace=True) \ntest.Functional.replace(np.nan , test.Functional.mode()[0]  , inplace=True) \ntest.SaleType.replace(np.nan , test.SaleType.mode()[0]  , inplace=True) ","a80434ce":"train.isna().sum().sum()","071dacfe":"test.isna().sum().sum()","849a49fd":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","12dc046d":"train2= train[['Id','OverallQual' , 'GarageArea' ,'GrLivArea' , 'TotalBsmtSF' , 'YearBuilt' , 'Fireplaces' , \n'FullBath' ,'MasVnrArea' , 'YearRemodAdd' , 'OpenPorchSF' , 'BsmtFinSF1' , 'WoodDeckSF' , 'LotFrontage','BsmtExposure'\n,'Foundation' ,'Exterior1st','BldgType','Neighborhood', 'LotConfig' , 'Exterior2nd'  , 'SalePrice']]\ntest2 = test[['Id','OverallQual' , 'GarageArea' ,'GrLivArea' , 'TotalBsmtSF' , 'YearBuilt' , 'Fireplaces' , \n'FullBath' ,'MasVnrArea' , 'YearRemodAdd' , 'OpenPorchSF' , 'BsmtFinSF1' , 'WoodDeckSF' , 'LotFrontage','BsmtExposure'\n,'Foundation' ,'Exterior1st','BldgType','Neighborhood', 'LotConfig' ,'Exterior2nd' ]]\ncombain = pd.merge(train2, test2 , how='outer')\ncombain","5ba6dbb7":"num_var = [f for f in combain.columns if combain.dtypes[f] != 'object']\n\nQ1 = combain.quantile(0.25)\n\nQ3 = combain.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\ncombain1 = combain[~((combain < (Q1 - 1.5 * IQR)) |(combain > (Q3 + 1.5 * IQR))).any(axis=1)]\ncombain1.shape","b46de4a5":"combain_data = pd.get_dummies(combain,drop_first=True)\ncombain_data.shape","6e47bb25":"train3=combain_data.iloc[:563]\ntest3=combain_data.iloc[563:]","e7d58d65":"num_var = [f for f in train.columns if train.dtypes[f] != 'object']\nnum_var.remove('SalePrice')\nnum_var.remove('Id')\ntrain1 = train.drop(['SalePrice'] , axis=1)\nX_train = train[num_var]\ny_train = train.SalePrice\nX_test=test[num_var]","6f78b9d6":"ss = StandardScaler()\nXs_train = ss.fit_transform(X_train)\nXs_test=ss.transform(X_test)\nprint(Xs_train.shape , y_train.shape)","ccf5b65a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nlr_baseline = LinearRegression()\nlr_baseline.fit(Xs_train,y_train)\nlr_baseline.score(Xs_train,y_train)","8e3ef849":"y_pred_test = lr_baseline.predict(Xs_test)\ny_pred_trai=lr_baseline.predict(Xs_train)","e21f1cdb":"print(f\"RMSE Train: {math.sqrt(mean_squared_error(y_train, y_pred_trai))}\")\nprint(f\"R Squared Train: {r2_score(y_train, y_pred_trai)}\")\nprint(np.std(train['SalePrice']))\nprint(np.mean(train['SalePrice']))","cfc23677":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": y_pred_test\n  })\nsubmission.to_csv('submission_BL_100.csv', index=False)","6b95d417":"from sklearn.linear_model import RidgeCV\n\nridgecv = RidgeCV()   \nridgecv.fit(Xs_train, y_train)","646bbe68":"ridgecv.score(Xs_train, y_train)","df58c8cd":"ridgecv.alpha_","c42165b9":"re_rigecv=ridgecv.predict(Xs_test)","5f9ee940":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": re_rigecv\n  })\nsubmission.to_csv('submission_Rigecv_100.csv', index=False)","964e6827":"from sklearn.linear_model import Ridge, Lasso, LinearRegression\n\nridge = Ridge( )\nreg=ridge.fit(Xs_train, y_train)\nscores = cross_val_score(reg, Xs_train, y_train, cv=10 )\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Cross-validated scores:\", scores.mean())","217b5f25":"print(reg.score(Xs_train, y_train))\n","3d95179e":"pre_ridge=ridge.predict(Xs_test)","8f2d7c49":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre_ridge\n  })\nsubmission.to_csv('submission_Rige_200.csv', index=False)","3c27b42c":" from sklearn.linear_model import LassoCV\nlassocv = LassoCV(random_state=0 , verbose =1)\nlassocv.fit(Xs_train, y_train)\nscores_lasso = cross_val_score(lassocv, Xs_train, y_train, cv=10)\nprint(\"Ridge Cross-validated R2-scores:\", scores_lasso)\nprint(\"Mean of Ridge Cross-validated R2-scores:\", scores_lasso.mean())","90a19051":"lassocv.score(Xs_train, y_train)","425951f7":"pre=lassocv.predict(Xs_test)","f7c24e49":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre\n  })\nsubmission.to_csv('submission_lasso_100.csv', index=False)","977da4e1":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nlasso.fit(Xs_train, y_train)\nscores = cross_val_score(lasso, Xs_train, y_train, cv=10)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Cross-validated scores:\", scores.mean())\n","7e7b11a3":"lasso.score( Xs_train, y_train)","2b4323de":"pre_lasso=lasso.predict(Xs_test)","a87865f0":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre_lasso\n  })\nsubmission.to_csv('submission_lasso_200.csv', index=False)","8d9f8a5b":" from sklearn.linear_model import ElasticNetCV\n\nregr = ElasticNetCV(l1_ratio= 0.5,cv=5, random_state=0 , n_alphas=100)\nregr.fit(Xs_train, y_train)\nprint(regr.alpha_)\nregr.score(Xs_train, y_train)","94e8394b":"pre_ElasticNetCV_100 = regr.predict(Xs_test)","73e7a6dc":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre_ElasticNetCV_100\n  })\n\nsubmission.to_csv('submission_ElasticNetCV_100.csv', index=False)","81d23e5c":">>> from sklearn.linear_model import ElasticNet\nel = ElasticNet( l1_ratio=0.5)\nel1=el.fit(Xs_train, y_train)\nscores = cross_val_score(el1, Xs_train, y_train, cv=10)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Cross-validated scores:\", scores.mean())\nprint(el.score(Xs_train, y_train))\ny_pre11=el.predict(Xs_test)","0fe0c441":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": y_pre11\n  })\n\nsubmission.to_csv('submission_ElasticNet.csv', index=False)","47db1a18":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor()\nregressor.fit(Xs_train, y_train)\nregressor.score(Xs_train, y_train)\npredictions = regressor.predict(Xs_test)","b6c483a8":"regressor.score(Xs_train, y_train)","d579a174":"#First, check score to see if the fitted model is reasonable\ny_pred_train = regressor.predict(Xs_train)\nprint(f\"RMSE Train: {math.sqrt(mean_squared_error(y_train, y_pred_train))}\")\nprint(f\"R Squared Train: {r2_score(y_train, y_pred_train)}\")\nprint(np.std(train['SalePrice']))\nprint(np.mean(train['SalePrice']))\n","9555cd6f":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": predictions\n  })\n\nsubmission.to_csv('submission_regressor_100.csv', index=False)","2c1d3cc2":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nlr_pipe2 = Pipeline([\n    ('sscaler2', StandardScaler()),\n    ('rf', RandomForestRegressor()\n)\n])\n\npipe_2_params = { 'sscaler2__with_mean': [True, False], \n                 'sscaler2__with_std': [True, False],\n                 'rf__bootstrap': [True],\n                  'rf__max_depth': [14],\n                 'rf__max_features': [2,6,8,10],\n                 'rf__min_samples_leaf': [3],\n                 'rf__min_samples_split': [8],\n                 'rf__n_estimators': [100,200,300]\n                                   }\n   \npipe_2_gridsearch = GridSearchCV(lr_pipe2, \n                                 pipe_2_params, \n                                 cv=5, \n                                 verbose=1 ,n_jobs=1 )\n\n\n","504c17ed":"pipe_2_gridsearch.fit(X_train, y_train)","64059170":"pre=pipe_2_gridsearch.predict(Xs_test)","1b8a73ac":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre\n  })\n\nsubmission.to_csv('submission_RandomForestRegresso_GS_100.csv', index=False)","5a6f28ec":"#pipe_2_gridsearch.best_score_\nmodel_lasso = Lasso(max_iter=100000)\ngrid_params = {'alpha':[1.e-5, 3.e-5, 0.0001, 0.0003, 0.001, 0.003, 0.1, 0.3]}\ngrid_params = {'alpha':np.linspace(0.0001, 0.001, 50)}\nreg_lasso = GridSearchCV(model_lasso, grid_params, \n                             cv=5, verbose=3, n_jobs=-1, \n                             scoring = 'neg_root_mean_squared_error',\n                             return_train_score=True) # r2\nmodel = reg_lasso.fit(Xs_train, y_train)\nreg_lasso.fit(Xs_train, y_train)","f409015e":"pre=reg_lasso.predict(Xs_test)","e5fc62f5":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pre\n  })\n\nsubmission.to_csv('submission_reg_lasso_GS_100.csv', index=False)","a2d7d1ac":"\nfrom sklearn.svm import SVR\nmodel_svr = SVR(C=20, epsilon=0.008, gamma=0.0003)\nmodel_svr.fit(Xs_train,y_train)\n","da81fc08":"\nmodel_svr.score(Xs_train,y_train)\n","89d031bc":"pred_svr= model_svr.predict(Xs_test)\n","f6e92eb7":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pred_svr\n  })\n\nsubmission.to_csv('submission_pred_svr_100.csv', index=False)","d2b80c99":"from sklearn.ensemble import GradientBoostingRegressor\nGboost=GradientBoostingRegressor()\nGboost.fit(Xs_train,y_train)\npred_Gboost= Gboost.predict(Xs_test)\nGboost.score(Xs_train, y_train)","25566e38":"submission = pd.DataFrame({\n   \"Id\": test[\"Id\"],\n    \"SalePrice\": pred_Gboost\n  })\n\nsubmission.to_csv('submission_pred_Gboost_100.csv', index=False)","bae48c38":"scores ={'LR': 4.5027,'RedgeCV': 0.326,'Redge': 0.327, 'LassoCV': 0.234, 'Lasso': 0.326,'elastic_net_CV': 0.392, 'elastic_net': 0.163, 'Random': 0.151, 'RandomGS': 1.025, 'LassoGS' :0.326, 'SVM': 0.416, 'GBoost': 0.145\n}","8b0dd1bb":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig  = plt.figure(figsize=(24, 12))\n\nax=sns.pointplot(x=list(scores.keys()), y=[score for score in scores.values()], markers=['o'], linestyles=['-'])\n# for i, score in enumerate(scores.values()):\n#     ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Various Models', size=20)\n\nplt.show();\n","2500f3a2":"![image.png](attachment:image.png)","47e76545":"\n### Support Vector Machine (SVM)","5a2a1600":"- this plot shows us how is the sale price diffres based on Foundation, Poured Contrete\t has highest price and Slab has lowes price ","2002a4c4":"### Random Forest Regressor model using Pipeline and Grid Search\n\n------------ ","eb363f80":"![image.png](attachment:image.png)","fbac1827":"## Step 5:  Modeling\n\n\n\n\n\n","4a43bd50":"\n\n### Step B: Dummies the columns: \n\n--------\n","2de151c0":"- The scatter plot shows moderate positive relation between Sale Price and total basement, the bigger the total area of basement the higher the price ","6ad828d5":" ### ElasticNet model :\n----","59f86072":"\n\n## Step 4: Data Preprocessing \n\n---","5dc84a39":" ### Ridge model :\n----","88b0e0e2":"![image.png](attachment:image.png)\n","292e5088":"\n### Step B: Combinig Data of train and test: \n\n--------","811fb49a":"## Step 2: Data Wrangling\n\n---","fc8e1b26":"\n# Introduction \n\n## Datasets Description \n\n[House prices ](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)\nThe dataset comes from a Kaggle competition named \u201cHouse Prices: Advanced Regression Techniques\u201d. It contains 1460 training data,1459 testing data and 80 features about houses and sales information.\n","14683954":"### Heatmap","f6e496ce":"- There is no relation between Saleprice and LotArea, but, we notice the LotArea of most houses less than 25000 square feet and most of them 'inside' lot.","a72c145e":"### Outliers detection\n\n**We are going to draw a boxplot to invistigate the outliers in our datasets**\n- for train data:","26062934":"## Executive Summary\nAfter investigating the dataset, we built several machine learning models to predict the selling price of new houses. we reach lowest RMSE with Gradient Boosting Regressor approximtly 0.14\n\n## Step1:  Problem Statment\nThe projet aims to predict the sale prices of new houses, which is given in the dataset, the problem is a supervised learning problem. Since house sales prices are continuous, this is a regression problem.\n\n\n\n## Importing the Libraries","43fca566":"- The scatter shows us there is a no relationship between SalePrice and WoodDeckSFe .","f1c9bdca":"![image.png](attachment:image.png)","762ef310":"![image-2.png](attachment:image-2.png)","23a631dc":"**The target is apparently skewed (with a skewness value > 1).**","2388f216":" ### LassoCV model :\n----","049e8d1a":"- the plot above show us is the sale price differs based on KitchenQual, if the  Kitchen was xcellent qulity has highest price and if it was Fair qulity has owes price ","bffba61c":"- this plot show us how is sale price diffrs based on the Neighborhood, the Northridge and Northridge Heights has highest sale price ","725060ff":" - It doesn't look like there is a relationship here between CentralAir and  HeatingQC.","87305080":"- The scatter plot shows a strongly positive relation between Sale Price and Groud Living Area, the bigger the area the higher the price ","67309f20":"- **Scalling**","6315e718":"- The plot above show us a comparsion between RMSE score of the implemented models in our project. And the model that have the lowest RMSE score is GBoost (Gradient Boosting Regressor) and it perform better than the baseline model. We believe our model generalized new data beacuse RSME score was 0.14 on new, unseen data that means it predict correctly beacuse the error rate was low.\n\n\n### The Approvid  model\n\n![image.png](attachment:image.png)","d4d51aba":"- The plot show us the importance of location zone to house sale price. The Floating Village Residential(FV) zone has highest sale price on the other hand the Commercial(C) zone has lowest sale price.","a593073c":"- The scatter shows us a gap when YearBulit was in 40's , We notice that in 2000's the quality of kitchen improved.and gitting really high and most of it were Excellent and So, SalePrice of house increased.","5dc82cdc":"----------------","55f12d89":"###### Sammary \n\n- Based on the above visualization, we revealed some insights from it. House sale price affected by built year. location zone seems to be important as well in deciding house price. there is a high quality house, based on its foundation material, also affects pricing.  More research could be done on variables like house living area, bathroom numbers, and bedroom numbers related to  house prices to prepare a house price prediction model.","f5f1be7c":"### Missing Data ","935af797":"- After detecting outliers in the dataset, we tried to remove the outlier but, the RMSE score getting higher.","c367c323":" ### Lasso model :\n----","659a1670":"- **Choosing the target and the predectors for the models:**","17415204":"### Lasso Grid Search ","f7def046":"![image.png](attachment:image.png)","f8a59acc":"- The plot show us the sale price is positively distrbuted.","51adca30":"- The above plot shows us distrbution of each feature","2c185b52":"### Baseline model:\n----\n\n","4b76f6e7":"![image.png](attachment:image.png)","1e81a50b":"- We investigated features on the dataset and we choose model's features based on their correlation with each other and with the target (sale price)","f331d876":"#### D: Categorical and Continuous\n\n- We built boxplots combined with swarmplots or use scatter plot with hue or use catplot from seaborn.","91136c22":"- The scatter shows us the relationship between the house sale price and house year bulit, the new houses has higher price. There is a gap in house sales when YearBulit was in 40's , At the begganing of 1950, the 1story became the popular house style.\n","ff657586":"- The scatter shows us the most of houses has All public Utilities (Electricity,Gas,Water, Septic Tank)","95fefd97":"![image.png](attachment:image.png)","c0ff0ed8":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\">\n<li><span><a href=\"#Introduction\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\">\n\n<li><span><a href=\"#Datasets-Description\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Dataset description<\/a><\/span><\/li>\n<li><span><a href=\"#Step1:--Problem-Statment\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Problem Statment<\/a><\/span><\/li>\n<li><span><a href=\"#Executive-Summary\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Executive Summary<\/a><\/span><\/li>\n<\/ul><\/li>\n<li><span><a href=\"#Importing-the-Libraries\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Importing the Libraries<\/a><\/span>\n\n<li><span><a href=\"#Step-2:-Data-Wrangling\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Wrangling<\/a><\/span>\n    \n<li><span><a href=\"#Step-3:-Data-Exploration\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Exploration<\/a><\/span>\n<li><span><a href=\"#Step-4:-Data-Preprocessing\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span>\n\n    \n<li><span><a href=\"#Step-5:--Modeling\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Prediction Models<\/a><\/span><ul class=\"toc-item\">\n\n<li><span><a href=\"#RidgeCV-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> RidgeCV model<\/a><\/span><\/li>\n    \n<li><span><a href=\"#Ridge-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Ridge model <\/a><\/span><\/li>\n    \n<li><span><a href=\"#LassoCV-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> LassoCV model<\/a><\/span><\/li>\n\n<li><span><a href=\"#Lasso-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Lasso model<\/a><\/span><\/li>\n\n<li><span><a href=\"#ElasticNetCV-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> ElasticNetCV model<\/a><\/span><\/li>\n\n<li><span><a href=\"#ElasticNet-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> ElasticNet model<\/a><\/span><\/li>\n    \n\n<li><span><a href=\"#Random-Forest-Regressor-model-:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Random Forest Regressor model <\/a><\/span><\/li>\n\n    \n<li><span><a href=\"#Random-Forest-Regressor-model-using-Pipeline-and-Grid-Search\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Random Forest Regressor model using Pipeline and Grid Search <\/a><\/span><\/li>\n\n<li><span><a href=\"#Lasso-Grid-Search\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Lasso Grid Search<\/a><\/span><\/li>\n    \n    \n<li><span><a href=\"#Support-Vector-Machine-(SVM)\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Support Vector Machine (SVM)<\/a><\/span><\/li>\n    \n    \n<li><span><a href=\"#Gradient-Boosting-Regressor\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span> Gradient Boosting Regressor <\/a><\/span><\/li> <\/ul><\/li>\n\n\n<li><span><a href=\"#Conclusion-and-Recommendations\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Conclusion and Recommendations<\/a><\/span><\/li><\/div>\n  ","024ea4a0":" ### ElasticNetCV model :\n----","4a9d6c67":"- **Calculate the RMSE error**","5c392014":"- The scatter shows us a strong relation between SalePrice and GarageArea, the SalePrice get higher when GarageArea between 700-900. We notice the most common type of Garage is Attched.","3fec4722":"<img src=\"http:\/\/imgur.com\/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n\n# Project 2 : House Prices - Advanced Regression Techniques\n#### By: Samaher Alharbi  ,  Hanan Almohammadi, Nesreen Alqahtani\n","98541f54":" ### RidgeCV model :\n----","fe3d0b23":" ### Random Forest Regressor model :\n------------","05365d2c":"![image.png](attachment:image.png)","a3eb18b1":"![image.png](attachment:image.png)","dc8af252":"#### A: Univariate Analysis\n\n- **Continuous variables**\n\n- We use histograms and boxplots for continuous features","97f00ca9":"- The scatter shows us there is a weak relationship between TotalBsmtSF and  BsmtFinSF1.","1adf888b":"![image.png](attachment:image.png)","d4594805":"**Sammary**\n\n- Based on the above visualization, we revealed some insights from it. House sale price affected by built year. location zone seems to be important as well in deciding house price. There are also other features that play roles in afficting the sale price of the house like the total area of its basement, the living area, kitchen's quality, its foundation and the neighborhood of the house.\n","2a4bab4a":"#### B: Bi-Variate Analysis\n\n**Continuous and Continuous**\n- We built scatter plots to see how two continuous variables interact with each other.","4fbb101a":"## Step 3: Data Exploration\n\n---","d27a0f33":"![image.png](attachment:image.png)","5f6a499f":"- for test data:\n","e11948f3":"### Gradient Boosting Regressor","084a5b9d":"- The scatter shows us there is a no relationship between GrLivArea and LotFrontage .","0c2c17c2":"## Conclusion and Recommendations\n\n- Our findings in this study is that house sale price affected by built year also, location zone seems to be important as well in deciding house price. There are also other features that play roles in afficting the sale price of the house like the total area of its basement, the living area, kitchen's quality, its foundation and the neighborhood of the house. Our target was predicting house sale price based on features.\n","22bd3dd8":"#### C: Categorical and Categorical\n\n- A stacked column chart is a good visualization that shows how the frequencies are spread between two categorical features.","09d4963a":"## Variable Identification\n\n**Question: What features do you think is going to be important for House sales price?**\n\n\n**Answer**: We select features according high corrlation with sale price (target)\n\nOverallQual , GarageArea ,GrLivArea , TotalBsmtSF , YearBuilt , Fireplaces , FullBath ,MasVnrArea , YearRemodAdd , OpenPorchSF , BsmtFinSF1  , WoodDeckSF , LotFrontage  \n\n- TotalBsmtSF: Total square feet of basement area\n- OverallQual: Rates the overall material and finish of the house\n- Fireplaces: Number of fireplaces\n- FullBath: Full bathrooms above grade\n- GarageArea: Size of garage in square feet\n- YearBuilt: Original construction date\n- GrLivArea: Above grade (ground) living area square feet\n- MasVnrArea: Masonry veneer area in square feet\n- YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n- OpenPorchSF: Open porch area in square feet\n- BsmtFinSF1: Type 1 finished square feet\n- WoodDeckSF: Wood deck area in square feet\n- LotFrontage: Linear feet of street connected to property\n\nWe will perform Univariate and Bi-variate analysis on the features you identified as important..\n\n\n\n\n - **Check Statistics:**","ca65a146":"### Step A: Dealing with Missing Values: \n"}}