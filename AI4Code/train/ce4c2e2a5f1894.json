{"cell_type":{"7fba3234":"code","fc2ea299":"code","2de6b751":"code","08b547e3":"code","cc58b73b":"code","83a31bed":"code","dbbc2b18":"code","085e2d74":"code","1d2aa5a4":"code","071e9730":"code","cdf151eb":"code","04220e15":"code","95f7b4b0":"code","5e20beac":"code","d2a0aafa":"code","e5dd231f":"code","d5cfdcb9":"code","0e9c41d6":"code","fb18b29c":"code","ea5032b4":"code","14b8c400":"markdown","60a11894":"markdown","23d39293":"markdown","70ed9e6a":"markdown","e2994204":"markdown","47b7922f":"markdown","e7ec9414":"markdown","81750566":"markdown","d83efb8c":"markdown","d18e2bc9":"markdown","48b06638":"markdown","1ee50f0a":"markdown","35234b8d":"markdown","812646de":"markdown","669f61e0":"markdown","2a42f601":"markdown"},"source":{"7fba3234":"%%capture\n!pip install wandb --upgrade","fc2ea299":"%%capture\n!pip install git+https:\/\/github.com\/qubvel\/efficientnet.git","2de6b751":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\nimport efficientnet.keras as efn \n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom functools import partial","08b547e3":"# Set the random seeds\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1' \nnp.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\ntf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)","cc58b73b":"import wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","83a31bed":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","dbbc2b18":"from kaggle_datasets import KaggleDatasets\n\nGCS_PATH = KaggleDatasets().get_gcs_path()\nIMAGE_SIZE = 380\nBATCH_SIZE = 128*strategy.num_replicas_in_sync\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nCLASS_NUMS = 5\n\nWORK_DIR = '..\/input\/cassava-leaf-disease-classification'\nos.listdir(WORK_DIR)","085e2d74":"# Decode image from TFRecord file.\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n#     image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    return image\n\ndef decode_valid_image(image, label):\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    return image, label\n\n# Read the TFRecord file.\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        label = tf.one_hot(label, depth=CLASS_NUMS)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\n# Return tf.data dataset\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","1d2aa5a4":"@tf.function\ndef gaussian_blur(image, kernel_size=23, padding='SAME'):\n\tsigma = tf.random.uniform((1,))* 1.9 + 0.1\n\n\tradius = tf.cast(kernel_size \/ 2, tf.int32)\n\tkernel_size = radius * 2 + 1\n\tx = tf.cast(tf.range(-radius, radius + 1), tf.float32)\n\tblur_filter = tf.exp(\n\t\t-tf.pow(x, 2.0) \/ (2.0 * tf.pow(tf.cast(sigma, tf.float32), 2.0)))\n\tblur_filter \/= tf.reduce_sum(blur_filter)\n\t# One vertical and one horizontal filter.\n\tblur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n\tblur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n\tnum_channels = tf.shape(image)[-1]\n\tblur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n\tblur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n\texpand_batch_dim = image.shape.ndims == 3\n\tif expand_batch_dim:\n\t\timage = tf.expand_dims(image, axis=0)\n\tblurred = tf.nn.depthwise_conv2d(\n\t\timage, blur_h, strides=[1, 1, 1, 1], padding=padding)\n\tblurred = tf.nn.depthwise_conv2d(\n\t\tblurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n\tif expand_batch_dim:\n\t\tblurred = tf.squeeze(blurred, axis=0)\n\treturn blurred\n\n@tf.function\ndef color_jitter(x, s=0.5):\n\tx = tf.image.random_brightness(x, max_delta=0.8*s)\n\tx = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n\tx = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n\tx = tf.image.random_hue(x, max_delta=0.2*s)\n\tx = tf.clip_by_value(x, 0, 1)\n\treturn x\n\n@tf.function\ndef random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMAGE_SIZE, IMAGE_SIZE, 3])\n\n  return cropped_image\n\n@tf.function\ndef random_apply(func, x, p):\n\treturn tf.cond(\n\t\ttf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n\t\t\t\ttf.cast(p, tf.float32)),\n\t\tlambda: func(x),\n\t\tlambda: x)\n\n@tf.function\ndef custom_augment(image, label):\n    # Random Crop\n    image = random_crop(image)\n    \n    # Random flip\n    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n    image = random_apply(tf.image.flip_up_down, image, p=0.5)\n    \n    # Randomly apply gausian blur\n    image = random_apply(gaussian_blur, image, p=0.5)\n    \n    # Randomly apply transformation (color distortions) with probability p.\n    image = random_apply(color_jitter, image, p=0.5)\n    \n    return image, label","071e9730":"def mixup(a, b):\n  alpha = [0.2]\n\n  (image1, label1), (image2, label2) = a, b\n\n  dist = tfd.Beta(alpha, alpha)\n  l = dist.sample(1)[0][0]\n  \n  img = l*image1+(1-l)*image2\n  lab = l*label1+(1-l)*label2\n\n  return img, lab","cdf151eb":"def get_dataloader(training_filenames, valid_filenames):\n    trainloader1 = load_dataset(training_filenames).shuffle(1024).map(custom_augment, num_parallel_calls=AUTOTUNE)\n    trainloader2 = load_dataset(training_filenames).shuffle(1024).map(custom_augment, num_parallel_calls=AUTOTUNE)\n\n    validloader = load_dataset(valid_filenames)\n\n    trainloader = tf.data.Dataset.zip((trainloader1, trainloader2))\n\n    trainloader = (\n        trainloader\n        .map(mixup, num_parallel_calls=AUTOTUNE)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTOTUNE)\n    )\n\n    validloader = (\n        validloader\n        .map(decode_valid_image, num_parallel_calls=AUTOTUNE)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTOTUNE)\n    )\n    \n    return trainloader, validloader","04220e15":"def show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.axis('off')\n\nTRAINING_TFRECORDS = np.array(tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/ld_train*.tfrec'))     \ntrainloader, validloader = get_dataloader(TRAINING_TFRECORDS[0], TRAINING_TFRECORDS[1])\n    \n\nimage_batch, label_batch = next(iter(trainloader))\nshow_batch(image_batch, label_batch)","95f7b4b0":"def get_model():\n  efn_model = efn.EfficientNetB4(weights='noisy-student')\n  base_model = Model(inputs=efn_model.input, outputs=efn_model.layers[-3].output)\n  base_model.trainabe = True\n\n  inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n  x = base_model(inputs, training=True)\n  x = Dropout(0.5)(x)\n  outputs = Dense(CLASS_NUMS, activation='softmax')(x)\n\n  return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","5e20beac":"earlystoper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=0, mode='auto',\n    restore_best_weights=True\n)","d2a0aafa":"# lr = 0.0003*strategy.num_replicas_in_sync\n# SCHEDULE_BOUNDARIES = [50, 200, 300]\n\n# lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES,\n#                                                                   values=[lr, lr*0.1, lr*0.001, lr*0.0001])\n\n# total_num_steps = 16*EPOCHS\n# rang = np.arange(total_num_steps)\n# y = [lr_schedule(x).numpy() for x in rang]\n# plt.plot(rang, y)\n# print('Learning rate per epoch:')","e5dd231f":"EPOCHS = 25\n\nstart_lr = 0.0001\nmin_lr = 0.0001\nmax_lr = 0.0005 * strategy.num_replicas_in_sync\nrampup_epochs = 5\nsustain_epochs = 5\nexp_decay = .9\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(EPOCHS)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","d5cfdcb9":"EPOCHS = 25\n\n# Stratified K-Fold Strategy\nskf = KFold(n_splits=5, shuffle=True, random_state=42)\n# Get training TFRecords\nTRAINING_TFRECORDS = np.array(tf.io.gfile.glob(GCS_PATH + '\/train_tfrecords\/ld_train*.tfrec'))\n\nACCUMULATED_VAL_ACC = []\n\n# Train for each fold and save model\nfor fold, (train_index, test_index) in enumerate(skf.split(TRAINING_TFRECORDS)):\n    # reinitialize TPU\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n    print(f\"FOLD NUMBER:{fold} , TRAIN IDX: {train_index}, TEST IDX: {test_index}\")\n    \n    # Get train-validation split for this fold\n    TRAINING_FILENAMES, VALID_FILENAMES = TRAINING_TFRECORDS[train_index.tolist()], TRAINING_TFRECORDS[test_index.tolist()]\n    # Get train and validation dataloader\n    trainloader, validloader = get_dataloader(TRAINING_FILENAMES, VALID_FILENAMES)\n    \n    # Initialize model\n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        model = get_model()\n        \n#         optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n        model.compile('adam', 'categorical_crossentropy', metrics=['acc'])\n    \n    # Initialize W&B run for experiment tracking\n    wandb.init(entity='ayush-thakur', project='cassava', job_type='train', name=f'fold_{fold}')\n\n    # Train model \n    _ = model.fit(trainloader,\n              epochs=EPOCHS,\n              validation_data=validloader,\n              callbacks=[earlystoper,\n                         lr_callback,\n                         WandbCallback()])\n    \n    # Save best instance of the model.\n    model.save(f'model_efficientnet_fold_{fold}.h5')\n    \n    # Evaluate model on hold out validation set for this fold.\n    loss, val_acc = model.evaluate(validloader)\n    ACCUMULATED_VAL_ACC.append(val_acc)\n    wandb.log({'val acc for fold': val_acc})\n    \n    del model\n    del trainloader\n    del validloader\n    \n    # Close run for that fold\n    wandb.join()","0e9c41d6":"print(f'Average Model Accuracy: {np.mean(ACCUMULATED_VAL_ACC)}')","fb18b29c":"model_files = [file for file in os.listdir('.\/') if file.endswith('.h5')]\nmodel_files","ea5032b4":"wandb.init(entity='ayush-thakur', project='cassava', job_type='producer')\n\nartifact = wandb.Artifact('model', type='model')\n\nfor model_file in model_files:\n    artifact.add_file(model_file)\n\nwandb.log_artifact(artifact)\nwandb.join()","14b8c400":"# \ud83d\udcf2 Callbacks\n\nWe will be using the following callbacks:\n\n* Early Stopping - Terminate model training when the `val_loss` degrades for over `patience` epochs\n* WandbCallback - To monitor model performance and system metrics.","60a11894":"## \u26a1 Setup TPU and Distribution Strategy","23d39293":"![100042118.jpg](attachment:100042118.jpg)\n\n* This notebook is an ongoing effort to build an image classifier for Cassava leaf disease classification. \n* [Check out this kernel for inference](https:\/\/www.kaggle.com\/ayuraj\/tensorflow-inference-no-tta?scriptVersionId=51996234)\n\nIf you like the work please upvote! :D\n\n# \ud83d\udca5 Includes\n\n* Starter code for training EfficientNet model using TensorFlow(Keras).\n* Training and validation data pipeline using `tf.data`.\n* Data augmetation - Gaussian Blur, Color Jitter, Color Drop and **Mixup**.\n* K-Fold cross validation training.\n* Experiment tracking using Weights and Biases.\n","70ed9e6a":"# \u26c4 Prepare Dataloader\n","e2994204":"# \ud83d\udc24 Model","47b7922f":"We will use [Weights and Biases](https:\/\/wandb.ai\/site) for experiment tracking.","e7ec9414":"# \ud83d\udcaa Augmentation Policies","81750566":"# \ud83d\udc0b Utilities","d83efb8c":"# \ud83d\udc40 Learning Rate Scheduling","d18e2bc9":"Since TensorFlow >2.3 is not available with TPU based session, we will be using this repo to use the efficientnet model as backbone.","48b06638":"**Click on the project link above to go to the Weights and Biases dashboard.**\n\n![image.png](attachment:image.png)","1ee50f0a":"# \ud83c\udfb6 Mixup","35234b8d":"# \u2697\ufe0f Save Model for Inference\n\nWe will also save the model as W&B artifacts for model versioning.","812646de":"# \ud83d\ude8b Train with W&B","669f61e0":"# \ud83d\udcc0 Set Hyperparameters \n\nFor a more through EDA check out this [excellent Kernel](https:\/\/www.kaggle.com\/ihelon\/cassava-leaf-disease-exploratory-data-analysis). **Note: There is class imbalance.**","2a42f601":"# \ud83e\uddf0 Setups, Installations and Imports"}}