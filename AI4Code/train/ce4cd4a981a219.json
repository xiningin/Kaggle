{"cell_type":{"a8c8280e":"code","9d9f19d8":"code","f8e58e4b":"code","6e6c4484":"code","61f6aa2a":"code","d28d7d65":"code","299792b3":"code","d656f96e":"code","f79fca4b":"code","7a57af99":"code","991d0cfe":"code","12a21c5c":"code","a47f04ed":"code","5ae309ce":"code","4bd0006a":"code","4995d73a":"code","8f59463f":"code","4df92b33":"code","f93c6886":"code","d98ee757":"code","a48c19b7":"code","32d84b9c":"code","f2458320":"code","944615d9":"code","6138f31e":"code","59ac0b70":"code","e2f6f05c":"code","d8e7084d":"code","d44d1733":"code","b291419c":"code","5187eaa9":"code","da6a636a":"code","5fd092a8":"code","4a30c768":"code","f4f16226":"code","01486c07":"code","a13e6f10":"code","e57d4d30":"code","b1d538b3":"code","b96b0a62":"code","6d4ee405":"code","8743ed1e":"code","a05f51d8":"code","e6bdbcb9":"code","4dc8cccf":"code","f68ace70":"code","634083b1":"code","daf46499":"code","858f3d24":"code","6a896415":"code","f5d29b11":"code","7d807677":"code","743b4b10":"code","26935f99":"code","d4ac3275":"code","0e7cb57b":"code","cce2efad":"code","fc448105":"code","73b6b060":"code","d7c45c60":"markdown","06d4e1f0":"markdown","78ff5947":"markdown","71cd3014":"markdown","2b39a010":"markdown","081681a7":"markdown","5c5eaca4":"markdown","cfe7bf3e":"markdown","1a06f7d7":"markdown","9fc568db":"markdown","473481b0":"markdown","dc005101":"markdown","42a84013":"markdown","1c82a7b2":"markdown","7ac98022":"markdown","269b0b17":"markdown"},"source":{"a8c8280e":"import sys\nreload(sys)\nsys.setdefaultencoding('utf8')","9d9f19d8":"import nltk","f8e58e4b":"nltk.download()","6e6c4484":"paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n               Thank you to all of you in this room. I have to congratulate \n               the other incredible nominees this year. The Revenant was \n               the product of the tireless efforts of an unbelievable cast\n               and crew. First off, to my brother in this endeavor, Mr. Tom \n               Hardy. Tom, your talent on screen can only be surpassed by \n               your friendship off screen \u2026 thank you for creating a t\n               ranscendent cinematic experience. Thank you to everybody at \n               Fox and New Regency \u2026 my entire team. I have to thank \n               everyone from the very onset of my career \u2026 To my parents; \n               none of this would be possible without you. And to my \n               friends, I love you dearly; you know who you are. And lastly,\n               I just want to say this: Making The Revenant was about\n               man's relationship to the natural world. A world that we\n               collectively felt in 2015 as the hottest year in recorded\n               history. Our production needed to move to the southern\n               tip of this planet just to be able to find snow. Climate\n               change is real, it is happening right now. It is the most\n               urgent threat facing our entire species, and we need to work\n               collectively together and stop procrastinating. We need to\n               support leaders around the world who do not speak for the \n               big polluters, but who speak for all of humanity, for the\n               indigenous people of the world, for the billions and \n               billions of underprivileged people out there who would be\n               most affected by this. For our children\u2019s children, and \n               for those people out there whose voices have been drowned\n               out by the politics of greed. I thank you all for this \n               amazing award tonight. Let us not take this planet for \n               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n               ","61f6aa2a":"sentences = nltk.sent_tokenize(paragraph)\nprint(len(sentences))\nsentences","d28d7d65":"words=[nltk.word_tokenize(sent) for sent in sentences]\nwords","299792b3":"from nltk.stem import PorterStemmer","d656f96e":"stemmer = PorterStemmer()\nwords=[nltk.word_tokenize(sent) for sent in sentences]\n","f79fca4b":"for i in range(len(sentences)):\n    words = nltk.word_tokenize(sentences[i])\n    words = [stemmer.stem(word) for word in words]\n    sentences[i] = ' '.join(words)      ","7a57af99":"sentences","991d0cfe":"from nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\nnewsentences = nltk.sent_tokenize(paragraph)","12a21c5c":"for i in range(len(newsentences)):\n    words = nltk.word_tokenize(newsentences[i])\n    words = [lemmatizer.lemmatize(word) for word in words]\n    newsentences[i] = ' '.join(words)      ","a47f04ed":"newsentences","5ae309ce":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","4bd0006a":"for i in range(len(newsentences)):\n    words = nltk.word_tokenize(newsentences[i])\n    words = [word for word in words if word not in stopwords.words('english')]\n    newsentences[i] = ' '.join(words)            ","4995d73a":"newsentences","8f59463f":"words = nltk.word_tokenize(paragraph)\ntagged_words=nltk.pos_tag(words)","4df92b33":"tagged_words","f93c6886":"word_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"-\"+tw[1])","d98ee757":"word_tags","a48c19b7":"tagged_paragraph = ' '.join(word_tags)\ntagged_paragraph","32d84b9c":"words = nltk.word_tokenize(paragraph)\nwords","f2458320":"tagged_words = nltk.pos_tag(words)\ntagged_words","944615d9":"namedEnt = nltk.ne_chunk(tagged_words)\n","6138f31e":"# Tokenize the article into sentences: sentences\nsentences = nltk.sent_tokenize(paragraph)\n# Tokenize each sentence into words: token_sentences\ntoken_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n\n# Tag each tokenized sentence into parts of speech: pos_sentences\npos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n\n# Create the named entity chunks: chunked_sentences\nchunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n\n# Test for stems of the tree with 'NE' tags\nfor sent in chunked_sentences:\n    for chunk in sent:\n        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n            print(chunk)\n","59ac0b70":"article= \"\"\"The Taj Mahal was built by Emperor Shah Jahan\"\"\"\nwords=nltk.word_tokenize(article)\ntagged_words=nltk.pos_tag(words)\ntagged_words","e2f6f05c":"chunked_words=nltk.ne_chunk(tagged_words)\n#chunked_words.draw()","d8e7084d":"import re\ndataset = nltk.sent_tokenize(paragraph)\nfor i in range(len(dataset)):\n    dataset[i] = dataset[i].lower()\n    dataset[i] = re.sub(r'\\W',' ',dataset[i])\n    dataset[i] = re.sub(r'\\s+',' ',dataset[i])","d44d1733":"dataset[0:6]","b291419c":"word2count = {}\nfor data in dataset:\n    words = nltk.word_tokenize(data)\n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1","5187eaa9":"word2count","da6a636a":"import heapq\nfreq_words = heapq.nlargest(25,word2count,key=word2count.get)\nfreq_words","5fd092a8":"import numpy as np\nX = []\nfor data in dataset:\n    vector = []\n    for word in freq_words:\n        if word in nltk.word_tokenize(data):\n            vector.append(1)\n        else:\n            vector.append(0)\n    X.append(vector)\n        \nX = np.asarray(X)","4a30c768":"X","f4f16226":"from collections import Counter\n# Tokenize the article: tokens\ntokens = nltk.word_tokenize(paragraph)\ntokens = [word for word in tokens if word not in stopwords.words('english')]\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens ]\nlower_tokens_1 = [re.sub(r'\\W',' ',t) for t in lower_tokens]\nlower_tokens_2 = [re.sub(r'\\s+',' ',t) for t in lower_tokens_1]\nalpha_only = [t for t in lower_tokens_2 if t.isalpha()]\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple =Counter(alpha_only)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(11))","01486c07":"paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n               Thank you to all of you in this room. I have to congratulate \n               the other incredible nominees this year. The Revenant was \n               the product of the tireless efforts of an unbelievable cast\n               and crew. First off, to my brother in this endeavor, Mr. Tom \n               Hardy. Tom, your talent on screen can only be surpassed by \n               your friendship off screen \u2026 thank you for creating a t\n               ranscendent cinematic experience. Thank you to everybody at \n               Fox and New Regency \u2026 my entire team. I have to thank \n               everyone from the very onset of my career \u2026 To my parents; \n               none of this would be possible without you. And to my \n               friends, I love you dearly; you know who you are. And lastly,\n               I just want to say this: Making The Revenant was about\n               man's relationship to the natural world. A world that we\n               collectively felt in 2015 as the hottest year in recorded\n               history. Our production needed to move to the southern\n               tip of this planet just to be able to find snow. Climate\n               change is real, it is happening right now. It is the most\n               urgent threat facing our entire species, and we need to work\n               collectively together and stop procrastinating. We need to\n               support leaders around the world who do not speak for the \n               big polluters, but who speak for all of humanity, for the\n               indigenous people of the world, for the billions and \n               billions of underprivileged people out there who would be\n               most affected by this. For our children\u2019s children, and \n               for those people out there whose voices have been drowned\n               out by the politics of greed. I thank you all for this \n               amazing award tonight. Let us not take this planet for \n               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n               \n               \n# Tokenize sentences\ndataset = nltk.sent_tokenize(paragraph)\nfor i in range(len(dataset)):\n    dataset[i] = dataset[i].lower()\n    dataset[i] = re.sub(r'\\W',' ',dataset[i])\n    dataset[i] = re.sub(r'\\s+',' ',dataset[i])\n\n\n# Creating word histogram\nword2count = {}\nfor data in dataset:\n    words = nltk.word_tokenize(data)\n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n            \n# Selecting best 100 features\nfreq_words = heapq.nlargest(100,word2count,key=word2count.get)\n\n\n# IDF Dictionary\nword_idfs = {}\nfor word in freq_words:\n    doc_count = 0\n    for data in dataset:\n        if word in nltk.word_tokenize(data):\n            doc_count += 1\n    word_idfs[word] = np.log(len(dataset)\/(1+doc_count))\n    \n# TF Matrix\ntf_matrix = {}\nfor word in freq_words:\n    doc_tf = []\n    for data in dataset:\n        frequency = 0\n        for w in nltk.word_tokenize(data):\n            if word == w:\n                frequency += 1\n        tf_word = frequency\/len(nltk.word_tokenize(data))\n        doc_tf.append(tf_word)\n    tf_matrix[word] = doc_tf\n    \n# Creating the Tf-Idf Model\ntfidf_matrix = []\nfor word in tf_matrix.keys():\n    tfidf = []\n    for value in tf_matrix[word]:\n        score = value * word_idfs[word]\n        tfidf.append(score)\n    tfidf_matrix.append(tfidf)   \n    \n# Finishing the Tf-Tdf model\nX = np.asarray(tfidf_matrix)\n\nX = np.transpose(X)\nX","a13e6f10":"from sklearn.feature_extraction.text import TfidfVectorizer","e57d4d30":"tfidf = TfidfVectorizer()\nresponse = tfidf.fit_transform([paragraph])","b1d538b3":"feature_names = tfidf.get_feature_names()\nfeature_names","b96b0a62":"for col in response.nonzero()[1]:\n    print (feature_names[col], ' - ', response[0, col])","6d4ee405":"import pandas as pd\ntfidf_df= pd.DataFrame()\ntfidf_df['Feature names']=feature_names\ntfidf_df['weights']=[response[0,col] for col in response.nonzero()[1]]\n","8743ed1e":"tfidf_df=tfidf_df.sort_values(by=['weights'],ascending=False)\ntfidf_df.head(10)","a05f51d8":"import random\n\n# Sample data\ntext = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth\u2019s atmosphere.\"\"\"\n\n# Order of the grams\nn = 3\n\n# Our N-Grams\nngrams = {}\n\n# Creating the model\nfor i in range(len(text)-n):\n    gram = text[i:i+n]\n    if gram not in ngrams.keys():\n        ngrams[gram] = []\n    ngrams[gram].append(text[i+n])","e6bdbcb9":"ngrams","4dc8cccf":"currentGram = text[0:n]\nresult = currentGram\nfor i in range(300):\n    if currentGram not in ngrams.keys():\n        break\n    possibilities = ngrams[currentGram]\n    nextItem = possibilities[random.randrange(len(possibilities))]\n    result += nextItem\n    currentGram = result[len(result)-n:len(result)]\n  \nprint(result)","f68ace70":"n = 3\n\n# Our N-Grams\nngrams = {}\n\n# Building the model\nwords = nltk.word_tokenize(text)\nfor i in range(len(words)-n):\n    gram = ' '.join(words[i:i+n])\n    if gram not in ngrams.keys():\n        ngrams[gram] = []\n    ngrams[gram].append(words[i+n])\n    \n# Testing the model\ncurrentGram = ' '.join(words[0:n])\nresult = currentGram\nfor i in range(30):\n    if currentGram not in ngrams.keys():\n        break\n    possibilities = ngrams[currentGram]\n    nextItem = possibilities[random.randrange(len(possibilities))]\n    result += ' '+nextItem\n    rWords = nltk.word_tokenize(result)\n    currentGram = ' '.join(rWords[len(rWords)-n:len(rWords)])\n\nprint(result)","634083b1":"def find_bigrams(words):\n  bigram_list = []\n  for i in range(len(words)-1):\n      bigram_list.append((words[i], words[i+1]))\n  return bigram_list","daf46499":"find_bigrams(words)","858f3d24":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n# Sample Data\ndataset = [\"The amount of polution is increasing day by day\",\n           \"The concert was just great\",\n           \"I love to see Gordon Ramsay cook\",\n           \"Google is introducing a new technology\",\n           \"AI Robots are examples of great technology present today\",\n           \"All of us were singing in the concert\",\n           \"We have launch campaigns to stop pollution and global warming\"]\n\ndataset = [line.lower() for line in dataset]","6a896415":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(dataset)\n\n# Visualizing the Tfidf Model\nprint(X[0])","f5d29b11":"# Creating the SVD\nlsa = TruncatedSVD(n_components = 4, n_iter = 4000)\nlsa.fit(X)","7d807677":"# First row of V\nrow1 = lsa.components_[0] ## the first concept. it has 4 concepts. so row1\n# is displaying the probability of the diff words to be in concept 0.\nlen(row1)","743b4b10":"row1","26935f99":"# Visualizing the concepts\nconcept_words = {}\nterms = vectorizer.get_feature_names()## the 42 words\nfor i,comp in enumerate(lsa.components_):\n    componentTerms = zip(terms,comp)\n    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n    sortedTerms = sortedTerms[:10]\n    concept_words[\"Concept \"+str(i)] = sortedTerms\n    print(\"\\nConcept\",i,\":\")\n    for term in sortedTerms:\n        print(term)","d4ac3275":"concept_words","0e7cb57b":"for key in concept_words.keys():\n    sentence_scores = []\n    for sentence in dataset:\n        words = nltk.word_tokenize(sentence)\n        score = 0\n        for word in words:\n            for word_with_score in concept_words[key]:\n                if word == word_with_score[0]:\n                    score += word_with_score[1]\n        sentence_scores.append(score)\n    print(\"\\n\"+key+\":\")\n    for sentence_score in sentence_scores:\n        print(sentence_score)","cce2efad":"from nltk.corpus import wordnet\n\n# Initializing the list of synnonyms and antonyms\nsynonyms = []\nantonyms = []\n\nfor syn in wordnet.synsets(\"correct\"):\n    for s in syn.lemmas():\n        synonyms.append(s.name())\n        for a in s.antonyms():\n            antonyms.append(a.name())\n            \n            \n# Displaying the synonyms and antonyms\nprint('Synonyms are',set(synonyms))\nprint('Antonyms are',set(antonyms))","fc448105":"sentence = \"I was not sad with the team's performance\"\n\nwords = nltk.word_tokenize(sentence)\n\nnew_words = []\n\ntemp_word = ''\nfor word in words:\n    if word == 'not':\n        temp_word = 'not_'\n    elif temp_word == 'not_':\n        word = temp_word + word\n        temp_word = ''\n    if word != 'not':\n        new_words.append(word)\n\nsentence = ' '.join(new_words)\nsentence","73b6b060":"sentence = \"I was not sad with the team's performance\"\n\nwords = nltk.word_tokenize(sentence)\n\nnew_words = []\n\ntemp_word = ''\nfor word in words:\n    antonyms = []\n    if word == 'not':\n        temp_word = 'not_'\n    elif temp_word == 'not_':\n        for syn in wordnet.synsets(word):\n            for s in syn.lemmas():\n                for a in s.antonyms():\n                    antonyms.append(a.name())\n        if len(antonyms) >= 1:\n            word = antonyms[0]\n        else:\n            word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n        temp_word = ''\n    if word != 'not':\n        new_words.append(word)\n\nsentence = ' '.join(new_words)\nsentence","d7c45c60":"#### Testing n gram model","06d4e1f0":"### Word Negation","78ff5947":"#### n-gram model","71cd3014":"#### Tfidf model","2b39a010":"#### Stopwords","081681a7":"#### Tokenization","5c5eaca4":"#### Lemmatization","cfe7bf3e":"#### Stemmer","1a06f7d7":"#### Parts of Speech Tagging","9fc568db":"### Bag of Words Model(better method)","473481b0":"##### Trigrams","dc005101":"### Synonyms and Antonyms","42a84013":"#### Bag of Words","1c82a7b2":"![Capture.PNG](attachment:Capture.PNG)","7ac98022":"### Latent Semantic Analysis","269b0b17":"#### Named Entity Recognition"}}