{"cell_type":{"a3b6d496":"code","7a1cc5f5":"code","6a38d91f":"code","6037e023":"code","4d27e211":"code","c9466bab":"code","cfdcd8b9":"code","1a9a7409":"code","11587412":"code","3bf0ea66":"code","e5a2b4ef":"code","03a694b9":"code","5119f74d":"code","e2dd8de5":"code","5c679a32":"code","46f476af":"code","9b30c7bb":"code","72b001e5":"code","6df7efdf":"code","db1fd598":"code","d2ea4990":"code","878e171e":"code","5685a316":"code","957bcd8b":"code","eaaed0c3":"code","7523688e":"code","56224029":"code","51e58790":"code","69aa71a7":"code","7e9999e6":"code","dfce8046":"code","5280bdbd":"code","2874d7d7":"code","6ea7c297":"code","75eceb56":"code","32f69834":"markdown","3aa4a38f":"markdown","6a7ae670":"markdown","ef9cc5eb":"markdown","794fe5cc":"markdown","c7e76993":"markdown","d6e3d1e0":"markdown","ec649eb1":"markdown","c9b318ab":"markdown","aa9a859b":"markdown","7e23477f":"markdown"},"source":{"a3b6d496":"# !pip install tensorflow==2.0.0-alpha0\n# kaggle default imports\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7a1cc5f5":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport time\nfrom wordcloud import WordCloud, STOPWORDS\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\n# Natural Language Tool Kit\nimport nltk\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom collections import Counter\nimport cufflinks as cf\ncf.go_offline()","6a38d91f":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\ntrain_df.head()","6037e023":"print(f\"Train data shape {train_df.shape}\")\nprint(f\"Test data shape {test_df.shape}\")","4d27e211":"missing = train_df.isnull().sum()\nmissing[missing>0].sort_values(ascending=False).iplot(kind='bar',\n                                                      title='Null values present in train Dataset',\n                                                      color=['blue'])","c9466bab":"train_df.target.value_counts().iplot(kind='bar',\n                                     text=['Fake', 'Real'],\n                                     title='Comparing Tweet is a real disaster (1) or not (0)',\n                                     color=['green'])","cfdcd8b9":"counts_train = train_df.target.value_counts(sort=False)\nlabels = counts_train.index\nvalues_train = counts_train.values\n\ndata = go.Pie(labels=labels, values=values_train, pull=[0.03, 0])\nlayout = go.Layout(title=\"Comparing Tweet is a real disaster (1) or not (0) in %\")\nfig = go.Figure(data=[data], layout=layout)\nfig.update_traces(hole=.3, hoverinfo=\"label+percent+value\")\nfig.update_layout(\n    # Add annotations in the center of the donut pies\n    annotations = [dict(text='Train', x=0.5, y=0.5, font_size=20, showarrow=False)]\n)\nfig.show()","1a9a7409":"train_df['length'] = train_df['text'].apply(len)\n\ndata = [\n    go.Box(\n        y=train_df[train_df['target']==0]['length'],\n        name='Fake'\n    ),\n    go.Box(\n        y=train_df[train_df['target']==1]['length'],\n        name='Real'\n    )\n]\nlayout = go.Layout(title='Comparison of text length in Tweets')\nfig = go.Figure(data=data, layout=layout)\nfig.show()","11587412":"print(f\"There are {train_df.keyword.nunique()} in the train set\") # Total number of unique keywords\n\ntrain_df.keyword.value_counts()[:20].iplot(kind='bar', title='Top 20 keywords in text', color='red')","3bf0ea66":"train_df.location.value_counts()[:20].iplot(kind='bar', title='Top 20 locations in tweet')","e5a2b4ef":"STOPWORDS.add('https')\n\ndef plot_world(text):\n    comment_words = ' '\n    stopwords = set(STOPWORDS)\n    for val in text:\n        # typecast val into string\n        val = str(val)\n        # split the tokens\n        tokens = val.split()\n        for i in range(len(tokens)):\n            # converts each token into lowercase\n            tokens[i] = tokens[i].lower()\n        for words in tokens:\n            comment_words = comment_words + words + ' '\n            \n    wordcloud = WordCloud(width = 5000,\n                          height = 4000,\n                          background_color = 'black',\n                          stopwords = stopwords,\n                          min_font_size = 10).generate(comment_words)\n    # plot the WordCloud image\n    plt.figure(figsize=(12,12), facecolor='k', edgecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()","03a694b9":"# text = train_df.text.values\n# plot_world(text)","5119f74d":"# How many http words has this text?\ntrain_df.loc[train_df['text'].str.contains('http')].target.value_counts()","e2dd8de5":"pattern = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\ndef remove_html(text):\n    no_html= pattern.sub('',text)\n    return no_html\n\n# Remove all text that start with html\ntrain_df['text'] = train_df['text'].apply(lambda x: remove_html(x))","5c679a32":"# lets check if this clean works\nprint(train_df.loc[train_df['text'].str.contains('http')].target.value_counts())\n\n# remove all text that start with html in test\ntest_df['text'] = test_df['text'].apply(lambda x: remove_html(x))","46f476af":"# now remove stopwords, change to lowercase\n\ndef clean_text(text):\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = text.lower()\n    # split to array (default delimiter is \" \")\n    text = text.split()\n    text = [w for w in text if not w in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    return text","9b30c7bb":"# apply text cleaning\ntrain_df['text'] = train_df['text'].apply(lambda x : clean_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x : clean_text(x))","72b001e5":"# How many unique words have this text\n\ndef counter_word(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count\ntext_values = train_df['text']\n\ncounter = counter_word(text_values)\nprint(f\"The len of words is: {len(counter)}\")\nlist(counter.items())[:10]","6df7efdf":"# The maximum number of words to be used. (most frequent)\n\nvocab_size = len(counter)\nembedding_dim = 32\n\n# Max number of words in each complaint\nmax_length = 20\ntrunc_type = 'post'\npadding_type = 'post'\n\n# oov_took its set for words out our word index\noov_tok = \"<XXX>\"\ntraining_size = 6200\nseq_len = 12\n\n# based on 80% of the data\ntraining_sentences = train_df.text[0:training_size]\ntraining_labels = train_df.target[0:training_size]\n\nvalid_sentences = train_df.text[training_size:]\nvalid_labels = train_df.target[training_size:]\n\nprint('The Shape of training ',training_sentences.shape)\nprint('The Shape of testing',valid_sentences.shape)","db1fd598":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index","d2ea4990":"# Lets see the first 10 elements\nprint(\"THe first word Index are: \")\nfor x in list(word_index)[0:15]:\n    print (\" {},  {} \".format(x,  word_index[x]))","878e171e":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(train_df.text[1])\nprint(training_sequences[1])","5685a316":"# check reverse to see how it works\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","957bcd8b":"# Lets see the first 10 elements\nprint(\"THe first reverse word Index are: \")\nfor x in list(reverse_word_index)[0:15]:\n    print (\" {},  {} \".format(x,  reverse_word_index[x]))","eaaed0c3":"def decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\ndecode(training_sequences[1]) # this can be useful in checking predictions","7523688e":"valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\nvalid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","56224029":"# Model Definition with LSTM\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid') # remember this is a binary classification\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","51e58790":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","69aa71a7":"start_time = time.time()\n\nnum_epochs = 40\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(valid_padded, valid_labels))\n\nfinal_time = (time.time()- start_time)\/60\nprint(f'The time in minutes: {final_time}')","7e9999e6":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","dfce8046":"model_loss[['accuracy','val_accuracy']].plot(ylim=[0,1])","5280bdbd":"predictions = model.predict_classes(valid_padded)\npredictions","2874d7d7":"from sklearn.metrics import classification_report,confusion_matrix\n\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \n# Showing Confusion Matrix\nplot_cm(valid_labels,predictions, 'Confution matrix of Tweets', figsize=(7,7))","6ea7c297":"testing_sequences2 = tokenizer.texts_to_sequences(test_df.text)\ntesting_padded2 = pad_sequences(testing_sequences2, maxlen=max_length, padding=padding_type, truncating=trunc_type)\npredictions = model.predict(testing_padded2)\n# sample of submission\n\nsub['target'] = (predictions > 0.5).astype(int)\nsub.head()","75eceb56":"sub.to_csv(\"submission.csv\", index=False, header=True)","32f69834":"### Model Evaluation","3aa4a38f":"Import libraries we would be making use of","6a7ae670":"the ratio of 1's to 0's is considerable, our dataset is balanced","ef9cc5eb":"load the data, do some basic pandas eda","794fe5cc":"### Cleaning the Text","c7e76993":"Using WordCloud to view word prominence","d6e3d1e0":"we can see that the location column has a lot more missing values than the keyword column. However, the text and target columns do not have any. Next we would be visualizing the fake and real disaster tweets to know if our data is balanced or imbalanced","ec649eb1":"### Train \/ Valid Split","c9b318ab":"Thank you very much to [Marco Vasquez E](https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud) for his wonderful kernel. This work is based off of his. Kindly visit and upvote","aa9a859b":"### Creating the model","7e23477f":"### Data Visualization\n\nIn the first visualization, we would be visualizing the count null values present in the train dataset."}}