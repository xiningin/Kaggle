{"cell_type":{"1f34c00d":"code","61a1ebfb":"code","4bfa4326":"code","4ab36d4a":"code","74ff425d":"code","cb82df76":"code","b5705eec":"code","0396222b":"code","8e46fac2":"code","8579ac41":"code","59ed3d73":"code","f90412aa":"code","d4d2825f":"code","597d544d":"code","2b83dcd7":"code","58511fd5":"code","3655c120":"code","c56dbd1f":"code","37faebd4":"code","0186b4a4":"code","9a59e003":"code","bb4740a5":"code","629d06fa":"code","1ff93fcd":"code","04d7952e":"code","9084b9bc":"code","35ee3f08":"code","e7f62b40":"code","a6496856":"code","63ba1d3b":"code","55d4ba9e":"code","bcb1ad76":"code","265a8828":"code","e8a6f12f":"markdown","d6d96c5c":"markdown","aa52aa2f":"markdown","e3bb9abf":"markdown","150a6ace":"markdown","07d48058":"markdown","23f291b3":"markdown","4ec63dec":"markdown","2b5fef21":"markdown","195be930":"markdown","de8bbb8a":"markdown","0e862648":"markdown","f5f81d81":"markdown","c533b0b4":"markdown","e44a9003":"markdown","5ac61346":"markdown","0b9f5f06":"markdown","745a9457":"markdown","19e6fee5":"markdown","83affb04":"markdown","914c357d":"markdown","79715879":"markdown","ed63d84b":"markdown","ab4c1045":"markdown","25bb3990":"markdown","b2f40a0d":"markdown","d742cb55":"markdown","91dbbca0":"markdown","78e921a5":"markdown"},"source":{"1f34c00d":"import pandas as pd\nimport numpy as np\nimport re\nnp.random.seed(500) \ntrain=pd.DataFrame(pd.read_csv('..\/input\/nlp-getting-started\/train.csv'))\ntrain.head()\ntrain=train.dropna(axis=1)\ntweets=train['text'].to_list()\n# print(train)\n# print(tweets)\nnonums=[]\nfor tweet in tweets:\n    nonums.append(re.sub(r'\\d+', '', tweet))\n# print(nonums)","61a1ebfb":"!pip install contractions\np=re.compile(r'\\<http.+?\\>', re.DOTALL)\n\ntweetswithouturls=[]\nfor tweet in nonums:\n    tweetswithouturls.append(re.sub(r\"http\\S+\", \"\", tweet))\n# print(tweetswithouturls)","4bfa4326":"import nltk\nimport contractions\ndef replace_contractions(text):\n    \"\"\"Replace contractions in string of text\"\"\"\n    return contractions.fix(text)\nnocontractions=[]\nfor tweet in tweetswithouturls:\n    \n\n    nocontractions.append(replace_contractions(tweet))\n# print(nocontractions)","4ab36d4a":"from nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\ntokens = [word_tokenize(sen) for sen in nocontractions]\n# print(tokens)","74ff425d":"def remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\nnopunct=[]\nfor listt in tokens:\n    nopunct.append(remove_punctuation(listt))\n# print(nopunct)","cb82df76":"import string, unicodedata\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        if(new_word.isalpha()):\n            new_words.append(new_word)\n    return new_words\nonlyascii=[]\nfor listt in nopunct:\n    onlyascii.append(remove_non_ascii(listt))\n# print(onlyascii)","b5705eec":"def to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\nlower=[]\nfor listt in onlyascii:\n    lower.append(to_lowercase(listt))\n# print(lower)","0396222b":"\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\n    \nnostopwords=[]\nfor listt in lower:\n    nostopwords.append(remove_stopwords(listt))\n# print(nostopwords)\n# print(stopwords.words('english'))","8e46fac2":"from nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet as wn\nimport collections\ntag_map = collections.defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\nFinal_words = []\nword_Lemmatized = WordNetLemmatizer()\nfor entry in nostopwords:\n#     print(entry)\n    words=[]\n    # Initializing WordNetLemmatizer()\n    \n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        \n        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n#         print(word_Final)\n        words.append(word_Final)\n    Final_words.append(words)\n# print(Final_words)\n# for entry in nostopwords:\n#     print(pos_tag(entry))","8579ac41":"train['Text_Final'] = [' '.join(sen) for sen in Final_words]\ntrain['tokens'] = Final_words\ndisaster = []\nnotdisaster = []\nfor l in train['target']:\n    if l == 0:\n        disaster.append(0)\n        notdisaster.append(1)\n    elif l == 1:\n        disaster.append(1)\n        notdisaster.append(0)\ntrain['Disaster']= disaster\ntrain['Not a Disaster']= notdisaster\n\ntrain = train[['Text_Final', 'tokens', 'target', 'Disaster', 'Not a Disaster']]\ntrain.head()","59ed3d73":"#repeating for test\ntest=pd.DataFrame(pd.read_csv('..\/input\/nlp-getting-started\/test.csv'))\ntest.head()\ntest=test.dropna(axis=1)\ntweetstest=test['text'].to_list()\n# print(train)\n# print(tweets)\nnonumstest=[]\nfor tweet in tweetstest:\n    nonumstest.append(re.sub(r'\\d+', '', tweet))\n# print(nonums)\n\n\ntweetswithouturlstest=[]\nfor tweet in nonumstest:\n    tweetswithouturlstest.append(re.sub(r\"http\\S+\", \"\", tweet))\n# print(tweetswithouturlstest)\n\nnocontractionstest=[]\nfor tweet in tweetswithouturlstest:\n    \n    nocontractionstest.append(replace_contractions(tweet))\n# print(nocontractionstest)\ntokenstest = [word_tokenize(sen) for sen in nocontractionstest]\n# print(tokenstest)\n\nnopuncttest=[]\nfor listt in tokenstest:\n    nopuncttest.append(remove_punctuation(listt))\n# print(nopuncttest)\n\nonlyasciitest=[]\nfor listt in nopuncttest:\n    onlyasciitest.append(remove_non_ascii(listt))\n# print(onlyasciitest)\n\nlowertest=[]\nfor listt in onlyasciitest:\n    lowertest.append(to_lowercase(listt))\n# print(lowertest)\n\nnostopwordstest=[]\nfor listt in lowertest:\n    nostopwordstest.append(remove_stopwords(listt))\n# print(nostopwordstest)\nFinalwordstest=[]\nfor entry in nostopwordstest:\n#     print(entry)\n    words=[]\n    # Initializing WordNetLemmatizer()\n    \n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n    for word, tag in pos_tag(entry):\n        # Below condition is to check for Stop words and consider only alphabets\n        \n        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n#         print(word_Final)\n        words.append(word_Final)\n    Finalwordstest.append(words)\n# print(Finalwordstest)","f90412aa":"test['Text_Final'] = [' '.join(sen) for sen in Finalwordstest]\ntest['tokens'] = Finalwordstest\ntest.head()","d4d2825f":"all_training_words = [word for tokens in train[\"tokens\"] for word in tokens]\n\n# count_vect = CountVectorizer()\n# X_train_counts = count_vect.fit_transform(all_training_words)\n# print(X_train_counts)\ntraining_sentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))\n### for svm split data\n\n\n","597d544d":"all_test_words = [word for tokens in test[\"tokens\"] for word in tokens]\ntest_sentence_lengths = [len(tokens) for tokens in test[\"tokens\"]]\nTEST_VOCAB = sorted(list(set(all_test_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\nprint(\"Max sentence length is %s\" % max(test_sentence_lengths))","2b83dcd7":"MAX_SEQUENCE_LENGTH = 50\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D,MaxPooling1D, Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\n\ntokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\ntokenizer.fit_on_texts(train[\"Text_Final\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(train[\"Text_Final\"].tolist())\ntrain_word_index = tokenizer.word_index\nprint(\"Found %s unique tokens.\" % len(train_word_index))\ntrain_cnn_data = pad_sequences(training_sequences, \n                               maxlen=MAX_SEQUENCE_LENGTH)","58511fd5":"test_sequences = tokenizer.texts_to_sequences(test[\"Text_Final\"].tolist())\ntest_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n# print(test_cnn_data.shape)\n","3655c120":"# # import gensim.downloader as api\n# # path = api.load(\"word2vec-google-news-300\", return_path=True)\npath='..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin'\nfrom gensim import models\nword2vec = models.KeyedVectors.load_word2vec_format(path, binary=True)\n","c56dbd1f":"EMBEDDING_DIM = 300\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights.shape)","37faebd4":"def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=False)\n    \n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    convs = []\n    filter_sizes = [2,3,4,5,6]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n\n\n    l_merge = concatenate(convs, axis=1)\n\n    x = Dropout(0.1)(l_merge)  \n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n\n    \n    \n    preds = Dense(2, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n#     model.summary()\n    return model","0186b4a4":"label_names = ['Disaster', 'Not a Disaster']\ny_train = train[label_names].values\n\n\n# print(y_train)\nx_train = train_cnn_data","9a59e003":"from keras.callbacks import EarlyStopping\nnum_epochs = 4 #3 is enough but just testing\nbatch_size = 24\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","bb4740a5":"# for i in range(5):\n#     print('Trial-',i)\nmodel = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n                len(list(label_names)))\n\nhist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size,callbacks=[es])\n","629d06fa":"predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\nprint(predictions)","1ff93fcd":"labels = [1, 0]\nprediction_labels=[]\nfor p in predictions:\n    prediction_labels.append(labels[np.argmax(p)])\n# print(prediction_labels)\ni=1\n# for p in prediction_labels:\n#     print(i,'-',p)\n#     i+=1","04d7952e":"test['target']=prediction_labels\n# print(test[['tokens','target']])\nsubmissions=pd.DataFrame(pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv'))\n# submissions['target']=prediction_labels\n# print(submissions)\n# submissions.to_csv('\/kaggle\/working\/submission.csv',index=False)","9084b9bc":"# submissions=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n# comparewithnb=pd.DataFrame(pd.read_csv('..\/input\/comparewithnb\/filename11.csv'))\n# cwnb=comparewithnb['0'].to_list()\n# print(len(cwnb))\n# count=0\n# mismatch=[]\n# for i in range(3263):\n#     if(cwnb[i]==prediction_labels[i]):\n#         count+=1\n#     else:\n#         mismatch.append(i)\n# print(count)\n        ","35ee3f08":"testlabels=pd.DataFrame(pd.read_csv('..\/input\/testlabels2\/submission.csv'))\nlabels=testlabels['target'].to_list()\ncount=0\nmismatch=[]\nfor i in range(3263):\n    if(labels[i]==prediction_labels[i]):\n        count+=1\n    else:\n        mismatch.append(i)\nprint(count)\n\n","e7f62b40":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\n##tfidf\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(train[\"Text_Final\"])\nprint(X_train_counts.shape)\n\n\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True).fit(X_train_counts)\nX_train_tf = tfidf_transformer.transform(X_train_counts)\nprint(X_train_tf.shape)","a6496856":"X_test_counts = count_vect.transform(test['Text_Final'])\nX_test_tf = tfidf_transformer.transform(X_test_counts)\nprint(X_test_counts.shape)\nprint(X_test_tf.shape)","63ba1d3b":"df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count_vect.get_feature_names(),columns=[\"idf_weights\"]) \ndf_idf.sort_values(by=['idf_weights'])\n# df_idf","55d4ba9e":"from sklearn.linear_model import SGDClassifier\nfrom sklearn import svm,metrics\n\ntrain_model=svm.SVC().fit(X_train_tf, train[\"target\"].values)\npredictions=train_model.predict(X_test_tf)\n\n# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n# hist2=SVM.fit(X_train_tf,train_y)\n# predictions_SVM = SVM.predict(X_test_tf)\n","bcb1ad76":"##SVM\ncount=0\ntestlabels=pd.DataFrame(pd.read_csv('..\/input\/testlabels2\/submission.csv'))\nlabels=testlabels['target'].to_list()\nmismatch=[]\nfor i in range(3263):\n    if(labels[i]==predictions[i]):\n        count+=1\n    else:\n        mismatch.append(i)\nprint(count)\nsubmissions['target']=predictions\n\nsubmissions.to_csv('\/kaggle\/working\/submission.csv',index=False)\n\n","265a8828":"2611\/3623","e8a6f12f":"# Using CNNs to classify disaster tweets\nLet's look at the training data first. We can use the pandas module to read our training data and store it in a dataframe. When we view the first few rows of the dataframe, we can see that there are 5 columns, and 2 columns have missing values. We then drop these columns as they aren't really useful for classification. \nBefore we send data into the model, we need to convert it into a ....\n## Text Preprocessing\nSo we begin text preprocessing by converting the 'text' column of the dataframe into a list. Each tweet is an element of this list. Now, from each tweet we remove numbers using regex pattern matching.","d6d96c5c":"## Convolutional Neural Network\nWe now define our CNN with an embedding layer. So any data that goes into this model is embedded through word2vec like we just did. So for test data this happens here, based on the train_embedding_matrix as weights, an embedding layer is created for the test input. This embedding layer is a layer of word2vec vectors of the test data. This layer acts as input to the convolutional layer. So for the convolutional layer is of depth 5. The 5 filters are of sizes 2,3,4,5,6. So the embedded input passes through each filter and a global max pooling layer that makes the number of parameters that pass through the subsequent layers smaller, by taking max parameter in a region. A relu activation function is also applied. The resultant is passed through a dropout layer with 10 percent dropout. Then a dense layer that retains 128 parameters and relu activation function is used and then another dropout layer. We end with a final dense layer and a relu activation layer that provides probabilities of 'disaster' and 'not disaster'. We then define loss function and the optimiser for backtracking and updating the weights. We return this model. ","aa52aa2f":"So here we assign the labels to y_train for training. We then also assign the padded tokenised tweets of training data to x_train.","e3bb9abf":"We then create a bag of training words as a list called \"all_training_words\". This contains all the words in all of the training tweets as one list. We create another array called \"training_sentences_length\" which stores a list of lengths of each tweet.\nThen a list called \"TRAINING_VOCAB\" is created to store the set of unique words in all of the tweets in the training set sorted alphabetically. So this gives us the vocabulary we're dealing with to train our model.","150a6ace":"We then tokenise each tweet. With tokenising, we transform each tweet into a list, with each element of this tweet list being each word in the tweet. These words are called tokens.","07d48058":"Here, we lemmatize the words to use only the root forms of the words, unless they're nouns.","23f291b3":"Next, we replace contraced forms of words like 'don't' and 'can't' with their expanded forms 'do not' and 'cannot'.","4ec63dec":"We define number of epochs and batch size for training.\nWe also define the early stopping criteria so training stops when validation loss reaches a minimum. This prevents overfitting.","2b5fef21":"We repeat the above process for test data as well.","195be930":"Next we add the tokenised test tweets as sentences in one column called \"Text_Final\" and as tokens in another column called \"tokens\".","de8bbb8a":"Here, we also tokenise the test data into a sequence of indices as well and also pad the sequences. But the indices used are same as the ones used for the train data. And unseen words are relaced with 0.","0e862648":"We now construct vectors for each word such that if the word already exists in the word2vec model, the vector for that word is used, and if it is not, a random vector is used. So each vector length is 300. So train_embedding_weights contains the vectors for each word.","f5f81d81":"We then remove stopwords, which don't necessarily add to convey the main idea of the tweet. These include words like i, our, of, for.","c533b0b4":"Some random code below pls ignore","e44a9003":"An alternate form of counting and vectorising is using tf-idf, where the frequency of a word is compared with frequency across all documents, so its frequency can be attributed to being because of its existence in a class. A word that is more common across all documents will have a lower idf weight. The smooth_idf parameter is used to calculate weights as id there is another document that contains all words in the collection exactly once. ","5ac61346":"We see that some of the tokens are punctiuation indicators like  .  ,  ?  , ! . These aren't necessary as well, as we need to understand the text in terms of the words and their context only. ","0b9f5f06":"We then remove any characters which are not ascii characters. So we retain only A-Z as we removed numbers already.","745a9457":"When you print the output, you can see that there are a bunch of URLs in the tweets as well. We remove these by matching any text that begins with 'http\\'.","19e6fee5":"We then repeat all the processing steps for the test data.","83affb04":"To maintain uniformity we convert all letters to lowercase.","914c357d":"We create an instance of our model and pass the training weights, max sequence length, no. of unique words, embedding dimension, and the no. of output labels to generate.","79715879":"We now add the predictions in the dataframe. We also write the predictions into the submissions file.","ed63d84b":"SVM does seem to do better than CNNs, but I will look into this later.","ab4c1045":"Next from the probability scores, we assign 1 if 'disaster' class is a higher probability and 0 otherwise.","25bb3990":"The model's predictions is stored. This is a list of probabilities for both 'disaster' and 'not a disaster'.","b2f40a0d":"Let's now add these preprocessed tokens next to the tweets in the dataframe. We add them as sentences or a string of tokens in one column called \"Text_Final\". Then in another column called \"tokens\", we add the them as a list of tokens. ","d742cb55":"Here, I've implemented the SVM, which tries to divide datapoints into classes by using a hyperplane. This hyperplane must be as distant from the two classes as possible. The support vectors are the points closest to the hyperplane.","91dbbca0":"We then use the Keras Tokeniser to create a set of indices for each word. So since TRAINING_VOCAB represents the set of unique words, the length of this list gives the max number of indices required. So each tweet is now represented as a set of numbers with each word replaced by an index. So what the fit_on_texts method does is, it looks at the frequencies of words appearing in all the tweets and gives a lower index if its frequency is higher. So a word like 'injured' might occur most frequently in this data set and might be given the index 1. Now the texts_to_sequences method replaces each word in every tweet with the index it was fit with. Train_word_index is a dictionary containing the index mapped to the corresponding word. So its length gives set of unique words. So now we have lists of indices as tweets. But each tweet is of different length, so we pad with zeroes. Now each tweet is a sequence of indices and its length is 50.","78e921a5":"## Creating word embeddings\nNow, we use a trained word2vec model. This model takes in a corpus of text as input, which in this case is the set of words in the tweets in the order in which they appear. This represents a continuous bag of words. It then creates vectors for each word. These vectors now constitute a 300 dimensional vector space with words sharing common context appearing closer together in the vector space. Creating word embeddings this way helps us create a representation for words in their linguistic contexts. So we load the path where the model is stored. We then load the model in."}}