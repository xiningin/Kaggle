{"cell_type":{"4d99fdc3":"code","47f8d283":"code","5c38c904":"code","a9934045":"code","c9feca4c":"code","88b1a86f":"code","db69ce1d":"code","cb753635":"code","3a0133c0":"code","8a87b7cf":"code","3e83aa90":"code","fa6a772b":"code","1e764edc":"code","864277fd":"code","facab1c4":"code","e4ce2667":"code","5cdce8bf":"code","e88c6f2e":"code","6019895e":"code","11487902":"code","4d637f94":"code","f2ec7a58":"code","34a72244":"code","64e3ea8f":"code","b1dc9522":"code","4d1a0f2c":"markdown","e4608a8b":"markdown","25b325b2":"markdown","8df15884":"markdown","40d40dba":"markdown","2b0170c8":"markdown","e1ab5fdc":"markdown","8869561f":"markdown","e33853f6":"markdown","b2f67fbe":"markdown","f238786e":"markdown","402f8815":"markdown","8680afe6":"markdown","d5b0458d":"markdown","d33d454e":"markdown","a362468c":"markdown","ec31f293":"markdown","0e57f804":"markdown","39808011":"markdown","818f16bc":"markdown","d1be32db":"markdown","7e378ad7":"markdown","da9c85ef":"markdown"},"source":{"4d99fdc3":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import nan\nfrom numpy import absolute\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport cufflinks as cf\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected = True)\ncf.go_offline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","47f8d283":"df = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\nprint(df.shape)\ndf.head()","5c38c904":"print(df.info(), '\\n')\nprint(df.describe(), '\\n')\ndf.isnull().sum()","a9934045":"region_count = pd.DataFrame(df.region.value_counts())\nregion_charge_sum = pd.DataFrame(df.groupby('region').sum()['charges'])\nregion_charge_mean = pd.DataFrame(df.groupby('region').mean()['charges'])\n\nfig, ax = plt.subplots(1, 2, figsize = (13, 5))\n\nax0 = sns.barplot(data = region_count, x = region_count.index.to_list(), \n                     y = region_count['region'], ax = ax[0])\nfor i in ax0.patches:\n    ax0.text(x = i.get_x() + 0.2, y = i.get_height(), s = i.get_height(), fontsize = 14)\nax0.set_title(\"Region Count\", fontsize = 12)   \n    \nax1 = sns.barplot(data = region_charge_sum, x = region_charge_sum.index.to_list(), \n                  y = region_charge_sum['charges'], ax = ax[1], alpha = 0.5)  \nax2 = ax[1].twinx()\nax1a = sns.lineplot(data = region_charge_mean, x = region_charge_mean.index.to_list(), \n                    y = region_charge_mean['charges'], marker = 'o', markersize = 8, \n                    color = 'purple', ax = ax2)  \nfor i in ax1.patches:\n    ax1.text(x = i.get_x() + 0.07, y = i.get_height() +1e5, s = \"{:.3g}\".format(i.get_height()), \n             fontsize = 14, color = \"red\")\n    \nax1.set_title(\"Region Sum (Bar) and Region Mean (Line)\", fontsize = 12)\n\nfor tick_label in ax1.axes.get_yticklabels():\n    tick_label.set_color(\"red\")\n    tick_label.set_fontsize(\"12\")\n\nfor tick_label in ax1a.axes.get_yticklabels():\n    tick_label.set_color(\"purple\")\n    tick_label.set_fontsize(\"12\")","c9feca4c":"plt.figure(figsize = (11, 8))\nsns.scatterplot(data = df, x = df.age, y = df.charges, hue = 'smoker', \n                style = 'sex', size = 'bmi', sizes = (20, 200), legend='auto')\nplt.legend(loc = 'upper right', bbox_to_anchor = (1.13, 1))\nplt.title(\"Charges vs Age, Smoker, bmi, sex\", fontsize = 12)\nplt.xlabel(\"Age\", fontsize = 12); plt.ylabel(\"Charges\", fontsize = 12)\nplt.show()","88b1a86f":"fig = px.box(df, x = 'children', y = 'charges', color = 'sex')\nfig.update_layout(title = \"Does the number of children affect the amount of charges for both genders?\", \n                  paper_bgcolor = 'rgb(243, 243, 243)', \n                 plot_bgcolor = 'rgb(243, 243, 243)')","db69ce1d":"px.box(df, x = 'region', y = 'charges', color = 'children')","cb753635":"fig = px.pie(df, values = \"charges\", names = \"children\")\nfig.update_layout(title = \"How much does each section of the number of children account for the charges\", \n                  paper_bgcolor = 'rgb(243, 243, 243)', \n                 plot_bgcolor = 'rgb(243, 243, 243)')","3a0133c0":"northeast = df.groupby(['region', 'children']).sum()['charges']['northeast']\nnorthwest = df.groupby(['region', 'children']).sum()['charges']['northwest']\nsoutheast = df.groupby(['region', 'children']).sum()['charges']['southeast']\nsouthwest = df.groupby(['region', 'children']).sum()['charges']['southwest']\n\nregion_list = [northeast, northwest, southeast, southwest]\nregion_name_list = ['northeast', 'northwest', 'southeast', 'southwest']\nplt.figure(figsize = (16,15), facecolor='white')\nfor num, (reg, name) in enumerate(zip(region_list, region_name_list)):\n    plt.subplot(2, 2, num+1)\n    labels = reg.values\n    reg.plot.pie(autopct = \"%.3g %%\", pctdistance = 0.8, fontsize =16)\n    plt.title(name, fontsize =16)\n    plt.ylabel(\"\")\nplt.suptitle(\"How much does the number of Children account for the total charges in each region\", \n             fontsize = 16)\nplt.show()","8a87b7cf":"numeric_data = df.select_dtypes(np.number)\nfig, ax = plt.subplots(2, 2, figsize = (10, 6), constrained_layout = True)\nax = ax.flatten()\nsns.set_style('darkgrid')\nfor num, col in enumerate(numeric_data.columns):\n    sns.distplot(numeric_data[col], ax = ax[num])\nplt.suptitle('Distribution of Numeric Data')\nplt.show()","3e83aa90":"cat_data = df[['sex', 'smoker', 'region', 'charges']]\nfig, ax = plt.subplots(3, 1, figsize =(10, 16))\nax = ax.flatten()\nfor num, col in zip(range(3), cat_data.columns):\n    sns.boxplot(data = cat_data, x = cat_data[col], y = cat_data['charges'], ax = ax[num])","fa6a772b":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        return X[self.attribute_names]\n\nnum_pipeline = Pipeline([\n    ('select_numeric', DataFrameSelector(['age', 'bmi', 'children'])),\n    ('minmax_scaler', MinMaxScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('select_cat', DataFrameSelector(['sex', 'smoker', 'region'])),\n    ('one_hot', OneHotEncoder())\n])\n\npreprocessing_pipeline = FeatureUnion(transformer_list = [\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline)\n])\n\ny_pipeline = Pipeline([\n    ('select_numeric', DataFrameSelector(['charges'])),\n    ('minmax_scaler', MinMaxScaler())\n])\n\nX = preprocessing_pipeline.fit_transform(df).toarray()\ny = y_pipeline.fit_transform(df)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","1e764edc":"# We are going to try and compare 12 different models\n\nlin_reg = LinearRegression()\nlasso = Lasso(random_state = 42)\nridge = Ridge(random_state = 42)\nelastic_net = ElasticNet(random_state = 42)\nsgd_reg = SGDRegressor(random_state = 42)\nrand_reg = RandomForestRegressor(random_state = 42)\ntree_reg = DecisionTreeRegressor(random_state = 42)\ngb_boost = GradientBoostingRegressor(random_state = 42)\nada_boost = AdaBoostRegressor(random_state = 42)\nknn_reg = KNeighborsRegressor()\nsvm = SVR(kernel='linear')\nxgb_reg = XGBRegressor(random_state = 42)\n\nregressor_list = [lin_reg, lasso, ridge, elastic_net, sgd_reg, rand_reg, \n                  tree_reg, gb_boost, ada_boost, knn_reg, svm, xgb_reg] \nregressor_name_list = [\"lin_reg\", \"lasso\", \"ridge\", \"elastic_net\", \"sgd_reg\", \"rand_reg\", \n                  \"tree_reg\", \"gb_boost\", \"ada_boost\", \"knn_reg\", \"svm\", \"xgb_reg\"] ","864277fd":"rmse = []\nmse = []\nmae = []\nr2 = []\ny_predicted = []\nfor reg in regressor_list:\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_predicted.append(y_pred)\n    rmse.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n    mse.append(mean_squared_error(y_test, y_pred))\n    mae.append(mean_absolute_error(y_test, y_pred))\n    r2.append(r2_score(y_test, y_pred))","facab1c4":"fig, ax = plt.subplots(4, 3, sharex = True, sharey = True, figsize = (15,13))\nmodels = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Elastic Net', 'SGD Regressor',\n         'RandomForest Regressor', 'DecisionTree Regressor', 'GradientBoost Regression', \n          'AdaBoost Regressor', 'KNN Regressor', 'SVM', 'XGBoost Regressor']\ny_pred_models = y_predicted\nax = ax.flatten()\nfor num, (pred, model) in enumerate(zip(y_pred_models, models)):\n    ax[num].scatter(pred, y_test, s=20)\n    ax[num].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n    ax[num].set_title(model, fontsize = 14)\n    \nfig.supxlabel('Predicted Values', fontsize = 14)\nfig.supylabel('True Values', fontsize = 14)\nplt.suptitle(\"True Values vs Predicted Values\", fontsize = 14)\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","e4ce2667":"compare_regressor = pd.DataFrame(regressor_name_list, columns = ['Model'])\ncompare_regressor['rmse'] = rmse\ncompare_regressor['mse'] = mse\ncompare_regressor['mae'] = mae\ncompare_regressor['r2'] = r2\ncompare_regressor.sort_values(by = 'rmse', ascending = True)","5cdce8bf":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X_train, X_test, y_train, y_test):\n    train_errors, test_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_test_predict = model.predict(X_test)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        test_errors.append(mean_squared_error(y_test, y_test_predict))\n\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(test_errors), \"b-\", linewidth=3, label=\"test\")\n    plt.legend(loc=\"upper right\", fontsize=14)   \n    plt.xlabel(\"Training set size\", fontsize=14) \n    plt.ylabel(\"RMSE\", fontsize=14)                ","e88c6f2e":"plt.figure(figsize = (15, 10))\nplot_learning_curves(gb_boost, X_train, X_test, y_train, y_test)","6019895e":"max_depths = np.arange(1, 10, 1)\nparam_range = max_depths\n\ntrain_results = []; test_results = []\nfor max_depth in max_depths:\n    model = GradientBoostingRegressor(max_depth = max_depth, random_state = 42)\n    model.fit(X_train, y_train)\n    \n    train_pred = model.predict(X_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    train_results.append(train_rmse)\n    \n    test_pred = model.predict(X_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n    test_results.append(test_rmse)\n\nplt.figure(figsize = (15, 8))\nplt.plot(param_range, train_results, 'b-', label = \"train\")\nplt.plot(param_range, test_results, 'r-', label = \"test\")\nplt.legend(loc = 'upper right', fontsize = 16)\nplt.xticks(param_range, rotation = 90, fontsize = 16)\nplt.ylabel(\"RMSE\", fontsize = 16)\nplt.xlabel(\"Depth of each tree (max_depths)\", fontsize = 16)\nplt.show()","11487902":"n_estimators = np.arange(10, 100, 5)\nparam_range = n_estimators\n\ntrain_results = []; test_results = []\nfor n_estimator in n_estimators:\n    model = GradientBoostingRegressor(n_estimators = n_estimator, random_state = 42)\n    model.fit(X_train, y_train)\n    \n    train_pred = model.predict(X_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    train_results.append(train_rmse)\n    \n    test_pred = model.predict(X_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n    test_results.append(test_rmse)\n\nplt.figure(figsize = (15, 8))\nplt.plot(param_range, train_results, 'b-', label = \"train\")\nplt.plot(param_range, test_results, 'r-', label = \"test\")\nplt.legend(loc = 'upper right', fontsize = 16)\nplt.xticks(param_range, rotation = 90, fontsize = 16)\nplt.ylabel(\"RMSE\", fontsize = 16)\nplt.xlabel(\"Number of trees (n_estimators)\", fontsize = 16)\nplt.show()","4d637f94":"learning_rates = np.arange(0.01, 0.05, 0.001)\n\ntrain_results = []; test_results = []\nfor eta in learning_rates:\n    model = GradientBoostingRegressor(learning_rate = eta, random_state = 42)\n    model.fit(X_train, y_train)\n    \n    train_pred = model.predict(X_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    train_results.append(train_rmse)\n    \n    test_pred = model.predict(X_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n    test_results.append(test_rmse)\n\nplt.figure(figsize = (15, 8))\nplt.plot(learning_rates, train_results, 'b-', label = \"train\")\nplt.plot(learning_rates, test_results, 'r-', label = \"test\")\nplt.legend(loc = 'upper right', fontsize = 16)\nplt.xticks(learning_rates, rotation = 90, fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.ylabel(\"RMSE\", fontsize = 16)\nplt.xlabel(\"Learning Rate\", fontsize = 16)\nplt.show()","f2ec7a58":"subsamples = np.arange(0.01, 0.2, 0.01)\nparam_range = subsamples\n\ntrain_results = []; test_results = []\nfor subsample in subsamples:\n    model = GradientBoostingRegressor(subsample = subsample, random_state = 42)\n    model.fit(X_train, y_train)\n    \n    train_pred = model.predict(X_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    train_results.append(train_rmse)\n    \n    test_pred = model.predict(X_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n    test_results.append(test_rmse)\n\nplt.figure(figsize = (15, 8))\nplt.plot(param_range, train_results, 'b-', label = \"train\")\nplt.plot(param_range, test_results, 'r-', label = \"test\")\nplt.legend(loc = 'upper right', fontsize = 16)\nplt.xticks(param_range, rotation = 90, fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.ylabel(\"RMSE\", fontsize = 16)\nplt.xlabel(\"Subsample\", fontsize = 16)\nplt.show()","34a72244":"gb_reg = GradientBoostingRegressor(max_depth = 2, n_estimators = 20, learning_rate = 0.02, \n                          subsample = 0.03, random_state = 42)\ngb_reg.fit(X_train, y_train)\ngb_reg_y_pred = gb_boost.predict(X_test)\ngb_reg_rmse = np.sqrt(mean_squared_error(y_test, gb_reg_y_pred))\nprint(\"rmse: \", gb_reg_rmse)\n\nplt.scatter(gb_reg_y_pred, y_test)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw = 2)\nplt.show()","64e3ea8f":"plt.figure(figsize = (15, 10))\nplot_learning_curves(gb_reg, X_train, X_test, y_train, y_test)","b1dc9522":"rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Gradient Boosting Regressor After Tuning\")\nprint(\"rmse: {}\".format(rmse))\nprint(\"mse: {}\".format(mse))\nprint(\"mae: {}\".format(mae))\nprint(\"r2: {}\".format(r2))","4d1a0f2c":"#### Hyperparameter Tunning Summary:\n1. Depth of each tree (max_depth): 2\n\n2. Number of trees (n_estimators): 20\n\n3. Learning rate (learning_rate, eta): 0.02\n\n4. Sub sample (subsample): 0.03","e4608a8b":"Indeed, Gradient Boosting Regressor gives the best result (i.e. has the less error). Its RMSE is close to zero, it means the model performs well if there is no overfitting. Therefore, the next thing to do is to check if overfitting occurs and fix overfitting if it happens.","25b325b2":"#### Let's choose subsample = 0.03","8df15884":"#### 1. Depth of each tree (max_depth)","40d40dba":"# Exploratory Data Analysis","2b0170c8":"Visualize the predictions","e1ab5fdc":"#### As we can see, two lines are close to each other. This means that there is no overfitting anymore.","8869561f":"# Plot the learning curves to check if there overfitting occurs","e33853f6":"# Tune the Gradient Boosting Regressor Model","b2f67fbe":"#### 2. Number of trees (n_estimators)","f238786e":"# Train and predict again after tunning the hyperparameters","402f8815":"### 3. Learning Rate","8680afe6":"#### Let's choose the Number of trees (n_estimators) as 20","d5b0458d":"#### 4. Subsample","d33d454e":"Observations :\n\n1. Charges increas as age increases\n2. Smokers cost much more than non-smokers\n3. Smokers with high bmi cost more (almost double the charges)\n4. Smokers with low bmi cost less than non-smokers with high bmi","a362468c":"# Build a Model\n\nI use MinMaxScaler to scale both the input features and the target. \n\nThe reason I also scale the target is that it is much eaiser to determine if the values of Root Mean Square Error (RMSE), Mean Squared Error (MSE), Mean Absolute Error (MAE), and R2 Score are large or not. For example, if the RMSE is larger than 1, it means your model perform worse than a naive prediction.","ec31f293":"# The hyperparameters that I am going to tune:\n1. Depth of each tree (max_depth)\n2. Number of trees (n_estimators)\n3. Learning rate (learning_rate)\n4. Sub sample (subsample)","0e57f804":"There is no missing value","39808011":"From the above graph, it seems like Gradient Boosting Regressor has the best results. Let's vertify that below.","818f16bc":"### Look at the distribution of numeric data","d1be32db":"#### Let's choose the depth of each tree (max_depth) as 2","7e378ad7":"Oh no, it's overfitting. The reason is that the test set has a much higher RMSE values than the training set. Another quick way to determine is that there is a big gap between the training set and the test set.\n\nWe notice that the gap is getting closer. This means that the model may have better performance if we feed more data to the model. However, we don't have any more data. Therefore, we need to tune the hyperparameters of Gradient Boosting Regressor to avoid overfitting from happening.","da9c85ef":"#### Let's choose the learning rate of 0.02"}}