{"cell_type":{"f7d6f835":"code","466bf9de":"code","072c7865":"code","b84d693d":"code","51dc5d53":"code","1ebc6de3":"code","487bc582":"code","67c95d27":"code","f2d46ae6":"code","8732a2d5":"code","c238e7e3":"code","75b76fa4":"code","88da0455":"code","e02be5bd":"code","594c8180":"code","9055adac":"code","2c627753":"code","b865e7e0":"code","407c6343":"code","2ceb9e3c":"code","9023cde0":"code","ba05cc97":"code","8bd63484":"code","7989cc52":"code","8e2adb85":"code","9b0dff55":"code","fed9316b":"code","67ae15eb":"code","f894e694":"code","2a7f501d":"code","c421791b":"code","651d33ad":"code","e96c4913":"code","3b4a22fe":"code","163b8434":"code","ce724f19":"code","b1ee15db":"code","2adaf3bb":"code","66e897fd":"code","1c782c56":"code","9bd18555":"code","f3df6bc8":"code","14318763":"code","a34fab4a":"code","068b0a25":"code","a6a4674b":"code","d02006a8":"code","57ef3f38":"code","03e7cdfb":"code","3176ff65":"code","35bcd2cf":"code","b96d00d1":"code","d303dc46":"code","9b1f53d6":"code","12bb08c3":"code","2ecd9644":"code","3974bcf3":"code","cc6b8c63":"code","37a59a3a":"code","2fea7e58":"code","f9f66abb":"code","df47e003":"code","2ca77a2f":"code","62048519":"code","839139e5":"code","246a6d12":"code","9fb31a06":"code","c6e5616c":"code","b9e43de5":"code","9d240136":"code","7d3c8642":"code","f3ddcf53":"code","2b88e1a4":"code","26c5b976":"code","17ae52cc":"code","a315187e":"code","2e7da336":"code","c577df7f":"code","bc8d8e26":"code","033faf4a":"code","a3573433":"code","4860bb62":"code","2bfd2532":"code","0c12e3c9":"code","0cf98da8":"code","1a286333":"code","a0582197":"code","b3e61d55":"code","66b4fdf3":"code","ae333a02":"code","4a25cb65":"code","e9219cde":"code","abdbad4d":"code","9936d213":"code","8005225e":"code","a7e77b6c":"code","f5be8dc4":"code","d2fe9fea":"code","c41255e6":"markdown","c98c8519":"markdown","b9d2eceb":"markdown","c2210f3b":"markdown","a60e4386":"markdown","c9e0a231":"markdown","effb22fc":"markdown","438d2cce":"markdown","7d8fe8e1":"markdown","9c5b5a26":"markdown","1ee11e88":"markdown","04c48b25":"markdown","bbc37ca3":"markdown","e98d491d":"markdown","98c96eb5":"markdown","cee8a492":"markdown","0a42610e":"markdown","52107ab2":"markdown","dd0b3849":"markdown","bd984e7c":"markdown","9f1f75ee":"markdown","f0098e2c":"markdown","016a4f7a":"markdown","72a7396e":"markdown","418bc1c2":"markdown","8fd1527d":"markdown","270a962d":"markdown","ab0ac7f2":"markdown","b54c1d82":"markdown","f44187ea":"markdown","022e6ac2":"markdown","deb5776a":"markdown","d6028c1d":"markdown","2dbc0dc2":"markdown","8c2a73e3":"markdown","9bc74c10":"markdown","91b0ee5f":"markdown","27ee208d":"markdown","833c9f93":"markdown"},"source":{"f7d6f835":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport time\n\n# 3D atavisualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\n\n\n# Machine Learning\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet, Lasso, Ridge\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor,GradientBoostingRegressor, RandomForestRegressor,  GradientBoostingRegressor\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, KFold, cross_val_score\nfrom sklearn.ensemble import StackingRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# You can go offline on demand by using\ncf.go_offline() \n# initiate notebook for offline plot\ninit_notebook_mode(connected=False)         \n\n# set some display options:\nplt.rcParams['figure.dpi'] = 100\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","466bf9de":"# Load all the data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntest_id = df_test.reset_index().drop('index',axis=1)['Id']","072c7865":"# Take a look of the shape\nprint(f'Train shape : {df_train.shape}')\nprint(f'Test shape : {df_test.shape}')\nprint(f'Submission shape : {df_submission.shape}')","b84d693d":"# Configuration to see all features\npd.set_option('display.max_row', 111)\npd.set_option('display.max_column', 111)","51dc5d53":"# Take a look on train set\ndf_train.head()","1ebc6de3":"# Take a look on test set\ndf_test.head()","487bc582":"# Take a look on submission exemple\ndf_submission.head()","67c95d27":"# Combine df_test + df_submisision\ndf_test_full = pd.merge(df_test,df_submission, how = 'inner')\ndf_test_full.shape","f2d46ae6":"# Concat df_train + df_test_full\ndata = pd.concat([df_train,df_test_full], axis=0)\ndata.shape\n\n","8732a2d5":"#Copy the dataset for secure\ndf = data.copy()","c238e7e3":"# Take a look on the differents variables\ndf.info()","75b76fa4":"# Type representation\ndf.dtypes.value_counts().plot.pie()","88da0455":"#finding the unique values in each column\nfor col in df.columns:\n    print('We have {} unique values in {} column'.format(len(df[col].unique()),col))\n    print('__'*30)","e02be5bd":"#describe our data\ndf[df.select_dtypes(exclude='object').columns].drop('Id',axis=1).describe().\\\nstyle.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","594c8180":"#find the null values in each column\n(df.isnull().sum()\/df.shape[0]*100).sort_values(ascending=False).to_frame().rename(columns={0:'Null values'})","9055adac":"#null percentage for each column\n\nnull_df = round(100*(df.isnull().sum().sort_values(ascending=False)\/len(df.index)),2)\\\n                    .to_frame().rename(columns={0:'Null values percentage'})[:15]\nnull_df","2c627753":"#Pie plot for the percentage values\n\nnull_df.reset_index().iplot(kind='pie',\n                            labels='index',\n                            title='Null values percentage',\n                            textinfo='label+text+percent',\n                            values='Null values percentage')","b865e7e0":"#visuaize the null values in each column\nplt.figure(figsize=(20,8));\nsns.heatmap(df.isnull(), cmap='viridis');","407c6343":"#lets see the correlation between columns and target column\ncorr = df.corr()\ncorr['SalePrice'].sort_values(ascending=False)[1:].to_frame()\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","2ceb9e3c":"#lets create a dataframe for the numeric columns with high skewness\n\nskewness = pd.DataFrame()\n\nnum_cols = []\nfor col in df.select_dtypes(exclude='object'):\n    num_cols.append(col)\n\nskewness[['Positive Columns','Skewness(+v)']] = df[num_cols].drop('Id',axis=1).skew().sort_values(ascending=False)[:10].reset_index()\nskewness[['Negative Columns','Skewness(-v)']] = df[num_cols].drop('Id',axis=1).skew().sort_values(ascending=True)[:10].reset_index()\n\nskewness.columns = pd.MultiIndex.from_tuples([('Positive Skewness', 'Columns'), ('Positive Skewness', 'Skewness'),\n                                              ('Negative Skewness', 'Columns'), ('Negative Skewness', 'Skewness')])\nskewness","9023cde0":"liste_skewness = ['MasVnrArea','BsmtHalfBath','ScreenPorch',\n                  'EnclosedPorch','BsmtFinSF2','KitchenAbvGr','3SsnPorch','LowQualFinSF',\n                  'LotArea','PoolArea','MiscVal']","ba05cc97":"fig, axes = plt.subplots(1, 2, sharex=False, figsize=(14,5))\nsns.histplot(ax=axes[0],data=df, x=\"SalePrice\", kde=True, color='orange')\naxes[0].set_title('Normal SalePrice')\nsns.histplot(ax=axes[1],data=df, x=np.log1p(df['SalePrice']), kde=True, color='g')\naxes[1].set_title('Log SalePrice')","8bd63484":"# Take a look on the numerical distributions : float type\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(3,4, figsize=(18, 8));\nplt.subplots_adjust(hspace = 0.7, wspace=0.2)\nfig.suptitle('Numerical Float Distributions', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('float').columns)  # number of rows\n\nfor i,col in zip(range(a),df.select_dtypes('float')):\n    sns.kdeplot(df[col], ax=axes[i\/\/4][i%4], fill= True);\n    axes[i\/\/4][i%4].set_title(col+' Distribution')","7989cc52":"# Take a look on the numerical distributions : int type\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(9,3, figsize=(18, 12));\nplt.subplots_adjust(hspace = 1.5, wspace=0.2)\nfig.suptitle('Numerical Int Distributions', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('int').columns)  # number of rows\n\nfor i,col in zip(range(a),df.select_dtypes('int')):\n    sns.kdeplot(df[col], ax=axes[i\/\/3][i%3], fill= True, color='g');\n    axes[i\/\/3][i%3].set_title(col+' Distribution')","8e2adb85":"for col in df.select_dtypes('object'):\n    print(f'{col :-<50} {df[col].unique()}')","9b0dff55":"#for col in df.select_dtypes('object'):\n#    plt.figure()\n#    df[col].value_counts().plot.pie()","fed9316b":"# Take a look on the numerical regression : float typee\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(3,4, figsize=(18, 8));\nplt.subplots_adjust(hspace = 0.7, wspace=0.2)\nfig.suptitle('Numerical float Regression', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('float').columns)  # number of rows\n\nfor i,col in zip(range(a),df.drop('SalePrice',axis=1).select_dtypes('float')):\n    sns.regplot(x=df['SalePrice'],y=df[col],marker=\"+\", ax=axes[i\/\/4][i%4]);\n    axes[i\/\/4][i%4].set_title(col+' Regression')","67ae15eb":"# Take a look on the numerical regression : int typee\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(9,3, figsize=(16, 20));\nplt.subplots_adjust(hspace = 1.2, wspace=0.4)\nfig.suptitle('Numerical Int Regression', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('int').columns)  # number of rows\n\nfor i,col in zip(range(a),df.drop(['Id','SalePrice'],axis=1).select_dtypes('int')):\n    sns.regplot(x=df['SalePrice'],y=df[col],marker=\"+\",color='g', ax=axes[i\/\/3][i%3]);\n    axes[i\/\/3][i%3].set_title(col+' Regression')","f894e694":"# Relationship beetween : SaleCondition and SalePrice with the best correalion feature : GrLivArea\ng = sns.lmplot(x=\"SalePrice\", y=\"GrLivArea\", hue=\"SaleCondition\", data=df)\nh = sns.lmplot(x=\"SalePrice\", y=\"GrLivArea\", col=\"SaleCondition\", hue=\"SaleCondition\",\n               data=df, col_wrap=2, height=3)","2a7f501d":"# Take a look on the categorical regression : object typee\nsns.set_style('whitegrid')\nfig, axes = plt.subplots(15,3, figsize=(14, 24));\nplt.subplots_adjust(hspace = 1.2, wspace=0.6)\nfig.suptitle('Categorical Features Visualizations', fontsize=20)\n\n# Take a look on the numerical distributions\na = len(df.select_dtypes('object').columns)  # number of rows\n\nfor i,col in zip(range(a),df.select_dtypes(include='object')):\n    sns.heatmap(pd.crosstab(df['SaleCondition'], df[col]), annot=True, fmt='d', ax=axes[i\/\/3][i%3]);\n    axes[i\/\/3][i%3].set_title(col)","c421791b":"# new feature :\ndf['SalePrice_bins'] = pd.cut(df['SalePrice'],bins=6, labels=False)\ndf['SalePrice_bins'].value_counts()","651d33ad":"sns.scatterplot(x='SalePrice_bins',y='SalePrice',data=df)","e96c4913":"dico_bins_sale = {\n    0 : '<200k',\n    1 : '<300k',\n    2 : \"<400k\",\n    3 : \"<500k\",\n    4 : \"<600k\",\n    5 : \">700k\"\n}\n\ndf['SalePrice_bins'] = df['SalePrice_bins'].map(dico_bins_sale)","3b4a22fe":"def visualisation_data(dataset,xlabel):\n    \n    #Visualization on your Data\n\n    #  plot Numerical Data\n\n    a = len(dataset.select_dtypes(include='object').columns)  # number of rows\n    b = 2  # number of columns\n    c = 1  # initialize plot counter\n\n\n    fig = plt.figure(figsize=(14,22))\n\n    for i in dataset.select_dtypes(include='object'):\n        if i != 'SalePrice_bins':\n            plt.subplot(a, b, c)\n        #plt.title('{} (heatmap), subplot: {}{}{}'.format(i, a, b, c))\n            plt.xlabel(xlabel)\n            sns.heatmap(pd.crosstab(df['SalePrice_bins'], dataset[i]), annot=True, fmt='d')\n            c = c + 1\n\n            plt.subplot(a, b, c)\n        #plt.title('{} (scatter), subplot: {}{}{}'.format(i, a, b, c))\n            plt.xlabel(xlabel)\n            sns.scatterplot(x=xlabel, y=\"SalePrice\", hue=i, alpha=.5, palette=\"muted\", data=dataset)\n            c = c + 1\n    \n    plt.show()","163b8434":"# 1 - Group Garage Option\ngarage_df = df.loc[:,['GarageFinish','GarageQual','GarageCond','GarageYrBlt','GarageType','SalePrice','SalePrice_bins']]\ngarage_df.info()","ce724f19":"visualisation_data(garage_df,'GarageYrBlt')","b1ee15db":"# Group 2 : BSM features\nbsmt_df = df.loc[:,['BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','SalePrice','SalePrice_bins','YearBuilt']]\nbsmt_df.info()","2adaf3bb":"visualisation_data(bsmt_df, 'YearBuilt')","66e897fd":"# Group 3 : other features\noth_df = df.loc[:,['MasVnrType','MasVnrArea','MSZoning','SalePrice','SalePrice_bins','YearBuilt']]\noth_df.info()","1c782c56":"#MasVnrAre vs Target\ng = sns.regplot(data=oth_df, x=\"MasVnrArea\", y=\"SalePrice\",marker='x')","9bd18555":"visualisation_data(oth_df,'YearBuilt')","f3df6bc8":"# Group 4 : Condtion Optional\noptional_df = df.loc[:,['Functional','Utilities','BsmtHalfBath','BsmtFullBath','SalePrice','SalePrice_bins','YearBuilt']]\noptional_df.info()","14318763":"fig = plt.figure(figsize=(12,8))\n\n#  subplot #1\nplt.subplot(121)\nplt.title('subplot: 211')\nsns.scatterplot(data=optional_df, x=\"YearBuilt\", y=\"SalePrice\",hue=\"BsmtHalfBath\")\n\n#  subplot #2\nplt.subplot(122)\nplt.title('subplot: 212')\nsns.scatterplot(data=optional_df, x=\"YearBuilt\", y=\"SalePrice\",hue=\"BsmtFullBath\")\n\nplt.show()","a34fab4a":"visualisation_data(optional_df,'YearBuilt')","068b0a25":"# Group 5 : Condition Optimal\noptimal_df = df.loc[:,['GarageArea','BsmtFinSF1','SaleType','GarageCars','BsmtUnfSF',\n                       'Electrical','Exterior2nd',\n                       'Exterior1st','KitchenQual','TotalBsmtSF','BsmtFinSF2',\n                       'SalePrice','SalePrice_bins','YearBuilt']]\noptimal_df.info()","a6a4674b":"fig = plt.figure(figsize=(12,10))\n\n# GarageArea\nplt.subplot(321)\nsns.scatterplot(data=optimal_df, x='GarageArea', y=\"SalePrice\")\n\n# GarageCars\nplt.subplot(322)\nsns.scatterplot(data=optimal_df, x='YearBuilt', y=\"SalePrice\", hue='GarageCars')\n\n# BsmtFinSF1\nplt.subplot(323)\nsns.scatterplot(data=optimal_df, x='BsmtFinSF1', y=\"SalePrice\")\n\n# BsmtFinSF2\nplt.subplot(324)\nsns.scatterplot(data=optimal_df, x='BsmtFinSF2', y=\"SalePrice\")\n\n# BsmtUnfSF\nplt.subplot(325)\nsns.scatterplot(data=optimal_df, x='BsmtUnfSF', y=\"SalePrice\")\n\n# TotalBsmtSF\nplt.subplot(326)\nsns.scatterplot(data=optimal_df, x='TotalBsmtSF', y=\"SalePrice\")","d02006a8":"visualisation_data(optimal_df,'YearBuilt')","57ef3f38":"# Group 8 : last\nlast_df = df.loc[:,['PavedDrive','HeatingQC','HalfBath','FullBath','YearBuilt','LotArea','Street','LotShape','LandContour','LotConfig','LandSlope',\n              'Condition1','Condition2','SalePrice_bins','SalePrice']]\n\nlast_df.info(),","03e7cdfb":"visualisation_data(last_df,'YearBuilt')","3176ff65":"# Group 8 : last\nlast_df_bis = df.loc[:,['BldgType','HouseStyle','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\n              'RoofStyle','RoofMatl','ExterQual','ExterCond','Foundation','Heating','MSSubClass','CentralAir','1stFlrSF','2ndFlrSF',\n              'LowQualFinSF','GrLivArea','SalePrice_bins','SalePrice']]\n\nlast_df_bis.info(),","35bcd2cf":"visualisation_data(last_df_bis,'YearBuilt')","b96d00d1":"#correlation heatmap\ncorr = df.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\n\ncmap = sns.diverging_palette(180, 30, as_cmap=True)\n\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(25, 25))\n    sns.heatmap(corr,  mask=mask, cmap=cmap, annot=True, center=0, vmin=-1, vmax=0.8,\n                square=True, cbar_kws={'shrink':.5, 'orientation': 'vertical'}, linewidth=.02);","d303dc46":"#Correlation Map\nax = sns.clustermap(df.select_dtypes(exclude='object').corr())","9b1f53d6":"sns.scatterplot(x='YearBuilt', y=\"SalePrice\", hue='Fence', alpha=.5, palette=\"muted\", data=df)","12bb08c3":"df = data.copy()","2ecd9644":"# Drop the columns with 80 + missing Value\ndf = df.loc[:,(df.isnull().sum()\/df.shape[0]*100) < 80]\ndf.shape","3974bcf3":"# Extract object features\nnum_cols = []\nfor col in df.select_dtypes(exclude='object'):\n    num_cols.append(col)\n    \n# Keep object features\ncat_cols = []\nfor col in df.select_dtypes(include='object'):\n    cat_cols.append(col)","cc6b8c63":"print(f'Numerical features : {num_cols}')\nprint('-'*180)\nprint(f'Categorical features : {cat_cols}')","37a59a3a":"# Split our Dataset\ndf_cat = df[cat_cols]\ndf_num = df[num_cols]","2fea7e58":"print(f'Categorical shape : {df_cat.shape}')\nprint(f'Numerical shape : {df_num.shape}')","f9f66abb":"#Visualize columns have corr with SalePrice\n\nhigh_corr = corr['SalePrice'].sort_values(ascending=False)[1:][:13].index.tolist()\n\nfig, axes = plt.subplots(4,3, figsize=(20, 10), sharey=True);\nplt.subplots_adjust(hspace = 0.7, wspace=0.1)\nfig.suptitle('Highest Correlation with sale price', fontsize=20);\n\nfor i,col in zip(range(12),high_corr):\n    sns.scatterplot(y=df['SalePrice'], x=df[col], ax=axes[i\/\/3][i%3])\n    axes[i\/\/3][i%3].set_title('SalesPrice with '+col)","df47e003":"# We can detect now our outliers\n\ndrop_index = df[((df['GarageArea']>1200) & (df['SalePrice']<300000))|\n                  ((df['GrLivArea']>3000) & (df['SalePrice']<300000))|\n                  ((df['1stFlrSF']>3000) & (df['SalePrice']<300000))|\n                  ((df['TotalBsmtSF']>5000) & (df['SalePrice']<300000))|\n                  ((df['MasVnrArea']>1200) & (df['SalePrice']<700000))|\n                  ((df['SalePrice']>600000))].index","2ca77a2f":"drop_index","62048519":"df_cat.head()","839139e5":"for col in df_cat:\n    print(f'{col :-<50} {df[col].unique()}')","246a6d12":"def encodage(df):\n    \"\"\" This function will encode our dataset df\n    with the OneHotEncoder Method\"\"\"\n    \n    for col in df:\n        df[col] = df[col].astype('category').cat.codes\n    \n    return df","9fb31a06":"def normalisation(df):\n    \"\"\" This function will normalize our dataset df\n    with the StandardScaler \/ RobustEncoder Method\"\"\"\n    \n    temp = pd.DataFrame(df['SalePrice'])\n    # temp_2 = pd.DataFrame(df['Id'])\n    \n    df_norm = df\n    \n    #Init our Scaler\n    #scaler = StandardScaler()\n    scaler = RobustScaler()\n    \n    #FitTransform our data\n    df = scaler.fit_transform(df)\n    \n    df_norm = pd.DataFrame(df, columns = df_norm.columns)\n    df_norm = df_norm.drop(['Id','SalePrice'], axis=1)\n    df_norm_final = pd.merge(df_norm, temp, how='inner',on=df_norm.index)\n    df_norm_final = df_norm_final.drop('key_0', axis=1)\n    #df_norm_final = pd.merge(df_norm_final, temp_2, how='inner',on=df_norm.index)\n    #df_norm_final = df_norm_final.drop('key_0', axis=1)\n    \n    x=np.log1p(df_norm_final['SalePrice'])\n    df_norm_final['SalePrice'] = x\n    \n    #R\u00e9indexing\n    df_norm_final = df_norm_final.reindex(columns=['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold', 'SalePrice'])\n    \n    return df_norm_final","c6e5616c":"def imputation(df):\n    \"\"\" Impute NaN feature by mean strategy\"\"\"\n    df_imputed = df\n    # Drop NaN\n    # Init Imputer\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df = imputer.fit_transform(df)\n    \n    df_imputed = pd.DataFrame(df, columns = df_imputed.columns)\n    #df_imputed = df.dropna(axis=0)\n    return df_imputed","b9e43de5":"def feature_engineering(df):\n    \"\"\" Create news features\"\"\"\n    \n    #df['SpaceRooms'] = df['GrLivArea'] \/ df['TotRmsAbvGrd']\n    #df['OveralSalePrice'] = df['SalePrice'] \/ df['OverallQual']\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['TotalAreaExt'] = df['GrLivArea'] + df['GarageArea']\n    df['TotalAreaInt'] = df['GrLivArea'] + df['TotalBsmtSF']\n\n    return df","9d240136":"def preprocessing(df):\n    \n    \"\"\" Preprocessing of your pipeline\"\"\"\n    \n    # Drop the columns with 80 + missing Value\n    df = df.loc[:,(df.isnull().sum()\/df.shape[0]*100) < 90]\n    df = df.drop(['Utilities','Street'], axis = 1) # Useless feature to your model\n    \n    # Drop Outliers\n    #df = df.drop(drop_index)\n    \n    # Extract object features\n    num_cols = []\n    for col in df.select_dtypes(exclude='object'):\n        num_cols.append(col)\n    \n    # Keep object features\n    cat_cols = []\n    for col in df.select_dtypes(include='object'):\n        cat_cols.append(col)\n    \n    # Split our Dataset\n    df_cat = df[cat_cols]\n    df_num = df[num_cols]\n    \n    print(f'Categorical shape : {df_cat.shape}')\n    print(f'Numerical shape : {df_num.shape}')\n    \n    # Preprocessing\n    df_encode = encodage(df_cat)\n    df_normalize = normalisation(df_num)\n    #df_normalize = df_num\n    #df = feature_engineering(df)\n    \n    # Join Dataset \n    df = df_encode.join(df_normalize)\n    \n    df = imputation(df)\n    print(f'After Imputation shape : {df.shape}, So : {round((df_num.shape[0]-df.shape[0])\/df_num.shape[0] * 100,2)} % of rows deleted')\n    \n    # Feature engineering\n    df = feature_engineering(df)\n    \n    X = df.drop('SalePrice', axis=1)\n    y = df['SalePrice']\n    \n    return X, y","7d3c8642":"# Preprocessing of our train set\nX_train, y_train = preprocessing(df_train)","f3ddcf53":"# Preprocessing of our test set\nX_test, y_test = preprocessing(df_test_full)","2b88e1a4":"drop_index","26c5b976":"# We will drop our outliers detected early\nX_train = X_train.drop(drop_index)\ny_train = y_train.drop(drop_index)","17ae52cc":"# First basic model \nmodel_1 = make_pipeline(LinearRegression())","a315187e":"# Evaluation Modeling\ndef evaluation(model,name_model):\n    \n    model.fit(X_train, y_train)\n    model_score = model.score(X_train,y_train)\n    ypred = model.predict(X_test)\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                              cv=5,scoring='neg_root_mean_squared_error',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n    #print('Training scores:\\n\\n', train_score)\n    #print('\\n', '-' * 70) # separator to make the output easy to read\n    #print('\\nValidation scores:\\n\\n', val_score)\n    \n    train_scores_mean = -train_score.mean(axis = 1)\n    validation_scores_mean = -val_score.mean(axis = 1)\n    \n    print(f'Model :{name_model}')\n    #print('\\n', '-' * 20) # separator\n    #print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = N))\n    #print('\\n', '-' * 20) # separator\n    #print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = N))\n    #print('\\n', '-' * 20) # separator\n    \n    print(f'Score R2 : {model_score}')\n    print('Mean training scores : ', train_scores_mean.mean())\n    print('Mean Validation scores : ', validation_scores_mean.mean())\n    print('MAE:', mean_absolute_error(y_test, ypred))\n    print('MSE:', mean_squared_error(y_test, ypred))\n    print('RMSE:', np.sqrt(mean_squared_error(y_test, ypred)))\n    \n    #print('\\n', '-' * 20) # separator\n    print('\\n','-' * 20) # separator\n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.xlabel('Number of train size')\n    plt.ylabel('neg_root_mean_squared_error')\n    plt.title(name_model)\n    plt.legend()","2e7da336":"evaluation(model_1,'LinearModel')","c577df7f":"#Init preprocessor\npreprocessor = make_pipeline(SelectKBest(f_regression, k=46))","bc8d8e26":"Elastic = make_pipeline(preprocessor, ElasticNet(alpha=0.0005, l1_ratio=0.9,random_state=0))\nLasso_model = make_pipeline(preprocessor,Lasso(alpha =0.0005,random_state=0))\nRidge_model = make_pipeline(preprocessor, Ridge(random_state=0))\nSVR_model =  make_pipeline(preprocessor, SVR())\nRandomForest = make_pipeline(preprocessor, RandomForestRegressor(random_state=0))\nAdaboost = make_pipeline(preprocessor, AdaBoostRegressor(random_state=0))\nXGboost = make_pipeline(preprocessor, XGBRegressor())\nGradientBoosting = make_pipeline(preprocessor, GradientBoostingRegressor(random_state=0))","033faf4a":"# define a dict of model\ndict_of_models = {\n                 'Elastic': Elastic,\n                 'Lasso_model': Lasso_model,\n                 'Ridge_model': Ridge_model,\n                 'SVR_model': SVR_model,\n                 'RamdomForest': RandomForest,\n                 'Adaboost': Adaboost,\n                 'XGboost' : XGboost,\n                 'GradientBoosting': GradientBoosting}","a3573433":"for name, model in dict_of_models.items():\n    evaluation(model, name)","4860bb62":"# prepare configuration for cross validation test\n\n#Create two dictionaries to store the results of R-Squared and RMSE \nr_2_results = {'R-Squared':{},'Mean':{},'std':{}}   \nrmse_results = {'RMSE':{},'Mean':{},'std':{}}\n\nn_folds = 5\nkfold = KFold(n_folds, shuffle=True, random_state=0).get_n_splits(X_train)\n\nfor name, model in dict_of_models.items():\n    r_2 = cross_val_score(model, X_train, y_train, scoring='r2', cv=kfold)   #R-Squared \n    rms = np.sqrt(-cross_val_score(model, X_train, y_train, cv=kfold,        #RMSE\n                                   scoring='neg_mean_squared_error'))\n    \n    #save the R-Squared reults\n    r_2_results['R-Squared'][name] = r_2\n    r_2_results['Mean'][name] = r_2.mean()\n    r_2_results['std'][name] = r_2.std()\n    \n    #save the RMSE reults\n    rmse_results['RMSE'][name] = rms\n    rmse_results['Mean'][name] = rms.mean()\n    rmse_results['std'][name] = rms.std()","2bfd2532":"#visualizing the results of R-Squared for each model\n\nr_2_cv_results = pd.DataFrame(index=r_2_results['R-Squared'].keys())\n\n#append the max R-Squared for each model to the dataframe\nr_2_cv_results['Max'] = [r_2_results['R-Squared'][m].max() for m in r_2_results['R-Squared'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nr_2_cv_results['Mean'] = [r_2_results['Mean'][m] for m in r_2_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nr_2_cv_results['Min'] = [r_2_results['R-Squared'][m].min() for m in r_2_results['R-Squared'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nr_2_cv_results['std'] = [r_2_results['std'][m] for m in r_2_results['std'].keys()]\n\nr_2_cv_results = r_2_cv_results.sort_values(by='Mean',ascending=False)\nr_2_cv_results.iplot(kind='bar',\n                 title='Max, Min, Mean, and standard deviation <br>For R-Squared values for each model')","0c12e3c9":"#visualizing the variance of R-Squared for each model\n\nscores = pd.DataFrame(r_2_results['R-Squared'])\nscores.iplot(kind='box',\n             title='Box plot for the variation of R-Squared for each model')","0cf98da8":"#visualize the results of RMSE for each model\n\nrmse_cv_results = pd.DataFrame(index=rmse_results['RMSE'].keys())\n\n#append the max R-Squared for each model to the dataframe\nrmse_cv_results['Max'] = [rmse_results['RMSE'][m].max() for m in rmse_results['RMSE'].keys()]\n#append the mean of all R-Squared for each model to the dataframe\nrmse_cv_results['Mean'] = [rmse_results['Mean'][m] for m in rmse_results['Mean'].keys()]\n#append the min R-Squared for each model to the dataframe\nrmse_cv_results['Min'] = [rmse_results['RMSE'][m].min() for m in rmse_results['RMSE'].keys()]\n#append the std of all R-Squared for each model to the dataframe\nrmse_cv_results['std'] = [rmse_results['std'][m] for m in rmse_results['std'].keys()]\n\nrmse_cv_results = rmse_cv_results.sort_values(by='Mean',ascending=True)\nrmse_cv_results.iplot(kind='bar',\n                 title='Maximum, Minimun, Mean values and standard deviation <br>For RMSE values for each model')","1a286333":"#visualize the variance of RMSE for each model\n\nscores = pd.DataFrame(rmse_results['RMSE'])\nscores.iplot(kind='box',\n             title='Box plot for the variation of RMSE values for each model')","a0582197":"# Look on hyperparams\nElastic.get_params()","b3e61d55":"parametersGrid = {\"elasticnet__max_iter\": [1, 5, 10, 100],\n                  \"elasticnet__alpha\": [0.0005, 0.005, 0.001, 0.01, 0.1, 1, 10, 100],\n                  \"elasticnet__l1_ratio\": np.arange(0.0, 1.0, 0.1)}","66b4fdf3":"kfold = KFold(n_splits=10)\n\nElastic_grid = GridSearchCV(Elastic, parametersGrid, scoring='neg_root_mean_squared_error', cv=kfold)\n\nElastic_grid.fit(X_train, y_train)\n\nprint(Elastic_grid.best_params_)","ae333a02":"evaluation(Elastic_grid.best_estimator_,'Elastic')","4a25cb65":"#see the results of the model for training\n\nElastic_score = round(Elastic_grid.best_estimator_.score(X_train, y_train)*100, 3)\npredic = Elastic_grid.best_estimator_.predict(X_train)\nElastic_rmse = round(np.sqrt(mean_squared_error(y_train, predic).mean())*100, 3)\nprint(' _'*15)\nprint('\\nElastic Results for trining test : \\n')\nprint(f'Score : {Elastic_score}%')\nprint(f'RMSE  : {Elastic_rmse}%')\nprint(' _'*15)","e9219cde":"base_models = (KernelRidge(),\n               make_pipeline(Lasso(alpha=0.0005, random_state=0)),\n               make_pipeline(ElasticNet(alpha=0.0005, l1_ratio=0.9)),             \n               make_pipeline(GradientBoostingRegressor(learning_rate=0.005, \n                                                                        loss='huber',\n                                                                        max_depth=4, \n                                                                        max_features='sqrt',\n                                                                        min_samples_leaf=15,\n                                                                        min_samples_split=10,\n                                                                        n_estimators=3000,\n                                                                        random_state=0)))\n\nmeta_model = LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, \n                           feature_fraction=0.2319, feature_fraction_seed=9,\n                           learning_rate=0.05, max_bin=55, min_data_in_leaf=6,\n                           min_sum_hessian_in_leaf=11, n_estimators=720, num_leaves=5,\n                           bagging_seed=9,objective='regression')","abdbad4d":"#Building the stacking model\n\nstack = StackingCVRegressor(regressors=base_models,\n                            meta_regressor=meta_model, \n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,cv=kfold,\n                            random_state=0)","9936d213":"#fitting the model to our data\nstack.fit(X_train,y_train)","8005225e":"#see the results of the model for training\n\nstack_score = round(stack.score(X_train, y_train)*100, 3)\npredictions = stack.predict(X_train)\nstack_rmse = round(np.sqrt(mean_squared_error(y_train, predictions).mean())*100, 3)\nprint(' _'*15)\nprint('\\nStacking Results for trining test : \\n')\nprint(f'Score : {stack_score}%')\nprint(f'RMSE  : {stack_rmse}%')\nprint(' _'*15)","a7e77b6c":"#lets make the predictions for the submission \n\ny_stacking = np.expm1(stack.predict(X_test)) #using expm1 (The inverse of log1p)","f5be8dc4":"#Make Submisison\nsubmission = pd.DataFrame({\n        \"Id\": df_test.Id,\n        \"SalePrice\": y_stacking\n    })","d2fe9fea":"submission.to_csv('submission_Stacking.csv', index=False)","c41255e6":"## Conclusion : \nBsmtQual --> we can see a linearity with year of build and salePrice!","c98c8519":"# GOAL OF THE NOTEBOOK\nPredict sales prices and practice feature engineering, RFs, and gradient boosting\n\n### File descriptions\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n### Data fields\n**SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n**MSSubClass**: The building class\n**MSZoning**: The general zoning classification\n**LotFrontage**: Linear feet of street connected to property\n**LotArea**: Lot size in square feet\n**Street**: Type of road access\n**Alley**: Type of alley access\n**LotShape**: General shape of property\n**LandContour**: Flatness of the property\n**Utilities**: Type of utilities available\n**LotConfig**: Lot configuration\n**LandSlope**: Slope of property\n**Neighborhood**: Physical locations within Ames city limits\n**Condition1**: Proximity to main road or railroad\n**Condition2**: Proximity to main road or railroad (if a second is present)\n**BldgType**: Type of dwelling\n**HouseStyle**: Style of dwelling\n**OverallQual**: Overall material and finish quality\n**OverallCond**: Overall condition rating\n**YearBuilt**: Original construction date\n**YearRemodAdd**: Remodel date\n**RoofStyle**: Type of roof\n**RoofMatl**: Roof material\n**Exterior1st**: Exterior covering on house\n**Exterior2nd**: Exterior covering on house (if more than one material)\n**MasVnrType**: Masonry veneer type\n**MasVnrArea**: Masonry veneer area in square feet\n**ExterQual**: Exterior material quality\n**ExterCond**: Present condition of the material on the exterior\n**Foundation**: Type of foundation\n**BsmtQual**: Height of the basement\n**BsmtCond**: General condition of the basement\n**BsmtExposure**: Walkout or garden level basement walls\n**BsmtFinType1**: Quality of basement finished area\n**BsmtFinSF1**: Type 1 finished square feet\n**BsmtFinType2**: Quality of second finished area (if present)\n**BsmtFinSF2**: Type 2 finished square feet\n**BsmtUnfSF**: Unfinished square feet of basement area\n**TotalBsmtSF**: Total square feet of basement area\n**Heating**: Type of heating\n**HeatingQC**: Heating quality and condition\n**CentralAir**: Central air conditioning\n**Electrical**: Electrical system\n**1stFlrSF**: First Floor square feet\n**2ndFlrSF**: Second floor square feet\n**LowQualFinSF**: Low quality finished square feet (all floors)\n**GrLivArea**: Above grade (ground) living area square feet\n**BsmtFullBath**: Basement full bathrooms\n**BsmtHalfBath**: Basement half bathrooms\n**FullBath**: Full bathrooms above grade\n**HalfBath**: Half baths above grade\n**Bedroom**: Number of bedrooms above basement level\n**Kitchen**: Number of kitchens\n**KitchenQual**: Kitchen quality\n**TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n**Functional**: Home functionality rating\n**Fireplaces**: Number of fireplaces\n**FireplaceQu**: Fireplace quality\n**GarageType**: Garage location\n**GarageYrBlt**: Year garage was built\n**GarageFinish**: Interior finish of the garage\n**GarageCars**: Size of garage in car capacity\n**GarageArea**: Size of garage in square feet\n**GarageQual**: Garage quality\n**GarageCond**: Garage condition\n**PavedDrive**: Paved driveway\n**WoodDeckSF**: Wood deck area in square feet\n**OpenPorchSF**: Open porch area in square feet\n**EnclosedPorch**: Enclosed porch area in square feet\n**3SsnPorch**: Three season porch area in square feet\n**ScreenPorch**: Screen porch area in square feet\n**PoolArea**: Pool area in square feet\n**PoolQC**: Pool quality\n**Fence**: Fence quality\n**MiscFeature**: Miscellaneous feature not covered in other categories\n**MiscVal**: Value of miscellaneous feature\n**MoSold**: Month Sold\n**YrSold**: Year Sold\n**SaleType**: Type of sale\n**SaleCondition**: Condition of sale","b9d2eceb":"## 3 - Preprocessing step : encodage, normalization, feature_engineering ...","c2210f3b":"#### BIG thanks to @Alaa Sedeeq with this notebook who helped me a lot ! --> https:\/\/www.kaggle.com\/alaasedeeq\/house-price-prediction-top-8","a60e4386":"RMSE","c9e0a231":" ##### 2 - TARGET \/ TARGET : Categorical Variables","effb22fc":"# I - BASIC EDA","438d2cce":"### Conclusion : Gradient Boosting look like the best model in first look but Elastic, Lasso and Ridge look good too when \n### we look at the evolution on the validation curve.\n### We will compare this model in detail with visualisation ","7d8fe8e1":"### Background ANALYSIS","9c5b5a26":"## Conclusion :\n\nMsZoning and MasVnrAre look good too","1ee11e88":"### Conlusion : \nWhen we look the difference between each model, we can extract some informations :\n- RandomForest get the best high socre but get some RMSE --> Gradboosting is better\n- SVR std is too large to continue with with\n- Elastic, Lasso and Ridge don't get the high score but have looks good in general\n\n","04c48b25":" ##### 1 - TARGET \/ TARGET : Numerical Variables","bbc37ca3":"### 2 - Split the dataset","e98d491d":"### TEST MORE MODEL","98c96eb5":"## CONCLUDE : Stacking upgrade our score very well ! ","cee8a492":"### LOOK ON OUTLIERS","0a42610e":"### Relationship Target\/Features :\n\n     1 - Numericals Features (float\/int)","52107ab2":"## Focus on Elastic model : try to optimize it","dd0b3849":"We can see that \"Utilities\" is useless to your model, so we will drop it later","bd984e7c":"## Procedure : \n\n1 - We will drop our columns with 90% + of missing values\n    -> Fence don't look like a good feature to your model, so we will drop it too\n    \n2 - Split the dataset into a numerical and a categorical set \n \n     Look for Outliers\n\n3 - Encodage \n\n4 - feature_engineering\n\n5 - Imputation\n\n6 - First look with a basic model","9f1f75ee":"# PHASE II : PREPROCESSING","f0098e2c":"### Take a look on your target","016a4f7a":"### We will take a look on your group of data that we identified more early","72a7396e":"## Conclusion :\n\n- GarageArea \/ GarageCars \/ BsmtFinSF1 \/ TotalBsmtSF get nice correlation too like we saw early","418bc1c2":"#### Vizualisation ","8fd1527d":"### First Step : We will concat the Train and Test set\n    - 1 : combine df_test + df_submissions\n    - 2 : Merge df_train + df _test","270a962d":"We will create a new variables : saleprice_bins to get a better vision","ab0ac7f2":"2 - Categoricals Features (object)","b54c1d82":"### First Conclusion :\n\n- A lot of NaN Values mostly on : \n\n      - PoolQC    99.657417\n      - MiscFeature    96.402878\n      - Alley    93.216855\n      - Fence    80.438506\n      \n- SalePrice get a high correlation mostly with :  \n\n    - GrLivArea\t0.588010\n    - OverallQual\t0.550911\n    - TotRmsAbvGrd\t0.469800\n    - GarageCars\t0.469249\n    - GarageArea\t0.464809\n    - 1stFlrSF\t0.462865\n    - TotalBsmtSF\t0.453224\n    - FullBath\t0.433710\n    - YearBuilt\t0.362066\n    - MasVnrArea\t0.355608\n    - Fireplaces\t0.353567\n    - YearRemodAdd\t0.350032\n    - GarageYrBlt\t0.325297\n    - LotFrontage\t0.318084\n    - LotArea\t0.296497\n    \n- We got a high skewness with :  \n\n    - 3SsnPorch\t11.381914\n    - LowQualFinSF\t12.094977\n    - LotArea\t12.829025\n    - PoolArea\t16.907017\n    - MiscVal\t21.958480","f44187ea":"## Conclusion : \nGarage type could be a argument for the sale price but 'GarageYrBlt' has a bad outlier to remove later","022e6ac2":"## PRE - MODELING","deb5776a":"### We will try to stack the models :","d6028c1d":"### First Look on your Data","2dbc0dc2":"#### CHECKLIST :\n\n##### Shape Analysis :\n- target variable : **'SalePrice'**\n- shape of your dataset : **row : 2919, columns : 81**\n- Features types : **float64(12), int64(26), object(43)**\n- Missing Values analysis : We have a lot of variables with NaN \n    - 1 **group : > 80% NaN**\n            - PoolQC\t99.657417\n            - MiscFeature\t96.402878\n            - Alley\t93.216855\n            - Fence\t80.438506\n     - **2 group : >15 % <50%**\n            - FireplaceQu\t48.646797\n            - LotFrontage\t16.649538\n     - **3 group : Garage Option**\n            - GarageFinish\t5.447071\n            - GarageQual\t5.447071\n            - GarageCond\t5.447071\n            - GarageYrBlt\t5.447071\n            - GarageType\t5.378554\n     - **4 group : < 3%**\n            - BsmtExposure\t2.809181\n            - BsmtCond\t2.809181\n            - BsmtQual\t2.774923\n            - BsmtFinType2\t2.740665\n            - BsmtFinType1\t2.706406\n     - **5 group :**\n            - MasVnrType\t0.822199\n            - MasVnrArea\t0.787941\n            - MSZoning\t0.137033\n      - **6 group : Condtion Optional**\n            - Functional\t0.068517\n            - Utilities\t0.068517\n            - BsmtHalfBath\t0.068517\n            - BsmtFullBath\t0.068517\n      - **7 group : Condition Optimal**\n            - GarageArea\t0.034258\n            - BsmtFinSF1\t0.034258\n            - SaleType\t0.034258\n            - GarageCars\t0.034258\n            - BsmtUnfSF\t0.034258\n            - Electrical\t0.034258\n            - Exterior2nd\t0.034258\n            - Exterior1st\t0.034258\n            - KitchenQual\t0.034258\n            - TotalBsmtSF\t0.034258\n            - BsmtFinSF2\t0.034258\n      - **8 group : Complete Features **\n              - TotRmsAbvGrd\t0.000000\n              - Fireplaces\t0.000000\n              - BedroomAbvGr\t0.000000\n              - PavedDrive\t0.000000\n              - WoodDeckSF\t0.000000\n              - OpenPorchSF\t0.000000\n              - EnclosedPorch\t0.000000\n              - 3SsnPorch\t0.000000\n              - ScreenPorch\t0.000000\n              - PoolArea\t0.000000\n              - MiscVal\t0.000000\n              - MoSold\t0.000000\n              - YrSold\t0.000000\n              - SaleCondition\t0.000000\n              - KitchenAbvGr\t0.000000\n              - HeatingQC\t0.000000\n              - HalfBath\t0.000000\n              - FullBath\t0.000000\n              - LotArea\t0.000000\n              - Street\t0.000000\n              - LotShape\t0.000000\n              - LandContour\t0.000000\n              - LotConfig\t0.000000\n              - LandSlope\t0.000000\n              - Neighborhood\t0.000000\n              - Condition1\t0.000000\n              - Condition2\t0.000000\n              - BldgType\t0.000000\n              - HouseStyle\t0.000000\n              - OverallQual\t0.000000\n              - OverallCond\t0.000000\n              - YearBuilt\t0.000000\n              - YearRemodAdd\t0.000000\n              - RoofStyle\t0.000000\n              - RoofMatl\t0.000000\n              - ExterQual\t0.000000\n              - ExterCond\t0.000000\n              - Foundation\t0.000000\n              -  Heating\t0.000000\n              - MSSubClass\t0.000000\n              - CentralAir\t0.000000\n              - 1stFlrSF\t0.000000\n              - 2ndFlrSF\t0.000000\n              - LowQualFinSF\t0.000000\n              - GrLivArea\t0.000000\n              - SalePrice\t0.000000 ","8c2a73e3":"R-Squared","9bc74c10":"#### 1 - Drop the missing value","91b0ee5f":"Street is a bit useful because it's not equilibrate we will maybe drop it tooRobustScaler","27ee208d":"## Multivariate Visualisation\n\n### Scatter Matrix","833c9f93":"Using Logarithms helps us to have a normal distribution which could help us in a number of different ways such as outlier detection.\n\nIn this data We have a right skewed distribution in which most Sales are between 0 and 340K."}}