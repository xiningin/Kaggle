{"cell_type":{"f6d60e27":"code","d5aef1d8":"code","46f2a62f":"code","989163c4":"code","2375bf38":"code","6fbdd7c2":"code","c67b59cd":"code","2e065645":"code","8c580982":"code","6e92836d":"code","ff449abd":"code","c6189053":"code","4c5e32f3":"code","5f4b29a1":"code","561752d1":"code","60a13c8e":"code","fe041f2f":"code","35e03c7f":"markdown","07b9ceee":"markdown","61821859":"markdown"},"source":{"f6d60e27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d5aef1d8":"import seaborn as sns","46f2a62f":"df=pd.read_csv(\"..\/input\/CreditCardUsage.csv\")","989163c4":"df.shape","2375bf38":"df.info()","6fbdd7c2":"df.columns","c67b59cd":"df.head()","2e065645":"df.describe().T","8c580982":"#Remove Unneccasary column\ndf.drop('CUST_ID', axis = 1, inplace = True)","6e92836d":"sns.heatmap(df.corr(), xticklabels=df.columns, yticklabels=df.columns)","ff449abd":"\nimport matplotlib.pyplot as plt\n","c6189053":"missing = df.isna().sum()\nprint(missing)","4c5e32f3":"df = df.fillna( df.median() )\n#We use standardScaler() to normalize our dataset.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nScaled_df = scaler.fit_transform(df)\ndf_scaled = pd.DataFrame(Scaled_df,columns=df.columns)\ndf_scaled.head()","5f4b29a1":"#df = df.fillna( df.median() )\n# Let's assume we use all cols except CustomerID\nvals = df_scaled.iloc[ :, :].values\nfrom sklearn.cluster import KMeans\n# Use the Elbow method to find a good number of clusters using WCSS\n\nwcss = []\nfor i in range( 1, 30 ):\n    kmeans = KMeans(n_clusters=i, init=\"k-means++\", n_init=10, max_iter=300) \n    kmeans.fit_predict( vals )\n    wcss.append( kmeans.inertia_ )\n   \nplt.plot( wcss, 'ro-', label=\"WCSS\")\nplt.title(\"Computing WCSS for KMeans++\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()","561752d1":"kmeans = KMeans(n_clusters=8, init=\"k-means++\", n_init=10, max_iter=300) \ny_pred = kmeans.fit_predict( vals )\nlabels = kmeans.labels_\ndf_scaled[\"Clus_km\"] = labels\n\n# As it's difficult to visualise clusters when the data is high-dimensional - we'll see\n# if Seaborn's pairplot can help us see how the clusters are separating out the samples.   \nimport seaborn as sns\ndf_scaled[\"cluster\"] = y_pred\ncols = list(df_scaled.columns)\n\n\nsns.lmplot(data=df_scaled,x='BALANCE',y='PURCHASES',hue='Clus_km')\n\n\n\n#plt.scatter(X[:,0], X[:,2], c=labels.astype(np.float), alpha=0.5)\n#plt.xlabel('BALANCE', fontsize=18)\n#plt.ylabel('PURCHASES', fontsize=16)\n\n#sns.pairplot( df[ cols ], hue=\"cluster\")","60a13c8e":"#using best cols  :\n\nbest_cols = [\"BALANCE\",\"PURCHASES\",\"CASH_ADVANCE\",\"CREDIT_LIMIT\",\"PAYMENTS\",\"MINIMUM_PAYMENTS\"]\nkmeans = KMeans(n_clusters=8, init=\"k-means++\", n_init=10, max_iter=300) \nbest_vals = df_scaled[best_cols].iloc[ :, :].values\ny_pred = kmeans.fit_predict( best_vals )\nwcss = []\nfor i in range( 1, 30 ):\n    kmeans = KMeans(n_clusters=i, init=\"k-means++\", n_init=10, max_iter=300) \n    kmeans.fit_predict( best_vals )\n    wcss.append( kmeans.inertia_ )\n\n","fe041f2f":"sns.set_palette('Set2')\nsns.scatterplot(df_scaled['BALANCE'],df_scaled['PURCHASES'],hue=labels,palette='Set1')","35e03c7f":"    Let's choose n=8 clusters. As it's difficult to visualize clusters when we have more than 2-dimensions, we'll see if Seaborn's pairplot can show how the clusters are segmenting the samples.","07b9ceee":"The goal was to segment the customers in order to define a marketing strategy. Unfortunately the colors of the plots change when this kernel is rerun - but here are some thoughts:\n\nBig Spenders with large Payments \n-------------------------------\n        - they make expensive purchases and have a credit limit that is between average and high. This is only a small group of customers.\n\nCash Advances with large Payments - \n-------------\n        - this group takes the most cash advances. They make large payments, but this appears to be a small group of customers.\n\nMedium Spenders with third highest Payments - \n-----------\n        - the second highest Purchases group (after the Big Spenders).\n\nHighest Credit Limit but Frugal - \n---------\n        - this group doesn't make a lot of purchases. It looks like the 3rd largest group of customers.\nCash Advances with Small Payments - this group likes taking cash advances, but make only small payments.\n\nSmall Spenders and Low Credit Limit -\n-------\n        - they have the smallest Balances after the Smallest Spenders, their Credit Limit is in the bottom 3 groups, the second largest group of customers.\n\nSmallest Spenders and Lowest Credit Limit \n--------\n        - this is the group with the lowest credit limit but they don't appear to buy much. Unfortunately this appears to be the largest group of customers.\n\nHighest Min Payments\n        --------\n                - this group has the highest minimum payments (which presumably refers to \"Min Payment Due\" on the monthly statement. This might be a reflection of the fact that they have the second lowest Credit Limit of the groups, so it looks like the bank has identified them as higher risk.)\n\n    So a marketing strategy that targeted the first five groups might be effective.","61821859":"Since the number of missing values is low (the total number of samples is 8950), we'll impute with median of the columns.\n\nWe'll then use the elbow method to find a good number of clusters with the KMeans++ algorithm\n\n    WCSS always decreases with the increase in the number of clusters. However, it should be noted that, the rate of drop in WCSS starts to drop as we increase the number of clusters. This would be our hint. We need to stop at the number of clusters from where the rate of drop in WCSS doesn\u2019t drop substantially (in other words, the rate of drop is very less). This is sometimes also termed as Elbow method."}}