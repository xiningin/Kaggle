{"cell_type":{"c1f388bb":"code","3071becd":"code","3a8cdc86":"code","d43ec035":"code","cc819e9b":"code","4af07823":"code","4a4048f1":"code","e54bd7ea":"code","ef53701b":"code","9814f2ac":"code","aab129be":"code","ae0991fe":"code","29cd7f49":"code","abb8cc7c":"markdown","0b4228dc":"markdown","2043f9d1":"markdown","caaae8f7":"markdown","fd39b291":"markdown","8f9b8bf8":"markdown","e9b2d130":"markdown","096cc7c3":"markdown","09dec615":"markdown","073a58b0":"markdown","64dc5033":"markdown","f502416c":"markdown","d1ce9a10":"markdown","657abe03":"markdown","09f6a91e":"markdown","d2fcbfc3":"markdown","660b2ee0":"markdown","03577ca2":"markdown","d9b75a24":"markdown","9827d678":"markdown","e47efc65":"markdown","98c6e589":"markdown","db0a990e":"markdown","4e22e9fe":"markdown"},"source":{"c1f388bb":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\n\nimport numpy as np\n\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\n\nimport pandas as pd\n\n\n#Matplotlib is used to generate graphs in just a few lines of code.\n\nimport matplotlib.pyplot as plt\n\n\n#DecisionTreeRegressor is the class of the algorithm we will be using.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor","3071becd":"#read the data from csv\ndataset = pd.read_csv('..\/input\/position-salaries\/Position_Salaries.csv')\n\n#set independent variable by using all rows, but just column 1.\nX = dataset.iloc[:, 1:2].values\n\n#set the dependent variable using all rows but only the last column. \ny = dataset.iloc[:, 2].values\n\n#take a look at our dataset\ndataset","3a8cdc86":"#create an object of the DecisionTree class.\nrf_regressor = RandomForestRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\nrf_regressor.fit(X, y) \n\n#create an object of the DecisionTree class.\ndt_regressor = DecisionTreeRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\ndt_regressor.fit(X, y) ","d43ec035":"#wrap in a function so I can use it for both. \ndef show_fit(regressor, title):    \n    #Create a grid, necessary because of the veritical jumps.\n    X_grid = np.arange(min(X), max(X), 0.01)\n    X_grid = X_grid.reshape((len(X_grid), 1))\n\n    #create a scatter plot\n    plt.scatter(X, y, color = 'red')\n\n    #plot the X values and the predictions \n    plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n\n    #Titles and labels.\n    plt.title(title)\n    plt.xlabel('Position level')\n    plt.ylabel('Salary')\n\n    #Show the plot.\n    plt.show()","cc819e9b":"show_fit(dt_regressor, 'Decision Tree Regression')\nshow_fit(rf_regressor, 'Random Forest Regression')","4af07823":"# Predicting a new result\ny_pred = dt_regressor.predict([[6.5]])\nprint(\"Decision Tree Predicted Salary: $\"+str(y_pred[0]))\n\ny_pred2 = rf_regressor.predict([[6.5]])\nprint(\"Random Forest Predicted Salary: $\"+str(y_pred2[0]))","4a4048f1":"#read the data from csv\ndataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\n\n#take a look at our dataset.  head() gives the first 5 lines. \ndataset.head()","e54bd7ea":"#drop the columns.\ndataset = dataset.drop(columns = ['Administration', 'State'])\n\n#look at the changes\ndataset.head()","ef53701b":"#set independent variable by using all rows, but just column 1.\nX = dataset.iloc[:, :-1].values\n\n#set the dependent variable using all rows but only the last column. \ny = dataset.iloc[:, -1].values","9814f2ac":"#create an object of the DecisionTree class.\nrf_regressor = RandomForestRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\nrf_regressor.fit(X, y) \n\n#create an object of the DecisionTree class.\ndt_regressor = DecisionTreeRegressor(random_state = 0)\n\n#fit it on the data, do not need to fit_transform her\ndt_regressor.fit(X, y) ","aab129be":"budgets = [[300000, 200000], [200000,300000], [100000,400000]]\n\nfor budget in budgets:\n    y_pred = dt_regressor.predict([budget])\n    y_pred2 = rf_regressor.predict([budget])\n    print(\"BUDGET:\", budget,\n          \"\\nDecision Tree Prediction:\", y_pred,\n          \"\\n Random Forest Prediction:\", y_pred2,\n          \"\\n--------------------------------------\")\n    ","ae0991fe":"#wrap it in a function so I can use it for both. \ndef show_3d(regressor, title):   \n    # First well create the 3d figure.\n    fig = plt.figure(figsize = (10,10))\n    ax = plt.axes(projection='3d')\n\n    #Next well pull out the datapoints for each axis\n    zdata = y\n    xdata = X[:, 0]\n    ydata = X[:, 1]\n    #Now we plot the points \n    ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Reds', s = 50);\n\n    #Next we need to make x and y dimensions for the data.\n    xline = np.linspace(min(X[:, 0]), max(X[:, 0]), 50)\n    yline = np.linspace(min(X[:, 1]), max(X[:, 1]), 50)\n    #combine those back into a dataset to apply the prediction on \n    z = np.concatenate((xline.reshape(-1,1),yline.reshape(-1,1)), axis = 1)\n    #call the predictions \n    zline = regressor.predict(z)\n    #plot the resulting line. \n    plt.title(title)\n    ax.plot3D(xline, yline, zline, 'black')","29cd7f49":"show_3d(dt_regressor, 'Decision Tree Model')\nshow_3d(rf_regressor, 'Random Forest Model')","abb8cc7c":"To create new predictions, we run a sample through each decision tree and track the prediction.\n\n![Predict.png](attachment:Predict.png)","0b4228dc":"Samples can be repeated, resulting in others being omitted. The latter are called the \u201cout-of-bag dataset\u201d, and are used to evaluate the accuracy of the model.\n\n![outofbag.png](attachment:outofbag.png)","2043f9d1":"With our imports complete, we now read in the data using Pandas.\n\nWe will set a independent variable (X) and a dependent variable (y).","caaae8f7":"Now lets predict some values.\n\nTo make this more interesting, lets imagine the company has a fixed budget of $500,000\n\nThey are considering 3 different spending options:\n\nR&D: 300,000,  Marketing: 200,000\n\nR&D: 200,000,  Marketing: 300,000\n\nR&D: 100,000,  Marketing: 400,000","fd39b291":"This is good, but I want to keep this even more simple, so I am going to drop the admin spend and state columns. ","8f9b8bf8":"## Implementation\n\nIn this section I will implement the code in its **simplest verison** so that it is understandable if you are brand new to machine learning. \n\nBelow we will predict salary based on the current role someone is in, and then make some predictions on start ups.  I will also include side by side examples showing how it performs compared to decision trees. \n\nThe first step is to start with \"imports\".","e9b2d130":"Overall, Random Forests vastly outperform decision trees, but in practice they are much slower to train; which is a problem that had been solved through the development of \u201cboosting\u201d algorithms.","096cc7c3":"To keep this conceptually simple and avoid abstraction, we are using a very small dataset here.  \n\nBecause of this we will not be splitting part of it out into a training\/validation\/test set.\n\nIf we had data on 1000's of employees, we would absolutely complete this step.\n\nSo now we can fit the model in just 2 lines of code.","09dec615":"We see that the random forest did predict a novel value, where the decision tree only reproduces a salary its already seen.\n\nLets repeat this with a more complex dataset, using info on start ups.","073a58b0":"To start, we must first build \u201cbootstrapped\u201d datasets by randomly selecting samples from our original dataset.\n\n![bootstrapped.png](attachment:bootstrapped.png)","64dc5033":"This is typically repeated over 100 times resulting in a \u201crandom forest\u201d of decision trees that vary based on the data and variables they saw.\n\n![rfvsdt.png](attachment:rfvsdt.png)","f502416c":"Now we can fit the new models.","d1ce9a10":"Here we see a much smoother prediction line for the random forest as it takes much smaller steps.  ","657abe03":"> # Enough to be Dangeous: Random Forest Regression\n\n> ### This is the 5th notebook of my **\"Enough to be Dangeous\"** notebook series\n\nSee the other notebooks here:\n\n[Simple Linear regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangeous-simple-linear-regression)\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)\n\n[Decision Tree](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-decision-tree-regression)","09f6a91e":"> ## This notebook is separated into two parts:\n\n### **1) Conceptual Overview:**  I will introduce the topic in 200 words or less.\n\n### **2) Implementation:**  I will implement the algorithm in as few lines as possible.","d2fcbfc3":"## Conceptual Overview\n\n*This assumes that you are familiar with decision trees.  If not,[ please review here.](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-decision-tree-regression)*\n\nRandom Forests harness the simplicity of Decision Trees and the power of \"ensemble methods\".\n\n![ensemble.png](attachment:ensemble.png)","660b2ee0":"They work by applying many decision trees across new subsets of the data\n\n![RFensemble.png](attachment:RFensemble.png)","03577ca2":"Here we can see that the random forest gives us smaller jumps, which should hopefully create better predictions between categories.  However, the Random Forest is less confident that the highest position should pay as well as it does. \n\nWe can try to feed in predictions to both. ","d9b75a24":"Here we see that with the decision tree, this only gave us two unique predictions, as the first and second budgets both landed in the same prediction arm despite being pretty different. \n\nWith random forest we see that each budget gave us a unique prediction, with smaller jumps between them. \n\nNow lets try to visualize this, but be warned itll be a little more complicated.","9827d678":"That's all it takes to fit the model.  \n\nNow we can visualize what it has predicted. ","e47efc65":"Now that I am happy with the data, I can name my x and y variables. ","98c6e589":"For each dataset, we build a decision tree using a random subset of the available variables at each step, creating greater variety in the resulting decision trees.\n\n![subsets.png](attachment:subsets.png)","db0a990e":"We then aggregate and average the results to generate our final prediction. This act of bootstrapping plus aggregation is known as \u201cbagging\u201d.\n\n![final_pred.png](attachment:final_pred.png)","4e22e9fe":"This results in a significantly more accurate model.\n\n![image.png](attachment:image.png)\n"}}