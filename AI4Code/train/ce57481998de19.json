{"cell_type":{"5cbb8cad":"code","22387990":"code","8ff8bd6d":"code","1244d071":"code","cde0c7b2":"code","007ad664":"code","d00e8737":"code","38e5263d":"code","1492e750":"code","067bff80":"code","8cc05222":"code","7dc91672":"markdown","8fd6a2bf":"markdown","0de602b0":"markdown","bbed162f":"markdown","db0aaa84":"markdown","83acc3bb":"markdown","f168c883":"markdown","81d4a35d":"markdown","f02f24dc":"markdown","505f58c8":"markdown","c19ac1b6":"markdown","36bda2b2":"markdown","cea7da91":"markdown"},"source":{"5cbb8cad":"import numpy as np\ndef generate_real_samples(n):\n    '''generate n real samples with class labels'''\n    x1 = np.random.rand(n) - 0.5 #generate a random number between [-0.5,0.5]\n    x2 = x1**3        #generate outputs\n    x1 = x1.reshape(n, 1)\n    x2 = x2.reshape(n, 1)\n    X = np.hstack((x1, x2))   #stack layers\n    y = np.ones((n, 1))     #generate class label\n    return X,y","22387990":"from keras.models import Sequential\nfrom keras.layers import Dense, LeakyReLU\nfrom keras.utils import plot_model\nimport matplotlib.pyplot as plt\n\ndef define_discriminator(inputs = 2):\n    ''' function to return the compiled discriminator model'''\n    model = Sequential()\n    model.add(Dense(25, activation = 'relu', kernel_initializer = 'he_uniform', input_dim = inputs))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(15, activation = 'relu', kernel_initializer = 'he_uniform'))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(5, activation = 'relu', kernel_initializer = 'he_uniform'))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\n\ndiscriminator_model = define_discriminator()\ndiscriminator_model.summary()\nplot_model(discriminator_model, to_file = 'discriminator_model.png', show_shapes = True, show_layer_names = True)","8ff8bd6d":"def define_generator(latent_dim, outputs = 2):\n    model = Sequential()\n    model.add(Dense(25, activation = 'relu', kernel_initializer= 'he_uniform', input_dim = latent_dim))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(15, activation = 'relu', kernel_initializer = 'he_uniform'))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(5, activation = 'relu'))\n    model.add(LeakyReLU(alpha = 0.01))\n    model.add(Dense(outputs, activation = 'linear'))\n    return model","1244d071":"latent_dim = 5\ngenerator_model = define_generator(latent_dim)\ngenerator_model.summary()\nplot_model(generator_model, to_file = 'generator_model.png', show_shapes = True, show_layer_names = True)","cde0c7b2":"def generate_latent_points(latent_dim, n):\n    '''generate points in latent space as input for the generator'''\n    x_input = np.random.rand(latent_dim*n) #generate points in latent space\n    x_input = x_input.reshape(n,latent_dim)  #reshape\n    return x_input\n\ndef generate_fake_samples(generator, latent_dim, n):\n    x_input = generate_latent_points(latent_dim, n) #genarate points in latent space\n    x = generator.predict(x_input) #predict outputs\n    y = np.zeros((n, 1))\n    return x, y","007ad664":"X, _ = generate_fake_samples(generator_model, latent_dim, 100)\nplt.scatter(X[:,0], X[:,1])\nplt.show()","d00e8737":"def define_gan(generator, discriminator):\n    '''define the combined generator and discriminator model'''\n    discriminator.trainable = False\n    model = Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n    return model","38e5263d":"gan_model = define_gan(generator_model, discriminator_model)\ngan_model.summary()\nplot_model(gan_model, to_file = 'gan_model.png', show_layer_names = True, show_shapes = True)","1492e750":"def train_gan(g_model,d_model,gan_model,latent_dim, num_epochs = 8000,num_eval = 2000, batch_size = 128):\n    ''' function to train gan model'''\n    half_batch = int(batch_size\/2)\n  #run epochs\n    for i in range(num_epochs):\n        X_real, y_real = generate_real_samples(half_batch) #generate real examples\n        d_model.train_on_batch(X_real, y_real)               # train on real data\n        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) #generate fake samples\n        d_model.train_on_batch(X_fake, y_fake)                #train on fake data\n        #prepare points in latent space as input for the generator\n        x_gan = generate_latent_points(latent_dim, batch_size)\n        y_gan = np.ones((batch_size, 1))    #generate fake labels for gan\n        gan_model.train_on_batch(x_gan, y_gan)\n        if (i+1) % num_eval == 0:\n            summarize_performance(i + 1, g_model, d_model, latent_dim)","067bff80":"def summarize_performance(epoch, generator, discriminator, latent_dim, n = 100):\n    '''evaluate the discriminator and plot real and fake samples'''\n    x_real, y_real = generate_real_samples(n)      #generate real samples\n    _, acc_real = discriminator.evaluate(x_real, y_real, verbose = 1)\n    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose = 1)\n    print('Epoch: ' + str(epoch) + ' Real Acc.: ' + str(acc_real) + ' Fake Acc.: '+ str(acc_fake))\n    plt.scatter(x_real[:,0], x_real[:,1], color = 'red')\n    plt.scatter(x_fake[:,0], x_fake[:,1], color = 'blue')\n    plt.show()","8cc05222":"train_gan(generator_model, discriminator_model, gan_model, latent_dim)","7dc91672":"References:\n1. <a href = 'https:\/\/machinelearningmastery.com\/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras\/' >This <\/a> blog article.\n2. The GAN paper by Ian Goodfellow: https:\/\/arxiv.org\/pdf\/1406.2661.pdf","8fd6a2bf":"The function defined below is called every two thousand epochs to summarize the performance of the training.","0de602b0":"The train_gan function simultaneously trains the discriminator and the GAN.","bbed162f":"Improvements and further insights possible:\n1. Try deeper layers in discriminator and generator models.\n2. Try experimenting with different activation functions and learning rates\n3. Try out more functions!","db0aaa84":"The function below generates n samples of the cubic function, the domain of the function is chosen to be n random integers in the range -0.5 to 0.5. The function returns a two dimensional vector(inputs in the first column and outputs in the second) and a one dimensional vector of class labels( in this case, 1)","83acc3bb":"We want the discriminator model to believe that the samples generated by the generator are real, so we label them as '1'(real). In the ideal case, the discriminator is fooled about half of the times into believing that the samples generated by the generator are real.","f168c883":"The function below combines the generator and discriminator models. The layers of the discriminator model are made non-trainable( because we do not want to update it's weights during the training of the generator). Here, the discriminator's only job is to classify real and fake samples.","81d4a35d":"The function below defines the discriminator model, it is one of the two components of a GAN. It's job is to classify whether the numbers generated by the generator model are real or fake( ie: if they are the output to our function or not). ","f02f24dc":"Note that the generator model isn't compiled here, it's because it is fit indirectly.","505f58c8":"The function generates_latent_points generates n points in the latent space. The generate_fake_samples function uses the generator model to generate 'fake' samples.","c19ac1b6":"As of now, the fake samples produced by the generator is garbage because we haven't trained it yet. It is supposed to closely follow our function after training.","36bda2b2":"The function below makes up the second component of a GAN. It takes an input point from a latent space and generate a vector with two dimensions(like the X vector returned by the generate_real_samples function)\n<br>\nA latent variable is a hidden variable, and the space it belongs to is called the latent space. We can arbitrarily assign a size to our latent space(here it is 5). The points in the latent space are meaningless until the generator model begins learning and starts assigning meaning to the points in the space. After training, the points in the latent space correspond to the generated samples.","cea7da91":"**This notebook implements a one-dimensional GAN which generates the output of a cubic function**"}}