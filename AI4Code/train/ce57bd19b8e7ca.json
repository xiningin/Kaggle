{"cell_type":{"e85422b8":"code","1628ef2e":"code","c3bc1d9a":"code","eddfdd63":"code","ac5b6a8a":"code","5ad49b75":"code","b99ad764":"code","18bee4e1":"code","5ff68d4e":"code","2702c4bd":"code","33382afa":"code","28d137a1":"code","ae0b22b7":"markdown","2d0ca8f6":"markdown","8474f268":"markdown"},"source":{"e85422b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1628ef2e":"#### \u5148\u6765\u770b\u770b\u6570\u636e\u662f\u4ec0\u4e48\u6837\u7684\ntrain_data = pd.read_csv('\/kaggle\/input\/ml2020spring-hw1\/train.csv',encoding = 'big5')\ntest_data = pd.read_csv('\/kaggle\/input\/ml2020spring-hw1\/test.csv',encoding = 'big5',names = ['id', '\u6e2c\u9805', '0', '1', '2', '3', '4', '5', '6', '7', '8'])\ntest_data['id'] = test_data['id'].str.split('_',expand = True)[1].astype('int')\n#train_data.info()\n#train_data.head(20)\ntest_data","c3bc1d9a":"#\u5148\u770b\u770bCH4\nch4 = train_data[train_data['\u6e2c\u9805'].isin(['CH4'])]\nch4.head()\n#\u770b\u770bRAINFALL\nrainfall = train_data[train_data['\u6e2c\u9805'].isin(['RAINFALL'])]\n#rainfall.info()\n#\u628aRAINFALL\u6309\u7167\u4e0a\u9762\u8bf4\u7684\u90a3\u6837replace\u4e00\u4e0b\ntrain_data.replace(\"NR\",0, inplace = True)\ntest_data.replace(\"NR\",0, inplace = True)\ntrain_data.head(18)\nrainfall = train_data[train_data['\u6e2c\u9805'].isin(['RAINFALL'])]\n#rainfall.info()\n#\nfor num in range(0,23) :\n    num = str(num)\n    train_data[num] = train_data[num].astype(float)\n\ntrain_data.head()\n#\u5148\u7f6e\u6362\u4e00\u4e0b\u884c\u548c\u5217\n#train_data_reshaped = train_data.pivot_table(train_data,index=[u'0'],columns=\"\u6e2c\u9805\")\n#train_data_reshaped.head()\n#columns=('idx','degree','weight','diameter')\ntrain_data_re = pd.DataFrame()\nfor num in range(24):\n    num = str(num)\n    train_data_re = train_data_re.append(\n                pd.DataFrame(train_data[num].values.reshape(1,-1), \n                index=['id_'+num], \n                columns=train_data['\u6e2c\u9805'].values)\n                )\ntrain_data_re.head(25)\n\n#\u628a\u65e0\u7528\u6570\u636e\u5220\u6389\ntrain_data.head()\ntrain_data_v2 = train_data.copy()\n#\u5220\u6389\u4e86\u6d4b\u7ad9\ntrain_data_v2 = train_data_v2.drop(columns = '\u6e2c\u7ad9')\ntrain_data_v2\n","eddfdd63":"#train_data\u8fdb\u884c\u8f6c\u6362\ntrain_data_v2\n\ntrain_data_new = pd.DataFrame()\nfor x in range(24):\n    train_data_first = train_data_v2[['\u65e5\u671f','\u6e2c\u9805',str(x)]].copy()\n    train_data_first['\u65e5\u671f'] = pd.to_datetime(train_data_first['\u65e5\u671f']+' '+str(x)+':00:00')\n    train_data_first = train_data_first.pivot(index = '\u65e5\u671f',columns = '\u6e2c\u9805', values = str(x))\n    train_data_new = pd.concat([train_data_new,train_data_first])\ntrain_data_new = train_data_new.astype('float64').sort_index().reset_index().drop(['\u65e5\u671f'], axis = 1)\ntrain_data_new","ac5b6a8a":"#feature scaling for train_data\n#(X-mean)\/std\ntrain_mean = train_data_new.mean().copy()\ntrain_std = train_data_new.std().copy()\ntrain_data_new1 = train_data_new.copy()\nfor liecolumn in train_data_new:\n        train_data_new[liecolumn] = (train_data_new[liecolumn] - train_mean[liecolumn])\/train_std[liecolumn]\n        #print(liecolumn,train_data_new[liecolumn])\n        \ntrain_data_new","5ad49b75":"tx = train_data_new.copy()\ntx.columns = tx.columns + '_0'\nfor i in range(1,10):\n    ty = train_data_new.copy()\n    if i == 9:\n        ty = ty[['PM2.5']]\n        # \u7ed3\u679c\u5217\u4e0d\u9700\u8981\u6807\u51c6\u5316\uff0c\u9700\u8981\u653e\u5927\u56de\u53bb\n        ty = ty * train_std['PM2.5'] + train_mean['PM2.5']\n    ty.columns = ty.columns + '_' + str(i)\n    for j in range(i):\n        ty = ty.drop([j])\n    tx = pd.concat([tx, ty.reset_index().drop(['index'], axis=1)], axis=1)\n\nfor i in range(12):\n    for j in range(9):\n        tx = tx.drop([480*(i+1)-9+j])\ntrain_data = tx\ntrain_data.describe()","b99ad764":"test_data_new = pd.DataFrame()\nfor i in range(9):\n    test_data_slice = test_data[['id', '\u6e2c\u9805', str(i)]].copy()\n    test_data_slice = test_data_slice.pivot(index='id', columns='\u6e2c\u9805', values=str(i))\n    test_data_slice.columns = test_data_slice.columns + '_' + str(i)\n    for j in range(18):\n        test_data_slice.iloc[:,[j]] = (test_data_slice.iloc[:,[j]].replace('NR', '0').astype('float64') - train_mean[j]) \/ train_std[j]\n    test_data_new = pd.concat([test_data_new, test_data_slice], axis=1)\n\ntest_data_new = test_data_new.replace('NR', '0').astype('float64').reset_index().drop(['id'], axis=1)\ntest_data_new","18bee4e1":"#\u5728\u540e\u9762\u90fd\u8981\u52a0\u4e0a\u4e00\u4e2a\u5e38\u6570 \u65b9\u4fbf\u540e\u9762\u505agradient descent\u7684\u65f6\u5019\u4f7f\u7528\ntrain_x = train_data.drop(['PM2.5_9'],axis = 1)\ntrain_y = train_data[['PM2.5_9']]\nx = np.hstack((train_x.values,np.ones((np.size(train_x.values,0),1),'double')))\ny = train_y.values\nx\ny","5ff68d4e":"# \u9884\u6d4b\u65b9\u6cd5\n# LOSS function\ndef loss(x,y,data):\n    return np.sum((y- x @ data)**2)\n\n# \u68af\u5ea6\u4e0b\u964d\ndef gradientDescent(x,y,data):\n    return ((train_x_x) @ data) - (train_x_y) +(1 * data)","2702c4bd":"data = np.random.random((np.size(x,1),1))\ndata\n#\u5b66\u4e60\u901f\u7387\nlearning_rate = 0.00000006\nregular_one = 1\n\n#\u628a\u8bad\u7ec3\u6570\u636e\u5206\u6210\u56db\u4efd \u6309\u71673:1\u7684\u6bd4\u4f8b\u8bbe\u7f6e\u4e3a \u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\ntrain_X = x[:4320]\ntrain_Y = y[:4320]\nvari_X = x[4320:]\nvari_Y = y[4320:]\n\n#GD =  gradientDescent(train_X,train_Y,data)\n#\u628agradientdescent\u91cc\u7684\u53c2\u6570\u63d0\u53d6\u51fa\u6765 \u8fd9\u6837\u5c31\u4e0d\u7528\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u53cd\u590d\u8ba1\u7b97 \u53ef\u4ee5\u8282\u7ea6\u65f6\u95f4\ntrain_x_x = train_X.T @ train_X\ntrain_x_y = train_X.T @ train_Y\n\nfor i in range(2000001):\n    data = data - learning_rate * gradientDescent(train_X,train_Y,data)\n    if i %50000 == 0 :\n        #\u8f93\u51fa\u8bad\u7ec3\u96c6\u8bef\u5dee\u548c\u9a8c\u8bc1\u96c6\u8bef\u5dee \n        print(i,loss(train_X,train_Y,data) \/ np.size(train_Y,0) , loss(vari_X,vari_Y,data) \/ np.size(train_Y,0))\n\n\nprint(np.size(data,0), np.size(data,1))","33382afa":"test_x = np.hstack((test_data_new.values, np.ones((np.size(test_data_new.values,0), 1), 'double')))\nprint(np.size(test_x,0), np.size(test_x,1))\n\ntest_y = test_x @ (data)\ntest_y\n\n","28d137a1":"test_data_id = test_data['id']\n\nsubmission = pd.DataFrame({\n        \"id\": test_data_id.unique(),\n        \"value\": test_y.T[0]\n    })\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","ae0b22b7":"np.size(x,y)\n\u5982\u679c\u4f20\u5165\u7684\u53c2\u6570\u53ea\u6709\u4e00\u4e2a\uff0c\u5219\u8fd4\u56de\u77e9\u9635\u7684\u5143\u7d20\u4e2a\u6570\n\u5982\u679c\u4f20\u5165\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f0\uff0c\u5219\u8fd4\u56de\u77e9\u9635\u7684\u884c\u6570\n\u5982\u679c\u4f20\u5165\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f1\uff0c\u5219\u8fd4\u56de\u77e9\u9635\u7684\u5217\u6570","2d0ca8f6":"\u8fd9\u4e2a\u6570\u636e\u603b\u611f\u89c9\u54ea\u91cc\u6709\u70b9\u5947\u602a \n* \u7279\u5f81\u5230\u5e95\u662f\u54ea\u4e9b?\n\u611f\u89c924\u5c0f\u65f6\u8be5\u4f5c\u4e3a\u7279\u5f81 \u4f46\u597d\u50cf\u6d4b\u9879\u7684\u6bcf\u4e2a\u6307\u6807\u624d\u662f\u7279\u5f81 \n* \u5173\u4e8e\u6d4b\u9879\u7684\u6307\u6807\u6570\u636e\n    1. CH4\u597d\u50cf\u4e00\u76f4\u90fd\u662f1.8?\u7b49\u4e0b\u53ef\u4ee5\u770b\u770b\u6240\u6709\u7684CH4\u662f\u4e0d\u662f\u90fd\u662f1.8\n    2. RAINFALL\u662fstring \u5e94\u8be5\u662f\u662f\u5426\u4e0b\u96e8\u7684\u610f\u601d \u8f6c\u6362\u4e3a0\u548c1\u5427 NR\u5e94\u8be5\u5c31\u662f\u6ca1\u4e0b\u96e8\u90a3\u5c310\u597d\u4e86 \u5176\u4ed6\u7684(\u4e0b\u96e8)1\n    3. \u8fc7\u4e8e\u7ec6\u81f4\u7684\u6570\u636e\u5e94\u8be5\u9700\u8981\u7528\u533a\u95f4\u6765\u66ff\u4ee3 \u4e0d\u7136\u4f1a\u51fa\u73b0overfitting\u7684\u60c5\u51b5\u5427?","8474f268":"\u62fc\u63a5\u6570\u7ec4\nnp.vstack():\u5728\u7ad6\u76f4\u65b9\u5411\u4e0a\u5806\u53e0\nnp.hstack():\u5728\u6c34\u5e73\u65b9\u5411\u4e0a\u5e73\u94fa"}}