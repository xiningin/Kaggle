{"cell_type":{"57592b5b":"code","5cb5756d":"code","9ef5317b":"code","5681282f":"code","3557cc69":"code","09641f11":"code","c9abe9a2":"code","137ca0f1":"code","13faf5d4":"code","1ce566ed":"code","bb3cdef6":"code","cabeb9b4":"code","003b2367":"code","fd3b8616":"code","efc5a081":"code","77dc5707":"code","0ca02213":"code","603b61a2":"code","a40274c7":"code","84ba6a75":"code","9ceca42d":"code","086abd27":"code","777b8638":"code","58f26b32":"code","16f30748":"code","09d434dc":"code","8fa257af":"code","93390b66":"code","365eee37":"code","658fde79":"code","dbc21e40":"code","2b729b1f":"code","1b798b1d":"code","5a26dfd2":"code","c70e500f":"code","31b26091":"code","21817d74":"code","df39810d":"code","8cf689db":"code","d403d498":"code","3a9efd0a":"code","31c972d8":"code","1942ba87":"code","991e3eab":"code","94c36341":"code","88b8187d":"code","54d525c4":"code","cd59745e":"code","a15d6960":"code","8dfa7250":"code","98bbb398":"code","bc7b2030":"code","d845dfe6":"code","682cdc90":"code","69ce79a7":"code","d9a55046":"code","098c2409":"code","ebb49477":"code","10df8d6b":"code","4db1614a":"code","b62042e0":"code","9d6b6418":"code","15fdb7d2":"code","066caf2e":"code","e918cf8d":"code","605d76f3":"code","be5b16ca":"code","494a1523":"code","94de1058":"code","c403d7cc":"code","72cf581a":"code","a9c1c05b":"code","1d9867bf":"code","6986b51e":"code","2336bde1":"code","6c1605ff":"code","3b09f803":"code","53dd130a":"code","a579e42f":"code","4e56ae8f":"code","5cc82dd9":"code","b3597ba0":"markdown","86d01fd5":"markdown","cf8eb1c0":"markdown","8b754f37":"markdown","4e6ddaaf":"markdown","bea3ffd6":"markdown","d005b2ae":"markdown","a9e15391":"markdown","9dbd8a42":"markdown","0bdb05cb":"markdown","a50f32cb":"markdown","a3297ceb":"markdown","9a4d6797":"markdown","36d293f4":"markdown","19c2e74e":"markdown","cb211ad3":"markdown","0648d9de":"markdown","bc1a0ea1":"markdown","582fc278":"markdown","5d32e91e":"markdown","6719d90c":"markdown","f54dd19a":"markdown","0a4f0478":"markdown","a6193144":"markdown","725b028f":"markdown","93faee71":"markdown","50962669":"markdown","81f2e9f1":"markdown","044baf73":"markdown","cf709aef":"markdown","8221b95b":"markdown","0cfcf8b1":"markdown","7be244c1":"markdown","ee4937a6":"markdown","eee149a1":"markdown"},"source":{"57592b5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cb5756d":"# on importe le dataset\nphilo_df = pd.read_csv(\"\/kaggle\/input\/history-of-philosophy\/philosophy_data.csv\")","9ef5317b":"# dataset shape\nphilo_df.shape","5681282f":"#V\u00e9rifier la qualit\u00e9 des donn\u00e9es\nphilo_df.describe()","3557cc69":"# Info sur les valeurs manquantes\ninfo_manq = philo_df.isnull().sum()\nprint(info_manq)\n#il n'y a pas de valeurs manquantes sur ces colonnes","09641f11":"# regarder la t\u00eate de cette dataframe\nphilo_df.head()","c9abe9a2":"#pour r\u00e9aliser cette exploration, on prends la premi\u00e8re ligne de dataframe\n#on fait des explorations sur les colonnes tel que \"sentence_spacy\", \"sentence_str\", \"tokenized_txt\"\nt1 = philo_df.loc[0, \"sentence_spacy\"]\nt1\nprint(type(t1))\nlen(t1)#len() retourne la taile d'une chaine caract\u00e8re ","137ca0f1":"t2 = philo_df.loc[0, \"sentence_str\"]\nt2\nprint(type(t2))\nlen(t2)","13faf5d4":"t3 = philo_df.loc[0, \"tokenized_txt\"]\nt3\n#type(tttt)#str\n#len(tttt)#187","1ce566ed":"t4 = philo_df.loc[0, \"lemmatized_str\"]\nt4\n#type(t4)#str\n#len(t4)#138 : pourquoi ","bb3cdef6":"def search(word):\n    selection = philo_df.loc[philo_df['sentence_str'].str.contains(word), 'sentence_str']\n    result = selection.apply(lambda s: (' ' * 25 + s + ' ' * 25)[s.find(word):s.find(word)+50+len(word)])    \n    return result","cabeb9b4":"search('idea').iloc[0]","003b2367":"##obtenir les noms de livres de ce dataset et les mettre dans une liste pour parcourir\nlist_book_names = philo_df[\"title\"].value_counts().index.tolist()\n\n## on \u00e9crit une fonction pour faire des histogrammes surles longueurs  de chaque livres.\ndef make_histplot(stat, x_label, y_label, ax):\n    # Draw the histogram and fit a density plot.\n    sns.histplot(x=philo_df.loc[philo_df[\"title\"]==book].sentence_length, kde=True, ax=ax)\n\n    # get the y-coordinates of the points of the density curve.\n    dens_list = ax.get_lines()[0].get_data()[1]\n\n    # find the maximum y-coordinates of the density curve.\n    max_dens_index = dens_list.argmax()\n\n    # find the mode of the density plot.\n    mode_x = ax.get_lines()[0].get_data()[0][max_dens_index]\n\n    # draw a vertical line at the mode of the histogram.\n    ax.axvline(mode_x, color='blue', linestyle='dashed', linewidth=1.5)\n    ax.text(mode_x * 1.05, 0.16, 'Mode: {:.4f}'.format(mode_x))\n\n    # Plot formatting\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n\n#stat_list = [[0.3, 0.5, 0.7, 0.3, 0.5], [0.2, 0.1, 0.9, 0.7, 0.4], [0.9, 0.8, 0.7, 0.6, 0.5],\n             #[0.2, 0.6, 0.75, 0.87, 0.91], [0.2, 0.3, 0.8, 0.9, 0.3], [0.2, 0.3, 0.8, 0.87, 0.92]]\nnum_subplots = len(list_book_names)\nncols = 4\n#nrows = (num_subplots + ncols - 1) \/\/ ncols\nnrows = 15\nfig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 6, nrows * 5))\n#colors = plt.cm.tab10.colors\nfor ax, book in zip(np.ravel(axes), list_book_names):\n    make_histplot(book, book, 'y_label', ax)\n#for ax in np.ravel(axes)[num_subplots:]:  # remove possible empty subplots at the end\n    #ax.remove()\nplt.show()","fd3b8616":"# On fait un displot en mettant toutes les livres sur la m\u00eame plot\nsns.displot(philo_df, x=\"sentence_length\", hue=\"title\", kind=\"kde\", fill=True, height=8.27, aspect=21.7\/8.27);\n","efc5a081":"#boxplot\nf, ax = plt.subplots(figsize=(25, 15))\nfig = sns.boxplot(x=\"title\", y=\"sentence_length\", data=philo_df)\nfig.axis(ymin=0, ymax=1500);\n#les titles sont tous m\u00e9lang\u00e9s sur ce boxplot, je sai spas comment le param\u00e9triser ??","77dc5707":"# regarder combien de auteurs il y en a : \nphilo_df[\"author\"].value_counts()\nlen(philo_df[\"author\"].value_counts())\n#du coup il y a 36 auteurs dans ce dataset","0ca02213":"# regarder le nombre d'\u00e9coles  : \nphilo_df[\"school\"].nunique()# il y a 13 \u00e9coles inclus dans ce dataset\nphilo_df[\"school\"].unique()","603b61a2":"# regarder combien de repr\u00e9senteurs (auteurs) dans chaque \u00e9cole : \nsns.catplot(y=\"school\", hue=\"author\", kind=\"count\", data=philo_df, height=15, aspect=11.7\/15);","a40274c7":"#Je vais selectionner les livres de Descartes : 2 sont disponibles dans ce dataset. \n#Cr\u00e9er une dataframe des livres de Descartes\nDescartes = philo_df.loc[philo_df[\"author\"] == \"Descartes\"]\nprint(\"Nomre de phrases par livre dans le corpus de Descartes \\n\", Descartes[\"title\"].value_counts())\nprint(\"\\n\")\nDescartes.head()","84ba6a75":"#on prends \"Meditations On First Philosophy\" et \"Discourse On Method\" de Descartes, on prends \"sentence_lowered\" et aussi \"lemmatized_str\"\nDescartes_als = Descartes[[\"title\",\"author\",\"sentence_lowered\",\"lemmatized_str\"]]\nDescartes_als.head()","9ceca42d":"#maintenant on va combiner les sentences de chaque livre de Descartes ensemble\nDescartes_groupby = Descartes_als.groupby(['title', 'author'], as_index = False).agg({'sentence_lowered': ' '.join, 'lemmatized_str': ' '.join})\nDescartes_groupby","086abd27":"#on va combiner les sentences de chaque livre de tout dataset\ndf_groupby = philo_df.groupby(['title', 'author','school'], as_index = False).agg({'sentence_lowered': ' '.join, 'lemmatized_str': ' '.join})\ndf_groupby","777b8638":"# fonction qui pertmet d'appliquer un tour de techniques de lavage de texte \nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    \n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","58f26b32":"#on fait le nettoyage\ntext_clean = df_groupby[\"sentence_lowered\"].apply(round1)\nlem_text_clean = df_groupby[\"lemmatized_str\"].apply(round1)","16f30748":"#on remplace l'ancien texte\ndf_groupby[\"text_clean\"] = text_clean\ndf_groupby[\"lem_text_clean\"] = lem_text_clean\ndf_clean = df_groupby.drop([\"sentence_lowered\",\"lemmatized_str\"], axis =1)","09d434dc":"##ici on regarde pour Descartes\n#on execute le nettroyage\ntext_clean = Descartes_groupby[\"sentence_lowered\"].apply(round1)\nlem_text_clean = Descartes_groupby[\"lemmatized_str\"].apply(round1)\n#on remplace l'ancien text\nDescartes_groupby[\"text_clean\"] = text_clean\nDescartes_groupby[\"lem_text_clean\"] = lem_text_clean\nDescartes_clean = Descartes_groupby.drop([\"sentence_lowered\",\"lemmatized_str\"], axis =1)","8fa257af":"#Si on veut regarder le text int\u00e9gr\u00e9\n#Descartes_clean.iloc[0,3]\n#Descartes_clean.iloc[0,2]","93390b66":"# on va cr\u00e9er une matrice \"dicument -term\" en utilisant CountVectorizer, et exclure les mots en anglais les plus fr\u00e9quents\n# Mise \u00e0 jour suite \u00e0 la relecture : on utilise maintenant TfidVectorizer, qui est normalis\u00e9\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncv = TfidfVectorizer(stop_words='english', decode_error='ignore')\ndata_cv = cv.fit_transform(Descartes_clean.text_clean)#le dataframe Descartes\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = Descartes_clean.index\ndata_dtm","365eee37":"#idem pour lem_text_clean\nlem_data_cv = cv.fit_transform(Descartes_clean[\"lem_text_clean\"])\nlem_data_dtm = pd.DataFrame(lem_data_cv.toarray(), columns=cv.get_feature_names())\nlem_data_dtm.index = Descartes_clean.index\nlem_data_dtm","658fde79":"# Je veux le pickle pour utiliser plus tard, mais j'ai l'impression que \u00e7a ne marche pas sur kaggle...\n# MR : si tu as le read qui fonctionne plus bas, c'est que \u00e7a marche. Je ne comprends juste pas pourquoi le fichier n'appara\u00eet pas !\ndata_dtm.to_pickle(\".\/dtm.pkl\")","dbc21e40":"# lire document-term matrix\ndata = pd.read_pickle('dtm.pkl')\ndata = data.transpose()\ndata.head()\n\n#l\u00e0, on voit que \"abondon\" et \"abondoned\" sont pareilles au niveu de sens, du coup c'est peut-\u00eatre mieux de prendre les textes \"lemmatized\"","2b729b1f":"# Trouve les 10 mots ls plus fr\u00e9quents de deux livres de descartes.\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False).head(10)\n    top_dict[c]= list(zip(top.index, top.values))\n\ntop_dict","1b798b1d":"lem_data = lem_data_dtm.transpose()\n# Trouve les 10 mots ls plus fr\u00e9quents de deux livres de descartes.\nlem_top_dict = {}\nfor c in lem_data.columns:\n    top = lem_data[c].sort_values(ascending=False).head(10)\n    lem_top_dict[c]= list(zip(top.index, top.values))\n\nlem_top_dict","5a26dfd2":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 30 words for each \nwords = []\nfor book in data.columns:\n    top = [word for (word, count) in top_dict[book]]\n    for t in top:\n        words.append(t)\n        \nwords","c70e500f":"!pip install wordcloud","31b26091":"from wordcloud import WordCloud, STOPWORDS\nstop_words = set(STOPWORDS)\n\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","21817d74":"from wordcloud import WordCloud, STOPWORDS\nstop_words = STOPWORDS\n\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","df39810d":"import matplotlib.pyplot as plt\n\nfull_names = ['Discourse On Method', 'Anthony Jeselnik']\nplt.rcParams['figure.figsize'] = [10, 6]\n# Create subplots for each book\nfor index, book in enumerate(data.columns):\n    wc.generate(Descartes_clean.text_clean[book])\n    \n    plt.subplot(1, 2, index+1)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(full_names[index])\n    \nplt.show()","8cf689db":"#on vas quand m\u00eame regarder si les wordcloud seront diff\u00e9rent si on prend les textes lemmeniz\u00e9s\n#lem_data = lem_data_dtm.transpose()\nplt.rcParams['figure.figsize'] = [10, 6]\n# Create subplots for each comedian\nfor index, book in enumerate(lem_data.columns):\n    wc.generate(Descartes_clean.lem_text_clean[book])\n    \n    plt.subplot(1, 2, index+1)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(full_names[index])\n    \nplt.show()\n#L\u00e0 on voit que \u00e7a change un peu en prenant le text lemmeniz\u00e9.","d403d498":"from sklearn.feature_extraction import text #ENGLISH_STOP_WORDS de text est diff\u00e9rent que celui de wordcloud\nadd_stop_words = [\"PRON\"]\nstop_words_1 = add_stop_words + list(stop_words)\n#comment supprimer PRON de notre lem_text_clean\n#soit on essye d'utiliser re expression\n#soit on mets le mot \"PRON \" dans les stop_words : on essaye d'abord cette m\u00e9thode.\nwc_1 = WordCloud(stopwords=stop_words_1, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\n","3a9efd0a":"plt.rcParams['figure.figsize'] = [10, 6]\n# Create subplots for each comedian\nfor index, book in enumerate(lem_data.columns):\n    wc_1.generate(Descartes_clean.lem_text_clean[book])\n    \n    plt.subplot(1, 2, index+1)\n    plt.imshow(wc_1, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(full_names[index])\n    \nplt.show()\n#L\u00e0 on voit que \u00e7a change un peu en prenant le text lemmeniz\u00e9.","31c972d8":"df_clean","1942ba87":"#on prends les colonnes pertinents pour clustering : \"text_clean\" et \"school\"\ndf_cluster = df_clean.loc[: ,[\"text_clean\", \"school\"]]\n#df_cluster","991e3eab":"df_clean.shape","94c36341":"from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorize\ntfidvectorizer = TfidfVectorizer(decode_error='ignore', stop_words='english')\ntfidf = tfidvectorizer.fit_transform(df_cluster[\"text_clean\"])\n","88b8187d":"tfidf_matrix = tfidf.todense()#This line can transform scipy.sparse.csr.csr_matrix to numpy.matrixlib.defmatrix.matrix and pd.DataFrame can work with data of this type.\ntype(tfidf_matrix)","54d525c4":"tfidf_matrix.shape","cd59745e":"from sklearn.cluster import KMeans\nSum_of_squared_distances = []\nK = range(1,20)#on a 13 schools dans le dataset\nfor k in K:\n   km = KMeans(n_clusters=k, max_iter=200, n_init=10, random_state=0)\n   km = km.fit(tfidf)\n   Sum_of_squared_distances.append(km.inertia_)\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","a15d6960":"true_k = 7\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\nmodel.fit(tfidf)\nlabels=model.labels_\n#\ndf_cluster[\"labels\"]=pd.Series(labels)\n","8dfa7250":"df_cluster.shape","98bbb398":"df_cluster['school'].unique()","bc7b2030":"print(df_cluster.sort_values(by=['labels']))\n# on peut voir tous les ouvrages cluster\u00e9s.\n# on voit par exemple que le cluster 0 contient surtout de l'analytique, mais aussi un ouvrage german idealism et rationalism.\n# quelle est la relation particuli\u00e8re entre ces deux ouvrages et l'\u00e9cole analytique ? \n# nous n'irons pas dans ce niveau de d\u00e9tail.","d845dfe6":" # Il faut faire un graphique de \u00e7a.\n# par exemple, si on regarde que empirism.\n#df_cluster[labels==1]['school'].value_counts().loc['empiricism']\ndf_cluster[labels==1]['school'].value_counts()","682cdc90":"def get_number_in_cluster (school, cluster):\n    \"\"\"\n    Enter a school as a String (like 'empriricism')\n    And a cluster number as an int(like 0, 1)\n    returns the number of observations of the considered school in the cluster\n    \"\"\"\n    if school in df_cluster[labels==cluster]['school'].value_counts().index:\n        return df_cluster[labels==cluster]['school'].value_counts().loc[school]\n    else:\n        return 0\ndef get_list_school_cluser (school, true_k):\n    \"\"\"\n    Enter a school as a String (like 'empiricism')\n    and the number of classes choosen for the clustering\n    returns a list which indicate the number of observation per classes.\n    \n    For exemple if the output is [0,3,1], it means the considered school has 4 observations\n    3 observations has been clustered in class 1,\n    1 observation has been custered in class 2.\n    \"\"\"\n    list_s = []\n    for i in range(true_k):\n        list_s.append(get_number_in_cluster (school, i))\n    return list_s\n# array de toutes les schools :\n#     df_cluster['school'].unique()\n\nprint('next is the numbe enf emiricism book in custer 5')\nprint(get_number_in_cluster ('empiricism', 5))\n\n\nprint(get_number_in_cluster ('empiricism', 1))\nprint(get_list_school_cluser('empiricism', true_k))\n\ndic={}\n# loop over all the school.\nfor j in df_cluster['school'].unique():\n    dic[j] = get_list_school_cluser(j, true_k)\nprint(dic)","69ce79a7":"len(dic)","d9a55046":"# graphique illisible qui repr\u00e9sente la cellule pr\u00e9c\u00e9dente.\n# nombre d'occurence de l'\u00e9cole en question par cluster\nfor j in dic:\n    plt.plot(dic[j], alpha=0.5, label = j)\nplt.legend(loc = 'upper right')","098c2409":"true_k = 13\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\nmodel.fit(tfidf)\nlabels=model.labels_\n#\ndf_cluster[\"labels\"]=pd.Series(labels)\n","ebb49477":"print(df_cluster.sort_values(by=['labels']))","10df8d6b":"def get_number_in_cluster (school, cluster):\n    \"\"\"\n    Enter a school as a String (like 'empriricism')\n    And a cluster number as an int(like 0, 1)\n    returns the number of observations of the considered school in the cluster\n    \"\"\"\n    if school in df_cluster[labels==cluster]['school'].value_counts().index:\n        return df_cluster[labels==cluster]['school'].value_counts().loc[school]\n    else:\n        return 0\ndef get_list_school_cluser (school, true_k):\n    \"\"\"\n    Enter a school as a String (like 'empiricism')\n    and the number of classes choosen for the clustering\n    returns a list which indicate the number of observation per classes.\n    \n    For exemple if the output is [0,3,1], it means the considered school has 4 observations\n    3 observations has been clustered in class 1,\n    1 observation has been custered in class 2.\n    \"\"\"\n    list_s = []\n    for i in range(true_k):\n        list_s.append(get_number_in_cluster (school, i))\n    return list_s\n# array de toutes les schools :\n#     df_cluster['school'].unique()\n\nprint('next is the numbe enf emiricism book in custer 5')\nprint(get_number_in_cluster ('empiricism', 5))\n\n\nprint(get_number_in_cluster ('empiricism', 1))\nprint(get_list_school_cluser('empiricism', true_k))\n\ndic={}\n# loop over all the school.\nfor j in df_cluster['school'].unique():\n    dic[j] = get_list_school_cluser(j, true_k)\nprint(dic)","4db1614a":"#On va faire un ACP pour r\u00e9duire la dimension et essaye de faire un 2-D plot (si les deux premi\u00e8res axes ont un ratios de variances expliqu\u00e9s assez \u00e9lev\u00e9...)\nfrom sklearn.decomposition import PCA \npca1=PCA(n_components = 2) \nPrincipalComponents2=pca1.fit_transform(tfidf_matrix)\n\npca2=PCA(n_components = 3) \nPrincipalComponents3=pca2.fit_transform(tfidf_matrix)","b62042e0":"pca1.explained_variance_ratio_","9d6b6418":"pca2.explained_variance_ratio_","15fdb7d2":"# visualize by by plotting 2-D\n# la couleur repr\u00e9sente les 7 clusters\nplt.scatter(PrincipalComponents2[:,0],PrincipalComponents2[:,1], c= df_cluster[\"labels\"], label = df_cluster[\"labels\"], cmap='rainbow') #plt.grid()\nplt.xlabel('The firt principal component')\nplt.ylabel('The second principal component')\nplt.show\n\n#on voit que le 2-d exploration de ces observations sont pas tr\u00e8s bien.","066caf2e":"# ici les couleurs repr\u00e9sentent les \u00e9coles\nfor j in df_cluster['school'].unique():\n    plt.scatter(PrincipalComponents2[df_cluster['school']==j,0],PrincipalComponents2[df_cluster['school']==j,1], label=j) #plt.grid()\nplt.legend()","e918cf8d":"# ici les couleurs repr\u00e9sentent les \u00e9coles\n# quand on se oncentre sur seulement deux \u00e9coles, on voit que  les deux \u00e9coles sont assez bien s\u00e9par\u00e9es\n# ci-dessous on voit l'analytic en bas \u00e0 droite et le german idealism en haut \u00e0 gauche.\n\nplt.scatter(PrincipalComponents2[df_cluster['school']=='analytic',0],PrincipalComponents2[df_cluster['school']=='analytic',1], label='analytic') #plt.grid()\nplt.scatter(PrincipalComponents2[df_cluster['school']=='german_idealism',0],PrincipalComponents2[df_cluster['school']=='german_idealism',1], label='german_idealism')\nplt.legend()","605d76f3":"# ici les couleurs repr\u00e9sentent les \u00e9coles\n# quand on se oncentre sur seulement deux \u00e9coles, on voit que  les deux \u00e9coles sont assez bien s\u00e9par\u00e9es\n# ici on teste avec 3 \u00e9coles que le clustering \u00e0 7 classes n'arrivait pas \u00e0 isoler.\n# ou trouve \u00e9tonnement que ces 3 \u00e9coles sont relativement s\u00e9par\u00e9es sur le plan.\n# n\u00e9anmoins, on n'affiche pas les 10 autres genres qui ont aussi influenc\u00e9 le custering..\n\nplt.scatter(PrincipalComponents2[df_cluster['school']=='continental',0],PrincipalComponents2[df_cluster['school']=='continental',1], label='continental') #plt.grid()\nplt.scatter(PrincipalComponents2[df_cluster['school']=='german_idealism',0],PrincipalComponents2[df_cluster['school']=='german_idealism',1], label='german_idealism')\nplt.scatter(PrincipalComponents2[df_cluster['school']=='feminism',0],PrincipalComponents2[df_cluster['school']=='feminism',1], label='feminism') #plt.grid()\nplt.legend()","be5b16ca":"# on fait un clustering seulement avec les donn\u00e9es \n# continental\n# german_idealism\n# feminism\n\ndf_cluster_3_genres = df_cluster[(df_cluster['school'] == 'continental') | (df_cluster['school'] == 'german_idealism') | (df_cluster['school'] == 'feminism')]\ndf_cluster_3_genres = df_cluster_3_genres.reset_index()\ntfidf_3_genres = tfidvectorizer.fit_transform(df_cluster_3_genres[\"text_clean\"])\ntrue_k = 3\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\nmodel.fit(tfidf_3_genres)\nlabels=model.labels_\n#\ndf_cluster_3_genres[\"labels\"]=pd.Series(labels)\n# df_cluster_3_genres\nprint(labels)","494a1523":"print(df_cluster_3_genres.sort_values(by=['labels']))","94de1058":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport seaborn as sns\n#import matplotlib.pyplot as plt","c403d7cc":"#prendre les colums\ndf_classficiation = philo_df.loc[:, [\"lemmatized_str\",\"author\", \"school\"]]#selectionner 2 colonnes\ndf_classficiation.head()\n\n#\"school\" sera variable cible et \"lemmatized_str\" will be la variable expliquative.\n#on va voir si les donn\u00e9es sont \u00e9quilibres\nplt.figure(figsize = (30,20))\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(x=\"school\", hue=\"author\", data=df_classficiation)\nplt.show()","72cf581a":"# split les donn\u00e9es\nX_train, X_test, y_train, y_test = train_test_split(df_classficiation['lemmatized_str'], df_classficiation['school'], test_size=0.4, random_state=42, stratify= df_classficiation['school'])\n#comme les algorithms que on va utiliser sera parametric, du coup \u00e7a converge vite, on a pas besoins de beaucoup de observation pour l'entraitement\n#on utilise stratify car nos donn\u00e9es ne sont pas \u00e9quilibres.","a9c1c05b":"# ici on classifie toute le corpus dans le corpus, sans test set\n# input : 59 lignes de matrices correspondant \u00e0 chaque corpus\nfrom sklearn.linear_model import SGDClassifier\nlr = SGDClassifier().fit(tfidf_matrix, df_cluster['school'].to_list())\nlr.predict(tfidf_matrix)","1d9867bf":"plot_confusion_matrix(lr,tfidf_matrix,  df_cluster['school'].to_list(), xticks_rotation = 'vertical')","6986b51e":"# ici on r\u00e9alise pour chaque phrase une nouvelle matrice, et on classifie petit \u00e0 petit.\nfrom sklearn.linear_model import SGDClassifier\nlr = SGDClassifier().fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\nplot_confusion_matrix(lr,tfidvectorizer.transform(X_test.to_list()),  y_test, xticks_rotation = 'vertical')\nprint(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\nprint(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))\n# \u00e0 vue de nez, c'est beaucoup mieux\n#classification_report(y_test, lr.predict(tfidvectorizer.transform(X_test.to_list())))\n\n# comme de dataset n'est pas balanc\u00e9, on doit exprimer en fonction du genre l'accuracy. : \u00e0 faire","2336bde1":"from sklearn.metrics import classification_report\nprediction = lr.predict(tfidvectorizer.transform(X_test.to_list()))\nprint(classification_report(y_test, prediction))","6c1605ff":"# onessaye d'autres mod\u00e8les.\nlr = SGDClassifier(loss='log').fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\nprint('train accuracy')\nprint(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\nprint('test accuracy')\nprint(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))\n\nprediction = lr.predict(tfidvectorizer.transform(X_test.to_list()))\nprint(classification_report(y_test, prediction))","3b09f803":"# onessaye d'autres mod\u00e8les. # meilleur\nlr = SGDClassifier(loss='modified_huber').fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\nprint('train accuracy')\nprint(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\nprint('test accuracy')\nprint(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))\n\n\nprediction = lr.predict(tfidvectorizer.transform(X_test.to_list()))\nprint(classification_report(y_test, prediction))","53dd130a":"lr = SGDClassifier(loss='perceptron').fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\nprint('train accuracy')\nprint(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\nprint('test accuracy')\nprint(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))\n\nprediction = lr.predict(tfidvectorizer.transform(X_test.to_list()))\nprint(classification_report(y_test, prediction))","a579e42f":"#explose la ram\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# lr = LinearDiscriminantAnalysis().fit(tfidvectorizer.transform(X_train.to_list()).toarray(), y_train.to_list())\n# print(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\n# print(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))","4e56ae8f":"# explose la ram\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# lr = LinearDiscriminantAnalysis().fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\n# print(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\n# print(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))","5cc82dd9":"# tr\u00e8s mauvais, on passe.\n# from sklearn import tree\n# lr = tree.DecisionTreeClassifier().fit(tfidvectorizer.transform(X_train.to_list()), y_train.to_list())\n# print(lr.score(tfidvectorizer.transform(X_train.to_list()), y_train.to_list()))\n# print(lr.score(tfidvectorizer.transform(X_test.to_list()), y_test.to_list()))","b3597ba0":"# NLP atelier : philosophie text exploration","86d01fd5":"Je vais faire une selection de donn\u00e9es de DB original, les colums que je vais prendre sont : \n\n    1. \"school\"\n    2. \"lemmatized_str\"","cf8eb1c0":"Pourquoi ici on a pris TF-IDF m\u00e9thode et son explication peut r\u00e9ferer au article ci-dessous : \n\nhttps:\/\/medium.com\/@cmukesh8688\/tf-idf-vectorizer-scikit-learn-dbc0244a911a","8b754f37":"Dans cet \u00e9tape, les r\u00e9alisations seront :\n\n1. cr\u00e9er une dataframe qui prends les livres de Descartes. Cela est r\u00e9serv\u00e9 pour r\u00e9pondre ma premi\u00e8re question de la Partie 1 :  Est-ce que des diff\u00e9rents livres de m\u00eame \u00e9cole partagent des sujets similaires ?\n\n2. cr\u00e9er une dataframe qui int\u00e9gre toutes phrases des livres de Descartes\n\n3. cr\u00e9er une dataframe qui contient les livres de school \"analytic\" : qui servira \u00e0 voir si ils partagent des sujets similaires dans une m\u00eame \u00e9cole.\n\n4. cr\u00e9er une fonction pour data cleaning (text cleaning ici)\n\n5. vectoriser le text","4e6ddaaf":"m\u00eame quand on augmente le nombre de cluster, on a toujours les \u00e9coles qui sont dispers\u00e9es \u00e0 travers les clusters\n - continental > meilleur\n - german idealism > toujours synd\u00e9 en 2\n - feminism > toujours divis\u00e9.","bea3ffd6":"## Partie 3 : data preparation","d005b2ae":"On voit que ici t1 et t2 ont la m\u00eame type et m\u00eame taille.","a9e15391":"**Les questions auxquelles on va r\u00e9pondre dans cet atelier** \n\nJe me suis pos\u00e9 quelques questions que je voudrais r\u00e9soudre en utilisant des m\u00e9thodes de NLP pour ces grands livres de philosophie :\n\n1. Est-ce que des diff\u00e9rentes livres de m\u00eame philosophe partagent des sujets similaires ? (mesur\u00e9s par des mots). Par example, on a deux livres de Descartes dans ce dataset, est-ce que ses deux livres partagent m\u00eame sujet rep\u00e9sent\u00e9 par m\u00e9thode wordcloud ?\n\n2.  Est-ce que des diff\u00e9rentes livres de m\u00eame school partagent des topics similaires ? Par example, le school de \"communism\"\n\n3. Est-ce que il y a des simularit\u00e9s entre des schools ? par example, les  'analytic' sont-ils proche aux 'empiricism' ? (on pourra faire une clustering en utilisant des m\u00e9thodes tel que k-means)\n\n4. Est-ce que c'est favorable de faire une classification et identifier quel type de philosophie il appartient en utilisant ses paroles ou le text quelqu'un a \u00e9crit ? ","9dbd8a42":"**M\u00e9thodologie : CRISP-DM reference model**\n\nPour r\u00e9aliser nos objectives, on prends la m\u00e9thodologie de CRISP-DM qui contient des d\u00e9marches standards pour data science :\n\n1. business understanding : quels ont nos objectifs ? \n\n2. data understanding : data description, data exploration, data quality\n\n3. data preparation : data selection, data cleaning, data construction, data int\u00e9gration\n\n4. modeling : technique selection \n\n5. evaluation\n\n6. deployment : on va pas r\u00e9aliser cette \u00e9tape dans cet atelier.\n","0bdb05cb":"m\u00eame essai avec un k (nombre de cluster) plus grand.","a50f32cb":"Analyse de ces plots : les tailles de phrases de chaque livre ne suivent pas une distribution normale. ils sont tous right-skewed. ","a3297ceb":"## Partie 1 : business understanding","9a4d6797":"Comaparer \"tockenized_str\" et \"lemmatized_str\" : La tokenisation cherche \u00e0 transformer un texte en une s\u00e9rie de tokens individuels. Dans l'id\u00e9e, chaque token repr\u00e9sente un mot\ncomme le cas ici. La lemmatisation d\u00e9signe un traitement lexical apport\u00e9 \u00e0 un texte en vue de son analyse. Dans le text ici, on voit que 's devient be, tous les he,she,they etc. deviennent '-PRON-'. ","36d293f4":"L\u00e0, les deux livres de Descarte partage un mot le plus important \"PRON\". \u00c7a veux dire quoi \"PRON\"? J'ai fait un check dans leurs texts : en fait il remplace tout les \"it, he, she,...\" par PRON quand on a pris le text lemmenniz\u00e9.","19c2e74e":"Je voudrais regarder si les topics sont similares dans un m\u00eame school : \n\n1. Est-ce que les repr\u00e9sentatives de Descartes partagent la m\u00eame topic ?\n2. Est-ce que on pourra faire un clustering des schools? y-a-il des schools sont plus proches que des autres ?","cb211ad3":"## Vectorization par TF-IDF Vectorizer scikit-learn\nMR : Quel est l'objectif ici ?\nIl faut r\u00e9ussir \u00e0 faire une calssification des phrases > l'\u00e9cole","0648d9de":"**Description de dataset :**\n\nIl y a 360808 lignes et 11 colonnes. Chaque ligne est une phrase d'un livre de philosophie. Les colonnes incluent le nom du livre, l'\u00e9cole, l'auteurs, et des variables concernant chaque sentence.\n\nIl y a 13 \u00e9coles de philosophie inclues dans ce dataset : 'plato', 'aristotle', 'empiricism', 'rationalism', 'analytic', 'continental', 'phenomenology', 'german_idealism', 'communism','capitalism', 'stoicism', 'nietzsche', 'feminism'.\n\nDes \u00e9coles tel que 'plato' et 'aristotle' ont seulment leurs livres inclus dans ce dataset ; d'autres tels que 'empiricism' sont represent\u00e9s par plusieurs auteurs comme Locke et Berkeley.","bc1a0ea1":"## Partie 2 : Data understanding","582fc278":"# EDA -Exploratory data analysis","5d32e91e":"https:\/\/www.nltk.org\/book\/ch01.html","6719d90c":"L\u00e0, on voit d\u00e9j\u00e0 que text lemmetiz\u00e9 a beacoup moins de mots.","f54dd19a":"### Document-Term Matrix","0a4f0478":"### Exploration 3: la distribution de la variable \"sentence_length\" pour chaque livre**","a6193144":"**Dataframe : Descartes**","725b028f":"## Clustering avec K-means","93faee71":"le mod\u00e8le ci-dessous est le meilleur de ce notebook.\nDans un second notebook, j'ai une accuracy de de 0.78 avec un r\u00e9seau de neuronnes.\n\nMais d'un autre c\u00f4t\u00e9, ce r\u00e9seau de neuronnes consomme beaucoup plus de ressources \u00e0 l'entra\u00eenement et \u00e0 la pr\u00e9diction que ce mod\u00e8le lin\u00e9aire.\nNotre choix de mod\u00e8le va reposer sur l'application, et les ressources dont on dispose.","50962669":"L\u00e0, on voit que les topics sont diff\u00e9rents pour diff\u00e9rents livres de Decartes.","81f2e9f1":"L\u00e0, on voit une ligne presque droite, mais on remarque qu'il y a un 'coude' quand k=6 ou 7. on essaye de cluster dans 7 groupes pour voir d\u00e9j\u00e0 si les 13 \u00e9coles sont bien dans les 7 grands groupes.","044baf73":"Analyze du r\u00e9sultat de plot ci-dessus : \n\nIci on voit que les donn\u00e9es ne sont pas \u00e9quilibres entres diff\u00e9rents schools. Par example il y a beacoup moins de donn\u00e9es de school \"stoicism\" ;  d'autre remarque est des schools tel que \"analytic\" ont plusieur auteurs diff\u00e9rents, alors des school comme \"plato\" et \"nietzsche\" ont que un auteur.  Du coup pour faire une meilleur mod\u00e8le, on pourra d'abord faire une s\u00e9lection de donn\u00e9es ici.\n\nQuesion ici est : quelle principe nous permet d'avoir une s\u00e9lection adapt\u00e9e ? Je me dit : okey, je vais prendre toutes les auteurs, mais je vais pas utiliser les metrics comme \"accuracy\" car celle-ci est plut\u00f4t adaptable pour des donn\u00e9es \u00e9quilibr\u00e9es. \n\nLes metrics que je vais utiliser pour mesurer la performance seront : ROC-curve ? matrix-confusion ? \u00e0 voir plus tard.","cf709aef":"### Exploration 2: le corpus 'sentence_str'","8221b95b":"### Exploration 1: quel est le lien entre variables \"sentence_spacy\", \"sentence_str\", \"tokenized_txt\" and \"lemmatized_str\" ?","0cfcf8b1":"On peut voir sur l'ouput dessus que pour certaines classes comme :\n - capitalism, \n - empiricism, \n - phenomenology\n - comunism\n - rationalism\n \nLe clustering fonctionne bien : toutes les observations des classes consid\u00e9r\u00e9es sont bien dans un cluster unique.\n\n\nD'un autre c\u00f4t\u00e9, cela ne fonctionne pas du tout pour des classes comme :\n - continental\n - german idealism\n - feminism","7be244c1":" - Le cluster 0 a bien seulement du german idealism\n - Le cluster 2 a bien seulement du feminism\n - Le cluster 1 a du continental et du germanidealism","ee4937a6":"# Classification","eee149a1":"Analyse :  on voit qu'il y a 11 variables qui sont : \"title\"--le non de livre, \"author\", \"school\", \"sentence_spacy\", \"sentence_str\", \"original_publication_date\", \"corpus_edition_date\", \"sentence_length\", \"sentence_lowered\", \"tokenized_txt\", \"lemmatized_str\".\n\nPour mieux comprendre ces colonnes, on se pose les questions ci-dessous : \n\n1. quel est le lien entre variables \"sentence_spacy\", \"sentence_str\", \"tokenized_txt\" and \"lemmatized_str\" ?\n\n2. la distribution de la variable \"sentence_length\" ?"}}