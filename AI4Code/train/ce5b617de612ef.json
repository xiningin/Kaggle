{"cell_type":{"998b6337":"code","3e73340f":"code","4d868725":"code","af0609a6":"code","2edb61a1":"code","39eadb46":"code","5c39a3ed":"code","0dd8fc27":"code","d025d7dc":"code","73fbdd23":"code","d0b2dd7c":"code","f99678bf":"code","5a983545":"code","b649acc7":"code","1b9ce0f0":"code","50fe086d":"code","993937e6":"code","dd1bd6d0":"code","687155e0":"code","7ec89cbc":"code","cde20aa3":"code","1955c409":"code","5c150fd5":"code","9e20184e":"code","cdda9c0e":"code","6808f015":"code","7f2a5c75":"code","71bf40b1":"code","7249de19":"code","948d8828":"code","14e63346":"code","06374b87":"code","9515735a":"code","91f99480":"code","2d5973ae":"code","de9683bb":"code","66dbbc3c":"code","e3d935c5":"code","65f04626":"code","1ab81714":"code","fede9530":"code","d6bc2fe6":"code","81c51083":"code","2651b157":"code","0a0fdd62":"code","68401e69":"code","b15e1d01":"code","f3dc07a2":"code","6d6fab12":"code","0d3e7eed":"code","0af8b0af":"code","f9a6e0b6":"code","e8c3dddd":"code","5e15aadc":"code","accb7b39":"code","301eb914":"code","fcbfc613":"code","024e4f20":"code","92e031f7":"code","99d6dec4":"code","13c68729":"code","b99a83dd":"code","a43d4a0f":"code","7e21a6cb":"code","a20015f6":"code","2b004990":"code","f6eaa83f":"code","4efad1fd":"code","8bd8e570":"code","bf488015":"code","83189386":"code","eae2ad21":"code","ee125fa7":"code","777eed78":"code","d1332a1c":"code","04141e39":"code","98673a5f":"code","cfd255d4":"code","8729c272":"code","3d32d2e4":"code","50cddb64":"code","bafa0ccd":"code","a556f7c8":"code","2b205f97":"code","93fbc6a2":"code","84f60f5d":"code","651f83a1":"code","21e63b3a":"code","290d83be":"code","445b7671":"code","d74bbd3e":"code","af36ac35":"code","83f67752":"code","bf64fb2b":"code","0929d159":"code","5f137c11":"code","482ff61b":"markdown","80932756":"markdown","3de79266":"markdown","10218e66":"markdown","4e509682":"markdown","180fcc86":"markdown","a1dff775":"markdown","7257f0e2":"markdown","75669de6":"markdown","82f7cdbc":"markdown","b9fda46c":"markdown","37f3b0dd":"markdown","7fab5a4f":"markdown","0fcbf2dc":"markdown","b6531afd":"markdown","28d03a0f":"markdown","ceaa974c":"markdown","501a2501":"markdown","454d315b":"markdown","63d783f1":"markdown","6e0b6b65":"markdown","9e89e4ac":"markdown","8285f325":"markdown","4c2f6ae9":"markdown","93130f8f":"markdown","b0cd5401":"markdown","91071018":"markdown","8e37a5dc":"markdown","1f5f016b":"markdown","d17d8381":"markdown","18f43539":"markdown","ef74ddc6":"markdown","eb5c89c3":"markdown","a27318ea":"markdown","1276d12a":"markdown","12e65190":"markdown","499135ec":"markdown","b9cda5c4":"markdown","48acc78d":"markdown","31c1a2e1":"markdown","f8745baf":"markdown","cb6004f9":"markdown","ff989566":"markdown","bea0dfbe":"markdown","c9f3388a":"markdown","44f7842b":"markdown","99221442":"markdown","1b9ccf71":"markdown","afc50e87":"markdown","5375c7dc":"markdown","2e68d55d":"markdown","4c9a748a":"markdown","5d21f8d3":"markdown"},"source":{"998b6337":"# !pip install plotly\n# !pip install cufflinks\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nnp.set_printoptions(precision=4)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom plotly.offline import init_notebook_mode,iplot\nimport plotly.graph_objects as go\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\n#----------------------\nfrom ipywidgets import interactive\nfrom ipywidgets import interact\n","3e73340f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4d868725":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","af0609a6":"train.head(10)","2edb61a1":"test.head()","39eadb46":"train.info() ","5c39a3ed":"test.info()","0dd8fc27":"train.describe()","d025d7dc":"test.describe()","73fbdd23":"print(\"Statistics of the target variable (Survived) :\")\nprint(\"--------------------------------\")\nprint(train['Survived'].describe())\nprint(\"Skewness: %f\" % (train['Survived']).skew())","d0b2dd7c":"AC = train.Survived\nAC.plot(kind='hist', figsize=(10, 4), linewidth=2, color='r')\nplt.xlabel(\"Survived\", labelpad=15)\nplt.ylabel(\"frequency\", labelpad=15)\nplt.title(\"The Survived Distrbution\", y=1.012, fontsize=22);\n","f99678bf":"def var_standardized(v):\n        stand= (v - v.mean()) \/ train.std()\n        return stand\n","5a983545":"df_train = var_standardized(train)\nfig, ax1 = plt.subplots(figsize=(20, 10))\nsns.boxplot(data=df_train, orient='h', fliersize=8, linewidth=1.5, saturation=1.5, ax=ax1)\nplt.xlabel('Numerical value', size=20);\n","b649acc7":"#shows null values \nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False ,cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False , cmap='viridis')\nax[1].set_title('Test data');","1b9ce0f0":"#collect Numrical values to represnt features distrbution.\nnum_var = [f for f in train.columns if train.dtypes[f] != 'object']\nnum_var.remove('Survived')\nnum_var.remove('PassengerId')\n","50fe086d":"#vn = pd.melt(train, value_vars=num_var)\n#fig = sns.FacetGrid(vn, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, palette='Set3') # we using FacetGrid \n#fig = fig.map(sns.distplot, \"value\")\n#plt.show()\n","993937e6":"f,ax=plt.subplots(1,2,figsize=(16,7))\ntrain['Survived'][train['Sex']=='male'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[0],shadow=True)\ntrain['Survived'][train['Sex']=='female'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.1f%%',ax=ax[1],shadow=True)\nax[0].set_title('Survived (male)')\nax[1].set_title('Survived (female)')\n\nplt.show()","dd1bd6d0":"plt.figure(figsize=(18,8));\nsns.violinplot(x =\"Sex\", y =\"Age\", hue =\"Survived\",  data = train, split = True , palette='Set3');\nplt.figtext(y=0.9,x=0.3,s=\"Distrubution of Passengers Age per Sex\",fontfamily='fantasy',fontsize =23);\n\n","687155e0":"plt.figure(figsize=(18,8));\ntrain['Fare_Range'] = pd.qcut(train['Fare'],4) \nsns.barplot(x ='Fare_Range', y ='Survived', \ndata = train, palette='Set3') ;\nax=plt.figtext(y=0.9,x=0.3,s=\"Range of Fare for Survived Passengers\",fontfamily='fantasy',fontsize =23)\n\nplt.show();","7ec89cbc":"ax=plt.figure(figsize=(18,10));\nax=sns.catplot(x ='Embarked', hue ='Survived',  kind ='count', col ='Pclass', data = train , palette='Set3' );\n#ax.set_title('Survived per Pclass and Embarked ports',high)\nax=plt.figtext(y=1.0,x=0.24,s=\"Survived per Embarkation ports for each Class\",fontfamily='fantasy',fontsize =23);\n#fontfamily=sans-serif', 'cursive', 'fantasy', 'monospace\n","cde20aa3":"sns.set_style(\"whitegrid\");\nsns.pairplot(train[[\"Survived\",\"Pclass\",\"Fare\",\"Age\"]], hue=\"Survived\", size=3);\nplt.figtext(y=1.0,x=0.3,s=\"Correlation between Age, PClass and Fare per Survival Rate \",fontfamily='fantasy',fontsize =18);\n\nplt.show()","1955c409":"\n#defining data\ntrace = go.Scatter(x = train['Age'],y=train['Fare'],text = train['Survived'],mode='markers')\ndata=[trace]\n#defining layout\nlayout = go.Layout(title='Fare Vs Age Scatter Plot',xaxis=dict(title='Age'),yaxis=dict(title='Fare'),hovermode='closest')\n#defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\niplot(figure)","5c150fd5":"# \nimport plotly.express as px\npx.histogram(train , x='Fare' , color='Sex',title='Distribution for Fare per Sex')","9e20184e":"# defining data\ntrace = go.Histogram(x=train['Age'],nbinsx=40,histnorm='percent')\ndata = [trace]\n# defining layout\nlayout = go.Layout(title=\"Age Distribution\")\n# defining figure and plotting\nfig = go.Figure(data = data,layout = layout)\niplot(fig)\n ","cdda9c0e":"from ipywidgets import interact\n\n@interact\ndef create_fare_plot(Feature = train.drop(['Name','PassengerId','Fare'], axis =1).columns):\n    sns.barplot(data = train, x = Feature, y ='Fare',palette='Set3')\n    plt.title(f'Bar Plot of the Fare grouped by the {Feature}')\n","6808f015":"lab = train[\"Survived\"].value_counts().keys().tolist()\n#values\nval = train[\"Survived\"].value_counts().values.tolist()\ntrace = go.Pie(labels=lab, \n                values=val, \n                marker=dict(colors=['red'])\n              )\ndata = [trace]\n\n#set title \nlayout = go.Layout(title=\"Survived Distribution\")\nfig = go.Figure(data = data,layout = layout)\niplot(fig)\n","7f2a5c75":"# What is the percentage of missing values\nTRM=train.isna().sum().sum()\nTSM=test.isna().sum().sum()\nprint(f'Mising Value percentage for train: {(TRM\/ len(train))*100}%')\nprint(f'Mising Value percentage for test: {(TSM \/ len(test))*100}%')","71bf40b1":"print(\"- Mising Value:\\n\", train.isna().sum() ,\"\\n\\n- Total of Mising Value : \", train.isna().sum().sum() )","7249de19":"print(\"- Mising Value:\\n\", test.isna().sum() ,\"\\n \\n- Total of Mising Value : \", test.isna().sum().sum() )","948d8828":"#Dealing with Missing Values of 'Embarked' Feature\n#on train:\ntrain.Embarked.replace(np.nan , 'S' , inplace=True)#the highest value \n#on test:\ntest.Embarked.replace(np.nan , 'S' , inplace=True)#the highest value \n\n#Dealing with Missing Values of 'Cabin' Feature\n#on train:\ntrain['Cabin'] = train['Cabin'].map(lambda x:0  if pd.notnull(x) == False  else 1 )\n#on test:\ntest['Cabin'] = test['Cabin'].map(lambda x:0  if pd.notnull(x) == False  else 1 )\n\n#Dealing with noise of 'Name' Feature\n#on train:\ntrain['Name'] = train['Name'].str.replace('(' , '')\ntrain['Name'] = train['Name'].str.replace(')' , '')\n#train['Name'] = np.where(train['Name'].isna(), author_extract, train['Name'])\n#on test:\ntest['Name'] = test['Name'].str.replace('(' , '')\ntest['Name'] = test['Name'].str.replace(')' , '')\n\n\n#Dealing with Missing Values of 'Fare' Feature\n#on test:\nmean_fare= pd.DataFrame(test.groupby('Pclass')[['Fare']].mean())\n\nfor item, i in test['Fare'].iteritems():\n    if pd.notnull(i)==False:\n        pclass_fare=test.Pclass.iloc[item]        \n        test['Fare'].iloc[item]=mean_fare.iloc[pclass_fare-1][0]\n\n","14e63346":"#Dealing with Missing Values of 'Age' Feature\nmean_age= pd.DataFrame(test.groupby('Pclass')[['Age']].mean())\nmean_age","06374b87":"#defining a function 'impute_age'\ndef impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']\n    \n    # Passing age_pclass[0] which is 'Age' to variable 'Age'\n    Age = age_pclass[0]\n    \n    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'\n    Pclass = age_pclass[1]\n    \n    #applying condition based on the Age and filling the missing data respectively \n   \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 30\n\n        else:\n            return 25\n\n    else:\n        return Age","9515735a":"#filling the missing value of 'Age' by mean\n#on train:\nfor item, i in train['Age'].iteritems():\n    if pd.notnull(i)==False:\n        Age_ver2 = impute_age([i, train.Pclass.iloc[item]] )\n        train['Age'].iloc[item] = Age_ver2\n        ","91f99480":"#filling the missing value of 'Age' by mean\n#on test:\nfor item, i in test['Age'].iteritems():\n    if pd.notnull(i)==False:\n        Age_ver2 = impute_age([i, test.Pclass.iloc[item]] )\n        test['Age'].iloc[item] = Age_ver2\n        ","2d5973ae":"train.isna().sum().sum()","de9683bb":"test.isna().sum().sum()","66dbbc3c":"#shows null values \nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False ,cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False , cmap='viridis')\nax[1].set_title('Test data');","e3d935c5":"\nfig, ax7 = plt.subplots(figsize=(16,8))\n\ntrain_correlations = train.corr();\nsns.heatmap(train_correlations, annot=True,ax=ax7 );        ","65f04626":"train= pd.get_dummies(train, columns=['Sex', 'Embarked'], drop_first=True)\ntrain = pd.get_dummies(train, columns=['Pclass'], drop_first=True)","1ab81714":"test= pd.get_dummies(test, columns=['Sex', 'Embarked'], drop_first=True)\ntest= pd.get_dummies(test, columns=['Pclass'], drop_first=True)","fede9530":"#remove non-characters from the Name feature on train : \ntrain['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain['Name'] = train['Name'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n# replaced values of Name to title (Miss , Mrs) to apply dummies easly \ntrain['Name'] = train['Name'].replace('Mlle', 'Miss')\ntrain['Name'] = train['Name'].replace('Ms', 'Miss')\ntrain['Name'] = train['Name'].replace('Mme', 'Mrs')\n    \ntrain= pd.get_dummies(train, columns=['Name'], drop_first=True)\n    ","d6bc2fe6":"#remove non-characters from the Name feature on test : \ntest['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n# merge Rare titles in data set to one column \ntest['Name'] = test['Name'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n#replaced values of Name to title (Miss , Mrs) to apply dummies easly \ntest['Name'] = test['Name'].replace('Mlle', 'Miss')\ntest['Name'] = test['Name'].replace('Ms', 'Miss')\ntest['Name'] = test['Name'].replace('Mme', 'Mrs')\n    \ntest= pd.get_dummies(test, columns=['Name'], drop_first=True)","81c51083":"train.head()","2651b157":"\ntest.head()","0a0fdd62":"train[\"Fam_size\"] = train['SibSp'] + train[\"Parch\"] + 1\ntest[\"Fam_size\"] = test['SibSp'] + test[\"Parch\"] + 1\n# Merge family members of passngers to one column (Fam_size) ","68401e69":"test.duplicated().sum()","b15e1d01":"train.duplicated().sum()","f3dc07a2":"y_train = train.Survived\nX_train = train.drop(['Survived', 'Ticket' ,'Fare_Range'] , axis=1)\nX_test = test.drop([ 'Ticket' ] , axis=1)","6d6fab12":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nXs_train = scaler.fit_transform(X_train)\nXs_test = scaler.transform(X_test)","0d3e7eed":"Xs_train.shape, Xs_test.shape , y_train.shape ","0af8b0af":"logreg = LogisticRegression()\nlogreg.fit(Xs_train, y_train)","f9a6e0b6":"Xs_test","e8c3dddd":"Y_pred=logreg.predict(Xs_test)","5e15aadc":"logreg.score(Xs_train, y_train)","accb7b39":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": Y_pred\n  })\n\nsubmission.to_csv('submission2_LG.csv', index=False)","301eb914":"lr_pipe2 = Pipeline([\n    ('sscaler2', StandardScaler()),\n    ('logreg2', LogisticRegression(penalty='l1', C=0.1, solver='liblinear'))\n])\n\n","fcbfc613":"pipe_2_params = {'sscaler2__with_mean': [True, False], \n                 'sscaler2__with_std': [True, False],\n                 'logreg2__C': [0.1, 0.2,0.3], \n                  'logreg2__solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                 'logreg2__fit_intercept': [True, False],\n                 'logreg2__penalty': ['l1', 'l2']}","024e4f20":"pipe_2_gridsearch = GridSearchCV(lr_pipe2, # What is the model we want to fit?\n                                 pipe_2_params, # What is the dictionary of hyperparameters?\n                                 cv=5, # What number of folds in CV will we use?\n                                 verbose=1)","92e031f7":"pipe_2_gridsearch.fit(X_train, y_train);\n","99d6dec4":"pipe_2_gridsearch.best_score_","13c68729":"pipe_2_gridsearch.best_estimator_\n","b99a83dd":"pre = pipe_2_gridsearch.predict(X_test)","a43d4a0f":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission1_LG_pipline.csv', index=False)","7e21a6cb":"\nlr_pipe2 = Pipeline([\n    ('sscaler2', StandardScaler()),\n    ('knn', KNeighborsClassifier())\n])\n\npipe_2_params = {'sscaler2__with_mean': [True, False], \n                 'sscaler2__with_std': [True, False],\n                 'knn__n_neighbors': [3, 5, 7, 9, 11, 20, 50, 100],\n                 'knn__weights': ['uniform', 'distance'],\n                 'knn__metric': ['manhattan', 'euclidean']}\n   \npipe_2_gridsearch = GridSearchCV(lr_pipe2, # What is the model we want to fit?\n                                 pipe_2_params, # What is the dictionary of hyperparameters?\n                                 cv=5, # What number of folds in CV will we use?\n                                 verbose=1 , scoring='roc_auc' )","a20015f6":"pipe_2_gridsearch.fit(X_train, y_train);\n","2b004990":"y_pre_GS_knn = pipe_2_gridsearch.predict(X_test)","f6eaa83f":"pipe_2_gridsearch.best_score_","4efad1fd":"pipe_2_gridsearch.best_estimator_\n","8bd8e570":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": y_pre_GS_knn\n  })\n\nsubmission.to_csv('submission2_GS_knn.csv', index=False)","bf488015":"knn = KNeighborsClassifier()","83189386":"knn.fit(Xs_train, y_train)","eae2ad21":"knn.score(Xs_train, y_train) #Better than the logistc Regression","ee125fa7":"cross_val_score(knn, Xs_train, y_train, cv=5).mean() #the score of cross valdation shows us no overfiting in our data beacuse 0.85 near to 0.81","777eed78":"pre=knn.predict(X_test)","d1332a1c":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission2_KNN.csv', index=False)","04141e39":"model=RandomForestClassifier()\n\nparam={'n_estimators':[100,200,300],\n        'max_depth':[1,3,5,7],\n        'criterion':[\"gini\"],\n        'max_features': [1,3,5],\n        \"min_samples_split\": [2,3,5]\n        }\n\nclf=GridSearchCV(estimator=model,\n                 param_grid=param,\n                 scoring=\"accuracy\",\n                 verbose=1,\n                 n_jobs=-1,\n                 cv=5)\n\nclf.fit(X_train, y_train)\nclf.best_estimator_\nclf.best_score_","98673a5f":"pre=clf.predict(X_test)\n","cfd255d4":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission2_RF_GS.csv', index=False)","8729c272":"\nlr_pipe2 = Pipeline([\n    ('sscaler2', StandardScaler()),\n    ('rf', RandomForestClassifier()\n)\n])\n\npipe_2_params = {'sscaler2__with_mean': [True, False], \n                 'sscaler2__with_std': [True, False],\n                 'rf__bootstrap': [True],\n                 'rf__max_depth': [1,3,5,7],\n                 'rf__max_features': [1, 3,5],\n                 'rf__criterion':[\"gini\"],\n                 'rf__min_samples_leaf': [3, 4, 5],\n                 'rf__min_samples_split': [2,3,5],\n                 'rf__n_estimators': [100, 200,300]}\n   \npipe_2_gridsearch = GridSearchCV(lr_pipe2, # What is the model we want to fit?\n                                 pipe_2_params, # What is the dictionary of hyperparameters?\n                                 cv=5, # What number of folds in CV will we use? \n                                 verbose=1  , scoring=\"accuracy\" , n_jobs=-1)","3d32d2e4":" pipe_2_gridsearch.fit(X_train, y_train);","50cddb64":"pipe_2_gridsearch.best_score_","bafa0ccd":"pre= pipe_2_gridsearch.predict(X_test)","a556f7c8":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission2_RF_pip_GS.csv', index=False)","2b205f97":"\nlr_pipe3 = Pipeline([\n    ('sscaler2', StandardScaler()),\n    ('dt', DecisionTreeClassifier()\n)\n])\n\npipe_3_params = {'sscaler2__with_mean': [True, False], \n                 'sscaler2__with_std': [True, False],\n                 'dt__max_depth': [10],\n                 'dt__random_state':[100],\n                 'dt__max_features': [1, 3,5],\n                 'dt__criterion':[\"gini\"],\n                 'dt__min_samples_leaf': [10],\n                 'dt__min_samples_split': [2,3,4]}\n   \npipe_3_gridsearch = GridSearchCV(lr_pipe3, # What is the model we want to fit?\n                                 pipe_3_params, # What is the dictionary of hyperparameters?\n                                 cv=5, # What number of folds in CV will we use? \n                                 verbose=1  , scoring=\"accuracy\" , n_jobs=-1)\n","93fbc6a2":" pipe_3_gridsearch.fit(X_train, y_train);","84f60f5d":"pipe_3_gridsearch.best_score_","651f83a1":"pre= pipe_3_gridsearch.predict(X_test)","21e63b3a":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission2_DT_pip_GS.csv', index=False)","290d83be":"tree = DecisionTreeClassifier(criterion='gini',max_depth=10,random_state=100,min_samples_leaf=10)\ntree.fit(X_train,y_train) \ny_predicted = tree.predict(Xs_test)\n","445b7671":"tree.score(Xs_train, y_train)","d74bbd3e":"pre= tree.predict(X_test)","af36ac35":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": pre\n  })\n\nsubmission.to_csv('submission2_DT.csv', index=False)","83f67752":"from sklearn.svm import SVC\nSVM = SVC()\nSVM.fit(Xs_train, y_train)\nSVM_predictions = SVM.predict(Xs_test)\nSVM.score(Xs_train, y_train)","bf64fb2b":"submission = pd.DataFrame({\n   \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\": SVM_predictions\n  })\n\nsubmission.to_csv('submission2_SVM.csv', index=False)","0929d159":"\nscores ={'LR_pip': 0.772,'LR': 0.770,'Knn_pip': 0.779, 'Knn': 0.669, 'RF_GS': 0.787,\n         'RF_pip_GS': 0.775, 'DT_pip_GS': 0.760, 'DT':0.779,\n         'SVM': 0.779}","5f137c11":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig  = plt.figure(figsize=(24, 12))\n\nax=sns.pointplot(x=list(scores.keys()), y=[score for score in scores.values()], markers=['o'], linestyles=['-'])\n# for i, score in enumerate(scores.values()):\n#     ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Various Models', size=20)\n\nplt.show();\n","482ff61b":"![image.png](attachment:image.png)","80932756":"- \n### Step B: Dummies the columns: \n\n---","3de79266":"- **Choosing the target and the predectors for the models:**","10218e66":"## Variable Identification\n\n**Question: What features do you think is going to be important for Titanic ?**\n\n### **Answer**: \n\n- The Predector : PassengerId, Survived,Pclass, Name, Sex, Age, SibSp, Parch, Ticket,Fare, Cabin, Embarked\n- The Target : Survived\n                      \n\n|Feature|Type|Dataset|Description|\n|---|---|---|---|\n|PassengerId|int| Train,Test|ID of Passenger| \n|Survived|int| Train,Test|Survival status 0 = No,1 = Yes|\n|Pclass|int| Train,Test|Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd|\n|Name|str| Train,Test|Name of Passenger|\n|Age|float| Train,Test|Age of Passenger|\n|SibSp|int| Train,Test|Number of siblings & spouses aboard the Titanic|\n|parch|int| Train,Test|Number of parents & children aboard the Titanic|\n|Ticket|str| Train,Test|Ticket number|\n|Fare|float| Train,Test|Passenger fare|\n|Cabin|int| Train,Test|Cabin number|\n|Embarked|str| Train,Test|Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton|\n\n\n\n\n\n - **Check the Statistics:**","4e509682":"- The heatmap show us the corrlation between features.","180fcc86":"-------------------------------\n## Random  Forest using Gride Search\n-----------------------------","a1dff775":"\n---------\n\n\n- **We cleaning Name from noise then apply dummies:**","7257f0e2":"## K Nearest Neighbors (KNN) Classifier Using Pipeline and Grid Search \n\n----------------------","75669de6":"---------------------------\n\n# Conclusion and Recommendations\n---------------------------\n\nOur findings in this study is that 61% of passengers not-survived, only 38% of them survived. We find that most of survived passengers were women although, the majority of passengers were men. the survival rate in titanic ship for women and men was high when age between 20 - 50. We find that most of passengers embarked from Southampton port and most of them from class 3. Our target was predicting wethier the passenger survived or not. \n\n------------------------------","82f7cdbc":"- The above plot shows the fare range that paid by the passengers and it is clear that the rate of surviving increased when the paid fare was higher.","b9fda46c":"- we can notice that there is many null values appear in two datasets in Cabin, Age colmun\n\n-------------------------------------------------","37f3b0dd":"--------------------","7fab5a4f":"- This plot shows us the distrbution of survived ","0fcbf2dc":"## Random Forest using pipline and Grid Search","b6531afd":"![image.png](attachment:image.png)","28d03a0f":"- The plot above show us a comparsion between scores of the implemented models in our project. The model that have the highest score is Random Forest Using Gride Search  (RF_GS) and it perform better than the baseline model. We believe our model generalized new data beacuse it give us 0.78 on new, unseen data that means it predict correctly most of the data.\n\n### The Approvid  model\n\n![image.png](attachment:image.png)\n","ceaa974c":"- The plot shows us, there is an outlier in Fare, Age, Parch, SibSp. ","501a2501":"--------------------------\n## Decision Tree Classifier\n----------------------------","454d315b":"- The above graph describe the number of survived in each class and which ports they embarked from and we notice that most of passengers embarked from port S and they from class 3, where the highest number of survival passengers was from class 1.","63d783f1":"- **check duplicate values in train, test datasets:**","6e0b6b65":"![image.png](attachment:image.png)","9e89e4ac":"- The above plot shows that ratio of survived female more than male ratio.","8285f325":"## Step 3: Data Exploration\n\n---","4c2f6ae9":"_______________________________________________________","93130f8f":"![image.png](attachment:image.png)","b0cd5401":"## Decision Tree Classifier using pipline and graid search \n---------------------------------------------","91071018":"![image.png](attachment:image.png)","8e37a5dc":"- The heatmap shows us the cleaned data ","1f5f016b":"\n\n# Introduction \n\n## Datasets Description \n\nThe dataset comes from Kaggle competition named [\"The Titanic - machine learning from disaster \u201d](https:\/\/www.kaggle.com\/c\/titanic\/data) which was about Titanic that sank in 1912 after hitting an iceberg. The dataset contains two similar datasets that include passenger information like name, age, gender, socio-economic class, the port they embarked from and whether they have relatives with them or not, etc. \nOne dataset is titled `train.csv` and the other is titled `test.csv`:\n- `train.csv`Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\n- The `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger.\n\n## Executive Summary\n\nIn this project after investigating Titanic dataset and perform visualization approches and applying machine learning models, we found out Random Forest Using Grid Search give us the accurate preduction with score 78%.\n\n\n## Step1:  Problem Statment\nThe project aims to use machine learning models in predicating whether the passengers in the titanic ship survived or not. The problem is a supervised learning problem and since we are tried a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data, it is a classification problem.\n","d17d8381":"- The plot shows us the relationship between Fare and Age of passengers ","18f43539":"## Step 4: Data Preprocessing \n\n---","ef74ddc6":"- **Outliers detection:**","eb5c89c3":"![image.png](attachment:image.png)","a27318ea":" \n\n\n## Importing the Libraries","1276d12a":"## Step 2: Data Wrangling\n\n---","12e65190":"The plot shows us Age distrbution, has positive skewness. we can notice that the majority of passengers were aged from 15 to 40 ","499135ec":"- The plot shows us distrbution for Fare per Sex, we can notice both distrbutions has positive skewness.","b9cda5c4":"![image.png](attachment:image.png)","48acc78d":"-------------------\n## Baseline model : Logistic Regression","31c1a2e1":"-------------------------\n## Step 5:  Modeling\n \n- since our problem is classifiaction problem, we choose the following machine learning models: Logistic Regression, SVM, Random Forest, KNN, beacuse these models are classifiaction models.","f8745baf":"![image.png](attachment:image.png)","cb6004f9":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\">\n<li><span><a href=\"#Introduction\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\">\n\n<li><span><a href=\"#Datasets-Description\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Dataset description<\/a><\/span>\n    \n<li><span><a href=\"#Executive-Summary\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Executive Summary<\/a><\/span><\/li>\n<li><span><a href=\"#Step1:--Problem-Statment\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Problem Statment<\/a><\/span><\/li><\/ul><\/li>\n\n<li><span><a href=\"##Importing-the-Libraries\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Importing the Libraries<\/a><\/span>\n\n<li><span><a href=\"#Step-2:-Data-Wrangling\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Wrangling<\/a><\/span>\n<li><span><a href=\"#Step-3:-Data-Exploration\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Exploration<\/a><\/span><ul class=\"toc-item\">\n<li><span><a href=\"#Variable Identification\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Variable Identification<\/a><\/span><\/li>\n    \n<li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Exploratory Data Analysis (EDA)<\/a><\/span>    <\/ul><\/li>\n    \n<li><span><a href=\"#Step-4:-Data-Preprocessing\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span>  <ul class=\"toc-item\">  \n\n<li><span><a href=\"#Step-A:-Dealing-with-Missing-Values:\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Dealing with Missing Values<\/a><\/span><\/li>\n\n<\/ul><\/li>\n\n<li><span><a href=\"#Step-5:--Modeling\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Prediction Models<\/a><\/span><ul class=\"toc-item\">\n<li><span><a href=\"#Baseline-model-:-Logistic-Regression\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Baseline-model: Logistic Regression<\/a><\/span>\n<li><span><a href=\"#Logistic-Regression-using-Pipeline-and-Grid-Search\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Logistic Regression using Pipeline and Grid Search<\/a><\/span>   \n<li><span><a href=\"#K-Nearest-Neighbors-(KNN)-Classifier-Using-Pipeline-and-Grid-Search\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>K Nearest Neighbors Classifier Using Pipeline and Grid Search<\/a><\/span>\n<li><span><a href=\"#K--Nearest--Neighbors-(KNN)\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>K-Nearest Neighbors<\/a><\/span>\n<li><span><a href=\"#Random--Forest-using-Gride-Search\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Random Forest Using Grid Search<\/a><\/span><\/li>\n<li><span><a href=\"#Random-Forest-using-pipline-and-Grid-Search\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Random Forest Using Pipeline and Grid Search<\/a><\/span><\/li>\n\n<li><span><a href=\"#Decision-Tree-Classifier-using-pipline-and-graid-search\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Decision Tree Classifier Using Pipeline and Grid Search<\/a><\/span><\/li>\n<li><span><a href=\"#Decision-Tree-Classifier\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Decision Tree Classifier<\/a><\/span><\/li>\n\n<li><span><a href=\"#Support-Vector-Machine-(SVM)\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Support Vector Machine<\/a><\/span><\/li>\n<\/ul><\/li>  \n\n<li><span><a href=\"#Conclusion-and-Recommendations\" ><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Conclusion and Recommendations<\/a><\/span><\/li><\/div>","ff989566":"- This graph shows the age range of passengers based on their sex  and  whether they survived or not. we can notice the survival rate for female and male is high when age between 20 - 50, and it is noticeable that the rate decreased in male as age increased.","bea0dfbe":"-------\n## Logistic Regression using Pipeline and Grid Search\n\n-------","c9f3388a":"### Step A: Dealing with Missing Values: \n","44f7842b":"----------------------------------\n## Support Vector Machine (SVM)\n-----------------------","99221442":"![image.png](attachment:image.png)","1b9ccf71":"- The plot shows us the correlation between Passengers Class, Age and Fare per Survival Rate ","afc50e87":"<img src=\"http:\/\/imgur.com\/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n\n# Project 2 : Titanic - Machine Learning from Disaster\n#### By: Samaher Alharbi  ,  Hanan Almohammadi, Nesreen Alqahtani","5375c7dc":"Dealing with Missing Values of featurs:","2e68d55d":"---------------------------------\n## K- Nearest  Neighbors (KNN)\n-------------------------------","4c9a748a":" - The above graph is interactive plot that shows how Fare grouped by different features based on the selected feature from drop down menu.","5d21f8d3":"## Exploratory Data Analysis (EDA)"}}