{"cell_type":{"a11c4a89":"code","7b667ada":"code","af2005e8":"code","cb3607be":"code","88abefa7":"code","e278f658":"code","18c306ff":"code","e9304b55":"code","03171544":"code","a3f771e6":"code","a5563a5b":"code","9d9a03e9":"code","400c197f":"code","713e3793":"code","b1dfeb09":"code","86289ee4":"code","6bddc4f3":"code","f52b8c98":"code","66af942d":"code","cc0c5ae4":"code","751f0600":"code","01edca41":"code","44437876":"code","5403ba9b":"code","ffc546d8":"code","2fe66be9":"code","7955e27a":"code","24a838f6":"code","f6edf7b6":"code","e0b24bc7":"code","29ce86b8":"code","0c6426c4":"code","38ae2d11":"code","db7e9dad":"code","c3e25efa":"code","e204fa5b":"code","32ecdd87":"code","a8996739":"code","81ed8025":"code","ccde3891":"code","2c32e778":"code","5767c08b":"code","ba1c9da8":"code","11924789":"code","cd56e2b2":"code","18db6418":"code","85b26c9b":"code","a19efc54":"code","00f35fb5":"markdown","b1d34fac":"markdown","c748dab1":"markdown","8083f16e":"markdown","65069879":"markdown","9bfc31f9":"markdown","eccee7b6":"markdown","8140b12f":"markdown","036207e8":"markdown","1c228ac2":"markdown","50811707":"markdown","39a484e5":"markdown","e087c6b5":"markdown","791311be":"markdown","d4250f5e":"markdown","0d9a34dd":"markdown","36e9097f":"markdown","4f96ecbd":"markdown"},"source":{"a11c4a89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b667ada":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report","af2005e8":"#I called our dataset and visualized it with the data.head() command.\ndata=pd.read_csv(\"\/kaggle\/input\/breast-cancer\/data.csv\",sep=\",\")\ndata.head()\n","cb3607be":"#569 rows and 33 columns.\ndata.shape","88abefa7":"#I decided whether it is true or false by looking at the types in the data set.\ndata.info()","e278f658":"#Statistical data brought.\ndata.describe()","18c306ff":"#Looked for empty data, lost data, missing value.\ndata.isna().sum()","e9304b55":"#I looked at the previous line for missing value and found it.\n#A column name Unnamed: 32, which contain all null values. So I can drop it.\ndata.drop(['Unnamed: 32'],axis=1,inplace= True)\n#The ID column would not help us contributing to predict about the cancer. I might as well drop it.\ndata=data.drop('id',axis=1)\ndata.head()\n","03171544":"#Now there are 31 columns.\ndata.shape","a3f771e6":"# I can check the column has been droped\ndata.columns\n# This gives the column name which is present in our data no Unnamed: 32 and id are not now there. ","a5563a5b":"le = LabelEncoder()\ndata['diagnosis']=le.fit_transform(data['diagnosis'])","9d9a03e9":"#I now see 0 and 1 instead of \"M\" and \"B\"\ndata","400c197f":"#After checking our data, we can move on to visualization.\n#I looked at the numbers of malign and benign patients.\ndata[\"diagnosis\"].value_counts()\n","713e3793":"plt.figure(figsize = (10,5))\nplt.subplot(1, 2, 1)\nsns.countplot(x=\"diagnosis\", data=data, palette='Purples')\nplt.title(\"Counts of Diagnosis\")\n\n\nplt.subplot(1, 2, 2)\nlabel=[\"0\",\"1\"]\ncolor=[\"#b3b3cc\",\"#666699\"]\ndata[\"diagnosis\"].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',shadow=True,labels=label,colors=color)\nplt.show() \n\n\n\n","b1dfeb09":"#Feature Importance\n\nX = data.drop(['diagnosis'], axis = 1)\nY = data['diagnosis']\n\nmodel = ExtraTreesClassifier(random_state=0)\nmodel.fit(X,Y)\nprint(model.feature_importances_) \n#plot graph of feature importances for better visualization\ncolor=\"#666699\"\nplt.figure(figsize = (8,10))\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='bar',color=color)\nplt.title('Feature importances for \\nBreast Cancer ')\nplt.show()","86289ee4":"#The measure of the relationship between them.\ndata.corr()","6bddc4f3":"# Heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),annot = True, cmap = 'Purples');\nplt.title(\"Correlation Between Features\")","f52b8c98":"#I can say that as the area_mean and radius_mean values increase their is a higher chance being diagnosed with Cancer.\n#I can say that as the area_mean and perimeter_mean values increase their is a higher chance being diagnosed with Cancer.\nplt.figure(figsize = (10,5))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x = 'area_mean', y = 'radius_mean', hue = 'diagnosis', data = data)\nplt.show()\nplt.figure(figsize = (10,5))\nplt.subplot(1, 2, 2)\nsns.scatterplot(x = 'area_mean', y = 'perimeter_mean', hue = 'diagnosis', data = data)\nplt.show()\n","66af942d":"#Now split our data into two as train and test.\nX = data.drop(['diagnosis'], axis = 1)\nY = data['diagnosis']\n","cc0c5ae4":"# Splitting the data into training and test sets.\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)\n","751f0600":"#applying feature scaling may improve accuracy.\n#Standardizing training data.\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","01edca41":"X_train","44437876":"#log reg.\nlog_reg = LogisticRegression(random_state=1, max_iter=100).fit(X_train, Y_train)\npredict_log=log_reg.predict(X_test)\npredict_log","5403ba9b":"acc_logreg = accuracy_score(Y_test, predict_log)\nprint(\"Logistic Regression Classifier Accuracy:\",metrics.accuracy_score(Y_test, predict_log))","ffc546d8":"ConfMatrix = confusion_matrix(Y_test,log_reg.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Purples\", fmt=\"d\", \n            xticklabels = ['0', '1'], yticklabels = ['0', '1'])\n\nplt.title(\"Confusion Matrix\");","2fe66be9":"print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict_log))\nprint('\\n')\nprint(classification_report(Y_test, predict_log))","7955e27a":"#randomforest\nforest=RandomForestClassifier(n_estimators=20,criterion=\"entropy\",random_state=0)\nforest.fit(X_train,Y_train)","24a838f6":"#predict for forest classifier.\nforest_pred=forest.predict(X_test)\nforest_pred","f6edf7b6":"\nacc_forest = accuracy_score(Y_test, forest_pred)\nprint(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(Y_test, forest_pred))","e0b24bc7":"ConfMatrix = confusion_matrix(Y_test,forest.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Purples\", fmt=\"d\", \n            xticklabels = ['0', '1'], yticklabels = ['0', '1'])\n\nplt.title(\"Confusion Matrix\");","29ce86b8":"print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, forest_pred))\nprint('\\n')\nprint(classification_report(Y_test, forest_pred))","0c6426c4":"#knn\nKNN=KNeighborsClassifier(n_neighbors=3)\nKNN.fit(X_train,Y_train)","38ae2d11":"#predict for KNN.\nknn_pred=KNN.predict(X_test)","db7e9dad":"acc_knn = accuracy_score(Y_test, knn_pred)\n\nprint(\"KNN Classifier Accuracy:\",metrics.accuracy_score(Y_test, knn_pred))","c3e25efa":"ConfMatrix = confusion_matrix(Y_test,KNN.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Purples\", fmt=\"d\", \n            xticklabels = ['0', '1'], yticklabels = ['0', '1'])\n\nplt.title(\"Confusion Matrix\");","e204fa5b":"print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, knn_pred))\nprint('\\n')\nprint(classification_report(Y_test, knn_pred))","32ecdd87":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,Y_train)\n\n\n","a8996739":"nb_pred=nb.predict(X_test)\nnb_pred","81ed8025":"acc_nb = accuracy_score(Y_test, nb_pred)\nprint(\"NB Classifier Accuracy:\",metrics.accuracy_score(Y_test, nb_pred))","ccde3891":"ConfMatrix = confusion_matrix(Y_test,nb.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Purples\", fmt=\"d\", \n            xticklabels = ['0', '1'], yticklabels = ['0', '1'])\n\nplt.title(\"Confusion Matrix\");","2c32e778":"print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, nb_pred))\nprint('\\n')\nprint(classification_report(Y_test, nb_pred))","5767c08b":"xgb=XGBClassifier(random_state=0,booster=\"gbtree\")\nxgb.fit(X_train,Y_train)","ba1c9da8":"xgb_pred=xgb.predict(X_test)\nxgb_pred","11924789":"acc_xgb = accuracy_score(Y_test, xgb_pred)\nprint(\"XGB Classifier Accuracy:\",metrics.accuracy_score(Y_test, xgb_pred))","cd56e2b2":"ConfMatrix = confusion_matrix(Y_test,xgb.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Purples\", fmt=\"d\", \n            xticklabels = ['0', '1'], yticklabels = ['0', '1'])\n\nplt.title(\"Confusion Matrix\");","18db6418":"print(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, xgb_pred))\nprint('\\n')\nprint(classification_report(Y_test, xgb_pred))","85b26c9b":"model = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'Random Forest Classifier', \n             'Naive Bayes', 'XgBoost'],\n    'Accuracy': [acc_logreg, acc_knn, acc_forest,acc_nb,acc_xgb]\n})\n\nmodel.sort_values(by = 'Accuracy', ascending = False)","a19efc54":" sns.barplot(x = model['Accuracy'], y = model['Model'], palette='Purples');","00f35fb5":"## Classification Task: KNN","b1d34fac":"#### We look at how many benign, how many malign tumors there are\n\n#### B (0) --> Benign (357)\n\n#### M (1) --> Malignant (212)","c748dab1":"## Data Preprocessing","8083f16e":"#### We convert our diagnosis property to int values.\n\n#### M --> 1\n\n#### B --> 0","65069879":"#### I saw concave points worst affect more diagnosis.","9bfc31f9":"## Classification Task: XGBoost","eccee7b6":"## Data Visualization","8140b12f":"## Compare Performance","036207e8":"## Classification Task: Logistic Regression","1c228ac2":"## Editing Data","50811707":"## Classification Task: Random Forest","39a484e5":"# Breast Cancer Analysis\n\n","e087c6b5":"## Classification Task: Naive Bayes","791311be":"#### The most efficient analysis method is logistic regression.\n#### I hope you like it. Please upvote the notebook. Thanks :))","d4250f5e":"## Libraries","0d9a34dd":"## Read Data","36e9097f":"## Correlation Analysis","4f96ecbd":"## Train-Test Split"}}