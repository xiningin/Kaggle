{"cell_type":{"34927d1a":"code","b116d37f":"code","087e552b":"code","a9ef1725":"code","b210d2b1":"code","63083f19":"code","a4b79050":"code","1a34ef4e":"code","f7ee876c":"code","53473714":"code","24a0123e":"code","51f2910d":"code","55c555cb":"code","3c282c5e":"code","9f346e6d":"code","169872a1":"code","4d9b283c":"code","243d67c0":"code","2634b2c0":"code","25f04305":"code","e467d496":"code","23b4904c":"code","363142fa":"code","ba246c94":"code","c0eaebd6":"code","362e9e92":"code","b5f54889":"code","92c7007e":"code","fdcadffb":"code","a319e4dc":"code","906bb529":"code","09e55fe8":"code","f76f19a8":"code","f25845e3":"code","e36c9ffa":"code","dc4c2304":"code","0c8ada7f":"code","e77f8228":"code","a158a14e":"code","e7fd1f58":"code","55eca011":"code","feff85a1":"code","31c49337":"code","fd4fd7cd":"code","09c19ec7":"code","9084ee4b":"code","da4d529b":"code","632d1364":"code","4ed77a82":"code","bb6e9934":"code","d689e4f7":"code","4805026b":"code","255eb82b":"code","87b1c95b":"code","c19f36e0":"code","160177dd":"code","1ba58b3a":"code","f810be5f":"code","969ceefa":"markdown","48249a46":"markdown","1ece221c":"markdown","772e90aa":"markdown","d40c40fd":"markdown","587fbeef":"markdown","953db09d":"markdown","ac2f2760":"markdown","aa1cf390":"markdown","798a5803":"markdown","28870ed4":"markdown","fc792e54":"markdown","8d587fd2":"markdown","c5b49199":"markdown","7ca8278a":"markdown","ef37e7fc":"markdown","650987ed":"markdown","eea7a40c":"markdown","68d8c9f3":"markdown","ba1cd5c6":"markdown","d3c75d65":"markdown","642cfd90":"markdown","3c0a52ae":"markdown","9993af2d":"markdown","d95ee414":"markdown","dde03e0f":"markdown","083afc66":"markdown","d6dcc0c8":"markdown","b57a0816":"markdown","f743be5d":"markdown","e6f2b697":"markdown","11332023":"markdown","dd69074d":"markdown","7de965be":"markdown","17838e9b":"markdown","ac3c0ea6":"markdown","b16710f3":"markdown","920b463f":"markdown","f4648320":"markdown","4ad10ce3":"markdown","44344d09":"markdown","c4553ae0":"markdown","2e397cd3":"markdown","cabbae2c":"markdown","f4bb9255":"markdown","32b84e7f":"markdown","b4000e35":"markdown","bbb1bae7":"markdown","b74f1365":"markdown","c28a1372":"markdown","79e5570f":"markdown","5cf8a503":"markdown","c6756a2c":"markdown","10c11160":"markdown","8de1f127":"markdown"},"source":{"34927d1a":"import numpy as np \nimport pandas as pd \nimport json\nfrom pandas.io.json import json_normalize\nimport datetime as dt\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport os\nIS_LOCAL=False\nif(IS_LOCAL):\n    PATH=\"..\/google-analytics-customer-revenue-prediction\/input\/\"    \nelse:\n    PATH=\"..\/input\/\"","b116d37f":"print(os.listdir(PATH))","087e552b":"onerow = pd.read_csv(PATH+'train_v2.csv',nrows=1)\npd.concat([onerow.T, onerow.dtypes.T], axis=1, keys=['Example', 'Type'])","a9ef1725":"#the columns that will be parsed to extract the fields from the jsons\ncols_to_parse = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\ndef read_parse_dataframe(file_name):\n    #full path for the data file\n    path = PATH + file_name\n    #read the data file, convert the columns in the list of columns to parse using json loader,\n    #convert the `fullVisitorId` field as a string\n    data_df = pd.read_csv(path, \n        converters={column: json.loads for column in cols_to_parse}, \n        dtype={'fullVisitorId': 'str'}, nrows=500000)\n    #parse the json-type columns\n    for col in cols_to_parse:\n        #each column became a dataset, with the columns the fields of the Json type object\n        json_col_df = json_normalize(data_df[col])\n        json_col_df.columns = [f\"{col}_{sub_col}\" for sub_col in json_col_df.columns]\n        #we drop the object column processed and we add the columns created from the json fields\n        data_df = data_df.drop(col, axis=1).merge(json_col_df, right_index=True, left_index=True)\n    return data_df","b210d2b1":"%%time\ntrain_df = read_parse_dataframe('train_v2.csv')","63083f19":"print(\"Train set:\",train_df.shape[0],\" rows, \", train_df.shape[1],\"columns\")","a4b79050":"train_df.head()","1a34ef4e":"print(dt.datetime.fromtimestamp(train_df['visitId'][0]).isoformat())","f7ee876c":"def process_date_time(data_df):\n    data_df['date'] = data_df['date'].astype(str)\n    data_df[\"date\"] = data_df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n    data_df[\"date\"] = pd.to_datetime(data_df[\"date\"])   \n    data_df[\"year\"] = data_df['date'].dt.year\n    data_df[\"month\"] = data_df['date'].dt.month\n    data_df[\"day\"] = data_df['date'].dt.day\n    data_df[\"weekday\"] = data_df['date'].dt.weekday\n    return data_df","53473714":"train_df = process_date_time(train_df)","24a0123e":"print(\"Train set:\",train_df.shape[0],\" rows, \", train_df.shape[1],\"columns\")","51f2910d":"%%time\ntest_df = read_parse_dataframe('test_v2.csv')\ntest_df = process_date_time(test_df)","55c555cb":"print(\"Test set:\",test_df.shape[0],\" rows, \", test_df.shape[1],\"columns\")","3c282c5e":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return df.loc[~(df['Total']==0)]\nmissing_data(train_df)","9f346e6d":"missing_data(test_df)","169872a1":"def get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","4d9b283c":"def draw_trace_bar(data, title, xlab, ylab,color='Blue'):\n    trace = go.Bar(\n            x = data.head(30)['index'],\n            y = data.head(30)['Number'],\n            marker=dict(color=color),\n            text=data.head(30)['index']\n        )\n    data = [trace]\n\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=15,\n                          tickfont=dict(\n                            size=9,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')","243d67c0":"draw_trace_bar(get_categories(train_df,'channelGrouping'), \"Channel grouping\", \"Channel grouping\", \"Number\", \"Lightblue\")","2634b2c0":"def get_feature_distribution(data, feature):\n    # Get the count for each label\n    label_counts = data[feature].value_counts()\n    # Get total number of samples\n    total_samples = len(data)\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = label_counts.index[i]\n        count = label_counts.values[i]\n        percent = int((count \/ total_samples) * 10000)\/100\n        print(\"{:<30s}:   {} or {}%\".format(label, count, percent))\n\nget_feature_distribution(train_df,'channelGrouping')","25f04305":"get_feature_distribution(train_df,'socialEngagementType')","e467d496":"device_cols = train_df.columns[train_df.columns.str.contains('device')].T.tolist()\nprint(\"There are \",len(device_cols),\"columns with device attributes:\\n\",device_cols)","23b4904c":"const_device_cols = []\nfor i, col in enumerate(device_cols):\n    if(len(train_df[col].value_counts())==1):\n        const_device_cols.append(col)\nprint(\"There are \",len(const_device_cols),\"columns with unique value for device attributes:\\n\",const_device_cols)","363142fa":"def show_features(data,features):\n    color = [\"red\", \"blue\", \"green\", \"magenta\", \"yellow\", \"lightblue\", \"gold\", \"tomato\", \"grey\",\n            \"lightgreen\", \"red\", \"blue\", \"green\", \"magenta\", \"yellow\", \"brown\", \"grey\", \"tomato\"]\n    for i,feature in enumerate(features):\n        draw_trace_bar(get_categories(train_df,feature), \n                    feature, feature, \"Number\", color[i])","ba246c94":"var_cols = [item for item in device_cols if item not in const_device_cols]\nshow_features(train_df,var_cols)","c0eaebd6":"def plot_heatmap_count(data_df, feature1, feature2, feature3='channelGrouping', color=\"Greens\", title=\"\", height=16, width=16):\n    tmp = data_df.groupby([feature1, feature2])[feature3].count()\n    df1 = tmp.reset_index()\n    matrix = df1.pivot(feature1, feature2, feature3)\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(width,height))\n    sns.heatmap(matrix, \n        xticklabels=matrix.columns,\n        yticklabels=matrix.index,ax=ax1,linewidths=.1,linecolor='black',annot=True,cmap=color)\n    plt.title(title, fontsize=14)\n    plt.show()\n    \ndef plot_heatmap_sum(data_df, feature1, feature2, feature3='channelGrouping', color=\"Greens\", title=\"\", height=16, width=16):\n    tmp = data_df.groupby([feature1, feature2])[feature3].sum()\n    df1 = tmp.reset_index()\n    matrix = df1.pivot(feature1, feature2, feature3)\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(width,height))\n    sns.heatmap(matrix, \n        xticklabels=matrix.columns,\n        yticklabels=matrix.index,ax=ax1,linewidths=.1,linecolor='black',annot=True,cmap=color)\n    plt.title(title, fontsize=14)\n    plt.show()","362e9e92":"plot_heatmap_count(train_df, 'device_browser', 'device_operatingSystem',color='Reds',title=\"Device Browsers vs. Device OS\")","b5f54889":"plot_heatmap_count(train_df, 'device_browser','device_deviceCategory', color='Blues',title=\"Device Browser vs. Device Category\", height=12, width=8)","92c7007e":"plot_heatmap_count(train_df, 'device_deviceCategory', 'device_isMobile', color='viridis',title=\"Device is mobile vs. Device Category\", width=6, height=4)","fdcadffb":"geo_cols = train_df.columns[train_df.columns.str.contains('geoNetwork')].T.tolist()\nprint(\"There are \",len(geo_cols),\"columns with geoNetwork attributes:\\n\",geo_cols)","a319e4dc":"const_geo_cols = []\nfor i, col in enumerate(geo_cols):\n    if(len(train_df[col].value_counts())==1):\n        const_geo_cols.append(col)\nprint(\"There are \",len(const_geo_cols),\"columns with unique value for geoNetwork attributes:\\n\",const_geo_cols)","906bb529":"var_cols = [item for item in geo_cols if item not in const_geo_cols]\nshow_features(train_df,var_cols)","09e55fe8":"tmp = train_df['geoNetwork_country'].value_counts()\ncountry_visits = pd.DataFrame(data={'geoNetwork_country': tmp.values}, index=tmp.index).reset_index()\ncountry_visits.columns = ['Country', 'Visits']","f76f19a8":"def plot_country_map(data, location, z, legend, title, colormap='Rainbow'):\n    data = dict(type = 'choropleth', \n                colorscale = colormap,\n                autocolorscale = False,\n                reversescale = False,\n               locations = data[location],\n               locationmode = 'country names',\n               z = data[z], \n               text = data[z],\n               colorbar = {'title':legend})\n    layout = dict(title = title, \n                 geo = dict(showframe = False, \n                         projection = {'type': 'natural earth'}))\n    choromap = go.Figure(data = [data], layout=layout)\n    iplot(choromap)","f25845e3":"plot_country_map(country_visits, 'Country', 'Visits', 'Visits', 'Visits per country')","e36c9ffa":"tot_cols = train_df.columns[train_df.columns.str.contains('totals')].T.tolist()\nprint(\"There are \",len(tot_cols),\"columns with Totals attributes:\\n\",tot_cols)","dc4c2304":"const_tot_cols = []\nfor i, col in enumerate(tot_cols):\n    if(len(train_df[col].value_counts())==1):\n        const_tot_cols.append(col)\nprint(\"There are \",len(const_tot_cols),\"columns with unique value for Totals attributes:\\n\",const_tot_cols)","0c8ada7f":"var_cols = [item for item in tot_cols if item not in const_tot_cols]\nshow_features(train_df,var_cols,12,4)","e77f8228":"train_df['totals_transactionRevenue'] = pd.to_numeric(train_df['totals_transactionRevenue'])\ndf = train_df[train_df['totals_transactionRevenue'] > 0]['totals_transactionRevenue']\nf, ax = plt.subplots(1,1, figsize=(16,4))\nplt.title(\"Distribution of totals: transaction revenue\")\nsns.kdeplot(df, color=\"green\")\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.ylabel('Density plot', fontsize=12)\nplt.xlabel('Transaction revenue', fontsize=12)\nlocs, labels = plt.xticks()\nplt.show()","a158a14e":"plt.figure(figsize=(12,6))\nsns.distplot(np.log1p(df),color=\"darkgreen\",bins=50)\nplt.xlabel(\"Log(total transaction revenue)\");\nplt.title(\"Logarithmic distribution of total transaction revenue (non-zeros)\");","e7fd1f58":"# select the visits with non-zero transaction revenue\nnon_zero = train_df[train_df['totals_transactionRevenue']>0]\ntmp = non_zero['geoNetwork_country'].value_counts()\ncountry_visits = pd.DataFrame(data={'geoNetwork_country': tmp.values}, index=tmp.index).reset_index()\ncountry_visits.columns = ['Country', 'Visits']","55eca011":"plot_country_map(country_visits, 'Country', 'Visits', 'Visits', 'Visits with non zero transactions')","feff85a1":"# select the visits with non-zero transaction revenue and calculate the sums\nnon_zero = train_df[train_df['totals_transactionRevenue']>0]\ntmp = non_zero.groupby(['geoNetwork_country'])['totals_transactionRevenue'].sum()\ncountry_total = pd.DataFrame(data={'total': tmp.values}, index=tmp.index).reset_index()\ncountry_total.columns = ['Country', 'Total']\ncountry_total['Total']  = np.log1p(country_total['Total'])","31c49337":"plot_country_map(country_total, 'Country', 'Total', 'Total(log)', 'Total revenues per country (log scale)')","fd4fd7cd":"non_zero[['fullVisitorId','visitNumber', 'totals_transactionRevenue', 'channelGrouping']].sort_values(['totals_transactionRevenue', 'fullVisitorId'], ascending=[0,0]).head(10)","09c19ec7":"non_zero[['fullVisitorId','visitNumber', 'totals_transactionRevenue', 'channelGrouping']].sort_values(['visitNumber', 'totals_transactionRevenue'], ascending=[0,0]).head(10)","9084ee4b":"# select the visits with non-zero transaction revenue and calculate the sums\nnon_zero = train_df[train_df['totals_transactionRevenue']>0]\ntmp = non_zero.groupby(['channelGrouping', 'geoNetwork_subContinent'])['totals_transactionRevenue'].sum()\nchannel_total = pd.DataFrame(data={'total': tmp.values}, index=tmp.index).reset_index()\nchannel_total.columns = ['Channel', 'Subcontinent', 'Total']","da4d529b":"plot_heatmap_sum(non_zero, 'geoNetwork_subContinent','channelGrouping',  'totals_transactionRevenue','rainbow',\"Total transactions per channel and subcontinent\", width=16, height=6)","632d1364":"ts_cols = train_df.columns[train_df.columns.str.contains('trafficSource')].T.tolist()\nprint(\"There are \",len(ts_cols),\"columns with Totals attributes:\\n\",ts_cols)","4ed77a82":"const_ts_cols = []\nfor i, col in enumerate(ts_cols):\n    if(len(train_df[col].value_counts())==1):\n        const_ts_cols.append(col)\nprint(\"There are \",len(const_ts_cols),\"columns with unique value for Traffic Source attributes:\\n\",const_ts_cols)","bb6e9934":"var_cols = [item for item in ts_cols if item not in const_ts_cols]\nshow_features(train_df,var_cols)","d689e4f7":"var_cols = ['year','month','day','weekday']\nshow_features(train_df,var_cols)","4805026b":"def plot_scatter_data(data, xtitle, ytitle, title, color='blue'):\n    trace = go.Scatter(\n        x = data.index,\n        y = data.values,\n        name=ytitle,\n        marker=dict(\n            color=color,\n        ),\n        mode='lines+markers'\n    )\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xtitle), yaxis = dict(title = ytitle),\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='lines')","255eb82b":"count_all = train_df.groupby('date')['totals_transactionRevenue'].agg(['size'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Date', 'Total','Total count of visits (including zero transactions)','green')","87b1c95b":"count_nonzero = train_df.groupby('date')['totals_transactionRevenue'].agg(['count'])\ncount_nonzero.columns = [\"Total\"]\ncount_nonzero = count_nonzero.sort_index()\nplot_scatter_data(count_nonzero['Total'],'Date', 'Total','Total non-zero transaction visits','darkblue')","c19f36e0":"total_nonzero = train_df.groupby('date')['totals_transactionRevenue'].agg(['sum'])\ntotal_nonzero.columns = [\"Total\"]\ntotal_nonzero = total_nonzero.sort_index()\nplot_scatter_data(total_nonzero['Total'],'Date', 'Total','Total non-zero transaction amounts','red')","160177dd":"channels = list(train_df['channelGrouping'].unique())\ndata = []\nfor channel in channels:\n    subset = train_df[train_df['channelGrouping']==channel]\n    subset = subset.groupby('date')['totals_transactionRevenue'].agg(['sum'])\n    subset.columns = [\"Total\"]\n    subset = subset.sort_index()\n    trace = go.Scatter(\n        x = subset['Total'].index,\n        y = subset['Total'].values,\n        name=channel,\n        mode='lines'\n    )\n    data.append(trace)\nlayout= go.Layout(\n    title= 'Total amount of non-zero transactions per day, grouped by channel',\n    xaxis = dict(title = 'Date'), yaxis = dict(title = 'Total'),\n    showlegend=True,\n)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='lines')","1ba58b3a":"opsys = list(train_df['device_operatingSystem'].unique())\ndata = []\nfor os in opsys:\n    subset = train_df[train_df['device_operatingSystem']==os]\n    subset = subset.groupby('date')['totals_transactionRevenue'].agg(['sum'])\n    subset.columns = [\"Total\"]\n    subset = subset.sort_index()\n    trace = go.Scatter(\n        x = subset['Total'].index,\n        y = subset['Total'].values,\n        name=os,\n        mode='lines'\n    )\n    data.append(trace)\nlayout= go.Layout(\n    title= 'Total amount of non-zero transactions per day, grouped by OS',\n    xaxis = dict(title = 'Date'), yaxis = dict(title = 'Total'),\n    showlegend=True,\n)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='lines')","f810be5f":"total_test = test_df.groupby('date')['fullVisitorId'].agg(['count'])\ntotal_test.columns = [\"Total\"]\ntotal_test = total_test.sort_index()\nplot_scatter_data(total_test['Total'],'Date', 'Total','Total count of visits per day (test set)','magenta')","969ceefa":"Let's plot the average amount of transactions, grouped by **total_pageviews**.","48249a46":"# <a id=\"4\">Conclusions<\/a>  \n\nFor the predictive model, there is this Kernel available: https:\/\/www.kaggle.com\/gpreda\/ga-customer-revenue-simple-lightgbm, using the results of this data analysis.\n\n","1ece221c":"Most of the transaction revenues are from Northern America, with Refferal, Direct and Organic Search channel.\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","772e90aa":"Only **Not Socially Engaged** type is present. For prediction this field is a very good candidate to be droped.  \n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","d40c40fd":"## <a id=\"33\">Social Engagement Type<\/a>\n\nLet's check the Social engagement type data distribution.","587fbeef":"Let's check the **trafficSource** attributes that have unique value.","953db09d":"## <a id=\"32\">Channel Grouping<\/a>\n\nLet's check the channelGrouping data distribution. We will only show the first 30 categories, where more than that.","ac2f2760":"Columns **device**, **geoNetwork**, **totals**, **trafficSource** are of type **Objects** and are storing data in **json** format. We will create a function to read the data and creates separate columns for every element in the jsons.    \n\nAs well, we will have to pay attention to the **fullVisitorID** field, which is an integer but we will have to read it as **str**, to not loose some of the prefixing **0** digits.  \n\nLet's load **train** data. We will use the procedure described in <a href='#5'>[1]<\/a> to flatten the json objects.  \n\n\nNote: we limit the number of rows to **500,000**.","aa1cf390":"# <a id=\"22\">Load the data<\/a>  \n\n\nWe load first the data. Let's see what data files we have.","798a5803":"Chrome with Windows are the most frequent combination, followed by Chrome with Macintosh, Chrome with Android and Safari with Macintosh and Safari with iOS.","28870ed4":"Let's plot the total amount of transactions per day.","fc792e54":"We will plot only the attributes of Traffic Source with multiple categories.","8d587fd2":"We can see that in top 10 by the total transaction revenue there is one fullVisitorId (1957458976293878100) that appears 5 times.\n\nLet's check also the top 10 of visit number.","c5b49199":"Let's plot the number of visits vs. date  and the amount of transaction revenues vs. date for the train set.\n\nFirst, let's show the number of visits finalized with a transaction per day.","7ca8278a":"## <a id=\"31\">Missing data<\/a>\n\n\nLet's check if there are columns with missing data. We will only show the columns with missing data.","ef37e7fc":"We will explore these into more details in the following section. Let's for now just extract date and time from the date field.","650987ed":"We can observe that majority of the visits with non-zero transactions and most of the revenues are from US.   \n\nLet's check the top 10 of the transaction revenue.","eea7a40c":"# <a id=\"3\">Data exploration<\/a>  \n","68d8c9f3":"Let's look into more details in the **channelGrouping** classes.","ba1cd5c6":"Google Merchandise Collection is the most important adCountent traffic source.  \n\nMajority of campaign attributes are not set.  Majority of keywords are not provided. Organic and refferal stands for the majority of mediums.   \nThe most important traffic source is google, followed by youtube.com. \n\n\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","d3c75d65":"Let's check again the dataset shape.","642cfd90":"Let's plot the total amount of non-zero transactions per day,  grouped by **channelGrouping**.","3c0a52ae":"# <a id=\"35\">Geographic\/Network attributes<\/a>\n\nLet's check the geographical\/network attributes. As we did with the devices attributes, we will first gather all columns with **geoNetwork** in the name.","9993af2d":"The columns with constant value will be droped for the model.\n\nWe will only show the number of visits for the devices attributes that have more than one value. ","d95ee414":"A part of the features with very high missing data percent in the **train** data have a lower percent of the missing data in the **test** set (~93%) and also there are some that are not appearing in the list with fields with missing values in the **test**.     \n\n\nWe can also see that there are fields that does appears only in the **train** and  set, for example  **trafficSource_campaignCode** . We will have to consider these aspects when we will decide what features to drop and what features to keep for the predictive model.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","dde03e0f":"Chrome on Desktop is the most frequent browser-category device combination, followed by Chrome with mobile and Safari with desktop and with mobile.","083afc66":"Let's also show the geographical features on a plotly map. We will show the country feature distribution.","d6dcc0c8":"The majority of the visits are using devices with Windows OS, Chrome browser, from a Desktop. From the mobile devices, majority are phones. \n\nThe most used OS are: Windows, Macintosh, Android, iOS and Linux.  \nThe most used browsers are Chrome, Safari, Firefox, Internet Explorer and Edge.    \nLet's check few of these features correlation.\n","b57a0816":"<h1><center><font size=\"6\">Google Analytics Customer Revenue Extensive EDA<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/lh3.googleusercontent.com\/JyFKFXvek5tgUMhZh4FhBrlSKKoq74s53I91nfXdMLJNHg8WzOPSS8DSog4V0FUJOA\"><\/img>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n    -<a href='#21'>Load packages<\/a>  \n     -<a href='#21'>Load the data<\/a>  \n- <a href='#3'>Data exploration<\/a>   \n    -<a href='#31'>Missing data<\/a>  \n    -<a href='#32'>Channel Grouping<\/a>  \n    -<a href='#33'>Social Engagement Type<\/a>  \n    -<a href='#34'>Device attributes<\/a>  \n    -<a href='#35'>Geographical\/Network attributes<\/a>  \n    -<a href='#36'>Total attributes<\/a>  \n    -<a href='#37'>Traffic source attributes<\/a>  \n    -<a href='#38'>Date and time<\/a>  \n- <a href='#4'>Conclusions<\/a>    \n- <a href='#5'>References<\/a>    ","f743be5d":"# <a id=\"2\">Prepare the data analysis<\/a>  \n\n\n\n# <a id=\"21\">Load the packages<\/a>  \n\n\n","e6f2b697":"# <a id=\"5\">References<\/a>  \n\n[1] [Juli\u00e1n Peller](https:\/\/www.kaggle.com\/julian3833), 1 - Quick start: read csv and flatten json fields, https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields   \n[2] [Shivam Bansal](https:\/\/www.kaggle.com\/shivamb), Exploratory Analysis - GA Customer Revenue, https:\/\/www.kaggle.com\/shivamb\/exploratory-analysis-ga-customer-revenue\/   \n[3] [SRK](https:\/\/www.kaggle.com\/sudalairajkumar), Simple Exploration+Baseline - GA Customer Revenue,  https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-ga-customer-revenue   \n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","11332023":"## <a id=\"38\">Date and time<\/a>  \n\n\nLet's check the distribution of values for date and time features. ","dd69074d":"## <a id=\"36\">Totals attributes<\/a>  \n\nLet's check the total attributes. ","7de965be":"# <a id=\"1\">Introduction<\/a>  \n\n## The competition\n\nIn this competition, Google Cloud and Kaggle partenered with [RStudio](http:\/\/www.rstudio.com) to challenge the Kagglers to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer.     \n\nThe data is provided both in *.csv format and in BigQuery format.\n\nIn this Kernel we will use the data in *.csv format.\n\nThe data fields are as following:\n\n* **fullVisitorId** - A unique identifier for each user of the Google Merchandise Store.\n* **channelGrouping** - The channel via which the user came to the Store.\n* **date** - The date on which the user visited the Store.\n* **device** - The specifications for the device used to access the Store.\n* **geoNetwork** - This section contains information about the geography of the user.\n* **sessionId** - A unique identifier for this visit to the store.\n* **socialEngagementType** - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n* **totals** - This section contains aggregate values across the session.\n* **trafficSource** - This section contains information about the Traffic Source from which the session originated.\n* **visitId** - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n* **visitNumber** - The session number for this user. If this is the first session, then this is set to 1.\n* **visitStartTime** - The timestamp (expressed as POSIX time).\n\nSome of the data fields are blobs with multiple attributes, as following: **device**, **geoNetwork**, **totals**, **trafficSource**.\n\nWe will need to predict the natural log of the sum of all transactions per user. For every user in the test set, the target is:\n\n$$target_{user}=\\sum_{i=0}^n {transaction_{user}}_i$$\n\n\n## This Kernel\n\nThis Kernel objective is to explore the dataset for [Google Analytics Customer Revenue Prediction competition](https:\/\/www.kaggle.com\/c\/google-analytics-customer-revenue-prediction). \n\nWe only use the data in *.csv format.\n\nFor the **predictive model**, a separate **Kernel** was developed: https:\/\/www.kaggle.com\/gpreda\/ga-customer-revenue-simple-lightgbm, with public **LB 1.6650**.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>\n","17838e9b":"\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","ac3c0ea6":"Before starting to plot the number of visits per various geoNetwork attributes, let's check if there are geoNetwork attributes that have a unique value.","b16710f3":"We can observe that there are both Desktop that appears as mobile (110) device and tablet (14) and mobile (150) set as not mobile. \n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>","920b463f":"Then, let's see the number of visits  with non-zero transactions per day.","f4648320":"Let's check first the columns and types of `train_v2.csv`.","4ad10ce3":"These columns are candidates to be dropped from the model.  For the rest of the columns, we show the number of the visits per each attribute. \nNote: We limit the number of shown values\/categories to 50, showing the most numerous first.","44344d09":"Not all the cities, network domains, metropolitan areas, continents are set.   \nMost numerous cities are not available in the dataset, as well as most metropolitan areas or network domains.  \nThe continent with the largest number of visits is America. The sub-continent with the largest number of visits is Northern America.","c4553ae0":"Let's check if there are columns with an unique value.","2e397cd3":"Let's show the distribution of visits per country, taking into consideration only the visits with non-zero transactions.","cabbae2c":"Before starting to plot the number of visits per various devices attributes, let's check if there are devices attributes that have a unique value.","f4bb9255":"It seems that **sessionId** is the result of concatenating **fullVisitorId** with **visitId**.  The field **visitStartTime** seems to be identical with **visitId** and also it is most probably the timestamp. Let's check if the value of first **visitId** is a timestamp.\n\n","32b84e7f":"For **totals_transactionRevenue**, let's also show the values distribution.","b4000e35":"\n## <a id=\"37\">Traffic Source attributes<\/a>  ","bbb1bae7":"Let's load also the test data. Then, let's process similarly teh test data.","b74f1365":"Let's check now the dataset shape.","c28a1372":"One fullVisitorId (1957458976293878100) ocupies all the positions in top 10 (by visitNumber).\n\nLet's calculate the sum of the transaction revenue per **channelGrouping**.","79e5570f":"## <a id=\"34\">Device attributes<\/a>  \n\nLet's check the device fields.","5cf8a503":"Let's plot the total amount of non-zero transactions per day,  grouped by **device_operatingSystem**.","c6756a2c":"\nLet's plot now the number of visits per day in the test set.","10c11160":"Some of the columns in the train dataset (8) have >97% of the values missing, majority columns with missing values being from **trafficSource**.   \n\nThese fields (especially the ones with high percent of missing values) are candidates to be droped when we will create a predictive model.    \n\nLet's check the status for the test dataset.\n\n","8de1f127":"Let's check as well the log of the total transaction revenue."}}