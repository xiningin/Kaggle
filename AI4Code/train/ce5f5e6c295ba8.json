{"cell_type":{"0f056aa4":"code","149accf8":"code","51e38997":"code","c7e863c8":"code","4413abda":"code","e8291061":"code","44f817aa":"code","df5f4f4c":"code","1113b955":"code","7ef8d0da":"code","4c935bf8":"code","99390b10":"code","2b780feb":"code","c3fee952":"code","9a7bcd7d":"code","2245cc41":"code","04d296f7":"code","9f3fd9de":"code","78e5bc9f":"code","455fc7d9":"code","b56396bb":"code","7c271360":"code","91905703":"code","39617afd":"code","8deecefe":"code","5bb99299":"code","bbf3d5b4":"code","49f3d3e0":"markdown","ec8ae110":"markdown","0e4d5e2c":"markdown","25ae1a09":"markdown","1a671f9a":"markdown","1dcb44b7":"markdown","069cf03e":"markdown","9f2b900b":"markdown","53203dcf":"markdown","633d19ca":"markdown"},"source":{"0f056aa4":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.models import  Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose,Dense,LeakyReLU,BatchNormalization,Dropout,InputLayer,MaxPool2D,Flatten,Activation\nfrom tensorflow.keras import regularizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nimport itertools\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile\nimport zipfile\nimport os\nimport random\nimport cv2\nfrom scipy.ndimage.filters import gaussian_filter\nimport random\nfrom tensorflow.keras.utils import to_categorical","149accf8":"physical_devices = tf.config.list_physical_devices(\"GPU\")\ntf.config.experimental.set_memory_growth(physical_devices[0], True)","51e38997":"class AutoEncoder(Model):\n    def __init__(self,input_shape):\n        super(AutoEncoder,self).__init__()\n        self.encoder = keras.Sequential([\n          InputLayer(input_shape=input_shape),\n          Conv2D(64,kernel_size=3,strides=2,padding='same'),\n          LeakyReLU(0.001),\n          Conv2D(64,kernel_size=3,strides=2,padding='same'),\n          LeakyReLU(0.001)\n        ])\n\n    \n        self.decoder = keras.Sequential([\n          Conv2DTranspose(64,kernel_size=3,strides=2,padding='same'),\n          LeakyReLU(0.001),\n          Conv2DTranspose(64,kernel_size=3,strides=2,padding='same'),\n          LeakyReLU(0.001),\n          Conv2D(input_shape[2],kernel_size=3,padding='same'),\n          LeakyReLU(0.001)\n        ])\n\n    def call(self,x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded","c7e863c8":"class ConvBlock(layers.Layer):\n    def __init__(self,out_channels,kernel_size=3):\n        super(ConvBlock,self).__init__()\n        self.conv = Conv2D(out_channels,kernel_size,padding='same')\n        self.leaky = LeakyReLU(0.2)\n        self.bn = BatchNormalization()\n        self.pool = MaxPool2D()\n        self.drop = Dropout(0.2)\n    def call(self, x):\n        x = self.leaky(self.conv(x))\n        x = self.bn(x)\n        x = self.pool(x)\n        x = self.drop(x)\n        return x\n    \nclass FCBlock(layers.Layer):\n    def __init__(self,out_channels,dor = 0.2):\n        super(FCBlock,self).__init__()\n        self.dense = Dense(out_channels)\n        self.leaky = LeakyReLU(0.2)\n        self.drop = Dropout(dor)\n        self.bn = BatchNormalization()\n    def call(self,x):\n        x = self.leaky(self.dense(x))\n        x = self.bn(x)\n        x = self.drop(x)\n        return x","4413abda":"class CNNClassifier(Model):\n    def __init__(self,number_of_classes):\n        super(CNNClassifier,self).__init__()\n        self.conv1 = ConvBlock(16)\n        self.conv2 = ConvBlock(32)\n        self.conv3 = ConvBlock(64)\n        self.conv4 = ConvBlock(128)\n        self.flat = Flatten()\n        self.fc1 = FCBlock(512, dor = 0.3)\n        self.fc2 = FCBlock(128, dor = 0.3)\n        self.fc3 = FCBlock(64, dor = 0.2)\n        self.fc4 = Dense(number_of_classes)\n        self.softmax = keras.layers.Activation('softmax')\n    def call(self,x,training = False):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flat(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        x = self.softmax(x)\n        return x","e8291061":"class Classifier:\n    def __init__(self,x_train,x_test,y_train,y_test,class_names):\n        self.cnn = CNNClassifier(len(class_names))\n        self.x_train = x_train\n        self.x_test = x_test\n        self.y_train = y_train\n        self.y_test = y_test\n        self.class_names = class_names\n        self.predictions = None\n        self.history = None\n\n    def train_classifier(self):\n        self.cnn.compile(\n          loss='categorical_crossentropy',\n          optimizer=keras.optimizers.Adam(lr=3e-4),\n          metrics=[\"accuracy\"],\n        )\n        self.history = self.cnn.fit(self.x_train, self.y_train, batch_size=256, epochs=100, verbose=2,validation_data=(self.x_test,self.y_test))\n        self.predictions = self.cnn.predict(x=self.x_test, batch_size=256, verbose=0)\n  \n    def predict(self,picture):\n        picture = tf.expand_dims(input=picture,axis = 0)\n        prediction = 0\n        with tf.GradientTape() as tape:\n            prediction = self.cnn(picture)\n\n        return prediction\n\n\n    def plot_accuracy(self):\n        plt.plot(self.history.history['accuracy'], label='accuracy')\n        plt.plot(self.history.history['val_accuracy'], label = 'val_accuracy')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.ylim([0.5, 1])\n        plt.legend(loc='lower right')\n  \n    def plot_loss(self):\n        plt.plot(self.history.history['loss'], label='loss')\n        plt.plot(self.history.history['val_loss'], label = 'val_loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.ylim([0, 3])\n        plt.legend(loc='lower right')\n\n\n    def plot_confusion_matrix(self,target_names,title='Matrica konfuzije',cmap=None,normalize=True):\n        tacne_labele = []\n        pred_labele = []\n        for i in range(len(self.y_test)):\n            tacne_labele.append(np.argmax(self.y_test[i]))\n            pred_labele.append(np.argmax(self.predictions[i]))\n\n        test = np.array(tacne_labele)\n        pred = np.array(pred_labele)\n    \n    \n    \n        cm = confusion_matrix(y_true=test, y_pred=pred)\n        accuracy = np.trace(cm) \/ np.sum(cm).astype('float')\n        misclass = 1 - accuracy\n\n        if cmap is None:\n            cmap = plt.get_cmap('magma')\n\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n\n        if target_names is not None:\n            tick_marks = np.arange(len(target_names))\n            plt.xticks(tick_marks, target_names, rotation=45)\n            plt.yticks(tick_marks, target_names)\n\n        if normalize:\n            cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n        thtest_labelsresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                        horizontalalignment=\"center\",\n                        color=\"red\")\n            else:\n                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                        horizontalalignment=\"center\",\n                        color=\"red\")\n\n\n        plt.tight_layout()\n        plt.ylabel('Tacne klase')\n        plt.xlabel('Prediktovane klase\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n        plt.show()\n\n\n    def procenat_tacnosti_pre_ciscenja(self,x_test_attacked):\n        broj_tacnih_predikcija = 0\n\n        predikcije = self.cnn.predict(x_test_attacked)\n        for i in range(len(predikcije)):\n            if(np.argmax(predikcije[i]) == np.argmax(y_test[i])):\n                broj_tacnih_predikcija+=1\n\n        procenat_tacnosti = broj_tacnih_predikcija\/len(self.y_test)\n\n        labels = 'Tacne predikcije', 'Netacne predikcije'\n        sizes = [procenat_tacnosti*100. ,(1-procenat_tacnosti)*100]\n        explode = (0, 0.1)  \n\n        fig1, ax1 = plt.subplots()\n        ax1.pie(sizes, explode = explode, labels=labels, autopct='%1.1f%%',\n                shadow=True, startangle=90)\n        ax1.axis('equal') \n\n        plt.show()\n\n    def procenat_tacnosti_posle_ciscenja(self,denoiser,x_test_attacked):\n        broj_tacnih_predikcija = 0\n\n        ociscen_skup = denoiser.autoencoder(x_test_attacked)\n        predikcije = self.cnn.predict(ociscen_skup)\n\n        for i in range(len(predikcije)):\n            if(np.argmax(predikcije[i]) == np.argmax(y_test[i])):\n                broj_tacnih_predikcija+=1\n        \n        procenat_tacnosti = broj_tacnih_predikcija\/len(self.y_test)\n\n        labels = 'Tacne predikcije', 'Netacne predikcije'\n        sizes = [procenat_tacnosti*100. ,(1-procenat_tacnosti)*100]\n        explode = (0, 0.1)  \n\n        fig1, ax1 = plt.subplots()\n        ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n                shadow=True, startangle=90)\n        ax1.axis('equal') \n\n        plt.show()","44f817aa":"class Denoiser:\n    def __init__(self,x_train,x_train_attacked,x_test_attacked,input_shape):\n        self.autoencoder = AutoEncoder(input_shape=input_shape)\n        self.x_train = x_train\n        self.x_train_attacked = x_train_attacked\n        self.x_test_attacked = x_test_attacked\n\n    def train_autoencoder(self):\n        self.autoencoder.compile(optimizer='adam',loss=keras.losses.MeanSquaredError())\n\n        self.autoencoder.fit(self.x_train_attacked, self.x_train,\n                    epochs=15,\n                    shuffle=True)\n        print(self.autoencoder.encoder.summary())\n        print(self.autoencoder.decoder.summary())\n        self.__rezultat_treniranja_ae()\n\n    def encoding_decoding(self,picture):\n        picture = tf.expand_dims(input = picture, axis = 0)\n        enc = self.autoencoder.encoder(picture)\n        dec = self.autoencoder.decoder(enc)\n\n        return dec\n\n\n    def __rezultat_treniranja_ae(self):\n        print(\"rezultat treniranja: \\n\")\n        n = 10\n        plt.figure(figsize=(20, 4))\n        for i in range(n):\n            cleaned = self.encoding_decoding(self.x_test_attacked[i])\n            ax = plt.subplot(2, n, i + 1)\n            plt.title(\"original\")\n            plt.imshow(tf.squeeze(self.x_test_attacked[i]))\n            plt.gray()\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)\n            bx = plt.subplot(2, n, i + n + 1)\n            plt.title(\"rekonstruisana\")\n            plt.imshow(tf.squeeze(cleaned))\n            plt.gray()\n            bx.get_xaxis().set_visible(False)\n            bx.get_yaxis().set_visible(False)\n        plt.show()","df5f4f4c":"def create_attack(x_train,x_test,attack_type,input_shape, classifier = None, y_test = None, eps = 0.1):\n  # random noise attack\n    if(attack_type == 1):\n        x_train_attacked = x_train + eps * tf.random.normal(shape=x_train.shape) \n        x_test_attacked = x_test + eps * tf.random.normal(shape=x_test.shape) \n\n        x_train_attacked = tf.clip_by_value(x_train_attacked, clip_value_min=0., clip_value_max=1.)\n        x_test_attacked = tf.clip_by_value(x_test_attacked, clip_value_min=0., clip_value_max=1.)\n\n        denoiser = Denoiser(x_train,x_train_attacked,x_test_attacked,input_shape)\n        return denoiser,x_train_attacked,x_test_attacked\n  #FGSM attack\n    elif(attack_type == 2):\n\n        slika = tf.convert_to_tensor(x_test[0])\n\n        probs = classifier.cnn(tf.expand_dims(input=slika,axis=0))\n\n        label = tf.one_hot(np.argmax(y_test[0]), probs.shape[-1])\n        label = tf.reshape(label, (1, probs.shape[-1]))\n\n        loss_object = keras.losses.CategoricalCrossentropy()\n        with tf.GradientTape() as tape:\n            tape.watch(slika)\n            predikcija = classifier.cnn(tf.expand_dims(input=slika,axis=0))\n            loss = loss_object(label,predikcija)\n\n        gradient = tape.gradient(loss,slika)\n\n        signed_grad = tf.sign(gradient)\n\n        x_train_attacked = x_train + eps * signed_grad\n        x_test_attacked = x_test + eps * signed_grad\n\n        x_train_attacked = tf.clip_by_value(x_train_attacked, clip_value_min=0., clip_value_max=1.)\n        x_test_attacked = tf.clip_by_value(x_test_attacked, clip_value_min=0., clip_value_max=1.)\n\n        denoiser = Denoiser(x_train,x_train_attacked,x_test_attacked,input_shape)\n\n        return denoiser,x_train_attacked,x_test_attacked\n  # Gauss-blur\n    elif(attack_type == 3):\n        x_train_attacked = []\n        x_test_attacked = []\n\n        for i in x_train:\n            x_train_attacked.append(gaussian_filter(i,sigma=0.9))\n\n        for i in x_test:\n            x_test_attacked.append(gaussian_filter(i,sigma=0.9))\n\n        x_train_attacked = np.array(x_train_attacked)\n        x_test_attacked = np.array(x_test_attacked)\n        denoiser = Denoiser(x_train,x_train_attacked,x_test_attacked,input_shape)\n        return (denoiser,x_train_attacked,x_test_attacked)\n  \n  \n  # Random missing pixels\n    elif(attack_type == 4):\n\n        amount = int(input_shape[0]*input_shape[1]*0.05)\n\n        x_train_attacked = []\n        x_test_attacked = []\n\n        for i in x_train:\n            pic = np.array(i)\n            cor = [(random.randrange(0, input_shape[0]), random.randrange(0, input_shape[1])) for i in range(amount)]\n            for j in range(input_shape[2]):\n                for z in cor:\n                    pic[z[0]][z[1]][j] = 0\n            x_train_attacked.append(pic)\n\n\n        for i in x_test:\n            pic = np.array(i)\n            cor = [(random.randrange(0, input_shape[0]), random.randrange(0, input_shape[1])) for i in range(amount)]\n            for j in range(input_shape[2]):\n                for z in cor:\n                    pic[z[0]][z[1]][j] = 0\n            x_test_attacked.append(pic)\n\n        x_train_attacked = np.array(x_train_attacked)\n        x_test_attacked = np.array(x_test_attacked)\n\n        denoiser = Denoiser(x_train,x_train_attacked,x_test_attacked,input_shape)\n\n        return (denoiser,x_train_attacked,x_test_attacked)","1113b955":"(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n\n\n\nx_train = x_train.astype(\"float32\") \/255.0\nx_test = x_test.astype(\"float32\") \/ 255.0\n\n\ny_train = to_categorical(y=y_train,num_classes=10)\ny_test = to_categorical(y=y_test,num_classes=10)","7ef8d0da":"class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']","4c935bf8":"classifier = Classifier(x_train,x_test,y_train,y_test,class_names)","99390b10":"classifier.train_classifier()","2b780feb":"classifier.plot_confusion_matrix(class_names,normalize=False)","c3fee952":"classifier.plot_accuracy()","9a7bcd7d":"classifier.plot_loss()","2245cc41":"val = create_attack(x_train,x_test,1,(32,32,3),eps=0.2)\nrnaDenoiser = val[0]\n\nrnaDenoiser.train_autoencoder()","04d296f7":"classifier.procenat_tacnosti_pre_ciscenja(val[2])","9f3fd9de":"classifier.procenat_tacnosti_posle_ciscenja(denoiser=rnaDenoiser,x_test_attacked=val[2])","78e5bc9f":"val = create_attack(x_train,x_test,2,(32,32,3),classifier=classifier,y_test=y_test,eps=0.2)\nfgsmDenoiser = val[0]\nfgsmDenoiser.train_autoencoder()","455fc7d9":"classifier.procenat_tacnosti_pre_ciscenja(val[2])","b56396bb":"classifier.procenat_tacnosti_posle_ciscenja(denoiser=fgsmDenoiser,x_test_attacked = val[2])","7c271360":"val = create_attack(x_train,x_test,3,(32,32,3))\ngaussDenoiser = val[0]\ngaussDenoiser.train_autoencoder()","91905703":"classifier.procenat_tacnosti_pre_ciscenja(val[2])","39617afd":"classifier.procenat_tacnosti_posle_ciscenja(denoiser=gaussDenoiser,x_test_attacked= val[2])","8deecefe":"val = create_attack(x_train,x_test,4,(32,32,3))\nmisPDenoiser = val[0]\nmisPDenoiser.train_autoencoder()","5bb99299":"classifier.procenat_tacnosti_pre_ciscenja(val[2])","bbf3d5b4":"classifier.procenat_tacnosti_posle_ciscenja(denoiser=misPDenoiser,x_test_attacked= val[2])","49f3d3e0":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Autoencoder class<\/h1><br>\n<p style=\"text-align:center\">First lets look at what an autoencoder actually is:<br>An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (\u201cnoise\u201d). <i>source: wikipedia<\/i>\n<\/p>\n<div style=\"text-align:center\">\n    <img src = \"https:\/\/www.iamnagdev.com\/wp-content\/uploads\/2020\/03\/encoder-decoder-1024x449-1.png\" style = \"width:800px;height:400px;padding-left:auto;padding-right:auto\"\/>\n<\/div>\n","ec8ae110":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Plotting results of classifier training<\/h1><br>","0e4d5e2c":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Loading data and defining classifier<\/h1><br>","25ae1a09":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Function for creating dataset attacks<\/h1><br>\n<p style=\"text-align:center\">\n    This function will create an attacked(corupted) copy of dataset that is given to it and beside dataset it will return autoencoder model for that attacked dataset.<br>\n    Attacks that i have used are:<br>\n    <ul style=\"text-align:center;list-style-type: none;\">\n        <li>Random noise attack<\/li>\n        <li>FGSM (fast gradient sign methon) attack<\/li>\n        <li>Gaussian blur attack<\/li>\n        <li>Random missing pixel attack<\/li>\n    <\/ul>\n<\/p>","1a671f9a":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Final note<\/h1><br>\n<p style=\"text-align:center\">I hope you like this notebook and that you have learned something new.<br>If you have some questions feel free to ask!<\/p>","1dcb44b7":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Utility classes<\/h1><br>\n<p style=\"text-align:center\">\n    Classes 'Classifier' and 'Denoiser' are utility classes that are used for better organizing my code.\n<\/p>\n","069cf03e":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Convolutional classifier class with custom layers<\/h1><br>","9f2b900b":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Creating atacked datasets and testing how an autoencoder can affect classifer accuracy<\/h1><br>\n\n<p style=\"text-align:center\">In this segment we will generate attacked datasets and train our autoencoders to deal with those datasets.<br>\nWe will test every attack that we can create with our function.<br>\nFirst pie chart is accuracy of our model before cleaning our dataset where the orange part represent precentage of wrongly predicted labels.<br>\nSecond pie char is accuracy of our model after cleaning our dataset.\n<\/p>","53203dcf":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">Results<\/h1><br>\n<p style=\"text-align:center\">We can see that autoencoders can infact boost our model accuracy.<br>Next part is to get more data and test with more attacks,<\/p>","633d19ca":"<h1 style=\"text-align:center;background-color:#0e4b90;color:white\">How autoencoders can boost accuracy of classifiers<\/h1><br>\n\n<p style=\"text-align:center\">This is a notebook for a uni project i did back in Jun. The project was about how can autoencoders improve accuracy of our classifiers when our data is corupted in some way.<br>\nSome parts of code are on Serbian so if you do not understand something feel free to ask!<br>\nThis implementation is done in tensorflow 2.x<br>\nAlso keep in mind this is the first project i did about deep learning and machine learning in general, so if i made some cardinal sin of ml please do not crucify me.<\/p>"}}