{"cell_type":{"05386e67":"code","39577699":"code","950dd1a9":"code","d198330b":"code","854e305b":"code","4f3844db":"code","0179f664":"code","de1197ce":"code","9b9b1682":"code","4a70df3a":"code","aecaafdb":"code","88011cfc":"code","e496659b":"code","b146d9a2":"code","971e3a70":"code","b636d294":"code","961ba215":"code","26e938d6":"code","e72f7006":"code","0cdc3a58":"code","6993fc86":"code","bb5bb4bc":"markdown","e61ca61b":"markdown","5cae52b1":"markdown","c9b7391e":"markdown","94461f5f":"markdown","681b05c6":"markdown","96fbb441":"markdown","a127b965":"markdown","c2f507c2":"markdown","21ce1555":"markdown","3b415dc1":"markdown","8d77a275":"markdown"},"source":{"05386e67":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","39577699":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()\ndf.drop(['id'], axis=1, inplace = True)","950dd1a9":"plt.rcParams['figure.dpi'] = 100\nfig, (ax4, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\nsize = 0.4\n\n\ncmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.arange(3)*4)\n\n\n#Stroke married\nax4.pie(df[df['ever_married']=='Yes']['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax4.pie(df[df['ever_married']=='No']['stroke'].value_counts(), radius=1-size, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax4.set(aspect=\"equal\", title='Married\\nOuter - 1, Inner -0')\n\n\n\n\n#Overall\nax2.text(0, 0, f'{df[\"stroke\"].count()} cases', color='black', va=\"center\", ha=\"center\")\nax2.pie(df['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax2.set(aspect=\"equal\", title='Stroke overall')\n\n\n\n#Hypertension\nax3.pie(df[df['hypertension']==1]['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax3.pie(df[df['hypertension']==0]['stroke'].value_counts(), radius=1-size, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax3.set(aspect=\"equal\", title='Hypertension\\nOuter - 1, Inner -0')\n\n\nfig.legend(['Stroke', 'No stroke'], loc='upper right')\n\n\nplt.show()","d198330b":"df.describe()","854e305b":"df.isnull().sum()","4f3844db":"#df.drop('bmi', axis=1, inplace=True)\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)","0179f664":"#Separate Data into numerical and categorical columns\ncat_cols = df.select_dtypes(['object', 'int']).columns\nnum_cols = df.select_dtypes(['float']).columns\n\nprint(f'Categorical columns: {cat_cols.values}. \\n Numerical columns: {num_cols.values}.')","de1197ce":"#Gender uniques\ndf['gender'].value_counts()","9b9b1682":"#Just one value, dropped\ndf.drop(df[df['gender'] == 'Other'].index, axis=0, inplace=True)","4a70df3a":"#Map binary categories to numerical\ndf['Residence_type'] = df['Residence_type'].map({'Rural': 0, 'Urban': 1})\ndf['ever_married'] = df['ever_married'].map({'No': 0, 'Yes': 1})\ndf['gender'] = df['gender'].map({'Male': 0, 'Female': 1})","aecaafdb":"df['smoking_status'].unique()","88011cfc":"df['smoking_status'] = df['smoking_status'].map({'Unknown': 0, 'never smoked': 0, 'formerly smoked': 1, 'smokes': 2})","e496659b":"ohe = OneHotEncoder()\n\n#I used index=df.index, cuz I deleted row before and OHE indexing dont match with df.\n\ndf = pd.concat([df, pd.DataFrame(ohe.fit_transform(df[['work_type']]).toarray(), columns=ohe.categories_[0], index=df.index)], \n          axis=1)\ndf.drop('work_type', axis=1, inplace=True)","b146d9a2":"df.head()","971e3a70":"# Take target column away and split X\ny = df['stroke']\nX = df.drop('stroke', axis=1)","b636d294":"#Scaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n","961ba215":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","26e938d6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, auc, precision_score, recall_score\nimport datetime","e72f7006":"models = [\n    {'name': 'LogisticRegression', 'estimator': LogisticRegression(random_state=42)},\n    {'name': 'GradientBoostingClassifier', 'estimator': GradientBoostingClassifier(random_state=42)},\n    {'name': 'AdaBoostClassifier', 'estimator': AdaBoostClassifier(random_state=42)},\n    {'name': 'BaggingClassifier', 'estimator': BaggingClassifier(random_state=42)},\n    {'name': 'XGBClassifier', 'estimator': XGBClassifier(verbosity=0, random_state=42)},\n    {'name': 'DecisionTreeClassifier', 'estimator': DecisionTreeClassifier(random_state=42)},\n    \n]\n","0cdc3a58":"print('Name           \\t Time          \\t Recall      ')\nprint('--------------------------------------------------------')\nfor model in models:\n    time_a = datetime.datetime.now()\n    \n    model['estimator'].fit(X_train, y_train)\n    model['preds'] =  model['estimator'].predict(X_test)\n    model['recall'] = recall_score(y_test, model['preds'], average='binary', pos_label=1)\n    \n    time_b = datetime.datetime.now()\n    print(f\"{model['name']}\\t {time_b-time_a}\\t {round(model['recall'],4)}\\t\")","6993fc86":"y_test.sum() \/ y_test.count()","bb5bb4bc":"### Visualisation","e61ca61b":"# WELL.....\n\n<img src= \"https:\/\/i.kym-cdn.com\/entries\/icons\/original\/000\/028\/021\/work.jpg\">","5cae52b1":"Options:\n1. Drop NaN values \n2. Drop whole column\n\nBMI feature doesn't seem to make some influence on stroke possibility. I decided to drop whole column.","c9b7391e":"Simple Models with default hyper-parameters","94461f5f":"### Intro\n\nSo my first notebook, learning through doing.\nPlease, point out my mistakes and stupidity in comments, i do apprecciate it.","681b05c6":"Data is heavy imbalanced, Ill try to improve results in next kernel versions. Stay in touch.\n\nBut I got one insight:\n\n *Dont get married if you dont want a stroke (6ix-time risk increased... yeah, let this dataset represenet the whole population*\n \n\n","96fbb441":"We've got a very imbalanced data. Let's make a model.","a127b965":"### Missing values","c2f507c2":"Best estimator gives us 0.1348 recall. Test target proportion:","21ce1555":"> ### Modelling","3b415dc1":"### Categorical values","8d77a275":"2 multinominal categories left:\n* smoking_status - label encoding (Unknown = never smoked)\n* work_type - OHE"}}