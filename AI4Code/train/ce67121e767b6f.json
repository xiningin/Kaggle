{"cell_type":{"a8c6ce65":"code","e4ea1db9":"code","9d5d084f":"code","dcf5f934":"code","8b2d4102":"code","10ae7a5f":"code","88eb2ad5":"code","bc7bafb3":"code","bf890faf":"code","6f5af41c":"code","3e5ddb77":"code","531c4acd":"code","5da402f4":"code","ae3dc3d9":"code","601eadc5":"code","ed4f581a":"code","740bd72e":"code","71a30058":"code","a234c030":"code","e29de490":"code","6210a364":"code","b07c1ce8":"code","898d6ead":"code","836ff0e6":"code","d96ac4d5":"code","5c8ff8b9":"code","2405f099":"markdown","a8d2aee7":"markdown","d129a83f":"markdown","e43b907b":"markdown","16bd4502":"markdown","7556e429":"markdown","ddbafb8e":"markdown","3ad12a53":"markdown","8a624922":"markdown","20360b86":"markdown","f8d42ecf":"markdown","c8041b45":"markdown","50ba4f22":"markdown","0d6f6222":"markdown","848efe62":"markdown","cd5cb3c0":"markdown","19f72c6c":"markdown","ebfd6841":"markdown","c0b8a4c2":"markdown","31d69400":"markdown","15fa8965":"markdown","60e6afe0":"markdown","6950023d":"markdown","8d1d2959":"markdown","897ac098":"markdown"},"source":{"a8c6ce65":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom textwrap import wrap\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","e4ea1db9":"data = pd.read_csv(\n    '\/kaggle\/input\/netflix-movies-and-tv-shows-2021\/netflixData.csv')","9d5d084f":"data.head()","dcf5f934":"data.info()","8b2d4102":"data.describe().T.style.bar()","10ae7a5f":"null = data.isnull().sum()\nnull","88eb2ad5":"plt.figure(figsize=(23, 3))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=True)","bc7bafb3":"total_null = null.sort_values(ascending=False)\nperc = (null \/ data.isnull().count()).sort_values(ascending=False)\ntotal = pd.concat([total_null, perc], axis=1, keys=[\n                  'Total null values', 'Percentages of null values'])\ntotal.T.style.bar()","bf890faf":"num_col = data._get_numeric_data().columns.tolist()\nprint(num_col)","6f5af41c":"cat_col = set(data.columns) - set(num_col)\nprint(cat_col)","3e5ddb77":"for i in cat_col:\n    data[i].fillna(data[i].mode()[0], inplace=True)\nfor i in num_col:\n    data[i].fillna(data[i].mean(), inplace=True)","531c4acd":"data.isnull().sum()","5da402f4":"data['Title'] = data['Title'].str.replace('#', '')","ae3dc3d9":"data['Imdb Score'] = data['Imdb Score'].str.replace('\/10', '')","601eadc5":"data['Imdb Score'] = data['Imdb Score'].apply(pd.to_numeric)","ed4f581a":"sample = data.sample(n=100)","740bd72e":"fig = plt.figure(figsize=(20, 4))\nax = plt.axes()\nplt.title('Distribution of Production Country')\nsns.histplot(sample['Production Country'], kde=True)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\nplt.xlabel('country')\nplt.ylabel('number of production')","71a30058":"fig = plt.figure(figsize=(20, 4))\nax = plt.axes()\nplt.title('Imdb Score')\nsns.histplot(sample['Imdb Score'], kde=True)\nplt.xlabel('Imdb Score')\nplt.ylabel('number')\nplt.text(3,7, 'Skewness coeff. is:' + str(data['Imdb Score'].skew()))","a234c030":"plt.boxplot(data['Imdb Score'], 0,'o',showbox=True,\n            showfliers=True, showcaps=True, showmeans=True)","e29de490":"import warnings\nwarnings.simplefilter('ignore')\nfig = plt.figure(figsize=(5, 5))\nax=plt.axes()\nsns.countplot(sample['Content Type'])","6210a364":"fig = plt.figure(figsize=(20, 4))\nax = plt.axes()\nplt.title('Duration')\nsns.histplot(sample['Duration'], kde=True)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(55)\nplt.xlabel('Time')\nplt.ylabel('number')","b07c1ce8":"pd.crosstab(index=sample['Rating'], columns=[sample['Imdb Score']],\n            margins=True).style.background_gradient(cmap='YlGn')","898d6ead":"pd.DataFrame(data.groupby('Production Country')['Imdb Score'].mean()).sort_values(ascending=False, by='Imdb Score')","836ff0e6":"pd.crosstab(index=sample['Content Type'], columns=[sample['Release Date']],\n            margins=True).style.background_gradient(cmap='YlGn')","d96ac4d5":"(pd.DataFrame(data.groupby(data['Director'])['Release Date'].count())).sort_values(ascending=False, by='Release Date')","5c8ff8b9":"Titles = (sample['Title']).values\nTitle = \" \".join(i for i in Titles)\nfig = plt.figure(figsize=(10, 10))\nfor i in range(4):\n    wordcloud = WordCloud(width=400, height=400, max_font_size=50,\n                          max_words=70, colormap=\"Dark2\").generate(Title).generate(Title)\n    plt.subplot(2, 2, i+1)\n    plt.title('Title of movies')\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")","2405f099":"Ra\u00fal Campos, Jan Suter have the first score in terms of producing movies","a8d2aee7":"# 1. Importing libraries","d129a83f":"<h4>Studying the directors' works<\/h4>\n\n<img src=\"https:\/\/cadenaser00.epimg.net\/programa\/imagenes\/2015\/10\/22\/la_script\/1445550974_794066_1445551003_noticia_normal.jpg\" alt=\"Copyandedit\" width=\"150\" height=\"150\" style=\"float:left\">","e43b907b":"### 2.2.3 Impute Missing Values\n\nOne approach would be replacing the missing values with sensible values. For the text datatype, I used the high frequent values and for integers, I used the average of the feature","16bd4502":"Converting Imdb Score to numerical values","7556e429":"<h4> Studying the Release Date <\/h4>","ddbafb8e":"### Checkout the titles\n<img src=\"https:\/\/occ-0-1068-1722.1.nflxso.net\/dnm\/api\/v6\/X194eJsgWBDE2aQbaNdmCXGUP-Y\/AAAABdoFMON1Dc4JAAhHB1pZcrkNlrHmaicfwCp52sBHArXPy_cxmrNICv_nS5K3dYTShdkcwXt636Q5W7HYtQFQ1WkwMjtFd6Ju-yBimpwJmB0JG2YQGuDeE1He3ayX.jpg?r=d8a\" alt=\"Copyandedit\" width=\"200\" height=\"200\" style=\"float:left\">","3ad12a53":"### 2.2.1. Visualising the null values","8a624922":"# 2. Importing dataset","20360b86":"<h4> Studying the IMDB score against Rating <\/h4>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/IMDB_Logo_2016.svg\" alt=\"Copyandedit\" width=\"100\" height=\"100\" style=\"float:left\">","f8d42ecf":"# Table of Contents\n\n* 1. Importing libs\n* 2. Importing data\n* 3. EDA","c8041b45":"### 2.2.2 Identifying datatype","50ba4f22":"The box plot illustrates the outliers of the dataset regarding the IMDB score","0d6f6222":"## 2.3. Feature engineering","848efe62":"<h4>From the histograms and count plots, we observe that the US has made most of the movies and series on Netflix. And the distribution of the IMDb score is almost normal with the calculated skewness. The time duration and number of TV shows against the series are also studies.<\/h4>","cd5cb3c0":"First of all, I am going to explore each feature separately to know the dataset better.\n****\nFor the each feature, I am using a different method to modify them and to have a more clean notebook, I do not print the modified feature","19f72c6c":"<h4>Studying the IMDB Score<\/h4>\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/IMDB_Logo_2016.svg\" alt=\"Copyandedit\" width=\"100\" height=\"100\" style=\"float:left\">","ebfd6841":"## 2.4. Statistical report","c0b8a4c2":"> Due to the high dimension of the dataset, I am analyzing the random sample of the collection","31d69400":"> The Highest rate of IMDB goes to Ireland, United States, United Kingdom","15fa8965":"## 2.2. Missing values","60e6afe0":"<h2>About this notebook<\/h2>\n\n<hr>\n<h4> Author: Seyedsaman Emami <\/h4>\n<h5> Release Date: 18.Jul.2021 <\/h5>\n<hr>\n\nIn the following notebook, I am going to have Exploratory Data Analysis or EDA.\n\n<img src=\"https:\/\/www.eluniverso.com\/resizer\/NKAfafwbiZpxBwpQEe6Thfxj-r8=\/1198x627\/smart\/filters:quality(70)\/cloudfront-us-east-1.images.arcpublishing.com\/eluniverso\/DKREWTEQUFG4RO52TEE6ILEK6Y.png\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\n\n<h4>Each calculation and analysis has a comprehensive description, and each part has a summary and a brief conclusion <\/h4>\n\n**<span style='color:red'> Implemented methods <\/span>**\n\nSome of the implemented methods in this notebook are as follows;\n<ul>\n    <li> Grouping dataset<\/li>\n    <li> Pivoting<\/li>\n    <li> Reviewing the missing values <\/li>\n    <li> Imputation <\/li>\n    <li> Changing data type<\/li>\n    <li> Statistical distributions<\/li>\n    <li> Outlier detection<\/li>\n    <li> wordcloud<\/li>   \n<\/ul>\n\n<h5>If you are interested in this problem and detailed analysis, you can copy this Notebook as follows<\/h5>\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\n\n\n::: Updates :::\n* Last update: 18.Jul.2021\n* Updates: \n    * Creating the notebook\n\n","6950023d":"> As the above table and following plot illustrates, there are too many null values in this dataset","8d1d2959":"## 2.1. Dataset overview","897ac098":"### 2.1.1. Data Info\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/434238\/824878\/30c0ef57882454a0419a348088aa2306\/dataset-card.jpg?t=2019-12-04-06-00-44\" alt=\"Copyandedit\" width=\"100\" height=\"100\" style=\"float:left\">"}}