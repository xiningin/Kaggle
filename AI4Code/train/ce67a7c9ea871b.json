{"cell_type":{"2beb2544":"code","4517510b":"code","9d1223ff":"code","423a225f":"code","1e9ea757":"code","872b656c":"code","c21dd285":"code","7dcc2075":"code","598c4751":"code","81adac28":"code","ab4f5844":"code","dff1f710":"code","356661e5":"code","005e90e2":"code","02917ce3":"code","a75ab63d":"code","0396593b":"code","cdbc93ea":"code","943c5bf3":"code","8742636d":"code","3c99804b":"code","99b5d40a":"code","d64d9076":"code","a152ad43":"code","f0296ecd":"code","c5df6e91":"code","c4865bf4":"code","e63c0eb4":"code","52c28445":"code","bf0924eb":"code","a107a742":"code","3f47d8d8":"code","4ee60662":"code","97f83e91":"code","93bae1c7":"code","8fd96f3c":"code","e1d5b7c3":"markdown","2ff5220c":"markdown","ef18bb73":"markdown","27588ade":"markdown","c5908b0b":"markdown","3e2553e2":"markdown","fc4ad256":"markdown","035ffd42":"markdown","b301f6c1":"markdown","f913c6ac":"markdown","22a11dec":"markdown","8872f80d":"markdown","1aa86538":"markdown","8588921b":"markdown","a1c0b635":"markdown","d63cb99e":"markdown","a4d508dd":"markdown"},"source":{"2beb2544":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport itertools\n\n\nimport warnings\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","4517510b":"train = pd.read_csv('..\/input\/demandforecasting\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demandforecasting\/test.csv', parse_dates=['date'])\ndf = pd.concat([train, test], sort=False) # veri \u00f6ni\u015fleme i\u00e7in test ve train bir araya getirdim\ndf.head()","9d1223ff":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n","423a225f":"check_df(df)","1e9ea757":"df[\"date\"].min()","872b656c":"df[\"date\"].max()","c21dd285":"# Sat\u0131\u015f da\u011f\u0131l\u0131m\u0131 nas\u0131l?\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","7dcc2075":"# Ka\u00e7 store var?\ndf[[\"store\"]].nunique()","598c4751":"# Ka\u00e7 item var?\ndf[[\"item\"]].nunique()","81adac28":"# Her store'da e\u015fit say\u0131da m\u0131 e\u015fsiz item var?\ndf.groupby([\"store\"])[\"item\"].nunique()","ab4f5844":"# Peki her store'da e\u015fit say\u0131da m\u0131 sales var?\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})","dff1f710":"# ma\u011faza-item k\u0131r\u0131l\u0131m\u0131nda sat\u0131\u015f istatistikleri\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","356661e5":"########################\n# Date Features\n########################\n\n\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df","005e90e2":"df = create_date_features(df)\n","02917ce3":"df.head()","a75ab63d":"# store-item-month k\u0131r\u0131l\u0131m\u0131nda sat\u0131\u015f istatistikleri\ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","0396593b":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))","cdbc93ea":"# buradaki feature s\u0131ralamas\u0131 \u00f6nemli, \u00f6nce store, item, date olark belirledim\n# lagler ge\u00e7mi\u015f de\u011ferleri ifade ediyor\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)","943c5bf3":"def lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe","8742636d":"# bir ay bir g\u00fcn \u00f6ncesi 91, bir ay bir hafta \u00f6ncesi 98, iki hafta \u00f6ncesi 105\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","3c99804b":"# Rolling Mean Features\n########################\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\ndf = roll_mean_features(df, [365, 546])\ndf.tail()","99b5d40a":"# Exponentially Weighted Mean Features\n########################\n# burada ka\u00e7 g\u00fcn \u00f6ncesine gideyim, 95 g\u00fcn m\u00fc, 98 g\u00fcn m\u00fc gibi ve\n# a\u011f\u0131rlakland\u0131rmay\u0131 nas\u0131l yapmal\u0131y\u0131m gibi\n# fonksiyonumuzu tan\u0131mlayal\u0131m ilk ba\u015fta;\n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\ndf.tail()","d64d9076":"\n# veri setinde kategorik de\u011fi\u015fkenler vard\u0131. bunlar i\u00e7in dummy de\u011fi\u015fkeni olu\u015fturuyorum\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])\n","a152ad43":"# Converting sales to log(1+sales)\n########################\ndf['sales'] = np.log1p(df[\"sales\"].values)","f0296ecd":"# Custom Cost Function\n########################\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","c5df6e91":"# Time-Based Validation Sets\n########################\ntest\n\n# 2017'nin ba\u015f\u0131na kadar (2016'n\u0131n sonuna kadar) train seti.\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# validasyon 2017 nin ilk \u00fc\u00e7 ay\u0131\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\n# ba\u011f\u0131ms\u0131z de\u011fi\u015fkenleri \u00e7\u0131kartt\u0131m\n# \u015fu an test setiyle ilgilenmiyorum\n# \u015fu an sadece trainle ilgileniyorum\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]","c4865bf4":"# train seti i\u00e7in ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin se\u00e7ilmesi\nY_train = train['sales']\n\n# train seti i\u00e7in ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin se\u00e7ilmesi\nX_train = train[cols]\n\n# validasyon seti i\u00e7in ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin se\u00e7ilmesi\nY_val = val['sales']\n\n# validasyon seti i\u00e7in ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin se\u00e7ilmesi\nX_val = val[cols] \n\n# kontrol\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape\n\n# train seti i\u00e7erisinde 730500 g\u00f6zlem var\n# test seti (y_val) i\u00e7erisinde 45000 g\u00f6zlem var.\n# y_valisdayosnu 2018 deki test setimize banzettik","e63c0eb4":"# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000, # light gbm en \u00f6nemli parametris boost say\u0131s\u0131, iterasyon say\u0131s\u0131\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain, # parametreler\n                  valid_sets=[lgbtrain, lgbval], # datalar\u0131m\u0131z\n                  num_boost_round=lgb_params['num_boost_round'], # yukar\u0131daki fonksiyonda \u00e7ektik\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'], # yukar\u0131daki fonksiyonda \u00e7ektik\n                  feval=lgbm_smape,\n                  verbose_eval=100) # her 100 iterasyonda rapor ver\n\n\n# en iyi iterasyon say\u0131s\u0131n\u0131 model.best_iterationla giriyoruz\n# sadece model.best_iterationu \u00e7al\u0131\u015ft\u0131rd\u0131\u011f\u0131m\u0131zda bize en iyi de\u011feri g\u00f6steriyor\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n","52c28445":"# en iyi iterasyon say\u0131s\u0131n\u0131 model.best_iterationla giriyoruz\n# sadece model.best_iterationu \u00e7al\u0131\u015ft\u0131rd\u0131\u011f\u0131m\u0131zda bize en iyi de\u011feri g\u00f6steriyor\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)","bf0924eb":"# burada d\u00f6n\u00fc\u015ft\u00fcrme i\u015flemi yapt\u0131k.\n# daha \u00f6nce standartla\u015ft\u0131rma i\u015flemi yap\u0131m\u0131\u015ft\u0131k. bunu gezi almam\u0131z gerekiyor\n# yukar\u0131da yapm\u0131\u015f oldu\u011fumuz logaritmik d\u00f6n\u00fc\u015f\u00fcm\u00fc geri almak i\u00e7in\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","a107a742":"########################\n# De\u011fi\u015fken \u00f6nem d\u00fczeyleri\n########################\ndef plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\n\nplot_lgb_importances(model, num=30)\nplot_lgb_importances(model, num=30, plot=True)\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()","3f47d8d8":"# Final Model\n########################\ntrain = df.loc[~df.sales.isna()] # sales de\u011fi\u015fkeninde eskik de\u011fer sahip olmayanlar\u0131 getiriyoruz\nY_train = train['sales'] # y ba\u011f\u0131ms\u0131z de\u011fi\u015fkeni\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()] # df i\u00e7erisindeki na olan salesleri se\u00e7tik\nX_test = test[cols]  #  test setinin ba\u011f\u0131msz\u0131 de\u011fi\u015fkenlerini se\u00e7tik,\n# test setinin ba\u011f\u0131ml\u0131 de\u011fi\u015fkeni yokki zaten bo\u015f\n\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\n\n# LightGBM dataset\n# train setini olu\u015fturuyoruz tekrar\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n# modeli kuraca\u011f\u0131m\n# lgbtrain_all, parametreler, be best iterasyonda modelde\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\n# test setini de tahmin edece\u011fiz\n# test stinin ba\u011f\u0131ml\u0131 de\u011fi\u015fkenini ahmin edece\u011fiz, daha sonra bunlar test_preds olarak\n# bunu da daha sonra kaggle g\u00f6ndermemiz laz\u0131m\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)\n\n","4ee60662":"test.head()","97f83e91":"# KAGGLE SUBMISSION\n# test setinin id ve sales de\u011fi\u015fkenlerini submission_df olarak olu\u015fturuyorum\nsubmission_df = test.loc[:, ['id', 'sales']]\n# standartl\u015fat\u0131rma i\u015flemi yap\u0131m\u0131\u015ft\u0131k bunlar\u0131 geri al\u0131yoruz,\n# submission_df'in i\u00e7ine sale olarak at\u0131yorum\nsubmission_df['sales'] = np.expm1(test_preds)\n# idler k\u00fcs\u00fcratl\u0131 geldi\u011fi i\u00e7in integera \u00e7evirdik\nsubmission_df['id'] = submission_df.id.astype(int)\n# submission dosyas\u0131n\u0131 kaggle i\u00e7in \u00e7\u0131kart\u0131yorum\nsubmission_df.to_csv('submission_demand.csv', index=False)\n# buradaki idler kagglen bekledi\u011fi formatta, yani\n# s\u0131f\u0131r\u0131nc\u0131 indeks; ma\u011faza birin, birinci \u00fcr\u00fcn\u00fcn\u00fcn, birinci aydaki tahminini ifade ediyor\n# birinci indeks: ma\u011faza birin, ikinci \u00fcr\u00fcn\u00fcn\u00fcn, birinci aydaki tahmini ifade ediyor\nsubmission_df.head(10)","93bae1c7":"# e\u011fer orijisanl test setine tahminlerimizi eklemek istersek\n# bunun i\u00e7in orjinal test setini tekrar \u00e7a\u011f\u0131ral\u0131m\ntest_orginal = pd.read_csv('..\/input\/demandforecasting\/test.csv', parse_dates=['date'])\ntest_orginal.head()","8fd96f3c":"# orjinal test seti ile tahmin edilen sat\u0131\u015f de\u011ferlerini birle\u015ftiriyoruz\npd.merge(test_orginal, submission_df, how = \"inner\", on = \"id\")","e1d5b7c3":"<a id = \"4\"><\/a><br>\n## 3.1 Random Noise","2ff5220c":"<a id = \"2\"><\/a><br>\n# 2. Load and Check Data","ef18bb73":"<a id = \"5\"><\/a><br>\n## 3.2 Lag\/Shifted Features","27588ade":"<a id = \"8\"><\/a><br>\n# 4. LightGBM Model\n<a id = \"9\"><\/a><br>\n## 4.1. One-Hot Encoding","c5908b0b":"<a id = \"16\"><\/a><br>\n# 5. Submission","3e2553e2":"<a id = \"17\"><\/a><br>\n# 6. References\n* https:\/\/github.com\/mvahit\n* https:\/\/www.veribilimiokulu.com\/\n* https:\/\/www.kaggle.com\/haticeebraralc","fc4ad256":"<a id = \"1\"><\/a><br>\n# 1. Libraries","035ffd42":"# Introduction\nThis competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.\n\nYou are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n\n\n\n<font color = 'blue'>\nContent:\n    \n1. [Libraries](#1)\n1. [Load and Check Data](#2)\n1. [Feature Engineering](#3)\n    * [3.1. Random Noise](#4)\n    * [3.2. Lag\/Shifted Features ](#5)\n    * [3.3. Rolling Mean Features ](#6)    \n    * [3.4. Exponentially Weighted Mean Features ](#7)\n1. [LightGBM Model](#8)\n    * [4.1. One-Hot Encoding](#9)\n    * [4.2. Converting sales to log(1+sales)](#10)\n    * [4.3. Custom Cost Function](#11) \n    * [4.4. Time-Based Validation Sets](#12)\n    * [4.5. LightGBM Parameters](#13)\n    * [4.6. Feature Importance](#14)\n    * [4.7. Final Model](#15)\n1. [Kaggle Submission](#16)\n1. [References](#17)\n    \n   ","b301f6c1":"<a id = \"15\"><\/a><br>\n## 4.7. Final Model","f913c6ac":"<a id = \"12\"><\/a><br>\n## 4.4. Time-Based Validation Sets","22a11dec":"<a id = \"7\"><\/a><br>\n## 3.4 Exponentially Weighted Mean Features","8872f80d":"<a id = \"11\"><\/a><br>\n## 4.3. Custom Cost Function","1aa86538":"<a id = \"13\"><\/a><br>\n## 4.5. LightGBM Parameters","8588921b":"<a id = \"14\"><\/a><br>\n## 4.6. Feature Importance","a1c0b635":"<a id = \"6\"><\/a><br>\n## 3.3 Rolling Mean Features","d63cb99e":"<a id = \"3\"><\/a><br>\n# 3. Feature Engineering","a4d508dd":"<a id = \"10\"><\/a><br>\n## 4.2. Converting Sales to Log (1+sales)"}}