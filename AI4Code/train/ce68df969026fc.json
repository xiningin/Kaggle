{"cell_type":{"68bf3c87":"code","f80126ed":"code","9e4d94b5":"code","022b44f9":"code","3e7a0652":"code","86e0e22e":"code","443bea96":"code","b85ec6d9":"code","9183c459":"code","229e8107":"code","aeb13b60":"code","b81c1f23":"code","624fc4c6":"code","d8c129a1":"code","fca3dfae":"code","095baef7":"code","171c7369":"code","e3d63846":"code","9a80032c":"code","8d918c91":"code","7b0e8ffd":"code","ecc60434":"code","01c2f5ef":"code","939387c8":"code","de62e200":"code","bc3e5259":"code","79d46aa6":"code","8df7024c":"code","7131bf96":"code","3e553c0c":"code","d342ef98":"code","3c8369e6":"code","878488e8":"code","0cb66c95":"code","26c22d8e":"code","48dc7085":"code","38ea5e3f":"code","bc9b1361":"code","00b9c8f2":"code","280f8085":"code","8ca76f92":"code","128e71ec":"code","cb47363b":"code","b7a7b4e5":"code","36b38ff4":"code","c47a8c5e":"code","588a57eb":"code","10428561":"code","f19bf704":"code","38006a7a":"code","b078cab8":"code","e1ec6165":"code","c1f636b4":"code","7c52644f":"code","25222914":"markdown","bb3cc38c":"markdown","9a7d3cdc":"markdown","2f363f28":"markdown","126634f0":"markdown","d8046206":"markdown","8f342f46":"markdown","d8eb3c7a":"markdown","64632c63":"markdown","eb420809":"markdown","d26c6f59":"markdown","014c001c":"markdown","1cc11221":"markdown","df407285":"markdown","d05a433b":"markdown","065469f3":"markdown","cbfbc021":"markdown","79cc4f42":"markdown","2834eddc":"markdown","923f5bfd":"markdown","a04d2cb3":"markdown","5a0a3ee7":"markdown"},"source":{"68bf3c87":"#### Importing libraries\nimport re\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport sklearn.metrics as metrics\nfrom nltk.stem import SnowballStemmer\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n#### Ignore all warnings\nwarnings.filterwarnings('ignore')\n#### allow plots to appear directly in the notebook\n%matplotlib inline\n#### set the maximum column width to unlimited\npd.set_option('display.max_colwidth', None)","f80126ed":"#### loading train dataSet as pandas dataFrame\ntrain_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\n#### Let\u2019s take a look at the top five rows of train dataset using the DataFrame\u2019s head() method\ntrain_data.head()","9e4d94b5":"#### loading test dataSet as pandas dataFrame\ntest_data  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\n#### Let\u2019s take a look at the top five rows of test dataset using the DataFrame\u2019s head() method\ntest_data.head()","022b44f9":"#### checking dimensionality of the train and test dataSet\n#### checking dimensionality of the train and test dataSet\nrows_train, cols_train = train_data.shape\nprint('\\033[1m' + \"Train Dataset :\" + '\\033[0m')\nprint(f'\\tThe no.of rows = {rows_train}')\nprint(f'\\tThe no.of columns = {cols_train}')\nprint('-'*50)\nrows_test, cols_test = test_data.shape\nprint('\\033[1m' + \"Train Dataset :\" + '\\033[0m')\nprint(f'\\tThe no.of rows = {rows_test}')\nprint(f'\\tThe no.of columns = {cols_test}')","3e7a0652":"#### Checking for sum of null values present in both train and test datset.\nprint('\\033[1m' + \"Sum of NULL values in Train dataSet:\" + '\\033[0m')\nprint(train_data.isnull().sum())\nprint('-'*20)\nprint('\\033[1m' + \"Sum of NULL values in Test dataSet:\" + '\\033[0m')\nprint(test_data.isnull().sum())","86e0e22e":"#### Dropping 'keyword' and 'location' columns in both train and test dataset\ntrain_data.drop(['keyword', 'location'], axis=1, inplace = True)\n\ntest_data.drop(['keyword', 'location'], axis=1, inplace = True)","443bea96":"ax = sns.countplot(train_data['target'])\nfor p in ax.patches:\n    ax.annotate('{:.3f}%'.format(p.get_height()\/train_data.shape[0]*100), (p.get_x()+0.25, p.get_height()\/2))","b85ec6d9":"#### Histogram for number of characters in tweets\nplt.figure(figsize=[15, 5])\n\nplt.suptitle('No of Characters in tweets', fontsize=20, fontweight='bold')\nplt.subplot(1, 2, 1)\nplt.title('Disaster Tweets')\nplt.hist(train_data[train_data['target']==0]['text'].str.len())\n\nplt.subplot(1, 2, 2)\nplt.title('Not Disaster Tweets')\nplt.hist(train_data[train_data['target']==1]['text'].str.len(), color='green')\nplt.show()","9183c459":"#### Histogram for number of words in tweets\nplt.figure(figsize=[15, 5])\n\nplt.suptitle('No of Words in tweets', fontsize=20, fontweight='bold')\nplt.subplot(1, 2, 1)\nplt.title('Disaster Tweets')\nplt.hist(train_data[train_data['target']==0]['text'].str.split().str.len())\n\nplt.subplot(1, 2, 2)\nplt.title('Not Disaster Tweets')\nplt.hist(train_data[train_data['target']==1]['text'].str.split().str.len(), color='green')\nplt.show()","229e8107":"#### Word Cloud for words in train dataset\nplt.figure(figsize=[16, 5])\nplt.suptitle('Word Cloud', fontsize=20, fontweight='bold')\nplt.subplot(1, 2, 1)\ntrain_text_combined = \" \".join(train_data['text'].str.strip().values)\nword_cloud = WordCloud(width = 700, \n                       height = 500,\n                       max_words = 150).generate(train_text_combined)\nplt.title('Train Dataset')\nplt.imshow(word_cloud)\nplt.axis(\"off\")\n\n#### Word Cloud for words in test dataset\nplt.subplot(1, 2, 2)\ntest_text_combined = \" \".join(test_data['text'].str.strip().values)\ntest_word_cloud = WordCloud(width = 700, \n                       height = 500,\n                       max_words = 150).generate(test_text_combined)\nplt.title('Test Dataset')\nplt.imshow(test_word_cloud)\nplt.axis(\"off\")\nplt.show()","aeb13b60":"#### Frequency Distribution plot for words in train dataset\nall_terms = train_text_combined.strip().split(\" \")\nfdist = FreqDist(all_terms)\ndf_dist = pd.DataFrame(fdist.items(), columns = [\"words\", \"freq\"]).sort_values(ascending = False, by = \"freq\").head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(df_dist['words'], df_dist['freq'], palette='Blues_r')\nplt.title('Frequency Distribution on Train Dataset', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n#### Frequency Distribution plot for words in test dataset\nall_terms = test_text_combined.strip().split(\" \")\nfdist = FreqDist(all_terms)\ndf_dist = pd.DataFrame(fdist.items(), columns = [\"words\", \"freq\"]).sort_values(ascending = False, by = \"freq\").head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(df_dist['words'], df_dist['freq'], palette='Greens_r')\nplt.title('Frequency Distribution on Test Dataset', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","b81c1f23":"#### No of words before cleaning\nprint(f'The number of words in train dataset = {len(train_text_combined.split())}')\nprint(f'The number of words in test dataset = {len(test_text_combined.split())}')","624fc4c6":"#### Removing all non alpha numeric and space characters\n# On train Datset\ntrain_reviews_combined_clean = re.sub(\"[^\\w\\s]+\", \"\", train_text_combined)\ntrain_all_terms_clean1 = word_tokenize(train_reviews_combined_clean.lower())\nprint(f'The number of distinct words after removing non alpha numeric and space characters in train dataset = {len(set(train_all_terms_clean1))}')\n# On test Datset\ntest_reviews_combined_clean = re.sub(\"[^\\w\\s]+\", \"\", test_text_combined)\ntest_all_terms_clean1 = word_tokenize(test_reviews_combined_clean.lower())\nprint(f'The number of distinct words after removing non alpha numeric and space characters in test dataset = {len(set(test_all_terms_clean1))}')","d8c129a1":"#### Removing Stopwords words\nstop_nltk = stopwords.words('english')\n# On train Datset\ntrain_all_terms_final_clean_stop=[term for term in train_all_terms_clean1 if term not in stop_nltk and len(term)>2]\nprint(f'The number of distinct words after removing stop words in train dataset = {len(set(train_all_terms_final_clean_stop))}')\n# On test Datset\ntest_all_terms_final_clean_stop=[term for term in test_all_terms_clean1 if term not in stop_nltk and len(term)>2]\nprint(f'The number of distinct words after removing stop words in test dataset = {len(set(test_all_terms_final_clean_stop))}')","fca3dfae":"#### Applying stemming to get root form of words\ns_stemmer = SnowballStemmer(language = 'english')\n# On train Datset\ntrain_all_terms_final_clean_stem = [s_stemmer.stem(term) for term in train_all_terms_final_clean_stop]\nprint(f'The number of distinct words after applying stemming in train dataset = {len(set(train_all_terms_final_clean_stem))}')\n# On test Datset\ntest_all_terms_final_clean_stem = [s_stemmer.stem(term) for term in test_all_terms_final_clean_stop]\nprint(f'The number of distinct words after applying stemming in test dataset = {len(set(test_all_terms_final_clean_stem))}')","095baef7":"#### Applying Lemmatization to getting root form of words\nlem = WordNetLemmatizer()\n# On train Datset\ntrain_all_terms_final_clean_lemm = [lem.lemmatize(word,pos='v') for word in train_all_terms_final_clean_stop]\nprint(f'The number of distinct words after applying lemmatization in train dataset = {len(set(train_all_terms_final_clean_lemm))}')\n# On test Datset\ntest_all_terms_final_clean_lemm = [lem.lemmatize(word,pos='v') for word in test_all_terms_final_clean_stop]\nprint(f'The number of distinct words after applying lemmatization in test dataset = {len(set(test_all_terms_final_clean_lemm))}')","171c7369":"#### Defining user defined function for text cleaning\ndef clean_txt(sent):\n    # Stripping white spaces before and after the text\n    sent = sent.strip()\n    # Replacing multiple spaces with a single space\n    result = re.sub(\"\\s+\", \" \", sent)\n    # Replacing Non-Alpha-numeric and non space charecters with nothing\n    result1 = re.sub(\"[^\\w\\s]+\", \"\", result)\n    # Normalize case, steam and remove shorter tokens\n    tokens = word_tokenize(result1.lower())\n    stemmed = [s_stemmer.stem(term) for term in tokens if term not in stop_nltk and len(term) > 2]\n    # Join all to form a single string which will be returned from the UDF\n    res = \" \".join(stemmed)\n    return(res)","e3d63846":"#### Applying user defined function clean_txt on train dataset\ntrain_data['clean_text'] = train_data['text'].apply(clean_txt)\ntrain_data.head()","9a80032c":"#### Applying user defined function clean_txt on test dataset\ntest_data['clean_text'] = test_data['text'].apply(clean_txt)\ntest_data.head()","8d918c91":"#### Word Cloud for words in train dataset after text cleaning\nplt.figure(figsize=[16, 5])\nplt.suptitle('Word Cloud', fontsize=20, fontweight='bold')\nplt.subplot(1, 2, 1)\ntrain_text_combined = \" \".join(train_data['clean_text'].str.strip().values)\nword_cloud = WordCloud(width = 700, \n                       height = 500,\n                       max_words = 150).generate(train_text_combined)\nplt.title('Train Dataset')\nplt.imshow(word_cloud)\nplt.axis(\"off\")\n\n#### Word Cloud for words in test dataset after text cleaning\nplt.subplot(1, 2, 2)\ntest_text_combined = \" \".join(test_data['clean_text'].str.strip().values)\ntest_word_cloud = WordCloud(width = 700, \n                       height = 500,\n                       max_words = 150).generate(test_text_combined)\nplt.title('Test Dataset')\nplt.imshow(test_word_cloud)\nplt.axis(\"off\")\nplt.show()","7b0e8ffd":"#### Frequency Distribution plot for words in train dataset\nall_terms = train_text_combined.strip().split(\" \")\nfdist = FreqDist(all_terms)\ndf_dist = pd.DataFrame(fdist.items(), columns = [\"words\", \"freq\"]).sort_values(ascending = False, by = \"freq\").head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(df_dist['words'], df_dist['freq'], palette='Blues_r')\nplt.title('Frequency Distribution on Train Dataset', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n#### Frequency Distribution plot for words in test dataset\nall_terms = test_text_combined.strip().split(\" \")\nfdist = FreqDist(all_terms)\ndf_dist = pd.DataFrame(fdist.items(), columns = [\"words\", \"freq\"]).sort_values(ascending = False, by = \"freq\").head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(df_dist['words'], df_dist['freq'], palette='Greens_r')\nplt.title('Frequency Distribution on Test Dataset', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","ecc60434":"#### Bigram Analysis using Count Vectorizer\n# On train Datset\ncount_vect_bg = CountVectorizer(ngram_range = (2, 2), max_features = 150)\nX_bg = count_vect_bg.fit_transform(train_data['clean_text'])\nDTM_bg = pd.DataFrame(X_bg.toarray(), columns = count_vect_bg.get_feature_names()).sum().sort_values(ascending = False)\nDTM_bg = pd.DataFrame(DTM_bg, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_bg.index, DTM_bg['freq'], palette='Blues_r')\nplt.title('Bigram Analysis on Train Dataset using Count Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n# On test Datset\ncount_vect_bg = CountVectorizer(ngram_range = (2, 2), max_features = 150)\nX_bg = count_vect_bg.fit_transform(test_data['clean_text'])\nDTM_bg = pd.DataFrame(X_bg.toarray(), columns = count_vect_bg.get_feature_names()).sum().sort_values(ascending = False)\nDTM_bg = pd.DataFrame(DTM_bg, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_bg.index, DTM_bg['freq'], palette='Greens_r')\nplt.title('Bigram Analysis on Tesr Dataset using Count Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","01c2f5ef":"#### Bigram Analysis using TFIDF Vectorizer\n# On train Datset\ntf = TfidfVectorizer(ngram_range=(2, 2), max_features = 150)\nx = tf.fit_transform(train_data['clean_text'])\nDTM_tf = pd.DataFrame(x.toarray(),columns=tf.get_feature_names()).sum().sort_values(ascending = False).head(25)\nDTM_tf = pd.DataFrame(DTM_tf, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_tf.index, DTM_tf['freq'], palette='Blues_r')\nplt.title('Bigram Analysis on Train Dataset using TFIDT Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n# On test Datset\ntf = TfidfVectorizer(ngram_range=(2, 2), max_features = 150)\nx = tf.fit_transform(test_data['clean_text'])\nDTM_tf = pd.DataFrame(x.toarray(),columns=tf.get_feature_names()).sum().sort_values(ascending = False).head(25)\nDTM_tf = pd.DataFrame(DTM_tf, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_tf.index, DTM_tf['freq'], palette='Greens_r')\nplt.title('Bigram Analysis on Test Dataset using TFIDT Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","939387c8":"#### Trigram Analysis using Count Vectorizer\n# On train Datset\ncount_vect_bg = CountVectorizer(ngram_range = (3, 3), max_features = 150)\nX_bg = count_vect_bg.fit_transform(train_data['clean_text'])\nDTM_bg = pd.DataFrame(X_bg.toarray(), columns = count_vect_bg.get_feature_names()).sum().sort_values(ascending = False)\nDTM_bg = pd.DataFrame(DTM_bg, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_bg.index, DTM_bg['freq'], palette='Blues_r')\nplt.title('Trigram Analysis on Train Dataset using Count Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n# On test Datset\ncount_vect_bg = CountVectorizer(ngram_range = (3, 3), max_features = 150)\nX_bg = count_vect_bg.fit_transform(test_data['clean_text'])\nDTM_bg = pd.DataFrame(X_bg.toarray(), columns = count_vect_bg.get_feature_names()).sum().sort_values(ascending = False)\nDTM_bg = pd.DataFrame(DTM_bg, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_bg.index, DTM_bg['freq'], palette='Greens_r')\nplt.title('Trigram Analysis on Tesr Dataset using Count Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","de62e200":"#### Trigram Analysis using TFIDF Vectorizer\n# On train Datset\ntf = TfidfVectorizer(ngram_range=(3, 3), max_features = 150)\nx = tf.fit_transform(train_data['clean_text'])\nDTM_tf = pd.DataFrame(x.toarray(),columns=tf.get_feature_names()).sum().sort_values(ascending = False).head(25)\nDTM_tf = pd.DataFrame(DTM_tf, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_tf.index, DTM_tf['freq'], palette='Blues_r')\nplt.title('Trigram Analysis on Train Dataset using TFIDT Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\n\n# On test Datset\ntf = TfidfVectorizer(ngram_range=(3, 3), max_features = 150)\nx = tf.fit_transform(test_data['clean_text'])\nDTM_tf = pd.DataFrame(x.toarray(),columns=tf.get_feature_names()).sum().sort_values(ascending = False).head(25)\nDTM_tf = pd.DataFrame(DTM_tf, columns = [\"freq\"]).head(25)\nplt.figure(figsize=(20, 5))\nsns.barplot(DTM_tf.index, DTM_tf['freq'], palette='Greens_r')\nplt.title('Trigram Analysis on Test Dataset using TFIDT Vectorizer', fontsize=20, fontweight='bold')\nplt.xticks(rotation=90)\nplt.show()","bc3e5259":"#### Get the X and y\nX = train_data['text'].values\ny = train_data['target'].values\n\n#### Split into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)\n\n#### Converting text to numbers\ntfidf_vectorizer = TfidfVectorizer()\n\n#### Extract the features which are only going to be used for training\n#### Apply the transformation on those extracted features\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n#### For the words used for training, apply the transformation on the test set\n#### If there are new words in the test set, we are just ignoring them\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n#### Printing the shape of the test and train datase\nprint(f'The shape of the train dataset = {X_train_tfidf.shape}')\nprint(f'The shape of the test dataset = {X_test_tfidf.shape}')","79d46aa6":"#### Defining User Defined Function for Classification Results\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\ndef classification_results(model, xR, yR, ON):\n    #### Predicting train\n    yR_predict = model.predict(xR)\n\n    #### Calculating accuracy score for train\n    acc = accuracy_score(yR, yR_predict)\n    print('\\033[1m' + ON + ':' +'\\033[0m')\n    print(f'Accuracy : {acc}')\n    print('-'*55)\n\n    #### Classification report for train\n    CR = classification_report(yR, yR_predict)\n    print(\"classification report : \\n\", CR)\n    print('-'*55)\n\n    #### Confusion matrix for train\n    CM = confusion_matrix(yR, yR_predict)\n    print(\"Confusion matrix :\\n\", CM)\n    sns.heatmap(CM, center = True, annot = True, fmt = 'g')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    return(yR_predict, acc)","8df7024c":"#### Defining User Defined Function for ROC curve\nimport sklearn.metrics as metrics\n\ndef ROC_Curve(y_train, y_train_predict, y_test, y_test_predict):\n    x,y = np.arange(0,1.1,0.1),np.arange(0,1.1,0.1)\n    plt.plot(x, y, '--')\n\n    #### Plot for train\n    fpr_train, tpr_train, thresholds = metrics.roc_curve(y_train, y_train_predict)\n    roc_auc_train = metrics.auc(fpr_train, tpr_train)\n    plt.plot(fpr_train, tpr_train, marker='o', label = 'Train AUC = %0.3f' % roc_auc_train)\n\n    #### Plot for test\n    fpr_test, tpr_test ,thresholds = metrics.roc_curve(y_test, y_test_predict)\n    roc_auc_test = metrics.auc(fpr_test, tpr_test)\n    plt.plot(fpr_test, tpr_test, marker='o', label = 'Test AUC = %0.3f' % roc_auc_test)\n    plt.legend(loc = 'lower right')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    return(roc_auc_train, roc_auc_test)","7131bf96":"#### Logistic Model instantiate and fit\nfrom sklearn.linear_model import LogisticRegression\nmodel_LR = LogisticRegression()\nmodel_LR.fit(X_train_tfidf, y_train)\n\n#### Classification result on train dataset\ny_train_predict_LR, acc_train_LR = classification_results(model_LR, X_train_tfidf, y_train, 'TRAIN')","3e553c0c":"#### Classification result on train dataset\ny_test_predict_LR, acc_test_LR = classification_results(model_LR, X_test_tfidf, y_test, 'TEST')","d342ef98":"#### Plotting ROC Curve\nroc_auc_train_LR, roc_auc_test_LR = ROC_Curve(y_train, y_train_predict_LR, y_test, y_test_predict_LR)","3c8369e6":"final_results = []\ndict_LR = {'MODEL':'Logistic Regression',\n           'Train_ACCURACY' : acc_train_LR,\n           'Test_ACCURACY' : acc_test_LR,\n           'Train_AUC' : roc_auc_train_LR,\n           'Test_AUC' : roc_auc_test_LR\n          }\nfinal_results.append(dict_LR)","878488e8":"#### MultinomialNB Model instantiate and fit\nfrom sklearn.naive_bayes import MultinomialNB\nmodel_mnb = MultinomialNB() \nmodel_mnb.fit(X_train_tfidf, y_train)\n\n#### Classification Results on train dataset\ny_train_predict_mnb, acc_train_mnb = classification_results(model_mnb, X_train_tfidf, y_train, 'TRAIN')","0cb66c95":"#### Classification Results on test dataset\ny_test_predict_mnb, acc_test_mnb = classification_results(model_mnb, X_test_tfidf, y_test, 'TEST')","26c22d8e":"#### Plotting ROC Curve\nroc_auc_train_mnb, roc_auc_test_mnb = ROC_Curve(y_train, y_train_predict_mnb, y_test, y_test_predict_mnb)","48dc7085":"dict_mnb = {'MODEL' : 'MultinomialNB',\n           'Train_ACCURACY' : acc_train_mnb,\n           'Test_ACCURACY' : acc_test_mnb,\n           'Train_AUC' : roc_auc_train_mnb,\n           'Test_AUC' : roc_auc_test_mnb\n          }\nfinal_results.append(dict_mnb)","38ea5e3f":"#### Decision Tree Classifier Model instantiate and fit\nimport sklearn.tree as tree\nmodel_dtc=tree.DecisionTreeClassifier(max_depth=100, random_state=10)\nmodel_dtc.fit(X_train_tfidf, y_train)\n\n#### Classification Results on train\ny_train_predict_dtc, acc_train_dtc = classification_results(model_dtc, X_train_tfidf, y_train, 'TRAIN')","bc9b1361":"#### Classification Results on test dataset\ny_test_predict_dtc, acc_test_dtc = classification_results(model_dtc, X_test_tfidf, y_test, 'TEST')","00b9c8f2":"#### Plotting ROC Curve\nroc_auc_train_dtc, roc_auc_test_dtc = ROC_Curve(y_train, y_train_predict_dtc, y_test, y_test_predict_dtc)","280f8085":"dict_dtc = {'MODEL' : 'Decision Tree classifier',\n           'Train_ACCURACY' : acc_train_dtc,\n           'Test_ACCURACY' : acc_test_dtc,\n           'Train_AUC' : roc_auc_train_dtc,\n           'Test_AUC' : roc_auc_test_dtc\n          }\nfinal_results.append(dict_dtc)","8ca76f92":"#### KNN Classifier Model instantiate and fit\nfrom sklearn import neighbors as nb\nmodel_knn=nb.KNeighborsClassifier()\n\n#### Parameter tuning\/Grid search\nimport sklearn.model_selection as model_selection\nmodel_knn=model_selection.GridSearchCV(model_knn,\n                                       param_grid={'n_neighbors':[3,5,7],\n                                                   'weights':['uniform','distance']},\n                                      n_jobs = -1)\nmodel_knn.fit(X_train_tfidf, y_train)\n\n#### Classification Results on train\ny_train_predict_knn, acc_train_knn = classification_results(model_knn, X_train_tfidf, y_train, 'TRAIN')","128e71ec":"#### Classification Results on test dataset \ny_test_predict_knn, acc_test_knn = classification_results(model_knn, X_test_tfidf, y_test, 'TEST')","cb47363b":"#### Plotting ROC Curve\nroc_auc_train_knn, roc_auc_test_knn = ROC_Curve(y_train, y_train_predict_knn, y_test, y_test_predict_knn)","b7a7b4e5":"dict_knn = {'MODEL' : 'KNN classifier',\n           'Train_ACCURACY' : acc_train_knn,\n           'Test_ACCURACY' : acc_test_knn,\n           'Train_AUC' : roc_auc_train_knn,\n           'Test_AUC' : roc_auc_test_knn\n          }\nfinal_results.append(dict_knn)","36b38ff4":"#### Random Forest Classifier Model instantiate and fit\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_RFC=RandomForestClassifier()\nmodel_RFC.fit(X_train_tfidf, y_train)\n\n#### Classification Results on train\ny_train_predict_RFC, acc_train_RFC = classification_results(model_RFC, X_train_tfidf, y_train, 'TRAIN')","c47a8c5e":"#### Classification Results on test dataset \ny_test_predict_RFC, acc_test_RFC = classification_results(model_RFC, X_test_tfidf, y_test, 'TEST')","588a57eb":"#### Plotting ROC Curve\nroc_auc_train_RFC, roc_auc_test_RFC = ROC_Curve(y_train, y_train_predict_RFC, y_test, y_test_predict_RFC)","10428561":"dict_RFC = {'MODEL' : 'Random Forest classifier',\n           'Train_ACCURACY' : acc_train_RFC,\n           'Test_ACCURACY' : acc_test_RFC,\n           'Train_AUC' : roc_auc_train_RFC,\n           'Test_AUC' : roc_auc_test_RFC\n          }\nfinal_results.append(dict_RFC)","f19bf704":"#### XGBoost Classifierr Model instantiate and fit\nfrom xgboost import XGBClassifier\nmodel_XGBC = XGBClassifier()\nmodel_XGBC.fit(X_train_tfidf, y_train)\n\n#### Classification Results on train dataset\ny_train_predict_XGBC, acc_train_XGBC = classification_results(model_XGBC, X_train_tfidf, y_train, 'TRAIN')","38006a7a":"#### Classification Results on test dataset\ny_test_predict_XGBC, acc_test_XGBC = classification_results(model_XGBC, X_test_tfidf, y_test, 'test')","b078cab8":"#### Plotting ROC Curve\nroc_auc_train_XGBC, roc_auc_test_XGBC = ROC_Curve(y_train, y_train_predict_XGBC, y_test, y_test_predict_XGBC)","e1ec6165":"dict_XGBC = {'MODEL' : 'XGBoost Classifier',\n           'Train_ACCURACY' : acc_train_XGBC,\n           'Test_ACCURACY' : acc_test_XGBC,\n           'Train_AUC' : roc_auc_train_XGBC,\n           'Test_AUC' : roc_auc_test_XGBC\n          }\nfinal_results.append(dict_XGBC)","c1f636b4":"df_results = pd.DataFrame(final_results)\ndf_results['ACCURACY'] = ((df_results['Train_ACCURACY']+df_results['Test_ACCURACY'])\/2)*100\ndf_results['AUC'] = ((df_results['Train_AUC']+df_results['Test_AUC'])\/2)*100\n\n(df_results.sort_values(by=['ACCURACY','AUC'],ascending=False)\n    .reset_index(drop=True)\n    .style.background_gradient(cmap='Greens'))","7c52644f":"test_tfidf = tfidf_vectorizer.transform(test_data['text'].values)\n#### predicting test_data\ny_testData_Predict = model_mnb.predict(test_tfidf)\ndf_sub = pd.DataFrame()\ndf_sub['id'] = test_data['id']\ndf_sub['target'] = y_testData_Predict\n\ndf_sub.to_csv('submission.csv', header=True,index=False)","25222914":"## Predicting 'test_data' results using MultinominalNB Classifier","bb3cc38c":"## ngram Analysis\n### Bigram Analysis :","9a7d3cdc":"## Exploratory Data Analysis","2f363f28":"### Trigram","126634f0":"## Text Cleaning","d8046206":"### KNN Classifier","8f342f46":"### Logistic Regression","d8eb3c7a":"<h1 align='center'style='font-size:50px'>NLP with Disaster Tweets<\/h1>\n\n# Problem Statement:\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\n# Objective:\nTo understand the \"Titanic_dataset\" and build a machine learning model(Classification Model) predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n# Data Dictionary:\n**id** a unique identifier for each tweet\n<br>**keyword** a particular keyword from the tweet\n<br>**location** the location the tweet was sent from\n<br>**text** the text of the tweet\n<br>**target** denotes whether a tweet is about a real disaster (1) or not (0)","64632c63":"* **There are more tweets with class 0(No disaster) than class 1(disaster tweets)**","eb420809":"### Random Forest Classifier","d26c6f59":"## Import Libraries","014c001c":"* **since stemmer reduces the number of distinct words in the corpus, we will be using stemming for frequency based analysis**","1cc11221":"### Naive Bayes Classifier - MultinomialNB","df407285":"## Importing Dataset","d05a433b":"<h1 align='center'>Thank you\ud83d\ude42<\/h1>","065469f3":"## Sentiment Prediction Stategy\n### Building our own model based on the Sentiment labels\nApplying Supervised learning on labelled text data - In this case we will leverage the sentiment label which is avaliable along with the text data","cbfbc021":"## Comparing ACCURACY and AUC for all models","79cc4f42":"* **From the above it is observed that Random forest and knn clasifier has overfitting. So, multinominalNB is the best model among all.**","2834eddc":"* **The distribution of both seems to be same**","923f5bfd":"### Decision Tree Classifier","a04d2cb3":"* **From the above it is observed that null values present in both `keyword` and `location`**","5a0a3ee7":"### XGBoost Classifier"}}