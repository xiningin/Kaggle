{"cell_type":{"99a40c68":"code","40e2642e":"code","e4f19aa2":"code","8fd95f08":"code","308ac037":"code","34ba05af":"code","47e29f04":"code","17754d64":"code","56936e96":"code","bfa625f0":"code","37a8f85c":"code","9b35efeb":"code","aac95be6":"code","13dd14a1":"code","665234d9":"code","709b4cb3":"code","49443fd3":"code","92410619":"code","13cbf52b":"code","de3305f3":"markdown","6ca44123":"markdown","b26bc2f5":"markdown","fca1cb5c":"markdown","33057db3":"markdown","1244b4c8":"markdown","539eaa07":"markdown","802f9350":"markdown","7e286e72":"markdown","52cde6c6":"markdown","264d542d":"markdown","5d6b51fe":"markdown","578505c6":"markdown","c41421ad":"markdown","870d8b30":"markdown","195e8d45":"markdown","c2f6afbd":"markdown","194dfe11":"markdown","9dc99f77":"markdown","9d59c180":"markdown","e8be47ef":"markdown","81223912":"markdown","c06f1a1b":"markdown","99af22db":"markdown","e6412170":"markdown","f7f7f6a4":"markdown","9ebb0643":"markdown","ef05bbee":"markdown","08713929":"markdown","aff55388":"markdown","ca7fe179":"markdown","6db2cac7":"markdown","a554557e":"markdown","aa4140ae":"markdown","1e09813a":"markdown","40694933":"markdown","c1eb7769":"markdown","31f140c7":"markdown"},"source":{"99a40c68":"# Make                Car Make\n# Model               Car Model\n# Year                Car Year (Marketing)\n# Engine Fuel Type    Engine Fuel Type\n# Engine HP           Engine Horse Power (HP)\n# Engine Cylinders    Engine Cylinders\n# Transmission Type   Transmission Type\n# Driven_Wheels       Driven Wheels\n# Number of Doors     Number of Doors\n# Market Category     Market Category\n# Vehicle Size        Size of Vehicle\n# Vehicle Style       Type of Vehicle\n# highway MPG         Highway MPG\n# city mpg            City MPG\n# Popularity          Popularity (Twitter)\n# MSRP                Manufacturer Suggested Retail Price","40e2642e":"import warnings\nwarnings.filterwarnings('ignore')","e4f19aa2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import defaultdict\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection, ensemble\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import Imputer, LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nimport time\nimport shap","8fd95f08":"df_cleaned = pd.read_csv(\"..\/input\/cars_cleaned.csv\").drop('Unnamed: 0', axis=1)\ndf_cleaned.head(5)","308ac037":"# Check missing values\nlen(df_cleaned) - df_cleaned.count()","34ba05af":"d = defaultdict(LabelEncoder)\ncolumns_to_encode = [\n    \"Make\",\n    \"Model\",\n    \"Engine Fuel Type\",\n    \"Transmission Type\",\n    \"Driven_Wheels\",\n    \"Market Category\",\n    \"Transmission Type\",\n    \"Vehicle Size\",\n    \"Vehicle Style\"]\n\ndf_cleaned.loc[:,columns_to_encode] = df_cleaned.loc[:,columns_to_encode].apply(lambda x: d[x.name].fit_transform(x.fillna('0')))\ndf_cleaned.info()","47e29f04":"# GET DUMMIES, FEATURES SELECTION, PROPERLY CLEANED AND INPUTED DATAS\n\ndf_cleaned_dummies = df_cleaned.copy()\ndf_cleaned_dummies = pd.get_dummies(df_cleaned_dummies)\n\nX = df_cleaned_dummies.drop(['MSRP'], axis=1)\ny = df_cleaned_dummies['MSRP']","17754d64":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state = 42)\n\n# Need index of car prices to predict, whatever the strategy is\nindexes_cars_predicted = X_test.index","56936e96":"def rmsle(predicted, real):\n    sum=0.0\n    for x in range(len(predicted)):\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p - r)**2\n    return (sum\/len(predicted))**0.5\n\nNUM_OF_FEATURES = X.shape[1]\n\ntrain_X_train, train_X_test, train_y_train, train_y_test = train_test_split(X_train, y_train, test_size=0.2)\n\nmodel = ensemble.RandomForestRegressor(n_jobs=-1, n_estimators = 100, random_state=42)\nmodel.fit(train_X_train, train_y_train)\n\n# Graphs section\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot(111)\nplt.plot(np.cumsum(model.feature_importances_))\nplt.axhline(0.85,color= 'r')\n\nNUM_OF_FEATURES = 200\n\ncol = pd.DataFrame({'importance': model.feature_importances_, 'feature': X_train.columns}).sort_values(\n    by=['importance'], ascending=[False])[:NUM_OF_FEATURES]['feature'].values\n\nX_train = X_train[col]\nX_test = X_test[col]","bfa625f0":"# Define evaluation method for a given model. we use k-fold cross validation on the training set. \n# The loss function is root mean square logarithm error between target and prediction\n# Note: train and y_train are feeded as global variables\n\nNUM_FOLDS = 5\ndef rmsle_cv(model,strategy):\n        kf = KFold(NUM_FOLDS, shuffle=True, random_state=42).get_n_splits(X_train.values)\n        rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n        return(rmse)\n\n# Ensemble method: model averaging\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # We define clones of the original models to fit the data in\n    # the reason of clone is avoiding affect the original base models\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]  \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    \n    # Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([ model.predict(X) for model in self.models_ ])\n        return np.mean(predictions, axis=1)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.055, \n                             gamma=1.5, \n                             learning_rate=0.02, \n                             max_depth=12, \n                             n_estimators=1000,\n                             subsample=0.7, \n                             objective='reg:linear',\n                             booster='gbtree',\n                             reg_alpha=0.0, \n                             eval_metric = 'rmse', \n                             silent=1, \n                             random_state =7,\n                            )\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=144,\n                              learning_rate=0.005, \n                              n_estimators=1000, \n                              max_depth=12,\n                              metric='rmse',\n                              is_training_metric=True,\n                              max_bin = 55, \n                              bagging_fraction = 0.8,\n                              verbose=-1,\n                              bagging_freq = 5, \n                              feature_fraction = 0.9)\n\nl_m = []\nl_std = []\ntime_d = {}\n\nprint(\"GET DUMMIES, FEATURES SELECTION, KEEP MISSING VALUES\\n\")\n\nstart = time.time()\nscore_1 = rmsle_cv(model_xgb,1)\nend = time.time()\n\nl_m.append(score_1.mean())\nl_std.append(score_1.std())\ntime_d['XGB']= end - start\nprint(\"    Xgboost score : {:.4f} ({:.4f})\".format(score_1.mean(), score_1.std()))\n\nstart = time.time()\nscore_1 = rmsle_cv(model_lgb,1)\nend = time.time()\n\nl_m.append(score_1.mean())\nl_std.append(score_1.std())\ntime_d['LGB']= end - start\nprint(\"    LGBM score    : {:.4f} ({:.4f})\" .format(score_1.mean(), score_1.std()))\n\naveraged_models_1 = AveragingModels(models = (model_xgb, model_lgb))\n\nscore_1 = rmsle_cv(averaged_models_1,1)\n\nl_m.append(score_1.mean())\nl_std.append(score_1.std())\nprint(\"    Averaged score: {:.4f} ({:.4f})\\n\" .format(score_1.mean(), score_1.std()))\n\nprint('\\nLGB faster than XGB ?',time_d['LGB']>time_d['XGB'])","37a8f85c":"model_lgb.fit(X_train.values, y_train)\npred = model_lgb.predict(X_test.values)","9b35efeb":"# load JS visualization code to notebook\nshap.initjs()","aac95be6":"# Using a random sample of the dataframe for better time computation\nX_sampled = X_train.sample(100, random_state=10)","13dd14a1":"# explain the model's predictions using SHAP values\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\nexplainer = shap.TreeExplainer(model_lgb)\nshap_values = explainer.shap_values(X_sampled)","665234d9":"# visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_sampled.iloc[0,:])","709b4cb3":"# visualize the training set predictions\nshap.force_plot(explainer.expected_value, shap_values, X_train)","49443fd3":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X_sampled)","92410619":"shap.summary_plot(shap_values, X_sampled, plot_type=\"bar\")","13cbf52b":"pred = np.array(pred)\noriginal = np.array(df_cleaned_dummies.loc[indexes_cars_predicted,'MSRP'])\n\ndf_cleaned_dummies.loc[indexes_cars_predicted,'MSRP']\n\ndef average_gap(l1,l2):\n    resu=0\n    for i in range(len(l1)):\n        resu += np.abs(l1[i]-l2[i])\n    resu = resu\/len(l1)\n    return(resu)\n\nprint(\"Over\",len(pred),\"cars, the average gap between the predicted price and the real price is\",round(average_gap(pred,original),0),\"$\")\n\nplt.figure(figsize=(15,7))\nsns.distplot(pred, color=\"blue\", label=\"Distrib Predictions\", hist = False)\nsns.distplot(original, color=\"red\", label=\"Distrib Original\", hist = False)\nplt.title(\"Distribution of pred and original MSRP\")\nplt.legend()","de3305f3":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---","6ca44123":"**<font color='blue' size='3'>2.2 Explanation on MSRP<\/font>**\n\n<div align='justify'><font size=3>MSRP is the manufacturer's suggested retail price, a price recommended for the sale of an item in all retail outlets. A vendor can require retailers to sell its products at the MSRP and refuse to sell its products to a discounter if they price products below it.<\/font><\/div>","b26bc2f5":"LGBM is a great solution if your PC runs slowly, hence the word 'light'. It trains faster than XGBOOST. However I don't manipulate a significant amount of datas and given the way each algorithm is parametrized, the computation depends. On average LGBM is performs faster but with less accuracy than XGB.","fca1cb5c":"#### <font color='blue' size='3'>4.1 Encoding categorical features<\/font>","33057db3":"## <div id=\"chap7\">Conclusion<\/div>","1244b4c8":"<div align='justify'><font size=3>To improve the model, some features could be added such as the weight of the car, the nationality of the car manufacturer, options sold with the car and many others.<\/font><\/div>","539eaa07":"**<font color='blue' size='3'>2.1 Explanation of MPG<\/font>** \n\n<div align='justify'><font size=3>A measure of how far a car can travel if you put just one gallon of petrol or diesel in its tank.<\/font><\/div>","802f9350":"## <div id=\"summary\">Table of contents<\/div>\n\n**<font size=\"2\"><a href=\"#chap1\">Introduction<\/a><\/font>**\n<br>\n**<br><font size=\"2\"><a href=\"#chap2\">2. Data description<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. Model<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap4\">4. SHapley Additive exPlanations<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap5\">5. Predictions<\/a><\/font>**\n<br>\n**<br><font size=\"2\"><a href=\"#chap6\">Conclusion<\/a><\/font>**","7e286e72":"We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):","52cde6c6":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---","264d542d":"<hr>\n<br>\n<div align='justify'><font color=\"#353B47\" size=\"4\">Thank you for taking the time to read this notebook. I hope that I was able to answer your questions or your curiosity and that it was quite understandable. <u>any constructive comments are welcome<\/u>. They help me progress and motivate me to share better quality content. I am above all a passionate person who tries to advance my knowledge but also that of others. If you liked it, feel free to <u>upvote and share my work.<\/u> <\/font><\/div>\n<br>\n<div align='center'><font color=\"#353B47\" size=\"3\">Thank you and may passion guide you.<\/font><\/div>","5d6b51fe":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---","578505c6":"## References\n\n* **Understanding of LGBM, XGB** : https:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db\n\n* **Interpretation of SHAP values** : https:\/\/github.com\/slundberg\/shap ","c41421ad":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---","870d8b30":"## <div id=\"chap5\"> 5. SHapley Additive exPlanations<\/div>","195e8d45":"### <div id=\"chap3\"> 3. Setup<\/div>","c2f6afbd":"#### <font color='blue' size='3'>4.6 The best model<\/font>","194dfe11":"> ### <div id=\"chap1\">Introduction<\/div>","9dc99f77":"To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. SHAP values represent a feature's responsibility for a change in the model output. Vertical dispersion represents interaction effects with other features. To help reveal these interactions dependence_plot automatically selects another feature for coloring.","9d59c180":"#### <font color='blue' size='3'>3.1 Librairies<\/font>","e8be47ef":"### <div id=\"chap2\"> 2. Data description<\/div>","81223912":"<div align='center'><font size=\"5\" color='#353B47'>CARS PRICING<\/font><\/div>\n<div align='center'><font size=\"4\" color=\"#353B47\">PART2<\/font><\/div>\n<br>\n<div align=\"center\"><img alt=\"Super Sports Cars Wallpaper  HD Car Wallpapers\" height=\"auto\" src=\"http:\/\/www.hdcarwallpapers.com\/download\/super_sports_cars-1366x768.jpg\" title=\"Super Sports Cars Wallpaper  HD Car Wallpapers\" width=\"65%\"><\/div>\n<br>\n<div align='center'><font size = '4' color='#353B47'>EDA and preprocessing of the data: <a href = 'https:\/\/www.kaggle.com\/bryanb\/simple-and-quick-eda'>CARS PRICING - PART 1<\/a><\/font><\/div>\n<hr>","c06f1a1b":"#### <font color='blue' size='3'>4.5 LGBM vs XGBOOST<\/font>","99af22db":"## <div id=\"chap6\"> 6. Predictions<\/div>","e6412170":"#### <font color='blue' size='3'>4.2 Strategy selection<\/font>","f7f7f6a4":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---","9ebb0643":"##### <font color='orange' size='2'>How to chose NUM_OF_FEATURES to select ?<\/font>","ef05bbee":"<div align='justify'><font size=3>The data come from the Kaggle dataset Car Features and MSRP of Kaggle. It describes almost 12 000 car models sold in the USA between 1990 and 2018 with the market price (new or used) and some features. This study aims at performing some data manipulations and define a statistical model to predict the price of a car.<\/font><\/div>","08713929":"#### <font color='blue' size='3'>5.1 What is a SHAP value<\/font>","aff55388":"Given that representative sample, when someone wants to buy a car, the criterias will be the city mpg, the car age and the engine HP of the car.","ca7fe179":"> - Prices retained\n> \n>     \u25e6 for vehicles sold in 2017 (1 668 vehicles), new price suggested by the manufacturer\n>\n>     \u25e6 for used vehicles (10 246 vehicles), edmunds.com True Market Value\n>     \nThis difference in method could lead to a discontinuity. However we see on different examples there isn't any significant gap.\n> \n> - All prices are recorded at the same time and can be compared. There is no need for inflation ajustment.\n> \n> - Used car prices are floored on edmunds.com. Oldest or cheapest vehicles have a minimum price of 2000 USD (1 036 vehicles).","6db2cac7":"## <div id=\"chap4\"> 4. Model<\/div>","a554557e":"The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our Nature BME paper).","aa4140ae":"#### <font color='blue' size='3'>4.4 Feature selection<\/font>","1e09813a":"SHAP is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, representing the only possible consistent and locally accurate additive feature attribution method based on expectations (More details : http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions).","40694933":"#### <font color='blue' size='3'>5.2 Visualization<\/font>","c1eb7769":"#### <font color='blue' size='3'>3.2 Import datas<\/font>","31f140c7":"#### <font color='blue' size='3'>4.3 Split the data<\/font>"}}