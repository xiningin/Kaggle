{"cell_type":{"1398fbed":"code","5dcdec7c":"code","1b3db6ad":"code","49c5c17e":"code","c32da5cd":"code","03cbd97d":"code","f2b7a09d":"code","7f7230cd":"code","d7c77050":"code","e98b2d93":"code","a5747306":"code","a0169109":"code","b4a597e6":"code","b1bcf07c":"code","5bc3f8d1":"code","903186eb":"code","8af24002":"code","75010871":"code","8900f8ee":"code","d33755b8":"code","9dea86ce":"code","ab89f6f8":"code","a5c7ae4b":"code","e543b6c8":"code","3594a08c":"code","a6e2f9a3":"code","1f020590":"code","5aa628a4":"code","b6b5daa5":"code","d2c4bfae":"code","b86911a3":"code","e7dfbd74":"code","c11818bb":"code","af67f468":"code","40538202":"code","4d9105f0":"code","ce2676dc":"code","d446cacb":"code","9fefcfdc":"code","a5a8b3c2":"code","faea7c80":"code","3d685971":"code","a14d8373":"code","e8fa539d":"code","7f5c83cf":"code","8afd4e00":"code","8609cc8a":"code","9336c4ae":"code","9eaabddf":"code","c2bbadbe":"code","392aed26":"code","aaa4072b":"code","a580114d":"code","9ede7cc8":"code","1d20bd65":"code","e9164954":"code","87991724":"code","780c5108":"markdown","bfb3fb57":"markdown","b097a348":"markdown","b784ff5f":"markdown","cd257f3e":"markdown","1df415f8":"markdown","d0891a58":"markdown","3636df4e":"markdown","9185059c":"markdown","73e84e56":"markdown","51500e28":"markdown","6a7770a9":"markdown","af3d7066":"markdown","c0b91709":"markdown","0e571de2":"markdown","59c53270":"markdown","71172692":"markdown","ccf2f257":"markdown","4aaa0aff":"markdown","2e0895c5":"markdown","d5031d4a":"markdown","66b482f2":"markdown","a9b6c4c6":"markdown","527a3553":"markdown","b2f622e0":"markdown","71834369":"markdown","6ea94218":"markdown","a2865802":"markdown","44e5cf03":"markdown","68a8f38e":"markdown","efaaf003":"markdown","2a0d6f83":"markdown","a815c2b8":"markdown","0193f641":"markdown","c483a2c1":"markdown"},"source":{"1398fbed":"import gc #garbage collector\nimport ast # operate with string representation of list\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom scipy.stats import shapiro # for normal distribution checks \n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly\nplotly.offline.init_notebook_mode(connected = True)\n\nimport matplotlib.pyplot as plt","5dcdec7c":"def convert_coords(bbox):\n    '''\n    Transform boundary box coordinates from pandas dataframe to cv2.rectangle values\n    Pandas df values: x, y  width, height\n    '''\n    x, y, width, height = bbox\n    start_point = x, y\n    end_point = (x + width), (y + height)\n    return start_point, end_point\n\ndef plot_samples(df, img_ids=None, threshold=6, title=''):\n    '''\n    Plot image grid from seleted dataframe \n    https:\/\/stackoverflow.com\/questions\/46615554\/how-to-display-multiple-images-in-one-figure-correctly\/46616645\n    '''\n    if img_ids is None:\n        img_ids = df['image_id_ext'].unique()[:threshold]\n    cols = 3\n    rows = len(img_ids) \/\/ cols + 1\n    fig = plt.figure(figsize = (15, 5 * rows))\n    for i, img_id in enumerate(img_ids):\n        bboxes_list = df[df['image_id_ext'] == img_id].bbox.to_list()\n        img = cv2.imread(os.path.join(TRAIN_DIR_PATH, img_id))\n        for item in bboxes_list:\n            bbox = list(map(int, ast.literal_eval(item)))\n            strart_point, end_point = convert_coords(bbox)\n            color = (255, 0, 0) #RGB\n            thickness = 2\n            img = cv2.rectangle(img, strart_point, end_point, color, thickness)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img)\n    plt.suptitle(title, fontsize=16)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    plt.show()\n    \n# Took ideas of these functions from https:\/\/www.kaggle.com\/aleksandradeis\/globalwheatdetection-eda\n\ndef get_image_brightness(image_id_ext):\n    img = cv2.imread(os.path.join(TRAIN_DIR_PATH, image_id_ext))\n    # convert to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # get average brightness\n    return np.array(gray).mean()\n\ndef get_percentage_of_green_pixels(image_id_ext):\n    img = cv2.imread(os.path.join(TRAIN_DIR_PATH, image_id_ext))\n    # convert to HSV\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (40, 40, 40) \n    hsv_higher = (70, 255, 255)\n    green_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(green_mask)) \/ 255 \/ (1024 * 1024)\n\ndef get_percentage_of_yellow_pixels(image_id_ext):\n    img = cv2.imread(os.path.join(TRAIN_DIR_PATH, image_id_ext))\n    # convert to HSV\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (25, 40, 40) \n    hsv_higher = (35, 255, 255)\n    yellow_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(yellow_mask)) \/ 255 \/ (1024 * 1024)","1b3db6ad":"MAIN_PATH = '\/kaggle\/input\/global-wheat-detection\/'\nTRAIN_DIR_PATH = '\/kaggle\/input\/global-wheat-detection\/train\/'\nTEST_DIR_PATH = '\/kaggle\/input\/global-wheat-detection\/test\/'","49c5c17e":"print('Train images amount', len(os.listdir(os.path.join(MAIN_PATH, 'train'))))\nprint('Test images amount', len(os.listdir(os.path.join(MAIN_PATH, 'test'))))","c32da5cd":"train_df = pd.read_csv(os.path.join(MAIN_PATH, 'train.csv'))\nsample_submission = pd.read_csv(os.path.join(MAIN_PATH, 'sample_submission.csv'))","03cbd97d":"train_df.head().T","f2b7a09d":"sample_submission.head().T","7f7230cd":"train_df['image_id_ext'] = train_df['image_id'] + '.jpg'","d7c77050":"ser = train_df['image_id']\n\nfig = px.histogram(ser, title = 'Amunt of bundary boxes on the each picture', \n                   labels={'x':'image id', 'y':'bbox amount'})\nfig.update_xaxes(categoryorder='total descending')\nfig.show()","e98b2d93":"temp_df = train_df.copy()\ntemp_df['dummy_column'] = 1\nser = temp_df.groupby(['image_id']).sum()['dummy_column']","a5747306":"fig = px.histogram(ser, title = 'Sum distribution of bboxes amount', \n                   labels={'y':'bbox amount'})\nfig.show()","a0169109":"print('Shapiro-Wilk test for normality result\\n statistic:{:.3f}, p-value:{:.3E}'.format(*shapiro(ser.to_list())))","b4a597e6":"del temp_df\ngc.collect()","b1bcf07c":"print('How many no-bbox images does exist in train dataset?', \n      len(os.listdir(TRAIN_DIR_PATH)) - len(train_df['image_id'].unique()))","5bc3f8d1":"no_bbox_img_ids = set(os.listdir(TRAIN_DIR_PATH)) - set((train_df['image_id_ext']).unique().tolist())","903186eb":"%%time\n# convert column '[x, y, width, height]' to the separate pandas dataframe\nsplitted_data = train_df['bbox'].str.split(r'[^\\d.]+')\nbbox_df = pd.DataFrame.from_dict(dict(zip(splitted_data.index,splitted_data))).T\nbbox_df.drop(columns = [0, 5], inplace = True) #drop empty columns\nbbox_df.columns = ['x', 'y', 'bwidth', 'bheight']\nbbox_df = bbox_df.astype(float)\nbbox_df['size'] = bbox_df['bwidth'] * bbox_df['bheight']","8af24002":"train_df = train_df.join(bbox_df)\ntrain_df.head()","75010871":"train_df['size'].describe()","8900f8ee":"max_bbox_size = train_df.groupby(['image_id']).max()['size'].to_frame()\n\nfig = px.histogram(max_bbox_size, title = 'Max distribution of bboxes area size', \n                   labels={'y':'bbox area size'})\nfig.show()","d33755b8":"MAX_ANOMALY_THRESHOLD = 120000\nprint('Size of anomaly frame: ', train_df[train_df['size'] > MAX_ANOMALY_THRESHOLD].shape)","9dea86ce":"plot_samples(train_df[train_df['size'] > MAX_ANOMALY_THRESHOLD], \n             threshold=15, \n             title='Images with anomaly big bboxes')","ab89f6f8":"min_bbox_size = train_df.groupby(['image_id']).min()['size'].to_frame()\n\nfig = px.histogram(min_bbox_size, title = 'Min distribution of bboxes area size', \n                   labels={'y':'bbox area size'})\nfig.show()","a5c7ae4b":"MIN_ANOMALY_THRESHOLD = 1000\nprint('Size of anomaly frame: ', train_df[train_df['size'] < MIN_ANOMALY_THRESHOLD].shape)","e543b6c8":"plot_samples(train_df[train_df['size'] < MIN_ANOMALY_THRESHOLD], \n             threshold=9, \n             title = 'Images with anomaly small bboxes')","3594a08c":"sum_bbox_areas =  train_df.groupby(['image_id']).sum()['size']\/(1024 * 1024)\n\nfig = px.histogram(sum_bbox_areas, title = 'Percentage distribution of image box coverage', \n                   labels={'y':'bbox area size'})\nfig.show()","a6e2f9a3":"print('Shapiro-Wilk test for normality result \\n statistic:{:.3f}, p-value:{:.3E}'.format(*shapiro(sum_bbox_areas.to_list())))","1f020590":"fig = px.histogram(train_df.groupby(['image_id', 'source']).mean().index.to_frame()['source'],\n                   title = 'Image sources distribution',\n                   labels={'x':'source name', 'y':'images amount'})\nfig.update_xaxes(categoryorder='total descending')\nfig.show()","5aa628a4":"max_bbox_ids = train_df['image_id_ext'].value_counts().index.to_list()[:6]\nplot_samples(train_df, img_ids=max_bbox_ids, title='Samples with the maximum amount of boundary boxes')","b6b5daa5":"min_bbox_ids = train_df['image_id_ext'].value_counts(ascending=True).index.to_list()\nplot_samples(train_df, img_ids=min_bbox_ids[:6], title='Samples with the minimum amount of boundary boxes')","d2c4bfae":"print('Sources of top 100 photost with the least amount of bboxes: ', \n      train_df[train_df['image_id_ext'].isin(min_bbox_ids[:100])]['source'].unique())    ","b86911a3":"fig = px.histogram(train_df[train_df['source'] == 'arvalis_3'].groupby(['image_id']).mean()['size'], \n                   title = 'Percentage distribution of image box coverage in arvalis_3 dataset')                 \nfig.show()","e7dfbd74":"sources_list = list(train_df['source'].unique())\nprint('Complete list of sources: \\n', sources_list)","c11818bb":"plot_samples(train_df[train_df['source'] == 'arvalis_1'], \n             title='Source: ARVALIS (Institut du vegetal is an applied agricultural research)')","af67f468":"plot_samples(train_df[train_df['source'] == 'arvalis_2'])","40538202":"plot_samples(train_df[train_df['source'] == 'arvalis_3'])","4d9105f0":"plot_samples(train_df[train_df['source'] == 'inrae_1'],\n             title='Source: INRAE (National Research Institute for Agriculture, Food and Environment)')","ce2676dc":"plot_samples(train_df[train_df['source'] == 'ethz_1'],\n             title='Source: ETHZ (Swiss Federal Institute of Technology in Zurich)')\n","d446cacb":"plot_samples(train_df[train_df['source'] == 'rres_1'],\n             title='Soure: Rothamsted Research Institute')","9fefcfdc":"plot_samples(train_df[train_df['source'] == 'usask_1'],\n             title='Source: University of Saskatchewan')","a5a8b3c2":"plot_samples(train_df, img_ids=list(no_bbox_img_ids)[:6], \n             title='Examples of empty images without boundary boxes')","faea7c80":"%%time\n#Calculate mean brightness\nser = train_df.groupby(['image_id_ext']).mean().reset_index()['image_id_ext']\nmean_brightness = ser.apply(get_image_brightness)\n# Add results to train_df\nbright_df = pd.DataFrame({'image_id_ext': ser, 'mean brightness': mean_brightness})\ntrain_df = train_df.merge(bright_df, on='image_id_ext')","3d685971":"# Brightness (min - max)\nfig = px.histogram(mean_brightness, title = 'Mean brightness distribution')\nfig.show()","a14d8373":"first_group_id = train_df[(train_df['mean brightness'] >= 76) & (train_df['mean brightness'] <= 78)]['image_id_ext'].unique()\nsecond_group_id = train_df[(train_df['mean brightness'] >= 106) & (train_df['mean brightness'] <= 108)]['image_id_ext'].unique()","e8fa539d":"plot_samples(train_df, img_ids=first_group_id[:6], \n             title='Examples of images with brightness [76 - 78]')","7f5c83cf":"plot_samples(train_df, img_ids=second_group_id[:6], \n             title='Examples of images with brightness [106 - 108]')\n","8afd4e00":"sorted_bright_images = train_df.groupby(['image_id_ext']).mean()['mean brightness']\\\n                       .sort_values(ascending=False).index.to_list()","8609cc8a":"plot_samples(train_df, img_ids=sorted_bright_images[:6], \n             title='The most bright images')","9336c4ae":"plot_samples(train_df, img_ids=sorted_bright_images[:-7:-1], \n             title='The most dark images')","9eaabddf":"%%time\n#Calculate color percentage \nser = train_df.groupby(['image_id_ext']).mean().reset_index()['image_id_ext']\ngreen_percentage = ser.apply(get_percentage_of_green_pixels)\nyellow_percentage = ser.apply(get_percentage_of_yellow_pixels)\n# Add results to train_df\ncolors_df = pd.DataFrame({'image_id_ext': ser, 'green %': green_percentage, 'yellow %': yellow_percentage})\ntrain_df = train_df.merge(colors_df, on='image_id_ext')","c2bbadbe":"gc.collect()","392aed26":"yellow_means = train_df.groupby(['image_id']).mean()['yellow %']\ngreen_means = train_df.groupby(['image_id']).mean()['green %']","aaa4072b":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=yellow_means, marker_color='#eeff00'))\nfig.add_trace(go.Histogram(x=green_means, marker_color='#55ff00'))\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.7)\nfig.update_layout(\n    #barmode='overlay',\n    title_text='Yellow and green percentage distribution', \n    xaxis_title_text='% of colored pixels', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    #bargap=0.2, # gap between bars of adjacent location coordinates\n    #bargroupgap=0.1 # gap between bars of the same location coordinates\n)\n\nfig.show()","a580114d":"sorted_yellow_images = train_df.groupby(['image_id_ext']).mean()['yellow %']\\\n                       .sort_values(ascending=False).index.to_list()\nsorted_green_images = train_df.groupby(['image_id_ext']).mean()['green %']\\\n                       .sort_values(ascending=False).index.to_list()\n","9ede7cc8":"plot_samples(train_df, img_ids=sorted_yellow_images[:3], \n             title='The most yellow images')","1d20bd65":"plot_samples(train_df, img_ids=sorted_yellow_images[:-4:-1], \n             title='The least yellow images')","e9164954":"plot_samples(train_df, img_ids=sorted_green_images[:3], \n             title='The most green images')","87991724":"plot_samples(train_df, img_ids=sorted_green_images[:-4:-1], \n             title='The least green images')","780c5108":"**Samples with the minimum amount of boundary boxes.**","bfb3fb57":"In view of the large difference in the quality of images of the dataset, consider the distribution of their brightness, perhaps there will be anomalies or patterns.","b097a348":"Boundary boxes looks correct, but I will separetely check their areas later. Some boxes and wheat heads overlap, that can cause false positive errors. ","b784ff5f":"According to the description of the competition and the transcription of acronyms from the dataset, the sources of the image are: \n* ```arvalis``` - ARVALIS, Institut du vegetal, France. \n* ```ethz``` - ETHZ, Swiss Federal Institute of Technology in Zurich, Switzerland\n* ```rres``` - Rothamsted Research Institute, UK\n* ```usask``` - University of Saskatchewan, Canada\n* ```inrae``` - INRAE, National Research Institute for Agriculture, Food and Environment, France","cd257f3e":"**Samples with the maximum amount of boundary boxes.**","1df415f8":"And again, we got something similar to the normal distribution, like it was in distribution of bboxes amount.\nWe can expect similar distribution on validation and test (if we will find additional data) datasets.","d0891a58":"All of this  image (all 49, I mean) are very dark and can be haracterize by empty ground. ","3636df4e":"Checking the size distribution of boundary boxes. ","9185059c":"# Dataset samples","73e84e56":"Different brightness corresponds to different datasets and different plant species.","51500e28":"We will review the image samples from the each source below.","6a7770a9":"Pictures usually have ~35 bboxes. Also, we have something looks like a nice normal distribution.<br>\nWe can use this values to validate our results: distribution on valdation dataset should be the same.","af3d7066":"# Global Wheat Detection. EDA\n\nThis notebook is dedicated to exploratory data analysis for [Global Wheat Detection](https:\/\/www.kaggle.com\/c\/global-wheat-detection) competition. The main focus will be on visualizing the differences between datasets from different sources and selecting key components for further data augmentation.\n","c0b91709":"### Utilities","0e571de2":"**Empty images without boundary boxes**","59c53270":"Note, that test dataset extremely small. Train dataset quite small as well, so I will have to use data augmentation.","71172692":"Well, we can see the bimodal distribution. There are 2 large groups of images with similar brightness in the dataset.","ccf2f257":"Yep, normal distribution.","4aaa0aff":"Looks like the second and the third photos are serial. <br>\nYou can also see that the unmarked photos are pictures of the ground that has been trampled. Let's dive a little deeper into this problem, maybe some significant parts of dataset have this  problem?","2e0895c5":"# Color hystograms","d5031d4a":"We definately have some anomalies here","66b482f2":"### Acknowledgments\nNotebooks, that was useful  for this research:\n* [GlobalWheatDetection EDA](https:\/\/www.kaggle.com\/aleksandradeis\/globalwheatdetection-eda)\n* [GWD: EDA + Starter Code](https:\/\/www.kaggle.com\/yashchoudhary\/gwd-eda-and-starter-code-beginner-friendly)","a9b6c4c6":"Let's check how many pictures from train set don't have any bboxes at all","527a3553":"Thus, only bboxes of excessive size are dangerous anomalies in the dataset. There are few of them and they can be removed without damaging the generality of the model.<br>\nUltra-small boundary boxes can be explained either by very careful tracing of plants and by zooming the photos during the creation of the dataset. Anyway it will not hurt the model quality. <br>\nThis is a signal for us, that we should use this crop tric during data augmentation as well.","b2f622e0":"# Source distribution","71834369":"**Samples from different sources**","6ea94218":"Yes, we have  problem images, but overall dataset behaviour the same with summary dataset.","a2865802":"Add additional column to train dataset with ```.jpg``` extention","44e5cf03":"### Constants","68a8f38e":"Let's see on the diversity of boundary boxes amounts.","efaaf003":"Save their ids into additional set","2a0d6f83":"# Conclusions\n\n*Dataset*\n\n* The images of plants in the dataset vary greatly in brightness and number of boxes. What's more, biological species are just as different.\n* A large number of targets and boundary boxes overlapping each other. It increases the risk of false negative error.\n* The distribution of the number of targets and the area of boxes relative to the image are subject to normal distribution. This can be used for model quality control during validation.\n* Training dataset are relatively small for this competition, data augmentation will be critical part.\n\n\n*Data Augmentation*\n\nWhat might work:\n* Flipping images horizontally and vertically\n* Crop-resize\n* Gamma, contrast and brightness tuning","a815c2b8":"It can be seen that different plant species are typical for different sources. Moreover, the photos were taken at different times of the day, from different angles and at different stages of harvest ripening.<br>\nPerhaps you should add a separate classifier to the model to determine the type of plants and train the appropriate model for it.","0193f641":"# Boundary boxes analysis","c483a2c1":"Idea to look at color distributions was taken  from [here](https:\/\/www.kaggle.com\/aleksandradeis\/globalwheatdetection-eda).<br>\nThe point was that plant with different  maturnity levels has different green pixels saturated. In this case we can find a kink between color and size of wheat heads. In the other hand empty pictures would have gray dominant pixels. "}}