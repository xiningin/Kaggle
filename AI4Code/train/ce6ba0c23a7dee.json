{"cell_type":{"e3c5b41f":"code","f5141e29":"code","95f909f9":"code","f071bff0":"code","cca37377":"code","25d12e9f":"code","112f3b3b":"code","2a4caba5":"code","3435acad":"code","33dcd9a1":"code","18aef3d6":"code","4575528d":"code","84e6fe41":"code","370a2e92":"code","86baf6b6":"code","499c5244":"code","ad13ded9":"code","91d3cf30":"code","650eb59c":"code","1459c617":"code","a29a8cf6":"code","d1a634c0":"code","8d92c071":"code","98a53f18":"code","b20e836a":"code","19223583":"code","438bcb3d":"code","9f9d9653":"code","7eea89ba":"code","f9d831c7":"code","6b586912":"code","8ddb3edc":"code","ca7620b3":"code","f076aeb6":"code","969066aa":"code","723d75db":"code","7f05a92a":"code","3b314dbd":"code","739d7fa3":"code","fbc0d91a":"code","61b0889c":"code","97554875":"code","b5a1af10":"code","bcb907d9":"code","e5d9cf57":"code","5a8901ea":"code","fff7db92":"code","a1fd3815":"code","7afb67b6":"code","7ca24498":"code","c61bc29f":"code","93513553":"code","942ca518":"code","af68df85":"code","43f50421":"code","babd594e":"code","09fe5087":"code","03c8b494":"code","0d2b7f86":"code","3cc90d57":"code","fb4d9239":"code","1a771d3d":"code","d675b097":"markdown","11d6af3d":"markdown","105807a4":"markdown","d537c96e":"markdown","230f9d89":"markdown","ff5fb920":"markdown","5f344a35":"markdown","ce687631":"markdown","796ced73":"markdown","be71bbc7":"markdown","132a81ed":"markdown","1bce2eeb":"markdown","4da64230":"markdown","ce1a411b":"markdown","d5fa3141":"markdown","f8c2a4a0":"markdown","869b77ba":"markdown","da86d1b5":"markdown","88fb8eda":"markdown","3a7789ed":"markdown","ba06a40e":"markdown","3a8a2ed6":"markdown","a2020501":"markdown","50359baa":"markdown"},"source":{"e3c5b41f":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# setting figure size\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 20,10\n\n# for normalizing data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))","f5141e29":"df = pd.read_csv('..\/input\/reliance-stock-2000-to-2020\/RELIANCE.csv')","95f909f9":"df.head(2)","f071bff0":"df.tail(2)","cca37377":"df.columns","25d12e9f":"df['Symbol'].unique()","112f3b3b":"df['Series'].unique()","2a4caba5":"df.drop(['Symbol', 'Series'], axis = 1, inplace = True)","3435acad":"df.shape","33dcd9a1":"df.isnull().sum()","18aef3d6":"df.drop(['Trades', 'Deliverable Volume', '%Deliverble'], axis = 1, inplace = True)","4575528d":"df.info()","84e6fe41":"# setting date as index\ndf['Date'] = pd.to_datetime(df.Date, format = '%Y-%m-%d')\ndf.index = df['Date']","370a2e92":"df1 = df.copy()","86baf6b6":"# plot\nplt.figure(figsize = (16,8))\nplt.plot(df['VWAP'], label= 'VWAP History')","499c5244":"data = df.sort_index(ascending=True, axis = 0)\nnew_data = pd.DataFrame(index = range(0, len(df)), columns= ['Date', 'VWAP'])\n\nfor i in range(0, len(data)):\n    new_data['Date'][i] = data['Date'][i]\n    new_data['VWAP'][i] = data['VWAP'][i]\n    ","ad13ded9":"# splitting into train and validation\ntrain = new_data[:4000]\nvalid = new_data[4000:]\n\n# shapes of train and validation data\nprint('\\n Shape of Training set: ')\nprint(train.shape)\nprint('\\n Shape of validation set: ')\nprint(valid.shape)","91d3cf30":"# we will create predictions for the validation set and check the RMSE using the actual values. \npreds =[]\nfor i in range(0, valid.shape[0]):\n    a = train['VWAP'][len(train)-1075+i:].sum() + sum(preds)\n    b = a\/1075\n    preds.append(b)","650eb59c":"# Checking the results (RMSE value)\nfrom sklearn.metrics import mean_squared_error\ny_actual = valid['VWAP']\ny_pred = np.array(preds)\nrms = np.sqrt(mean_squared_error(y_actual, preds))\nprint(rms)\n","1459c617":"# Plot\nplt.figure(figsize = (10,6))\nvalid['predictions'] = y_pred\nplt.plot(train['VWAP'])\nplt.plot(valid[['VWAP', 'predictions']]);","a29a8cf6":"# create features\nfrom fastai.tabular import add_datepart","d1a634c0":"add_datepart(new_data, 'Date')\nnew_data.drop('Elapsed', axis = 1, inplace = True)","8d92c071":"# adding new feature\n# if the day is mon\/fri then value is 1 and if the day is tue\/wed\/thur value is 0\n\nnew_data['mon_fri'] = 0\nfor i in range(0, len(new_data)):\n    if (new_data['Dayofweek'][i] == 0 or new_data['Dayofweek'][i]==4):\n        new_data['mon_fri'][i] = 1\n    else:\n        new_data['mon_fri'][i] =0","98a53f18":"new_data[['Dayofweek', 'mon_fri']].head()","b20e836a":"# splitting the data into train and validation\ntrain = new_data[:4000]\nvalid = new_data[4000:]\n\nx_train = train.drop('VWAP', axis = 1)\ny_train = train['VWAP']\nx_valid = valid.drop('VWAP', axis = 1)\ny_valid = valid['VWAP']","19223583":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)","438bcb3d":"y_pred_lr = lr.predict(x_valid)\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred_lr))\nprint(rmse)","9f9d9653":"# plot\nvalid['predictions'] = y_pred_lr\n\nplt.figure(figsize=(10,7))\nplt.plot(train['VWAP'])\nplt.plot(valid.loc[:,('VWAP', 'predictions')]);","7eea89ba":"# importing required libraries\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV","f9d831c7":"# scaling data\nx_train_scaled = scaler.fit_transform(x_train)\nx_train = pd.DataFrame(x_train_scaled)\nx_valid_scaled = scaler.fit_transform(x_valid)\nx_valid = pd.DataFrame(x_valid_scaled)","6b586912":"# Gridsearch CV to find the best parameters\nparams = {'n_neighbors': [2,3,4,6,7,8,9]}\nknn = KNeighborsRegressor()\nmodel = GridSearchCV(knn, params, cv =5)\n\n#fit the model and make predictions\nmodel.fit(x_train, y_train)\ny_pred_knn = model.predict(x_valid)","8ddb3edc":"# RMSE\nrmse = np.sqrt(mean_squared_error(y_valid, y_pred_knn))\nprint(rmse)","ca7620b3":"# plot\nvalid['predictions'] = y_pred_knn\nplt.figure(figsize=(10,6))\nplt.plot(valid[['VWAP', 'predictions']])\nplt.plot(train['VWAP'])","f076aeb6":"pip install pmdarima","969066aa":"from pmdarima import auto_arima","723d75db":"data = df.sort_index(ascending = True, axis = 0)\n\ntrain = data[:4000]\nvalid = data[4000:]\n\ntraining = train['VWAP']\nvalidation = valid['VWAP']\n\nmodel = auto_arima(training, start_p = 1, start_q=1, max_p=3,\n                  max_q=3, m= 12, start_P=0, seasonal=True, d = 1, \n                  D=1, trace=True, error_action='ignore',\n                   suppress_warnings=True)\nmodel.fit(training)\n\ny_pred_aa = model.predict(n_periods = 1075)\ny_pred_aa = pd.DataFrame(y_pred_aa, index = valid.index, columns = ['prediction'])","7f05a92a":"# RMSE\nrmse = np.sqrt(mean_squared_error(validation, y_pred_aa))\nprint(rmse)","3b314dbd":"# plot\nplt.plot(train['VWAP'])\nplt.plot(valid['VWAP'])\nplt.plot(y_pred_aa)","739d7fa3":"df.reset_index(drop=True, inplace = True)\nlag_features = ['High', 'Low', 'Volume', 'Turnover']\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window = window1, min_periods = 0)\ndf_rolled_7d = df[lag_features].rolling(window = window2, min_periods = 0)\ndf_rolled_30d = df[lag_features].rolling(window = window3, min_periods = 0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n    \ndf.fillna(df.mean(), inplace = True)\n\ndf.set_index('Date', drop=False, inplace = True)\ndf.head(2)","fbc0d91a":"df['month'] = df.Date.dt.month\ndf['week'] = df.Date.dt.week\ndf['day'] = df.Date.dt.day\ndf['day_of_week'] =df.Date.dt.dayofweek\ndf.head(1)","61b0889c":"df_train = df[df.Date<'2019']\ndf_valid = df[df.Date>= '2019']","97554875":"df_train.shape, df_valid.shape","b5a1af10":"exogenous_features = ['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Low_mean_lag3', 'Low_mean_lag7', 'Low_mean_lag30', 'Low_std_lag3',\n       'Low_std_lag7', 'Low_std_lag30', 'Volume_mean_lag3', 'Volume_mean_lag7',\n       'Volume_mean_lag30', 'Volume_std_lag3', 'Volume_std_lag7',\n       'Volume_std_lag30', 'Turnover_mean_lag3', 'Turnover_mean_lag7',\n       'Turnover_mean_lag30', 'Turnover_std_lag3', 'Turnover_std_lag7',\n       'Turnover_std_lag30']\n","bcb907d9":"model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features],\n                  trace = True, error_action='ignore', suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\ny_pred_aarima = model.predict(n_periods = len(df_valid), exogenous =df_valid[exogenous_features])\ndf_valid['Forecast_ARIMAX']  = y_pred_aarima","e5d9cf57":"plt.figure(figsize=(14,7))\nplt.plot(df_train['VWAP'])\nplt.plot(df_valid[['VWAP', 'Forecast_ARIMAX']])","5a8901ea":"plt.plot(df_valid[['VWAP', 'Forecast_ARIMAX']])","fff7db92":"print('RMSE of Auto ARIMAX: ', np.sqrt(mean_squared_error(df_valid['VWAP'], df_valid['Forecast_ARIMAX'])))","a1fd3815":"#importing prophet\nfrom fbprophet import Prophet","7afb67b6":"# creating dataframe\nnew_data = pd.DataFrame()\nnew_data['ds'] = data['Date']\nnew_data['y'] = data['VWAP']\n\n#splitting the data\ntrain = new_data[:4000]\nvalid = new_data[4000:]","7ca24498":"# fit the model\nmodel = Prophet()\nmodel.fit(train)\n\n#predictions\nVWAP_prices = model.make_future_dataframe(periods = len(valid))\nforcast = model.predict(VWAP_prices)","c61bc29f":"# Results\ny_true = valid['y']\ny_pred_fp = forcast['yhat'][4000:]\nrmse = np.sqrt(mean_squared_error(y_true, y_pred_fp))\nprint(rmse)","93513553":"# Plot\nvalid['predictions'] = y_pred_fp.values\n\nplt.figure(figsize=(10, 8))\nplt.plot(train['y'])\nplt.plot(valid[['y', 'predictions']])","942ca518":"plt.figure(figsize=(15, 8))\nplt.plot(valid[['y', 'predictions']])","af68df85":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM","43f50421":"#creating Dataframe\nnew_data = pd.DataFrame()\nnew_data['Date'] =data['Date']\nnew_data['VWAP'] = data['VWAP']\n\n# setting index\nnew_data.index = new_data.Date\nnew_data.drop('Date', axis = 1, inplace = True)\n\ndataset = new_data.values\n","babd594e":"#creating train and test sets\ntrain = dataset[:4000, :]\nvalid = dataset[4000:, :]","09fe5087":"# converting dataset into x_train and y_train\n\nscaled_data = scaler.fit_transform(dataset)\n\nx_train, y_train = [], []\n\nfor i in range(60, len(train)):\n  x_train.append(scaled_data[i-60:i,0])\n  y_train.append(scaled_data[i,0])\n\nx_train, y_train = np.array(x_train), np.array(y_train)\n\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))","03c8b494":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape = (x_train.shape[1],1)))\nmodel.add(LSTM(units = 50))\n\nmodel.add(Dense(1))\n\nmodel.compile(loss = 'mean_squared_error', optimizer = 'adam')\nmodel.fit(x_train, y_train, epochs = 1, batch_size = 1, verbose = 2)","0d2b7f86":"# predicting the 1075 values of the valid, using past 60 from the train data\ninputs = new_data[len(new_data) -len(valid) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs = scaler.transform(inputs)\n\nX_test  = []\nfor i in range(60, inputs.shape[0]):\n  X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)\n\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\n\nVWAP_price = model.predict(X_test)\nVWAP_price = scaler.inverse_transform(VWAP_price)","3cc90d57":"# Results\nrmse = np.sqrt(mean_squared_error(valid, VWAP_price))\nprint(rmse)","fb4d9239":"# Plot\ntrain = new_data[:4000]\nvalid = new_data[4000:]\nvalid['prediction'] = VWAP_price\nplt.plot(train['VWAP'])\nplt.plot(valid[['VWAP', 'prediction']])","1a771d3d":"plt.figure(figsize=(15,9))\nplt.plot(valid[['VWAP', 'prediction']])","d675b097":"### 7. Long Short Term Memory(LSTM)","11d6af3d":"### 5. Auto Arima with feature engineering","105807a4":"#### Introduction   \n\nLSTMs are widely used for sequence prediction problems and have proven to be extremely effective. The reason they work so well is because LSTM is able to store past information that is important, and forget the information that is not. LSTM has three gates:\n\n**The input gate:** The input gate adds information to the cell state.  \n**The forget gate:** It removes the information that is no longer required by the model.  \n**The output gate:** Output Gate at LSTM selects the information to be shown as output","d537c96e":"### 6. Facebook Prophet  \n\n**Introduction**  \nThere are a number of time series techniques that can be implemented on the stock prediction dataset, but most of these techniques require a lot of data preprocessing before fitting the model. Prophet, designed and pioneered by Facebook, is a time series forecasting library that requires no data preprocessing and is extremely simple to implement. The input for Prophet is a dataframe with two columns: date and target (ds and y).\n\nProphet tries to capture the seasonality in the past data and works well when the dataset is large. ","230f9d89":"#### Inference\nNot at all a good model","ff5fb920":"#### Inference\nThe RMSE is 318, that is less than the earlier model but still very poor prediction","5f344a35":"The predicted closing price for each day will be the average of a set of previously observed values. Instead of using the simple average, we will be using the moving average technique which uses the latest set of values for each prediction. In other words, for each subsequent step, the predicted values are taken into consideration while removing the oldest observed value from the set.","ce687631":"**data is from 2000-01-03 to 2020-05-29**","796ced73":"Let's add some basic features like lag values of available numeric features that are widely used for time series problems. Since we need to predict the price of the stock for a day, we cannot use the feature values of the same day since they will be unavailable at actual inference time. We need to use statistics like mean, standard deviation of their lagged values.\n\nWe will use three sets of lagged values, one previous day, one looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics.","be71bbc7":"### 5. Auto ARIMA","132a81ed":"## About the Notebook\nIn this notebook, I implemented different models to predict the Stock Price. Although, Pridicting the performance of stock market is quite a difficult task as there are so many factors on which stock price depend yet Machine learning provides a potential plateform to unearth the patterns and insights that are difficult to look into otherwise.  \n\nI would like to acknowledge *Analytics Vidhya* for guiding me throughtout this notebook.","1bce2eeb":"### 1. Understanding the Problem Statement\nIdentify the trends and predicting the stock price by reading charts and statistical figures.  \nHere I am using the nifty dataset and out of all the stocks, **Reliance** stock will be studied in this notebook. ","4da64230":"### 4. k-Nearest Neighbours","ce1a411b":"#### Inference\nLSTM Model done a great job as compared to the earlier models.","d5fa3141":"This creates features such as:\n\n\u2018Year\u2019, \u2018Month\u2019, \u2018Week\u2019, \u2018Day\u2019, \u2018Dayofweek\u2019, \u2018Dayofyear\u2019, \u2018Is_month_end\u2019, \u2018Is_month_start\u2019, \u2018Is_quarter_end\u2019, \u2018Is_quarter_start\u2019,  \u2018Is_year_end\u2019, and  \u2018Is_year_start\u2019.","f8c2a4a0":"### 3. Linear Regression","869b77ba":"#### Inference  \nThe RMSE value is 334 but from the plot it can be seen that results are not vary promising","da86d1b5":"Splitting the data into train and validation along with features","88fb8eda":"**Note** While splitting the data into train and validation set, we can't use random splitting since that will destroy the time component. So, here we have set 2 year's data into validation and 4 years data before that into train set.","3a7789ed":"#### Feature Engineering","ba06a40e":"## About the dataset\nThere are multiple variables in the dataset \u2013 date, open, high, low, last, close, total_trade_quantity, and turnover.\n\n* The columns Open and Close represent the starting and final price at which the stock is traded on a particular day.\n* High, Low and Last represent the maximum, minimum, and last price of the share for the day.\n* The historic VWAP (Volume Weighted Average Price) is the target variable to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.","3a8a2ed6":"## Table of Contents:\n1. Understanding the Problem Statement\n2. Moving Average\n3. Linear Regression\n4. k-Nearest Neighbors\n5. Auto ARIMA\n6. Facebook Prophet\n7. Long Short Term Memory (LSTM)","a2020501":"### 2. Moving Average","50359baa":"**Note**  Here I am dropping the Columns having null value (Trades, Deliverable Volume and %Deliverble)"}}