{"cell_type":{"f46b4ec3":"code","f4f564a1":"code","175775b6":"code","46c0de97":"code","50dee2df":"code","ef37012a":"code","6d3a7739":"code","93bc90ef":"code","4f616e33":"code","ea34a182":"code","2bf28e51":"code","174c6a54":"code","2f1dd34c":"code","a4393534":"code","757a4d06":"code","a7abaac6":"code","1eb922b2":"code","8ee84947":"code","10d5b8c8":"code","768c92f7":"code","e4b25052":"code","147169ed":"code","d7ee9495":"code","f19d70d8":"code","d9e084df":"code","43ffcfd1":"code","77b30920":"code","493d5043":"code","80699924":"code","e196aa22":"code","86d790c2":"code","98579c02":"markdown","d9383f8d":"markdown","71b3f709":"markdown","6dbccb60":"markdown","59f49a55":"markdown","3800fbba":"markdown","5e6f0ed0":"markdown","f01540dc":"markdown","9328f949":"markdown","bbc94861":"markdown","28a824c0":"markdown","1faa0053":"markdown","77b96af4":"markdown","dec5fb3f":"markdown","6e6adeac":"markdown","3f58d53a":"markdown","05a80433":"markdown"},"source":{"f46b4ec3":"import numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport os\nimport math\nfrom PIL import Image, ImageOps\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport numbers\nimport pandas as pd\nimport argparse\nimport os.path as osp\n\nfrom sklearn.preprocessing import StandardScaler\nimport random\nimport torch.utils.data as util_data\nimport matplotlib.pyplot as plt","f4f564a1":"class Config():  \n    training_dir = \"..\/input\/digixai-image-retrieval\/train_data\/\"\n    label_dir = \"..\/input\/digixai-image-retrieval\/train_data\/label.txt\"\n    gallery_dir = \"..\/input\/digixalgoai\/test_data_B\/gallery\/\"\n    query_dir = \"..\/input\/digixalgoai\/test_data_B\/query\/\"\n    outfile = \"\/content\/drive\/My Drive\/log.txt\"\n    feature_file = \".\/features.pth\"\n    model = \".\/model.pth\"\n    \n    feature_batch_size = 64            #batch size used while extracting features\n    train_batch_size = 128             #batch size used while training the network with extracted features\n    train_number_epochs = 50          #batch size used while extracting features\n    #some tranformations to be applied on the images\n    transformer = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])","175775b6":"def imshow(img,text=None,should_save=False):\n    plt.figure(figsize=(21,12))\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.figure(figsize=(14,8))\n    plt.plot(iteration,loss)\n    plt.show()","46c0de97":"def pairwise_loss_updated(outputs1,outputs2,label1,label2):\n    dot_product = torch.mm(outputs1, outputs2.t())\n    similarity = (label1.view(-1,1) == label2.view(-1,1).t()).float()\n    \n    mask_positive = similarity.data > 0\n    mask_negative = similarity.data <= 0\n    exp_loss = torch.log(1+torch.exp(dot_product)) -similarity * dot_product\n\n    #weight\n    S1 = torch.sum(mask_positive.float())\n    S0 = torch.sum(mask_negative.float())\n    S = S0+S1\n\n    exp_loss[similarity.data > 0] = exp_loss[similarity.data > 0] * (S \/ S1)\n    exp_loss[similarity.data <= 0] = exp_loss[similarity.data <= 0] * (S \/ S0)\n\n    loss = torch.sum(exp_loss) \/ S\n    return loss","50dee2df":"resnet_dict = {\"ResNet18\":models.resnet18, \"ResNet34\":models.resnet34, \"ResNet50\":models.resnet50, \"ResNet101\":models.resnet101, \"ResNet152\":models.resnet152} \nclass ResNetFc(nn.Module):\n    def __init__(self, name, hash_bit):\n        super(ResNetFc, self).__init__()\n        model_resnet = resnet_dict[name](pretrained=True)\n        self.conv1 = model_resnet.conv1\n        self.bn1 = model_resnet.bn1\n        self.relu = model_resnet.relu\n        self.maxpool = model_resnet.maxpool\n        self.layer1 = model_resnet.layer1\n        self.layer2 = model_resnet.layer2\n        self.layer3 = model_resnet.layer3\n        self.layer4 = model_resnet.layer4\n        self.avgpool = model_resnet.avgpool\n\n        #convolution layers\n        self.feature_layers = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, \\\n                             self.layer1, self.layer2, self.layer3, self.layer4, self.avgpool)\n\n\n        #feature layers\n        self.fc = nn.Sequential(nn.Linear(model_resnet.fc.in_features,512),\n                                nn.Dropout(0.2),\n                                nn.ReLU()\n                                )\n        #hash layers\n        self.hash_layer = nn.Linear(512, hash_bit)\n        #initializing the layers\n        self.hash_layer.weight.data.normal_(0, 0.01)\n        self.hash_layer.bias.data.fill_(0.0)\n\n        self.iter_num = 0\n        self.__in_features = hash_bit\n        self.step_size = 200\n        self.gamma = 0.005\n        self.power = 0.5\n        self.init_scale = 1.0\n        self.activation = nn.Tanh()\n        self.scale = self.init_scale\n\n    #passing the extracted features through the rest of the network for training\n    def train_from_features(self,x):\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        y = self.hash_layer(x)\n        if self.iter_num % self.step_size==0:\n            self.scale = self.init_scale * (math.pow((1.+self.gamma*self.iter_num), self.power))\n        y = self.activation(self.scale*y)\n        return y\n\n    #passing only through the convolution layers to extract intermediate features\n    def extractor(self,x):\n        x = self.feature_layers(x)    \n        return x\n    \n    #passing through the whole network at once\n    def forward(self, x):\n        if self.training:\n            self.iter_num += 1\n        x = self.feature_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        y = self.hash_layer(x)\n        if self.iter_num % self.step_size==0:\n            self.scale = self.init_scale * (math.pow((1.+self.gamma*self.iter_num), self.power))\n        y = self.activation(self.scale*y)\n        return y\n\n    def output_num(self):\n        return self.__in_features","ef37012a":"class train_dataset(Dataset):\n\n    def __init__(self, encoding_dict):\n\n      self.encodings = encoding_dict['encodings']\n      labels = encoding_dict['labels']\n      self.labels = labels\n\n    def __len__(self):  \n      return len(self.encodings)\n\n    def __getitem__(self, idx):\n      if torch.is_tensor(idx):\n        idx = idx.tolist()\n\n      label1 = encoding_dict['labels'][idx]\n      \n      should_get_same_class = random.randint(0,1) \n\n      if should_get_same_class:\n          while True:\n                #keep looping till the same class image is found\n                idx2 = idx + random.randint(-10,10)\n                while (idx2>=len(encoding_dict['encodings']) or idx2<0):\n                    idx2 = idx + random.randint(-10,10)\n                label2 = encoding_dict['labels'][idx2] \n                if label1 == label2:\n                    break\n                    \n      else:\n          while True:\n                #keep looping till a different class image is found        \n                idx2 = int(random.randint(0,len(encoding_dict['encodings'])-1))\n                label2 = encoding_dict['labels'][idx2] \n                if label1 != label2:\n                    break\n\n\n      img1 = encoding_dict['encodings'][idx]\n      img2 = encoding_dict['encodings'][idx2]\n      return img1, img2 , label1, label2","6d3a7739":"class Encodings(Dataset):\n\n    def __init__(self, image_dir, path, transform=None):\n\n        data = pd.read_csv(path, header=None)\n        imgs = data.iloc[:,0]\n        self.img_path = image_dir+imgs\n        self.labels = data.iloc[:,1]\n        #self.data =  pd.DataFrame(img_list)\n        self.image_dir = image_dir\n        self.transform = transform\n        \n\n    def __len__(self):\n        return len(self.img_path)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = self.img_path.iloc[idx]\n        image = Image.open(img_name)\n        image = image.convert(\"RGB\")\n        \n\n        if self.transform:\n            image = self.transform(image)\n\n        sample = {'image':image, 'name':self.img_path.iloc[idx], 'labels':self.labels.iloc[idx]}\n\n        return sample","93bc90ef":"def feature_extracter(encoding_dataloader, net, verbosity):\n  net.eval()\n  with torch.no_grad(): \n    for i_batch, sample in enumerate(encoding_dataloader):\n      if torch.cuda.is_available():\n        sample['image'] = sample['image'].cuda()\n      encoding = net.extractor(sample['image'])\n      if not i_batch: \n        encodings = encoding\n        names = sample['name']\n        labels = sample['labels']\n        continue\n      encodings = torch.cat([encodings, encoding], dim=0)\n      labels = torch.cat([labels,sample['labels']],dim=0)\n      names.extend(sample['name'])\n      if verbosity: \n        for img in sample['name']:\n          print(\"{} succesfully extracted\".format(img))\n  return encodings, names, labels","4f616e33":"    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' \n    config = dict()\n    config['epoch'] = 50\n    config[\"hash_bit\"] = 48\n   \n    config[\"network\"] = {}\n    config[\"network\"][\"type\"] = ResNetFc\n    config[\"network\"][\"params\"] = {\"name\":\"ResNet50\", \"hash_bit\":config[\"hash_bit\"]}\n    hash_bit = config[\"hash_bit\"]\n\n    ## set base network\n    net_config = config[\"network\"]\n    base_network = net_config[\"type\"](**net_config[\"params\"])\n\n    use_gpu = torch.cuda.is_available()\n    if use_gpu:\n        base_network = base_network.cuda()\n\n    ## collect parameters\n    parameter_list = [{\"params\":base_network.fc.parameters()},\n                      {\"params\":base_network.hash_layer.parameters()}]\n \n    optimizer = optim.Adam(parameter_list, lr=0.0001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n    ","ea34a182":"encoding_dataset = Encodings(Config.training_dir, Config.label_dir, Config.transformer)\nencoding_dataloader = DataLoader(encoding_dataset,\n                        shuffle=False,\n                        num_workers=8, \n                        batch_size=Config.train_batch_size)","2bf28e51":"extracted_feats = feature_extracter(encoding_dataloader, base_network, verbosity=0)\nextracted_features = {'encodings':extracted_feats[0], 'names':extracted_feats[1],'labels':extracted_feats[2]}","174c6a54":"torch.save(extracted_features,Config.feature_file)","2f1dd34c":"encoding_dict = torch.load(Config.feature_file)","a4393534":"train_set = train_dataset(encoding_dict)\ntrain_loader = util_data.DataLoader(train_set,\n                                    batch_size=Config.train_batch_size,\n                                    shuffle=True, num_workers=0)\n    \nlen_train = len(train_loader) - 1\ntransfer_loss_value = classifier_loss_value = total_loss_value = 0.0\nbest_acc = 0.0\n\ncounter = []\nloss_history = [] \niteration_number= 0","757a4d06":"print(base_network)","a7abaac6":"base_network.train(True)\n\nfor epoch in range(config['epoch']+1):  \n  \n    iterator = iter(train_loader)\n  \n    for i in range(len_train):\n        \n        optimizer.zero_grad()\n        inputs1, inputs2, labels1, labels2 = iterator.next()\n        \n        if use_gpu:\n            inputs1, inputs2, labels1, labels2 = \\\n                inputs1.cuda(), inputs2.cuda(), \\\n                labels1.cuda(), labels2.cuda()\n        else:\n            inputs1, inputs2, labels1, labels2 = inputs1, \\\n                inputs2, labels1, labels2\n            \n        inputs = torch.cat((inputs1, inputs2), dim=0)\n        outputs = base_network.train_from_features(inputs)\n        similarity_loss = pairwise_loss_updated(outputs.narrow(0,0,inputs1.size(0)), \\\n                                 outputs.narrow(0,inputs1.size(0),inputs2.size(0)), \\\n                                 labels1, labels2)\n                                 \n        similarity_loss.backward()\n        #print(\"Epoch : %d, batch : %d, loss: %.6f\" % (epoch,i, similarity_loss.item()))\n        optimizer.step()\n        \n        if i%10==0:\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(similarity_loss.item())\n        \n    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,similarity_loss.item()))\n    \n    scheduler.step()","1eb922b2":"torch.save(base_network.state_dict(),Config.model)","8ee84947":"show_plot(counter[100:],loss_history[100:])","10d5b8c8":"class testing_dataset(Dataset):\n\n\n    def __init__(self, image_dir, transform=None):\n        \n        img_list = os.listdir(image_dir)\n        self.data =  pd.DataFrame(img_list)\n        self.image_dir = image_dir\n        self.transform = transform        \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.image_dir,\n                                self.data.iloc[idx, 0])\n        image = Image.open(img_name)\n        image = image.convert(\"RGB\")\n\n        if self.transform:\n          image = self.transform(image)\n\n        sample = {'image':image, 'name':self.data.iloc[idx,0]}\n\n        return sample","768c92f7":"base_network.load_state_dict(torch.load(Config.model))","e4b25052":"#this function passes all the images through the network and returns the hashbits\n\ndef encoding_extractor(dataloader, net, verbosity):\n  net.eval()\n  with torch.no_grad(): \n    for i_batch, sample in enumerate(dataloader):\n      sample['image'] = sample['image'].cuda()\n      encoding = net(sample['image'])\n      if not i_batch: \n        encodings = encoding\n        names = sample['name']\n        continue\n      encodings = torch.cat([encodings, encoding], dim=0)\n      #names = pd.concat([names, sample['name']], axis=0)\n      names.extend(sample['name'])\n      if verbosity: \n        for img in sample['name']:\n          print(\"{} succesfully extracted\".format(img))\n  return encodings, names","147169ed":"def Euclidean_distance(verbosity):\n    result=[]\n    gallery = gallery_params['encodings']\n\n    for query in query_params['encodings']:\n      i=0\n      while(i<gallery.shape[0]):\n        matrix = torch.sum((gallery[i:i+256]-query).square(), dim=1)\n        if not i: dissimilarity_matrix = matrix\n        else: dissimilarity_matrix = torch.cat([dissimilarity_matrix, matrix], dim=0)\n        i = i+256\n      top_10 = dissimilarity_matrix.argsort(dim=0)[:10].cpu()\n      if verbosity: print(top_10)\n      result.append(top_10)\n    return result\n\ndef Hamming_distance(verbosity):\n    result=[]\n    gallery = gallery_params['encodings']\n\n    for query in query_params['encodings']:\n      i=0\n      while(i<gallery.shape[0]):\n        matrix = torch.matmul(gallery[i:i+1024], query.view(-1,1)) * (-1.0)\n        if not i: similarity_matrix = matrix\n        else: similarity_matrix = torch.cat([similarity_matrix, matrix], dim=0)\n        i = i+1024\n      top_10 = similarity_matrix.argsort(dim=0)[:10].view(-1,).cpu()\n      if verbosity: print(top_10)\n      result.append(top_10)\n    return result","d7ee9495":"gallery_encoder = testing_dataset(Config.gallery_dir,\n                                    transform=Config.transformer)\n\ngallery_dataloader = DataLoader(gallery_encoder,\n                        shuffle=False,\n                        num_workers=8, \n                        batch_size=Config.feature_batch_size)","f19d70d8":"query_encoder = testing_dataset(Config.query_dir,\n                              transform=Config.transformer)\n\nquery_dataloader = DataLoader(query_encoder,\n                        shuffle=False,\n                        num_workers=8, \n                        batch_size=Config.feature_batch_size)","d9e084df":"query_feats, query_images = encoding_extractor(query_dataloader, base_network, verbosity=0)\nquery_params = {'encodings':query_feats, 'names':query_images}","43ffcfd1":"gallery_feats, gallery_images = encoding_extractor(gallery_dataloader,base_network,  verbosity=0)\ngallery_params = {'encodings':gallery_feats, 'names':gallery_images}","77b30920":"result = Euclidean_distance(verbosity=0)","493d5043":"output = pd.DataFrame()\n\ngallery_names = pd.Series(gallery_params['names'])\n\nfor img_num in range(len(result)):\n    indexes = result[img_num].tolist()\n    retrieved = gallery_names[indexes].to_list()\n    output[query_params['names'][img_num]] = retrieved","80699924":"output = output.transpose()\noutput.reset_index(inplace=True)","e196aa22":"output.to_csv('.\/submission.csv',header=None,index=None)","86d790c2":"result = pd.read_csv('.\/submission.csv',header=None)\n\nnum = 121       # choose any integer between [0,41573]\n\nquery_img = Image.open(Config.query_dir+result.iloc[num,0])\nquery_img = Config.transformer(query_img).unsqueeze(0)\n\nimg_list = query_img\nfor img in result.iloc[num,1:]:\n    image = Image.open(Config.gallery_dir+img)\n    image = Config.transformer(image).unsqueeze(0)\n    img_list = torch.cat([img_list,image])\n\nimshow(torchvision.utils.make_grid(img_list, nrow=img_list.shape[0]))","98579c02":"## Train setup\n\nWe shall setup the training hyper parameters and other stuff.","d9383f8d":"## Train","71b3f709":"## Output","6dbccb60":"# **Image Retrieval Using HASHNET**","59f49a55":"Accurate image retrieval is a core technology for shopping by picture taking, and also becomes a hotspot in the academia and industry. Here, we take a digital device image dataset in a real snap-to-shop scenario, provided in Huawei DIGIX Global AI Challenge. In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the semantic gap.\n\nDeep Hashing methods are most commonly used to implement content based image retrieval. If you do not know about image retrieval Check out this [paper](https:\/\/arxiv.org\/pdf\/2006.05627.pdf).\n\nOut of all the Deep Hashing methods we shall implement the Hashnet for content based image retrieval. Here is the [paper](https:\/\/arxiv.org\/pdf\/1702.00758.pdf).\n\nIf you find the notebook insightful, please do upvote it.","3800fbba":"## Testing \n\nLets run our code on the test dataset. The test dataset comprises of \n\n1. Gallery images\n2. Query images\n\nFor every query image we have to find the gallery images which are similar to the query images. So first we shall pass both the query as well as gallery images through the network and then comparing the hash bits find the similar images.","5e6f0ed0":"## Feature extractor","f01540dc":"Add train data to your notebook using this dataset https:\/\/www.kaggle.com\/varenyambakshi\/digixai-image-retrieval.","9328f949":"## Extracting Features","bbc94861":"Here we define two types of distances namely : Euclidean distance and Hamming distance. \n\n* Euclidean distance: gives slightly better results but takes more time\n* Hamming distance: gives slightly poorer results but takes less time","28a824c0":"## Dataset\n\nDefining dataset class for the images. It takes the images applies the tranformations and returns them.","1faa0053":"Now defining the dataset and the dataloader.","77b96af4":"## Helper Functions\n\nHere we define two helper functions. `imshow` is for displaying images while `show_plot` is for plotting the loss function.","dec5fb3f":"## Loss\n\n\nHere we define the loss function of Hashnet. We basically pass two batches of images encoded in hashbits where some images are semantically (content-wise) similar while others are different. The more the distance between hashbits of similar images, the more heavily loss function punishes and vice-versa. To know about the maths you can refer the [paper](https:\/\/arxiv.org\/pdf\/1702.00758.pdf).  \n\n","6e6adeac":"Add test data to your notebook using this dataset https:\/\/www.kaggle.com\/varenyambakshi\/digixalgoai.","3f58d53a":"## Network\n\nAlthough there are many models available, we shall be using RESNET50. We will have broadly three types of layers namely \n1. Convolution layers\n1. Fully connected layers \n1. Hashing layer\n\nInstead of training the whole network at once we can use transfered learing and train few of the last layers. To further speed up training we can extract the intermediate feaures so that we do not have to pass the images everytime through the whole network. Instead we can just train on the extracted features.","05a80433":"## Setup\n\nLets start by importing necessary packages. Also make sure that you have added Google Cloud Services to your notebook (if not go to the \"Add-ons\" section and attach it to your notebook) and are using GPU."}}