{"cell_type":{"3eab01cf":"code","9a283239":"code","782a4500":"code","3e1accd5":"code","8f3e914e":"code","d2f9ad56":"code","91103ff5":"code","c3b3e025":"code","5eaba1ff":"code","8d1047e0":"code","c139ce81":"code","cdbda955":"code","3cb10dda":"code","145e19cd":"code","35168ac3":"code","77a0539f":"code","2cba276f":"code","76a8592e":"code","2dc531d5":"code","a2e9ed07":"code","190c94e4":"code","ffc497b3":"code","72c7a766":"code","15ef6af8":"code","aab71bd4":"code","b1d06591":"code","a84fe4a6":"code","90dfa73a":"code","eab3b9d3":"code","bbb631e4":"code","e8fcde7e":"code","735e4169":"code","23ece94a":"code","068ec951":"code","940be942":"code","f7c14001":"code","15bee3fb":"code","431ad1f4":"code","4ac43a73":"code","31845f08":"code","a5bc5729":"code","12c090db":"code","04fe33fc":"code","2de5da62":"code","7c6e7e76":"code","a0d73d58":"code","81ed5695":"code","1b533d51":"code","dd629554":"code","baef2248":"code","07682a24":"code","915a182f":"code","3cd865de":"code","04041d4a":"code","233c968c":"code","8d90eedf":"code","b06ad59f":"markdown"},"source":{"3eab01cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9a283239":"import pandas as pd\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","782a4500":"test.describe()","3e1accd5":"train.describe()","8f3e914e":"train.columns.values","d2f9ad56":"test.columns.values","91103ff5":"train.shape","c3b3e025":"train.head(2)","5eaba1ff":"test.head(2)","8d1047e0":"#locate missing values within the train columns\npercent_null = train.isnull().sum()\/train.isnull().count()\npercent_null.sort_values(ascending=False)\n#we will drop the Cabin featue later due to its incompleteness","c139ce81":"#locate missing values within the test columns\npercent_null = test.isnull().sum()\/test.isnull().count()\npercent_null.sort_values(ascending=False)","cdbda955":"train['Embarked'].value_counts()","3cb10dda":"train['Age'] = train['Age'].fillna(train.mean)","145e19cd":"test.describe()\n#some of the age and fare rates are missing.","35168ac3":"#locate missing values within the features\npercent_null_test = test.isnull().sum()\/test.isnull().count()\npercent_null_test.sort_values(ascending=False)","77a0539f":"test.shape","2cba276f":"#Survival rate\n#1 denotes a survived passenger\ntrain.groupby('Pclass').Survived.value_counts()","76a8592e":"survived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived)) \/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","2dc531d5":"train [['Survived','Pclass']].groupby(['Pclass'], as_index=False).mean()","a2e9ed07":"train.corr()[\"Survived\"].sort_values(ascending=False)","190c94e4":"#drop any column within +-.1 of 0\ndrop_columns = ['PassengerId','Parch','SibSp','Ticket','Cabin']\nfor column in drop_columns:\n    train = train.drop(column, axis=1)","ffc497b3":"#look at data after we remove unneeded features\ntrain.describe()","72c7a766":"train.head()","15ef6af8":"#creates a columns of the Title\ntrain_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')","aab71bd4":"train.head()","b1d06591":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","a84fe4a6":"# make it numeric\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","90dfa73a":"for dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","eab3b9d3":"#check our progress\ntrain.tail()","bbb631e4":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","e8fcde7e":"for dataset in train_test_data:\n    #print(dataset.Embarked.unique())\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","735e4169":"train.columns.values","23ece94a":"#locate missing values within the features\npercent_null = train.isnull().sum()\/train.isnull().count()\npercent_null.sort_values(ascending=False)\n#we will drop the Cabin featue later due to its incompleteness","068ec951":"train.Embarked.value_counts()\n# notice there is not any more nan values","940be942":"test.columns.values","f7c14001":"#let drop more festures\ndrop_train = ['Name','Age']\nfor column in drop_train:\n    train = train.drop(column, axis=1)\ndrop_test = ['Cabin','Name','Ticket','Age','Parch','SibSp']\nfor column in drop_test:\n    test = test.drop(column,axis=1)","15bee3fb":"test.head()","431ad1f4":"#make sure columns align between train and test data sets\ntrain.columns.values","4ac43a73":"test.columns.values","31845f08":"#need to concat the people_dummies with the numerical values","a5bc5729":"test=test.fillna(test.mean())","12c090db":"X_train = train.drop('Survived', axis=1)\ny_train = train['Survived']\nX_test = test.drop(\"PassengerId\", axis=1).copy()\n\nX_train.shape, y_train.shape, X_test.shape","04fe33fc":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","2de5da62":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_log_reg) + ' percent')","7c6e7e76":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train)*100, 3)\nprint(str(acc_svc) + ' percent')","a0d73d58":"#decision tree\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision = round(clf.score(X_train, y_train)*100, 2)\nprint (str(acc_decision)+ ' percent')","81ed5695":"y_pred_decision_tree = clf.predict(X_test)","1b533d51":"y_pred_decision_tree","dd629554":"submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':y_pred_decision_tree})","baef2248":"submission.head()","07682a24":"filename = 'Titanic Predictions 2.csv'\n\nsubmission.to_csv(filename,index=False)\nprint('Saved File: ' + filename)","915a182f":"#incorporate xgboost\n#xg boost here\nclf = XGBClassifier()\nclf.fit(X_train, y_train)\ny_pred_XGBClassifier = clf.predict(X_test)\nacc_XGB = round(clf.score(X_train, y_train)*100, 2)\nprint(str(acc_XGB)+ ' percent')","3cd865de":"y_pred_XGBClassifier","04041d4a":"y_pred_decision_tree = clf.predict(X_test)","233c968c":"submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':y_pred_decision_tree})","8d90eedf":"filename = 'Titanic Predictions xgb.csv'\n\nsubmission.to_csv(filename,index=False)\nprint('Saved File: ' + filename)","b06ad59f":"Model Training"}}