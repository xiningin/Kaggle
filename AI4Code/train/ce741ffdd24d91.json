{"cell_type":{"00dd4966":"code","1274ac63":"code","eec06d42":"code","bb357c91":"code","74ccb70f":"code","bfc847ea":"code","c68e4f95":"code","8cf2a0f8":"code","c83b4a1c":"code","cd3200c9":"code","a0461205":"code","eeeb7a1c":"code","f218136b":"code","9fd9bcec":"code","f706ddda":"code","653d5da2":"code","646332c5":"code","828f6cfc":"code","c0fef030":"code","5f7e358b":"code","2d7e220a":"code","d74240ad":"code","6b873559":"code","866daaca":"markdown","cba4882f":"markdown","485e90c7":"markdown","1e0b2596":"markdown","93e60482":"markdown","8cb3097b":"markdown","8ffdc340":"markdown","a2e8ac5e":"markdown","802241e5":"markdown","308f01a7":"markdown","ed99c3e3":"markdown"},"source":{"00dd4966":"%matplotlib inline\n\nimport tensorflow as tf\nfrom scipy.io import loadmat\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential , load_model\nfrom tensorflow.keras.layers import Dense , Flatten ,Conv2D , MaxPool2D , Dropout , BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.io import loadmat\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np","1274ac63":"# Load the dataset from your Drive folder\n\ntrain = loadmat('..\/input\/svhn-dataset\/train_32x32.mat')\ntest = loadmat('..\/input\/svhn-dataset\/test_32x32.mat')","eec06d42":"x_train = train[\"X\"]\ny_train = train[\"y\"]\nx_test = test[\"X\"]\ny_test = test[\"y\"]\nx_train.shape , y_train.shape","bb357c91":"fig , ax = plt.subplots(1, 10 , figsize = (15 , 3))\nfor i in range(10):\n    \n    ax[i].set_axis_off()\n    sample = random.randint(1 , x_train.shape[3])\n    ax[i].imshow(x_train[...,sample])\n    ax[i].set_title(y_train[sample])","74ccb70f":"# make 3 channel images grayscal images\nx_train_greyScale = np.average(x_train, axis=2)\n# expand dimention so that the channel dim is 1\nx_train_greyScale = np.expand_dims(x_train_greyScale, axis=2)\n\nx_test_greyScale = np.average(x_test, axis=2)\nx_test_greyScale = np.expand_dims(x_test_greyScale, axis=2)\n\nx_train_greyScale = np.transpose(x_train_greyScale, (3, 0, 1, 2))\nx_test_greyScale = np.transpose(x_test_greyScale , (3, 0, 1, 2))\n\nprint('the dimention of training data before processing ' , x_train.shape)\n\nprint('the dimention of training data after processing ' , x_train_greyScale.shape)\n         \n\nenc = OneHotEncoder().fit(y_train)\ny_train_oh = enc.transform(y_train).toarray()\ny_test_oh = enc.transform(y_test).toarray()\n\nprint('training labels before processing ' , y_train.shape)  \nprint('training labels after processing ' , y_train_oh.shape)             ","bfc847ea":"y_train_oh[1]    ","c68e4f95":"def get_MLP_model():\n    model = Sequential([Flatten(input_shape= [32 , 32 , 1]),\n                    Dense(256 , activation = 'relu'),\n                    Dense(128 , activation='relu'),\n                    Dense(64  , activation = 'relu'),\n                    Dense(10 , activation= 'softmax') \n                    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam() , \n                  loss = 'categorical_crossentropy' ,\n                  metrics=['accuracy'])\n    \n    return model\n","8cf2a0f8":"mlp_model = get_MLP_model()\nmlp_model.summary()","c83b4a1c":"earlyStopping = EarlyStopping(monitor='val_loss', patience=5 , mode='min', verbose=1)\n\ncallbacks = [earlyStopping] ","cd3200c9":"training_history = mlp_model.fit(x_train_greyScale , y_train_oh ,\n                             validation_split=0.2 , epochs=20 ,\n                             batch_size = 64 , verbose = 1 ,\n                             callbacks=callbacks)","a0461205":"try:\n    plt.plot(training_history.history['accuracy'])\n    plt.plot(training_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(training_history.history['acc'])\n    plt.plot(training_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","eeeb7a1c":"plt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss') \nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","f218136b":"test_history = mlp_model.evaluate(x_test_greyScale , y_test_oh , verbose=2)","9fd9bcec":"def get_CNN_model():\n    CNN_model = Sequential([Conv2D(16 , kernel_size = (3,3), input_shape = (32,32,1),\n                                   padding = \"SAME\" , activation= 'relu',\n                                   kernel_initializer='he_uniform'),\n                            MaxPool2D(pool_size= (2,2)),\n                            Flatten(),\n                            Dropout(0.2),\n                            BatchNormalization(),\n                            Dense(64 , activation='relu'),\n                            Dense (10 , activation='softmax')])\n    CNN_model.compile(optimizer='adam' , loss = 'categorical_crossentropy'\n                      , metrics=['accuracy'])\n    \n    return CNN_model","f706ddda":"cnn_model = get_CNN_model()\ncnn_model.summary()","653d5da2":"CNN_earlyStopping = EarlyStopping(monitor='val_loss', \n                                  patience=5 ,\n                                  mode='min', verbose=1)\nCNN_callbacks = [CNN_earlyStopping] ","646332c5":"CNN_history = cnn_model.fit(x_train_greyScale , y_train_oh , epochs=10 ,\n                            batch_size=64 , callbacks=CNN_callbacks,\n                            validation_split = 0.2,\n                             verbose=1) ","828f6cfc":"try:\n    plt.plot(CNN_history.history['accuracy'])\n    plt.plot(CNN_history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(CNN_history.history['acc'])\n    plt.plot(CNN_history.history['val_acc'])\nplt.title('Accuracy vs. epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","c0fef030":"plt.plot(CNN_history.history['loss'])\nplt.plot(CNN_history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss') \nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show() ","5f7e358b":"CNN_test_history = cnn_model.evaluate(x_test_greyScale , y_test_oh , verbose=2)","2d7e220a":"num_test_images = x_test_greyScale.shape[0]\n\nrandom_inx = np.random.choice(num_test_images, 5)","d74240ad":"# CNN model \nrandom_test_images = x_test_greyScale[random_inx, ...]\nrandom_test_labels = y_test[random_inx, ...]\n\npredictions_CNN = cnn_model.predict(random_test_images)\n\nfig, axes = plt.subplots(5, 2, figsize=(20, 12))\nfig.subplots_adjust(hspace=0.4, wspace=-0.2)\n\nfor i, (prediction_CNN, image, label) in enumerate(zip(predictions_CNN , random_test_images, random_test_labels)):\n    axes[i, 0].imshow(np.squeeze(image))\n    axes[i, 0].get_xaxis().set_visible(False)\n    axes[i, 0].get_yaxis().set_visible(False)\n    axes[i, 0].text(10., -3, f'Digit {label}')\n    axes[i, 1].bar(np.arange(1,11), prediction_CNN)\n    axes[i, 1].set_xticks(np.arange(1,11))\n    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {np.argmax(prediction_CNN) + 1}\") \nplt.show()","6b873559":"# MLP model\nrandom_test_images = x_test_greyScale[random_inx, ...]\nrandom_test_labels = y_test[random_inx, ...]\n\npredictions_MLP = mlp_model.predict(random_test_images)\n\nfig, axes = plt.subplots(5, 2, figsize=(20, 12))\nfig.subplots_adjust(hspace=0.4, wspace=-0.2)\n\nfor i, (prediction_MLP, image, label) in enumerate(zip(predictions_MLP , random_test_images, random_test_labels)):\n    axes[i, 0].imshow(np.squeeze(image))\n    axes[i, 0].get_xaxis().set_visible(False)\n    axes[i, 0].get_yaxis().set_visible(False)\n    axes[i, 0].text(10., -3, f'Digit {label}')\n    axes[i, 1].bar(np.arange(1,11), prediction_MLP)\n    axes[i, 1].set_xticks(np.arange(1,11))\n    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {np.argmax(prediction_MLP) + 1}\") \nplt.show()","866daaca":"# importing libraries","cba4882f":"# CNN model","485e90c7":"## training the mlp model","1e0b2596":"## Mlp model","93e60482":"# MLP model","8cb3097b":"## mlp training and validation accuracy plot","8ffdc340":"## Cnn model","a2e8ac5e":"# loading and processing data","802241e5":"## cnn model evaluation","308f01a7":"# Making prediction","ed99c3e3":"## mlp model evaluation"}}