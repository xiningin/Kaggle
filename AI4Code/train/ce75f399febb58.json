{"cell_type":{"e2a9e32b":"code","0d8938bb":"code","32c560a2":"code","2003c1af":"code","56f2941e":"code","b736be17":"code","e368494a":"code","798c4d40":"code","2042bf8e":"code","0bd04e5b":"code","28007708":"code","54cd6936":"code","24a78bcd":"code","158d8e32":"markdown","3b248b93":"markdown","2c11cd03":"markdown","1191a0c1":"markdown","ac09d511":"markdown","5bb5c967":"markdown","cf4b8136":"markdown","3999f8d9":"markdown","3102ee11":"markdown","61faee22":"markdown","98cf663e":"markdown","b7a4de94":"markdown","35026fa0":"markdown","e010e8d9":"markdown","71d1b0c6":"markdown","cdab51bd":"markdown","2d0a6aa4":"markdown","9db86363":"markdown","82e20558":"markdown","27be3120":"markdown","292b0920":"markdown","36d592bd":"markdown","e0945aa8":"markdown","caa373a9":"markdown","b18bf637":"markdown","f1b663ae":"markdown","e5fbb1c0":"markdown","0650ec53":"markdown","46938383":"markdown"},"source":{"e2a9e32b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n\n# warning library\nimport warnings\nwarnings.filterwarnings('ignore')","0d8938bb":"random_state = 42\n\nn_samples = 200\nn_features = 10\nn_classes = 2\nn_estimators = 10\nnoise_class = 0.1\nnoise_moon = 0.1\nnoise_circle = 0.1\n","32c560a2":"x,y = make_classification(n_samples = n_samples,\n                          n_features = n_features,\n                          n_classes = n_classes,\n                          n_repeated = 0,\n                          n_redundant = 0,\n                          n_informative = n_features - 1,\n                          random_state = random_state,\n                          n_clusters_per_class = 1,\n                          flip_y = noise_class)\n","2003c1af":"data = pd.DataFrame(x)\ndata[\"target\"] = y\nplt.figure()\nsns.scatterplot(x = data.iloc[:,0], y = data.iloc[:,1], hue = \"target\", data = data);","56f2941e":"data_classification = (x,y)","b736be17":"# Moon dataset\nmoon = make_moons(n_samples = n_samples,\n                  noise = noise_moon,\n                  random_state = random_state)","e368494a":"data_moon = pd.DataFrame(moon[0])\ndata_moon[\"target\"] = moon[1]\nplt.figure\nsns.scatterplot(x = data_moon.iloc[:,0], y = data_moon.iloc[:,1], hue = \"target\", data = data_moon);","798c4d40":"#Circle dataset\ncircle = make_circles(n_samples = n_samples,\n                  noise = noise_circle,\n                  factor = 0.1,\n                  random_state = random_state)","2042bf8e":"data_circle = pd.DataFrame(circle[0])\ndata_circle[\"target\"] = circle[1]\nplt.figure\nsns.scatterplot(x = data_circle.iloc[:,0], y = data_circle.iloc[:,1], hue = \"target\", data = data_circle);","0bd04e5b":"datasets = [moon, circle]","28007708":"# KNN, SVM, Decision Tree \/\/ Random Forest, AdaptiveBoosting","54cd6936":"svc = SVC()\nknn = KNeighborsClassifier(n_neighbors=15)\ndt = DecisionTreeClassifier(random_state = random_state)\n\nrf = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\nada = AdaBoostClassifier(base_estimator = dt, n_estimators = n_estimators, random_state = random_state)\nv1 = VotingClassifier(estimators = [(\"svc\",svc),(\"KNN\",knn),(\"Decision Tree\",dt),(\"Random Forest\",rf),(\"AdaBoost\",ada)])\n\n\nclassifiers = [svc, knn, dt, rf, ada, v1]\nnames = [\"SVC\", \"KNN\", \"Decision Tree\",\"Random Forest\",\"AdaBoost\",\"Voting Classifier\"]","24a78bcd":"h = 0.2\ni = 1\nfigure = plt.figure(figsize=(18, 6))\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = RobustScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=random_state)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    \n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n        \n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,marker = '^', edgecolors='k')\n    \n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    print(\"Dataset # {}\".format(ds_cnt))\n\n    for name, clf in zip(names, classifiers):\n        \n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        \n        clf.fit(X_train, y_train)\n        \n        score = clf.score(X_test, y_test)\n        \n        print(\"{}: test set score: {} \".format(name, score))\n        \n        score_train = clf.score(X_train, y_train)  \n        \n        print(\"{}: train set score: {} \".format(name, score_train))\n        print()\n        \n        \n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,marker = '^',\n                   edgecolors='white', alpha=0.6)\n\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        score = score*100\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.1f' % score),\n                size=15, horizontalalignment='right')\n        i += 1\n    print(\"-------------------------------------\")\n\nplt.tight_layout()\nplt.show()\n\ndef make_classify(dc, clf, name):\n    x, y = dc\n    x = RobustScaler().fit_transform(x)\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.4, random_state=random_state)\n    \n    for name, clf in zip(names, classifiers):\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print(\"{}: test set score: {} \".format(name, score))\n        score_train = clf.score(X_train, y_train)  \n        print(\"{}: train set score: {} \".format(name, score_train))\n        print()\n\nprint(\"Dataset # 2\")   \nmake_classify(data_classification, classifiers,names)  ","158d8e32":"<b> change max_depth to 2 <\/b>","3b248b93":"This dataset for binary classification.And it will be called \" Dataset # 1 \"\n\nIn order to be able to visualize in the future, we will have to combine data sets;","2c11cd03":"### So what if too much noise is increased, lets check","1191a0c1":"<b>Dataset # 0<\/b>\n\n- Decision Tree: test set score: 0.72125 \n- Decision Tree: train set score: 1.0 \n\n        \n- Random Forest: test set score: 0.76125 \n- Random Forest: train set score: 0.9791666666666666 \n\n<b>Dataset # 1<\/b>\n\n- Decision Tree: test set score: 0.575 \n- Decision Tree: train set score: 1.0 \n\n        \n- Random Forest: test set score: 0.6425 \n- Random Forest: train set score: 0.9816666666666667 \n\n<b>Dataset # 2<\/b>\n\n- Decision Tree: test set score: 0.5175 \n- Decision Tree: train set score: 1.0 \n\n        \n- Random Forest: test set score: 0.52875 \n- Random Forest: train set score: 0.9816666666666667 \n\n","ac09d511":"This dataset for multiclass. And it will be called \" Dataset # 2 \"","5bb5c967":"------------------------------\n- n_samples = 200\n- n_features = 10\n- n_classes = 2\n- n_estimators = 10\n- noise_moon = 0.1 noise_circle = 0.1 noise_class = 0.2\n\n------------------------------\n\n## Results :","cf4b8136":"<b>We couldn't decreased the overfitting with this way<\/b>","3999f8d9":"This dataset for binary classification.And it will be called \" Dataset # 0 \"","3102ee11":"## Create Data Sets","61faee22":"# SUMMARY","98cf663e":"**Models : KNN, SVM, Decision Tree, Random Forest, Adaboosting, Voting Classifier**","b7a4de94":"------------------------------\n- n_samples = 200\n- n_features = 10\n- n_classes = 2\n- n_estimators = 10\n- noise_moon = 0.1 noise_circle = 0.1 noise_class = 0.2\n\n------------------------------\n\n## Results :\n\n<b>Data Set # 0 ( Moon )<\/b>\n- Decision Tree: test set score: 0.975 \n- Decision Tree: train set score: 1.0 \n\n\n- Random Forest: test set score: 0.95 \n- Random Forest: train set score: 0.9916666666666667 \n\n<b>Data Set # 1 ( Circle )<\/b>\n- Decision Tree: test set score: 0.9875 \n- Decision Tree: train set score: 1.0 \n\n\n- Random Forest: test set score: 1.0 \n- Random Forest: train set score: 1.0 \n\n<b>Data Set # 2 ( Classification )  #There is overfitting problem <\/b>\n- Decision Tree: test set score: 0.65 \n- Decision Tree: train set score: 1.0 \n\n\n- Random Forest: test set score: 0.75 \n- Random Forest: train set score: 0.9666666666666667 \n\n<b> (In Data Set # 2 Overfitting decreased a little when random forest was made on the decision tree )<\/b>","35026fa0":"We are combining these two data sets as we will make binary classification.","e010e8d9":"1. Random Forest is based on bagging technique while Adaboost is based on boosting technique.\n\n2. In Random Forest, certain number of full sized trees are grown on different subsets of the training dataset. Adaboost uses stumps (decision tree with only one split). So, Adaboost is basically a forest of stumps. These stumps are called weak learners. These weak learners have high bias and low variance. \n\n3. Each tree in the Random Forest is made up using all the features in the dataset while stumps use one feature at a time.\n\n4. In Random Forest, each decision tree is made independent of each other. So, the order in which trees are made is not important. While in Adaboost, order of stumps do matter. The error that first stump makes, influence how the second stump is made and so on. Each stump is made by taking the previous stump's mistakes into account. It takes the errors from the first round of predictions, and passes the errors as a new target to the second stump. The second stump will model the error from the first stump, record the new errors and pass that as a target to the third stump. And so forth. Essentially, it focuses on modelling errors from previous stumps.\n\n5. Random Forest uses parallel ensembling while Adaboost uses sequential ensembling. Random Forest runs trees in parallel, thus making it possible to parallelize jobs on a multiprocessor machine. Adaboost instead uses a sequential approach. \n\n6. Each tree in the Random Forest has equal amount of say in the final decision while in Adaboost different stumps have different amount of say in the final decision. The stump which makes less error in the prediction, has high amount of say as compared to the stump which makes more errors.\n\n7. Random Forest aims to <b>decrease variance<\/b> not bias while Adaboost aims to <b>decrease bias<\/b> not variance.\n\n8. There are <b>rare<\/b> chances of Random Forest to <b>overfit<\/b> while there are <b>good<\/b> chances of Adaboost to <b>overfit<\/b>.","71d1b0c6":"- Our models work well as there is a distinction between our classes.\n- Increasing the feature number makes the SVM model better.\n- SVM performance decreases if noise increases\n- If the number of classes increases, KNN will give good results.\n- If the number of features increases, SVM will give good results.\n- When the number of samples reaches 20000 30000, it may make sense to turn to a neural network.","cdab51bd":"###### ------------------------------\n- n_samples = 200\n- n_features = 10\n- n_classes = 2\n- n_estimators = 10\n- noise_moon = 0.6 noise_circle = 0.6 noise_class = 0.8\n\n------------------------------\n\n## Results :","2d0a6aa4":"### How can we fix overfitting problem ?","9db86363":"<b>We can say fix the overfitting problem but our test score dropped considerably in general<\/b>","82e20558":"## KNN\n- Very simple algorithm\n- Does not require training\n- As the size of the data increases, the decision-making time in KNN may take longer\n- For training : time efficient | for test : no time efficient\n- Affected by outliers","27be3120":"References: \n    \n- https:\/\/www.udemy.com\/course\/python-ile-makine-ogrenmesi-yapay-zeka-projeleri-52\n\n- http:\/\/theprofessionalspoint.blogspot.com\/2019\/03\/difference-between-random-forest-and.html#:~:text=Random%20Forest%20uses%20parallel%20ensembling,instead%20uses%20a%20sequential%20approach.","292b0920":"<b>Dataset # 0<\/b>\n\n- Decision Tree: test set score: 0.78875 \n- Decision Tree: train set score: 0.8108333333333333 \n\n\n- Random Forest: test set score: 0.785 \n- Random Forest: train set score: 0.8016666666666666 \n\n<b>Dataset # 1<\/b>\n\n- Decision Tree: test set score: 0.58875 \n- Decision Tree: train set score: 0.6241666666666666 \n\n\n- Random Forest: test set score: 0.6475 \n- Random Forest: train set score: 0.705 \n\n<b>Dataset # 2<\/b>\n\n- Decision Tree: test set score: 0.585 \n- Decision Tree: train set score: 0.6108333333333333 \n\n\n- Random Forest: test set score: <b>0.58875<\/b> ( old 52)\n- Random Forest: train set score: <b>0.6241666666666666<\/b> (old 98) ","36d592bd":"### Lets increase the number of samples 200 to 2000","e0945aa8":"## Between KNN AND SVM\n\n- If dimension ( number of feature) is increasing : choose SVM\n- If number of class is increasing : choose KNN\n- SVM for binary classification\n- KNN for multiclass problems","caa373a9":"## Decision Tree\n- The biggest problem is overfitting\nFor fix this:\n1) Regularization\n2) Random Forest\n- Very effective in simple data sets\n- But very affected by outliers","b18bf637":"## Random Forest\n- Consists of trees\n- Doing random forest with decision tree when overfitting has reduced overfitting a bit \n- Feature importance estimation is really good","f1b663ae":"## Difference Between Random Forest and Adaboosting","e5fbb1c0":"Dataset # 0\n\n- Decision Tree: test set score: 0.9875 \n- Decision Tree: train set score: 1.0 \n\n\n- Random Forest: test set score: 0.99875 \n- Random Forest: train set score: 1.0 \n\n\n\n\nDataset # 1\n\n- Decision Tree: test set score: 0.99625 \n- Decision Tree: train set score: 1.0 \n\n\n- Random Forest: test set score: 0.99125 \n- Random Forest: train set score: 1.0 \n\nDataset # 2\n\n- Decision Tree: test set score: <b>0.775<\/b>  (old 0.65)\n- Decision Tree: train set score: <b>1.0<\/b> \n\n\n- Random Forest: test set score: <b>0.84875<\/b>  (old 75)\n- Random Forest: train set score: <b>0.985<\/b>  ( old 96)\n\n\nOverfitting has dropped a little\n","0650ec53":"<a href=\"https:\/\/www.oguzerdogan.com\/\">\n    <img src=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/08\/logo_.png\" width=\"200\" align=\"center\">\n<\/a>","46938383":"## SVM\n\n- Difficult to understand, not an easy-to-learn model\n- It is not easy to interpret the results\n- If the number of feature is set correctly, it can be successful despite high noise\n"}}