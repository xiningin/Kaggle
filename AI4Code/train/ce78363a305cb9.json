{"cell_type":{"7687feb0":"code","77e5cd27":"code","066fda2f":"code","8f06997f":"code","6eeb37c2":"code","cb8c199b":"code","81c5c356":"code","410a9dd9":"code","15c57595":"code","1db41279":"code","bfdc5783":"code","dda1f2dc":"code","eff358d1":"code","3d56f723":"code","c8c4ff3b":"code","3fdc24da":"code","0d2e6d04":"markdown","fd4718f3":"markdown"},"source":{"7687feb0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77e5cd27":"pip install blitz-bayesian-pytorch","066fda2f":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport blitz\nfrom blitz.modules import BayesianLSTM\nfrom blitz.utils import variational_estimator\n \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n \nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nfrom collections import deque","8f06997f":"#importing the dataset\namazon=\"..\/input\/stock-time-series-20050101-to-20171231\/AMZN_2006-01-01_to_2018-01-01.csv\"\nibm=\"..\/input\/stock-time-series-20050101-to-20171231\/IBM_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(ibm)\n#print(df)\nwindow_size = 21\n#scaling and selecting data\nclose_prices = df[\"Close\"]\n#print(close_prices)\nscaler = StandardScaler()\n#print(scaler) \n#print(np.array(close_prices))\nclose_prices_arr = np.array(close_prices).reshape(-1, 1)\n#print(close_prices_arr)\nclose_prices = scaler.fit_transform(close_prices_arr)\n#print(close_prices )\nclose_prices_unscaled = df[\"Close\"]","6eeb37c2":"\ndef create_timestamps_ds(series,\n                         timestep_size= window_size):\n    time_stamps = []\n    labels = []\n    aux_deque = deque(maxlen=timestep_size)\n     \n    #starting the timestep deque\n    for i in range(timestep_size):\n        aux_deque.append(0)\n        #print(aux_deque)\n        \n    #feed the timestamps list\n    for i in range(len(series)-1):\n        aux_deque.append(series[i])\n        #print(aux_deque)\n        time_stamps.append(list(aux_deque))\n        #print(time_stamps)\n        \n    #feed the labels lsit\n    for i in range(len(series)-1):\n        labels.append(series[i + 1])\n     \n    assert len(time_stamps) == len(labels), \"Something went wrong\"\n     \n    #torch-tensoring it\n    features = torch.tensor(time_stamps[timestep_size:]).float()\n    labels = torch.tensor(labels[timestep_size:]).float()\n           \n    return features, labels\n    \n\nXs, ys = create_timestamps_ds(close_prices)","cb8c199b":"@variational_estimator\nclass NN(nn.Module):\n    def __init__(self):\n        super(NN, self).__init__()\n        self.lstm_1 = BayesianLSTM(1, 10, prior_sigma_1=1, prior_pi=1, posterior_rho_init=-3.0)\n        self.rnn_test = nn.RNN(10, 10, 2)\n        self.linear = nn.Linear(10, 1)\n            \n    def forward(self, x):\n        x_, x_1 = self.lstm_1(x)\n        x_, x_2 = self.rnn_test(x_)\n        \n        #gathering only the latent end-of-sequence for the linear layer\n        x_ = x_[:, -1, :]\n        x_ = self.linear(x_)\n        return x_","81c5c356":"Xs, ys = create_timestamps_ds(close_prices)\nX_train, X_test, y_train, y_test = train_test_split(Xs,\n                                                    ys,\n                                                    test_size=.25,\n                                                    random_state=42,\n                                                    shuffle=False)\n\n\n#print(X_train)\nds = torch.utils.data.TensorDataset(X_train, y_train)\n#print(ds[0])\ndataloader_train = torch.utils.data.DataLoader(ds, batch_size=8, shuffle=True)\n\nnet = NN()\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n","410a9dd9":"iteration = 0\nfor epoch in range(30):\n    for i, (datapoints, labels) in enumerate(dataloader_train):\n        optimizer.zero_grad()\n        \n        loss = net.sample_elbo(inputs=datapoints,\n                               labels=labels,\n                               criterion=criterion,\n                               sample_nbr=3,\n                               complexity_cost_weight=1\/X_train.shape[0])\n        \n        loss.backward()\n        optimizer.step()\n        \n        iteration += 1\n        if iteration%250==0:\n            preds_test = net(X_test)[:,0].unsqueeze(1)\n            loss_test = criterion(preds_test, y_test)\n            print(\"Iteration: {} Val-loss: {:.4f}\".format(str(iteration), loss_test))\n            ","15c57595":"#pred_unscaled\noriginal = close_prices_unscaled[1:][window_size:]","1db41279":"df_pred = pd.DataFrame(original)\ndf_pred[\"Date\"] = df.Date\ndf[\"Date\"] = pd.to_datetime(df_pred[\"Date\"])\ndf_pred = df_pred.reset_index()\n#df_pred = df_pred.set_index('Date')","bfdc5783":"def pred_stock_future(X_test,\n                      future_length,\n                      sample_nbr=10):\n    \n    #sorry for that, window_size is a global variable, and so are X_train and Xs\n    global window_size\n    global X_train\n    global Xs\n    global scaler\n    \n    #creating auxiliar variables for future prediction\n    preds_test = []\n    test_begin = X_test[0:1, :, :]\n    test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n\n    idx_pred = np.arange(len(X_train), len(Xs))\n    \n    #predict it and append to list\n    for i in range(len(X_test)):\n        print(i)\n        as_net_input = torch.tensor(test_deque).unsqueeze(0).unsqueeze(2)\n        pred = [net(as_net_input).cpu().item() for i in range(sample_nbr)]\n        \n        \n        test_deque.append(torch.tensor(pred).mean().cpu().item())\n        preds_test.append(pred)\n        \n        if i % future_length == 0:\n            #our inptus become the i index of our X_test\n            #That tweak just helps us with shape issues\n            test_begin = X_test[i:i+1, :, :]\n            test_deque = deque(test_begin[0,:,0].tolist(), maxlen=window_size)\n\n    #preds_test = np.array(preds_test).reshape(-1, 1)\n    #preds_test_unscaled = scaler.inverse_transform(preds_test)\n    \n    return idx_pred, preds_test","dda1f2dc":"def get_confidence_intervals(preds_test, ci_multiplier):\n    global scaler\n    \n    preds_test = torch.tensor(preds_test)\n    \n    pred_mean = preds_test.mean(1)\n    pred_std = preds_test.std(1).detach().cpu().numpy()\n\n    pred_std = torch.tensor((pred_std))\n    #print(pred_std)\n    \n    upper_bound = pred_mean + (pred_std * ci_multiplier)\n    lower_bound = pred_mean - (pred_std * ci_multiplier)\n    #gather unscaled confidence intervals\n\n    pred_mean_final = pred_mean.unsqueeze(1).detach().cpu().numpy()\n    pred_mean_unscaled = scaler.inverse_transform(pred_mean_final)\n\n    upper_bound_unscaled = upper_bound.unsqueeze(1).detach().cpu().numpy()\n    upper_bound_unscaled = scaler.inverse_transform(upper_bound_unscaled)\n    \n    lower_bound_unscaled = lower_bound.unsqueeze(1).detach().cpu().numpy()\n    lower_bound_unscaled = scaler.inverse_transform(lower_bound_unscaled)\n    \n    return pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled","eff358d1":"future_length=7\nsample_nbr=4\nci_multiplier=5\nidx_pred, preds_test = pred_stock_future(X_test, future_length, sample_nbr)\npred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled = get_confidence_intervals(preds_test,\n                                                                                          ci_multiplier)","3d56f723":"y = np.array(df.Close[-245:]).reshape(-1, 1)\nunder_upper = upper_bound_unscaled > y\nover_lower = lower_bound_unscaled < y\ntotal = (under_upper == over_lower)\n\nprint(\"{} our predictions are in our confidence interval\".format(np.mean(total)))","c8c4ff3b":"params = {\"ytick.color\" : \"w\",\n          \"xtick.color\" : \"w\",\n          \"axes.labelcolor\" : \"w\",\n          \"axes.edgecolor\" : \"w\"}\n\nplt.rcParams.update(params)\n\nplt.title(\"IBM Stock prices\", color=\"white\")\n\nplt.plot(df_pred.index,\n         df_pred.Close,\n         color='black',\n         label=\"Real\")\n\nplt.plot(idx_pred,\n         pred_mean_unscaled,\n         label=\"Prediction for {} days, than consult\".format(future_length),\n         color=\"red\")\n\nplt.fill_between(x=idx_pred,\n                 y1=upper_bound_unscaled[:,0],\n                 y2=lower_bound_unscaled[:,0],\n                 facecolor='green',\n                 label=\"Confidence interval\",\n                 alpha=0.5)\n\nplt.legend()","3fdc24da":"params = {\"ytick.color\" : \"w\",\n          \"xtick.color\" : \"w\",\n          \"axes.labelcolor\" : \"w\",\n          \"axes.edgecolor\" : \"w\"}\nplt.rcParams.update(params)\n\nplt.title(\"IBM Stock prices\", color=\"white\")\n\n\nplt.fill_between(x=idx_pred,\n                 y1=upper_bound_unscaled[:,0],\n                 y2=lower_bound_unscaled[:,0],\n                 facecolor='green',\n                 label=\"Confidence interval\",\n                 alpha=0.75)\n\nplt.plot(idx_pred,\n         df_pred.Close[-len(pred_mean_unscaled):],\n         label=\"Real\",\n         alpha=1,\n         color='black',\n         linewidth=0.5)\n\nplt.plot(idx_pred,\n         pred_mean_unscaled,\n         label=\"Prediction for {} days, than consult\".format(future_length),\n         color=\"red\",\n         alpha=0.5)\n\nplt.legend()","0d2e6d04":"# Citing\n@misc{esposito2020blitzbdl,\n    author = {Piero Esposito},\n    title = {BLiTZ - Bayesian Layers in Torch Zoo (a Bayesian Deep Learing library for Torch)},\n    year = {2020},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https:\/\/github.com\/piEsposito\/blitz-bayesian-deep-learning\/}},\n}\n\n# Blitz - Bayesian Layers in Torch Zoo\nBLiTZ is a simple and extensible library to create Bayesian Neural Network Layers (based on whats proposed in [Weight Uncertainty in Neural Networks paper](https:\/\/arxiv.org\/abs\/1505.05424)) on PyTorch. By using BLiTZ layers and utils, you can add uncertanity and gather the complexity cost of your model in a simple way that does not affect the interaction between your layers, as if you were using standard PyTorch.\n","fd4718f3":"tensor([[-0.7263, -0.5769, -0.1140,  0.2942, -0.7400,  0.1316,  0.5767,  0.7429,\n          0.5209,  0.2107]], grad_fn=<SelectBackward>)"}}