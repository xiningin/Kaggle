{"cell_type":{"bec5e1b0":"code","4f2c3c66":"code","2eb09385":"code","0602dcca":"code","6bc4d1de":"code","fbeedec2":"code","35782485":"code","e329cefc":"code","c7434019":"code","72afa036":"code","6f9d826a":"code","459b8cf0":"code","bf8b51bb":"code","ed1b586d":"code","25f3730a":"code","df2bad70":"code","45042a30":"code","a95f97d1":"code","3d1798b1":"code","ef6da44a":"code","2d0a7bab":"code","253c7338":"code","d06527db":"code","cb020f06":"code","e1c73c4c":"code","cbc2ba20":"code","c3278cf8":"code","a52aa658":"markdown","49baf06d":"markdown","f851d289":"markdown","bf939a5b":"markdown","aa86d71f":"markdown","ce3d1e57":"markdown","e9b96f67":"markdown","1619d635":"markdown","5c699175":"markdown","39d4cab4":"markdown","2a28dcb0":"markdown","0a28206c":"markdown","1543844a":"markdown","85c56e3a":"markdown","3b6ca029":"markdown","8e48f7b5":"markdown","c37e2f5c":"markdown","c2b20b8b":"markdown","c9e7d312":"markdown","6ade0e8f":"markdown","5b276545":"markdown","0b3c1f2c":"markdown","46785bca":"markdown","77cdba52":"markdown","80ec239a":"markdown","a0796c39":"markdown","b046b60a":"markdown","4d8af17c":"markdown","1d6f60f7":"markdown","48737ca2":"markdown","972c39dc":"markdown"},"source":{"bec5e1b0":"# program for analyzing some of my facebook conversations\nimport numpy as np\nimport pandas as pd\nimport sys\nimport os\nimport json\nimport re\nimport glob\nfrom datetime import datetime\nfrom collections import Counter\n\n# nlp libraries\nimport spacy\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\n# wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\n\npath = 'D:\\\\Python\\\\fb convo analysis\\\\'","4f2c3c66":"# function that gets all json files from directory depending on the query\ndef get_message(query):\n    message_list = []\n    message_path = os.path.join(path, 'facebook-shawnliu90\\\\messages\\\\inbox')\n    # searching through message_path for query\n    message_folder = os.listdir(message_path)\n    # combining message path with the file name to get that specific person's messages\n    message_file = os.path.join(message_path, list(filter(lambda x: query in x, message_folder))[0])\n    # changing file path to message_file and finding all json files containing messages\n    os.chdir(message_file)\n    read_files = glob.glob('*.json')\n    # appends each json to a list of jsons\n    for file in read_files:\n        with open(file, \"r\", encoding = 'utf-8') as infile:\n            message_list.append(json.load(infile))\n    return message_list","2eb09385":"# function that formats the message into one big string for word cloud\ndef format_message(text):\n    message_string = ''\n    for content in text['messages']:\n        if 'content' in content.keys():\n            # Appends all text into one big string\n            message_string = message_string + ' ' + content['content']\n    # Removing leading and trailing spaces\n    message_string = message_string.rstrip().lstrip()\n    message_string.replace('\u00e2\\x80\\x99', '\\'')\n    return message_string","0602dcca":"# removing junk words by adding to stopwords\njunk_words = {'lol', 'yeah', 'okay', 'thank', 'thanks', 'you', 'man'\n              'go', 'think', 'oh', 'ok','that', 'thats', 'yo', 'ya', 'bro', 'im', 'ye'}\n\nfor word in junk_words:\n    STOPWORDS.add(word)\n\n# wordcloud function\ndef create_wordcloud(text):\n    # mask image for the shape of the cloud\n    mask = np.array(Image.open(os.path.join(path, 'mask.png')))\n    # filtering out stopwords like the, and, is, etc.\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(background_color = 'white', mask = mask, max_words = 200, stopwords = stopwords)\n    # generating the wordcloud\n    wc.generate(text)\n    return wc","6bc4d1de":"# function that converts the date in text for purposes of plotting \ndef date_conversion(text):\n    # converting timestamps into dates, first by dividing out the milliseconds in the timestamps\n    text['timestamp_ms'] = text['timestamp_ms']\/\/1000\n    # iterating and appending onto a list, then replacing the timestamp column with the date list\n    date_list = []\n    for timestamp in text['timestamp_ms']:\n        timestamp = datetime.fromtimestamp(timestamp)\n        date_list.append(timestamp)\n    text['timestamp_ms'] = date_list\n    return text","fbeedec2":"# internet slang that might get lost in translation if not mapped correctly\ninternet_slang = {'lol': 'laugh out loud','lmao': 'laughing my ass off', 'idk': 'i don\\'t know', 'idc': 'i don\\'t care', \n                  'wtf': 'what the fuck', 'omg' : 'oh my god', 'kk': 'okay', 'okie': 'okay', 'u': 'you', \n                  'ur': 'your', 'jk': 'just kidding'}\n\n# function that cleans text by replacing odd strings, internet slang, and passes it into a spacy object if need be\ndef cleaner(df, entity = False):\n    output_text = []\n    for sentence in df['content']:\n        sentence = sentence.replace('\u00e2\\x80\\x99', '\\'')\n        # mapping internet slang\n        sentence = ' '.join([internet_slang.get(i, i) for i in sentence.split()])\n        output_text.append(sentence)\n    if entity == True:\n        # loading pre-trained spacy model\n        nlp = spacy.load(r'C:\\\\Users\\\\sliu742\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.2.5')\n        # making everything lowercase to avoid repeat words\n        output_text = list(map(lambda x: x.lower(), output_text))\n        # joining everything into one big string\n        output_text = ' '.join(output_text)\n        # passing output into nlp to get useful spacy features\n        output_text = nlp(output_text)\n    return output_text","35782485":"# function that performs sentiment analysis on cleaned text using vader\ndef sentiment_analyzer(clean_text):\n    analyzer = SentimentIntensityAnalyzer()\n    score_list = []\n    for sentence in clean_text:\n        score = analyzer.polarity_scores(sentence)\n        score_list.append(score)\n    return score_list","e329cefc":"# executing function that gets the list of message files \nmain_text = get_message('AlysaaCoco')","c7434019":"# formatting and creating wordcloud\nmain_text_wordcloud = ''\n\n# iterate over each json and extraxt all content into one string\nfor text in main_text:\n    main_text_wordcloud = main_text_wordcloud + format_message(text).lower()\ncloud = create_wordcloud(main_text_wordcloud)","72afa036":"# showing the wordcloud in a plot\nplt.figure(figsize=(20, 10))\nplt.imshow(cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.axis(\"off\")\nplt.show()","6f9d826a":"# what does my wordcloud with a friend look like?\ncomparison_text = get_message('AngusLai')\ncomparison_text_wordcloud = \"\"\n\n# iterate over each json and extraxt all content into one string\nfor text in comparison_text:\n    comparison_text_wordcloud = comparison_text_wordcloud + format_message(text).lower()\ncloud = create_wordcloud(comparison_text_wordcloud)\n\n# showing the wordcloud in a plot\nplt.figure(figsize=(20,10))\nplt.imshow(cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.axis(\"off\")\nplt.show()","459b8cf0":"# iterate over each json, converting it to a dataframe\nmain_text_list = []\nfor text in main_text:\n    text = pd.DataFrame.from_dict(text['messages'])\n    main_text_list.append(text)","bf8b51bb":"# merge into on dataframe\nmain_text_df = pd.concat(main_text_list, sort = False)\n\n# grabbing the timestamp, sender, content, and type\nmain_text_df = main_text_df[['sender_name', 'timestamp_ms', 'content', 'type']]\n\n# converting timestamp to datetime\nmain_text_df_ts = date_conversion(main_text_df)\nmain_text_df_ts.head()","ed1b586d":"# how many total messages did we send to each other?\ntotal_msgs = len(main_text_df_ts)\nprint(total_msgs)","25f3730a":"# who sent more messages?\nsns.set()\nmain_text_df_ts.groupby('sender_name')['content'].count().plot(kind = 'bar')","df2bad70":"# grouping by day and week\nmessages_day = main_text_df_ts.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'D')).count()\nmessages_week = main_text_df_ts.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W')).count()\n\n# plotting daily and weekly messages\nplt.figure(figsize=(20,10))\nplt.plot(messages_day['content'], color = 'b', alpha = 1)\nplt.plot(messages_week['content'], color = 'g', alpha = 0.8, linestyle = '--')\nplt.xlabel('date')\nplt.ylabel('number of messages')\nlegend_daily = mpatches.Patch(color = 'blue', label = 'daily')\nlegend_weekly = mpatches.Patch(color = 'green', label = 'weekly')\nplt.legend(handles=[legend_daily, legend_weekly])\n\nplt.show()","45042a30":"# splitting up weekly messages by sender name\nmessages_day_shawn = main_text_df_ts[main_text_df_ts['sender_name'] == 'Shawn Liu']\\\n.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W')).count()\nmessages_day_other = main_text_df_ts[main_text_df_ts['sender_name'] != 'Shawn Liu']\\\n.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W')).count()\n\n# plotting weekly messages by sender\nplt.figure(figsize=(20,10))\nplt.plot(messages_day_shawn['content'], color = 'c', alpha = 0.8)\nplt.plot(messages_day_other['content'], color = 'm', alpha = 0.8)\nplt.xlabel('date')\nplt.ylabel('number of messages')\nlegend_shawn = mpatches.Patch(color = 'cyan', label = 'shawn')\nlegend_other = mpatches.Patch(color = 'magenta', label = 'other')\nplt.legend(handles=[legend_shawn, legend_other])\n\nplt.show()","a95f97d1":"# dictionary and list of weekdays to map onto df and sort\nweekdays_dict = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\nweekdays_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# creating a weekday column with dt.dayoftheweek, then mapping to names of weekdays for clarity\nmain_text_df_ts['weekday'] = main_text_df_ts['timestamp_ms'].dt.dayofweek\nmain_text_df_ts['weekday'] = main_text_df_ts['weekday'].map(weekdays_dict)\n\n# sorting by weekdays to maintain the order to days in the plot\nmain_text_df_ts['weekday'] = pd.Categorical(main_text_df_ts['weekday'], categories = weekdays_list, ordered = True)\nmain_text_df_ts = main_text_df_ts.sort_values('weekday')\n\n# weekday plots\nmain_text_df_ts.groupby('weekday')['content'].count().plot(kind = 'bar',figsize = (10,5), color = 'r')","3d1798b1":"# creating a hour column with dt.hour, then sorting\nmain_text_df_ts['hour'] = main_text_df_ts['timestamp_ms'].dt.hour\nmain_text_df_ts = main_text_df_ts.sort_values('hour')\n\n# hourly plot\nmain_text_df_ts.groupby('hour')['content'].count().plot(kind = 'bar', figsize = (10,5), color = 'g')","ef6da44a":"# re-sorting by timestamp and reindexing \nmain_text_df_ts.sort_values('timestamp_ms', ascending = False, inplace = True)\nmain_text_df_ts = main_text_df_ts.reset_index(drop=True)\n\n# dropping nan values, which occur because of there being a pic\/video instead of text\nmain_text_df_ts.dropna(inplace = True)","2d0a7bab":"# execute normalization function to get ready for sentiment analysis\ncleaned_content = cleaner(main_text_df_ts)\n\n# extracting compound scores\nsentiment_compound = []\nsentiment = sentiment_analyzer(cleaned_content)\nfor score in sentiment:\n    sentiment_compound.append(score['compound'])","253c7338":"# adding cleaned text and sentiment scores to dataframe\nmain_text_df_ts['content'] = cleaned_content\nmain_text_df_ts['sentiment_score'] = sentiment_compound\n\nmain_text_df_ts.head()","d06527db":"# plotting daily and weekly sentiment scores\nsentiment_day = main_text_df_ts.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'D'))['sentiment_score'].mean()\nsentiment_week = main_text_df_ts.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W'))['sentiment_score'].mean()\n\n# plotting daily and weekly messages\nplt.figure(figsize=(20,10))\nplt.plot(sentiment_day, color = 'r', alpha = 0.6)\nplt.plot(sentiment_week, color = 'k', alpha = 1, linestyle = '--')\nplt.axhline(y = 0.0, xmin = -1, xmax = 1,color = 'gray',linestyle = '--')\nplt.xlabel('date')\nplt.ylabel('average sentiment score')\nlegend_daily = mpatches.Patch(color = 'red', label = 'daily sentiment')\nlegend_weekly = mpatches.Patch(color = 'black', label = 'weekly sentiment')\nplt.legend(handles=[legend_daily, legend_weekly])\n\nplt.show()","cb020f06":"# splitting up each person's sentiment scores\nshawn_sentiment = main_text_df_ts[main_text_df_ts['sender_name'] == 'Shawn Liu']\nshawn_sentiment = shawn_sentiment.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W'))['sentiment_score'].mean()\n\nother_sentiment = main_text_df_ts[main_text_df_ts['sender_name'] != 'Shawn Liu']\nother_sentiment = other_sentiment.groupby(pd.Grouper(key = 'timestamp_ms', freq = 'W'))['sentiment_score'].mean()\n\n# plotting weekly messages by sender\nplt.figure(figsize=(20,10))\nplt.plot(shawn_sentiment, color = 'c', alpha = 0.8)\nplt.plot(other_sentiment, color = 'm', alpha = 0.8)\nplt.axhline(y = 0.0, xmin = -1, xmax = 1,color = 'gray',linestyle = '--')\nplt.xlabel('date')\nplt.ylabel('number of messages')\nlegend_shawn = mpatches.Patch(color = 'cyan', label = 'shawn')\nlegend_other = mpatches.Patch(color = 'magenta', label = 'other')\nplt.legend(handles=[legend_shawn, legend_other])\n\nplt.show()","e1c73c4c":"# parsing all text with spacy\nmain_text_spacy = cleaner(main_text_df_ts, entity = True)","cbc2ba20":"# displaying all labels and their frequencies\nlabels = [x.label_ for x in main_text_spacy.ents]\nCounter(labels)","c3278cf8":"# displaying most common items\nitems = [x.text for x in main_text_spacy.ents]\nCounter(items).most_common(10)","a52aa658":"Odd that Tuesday is the day of the week with the most messages sent. It was probably because that's when we started to miss each other the most (we always hung out on the weekends). It also makes sense that Friday and Saturday had the least messages as we were most likely spending time in-person.","49baf06d":"### Specific times of the day?","f851d289":"## Time Series Plots\n\nI was interesting in seeing our different patterns of communication in terms of number of messages over time. Using matplotlib I plotted some interesting graphs.","bf939a5b":"This is so funny! My sentiment is lower than hers until we started dating in early August. Since then, mine increased to always be slightly higher. Maybe she chased harder in the beginning and now we're on equal footing?","aa86d71f":"Seems fairly typical. The days when we're apart from each other tends to result in late-night conversations. ","ce3d1e57":"### How many messages have we sent each other?","e9b96f67":"### Messaging pattern over time\n\nIt'd be interesting to see how frequently we messaged over the course of us knowing each other, from when we first met to when we started dating to the present day.","1619d635":"# Facebook Conversation Analysis\n\nHere is some analysis I performed on my message history with my girlfriend of around 5 months. Seeing as we often have questions regarding our history of talking to each other, I thought it'd be interesting to analyze our Facebook Messenger message history and try to answer some of these questions, as well as uncovering some interesting information.\n\nI started by importing the necessary packages to not only find the data in my file directory, but also plotting libraries, NLP libraries, and even a wordcloud library as part of my analysis.","5c699175":"## Sentiment Analysis\n\nUsing the pre-trained VADER model from NLTK I did a sentiment analysis on each message we sent each other. The higher the score, the more positive the messages. \n\nPositivity could be associated with words such as \"love\", \"nice\", \"good\", \"great\", while negativity is tied to words such as \"hurt\", \"ugly\", \"sad\", \"bad\", \"worse\". From my understanding, the VADER model can also detect the instensity of the emotion with the use of capitalization (\"GOOD\" is more positive than \"good\"), punctuation (\"GOOD!!!\" is way more positive than \"good...\"), degree modifiers (\"EXTREMELY GOOD!!!\" more positive than \"a little good\"). VADER can also detect the use of conjunctions to change the polarity of a message (\"food here was good *but* the service is horrible\") and can decode the meaning of certian emojis. \n\nSeeing as our chat is full of emojis, VADER should do a good job of detecting some nuances in social media conversations that couldn't have been capture otherwise. So all in all this should be fun!","39d4cab4":"Wow! A whopping 20,000 messages! Not sure whether that's a lot or not, but I text my parents a max of 5 times per week, so that says a lot about how much I like to keep in regular contact with people in my life.","2a28dcb0":"Apparently I need to step up my messaging game, my girlfriend is almost 3000 messages ahead of me! I guess I'm a man of few words?","0a28206c":"Interesting! Individual words don't really mean anything but can serve as a proxy to prevalent topics we talk about. We seem to talk about work a lot, which makes sense considering we started talking in the summer when we were both working. \"Time\" also makes a lot of sense as we frequently ask each other what time we're getting off work or end class. \"Good\" and \"really\" can really stem from anything, maybe looking at bigrams later on will help?\n\nLet's look at the wordcloud for one of my buddies and see how that fares.","1543844a":"We seem to talk about other people a lot, shame on us! Because we are constantly planning when next to see each other, it makes sense that \"DATE\" and \"TIME\" are commonly used as labels. Wish we talked more about works of art though!","85c56e3a":"Cool to see the average weekly sentiment increase as we start talking and dating! Unfortunately no sentiment data in the middle as little to no messages were sent during that period of our Facebook friendship.","3b6ca029":"## Functions\n\nBelow are some functions I built to make my code look a little cleaner and more understandable. \n\n### get_message\n\nThe purpose of get_message is to successfully retrieve any json message files from my file directory (whose contents aren't in the repo for obvious reasons) based on a query, which could be the name of any person I talked to on Facebook. Since the json files containing the messages have a max of 10,000 messages, the file folder containing the messages will typically contain multiple json files. I took this into account as I built this function and added the functionality of being able to detect all json files within the directory and append them all onto a list, which is the output of this function.","8e48f7b5":"##  Wordcloud\n\nHere is the portion of the notebook where I use my messages to create some wordclouds. I wanted to know how frequent words that we use and if they're indicative of our relationship.","c37e2f5c":"### cleaner\n\nData cleaning function (tokenization, stemming, substitutions, and conversions) that prepares the actual text for sentiment analysis. Also tokenizes, lemmatizes, position tag, and identifies named entities for each message depending on the purpose.","c2b20b8b":"Seems to have many more acronyms and short forms, which makes sense as we're a lot more casual with each other than I am with my girlfriend. I distinctly remember \"Dude\", \"idk\", and \"lmao\" as a major part of our daily lexicon. There's also more big words in this wordcloud, mostly has to do with me and my friend being very short and concise with each other. I guess this means more varied, flowery language is used when me and my girlfriend talk!","c9e7d312":"### format_message\n\nThis function formats all text within each json in the output of get_message and appends them onto a string, effectively cleaning the data to enter into the wordcloud.","6ade0e8f":"It seems as though I am, as previously mentioned, more concise and to-the-point, even after we started dating! Though the gap between us is now much smaller, must mean I really like talking to her.","5b276545":"The fact that \"china\" makes it to the top 10 more common items is hilarious, as we constantly talk about the situation in Hong Kong and the wrongdoings of the Chinese government. \"Today\" is no surprise either, as previously mentioned we love to plan ahead on what to do together every single day. \"Morning\" stems from the us wishing good morning very often, and \":)\" is self-explanatory!","0b3c1f2c":"### What about on specific weekdays?","46785bca":"## Named Entities\n\nAre there any particular topics we like talking about the most?\n\nA named entity is a \u201creal-world object\u201d that\u2019s assigned a name for example, a person, a country, or a place. The spaCy library can recognize various types of named entities in a document, by asking the model for a prediction. Similarly, spaCy can also perform tokenization, finding common words used throughout the string. Because models are statistical and strongly depend on the examples they were trained on, this doesn\u2019t always work perfectly and might need some tuning later. In short, spaCy will allow me to look for common labels and tokens we talk about.","77cdba52":"### What's the overall sentiment these past couple of years?","80ec239a":"### date_conversion\n\nThis is the function that converts timestamps into datetime objects. Seeing as I'll be needing correct dates in any analysis I perform, I thought it would be the easiest if I made the process into a function to increase readibility and save some lines of code.","a0796c39":"### Who sends more messages?","b046b60a":"Wow! It's cool to see the entire message history mapped out over the last 2 years. At the very beginning we barely talked because we just met and weren't that close, followed by that summer when we both worked at the same company. That is followed by a 1.5 year gap with almost no messages. This gap is due to the fact that she had started her own business and therefore had little time to message her friends.\n\nIn early-mid April we started chatting again after she stopped working on her business. This was followed by an increasing amount of messages as we started becoming more \"interested\" in each other. The cause of the huge spike in early-august is a lengthy conversation after dinner. Weekly messages went down as we started seeing each in-person at school in the fall and back up again when she left to go on vacation in San Francisco. The exam period then started, resulting in less messages overall and was quickly ended when she left for Europe for a month around early December.","4d8af17c":"### Who has a higher sentiment score on average?","1d6f60f7":"### sentiment_analyzer\n\nFunction that performs sentiment analysis using the VADER sentiment intensity analyzer. This will be useful in assigning a score to each message to measure overall positive and negative sentiment.","48737ca2":"### Again, who consistently sends more messages?","972c39dc":"### create_wordcloud\n\nHere is the function that creates the wordcloud of all the unique words used in our conversation. I added some junk words into the stopwords to exclude from the wordcloud as I'm guilty of overusing these words in all my conversations. "}}