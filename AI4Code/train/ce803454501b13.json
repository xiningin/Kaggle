{"cell_type":{"04059498":"code","1bdca840":"code","382fa212":"code","01ed8595":"code","f0b78219":"code","912d8371":"code","783b7ca4":"code","246b5e68":"code","dd2fbd54":"code","19f3a031":"code","655190cf":"code","301f3824":"code","3b854ca9":"code","6e2a2122":"code","64c9e8ff":"code","3b754aad":"code","094bd024":"code","6f21bdc9":"code","d0506274":"code","20590c79":"code","a66499fb":"code","52c2d87d":"code","6dacc894":"code","5e09f1e4":"code","7b19f69b":"code","ad2bde73":"code","0ba1f2ee":"code","89a67429":"code","f0ad3732":"code","0b32dd26":"code","844a50a6":"code","c379a375":"code","5f080ed5":"code","bb243629":"code","8842c7fa":"code","bbefa9bc":"code","db517558":"code","de505014":"code","7a422ba6":"code","97de1fe0":"markdown","bf5ed2b4":"markdown","d7ee51a2":"markdown","924ace8a":"markdown","d2177c64":"markdown","45023b4d":"markdown","a0e60014":"markdown","b3677382":"markdown","0a06f512":"markdown","ccccf9b3":"markdown","beea3f07":"markdown","0495fd94":"markdown","e7611c70":"markdown","cfa86da7":"markdown","b0e948b1":"markdown","154365a1":"markdown","7bc6db7f":"markdown","9af06501":"markdown","07fd6335":"markdown","fa750bbf":"markdown","75f2261d":"markdown","fed65bf4":"markdown","4e140d59":"markdown","576f4d0e":"markdown","04683702":"markdown","08c2bdee":"markdown","762aa580":"markdown","1c842ee1":"markdown","427b02a8":"markdown","ac407918":"markdown","529235b1":"markdown","e853063c":"markdown","c0c88fe4":"markdown","2cc6864c":"markdown","8831adb6":"markdown","6f8eedee":"markdown","2975c495":"markdown","d7e6889a":"markdown","f127e56c":"markdown","c90aafbd":"markdown"},"source":{"04059498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1bdca840":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import make_column_selector\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.feature_selection import mutual_info_regression\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport optuna\nfrom optuna.visualization import plot_contour, plot_optimization_history, plot_parallel_coordinate","382fa212":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\",index_col = 'id')\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\",index_col = 'id')\nsub = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\",index_col = 'id')\ntrain.head()","01ed8595":"X = train.copy()\ny = X.pop('target')","f0b78219":"X_cont = X.copy()\nX_cont = X_cont[[col for col in X_cont.columns if 'cont' in col]]\n\n# the wrong way to normalize your data \nX_wrong = X_cont.copy()\nX_wrong_norm = (X_wrong - X_wrong.mean()) \/ X_wrong.std()\nX_wrong_train, X_wrong_val, y_wrong_train, y_wrong_val = train_test_split(X_wrong_norm, y, test_size = 0.2, random_state = 1)\n\n# the right way to normalize your data\nX_right = X_cont.copy()\nX_right_train, X_right_val, y_right_train, y_right_val = train_test_split(X_right, y, test_size = 0.2, random_state = 1)\nX_right_train_mean = X_right_train.mean()\nX_right_train_std = X_right_train.std()\nX_right_train_norm = (X_right_train - X_right_train_mean) \/ X_right_train_std\nX_right_val_norm = (X_right_val - X_right_train_mean) \/ X_right_train_std\n\n\n# print the first few rows of both validation sets, they're different!\nprint(\"Wrong way:\")\nprint(X_wrong_val[['cont0']].head(3))\nprint(\"Right way:\")\nprint(X_right_val_norm[['cont0']].head(3))","912d8371":"# pipeline step for normalizing data\nclass CustomNormalizer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\">>> CustomNormalizer.__init__() has been called.\")\n        self.norm_mean = []\n        self.norm_std = []\n    \n    def fit(self, X_train, y = None): \n        print(\">>> CustomNormalizer.fit() has been called.\")\n        self.norm_mean = X_train.mean()\n        self.norm_std = X_train.std()\n        return self\n    \n    def transform(self, X):\n        print(\">>> CustomNormalizer.transform() has been called.\")\n        return (X - self.norm_mean) \/ self.norm_std","783b7ca4":"my_pipeline = Pipeline(steps = [\n    ('Normalizer', CustomNormalizer())\n])","246b5e68":"X_cont_train, X_cont_val, y_cont_train, y_cont_val = train_test_split(X_cont, y, test_size = 0.2, random_state = 1)\nmy_pipeline.fit(X_cont_train,y_cont_train)","dd2fbd54":"step = 'Normalizer' # index by the name of the step in our pipeline\nmy_pipeline[step].norm_mean # get the desired property of that step","19f3a031":"# apply transform to training data, requires we manually did a fit first\nmy_pipeline.transform(X_cont_train)","655190cf":"# or we can apply fit and transform to the training data simultaneously\nmy_pipeline.fit_transform(X_cont_train, y)","301f3824":"X_norm_val = my_pipeline.transform(X_cont_val)","3b854ca9":"print(\"The hand coded way:\")\nprint(X_right_val_norm[['cont0']].head(3))\nprint(\"The pipeline way:\")\nprint(X_norm_val[['cont0']].head(3))","6e2a2122":"custom_pipeline = Pipeline(steps = [\n    ('Normalizer', CustomNormalizer())\n    ,('Model', LinearRegression())\n])\ncustom_pipeline.fit(X_cont_train, y_cont_train)\ncustom_preds = custom_pipeline.predict(X_cont_val)","64c9e8ff":"builtin_pipeline = Pipeline(steps=[\n    ('Normalization',StandardScaler())\n    ,('Model',LinearRegression())\n])\nbuiltin_pipeline.fit(X_cont_train, y_cont_train)\nbuiltin_preds = builtin_pipeline.predict(X_cont_val)","3b754aad":"print(custom_preds)\nprint(builtin_preds)","094bd024":"Xcn = CustomNormalizer().fit_transform(X_cont_train,y_cont_train)\nXss = StandardScaler().fit_transform(X_cont_train,y_cont_train)\nprint('\\nCustomNormalizer outputs a pandas dataframe:\\n')\nprint(Xcn.head())\nprint('\\nStandardScaler outputs a numpy array:\\n')\nprint(Xss[0:5,:])","6f21bdc9":"# scaler that preserves dataframe index and column names\nclass ScalerTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.scaler = StandardScaler()\n        \n    def fit(self, X, y=None):\n        self.scaler.fit(X.values)\n        return self\n    \n    def transform(self, X, y=None):\n        return pd.DataFrame(self.scaler.transform(X.values),columns=X.columns, index=X.index)\nXst = ScalerTransformer().fit_transform(X_cont_train,y_cont_train)\nprint(Xst.head())","d0506274":"normalize_columns = DataFrameMapper([\n    (['cont0'],StandardScaler())\n    ,(['cont0'],CustomNormalizer())], df_out = True)\nnormalize_columns.fit_transform(X_cont_train)","20590c79":"normalize_columns = DataFrameMapper([\n    (['cont0'],StandardScaler(),{'alias':'cont0_standard'})\n    ,(['cont0'],CustomNormalizer(),{'alias':'cont0_custom'})], df_out = True)\nnormalize_columns.fit_transform(X_cont_train)","a66499fb":"norm_mapper = DataFrameMapper([\n    (make_column_selector(dtype_include=float),StandardScaler(),{'alias':'norm'})\n     ], df_out = True)\nnorm_mapper.fit_transform(X_cont_train)","52c2d87d":"norm_mapper = DataFrameMapper([\n    (make_column_selector(dtype_include=float),StandardScaler(),{'alias':'norm'})\n     ], df_out = True)\ndf_pipeline = Pipeline(steps=[\n    ('normalize',norm_mapper)\n    ,('model',LinearRegression())])\ndf_pipeline.fit(X_cont_train, y_cont_train)\ndf_preds = builtin_pipeline.predict(X_cont_val)\n# check the answers are still the same\nprint(custom_preds)\nprint(builtin_preds)\nprint(df_preds)","6dacc894":"CategoricalSelector = DataFrameMapper([\n    (make_column_selector(dtype_include=object),None,{'alias':'cat'})\n     ], df_out = True)\nContinuousSelector = DataFrameMapper([\n    (make_column_selector(dtype_include=float),None,{'alias':'cont'})\n     ], df_out = True)","5e09f1e4":"# normalize continuous data\nNormalizer = DataFrameMapper([\n    (make_column_selector(dtype_include=float),StandardScaler(),{'alias':'norm'})\n     ], df_out = True)\n\n# ordinal encoding for categoricals\nOrdinals = DataFrameMapper([\n    (make_column_selector(dtype_include=object),OrdinalEncoder(dtype = 'int'),{'alias':'ordinal'})\n     ], df_out = True)\n\n# one hot encoding for categoricals\nOneHot = DataFrameMapper([\n    (make_column_selector(dtype_include=object),OneHotEncoder(dtype = 'int',handle_unknown = 'ignore'),{'alias':'onehot'})\n     ], df_out = True)\n\n# make feature union return dataframe, assumes all pipelines being union'd output dataframes\nclass FeatureUnion_df(BaseEstimator, TransformerMixin):\n    def __init__(self,transformer_list):\n        # store the list of pipelines to horzcat\n        self.transformer_list = transformer_list\n        self.transformer_results = []\n    \n    def fit(self, X, y=None):\n        # loop through each an fit it\n        for ii in self.transformer_list:\n            ii[1].fit(X,y)\n        return self\n    \n    def transform(self, X):\n        # for each transformer, transform and save to list, then concat\n        self.transformer_results = [] # clear with each new transform\n        for ii in self.transformer_list:\n            self.transformer_results.append(ii[1].transform(X))\n        return pd.concat(self.transformer_results,axis=1)","7b19f69b":"p1 = Pipeline(steps=[('cont',ContinuousSelector),('norm',Normalizer)])\np2 = Pipeline(steps=[('cat',CategoricalSelector),('ord',Ordinals)])\np3 = Pipeline(steps=[('cat',CategoricalSelector),('oh',OneHot)])\nfp = FeatureUnion_df(transformer_list = [('numerical',p1),('ordinal',p2),('onehot',p3)])\nXf = fp.fit_transform(X,y)\nXf.head()","ad2bde73":"# return the PCA projection, assume already scaled\nPcaTransformer = DataFrameMapper([\n    (make_column_selector(dtype_include=float),PCA(),{'alias':'PCA'})\n     ], df_out = True)\n\n# add columns with kmeans, assuming input already scaled\nclass KmeansTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, num_clusters = 4):\n        self.num_clusters = num_clusters\n        self.kmeans = KMeans(n_clusters=self.num_clusters, n_init=3, random_state=0)\n    \n    def fit(self, X, y=None):\n        self.kmeans.fit(X)\n        return self\n    \n    def transform(self, X, y=None):\n        Xkf = self.kmeans.transform(X)\n        df = pd.DataFrame()\n        for i in range(self.num_clusters):\n            df[f\"Centroid_{i}\"] = Xkf[:,i]\n        df.index = X.index # preserve index, important for FeatureUnion\n        return df\n\n# feature selection - keep top N features according to mutual_info_regression\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, num_features = 10):\n        self.num_features = num_features\n        self.selected_features = []\n        \n    def fit(self, X, y):\n        # MI uses lots of memory, subselect first or run out of RAM\n        Xt = X.sample(n=20000, random_state = 1)\n        yt = y[Xt.index]\n        # find discrete features, assume are ints\n        disc_f = Xt.dtypes == int\n        # find and sort scores\n        mi_scores = mutual_info_regression(Xt, yt, discrete_features=disc_f)\n        mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n        mi_scores = mi_scores.sort_values(ascending=False)\n        # save top feature names\n        self.selected_features = mi_scores.index[range(self.num_features)]\n        return self\n    \n    def transform(self, X):\n        return X[self.selected_features]   ","0ba1f2ee":"def buildFeaturesPipeline(num_clusters = 4, num_features = 10):\n    # different categorical encodings\n    categorical_features = FeatureUnion_df(transformer_list = [\n        ('ordinal_features', Pipeline(steps=[('categorical',CategoricalSelector),\n                                            ('encoder',Ordinals)])),\n        ('onehot_features', Pipeline(steps=[('categorical',CategoricalSelector),\n                                            ('encoder',OneHot)]))\n    ])\n    # starting point for numerics workflow, select the columns, and apply our custom scaler (minmax in this case)\n    numerical_starter = Pipeline(steps=[('selector',ContinuousSelector),('normalizer',Normalizer)])\n    # preserve original features and add kmeans and pca features\n    # parameterize the number of kmeans and pca features used\n    numerical_features = FeatureUnion_df(transformer_list = [\n                         ('original_features',numerical_starter)\n                         ,('kmeans_features',Pipeline(steps=[\n                             ('numerics',numerical_starter),\n                             ('kmeans',KmeansTransformer(num_clusters))\n                             ])\n                         ),\n                          ('pca_features',Pipeline(steps=[\n                              ('numerics',numerical_starter),\n                              ('pca',PcaTransformer)\n                              ])\n                          )\n                         ])\n    # union the categorical and numerical pipelines\n    combined_pipeline = FeatureUnion_df(transformer_list = [('categorical_pipeline',categorical_features),\n                                                      ('numerical_pipeline',numerical_features)])\n    # select features\n    features_pipeline = Pipeline(steps=[\n        ('feature_extraction',combined_pipeline),\n        ('feature_selection',FeatureSelector(num_features))\n    ])\n    return features_pipeline","89a67429":"fp = buildFeaturesPipeline(num_clusters = 4, num_features = 10)\nfp.fit_transform(X,y)\nfp.transform(X.head(10))","f0ad3732":"class ModelTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self,space):\n        self.space = space\n        self.fracXG = space['fracXG']\n        # create XGB model\n        self.xgmodel = XGBRegressor(n_estimators = self.space['n_estimators'], learning_rate = self.space['learning_rate']\n                                   , max_depth = self.space['max_depth'], reg_alpha = self.space['reg_alpha']\n                                   , reg_lambda = self.space['reg_lambda'], colsample_bytree = self.space['colsample_bytree']\n                                   , gamma = self.space['gamma'], tree_method = 'hist'\n                                   , subsample = 0.9, random_state = self.space['reg_alpha']*self.space['reg_lambda']+self.space['n_estimators'])\n        # create LGBM model\n        self.lgmodel = LGBMRegressor(n_estimators = self.space['n_estimators'], learning_rate = self.space['learning_rate']\n                                   , max_depth = self.space['max_depth'], reg_alpha = self.space['reg_alpha']\n                                   , reg_lambda = self.space['reg_lambda'], colsample_bytree = self.space['colsample_bytree']\n                                   # , gamma = self.space['gamma'], tree_method = 'gpu_hist'\n                                   , subsample = 0.9, random_state = self.space['reg_alpha']*self.space['reg_lambda']*self.space['max_depth'])\n        \n    def fit(self, X, y):\n        X = X.apply(pd.to_numeric, errors='coerce', downcast='float') # needed because some features are type int\n        # small internal train\/val split, needed for early stopping rounds\n        X_t, X_v, y_t, y_v = train_test_split(X, y, test_size = 0.1, random_state = 3) \n        # fit the models\n        self.xgmodel.fit(X_t, y_t, eval_set=[(X_v, y_v)],\n                      eval_metric='rmse',early_stopping_rounds=10,verbose=0)\n        self.lgmodel.fit(X_t, y_t, eval_set=[(X_v, y_v)],\n                      eval_metric='rmse',early_stopping_rounds=10,verbose=0)\n        return self\n    \n    def predict(self, X, y=None):\n        X = X.apply(pd.to_numeric, errors='coerce', downcast='float') # needed because some features are type int\n        # predict with both models\n        xgpred = self.xgmodel.predict(X)\n        lgpred = self.lgmodel.predict(X)\n        # perform weighted averaging of the models based on the hyperparameter fracXG\n        preds = xgpred*self.fracXG + lgpred*(1.0-self.fracXG)\n        return preds","0b32dd26":"def objective(trial, X=X, y=y):\n    params = {\n        # model hyperparameters to be tuned - picking which ones to tune and their ranges is a bit of an artform and not covered here\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, step = 50),\n        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.20, step = 0.01),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 100, log=True),\n        'reg_lambda': trial.suggest_int('reg_lambda', 1, 100, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5, step = 0.01),\n        'gamma': trial.suggest_float('gamma', 0.0, 1.0, step = 0.1),\n        # feature engineering hyperparameters\n        'num_clusters': trial.suggest_int('num_clusters', 1, 5),\n        'num_features': trial.suggest_int('num_features', 10, 40),\n        # model weight hyperparameters\n        'fracXG': trial.suggest_float('fracXG', 0.0, 1.0, step = 0.1)\n    }\n    fp = buildFeaturesPipeline(params['num_clusters'],params['num_features'])\n    full_pipeline = Pipeline(steps=[('features',fp),('model',ModelTransformer(params))])\n    cv = KFold(n_splits=5)\n    rmse_score = cross_val_score(full_pipeline, X, y, cv = cv, scoring = 'neg_root_mean_squared_error')\n    return -rmse_score.mean()","844a50a6":"#optuna.logging.set_verbosity(optuna.logging.WARNING) # disable normal logging for long runs or results notebook hangs browser!\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50, timeout=60*60*2) # 50 trials or 2 hours runtime, whichever comes first\nprint('Number of finished trials:', len(study.trials))\nprint('Best RMSE:', study.best_trial.value)\nprint('Best trial:', study.best_trial.params)","c379a375":"# The optimization history shows us how prediction accuracy improves over time. \n# If it stops improving after a point, that's a clue you can shorten the runtime limit on future optimizations.\nplot_optimization_history(study)","5f080ed5":"# Parallel coordinate plots can show if certain hyperparameters typically give better results in a particular range.\nplot_parallel_coordinate(study)","bb243629":"# Contour plots show more details on the performance tradeoffs of different combinations of hyperparameters\nplot_contour(study, params=['n_estimators','learning_rate'] )","8842c7fa":"# Contour plots show more details on the performance tradeoffs of different combinations of hyperparameters\nplot_contour(study, params=['num_clusters','num_features','fracXG'] )","bbefa9bc":"# Optuna also provides us with a ranked measure of feature importance, useful for narrowing down our search in the future\noptuna.importance.get_param_importances(study)","db517558":"fp = buildFeaturesPipeline(study.best_trial.params['num_clusters'],study.best_trial.params['num_features'])\nfinal_pipeline = Pipeline(steps=[('features',fp),('model',ModelTransformer(study.best_trial.params))])\nfinal_pipeline.fit(X,y)\nsub.target = final_pipeline.predict(test)\nsub.head()","de505014":"final_pipeline['features']['feature_selection'].selected_features","7a422ba6":"sub.to_csv(\"submission.csv\")\nprint(\"Complete\")","97de1fe0":"# Understanding Pipelines\nWhile many tools from sklearn (e.g. OrdinalEncoder, PolynomialFeatures, etc.) and even other packages can be plugged directly into a Pipeline, I think the best way to understand how a part of a Pipeline works is to build a simple custom step from scratch. The code section below shows how to manually build your own normalize step for a Pipeline.\n\nTo create a custom step, we must define it as a class with at least three methods. (Note that the class must inherit from BaseEstimator and TransformerMixin. The details for why will be skipped, just add this to your custom steps.)\n1. The **\\_\\_init\\_\\_** method is run when the class gets initialized. Properties for this step in the pipeline should be intialized here. If you wish to pass in these parameters as inputs, they can be specified as inputs to the \\_\\_init\\_\\_ method. Note that nothing gets returned.\n2. The **fit** method is run whenever the Pipeline is fit to training data. It must accept both X (input) data and y (target) data, even if this step (such as our normalize step) doesn't use the target data. Anything that should happen based on the training data, then get saved and reapplied to validation\/test data, should be done here and saved to a property created in the \\_\\_init\\_\\_ step. In this example, we find the mean and standard deviation of the input data and save them to the properties of the class. Note that we must return in this step.\n3. The **transform** method is run whenever we need to apply the transformation to data, whether that be training data, or validation\/test data. It is in this step that we apply the normalization and return it.\n\nIf you are unsure about exactly when each method gets used, it can be helpful to put print statements inside the methods.","bf5ed2b4":"# Step 8 - Visualize optimization results\nUsing Optuna we can now visualize how the optimization study went. This can give us insight on if certain parameters can have their ranges restricted for more efficient searching in the future.","d7ee51a2":"# Part 0 - Exploratory Data Analysis\nExploratory Data Analysis (EDA) is an important part of building intuition for your data and deciding what kind of data preparation, feature extraction, and modeling you want to try. However for the purposes of this tutorial on Pipelines we will skip this step and simply separate out our target variable.","924ace8a":"# Motivation for using Pipelines\n\nAs you hopefully learned previously, data leakage is a huge challenge in machine learning. If, for example, you want to normalize your data so that each sensor has a mean of zero and standard deviation of one, this normalizing needs to be performed first on the training data, and then its scaling parameters saved and reapplied to the validation\/test data. Applying normalization before splitting your training data is a good way to introduce data leakage!","d2177c64":"Because a DataFrameMapper has the same fit and transform methods, we can use it in a pipeline step just like we did our CustomNormalizer, but now we only need a single command to create the step rather than a whole custom class.","45023b4d":"However this approach requires manually picking out each variable and hardcoding the replacement name. sklearn provides a helper function make_column_selector that can help pick out the relevant columns, and providing an alias will create dynamically named columns.","a0e60014":"# Building a simple modeling Pipeline using built-in normalization\n\nIn this example we built a custom normalization step to illustrate how pipeline steps work. However we normally wouldn't build a custom step for this since sklearn already has a built-in way to normalize data, the StandardScaler. This object (and many others from sklearn) already has all the methods needed to drop it into a pipeline. Here's the same pipeline as in the previous step but using the built-in StandardScaler.","b3677382":"# Step 6 - Create objective function for hyperparameter tuning\nWhile this technically is a separate topic from pipelines, using pipelines makes hyperparameter tuning much easier for complex modeling. To do so, we must define a function that will accept X and y data then return an evaluation metric like RMSE.\n\nFirst we define a dictionary with all the various hyperparameters we wish to optimize. These could be hyperparameters used by models like XGBoost, or custom hyperparameters for other steps, like feature engineering or model ensembling.\n\nNext we build our pipeline using the dictionary as inputs. Then we perform cross validation so we can train and validate on all of the available data. Finally we return the RMSE.","0a06f512":"Let's try out our features_pipeline to see if it works. It is returning a dataframe with only the top 10 features in descending order.\n\nNote that the 11th principal component (PCA_10) is selected while none of the higher principal components are used. Also the one hot encoding of the sixth categorical variable is selected even though the ordinal of that variable is already selected. This is unusual, as PCA typically encodes the more useful information in the higher principal components, and redundant features on the sixth categorical contain very similar information. This suggests that there is very little separating the usefulness of these features.\n\nTo test this, go into the FeatureSelector and change the random_state used when subsampling the input data. Changing from the original value of 1 to 2 will greatly change the top 10 features selected. This further suggests that there's very little separating the usefulness of these features, and that this method of feature selection may not be helpful for this dataset.","ccccf9b3":"Create submission","beea3f07":"And we can now check that we still get the same answer as when we hand coded it earlier.","0495fd94":"There are many more details of DataFrameMapper covered in sklearn-pandas [documentation](https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas). For now, we know enough to start building our pipeline for the competition.","e7611c70":"We can now see that the fit() method has been called. We can check that the pipeline's properties updated by indexing into the pipeline object","cfa86da7":"Doing things the right way gets increasingly complicated as more steps get added to the workflow. Fortunately sklearn provides Pipelines which handle this for you! ","b0e948b1":"Let's test out that our customized steps work so far. Let's normalize the continuous variables, ordinalize the categoricals, and merge them back together.","154365a1":"We can now apply our new pipeline to our data. Note that we still need to remember to do our data split first. We can then fit the pipeline to our training data.","7bc6db7f":"We can now use this custom step in a pipeline. We'll start with a trivial one step pipeline that only applies our custom normalizer. The main input to Pipeline is a list of tuples containing a name for the step and a class that can be used in that step. Note that the class initialization message gets printed as soon as we create the pipeline.","9af06501":"It turns out that while many tools in sklearn will accept a pandas dataframe as an input, they do not return dataframes as outputs. This means we lose valuable information in the form of indicies and names. We can always write a short custom wrapper to solve this problem:","07fd6335":"# Combining Pipelines with dataframes using sklearn-pandas\n\nThe [sklearn-pandas](https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas) library seeks to make it easier to use sklearn tools on pandas dataframes. From its github page:\n\"This module provides a bridge between [Scikit-Learn](http:\/\/scikit-learn.org\/stable)'s machine learning methods and [pandas](https:\/\/pandas.pydata.org\/)-style Data Frames. In particular, it provides a way to map DataFrame columns to transformations, which are later recombined into features.\"","fa750bbf":"# Step 7 - Optimize those hyperparameters!\nWe can now optimize the hyperparameters using the Optuna package (there are also many other packages for hyperparameter tuning). Here we'll just do a few trials, but the limit is just how long you're willing to wait.\n\nBy default you will get a print out for each iteration. Be careful! I've tried this with long runs and many iterations, and Kaggle will crash when attempting to display notebooks that save long runs. So turn off these displays by uncommenting the first line before you do a long run.","75f2261d":"# Building a sophisticated modeling Pipeline for the competition.\nUp until here has been a basic introduction to pipelines. We can build much more sophisticated pipelines than we we've seen so far. Below we create additional custom steps to get excellent performance on competition data.","fed65bf4":"This provides an easier way to retain dataframes than by defining a custom class, and there are additional options to make this easier and more useful. First off, the column names in the above dataframe are identical. It'd be helpful if they could be differently named.","4e140d59":"However, if we look at the output of just the StandardScaler, we can see that it differs from our CustomNormalizer.","576f4d0e":"If we compare the output of these two pipelines, we can see that they are exactly equal to each other.","04683702":"The main tool from sklearn-pandas is the DataFrameMapper, which works on a column-by-column basis to apply tools from sklearn like StandardScaler. In the example below, the same column, 'cont0', is run through both the StandardScaler and our CustomNormalizer to show they give the same results. Note that we need to specify 'df_out = True' in order to get a dataframe returned.","08c2bdee":"# Step 3 - Custom feature extraction and selection \nWe can add custom feature extraction to the pipeline. Here we create two transformers that will create features based on distance to kmeans clusters and principal components from PCA. Becaue we don't need to parameterize PCA for hyperparameter tuning, we can again use the DataFrameMapper. However if we want the number of clusters in kmeans to be tunable, we must create a custom class since DataFrameMapper requires us to specify all values at creation time. We can also create a step for feature selection, so we can determine if these new features are adding any value.","762aa580":"# Step 5 - Custom model ensembling\nHere we'll do some ensembling of lightGBM and XGBoost. In the \\_\\_init\\_\\_ method you'll see we use a dictionary called 'space' which contains the hyperparameters that are optimized using a later step. In the predict method, we predict with both models and combine them with a weighted average that is tuned by a hyperparameter.","1c842ee1":"Import the data.","427b02a8":"# Step 4 - Assemble the feature extraction pipeline\nNow we have all the parts we need to build the pipeline prior to the point where we introduce the model. Note that here we use the FeatureUnion to horizontally concatenate different parts of the pipeline. This is a great way to tack on independently computed features. In the final step, we will use our FeatureSelector to pick out the top features that should be useful for modeling.","ac407918":"Finally when we want to apply these normalization parameters to new data, we can use transform on data not in our training set.","529235b1":"However we would have to write a custom class every time we want to combine sklearn with dataframes. While possible, it's a bit easier to use the sklearn-pandas package.","e853063c":"# Step 9 - Predict on test data and submit!\nNow that we have optimized our hyperparameters, we can recreate a final version of our pipeline, train it on all available data, and predict on the test data for submission.","c0c88fe4":"# Step 1 - Separating categorical and continuous variables\nDifferent data types often need to be treated differently, so we'll start by defining simple custom steps to subselect a particular type according to its datatype in the competition data. An easy way to create a pipeline step to select a datatype is to create a DataFrameMapper with no transformation applied.","2cc6864c":"However, we still haven't updated our training data. This is because data is only modified when calling the transform method. While we could now call transform on our training data, performing first a fit and then a transform is so common that there is a utility for it, fit_transform. Note which method's print statements are seen each time.","8831adb6":"Let's start by importing the modules we'll use","6f8eedee":"# Step 2 - Modifying built-in steps to return dataframes\nThe scalers\/normalizers\/encoders in sklearn don't natively work on whole dataframes, which is annoying, so use sklearn-pandas. Also the FeatureUnion step only returns a numpy array not dataframe. Create a custom Pipeline transformer to have it accept and return a dataframe.","2975c495":"What are the final features used? Print them in order of usefulness according to MI.","d7e6889a":"# Building a simple modeling Pipeline with our custom normalization\nMany tools from sklearn and other modules already have the necessary methods to be plugged into a pipeline. Here we'll use a simple LinearRegression model with our normalization step to train and then predict on validation data. (Note that models have a predict method instead of a transform method.)","f127e56c":"You can see that the \\_\\_init\\_\\_ method was called when creating the pipeline object. The fit and transform methods were both called when fitting the pipeline. The fit method was called to find the mean and std of the training data, and the transform method was then applied to the training data so the LinearRegression model would receive a normalized input. Finally the transform method was called again when predicting on the validation data to normalize the data prior to predicting with the model. Again, note that the same mean and std computed on the training data are applied when transforming the validation data.\n\nNow that all our work is in a Pipeline, training on one dataset and predicting on another is just two lines! We no longer have to think about keeping track of all the intermediate steps that could lead to data leakage.","c90aafbd":"# Introduction\n\nThe sklearn Pipelines are fantastic tools for managing complex machine learning workflows. However they are not beginner-friendly, especially if they don't support what you want out-of-the-box. They also don't natively work well with Pandas dataframes, requiring the addition of the [sklearn-pandas](https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas) package for fluid integration.\n\nThis notebook shows how to build a custom Pipeline that preserves dataframes while incorporating customizable steps for:\n* Data preparation\n* Feature extraction\n* Feature selection\n* Ensembling of multiple models\n* Cross validation\n* Hyperparameter tuning"}}