{"cell_type":{"9cd887be":"code","7acec6d8":"code","7a25a4b7":"code","12062324":"code","a1fdb17f":"code","853401b1":"code","db4fcd21":"code","4b3c16de":"code","5ae2eb8f":"code","1dc9f939":"code","bcb8ba66":"code","d2d53ab7":"code","f248e0c6":"code","3dd57c22":"code","0fb53e3f":"code","ea81edaf":"code","57471c29":"code","d25a110b":"code","877f59ed":"code","af0a6f4f":"code","617c0e15":"code","44db52a9":"code","9a913cc0":"code","252c57ac":"code","9800d124":"code","86730ecc":"code","e90cf98f":"markdown","5b948dce":"markdown","fa5d7ec7":"markdown","d4f9e9d7":"markdown","7474f692":"markdown","8869f079":"markdown","037467b7":"markdown","da090e1a":"markdown","db85c202":"markdown","de2a26bf":"markdown","b5ebba80":"markdown","59ed99f2":"markdown","4c358aa8":"markdown"},"source":{"9cd887be":"import pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport random\nfrom skimage import io\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom torchvision import models\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","7acec6d8":"random.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","7a25a4b7":"DATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = torch.device(\"cuda\")","12062324":"class SimpsonsDataset(Dataset):\n    \"\"\"\n    \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0430\u0440\u0430\u043b\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0438\u0445 \u0438\u0437 \u043f\u0430\u043f\u043e\u043a\n    \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442 \u0441\u043a\u0430\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u0432 \u0442\u043e\u0440\u0447\u0435\u0432\u044b\u0435 \u0442\u0435\u043d\u0437\u043e\u0440\u044b\n    \"\"\"\n    def __init__(self, files, mode):\n        super().__init__()\n        # \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0430\u0439\u043b\u043e\u0432 \u0434\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438\n        self.files = sorted(files)\n        # \u0440\u0435\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u044b\n        self.mode = mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index):\n        # \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b PyTorch \u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0445\u043e\u0434\u0430\n        if self.mode != 'train':\n            transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n          ])\n        # \u0435\u0441\u043b\u0438 \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0441\u0435\u0442\u0438, \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u043c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e\n        else:\n            transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.RandomChoice(\n                                  [transforms.ColorJitter(0.5, 0.5),\n                                  transforms.RandomPerspective(),\n                                  transforms.RandomGrayscale(),\n                                  transforms.RandomHorizontalFlip(),\n                                  transforms.RandomVerticalFlip(),\n                                  transforms.RandomRotation(180)]),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n          ])\n        x = self.load_sample(self.files[index])\n        x = self._prepare_sample(x)\n        x = transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)","a1fdb17f":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow \u0434\u043b\u044f \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","853401b1":"import os\nimport shutil\n\nTRAIN_DIR = Path('\/kaggle\/input\/simpsons4\/train\/simpsons_dataset\/')\nTEST_DIR = Path('\/kaggle\/input\/simpsons4\/testset\/testset')\n\nshutil.copytree(TRAIN_DIR, '\/kaggle\/working\/simpsons_dataset')\nTRAIN_DIR = Path('\/kaggle\/working\/simpsons_dataset')\n\nfolders = ([name for name in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, name))])\n\nfor folder in folders:\n    contents = os.listdir(os.path.join(TRAIN_DIR,folder))\n    while len(contents) < 500:\n        i = 1\n        for file in contents:\n            base, extension = os.path.splitext(file)\n            new_file = os.path.join(TRAIN_DIR,folder, base + str(i) + extension)\n            shutil.copy(os.path.join(TRAIN_DIR,folder, file), os.path.join(TRAIN_DIR,folder, new_file))\n        i += 1\n        contents = os.listdir(os.path.join(TRAIN_DIR,folder))\n        print(folder, len(contents))\n\n              \ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","db4fcd21":"from sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n                                          stratify=train_val_labels)","4b3c16de":"val_dataset = SimpsonsDataset(val_files, mode='val')","5ae2eb8f":"fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    im_val, label = val_dataset[random_characters]\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","1dc9f939":"def fit_epoch(model, train_loader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss \/ processed_data\n    train_acc = running_corrects.cpu().numpy() \/ processed_data\n    return train_loss, train_acc","bcb8ba66":"def eval_epoch(model, val_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss \/ processed_size\n    val_acc = running_corrects.double() \/ processed_size\n    return val_loss, val_acc","d2d53ab7":"def train(train_files, val_files, model, epochs, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        # \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 lr, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c scheduler\n        opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size = 14, gamma=0.1)\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n            scheduler.step()\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","f248e0c6":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","3dd57c22":"n_classes = len(np.unique(train_val_labels))\nprint(\"We will classify {} classes\".format(n_classes))","0fb53e3f":"if val_dataset is None:\n    val_dataset = SimpsonsDataset(val_files, mode='val')\n    \ntrain_dataset = SimpsonsDataset(train_files, mode='train')","ea81edaf":"model = models.resnet34(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, n_classes)\nmodel = model.to(DEVICE)","57471c29":"history = train(train_dataset, val_dataset, model=model, epochs=30, batch_size=32)","d25a110b":"loss, acc, val_loss, val_acc = zip(*history)","877f59ed":"plt.figure(figsize=(15, 9))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","af0a6f4f":"idxs = list(map(int, np.random.uniform(0,1000, 50)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)","617c0e15":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","44db52a9":"y_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\n\npreds_class = [i for i in y_pred]","9a913cc0":"from sklearn.metrics import f1_score\n\nf1_score(actual_labels, preds_class, average='macro')\n","252c57ac":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=32)\nprobs = predict(model, test_loader)\n\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\ntest_filenames = [path.name for path in test_dataset.files]","9800d124":"submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nsubmit.head()","86730ecc":"submit.to_csv('submission.csv', index=False)","e90cf98f":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u043d\u0430\u043c \u0431\u0438\u0431\u043b\u043e\u0442\u0435\u043a\u0438.","5b948dce":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0442\u0438\u043f\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432, \u0440\u0430\u0437\u043c\u0435\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u044b\u043a\u0438 \u0438 \u0434\u0435\u0432\u0430\u0439\u0441.","fa5d7ec7":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u0430\u0442\u0430 \u0432\u0440\u0430\u043f\u043f\u0435\u0440 \u0438\u0437 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430 \u043d\u0430 \u043a\u043e\u043b\u0430\u0431\u0435.","d4f9e9d7":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u044b","7474f692":"\u041f\u0440\u043e\u0441\u043c\u043e\u0440\u0442 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0438\u0437 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 (\u043c\u043e\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0439 \u0448\u0430\u0433)","8869f079":"\u0424\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u0441\u043e\u0442\u0438","037467b7":"\u041d\u0430\u0448 Submit","da090e1a":"\u0424\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f Train \u0438\u0437 \u043a\u043e\u043b\u0430\u0431\u0430","db85c202":"\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u0438","de2a26bf":"\u0421\u0442\u0440\u043e\u0438\u043c \u043a\u0440\u0438\u0432\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","b5ebba80":"**\u0418\u0441\u0445\u043e\u0434\u043d\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0443\u0440\u0443 \u043c\u044b \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 Transfer Learning**\n\n\u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u043b\u0443\u0447\u0430\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c Pretreined Resnet18, \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043c\u043e\u0440\u043e\u0437\u0438\u043c","59ed99f2":"\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0448\u0430\u0433\u0435 \u043c\u044b \u0440\u0430\u0437\u043c\u043d\u043e\u0436\u0430\u0435\u043c \u043c\u0430\u043b\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b (\u043f\u0440\u0430\u0432\u0434\u0430, \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043d\u0430\u043c \u044d\u0442\u043e \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438\u043b\u0438 \u0436\u0435 \u043d\u0435\u0442, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0441\u043a\u043e\u0440 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u043b\u0441\u044f \u043c\u043d\u043e\u0439 \u0438 \u0431\u0435\u0437 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0448\u0430\u0433\u0430, \u043d\u043e \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u043b\u0438\u0447\u0435\u0442\u0441\u0432\u043e\u043c \u044d\u043f\u043e\u0445)","4c358aa8":"\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u044b\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435"}}