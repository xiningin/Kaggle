{"cell_type":{"32c74b63":"code","51146921":"code","df8b20f7":"code","1aa91de3":"code","245563e1":"code","4c0b3a9e":"code","26d09978":"code","2765058f":"code","1b3dff7a":"code","aa095df9":"code","8f94163e":"code","bcecab70":"code","e43449bb":"code","c27ff11b":"code","3093201e":"code","9120cd90":"code","1a07f1b3":"code","12220837":"code","b97ffcb0":"code","b848e083":"code","9bb34d85":"code","6b4782e1":"code","309a4595":"code","3fe19912":"code","ac9b8173":"code","ef58c1a8":"code","ae3d6914":"code","5b3a2277":"code","09bd0a60":"code","484111b7":"code","f58ac0e9":"code","19fd7fb4":"code","3ae81309":"markdown","c821a4d5":"markdown","5119d1e7":"markdown"},"source":{"32c74b63":"!pip install -U torch","51146921":"\nimport pyarrow\ntds = pyarrow.parquet.ParquetDataset('\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/',)\nTrade = tds.read().to_pandas()\nstock_ids = list(sorted(Trade.stock_id.unique()))\nntimeids = Trade.time_id.nunique()","df8b20f7":"timeids = sorted(Trade.time_id.unique())\ntimeid_to_ix = {time_id:i for i,time_id  in enumerate(timeids)}\nstock_id_to_ix = {stock_id:i for i,stock_id  in enumerate(stock_ids)}\nntimeids = Trade.time_id.nunique()","1aa91de3":"import pyarrow as pa\nimport pyarrow\nimport pyarrow.parquet as pq\nimport pandas as pd\n# \n","245563e1":"def calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\ndef book_features(df):\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    return df\n    ","4c0b3a9e":"book = book_features(Book)","26d09978":"for stock in stock_ids[:20]:\n    stock_ix = stock_id_to_ix[stock]\n    \n    #loads trades for the stock\n    tds = pyarrow.parquet.ParquetDataset('\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/',filters=[('stock_id','=',f'{stock}')],validate_schema=True)\n    Trade = tds.read().to_pandas()\n    Trade['log_return_trade'] = Trade.groupby('time_id')['price'].apply(log_return).fillna(0)\n    \n    #loads book\n    bds = pyarrow.parquet.ParquetDataset('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/',filters=[('stock_id','=',f'{stock}')],validate_schema=True,)\n    Book  =bds.read()\n    Book = Book.to_pandas()\n    Book = book_features(Book)\n    #Join trade and book, I call it Asof like an asof join\n    Asof = pd.merge(Book,Trade,on=['seconds_in_bucket','time_id'],how='left')\n    Asof = Asof.sort_values(['time_id','seconds_in_bucket']).fillna(0)\n    Asof = Asof.rename(columns={'stock_id_x':'stock_id'})\n    for time_id in timeids:\n        time_ix = timeid_to_ix[time_id]\n        asof_input = Asof[Asof.time_id==time_id]\n        arr[time_ix,stock_ix,:len(asof_input),:] = asof_input.values\n    #Store it. I'm not doing this perfectly, in a Spark context you'd pay more attention to the partitioning. \n    table = pa.Table.from_pandas(Asof)\n    pq.write_to_dataset(table,'.\/asof5',partition_cols=['stock_id'])\n\n            ","2765058f":"import numpy as np\nnstocks = len(stock_ids)\nnfeatures = len(Asof.columns)\nnseq =  600\narr =np.memmap('map.array',dtype='float64',mode='w+',shape=(ntimeids,nstocks,nseq,nfeatures))","1b3dff7a":"arr[22,10,22,:]","aa095df9":"arr[0,0][-1,:]","8f94163e":"Asof","bcecab70":"a = np.array([])\na.fields","e43449bb":"Book[['bid_price1','bid_price2','ask_price1','ask_price2','seconds_in_bucket']].pct_change().fillna(0).add_suffix('_change')","c27ff11b":"Book[['bi'.pct_change()","3093201e":"import pyarrow\n#Notice, I'm using parquets partitioning to only load data for stock 5 into memory. Everything else is on disk \n# loadedAsof = pyarrow.parquet.ParquetDataset('.\/asof5',filters=[('stock_id','in','[5,6,7,8]')],validate_schema=True,).read().to_pandas()\nloadedAsof = pyarrow.parquet.ParquetDataset('.\/asof5',validate_schema=True,).read().to_pandas()\nloadedAsof.tail()\n\n","9120cd90":"import pandas as pd\ntargets = pd.read_csv('\/kaggle\/input\/optiver-realized-volatility-prediction\/train.csv')\n","1a07f1b3":"nstocks = targets.stock_id.max()","12220837":"import numpy as np \nfeatures = 'bid_price1\task_price1\tbid_price2\task_price2\tbid_size1\task_size1\tbid_size2\task_size2\tprice\tsize\torder_count\tstock_id\tseconds_in_bucket'.split('\t')\nnfeatures = len(features)\ndef batcher(df,targets_df, batch_size=3):\n    stocks = df.stock_id.unique()\n    timeids = df.time_id.unique()\n    rstock = np.random.choice(stocks, batch_size)\n    rtimes = np.random.choice(timeids, batch_size)\n    batches = []\n    targets = np.zeros(batch_size)\n    lens = []\n    max_seq = 0\n    for i,(sid, tid) in enumerate(zip(rstock, rtimes)):\n        val = df[(df.stock_id == sid) & (df.time_id == (tid))].sort_values('size',ascending=False)[:100]\n        lens.append(len(val))\n        max_seq = max(max_seq, len(val))\n        batches.append(val)\n        sub_targets =targets_df[(targets_df.stock_id==sid) & (targets_df.time_id==tid)].target\n        if len(sub_targets)>0:\n            targets[i] = targets_df[(targets_df.stock_id==sid) & (targets_df.time_id==tid)].target.values[0]\n        else:\n            #hack to overcome data points with no target\n            targets[i]=0\n\n    batch = np.ones([batch_size, max_seq, nfeatures])\n    mask = np.ones([batch_size, max_seq+1])\n    for i, b in enumerate(batches):\n        \n        batch[i, : lens[i]] = b[features].values\n        mask[i,lens[i]+1:] =0\n    \n    return batch,mask,targets\n","b97ffcb0":"import torch\nfrom torch import nn\nemb_size =27\npdim =96\ndim = pdim +emb_size*2\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stock_embedding = nn.Embedding(nstocks+1,emb_size)\n        self.time_embedding = nn.Embedding(601,emb_size)\n        l = nn.TransformerEncoderLayer(d_model=dim,dim_feedforward=256,nhead=10,batch_first=True,dropout=0.002)\n        self.E = nn.TransformerEncoder(encoder_layer=l,num_layers=6,)\n        self.fproj = torch.nn.Linear(in_features=len(features)-2,out_features=pdim,)\n        self.lin1 = torch.nn.Linear(in_features=dim,out_features=16,)\n        self.lin2 = torch.nn.Linear(in_features=16,out_features=1,)\n        self.target = torch.nn.Parameter(torch.nn.init.uniform_(torch.empty(dim)),requires_grad=True)\n        self.eps = torch.Tensor([0.00000001]).cuda()\n    def forward(self,book,mask):\n        features = book[:,:,:-2]\n        features = self.fproj(features)\n        features = nn.functional.relu(features)\n        stock_id = book[:,:,-2].type(torch.LongTensor).cuda()\n        sec_id = book[:,:,-1].type(torch.LongTensor).cuda()\n        stock_emb = self.stock_embedding(stock_id)\n        sec_emb = self.time_embedding(sec_id)\n        expanded_target = self.target.expand(features.shape[0],1,dim)\n        final_features = torch.cat([features,stock_emb,sec_emb],dim=2)\n        final_features = torch.cat([expanded_target,final_features],dim=1)\n        seq = self.E(final_features,src_key_padding_mask=mask)\n        pred = seq[:,0,:]\n        pred = torch.tanh(self.lin1(pred))\n        pred = self.lin2(pred)+self.eps\n        \n        return torch.squeeze(pred)\nmodel = Model().train()","b848e083":"target = torch.nn.Parameter(torch.nn.init.uniform_(torch.empty(emb_size)),requires_grad=True)\nemp = torch.empty(12,5,emb_size)\nemp.shape,target.shape","9bb34d85":"expanded = target.expand(emp.shape[0],1,emb_size)\nres = torch.cat([expanded,emp],dim=1)\nres[:,0,1]","6b4782e1":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nclass LSTMModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stock_embedding = nn.Embedding(nstocks+1,emb_size)\n        self.time_embedding = nn.Embedding(601,emb_size)\n        self.rnn = nn.GRU(dim,128,2,batch_first=True)\n        self.h0 = torch.zeros(2,128).unsqueeze(1)\n        self.lin = torch.nn.Linear(in_features=128,out_features=1,)\n        \n    def forward(self,book,mask):\n        features = book[:,:,:-2]\n        stock_id = book[:,:,-2].type(torch.LongTensor).cuda()\n        sec_id = book[:,:,-1].type(torch.LongTensor).cuda()\n        stock_emb = self.stock_embedding(stock_id)\n        sec_emb = self.time_embedding(sec_id)\n        final_features = torch.cat([features,stock_emb,sec_emb],dim=2)\n        lens = (torch.sum(mask,axis=1)).type(torch.LongTensor)\n        packed = pack_padded_sequence(final_features, lens, batch_first=True, enforce_sorted=False)\n        \n        h0 = torch.zeros(2,features.shape[0],128).cuda()\n        output,hn = self.rnn(packed,h0)\n\n        return self.lin(hn[1]).squeeze()\n        return self.lin(output[:,last_el])\n\n\n    ","309a4595":"\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        val = self.mse(yhat,y)\n        return torch.sqrt(val)\n\ncrit = RMSELoss()\noptim = torch.optim.AdamW(lr=0.052,params=model.parameters())","3fe19912":"!export CUDA_LAUNCH_BLOCKING=1","ac9b8173":"arr.flush()","ef58c1a8":"model = LSTMModel().train()\nmodel = model.cuda()\npred = model(inp,mask,)\npred.shape","ae3d6914":"lin = torch.nn.Linear(in_features=128,out_features=1,).cuda()\nlin(pred[:,1,:]).shape","5b3a2277":"loadedAsof = loadedAsof.drop_duplicates()\nx = loadedAsof[(loadedAsof.stock_id==10) & (loadedAsof.time_id==16877)]\nx.seconds_in_bucket.value_counts()","09bd0a60":"import torch\n# del model\n# model = LSTMModel().train()\nmodel = model.cuda()\noptim = torch.optim.AdamW(lr=0.00052,params=model.parameters())\none = torch.Tensor([1.0]).cuda()\neps = torch.Tensor([0.00000001]).cuda()\nfor i in range(100000):\n        batch,mask,target = batcher(loadedAsof,targets_df=targets,batch_size=4)        \n        targ = torch.Tensor(target).cuda()\n        mask=torch.Tensor(mask).cuda()\n        inp =torch.Tensor(batch).cuda()\n        \n        for j in range(5):\n            optim.zero_grad()\n            pred = model(inp,mask,)\n            ploss = torch.sqrt(torch.mean((pred-targ)\/targ)**2)\n            loss = torch.mean((one-(pred\/(targ+eps)))**2)\n            loss.backward()\n            optim.step()\n            if (i+j)%50==0:\n                print(ploss.item(),loss.item())\n","484111b7":"# pred = model(torch.Tensor(batch).cuda(),mask=torch.Tensor(mask).cuda(),)\nbatch,mask,target = batcher(loadedAsof,targets_df=targets,batch_size=64)        \nmask.cuda()\nbatch.shape","f58ac0e9":"targ","19fd7fb4":"import torch\nsrc = torch.rand(10, 32, dim)\nx = torch.FloatTensor(X[:10].values)\n# E()\nx.shape\n","3ae81309":"## First, get the list of stock ids\nLoad all the trades in parquet, and get the list of stock ids","c821a4d5":"# After Running Above\nAfter running data, you'll have data in asof5 (or whatever you rename the directory to). Let's see what it looks like","5119d1e7":"# Merging\nIn this notebook I'll show how I merge\/join the trades and order book for all stocks in a memory efficient way. \nAt the end, we'll have a new collection of parquet files, one per stock, where each row corresponds to a row in the stocks book file, and also has the trades on it if availble\n\nWe leverage the fact that the original data is partitioned by stock_id, and that we are given that time_ids are independent. Using that, we iterate over the original data, stock by stock, join and then save to parquet. \nReady ? Here goes "}}