{"cell_type":{"c4ca58e0":"code","c4c15573":"code","f05f6685":"code","89ba7164":"code","816e0f07":"code","d44b63f3":"code","3e20c8fd":"code","f8738924":"code","5cb965e9":"code","6c3d36d5":"code","b67dbecf":"code","9fb0ff66":"code","faf8aece":"code","283097af":"code","c89aa3c4":"code","ee353694":"code","9dd1d0e2":"code","dd5d606d":"code","e3a8dca1":"code","35970466":"code","5ee4d449":"code","4d8e0794":"code","697628ba":"code","200db4d1":"code","6b7b72b2":"code","34bd0a48":"code","13e2178c":"code","074e366f":"code","f747305b":"code","243d2566":"code","e5ddc687":"code","4405d82c":"markdown","a62456e8":"markdown","3bab8eb1":"markdown","94f64f07":"markdown","f06e6020":"markdown","fb26cf64":"markdown","7dbc1404":"markdown","42f79fcc":"markdown","8a4c4c50":"markdown","2e3bf319":"markdown","e522f7bf":"markdown","da2810a2":"markdown","8d57f99c":"markdown","6ba87d09":"markdown","10f7cefc":"markdown","85f3e496":"markdown","7c98b653":"markdown","30ed7d8b":"markdown","ccf2b311":"markdown","8b523a62":"markdown"},"source":{"c4ca58e0":"# data processing and linear algebra\n\nimport numpy as np \nimport pandas as pd \nfrom numba import njit\n\n#Data Visualiation\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport cv2 as cv\n\n# Machine learning algorithme\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom glob import glob\nfrom skimage import io\nfrom os import listdir\nimport pickle\n\nimport random\nfrom pprint import pprint\nimport time\nimport copy\nfrom tqdm.notebook import tqdm\ntqdm().pandas();5\n\nprint('import complete')","c4c15573":"data = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\")\nlen(data)","f05f6685":"file = listdir(\"..\/input\/breast-histopathology-images\")\nlen(file)","89ba7164":"patient_file = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\")\nlen(patient_file)","816e0f07":"patient_file","d44b63f3":"zero = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/0')\none = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/1')","3e20c8fd":"train = '..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/'\npatient_ids = listdir(train)","f8738924":"class_0_total = 0\nclass_1_total = 0","5cb965e9":"for patient_id in patient_ids:\n    class_0_files = listdir(train + patient_id + '\/0')\n    class_1_files = listdir(train + patient_id + '\/1')\n\n    class_0_total += len(class_0_files)\n    class_1_total += len(class_1_files) \n\ntotal_images = class_0_total + class_1_total\n    \nprint(f'Number of patches in Class 0: {class_0_total}')\nprint(f'Number of patches in Class 1: {class_1_total}')\nprint(f'Total number of patches: {total_images}')","6c3d36d5":"columns = [\"patient_id\",'x','y',\"target\",\"path\"]\ndata_rows = []\ni = 0\niss = 0\nisss = 0\n\n# note that we loop through the classes after looping through the \n# patient ids so that we avoid splitting our data into [all class 0 then all class 1]\nfor patient_id in patient_ids:\n    for c in [0,1]:\n        class_path = train + patient_id + '\/' + str(c) + '\/'\n        imgs = listdir(class_path)\n        \n        # Extracting Image Paths\n        img_paths = [class_path + img + '\/' for img in imgs]\n        \n        # Extracting Image Coordinates\n        img_coords = [img.split('_',4)[2:4] for img in imgs]\n        x_coords = [int(coords[0][1:]) for coords in img_coords]\n        y_coords = [int(coords[1][1:]) for coords in img_coords]\n\n        for (path,x,y) in zip(img_paths,x_coords,y_coords):\n            values = [patient_id,x,y,c,path]\n            data_rows.append({k:v for (k,v) in zip(columns,values)})\n","b67dbecf":"# We create a new dataframe using the list of dicts that we generated above\ndata = pd.DataFrame(data_rows)\nprint(data.shape)\ndata","9fb0ff66":"#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'","faf8aece":"cancer_perc = data.groupby(\"patient_id\").target.value_counts()\/ data.groupby(\"patient_id\").target.size()\ncancer_perc = cancer_perc.unstack()\n\nfig, ax = plt.subplots(1,3,figsize=(25,5))\n\n# Plotting Frequency of Patches per Patient\nsns.distplot(data.groupby(\"patient_id\").size(), ax=ax[0], color=\"green\", kde=False, bins=20)\nax[0].set_xlabel(\"Number of patches\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"How many patches do we have per patient?\")\n\n# Plotting Percentage of an image that is covered by Invasive Ductile Carcinoma\nsns.distplot(cancer_perc.loc[:, 1]*100, ax=ax[1], color=\"red\", kde=False, bins=20)\nax[1].set_title(\"How much percentage of an image is covered by IDC?\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"% of patches with IDC\")\n\n# Plotting number of patches that show IDC\nsns.countplot(data.target, palette='pastel', ax=ax[2]);\nax[2].set_ylabel(\"Count\")\nax[2].set_xlabel(\"no(0) versus yes(1)\")\nax[2].set_title(\"How many patches show IDC?\");","283097af":"positive_tissue = np.random.choice(data[data.target==1].index.values, size=100, replace=False)\nnegative_tissue = np.random.choice(data[data.target==0].index.values, size=100, replace=False)","c89aa3c4":"n_rows = 5\nn_cols = 5","ee353694":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","9dd1d0e2":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indices\n        idx = negative_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","dd5d606d":"def get_patient_df(patient_id):\n    return data.loc[data['patient_id']== patient_id,:]","e3a8dca1":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols","35970466":"\ncolors = ['pink', 'red']\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 22))\n\npatient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        \n        ax[row,col].scatter(patient_df.x.values, \\\n                            patient_df.y.values, \\\n                            c=patient_df.target.values,\\\n                            cmap=ListedColormap(colors), s=20)\n        ax[row,col].set_title(\"patient \" + patient_id)","5ee4d449":"def visualise_breast_tissue(patient_id, df = data,pred = False, crop_dimension = [50,50]):\n    # Plotting Settings\n    plt.xticks([])\n    plt.yticks([])\n    # Get patient dataframe\n    p_df = get_patient_df(patient_id)\n    # Get the dimensions of the breast tissue image\n    max_coord = np.max((*p_df.x,*p_df.y))\n    # Allocate an array to fill image pixels in,use uint8 type as you don't need an int over 255\n    grid = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    mask = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    # Replace array values with values of the image\n    for x,y,target,path in zip(p_df['x'],p_df['y'],p_df['target'],p_df['path']):\n        try:\n            img = io.imread(path)\n            # Replace array values with cropped image values\n            grid[y:y+crop_dimension[1],x:x+crop_dimension[0]] = img\n            # Check if target is cancerous or not\n            if target != 0:\n                # If the target is cancerous then, replace array values with the color blue\n                mask[y:y+crop_dimension[1],x:x+crop_dimension[0]] = [0,0,255]\n        except: pass\n    # if prediction is not specifies then show the image normally\n    if pred == False:\n        io.imshow(grid)\n        img = grid\n    # if prediction is specified then apply a mask to the areas that contain predicted cancerous cells\n    else:\n        # Specify the desired alpha value\n        alpha = 0.78\n        # This is step is very important, adding 2 numpy arrays sets the values to float64, which is why convert them back to uint8\n        img = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        io.imshow(img)\n    return img","4d8e0794":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols","697628ba":"\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        p_id = patient_ids[col + n_cols*row]\n        \n        img = visualise_breast_tissue(p_id, pred = True)\n        ax[row,col].grid(False)\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])\n        ax[row,col].set_title(\"Breast tissue slice of patient: \" + p_id)        \n        ax[row,col].imshow(img)","200db4d1":"def get_classes_split(series):\n    ratio = np.round(series.value_counts()\/series.count()*100,decimals = 1)\n    return ratio\ngroups = [df for _, df in data.groupby('patient_id')]\nrandom.shuffle(groups)\nshuffled_data = pd.concat(groups).reset_index(drop=True)\n(get_classes_split(data['target']))","6b7b72b2":"from sklearn.model_selection import train_test_split","34bd0a48":"# Set the patient ids as indices for the dataframe\nshuffled_data = data.set_index('patient_id')\n# Select all columns except the target column and store it in X\nX = shuffled_data.loc[:, shuffled_data.columns != 'target']\n# Select the target column and store it in y \ny = shuffled_data['target']\n\n# OS stands for 'Out of Sample'\nX_data, OSX_df, y_data, OSy_df = train_test_split(X, y, test_size=0.1, shuffle = False)\n# We split it even further and obtain the training and testing dataframes\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_data, y_data, test_size=0.3, shuffle = False)\n\nprint(f'y_data: {get_classes_split(y_data)}')\ndisplay(get_classes_split(y_test_df))\ndisplay(get_classes_split(OSy_df))","13e2178c":"# Dataframe containing cancerous images\nc_df = data.loc[data.target == 1,:]\n# Dataframe containing normal images\nn_df = data.loc[data.target == 0,:]\n\nfraction_c = np.round(0.7*c_df.shape[0]).astype(int)\nfraction_n = np.round(0.2*n_df.shape[0]).astype(int)\n\nrest_c_df = c_df.iloc[fraction_c:-1]\nrest_n_df = n_df.iloc[fraction_n:-1]\n\nc_df = c_df.iloc[0: fraction_c]\nn_df = n_df.iloc[0: fraction_n]\n\nnc = c_df.shape[0]\nnn = n_df.shape[0]\n\nnrc = rest_c_df.shape[0]\nnrn = rest_n_df.shape[0]\n\ntotal_test = nn+nc\ntotal_train = nrn + nrc\n\nprint(\"Testing Data:\") \nprint(f'percent cancerous : {round(nc\/total_test*100,1)}%')\nprint(f'percent non-cancerous : {round(nn\/total_test*100,1)}%\\n')\nprint(\"Training Data:\")\nprint(f'percent cancerous : {round(nrc\/total_train*100,1)}%')\nprint(f'percent non-cancerous : {round(nrn\/total_train*100,1)}%')","074e366f":"n_train_df = c_df.append(n_df, sort = True).reset_index(drop=True)\nn_test_df = rest_c_df.append(rest_n_df,sort = True).reset_index(drop=True)","f747305b":"def shuffle_patients(frame):\n    groups = [df for _, df in frame.groupby('patient_id')]\n    random.shuffle(groups)\n    shuffled_df = pd.concat(groups).reset_index(drop=True)\n    return shuffled_df\nn_train_df = shuffle_patients(n_train_df)\nget_classes_split(n_train_df['target'])","243d2566":"import gc\nfrom sklearn.decomposition import IncrementalPCA\ndef rgb_to_grayscale(img_paths, batch_size = 15000):\n    # get the total number of images\n    num_of_imgs = img_paths.shape[0]\n    # initialize counter that keeps track of position of image being loaded\n    pos = 0\n    # initialize empty array in order fill in the image values\n    grid = np.zeros((num_of_imgs*2500, 3))\n\n    for img_path in tqdm(img_paths, total=num_of_imgs):\n        # Read the image into a numpy array \n        img = io.imread(img_path)\n        # reshape the image to such that the rgb values are the columns of the matrix\n        img = img.reshape(-1, 3)\n        # replace the empty array with the values inside the image\n        grid[pos: pos + img.shape[0],:] = img\n        # update position counter\n        pos += img.shape[0]\n        \n    # initialize pca to reduce rgb scale to a single dimensional scale\n    ipca = IncrementalPCA(n_components=1, batch_size=batch_size)\n    # fit pca object to the contents within the grid\n    ipca.fit(grid)\n    # delete grid to free up sum memory\n    del grid\n    gc.collect()\n    ","e5ddc687":"img_paths = n_train_df['path']\nrgb_pca = rgb_to_grayscale(img_paths)","4405d82c":"### 1. Splitting Data to Cancerous and non-Cancerous data\n### 2. Creating New Training & Testing Dataset\n### 3. Shuffling Our Data\n","a62456e8":"#### We have a total number of about 280 sub-folders, let's take a peak into the folder and try and understand what those sub-folders are.\n\n#### Each subfolder seems to be the ID of the corresponding patient","3bab8eb1":"#### Each file has 2 sub-folders, labeled 1 and 0\n1. Folder 0: Non-Invasive Ductal Carcinoma (IDC)\n2. Folder 1 : Invasive Ductal Carcinoma (IDC)","94f64f07":"## conclusion","f06e6020":"## A) import library and packages","fb26cf64":"### Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.","7dbc1404":"let's split our data into training and testing sets. We will split our data as follows:\n\n1. Training Data : Data that we will use to train our model\n2. Testing Data : Data that we will use to test our model\n3. Out of Sample Data : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.","42f79fcc":"Let's now visualize what some of the reconstructed tissue cells look like by using the function above.","8a4c4c50":"# Data Analysis","2e3bf319":"### A ) Cancerous Patches","e522f7bf":"<a id=\"1\"><\/a> <br>\n<font color='red'>\n# 1) data preparation \n","da2810a2":"# CHECK OUT ALL MY WORK [HERE](https:\/\/www.kaggle.com\/midouazerty\/code)\n# YOU CAN FIND BREAST CANCER CLASSIFICATION WITH 98% [HERE](https:\/\/www.kaggle.com\/midouazerty\/breast-cancer-prediction-99)\n\n## Inspiration\n\nAbstract\nBreast cancer is a common cancer in women, and one of the major causes of death among women around the world. Invasive ductal carcinoma (IDC) is the most widespread type of breast cancer with about 80% of all diagnosed cases. Early accurate diagnosis plays an important role in choosing the right treatment plan and improving survival rate among the patients. In recent years, efforts have been made to predict and detect all types of cancers by employing artificial intelligence. An appropriate dataset is the first essential step to achieve such a goal. This paper introduces a histopathological microscopy image dataset of 922 images related to 124 patients with IDC. The dataset has been published and is accessible through the web at: http:\/\/databiox.com. The distinctive feature of this dataset as compared to similar ones is that it contains an equal number of specimens from each of three grades of IDC, which leads to approximately 50 specimens for each grade.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/47\/Lobules_and_ducts_of_the_breast.jpg\/504px-Lobules_and_ducts_of_the_breast.jpg)\n\n## Content\nThe original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that, 277,524 patches of size 50 x 50 were extracted (198,738 IDC negative and 78,786 IDC positive). Each patch\u2019s file name is of the format: uxXyYclassC.png \u2014 > example 10253idx5x1351y1101class0.png . Where u is the patient ID (10253idx5), X is the x-coordinate of where this patch was cropped from, Y is the y-coordinate of where this patch was cropped from, and C indicates the class where 0 is non-IDC and 1 is IDC.\n\n## Introduction\n\nCancer is a serious public health issue worldwide and the second leading cause of death in the United States . According to the International Agency for Research on Cancer (IARC), about 18.1 million new cases and 9.6 million deaths caused by cancer were reported in 2018 .\n\nAs shown in Fig. 1, breast cancer is a common cancer and one of the major causes of death worldwide with 627,000 deaths among 2.1 million diagnosed cases in 2018 .\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S2352914820300757-gr1.jpg)\n","8d57f99c":"## Storing the image_path, patient_id, target and x & y coordinates","6ba87d09":"# What do we know about our data?\n\n1. Now that we have a good understanding of the file structure let's try and understand how much data we are about to process.\n\n2. How many patients do we have?\n\n    * It seems that we have a total number of 280 patients. \n    * This sample size is relatively small therefore we have to be careful not to overfit our model. We need to implement our model in such a way that it maximizes generalization.\n\n3. Each patient has a batch of patches that were extracted, therefore the total number of patches is likely much greater than 280.\n\n# NEXT STEP \n\n1. How many patches do we have in total?\n\n2. Which of them are IDC patches and which are Non-IDC?\n\n    * In order to train our model we need to feed our model each patch individually, therefore each patch will act as an input.\n\n    * The snippet below loops through the entire file structure and extracts the total number of crops for each of the classes.","10f7cefc":"### B) Non-Cancerous Patches","85f3e496":"### Visualizing the Breast Tissue\n\nEarlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue.\n\nWe can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the most common areas where the cancer appears.\n\nIf position of the crop has significance then perhaps we can use it as an input feature for our model.\n\nTo simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.","7c98b653":"# RGB PCA","30ed7d8b":"## B) Exploring the data","ccf2b311":"### Healthy Tissue Patches Vs Cancerous Tissue Patches\nLet us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. \nUsually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.","8b523a62":"###  Insights\n\n1. Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.\n2. Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.\n\n### Repatching the Actual Breast Tissue Image\n\nNow it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.\n\nuint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers."}}