{"cell_type":{"34d7b44a":"code","3afe5330":"code","d0304e1d":"code","9ea8f270":"code","4b34bdac":"code","666fd2cc":"code","c202cade":"code","aab8d9dc":"code","5404b15f":"code","781687a1":"code","f25426e3":"code","641e257d":"code","2154c2eb":"code","5163be30":"code","1e914c1c":"code","5c8449f2":"markdown"},"source":{"34d7b44a":"#Getting all the necessary imports\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom fastai import *\nfrom fastai.vision import *\nimport torch\nimport os","3afe5330":"#Creating the data path variable and initializing the transforms\ndata_folder = Path(\"..\/input\/flower_data\/flower_data\")\ntrfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=10.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75)","d0304e1d":"#creating the data loader\ndata = (ImageList.from_folder(data_folder)\n        .split_by_folder()\n        .label_from_folder()\n        .add_test_folder(\"..\/input\/test set\/test set\")\n        .transform(trfms, size=128)\n        .databunch(bs=64, device= torch.device('cuda:0'))\n        .normalize())","9ea8f270":"#Testing the data loader\ndata.show_batch(3, figsize=(6,6), hide_axis=False)","4b34bdac":"#defining the learner\nlearn = cnn_learner(data, models.densenet161, metrics=[error_rate, accuracy], model_dir = \"..\/..\/..\/working\") #Using densenet as discussed above","666fd2cc":"#finding the learning rate\nlearn.lr_find(stop_div=False, num_it=200)","c202cade":"#plotting loss against learning rate\nlearn.recorder.plot(suggestion = True)\nmin_grad_lr = learn.recorder.min_grad_lr","aab8d9dc":"#using the learning rate and starting the training\nlr = min_grad_lr\nlearn.fit_one_cycle(60, slice(lr)) #For final model, keep number of epochs = 60","5404b15f":"#Saving the model\nlearn.export(file = '..\/..\/..\/working\/export.pkl')","781687a1":"#Reloading the model into the memory and using it over test data\nlearn = load_learner(os.getcwd(), test=ImageList.from_folder('..\/input\/test set\/test set')) #pointing the learner towards the test data","f25426e3":"#Getting the labels from the JSON\nimport json\nwith open('..\/input\/cat_to_name.json') as f:\n  conversion_data = json.load(f)","641e257d":"#Creating a final list with file name, prediction category and the corresponding name\nfinal_result = []\nfor i in range (len(learn.data.test_ds)):\n    filename = str(learn.data.test_ds.items[i])[27:]\n    pred_category  = int(learn.predict(learn.data.test_ds[i][0])[1])\n    category_name = conversion_data[str(pred_category)]\n    final_result.append((filename, pred_category, category_name))","2154c2eb":"#Sorting the list alphabetically\nfinal_result = sorted(final_result,key=lambda x: x[0])","5163be30":"#Saving the Final Output to a CSV\nfinal_output = pd.DataFrame(final_result, columns=[\"Filename\", \"Predicted_Category\",\"Category_Name\"])\nfinal_output.to_csv('final_output.csv', index=False)","1e914c1c":"#Checking that the CSV is created properly\ntest_csv = pd.read_csv(\"final_output.csv\")\ntest_csv","5c8449f2":"Abhishek Lalwani\nSlack handle - @Abhishek Lalwani\nEmail ID - abhisheklalwani96@gmail.com\nAs the name of the Kernel suggests, with this Hackathon, I wanted to explore FastAI and the benefits which it proposes along with it's shortcomings.\n1. Model Selection\nGiven the ease of use FastAI provides, I decided to try out multiple pre-trained models (for a small number of epochs) before arriving at DenseNet as the clear winner in terms of the loss and the validation accuracy achieved. The other models which I tried out were Alexnet,Squeezenet and resnet152.\n2. Hyperparameter\nI used lr_find to find the optimal learning rate\n3. Transforms and Normalization\nI decided to try out the default transforms as well as some custom transforms better suited to this dataset such as random_resize_crop etc. Higher accuracy was achieved with default transforms. For Normalizattion purposes, I tried using imagenet_stats, but shifted to using batch values only for higher accuracy.\n\nSince the split of training and validation was already provided and due to shortage of time, I wasn't able to experiment with that.\nI initially also planned to submit a kernel which involved another approach quite similar to what was taught in our first lesson (Intro to Deep Learning with PyTorch) but due to shortage of time wasn't able to work on it."}}