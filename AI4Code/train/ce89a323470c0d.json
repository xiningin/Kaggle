{"cell_type":{"8d99eb22":"code","ee14388e":"code","10f0d637":"code","ce9b6c91":"code","f5c95705":"code","b61dd278":"code","01dcddd9":"code","d94e5a87":"code","59280af1":"code","e7c1871e":"code","d1a1c614":"code","488e061b":"code","a8ed398e":"code","142af271":"code","872a4dc6":"code","dbabcb12":"code","86f70b72":"code","85b90845":"code","96bf3084":"code","bae1a7e8":"code","db20b43a":"code","5d6b6bb6":"code","b2cfb6f4":"code","0e2cadf8":"code","4bd74cb2":"markdown","a2dc2d7d":"markdown","bee16bd0":"markdown","4dd94bed":"markdown","ced923cf":"markdown","1292db7f":"markdown","379f6644":"markdown","4dee59bb":"markdown"},"source":{"8d99eb22":"import pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nsns.set()\nimport pylab as plot","ee14388e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")","10f0d637":"train.head()","ce9b6c91":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","f5c95705":"train.describe().style.background_gradient(cmap=\"Pastel1\")","b61dd278":"target_count = train['target'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['target(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('target(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","01dcddd9":"colors_4 = ['magenta','yellow','orange','red','maroon','blue','purple','lime','chocolate','silver']\ntarget_count.plot.pie(subplots=True, figsize=(20,10), labels=target_count.index,autopct='%1.1f%%', colors=colors_4)\nplt.show()","d94e5a87":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nlabelencoder=LabelEncoder()\ntrain['target']     = labelencoder.fit_transform(train['target'])\nsubmission['target']     = labelencoder.fit_transform(submission['target'])","59280af1":"train.drop([\"row_id\"] , axis = 1 , inplace = True)\ny=train['target']\nX=train.drop(labels=['target'], axis=1)","e7c1871e":"test.head()","d1a1c614":"print(f'Number of rows: {test.shape[0]};  Number of columns: {test.shape[1]}; No of missing values: {sum(test.isna().sum())}')","488e061b":"test.describe().style.background_gradient(cmap=\"Pastel1\")","a8ed398e":"test.drop([\"row_id\"] , axis = 1 , inplace = True)\nx_test=test","142af271":"submission.head()","872a4dc6":"submission.drop([\"row_id\"] , axis = 1 , inplace = True)\ny_test=submission.target","dbabcb12":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","86f70b72":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nclf = RandomForestClassifier(n_estimators=13)\n#Cross Validation (K-fold)\n\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","85b90845":"# Random Forest Score\nround(np.mean(score)*100, 2)","96bf3084":"# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)","bae1a7e8":"# calculate manually\ndef my_function(y,y_preds):\n  \n  d = y - y_preds\n  mse_f = np.mean(d**2)\n  mae_f = np.mean(abs(d))\n  rmse_f = np.sqrt(mse_f)\n\n\n  print(\"Results by manual calculation:\")\n  print(\"MAE:\",mae_f)\n  print(\"MSE:\", mse_f)\n  print(\"RMSE:\", rmse_f)","db20b43a":"print('randomforset model')\ny_preds_Random = clf.predict(x_test )\nmy_function(y_test,y_preds_Random)","5d6b6bb6":"# evaluate extra trees algorithm for classification\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import ExtraTreesClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=4)\n# define the model\nmodel = ExtraTreesClassifier()\n# evaluate the model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","b2cfb6f4":"# make predictions using extra trees for classification\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# define the model\nmodel = ExtraTreesClassifier(n_estimators=100)\n# fit the model on the whole dataset\nmodel.fit(X_train, y_train)\n\n","0e2cadf8":"# make a prediction\n\ny_preds_Extra = model.predict(x_test )\nmy_function(y_test,y_preds_Extra)\n","4bd74cb2":"# 1. Overview\nScikit-Learn is packed with amazing algoritms which serves a wide variety of purposes. Needless to say one algoritm does not fit all datasets. Each algoritm comes with its pros and cons and our role is to keep experimenting and pick the algorithm which serve our needs. In this notebook, we will look at a special case of the famous Random Forest ensemble called Extremely Randomized Trees or Extra Trees and will see how can this ensemble become helpful in our projects. We will also compare it with the traditional Random Forest in terms of computational complexity. To get started lets first look at a basic Decision Tree.","a2dc2d7d":"# 2. Decision Tree\nAs the name suggest, Decision Tree is a tree with levels (or a depth), each level has nodes and each node takes a decision based on a certain threshold. Each node splits the dataset into two which helps in classifying any instance into categories and collectively into target classes. Look at the following figure for a better understanding","bee16bd0":"Here, the first node divides the dataset by sex, at the second level the dataset is divided by the features Age and Pclass and so on. The important thing to note here is that the threshold to divide the tree at a particular node is selected by the algorithm on its own. It calculates this threshold by analysing the feature and threshold that will give the least gini impurity. In simple terms, if we have a dataset which classifies students into pass or fail and we have only one feature i.e Marks, the decision tree will find the passing score(threshold) on its own and train a model. The next time we enter a student's marks, the algorithm classifies the student into pass or fail using the computed passing score.","4dd94bed":"# 3. Random Forest\u00b6\nGoing from Decision Tree to Random Forest is simple. Instead of a single tree on an empty land(let's say apple tree), imagine a forest with 1000s of trees(but not every tree is an apple tree). Each tree bear a slightly different fruit with different taste, colour, or altogether a different fruit. This variety usually makes the harvest of the forest much more high-yielding.\n\nThe Decision Tree takes the whole dataset and creates a tree. While Random Forest produces many trees but each tree is not shown the entire training dataset, it is shown only a part of the dataset and then it predicts the class or value. Later, all predicted values of all trees are cumulated to make the final prediction.\n\nThe random partial data is shown to each tree to bring diversity. Imagine if all the trees are shown the entire dataset, then each tree will bear the same identical apple fruit, which will add no additional value than just using a single Decision Tree algoritm. Look at the following image for better understanding","ced923cf":"![tree.png](https:\/\/miro.medium.com\/max\/700\/1*fGX0_gacojVa6-njlCrWZw.png)","1292db7f":"# Random Forest vs Extra Trees\u00b6\n# ","379f6644":"# 4. Extra Trees\nThe \"Random\" in Random Forest is bceause we are using a random subset of the dataset. But what if instead of choosing the best possible threshold for each tree at each node, we simply choose a random threshold too. If you remember the example taken in Decision Trees, imagine the algorithm is not bothered to compute the passing score in the first attempt, instead it takes a random marks value which divides the dataset(let that value be 90). Now at the first level, the students are divided into two categories (greater than and less than 90). If a student scores more than 90 he will be declared as pass, but if he scores less than 90, then the algorithm will find another random threshold (between 0 and 90) and categorize the remaining instances further. This algorithm too will eventually create an accurate model but it will have far more number of levels and nodes than a Decision Tree. However, the random nature of choosing the threshold value will make it much more faster.","4dee59bb":"![tr.png](https:\/\/miro.medium.com\/max\/574\/0*a8KgF1IINziv7KIQ.png)"}}