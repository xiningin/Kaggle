{"cell_type":{"558cbb5e":"code","101b2beb":"code","4fbe8e34":"code","3b1803b1":"code","3a4f7308":"code","fa6c1084":"markdown","a53dc19b":"markdown","52792873":"markdown","df507ac1":"markdown","c99992f6":"markdown"},"source":{"558cbb5e":"!pip install git+https:\/\/github.com\/ssut\/py-googletrans.git","101b2beb":"import os\nimport pandas as pd\nfrom googletrans import Translator","4fbe8e34":"text = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/' \\\n                       'jigsaw-toxic-comment-train.csv', \n                        nrows=10_000)\n\n\n# TODO: Get the proportion of languages in the test set and set a randomized language per comment with np.random.choice()\n\ntranslator = Translator()\nfor i,t in enumerate(text.comment_text[19:22]):\n    try:\n        encoded = translator.translate(t, dest='fr').text\n        decoded = translator.translate(encoded, dest='en').text\n        print(f\"\\nSet {i}\\n\"\n              f\"Original: {t}\\n\\n\"\n              f\"Recoded: {decoded}\\n\")\n    except: pass","3b1803b1":"import markovify as mk","3a4f7308":"doc = text.loc[text.toxic == 1, 'comment_text'].tolist()\ntext_model = mk.Text(doc)\nfor i in range(10):\n    print(text_model.make_sentence())\n","fa6c1084":"### Synthetic comments\n\nHere I'm using [Markovify](https:\/\/github.com\/jsvine\/markovify) to generate additional toxic commnents. This package uses Markov chains to string together new sequences of words based on previous sequences.","a53dc19b":"This notebook is currently a test site for augmenting data by looking at previous ideas and possibly trying new ones. Here are three ideas, one for adding translations, one for creating synthetic data, and one for interjecting noise\/variations.\n\n\n### Translations\n\nThis idea originally came from the first Toxic challenge in which translation was used as an encoder-decoder. Pavel Ostyakov's [A simple technique for extending dataset](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/48038) explains how translating an english comment to another language, and then back to english, can improve model accuracy. With this being a multilingual competition, there have been similar ideas implemented but maybe not in this exact way.\n\nIn this notebook I'm using Google Translator via the googletrans package.","52792873":"There's definiteiy more to explore here. Maybe these ideas can improve your model. Good luck!","df507ac1":"Only a few lines of code and you too can sound like an angry 5th grader! Sometimes this technique produces a bit of nonsense but the toxic keywords are in there. It may be a way to add toxic comments and get a more balanced dataset.","c99992f6":"### Various variations\n\nThe [nlpaug](https:\/\/github.com\/makcedward\/nlpaug) package contains a variety of augmentations to supplement text data and introduce noise that may help your model generalize. Here is a summary of a few functions:\n\n<img src=\"https:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/res\/textual_example.png?raw=true\" width=\"600\">\n\nThe package has many more augmentations -at the character, word, and sentence levels. \n\n - Character Augmenter\n    - OCR\n    - Keyboard\n    - Random\n - Word Augmenter\n    - Spelling\n    - Word Embeddings\n    - TF-IDF\n    - Contextual Word Embeddings\n    - Synonym\n    - Antonym\n    - Random Word\n    - Split\n - Sentence Augmenter\n    - Contextual Word Embeddings for Sentence"}}