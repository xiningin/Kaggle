{"cell_type":{"0ae6b01e":"code","25b7a4f4":"code","a80d8a54":"code","96d8d751":"code","35ce13a4":"code","50ff0743":"code","fc7719f3":"code","39767add":"code","8aaf3282":"code","e6b26dbf":"code","0186bc7e":"markdown","0902b18d":"markdown","9a55d44b":"markdown","2bbe3b3f":"markdown","b7492d75":"markdown","b8759cb1":"markdown","12c28268":"markdown","d1a96ab6":"markdown","9d3903ff":"markdown","9e4679d1":"markdown","1ca29304":"markdown","8465c61d":"markdown","34ab354d":"markdown","76e121e3":"markdown"},"source":{"0ae6b01e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.datasets import load_iris\n\nimport warnings\nwarnings.filterwarnings('ignore')","25b7a4f4":"class SvmVisualizer():\n    \"\"\"\n    Class to plot the behaviour of a SVM in a dataset. \n    \n    The code for this would not have been possible without the following resources:\n        - https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py\n        - https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.07-support-vector-machines.html\n        - https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_svm_kernels.html\n    \"\"\"\n    def __init__(self, X_train, y_train, X_test, y_test, \n                 n_svm_x_axis=1, n_svm_y_axis=1, figsize=(12, 9)):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.n_svm_x_axis = n_svm_x_axis\n        self.n_svm_y_axis = n_svm_y_axis\n        self.plt = plt.figure(figsize=figsize)\n        \n        # NOTE: we should do this based on the given data, now it works because we know\n        # we scaled the data\n        self.xx, self.yy = np.meshgrid(np.linspace(-3, 3, 200),\n                                       np.linspace(-3, 3, 200))\n        self.grid = np.c_[self.xx.ravel(), self.yy.ravel()]\n\n    def add_svm_plot(self, svm, title, k=0):\n        # visualize decision function for these parameters\n        plt.subplot(self.n_svm_x_axis, self.n_svm_y_axis, k + 1)\n        plt.title(title, size='medium')\n\n        # evaluate decision function in a grid\n        preds = svm.predict(self.grid).reshape(self.xx.shape)\n        \n        # visualize parameter's effect on decision function\n        plt.pcolormesh(self.xx, self.yy, preds, cmap=ListedColormap(['lightblue', 'lightcoral']), \n                       vmin=0, vmax=1)\n        plt.scatter(self.X_train[:, 0], self.X_train[:, 1], label=\"Train data\",\n                    edgecolors='k', c=self.y_train, cmap=ListedColormap(['blue', 'red']))\n        #plt.scatter(self.X_test[:, 0], self.X_test[:, 1], label=\"Test data\", edgecolors='k')\n        plt.xticks(())\n        plt.yticks(())\n        plt.tight_layout()\n\n        # plot the decision hyperplane and support vectors\n        Z = svm.decision_function(self.grid).reshape(self.xx.shape)\n        plt.contour(self.xx, self.yy, Z, colors='k', levels=[-1, 0, 1], \n                    alpha=0.5, linestyles=['--', '-', '--'])\n        plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=100,\n                    facecolors='none', zorder=10, edgecolors='k')","a80d8a54":"def load_iris_2d(keep=\"nonlinear_relationship\"):\n    \"\"\"\n    Returns the iris dataset.\n\n    2 features:\n      - Sepal length\n      - Sepal width\n\n    2 target classes:\n      If keep == \"nonlinear_relationship\":\n        - Iris versicolour as 0\n        - Iris virginica as 1\n      otherwise:\n        - Iris setosa as 0\n        - Iris versicolour as 1\n    \"\"\"\n    iris = load_iris()\n    X_2d = iris.data[:, :2]\n    y_2d = iris.target\n\n    if keep == \"nonlinear_relationship\":\n        X_2d = X_2d[y_2d > 0]\n        y_2d = y_2d[y_2d > 0]\n        y_2d -= 1\n    else:\n        X_2d = X_2d[y_2d < 2]\n        y_2d = y_2d[y_2d < 2]\n\n    return X_2d, y_2d","96d8d751":"# Load and split the dataset\nX_2d, y_2d = load_iris_2d(keep=\"linear_relationship\")\nX_train, X_test, y_train, y_test = train_test_split(X_2d, y_2d, test_size=0.3, \n                                                    random_state=8)\n\n\n# Scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Plot the data\nplt.figure(figsize=(8, 6))\nplt.scatter(X_train[y_train==0][:, 0], X_train[y_train==0][:, 1], color=\"b\", \n            label=\"Train iris setosa\", edgecolors='k', marker=\"D\")\nplt.scatter(X_test[y_test==0][:, 0], X_test[y_test==0][:, 1], color=\"b\", \n            label=\"Test iris setosa\", edgecolors='k', marker=\"o\")\nplt.scatter(X_train[y_train==1][:, 0], X_train[y_train==1][:, 1], color=\"r\", \n            label=\"Train iris versicolor\", edgecolors='k', marker=\"D\")\nplt.scatter(X_test[y_test==1][:, 0], X_test[y_test==1][:, 1], color=\"r\", \n            label=\"Test iris versicolor\", edgecolors='k', marker=\"o\")\nplt.legend()\nplt.title(\"Sepal length and width of setosa and versicolor iris flowers\")\nplt.xlabel(\"Z-score of sepal length\")\nplt.ylabel(\"Z-score of sepal width\");","35ce13a4":"C_range = [5e-3, 1e-2, 5e-2, 1e-1, 0.5, 1, 3, 5, 10]\nvisualizer = SvmVisualizer(X_train, y_train, X_test, y_test, 3, 3)\nk = 0\n\nfor C in C_range:\n    clf = svm.SVC(C=C, kernel=\"linear\", random_state=8)\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    title = f\"R2 score={score:.2f}. C={C}\"\n    visualizer.add_svm_plot(clf, title, k)\n    k += 1","50ff0743":"C_range = [1e-1, 1, 1e1]\ngamma_range = [1e-1, 1, 1e1]\nvisualizer = SvmVisualizer(X_train, y_train, X_test, y_test, 3, 3)\nk = 0\n\nfor C in C_range:\n    for gamma in gamma_range:\n        clf = svm.SVC(C=C, gamma=gamma, kernel=\"rbf\", random_state=8)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        title = f\"R2 score={score:.2f}. C={C}, gamma={gamma}\"\n        visualizer.add_svm_plot(clf, title, k)\n        k += 1","fc7719f3":"C_range = [1]\ngamma_range = [1e-1, 1, 1e1]\ncoef0_range = [-50, -1, 0, 1, 50]\nvisualizer = SvmVisualizer(X_train, y_train, X_test, y_test, 3, 5, figsize=(16, 10))\nk = 0\n\nfor C in C_range:\n    for gamma in gamma_range:\n        for coef0 in coef0_range:\n            clf = svm.SVC(C=C, gamma=gamma, coef0=coef0, kernel=\"sigmoid\", random_state=8)\n            clf.fit(X_train, y_train)\n            score = clf.score(X_test, y_test)\n            title = f\"R2 score={score:.2f}. C={C},\\n gamma={gamma}, r={coef0}\"\n            visualizer.add_svm_plot(clf, title, k)\n            k += 1","39767add":"x = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\nfig, axs = plt.subplots(3, 1, figsize=(12, 6))\n\naxs[0].plot(x, np.tanh(x - 2))\naxs[0].axvline(2, c=\"coral\")\naxs[0].set_title(\"tanh(z - 2)\")\n\naxs[1].plot(x, np.tanh(x))\naxs[1].axvline(0, c=\"coral\")\naxs[1].set_title(\"tanh(z)\")\n\naxs[2].plot(x, np.tanh(x + 2))\naxs[2].axvline(-2, c=\"coral\")\naxs[2].set_title(\"tanh(z + 2)\")\n\nplt.tight_layout()","8aaf3282":"C_range = [1]\ngamma_range = [0.1, 1]\ndegree_range = [2, 3]\ncoef0_range = [-50, -1, 0, 1, 50]\nvisualizer = SvmVisualizer(X_train, y_train, X_test, y_test, 4, 5, figsize=(20, 16))\nk = 0\n\nfor C in C_range:\n    for degree in degree_range:\n        for gamma in gamma_range:\n            for coef0 in coef0_range:\n                clf = svm.SVC(C=C, gamma=gamma, degree=degree, coef0=coef0, kernel=\"poly\", random_state=8)\n                clf.fit(X_train, y_train)\n                score = clf.score(X_test, y_test)\n                title = f\"R2 score={score:.2f}. C={C}, gamma={gamma},\\n d={degree}, r={coef0}\"\n                visualizer.add_svm_plot(clf, title, k)\n                k += 1","e6b26dbf":"from IPython.display import IFrame\nIFrame(\"https:\/\/dash-gallery.plotly.host\/dash-svm\/\", height=500, width=\"100%\")","0186bc7e":"__Higher positive and negative values for the $r$ constant dominates the result, it makes it harder for the scalar product to have an impact on the result of the function.__\n\nIf we interpret the scalar product as a measure of similarity between two vectors, kernel functions also represent similarities. So we could interpret this effect of $r$ as biasing the result, which can be useful if we know that some class is prefered over the other?","0902b18d":"Now the $C$ hyperparameter has been fixed because as shown above it affects the margin. From top to bottom can be seen __the effect of varying $\\gamma$, which helps making separations more complex as it increases__, just like with the RBF kernel function.\n\nThe $r$ hyperparameter doesn't have a very clear visual effect with small changes. However, looking at the function definition, $K(\\vec{x_i}, \\vec{x_j}) = { tanh(\\gamma  \\vec{x_i} \\cdot \\vec{x_j} + r) }$, __the effect of this constant $r$ is to shift the hyperbolic tangent__.","9a55d44b":"## Table of Contents\n1. [Introduction to SVMs](#Introduction)\n2. [Dataset](#Dataset)\n3. [Popular kernel functions](#Popular-kernels)\n    1. [Hyperparameters of Linear Kernel Function](#Linear)\n    2. [Hyperparameters of RBF Kernel Function](#RBF)\n    3. [Hyperparameters of Sigmoid Kernel Function](#Sigmoid)\n    4. [Hyperparameters of Polynomial Kernel Function](#Polynomial)\n4. [Conclusions](#Conclusions)\n5. [Acknowledgments](#Acknowledgments)","2bbe3b3f":"<a id=\"Popular-kernels\"><\/a>\n## Popular kernel functions\nThis section introduces the four most common kernel functions and a visualization with the effect that each hyperparameter has on the result of the SVM.\n\n<a id=\"Linear\"><\/a>\n### Linear kernel function\n$K(\\vec{x_i}, \\vec{x_j}) = \\vec{x_i} \\cdot \\vec{x_j}$\n\nThis is the simplest kernel function because it is equivalent to not use any kernel function. In other words, it directly computes the scalar product between the inputs. It does not add any extra hyperparameters to the SVM and it is perfect to see the effect of the hyperparameter $C$ that regulates the margin.\n\nNext plots shows the result of training the SVM with a linear kernel on the training dataset","b7492d75":"The background color represents the decision of the SVM. The training dataset is represented as points in the plane and their class is represented also with color. Highlighted points represent the Support Vectors, that is, the data points that define the hyperplane. Dash-lines represent the margin of the SVM. And above each plot you can find the R2 score of that SVM on the validation dataset and the value of the hyperparameter used.\n\nAs seen in the plots, __the effect of incrementing the hyperparameter $C$ is to make the margin tighter__ and, thus, less Support Vectors are needed to define the hyperplane.","b8759cb1":"<a id=\"Sigmoide\"><\/a>\n### Sigmoid Kernel Function\n$K(\\vec{x_i}, \\vec{x_j}) = { tanh(\\gamma  \\vec{x_i} \\cdot \\vec{x_j} + r) }$\n\nIn this function there are two extra hyperparameters: $r$ and another one also called $\\gamma$ that, as it can be seen below, it also affects the complexity of the separation.","12c28268":"<a id=\"Dataset\"><\/a>\n## Dataset\nThe dataset is a modification of the iris dataset to have just two dimensions so it can be easily visualized. In particular, it consists of sepal length and width for setosa and versicolor iris flowers.\n\nThe next figure shows how this dataset was divided in training and validation sets. Furthermore, the data has been preprocessed to have a mean of 0 and unit variance. This is a good practise because __SVMs use scalar products, so if a variable has a larger range of values than the rest, it will dominate the result of the scalar product__.","d1a96ab6":"# Visualizing the effect of hyperparameters and kernel functions on Support Vector Machines (SVMs)\n\nThe goal of this notebook is to visually see the effect of each hyperparameter and kernel function of a SVM to understand their effects on the models.\n\nFirst a brief introduction to SVMs is presented followed by the dataset. For the purpose of this project, the dataset is a simple one with just two features to easily visualize it in two dimensions. The dataset will be divided into a training and validation dataset. And, for each kernel function, a visualization is presented showing the effects of different hyperparameter.\n\n![](https:\/\/images.pexels.com\/photos\/5835359\/pexels-photo-5835359.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n<p style=\"text-align: center\">Photo by <a href=\"https:\/\/www.pexels.com\/@tim-samuel?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels\">Tim Samuel<\/a> from <a href=\"https:\/\/www.pexels.com\/photo\/ethnic-male-drivers-coping-with-trouble-in-car-5835359\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels\">Pexels<\/a><\/p>","9d3903ff":"From top to bottom can be seen that __the effect of incrementing the degree of the polynomial and $\\gamma$ is to make more complex separations__.\n\nFrom left to right can be seen that __the effect of varying $r$ looks similar to what happens with the $r$ in the sigmoid function, at least for lower degrees of the polynomial__\n\n<a href=\"#Conclusions\"><\/a>\n## Conclusions\nThis notebook started with a brief introduction to SVMs, followed by the dataset used. Then, it showed the visual effect, in the dimension of the original data, of varying the hyperparameters of the most common kernel functions. These effects could be classified in two categories following the visual intuition: either they affect the margin of the SVM and\/or they affect the non-linearity of the separation.\n\nIt is commonly advised to use the RBF kernel as a starting kernel function as it is one with just two hyperparameters to tune and it can model non-linear separations. But, as many things, this might not be the perfect kernel function for any task, depending on the data there could be a kernel function that models better the separation between the classes. Hopefully this notebook gives the readers an intuition to help choose the kernel function to use and\/or better understand SVMs \ud83d\ude42\n\n<a href=\"Acknowledgments\"><\/a>\n## Acknowledgments\nSpecial thanks to [Angel Igareta](https:\/\/www.linkedin.com\/in\/angeligareta\/) for reviewing this notebook before publishing it \ud83d\ude0a\n\n---\n\nThank you for reading this notebook! \ud83d\ude04 Feel free to leave any comment below, any feedback will be greatly appreciated \ud83d\ude04\n\nHave a nice day! \ud83d\ude42","9e4679d1":"<a id=\"Polynomial\"><\/a>\n### Polynomial Kernel Function\n$K(\\vec{x_i}, \\vec{x_j}) = { (\\gamma  \\vec{x_i} \\cdot \\vec{x_j} + r)^d }$\n\nThis last kernel function has three hyperparameters: a factor $\\gamma$ that affects the scalar product, a constant $r$ and the degree of the polynomial, $d$.","1ca29304":"From top to bottom we can see the effect of incrementing the hyperparameter $C$, which affects the margin of the SVM as seen previously.\n\nOn the other hand, from left to right, we can see the effect of incrementing __$\\gamma$: lower values result in separations that look more linear and, as $\\gamma$ increases, it results in more complex separations__.\n\nWith a sufficiently high $\\gamma$, every observation of the training set are Support Vectors. In other words, every training point is used to define the hyperplane, which indicates a clear overfitting. Moreover, the more support vectors used, the more computationally expensive is the SVM and the more time it needs to make a prediction.\n\nFinally, it might be interesting to note that, looking at the separations in the original dimension of the data, it looks like a gaussian, just like the kernel function looks like a gaussian","8465c61d":"<a id=\"Introduction\"><\/a>\n## Introduction to SVMs\nSupport Vector Machines are supervised Machine Learning models used for classification (or regression) tasks. In the case of binary classification, there is a dataset made of $n$ observations, each observation made of a vector $x_i$ of $d$ dimensions and a target variable $y_i$ which can be either $-1$ or $1$ depending on whether the observation belongs to one class or the other.\n\nUsing this data, a SVM learns the <font color=\"darkblue\">parameters<\/font> of a hyperplane, $\\color{darkblue}{w} \\cdot x - \\color{darkblue}{b} = 0$ that separate the space in two parts: one for the observations of one class and the other part of the space for the observations of the other class. Furthermore, among all possible hyperparameters that separate both classes, a SVM learns the one that separates them the most, that is, leaving as much distance as possible between each class and the hyperplane\n\n![](https:\/\/camo.githubusercontent.com\/71c3e4d0f1c7c8b3ea2fabade4e67323da6b4165\/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f372f37322f53564d5f6d617267696e2e706e672f35313270782d53564d5f6d617267696e2e706e67)\n<p style=\"text-align: center;\">\n    Maximum-margin hyperplane and margin for an SVM trained on two classes. Samples on margins are called support vectors because they are the only samples needed to define the hyperplane\n    <br\/> \n    Larhmam, CC BY-SA 4.0 &lt;<a href=\"https:\/\/creativecommons.org\/licenses\/by-sa\/4.0\">https:\/\/creativecommons.org\/licenses\/by-sa\/4.0<\/a>&gt;, via Wikimedia Commons\n<\/p>\n\n\nTo learn the parameters of the hyperplane, a SVM tries to maximize the margin between the observations, with the additional constraint that observations of different classes have to be on different sides of the hyperplane.\n\nThis notebook will not get into much detail about the math behind how SVMs get these parameters. However, to provide some intuition that may be useful, it is interesting to note that __training data, $x$, will only appear as part of scalar products when computing the parameters of the hyperplane__.\n\nThanks to this observation, SVMs can get the advantage of what is called a kernel function. This is a function that returns the same thing as the scalar product between two vectors but without needing to calculate the coordinates of those vectors. And this is very useful because we can simulate applying operations that increase the dimensionality of $x$ without the need of going to higher dimensions which is computationally cheaper (this is called the _kernel trick_ ).\n\nThanks to this, SVMs can work for data that is not linearly separable. Instead of trying to fit some complicated function to the data, __a SVM goes to higher dimensions where the data could be linearly separable and finds the hyperplane to separate the data there__ (and back in the original dimension, it won't look like a linear separation).\n\nAnd now is the time to say that, because of this, SVMs do not calculate the parameters of the hyperplane, instead they remember the $x$'s that they need to calculate the hyperplane and, when a new input data comes, SVMs performs the scalar products between these $x$'s, called Support Vectors, and the input data. Again, this is to keep using scalar products and take advantage of the kernel functions.\n\nAnother interesting thing to note is that it is common to not find a perfect separation between two classes. And this is bad for SVMs, because during training (calculating the hyperplane parameters) they try to maximize the margin between classes with the constraint that observations of one class must be on one side of the hyperplane and observations of the other class, on the other side. To help with this, SVMs have \"soft margins\", which is a parameter that relaxes this by regulating how many observations are allowed to pass the margin.","34ab354d":"<a id=\"RBF\"><\/a>\n### RBF Kernel Function (Radial Basis Function)\n$K(\\vec{x_i}, \\vec{x_j}) = e^{ -\\gamma  {\\left\\lVert (\\vec{x_i} - \\vec{x_j}) \\right\\rVert}^2 }$\n\nThis function adds an extra hyperparameter to tune, $\\gamma$. But, unlike in the case of the linear kernel function, this function does map the data to a higher dimension. And, as it can be seen in the plots below, now the SVM can represent a non-linear separation.","76e121e3":"---\nEdit (2021\/02\/10): I just found [this amazing dashboard on Dash gallery were we can play around with different SVMs hyperparameters](https:\/\/dash-gallery.plotly.host\/dash-svm\/) and it shows a similar visualization than the ones above. It can be very useful if you want to see these effects by yourself \ud83d\ude42"}}