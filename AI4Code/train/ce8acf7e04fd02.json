{"cell_type":{"ad8038ef":"code","6847120b":"code","541a2304":"code","774d3850":"code","683a9e19":"code","dc1852f0":"code","d985c858":"code","c4fc092c":"code","fba9b62b":"code","45ad1c94":"code","f382d7e0":"code","63f2cecc":"code","81879e9b":"code","0c50b31d":"code","201f7ccb":"code","21e674a2":"code","f758d220":"code","c0fe14ec":"code","221174e7":"code","8daba274":"code","12bda75c":"code","7b72b12f":"code","07c4e251":"code","cc1d2279":"code","3b5fb99b":"markdown","9cec5556":"markdown","7e889c22":"markdown","224d3bc0":"markdown","06d4ea25":"markdown","fb7e63a7":"markdown","ffd6cd30":"markdown","f85d88e0":"markdown","71195e65":"markdown"},"source":{"ad8038ef":"#Imports\n\n#Computing\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#Clustering\nfrom sklearn.cluster import KMeans,SpectralClustering\nfrom yellowbrick.cluster import KElbowVisualizer\n\n#Decomposition\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n#Text\nimport unicodedata, re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\n\n#Default\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6847120b":"#Read the dataset\ndf=pd.read_csv('\/kaggle\/input\/anime-dataset\/anime.csv')\ndf.head()","541a2304":"#Some basic information\ndf.info()","774d3850":"animes=df[['title','description']].dropna()\nanimes.head()\n#animes.count()","683a9e19":"\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef remove_len_2(words):\n    \"\"\"Remove all the words with len <= 2\"\"\"\n    new_words = []\n    for word in words:\n        if len(word)<=2:\n            pass\n        else:\n            new_words.append(word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = remove_len_2(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n    words = remove_stopwords(words)\n    return words","dc1852f0":"#Tokenize text\nanimes['Tokenized']=animes['description'].apply(nltk.word_tokenize)\nanimes.head()","d985c858":"#Normalize text\nanimes['Clean_text']=animes['Tokenized'].apply(lambda y: normalize(y))\nanimes['Clean_text'][:10]","c4fc092c":"\"\"\"Function to reconvert a tokenization into a single string\nThis is needed for the TfIdfVectorizer method \"\"\"\ndef conv2str(y):  \n     \n    str1 = \" \"   \n    return (str1.join(y)) ","fba9b62b":"animes['Clean_text1']=animes['Clean_text'].apply(lambda y: conv2str(y))\nanimes.head()","45ad1c94":"\"\"\"Apply the TF_idf vectorizer to get the sparse matrix of the TF_IDF process\"\"\"\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(animes['Clean_text1'])\n","f382d7e0":"#First row and the element extracted are the same.\n\nprint(X)\nprint(X.toarray()[0][20871])","63f2cecc":"\"\"\"A simple view of the feature names\"\"\"\nprint(vectorizer.get_feature_names()[:10])\n","81879e9b":"\"\"\"First cluster using KMeans and run the elbow visualizer to find the best number of clusters\"\"\"\nmodelKm = KMeans(random_state=12)\nvisualizer = KElbowVisualizer(modelKm, k=(1,12))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show() ","0c50b31d":"\"\"\"First cluster using Spectral and run the elbow visualizer to find the best number of clusters\"\"\"\nmodelSc = SpectralClustering(random_state=5)\nvisualizer = KElbowVisualizer(modelSc, k=(1,12))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show() ","201f7ccb":"\"\"\"Train the Kmeans with the best n of clusters\"\"\"\nmodelKm = KMeans(n_clusters=4,random_state=12)\nmodelKm.fit(X)\ny_kmeans = modelKm.predict(X)\n\n\"\"\"Dimensionality reduction used to plot in 2d representation\"\"\"\npc=TruncatedSVD(n_components=2)\nX_new=pc.fit_transform(X)\ncentr=pc.transform(modelKm.cluster_centers_)\n\nprint(centr)\nplt.scatter(X_new[:,0],X_new[:,1],c=y_kmeans, cmap='viridis')\nplt.scatter(centr[:,0],centr[:,1],marker='X',alpha=0.5,color='red',s=1000)","21e674a2":"modelSc = SpectralClustering(n_clusters=4, random_state=5)\ny_spc=modelSc.fit_predict(X)\n\n\npc=TruncatedSVD(n_components=2)\nX_new=pc.fit_transform(X)\n\nplt.scatter(X_new[:,0],X_new[:,1],c=y_spc, cmap='viridis')\n","f758d220":"#Rebuild the clusters in pandas df.\n\nanimes['ClusterKmeans']=y_kmeans\nanimes['ClusterSpectral']=y_spc\nanimes.head()","c0fe14ec":"#Extract text based on cluster \nclus0_text=animes[animes['ClusterKmeans']==0]\nclus1_text=animes[animes['ClusterKmeans']==1]\nclus2_text=animes[animes['ClusterKmeans']==2]\nclus3_text=animes[animes['ClusterKmeans']==3]","221174e7":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus0_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","8daba274":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus1_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","12bda75c":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus2_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","7b72b12f":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus3_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","07c4e251":"#Extract text based on cluster \nclus0_text_sp=animes[animes['ClusterSpectral']==0]\nclus1_text_sp=animes[animes['ClusterSpectral']==1]\nclus2_text_sp=animes[animes['ClusterSpectral']==2]\nclus3_text_sp=animes[animes['ClusterSpectral']==3]\n","cc1d2279":"plt.figure(figsize=(12,8))\nplt.subplot(221)\nplt.title('Cluster1')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus0_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(222)\nplt.title('Cluster2')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus1_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(223)\nplt.title('Cluster3')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus2_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(224)\nplt.title('Cluster4')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus3_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\n\nplt.show()","3b5fb99b":"## Goal : Clusterize animes.\n\nWhat I want is try to clusters anime by content, in this case I will only consider the description of the series as the only information that I have.\nThe purpouse of this notebook is to sharpen my text analisys and clustering skills.\n\n","9cec5556":"## With the spectral clustering more or less we found the same clusters:\n","7e889c22":"### For both clustering algorithm the result seems similar from a graphical point of view. But let's inspect with wordcloud the most important words of each cluster.","224d3bc0":"## More or less the result of the two clustering algorithms is the same, based on the tf-idf value of the text contained in 'Description'.\n","06d4ea25":"### As seen by the info method, a lot of animes have a null description. For the purpouse of this notebook, I will discard all the element that have a null description and I will focus on the not-null description animes.","fb7e63a7":"We are left with 8173 records in the dataset.\nSince this is a text clustering notebook, the first thing I'm going to do is to clean text. \n","ffd6cd30":"Function to clean the text, Taken  by : https:\/\/www.kaggle.com\/oragula\/sentiment-analysis-rotten-tomato-movie-reviews","f85d88e0":"### Inspecting the worldcloud view, is it possible to see that we have 4 clusters based on these words:\n(The order of the clusters can be different)\n\n* Cluster 1: Based on episodes, dungeon, recap episode, first season -> This cluster is kinda confusing me.\n* Cluster 2: Clearly a cluster based on school anime. -> School anime.\n* Cluster 3: Family, life, human, ... -> Thematics connected to this.\n* Cluster 4: Based on season? -> Maybe it is a cluster where all the second,third,.. seasons are.\n\nLet's see if with the spectral cluster we are more lucky.\n","71195e65":"X is a sparse matrix, that means it can be used as training and it can be used as a matrix.\nBut the representation as a sparse matrix is really optimal because a lot of elements of the matrix are 0.\n\n* (0,20871) means that in position (0,20871) we have the value 0.09422377859075083\n* And so on..."}}