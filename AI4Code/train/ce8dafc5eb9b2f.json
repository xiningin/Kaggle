{"cell_type":{"b62cb8c1":"code","dbd6ee0e":"code","4b3c47d7":"code","e773bdd4":"code","b33af4a1":"code","c31cd59e":"code","bc6d0fae":"code","f1103264":"code","2932a1bb":"code","ce42140a":"code","f963f2ec":"code","f5141189":"code","6b64eb33":"code","75b55cd1":"code","7c75e0e6":"code","2dec40e3":"code","0bb9313c":"code","be9a8e5c":"code","7d208ade":"code","2fc27241":"code","f70834ef":"code","cbc82551":"code","c5a4ae23":"code","f884dea2":"code","3e24825a":"code","4468de2e":"code","d3737c41":"code","05453970":"code","0f20bba1":"code","59ccd3bd":"code","6e0baf67":"code","386cbbcd":"code","e1b30559":"code","ab467622":"code","6b633e67":"code","768bde77":"code","966a04aa":"code","bff564f0":"code","9b6d0c72":"code","f0594867":"code","a7e67b74":"code","27760606":"code","01438025":"code","5feb0119":"code","210c21dd":"code","a9fd3263":"code","f25f8458":"code","874f5a12":"code","874f5317":"code","061bea58":"code","a9f99241":"code","fc8f10b5":"code","4aeb37a7":"code","3e9b71e1":"code","bdfb5425":"markdown","0d8c44ed":"markdown","37c007d3":"markdown","fd4e6d55":"markdown","c39d1270":"markdown","6d30aa87":"markdown","a2dcf6d9":"markdown","701d7fd7":"markdown","d053fafa":"markdown","4d063c1f":"markdown","e115e576":"markdown","18cf15a9":"markdown","fdcdb8d1":"markdown","116bdac9":"markdown","0b956b4f":"markdown","4d2fa6e5":"markdown","ff2b0914":"markdown","64adf262":"markdown","4436fd8c":"markdown","fde35712":"markdown","e30f3148":"markdown","c5cba8c5":"markdown","d2a99a3e":"markdown","3c962b0b":"markdown","24dc6a36":"markdown","6f8aa0f5":"markdown","ba62d2a5":"markdown","2beda61b":"markdown","c16fa755":"markdown","39fe70ff":"markdown","84eae999":"markdown","be1a928f":"markdown","ffb0f94b":"markdown","5fa8b88f":"markdown","45330990":"markdown","9b02e832":"markdown","75c7fc4d":"markdown","eddc8a61":"markdown","f3432450":"markdown","034c227e":"markdown","52f2b25a":"markdown"},"source":{"b62cb8c1":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dbd6ee0e":"# Reading the data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","4b3c47d7":"# Lets take a look at the first few rows of the dataset\ntrain_df.head()","e773bdd4":"# Lets look at the shape\ntrain_df.shape","b33af4a1":"# Setting the grid style\nsns.set_style('darkgrid')\nsns.set_color_codes(palette='dark')\n\n# Setting plot area\nf, ax = plt.subplots(figsize=(9, 9))\n\n# plotting the distribution plot\nsns.distplot(train_df['SalePrice'], color=\"m\", axlabel='SalePrice')\nax.set(title=\"Histogram for SalePrice\")\nplt.show()","c31cd59e":"# Calc correlation matrix\ncorr_mat = train_df.corr()\n\n# Set plot size\nplt.subplots(figsize=(12,10))\n\n# Plot heatmap\nsns.heatmap(corr_mat, \n            square=True, \n            robust=True, \n            cmap='OrRd', # use orange\/red colour map\n            cbar_kws={'fraction' : 0.01}, # shrink colour bar\n            linewidth=1) # space between cells","bc6d0fae":"# number of variables we want on the heatmap\nk = 10 \n\n# Filter in the Top k variables with highest correlation with SalePrice\ncols = corr_mat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\n\ncmap_ch = sns.cubehelix_palette(as_cmap=True, light=.95)\n# Creating the heatmap\nhm = sns.heatmap(cm,\n                 cmap = cmap_ch,\n                 cbar = True, \n                 annot = True, # Since we want to know the the correlation coeff as well.\n                 square = True, \n                 robust = True,\n                 cbar_kws={'fraction' : 0.01}, # shrink colour bar\n                 annot_kws={'size': 8}, # setting label size\n                 yticklabels=cols.values, # set y labels\n                 xticklabels=cols.values,\n                 linewidth=1) # Set xlabels\nplt.show()","f1103264":"# Creating a boxplot\nchart = sns.catplot(data = train_df ,\n                    x = 'OverallQual',\n                    y='SalePrice',\n                    kind='box',\n                    height=8,\n                    palette='Set2')\n\n# Setting X axis labels\nchart.set_xticklabels(fontweight='light',fontsize='large')","2932a1bb":"# Creating a boxplot\nchart = sns.catplot(data = train_df ,x = 'YearBuilt',y='SalePrice',\n                    kind='box', # we want a boxplot\n                    height=5,\n                    aspect=4,\n                    palette='Set2')\n\n# Setting X axis labels\nchart.set_xticklabels(fontweight='light',\n                      fontsize='large',\n                      rotation=90,\n                      horizontalalignment='center')","ce42140a":"plt.figure(figsize=(10,8))\n\n# Creating a scatterplot\nsns.scatterplot(data = train_df ,\n                x = 'TotalBsmtSF',\n                y ='SalePrice',\n                alpha = 0.65,\n                color = 'g') ","f963f2ec":"plt.figure(figsize=(10,8))\n\n# Creating a scatterplot\nsns.scatterplot(data = train_df ,\n                x = 'GrLivArea',\n                y ='SalePrice',\n                alpha = 0.65,\n                color = 'b') ","f5141189":"# Creating a boxplot\nchart = sns.catplot(data = train_df ,\n                    x = 'GarageCars',\n                    y='SalePrice',\n                    kind='box',\n                    height=6,\n                    palette='Set2')\n\n# Setting X axis labels\nchart.set_xticklabels(fontweight='light',fontsize='large')","6b64eb33":"# Storing the IDs in a separate DF\ntrain_df_IDs = train_df['Id']\ntest_df_IDs = test_df['Id']\n\n# Dropping the columns\ntrain_df.drop(['Id'], axis=1, inplace=True)\ntest_df.drop(['Id'], axis=1, inplace=True)\n\n# Checking the shape of both DFs\nprint(train_df.shape) \nprint(test_df.shape)","75b55cd1":"# Log transforming SalePrice\ntrain_df[\"SalePrice_log\"] = np.log(train_df[\"SalePrice\"])\n\n# Plotting to vizualize the transformed variable\nsns.distplot(train_df['SalePrice_log'], color=\"m\", axlabel='SalePrice_log')","7c75e0e6":"# Dropping SalePrice_log as we will clean it in the next steps with pipeline\ntrain_df = train_df.drop('SalePrice_log',axis=1)","2dec40e3":"# Dropping the outliers\ntrain_df.drop(train_df[(train_df['GrLivArea']>4500) & (train_df['SalePrice']<300000)].index, inplace=True)\n\n# Vizualizing the new scatterplot\nplt.figure(figsize=(10,8))\n\n# Creating a scatterplot\nsns.scatterplot(data = train_df ,\n                x = 'GrLivArea',\n                y ='SalePrice',\n                alpha = 0.65,\n                color = 'b') ","0bb9313c":"# Separating Predictor and Labels\nhousing = train_df.drop(\"SalePrice\",axis=1)\nhousing_labels = train_df['SalePrice']","be9a8e5c":"# calc total missing values\ntotal_series = housing.isnull().sum().sort_values(ascending=False)\n\n# calc percentages\nperc_series = (housing.isnull().sum()\/housing.isnull().count()).sort_values(ascending = False)\n\n# concatenating total values and percentages\nmissing_data = pd.concat([total_series, perc_series*100], axis=1, keys=['Total #', 'Percent'])\n\n# Looking at top 20 entries\nmissing_data.head(20)","7d208ade":"# converting numeric columns to categorical\ncols_int_to_str = ['MSSubClass','YrSold','MoSold','GarageYrBlt']\n\nfor col in cols_int_to_str:\n    housing[col] = housing[col].astype(str)\n    test_df[col] = test_df[col].astype(str)","2fc27241":"# Creating a list of numerics we want for the mask\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\n# Creating a dataframe for numeric features\nhousing_num = housing.select_dtypes(include=numerics)\nprint(housing_num.shape)","f70834ef":"# List of Categorical features that are to be filled with None\ncat_none = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond',\\\n            'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n# Creating a dataframe of features for \"none\"\nhousing_cat_none = housing[cat_none]\n\n# All the other categorical features\nhousing_cat_freq = housing[(housing.columns.difference(cat_none)) & (housing.columns.difference(housing_num.columns))]\n","cbc82551":"# Importing the modules\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# getting index of relevant columns instead of hardcoding\nBsmtFinSF1_ix, BsmtFinSF2_ix, flr_1_ix, flr_2_ix,\\\nFullBath_ix, HalfBath_ix, BsmtFullBath_ix, BsmtHalfBath_ix,\\\nOpenPorchSF_ix, SsnPorch_ix, EnclosedPorch_ix, ScreenPorch_ix, WoodDeckSF_ix = [\n    list(housing_num.columns).index(col)\n    for col in (\"BsmtFinSF1\", \"BsmtFinSF2\",\"1stFlrSF\",\"2ndFlrSF\",\\\n                \"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\",\\\n                \"OpenPorchSF\",\"3SsnPorch\",\"EnclosedPorch\",\"ScreenPorch\",\"WoodDeckSF\")]\n\n# Creating CombinedAttributesAdder class for creating the features\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, Total_sqr_footage = True, Total_Bathrooms=True,Total_porch_sf=True): \n        self.Total_sqr_footage = Total_sqr_footage\n        self.Total_Bathrooms = Total_Bathrooms\n        self.Total_porch_sf = Total_porch_sf\n        \n    def fit(self, X, y=None):\n        return self \n    \n    def transform(self, X, y=None):\n        if self.Total_sqr_footage: # Calculate total footage\n            Total_sqr_footage = X[:, BsmtFinSF1_ix] + X[:, BsmtFinSF2_ix] + X[:,flr_1_ix] + X[:,flr_2_ix]\n       \n        if self.Total_Bathrooms: # Calculate total bathrooms\n            Total_Bathrooms = X[:, FullBath_ix] + X[:, HalfBath_ix] + X[:,BsmtFullBath_ix] + X[:,BsmtHalfBath_ix]\n            \n        if self.Total_porch_sf: # Calculate total porch area\n            Total_porch_sf = X[:, OpenPorchSF_ix] + X[:, SsnPorch_ix] + X[:,EnclosedPorch_ix] + X[:,ScreenPorch_ix] + X[:,WoodDeckSF_ix]\n            \n        return np.c_[X, Total_sqr_footage,Total_Bathrooms,Total_porch_sf]\n    ","c5a4ae23":"# Importing necessary libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Creating numerical pipeline\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"mean\")), # To impute na values with mean \n        ('attribs_adder', CombinedAttributesAdder()), # Create new features\n        ('std_scaler', StandardScaler()), # Scale the numeric features\n    ])","f884dea2":"# Importing necessary libraries\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Creating pipeline for categorical variables with missing value should be \"None\"\ncat_pipeline_none = Pipeline([\n        ('imputer', SimpleImputer(strategy='constant',fill_value='None')),\n        ('encoder', OneHotEncoder(sparse=False,handle_unknown='ignore'))\n    ])\n\n# Creating pipeline for categorical variables where we plug missing value with most frequent value\ncat_pipeline_freq = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(sparse=False,handle_unknown='ignore'))\n    ])","3e24825a":"# Importing ColumnTransformer\nfrom sklearn.compose import ColumnTransformer\n\n# Creating full pipeline to process numeric and categorical features\nfull_pipeline = ColumnTransformer(transformers=[\n        (\"num\", num_pipeline, housing_num.columns),\n        (\"cat_none\", cat_pipeline_none, housing_cat_none.columns),\n        (\"cat_freq\", cat_pipeline_freq, housing_cat_freq.columns),\n    ])\n\n# Instatiating the full pipelines object\ntransf = full_pipeline.fit(housing)\n\n# Creating prepared data by passing the training set without labels\nhousing_prepared = transf.transform(housing)","4468de2e":"# Checking the shape of the newly created data\nhousing_prepared.shape","d3737c41":"# Cleaning the test data \ntest_prepared = transf.transform(test_df)","05453970":"# Checking Test data's shape\ntest_prepared.shape","0f20bba1":"# Defining CV Score function we will use to calculate the scores\ndef cv_score(score):\n    rmse = np.sqrt(-score) # -score because we are using \"neg_mean_squared_error\" as our metric\n    return (rmse)","59ccd3bd":"%%time\n\n# Import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Instantiate RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\n\n# Creating Parameter grid for GridSearch CV\nparams_rf = {\n    'n_estimators': [500,1000], # No of trees\n    'max_depth': [10,15], # maximum depth to explore\n    'min_samples_split':[5], # minimum samples required for split\n    'min_samples_leaf':[5], # minimum samples required at leaf\n    'max_features': [ 'auto'] # number of features for the best split\n}\n\n# Instantiate grid_rf\ngrid_rf = GridSearchCV(estimator = rf, # regressor we want to use\n                       param_grid = params_rf, # Hyperparameter space\n                       scoring ='neg_mean_squared_error', # MSE will be performance metric\n                       cv = 3, # #of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_rf.fit(housing_prepared,housing_labels)","6e0baf67":"# Lets look at the Cross Validation score for RandomForestRegressor\nprint('CV Score for best RandomForestRegressor model: {:.2f}'.format(cv_score(grid_rf.best_score_)))","386cbbcd":"# Store the best model \nbest_model_RF = grid_rf.best_estimator_","e1b30559":"%%time\n\n# Importing the necessary package\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate the Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(subsample = 0.9, # this is essentially stochastic gradient boosting\n                                max_features = 0.75,\n                                random_state = 42,\n                                warm_start = True,\n                                learning_rate= 0.01) # low learning rate\n\n# Creating Parameter grid for GridSearch CV\nparams_gbr = {\n    'n_estimators': [8000], # Given that the learning rate is very low, we are increasing the num of estimators\n    'max_depth': [2,3], \n    'min_samples_split':[5],\n    'min_samples_leaf':[5],\n    'max_features': ['sqrt']\n}\n\n# Instantiate grid search using GradientBoostingRegressor\ngrid_gbr = GridSearchCV(estimator = gbr, # regressor we want to use\n                       param_grid = params_gbr, # Hyperparameter space\n                       scoring ='neg_mean_squared_error',\n                       cv = 3, # No of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_gbr.fit(housing_prepared,housing_labels)","ab467622":"# Lets look at the Cross Validation score for GradientBoostingRegressor\nprint('CV Score for best GradientBoostingRegressor model: {:.2f}'.format(cv_score(grid_gbr.best_score_)))","6b633e67":"# Store the best model \nbest_model_GBR = grid_gbr.best_estimator_","768bde77":"%%time\n\nfrom sklearn.linear_model import Ridge\n\n# Instantiate the Ridge Regressor\nridge = Ridge(random_state=42)\n\n# Creating Parameter grid for GridSearch CV\nparams_ridge = {\n    'alpha': [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100], #L2 parameter space\n    'solver': ['auto','saga','sag','cholesky']\n}\n\n# Instantiate grid search using GradientBoostingRegressor\ngrid_ridge = GridSearchCV(estimator = ridge, # regressor we want to use\n                       param_grid = params_ridge, # Hyperparameter space\n                       scoring ='neg_mean_squared_error',\n                       cv = 3, # No of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_ridge.fit(housing_prepared,housing_labels)\n","966a04aa":"# Lets look at the Cross Validation score for RidgeRegressor\nprint('CV Score for best RidgeRegressor model: {:.2f}'.format(cv_score(grid_ridge.best_score_)))","bff564f0":"# Store the best model \nbest_model_ridge = grid_ridge.best_estimator_","9b6d0c72":"%%time\nfrom sklearn.linear_model import ElasticNet\n\n# Instantiate the Ridge Regressor\nelastic = ElasticNet(random_state=42)\n\n# Creating Parameter grid for GridSearch CV\nparams_elastic = {\n    'alpha': [ 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15], #L2 regularization parameter space\n    'l1_ratio': [0.01,0.1,0.3,0.5,0.8] #L1 regularization parameter space\n}\n\n# Instantiate grid search using GradientBoostingRegressor\ngrid_elastic = GridSearchCV(estimator = elastic, # regressor we want to use\n                       param_grid = params_elastic, # Hyperparameter space\n                       scoring ='neg_mean_squared_error',\n                       cv = 3, # No of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_elastic.fit(housing_prepared,housing_labels)\n","f0594867":"# Lets look at the Cross Validation score for ElasticNet\nprint('CV Score for best ElasticNet model: {:.2f}'.format(cv_score(grid_elastic.best_score_)))","a7e67b74":"# Store the best model \nbest_model_elastic = grid_elastic.best_estimator_","27760606":"%%time\nfrom sklearn.svm import SVR\n\n# Instantiate the SVM Regressor\nsvr = SVR()\n\n# Creating Parameter grid for GridSearch CV\nparams_svr = {\n    'kernel': ['poly'], # we want a polynomial kernel\n    'degree': [5,8], # degrees to test\n    'gamma':[0.01,0.05], \n    'epsilon': [1.5,3], \n    'coef0':[3,5], # since we are selecting a polynomial kernel\n    'C': [10,30], # Penalty parameter\n    'tol':[1e-7,1e-5] # Tolerance for stopping\n}\n\n# Instantiate grid search using GradientBoostingRegressor\ngrid_svr = GridSearchCV(estimator = svr, # regressor we want to use\n                       param_grid = params_svr, # Hyperparameter space\n                       scoring ='neg_mean_squared_error',\n                       cv = 3, # No. of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_svr.fit(housing_prepared,housing_labels)\n","01438025":"# Lets look at the Cross Validation score for SVM\nprint('CV Score for best SVM model: {:.2f}'.format(cv_score(grid_svr.best_score_)))","5feb0119":"# Store the best model \nbest_model_svr = grid_svr.best_estimator_","210c21dd":"%%time\nimport xgboost as xgb\n\n# Instantiate the SVM Regressor\nxgbr = xgb.XGBRegressor(learning_rate=0.01,objective='reg:linear',booster='gbtree')\n\n# Creating Parameter grid for GridSearch CV\nparams_xgb = {\n    'n_estimators': [8000,10000], #4000,12000\n    'max_depth': [2],\n    'gamma':[0.1,0.2], # Minimum loss reduction to create new tree split ,0.5,0.9\n    'subsample':[0.7], \n    'reg_lambda':[0.1], \n    'reg_alpha':[0.1,0.8] \n}\n\n# Instantiate grid search using GradientBoostingRegressor\ngrid_xgb = GridSearchCV(estimator = xgbr, # regressor we want to use\n                       param_grid = params_xgb, # Hyperparameter space\n                       scoring ='neg_mean_squared_error',\n                       cv = 3, # No. of folds\n                       verbose = 1,\n                       n_jobs = -1) # use all cores\n\n# fit the model\ngrid_xgb.fit(housing_prepared,housing_labels)\n","a9fd3263":"# Lets look at the Cross Validation score for XGBRegressor\nprint('CV Score for best XGBRegressor model: {:.2f}'.format(cv_score(grid_xgb.best_score_)))","f25f8458":"# Store the best model \nbest_model_xgb = grid_xgb.best_estimator_","874f5a12":"# importing Voting Regressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Instantiate the Regressor\nvoting_reg = VotingRegressor(\n    estimators=[('rf', best_model_RF), ('gbr', best_model_GBR), ('elastic', best_model_elastic),\n               ('ridge',best_model_ridge),('svr',best_model_svr),('xgb',best_model_xgb)])","874f5317":"# Importing cross validation module\nfrom sklearn.model_selection import cross_val_score\n\n# Calculate cross validation score for the Voting regresssor\nscores = cross_val_score(voting_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)","061bea58":"# Given that we have negative MSE score, lets first get the root squares to get RMSE's and then calculate the mean\nvoting_reg_score = np.sqrt(-scores)\n\n# Calc mean for RMSE\nprint(voting_reg_score.mean())","a9f99241":"# Fitting the voting regressor on the entire training dataset\nvoting_reg.fit(housing_prepared, housing_labels)\n\n# Predict on test set\npred = voting_reg.predict(test_prepared)","fc8f10b5":"# converting to dataframe\npreds_df = pd.DataFrame({'Id':test_df_IDs,'SalePrice':pred},index=None)","4aeb37a7":"# looking at the first 5 rows\npreds_df.head()","3e9b71e1":"# Submitting the predictions\npreds_df.to_csv('submissions.csv',index=False)","bdfb5425":"#### 1.4.1 SalePrice and OverallQual","0d8c44ed":"#### 2.2 Normalizing SalePrice\n\n- As we saw earlier, SalePrice is not normally distributed. ML models don't perform well with skewed data.\n- In order to fix this, we will do a log transformation of SalePrice","37c007d3":"- Now, Overall quality itself seems to be a calculated metric but it definitely has a high correlation with SalePrice. \n- Also, within the 10 possible values, the lower values (1,2 etc.) have a lesser spread as compared to higher values (9,10 etc.), which have a few outliers as well.","fd4e6d55":"#### 2.3 Removing Outliers\n\n- We looked at the 2 observation on the 'GrLivArea - SalePrice' scatterplot. Let us remove them.","c39d1270":"#### 1.3.2 Zoomed in Scatterplot\n- Let's vizualize only the variables with high correlation coefficient.","6d30aa87":"### 1.1 Let's try and get a sense of our data","a2dcf6d9":"- The definition for GarageCars is \"Size of garage in car capacity\". You would expect SalePrice to keep on increasing with the garage size. However, median SalePrice decreases for the highest car capacity bucket. It could be because of a smaller sample size.\n- Further, looking at the heatmap, the variables 'GarageArea' and 'GarageCars' seem to be highly correlated. This could lead to multicollinearity. \n- Also, GarageCars seems to be correlated to OverallQual, indicating that it could be one of the factors involved in calculating OverallQual (assuming it is a calcualted variable).","701d7fd7":"# 1. EDA","d053fafa":"#### 2.1 Removing IDs\n- We need to remove the ID column since it is a unique identifier","4d063c1f":"## Objective:\n\n- The Objective is to predict Sale Price of each house given 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.\n\n## The approach:\n\n1. Exploratory Data Analysis to get a sense of the data.\n\n2. Feture engineering: \n    2.1 We will be creating a few features by the combining the existing features. \n    2.2 We will also fill in the missing values.\n    All of this will be done using sklearn pipelines.\n    \n3. Training and Predicting:\n    \n    3.1 We will perform Grid search cross validation of 5 different algorithms: Ridge Regression, Elastic Net, SVM, XGBoost, GradientBoost. The hyper parameter space for a few algorithms has been thinned down as it the runtime is too high here which is not the case with a local version.\n    \n    3.2 We will combine the 5 algorithms using a voting regressor and use that to train on the entire dataset and predict on the test set.\n    \n    ","e115e576":"- Seems like SalePrice is right skewed and we will have to normalize it.","18cf15a9":"# Resources:\n\nKernels: The following Kernels have been immensely helpful and shaped my approach towards the problem! \n\n1. [Lavanya Shukla's Kernel](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition)\n\n2. [Serigne's Stacked Regressions Kernel](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n\n3. [Pedro Marcelino's EDA Kernel](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n\n\nOther online resoruces:\n    \n1. [Rotating labels:](https:\/\/www.drawingfromdata.com\/how-to-rotate-axis-labels-in-seaborn-and-matplotlib)\n\n2. [Getting feature names after one-hot encoder:](https:\/\/stackoverflow.com\/questions\/54646709\/sklearn-pipeline-get-feature-name-after-onehotencode-in-columntransformer\/54648023)\n\n3. [Random Forest Vs Gradient boosting:](https:\/\/stats.stackexchange.com\/questions\/173390\/gradient-boosting-tree-vs-random-forest)","fdcdb8d1":"#### 2.6 Categorical Pipeline","116bdac9":"#### 2.5 Numerical Pipeline\n\n- We will be creating 3 addtitional features:\n        - Total_sqr_footage: The total sqr footage of the house i.e. basement + 1st floot+ 2nd floor.\n        - Total_Bathrooms: Total number of bathrooms.\n        - Total_porch_sf: Total sqr footage of the porch.\n        \nThis idea for the features is being leveraged from Lavanya Shukla's Kernel (see resoruces section)","0b956b4f":"#### 1.4.5. SalePrice and GarageCars","4d2fa6e5":"#### 1.4.4. SalePrice and GrLivArea","ff2b0914":"#### 1.4.2 SalePrice and YearBuilt","64adf262":"### 3.3 Ridge Regression","4436fd8c":"### 3.6 Ensemble","fde35712":"### 3.1 RandomForestRegressor\n\n- In all honesty, I have removed a few values from the parameter grid as the kernel takes too long to run. \n- The runtime would be far better on a local version of the notebook and one can consider a borader hyperparameter space.","e30f3148":"### 1.2 Let's see how our dependant variable looks","c5cba8c5":"### 3.2 Gradient Boosting\n\n- GradientBoostingRegressor could have a very high runtime (for a small learning rate), so I have removed thinned down the hyperparam space considered here. I tested out a lot more values on my local version. ","d2a99a3e":"- It is important to take a step back here and go through the data description. Obviously, we have 2 types of data points. Numeric and categorical. Further, for some categorical features, missing values can be filled with \"None\", whereas for some there is no clear intuition so we will go ahead with the most frequent value. Let's split the dataframe in 3 categories: \n    - Numeric\n    - Categorical to be filled with \"None\"\n    - Categorical to be filled with most frequent","3c962b0b":"- Seems we have 1460 observations and 79 features (if we exlcude ID and Dependent variable i.e. SalePrice)","24dc6a36":"- The Years range from 1872 to 2010. Post mid-30s, the general trend for median house price is going up. However, we don't know if the prices are adjusted for inflation. They most probably aren't.","6f8aa0f5":"### 1.3 Let's vizualize the correlation matrix","ba62d2a5":"- The numbers of columns have reduced by 1. ID has been removed.","2beda61b":"- Overall, the positive corrrelation is quite obvious.\n- There are 2 sets of outliers here:\n    - The 2 observations on the bottom right, with >4000 sqft of GrLivArea but < 200k of SalePrice. We could consider drropping these points.\n    - The 2 observations on the top right, with >4000 sqft of GrLivArea and >700k of SalePrice. Given, that these observations, although extreme, are still on the same trend as the other observations, we should keep them.","c16fa755":"- We have at least 19 columns with missing values.\n- Additionally, there are some columns like MSSubClass,YrSold,MoSold,GarageYrBlt that should cateegorical but are listed as numeric","39fe70ff":"#### 2.4 Missing Data","84eae999":"# 3. Training Models","be1a928f":"### 3.4 ElasticNet Regression","ffb0f94b":"### 3.5 SVM","5fa8b88f":"### 1.4 Bivariate Analysis","45330990":"- The positive correlation is clear.\n- Also, some outliers exist. Vis-a-vis: Point on the right with more than 6000 sq ft basement and a low SalePrice. (I wonder if this is a bunker!)","9b02e832":"#### 2.7 Creating Full pipeline","75c7fc4d":"- Let's do a quick bivariate analysis of a few features and SalePrice","eddc8a61":"#### 1.4.3. SalePrice and TotalBsmtSF","f3432450":"- SalePrice looks normally distributed now.","034c227e":"# 2. Feature engineering","52f2b25a":"### 3.5 XGBoost"}}