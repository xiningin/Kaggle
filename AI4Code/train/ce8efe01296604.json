{"cell_type":{"e26349b6":"code","cf11b89e":"code","149dcb4a":"code","8dcff288":"code","f4c239a5":"code","2566b731":"code","d6295bb2":"code","e2558877":"code","d9a4f09a":"code","27bfad44":"code","651e6135":"code","9e38c343":"code","b9b26a6c":"code","b525bed2":"code","6091a05d":"code","1db72e78":"code","2a07e125":"code","d6be15db":"code","c23fefd0":"code","9712b48d":"markdown","18c66615":"markdown","5bd55f51":"markdown","4b7d0f17":"markdown","19f7ef61":"markdown","7eb629c4":"markdown","47ac0d19":"markdown","083beee8":"markdown","f46946da":"markdown","763fefe6":"markdown","785f6994":"markdown","b9ef7e81":"markdown","8f94d9dd":"markdown","9a7282c8":"markdown","d27db305":"markdown","355d86ac":"markdown"},"source":{"e26349b6":"# @author: Justfor - on Kaggle:  https:\/\/www.kaggle.com\/justfor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,  BernoulliNB\nfrom sklearn.metrics import accuracy_score, log_loss,jaccard_similarity_score\nimport math\nfrom sklearn.preprocessing import normalize\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","cf11b89e":"import logging\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline","149dcb4a":"DATA_DIR = \"..\/input\"\nSUBMISSION_FILE = \"{0}\/sample_submission.csv\".format(DATA_DIR)\nCACHE=False\n\nNFOLDS = 5\nSEED = 1337\nMETRIC = log_loss\n\nID = 'Id'\nTARGET = 'Cover_Type'\n\nnp.set_printoptions(precision=5)\nnp.set_printoptions(suppress=True)\n\nnp.random.seed(SEED)\n#logging.basicConfig(level=logging.DEBUG)\nlogging.basicConfig(level=logging.WARNING)","8dcff288":"def add_feats(df):\n    df['HF1'] = df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points']\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points']+df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    df['EV1'] = df.Elevation+df.Vertical_Distance_To_Hydrology\n    df['EV2'] = df.Elevation-df.Vertical_Distance_To_Hydrology\n    df['Mean_HF1'] = df.HF1\/2\n    df['Mean_HF2'] = df.HF2\/2\n    df['Mean_HR1'] = df.HR1\/2\n    df['Mean_HR2'] = df.HR2\/2\n    df['Mean_FR1'] = df.FR1\/2\n    df['Mean_FR2'] = df.FR2\/2\n    df['Mean_EV1'] = df.EV1\/2\n    df['Mean_EV2'] = df.EV2\/2    \n    df['Elevation_Vertical'] = df['Elevation']+df['Vertical_Distance_To_Hydrology']    \n    df['Neg_Elevation_Vertical'] = df['Elevation']-df['Vertical_Distance_To_Hydrology']\n    \n    # Given the horizontal & vertical distance to hydrology, \n    # it will be more intuitive to obtain the euclidean distance: sqrt{(verticaldistance)^2 + (horizontaldistance)^2}    \n    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df.slope_hyd_sqrt=df.slope_hyd_sqrt.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    df['slope_hyd2'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)\n    df.slope_hyd2=df.slope_hyd2.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    #Mean distance to Amenities \n    df['Mean_Amenities']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways) \/ 3 \n    #Mean Distance to Fire and Water \n    df['Mean_Fire_Hyd1']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) \/ 2\n    df['Mean_Fire_Hyd2']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) \/ 2\n    \n    #Shadiness\n    df['Shadiness_morn_noon'] = df.Hillshade_9am\/(df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon\/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am\/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am+df.Hillshade_Noon)\/2\n    df['Shadiness_afternoon'] = (df.Hillshade_Noon+df.Hillshade_3pm)\/2\n    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am']  + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) \/ 3    \n    \n    # Shade Difference\n    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n    # Mountain Trees\n    df[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n    # Only some trees can grow on steep montain\n    \n    ### More features\n    df['Neg_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Neg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Neg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    \n    df['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\/2\n    df['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\/2\n    df['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\/2   \n        \n    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n    \n    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n    \n    # Bin Features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), # Elevation is different in train vs. test!?\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name]\/bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","f4c239a5":"def load_and_process_dataset():\n    train = pd.read_csv(\"{0}\/train.csv\".format(DATA_DIR))\n    test = pd.read_csv(\"{0}\/test.csv\".format(DATA_DIR))\n\n    y_train = train[TARGET].ravel() -1 # XGB needs labels starting with 0!\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are %i classes: %s \" % (num_classes, classes))        \n\n    train.drop([ID, TARGET], axis=1, inplace=True)\n    test.drop([ID], axis=1, inplace=True)\n    \n    train = add_feats(train)    \n    test = add_feats(test)    \n    \n    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                       'Horizontal_Distance_To_Fire_Points', \n                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                       'Shadiness_morn_avg', \n                       'Shadiness_afternoon', \n                       'Shadiness_mean_hillshade',\n                       'HF1', 'HF2', \n                       'HR1', 'HR2', \n                       'FR1', 'FR2'\n                       ]\n\n    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n    test[cols_to_normalize] = normalize(test[cols_to_normalize])\n\n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    train = train.drop('Elevation', axis=1)\n    test = test.drop('Elevation', axis=1)    \n    \n    x_train = train.values\n    x_test = test.values\n\n    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}\n","2566b731":"dataset = Dataset(preprocessor=load_and_process_dataset, use_cache=True)  ","d6295bb2":"# Parameters for the classifiers\nrf_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\nrf1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\nlgb_params = {\n    'n_estimators': 200, \n    'learning_rate':0.1\n}\n\nlogr_params = {\n        'solver' : 'liblinear',\n        'multi_class' : 'ovr',\n        'C': 1,\n        'random_state': 0}","e2558877":"rf = Classifier(dataset=dataset, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf_params,name='rf')\net = Classifier(dataset=dataset, estimator = ExtraTreesClassifier, use_cache=CACHE, parameters=et_params,name='et')   \nrf1 = Classifier(dataset=dataset, estimator=RandomForestClassifier, use_cache=CACHE, parameters=rf1_params,name='rf1')\net1 = Classifier(dataset=dataset, use_cache=CACHE, estimator=ExtraTreesClassifier, parameters=et1_params,name='et1')\nlgbc = Classifier(dataset=dataset, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb_params,name='lgbc')\ngnb = Classifier(dataset=dataset,estimator=GaussianNB, use_cache=CACHE, name='gnb')\nlogr = Classifier(dataset=dataset, estimator=LogisticRegression, use_cache=CACHE, parameters=logr_params,name='logr')","d9a4f09a":"def xgb_first(X_train, y_train, X_test, y_test=None):\n    xg_params = {\n        'seed': 0,\n        'colsample_bytree': 0.7,\n        'silent': 1,\n        'subsample': 0.7,\n        'learning_rate': 0.1,\n        'objective': 'multi:softprob',   \n        'num_class': 7,\n        'max_depth': 4,\n        'min_child_weight': 1,\n        'eval_metric': 'mlogloss',\n        'nrounds': 200\n    }    \n    X_train = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n    return model.predict(xgb.DMatrix(X_test))","27bfad44":"xgb_first = Classifier(estimator=xgb_first, dataset=dataset, use_cache=CACHE, name='xgb_first')  ","651e6135":"pipeline = ModelsPipeline(rf, et, et1, lgbc, logr, gnb, xgb_first) \n\nstack_ds = pipeline.stack(k=NFOLDS,seed=SEED)","9e38c343":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : SEED, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","b9b26a6c":"# Validate results using k-fold cross-validation\nresults = stacker.validate(k=NFOLDS,scorer=log_loss)","b525bed2":"models = [rf, et, et1, lgbc, logr, gnb, xgb_first]       \nprint(\"Log Loss\")\nfor index, element in enumerate(models):\n    print(index, element.name)\n    element.validate(k=NFOLDS,scorer=log_loss)","6091a05d":"preds_proba = stacker.predict() \n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(preds_proba, axis=1)).astype(int) + 1","1db72e78":"submission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking_with_heamy_logregr.sub.csv', index=None)","2a07e125":"# Use a xgb-model as 2nd-stage model\n\ndtrain = xgb.DMatrix(stack_ds.X_train, label=stack_ds.y_train)\ndtest = xgb.DMatrix(stack_ds.X_test)\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.8,\n    'silent': 1,\n    'subsample': 0.6,\n    'learning_rate': 0.05,\n    'objective': 'multi:softprob',\n    'num_class': 7,        \n    'max_depth': 6,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': 'mlogloss',\n}\n\nres = xgb.cv(xgb_params, dtrain, num_boost_round=1000, \n             nfold=NFOLDS, seed=SEED, stratified=True,\n             early_stopping_rounds=20, verbose_eval=5, show_stdv=True)\n\nbest_nrounds = res.shape[0] - 1\ncv_mean = res.iloc[-1, 2]\ncv_std = res.iloc[-1, 3]\n\nprint('Ensemble-CV: {0}+{1}, best nrounds = {2}'.format(cv_mean, cv_std, best_nrounds))","d6be15db":"# Train with best rounds\nmodel = xgb.train(xgb_params, dtrain, best_nrounds)\n\nxpreds_proba = model.predict(dtest)\n\n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(xpreds_proba, axis=1)).astype(int) + 1","c23fefd0":"submission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking_with_heamy_cv_mlogloss_' + str(cv_mean) + '.sub.csv', index=None)","9712b48d":"# Forest Cover -Ensembling and Stacking with heamy\n## Introduction\n\n@author: Justfor - on Kaggle:  https:\/\/www.kaggle.com\/justfor\n\nThis kernel shows **Ensemble** and **Stacking**  and has the the intention to show the use of the heamy package and to get a very high Leaderboard score.\n\nThere are various packages for stacking available, but the heamy package has one great advantage: **Caching** !\nThis means your Classifier learns once and is then stored. When you need it again with the same date it is simply fetched. A great **time-saver**.\n\n**heamy**  is available from rushter's (Artem Golubin) github page:  https:\/\/github.com\/rushter\/heamy\n\nFurthermore I got Inspiration from the great kernels \n- Based on: https:\/\/www.kaggle.com\/mmueller\/allstate-claims-severity\/stacking-starter (c) Faron (www.kaggle.com\/mmueller)\n- Work frum rushter https:\/\/www.kaggle.com\/rushter\/stacking-using-heamy\n\nIdeas for Feature engineering from \n- TheGruffelo (https:\/\/www.kaggle.com\/cbryant) in the  Notebook (https:\/\/www.kaggle.com\/cbryant\/new-features-and-recursive-feature-extraction) and \n-  from schlerp (https:\/\/www.kaggle.com\/schlerp) in the Notebook https:\/\/www.kaggle.com\/schlerp\/lgb-cv-ensemble-normalisation-fe\n","18c66615":"So Stacking works nice here.","5bd55f51":"## Load and process the Data\n","4b7d0f17":"## Features","19f7ef61":"## Select and Initialize Classifiers\nI have tried to select various classifiers. The key is a good validation score and if possible the use of a diffferent method\/classifier for the ensembling.","7eb629c4":"##  Second Stage - Train LogisticRegression on stacked data\n\nA LogisticRegression is often used as Classifier for the second stage, so let's try it out.","47ac0d19":"## Stack the classifiers\/models\n\nStack the models  and returns new dataset with out-of-fold predictions","083beee8":"##  Further ressources\nFurther reading about the heamy package and its use can be found in the walkthrough (https:\/\/github.com\/rushter\/heamy\/blob\/master\/examples\/walkthrough.ipynb) or the documentation (https:\/\/heamy.readthedocs.io\/en\/latest\/)\n\nFor ensembling the \"classic\" guide is the excellent Kaggle-Ensemble-Guide For more information: http:\/\/mlwave.com\/kaggle-ensembling-guide\/\nAnother good explanation can be found here: http:\/\/blog.kaggle.com\/2016\/12\/27\/a-kagglers-guide-to-model-stacking-in-practice\/\n","f46946da":"Notice the lower cross-validation score in comparison to the logistic regression stacker.\n\nUsing xgboost as stacker works even better!\nThis looks fine!","763fefe6":"Models with sklearn-interface can be included.","785f6994":"## Alternative: Second Stage with xgboost\nWe can also try another classifier at the second stage. Xgboost is famous\/classic and has the nice option for early-stopping. \nSo let's use a xgb-model as 2nd-stage model","b9ef7e81":"## Create the Dataset","8f94d9dd":"## Setup\nSetting up base directories, nfolds and further  options.\n\n**Caching**: We can also select, whether caching should take place (recommend to do it at home, as kaggle kernel maybe not working).","9a7282c8":"How did stacking work in this case? \nLet's compare the single classifiers.\n\nFor Verification - evaluation of the single models","d27db305":"## Predict & submit\nThe prediction gives the probabilities for the different  classes. In the next step the respective class with the highest probability is selected. To  get the the final label the addition of 1 is needed.","355d86ac":"Models can also be defined and used directly via a function"}}