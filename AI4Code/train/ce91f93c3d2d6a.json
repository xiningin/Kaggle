{"cell_type":{"d2415dd1":"code","fb1f8ad8":"code","b52cc553":"code","ab4b92ba":"code","d6ae38b4":"code","226f2493":"code","fd4449df":"code","a9cc40fe":"code","5129944d":"code","eb3a8382":"code","c808adde":"code","9cdfe9e1":"code","1b999674":"code","d46808aa":"code","cf686d69":"code","62c377dc":"code","fe42d3f0":"code","1351ecfc":"code","82528fe6":"code","3f081a4b":"code","06538f71":"code","631943c1":"markdown","1349e21f":"markdown"},"source":{"d2415dd1":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nseed = 100\n\n# python RNG\nimport random\nrandom.seed(seed)\n\n# pytorch RNGs\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\n# numpy RNG\nnp.random.seed(seed)","fb1f8ad8":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\nfrom fastai.callbacks import *","b52cc553":"import pytorch_pretrained_bert\nimport fastai\nimport scipy\nimport random\nimport pathlib\nimport typing\nprint (torch.__version__)\nprint (fastai.__version__)\nprint (np.__version__)\nprint (pd.__version__)\nprint (pytorch_pretrained_bert.__version__)\nprint (scipy.__version__)","ab4b92ba":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n\nconfig = Config(\n    testing=False,\n    bert_model_name=\"bert-base-uncased\",\n    max_lr=3e-5,\n    epochs=4,\n    use_fp16=True,\n    bs=32,\n    discriminative=False,\n    max_seq_len=256,\n)","d6ae38b4":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    config.bert_model_name,\n)","226f2493":"def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]=BOS):\n    \"\"\"Borrowed from fast.ai source\"\"\"\n    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n    if is1d(texts): texts = texts[:,None]\n    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n    for i in range(1,len(df.columns)):\n        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n    return text_col.values","fd4449df":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","a9cc40fe":"from sklearn.model_selection import train_test_split\n\ndf_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')\n\npath='\/kaggle\/working\/'\ndf_train.text=df_train.text+' This comment is about '+df_train.drug\ndf_test.text=df_test.text+' This comment is about '+df_test.drug\ntrain_df,valid_df= train_test_split(df_train,test_size=0.2, random_state=seed)","5129944d":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","eb3a8382":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])","c808adde":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","9cdfe9e1":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","1b999674":"from sklearn.model_selection import StratifiedKFold\nfolds=StratifiedKFold(n_splits=5,random_state=seed,shuffle=True)\nfolds=[(tr,val) for tr,val in folds.split(df_train,df_train.sentiment)]","d46808aa":"def get_preds_as_nparray(learner_,dbunch_,ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner_.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in dbunch_.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\n","cf686d69":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n","62c377dc":"wts=    wts=(0.25\/df_train.sentiment.value_counts(1).sort_values()).tolist()\n","fe42d3f0":"import gc\ngc.collect()","1351ecfc":"from fastai.callbacks import *","82528fe6":"test_preds_list2=[]\nscores=[]\nfor i in range(5):\n    train_df=df_train.iloc[folds[i][0],:]\n    valid_df=df_train.iloc[folds[i][1],:]\n    # this will produce a virtually identical databunch to the code above\n    bert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=3)\n    databunch = BertDataBunch.from_df(path, train_df, valid_df,test_df= df_test,\n                      tokenizer=fastai_tokenizer,\n                      vocab=fastai_bert_vocab,\n                      text_cols=\"text\",\n                      label_cols='sentiment',\n                      bs=config.bs,\n                      collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n                 )\n\n    learner = Learner(\n        databunch, bert_model,\n    metrics=[FBeta(average='macro',beta=1),accuracy]\n    )\n    if config.use_fp16: learner = learner.to_fp16()\n\n    learner.loss_func=CrossEntropyFlat(weight=torch.Tensor(wts).float().cuda())    \n\n    cb = [callbacks.tracker.SaveModelCallback(learner, every='improvement', monitor='f_beta', name='best-model_{}'.format(i))]\n    learner.fit_one_cycle(10,3e-5,callbacks=cb)\n    learner.load('best-model_{}'.format(i))\n    scores.append(np.array(learner.recorder.metrics).max(axis=0).tolist())\n    test_preds = get_preds_as_nparray(learner,databunch,DatasetType.Test)\n    temp_val2=test_preds.argmax(axis=1)\n    test_preds_list2.append(temp_val2)\n    if i==0:\n        !rm -r \/kaggle\/working\/models\/best-model_0.pth\n    if i==1:\n        !rm -r \/kaggle\/working\/models\/best-model_1.pth\n    if i==2:\n        !rm -r \/kaggle\/working\/models\/best-model_2.pth\n    if i==3:\n        !rm -r \/kaggle\/working\/models\/best-model_3.pth\n        ","3f081a4b":"print (scores)","06538f71":"from scipy.stats import mode\nbagg_preds2=pd.DataFrame(np.column_stack(test_preds_list2))\npreds2=bagg_preds2.mode(axis=1)[0]\nbagg_preds2['unique_hash']=df_test.unique_hash\nbagg_preds2.to_csv('raw_preds_bert_uncased.csv',index=False)\npd.DataFrame({'unique_hash':df_test.unique_hash,'sentiment':preds2}).to_csv('sub__bert_uncased.csv',index=False)","631943c1":"Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)","1349e21f":"# Model"}}