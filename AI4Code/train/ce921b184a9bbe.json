{"cell_type":{"94cda9bc":"code","a66f7430":"code","92660a70":"code","118ada34":"code","f521ec49":"code","9cbb3452":"code","bad9deab":"code","58b1e087":"code","0f2d6235":"code","5c414d75":"code","c962a311":"code","3df9ccf9":"code","78650dc9":"code","c0316693":"markdown","399c9c92":"markdown","0cea2681":"markdown","ee1b1fa3":"markdown","bf176558":"markdown","e596a3f6":"markdown","2d19e98c":"markdown","100944a9":"markdown","d6ee80de":"markdown","edf56c18":"markdown","93939495":"markdown","67febfac":"markdown","2106980a":"markdown","e328d2e9":"markdown","e27886df":"markdown","1a7ef348":"markdown","fb2654c4":"markdown","4fb5aa7c":"markdown"},"source":{"94cda9bc":"#using in C# is import in Python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy import stats\nfrom scipy.stats import norm\n\n#These 2 libraries are used to plot graphs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#SimnpleImputer will be used for feature Engineering\nfrom sklearn.impute import SimpleImputer","a66f7430":"#pd is alias for pandas library\ntrain_raw = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv') \ntest_raw = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = train_raw.copy()\ntest = test_raw.copy()\nprint(\"Shape -> Train :\", train.shape, \"Test :\", test.shape)","92660a70":"#Skew & Kurt Before\nprint(\"Before -> Skew :\", train.SalePrice.skew(), \"Kurt :\", train.SalePrice.kurt())\n\n#plotting distribution of SalePrice\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(20,5))\nplt.subplot(1,2,1) #args->row,col,index\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\nsns.distplot(train.SalePrice,fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(train['SalePrice'], plot=plt)\nplt.ylabel('SalePrice')\nplt.title('SalePrice Probility Plot')\nplt.show()","118ada34":"\n#apply log to SalePrice\ntrain.SalePrice = np.log1p(train.SalePrice)\n\n#Skew & Kurt After\nprint(\"After -> Skew :\", train.SalePrice.skew(), \"Kurt :\", train.SalePrice.kurt())\n\n#plotting distribution of SalePrice\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(20,5))\nplt.subplot(1,2,1) #args->row,col,index\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\nsns.distplot(train.SalePrice,fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(train['SalePrice'], plot=plt)\nplt.ylabel('SalePrice')\nplt.title('SalePrice Probility Plot')\nplt.show()\n","f521ec49":"#droping Id column\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis=1,inplace=True)\nprint(\"Shape -> Train :\", train.shape, \"Test :\", test.shape)","9cbb3452":"#Keep Only Numeric Columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ntrain = train.select_dtypes(include=numerics)\nnumericColList = train.columns.tolist()\nnumericColList.remove('SalePrice')\ntest = test[numericColList]\nprint(\"Shape -> Train :\", train.shape, \"Test :\", test.shape)","bad9deab":"#Plot Heat Map to find relationship\ncorrmat = train.corr()\nfig, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,vmax=0.8,square=True)\nplt.show()","58b1e087":"#SalePrice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nfig, ax = plt.subplots(figsize=(12,9))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","0f2d6235":"#Columns Selected (removed columns which can be derived)\nselectedCols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ntrain = train[selectedCols]\nselectedCols.remove('SalePrice')\ntest = test[selectedCols]","5c414d75":"#Removing Columns missing data in \nprint(\"Training Missing before : \" ,train.isnull().sum().sum())\nprint(\"Test Missing before : \" ,test.isnull().sum().sum())\ntest_na = test.isnull().sum()\/len(train)*100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' : test_na})\n#Print Missing Data\nprint(missing_data)\n#Rows missing Data\nprint(test[test.isnull().any(axis=1)])\n#replacing missing data with 0\ntest.fillna(0,inplace=True)\nprint(\"Training Missing after : \" ,train.isnull().sum().sum())\nprint(\"Test Missing after : \" ,test.isnull().sum().sum())","c962a311":"train_x = train.loc[:,train.columns != 'SalePrice']\ntrain_y = train.SalePrice\ntest_x = test\ntrain_x.shape, train_y.shape, test_x.shape","3df9ccf9":"#Regression\nmodel = RandomForestRegressor()\nmodel.fit(train_x,train_y)\npred_y = model.predict(test_x)\npred_y = np.expm1(pred_y)","78650dc9":"#Submission\nmy_submission = pd.DataFrame({'Id':test_raw.Id, 'SalePrice' : pred_y})\nmy_submission.to_csv('submission.csv', index=False)","c0316693":"Now let's keep pop out SalePrice from Matrix for training","399c9c92":"GarageCars and GarageArea has strong corellation of 0.88. So, lets keep only GarageCars and remove GarageArea.\nTotalBsmtSF and 1stFlrSF has strong relationship of 0.82. So, lets remove 1stFlrSF.\nYearRemodAdd are negatively related to many Columns. So, lets lets remove this column as well.","0cea2681":"So now from above we can see that Distribution plot for Sales Price is more symetric and Skew is also under 1.","ee1b1fa3":"Our training is complete. let's prepare the data for submission!!","bf176558":"We will now train the model with data we have. We are using RandomForestRegressor for this training. ","e596a3f6":"Above we checked ratio of missing values and observed it is not to high. If it were too high we would have considered to drop such columns.","2d19e98c":"There are some columns which have missing data. Ideally se should be imputing such missing values with average, std Deviation etc. But to keep it simple, lets update those columns as zero.","100944a9":"above correlation matrix has too many columns. To keep Model simple, lets focus only on top 10 corelated columns.","d6ee80de":"Training data has 1460 rows and 81 columns, where as test data has 1459 rows and 80 columns. 1 extra column in training data is the actual recorded Salse Values, which we will be using to predict the Sales.","edf56c18":"We have removed ID column. Now lets remove the Non-numeric Columns. We will do this by keeping only Numeric Data Types.\nData Type in Python are quite similar to C# Data Types :\nint16 --> short\nint32 --> int\nint64 --> long\nfloat16 --> No such data type in C#\nfloat32 --> float\nfloat64 --> double","93939495":"In C# reading from a CSV and loading into an array was not easy. you need to reference IO namespace, then Open File Stream and close after reading from File Stream.  But in Python you just use pd.read_csv :-)\nApart from reading CSV we will now create a copy for the loaded data see structure of the data.","67febfac":"We have now removed all non-numeric data types. \nLets see how columns are related to Sales Price","2106980a":"Models don't work well if Training data is not well distributed. So, to know if Training data is well distributed we will use plot distribution graph.","e328d2e9":"On this Notebook I will be shareing the first notebook I submitted in Kaggle. I have been doing development in .NET using C# for several years. On this notebook I will be explaining similarities that Python have with C#. So it is more understadable to developers from .NET background.","e27886df":"From above we can see SalePrice are concentrated between 1000 to 3000 and the graph is not symetric. Skew value is also higher then 1. \nWe can apply  log1P on SalePrice to make the graph more symetric. Let's apply log1p and check if this makes difference.","1a7ef348":"Id column in Data Set dosen't have any relationship with Sale Price so we will drop this column. \nIn C# dropping a column from Matrix is lot of hassel. But with Python it is quite easy.","fb2654c4":"if some cell is too bright then it is positively related. If it is very dark then it is negatively corelated. Columns in middle are not much significant.\nIf a column is positively or negaatively related to another column then these to columns are redundant. We can keep the columns which is more related to SalesPrice and drop the other column.","4fb5aa7c":"Similar to C# first step is to reference the libraries. here we use import instead of using. \nIf you have already done trainings on Machine Learning then you might be already be aware of the 3 libraries which we will need to use in almost every Model we develop.\nnumpy --> Numpy contains the mathmatical libraries such as log, exp, linear algebra. More advaned Scientific functions are in SciPy library.\npandas --> This library provides utility functions such as reading from files and loadeding in arrays. There are many different kinds of arrays in python. And also there are many different ways how you can access arrays in Python.\nsklearn --> contains most of the ML models. You can just use these builtin model on your data. "}}