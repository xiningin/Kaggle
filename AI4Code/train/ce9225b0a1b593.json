{"cell_type":{"31096c70":"code","468d4f7b":"code","750ef55b":"code","7115cf61":"code","ce39757b":"code","83c5d7bf":"code","969a697a":"code","6b7e92d0":"code","a988f174":"code","0163cac0":"code","7d8abfd9":"code","e53c189d":"code","c5324dbf":"code","737c960e":"code","e3fc6c0a":"code","c023f16a":"code","7169763f":"code","e5ec280c":"code","7f0511dd":"code","aec6b2cc":"code","6d224f21":"code","67a9678a":"code","7a47c3e7":"code","4aacd45e":"code","ef967935":"code","da2834dd":"code","e1918d26":"code","4a8bfd8e":"code","afec1aeb":"code","0bf31066":"code","f5891220":"code","1b76f3f6":"code","f495debd":"code","1dc340cf":"code","985ef694":"code","61e274cf":"code","f16a6ef2":"code","696e3a58":"code","69ee9169":"code","30c5231b":"code","8fbbd9bf":"code","a10fd255":"code","ac15ad7c":"code","92c3a20d":"code","e6ecca94":"code","bbbc9997":"code","abf52b0f":"code","8c4cddfd":"code","076d0df9":"code","f2a49147":"code","8c216d0d":"code","6de3cbc3":"code","7ec63895":"code","b86bb37a":"code","4324a48f":"markdown","1bc62b72":"markdown","141173ed":"markdown","5bf68df9":"markdown","be6fd90f":"markdown","2e458d43":"markdown"},"source":{"31096c70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\n\nfrom tqdm.notebook import tqdm\n\nfrom functools import partial\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt","468d4f7b":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(seed=0)","750ef55b":"train = False","7115cf61":"weights_lambda = 0.90 # reflect the weight decay for distant days","ce39757b":"print([weights_lambda ** i for i in range(100)])","83c5d7bf":"train_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv')\ntrain_df = train_df.fillna('')\ntrain_df.tail()","969a697a":"test_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/test.csv')\ntest_df = test_df.fillna('')\ntest_df.tail()","6b7e92d0":"num_dates_total = len(np.unique(list(train_df['Date']) + list(test_df['Date'])))\nprint(num_dates_total)\nnum_dates_test = len(np.unique(list(test_df['Date'])))\nprint(num_dates_test)\nnum_dates_train = len(np.unique(list(train_df['Date'])))\nprint(num_dates_train)","a988f174":"sample_submission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/submission.csv')\nsample_submission.head()","0163cac0":"len(sample_submission) \/ 3","7d8abfd9":"cases = train_df[\"TargetValue\"][train_df[\"Target\"] == 'ConfirmedCases'].values.reshape((-1, num_dates_train))\ncases","e53c189d":"fatalities = train_df[\"TargetValue\"][train_df[\"Target\"] == 'Fatalities'].values.reshape((-1, num_dates_train))\nfatalities","c5324dbf":"#making the weights equal\nfatalities = fatalities * 10","737c960e":"population = train_df[\"Population\"].values[::num_dates_train * 2]\npopulation\nlen(population)","e3fc6c0a":"weignts_cases = train_df[\"Weight\"][train_df[\"Target\"] == 'ConfirmedCases'].values[::num_dates_train]\nweignts_cases\nlen(weignts_cases)","c023f16a":"import lightgbm as lgbm\nf = lgbm.LGBMRegressor()","7169763f":"if train:\n    days_to_predict = 45\nelse:\n    days_to_predict = 31\ntrain_days_used = num_dates_total - days_to_predict\nassert train_days_used <= num_dates_train\n\n\nfeatures_days = 40\ntest_days_predict = 40\ntrain_data = []\ntarget_data = []\ntest_data = []\nweights = []\n\n\nfat_index_add = features_days\nfat_index_add_test = test_days_predict ","e5ec280c":"for i in range(len(population)):\n    for j in range(train_days_used - 1 - features_days):\n        full_train = True\n        \n        data = []\n        data = data + [population[i]]\n        max_cases = cases[i][:j+features_days].max()\n        data = data + [max_cases]\n        max_fatalities = fatalities[i][:j+features_days].max()\n        data = data + [max_fatalities]\n        data = data + list(cases[i][j:j+features_days]\/(max_cases+1))\n        data = data + list(fatalities[i][j:j+features_days]\/(max_fatalities+1))\n        train_data.append(data)\n\n        target = []\n        target_list = list(cases[i][j+features_days:j+features_days + test_days_predict]\/(max_cases+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list\n\n        target_list = list(fatalities[i][j+features_days:j+features_days + test_days_predict]\/(max_fatalities+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list           \n\n        target_data.append(target)\n        weights.append(weignts_cases[i]*weights_lambda**(train_days_used - test_days_predict - features_days - j - 1))\n    \n    test = []\n    test = test + [population[i]]\n    max_cases = cases[i][:train_days_used].max()\n    test = test + [max_cases]\n    max_fatalities = fatalities[i][:train_days_used].max()\n    test = test + [max_fatalities]\n    test = test + list(cases[i][train_days_used - features_days:train_days_used]\/(max_cases+1))\n    test = test + list(fatalities[i][train_days_used - features_days:train_days_used]\/(max_fatalities+1))\n    test_data.append(test)        \n\ninitial_train_data = np.stack(train_data)\ntarget_data = np.stack(target_data)\nmax_features_days = features_days\nmax_test_days_predict = test_days_predict\ninitial_test_data = np.stack(test_data)\nweights = np.array(weights)    ","7f0511dd":"i = 0\ntest = []\ntest = test + [population[i]]\nmax_cases = cases[i][:train_days_used].max()\ntest = test + [max_cases]\nmax_fatalities = fatalities[i][:train_days_used].max()\ntest = test + [max_fatalities]\ntest = test + list(cases[i][train_days_used - features_days:train_days_used]\/(max_cases+1))\ntest = test + list(fatalities[i][train_days_used - features_days:train_days_used]\/(max_fatalities+1))\ntest_data.append(test)   ","aec6b2cc":"predictions_cases_global = []\npredictions_cases_max_global = np.zeros((len(population), days_to_predict))\npredictions_cases_min_global = np.zeros((len(population), days_to_predict)) + 1000000\n\npredictions_fatalities_global = []\npredictions_fatalities_max_global = np.zeros((len(population), days_to_predict))\npredictions_fatalities_min_global = np.zeros((len(population), days_to_predict)) + 1000000","6d224f21":"for features_days in tqdm(range(5, 40, 5)):\n    fat_index_add = features_days\n    train_data = np.concatenate([initial_train_data[:,:3], \n                                 initial_train_data[:,3+max_features_days-features_days:3+max_features_days],\n                                 initial_train_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                                ], axis = 1).copy()\n    \n    case_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor()\n        mask = np.logical_not(pd.isnull(target_data[:,i]))\n        f.fit(train_data[mask], target_data[:,i][mask],sample_weight = weights[mask], verbose=False)\n        case_predictors.append(f)\n\n    fatalities_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor()\n        mask = np.logical_not(pd.isnull(target_data[:,max_test_days_predict + i]))\n        f.fit(train_data[mask], target_data[:,max_test_days_predict + i][mask],sample_weight = weights[mask], verbose=False)\n        fatalities_predictors.append(f)\n        \n    \n    for test_days_predict in range(1,32, 3):\n        \n        test_data = np.concatenate([initial_test_data[:,:3], \n                             initial_test_data[:,3+max_features_days-features_days:3+max_features_days],\n                             initial_test_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                            ], axis = 1).copy()\n        \n        predictions_cases_sum = np.zeros((len(population), days_to_predict))\n        predictions_cases_max = np.zeros((len(population), days_to_predict))\n        predictions_cases_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_cases_counts = np.zeros(days_to_predict)\n\n\n        predictions_fatalities_sum = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_max = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_fatalities_counts = np.zeros(days_to_predict)\n\n        for step in range(days_to_predict - test_days_predict + 1):\n            predictions_cases_local = np.zeros((len(population), test_days_predict))\n            predictions_fatalities_local = np.zeros((len(population), test_days_predict))\n\n            for i in range(test_days_predict):\n                predictions_cases_local[:,i] = case_predictors[i].predict(test_data) * (test_data[:,1] + 1)\n\n            for i in range(test_days_predict):\n                predictions_fatalities_local[:,i] = fatalities_predictors[i].predict(test_data) * (test_data[:,2] + 1)\n\n\n            predictions_cases_sum[:,step:step+test_days_predict] += predictions_cases_local\n            predictions_cases_max[:,step:step+test_days_predict] = np.maximum(predictions_cases_max[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_min[:,step:step+test_days_predict] = np.minimum(predictions_cases_min[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_counts[step:step+test_days_predict] += 1\n\n            current_predictions_cases = predictions_cases_sum[:,step] \/ predictions_cases_counts[step]\n\n\n            predictions_fatalities_sum[:,step:step+test_days_predict] += predictions_fatalities_local\n            predictions_fatalities_max[:,step:step+test_days_predict] = np.maximum(predictions_fatalities_max[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_min[:,step:step+test_days_predict] = np.minimum(predictions_fatalities_min[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_counts[step:step+test_days_predict] += 1\n\n            current_predictions_fatalities = predictions_fatalities_sum[:,step] \/ predictions_fatalities_counts[step]\n\n            test_data[:,3:3+features_days-1] = test_data[:,4:3+features_days]\n            new_max = np.maximum(test_data[:,1], current_predictions_cases)\n            test_data[:,3:3+features_days -1] *= ((test_data[:,1] + 1) \/ (new_max + 1)).reshape((-1,1))\n            test_data[:,2+features_days] = current_predictions_cases \/ (new_max+1)\n            test_data[:,1] = new_max\n\n\n\n            test_data[:,3+features_days:3+features_days-1+features_days] = test_data[:,4+features_days:3+features_days+features_days]\n            new_max = np.maximum(test_data[:,2], current_predictions_fatalities)\n            test_data[:,3+features_days:3+features_days+features_days-1] *= ((test_data[:,2] + 1) \/(new_max + 1) ).reshape((-1,1))\n            test_data[:,2+features_days+features_days] = current_predictions_fatalities \/ (new_max+1)\n            test_data[:,2] = new_max\n\n\n        predictions_cases_global.append(predictions_cases_sum \/ predictions_cases_counts)\n        predictions_cases_min_global = np.minimum(predictions_cases_min_global, predictions_cases_min)\n        predictions_cases_max_global = np.maximum(predictions_cases_max_global, predictions_cases_max)\n\n        predictions_fatalities_global.append(predictions_fatalities_sum \/ predictions_fatalities_counts)\n        predictions_fatalities_min_global = np.minimum(predictions_fatalities_min_global, predictions_fatalities_min)\n        predictions_fatalities_max_global = np.maximum(predictions_fatalities_max_global, predictions_fatalities_max)\n","67a9678a":"final_predictions_cases = np.stack(predictions_cases_global).mean(0)\nfinal_predictions_fatalities = np.stack(predictions_fatalities_global).mean(0)","7a47c3e7":"final_predictions_cases_std = np.stack(predictions_cases_global).std(0)\nfinal_predictions_fatalities_std = np.stack(predictions_fatalities_global).std(0)","4aacd45e":"valid_len = num_dates_train + days_to_predict - num_dates_total\nvalid_len","ef967935":"valid_true_cases = cases[:,-valid_len:]\nvalid_true_fatalities = fatalities[:, -valid_len:]","da2834dd":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","e1918d26":"def compute_loss_L(true_array, predicted_array, tau, weight):\n    array = predicted_array * (predicted_array > 0)\n    abs_diff = np.absolute(true_array - array)\n    result = abs_diff * (1 -tau) * (array > true_array) + abs_diff * (tau) * (array <= true_array)\n    result = (result.mean(1)) * weight\n#     print(result.mean())\n    return result.mean()","4a8bfd8e":"def compute_loss(true_array, mean_array, min_array, max_array, weights):\n    result = (compute_loss_L(true_array, max_array, 0.95, weights) + \n              compute_loss_L(true_array, min_array, 0.05, weights) + \n              compute_loss_L(true_array, mean_array, 0.5, weights))\n    return result \/ 3","afec1aeb":"x0 = [1,0,0,0]\n\ndef normalize(x, mean_array, min_array, max_array, base = None):\n    if base is None:\n        base = np.zeros_like(mean_array)\n        lamb = 0\n    else:\n        lamb = x[3]\n    deviation = np.array([lamb * n for n in range(base.shape[1])])\n    new_array = base + (x[0] * mean_array + x[1] * min_array + x[2]*max_array) * (deviation + 1).reshape((1,-1))\n#     print(deviation)\n    return new_array","0bf31066":"def fun(x, mean_array, min_array, max_array, true_array, weights, tau, base = None):\n    new_array = normalize(x, mean_array, min_array, max_array, base = base)\n    return compute_loss_L(true_array, new_array, tau, weights)","f5891220":"from scipy.optimize import minimize\nfrom functools import partial","1b76f3f6":"x = [ 1.05220987e+00,  1.99235423e-02, -1.52479300e-01,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.5, true_array = valid_true_cases)\n    res = minimize(part_func, x0 = [1,0,0,0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_cases = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global)\n\n\nx = [-0.57039983,  0.53805868, -0.29206055, -0.02496633]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.05, true_array = valid_true_cases, \n                        base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_min_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\n\nx = [0.68391658, -0.12920086,  0.1162962,   0.00088057]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.95, true_array = valid_true_cases,\n                       base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_max_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\nfinal_predictions_cases = new_final_predictions_cases\npredictions_cases_min_global = new_predictions_cases_min_global\npredictions_cases_max_global = new_predictions_cases_max_global","f495debd":"x = [ 0.62488135,  0.30215376, -0.10868626,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.5, true_array = valid_true_fatalities)\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_fatalities = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global)\n\n\nx = [-0.91401564,  0.79599391, -0.22610482, -0.03988415]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.05,\n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_min_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\n\nx = [0.87657726, -0.021317,   -0.04123832,  0.02195802]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.95, \n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_max_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\nfinal_predictions_fatalities = new_final_predictions_fatalities\npredictions_fatalities_min_global = new_predictions_fatalities_min_global\npredictions_fatalities_max_global = new_predictions_fatalities_max_global","1dc340cf":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","985ef694":"if train:\n    total_loss = (compute_loss(valid_true_cases, predict_mean_cases, predict_min_cases, \n                                   predict_max_cases , weignts_cases) + \n                     compute_loss(valid_true_fatalities, predict_mean_fatalities, predict_min_fatalities, \n                                  predict_max_fatalities, weignts_cases)) \/ 2\n\n    print(total_loss)","61e274cf":"submission_mean_cases = np.zeros((len(population), 45))\nsubmission_min_cases = np.zeros((len(population), 45))\nsubmission_max_cases = np.zeros((len(population), 45))\n\nsubmission_mean_fatalities = np.zeros((len(population), 45))\nsubmission_min_fatalities = np.zeros((len(population), 45))\nsubmission_max_fatalities = np.zeros((len(population), 45))","f16a6ef2":"submission_mean_cases[:, -days_to_predict:] = final_predictions_cases\nsubmission_min_cases[:, -days_to_predict:] = predictions_cases_min_global\nsubmission_max_cases[:, -days_to_predict:] = predictions_cases_max_global\n\n\nsubmission_mean_fatalities[:, -days_to_predict:] = final_predictions_fatalities \/ 10\nsubmission_min_fatalities[:, -days_to_predict:] = predictions_fatalities_min_global \/ 10\nsubmission_max_fatalities[:, -days_to_predict:] = predictions_fatalities_max_global \/ 10","696e3a58":"submission_mean_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_min_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_max_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\n\n\nsubmission_mean_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10\nsubmission_min_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10\nsubmission_max_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10 ","69ee9169":"loss_1 = compute_loss(fatalities[:,-14:] \/ 10, submission_mean_fatalities[:,:14] , submission_min_fatalities[:,:14], \n                                   submission_max_fatalities[:,:14],weignts_cases * 10)","30c5231b":"loss_2 = compute_loss(cases[:,-14:], submission_mean_cases[:,:14] , submission_min_cases[:,:14], \n                                   submission_max_cases[:,:14],weignts_cases)","8fbbd9bf":"loss_1","a10fd255":"loss_2","ac15ad7c":"(loss_1 + loss_2) \/ 2","92c3a20d":"#0.25734741800417726","e6ecca94":"submission_file = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/submission.csv')","bbbc9997":"submission_file","abf52b0f":"submission = []\nfor i in range(len(submission_mean_cases)):\n    for j in range(len(submission_mean_cases[0])):\n        submission.append(submission_min_cases[i][j])\n        submission.append(submission_mean_cases[i][j])\n        submission.append(submission_max_cases[i][j])\n        \n        submission.append(submission_min_fatalities[i][j])\n        submission.append(submission_mean_fatalities[i][j])\n        submission.append(submission_max_fatalities[i][j])","8c4cddfd":"submission = [max(0,x) for x in submission]","076d0df9":"submission_file['TargetValue'] = submission","f2a49147":"submission_file.to_csv('submission.csv', index = False)","8c216d0d":"submission_file","6de3cbc3":"import matplotlib.pyplot as plt\ndef plot_results(true_array, mean_array, max_array, min_array):\n    nans = np.array([None]*len(true_array))\n    plt.plot(true_array)\n    plt.plot(np.concatenate([nans,mean_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,min_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,max_array[-days_to_predict:]]))\n    \n    plt.show()","7ec63895":"for i in np.random.randint(0, len(cases),30):\n    plot_results(cases[i], submission_mean_cases[i], submission_max_cases[i], submission_min_cases[i])","b86bb37a":"for i in np.random.randint(0, len(cases),10):\n    plot_results(fatalities[i], submission_mean_fatalities[i], submission_max_fatalities[i], submission_min_fatalities[i])","4324a48f":"# EDA","1bc62b72":"# Preparing data","141173ed":"# Visualisation of predictions","5bf68df9":"# Making simple submission and valiadtion","be6fd90f":"# Making prediction file","2e458d43":"Quite messy notebook using mix of several lightgmb models predictions."}}