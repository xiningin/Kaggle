{"cell_type":{"f75fdc74":"code","16f3b12f":"code","44b94113":"code","d1b173b7":"code","1ac87a46":"code","84a168de":"code","fd7b3b82":"code","e4265481":"code","87e0ae28":"code","aaabac66":"code","162ca81d":"code","56ed8872":"code","f066187b":"code","6a9b035d":"code","1c271433":"code","f831d148":"code","d22eca00":"code","025330c8":"code","97cc3dbf":"code","94e34cf3":"code","b7f4536f":"code","798f5f7d":"code","2ead9249":"code","ab2757c7":"code","6db8bd05":"code","972fd5f3":"code","a5fb8d26":"code","8d10c945":"code","a1413389":"markdown","6140df92":"markdown","63ff4a5b":"markdown","4f8e15dd":"markdown","bcbe23a7":"markdown","1df4127a":"markdown","81d5ac62":"markdown","84304e4a":"markdown","406948ce":"markdown","b9b2dc78":"markdown","d7c6e883":"markdown","4e3cecfb":"markdown","bc2c5526":"markdown"},"source":{"f75fdc74":"\npip install nlpaug","16f3b12f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport nltk\nnltk.download('all')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\n\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport os\n","44b94113":"import nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as naf\n\nfrom nlpaug.util import Action","d1b173b7":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","1ac87a46":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","84a168de":"tweet=tweet.drop(['keyword','location'],axis=1)","fd7b3b82":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","e4265481":"# model_type: word2vec, glove or fasttext\naug_w2v = naw.WordEmbsAug(\n#     model_type='word2vec', model_path='..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin',\n    model_type='glove', model_path='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt',\n    action=\"substitute\")\n\n","87e0ae28":"text = tweet.iloc[0]['text']\ntext","aaabac66":"aug_w2v.aug_p=0.2\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_w2v.augment(text)\n    print(augmented_text)","162ca81d":"train,valid=train_test_split(tweet,test_size=0.15)\nprint('Shape of train',train.shape)\nprint(\"Shape of Validation \",valid.shape)","56ed8872":"from sklearn.utils import shuffle\n\ndef augment_text(df,samples=300,pr=0.2):\n    aug_w2v.aug_p=pr\n    new_text=[]\n    \n    ##dropping samples from validation\n    df_n=df[df.target==1].reset_index(drop=True)\n\n    ## data augmentation loop\n    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n        \n            text = df_n.iloc[i]['text']\n            augmented_text = aug_w2v.augment(text)\n            new_text.append(augmented_text)\n    \n    \n    ## dataframe\n    new=pd.DataFrame({'text':new_text,'target':1})\n    df=shuffle(df.append(new).reset_index(drop=True))\n    return df\n    \n    \n    \n    \n    \n    \n    ","f066187b":"train = augment_text(train,samples=400)   ## change samples to 0 for no augmentation\ntweet = train.append(valid).reset_index(drop=True)\n","6a9b035d":"df=pd.concat([tweet,test])","1c271433":"df.shape","f831d148":"\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n        \n        ","d22eca00":"corpus=create_corpus(df)","025330c8":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","97cc3dbf":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","94e34cf3":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","b7f4536f":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","798f5f7d":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","2ead9249":"model.summary()","ab2757c7":"train_df=tweet_pad[:tweet.shape[0]]\ntest_df=tweet_pad[tweet.shape[0]:]","6db8bd05":"X_train,y_train = train_df[:train.shape[0]],tweet['target'][:train.shape[0]]\n\nX_test,y_test= train_df[train.shape[0]:],tweet['target'][train.shape[0]:]\n","972fd5f3":"history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","a5fb8d26":"y_pre=model.predict(X_test)\ny_pre=np.round(y_pre).astype(int).reshape(1142)\n","8d10c945":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_pre,y_test))","a1413389":"## Loading the data and getting basic idea ","6140df92":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.","63ff4a5b":"## GloVe for Vectorization","4f8e15dd":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.\nThis pipeline is described in [this](https:\/\/neptune.ai\/blog\/document-classification-small-datasets) article.","bcbe23a7":"# Evaluation","1df4127a":"# Downloading required data","81d5ac62":"# ***IF YOU LOVE MY NOTEBOOK PLEASE FREE TO UPVOTE***","84304e4a":"# Train the model","406948ce":"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","b9b2dc78":"# Data Augmentation","d7c6e883":"# Importing packages","4e3cecfb":"## Baseline Model","bc2c5526":"## Class distribution"}}