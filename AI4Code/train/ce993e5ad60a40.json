{"cell_type":{"46cfa376":"code","6ad36d33":"code","e8d2b1c5":"code","19f15b2f":"code","3a24c6f9":"code","848bcbbf":"code","b3c5df73":"code","3dbc8b02":"code","4fc470a7":"code","59528ca2":"code","8d26b6d9":"code","557cb602":"code","febee9a9":"code","5b965825":"code","50efcb50":"code","3384aa4f":"code","b0a15f8e":"code","44aecbec":"code","9392db5f":"code","1dc33b04":"code","1c37bca8":"code","afe638cc":"code","df00059c":"code","e5570ade":"code","264a8847":"code","0e98c92c":"code","34509f2b":"code","d0ddf059":"code","3b93b3fa":"code","bb3f7078":"code","eea042f9":"code","0cba2cd3":"code","5a128a8b":"code","10bb5e8b":"code","c503f7fc":"code","39eac821":"code","1f564acc":"markdown","a03db967":"markdown","737d1c74":"markdown","e0f2f254":"markdown","0e3dff66":"markdown","e4d241f9":"markdown","32f16d89":"markdown","4ee4cc41":"markdown","799103fb":"markdown","1bf1637b":"markdown","eb904f8e":"markdown","dab4f030":"markdown","fd9fb702":"markdown","4d4fc1f8":"markdown"},"source":{"46cfa376":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (8,5)","6ad36d33":"data=pd.read_csv('..\/input\/anonymous-data\/or_data.csv')","e8d2b1c5":"data.shape","19f15b2f":"data.head()","3a24c6f9":"data.tail()","848bcbbf":"# Here we get rid of the last two rows of data.\ndata=data[:-2]","b3c5df73":"data.tail()","3dbc8b02":"data.info()","4fc470a7":"data=data.replace('[^\\d.]' , '' , regex=True ).astype(float)","59528ca2":"data.info()","8d26b6d9":"# Here I get the number of values in each of our classes.\ndata[\"Y\"].value_counts()","557cb602":"matplotlib.rcParams['figure.figsize'] = (5,5)\nsns.countplot(data[\"Y\"])\nplt.title(\"Clarify the balance of the data\")\nplt.show();","febee9a9":"cols =data.columns\ncolours = [ '#ffff00','#000099'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(data[cols].isnull(), cmap=sns.color_palette(colours))","5b965825":"print('data shape befor remove duplicate = ' , data.shape)","50efcb50":"data.duplicated().sum()","3384aa4f":"data = data.drop('YEAR', axis=1).drop_duplicates()\n\nprint('data shape after removing duplicate = ' , data.shape)","b0a15f8e":"# Separate the target column from the rest of the data\nfrom sklearn.model_selection import train_test_split\ntarget=data[\"Y\"]\nfeatures=data.drop([\"Y\"],axis=1)","44aecbec":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(features)","9392db5f":"new_features = scaler.transform(features)\nprint(new_features)","1dc33b04":"print(new_features.shape)","1c37bca8":"from sklearn import preprocessing \nlab_enc = preprocessing.LabelEncoder()\ny = lab_enc.fit_transform(target)","afe638cc":"print(y.shape)","df00059c":"# Complete the data separation. \nx_train,x_test,y_train,y_test=train_test_split(new_features,target,test_size=0.1,random_state=0)","e5570ade":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","264a8847":"pip install lazypredict","0e98c92c":"from lazypredict.Supervised import LazyClassifier","34509f2b":"model = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = model.fit(x_train, x_test, y_train, y_test)","d0ddf059":"print(predictions)","3b93b3fa":"from sklearn.ensemble import ExtraTreesClassifier","bb3f7078":"model = ExtraTreesClassifier(n_estimators=50, random_state=0, min_samples_leaf=1,max_features='log2',bootstrap=True, oob_score=True)","eea042f9":"model.fit(x_train, y_train)","0cba2cd3":"print('ExtraTreesClassifiermodel Train Score is : ' , model.score(x_train, y_train))","5a128a8b":"print('ExtraTreesClassifiermodel Test Score is : ' , model.score(x_test, y_test))\nprint('ExtraTreesClassifiermodel features importances are : ' , model.feature_importances_)","10bb5e8b":"#Calculating Prediction\ny_predict = model.predict(x_test)\ny_predict_prob = model.predict_proba(x_test)\nprint('Predicted Value for ExtraTreesClassifiermodel is : ' , y_predict[:10])\nprint('the real values which we want to predict    is :\\n' , y_test[:10])\nprint('Prediction Probabilities Value for ExtraTreesClassifiermodel is : ' , y_predict_prob[:10])","c503f7fc":"#Calculating Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nconfusion_matrix(y_test,y_predict)","39eac821":"plot_confusion_matrix(model,x_test,y_test);","1f564acc":"The problem of unbalanced data is a real problem, but we do not have a reference to the source of that data in order to work on increasing the data and reducing the imbalance between the data categories. We do this step because the data has limits and must be taken into account because if we do not do this we will get a misleading algorithm.","a03db967":"# reading data","737d1c74":"# ExtraTreesClassifier Algorithm","e0f2f254":"This is very bad the data is unbalanced and this will affect the algorithm that we will definitely use.","0e3dff66":"# missing values","e4d241f9":"# Data Scaling","32f16d89":"The data here has an error in the data type, so we must convert the data to the float type so that we do not have a problem while scaling.","4ee4cc41":"# Quick measurement of the performance of classification algorithms","799103fb":"# We get rid of duplicate data","1bf1637b":"**I think I'm almost done, as you notice, the model that we built suffers from an over-processing problem, and that problem and the reason for it I explained above, but don't worry, I will update that kernel as soon as possible and the new update will include solving these problems.**","eb904f8e":"**The model that we built and trained has a problem which is overfitting, so to solve this problem we have to increase the data from the original data source so that there is homogeneity in the data and thus it is easy to form or discover a pattern in the data, because the good and the bad in machine learning algorithms is that The data didn't have a pattern he would create and that's a problem.**\n\nSo we have to increase the amount of data, and the data also suffers from some defects from the point of view of the one based on building the algorithm and processing that data, which is the large number of zeros in the data. This will cause a defect, but we worked to reduce this defect by scaling the data, and this may not be a defect but rather it The correct values \u200b\u200band this is also a problem because we do not have a reference to the basis of that data until we know or get answers about the questions we face during data processing. ","dab4f030":"# Thank you, and I hope you will show us your experience in a new kernel.","fd9fb702":"# Calling some important libraries","4d4fc1f8":"There is nothing on the heat map that shows just any data. So we will move on to the next step."}}