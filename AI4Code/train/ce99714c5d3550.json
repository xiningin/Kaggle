{"cell_type":{"5d0a8a05":"code","156b87d0":"code","faa673cf":"code","c49232c5":"code","a8d876bf":"code","0448a7ea":"code","a7d48497":"code","98168ba4":"code","d1a7a771":"code","a827aa6f":"code","a47393b9":"code","af3d235a":"code","98632484":"code","d9c18330":"code","c74c1c00":"code","1f6f08b6":"code","fcf6431b":"code","9e9fc601":"markdown","3ae78282":"markdown","83f9e058":"markdown","16f85033":"markdown","37354e83":"markdown","fb709faf":"markdown","47aad6c5":"markdown","aafecb8f":"markdown","b8e759a8":"markdown","54bea798":"markdown"},"source":{"5d0a8a05":"import datetime\nimport uuid\n\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport sys\nimport subprocess\n\nfrom tensorflow.keras import datasets, layers, models\nfrom sklearn.model_selection import train_test_split\n\n# If in Colab install tensorflow_cloud\nif \"google.colab\" in sys.modules and not os.environ.get(\n    \"TF_KERAS_RUNNING_REMOTELY\"):\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'tensorflow-cloud', '-q'])\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'google-cloud-storage', '-q'])\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'fsspec', '-q'])\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'gcsfs', '-q'])\n\nimport tensorflow_cloud as tfc\nprint(tfc.__version__)","156b87d0":"# Set Google Cloud Specific parameters\n\n# TODO: Please set GCP_PROJECT_ID to your own Google Cloud project ID.\nGCP_PROJECT_ID = 'YOUR_PROJECT_ID'  #@param {type:\"string\"}\n\n# TODO: Change the Service Account Name to your own Service Account\nSERVICE_ACCOUNT_NAME = 'YOUR_SERVICE_ACCOUNT_NAME' #@param {type:\"string\"}\nSERVICE_ACCOUNT = f'{SERVICE_ACCOUNT_NAME}@{GCP_PROJECT_ID}.iam.gserviceaccount.com'\n\n# TODO: set GCS_BUCKET to your own Google Cloud Storage (GCS) bucket.\nGCS_BUCKET = 'YOUR_GCS_BUCKET_NAME' #@param {type:\"string\"}\n\n# DO NOT CHANGE: Currently only the 'us-central1' region is supported.\nREGION = 'us-central1'","faa673cf":"# Set Tuning Specific parameters\n\n# OPTIONAL: You can change the project name to any string.\nJOB_NAME = 'wide_and_deep' #@param {type:\"string\"}\n\n# OPTIONAL:  Set Number of concurrent tuning jobs that you would like to run.\nNUM_JOBS = 5 #@param {type:\"string\"}\n\n# TODO: Set the study ID for this run. Study_ID can be any unique string.\n# Reusing the same Study_ID will cause the Tuner to continue tuning the\n# Same Study parameters. This can be used to continue on a terminated job,\n# or load stats from a previous study.\n# Note Study ID MUST NOT be auto generated for example using random or time,\n# as at run time each job will rerun this code and will create a separate study.\nSTUDY_NUMBER = '00001' #@param {type:\"string\"}\nSTUDY_ID = f'{GCP_PROJECT_ID}_{JOB_NAME}_{STUDY_NUMBER}'\n\n# Setting location were training logs and checkpoints will be stored\nGCS_BASE_PATH = f'gs:\/\/{GCS_BUCKET}\/{JOB_NAME}\/{STUDY_ID}'\nTENSORBOARD_LOGS_DIR = os.path.join(GCS_BASE_PATH,\"logs\")","c49232c5":"# Using tfc.remote() to ensure this code only runs in notebook\nif not tfc.remote():\n\n    # Authentication for Kaggle Notebooks\n    if \"kaggle_secrets\" in sys.modules:\n        from kaggle_secrets import UserSecretsClient\n        UserSecretsClient().set_gcloud_credentials(project=GCP_PROJECT_ID)\n\n    # Authentication for Colab Notebooks\n    if \"google.colab\" in sys.modules:\n        from google.colab import auth\n        auth.authenticate_user()\n        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID","a8d876bf":"train_URL = f'{GCS_BASE_PATH}\/caiis-dogfood-day-2020\/train.csv'\ndata = pd.read_csv(train_URL)\ntrain, test = train_test_split(data, test_size=0.1)","0448a7ea":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(df, shuffle=True, batch_size=32):\n  df = df.copy()\n  labels = df.pop('target')\n  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(df))\n  ds = ds.batch(batch_size)\n  return ds","a7d48497":"sm_batch_size = 1000  # A small batch size is used for demonstration purposes\ntrain_ds = df_to_dataset(train, batch_size=sm_batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=sm_batch_size)","98168ba4":"from tensorflow.keras.layers.experimental import preprocessing\n\ndef create_model_inputs():\n    inputs ={}\n    for name, column in data.items():\n        if name in ('id','target'):\n            continue\n        dtype = column.dtype\n        if dtype == object:\n            dtype = tf.string\n        else:\n            dtype = tf.float32\n\n        inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n\n    return inputs","d1a7a771":"#Preprocessing the numeric inputs, and runing them through a normalization layer.\ndef preprocess_numeric_inputs(inputs):\n\n    numeric_inputs = {name:input for name,input in inputs.items()\n                      if input.dtype==tf.float32}\n\n    x = layers.Concatenate()(list(numeric_inputs.values()))\n    norm = preprocessing.Normalization()\n    norm.adapt(np.array(data[numeric_inputs.keys()]))\n    numeric_inputs = norm(x)\n    return numeric_inputs","a827aa6f":"# Preprocessing the categorical inputs.\ndef preprocess_categorical_inputs(inputs):\n    categorical_inputs = []\n    for name, input in inputs.items():\n        if input.dtype == tf.float32:\n            continue\n\n        lookup = preprocessing.StringLookup(vocabulary=np.unique(data[name]))\n        one_hot = preprocessing.CategoryEncoding(max_tokens=lookup.vocab_size())\n\n        x = lookup(input)\n        x = one_hot(x)\n        categorical_inputs.append(x)\n\n    return layers.concatenate(categorical_inputs)","a47393b9":"import kerastuner\n\n# Configure the search space\nHPS = kerastuner.engine.hyperparameters.HyperParameters()\nHPS.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n\nHPS.Int('num_layers', min_value=2, max_value=5)\nfor i in range(5):\n    HPS.Float('dropout_rate_' + str(i), min_value=0.0, max_value=0.3, step=0.1)\n    HPS.Choice('num_units_' + str(i), [32, 64, 128, 256])","af3d235a":"from tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef create_wide_and_deep_model(hp):\n\n    inputs = create_model_inputs()\n    wide = preprocess_categorical_inputs(inputs)\n    wide = layers.BatchNormalization()(wide)\n\n    deep = preprocess_numeric_inputs(inputs)\n    for i in range(hp.get('num_layers')):\n        deep = layers.Dense(hp.get('num_units_' + str(i)))(deep)\n        deep = layers.BatchNormalization()(deep)\n        deep = layers.ReLU()(deep)\n        deep = layers.Dropout(hp.get('dropout_rate_' + str(i)))(deep)\n\n    both = layers.concatenate([wide, deep])\n    outputs = layers.Dense(1, activation='sigmoid')(both)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    metrics = [\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        'accuracy',\n        'mse'\n    ]\n\n    model.compile(\n        optimizer=Adam(lr=hp.get('learning_rate')),\n        loss='binary_crossentropy',\n        metrics=metrics)\n    return model","98632484":"from tensorflow_cloud import CloudTuner\n\ntuner = CloudTuner(\n    create_wide_and_deep_model,\n    project_id=GCP_PROJECT_ID,\n    project_name=JOB_NAME,\n    region=REGION,\n    objective='accuracy',\n    hyperparameters=HPS,\n    max_trials=100,\n    directory=GCS_BASE_PATH,\n    study_id=STUDY_ID,\n    overwrite=False,\n    distribution_strategy=None)","d9c18330":"# Configure Tensorboard logs\ncallbacks=[\n    tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_LOGS_DIR)]\n\n# Setting to run tuning remotely, you can run tuner locally to validate it works first.\nif tfc.remote():\n    tuner.search(train_ds, epochs=20, validation_data=test_ds, callbacks=callbacks)","c74c1c00":"if not tfc.remote():\n    print('Training on Google Cloud AI Platform through TensorFlow Cloud.')\n\n    tfc.run_cloudtuner(\n        distribution_strategy='auto',\n        docker_config=tfc.DockerConfig(\n            image_build_bucket=GCS_BUCKET\n            ),\n        chief_config=tfc.MachineConfig(\n            cpu_cores=16,\n            memory=60,\n        ),\n        job_labels={'job': JOB_NAME},\n        service_account=SERVICE_ACCOUNT,\n        num_jobs=NUM_JOBS\n    )","1f6f08b6":"# %load_ext tensorboard\n# %tensorboard --logdir TENSORBOARD_LOGS_DIR","fcf6431b":"if not tfc.remote():\n    tuner.results_summary(1)\n    best_model = tuner.get_best_models(1)[0]\n    best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\n    # References to best trial assets\n    best_trial_id = tuner.oracle.get_best_trials(1)[0].trial_id\n    best_trial_dir = tuner.get_trial_dir(best_trial_id)","9e9fc601":"# Training Results\nWhile the training is in progress you can use Tensorboard to view the results.","3ae78282":"## Preprocess the data\n\nSetting up preprocessing layers for categorical and numerical input data. For more details on preprocessing layers please refer to [working with preprocessing layers](https:\/\/www.tensorflow.org\/guide\/keras\/preprocessing_layers).\n\n","83f9e058":"# Tuning a wide and Deep model using Google Cloud\n\nIn this example we will use CloudTuner and Google Cloud to Tune a [Wide and Deep Model](https:\/\/ai.googleblog.com\/2016\/06\/wide-deep-learning-better-together-with.html) based on the tunable model introduced in [structured data learning with Wide, Deep, and Cross networks](https:\/\/keras.io\/examples\/structured_data\/wide_deep_cross_networks\/). In this example we will use the data set from [CAIIS Dogfood Day](https:\/\/www.kaggle.com\/c\/caiis-dogfood-day-2020\/overview)\n\n<table align=\"left\">\n    <td>\n        <a href=\"https:\/\/colab.research.google.com\/github\/tensorflow\/cloud\/blob\/master\/examples\/hp_tuning_wide_and_deep_model.ipynb\">\n            <img width=\"50\" src=\"https:\/\/cloud.google.com\/ml-engine\/images\/colab-logo-32px.png\" alt=\"Colab logo\">Run in Colab\n        <\/a>\n    <\/td>\n    <td>\n        <a href=\"https:\/\/github.com\/tensorflow\/cloud\/blob\/master\/examples\/hp_tuning_wide_and_deep_model.ipynb\">\n            <img src=\"https:\/\/cloud.google.com\/ml-engine\/images\/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n        <\/a>\n     <\/td>\n    <td>\n        <a href=\"https:\/\/www.kaggle.com\/nitric\/hp-tuning-wide-and-deep-model\">\n            <img width=\"90\" src=\"https:\/\/www.kaggle.com\/static\/images\/site-logo.png\" alt=\"Kaggle logo\">Run in Kaggle\n        <\/a>\n     <\/td>\n<\/table>","16f85033":"## Load the data\nRead raw data and split to train and test data sets. For this step you will need to copy the dataset to your GCS bucket so it can be accessed during training. For this example we are using the dataset from https:\/\/www.kaggle.com\/c\/caiis-dogfood-day-2020.\n\nTo do this you can run the following commands to download and copy the dataset to your GCS bucket, or manually download the dataset vi [Kaggle UI](https:\/\/www.kaggle.com\/c\/caiis-dogfood-day-2020\/data) and upload the `train.csv` file to your [GCS bucket vi GCS UI](https:\/\/console.cloud.google.com\/storage\/browser).\n\n```python\n# Download the dataset\n!kaggle competitions download -c caiis-dogfood-day-2020\n\n# Copy the training file to your bucket\n!gsutil cp .\/caiis-dogfood-day-2020\/train.csv $GCS_BASE_PATH\/caiis-dogfood-day-2020\/train.csv\n```","37354e83":"## Authenticating the notebook to use your Google Cloud Project\n\nFor Kaggle Notebooks click on \"Add-ons\"->\"Google Cloud SDK\" before running the cell below.","fb709faf":"## Start the remote training\n\nThis step will prepare your code from this notebook for remote execution and start NUM_JOBS parallel runs remotely to train the model. Once the jobs are submitted you can go to the next step to monitor the jobs progress via Tensorboard.","47aad6c5":"## Define the model architecture and hyperparameters\nNow we will define the model structure and the hyperparameters.","aafecb8f":"## Configure a CloudTuner\nIn this section we configure the CloudTuner for both remote and local execution.","b8e759a8":"You can access the training assets as follows:","54bea798":"Setting project parameters. For more details on Google Cloud Specific parameters please refer to [Google Cloud Project Setup Instructions](https:\/\/www.kaggle.com\/nitric\/google-cloud-project-setup-instructions\/)."}}