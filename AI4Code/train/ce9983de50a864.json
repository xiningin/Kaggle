{"cell_type":{"16c3ba09":"code","233033b9":"code","dd8b98eb":"code","240540d0":"code","addd5a2e":"code","99aea577":"markdown"},"source":{"16c3ba09":"# importing all the DL heavenly deities before we begin our code \nimport torch\n# For building a custom dataset and dataloader\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport cv2\nimport albumentations\nimport pandas as pd\nimport numpy as np\n# For building a custom CNN\nimport torch.nn as nn\nimport torch.nn.functional as F","233033b9":"class args:\n    batch_size = 16\n    image_size = 224\n\n\ntrain_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in train_df[\"Id\"].values]\n\nclass args:\n    batch_size = 16\n    image_size = 384\n    \ntrain_aug = albumentations.Compose(\n    [\n        albumentations.Resize(args.image_size, args.image_size, p=1),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)\n\nclass PawPularityDataset(Dataset):\n    def __init__(self,image_paths,train_df,augmentations):\n        self.image_paths = image_paths\n        self.train_df = train_df\n        self.features = train_df.columns[1:-1]\n        self.augmentations = augmentations\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self,idx):\n        image = cv2.imread(self.image_paths[idx])\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        \n        target = train_df['Pawpularity'][idx]\n        \n        features = list(train_df.loc[0])[1:-1]\n        \n        return (\n            torch.tensor(image,dtype = torch.float32),\n            torch.tensor(target,dtype = torch.float32),\n            torch.tensor(features,dtype = torch.float32)\n        )\n\n\ntrain_loader = torch.utils.data.DataLoader(PawPularityDataset(train_img_paths,train_df,train_aug), batch_size = args.batch_size, shuffle=True)","dd8b98eb":"\nclass block(nn.Module):\n    def __init__(self,in_channels,out_channels,identity_downsample = None, stride = 1):\n        super(block,self).__init__()\n\n        self.expansion = 4\n        self.identity_downsample  = identity_downsample\n\n        self.convblock1 = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels,kernel_size = 1, stride = 1, padding =0),\n                nn.BatchNorm2d(out_channels),\n                nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding =1),\n                nn.BatchNorm2d(out_channels),\n                nn.Conv2d(out_channels, self.expansion*out_channels, kernel_size =1,stride = 1, padding = 0),\n                nn.BatchNorm2d(out_channels*self.expansion)\n        )\n\n    def forward(self,x):\n        identity = x\n\n        x = self.convblock1(x)\n\n        if self.identity_downsample is not None :\n            identity = self.identity_downsample(identity)\n        \n        x += identity\n        x = F.relu(x)\n        return x\n    \nclass ResNet(nn.Module):\n    def __init__(self, block, layers, image_channels, num_classes):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Essentially the entire ResNet architecture are in these 4 lines below\n        self.layer1 = self._make_layer(\n            block, layers[0], intermediate_channels=64, stride=1\n        )\n        self.layer2 = self._make_layer(\n            block, layers[1], intermediate_channels=128, stride=2\n        )\n        self.layer3 = self._make_layer(\n            block, layers[2], intermediate_channels=256, stride=2\n        )\n        self.layer4 = self._make_layer(\n            block, layers[3], intermediate_channels=512, stride=2\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(in_features = 512*4, out_features = 1024)\n        self.Regressor = nn.Sequential(                              #layer where we concatenate the metadata of the image\n            nn.Linear(in_features = 1024 + 12, out_features = 2000),\n            nn.Linear(in_features = 2000, out_features = 1000),\n            nn.Linear(in_features = 1000, out_features = 500),\n            nn.Linear(in_features = 500, out_features = 100),\n            nn.Linear(in_features = 100, out_features = 1)\n        )\n\n    def forward(self, x, x2):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)              # output shape = torch.Size([batch_size, 2048, 1, 1])\n        x = x.reshape(x.shape[0], -1)    # output shape = torch.Size([batch_size, 2048])\n        x = self.fc(x)                   # output shape = torch.Size([batch_size, 1024])\n        x = x.view(-1,1024)              # output shape = torch.Size([batch_size, 1024])\n        x = torch.cat((x,x2),axis = 1)   # output shape = torch.Size([batch_size, 1036])\n        x = self.Regressor(x)      # output shape = torch.Size([batch_size, 1])\n        x = x.view(-1,1)                 # output shape = torch.Size([batch_size, 1])\n\n        return x\n\n    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n        identity_downsample = None\n        layers = []\n\n        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n        # we need to adapt the Identity (skip connection) so it will be able to be added\n        # to the layer that's ahead\n        if stride != 1 or self.in_channels != intermediate_channels * 4:\n            identity_downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.in_channels,\n                    intermediate_channels * 4,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False\n                ),\n                nn.BatchNorm2d(intermediate_channels * 4),\n            )\n\n        layers.append(\n            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n        )\n\n        # The expansion size is always 4 for ResNet 50,101,152\n        self.in_channels = intermediate_channels * 4\n\n        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n        # and also same amount of channels.\n        for i in range(num_residual_blocks - 1):\n            layers.append(block(self.in_channels, intermediate_channels))\n\n        return nn.Sequential(*layers)\n\n\ndef ResNet50(img_channel=3, num_classes=1000):\n    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n\n\ndef ResNet101(img_channel=3, num_classes=1000):\n    return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n\n\ndef ResNet152(img_channel=3, num_classes=1000):\n    return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)\n\n\ndef test():\n    net = ResNet101(img_channel=3, num_classes=1000)\n    y = net(torch.randn(4, 3, 224, 224)).to(\"cuda\")\n    print(y.size())","240540d0":"tr = next(iter(train_loader))\ntr[0]","addd5a2e":"model = ResNet152()\noutput = model(tr[0],tr[2])\nprint(output)\noutput.shape","99aea577":"# Building a Custom ResNet for PawPularity predictions\n\nI found it difficult to finetune a predefined ResNet(PyTorch) as we have to include the Metadata in our training as well. It looked as if we would have to overwrite the **forward(self,x)** method itself to accept two inputs i.e. **forward(self,x1,x2)**. That is exactly what I have done in this code.\n\nSources -\n1. [Abhishek's notebook](https:\/\/www.kaggle.com\/abhishek\/tez-pawpular-swin-ference)\n2. [Youtube](https:\/\/www.youtube.com\/watch?v=qaDe0qQZ5AQ)"}}