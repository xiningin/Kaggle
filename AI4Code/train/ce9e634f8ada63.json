{"cell_type":{"c05ba6f2":"code","78e63cd1":"code","00ce7c7e":"code","e4cabc1c":"code","e13c77a4":"markdown","4b451a40":"markdown","311218c0":"markdown","1fe353b2":"markdown"},"source":{"c05ba6f2":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport tensorflow as tf\n\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Conv2D, Dense, Flatten, PReLU, Activation, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.initializers import glorot_normal, glorot_uniform, he_normal, he_uniform, Constant","78e63cd1":"def preprocess_dataset(dataset):\n    '''The function converts pandas DataFrame to numpy array, reshapes data and scales values between 0 and 1'''\n    dataset = (dataset.to_numpy().reshape(-1, 28, 28, 1) \/ 255.0).astype(np.float32)\n    return dataset\n\n# Loading train, test datasets and submission file\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsample = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\n# Train labels\nY = train_df['label']\nY = to_categorical(Y, num_classes = 10)\ntrain_df.drop('label', axis = 1, inplace = True)\n\n# Converting and scaling train \/ test datasets\ntrain_df = preprocess_dataset(train_df)\ntest_df = preprocess_dataset(test_df)","00ce7c7e":"def learning_curves(m, title):\n    '''The function plots learning curves of specific model \"m\"'''\n    H = m.history.history\n    plt.plot(H['accuracy'], label = 'acc')\n    plt.plot(H['val_accuracy'], label = 'val_acc')\n    plt.plot(H['loss'], label = 'loss')\n    plt.plot(H['val_loss'], label = 'val_loss')\n    plt.title(f'{title} learning curves')\n    plt.grid(); plt.legend()\n    \ndef plot_weights(title):\n    '''Plot weights of model layers'''\n    for layer in model.layers:\n        weights = layer.get_weights()\n        if weights:\n            ax.hist(weights[0].flatten(), bins = 100, label = layer.name, alpha = 0.5)\n        plt.legend()\n        plt.title(title)\n    plt.yscale('log')","e4cabc1c":"seed = 666\n\n# Defining glorot and he distributions\ng_norm = glorot_normal(seed = seed)\ng_unif = glorot_uniform(seed = seed)\nhe_norm = he_normal(seed = seed)\nhe_unif = he_uniform(seed = seed)\n\n# Creating combinations of activation\/distribution\nmodels = {\n         'ReLU-glorot_normal': ('relu', g_norm),\n         'ReLU-glorot_uniform': ('relu', g_unif),\n         'ReLU-he_normal': ('relu', he_norm),\n         'ReLU-he_uniform': ('relu', he_unif),\n         'PReLU-glorot_normal': ('prelu', g_norm),\n         'PReLU-glorot_uniform': ('prelu', g_unif),\n         'PReLU-he_normal': ('prelu', he_norm),\n         'PReLU-he_uniform': ('prelu', he_unif)\n         }\n\n# Loop through all models\nfor m in models:\n    init = models[m][1] # Weights initializer\n    act = models[m][0] # Activation\n    \n    model = Sequential()\n    model.add(Conv2D(32, 3, input_shape = (28, 28, 1), kernel_initializer = init, padding = 'same'))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(Conv2D(32, 3, kernel_initializer = init, padding = 'same'))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(MaxPooling2D())\n\n    model.add(Conv2D(64, 3, kernel_initializer = init, padding = 'same'))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(Conv2D(64, 3, kernel_initializer = init, padding = 'same'))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(MaxPooling2D())\n\n    model.add(Flatten())\n    model.add(Dense(128, kernel_initializer = init))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(Dense(100, kernel_initializer = init))\n    model.add(Activation('relu')) if act == 'relu' else model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n    model.add(Dense(10, activation = 'softmax', kernel_initializer = init))\n\n    model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    print('_'*20, m, '_'*20) # Print model name to separate plots\n    \n    # Creating plot of layers weights before train process\n    fig = plt.figure(figsize = (19, 12))\n    gs = gridspec.GridSpec(2, 2)\n    ax = fig.add_subplot(gs[0, 0])\n    plot_weights(f'{m} initial weights')\n    \n    # Training model\n    model.fit(train_df, Y, epochs = 20, batch_size = 64, validation_split = 0.1, verbose = 0)\n    \n    # Predicting test data and creating submission file\n    preds = model.predict(test_df)\n    preds = np.argmax(preds, axis = 1)\n    sample['Label'] = preds\n    sample.to_csv(f'{m}.csv', index = False)\n    \n    # Creating plot of layers weights after training process\n    ax = fig.add_subplot(gs[0, 1])\n    plot_weights(f'{m} learned weights')    \n    \n    # Plot learning curves\n    ax = fig.add_subplot(gs[1, :])\n    learning_curves(model, m)\n    plt.show()","e13c77a4":"### Goal\n\nLayers weights initialization is a very important aspect of deep learning that can improve or decrease perfomance of your model. In this short kernel I want to run an experiment about using Glorot (Xavier) and He distributions for layer weights initialization and it's influence on training process with ReLU and PReLU activations.\n\nTo be more concrete - I want to use glorot\/he normal and glorot\/he uniform distributions and ReLU\/PReLU activations, so in total I will get 8 models to train: \n* 'ReLU-glorot_normal\n* 'ReLU-glorot_uniform\n* 'ReLU-he_normal'\n* 'ReLU-he_uniform'\n* 'PReLU-glorot_normal'\n* 'PReLU-glorot_uniform'\n* 'PReLU-he_normal':\n* 'PReLU-he_uniform'\n\nLet's start coding:\n","4b451a40":"### Model creation and training\n\nHere I will use simple VGG-like architecture with 6 hidden layers. There will be 8 models in total, each model will be trained on 20 epochs and after that - predictions on test data will be made and submitted.","311218c0":"### Results\n\nHe distribution was designed specifically for ReLU activation, which is non differentiable at 0 and, as we can see from these plots - the  models, that use he normal\/uniform distribution converges faster and overall training process seems to be more stable than models that use glorot distribution for weights initialization.\n\nWhen I submitted predictions on test dataset, I got next results:\n* ReLU-he_uniform      - 0.98928\n* PReLU-he_normal      - 0.98885\n* ReLU-he_normal       - 0.98785\n* PReLU-he_uniform     - 0.98728\n* ReLU-glorot_normal   - 0.98700\n* PReLU-glorot_uniform - 0.98657\n* PReLU-glorot_normal  - 0.98585\n* ReLU-glorot_uniform  - 0.98471\n\nThe models, that use he distribution for weights initialization gave us slightly better score than glorot, but it can be critical for kaggle competitions. Also looks like standard ReLU is more prefferable on such simple networks.","1fe353b2":"### Data loading and preprocessing\n\nIn this experiment I want to keep things simple - I'm not going to use a data augmentation, because I want to reduce randomness of the experiment to minimum. "}}