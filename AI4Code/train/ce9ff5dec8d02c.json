{"cell_type":{"3e57b87a":"code","5fc2d180":"code","ef3bdd1d":"code","641b3928":"code","8666b7e9":"code","a3fd76cb":"code","7e967378":"code","cc343134":"code","401dabe5":"code","bfaae1cc":"code","7cc8bf2c":"code","ecc3d33a":"markdown","4986cf2f":"markdown","3c6af803":"markdown","a14b1b9c":"markdown","26b2530d":"markdown","be3a1cb3":"markdown","36b693b0":"markdown","61e01e8a":"markdown","89ab70db":"markdown","d6d59533":"markdown","d3bf68f8":"markdown","85ea418f":"markdown","7cb9c742":"markdown","52af1de8":"markdown"},"source":{"3e57b87a":"import gc\nimport os\nimport time\nimport random\nimport datetime\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GridSearchCV, cross_val_score","5fc2d180":"%matplotlib inline\npd.options.display.precision = 15\nrandom.seed(6) #totally random seed (got from a dice)","ef3bdd1d":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\nfs = 4000000 #sampling frequency of the sensor signal","641b3928":"print(f'Train: rows:{train.shape[0]} cols:{train.shape[1]}')","8666b7e9":"train_acoustic_data_small = train['acoustic_data'].values[::50]\ntrain_time_to_failure_small = train['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title('Acoustic_data and time_to_failure (sampled)')\nplt.plot(train_acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_small, color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel train_acoustic_data_small\ndel train_time_to_failure_small","a3fd76cb":"plt.figure()\nplt.hist(train['time_to_failure'].values[::50], bins='auto', density=True)  # arguments are passed to np.histogram\nplt.title('Histogram of time_to_failure')\nplt.xlabel('time_to_failure')\nplt.ylabel('count')\nplt.show()","7e967378":"# Create a training file with simple derived features\nsegment_size = 150000","cc343134":"def generate_segment_start_ids(sampling_method):\n    \"\"\" Generates the indeces where the segments for the training data start \"\"\"\n    if sampling_method == 'uniform':\n        # With this approach we obtain 4194 segments\n        num_segments_training = int(np.floor(train.shape[0] \/ segment_size))\n        segment_start_ids = [i * segment_size for i in range(num_segments_training)]\n    elif sampling_method == 'uniform_no_jump':\n        # With this approach we obtain 4178 segments (99.5% of 'uniform')\n        already_sampled = np.full(train.shape[0], False)\n        num_segments_training = int(np.floor(train.shape[0] \/ segment_size))\n        time_to_failure_jumps = np.diff(train['time_to_failure'].values)\n        num_good_segments_found = 0\n        segment_start_ids = []\n        for i in range(num_segments_training):\n            idx = i * segment_size\n            # Detect if there is a discontinuity on the time_to_failure signal within the segment\n            max_jump = np.max(time_to_failure_jumps[idx:idx + segment_size])\n            if max_jump < 5:\n                segment_start_ids.append(idx)\n                num_good_segments_found += 1\n            else:\n                print(f'Rejected candidate segment since max_jump={max_jump}')\n        segment_start_ids.sort()\n    elif sampling_method == 'random_no_jump':\n        # With this approach we obtain 4194 segments\n        num_segments_training = int(np.floor(train.shape[0] \/ segment_size)) #arbitrary choice\n        time_to_failure_jumps = np.diff(train['time_to_failure'].values)\n        num_good_segments_found = 0\n        segment_start_ids = []\n        while num_segments_training != num_good_segments_found:\n            # Generate a random sampling position\n            idx = random.randint(0, train.shape[0] - segment_size - 1)\n            # Detect if there is a discontinuity on the time_to_failure signal within the segment\n            max_jump = np.max(time_to_failure_jumps[idx:idx + segment_size])\n            if max_jump < 5:\n                segment_start_ids.append(idx)\n                num_good_segments_found += 1\n            else:\n                print(f'Rejected candidate segment since max_jump={max_jump}')\n        segment_start_ids.sort()\n    else:\n        raise NameError('Method does not exist')\n    return segment_start_ids","401dabe5":"print(f'Generating uniformly sampled training set')\nsegment_start_ids_uniform = generate_segment_start_ids('uniform')\n\nprint(f'Generating uniformly sampled training set excluding discontinuities in time_to_failure.')\nsegment_start_ids_uniform_no_jump = generate_segment_start_ids('uniform_no_jump')\n\nprint(f'Generating randomly sampled training set excluding discontinuities in time_to_failure.')\nprint(f'This method may yield overlaping segments')\nsegment_start_ids_random_no_jump = generate_segment_start_ids('random_no_jump')\n\n\ny_tr_samples_uniform = train['time_to_failure'].values[np.array(\n    segment_start_ids_uniform) + segment_size - 1]\ny_tr_samples_uniform_no_jump = train['time_to_failure'].values[\n    np.array(segment_start_ids_uniform_no_jump) + segment_size - 1]\ny_tr_samples_random_no_jump = train['time_to_failure'].values[\n    np.array(segment_start_ids_random_no_jump) + segment_size - 1]\n\nplt.subplots(figsize=(16, 5))\nplt.subplot(1, 3, 1)\nplt.hist(y_tr_samples_uniform, bins='auto', alpha=0.5, density=True)\nplt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\nplt.title('With discontinuities (contiguous)')\nplt.legend(['Sampled', 'All data'])\n\nplt.subplot(1, 3, 2)\nplt.hist(y_tr_samples_uniform_no_jump, bins='auto', alpha=0.5, density=True)\nplt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\nplt.title('Discarding discontinuities (contiguous)')\nplt.legend(['Sampled', 'All data'])\n\nplt.subplot(1, 3, 3)\nplt.hist(y_tr_samples_random_no_jump, bins='auto', alpha=0.5, density=True)\nplt.title('Discarding discontinuities (rand)')\nplt.hist(train['time_to_failure'].values[::50], bins='auto', alpha=0.5, density=True)\nplt.legend(['Sampled', 'All data'])\nplt.show()","bfaae1cc":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","7cc8bf2c":"n_fold = folds.get_n_splits()\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(y_tr_samples_uniform_no_jump)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    y_train, y_valid = y_tr_samples_uniform_no_jump[train_index], y_tr_samples_uniform_no_jump[valid_index]\n\n    plt.figure()\n    plt.hist(y_train, bins='auto', alpha=0.5, density=True)\n    plt.hist(y_valid, bins='auto', alpha=0.5, density=True)\n    plt.title(f\"Histogram with fold {fold_n}\")\n    plt.legend([f'Train (median={np.median(y_train):.4f})', f'Validation (median={np.median(y_valid):.4f})'])\n    plt.show()","ecc3d33a":"As we see here, the distributions of the training and validation datasets are not identical. It is to be seen if a more clever split can be done. ","4986cf2f":"Let's visualize the data","3c6af803":"[@konradb](https:\/\/www.kaggle.com\/konradb), on a [post](https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/85108#496570) on March 22ds, he pointed out that \"By adjusting the median of my prediction to match the target, I got a boost both in cv (2.0702 -> 2.0538) and lb (1.512 -> 1.489).\"\nLet's explore what happens to the distribution when making a k-fold.","a14b1b9c":"Let's visualize what happens to the distribution of the `time_to_failure` on teh different samplign strategies:","26b2530d":"# Preliminaries\nLet's import everything we need:","be3a1cb3":"Here we conclude that the histogram of the `time_to_failure` does not change significantly when excluding *all* jumps. However, the histogram starts to be significantly distorded when randon sampling is applied.","36b693b0":"It seems that the reference data (green line) won't be uniformly distributed. We can visualize it ","61e01e8a":"Let's set some configurations","89ab70db":"Currently, we have three strategies implemented.\n- Uniform sampling: Here the training set is built from consecutive chunks of data. No special care is taken into what's the content of each segment.\n- Uniform sampling with rejection: Here the training set is built from consecutive chunks of data. There is a control to avoid having in a segment the jump of the `time_to_failure` signal from zero to a high value. *This is very dangerous when doing the splitting between training and validation*.\n- Random sampling with rejection: Here the training set is built from sampling randomly the data. There is a control to avoid having in a segment the jump of the `time_to_failure` signal from zero to a high value. The segments will most likely overlap.","d6d59533":"# General information\nIn this notebook, we explore the statistical distribution of the reference signal (`time_to_failure`) and the implications of chopping the signals in blocks of 150.000 samples.","d3bf68f8":"# Sampling the training set\nSince the testign set is build of sequences of 150.000 samples, let's chop the training set on comparable chunks. \n\nSeveral strategies of how to select the chunks are possible:\n- Deterministic\n- Random\n\nMoreover, the *jumps* on the `time_to_failure` could be an issue (to be seen the impact). In this situation, we can\n- Include the *jumps* on the training set\n- Exlcude the *jumps* on the training set\n\nLet's see what happens to the histogram in some of these situations:","85ea418f":"The time to training is not uniforally distribured on the training set. Resampling could be interesting.","7cb9c742":"Next questions to expore:\n- For training, should we augment the dataset to have more equalized histogram?","52af1de8":"# Training data\nWe load the raw input data as is."}}