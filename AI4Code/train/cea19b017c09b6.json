{"cell_type":{"46700685":"code","86128966":"code","ba02859d":"code","8a84e173":"code","c5c56655":"code","c339acec":"code","d516ec43":"code","466af882":"code","73c4632e":"code","ee279321":"code","4fecba44":"code","e1347864":"code","e1dc0e99":"code","cc6dad66":"code","59e01a6e":"code","9fec0429":"markdown","98d2ac9a":"markdown","38921ef4":"markdown","63a78d2b":"markdown","7b11df58":"markdown","282acdbe":"markdown","e2308ca9":"markdown","d8d75d20":"markdown","e7c3c860":"markdown","f0c32a08":"markdown","49461adc":"markdown","6603529d":"markdown","202931a8":"markdown","bcee7558":"markdown","fa26244d":"markdown","96d1b8d0":"markdown","69fcdea5":"markdown","87ac10a4":"markdown","826927b7":"markdown","bd6904d3":"markdown","fc227832":"markdown","b592c476":"markdown","e5505051":"markdown","f77d6550":"markdown","8fa19cc5":"markdown","6e1bc6d6":"markdown"},"source":{"46700685":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kmodes.kmodes import KModes\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\npd.set_option('mode.chained_assignment', None)","86128966":"# Define Methods\n\ndef transform_true_false(x):\n    if x == 'T':\n        return 1\n    elif x == 'F':\n        return 0\n    else:\n        return -1\n    \ndef transform_yes_no(x):\n    if x == 'Y':\n        return 1\n    elif x == 'N':\n        return 0\n    else:\n        return -1\n    \ndef transform_ord_0(x):\n    if x == 1.0:\n        return 0\n    elif x == 2.0:\n        return 1\n    elif x == 3.0:\n        return 2\n    else:\n        return -1\n    \ndef transform_ord_1(x):\n    if x == 'Novice':\n        return 0\n    elif x == 'Contributor':\n        return 1\n    elif x == 'Expert':\n        return 2\n    elif x == 'Master':\n        return 3\n    elif x == 'Grandmaster':\n        return 4\n    else:\n        return -1\n    \ndef transform_ord_2(x):\n    if x == 'Freezing':\n        return 0\n    elif x == 'Cold':\n        return 1\n    elif x == 'Warm':\n        return 2\n    elif x == 'Hot':\n        return 3\n    elif x == 'Boiling Hot':\n        return 4\n    elif x == 'Lava Hot':\n        return 5\n    else:\n        return -1   \n    \nord_3_dict = { 'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14 }\nord_4_dict = { 'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25 }","ba02859d":"directory = \"..\/input\/cat-in-the-dat-ii\/\"\nfeature_tables = ['train.csv','test.csv','sample_submission.csv']\n\ndf_train = directory + feature_tables[0]\ndf_test = directory + feature_tables[1]\nsample_submission = directory + feature_tables[2]\n\n# Create dataframes\nprint(f'Reading csv from {df_train}...')\ntrain = pd.read_csv(df_train)\nprint('...Complete')\n\nprint(f'Reading csv from {df_test}...')\ntest = pd.read_csv(df_test)\nprint('...Complete')\n\nprint(f'Reading csv from {sample_submission}...')\nsample_submission = pd.read_csv(sample_submission)\nprint('...Complete')","8a84e173":"train.head()","c5c56655":"bin_features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\nnom_features = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']\nord_features = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4']\ntime_features = ['day', 'month']\ntarget = ['target']\n\nall_features = bin_features + nom_features + ord_features + time_features + target","c339acec":"for col in all_features:\n    print(f'Filling in missing catagorical data in {col} with mode of {col}...')\n    train[col].fillna(train[col].mode()[0], inplace = True)\n    \n    print(f'Number of unique catagories in {col} = {train[col].nunique()}\\n')","d516ec43":"train_bin = train[bin_features]\ntrain_bin['bin_3'] = train_bin['bin_3'].apply(transform_true_false)\ntrain_bin['bin_4'] = train_bin['bin_4'].apply(transform_yes_no)\ntrain_bin = train_bin.astype('int64')\ntrain_bin.head()","466af882":"train_nom = train[nom_features]\nfor col in nom_features:\n    le = preprocessing.LabelEncoder()\n    train_nom[col] = le.fit_transform(train_nom[col])\n    \ntrain_nom.head()","73c4632e":"train_ord = train[ord_features]\ntrain_ord['ord_0'] = train_ord['ord_0'].apply(transform_ord_0)\ntrain_ord['ord_1'] = train_ord['ord_1'].apply(transform_ord_1)\ntrain_ord['ord_2'] = train_ord['ord_2'].apply(transform_ord_2)\ntrain_ord['ord_3'] = train_ord['ord_3'].map(ord_3_dict)\ntrain_ord['ord_4'] = train_ord['ord_4'].map(ord_4_dict)\n\ntrain_ord.head()","ee279321":"train_time = train[time_features]\ntrain_time['day'] = train_time['day'].apply(lambda x: x-1)\ntrain_time['month'] = train_time['month'].apply(lambda x: x-1)\ntrain_time = train_time.astype('int64')\n\ntrain_time.head()","4fecba44":"train_final = pd.concat([train_bin, train_nom, train_ord, train_time, train[target]], axis = 1)\ntrain_final.head()","e1347864":"corr = train_final.corr(method='spearman')\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 18))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"YlGnBu\", vmax=.30, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","e1dc0e99":"cost = []\nK = range(1,5)\nfor num_clusters in list(K):\n    kmode = KModes(n_clusters=num_clusters, init = \"Cao\", n_init = 1, verbose=1)\n    kmode.fit_predict(train_final)\n    cost.append(kmode.cost_)\n    \nplt.plot(K, cost, 'bx-')\nplt.xlabel('k clusters')\nplt.ylabel('Cost')\nplt.title('Elbow Method For Optimal k')\nplt.show()","cc6dad66":"km = KModes(n_clusters=2, init = \"Cao\", n_init = 1, verbose=1)\ncluster_labels = km.fit_predict(train_final)\ntrain['Cluster'] = cluster_labels","59e01a6e":"for col in all_features:\n    plt.subplots(figsize = (15,5))\n    sns.countplot(x='Cluster',hue=col, data = train)\n    plt.show()","9fec0429":"### Nominal Features","98d2ac9a":"We will go with 2 Clusters for our analysis, since it looks like our optimal number of clusters based on the elbow in the above cost plot AND it aligns with the fact we have a binary target. Once we have our data labeled with their appropriate cluster we can append this data to our original train dataFrame for further visualization.","38921ef4":"Based on the plots above, these are the insights that I was able to gather. There may be more insights that I'm not seeing, or other visualization methods could show more interesting observations. Please comment if you feel there is anything I missed!\n\n* bin_3: Larger proportion of 'False' in cluster 0, Larger proportion of 'True' in cluster 1\n* bin_4: Larger proportion of 'No' in cluster 0, Larger proportion of 'Yes' in cluster 1\n* nom_0: Larger proportion of 'Red' in cluster 0\n* nom_1: Larger proportion of 'Triangle' in cluster 0\n* nom_2: Larger proportion of 'Hamster' in cluster 0\n* nom_3: Larger proportion of 'India' in cluster 0\n* nom_4: Larger proportion of 'Theremin' in cluster 0\n* ord_0: Larger proportion of 1.0 in cluster 0\n* ord_1: Larger proportion of 'Novice' in cluster 0\n* ord_2: Larger proportion of 'Freezing' in cluster 0\n* ord_4: Larger proportion of 'N' in cluster 0, \n* day: Larger proportion of day 3 in cluster 0\n* month: Larger proportion of month 8 in cluster 0\n* Target: Larger proportion of target 0 in cluster 0","63a78d2b":"## Import Data","7b11df58":"The time features are starting at an index of 1 currently. We will use a simple lambda function to start the time features at index of 0 in order to align with the other features.","282acdbe":"In order to determine any trends or insights from the clustered data, we will look at count plots for each feature. **Please let me know if you have any other interesting ways to visualize this type of cluster analysis.**","e2308ca9":"### Time Features","d8d75d20":"## K-Modes Clustering","e7c3c860":"Since these are ordinal features, we would like to maintain the positional information within. Therefore, we will apply our own transformation functions and dictionaries to the features.","f0c32a08":"Now, we will concatenate all of our newly transformed features into our final DataFrame that will be used for cluster analysis","49461adc":"The only thing that needed to be done to the binary features was to encode bin_3 and bin_4.","6603529d":"For the nominal features, we will use a simple Label Encoder.","202931a8":"As an additional visualization tool, lets take at a correlation heatmap for these features. Note that I am not doing any sort of normalization to the catagorical dataFrame I have created, which is typically a good practice. I am however looking at Spearman instead of Pearson correlation as Spearman is typically better for ordinal data.\n\nBased on the heatmap below, I'm not seeing much correlation between variables other than the 'target' variable. Perhaps there is some interdependence in the variables that we decided to remove from the analysis, but for the variables we chose to analyze we do not see that. The largest correlation is between target and ord_3, and the ordinal features in general look to have a larger correlation than the the binary or nominal features.","bcee7558":"I decided to limit my k-modes clustering assessment to variables that contain a \"reasonable\" number of catagories. I removed **nom_5 thru nom_9** as well as **ord_5** since they were too large to properly visualize in the end.\n","fa26244d":"## Cluster Visualization","96d1b8d0":"Now we can begin our cluster analysis. First, let's do a sweep of the number of clusters and look at how cost changes as we increase the cluster number. I will be using the elbow method to determine the optimal number of clusters. The elbow of the cost plot represents the point where we stop seeing significant improvement in our clustering cost function as we continue to increase the number of clusters. ","69fcdea5":"We need to properly fill missing data. We will do this by filling NaNs with the mode for each column. We will also print the number of unique catagories per column to help us understand the data.","87ac10a4":"![image.png](attachment:image.png)\n\nGiven the fact that the dataset provided for Categorical Feature Encoding II contains 100% catagorical variables, I figured that it would be a good exercise to perform k-modes clustering. The difference between k-means and k-modes lies in the metric used for clustering. Where k-means uses a distance metric between two objects to cluster similar datapoints, k-modes utilizes the total number of mismatches between two objects. In other words, it uses a mode metric instead of a mean distance metric. \n\nI hope you find this kernel helpful and some **UPVOTES** would be very much appreciated.\n\nLast Updated: 1\/21\/2020","826927b7":"### Final Concatenation","bd6904d3":"## Conclusions","fc227832":"# **Clustering Catagorical Data: K-modes**","b592c476":"## Import Packages and Define Encoder Methods","e5505051":"### Binary Features","f77d6550":"### Ordinal Features","8fa19cc5":"## Correlation Heatmap","6e1bc6d6":"## Clean and Transform Data"}}