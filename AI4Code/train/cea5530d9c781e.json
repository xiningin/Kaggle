{"cell_type":{"2a287208":"code","5e6b526f":"code","35c86e36":"code","e615c8d4":"code","21db42e8":"code","b29bf2c1":"code","0f56fe3b":"code","808c1a84":"code","0dc4af78":"code","607cb16f":"code","f16ab3d0":"code","5390111b":"code","e1b44f83":"code","b9585f0a":"code","b0b31b13":"code","a331aa5f":"code","ee936abf":"code","fa7849e3":"code","18bfa502":"code","27d85ae1":"code","6797d38a":"code","ff83ce4d":"code","e18fa3fe":"code","634230b7":"markdown","3b05a0ec":"markdown","1815b633":"markdown","c0234545":"markdown","f27f4987":"markdown","56871f58":"markdown","6583f83b":"markdown","c8fa6e77":"markdown","7611a094":"markdown","23534895":"markdown","5317ecc7":"markdown","a89934fb":"markdown","71975e49":"markdown","408a2d52":"markdown","607db928":"markdown","ad35b584":"markdown","b1a44fa4":"markdown","c05c845d":"markdown","46873e20":"markdown","b3e0c205":"markdown","e8b705e9":"markdown","4cd8c565":"markdown","2bc267b8":"markdown","ac9dd459":"markdown","017309aa":"markdown","70ee4288":"markdown","6723e7c4":"markdown","ab9f98ef":"markdown","83ae1baa":"markdown"},"source":{"2a287208":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge","5e6b526f":"test_seg = pd.read_csv('..\/input\/test\/seg_004cd2.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","35c86e36":"len(test_seg)","e615c8d4":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","21db42e8":"chunk_row = 150_000\nrows = 150_000\nsegments = int(np.floor(train.shape[0] \/ rows))","b29bf2c1":"segments","0f56fe3b":"X_tr = pd.DataFrame(index=range(segments), dtype=np.float64)\n\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","808c1a84":"def create_features(X, segment_num, chunk):\n    def add_trend_feature(arr, abs_values=False):\n        idx = np.array(range(len(arr)))\n        if abs_values:\n            arr = np.abs(arr)\n        lr = LinearRegression()\n        lr.fit(idx.reshape(-1, 1), arr)\n        return lr.coef_[0]\n    \n    percentiles = [10, 50, 75, 90, 99]\n    moving_avg_window = [1500, 3000, 6000, 9000, 12000]\n    first_segments = [10000, 50000]\n    last_segments = [10000, 50000]\n    rolling_percentiles = [1, 5, 50, 95, 99]\n    \n    x = pd.Series(chunk['acoustic_data'].values)\n\n    X.loc[segment_num, 'mean'] = x.mean()\n    X.loc[segment_num, 'std'] = x.std()\n    X.loc[segment_num, 'var'] = np.var(x)\n    X.loc[segment_num, 'max'] = x.max()\n    X.loc[segment_num, 'min'] = x.min()\n    \n    z = np.fft.fft(x)\n    realFFT = np.real(z)\n    imagFFT = np.imag(z)\n    X.loc[segment_num, 'Real_mean'] = realFFT.mean()\n    X.loc[segment_num, 'Real_std'] = realFFT.std()\n    X.loc[segment_num, 'Imag_mean'] = imagFFT.mean()\n    X.loc[segment_num, 'Imag_std'] = imagFFT.std()\n    \n    for f_seg in first_segments:\n        X.loc[segment_num, 'std_first_{}'.format(f_seg)] = x[:f_seg].std()\n        X.loc[segment_num, 'mean_first_{}'.format(f_seg)] = x[:f_seg].mean()\n        X.loc[segment_num, 'Rstd_last_{}'.format(f_seg)] = realFFT[:f_seg].std()\n        X.loc[segment_num, 'Rmean_last_{}'.format(f_seg)] = realFFT[:f_seg].mean()\n\n    for l_seg in last_segments:\n        X.loc[segment_num, 'std_last_{}'.format(l_seg)] = x[-l_seg:].std()\n        X.loc[segment_num, 'mean_last_{}'.format(l_seg)] = x[-l_seg:].mean()\n        X.loc[segment_num, 'Rstd_last_{}'.format(l_seg)] = realFFT[-l_seg:].std()\n        X.loc[segment_num, 'Rmean_last_{}'.format(l_seg)] = realFFT[-l_seg:].mean()\n        \n    for percent in percentiles:\n        X.loc[segment_num, 'percentile_{}'.format(percent)] = np.percentile(x, percent)\n    \n    X.loc[segment_num, 'mad'] = x.mad()\n    X.loc[segment_num, 'kurt'] = x.kurtosis()\n    X.loc[segment_num, 'skew'] = x.skew()\n    X.loc[segment_num, 'med'] = x.median()\n    \n    X.loc[segment_num, 'Hilbert_mean'] = np.abs(hilbert(x)).mean()\n    X.loc[segment_num, 'Hann_window_mean'] = (convolve(x, hann(150), mode='same') \/ sum(hann(150))).mean()\n    \n    X.loc[segment_num, 'trend'] = add_trend_feature(x)\n    X.loc[segment_num, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n    \n    for mv_avg_window in moving_avg_window:\n        X.loc[segment_num, 'Moving_average_{}_mean'.format(mv_avg_window)] = x.rolling(window=mv_avg_window).mean().mean(skipna=True)\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n\n        X.loc[segment_num, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[segment_num, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[segment_num, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[segment_num, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        \n        X.loc[segment_num, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[segment_num, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[segment_num, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[segment_num, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n\n        for roll_perc in rolling_percentiles:\n            X.loc[segment_num, 'p_{}_roll_std_{}'.format(roll_perc, windows)] = np.percentile(x_roll_std, roll_perc)\n            X.loc[segment_num, 'p_{}_roll_mean_{}'.format(roll_perc, windows)] = np.percentile(x_roll_mean, roll_perc)\n\n        X.loc[segment_num, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[segment_num, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        X.loc[segment_num, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        X.loc[segment_num, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X.loc[segment_num, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        X.loc[segment_num, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()","0dc4af78":"for segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+chunk_row]\n    y = seg['time_to_failure'].values[-1]\n    y_tr.loc[segment, 'time_to_failure'] = y\n    create_features(X_tr, segment, seg)","607cb16f":"print(f'{X_tr.shape[0]} samples in new train data and {X_tr.shape[1]} columns.')","f16ab3d0":"np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(10)","5390111b":"means_dict = {}\nfor col in X_tr.columns:\n    if X_tr[col].isnull().any():\n        mean_value = X_tr.loc[X_tr[col] != -np.inf, col].mean()\n        X_tr.loc[X_tr[col] == -np.inf, col] = mean_value\n        X_tr[col] = X_tr[col].fillna(mean_value)\n        means_dict[col] = mean_value","e1b44f83":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","b9585f0a":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    create_features(X_test, seg_id, seg)\n    \nfor col in X_test.columns:\n    if X_test[col].isnull().any():\n        X_test.loc[X_test[col] == -np.inf, col] = means_dict[col]\n        X_test[col] = X_test[col].fillna(means_dict[col])\n        \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","b0b31b13":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","a331aa5f":"def train_model(X=X_train_scaled, X_test=X_test_scaled, y=y_tr, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', task_type='GPU', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance, model\n        return oof, prediction, model\n    \n    else:\n        return oof, prediction, model","ee936abf":"params = {'num_leaves': 128,\n          'min_data_in_leaf': 79,\n          'objective': 'huber',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8126672064208567,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501\n         }\noof_lgb, prediction_lgb, feature_importance, model = train_model(params=params,\n                                                                 model_type='lgb',\n                                                                 plot_feature_importance=True)","fa7849e3":"prediction = oof_lgb\ny_values = y_tr\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Predictions Vs Actual time to failure (Training data)\")\nplt.plot(prediction, color='aqua')\nax1.set_ylabel('time to failure', color='b')\nplt.legend(['predictions'])\nax2 = ax1.twinx()\nplt.plot(y_values, color='g')\nax2.set_ylabel('actual value', color='g')\nplt.legend(['actual value'], loc=(0.875, 0.9))\nplt.grid(False)","18bfa502":"feature_importance_values = np.abs(X_test_scaled.corrwith(pd.DataFrame(prediction_lgb)[0]))","27d85ae1":"feature_importance_values.sort_values(ascending=False).head(10)","6797d38a":"feature_importance_values[feature_importance_values > 0.6].sort_values(ascending=False)","ff83ce4d":"submission['time_to_failure'] = prediction_lgb","e18fa3fe":"submission.to_csv('submission_rolling_v6.csv')","634230b7":"# Investigating Feature Importance and parameter fine tuning","3b05a0ec":"The predictions are too cyclical and thus missing out to capture the different time_to_failure after the quakes have occured. The predicions pretty much think that the time will be rest the same way (thus, near perfect cycle). I think this is the next step I have to work on without overfitting.","1815b633":"Rolling windows are the most important features! I am figuring whether I should reduce training my model with just the important features thresholded by certain value. For, example, only the features where the feature importance is above 0.6","c0234545":"I just want to train with one machine learning method (light gradient boosting) rather than using xgboost, catboot and SVN. I didn't see the results any getting better with multiple models.","f27f4987":"I increased the number of features during feature engineering process. I added Fast Fourier Transform to extract some features from the data. I think it makes sense since the acoustic emission might probably be broken down into a bunch of sine waves.","56871f58":"Right now I am just going to take the LGB's output predictions. I will think about what to do with XGB outputs. I don't want to average the two results either.","6583f83b":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(filename='submission_rolling_v5.csv')\n","c8fa6e77":"selected_cols = feature_importance_values[feature_importance_values > 0].index\nreduced_X_tr = X_tr[selected_cols]\nreduced_X_test = X_test[selected_cols]\n\nscaler = StandardScaler()\nscaler.fit(reduced_X_tr)\nreduced_X_train_scaled = pd.DataFrame(scaler.transform(reduced_X_tr), columns=reduced_X_tr.columns)\nreduced_X_test_scaled = pd.DataFrame(scaler.transform(reduced_X_test), columns=reduced_X_test.columns)","7611a094":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n\nparams = {'num_leaves': 100,\n          'min_data_in_leaf': 80,\n          'objective': 'huber',\n          'max_depth': 14,\n          'learning_rate': 0.008,\n          \"boosting\": \"dart\",\n          \"bagging_freq\": 2,\n          \"bagging_fraction\": 0.6,\n          \"feature_fraction\": 0.75,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'lambda_l1': 0.18,\n          'lambda_l2': 0.25\n         }\nreduced_oof_lgb, reduced_prediction_lgb, reduced_feature_importance = train_model(X=reduced_X_train_scaled,\n                                                                                  X_test=reduced_X_test_scaled,\n                                                                                  y=y_tr,\n                                                                                  params=params,\n                                                                                  folds=folds,\n                                                                                  model_type='lgb',\n                                                                                  plot_feature_importance=True)","23534895":"Checking how well the added features correlate to labels","5317ecc7":"plt.figure(figsize=(44, 24))\ncols = list(feature_importance_values.sort_values(ascending=False).head(20).index)\nfor i, col in enumerate(cols):\n    plt.subplot(5, 4, i + 1)\n    plt.plot(X_tr[col], color='blue')\n    plt.title(\"{} - {:.3f}\".format(col, feature_importance_values[col]))\n    ax1.set_ylabel(col, color='b')\n\n    ax2 = ax1.twinx()\n    plt.plot(y_tr, color='g')\n    ax2.set_ylabel('time_to_failure', color='g')\n    plt.legend([col, 'time_to_failure'], loc=(0.875, 0.9))\n    plt.grid(False)","a89934fb":"Sometimes, I just play around with stuff and don't want to commit the whole thing just to download the submission file. Here is a great solution to get around that. Feature engineering and hyper parameter tuning takes a long time so I would like to have a download link of submission file when I have played around.","71975e49":"Now, let's create some placeholder dataframes for features (X_tr) and labels (y_tr)","408a2d52":"Plots of time_to_failure values (training set) with rest to important features","607db928":"Perform the same kind of feature engineering to the test data","ad35b584":"xgb_preds = oof_xgb\nlgb_preds = oof_lgb\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"LGB Vs XGB prediction values on test data\")\nplt.plot(xgb_preds, color='red', alpha=0.9)\nax1.set_ylabel('time to failure', color='red')\nplt.legend(['xgb'])\nax2 = ax1.twinx()\nplt.plot(lgb_preds, color='blue', alpha=0.5)\nax2.set_ylabel('time to failure', color='blue')\nplt.legend(['lgb'], loc=(0.875, 0.9))\nplt.grid(False)","b1a44fa4":"Greatly inspired by: [Earthquakes FE. More features and samples](https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples).","c05c845d":"xgb_params = {'eta': 0.03,\n              'max_depth': 10,\n              'subsample': 0.9,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': 4}\noof_xgb, prediction_xgb = train_model(X=X_train_scaled, X_test=X_test_scaled, params=xgb_params, model_type='xgb')","46873e20":"I chunked the training data with 150,000 rows per chunk. I also tried what would it be like when data is supersample, that is, taking overlapping rows, where each starting point differs by 10,000 rows. It turns out that the model overfits quite a bit for that kind of scheme. Maybe, I can reduce the overlap and also tune the parameters for thraining method to reduce overfitting.","b3e0c205":"This is just for the sake of testing. I actually am not planning to use XGB outputs for submission.","e8b705e9":"But right now, just a simple chunking with 150,000 rows per chunk.","4cd8c565":"## Building models","2bc267b8":"I have heavily fine-tuned num_leaves in tandem with max_depth. Reduced learning rate. Increased lambda_l1 (L1 regularization).","ac9dd459":"For all the missing values, create a dictionary with mean values which will be used both for replacing the missing values in training dataset as well as testing dataset.","017309aa":"### Reading test data","70ee4288":"What would happen if we only train the features with feature importance greater than 0.5?","6723e7c4":"4,194 segments are present so there will be 4,194 rows in our training dataset. The main concern I have is that the model will overfit with such a small dataset.","ab9f98ef":"In test set, the data comes in segments and not like a big chunk in training dataset. I want to know how long the segments are in test segments, we are going to chunk the training data into chunk sizes which are equal to the testing set chunk sizes.","83ae1baa":"Standardize features by removing the mean and scaling to unit variance"}}