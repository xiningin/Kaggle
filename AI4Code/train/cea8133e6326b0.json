{"cell_type":{"a0b19c0b":"code","0a687b6c":"code","52765f67":"code","a2e71df2":"code","402940d4":"code","6551d2cf":"code","8441a1ef":"code","0f3c04da":"code","4cb11f22":"code","28528ce8":"code","0874fa7d":"code","2765ec46":"code","f323ca17":"code","85376ce8":"code","9eb9aec3":"code","50d0bef4":"code","e1906811":"code","1f06d145":"code","7a485e6c":"code","10614f61":"code","b2e12132":"code","6d8e5e83":"code","4b4bd549":"code","204d0448":"code","f17c8c00":"code","735f7999":"code","d0948287":"code","4e51a1e3":"code","ba202fdb":"code","371745ae":"code","22767b77":"code","6ddacdc4":"markdown","470b740a":"markdown","795fa413":"markdown"},"source":{"a0b19c0b":"import numpy as np\nimport tensorflow as tf\nimport keras\n\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Conv2D ,LSTM, MaxPooling2D , Flatten , Dropout \nfrom keras.layers import BatchNormalization,TimeDistributed\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,5)\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport random\nimport time\nimport os\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom scipy import signal\nfrom scipy.fft import fftshift\n","0a687b6c":"ACTIONS = [\"kiri\", \"maju\",\"idle\",\"kanan\"]\nreshape = (-1,8, 60,1)\nreshape2 = (-1,8, 60)\n","52765f67":"def create_data(starting_dir=\"..\/input\/datasheeteegnew\/datasheet\/ivan\"):\n    training_data = {}\n    for action in ACTIONS:\n        if action not in training_data:\n            training_data[action] = []\n        data_dir = os.path.join(starting_dir,action)\n        for item in os.listdir(data_dir):\n            data = np.load(os.path.join(data_dir, item))\n            for item in data:\n                training_data[action].append(item)\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n\n    for action in ACTIONS:\n        np.random.shuffle(training_data[action])  \n        training_data[action] = training_data[action][:min(lengths)]\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n    combined_data = []\n    for action in ACTIONS:\n        for data in training_data[action]:\n            if action == \"kiri\":\n                combined_data.append([data, [1, 0, 0,0]])\n            elif action == \"maju\":\n                combined_data.append([data, [0, 1, 0, 0]])\n            elif action == \"idle\":\n                combined_data.append([data, [0, 0, 1, 0]])\n            elif action == \"kanan\":\n                combined_data.append([data, [0, 0, 0, 1]])\n\n    np.random.shuffle(combined_data)\n    print(\"length:\",len(combined_data))\n    return combined_data\n","a2e71df2":"print(\"creating training data\")\ntraindata = create_data(starting_dir=\"..\/input\/datasheeteegnew\/datasheet\/ivan\")\ntrain_X = []\ntrain_y = []\n\nfor X, y in traindata:\n    train_X.append(X)\n    train_y.append(y)\n\ntrain_X = np.array(train_X).reshape(reshape)\ntrain2_X = np.array(train_X).reshape(reshape2)\ntrain_y = np.array(train_y)\n","402940d4":"\nidle = np.load(\"..\/input\/datasheeteegnew\/datasheet\/ivan\/idle\/1610626457.npy\")\nkanan = np.load(\"..\/input\/datasheeteegnew\/datasheet\/ivan\/kanan\/1610626522.npy\")\nkiri = np.load(\"..\/input\/datasheeteegnew\/datasheet\/ivan\/kiri\/1610626556.npy\")\nmaju = np.load(\"..\/input\/datasheeteegnew\/datasheet\/ivan\/maju\/1610626596.npy\")\n\nkelas1=idle[0][16]\nkelas2=maju[0][16]\nkelas3=kanan[0][16]\nkelas4=kiri[0][16]\n\nkelas1a=idle[175]\nkelas2a=maju[175]\nkelas3a=kanan[175]\nkelas4a=kiri[175]\n\n\n\nf1, t1,Sxx= signal.spectrogram(kelas1,fs=60, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd',\n                            )\ndbs1 = 10*np.log10(Sxx)\nf2, t2,Sxx= signal.spectrogram(kelas2,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd',\n                            )\ndbs2 = 10*np.log10(Sxx)\nf3, t3,Sxx= signal.spectrogram(kelas3,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd'\n                            )\ndbs3 = 10*np.log10(Sxx)\nf4, t4,Sxx= signal.spectrogram(kelas4,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd'\n                            )  \ndbs4 = 10*np.log10(Sxx)\n\n","6551d2cf":"fig, axs = plt.subplots(nrows=4, ncols=4,figsize=(20,10))\naxs = axs.flatten()\n\naxs[0].set_title(\"siggle\")\naxs[0].plot(kelas1)\n\naxs[1].set_title(\"global\")\naxs[1].plot(kelas1a)\n\naxs[2].set_title(\"Spectogram\")\naxs[2].pcolormesh(t1, f1,dbs1,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\n\ncwt1 = signal.cwt(kelas1,signal.ricker,widths=np.arange(1,50))\ncwt2 = signal.cwt(kelas2,signal.ricker,widths=np.arange(1,50))\ncwt3 = signal.cwt(kelas3,signal.ricker,widths=np.arange(1,50))\ncwt4 = signal.cwt(kelas4,signal.ricker,widths=np.arange(1,50))\naxs[3].set_title(\"wavelet\")\naxs[3].imshow(cwt1,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt1).max(),vmin=-abs(cwt1).max())\n\naxs[4].plot(kelas2)\naxs[5].plot(kelas2a)\naxs[6].pcolormesh(t2, f2,dbs2,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\naxs[7].imshow(cwt2,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt2).max(),vmin=-abs(cwt2).max())\n\naxs[8].plot(kelas3)\naxs[9].plot(kelas3a)\naxs[10].pcolormesh(t3, f3,dbs3,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\naxs[11].imshow(cwt3,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt3).max(),vmin=-abs(cwt3).max())\n\naxs[12].plot(kelas4)\naxs[13].plot(kelas4a)\naxs[14].pcolormesh(t4, f4,dbs4,  \n                  shading='gouraud',\n                  cmap='viridis')\naxs[15].imshow(cwt4,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt4).max(),vmin=-abs(cwt4).max())\nplt.show()\n","8441a1ef":"ye=pd.DataFrame(train_y)\n\nye.columns=[\"kiri\", \"maju\",\"idle\",\"kanan\"]\ncategories = list(ye.columns.values)\nsns.set(font_scale = 1)\nplt.figure(figsize=(10,8))\nax= sns.barplot(categories, ye.iloc[:,0:].sum().values)\nplt.title(\"Number of samples labeled as active (1) out of {0} length data\".format((ye.shape[0])),fontsize=20)\n\nplt.ylabel('Number of events', fontsize=18)\nplt.xlabel('Event Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = ye.iloc[:,0:].sum().values\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=10)\nplt.show()","0f3c04da":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.5, min_lr=0.0001)","4cb11f22":"\ndef cnn_model():    \n\n    model = Sequential()\n    \n    model.add(Conv2D(filters = 64, kernel_size = (8,8), padding = \"same\", activation = \"relu\", input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (4,4)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    \n    model.add(Conv2D(filters = 64, kernel_size = (4,4), padding = \"same\", activation = \"relu\",input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (2,2)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(filters = 64, kernel_size = (2,2), padding = \"same\", activation = \"relu\",input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (1,1)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    \n\n    model.add(Flatten())\n    \n    model.add(Dense(32, activation = \"relu\"))\n    model.add(Dense(4, activation = \"softmax\"))\n   \n    model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", \n                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n                             tf.keras.metrics.Precision(),\n                             tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n                             ])\n    \n    return model\n","28528ce8":"model = cnn_model()\nmodel.summary()","0874fa7d":"\n#set early stopping criteria\npat = 5\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1CNN.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=100, BATCH_SIZE=32):\n    model = None\n    model = cnn_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n                        callbacks=[#learning_rate_reduction,\n                                   early_stopping,\n                                   model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    \n    return results\n\nn_folds=5\nepochs=100\nbatch_size=32\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \n\nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train_X, train_y, test_size=0.2, \n                                               random_state = np.random.randint(1,1000, 1)[0])\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")","2765ec46":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1CNN.h5')\n\na = model.evaluate(val_x, val_y)\nprint('loss',a[0])\nprint('Accuracy',a[1]*100)\nprint('AUC',a[2])\nprint('precision',a[3])\nprint('recall',a[4])\nprint('specificity_at_sensitivity',a[5])\nprint('sensitivity_at_specificity',a[6])","f323ca17":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","85376ce8":"rounded_predictions = model.predict_classes(x_test, batch_size=32, verbose=1)\nrounded_labels=np.argmax(y_test, axis=1)\nrounded_labels[1],rounded_predictions[1]\nprint(classification_report(rounded_labels, rounded_predictions))\n\npredictions = model.predict_classes(x_train, batch_size=32, verbose=1)\nlabels=np.argmax(y_train, axis=1)\npredictions[1],labels[1]\n\ncm = confusion_matrix(labels,predictions)\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')\n","9eb9aec3":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","50d0bef4":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","e1906811":"\nMODEL_NAME ='subjek1CNN.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/datasheeteegnew\/datasheet\/ivan'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n   # argmax_dict = {0: 0, 1: 0, 2: 0,3:0}\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(-1, 8, 60,1)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","1f06d145":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7a485e6c":"from sklearn.metrics import roc_curve, auc\n\n#y_score = clf.decision_function(xval)\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    #plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % ACTIONS[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","10614f61":"def lstm_model():\n    model = Sequential()\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n \n    model.add(LSTM(64))\n    \n    model.add(Flatten())\n    model.add(Dense(32, activation = \"relu\"))\n    \n    #model.add(BatchNormalization())\n    model.add(Dense(4, activation='softmax'))\n   \n\n    model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", \n                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n                             tf.keras.metrics.Precision(),\n                             tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n\n                             ])\n\n    return model","b2e12132":"model = lstm_model()\nmodel.summary()","6d8e5e83":"#set early stopping criteria\npat = 5\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1LSTM.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=100, BATCH_SIZE=32):\n    model = None\n    model = lstm_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n                        callbacks=[#learning_rate_reduction,\n                            early_stopping, \n                            model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    return results\n    ","4b4bd549":"n_folds=5\nepochs=100\nbatch_size=32\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train2_X, train_y, test_size=0.2, \n                                               random_state = np.random.randint(1,1000, 1)[0])\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")\n","204d0448":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\n\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1LSTM.h5')\nb = model.evaluate(val_x, val_y)\n\nprint('loss',b[0])\nprint('Accuracy',b[1]*100)\nprint('AUC',b[2])\nprint('precision',b[3])\nprint('recall',b[4])\nprint('specificity_at_sensitivity',b[5])\nprint('sensitivity_at_specificity',b[6])","f17c8c00":"x_train,x_test,y_train,y_test=train_test_split(train2_X,train_y,test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","735f7999":"rounded_predictions = model.predict_classes(x_test, batch_size=32, verbose=1)\nrounded_labels=np.argmax(y_test, axis=1)\nrounded_labels[1],rounded_predictions[1]\nprint(classification_report(rounded_labels, rounded_predictions))\n\npredictions = model.predict_classes(x_train, batch_size=32, verbose=1)\nlabels=np.argmax(y_train, axis=1)\npredictions[1],labels[1]\n\ncm = confusion_matrix(labels,predictions)\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","d0948287":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","4e51a1e3":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","ba202fdb":"\nMODEL_NAME ='subjek1LSTM.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/datasheeteegnew\/datasheet\/ivan'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n   # argmax_dict = {0: 0, 1: 0, 2: 0,3:0}\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(-1, 8, 60)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","371745ae":"x_train,x_test,y_train,y_test=train_test_split(train2_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","22767b77":"from sklearn.metrics import roc_curve, auc\n\n#y_score = clf.decision_function(xval)\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","6ddacdc4":"# DATA","470b740a":"# LSTM","795fa413":"# CNN"}}