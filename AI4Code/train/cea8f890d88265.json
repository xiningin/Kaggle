{"cell_type":{"682e870e":"code","2de858f9":"code","e426bf1c":"code","d4747a9d":"code","57a4e544":"code","a9f290f2":"code","e406bcfa":"code","6a9594bc":"code","f8760ab5":"code","d54922fd":"code","24d9dc95":"code","2e63afa4":"code","27fc4a7a":"code","301484f3":"code","181aa1c1":"code","130dbc91":"code","4fb9718c":"code","a86a4555":"code","c8b2200c":"code","106ea1ed":"code","6358bd17":"code","b9ea2514":"code","3de48703":"code","a77267ff":"code","cf72afc3":"code","a85f733e":"code","913d66ad":"code","fa118ff9":"markdown","1763c8e8":"markdown","b2b26e18":"markdown","c974baf6":"markdown","56bddd5b":"markdown","9ba61674":"markdown","24b462e6":"markdown","15e59b0c":"markdown","3a22356c":"markdown","7b13770c":"markdown","b38c677a":"markdown","5e595171":"markdown","9d5936ef":"markdown","878e6182":"markdown"},"source":{"682e870e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","2de858f9":"## THIS NOTEBOOK ONLY EXPLAINS THE OBTAINED RESULTS AND NOT WHY WE USED THE SPECIFIC MODELS ,ARCHITECTURES etc.","e426bf1c":"import glob\nimport imageio\nimport time\nfrom IPython import display\nfrom skimage.io import imread\nimport os\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom tensorflow.keras.preprocessing.image import load_img ,img_to_array\nimport pickle\nfrom tensorflow.keras import *\nimport os\nfrom tensorflow.keras.regularizers import *\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import LearningRateScheduler","d4747a9d":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport numpy as np\nimport PIL\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport scipy\nimport imageio\nfrom PIL import Image,ImageOps\nfrom sklearn.model_selection import train_test_split\nfrom scipy import ndimage\nfrom tensorflow.keras.utils import to_categorical\nfrom scipy.io import loadmat\nimport h5py\nimport cv2\nfrom tensorflow.keras.models import model_from_json\nimport pandas as pd\nfrom tensorflow.keras.optimizers import *\nfrom IPython.display import FileLink\n%matplotlib inline","57a4e544":"# Setting all seed values to zero for similar results\nseed=0\n\nfrom numpy.random import seed\nseed(0)\ntf.random.set_seed(0)","a9f290f2":"dataset_train_dir='..\/input\/idao2021\/track1\/train' #Path pointing to training dataset\n\ner_dir='..\/input\/idao2021\/track1\/train\/ER' #path pointing to ER folder\nnr_dir='..\/input\/idao2021\/track1\/train\/NR' #path pinting to NR folder\n\ncls_model_arch='..\/input\/882-score-idao\/model_clss.json'\ncls_model_dir='..\/input\/882-score-idao\/model_cls.h5' # This path could have changed due to newer version creations,So kindly check it via submission_models dataset\nreg_model_dir='..\/input\/882-score-idao\/model_reg.h5' # This path could have changed due to newer version creations,So kindly check it via submission_models dataset\nreg_model_arch='..\/input\/882-score-idao\/model_reg.json'\n\nprivate_tar_dir='..\/input\/dataset-creation\/idao.tar.gz' #This dataset is currently not public so kindly either download and extract the private test dataset and use it directly\n\nprivate_dir='.\/private_test'  # path pointing to public test dataset\npublic_dir='..\/input\/idao2021\/track1\/public_test' # path pointing to public test dataset","e406bcfa":"#Manually cropping resizing and processing the images.\ndef crop_center(img,cropx,cropy):\n    y,x = img.shape\n    startx = x\/\/2-(cropx\/\/2)\n    starty = y\/\/2-(cropy\/\/2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n\ndef create_dataset(img_folder):\n    i=0\n    img_data_array=[]\n    class_name=[]\n    for dir1 in os.listdir(img_folder):\n        for file in os.listdir(os.path.join(img_folder, dir1)):\n            image_path= os.path.join(img_folder, dir1,  file)\n            image=imread(image_path)\n            i+=1\n            image=resize(image, (288,288))\n            image=np.array(image)\n            image = image.astype('float32')\n            image=crop_center(image,96,96)\n            img_data_array.append(image)\n            class_name.append(dir1)\n            if i%1000==0:\n                print('Loaded ' + str(i) + 'files')\n    return img_data_array, class_name\n\n\n\nimg_data,class_name =create_dataset(dataset_train_dir)\ntarget_dict={k: v for v, k in enumerate(np.unique(class_name))}\ntarget_val= to_categorical(np.array([target_dict[class_name[i]] for i in range(len(class_name))]),2)","6a9594bc":"# Stacking Image to create 3 channels,so that we will be able to use transfer learning for classification and regression\nimg_dat=np.stack((img_data,img_data,img_data),axis=-1)\n\n# Deleting cache to clear RAM memory to avoid memory overflow\ndel img_data,class_name","f8760ab5":"# Getting Regression training labels\n\nlv=[]\nimport re\nfor each in os.listdir(er_dir):\n    n=re.split('_|;',each)\n    lv.append(int(n[6]))\nfor each in os.listdir(nr_dir):\n    n=re.split('_|;',each)\n    lv.append(int(n[7]))","d54922fd":"# training and testing sets for classification Model\nX_train,x_test,y_train,y_test=train_test_split(img_dat,target_val,test_size=0.2)\n# training and testing sets for regression Model\nx_treg,x_vreg,y_treg,y_vreg=train_test_split(img_dat,np.array(lv))","24d9dc95":"'''\nWe used transfer learning from the existing ResNet-50 model with fine tuning the parameters according to the need of the task,\nSo as to obtain a good classification score.\nOur logic mainly relied on to achieve a good classification score which will help us in obtaining better MAE scores in the \nregression task as can be seen later, but the basic building block heavily relied on the classification model.\n'''\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nres=ResNet50(input_shape=(96,96,3), weights='imagenet', include_top=False)\nx = Flatten()(res.output)\nx=Dense(1024,name='Dense3',activation='relu')(x)\nx=Dense(128,name='Dense2',activation='relu')(x)\nprediction_cls=Dense(2,name='output' , activation='softmax')(x)\nmodel_cls = Model(inputs=res.input, outputs=prediction_cls)\n#model_cls.summary()","2e63afa4":"json_model = model_cls.to_json()\n#save the model architecture to JSON file\nwith open('model_cls.json', 'w') as json_file:\n    json_file.write(json_model)","27fc4a7a":"model_cls.compile(optimizer='adam',loss='binary_crossentropy' , metrics=['accuracy','AUC'])\n#model_cls.fit(X_train,y_train,batch_size=16,epochs=50,validation_split=0.1)","301484f3":"model_cls.load_weights(cls_model_dir)\nmodel_cls.evaluate(X_train,y_train)\nmodel_cls.evaluate(x_test,y_test)","181aa1c1":"'''\nWe used transfer learning from the existing ResNet-50 model with fine tuning the parameters according to the need of the task,\nSo as to obtain a good regression based MAE score.\nOur logic mainly relied on to achieve a good classification score and hence we trained a moderately accurate enough regression \nmodel which can predict the value of energy with the least minimum possible delta but we did not focus more onto it.\n'''\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nresr=ResNet50(input_shape=(96,96,3), weights='imagenet', include_top=False)\nx = Flatten()(resr.output)\nx=Dense(10000,name='Dense3',activation='relu')(x)\nx=Dense(100,name='Dense2',activation='relu')(x)\nprediction_reg=Dense(1,name='output')(x)\nmodel_reg = Model(inputs=resr.input, outputs=prediction_reg)\nlr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.75 ** np.floor(epoch \/ 2)))\n#model_reg.summary()\n","130dbc91":"model_reg.compile(optimizer=tf.keras.optimizers.RMSprop(),loss='mae' , metrics=['mae'])\n#model_reg.fit(x_treg,y_treg,batch_size=16,epochs=50,validation_split=0.1,callbacks=[lr_sched])","4fb9718c":"model_reg.load_weights(reg_model_dir)\nmodel_reg.evaluate(x_treg,y_treg)\nmodel_reg.evaluate(x_vreg,y_vreg)","a86a4555":"del x_treg,x_vreg,y_treg,y_vreg,X_train,x_test,y_train,y_test,img_dat","c8b2200c":"## We unextract the tar file first to predict the results\n\nimport zipfile\nwith zipfile.ZipFile(private_tar_dir,\"r\") as zip_ref:\n    zip_ref.extractall(\".\/\")","106ea1ed":"def crop_center(img,cropx,cropy):\n    y,x = img.shape\n    startx = x\/\/2-(cropx\/\/2)\n    starty = y\/\/2-(cropy\/\/2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n\n\nid_l=[]\ndef create_dataset(l):\n    i=0\n    img_data_array=[]\n    class_name=[]\n    for dir1 in l:\n        for file in os.listdir(os.path.join(dir1)):\n            if file!='.DS_Store':\n                image_path= os.path.join(dir1,  file)\n                image=imread(image_path)\n                id_l.append(file.split('.')[0])\n                i+=1\n                image=resize(image, (288,288))\n                image=np.array(image)\n                image = image.astype('float32')\n                image=crop_center(image,96,96)\n                img_data_array.append(image)\n                class_name.append(dir1)\n                if i%1000==0:\n                    print('Loaded ' + str(i) + 'files')\n    return img_data_array\n\nf=[private_dir,public_dir]\n\n\ntest_data =create_dataset(f)\ntest_dat=np.stack((test_data,test_data,test_data),axis=-1)\ndel test_data","6358bd17":"with open(cls_model_arch, 'r') as json_file:\n    json_savedModel= json_file.read()\nmodel_cls=tf.keras.models.model_from_json(json_savedModel)\nmodel_cls.load_weights(cls_model_dir)\ncls=model_cls.predict(test_dat)\ncls=[each[0] for each in cls]","b9ea2514":"with open(reg_model_arch, 'r') as json_file:\n    json_savedModel= json_file.read()\nmodel_reg=tf.keras.models.model_from_json(json_savedModel)\nmodel_reg.load_weights(reg_model_dir)\nreg=model_reg.predict(test_dat)\nreg=[each[0] for each in reg]","3de48703":"new_pred=pd.DataFrame(zip(id_l,cls,reg),columns=['id','classification_predictions','regression_predictions'])\nnew_pred=new_pred.sort_values(by=['id'], ascending=[True],ignore_index=True)\nnew_pred","a77267ff":"public_id=[]\ndef public(l):\n    i=0\n    img_data_array=[]\n    class_name=[]\n    for dir1 in l:\n        for file in os.listdir(os.path.join(dir1)):\n            if file!='.DS_Store':\n                public_id.append(file.split('.')[0])\n                if i%1000==0:\n                    print('Loaded ' + str(i) + 'ids')\n                i+=1\n                \npublic([public_dir])","cf72afc3":"pred=new_pred.copy()\n\n\nz=list(zip(pred['id'],pred['classification_predictions'],pred['regression_predictions']))\n\nf=[]\ndef bins(idv,clsv, reg):\n    if idv in public_id:\n        if clsv>0.85:\n            if reg <=5:\n                reg=3.0\n            elif reg>=18:\n                reg=30.0\n            elif reg>5 and reg<18:\n                reg=10.0\n        elif clsv<0.25:\n            if reg>=15:\n                reg=20.0\n            elif reg<3.1:\n                reg=1.0\n            elif reg>4 and reg<9:\n                reg=6.0\n\n    else:\n        if clsv<0.25:\n            if reg <=5:\n                reg=3.0\n            elif reg>=18:\n                reg=30.0\n            elif reg>5 and reg<18:\n                reg=10.0\n        elif clsv>0.75:\n            if reg>=15:\n                reg=20.0\n            elif reg<3.3:\n                reg=1.0\n            elif reg>4 and reg<15:\n                reg=6.0\n    f.append([idv,clsv,reg])\n\nfor each in z:\n    idv,clsv,reg=each\n    bins(idv,clsv,reg)\nidl,clsl,regl=zip(*f)\npred['id'],pred['classification_predictions'],pred['regression_predictions']=idl,clsl,regl\npred","a85f733e":"import pandas as pd\npred=pd.DataFrame([1,2,3])","913d66ad":"pred.to_csv('final_sub.csv',index=False)\nos.chdir(r'.\/')\nFileLink(r'final_sub.csv')","fa118ff9":"## This Notebook Contains code of team SABHYA LOG for generation of submission for IDAO 2021","1763c8e8":"#### We trained the model till it converged both on the train-val and testing split and when we saw that the the loss on the training set was of the order of 10^(-5) and on test set of the order of 10^(-3) we stopped training further and  saved the model weights and its structure in h5 and json files respectively, so that we can reproduce those results afterwards.","b2b26e18":"## Training Classification Model","c974baf6":"\n#### During our analysis of the dataset, we obatined a few facts which came along the lines that if we purely rely on a regression based model, the Main problem was in predicting values of 1-3,6-10,20-30 which can be easily seen by applying PCA and K-Means clustering as to define the overlapping region between predicted values and true values.\n\n#### To tackle this situation we came along with an interesting idea as of binning the solution according the the predicted classification label and as stated earlier to achieve this result our classification model should have been near to perfect but should overfit the training data as it would highly affect our idea. \n#### Therefore this way if we can accurately classify the testing dataset then a small delta in the regression values will also be overcomed by binning on the basis of classification.\n#### This solution also removed the confusion between the energy labels being 1 or 3 where bright spots were visibly inexistent and thus creating confusion.\n\n#### We overcame this problem by Binnning Our predicted values.\n","56bddd5b":"**We cropped our images from center of size (96, 96) which we thought covers all important required features of given images and it is also good for computational purpose.**","9ba61674":"## Prediction of Results\n<div id='pred'><\/div>","24b462e6":"## Training Regression Model","15e59b0c":"## Main Idea","3a22356c":"## Datasets\n\n#### idao2021          -> Track1 dataset used for training\n\n#### dataset-creation  -> Final private test dataset\n\n#### 882-score-idao    -> Saved classifiaction model (mode_cls.h5, model_cls.json) and saved regression model (mode_reg.h5, model_reg.json)","7b13770c":"\n#### There can be maximum delta of 10**(-5) in the predicted values and submitted values due to randomness in gpu computation but that should not affect the submission by more than an error of 10^(-4).\n\n## Thank YOU!\n","b38c677a":"## Creation of training dataset(Slow)","5e595171":"# IMPORTING ALL NECESSARY LIBRARIES","9d5936ef":"<a href='#pred'><h3>Click here to directly jump to predictions using Saved Model and Weights<\/h3><\/a>","878e6182":"\n#### Again we trained the model till it converged both on the train-val and testing split  and when we saw that the the loss on the training set was of the order of 10^(-2) and on test set of the order of 10^(-2) we stopped training further and saved the model weights and its structure in h5 and json files respectively, so that we can reproduce those results afterwards.\n\n#### We additionally used an learning rate scheduler so as to avoid gradient jumps while training which could cause us trouble in the regression task.\n"}}