{"cell_type":{"0d5b1f30":"code","4cf5f8ed":"code","6ebab3f9":"code","75ee706f":"code","e1512f97":"code","f9282734":"code","4c80a47a":"code","bd51a106":"code","ca72a660":"code","e4430636":"code","132c55c1":"code","5eed5fde":"code","465d5f41":"code","e68fe2e5":"code","7ba0ba7b":"code","2098207f":"code","31487b11":"code","b43ee761":"code","a5213786":"code","89490317":"code","337e3cf5":"code","4dde22f8":"code","8948c84b":"code","4bfb14b8":"code","e29342c5":"code","6d4b39e1":"code","5663f91a":"code","46043a90":"code","18e0c850":"code","bcc675ed":"code","f0a147a9":"code","14c440b0":"code","b7dc63e4":"code","cedc6aaf":"code","61ff6c68":"code","86d1e924":"code","ff49b809":"code","c605e491":"code","35d6c63d":"code","d193aec5":"code","0580a6cc":"code","0935570d":"code","ee0666eb":"code","afc27858":"code","b15f7a43":"code","6fdde753":"code","c6184572":"code","88f00b38":"code","cc3af678":"code","4d0d1db9":"code","b479e6c0":"code","7899c0b3":"code","4d4747c6":"code","57a4f3bf":"code","1593e667":"code","c3a5b002":"code","484b99c5":"code","179570f7":"code","592d6d87":"code","619e20bd":"code","f60ab789":"code","501c2d05":"code","193f4495":"code","d5fafe01":"markdown","ef11adbb":"markdown","2dbda775":"markdown","52a481a2":"markdown","9da03b90":"markdown","adf5cc4c":"markdown","b6470155":"markdown","93a20a83":"markdown","e38218cb":"markdown","cccc31cc":"markdown","b7981770":"markdown","38bb2efc":"markdown","1648d484":"markdown","0ed7e612":"markdown","485eda52":"markdown","a17a28e6":"markdown","21ea5bd6":"markdown","46e0c5dd":"markdown","0d2986e6":"markdown","669f033b":"markdown","1d064e60":"markdown","bf575301":"markdown","30d43300":"markdown","d439170c":"markdown","f21ab115":"markdown","3abdc24d":"markdown","d55efe05":"markdown","c8eebcbf":"markdown","f379b7eb":"markdown","2ef0ecef":"markdown","b256a3c6":"markdown","8a3f5a75":"markdown","1e234cfb":"markdown"},"source":{"0d5b1f30":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom statsmodels.graphics.gofplots import qqplot\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import norm, skew, boxcox_normmax","4cf5f8ed":"hp_train = pd.read_csv(\"..\/input\/train.csv\")\nhp_test = pd.read_csv(\"..\/input\/test.csv\")","6ebab3f9":"hp_train.head()","75ee706f":"print(\"Train set size:\", hp_train.shape)\nprint(\"Test set size:\", hp_test.shape)","e1512f97":"train_ID = hp_train['Id']\ntest_ID = hp_test['Id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\nhp_train.drop(['Id'], axis=1, inplace=True)\nhp_test.drop(['Id'], axis=1, inplace=True)","f9282734":"col_name = ['GrLivArea','TotalBsmtSF','1stFlrSF','BsmtFinSF1','LotArea']\noutlier = [4500, 3000, 2500, 2000, 55000]\nfor i, c in zip(range(5), col_name):\n    fig = plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    plt.scatter(np.abs(hp_train[hp_train[c] < outlier[i]][c]), np.array(hp_train[hp_train[c] < outlier[i]]['SalePrice']), c='b')\n    plt.scatter(np.abs(hp_train[hp_train[c] >= outlier[i]][c]), np.array(hp_train[hp_train[c] >= outlier[i]]['SalePrice']), c='r')\n    plt.title('Before removing outliers for '+c)\n    plt.xlabel(c)\n    plt.ylabel('SalePrice')\n    \n    \n    plt.subplot(1,2,2)\n    plt.scatter(np.abs(hp_train[hp_train[c] < outlier[i]][c]), np.array(hp_train[hp_train[c] < outlier[i]]['SalePrice']), c='b')\n    plt.title('After removing outliers for '+c)\n    plt.xlabel(c)\n    plt.ylabel('SalePrice')\n    plt.show()\n","4c80a47a":"# removing outliers\nprint(hp_train.shape)\nhp_train = hp_train[hp_train['GrLivArea'] < 4500]\nhp_train = hp_train[hp_train['LotArea'] < 550000]\nhp_train = hp_train[hp_train['TotalBsmtSF'] < 3000]\nhp_train = hp_train[hp_train['1stFlrSF'] < 2500]\nhp_train = hp_train[hp_train['BsmtFinSF1'] < 2000]","bd51a106":"#Describing SalePrice\nhp_train.SalePrice.describe()","ca72a660":"#Understanding the distribution of SalePrice\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(hp_train['SalePrice'])\nplt.title('Understanding the distribution of SalePrice')\n\nplt.subplot(1,2,2)\nstats.probplot((hp_train['SalePrice']), plot=plt)\nplt.show()","e4430636":"print(\"Skewness: %f\" % hp_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % hp_train['SalePrice'].kurt())","132c55c1":"fig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.sqrt(hp_train['SalePrice']))\nplt.title('Distribution of SalePrice after square root transformation')\n\nplt.subplot(1,2,2)\nstats.probplot(np.sqrt(hp_train['SalePrice']), plot=plt)\nplt.title('Square root transformation')\nplt.show()\n\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.cbrt(hp_train['SalePrice']))\nplt.title('Distribution of SalePrice after cube root transformation')\n\nplt.subplot(1,2,2)\nstats.probplot(np.cbrt(hp_train['SalePrice']), plot=plt)\nplt.title('Cube root transformation')\nplt.show()\n\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.log1p(hp_train['SalePrice']))\nplt.title('Distribution of SalePrice after log transformation')\n\nplt.subplot(1,2,2)\nstats.probplot(np.log1p(hp_train['SalePrice']), plot=plt)\nplt.title('Log transformation')\nplt.show()\n","5eed5fde":"print(\"After log transformation\")\nhp_train['SalePrice'] = np.log1p(hp_train['SalePrice']) \nprint(\"Skewness: %f\" % (hp_train['SalePrice'].skew()))\nprint(\"Kurtosis: %f\" % (hp_train['SalePrice'].kurt()))","465d5f41":"hp_train.SalePrice.describe()","e68fe2e5":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = hp_train.corr()\n\n# select top 10 highly correlated variables with SalePrice\nnum = 10\ncol = corrmat.nlargest(num, 'SalePrice')['SalePrice'].index\ncoeff = np.corrcoef(hp_train[col].values.T)\n\nmask = np.zeros_like(coeff, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nfig = plt.figure(figsize=(15,10))\nsns.heatmap(coeff, vmin = -1, annot = True, mask = mask, square=True, xticklabels = col.values, yticklabels = col.values);","7ba0ba7b":"#Categorical Variables\nfig, axes = plt.subplots(ncols=4, nrows=4, \n                         figsize=(4 * 4, 4 * 4), sharey=True)\n\naxes = np.ravel(axes)\n\ncols = ['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n        'BsmtCond','GarageQual','GarageCond', 'MSSubClass','MSZoning',\n        'Neighborhood','BldgType','HouseStyle','Heating','Electrical','SaleType']\n\nfor i, c in zip(np.arange(len(axes)), cols):\n    ax = sns.boxplot(x=c, y='SalePrice', data=hp_train, ax=axes[i], palette=\"Set2\")\n    ax.set_title(c)\n    ax.set_xlabel(\"\")","2098207f":"ntrain = hp_train.shape[0]\nntest = hp_test.shape[0]\ny_train = hp_train.SalePrice.values\ndf_all = pd.concat((hp_train, hp_test), sort=False).reset_index(drop=True)\ndf_all.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"df_all size after concatenation of train and test data is : {}\".format(df_all.shape))","31487b11":"df_all.info()","b43ee761":"missing_data = pd.DataFrame({'total_missing': df_all.isnull().sum(), 'perc_missing': (df_all.isnull().sum()\/len(df_all))*100})\nlen(missing_data[missing_data.total_missing>0])","a5213786":"fig = plt.figure(figsize=(15,10))\nmissing_data[missing_data.total_missing>0].sort_values(by='perc_missing')['perc_missing'].plot(kind='barh')\nplt.xlabel('Percentage of missing values')\nplt.ylabel('Features')\nplt.title('Percentage of missing values for different features')\nplt.show()","89490317":"#Creating a list for features with No amenities\nno_amen_cat = ['PoolQC','MiscFeature','Alley','Fence','GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'FireplaceQu']\nno_amen_num = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']","337e3cf5":"df_all1 = df_all.copy() #Creating a copy of original dataframe","4dde22f8":"for col in no_amen_cat:\n    df_all1[col] = df_all1[col].fillna('None')","8948c84b":"for col in no_amen_num:\n    df_all1[col] = df_all1[col].fillna(0)","4bfb14b8":"mode_replace = ['Electrical', 'MSZoning', 'Utilities', 'SaleType', 'KitchenQual', 'Exterior2nd', 'Exterior1st']\nfor col in mode_replace:\n    df_all1[col] = df_all1[col].fillna(df_all1[col].mode()[0]) ","e29342c5":"df_all1[\"Functional\"] = df_all1[\"Functional\"].fillna(\"Typ\")","6d4b39e1":"df_all1.isna().sum()[df_all1.isna().sum()>0].sort_values(ascending=False)","5663f91a":"df_all1[\"LotFrontage\"] = df_all1.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","46043a90":"df_all1.isna().sum()[df_all1.isna().sum()>0].sort_values(ascending=False)","18e0c850":"df_all2 = df_all1.copy() #Creating a copy of original dataframe","bcc675ed":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_all2[c].values)) \n    df_all2[c] = lbl.transform(list(df_all2[c].values))\n\n# shape        \nprint('Shape : {}'.format(df_all2.shape))","f0a147a9":"numeric_feats = df_all2.dtypes[df_all2.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = df_all2[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","14c440b0":"skewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.45\nfor feat in skewed_features:\n    df_all2[feat] = boxcox1p(df_all2[feat], lam)","b7dc63e4":"#Dropping dominating features with more than 95% same values\ndf_all2 = df_all2.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis = 1)","cedc6aaf":"df_all2 = pd.get_dummies(df_all2).reset_index(drop=True)\nprint(df_all2.shape)","61ff6c68":"#Adding new features from existing features\ndf_all2['TotalSF']=df_all2['TotalBsmtSF'] + df_all2['1stFlrSF'] + df_all2['2ndFlrSF']\n\ndf_all2['Total_sqr_footage'] = (df_all2['BsmtFinSF1'] + df_all2['BsmtFinSF2'] +\n                                 df_all2['1stFlrSF'] + df_all2['2ndFlrSF'])\n\ndf_all2['Total_Bathrooms'] = (df_all2['FullBath'] + (0.5 * df_all2['HalfBath']) +\n                               df_all2['BsmtFullBath'] + (0.5 * df_all2['BsmtHalfBath']))\n\ndf_all2['Total_porch_sf'] = (df_all2['OpenPorchSF'] + df_all2['3SsnPorch'] +\n                              df_all2['EnclosedPorch'] + df_all2['ScreenPorch'] +\n                              df_all2['WoodDeckSF'])\n","86d1e924":"#Creating a binary features from existing features\ndf_all2['haspool'] = df_all2['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all2['has2ndfloor'] = df_all2['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all2['hasgarage'] = df_all2['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all2['hasbsmt'] = df_all2['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all2['hasfireplace'] = df_all2['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","ff49b809":"df_all2.shape","c605e491":"train = df_all2[:ntrain]\ntest = df_all2[ntrain:]\n","35d6c63d":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom  sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.linear_model import LassoLarsCV, RidgeCV, ElasticNetCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVR\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\nfrom tpot.builtins import ZeroCount\nfrom sklearn.decomposition import PCA\nfrom imblearn.pipeline import make_pipeline","d193aec5":"#Defining score as we will need to score for multiple models\ndef score(model, train_x, train_y):\n    score = np.sqrt(-cross_val_score(model, train_x, train_y, cv=5, scoring=\"neg_mean_squared_error\"))\n    print(score.mean(), score.std())","0580a6cc":"from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=1,verbosity=2,scoring='neg_mean_squared_error')","0935570d":"tpot.fit(train.values, y_train)","ee0666eb":"#top performer\ntpot.score(train.values, y_train)","afc27858":"models_tested = pd.DataFrame(tpot.evaluated_individuals_).transpose()","b15f7a43":"#We will using top 5 models for our final submission\nmodels_tested.sort_values(['internal_cv_score'], ascending=False).head(10)","6fdde753":"exported_pipeline1 = make_pipeline(\n    StandardScaler(),\n    ElasticNetCV(l1_ratio=0.7000000000000001, tol=0.001, cv=5)\n)\n\nexported_pipeline1.fit(train.values, y_train)","c6184572":"score(exported_pipeline1, train.values, y_train)\nexported_pipeline_pred1 = exported_pipeline1.predict(train.values)\nscore(exported_pipeline1, train.values, exported_pipeline_pred1)","88f00b38":"exported_pipeline2 = make_pipeline(\n    MinMaxScaler(),\n    ElasticNetCV(l1_ratio=0.7000000000000001, tol=0.01, cv=5)\n)\n\nexported_pipeline2.fit(train.values, y_train)","cc3af678":"score(exported_pipeline2, train.values, y_train)\nexported_pipeline_pred2 = exported_pipeline2.predict(train)\nscore(exported_pipeline2, train.values, exported_pipeline_pred2)","4d0d1db9":"exported_pipeline3 = make_pipeline(\n    ZeroCount(),\n    PCA(iterated_power=4, svd_solver='randomized'),\n    RidgeCV()\n)\n\nexported_pipeline3.fit(train.values, y_train)","b479e6c0":"score(exported_pipeline3, train.values, y_train)\nexported_pipeline_pred3 = exported_pipeline3.predict(train)\nscore(exported_pipeline3, train.values, exported_pipeline_pred3)","7899c0b3":"exported_pipeline4 = make_pipeline(\n    LassoLarsCV(normalize=True, max_iter=60, cv=5)\n)\n\nexported_pipeline4.fit(train.values, y_train)","4d4747c6":"score(exported_pipeline4, train.values, y_train)\nexported_pipeline_pred4 = exported_pipeline4.predict(train)\nscore(exported_pipeline4, train.values, exported_pipeline_pred4)","57a4f3bf":"exported_pipeline5 = make_pipeline(\n    RidgeCV()\n)\n\nexported_pipeline5.fit(train.values, y_train)","1593e667":"score(exported_pipeline5, train.values, y_train)\nexported_pipeline_pred5 = exported_pipeline5.predict(train)\nscore(exported_pipeline5, train.values, exported_pipeline_pred5)","c3a5b002":"#Checking score after applying equal wightage to all models\nnp.sqrt(mean_squared_error(y_train,(exported_pipeline_pred1*0.2 + exported_pipeline_pred2*0.2 +\n               exported_pipeline_pred3*0.2 + exported_pipeline_pred4*0.2 + exported_pipeline_pred5*0.2)))","484b99c5":"#Here we will be deciding weightage of the models based on random selection which gives the least loss\nbest_value = []\nmin_value=1\nfor i in range(10000):\n    random = np.random.dirichlet(np.ones(5),size=1)\n    best_value.append(np.sqrt(mean_squared_error(y_train,(exported_pipeline_pred1*random[0][0] + exported_pipeline_pred2*random[0][1] +\n               exported_pipeline_pred3*random[0][2] + exported_pipeline_pred4*random[0][3] + exported_pipeline_pred5*random[0][4]))))\n     \n    if(np.min(best_value) < min_value):\n        min_value = np.min(best_value)\n        min_array = random","179570f7":"np.min(best_value)","592d6d87":"min_array","619e20bd":"def blend_models(sub):\n    return (exported_pipeline1.predict(sub)*min_array[0][0] + \n            exported_pipeline2.predict(sub)*min_array[0][1] + \n            exported_pipeline3.predict(sub)*min_array[0][2] + \n            exported_pipeline4.predict(sub)*min_array[0][3] + \n            exported_pipeline4.predict(sub)*min_array[0][4])","f60ab789":"print('Predict submission')\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models(test)))","501c2d05":"submission.head()","193f4495":"submission.to_csv('submission.csv',index=False)","d5fafe01":"**Keep calm and lets start treating missing values**","ef11adbb":"For categorical features without any orders we will be using get_dummies to convert into numerical form","2dbda775":"**Kudos!! We did it**","52a481a2":"In the beginning of the notebook, we already transformed right skewed target feature (SalePrice) into normal distribution, but there we compared multiple transformation and finally went with log transformation. Here instead we will be using Box Cox transform, which actually helped to increase the accuracy of our prediction<br>","9da03b90":"#### Competition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. <br>\n\nApproach : For this problem we will first analyze all the features affecting the price of the house and thereby building a predictive model to predict house price (price is a number from some defined range, so it will be regression task). **Let's begin**","adf5cc4c":"### Treating Missing values","b6470155":"There is famous quote in data science \"*Quality of your inputs decide the quality of your output*\", so let's get started!!","93a20a83":"## Introduction\n\n### Welcome!\nHey All, this is my first full end-to-end Kaggle project. We all will learn about automated machine learning with help TPOT library along with the basic framework for any data science competition. My goal is to learn and contribute to the data science community. \n\nYou all must be wondering....**What's in it for me?**\n* Understanding the problem\n* EDA and data visualization\n* Feature Engineering\n* Modelling \n\n\nI have referred to some of the best kernel, to name a few\n* <a href=\"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-pythonComprehensive\"> Data Exploration with Python<\/a>\n* <a href=\"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">\nStacked Regressions to predict House Prices<\/a>","e38218cb":"First, lets understand each features with missing values<br>\nAnd, what NA in the following features means - \n\n| Features | NA here means | Treament (Categorical \/ Numeric) |\n| --- | --- | --- |\n| PoolQC | No pool | None \/ 0 |  \n| MiscFeature | No misc feature | None \/ 0 |\n| Alley | No alley access | None \/ 0 |\n| Fence | No fence | None \/ 0 |\n| FireplaceQu | No fireplace | None \/ 0 |\n| Garage-related features | No garage | None \/ 0 |\n| Basement-related features | No basement | None \/ 0 |\n| Masonay-related features | No masonry veneer | None \/ 0 |\n| LotFrontage | Information not available | KNN |\n| Electrical | Information not available | Mode |\n| MSZoning | Information not available | Mode |\n| Utilities | Information not available | Mode |\n| SaleType | Information not available | Mode |\n| KitchenQual | Information not available | Mode |\n| Exterior - related | Information not available | Mode |\n| Functional | Typical | Typ |\n","cccc31cc":"## Modeling","b7981770":"#### Transforming distribution of target variable into normal distribution","38bb2efc":"### Outliers\nA value that \"lies outside\" (is much smaller or larger than) most of the other values in a set of data.<br>\nThere are numerous unfavourable impacts of outliers in the data set:\n* It increases the error variance and reduces the power of statistical tests\n* If the outliers are non-randomly distributed, they can decrease normality\n* They can bias or influence estimates that may be of substantive interest\n* They can also impact the basic assumption of Regression and other statistical model assumptions.\n","1648d484":"Train Test Split","0ed7e612":"### Our Target variable is **SalePrice**","485eda52":"Common transformations of right skewed data includes square root, cube root, and log transformations.","a17a28e6":"> Selecting best 5 models from TPOT","21ea5bd6":"### Importing required libraries","46e0c5dd":"### Reading the housing price data","0d2986e6":"## Feature Engineering","669f033b":"1. I hope this kernel helps you all!","1d064e60":"> ## Performing EDA to know more about data followed by preprocessing and data preparation","bf575301":"### Concatenation of train and test datasets","30d43300":"For our model selection we will be using TPOT, which is *An Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming*. Go through <a href=\"https:\/\/github.com\/EpistasisLab\/tpot\"> TPOT<\/a> for further information\n","d439170c":"So, we treated 33 out of 34 features with missing value. **Hold on for one last feature**<br>\nWe will be filling all the missing values for LotFrontage using median value from its neighbours, as area of each street connected to the house property most likely to have a similar area to other houses in its neighborhood","f21ab115":"## Understanding the problem","3abdc24d":"Lets now look at the skewness of the features. **Now why is this important?** <br>\nParameter estimation is based on the minimization of squared error. observations in skewed data will make a disproportionate effect on the parameter estimates. Hence we need to transform highly skewed features into normal distribution ","d55efe05":"### Relationships with SalePrice\nWe studied the target variable, now we will be analyzing the features which influences the target variable.","c8eebcbf":"Observations from the distribution\n* The Sales price distribution is right skewed, which is not normal. Hence we need to transform into a normal distribution\n* Also from Quantile-Quantile plot it is evident that the distribution is not normal\n* If we observe distribution properly we see that it has peak, which is also evident from kurtosis value\n","f379b7eb":"We will go forward with **log transformation**, as it is good fit compared to other transformation","2ef0ecef":"**Next Steps:**\n* We will train TPOT model for further generations\n* We can further use deep learning to solve this, as it is best choice for non-linear features ","b256a3c6":"Well it is no surprise that our task now is to somehow extract the information out of the categorical variables, but choosing the right encoder is also important<br>\nFollowing is the article on same topic\nhttps:\/\/towardsdatascience.com\/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b<br>\nLabel Encoding refers to converting the labels into numeric form for ordinal features so as to convert it into the machine-readable form by retaining the order","8a3f5a75":"We can extend our observations on missing data and the datatypes here:\n\n* Out of 79 columns, 34 columns have incomplete data, which we need to treat\n* Quite a lot of data seems to be missing in PoolQC, MiscFeature, Alley and Fence, as most houses won't have pool, fence and alley\n* Alot of the columns have strings (object datatype), which needs to be parsed into the category datatype ","1e234cfb":"* Based on the correlation matrix, we can see that the features related with quality (OverallQual,FullBath, YearBuilt, YearRemodAdd) and the size (GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF) influences the sale price, which might impact our predictions\n* And based on the correlation plot for the categorical features we can conclude that some variables have influence on the SalePrice and the OverAllQuality seems to have the highest influence"}}