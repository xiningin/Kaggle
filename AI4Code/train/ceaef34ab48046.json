{"cell_type":{"1cd082ca":"code","1a4726e1":"code","d4f9eb54":"code","2112ac9f":"code","53dd4921":"code","c2e0fed2":"code","44c01a05":"code","30b9800d":"code","56617f9d":"code","e11f5b1b":"code","f4a9d166":"code","e7bc0aec":"code","192610df":"code","2ff518d7":"code","c1e1ba0b":"code","7b8adb7f":"code","1083d91c":"code","f0057632":"code","d9bc1077":"code","ced725ac":"code","c0b65f86":"code","535e7888":"code","d3d2acaa":"code","1a941120":"code","747e5525":"code","0acf37ea":"code","21304296":"code","8278783b":"code","12c123ad":"code","6237d3c2":"code","1db59bc4":"code","80262eea":"code","e467f2b0":"code","51e96c18":"markdown","6367e21a":"markdown","8e169d13":"markdown","0f914f99":"markdown","3f12f3f3":"markdown","d955319b":"markdown","adb6419d":"markdown","0c7f880d":"markdown","150ace1d":"markdown","000d611a":"markdown","d19dd043":"markdown","9d1425b9":"markdown","9eabe437":"markdown","121497dd":"markdown","42e304a5":"markdown","2d37ffab":"markdown","752d2340":"markdown","143538af":"markdown","5fed6b2d":"markdown","b0735786":"markdown","2710c715":"markdown","cea19471":"markdown","ec667cb5":"markdown","bb8eb87b":"markdown","622de19c":"markdown","fa5cb875":"markdown","7ecbff2d":"markdown","6779c177":"markdown"},"source":{"1cd082ca":"#First we are importing the libraries we are going smeelingly use. Later on, we will import other libraries if they are necessary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport glob\nimport json\nimport re\nimport string\nimport networkx as nx\nimport gc\n\nfrom IPython.utils import io\n#with io.capture_output() as captured:\n\n # !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n\nfrom pprint import pprint\nfrom copy import deepcopy\n\nfrom collections import  Counter\nfrom tqdm.notebook import tqdm\n#LANGUAGE DETECTION\n!pip install langdetect\nimport langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n#NATURAL LANGUAGE PROCESSING\n\n#1. NLTK\n\n\n!pip install nltk\nimport nltk as nlp\n#nltk.download()\nfrom nltk import tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\n\n#from rake_nltk import Rake\n#2. SPACY\n\n#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz \n!pip install scispacy\nimport scispacy\n#import en_ner_bionlp13cg_md\n#import en_core_sci_lg\n\nfrom spacy import displacy\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\n\n#STOPWORDS IN ENGLISH SPACY\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n#TOKENIZER\nfrom spacy.tokens import Span\n\n#GENSIM\n\nimport gensim\n# SPACY MODEL FOR LDA visualization\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\n#TENSOR FLOWS\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\n#MATCHER\nfrom spacy.matcher import Matcher \n\n#SKLEARN\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#from sklearn.cluster import DBSCAN\n#os.listdir('..\/input\/CORD-19-research-challenge\/')\n#with open('..\/input\/CORD-19-research-challenge\/metadata.readme', 'r') as f:\n#   data = f.read()\n#    print(data)","1a4726e1":"os.listdir( '\/kaggle\/input\/all-cleaned')\n","d4f9eb54":"biorxiv_clean = pd.read_csv('\/kaggle\/input\/all-cleaned\/biorxiv_clean.csv')\n","2112ac9f":"biorxiv_clean.head(2)","53dd4921":"biorxiv_clean.text[0]","c2e0fed2":"df = biorxiv_clean\ndf = df.abstract.dropna()\ndata = df.values.tolist()","44c01a05":"%%time\n#ref : https:\/\/gist.github.com\/gaurav5430\/8d7810495ec3f914ffb151458f352c60\n\n'''import tensorflow_hub as hub\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [sentence_list[i] for i in indices]\n\n\nmeta=pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\nmodule_url = \"..\/input\/universalsentenceencoderlarge4\" \nembed = hub.load(module_url)\n\n\n# Creating an empty Dataframe with column names only\nsimsentence = pd.DataFrame()\n\ntitles=meta['title'].fillna(\"Unknown\")\nembed_vectors=embed(titles[:5000].values)['outputs'].numpy()\nsentence_list=titles.values.tolist()\nfor i in range(5):\n\n    sentences=titles.iloc[i]\n    #print(\">>>>>>>>>>>>Using title Find similar research papers for :\",sentences, \"<<<<<<<<<<<<\")\n\n    similarity_matrix=prepare_similarity(embed_vectors)\n    similar=get_top_similar(sentences,sentence_list,similarity_matrix,6)\n    for sentence in similar:\n        #print(sentence)\n        simsentence = simsentence.append({'sentence': sentences, 'similar': sentence}, ignore_index=True)\n        #print(\"\\n\") '''","30b9800d":"simsentence = pd.read_csv('..\/input\/simsentence\/simsentence.csv')","56617f9d":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","e11f5b1b":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=20) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=20)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[1]]])","f4a9d166":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis',\n                   'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n                   'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n                   'al.', 'Elsevier', 'PMC', 'CZI', 'www'])","e7bc0aec":"#https:\/\/github.com\/cjriggio\/classifying_medical_innovation\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","192610df":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[1])\nprint(data_lemmatized[:1])","2ff518d7":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","c1e1ba0b":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","7b8adb7f":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=8, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True\n                                        )\n#pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]\n%%time\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","1083d91c":"pyLDAvis.save_html(vis, '.\/l\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","f0057632":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=100,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False, \n                               num_words=30)\n\nfig, axes = plt.subplots(5, 1, figsize=(10,20), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=500)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i + 1), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","d9bc1077":"from matplotlib.ticker import FuncFormatter\n\n# Plot\n\nfig, ax1  = plt.subplots(1, figsize=(10, 10))\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x + 1)+ ':\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Number of Documents')\nax1.set_ylim(0, 2000)\n\nplt.show()\ndf_dominant_topic_in_each_doc\nfig, ax2  = plt.subplots(1, figsize=(10, 10))\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","ced725ac":"from sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 5\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=800, plot_height=600)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","c0b65f86":"from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n    # # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n    # # Run in terminal: python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n    # # Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:2])","535e7888":"vectorizer = CountVectorizer(analyzer='word', min_df=10,                        # minimum reqd occurences of a word \n                              stop_words='english',             # remove stop words\n                              lowercase=True,                   # convert all words to lowercase\n                              token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n                              # max_features=50000,             # max number of uniq words\n                             )\ndata_vectorized = vectorizer.fit_transform(data_lemmatized)","d3d2acaa":"## Define Search Param for GridSearch\n%%time\nsearch_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation(n_jobs=-1)\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n\n# Do the Grid Search\nmodel.fit(data_vectorized)","1a941120":"best_lda_model = model.best_estimator_\n\n# Model Parameters\nprint(\"Best Model's Params: \", model.best_params_)\n\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", model.best_score_)\n\n# Perplexity\nprint(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\ncategories = list(df.Dominant_Topic.unique())\ncategories\nX_embedded[:,1].shape\n# sns settings\nsns.set(rc={'figure.figsize':(10,10)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered\")\n# plt.savefig(\"plots\/t-sne_covid19_label.png\")\nplt.show()","747e5525":"#CAMBIAR NON-COMMON POR EL M\u00cdO\ntype(clean_noncomm_use.abstract.dropna().tolist())\nfrom gensim.summarization.summarizer import summarize\nsummarize(clean_noncomm_use.abstract.dropna().to_string())\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re\n#ref : https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in clean_noncomm_use.abstract.dropna():\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list\nsentences[:3]","0acf37ea":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()\nlen(word_embeddings)\n# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","21304296":"def remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\n# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\nVector Representation of Sentences\n\n# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()\nsentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","8278783b":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\nimport torch\ntorch.cuda.is_available()\n%%time\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(1000):\n    for j in range(1000):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n            #print(sim_mat[i][j])","12c123ad":"%%time\nimport networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)","6237d3c2":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\nfor i in range(50):\n    print(ranked_sentences[i][1])\n    print('\\n\\n')","1db59bc4":"import os\nimport json\nfrom transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nBART_PATH = 'bart-large'\nbart_model = BartForConditionalGeneration.from_pretrained(BART_PATH, output_past=True)\nbart_tokenizer = BartTokenizer.from_pretrained(BART_PATH)\ndef bart_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text)\n    input_text = ' '.join(input_text.split())\n    input_tokenized = bart_tokenizer.encode(input_text, return_tensors='pt')\n    summary_ids = bart_model.generate(input_tokenized,\n                                      num_beams=int(num_beams),\n                                      no_repeat_ngram_size=3,\n                                      length_penalty=2.0,\n                                      min_length=100,\n                                      max_length=int(num_words),\n                                      early_stopping=True)\n    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    return output[0]\ndf = clean_pmc\ndf = df.abstract.dropna()\nabstracts = df.values.tolist()\n\nlen(abstracts)\n#summarizing\nfor i in range(20):\n    try:\n        print('paper  ',i + 1, \" : \\n\" )\n        print(bart_summarize(abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")","80262eea":"df1 = df1.abstract.dropna()\ndf1abstracts = df.values.tolist()\n\nlen(df1abstracts)\nT5_PATH = 't5-base'\nt5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH, output_past=True)\nt5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef t5_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text).replace('\\n', '')\n    input_text = ' '.join(input_text.split())\n    input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    summary_task = torch.tensor([[21603, 10]]).to(device)\n    input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n    summary_ids = t5_model.generate(input_tokenized,\n                                    num_beams=int(num_beams),\n                                    no_repeat_ngram_size=3,\n                                    length_penalty=2.0,\n                                    min_length=30,\n                                    max_length=int(num_words),\n                                    early_stopping=True)\n    output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    return output[0]","e467f2b0":"%%time\nfor i in range(20):\n    try:\n        print('BioArvix paper  ',i + 1, \" : \\n\" )\n        print(t5_summarize(df1abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")\nsimsentence.head(5)\n\"\"\"\nThis is a simple application for sentence embeddings: semantic search\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n# taken from : https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/examples\/application_semantic_search.py\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = df.values.tolist()\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['Range of incubation periods for the disease in humans','risk factors of covid-19','cure for covid-19', 'antiviral covid-19 success treatment','Does smoking or pre-existing pulmonary disease increase risk of COVID-19?', 'risk of fatality among symptomatic hospitalized patients']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))","51e96c18":"MINING COVID \n\nThis part of the code is directly developped following the mod\n\nThe first step would be to concatenate all the text contained in the articles Then split the text into individual sentences In the next step, we will find vector representation (word embeddings) for each and every sentence Similarities between sentence vectors are then calculated and stored in a matrix The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation Finally, a certain number of top-ranked sentences form the final summary\n\nCONVERT ABSTRACT TO LIST","6367e21a":"Create Dictionary,Corpus and Document Frequency","8e169d13":"Build the bigram and trigram models using gensim","0f914f99":"Download GloVe Word Embeddings","3f12f3f3":"Now let's save this new dataframe as csv file for possible further research","d955319b":"Summarization Task using T5 model","adb6419d":"For this model, we used mostly the ideas from the https:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers\/comments notebook, but we adapted the code to our particular case. \n\nFor the pre-processing, go here: https:\/\/www.kaggle.com\/patriciacanton\/proyect-pre-processing\n\nThis model explores one of the possibilities. The others can be found in my main page: https:\/\/www.kaggle.com\/patriciacanton","0c7f880d":"Human readable format of corpus (term-frequency)","150ace1d":"Save Topics as html format","000d611a":"Wordcloud of Top N words in each topic","d19dd043":"summariing first 20 papers abstract of bioarvix\n\n","9d1425b9":"Get topic weights and dominant topics","9eabe437":"We go and reach for the stopwords previously defined stopword, from https:\/\/www.kaggle.com\/patriciacanton\/proyect-pre-processing","121497dd":"checking everythin is ok with the first paper\n","42e304a5":"Define functions for stopwords, bigrams, trigrams and lemmatization","2d37ffab":"Similarity Matrix preparation","752d2340":"Topic Distribution Plot","143538af":"WE ARE GOING TO FOCCUS ON BIORXIV subset and on the abstract part","5fed6b2d":"Summary Extraction : Extract top 50 sentences as the summary","b0735786":"*******************\n","2710c715":"The function below converts sentences to words using gensim","cea19471":"**************************************\n","ec667cb5":"BART","bb8eb87b":"REMOVE STOPWORDS","622de19c":"Applying PageRank Algorithm","fa5cb875":"Best Model","7ecbff2d":"Topic modeling with LDA, in order to have a first sight at the most common words\n","6779c177":"Find similar research papers using universalsentenceencoderlarge4"}}