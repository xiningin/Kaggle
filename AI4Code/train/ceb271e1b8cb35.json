{"cell_type":{"1f09c812":"code","0527d0ef":"code","2c96d974":"code","49bf18d7":"code","ebc0b4a4":"code","95cb4891":"code","aba9e87a":"code","4a811f81":"code","d9ff99e1":"code","71303e1d":"code","7a5ee878":"code","bcceb974":"code","afd9130a":"code","21838e09":"code","4ffa3865":"code","d311ce4b":"code","4b3a17ed":"code","82c4d233":"code","8eb8b85a":"code","138693cc":"code","68fddddb":"code","4c8eca83":"code","b49be001":"code","7f981231":"code","da028453":"code","7e36bf82":"code","c41b0116":"code","ba44fb3a":"code","8480db38":"code","b148e4bf":"code","1699472b":"code","ba6ebed3":"code","05063836":"code","ba6da79f":"code","4cd2d351":"code","31501e81":"code","5e5d1891":"code","28146f38":"code","c96e5039":"code","b6129961":"code","189d194b":"code","d848dfad":"code","9e7f5147":"code","f81d8e79":"code","28caccbd":"code","1f810f9c":"code","c1a4fa35":"code","46dd0d23":"code","e0fb37f0":"code","67fcf479":"code","67757aff":"code","89b12d61":"code","89eec403":"code","a7f53893":"code","55dc7129":"code","eaa31634":"code","a9558309":"code","689e8e19":"code","26943c85":"code","de36e239":"code","40c1ab29":"code","cd9a8089":"code","75d8e86a":"code","d352dd91":"code","b529fb59":"code","33511084":"code","10e32b40":"code","37980676":"code","cb8ddfcd":"code","a4dd7417":"code","c91d7c48":"code","457c18f1":"code","f9271ead":"code","958b2573":"code","b87a76c9":"code","162bb0a1":"code","28acb8d2":"code","4caa96ff":"code","e85eae00":"code","95b9ae0b":"code","2cb836b5":"code","6ee565ff":"code","1d963106":"code","0d496591":"code","497d6ae0":"code","4e023470":"code","a511f2c7":"code","7c4203b5":"code","af403326":"code","05b1f940":"code","b236dce4":"code","046853c2":"code","44636298":"code","ca691f8b":"code","5809e881":"code","ff0c991d":"code","627dea56":"code","5da78d0f":"code","aa371a95":"code","056d1891":"code","09c895e3":"code","ca1ad3a4":"code","88655fb5":"code","1a54cfd0":"code","5279b86e":"code","980add76":"code","42d0ba57":"code","83ef8c9c":"code","ea2820af":"code","0fa61ff6":"code","6e050398":"code","e90ddda8":"code","53fdaf59":"code","6c16c632":"code","427a13fe":"code","f55dee65":"code","ca763513":"code","8e8a03a3":"markdown","997cecb4":"markdown","23e0bd23":"markdown","5c9ddfa4":"markdown","40764512":"markdown","8133b3ae":"markdown","658769b1":"markdown","e2f9410c":"markdown","e1f7716d":"markdown","3ae981f4":"markdown","b286bb0b":"markdown","32a7015a":"markdown","d54d1559":"markdown","fa59df79":"markdown","6c2c69a5":"markdown","dde2bb7f":"markdown","d8dc4347":"markdown","9645cf0a":"markdown","119751c2":"markdown","9ab29e1e":"markdown","12390c2a":"markdown","89e05662":"markdown","9d9372ba":"markdown","38430001":"markdown","f698be6c":"markdown","0925f2f7":"markdown","ec31161a":"markdown","af0f7f4d":"markdown","29952644":"markdown","9081883c":"markdown","c9f321a0":"markdown","c2e13c1d":"markdown","e7d16590":"markdown","8804d065":"markdown","2be07171":"markdown","7a98b755":"markdown","edbd6f0d":"markdown","15ea86de":"markdown","1ba1430d":"markdown","16806602":"markdown","67d9e9a5":"markdown","742fa149":"markdown","cfdab7bc":"markdown","d0588d2b":"markdown","9cec0655":"markdown","536e1e63":"markdown","23465677":"markdown","6936024c":"markdown","154bb268":"markdown","0fc114b2":"markdown","685f98e2":"markdown","09b4b2f7":"markdown","13ccc86f":"markdown","8a053340":"markdown","c2c70f1d":"markdown","0f2bcad2":"markdown","cca4f41e":"markdown","85aac663":"markdown","dd345550":"markdown","3ca479f8":"markdown","e0eb8524":"markdown","630e271f":"markdown","7e0630ae":"markdown","9f9594b3":"markdown","fa160ff3":"markdown","0631a573":"markdown","76e59a16":"markdown","bd94eb35":"markdown","3942b9d8":"markdown","504694b9":"markdown","b37f6075":"markdown","77ff7309":"markdown","4c137cc0":"markdown","f106c1f4":"markdown","62b3d454":"markdown","4cfb58d2":"markdown","81ee3a94":"markdown","4bae2877":"markdown","9a475a63":"markdown","b08afd1a":"markdown","69bc8a14":"markdown","27bb2d5e":"markdown","bd94c75a":"markdown","0c7d8fb8":"markdown","e9dc374c":"markdown","f34da53e":"markdown","f562f8b9":"markdown","2f42e951":"markdown","f9bdd0ec":"markdown","82c1642a":"markdown","7dd4ac1c":"markdown","d4ca50b8":"markdown","e72ba634":"markdown","5c0bd741":"markdown","3eeeef8e":"markdown","4978f135":"markdown","0539149a":"markdown","8d17651b":"markdown","70238913":"markdown","38c28c8e":"markdown","a07867a7":"markdown","52faa634":"markdown","de3300f6":"markdown"},"source":{"1f09c812":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore')\nimport missingno as msno\n%matplotlib inline","0527d0ef":"def response_variable_distribution(response_variable, train_df):\n    '''\n    Gets the distribution, mean and standard deviation of the response variable. Also provides a probability plot of sample data against the quantiles of a \n    specified theoretical distribution as well as providing a best-fit line for the data.\n    \n    Parameters\n    ----------\n    \n    response_variable: str\n        str object containing the response variable in the analysis.\n    \n    train_df: dataframe\n        dataframe object containing the original values of the response variable\n    \n    Returns\n    -------\n    \n    distplot: plot\n        distplot of y variable as well as its mu and sigma values.\n    \n    probplot: plot\n        probplot of y variable as well quantiles and least squares fit.\n    '''\n    \n    # set up the axes figure layout and total size for the plot\n    fig, ax = plt.subplots(nrows=1,ncols=2)\n    fig.set_size_inches(12, 12)\n\n    # plot the distribution and fitted parameters on the first subplot\n    plt.subplot(1,2,1)\n    sns.distplot(train_df[response_variable] , fit=norm);\n    (mu, sigma) = norm.fit(df_train[response_variable])\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title(response_variable + ' distribution')\n\n    # plot the the QQ-plot with quantiles and least squares fit on the second subplot\n    plt.subplot(1,2,2)\n    res = stats.probplot(train_df[response_variable], plot=plt)\n    plt.show()","2c96d974":"def percent_missing_data(df):\n    '''\n    Gets the percentage of data missing (where data missing is greater than 0) for every column in the dataframe and provides this information in a barplot \n    \n    Parameters\n    ----------\n    \n    df: dataframe\n        dataframe object containing the dataframe to be cheacked for missing values.\n    \n    Returns\n    -------\n    \n    barplot: plot\n        barplot of percent of all missing values that are greater than 0.\n    '''\n    # calculate total isnull values then work out their percentage of total values then concat these into a dataframe\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    # create a mask to remove any columns from dataframe that dont have missing values\n    mask = missing_data['Percent'] > 0\n    missing_data = missing_data[mask]\n    \n    \n    # plot the the barplot with missing value percentage, and x labels rotated by 90 degrees, on the created axis\n    f, ax = plt.subplots(figsize=(15, 12))\n    plt.xticks(rotation='90')\n    sns.barplot(x=missing_data.index, y=missing_data.Percent)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)","49bf18d7":"def variable_imputation_check(string,df,train_df,response_value,subplots='N',ncols=1,add_rows=1,fig_size=(12,12)):\n    '''\n    Gets the counts of the categories present, within the various columns, containing the given string. For each of these a countplot or regplot is then displayed.\n    \n    Parameters\n    ----------\n    \n    string: str\n        str object containing the string to select columns from the dataframe.\n        \n    df: dataframe\n        dataframe object containing the datapoints that need to be counted.\n        \n    train_df: dataframe\n        dataframe object containing the original values of the variable that needs to be predicted datapoints that need to be counted.\n        \n    response_value: str\n        str object containing the response varaible from the tainset as a string.\n        \n    subplots: str (default='N')\n        str object containing 'Y' or 'N'. Used if multiple countplots need to be drawn\n        \n    ncols: int (default=1)\n        int value specifiying the amount of columns to create within the sub plot\n        \n    add_rows: int (default=1)\n        int value specifiying the amount of rows to add to the subplot, should they be needed\n        \n    fig_size: array (default=(12,12))\n        array-like object containing the lenght and width of the figure to be drawn in inches\n    \n    Returns\n    -------\n    \n    countplots: plot\n        countplots of column values specified within string.\n        \n    regplots: plot\n        regplots of column values, that have 20 > unique values, against the response variable.\n    '''\n    \n    #create the base dataframe to work with\n    String_cols = list([col for col in df.columns if string in col])\n    df_string = df[String_cols]\n\n    # if there are going to be subplots\n    if subplots == 'Y':\n        \n        # define number of rows(based on number of items in String_cols), create plot axis and their size\n        nrows = int(round((len(String_cols)\/ncols),0)+add_rows)\n        fig, ax = plt.subplots(nrows=nrows,ncols=ncols)\n        fig.set_size_inches(fig_size)\n        \n        # instantiate count parameter(used in subplot positioning)\n        count = 1\n        \n        # loop through the elements of String_cols\n        for x in String_cols:\n            \n            # if number of categories of x > 20 create a regplot of this category on a given subplot and increase count by 1 \n            if len(df[x].unique()) > 16:\n                df_string[response_value] = train_df[response_value]\n                plt.subplot(nrows,ncols,count)\n                sns.regplot(y=df_string[response_value], x=df_string[x])\n                count += 1\n                \n            # if number of categories of x < 20 create a countplot of this category on a given subplot and increase count by 1 \n            else:\n                plt.subplot(nrows,ncols,count)\n                sns.countplot(df_string[x])\n                count += 1\n                \n    # if there are not going to be subplots create a countplot\n    else:\n        sns.countplot(df[string])  ","ebc0b4a4":"def variable_group_check(string,df):\n    '''\n    Generates a subsetted dataframe containing only columns that match the given string as well as the description of that dataframe.\n    \n    Parameters\n    ----------\n        \n    string: str\n        str object containing the string to select columns from the dataframe.\n        \n    df: dataframe\n        dataframe object to be subsetted by string.\n    \n    Returns\n    -------\n    \n    df_string_description: dataframe\n        dataframe containing the description of all elements present within the dataframe(categorical and numerical)\n    \n    df_string: dataframe\n        dataframe containing the datapoints of the subsetted orginal dataframe.\n    '''\n    \n    # create a dataframe containing only columns that match the string parameter\n    String_cols = list([col for col in df.columns if string in col])\n    df_string = df[String_cols]\n    \n    # return both the dataframe as well as the description of its contents\n    return df_string.describe(include='all'), df_string","95cb4891":"def variable_correlation_check(string,df,train_df,response_variable,ordinal_variables=[],categorical_variables=[],color='PiYG',\n                               fig_size=(12,12)):\n    '''\n    Gets the correlation between all columns containing the string and the response variable and displays this as a heatmap. \n    It also labels ordinal variables based on the measurement scale and value scale provided as well as dummy encoding all categorical variables.\n    \n    Parameters\n    ----------\n    \n    string: str\n        str object containing the string to select columns from the dataframe.\n    \n    df: dataframe\n        dataframe object to be subsetted by string.\n        \n    train_df: dataframe\n        dataframe object containing the original values of the response variable.\n        \n    response_variable: str\n        str object containing the response variable.\n        \n    ordinal_variables: list (default=[])\n        list-like object containing the ordinal variables to encode.\n        \n    categorical_variables: list\n        list-like object containing the categorical variables to dummy encode.\n        \n    color: str (default='PiYG')\n        str object containing the cmap for the heatmap color scheme.\n        \n    fig_size: array (default=(12,12))\n        array-like object containing the lenght and width of the figure to be drawn in inches\n    \n    Returns\n    -------\n    \n    sns.heatmap: plot\n        annotated heatmap of all variables.\n    '''\n    \n    #create the base dataframe to work with\n    String_cols = list([col for col in df.columns if string in col])\n    df_string = df[String_cols]\n    \n    # if there are no categorical variables\n    if categorical_variables == []:\n        \n        # loop through list of ordinal variables and replace values with user defined input\n        for x in ordinal_variables: \n            \n            # strip unnessecary characters\n            x = str(x)    \n            x = x.replace('\"' , '')\n            x = x.replace('[' , '')\n            x = x.replace(\"'\" , '')\n            x = x.replace(']' , '')\n            \n            # ask for user input for the measurement scale and value scale and replace their values\n            measurement_scale = input('Input the measurement scale to be used for ' + x + ':')\n            measurement_scale = measurement_scale.split()\n            value_scale = input('Input the value scale to be used for ' + x + ':')\n            value_scale = value_scale.split()\n            df_string[x] = df_string[x].replace(to_replace=measurement_scale, value=value_scale)\n            \n            # convert replaced values into int type within dataframe\n            df_string[x] = df_string[x].astype(int)\n\n        # get response variable from train data and plot heatmap\n        df_string[response_variable] = train_df[response_variable]\n        corrmat = df_string.corr()\n        f, ax = plt.subplots(figsize=fig_size)\n        sns.heatmap(corrmat, vmax=.8, square=True, cmap=color, annot=True)\n    \n    # if there are no ordinal variables\n    elif ordinal_variables == []:\n        \n        # loop through and dummy encode all categorical variable values    \n        for x in categorical_variables:\n            dummies = pd.get_dummies(df_string[str(x)], prefix=str(x))\n            df_string = pd.concat([df_string, dummies], axis=1)\n            \n        # get response variable from train data and plot heatmap\n        df_string[response_variable] = train_df[response_variable]\n        corrmat = df_string.corr()\n        f, ax = plt.subplots(figsize=fig_size)\n        sns.heatmap(corrmat, vmax=.8, square=True, cmap=color, annot=True)\n    \n    # if there are both ordinal and categorical variables\n    else:\n        \n        #loop through list of ordinal variables and replace values with user defined input\n        for x in ordinal_variables:\n            \n            # strip unnessecary characters\n            x = str(x)    \n            x = x.replace('\"' , '')\n            x = x.replace('[' , '')\n            x = x.replace(\"'\" , '')\n            x = x.replace(']' , '')\n            \n            # ask for user input for the measurement scale and value scale and replace their values\n            measurement_scale = input('Input the measurement scale to be used for ' + x + ':')\n            measurement_scale = measurement_scale.split()\n            value_scale = input('Input the value scale to be used for ' + x + ':')\n            value_scale = value_scale.split()\n            df_string[x] = df_string[x].replace(to_replace=measurement_scale, value=value_scale)\n            \n            # convert replaced values into int type within dataframe\n            df_string[x] = df_string[x].astype(int)\n            \n        # loop through and dummy encode all categorical variable values  \n        for x in categorical_variables:\n            dummies = pd.get_dummies(df_string[str(x)], prefix=str(x))\n            df_string = pd.concat([df_string, dummies], axis=1)\n            \n        # get response variable from train data and plot heatmap\n        df_string[response_variable] = train_df[response_variable]\n        corrmat = df_string.corr()\n        f, ax = plt.subplots(figsize=fig_size)\n        sns.heatmap(corrmat, vmax=.8, square=True, cmap=color, annot=True)","aba9e87a":"def outlier_detection(corrmat,correlator,corr_score,df,train_df,ncols):\n    '''\n    Gets the regplots of the top correlated variables to the specified correlator\n    \n    Parameters\n    ----------\n    \n    corrmat: dataframe\n        dataframe object containing the correlation matrix to be subsetted. \n    \n    correlator: str\n        str object containing the variable to check for correlations with in the analysis.\n        \n    corr_score: int\n        int value specifiying the cutoff correlation value for varaibles to make the plot\n        \n    df: dataframe\n        dataframe object containing the dataframe to be subsetted.\n        \n    train_df: dataframe\n        dataframe object containing the original values of the variable that needs to be predicted datapoints that need to be counted.\n        \n    ncols: int \n        int value specifiying the amount of coloumns to create within the sub plot\n        \n    Returns\n    -------\n        \n    regplots: plot\n        regplots of variables meeting the corr_score requirements.\n    '''\n    \n    # create a mask that selects, based on corr_score, columns from the orginal corrmat\n    mask = corrmat[correlator] >= corr_score\n    corrmat = corrmat[mask]\n    corrmat_cols = list(corrmat[correlator].index)\n    \n    # define number of rows(based on number of items in corrmat_cols)\n    nrows = int(round(len(corrmat_cols)\/3,0))+1\n    \n    # create plot axis and their size and subset to contain only train data\n    fig, ax = plt.subplots(nrows=nrows,ncols=ncols)\n    fig.set_size_inches(32, 40)\n    variable = df[:ntrain]\n    variable[correlator] = train_df[correlator]\n    \n    # instantiate count\n    count = 0\n    \n    # loop through columns in corrmat_cols and create regplots for each who's index is based on count\n    for col in corrmat_cols:\n        count+=1\n        plt.subplot(nrows,ncols,count)\n        sns.regplot(y=variable[correlator], x=variable[col])\n        #plt.title('Exterior1st Countplot', fontsize=15)","4a811f81":"def ordered_label_encoder(df, col_names, to_replace, replace_values):\n    '''\n    Replaces the values of data, for the columns in col_list, in the to_replace list with the values present in the replace_values list. These changes are done to the base dataframe.\n    \n    Parameters\n    ----------\n    \n    df: dataframe\n        dataframe object containing the dataframe to be subsetted.\n    \n    col_names: list\n        list_like object containing the list of columns that the replacement must be applied to\n        \n    to_replace: list\n        list_like object containing the measurement scale of the ordinal variable\n        \n    replace_values: list\n        list_like object containing the value scale of the replacement elements\n    \n    Returns\n    -------\n    \n    df: dataframe\n        altered original dataframe.\n    '''\n    # loop through columns present in col_names\n    for col in col_names:\n        \n        # replace to_replace values with replace_values\n        df[col] = df[col].replace(to_replace=to_replace, value=replace_values)","d9ff99e1":"# import from kaggle dataset\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\n#Save the 'Id' column for later use in model predictions\ndf_train_Id = df_train['Id']\ndf_test_Id = df_test['Id']\n\n#Now drop the  'Id' column from the base dataframe as it interferes with the missing number calculations\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)","71303e1d":"response_variable_distribution('SalePrice',df_train)","7a5ee878":"# Use log transformation (log(1+x)) via the the numpy fuction log1p to SalePrice\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","bcceb974":"response_variable_distribution('SalePrice',df_train)","afd9130a":"# split off sales price from train data into y_train for later use in the modelling section\ny_train = df_train.SalePrice.values\n\n# create mask varaibles for test and train subsetting later on\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\n# concatenate train and test dataframes, drop SalePrice column and print the shape of the new all_data dataframe\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","21838e09":"percent_missing_data(all_data)","4ffa3865":"variable_imputation_check('Pool',all_data,df_train,'SalePrice',subplots='Y',ncols=2,add_rows=0)","d311ce4b":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')","4b3a17ed":"variable_imputation_check('MiscFeature',all_data,df_train,'SalePrice')","82c4d233":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","8eb8b85a":"variable_imputation_check('MiscFeature',all_data,df_train,'SalePrice')","138693cc":"# was initially dropped\n#all_data = all_data.drop(['MiscFeature'], axis=1)","68fddddb":"variable_imputation_check('Alley',all_data,df_train,'SalePrice')","4c8eca83":"all_data['Alley'] = all_data['Alley'].fillna('None')","b49be001":"variable_imputation_check('Fence',all_data,df_train,'SalePrice')","7f981231":"all_data['Fence'] = all_data['Fence'].fillna('None')","da028453":"variable_imputation_check('Fireplace',all_data,df_train,'SalePrice',subplots='Y', ncols=2, add_rows=0, fig_size=(12, 12))","7e36bf82":"all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')","c41b0116":"variable_correlation_check(string='Fireplace',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['FireplaceQu'],color='YlGn')","ba44fb3a":"# subset orginal dataframe by train (to prevent missing SalePrice values from affecting the plots) and then concatenate SalePrice to this new dataframe\nlot_frontage = all_data[:ntrain]\n\n#concatenate SalePrice to this new dataframe and then subset it to contain only SalePrice and LotFrontage\nlot_frontage = pd.concat([lot_frontage, df_train[['SalePrice']]], axis=1)\nlot_frontage = lot_frontage[['SalePrice','LotFrontage']]\n\n# subset orginal dataframe by train (to prevent missing SalePrice values from affecting the plots) and then concatenate SalePrice to this new dataframe\nlot_frontage_0fill = all_data[:ntrain]\n\n#concatenate SalePrice to this new dataframe, fill NA values with 0 and then subset it to contain only SalePrice and LotFrontage\nlot_frontage_0fill = pd.concat([lot_frontage_0fill, df_train[['SalePrice']]], axis=1).fillna(value=0)\nlot_frontage_0fill = lot_frontage_0fill[['SalePrice','LotFrontage']]\n\n# subset orginal dataframe by train (to prevent missing SalePrice values from affecting the plots) and then concatenate SalePrice to this new dataframe\nlot_frontage_grouped_medianfill = all_data[:ntrain]\n\n#concatenate SalePrice to this new dataframe, fill NA values wiht the median by grouped neighborhood and then subset it to contain only SalePrice and LotFrontage\nlot_frontage_grouped_medianfill = pd.concat([lot_frontage_grouped_medianfill, df_train[['SalePrice']]], axis=1)\nlot_frontage_grouped_medianfill = df_train.groupby(\"Neighborhood\")['LotFrontage', 'SalePrice'].transform(\n    lambda x: x.fillna(x.median()))","8480db38":"# set up the axes, figure layout and total size for the plot\nfig, ax = plt.subplots(nrows=1,ncols=3)\nfig.set_size_inches(12, 12)\n\n# plot the first subplot (Lot Frontage Without Fill)\nplt.subplot(1,3,1)\nsns.regplot(y=lot_frontage['SalePrice'], x=lot_frontage['LotFrontage'])\nplt.title('Lot Frontage Without Fill', fontsize=12)\n\n# plot the second subplot (Lot Frontage With 0 Fill)\nplt.subplot(1,3,2)\nsns.regplot(y=lot_frontage_0fill['SalePrice'], x=lot_frontage_0fill['LotFrontage'])\nplt.title('Lot Frontage With 0 Fill', fontsize=12)\n\n# plot the third subplot (Lot Frontage Grouped Median Fill)\nplt.subplot(1,3,3)\nsns.regplot(y=lot_frontage_grouped_medianfill['SalePrice'], x=lot_frontage_grouped_medianfill['LotFrontage'])\nplt.title('Lot Frontage With Grouped Median Fill', fontsize=12)","b148e4bf":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","1699472b":"variable_imputation_check('Lot',all_data,df_train,'SalePrice',subplots='Y', ncols=2, add_rows=0, fig_size=(12,12))","ba6ebed3":"variable_correlation_check(string='Lot',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['LotShape'],categorical_variables=['LotConfig'])","05063836":"variable_imputation_check('Garage',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=3, fig_size=(16, 16))","ba6da79f":"all_data[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']] = all_data[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']].fillna('None')","4cd2d351":"all_data[['GarageYrBlt', 'GarageArea', 'GarageCars']] = all_data[['GarageYrBlt', 'GarageArea', 'GarageCars']].fillna(0)","31501e81":"variable_correlation_check(string='Garage',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['GarageQual','GarageCond','GarageFinish'],categorical_variables=['GarageType'],fig_size=(16,16))","5e5d1891":"variable_imputation_check('Bsmt',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=3, add_rows=0, fig_size=(18, 18))","28146f38":"all_data[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = all_data[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']].fillna(0)","c96e5039":"all_data[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']] = all_data[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].fillna('None')","b6129961":"variable_correlation_check(string='Bsmt',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['BsmtQual','BsmtCond','BsmtExposure'],categorical_variables=['BsmtFinType1','BsmtFinType2'],fig_size=(16,16))","189d194b":"variable_imputation_check('Mas',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=2, add_rows=0, fig_size=(12, 12))","d848dfad":"all_data[['MasVnrType']] = all_data[['MasVnrType']].fillna('None')","9e7f5147":"all_data[['MasVnrArea']] = all_data[['MasVnrArea']].fillna(0)","f81d8e79":"variable_correlation_check(string='Mas',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['MasVnrType'])","28caccbd":"variable_imputation_check('MS',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=2, add_rows=0, fig_size=(12, 12))","1f810f9c":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","c1a4fa35":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","46dd0d23":"# convert MSSubclass to string as it is currently represented as an integer\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)","e0fb37f0":"variable_correlation_check(string='MS',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['MSSubClass','MSZoning'],fig_size=(18,18))","67fcf479":"variable_imputation_check('Utilities',all_data,df_train,response_value='SalePrice')","67757aff":"all_data['Utilities'] = all_data['Utilities'].fillna(\"None\")","89b12d61":"variable_imputation_check('Functional',all_data,df_train,'SalePrice')","89eec403":"all_data['Functional'] = all_data['Functional'].fillna('Typ')","a7f53893":"variable_imputation_check('Electrical',all_data,df_train,response_value='SalePrice')","55dc7129":"all_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')","eaa31634":"variable_imputation_check('Kitchen',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=2, add_rows=0, fig_size=(12, 12))","a9558309":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna('TA')","689e8e19":"variable_correlation_check(string='Kitchen',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['KitchenQual'], color='YlGn')","26943c85":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","de36e239":"variable_imputation_check('SaleType',all_data,df_train,response_value='SalePrice')","40c1ab29":"all_data['SaleType'] = all_data['SaleType'].fillna('WD')","cd9a8089":"# create total and percent measures of missing data and then concatenate these into a dataframe\ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(1)","75d8e86a":"variable_imputation_check('SF',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=3, add_rows=0, fig_size=(18, 18))","d352dd91":"variable_correlation_check(string='SF',df=all_data,train_df=df_train,response_variable='SalePrice')","b529fb59":"# feature enginner a TotalSF measure to see if it correlates better with SalePrice\nall_data['TotalSF'] = all_data['1stFlrSF'] + all_data['2ndFlrSF'] +  all_data['TotalBsmtSF']","33511084":"variable_imputation_check('Qual',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=3, add_rows=0, fig_size=(18, 18))","10e32b40":"variable_correlation_check(string='Qual',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['BsmtQual','GarageQual','ExterQual','KitchenQual'])","37980676":"variable_imputation_check('Cond',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=3, add_rows=1, fig_size=(18, 18))","cb8ddfcd":"variable_correlation_check(string='Cond',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['BsmtCond','GarageCond','ExterCond',],categorical_variables=['SaleCondition','Condition1','Condition2'],fig_size=(18,18))","a4dd7417":"variable_imputation_check('Street',all_data,df_train,response_value='SalePrice')","c91d7c48":"variable_imputation_check('Land',all_data,df_train,response_value='SalePrice',subplots='Y', ncols=2, add_rows=0)","457c18f1":"variable_correlation_check(string='Land',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['LandSlope'],categorical_variables=['LandContour'])","f9271ead":"variable_imputation_check('Neighborhood',all_data,df_train,response_value='SalePrice',fig_size=(18, 18))","958b2573":"variable_correlation_check(string='Neighborhood',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['Neighborhood'],fig_size=(18,18),color='YlGn')","b87a76c9":"variable_imputation_check('BldgType',all_data,df_train,response_value='SalePrice')","162bb0a1":"variable_correlation_check(string='BldgType',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['BldgType'])","28acb8d2":"variable_imputation_check('HouseStyle',all_data,df_train,response_value='SalePrice')","4caa96ff":"variable_correlation_check(string='HouseStyle',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['HouseStyle'])","e85eae00":"fig, ax = plt.subplots(nrows=1,ncols=2)\nfig.set_size_inches(16, 16)\n\nplt.subplot(1,2,1)\nsns.countplot(all_data['RoofStyle'])\nplt.title('RoofStyle Countplot', fontsize=15)\n\nplt.subplot(1,2,2)\nsns.countplot(all_data['RoofMatl'])\nplt.title('RoofStyle Countplot', fontsize=15)","95b9ae0b":"variable_correlation_check(string='Roof',df=all_data,train_df=df_train,response_variable='SalePrice',categorical_variables=['RoofStyle','RoofMatl'],fig_size=(16,16))","2cb836b5":"variable_imputation_check(string='Heating',df=all_data,train_df=df_train,response_value='SalePrice',subplots='Y',ncols=2,add_rows=0,fig_size=(12,12))","6ee565ff":"variable_correlation_check(string='Heating',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['HeatingQC'],categorical_variables=['Heating'])","1d963106":"variable_imputation_check(string='CentralAir',df=all_data,train_df=df_train,response_value='SalePrice')","0d496591":"variable_correlation_check(string='CentralAir',df=all_data,train_df=df_train,response_variable='SalePrice',ordinal_variables=['CentralAir'],color='YlGn')","497d6ae0":"#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","4e023470":"ordered_label_encoder(all_data,['FireplaceQu','BsmtQual','GarageFinish','GarageQual','ExterQual','KitchenQual','HeatingQC','ExterCond','BsmtCond','GarageCond','PoolQC'],['None','Po','Fa','TA','Gd','Ex'],[0,1,2,3,4,5])\nordered_label_encoder(all_data,['LotShape'],['IR3','IR2','IR1','Reg'],[0,1,2,3])\nordered_label_encoder(all_data,['GarageFinish'],['None','Unf','RFn','Fin'],[0,1,2,3])\nordered_label_encoder(all_data,['CentralAir','PavedDrive'],['N','Y'],[0,1])\nordered_label_encoder(all_data,['Street'],['Grvl','Pave'],[0,1])\nordered_label_encoder(all_data,['Alley'],['None','Grvl','Pave'],[0,1,2])\nordered_label_encoder(all_data,['LandSlope'],['Sev','Mod','Gtl'],[0,1,2])\nordered_label_encoder(all_data,['BsmtExposure'],['None','No','Mn','Av','Gd'],[0,1,2,3,4])\nordered_label_encoder(all_data,['Functional'],['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],[0,1,2,3,4,5,6,7])","a511f2c7":"# subset the original dataframe by the train mask created earlier and add SalePrice\nnumerical_heatmap = all_data[:ntrain]\nnumerical_heatmap['SalePrice'] = df_train['SalePrice']\n\n# plot heatmap of correlation values\ncorrmat = numerical_heatmap.corr()\nf, ax = plt.subplots(figsize=(32, 32))\nsns.heatmap(corrmat, vmax=.8, square=True, cmap=\"PiYG\", annot=True);","7c4203b5":"outlier_detection(corrmat=corrmat, correlator='SalePrice', corr_score=0.30, df=all_data, train_df=df_train, ncols=3)","af403326":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","05b1f940":"train = all_data[:ntrain]\nprint(train.shape)\ntest = all_data[ntrain:]\nprint(train.shape)","b236dce4":"train = all_data[:ntrain]\ntrain['SalePrice'] = df_train['SalePrice']\n\n# outliers were dropped using the previous graphs as visual aids for value criteria\ntrain = train.drop(train[(train['1stFlrSF']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['BsmtFinSF1']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['GarageArea']>1220) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['LotFrontage']>300) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['OpenPorchSF']>500) & (df_train['SalePrice']<11)].index)\ntrain = train.drop(train[(train['TotalBsmtSF']>6000) & (df_train['SalePrice']<13)].index)\ny_train = train.SalePrice.values\n\n# SalePrice was dropped from the train data as it is the response variable\ntrain = train.drop(['SalePrice'], axis=1)\nprint(train.shape)\nprint(y_train.shape)","046853c2":"from sklearn.linear_model import ElasticNet, Lasso, Ridge, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport optuna","44636298":"#Validation function\nn_folds = 5\n\n# \ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","ca691f8b":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    alpha = trial.suggest_loguniform('alpha', 1e-10, 1)\n    fit_intercept = trial.suggest_categorical('fit_intercept', [True,False])\n    normalize=trial.suggest_categorical('normalize', [True,False])\n    precompute=trial.suggest_categorical('precompute', [True,False])\n    max_iter=trial.suggest_int('max_iter', 1000, 10000)\n    tol=trial.suggest_loguniform('tol', 1e-10, 1)\n    warm_start=trial.suggest_categorical('warm_start', [True,False])\n    positive=trial.suggest_categorical('positive', [True,False])\n    random_state=trial.suggest_int('random_state', 1, 10)\n    selection=trial.suggest_categorical('selection', ['cyclic','random'])\n    \n    # create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = Lasso(alpha=alpha,\n                           fit_intercept=fit_intercept,\n                           normalize=normalize,\n                           precompute=precompute,\n                           max_iter=max_iter,\n                           tol=tol,\n                           warm_start=warm_start,\n                           positive=positive,\n                           random_state=random_state,\n                           selection=selection)\n\n    # define x and y variables\n    x, y = train,y_train\n    \n    # check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y)\n    accuracy = score.mean()\n    \n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy  \n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study() \nstudy.optimize(objective, n_trials=1000)","5809e881":"# used to print the optimal hyperparameters found by the objective function\nstudy.best_params","ff0c991d":"# run the model using the the optimised hyperparameters\nlasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0007699830238031739,\n                                            fit_intercept=True,\n                                            normalize=False,\n                                            precompute=False,\n                                            max_iter=8117,\n                                            tol=1.203470716648057e-07,\n                                            warm_start=False,\n                                            positive=True,\n                                            random_state=7,\n                                            selection='cyclic'))","627dea56":"# check the cross-validation score of the model from the train data\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5da78d0f":"# scale the data using RobustScaler then fit the model to our dataframe\nscaler = RobustScaler()\ntrain2 = scaler.fit_transform(train)\nlasso2 = Lasso(alpha=0.0007699830238031739,\n               fit_intercept=True,\n               normalize=False,\n               precompute=False,\n               max_iter=8117,\n               tol=1.203470716648057e-07,\n               warm_start=False,\n               positive=True,\n               random_state=7,\n               selection='cyclic')\nlasso2.fit(train, y_train)\n\n# get coeffecients from the lasso model and use them to remove all columns that have a coeffecient of 0\ncoeff = pd.DataFrame(lasso2.coef_, all_data.columns, columns=['Coefficient']).reset_index()\nCols_to_remove = coeff['Coefficient'] == 0 \nCols_to_remove = coeff[Cols_to_remove]\nCols_to_remove = list(Cols_to_remove['index'])\nall_data = all_data.drop(Cols_to_remove, axis=1)","aa371a95":"train = all_data[:ntrain]\nprint(train.shape)\ntest = all_data[ntrain:]\nprint(train.shape)","056d1891":"train = all_data[:ntrain]\ntrain['SalePrice'] = df_train['SalePrice']\n#train = train.drop(train[(train['1stFlrSF']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['BsmtFinSF1']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['GarageArea']>1220) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['LotFrontage']>300) & (df_train['SalePrice']<13)].index)\ntrain = train.drop(train[(train['OpenPorchSF']>500) & (df_train['SalePrice']<11)].index)\n#train = train.drop(train[(train['TotalBsmtSF']>6000) & (df_train['SalePrice']<13)].index)\ny_train = train.SalePrice.values\ntrain = train.drop(['SalePrice'], axis=1)\nprint(train.shape)\nprint(y_train.shape)","09c895e3":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    alpha = trial.suggest_loguniform('alpha', 1e-10, 1)\n    fit_intercept = trial.suggest_categorical('fit_intercept', [True,False])\n    normalize=trial.suggest_categorical('normalize', [True,False])\n    precompute=trial.suggest_categorical('precompute', [True,False])\n    max_iter=trial.suggest_int('max_iter', 1000, 10000)\n    tol=trial.suggest_loguniform('tol', 1e-10, 1)\n    warm_start=trial.suggest_categorical('warm_start', [True,False])\n    positive=trial.suggest_categorical('positive', [True,False])\n    random_state=trial.suggest_int('random_state', 1, 10)\n    selection=trial.suggest_categorical('selection', ['cyclic','random'])\n    \n    # create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = Lasso(alpha=alpha,\n                           fit_intercept=fit_intercept,\n                           normalize=normalize,\n                           precompute=precompute,\n                           max_iter=max_iter,\n                           tol=tol,\n                           warm_start=warm_start,\n                           positive=positive,\n                           random_state=random_state,\n                           selection=selection)\n\n    # define x and y variables\n    x, y = train,y_train\n    \n     # check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y)\n    accuracy = score.mean()\n    \n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study() \nstudy.optimize(objective, n_trials=1000)","ca1ad3a4":"# used to print the optimal hyperparameters found by the objective function\nstudy.best_params","88655fb5":"# run the model using the the optimised hyperparameters\nlasso = make_pipeline(RobustScaler(), Lasso(alpha=3.0340354779991275e-05,\n                                            fit_intercept=True,\n                                            normalize=True,\n                                            precompute=True,\n                                            max_iter=5941,\n                                            tol=9.623735660996391e-10,\n                                            warm_start=False,\n                                            positive=True,\n                                            random_state=3,\n                                            selection='cyclic'))","1a54cfd0":"# check the cross-validation score of the model from the train data\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5279b86e":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    alpha = trial.suggest_loguniform('alpha', 1e-10, 1)\n    l1_ratio = trial.suggest_uniform('l1_ratio', 0.0, 1.0)\n    fit_intercept = trial.suggest_categorical('fit_intercept', [True,False])\n    normalize=trial.suggest_categorical('normalize', [True,False])\n    precompute=trial.suggest_categorical('precompute', [True,False])\n    max_iter=trial.suggest_int('max_iter', 1000, 10000)\n    tol=trial.suggest_loguniform('tol', 1e-10, 1)\n    warm_start=trial.suggest_categorical('warm_start', [True,False])\n    positive=trial.suggest_categorical('positive', [True,False])\n    random_state=trial.suggest_int('random_state', 1, 10)\n    selection=trial.suggest_categorical('selection', ['cyclic','random'])\n    \n    # create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = ElasticNet(alpha=alpha,\n                                l1_ratio=l1_ratio,\n                                fit_intercept=fit_intercept,\n                                normalize=normalize,\n                                precompute=precompute,\n                                max_iter=max_iter,\n                                tol=tol,\n                                warm_start=warm_start,\n                                positive=positive,\n                                random_state=random_state,\n                                selection=selection)\n\n    # define x and y variables\n    x, y = train,y_train\n    \n     # check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y)\n    accuracy = score.mean()\n    \n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study() \nstudy.optimize(objective, n_trials=1000)","980add76":"# used to print the optimal hyperparameters found by the objective function\nstudy.best_params","42d0ba57":"# run the model using the the optimised hyperparameters\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=6.869814939303578e-05,\n                                                l1_ratio=0.15676425376728362,\n                                                fit_intercept=True,\n                                                normalize=True,\n                                                precompute=True,\n                                                max_iter=9216,\n                                                tol=0.0220818991822515,\n                                                warm_start=False,\n                                                positive=True,\n                                                random_state=7,\n                                                selection='random'))","83ef8c9c":"# check the cross-validation score of the model from the train data\nscore = rmsle_cv(ENet)\nprint(\"\\nENet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ea2820af":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    n_iter=trial.suggest_int('n_iter', 1, 1000)\n    tol=trial.suggest_loguniform('tol', 1e-10, 1)\n    alpha_1=trial.suggest_loguniform('alpha1', 1e-10, 10)\n    alpha_2=trial.suggest_loguniform('alpha2', 1e-10, 10)\n    lambda_1=trial.suggest_loguniform('lambda_1', 1e-10, 10)\n    lambda_2=trial.suggest_loguniform('lambda_2', 1e-10, 10)\n    compute_score=trial.suggest_categorical('compute_score', [True,False])\n    fit_intercept=trial.suggest_categorical('fit_intercept', [True,False])\n    normalize=trial.suggest_categorical('normalize', [True,False])\n    verbose=trial.suggest_categorical('verbose', [True,False])\n    \n    # create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = BayesianRidge(n_iter=n_iter,\n                                   tol=tol,\n                                   alpha_1=alpha_1,\n                                   alpha_2=alpha_2,\n                                   lambda_1=lambda_1,\n                                   lambda_2=lambda_2,\n                                   compute_score=compute_score,\n                                   fit_intercept=fit_intercept,\n                                   normalize=normalize,\n                                   verbose=verbose)\n\n    # define x and y variables\n    x, y = train,y_train\n    \n     # check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y)\n    accuracy = score.mean()\n    \n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study() \nstudy.optimize(objective, n_trials=1000)","0fa61ff6":"# used to print the optimal hyperparameters found by the objective function\nstudy.best_params","6e050398":"# run the model using the the optimised hyperparameters\nBayesianRidgeRegression = make_pipeline(RobustScaler(), BayesianRidge(n_iter=879,\n                                                                      tol=2.99590539046086e-10,\n                                                                      alpha_1=0.019444194793005434,\n                                                                      alpha_2=4.7042655856621804,\n                                                                      lambda_1=0.08808043997430755,\n                                                                      lambda_2=0.045167765758061834,\n                                                                      compute_score=True,\n                                                                      fit_intercept=True,\n                                                                      normalize=False,\n                                                                      verbose=False,))","e90ddda8":"# check the cross-validation score of the model from the train data\nscore = rmsle_cv(BayesianRidgeRegression)\nprint(\"\\nBayesianRidgeRegression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","53fdaf59":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","6c16c632":"averaged_models = AveragingModels(models = (lasso,ENet,BayesianRidgeRegression))\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","427a13fe":"# new rmsle for predicted values not the pipeline objects\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","f55dee65":"averaged_models.fit(train.values, y_train)\naveraged_train_pred = averaged_models.predict(train.values)\naveraged_models_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, averaged_train_pred))","ca763513":"sub = pd.DataFrame()\nsub['Id'] = df_test_Id\nsub['SalePrice'] = averaged_models_pred\nsub.to_csv('submission.csv',index=False)","8e8a03a3":">The approach was slightly differnt for this particular variable. We first needed to decide how were were going to fill the missing Lot Frontage vaules before we could procede with the rest of the EDA.","997cecb4":"## Modeling","23e0bd23":"#### 5. Concatenate train and test data into one dataframe","5c9ddfa4":"Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","40764512":"#### 3. Second Lasso Modeling","8133b3ae":"- Feature Set = Basement","658769b1":"- Feature Set = Roof","e2f9410c":"We then created a second, separate, lasso model (using the optimised hyperparameters) to subset the original dataframe with","e1f7716d":"- The measurement scale used for all Qual: **None Po Fa TA Gd Ex**\n- The value scale to be used for all Qual: **0 1 2 3 4 5**","3ae981f4":"- Feature = Alley","b286bb0b":">Data description says says NA means \"no fireplace\"","32a7015a":"#### 8. Check for any remaining missing values.","d54d1559":"#### 2. Initial Lasso Modeling","fa59df79":"This is done in order to simplfy the process of missing variable replacement as well as to make visulising the distributions of the various variables easier","6c2c69a5":"Regplots were used to identify potential outliers for all features that were correlated more than 0.30 with SalePrice. This was done in order to ensure that outliers present in the top correlated variables would not skew the models that will be run on the dataframe. Dummy variables were generated for all remaining categorical features in the dataframe and it was split into train and test subsets using the train and test masks created previously. Finally all outliers were dropped and the shape of train and test was checked to ensure that it matches (important for the running of models)","dde2bb7f":"- The measurement scale used for FireplaceQu: **None Po Fa TA Gd Ex**\n- The value scale to be used: FireplaceQu: **0 1 2 3 4 5**","d8dc4347":"- The measurement scale used for all Land: **Sev Mod Gtl**\n- The value scale to be used for all Land: **0 1 2**","9645cf0a":"We initially used a Lasso Regression model on our data as it provides feature selection as a part of the model. We found it more accurate to use a lasso model to perform feature selection than manually removing features ourselves","119751c2":">Replace missing values with 'WD' as it is the most common value. ","9ab29e1e":"We performed this by proceeding sequentially through the features that remained. For each of the features we performed a general distribution check (which informed us on what to replace missing values with), replaced missing values, and in the case of the feature being part of a larger feature set we constructed a heatmap showing the correlations between the feature set, containing our feature, and the response variable (in this case SalePrice)","12390c2a":"This is the aformentioned objective hyperparameter tuning function","89e05662":"We transformed some numerical variables into categorical so that they are encoded correctly in further procedures","9d9372ba":">Replacing missing values with Typ as it is the most common value found in the dataset.","38430001":"- Feature = Fence","f698be6c":"1. The *response_variable_distribution* function gets the distribution, mean and standard deviation of the selected response variable. It also provides a probability plot of sample data as well as providing a best-fit line for this variable.","0925f2f7":">Replacing missing values with SBrkr as it is the most common value found in the dataset.","ec31161a":"- Feature = House Style","af0f7f4d":"- Feature Set = Lot","29952644":"2. The *percent_missing_data* function gets the percentage of data missing (where data missing is greater than 0) for every column in a given dataframe and provides this information in a barplot.","9081883c":"#### 2. Some functions we created to aid in the EDA proccess","c9f321a0":"- The measurement scale used for CentralAir: **N Y**\n- The value scale to be used for CentralAir: **0 1**","c2e13c1d":"4. The *variable_group_check* function generates a subsetted dataframe containing only columns that match the given string as well as the description of that dataframe. This is usefull in determining the measurement scales used later on in the *variable_correlation_check* function","e7d16590":"Ordinal feature encoding was performed here as the previous analyses were performed on subsetted dataframes and did not make any changes to the original dataframe","8804d065":"- The measurement scale used for all Cond: **None Po Fa TA Gd Ex**\n- The value scale to be used for all Cond: **0 1 2 3 4 5**","2be07171":"KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","7a98b755":"#### 4. Check the distribution of the Response Variable (SalePrice)","edbd6f0d":"Check train and test size","15ea86de":"## Final Submission","1ba1430d":"Drop outliers as a new dataframe has been created","16806602":"# An Exploratory Data Analysis Driven Approach To Applying Machine Learning On The Ames Housing Dataset","67d9e9a5":"A heatmap of all numerical variables was drawn here to check the collinearity of the various features present in the dataframe","742fa149":"#### 6. Find the categories that contain missing values and calculate the percentage.","cfdab7bc":"#### 4. Elastic Net Modeling","d0588d2b":">Replace missing KitchenQual values with 'TA' as it is the most common value. ","9cec0655":">Replace missing MSZoning values with 'RL' as it is the most common value. \n> Replace missing MSSubClass values with none as NA most likely means No building class.","536e1e63":"#### 10. Label encode features based on measurement and value scales shown prevoiusly","23465677":"We performed this by proceeding sequentially through features with missing values. For each of the features we performed a general distribution check (which informed us on what to replace missing values with), replaced missing values, plotted new distribution with missing values replaced and in the case of the feature being part of a larger feature set we constructed a heatmap showing the correlations between the feature set, containing our feature, and the response variable (in this case SalePrice)","6936024c":"- Feature Set = Land","154bb268":"- Feature Set = Pool","0fc114b2":"#### 2. Some functions we created to aid in the Modeling proccess","685f98e2":"#### 3. Importing the train and test data","09b4b2f7":"- Feature = Utilities","13ccc86f":"## Exploratory Data Analysis","8a053340":">Data description says says NA means \"no misc feature\"","c2c70f1d":"- Feature Set = Kitchen","0f2bcad2":"- The measurement scale used for KitchenQual: **None Po Fa TA Gd Ex**\n- The value scale to be used for KitchenQual: **0 1 2 3 4 5**","cca4f41e":"- Feature Set = MS","85aac663":"SaleType : Fill in again with most frequent which is \"WD\"","dd345550":"5. The *variable_correlation_check* function gets the correlation, between all columns containing the given string and the response variable, and displays this as a heatmap. It also labels ordinal variables based on a user provided measurement scale and value scale. Furthermore it performs  dummy encoding for all categorical variables present within the dataframe. The measurement scales to be used can be obtained by using the *variable_group_check* function shown previously. These changes do not affect the vaules contained in the base dataframe","3ca479f8":">Data description says says NA means \"no fence\"","e0eb8524":"- Feature Set = Fireplace","630e271f":"- Feature = Building Type","7e0630ae":">Replacing missing data with None as most of the missing values indicate that there is no garage present for that house","9f9594b3":"- Feature = Air Conditioning","fa160ff3":">Data description says NA means \"No Pool\".","0631a573":"#### 11. Heatmap of all numerical variables correlations to SalePrice (includes all ordinal variables that have been encoded)","76e59a16":"6. The *outlier_detection* function generates regplots of the top correlated variables to the specified correlator. These can then be used to visually detect any outliers present in the data that may later affect the quality of the model. These changes do not affect the vaules contained in the base dataframe.","bd94eb35":"Functional : data description says NA means typical","3942b9d8":"We performed a second lasso modeling process on the dataframe that has had feature selection performed on it ","504694b9":">Filling the missing with 0's not a good option as it skewes the correlation to SalesPrice. Most houses in areas have similar Lot Frontage values therefore we can fill in missing values by the median LotFrontage of the neighborhood the house resides in.","b37f6075":"- Feature = Neighborhood","77ff7309":"- The measurement scale used for BsmtQual and BsmtCond: **None Po Fa TA Gd Ex**\n- The value scale to be used for BsmtQual and BsmtCond: **0 1 2 3 4 5**\n- The measurement scale used for BsmtExposure: **None No Mn Av Gd**\n- The value scale to be used for BsmtExposure: **0 1 2 3 4**","4c137cc0":"- Feature = Street","f106c1f4":"- Feature Set = Garage","62b3d454":">Replacing missing values with None as it probably means no utilities are present","4cfb58d2":"7. The *ordered_label_encoder* function replaces, for the selected columns, the values of data given in the to_replace list with the values present in the replace_values list. These changes are done to the base dataframe permanently changing the vaules for all subsequent analyses.\n    ","81ee3a94":"- Feature Set = Heating","4bae2877":"1. The *rmsle_cv* function gets the RMSLE score for a given pipeline object created for the purpose of modeling. It provides an cross validated estimate of the accuracy of the model based on the training dataset","9a475a63":"- Feature = Functional","b08afd1a":"- The measurement scale used for LotShape: **IR3 IR2 IR1 Reg**\n- The value scale to be used: LotShape: **0 1 2 3**","69bc8a14":"Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","27bb2d5e":">Replace missing MasVnrType values with None as NA probably refers to the house having no masonary present. \n> Replace missing MasVnrArea values with 0 as NA most likely means no masonary.","bd94c75a":"- Feature Set = Masonary","0c7d8fb8":"#### 9. Check the remaining features.","e9dc374c":"- Feature Set = Condition","f34da53e":">Replacing missing categorical data with None and missing numerical data with 0 as most of the missing values indicate that there is no basement present for that house","f562f8b9":"- Feature = MiscFeature","2f42e951":">Data description says says NA means \"no alley access\"","f9bdd0ec":"2. The *objective* function acts as a hyperparameter tuner using the optuna package. This function needs to be re-defined everytime that it is used for a differnt model as the hyperparameters vary from model to model","82c1642a":"#### 1. Importing the necessary librairies for EDA","7dd4ac1c":"- Feature = SaleType","d4ca50b8":"#### 12. Detecting outliers and final preparation of the dataframe for modeling","e72ba634":"3. The *variable_imputation_check* function gets the counts of the categories present, within selected columns containing the given string. If the column contains more than 20 unique values a regplot is drawn(i.e if the data is numerical). Otherwise a standard countplot is drawn. For cases where multiple plots need to be made the number of columns, to use in the subplot, can be specified as well as the figure size in inches.","5c0bd741":"- The measurement scale used for all Heating: **None Po Fa TA Gd Ex**\n- The value scale to be used for all Heating: **0 1 2 3 4 5**","3eeeef8e":"- Feature = Electrical","4978f135":"Several functions were created for the EDA analsis. These functions were designed in such a way that they can easily be reporposed to work with an entirly different dataframe and variable set","0539149a":"- Feature Set = Square Footage","8d17651b":"#### 5. Bayesian Ridge Modeling","70238913":"#### 7. Impute the missing values.","38c28c8e":"- The measurement scale used for GarageQual and GarageCond: **None Po Fa TA Gd Ex**\n- The value scale to be used for GarageQual and GarageCond: **0 1 2 3 4 5**\n- The measurement scale used for GarageFinish: **None Unf RFn Fin**\n- The value scale to be used for GarageFinish: **0 1 2 3**","a07867a7":"#### 1. Importing the necessary librairies for Modeling","52faa634":"Fence : data description says NA means \"no fence\"","de3300f6":"- Feature Set = Quality"}}