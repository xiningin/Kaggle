{"cell_type":{"f76eb0c8":"code","b52043e2":"code","35617de2":"code","406a8e37":"code","b88acf23":"code","e539363a":"code","9cb5d447":"code","d7c35a5d":"code","f787b5d1":"code","f81bd39f":"code","7d08f11d":"markdown","b63be011":"markdown","2cd0f94a":"markdown","466695d0":"markdown","78990dff":"markdown","aaa12cac":"markdown"},"source":{"f76eb0c8":"# Import libraries and modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score","b52043e2":"train_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","35617de2":"# We'll sepearte features and labels now\nX = train_df.drop(['label'], axis = 1)\ny = train_df['label']\n\n# And now, normalize the features\nX = X\/255.0\ntest_df = test_df\/255.0\n\n# Converting the data to np.array\nX = X.values\ntest_df = test_df.values","406a8e37":"print(X.shape, y.shape)","b88acf23":"# Let's split the available dataset into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 89)\n\nprint(X_train.shape, X_val.shape)\nprint(y_train.shape, y_val.shape)","e539363a":"# Let's visualize some samples\ndef drawImg(sample):\n    sample = sample.reshape((28,28))\n    plt.imshow(sample, cmap = 'gray')\n\nimg = X_train[11]\ndrawImg(img)\nprint(\"Image label: \", y_train[11])","9cb5d447":"svm_classif = svm.SVC(kernel = 'linear', C=1.0)\n\nsvm_classif.fit(X, y)","d7c35a5d":"## Making final predictions\n\npredictions = svm_classif.predict(test_df)","f787b5d1":"submission = pd.DataFrame({'ImageId' : range(1,28001), 'Label' : list(predictions)})\nsubmission.head()","f81bd39f":"submission.to_csv('submisson.csv', index = False)","7d08f11d":"### One vs Rest Classification\n\nThis method will require lesser number of classifiers than before but will take a bit more time. In this method, we take a binary classifier to compare a certain class against rest of the labels. Each classifier will predict if the current label is voted by the majority and then returned back as predicted label. \n\nThe scikit-learn implementation will use this method only.","b63be011":"## Multiclass Classification with SVMs\n\nWe already know that in the SVM algorithm, we choose the best seperating line\/plane for classification. Therefore, it is a binary classifier. But for this dataset, we require the classifier to be able to recognize multiple labels and therefore, we will be taking a look at two techniques for this classification.","2cd0f94a":"## Load and prepare data","466695d0":"## Digit Recognition with Support Vector Machine\n\nFor this submission, we will use multiclass classifictation with SVMs to train and predict the handwritten digit from the image. ","78990dff":"### One vs One Classification\n\nIn this method, we will choose a number of classifiers where each classifier will perform a binary classification on two of the classes at a time. If we have a total of N classes then we will require  {N choose 2}  classifiers. Then, the class with the majority votes will be identifed as the predicted label.","aaa12cac":"## Training and making predictions"}}