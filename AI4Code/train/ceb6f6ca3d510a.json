{"cell_type":{"9ff26aba":"code","f293df62":"code","b09d6aeb":"code","531d7c9b":"code","caed6996":"code","1fafa00e":"code","79902403":"code","938dc566":"code","02794552":"code","71ca27e7":"code","861cc49c":"code","a675190e":"code","620d55df":"code","161d7b59":"code","10a4c294":"code","b9252c16":"code","3a0e3b09":"code","824d29ba":"code","f7ba1dd4":"code","de0d456d":"code","5e96027c":"code","35d7c8fe":"code","ef2e6f8c":"code","94c11aef":"code","33411540":"code","d9cf40ab":"code","b62feb5c":"code","1eb1b810":"code","ae0fe8b6":"code","73e85508":"code","69abb0e9":"code","40ca4e18":"code","778fd363":"code","ab081732":"code","05fbde72":"code","d2302124":"code","c5419199":"code","45b6652f":"code","cefdda48":"code","2e3047fe":"code","827868d0":"code","ab530c95":"code","be37b6ae":"code","f054acf4":"markdown","a6f7c18f":"markdown","dda67d92":"markdown","91576053":"markdown","f8b34b09":"markdown","c11dcef8":"markdown","38c09d3b":"markdown","9b5459e4":"markdown","cba88514":"markdown","05c4bb3e":"markdown","a3b7f4b9":"markdown","f4ecf4d5":"markdown","45ea8a0e":"markdown","dbcbbcac":"markdown","c86b4ebc":"markdown","60691b6d":"markdown","f237d018":"markdown","1adade9e":"markdown","f811e0a8":"markdown","e43f4e7c":"markdown","810ec1f5":"markdown","d9faadae":"markdown","80d8f34b":"markdown","f30c628d":"markdown","759ef186":"markdown","6dfa2db6":"markdown","d8ea6930":"markdown","a8fc6a52":"markdown","842482d5":"markdown","5925c05b":"markdown","92169aec":"markdown","90973d56":"markdown","8ac344f3":"markdown"},"source":{"9ff26aba":"%matplotlib inline\nimport re\nimport nltk\nimport string\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()","f293df62":"# nltk.download(['twitter_samples', 'stopwords'])","b09d6aeb":"from colorama import init, Fore, Style\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import twitter_samples, stopwords","531d7c9b":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","caed6996":"positive_twts = twitter_samples.strings('positive_tweets.json')\nnegative_twts = twitter_samples.strings('negative_tweets.json')","1fafa00e":"print(Fore.GREEN + 'Positive Tweets: ' + str(len(positive_twts)))\nprint(Fore.RED + 'Negative Tweets: ' + str(len(negative_twts)))","79902403":"fig, ax = plt.subplots(figsize=(5, 5))\nax.pie(\n    [len(positive_twts), len(negative_twts)],\n    labels=['Positive', 'Negative'],\n    autopct='%2.2f%%',\n    startangle=90\n)\nplt.show()","938dc566":"print(Fore.GREEN + positive_twts[random.randint(0, 5000)])\nprint(Fore.RED + negative_twts[random.randint(0, 5000)])","02794552":"print(stopwords.words('english'))","71ca27e7":"print(string.punctuation)","861cc49c":"def process_tweet(tweet):\n    \"\"\"\n    Preproceeses a Tweet by removing hashes, RTs, @mentions,\n    links, stopwords and punctuation, tokenizing and stemming \n    the words.\n    \n    Accepts:\n        tweet {str} -- tweet string\n    \n    Returns:\n        {list<str>}\n    \"\"\"\n    \n    proc_twt = re.sub(r'^RT[\\s]+', '', tweet)\n    proc_twt = re.sub(r'@[\\w_-]+', '', proc_twt)\n    proc_twt = re.sub(r'#', '', proc_twt)\n    proc_twt = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', proc_twt)\n    tokenizer = TweetTokenizer(\n        preserve_case=False, \n        strip_handles=True,\n        reduce_len=True\n    )\n    \n    twt_clean = []\n    twt_tokens = tokenizer.tokenize(proc_twt)\n    stopwords_en = stopwords.words('english')\n    for word in twt_tokens:\n        if word not in stopwords_en and word not in string.punctuation:\n            twt_clean.append(word)\n            \n    twt_stems = []\n    stemmer = PorterStemmer()\n    for word in twt_clean:\n        twt_stems.append(stemmer.stem(word))\n        \n    return twt_stems","a675190e":"def build_freqs(tweets, labels):\n    \"\"\"\n    Builds frequencies for a set of twwets and\n    matching labels or sentiments\n    \n    Accepts:\n        tweets {list<str>} -- tweets in the dataset\n        labels {float} -- label (0. = -ve; 1. = +ve)\n    \n    Returns:\n        {list<float>}\n    \"\"\"\n        \n    labels = np.squeeze(labels).tolist()\n\n    freqs = {}\n    for (tweet, label) in zip(tweets, labels):\n        for word in process_tweet(tweet):\n            if (word, label) in freqs:\n                freqs[(word, label)] += 1\n            else:\n                freqs[(word, label)] = 1\n                \n    return freqs","620d55df":"labels = np.append(\n    np.ones(len(positive_twts)),\n    np.zeros(len(negative_twts))\n)\nlabels","161d7b59":"freqs = build_freqs(positive_twts + negative_twts, labels)","10a4c294":"keys = [\n    'happi', 'merri', 'nice', 'good', \n    'bad','sad', 'mad', 'best', 'pretti',\n    '\u2764', ':)', ':(', '\ud83d\ude12', '\ud83d\ude2c', '\ud83d\ude04',\n    '\ud83d\ude0d', '\u265b', 'song', 'idea', 'power', \n    'play', 'magnific', 'grind', 'bruise'\n]\n\ndata = []\n\nfor word in keys:\n    pos_freq = neg_freq = 0\n    if (word, 1) in freqs:\n        pos_freq = freqs[(word, 1)]\n    if (word, 0) in freqs:\n        neg_freq = freqs[(word, 0)]\n    data.append([word, pos_freq, neg_freq])\n    \ndata","b9252c16":"fig, ax = plt.subplots(figsize = (8, 8))\n\nx = np.log([x[1] + 1 for x in data])  \ny = np.log([x[2] + 1 for x in data]) \n\nax.scatter(x, y)  \n\nplt.xlabel(\"Log Positive Count\")\nplt.ylabel(\"Log Negative Count\")\n\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color='red')\n\nplt.show()","3a0e3b09":"def extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1, 2)\n    '''\n    word_l = process_tweet(tweet)\n    x = np.zeros((1, 2)) \n    for word in word_l:\n        x[0, 0] += freqs.get((word, 1), 0)\n        x[0, 1] += freqs.get((word, 0), 0)\n    assert(x.shape == (1, 2))\n    return x","824d29ba":"extract_features('This is a sample tweet in which I am happy :)', freqs)","f7ba1dd4":"x_train_pos, x_test_pos, y_train_pos, y_test_pos = train_test_split(\n    positive_twts, labels[:5000],\n    test_size=0.2, random_state=42\n)","de0d456d":"x_train_neg, x_test_neg, y_train_neg, y_test_neg = train_test_split(\n    negative_twts, labels[5000:],\n    test_size=0.2, random_state=42\n)","5e96027c":"x_train = x_train_pos + x_train_neg\ny_train = np.append(np.ones(len(x_train_pos)), np.zeros(len(x_train_neg)))","35d7c8fe":"x_test = x_test_pos + x_test_neg\ny_test = np.append(np.ones(len(x_test_pos)), np.zeros(len(x_test_neg)))","ef2e6f8c":"print(Fore.GREEN + 'Length of training set: ', str(len(x_train)))\nprint(Fore.RED + 'Length of testing set: ' + str(len(x_test)))","94c11aef":"print(Fore.GREEN + 'Shape of training set labels: ', str(y_train.shape))\nprint(Fore.RED + 'Shape of testing set labels: ' + str(y_test.shape))","33411540":"fig = plt.figure(figsize=(5, 5))\nplt.scatter(\n    np.array([extract_features(x, freqs)[0] for x in x_train])[:, 1],\n    np.array([extract_features(x, freqs)[0] for x in x_train])[:, 0],\n    c=[['green', 'red'][int(y)] for y in y_train],\n    s=1\n)\nplt.show()","d9cf40ab":"fig = plt.figure(figsize=(5, 5))\nplt.scatter(\n    np.array([extract_features(x, freqs)[0] for x in x_test])[:, 1],\n    np.array([extract_features(x, freqs)[0] for x in x_test])[:, 0],\n    c=[['green', 'red'][int(y)] for y in y_test],\n    s=1\n)\nplt.show()","b62feb5c":"x_lr_train = np.array([extract_features(x, freqs)[0] for x in x_train])\ny_lr_train = y_train\n\nx_lr_test = np.array([extract_features(x, freqs)[0] for x in x_test])\ny_lr_test = y_test","1eb1b810":"from sklearn.linear_model import SGDClassifier\n\nlogRegModel = SGDClassifier(loss='log')\nlogRegModel.fit(x_lr_train, y_lr_train)\n\ny_lr_pred = logRegModel.predict(x_lr_test)","ae0fe8b6":"print('Confusion Matrix:')\nsns.heatmap(confusion_matrix(y_test, y_lr_pred), cmap='YlGnBu')\nplt.show()","73e85508":"print('Classification Report:')\nprint(classification_report(y_test, y_lr_pred, target_names=['-ve', '+ve']))","69abb0e9":"acc_logReg = accuracy_score(y_test, y_lr_pred) * 100\nprint(f'Accuracy: {acc_logReg:2.2f}%')","40ca4e18":"x_nb_train = np.array([extract_features(x, freqs)[0] for x in x_train])\ny_nb_train = y_train\n\nx_nb_test = np.array([extract_features(x, freqs)[0] for x in x_test])\ny_nb_test = y_test","778fd363":"from sklearn.naive_bayes import CategoricalNB\n\nnbModel = CategoricalNB()\nnbModel.fit(x_nb_train, y_nb_train)\n\ny_nb_pred = nbModel.predict(x_nb_test)","ab081732":"print('Confusion Matrix:')\nsns.heatmap(confusion_matrix(y_test, y_nb_pred), cmap='YlGnBu')\nplt.show()","05fbde72":"print('Classification Report:')\nprint(classification_report(y_test, y_nb_pred, target_names=['-ve', '+ve']))","d2302124":"acc_nb = accuracy_score(y_test, y_nb_pred) * 100\nprint(f'Accuracy: {acc_nb:2.2f}%')","c5419199":"x_knn_train = np.array([extract_features(x, freqs)[0] for x in x_train])\ny_knn_train = y_train\n\nx_knn_test = np.array([extract_features(x, freqs)[0] for x in x_test])\ny_knn_test = y_test","45b6652f":"from sklearn.neighbors import KNeighborsClassifier\n\nkMax = 20\nkVals = list(range(1, kMax + 1))\nmean_acc = np.zeros(len(kVals))\nstd_acc = np.zeros(len(kVals))\n\nfor i in kVals:\n    knnModel = KNeighborsClassifier(n_neighbors=i).fit(x_knn_train, y_knn_train)\n    yHat = knnModel.predict(x_knn_test)\n    mean_acc[i - 1] = np.mean(yHat == y_knn_test);\n    \nbestK = pd.DataFrame({'k':kVals, 'mean_acc':mean_acc}).set_index('k')['mean_acc'].idxmax()\nprint(Fore.YELLOW + 'Best k =', bestK)","cefdda48":"knnModel = KNeighborsClassifier(n_neighbors=bestK).fit(x_knn_train, y_knn_train)\n\ny_knn_pred = knnModel.predict(x_knn_test)","2e3047fe":"print('Confusion Matrix:')\nsns.heatmap(confusion_matrix(y_test, y_knn_pred), cmap='YlGnBu')\nplt.show()","827868d0":"print('Classification Report:')\nprint(classification_report(y_test, y_knn_pred, target_names=['-ve', '+ve']))","ab530c95":"acc_knn = accuracy_score(y_test, y_knn_pred) * 100\nprint(f'Accuracy: {acc_knn:2.2f}%')","be37b6ae":"ax = sns.barplot(['LogReg', 'NaiveBayes', 'KNN'], [acc_logReg, acc_nb, acc_knn])\nfor p in ax.patches:\n    ax.annotate(\n        f'{p.get_height():2.2f}%', \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, -20), textcoords = 'offset points'\n    )\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.show()","f054acf4":"As the Twiiter dataset is already split into two files based on whether they imply a positive sentiment or not, we import them individually.","a6f7c18f":"# About NLP and NLTK\n\n> NLP is important for scientific, economic, social, and cultural reasons. NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies. For this reason it is important for a wide range of people to have a working knowledge of NLP. Within industry, this includes people in human-computer interaction, business information analysis, and web software development. Within academia, it includes people in areas from humanities computing and corpus linguistics through to computer science and artificial intelligence.\n>\n> NLTK was originally created in 2001 as part of a computational linguistics course in the Department of Computer and Information Science at the University of Pennsylvania. Since then it has been developed and expanded with the help of dozens of contributors. It has now been adopted in courses in dozens of universities, and serves as the basis of many research projects.\n\n_Source: [https:\/\/www.nltk.org\/book\/](https:\/\/www.nltk.org\/book\/)_","dda67d92":"# Logistic Regression Classifier","91576053":"In logistic regression, we have the sigmoid function for the prediction which maps the input logits $z$ to a value that ranges between 0 and 1, and so it can be treated as a probability. It applies the sigmoid to the output of the linear regression logits.\n\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n\n$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n\nThe cost function used for logistic regression is the average of the log loss across $m$ training examples:\n\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))$$\n\nIn order to optimise the model and find the optimum parameter vector $\\theta$, we implement gradient descent.\n\n$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\frac{\\partial J(\\theta)}{\\partial \\theta}$$","f8b34b09":"Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.","c11dcef8":"# Data Pre-processing and Feature Extraction","38c09d3b":"Punctuations do not add meaning to the textual data in this context, we can safely remove them.","9b5459e4":"According to Bayes' theorem,\n\n![](https:\/\/www.saedsayad.com\/images\/Bayes_rule.png \"\")\n\nNaive Bayes is an algorithm that can also be used for sentiment analysis. $P(D_{pos})$ is the probability that the document is positive. $P(D_{neg})$ is the probability that the document is negative. Here, $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets.\n\n$$P(D_{pos}) = \\frac{D_{pos}}{D}$$\n\n$$P(D_{neg}) = \\frac{D_{neg}}{D}$$\n\nAccording to the Naive Bayes algorithm,\n\n$$p = \\frac{P(pos)}{P(neg)} \\prod_{i=1}^{m} \\frac{P(word_i | pos)}{P(word_i | neg)}\\$$\n\nTo compute the positive and negative probability for a specific word using these formulae:\n\n$$ P(pos) = \\frac{freq_{pos} + 1}{N_{pos} +|V|}$$\n\n$$ P(neg) = \\frac{freq_{neg} + 1}{N_{neg} +|V|}$$\n\nHere, $1$ is added to the numerator to account for words having zero probabilities and $|V|$, the size of the vocabulary is added to the denominator to account for the $1$ in the numerator. This is Laplacian or Additive Smoothing. \n\nThe term $\\frac{P(pos)}{P(neg)}$ can lead to underflow, therefore, $\\log \\frac{P(pos)}{P(neg)} = \\log{P(pos)} - \\log{P(neg)}$ is calculated and is known as the $\\log$ prior.\n\nTo compute the $\\log$ likelihood, i.e., the ratio of the probabilities of a word being in one of the classes of that very same word, we can implement the following equation which will result in a value within $[0, \\infty)$:\n\n$$\\log \\text{likelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)$$\n\nA $\\log$ likelihood of $1$ shall mean that the word is neutral, a value $>1$ means that the word is positive and negative otherwise.\n\nNow, we sum the $\\log$ prior and the $\\log$ likelihood to get $p$.\n\nThe evaluation is done by computing the following metric:\n\n$$J = \\frac{1}{m} \\sum_{i=1}^n pred_i = y$$","cba88514":"We cherrypick some stems and emojis and construct a n x 3 dimensional matrix to represent the positive and negative frequencies for each stem and emoji.","05c4bb3e":"We visualize the aforementioned matrix on a scatter plot. We draw a line to mark a rough decision boundary. We see that data points on the left of the boundary have negative sentiment and those on the right have positive sentiment.","a3b7f4b9":"# Data Exploration","f4ecf4d5":"We define a function that cleans a tweet by removing RTs, @mentions, URLs and hashes in the hashtags. Furthermore, we undertake tokenization and stemming of the words.","45ea8a0e":"We take a quick look at the size of the dataset and it's general distribution. We can note that it is a balanced dataset.","dbcbbcac":"# Dataset Overview","c86b4ebc":"# Result","60691b6d":"NLTK was designed with four primary goals in mind:\n\n- **Simplicity**: To provide an intuitive framework along with substantial building blocks, giving users a practical knowledge of NLP without getting bogged down in the tedious house-keeping usually associated with processing annotated language data.\n- **Consistency**: To provide a uniform framework with consistent interfaces and data structures, and easily-guessable method names.\n- **Extensibility**: To provide a structure into which new software modules can be easily accommodated, including alternative implementations and competing approaches to the same task.\n- **Modularity**: To provide components that can be used independently without needing to understand the rest of the toolkit.","f237d018":"The KNN Model with hyperparameter `k = 4` performed best in classifying the sentiments of the tweets.","1adade9e":"**Please upvote this notebook if it was useful :)**","f811e0a8":"We also take a look at two random tweets in the dataset that correspond to the two sentiments.","e43f4e7c":"We build the labels vector for the dataset as we already know that the datasets have been split on the basis of the sentiment. Then we build the frequency mapping for a certain word and a sentiment.","810ec1f5":"# KNN Classifier","d9faadae":"# Validation of Processed Data","80d8f34b":"# Dependencies","f30c628d":"As far as model performance metrics are concerned, we can plot the model accuracies in a bar plot and determine which model performed the best on the dataset and our way of feature extraction.","759ef186":"We define a function that returns the frequency mapping for each word for each tweet in a set of tweets and the two sentiment classes.","6dfa2db6":"## Testing Dataset","d8ea6930":"# Naive Bayes Classifier","a8fc6a52":"# Train Test Split","842482d5":"# Feature Extraction\n\nFor each tweet, we can compute positive and negative class frequencies for each word in that tweet. In this case, we are considering the sum of positive and negative frequencies as the feature vector for each tweet.","5925c05b":"# Conclusion","92169aec":"## Training Dataset","90973d56":"Language processing tasks and corresponding NLTK modules with examples of functionality:\n\n| Task | NLTK Module(s)\t| Functionality |\n| :-: | :-: | :-: |\n|Accessing corpora|corpus|standardized interfaces to corpora and lexicons|\n|String processing|tokenize, stem|tokenizers, sentence tokenizers, stemmers|\n|Collocation discovery|collocations|t-test, chi-squared, point-wise mutual information|\n|Part-of-speech tagging|tag\t|n-gram, backoff, Brill, HMM, TnT|\n|Machine learning|classify, cluster, tbl|decision tree, maximum entropy, naive Bayes, EM, k-means|\n|Chunking|chunk|regular expression, n-gram, named-entity|\n|Parsing|parse, ccg|chart, feature-based, unification, probabilistic, dependency|\n|Semantic interpretation|sem, inference|lambda calculus, first-order logic, model checking|\n|Evaluation metrics|metrics|precision, recall, agreement coefficients|\n|Probability and estimation|probability|frequency distributions, smoothed probability distributions|\n|Applications|app, chat|graphical concordancer, parsers, WordNet browser, chatbots|\n|Linguistic|fieldwork|toolbox\tmanipulate data in SIL Toolbox format|","8ac344f3":"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) between word vectors in this case.\n\nWe can compute the similarity between two vectors using the Euclidean distance. Euclidean distance is defined as:\n\n$$ \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n\n\nCosine similarity, based on the intuition that two vectors make an angle $\\theta$, can also be used as a similaroty metric, even though it has no relation to the distance metric.\n\n$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$$\n\nTo select the $k$ that\u2019s right for the data, we run the KNN algorithm several times with different values of $k$ and choose the $k$ that reduces the number of errors we encounter while maintaining the algorithm\u2019s ability to accurately make predictions when it\u2019s given data it hasn\u2019t seen before."}}