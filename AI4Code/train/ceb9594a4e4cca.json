{"cell_type":{"8f3b5c5c":"code","eb5b0d37":"code","e3d15191":"code","9f46fc6a":"code","c32b0e40":"code","347f1a62":"code","6330a809":"code","b5454b01":"code","7880cca4":"code","f19d20fa":"code","ffdeeafe":"code","6c91cbaa":"code","64952729":"markdown","6bc34858":"markdown","8e2ac4a8":"markdown","e9c62e5d":"markdown","d951981d":"markdown","4f2ca87e":"markdown","3b85e27c":"markdown","657e316a":"markdown"},"source":{"8f3b5c5c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.optimizers import *\nfrom keras.layers import *\nimport gensim\nimport string\nimport os\nprint(os.listdir(\"..\/input\"))","eb5b0d37":"# Use pandas to read the dataset\ndataset = pd.read_csv('..\/input\/articles.csv')\n# Convert the dataset to numpy array and gets the text row only\ndataset = dataset.values[:,1]\n\nprint (dataset.shape)","e3d15191":"# Create a translator to map every character in the outlab to inlab\ninlab = ''\noutlab = string.punctuation + '\u2013'\ntranlator = str.maketrans(inlab, inlab, outlab)\n\n# array to add the words\nbrokePhrases = []\n\n# for every news\nfor i in range (0, dataset.shape[0]):\n  # get the news\n  new = str(dataset[i])  \n  # for every phrase in that news \n  for phrase in new.split('.')[:-1]:\n    # make all characteres lower\n    editedPhrase = phrase.lower()\n    # put none in every position where a character is found\n    editedPhrase = editedPhrase.translate(tranlator)\n    # break the phrase in words and add that splited phrase to the list\n    brokePhrases.append([w for w in editedPhrase.split(' ') if w != ''])\n    \nprint (brokePhrases[0])","9f46fc6a":"w2v_model = gensim.models.Word2Vec(brokePhrases, size=50, window=7, min_count=10, workers=8)","c32b0e40":"print(w2v_model.wv.most_similar('13', topn=5))\nprint(w2v_model.wv.most_similar('deus', topn=5))\nprint(w2v_model.wv.most_similar('pa\u00eds', topn=5))\nprint(w2v_model.wv.most_similar('computador', topn=5))\nprint(w2v_model.wv.most_similar(positive=['rei', 'mulher'], negative=['homem'], topn=5))\nprint(w2v_model.wv.most_similar(positive=['m\u00e3e', 'menino'], negative=['pai'], topn=5))\nprint(w2v_model.wv.most_similar(positive=['doutor', 'mulher'], negative=['homem'], topn=5))","347f1a62":"# get the weights of the model\nw2v_weights = w2v_model.wv.vectors\nprint(w2v_weights.shape)","6330a809":"# size of the sequence to train and predict\nsequenceSize = 10\n# number of samples for the LSTM dataset \nmaxNumberSamples = 1000000\n\n# alloc the dataset\ndatasetLSTM_X = np.zeros(shape=(maxNumberSamples,sequenceSize, w2v_weights.shape[1]))\ndatasetLSTM_Y = np.zeros(shape=(maxNumberSamples, w2v_weights.shape[1]))\n\n# Every sample is a sequence of a vector with the size of the model output\nprint(datasetLSTM_X.shape)\n# Every sample is a vector with the size of the model output\nprint(datasetLSTM_Y.shape)","b5454b01":"# get the maxNumberSamples first in the text\nfor i in range (0, maxNumberSamples):\n    # verify if the news has at least sequenceSize number of words\n    if len(brokePhrases[i]) > sequenceSize:\n        # iterator to index the news              \n        for j in range (0, len(brokePhrases[i])-sequenceSize-1):            \n            try:\n                # gets the sequenceSize words to create X\n                datasetLSTM_X[i] = np.array([w2v_model.wv[w] for w in brokePhrases[i][j:j+sequenceSize]])\n                # the the next word of the sequenceSize words before\n                datasetLSTM_Y[i] = np.array([w2v_model.wv[brokePhrases[i][j+sequenceSize+1]]])\n            except KeyError:\n                pass\n                                  ","7880cca4":"#split the datasets\ntrain_x = datasetLSTM_X[:int(0.9*maxNumberSamples)]\ntrain_y = datasetLSTM_Y[:int(0.9*maxNumberSamples)]\nval_x = datasetLSTM_X[int(0.9*maxNumberSamples):]\nval_y = datasetLSTM_Y[int(0.9*maxNumberSamples):]","f19d20fa":"sampleToShow = 2000\nfor i in range(0, sequenceSize):\n    print(w2v_model.most_similar(train_x[sampleToShow][i].reshape(1,w2v_weights.shape[1]))[0][0], end=\" \")\n\nprint(w2v_model.most_similar(train_y[sampleToShow].reshape(1,w2v_weights.shape[1]))[0][0])\n","ffdeeafe":"# creates and compile the model\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(sequenceSize,w2v_weights.shape[1])))\nmodel.add(Dense(w2v_weights.shape[1],  activation='linear'))\nmodel.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics=['mse'])","6c91cbaa":"# the initial phrase\ninitialPhrase = ['na', '\u00faltima', 'semana', 'o', 'brasil', 'ficou', 'sabendo', 'de', 'uma', 'not\u00edcia']\n\nk = 0\nwhile (k < 10):\n    # train for one epoch\n    history = model.fit(train_x, train_y, epochs=1, batch_size=2048, shuffle=False, validation_data=(val_x, val_y), verbose=1)\n    \n    if k % 2 == 0:\n        i = 0\n        # convert the array to wordvec\n        array = np.array([w2v_model.wv[initialPhrase[0]], w2v_model.wv[initialPhrase[1]], \\\n                          w2v_model.wv[initialPhrase[2]], w2v_model.wv[initialPhrase[3]], \\\n                          w2v_model.wv[initialPhrase[4]], w2v_model.wv[initialPhrase[5]], \\\n                          w2v_model.wv[initialPhrase[6]], w2v_model.wv[initialPhrase[7]], \\\n                          w2v_model.wv[initialPhrase[8]], w2v_model.wv[initialPhrase[9]]])\n\n        # print the next ten words predicted\n        print(initialPhrase, end=\" \")\n        \n        while(i < 10):\n            # gets the net output\n            outputLSTM = model.predict(array.reshape(1,sequenceSize,w2v_weights.shape[1]))\n            # shift to left the phrase\n            array = np.roll(array, -1)\n            # and puts the predicted word as the last word of the array to predict\n            array[-1] = (w2v_model.wv[(w2v_model.most_similar(outputLSTM)[0][0])])\n            # gets the most similar word that represents the predicted word \n            print(w2v_model.most_similar(array[-1].reshape(1,w2v_weights.shape[1]))[0][0], end=\" \")\n\n            i += 1\n\n        print(\"\\n\")\n    k+=1","64952729":"**Read the dataset**","6bc34858":"**Create the Phrases to train the Word2Vec**","8e2ac4a8":"**Visualize the samples**","e9c62e5d":"**Create the LSTM dataset**","d951981d":"**Model**","4f2ca87e":"**Train the Word2Vec**","3b85e27c":"**Valid the embedding**","657e316a":"**Split dataset**"}}