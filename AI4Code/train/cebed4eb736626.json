{"cell_type":{"5f4c788e":"code","ba82de63":"code","0b9e91c7":"code","449a9b13":"code","83a23dba":"code","1c98fa9b":"code","fc28518f":"code","b96deb4e":"code","88681b2a":"code","b20c6c58":"code","faf3ccc8":"code","c222d2e8":"code","1b10776c":"code","29132e62":"code","20e11687":"code","99442349":"code","99c89b1a":"code","577de666":"code","272e4285":"code","1fd089ab":"code","5df4c41e":"code","91e181cd":"code","240ca833":"markdown","1693b19a":"markdown","e1b2b7f2":"markdown","65e926d6":"markdown","63f4d52f":"markdown","d8b5e63e":"markdown","9d80b324":"markdown","6fc4d108":"markdown"},"source":{"5f4c788e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ba82de63":"train_data = pd.read_csv(\"..\/input\/sign_mnist_train.csv\")\ntest_data =  pd.read_csv(\"..\/input\/sign_mnist_test.csv\")","0b9e91c7":"train_data.shape","449a9b13":"train_data.head()","83a23dba":"labels = train_data[\"label\"].values","1c98fa9b":"label_class = np.unique(np.array(labels))","fc28518f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport math","b96deb4e":"plt.figure(figsize =(18,8))\nsns.countplot(x=labels)","88681b2a":"train_data.drop(\"label\" , axis=1 , inplace=True)","b20c6c58":"one_hot = pd.get_dummies(labels)","faf3ccc8":"one_hot = np.array(one_hot)\none_hot.shape","c222d2e8":"image_data = train_data.values\nimage_data","1b10776c":"#print(image_data[0].reshape(28,28))","29132e62":"plt.imshow(image_data[0].reshape(28,28))","20e11687":"images = np.array([image_data[i].reshape(28,28) for i in range(image_data.shape[0])])","99442349":"flatten_images = np.array([i.flatten() for i in images])","99c89b1a":"def create_placeholders(n_x, n_y):\n    \n    X = tf.placeholder(tf.float32, shape= [n_x , None] , name=\"X\")\n    Y = tf.placeholder(tf.float32, shape= [n_y , None] , name=\"Y\")\n    \n    return X,Y","577de666":"def initialise_parameters():\n    \n    W1 = tf.get_variable(\"W1\", [25,784], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer()) \n    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer()) \n    W3 = tf.get_variable(\"W3\", [24,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b3 = tf.get_variable(\"b3\", [24,1], initializer = tf.zeros_initializer()) \n    \n    parameters = {\n                    \"W1\":W1,\n                    \"W2\":W2,\n                    \"W3\":W3,\n                    \"b1\":b1,\n                    \"b2\":b2,\n                    \"b3\":b3\n    }\n    \n    return parameters    ","272e4285":"def forward_propogation(X,parameters):\n    #getting all the parameters \n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    b1 = parameters[\"b1\"]\n    b2 = parameters[\"b2\"]\n    b3 = parameters[\"b3\"]\n    \n    Z1 = tf.add(tf.matmul(W1,X),b1)\n    A1 = tf.nn.relu(Z1)\n    Z2 = tf.add(tf.matmul(W2,A1),b2)\n    A2 = tf.nn.relu(Z2)\n    Z3 = tf.add(tf.matmul(W3,A2),b3)   #It is important to note that the forward propagation stops at z3. \n                                        #The reason is that in tensorflow the last linear layer output is given as input to the function computing the loss. \n                                        #Therefore, you don't need a3\n    return Z3\n    ","1fd089ab":"def compute_cost(Z3,Y):\n    \n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    cost = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(logits = logits , lables=labels))\n    return cost","5df4c41e":"def BackPropogation(cost, learning_rate):\n    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(cost)\n    return optimizer","91e181cd":"def mini_batches(X,Y, minibatch_size):\n    \n    m = X.shape[0]\n    minibatch = []\n    \n    permutation = list(np.random.permutation(m))\n    \n    #shuffling the data\n    shuffled_X = X[permutation,:]\n    shuffled_Y = Y[permutation,:]\n    \n    number_of_batches = math.floor(m\/minibatch_size)\n    \n    for i in range(0, number_of_batches):\n        \n        minibatch_X = shuffled_X[ i*minibatch_size: (i+1)*minibatch_size , :]\n        minibatch_Y = shuffled_Y[ i*minibatch_size: (i+1)*minibatch_size , :]\n        \n        minibatch_tuple = (minibatch_X , minibatch_Y)\n        minibatch.append(minibatch_tuple)\n        \n    # handling last batch of the set\n    \n    if m% minibatch_size !=0:\n        \n        minibatch_X = shuffled_X[:(m-(number_of_batches*minibatch_size)) , :]\n        minibatch_Y = shuffled_Y[:(m-(number_of_batches*minibatch_size)) , :]\n        \n        minibatch_tuple = (minibatch_X , minibatch_Y)\n        minibatch.append(minibatch_tuple)\n        \n        \n    return minibatch","240ca833":"### Forward propogation","1693b19a":"### Backpropogation ","e1b2b7f2":"### fucntion to define placeholders","65e926d6":"### Computing cost of the model ","63f4d52f":"### initialising the parameters","d8b5e63e":"### Fetching all images from training data","9d80b324":"### creating mini batches of data ","6fc4d108":"## Building model for classification "}}