{"cell_type":{"81029193":"code","0ba32ae1":"code","f363c331":"code","ba781271":"code","44eb3d03":"code","941b3136":"code","faec0449":"code","e9804026":"code","45087b1f":"code","9c84f8a0":"code","6ea4d5ac":"code","ee2a8611":"code","1663332e":"code","ab90da8c":"code","cd989469":"code","40533ae4":"code","291d3113":"code","23e15a04":"code","c933062f":"code","16ddf9b7":"code","5f863ec4":"code","8bd5e1e7":"code","e22173bd":"code","a0329f90":"code","7b99c2ca":"code","6f0c35d3":"code","d51e457b":"code","3c298e93":"code","6564140c":"code","0565251c":"code","66e2b8da":"code","5d4c4588":"code","449d44a5":"code","87beb008":"code","a5ae3c3a":"code","05b1df1b":"code","6712151b":"code","6f5c5b6a":"code","e484ff56":"code","51559903":"code","3847e01c":"code","1acf6b7f":"code","dd9fab8e":"code","02e883b7":"code","7bf600e4":"code","14209f7a":"code","637ff60b":"code","3e24c40a":"code","a3658255":"code","e853a07e":"code","80c599fd":"code","7b23d21e":"code","ad325e2f":"code","a8c1f99b":"code","a11ed196":"code","5059196f":"code","2b4697b8":"code","ae755dbe":"code","7ea6d57c":"code","ab0f9bef":"code","399f4aab":"code","181b0884":"code","b7d74822":"code","43ba38de":"code","0ad9a686":"code","2dd19a3e":"code","368d8b8b":"code","a7fff210":"code","5f1b7025":"code","3c63385b":"code","fc2a435a":"code","741671fc":"code","375d1e32":"code","a0c197e8":"code","077f1612":"code","034ec4e8":"code","8693879f":"code","ed4511ae":"code","9cc3cb87":"code","4b1d14cd":"code","81a4e6b4":"code","0fafcf7e":"code","212f8dd1":"code","34cb967e":"code","2988b673":"code","745d857d":"code","d221030e":"code","212c5825":"code","2cf33781":"code","dcf5efbf":"code","207b6976":"code","dd890642":"code","194c643c":"code","9b329445":"code","742280eb":"code","a76f108e":"code","0de8e5ff":"code","1688ed05":"code","3ed7598d":"code","217f0fbf":"code","10c3307c":"code","ccddeb61":"code","8c468fdb":"code","93299559":"code","8f0dc6a7":"code","987df19e":"code","ce96b38f":"code","bc964ec3":"code","928be245":"code","2e585fcd":"code","e8929f0b":"code","1f988237":"code","a76ce8f5":"code","4a65e61f":"code","15d23d2c":"code","7815b277":"code","5bcda576":"code","28abf22d":"code","650fba0f":"code","8e290c69":"code","a9e61591":"code","c6781fb8":"code","a55f65e2":"code","6817711c":"code","4b37e09a":"code","0c62d60d":"code","55332a92":"code","915a9ce8":"code","3df6cd52":"code","a7806f92":"code","3c028d13":"code","d5ade1d3":"code","4a0105d6":"code","b07d864d":"code","9ef35ee4":"code","3a7129f2":"code","a8cb3eaa":"code","eacf7e67":"code","400dd7bd":"code","d671a181":"code","1d2db5d9":"code","8cdae620":"code","88e7d367":"code","29757d10":"code","7a4d9bb5":"code","5fc307f7":"code","c3aa3a51":"code","00b40dac":"code","fcf27865":"code","ad232f57":"code","f9aa193b":"code","b1a0f3c3":"markdown","b7beb6e3":"markdown","60f33ab4":"markdown","8edb0244":"markdown","d380474b":"markdown","76cb5e50":"markdown","54676897":"markdown","b50191a2":"markdown","ffd61e72":"markdown","4b491005":"markdown","20b44d58":"markdown","1ccfa836":"markdown","60fdf7e5":"markdown","f7d1ffd9":"markdown","6fc053ae":"markdown","9ce92559":"markdown","62518077":"markdown","963bb0cc":"markdown","80b0022a":"markdown","bc2ab02a":"markdown","5527da4b":"markdown","201f9289":"markdown","a92d2a04":"markdown","4a9dd097":"markdown","74d1a8df":"markdown","9cda0d98":"markdown","380772c2":"markdown","cdb9dfc0":"markdown","5573a47f":"markdown","5d053934":"markdown","9f02099c":"markdown","ebe11ff9":"markdown","38e49d2d":"markdown","ca5a0dc1":"markdown","dae0fc01":"markdown","13be6c0d":"markdown","d6b656eb":"markdown","32a79e3f":"markdown","79b446e2":"markdown","c624f96f":"markdown","4393bd2e":"markdown","355751e4":"markdown","e8d04e63":"markdown","5ae0149c":"markdown","f144e2f3":"markdown","904a2031":"markdown","2aea8be7":"markdown","27fe5de5":"markdown","265dcc2f":"markdown","5fd23a55":"markdown","b5e894d0":"markdown","9751c7ed":"markdown","d1363ac5":"markdown"},"source":{"81029193":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0ba32ae1":"#Importing python libraries\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Perceptron\nimport os\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn import linear_model","f363c331":"from scipy.stats import skew, norm, probplot, boxcox\n","ba781271":"# import train and test to play with it\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df_dup = train_df.copy()\ntest_df = pd.read_csv('..\/input\/test.csv')\nlabels_df = train_df.pop('SalePrice') \n\npd.set_option('display.max_rows',100)# amount of rows that can be seen at a time\n\n","44eb3d03":"labels_df.describe()","941b3136":"train_df.shape,test_df.shape","faec0449":"data = pd.concat([train_df, test_df], keys=['train_df', 'test_df'])\nprint(data.columns) # check column decorations\nprint('rows:', data.shape[0], ', columns:', data.shape[1]) # count rows of total dataset\nprint('rows in train dataset:', train_df.shape[0])\nprint('rows in test dataset:', test_df.shape[0])","e9804026":"data.info()","45087b1f":"nans = pd.concat([train_df.isnull().sum(), train_df.isnull().sum() \/ train_df.shape[0], \n                  test_df.isnull().sum(), test_df.isnull().sum() \/ test_df.shape[0]], axis=1, \n                 keys=['Train', 'Percentage', 'Test', 'Percentage'])\nprint(nans[nans.sum(axis=1) > 0])","9c84f8a0":"labels_df.describe()","6ea4d5ac":"\ndef rstr(df, pred): \n    obs = df.shape[0]\n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = df.apply(lambda x: [x.unique()])\n    nulls = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing_ration = (df.isnull().sum()\/ obs) * 100\n    skewness = df.skew()\n    kurtosis = df.kurt() \n    print('Data shape:', df.shape)\n    \n    if pred:\n        corr = df.corr()[pred]\n        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis, corr], axis = 1, sort=False)\n        corr_col = 'corr '  + pred\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing_ration', 'uniques', 'skewness', 'kurtosis', corr_col ]\n    \n    str.columns = cols\n    dtypes = str.types.value_counts()\n    print('___________________________\\nData types:\\n',str.types.value_counts())\n    print('___________________________')\n    return str","ee2a8611":"details = rstr(train_df_dup, 'SalePrice')\ndisplay(details.sort_values(by='corr SalePrice', ascending=False))","1663332e":"train_df_dup.corr()['SalePrice']","ab90da8c":"test_df.head()","cd989469":"test_df.isnull().sum()","40533ae4":"# Now its time for insightful visualization","291d3113":"# We can clearly see a clear and strong relationship between the 'SalePrice' and 'OverQual' which makes perfect sense as we all\n# know that ovelall quality of house matters the most while buying the house.","23e15a04":"labels_df.isnull().sum() # Hence no nan values in Salesprice","c933062f":"labels_df.shape,train_df.shape # Thus both have same no of records so no issue with Salesprice ","16ddf9b7":"labels_df.describe() ","5f863ec4":"import seaborn as sns\nplt.figure(figsize=(16, 6))\nsns.set(style=\"whitegrid\")\nax = sns.boxplot(x=labels_df)","8bd5e1e7":"import matplotlib.pyplot as plt\nplt.figure(figsize=(16, 6))\nsns.distplot(train_df_dup['SalePrice']);","e22173bd":"def fig_plot(data, measure):\n    fig = plt.figure(figsize=(20,7))\n\n    #Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(data)\n\n    #Kernel Density plot\n    fig1 = fig.add_subplot(121)\n    sns.distplot(data, fit=norm)\n    fig1.set_title(measure + ' Distribution ( mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma), loc='center')\n    fig1.set_xlabel(measure)\n    fig1.set_ylabel('Frequency')\n\n    #QQ plot\n    fig2 = fig.add_subplot(122)\n    res = probplot(data, plot=fig2)\n    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f} and kurtosis: {:.6f} )'.format(data.skew(), data.kurt()), loc='center')\n\n    plt.tight_layout()\n    plt.show()\n","a0329f90":"fig_plot(train_df_dup.SalePrice, 'Sales Price')","7b99c2ca":"labels_df = pd.DataFrame(labels_df)","6f0c35d3":"labels_df['SalePrice'].head()\n","d51e457b":"# We will try the log transformation to see that can we change it to a normal distribution.","3c298e93":"\nlabels_df.SalePrice = np.log1p(labels_df.SalePrice)\n\nfig_plot(labels_df.SalePrice, 'Log1P of Sales Price')","6564140c":"def rstr(df, pred): \n    obs = df.shape[0]\n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = df.apply(lambda x: [x.unique()])\n    nulls = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing_ration = (df.isnull().sum()\/ obs) * 100\n    skewness = df.skew()\n    kurtosis = df.kurt() \n    print('Data shape:', df.shape)\n    \n    if pred:\n        corr = df.corr()[pred]\n        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis, corr], axis = 1, sort=False)\n        corr_col = 'corr '  + pred\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing_ration', 'uniques', 'skewness', 'kurtosis', corr_col ]\n    \n    str.columns = cols\n    dtypes = str.types.value_counts()\n    print('___________________________\\nData types:\\n',str.types.value_counts())\n    print('___________________________')\n    return str","0565251c":"details = rstr(train_df_dup, 'SalePrice')\ndisplay(details.sort_values(by='corr SalePrice', ascending=False))","66e2b8da":"fig_plot(train_df_dup.OverallQual, 'OverallQual')","5d4c4588":"data.set_index('Id',inplace =True)","449d44a5":"data.head()","87beb008":"data.isnull().sum() > 0\nprint(data.shape)","a5ae3c3a":"data['HouseArea'] = data['GrLivArea']+data['1stFlrSF'] \n+ data['2ndFlrSF']- data['LowQualFinSF']\ndata.drop(['GrLivArea','1stFlrSF','2ndFlrSF','LowQualFinSF'],axis = 1,inplace = True)\nprint(data.shape)","05b1df1b":"train_df_dup['HouseArea'] = train_df_dup['GrLivArea']+train_df_dup['1stFlrSF'] \n+ train_df_dup['2ndFlrSF']- train_df_dup['LowQualFinSF']\ntrain_df_dup.drop(['GrLivArea','1stFlrSF','2ndFlrSF','LowQualFinSF'],axis = 1,inplace = True)\nprint(train_df_dup.shape)","6712151b":"test_df['HouseArea'] = test_df['GrLivArea']+test_df['1stFlrSF'] \n+ test_df['2ndFlrSF']- test_df['LowQualFinSF']\ntest_df.drop(['GrLivArea','1stFlrSF','2ndFlrSF','LowQualFinSF'],axis = 1,inplace = True)\nprint(test_df.shape)","6f5c5b6a":"train_df_dup.isnull().sum() > 0 ","e484ff56":"data.isnull().sum()","51559903":"data.describe()","3847e01c":"# Now basement\nprint(train_df_dup['BsmtFinSF1'].isna().sum());\nprint(train_df_dup['BsmtFinSF2'].isna().sum());\nprint(train_df_dup['BsmtUnfSF'].isna().sum());\nprint(test_df['BsmtFinSF1'].isna().sum());\nprint(test_df['BsmtFinSF2'].isna().sum());\nprint(test_df['BsmtUnfSF'].isna().sum());\nprint(data['BsmtFinSF1'].isna().sum());\nprint(data['BsmtFinSF2'].isna().sum());\nprint(data['BsmtUnfSF'].isna().sum());","1acf6b7f":"test_df['BsmtFinSF1'].fillna(0,inplace =True)\ntest_df['BsmtFinSF2'].fillna(0,inplace =True)\ntest_df['BsmtUnfSF'].fillna(0,inplace = True)\nprint(test_df['BsmtFinSF1'].isna().sum());\nprint(test_df['BsmtFinSF2'].isna().sum());\nprint(test_df['BsmtUnfSF'].isna().sum());\ndata['BsmtFinSF1'].fillna(0,inplace =True)\ndata['BsmtFinSF2'].fillna(0,inplace =True)\ndata['BsmtUnfSF'].fillna(0,inplace = True)\nprint(data['BsmtFinSF1'].isna().sum());\nprint(data['BsmtFinSF2'].isna().sum());\nprint(data['BsmtUnfSF'].isna().sum());\n\n","dd9fab8e":"data['BasementArea'] = (data['BsmtFinSF1'] ** 2 \/ data['TotalBsmtSF'])\n+(data['BsmtFinSF2'] ** 2 \/ data['TotalBsmtSF'])\n-(data['BsmtUnfSF'] ** 2 \/ data['TotalBsmtSF']);\ndata.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis = 1,inplace = True)\nprint(data.shape)","02e883b7":"train_df_dup['BasementArea'] = (train_df_dup['BsmtFinSF1'] ** 2 \/ train_df['TotalBsmtSF'])\n+(train_df_dup['BsmtFinSF2'] ** 2 \/ train_df['TotalBsmtSF'])\n-(train_df_dup['BsmtUnfSF'] ** 2 \/ train_df['TotalBsmtSF']);\ntrain_df_dup.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis = 1,inplace = True)\nprint(train_df_dup.shape)","7bf600e4":"test_df['BasementArea'] = (test_df['BsmtFinSF1'] ** 2 \/ test_df['TotalBsmtSF'])\n+(test_df['BsmtFinSF2'] ** 2 \/ test_df['TotalBsmtSF'])\n-(test_df['BsmtUnfSF'] ** 2 \/ test_df['TotalBsmtSF']);\ntest_df.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis = 1,inplace = True)\nprint(test_df.shape)","14209f7a":"print(train_df_dup['BasementArea'].isna().sum(),\ndata['BasementArea'].isna().sum())\n\n","637ff60b":"train_df_dup['BasementArea'].fillna(0,inplace = True)\ntest_df['BasementArea'].fillna(0,inplace =True)\ndata['BasementArea'].fillna(0,inplace =True)\n","3e24c40a":"print(train_df_dup['BasementArea'].isna().sum(),\ntest_df['BasementArea'].isna().sum(),\ndata['BasementArea'].isna().sum())","a3658255":"data['LotFrontage'].fillna(data['LotFrontage'].median()\n,inplace =True)","e853a07e":"data['LotFrontage'].isnull().sum()","80c599fd":"train_df_dup['LotFrontage'].fillna(train_df_dup['LotFrontage'].median()\n,inplace =True)\n","7b23d21e":"train_df_dup['LotFrontage'].isna().sum()","ad325e2f":"test_df['LotFrontage'].fillna(test_df['LotFrontage'].median()\n,inplace =True)","a8c1f99b":"train_df_dup.corr()['SalePrice']","a11ed196":"data.drop(['MSSubClass'],axis = 1,inplace = True)\nprint(data.shape)","5059196f":"train_df_dup.drop(['MSSubClass'],axis = 1,inplace = True)\nprint(train_df_dup.shape)","2b4697b8":"test_df.drop(['MSSubClass'],axis = 1,inplace = True)\nprint(test_df.shape)","ae755dbe":"import seaborn as sns\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=\"MSZoning\", y=\"SalePrice\", data=train_df_dup)","7ea6d57c":"data['MSZoning'].isna().sum()\ndata['MSZoning'].fillna('FV',inplace =True)\ndata['MSZoning'].isna().sum()\n","ab0f9bef":"Ms = pd.get_dummies(train_df_dup['MSZoning'])\nMs1 = pd.get_dummies(test_df['MSZoning'])\nMs2 = pd.get_dummies(data['MSZoning'])","399f4aab":"data['MSZoning'].value_counts()\n","181b0884":"\n\ntrain_df_dup['MSZoning'].value_counts()","b7d74822":"train_df_dup = pd.concat([train_df_dup,Ms],axis = 1)\nprint(train_df_dup.shape)\ntest_df =  pd.concat([test_df,Ms1],axis = 1)\nprint(test_df.shape)","43ba38de":"data = pd.concat([data,Ms2],axis = 1)\nprint(data.shape)","0ad9a686":"print(train_df_dup['LotFrontage'].isna().sum(),\ndata['LotFrontage'].isna().sum())\n","2dd19a3e":"print(train_df_dup['LotArea'].isna().sum(),\ndata['LotArea'].isna().sum())\n","368d8b8b":"print(train_df_dup['Street'].value_counts())\nprint(train_df_dup['Alley'].value_counts())\nprint(test_df['Street'].value_counts())\nprint(test_df['Alley'].value_counts())\nprint(data['Street'].value_counts())\nprint(data['Alley'].value_counts())\n","a7fff210":"replace_names = {\"Street\" : {\"Grvl\" : 0,\"Pave\" : 1},\n                 \"Alley\" : {\"Grvl\": 1 ,\"Pave\" : 2 , \"NA\" : 0}}\ntrain_df_dup.replace(replace_names,inplace =True)\ntest_df.replace(replace_names,inplace =True)\ndata.replace(replace_names,inplace =True)\nprint(data['Street'].value_counts())\nprint(data['Alley'].value_counts())\nprint(train_df_dup['Street'].value_counts())\nprint(train_df_dup['Alley'].value_counts())\nprint(test_df['Street'].value_counts())\nprint(test_df['Alley'].value_counts())","5f1b7025":"dict(train_df_dup.groupby('LotShape')['SalePrice'].mean())","3c63385b":"print(train_df_dup.shape,\ntest_df.shape,data.shape)","fc2a435a":"from scipy import stats\nsns.distplot(train_df_dup['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_df_dup['SalePrice'], plot=plt)","741671fc":"train_df_dup.isnull().sum()\n","375d1e32":"train_df_dup['Alley'].isna().sum()\n","a0c197e8":"test_df['Alley'].isna().sum()\n","077f1612":"data['Alley'].isna().sum()","034ec4e8":"train_df_dup['Alley'].value_counts()","8693879f":"test_df['Alley'].value_counts()","ed4511ae":"data['Alley'].value_counts()","9cc3cb87":"train_df_dup.Alley.fillna(0,inplace = True)\n","4b1d14cd":"test_df.Alley.fillna(0,inplace = True)\n","81a4e6b4":"data.Alley.fillna(0,inplace = True)","0fafcf7e":"data.Alley.value_counts()\n","212f8dd1":"data.isna().sum()","34cb967e":"train_df_dup['Alley'].value_counts()","2988b673":"test_df['Alley'].value_counts()","745d857d":"display(train_df_dup.Electrical.value_counts())\ndisplay(test_df.Electrical.value_counts())\n\ntrain_df_dup.Electrical.fillna('SBrKr',inplace = True)\ntrain_df_dup.Electrical.isnull().sum()","d221030e":"data.Electrical.fillna('SBrKr',inplace = True)\ndata.Electrical.isnull().sum()","212c5825":"list = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']","2cf33781":"for i in list:\n    print(train_df_dup[i].value_counts())\n    train_df_dup[i].fillna(train_df_dup[i].mode()[0],inplace = True)\n    print(train_df_dup[i].isnull().sum())","dcf5efbf":"list1 = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','TotalBsmtSF','MasVnrType'\n         ,'MasVnrArea','Utilities','Exterior1st','MSZoning','Exterior2nd','SaleType']\nfor j in list1:\n    print(test_df[j].value_counts())\n    test_df[j].fillna(test_df[j].mode()[0],inplace = True)\n    print(test_df[j].isnull().sum())","207b6976":"list2 = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','TotalBsmtSF','MasVnrType'\n         ,'MasVnrArea','Utilities','Exterior1st','MSZoning','Exterior2nd','SaleType']\nfor j in list2:\n    print(data[j].value_counts())\n    data[j].fillna(data[j].mode()[0],inplace = True)\n    print(data[j].isnull().sum())","dd890642":"data.isna().sum()","194c643c":"train_df_dup.isna().sum()","9b329445":"train_df_dup.MiscFeature.value_counts()","742280eb":"train_df_dup.MiscFeature.fillna('None',inplace = True)","a76f108e":"train_df_dup.MiscFeature.value_counts()","0de8e5ff":"train_df_dup.Fence.value_counts()","1688ed05":"train_df_dup.Fence.fillna('None',inplace = True)","3ed7598d":"train_df_dup.Fence.value_counts()","217f0fbf":"train_df_dup.PoolQC.value_counts()","10c3307c":"train_df_dup.PoolQC.fillna('None',inplace = True)","ccddeb61":"train_df_dup.PoolQC.value_counts()","8c468fdb":"l3 = ['PoolQC','Fence','MiscFeature']\nfor i in l3:\n    test_df[i].fillna('None',inplace = True)\n    print(test_df[i].isna().sum())","93299559":"l4 = ['PoolQC','Fence','MiscFeature']\nfor i in l4:\n    data[i].fillna('None',inplace = True)\n    print(data[i].isna().sum())","8f0dc6a7":"l1 = ['FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond'] \nfor j in l1:\n    print(train_df_dup[j].value_counts())\n    train_df_dup[j].fillna(train_df_dup[j].mode()[0],inplace = True)\n    print(train_df_dup[j].isnull().sum())","987df19e":"l2 = ['FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','GarageCars','GarageArea'\n      ,'BsmtFullBath','BsmtHalfBath','Functional','KitchenQual'] \nfor j in l2:\n    print(test_df[j].value_counts())\n    test_df[j].fillna(test_df[j].mode()[0],inplace = True)\n    print(test_df[j].isnull().sum())","ce96b38f":"l5 = ['FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','GarageCars','GarageArea'\n      ,'BsmtFullBath','BsmtHalfBath','Functional','KitchenQual'] \nfor j in l2:\n    print(data[j].value_counts())\n    data[j].fillna(data[j].mode()[0],inplace = True)\n    print(data[j].isnull().sum())","bc964ec3":"train_df_dup.isnull().sum().sum()","928be245":"data.isnull().sum().sum()","2e585fcd":"test_df.shape\n","e8929f0b":"train_df_dup.shape","1f988237":"data.shape","a76ce8f5":"\n\n#Changing OverallCond into a categorical variable\ndata['OverallCond'] = data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","4a65e61f":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'OverallCond', \n        'YrSold', 'MoSold')","15d23d2c":"for c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(data[c].values)\n    data[c] = lbl.transform(data[c].values)\n\n# shape        \nprint('Shape all_data: {}'.format(data.shape))","7815b277":"data = pd.get_dummies(data)\nprint(data.shape)","5bcda576":"Train = data[:1460]","28abf22d":"Train.shape","650fba0f":"Test = data[1460:]","8e290c69":"Test.shape","a9e61591":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","c6781fb8":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(Train.values)\n    rmse= np.sqrt(-cross_val_score(model, Train.values, labels_df, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","a55f65e2":"labels_df.describe()","6817711c":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","4b37e09a":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","0c62d60d":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","55332a92":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","915a9ce8":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","3df6cd52":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","a7806f92":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","3c028d13":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(),\n                                                score.std()))","d5ade1d3":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4a0105d6":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b07d864d":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9ef35ee4":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3a7129f2":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","a8cb3eaa":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eacf7e67":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","400dd7bd":"model_xgb.fit(Train,labels_df)","d671a181":"Submission = pd.DataFrame()\nSubmission['LogSalePrice'] = model_xgb.predict(Test)","1d2db5d9":"predictions = model_xgb.predict(Test)\npredictions = np.expm1(predictions)\nSubmission['SalePrice'] = predictions","8cdae620":"Submission['Id'] = test_df['Id']","88e7d367":"Submission.head()","29757d10":"Submission.drop('LogSalePrice',axis = 1,inplace =True)","7a4d9bb5":"Submission.set_index('Id',inplace =True)","5fc307f7":"Submission['SalePrice'].describe()","c3aa3a51":"labels_df.SalePrice = np.log1p(labels_df.SalePrice)\n\nfig_plot(labels_df.SalePrice, 'Log1P of Sales Price')","00b40dac":"\nfig_plot(Submission.SalePrice, 'Sales Price')","fcf27865":"Submission.head()","ad232f57":"Output = Submission.to_csv('Output.csv')","f9aa193b":"Submission.to_csv('Output.csv')","b1a0f3c3":"***We have most null values in 'LotFrontage','Alley','FireplaceQu','Fence' and 'MiscFeature'.***\n<br>\n*Other features only have a few missing values.*","b7beb6e3":"Clearly we can see that there is skewness in pdf, so we need to fix this using transformation.","60f33ab4":"Nulls: The data have 19 features with nulls, five of then area categorical and with more then 47% of missing ration. They are candidates to drop or use them to create another more interesting feature:<br>\n* PoolQC\n* MiscFeature\n* Alley\n* Fence\n* FireplaceQu\n\nFeatures high skewed right, heavy-tailed distribution, and with high correlation to Sales Price. It is important to treat them (boxcox 1p transformation, Robustscaler, and drop some outliers):<br>\n* TotalBsmtSF\n* 1stFlrSF\n* GrLivArea<br>\n\nFeatures skewed, heavy-tailed distribution, and with good correlation to Sales Price. It is important to treat them (boxcox 1p transformation, Robustscaler, and drop some outliers):<br>\n\n\n* LotArea\n* KitchenAbvGr\n* ScreenPorch\n* EnclosedPorch\n* MasVnrArea\n* OpenPorchSF\n* LotFrontage\n* BsmtFinSF1\n* WoodDeckSF\n* MSSubClass\n\n<br>\nFeatures high skewed, heavy-tailed distribution, and with low correlation to Sales Price. Maybe we can drop these features, or just use they with other to create a new more importants feature:\n<br>\n\n* MiscVal\n* TSsnPorch\n* LowQualFinSF\n* BsmtFinSF2\n* BsmtHalfBa\n<br>\n\nFeatures low skewed, and with good to low correlation to Sales Price. Just use a Robustscaler probably reduce the few distorcions:\n\n\n* BsmtUnfSF\n* 2ndFlrSF\n* TotRmsAbvGrd\n* HalfBath\n* Fireplaces\n* BsmtFullBath\n* OverallQual\n* BedroomAbvGr\n* GarageArea\n* FullBath\n* GarageCars\n* OverallCond\n<br>\n\nTransforme from Yaer Feature to Age, 2011 - Year feature, or YEAR(TODAY()) - Year Feature\n<br>\n* YearRemodAdd:\n* YearBuilt\n* GarageYrBlt\n* YrSold","8edb0244":"Now we have done this I guess finding the skewness and kurtosis and skewness of all feature all at once would be better","d380474b":"**Street**","76cb5e50":"![Overall Quality vs Saleprice](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/213949\/465252\/Qual%20vs%20Price.PNG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559899756&Signature=Gd9lVEPuFABK7TyzmQl6n1NkhzdVSyfa3eOx8FldAonVWAEQC6BREF2HJbHPiisElrZzwV94XLyx8ajdjkoq6QUeKP2BgsGeCQPmR8XE2BdnMEtZ7muEc7B%2F8fMksVM5a67r3wjhnlkJ2lF4WkblyXzVEY5Ry5b5bUAINHlhESJWB589PGVKvfvznKPssImj2PjVo6G3rFUATXYzuUN4wLU5KT9rVTy8TnZEK6CXqFRHCrtGdgCz1W4%2FuqdfpK0%2F1c4kLLww32Oj3snZgfg2Cbh2ryyz7Z3%2FZ2tOe35nIVudy0vrw9wCE0bn56VpWIpdHAwVBeJ9MXQpkg40VZaLLg%3D%3D)","54676897":"**Getting dummy categorical features\n**","b50191a2":"* Lasso Regression","ffd61e72":"# Visualisation","4b491005":"**Lot Frontage**","20b44d58":"* Gradient Boosting Regression :","1ccfa836":"From the above we can see that there are several outliers in sales prices.Now lets more investigate more using distribution plot\nand look deep into it.","60fdf7e5":"For area part I think we divide house into three parts :\n* HouseArea\n* Basement Area\n* Garage Area\n* Total Bath\n","f7d1ffd9":"\n**MSSubClass**","6fc053ae":"**Base Models**","9ce92559":"**Averaged base models score**","62518077":"We will use find and replace technique for this categorical feature.","963bb0cc":"# Modelling","80b0022a":"# Now lets start analysing each feature and target variable.","bc2ab02a":"# Exploratory Data Analysis","5527da4b":"We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","201f9289":"Since the correlation of the matrix is so low and negative so I guess its better to drop out from the Dataframe.","a92d2a04":"***Before preprocessing and after starting we must explore data using pandas functions and know about various minute details which must later be inspected.This step is crucial as we don't need to directly rush the process thus making some wrong assumptions and making our data inefficient. This is basically called EDA(Exploratory Data Analysis)***","4a9dd097":"# Importing Libraries","74d1a8df":"* First we will understand the relationship of most important varible with 'SalePrice'. \n* For Visualisation purpose I use Tableau Software with easy to visualise-use-create data.\n* We will analyse bivariate relationship of each data with 'SalePrice' as one variable. Also we will use max.correlation as a measure.\n","9cda0d98":"We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","380772c2":"* Kernel Ridge Regression","cdb9dfc0":"Stacking models","5573a47f":"The more is the size of garage the more the house cost but it seems an exceptions for 4 maybe because others factors may downgraded. This may be considered as outliers.","5d053934":"![Ground-Living-Area vs SalesPrice](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/216698\/469917\/GrlivArea%20vs%20Sale.PNG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559902333&Signature=eGr0DfXQWH825dbL8QG0PbElbzb1w7hSHMOG%2BrCKdDzH89v1svw%2FPaR%2F%2BjM026pdP5LcsZYtmnyEv9mvGRkTz7mUjtgBIiztv2QFw0QhGjmA4M7%2B1uBL68982XW%2BieUPuOk8RXRpSWZ8Tq%2FHYylFHkSktKxgBO5yt7vztp4DdM%2FkWphji%2BmL2bNTigH1amxpx%2BjMqDNEYJ6OXUFGc0%2BwJkhSB1mcfY7bonkPXmS6AaMiDXdHQTbOy9NARLaN0mY3sK5PUYFE8NvPvPEAqeY3AhUbfh4zE%2FHrcIag7I0c3FE6MMqszjGg7ECqBgH%2F8t1dpJh65FzfdokvXMG1bsygAQ%3D%3D)","9f02099c":"**GrlivArea vs Med.SalePrice**","ebe11ff9":"<p>Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.<p>","38e49d2d":"This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","ca5a0dc1":"There is somewhat a positive correleation between the sale price and Living Area as the size of living area is priority among most people\nbecause of family size and other reasons. But as the end there is sharp downfall of price maybe other factors played a more important role at that place. Well that last part can be considered as a possible outliers.","dae0fc01":"![](http:\/\/)","13be6c0d":"**Overall - quality**","d6b656eb":"**Garage Size(in cars) vs Saleprice**","32a79e3f":"* LightGBM :","79b446e2":"**MSZoning**","c624f96f":"**SalesPrice**","4393bd2e":"**LotFrontage**","355751e4":"* Elastic Regresssion","e8d04e63":"Lets design the function we can make boxplot,probablity distribution function(pdf) so that we can get a good reproducible code","5ae0149c":"![**Garage Size(in cars) vs Saleprice**](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/216698\/469917\/Garagecars.PNG?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559902325&Signature=mecgfn1IjMguVsSWmuXQNISTiCvHHF63IGssqdD7R%2FS9FPZ4ARVMMnHzQ2QYr%2B6XPA%2F7%2FzbAB0wEN%2FABLBqb7wJK0tkP9iGuhA%2FZE7Xx0ksHgm7CwdYb%2FIqTxih3Pl%2Fyh1TSzkM0XF9PXrKr0BNPVp%2BYI5jWi6CY59%2Fs%2BLQ25%2BvF%2BQOD0OJRXwU%2FvTXcsyKHe4y41QaRyt6AV4FAGLuPYHaD585t3tTEeFQZP%2BfAQoO%2FRbl0dll1UUpLPz2BrZlMFq%2F8XSa14OtwxgjXI5USmpNzdCFM%2FPwTzoQ8xltY2JAVG2UV5NpAZY004gg2PbswBu6zuZSseTVgqtcp1pwl5w%3D%3D)","f144e2f3":"While going through various kernels ,I got this good reusable function made by Marcelo Marques in his kernel which is quite similar to function str function in R which returns the types, counts, distinct, count nulls, missing ratio and uniques values of each field\/feature.\n\n<br>\nThis is the link to his kernel https:\/\/www.kaggle.com\/mgmarques\/houses-prices-complete-solution","904a2031":"# Label Encoding","2aea8be7":"* \n\n    XGBoost ","27fe5de5":"<h1><b>Competition Description<h1>\n    \n![image.png](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)\n","265dcc2f":"***Base models scores***","5fd23a55":"**Define a cross validation strategy**\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation\n","b5e894d0":"**Overall Quaity vs Saleprice**","9751c7ed":"**LotArea**","d1363ac5":"Lets visualise this above data with Boxplot to getter a better look at it"}}