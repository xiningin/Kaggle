{"cell_type":{"06399b85":"code","c359aa62":"code","8035b1df":"code","1e6adcb1":"code","32b6b196":"code","db5ad80d":"code","f4fba446":"code","737e79de":"code","16e28a87":"code","29b499e7":"code","0e87d2ac":"code","f79a4bf4":"code","000e0cf4":"code","d493e9a1":"code","5c6fffd0":"code","d622a874":"code","8a87c64d":"code","e3a3a87c":"code","ccb53369":"code","c674a531":"code","bbd580c8":"code","033289e3":"code","f2bf1ada":"code","487e0c3b":"code","874099a9":"code","30e38ec5":"code","2dc4df6f":"code","f8d3c5cc":"code","3a5b141a":"code","62297a07":"code","4a56165f":"code","1e762fb0":"code","fec94943":"code","8a24d922":"code","d078bb29":"code","22a36e84":"code","9b1157f1":"code","c2ee8afa":"code","1a11b2c7":"code","7f812934":"code","a4d398f9":"code","cdc0a78d":"code","bf04857e":"code","1d0b00be":"code","c7a2aac8":"code","9babf1f3":"code","88267ce2":"code","120c45c4":"markdown","d098cea8":"markdown","0be05d3c":"markdown","cc27766c":"markdown","0cd21005":"markdown","c0b6d06d":"markdown","423364f5":"markdown","23ca2138":"markdown","1a6ddb51":"markdown","cc39ffe3":"markdown","713e823f":"markdown","1450497e":"markdown","317fd66d":"markdown","f860a57e":"markdown","14f810b8":"markdown","5ecac253":"markdown","314b27dd":"markdown","508d5033":"markdown"},"source":{"06399b85":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c359aa62":"data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","8035b1df":"# data shape\nprint('Data Shape is : ', data.shape)","1e6adcb1":"#check null values\ndata.isna().sum()","32b6b196":"# Features correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(data.corr(),annot = True)","db5ad80d":"def Get_Features_types(data):\n    categorical_features = []\n    numerical_features = []\n    for col in data.iloc[:,:-1]:\n        if data[col].nunique() < 10:\n            categorical_features.append(col)\n        else : \n            numerical_features.append(col)\n    return categorical_features , numerical_features\n\nGet_Features_types(data)\n\ncategorical_features = Get_Features_types(data)[0]\nnumerical_features = Get_Features_types(data)[1]\n\nprint(categorical_features)\nprint(numerical_features)","f4fba446":"sns.countplot( x = 'DEATH_EVENT' , data = data)","737e79de":"def CountPlot_hue_Categorical_data(data):\n    for feature in categorical_features:\n        plt.figure()\n        sns.countplot(x = \"DEATH_EVENT\" , data = data , hue=feature)\n        plt.title(feature)\n\nCountPlot_hue_Categorical_data(data)","16e28a87":"def Get_Percentage_data(data):\n    percentage_death_true_case = []\n    percentage_death_false_case = []\n    for col in categorical_features:\n        true_case = round(data['DEATH_EVENT'][data[col] == 1].value_counts(normalize = True)[1] * 100 , 2)\n        false_case = round(data['DEATH_EVENT'][data[col] == 0].value_counts(normalize = True)[1] * 100,2)\n        percentage_death_true_case.append(true_case)\n        percentage_death_false_case.append(false_case)\n    Percentage = pd.DataFrame(list(zip(percentage_death_true_case , percentage_death_false_case)) ,\n                              index = categorical_features ,\n                              columns = ['% Percentage Death (IF 1)' , '% Percentage Death (IF 0)'])\n    return Percentage\n        \nGet_Percentage_data(data)","29b499e7":"def BOX_Plot_numerical_features(data):\n    for col in numerical_features:\n        plt.figure()\n        sns.boxplot(data[col])","0e87d2ac":"BOX_Plot_numerical_features(data)","f79a4bf4":"print(\"Numerical Features are \" , numerical_features)","000e0cf4":"data[data['ejection_fraction'] > 65]","d493e9a1":"## Age \nsns.distplot(data[data['DEATH_EVENT'] == 0]['age'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['age'] , label = \"Survived\")\nplt.legend()","5c6fffd0":"plt.figure()\nsns.countplot(data[(data['age'] < 70)]['DEATH_EVENT'])\nprint(data[(data['age'] < 70)]['DEATH_EVENT'].value_counts())\nplt.title(\"Age Under than 70\")\nplt.show()\n####\nplt.figure()\nsns.countplot(data[(data['age'] > 70)]['DEATH_EVENT'])\nprint(data[(data['age'] > 70)]['DEATH_EVENT'].value_counts())\nplt.title(\"Age Higher than 70\")\nplt.show()","d622a874":"## Creatinine_phosphokinase\nsns.distplot(data[data['DEATH_EVENT'] == 0]['creatinine_phosphokinase'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['creatinine_phosphokinase'] , label = \"Survived\")\nplt.legend()","8a87c64d":"## Platelets\nsns.distplot(data[data['DEATH_EVENT'] == 0]['platelets'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['platelets'] , label = \"Survived\")\nplt.legend()","e3a3a87c":"## Serum_creatinine\nsns.distplot(data[data['DEATH_EVENT'] == 0]['serum_creatinine'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['serum_creatinine'] , label = \"Survived\")\nplt.legend()","ccb53369":"## Serum_sodium\nsns.distplot(data[data['DEATH_EVENT'] == 0]['serum_sodium'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['serum_sodium'] , label = \"Survived\")\nplt.legend()","c674a531":"# Time\nsns.distplot(data[data['DEATH_EVENT'] == 0]['time'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['time'] , label = \"Survived\")\nplt.legend()","bbd580c8":"plt.figure()\nsns.countplot(data[data['time'] > 90]['DEATH_EVENT'])\nprint(data[data['time'] > 90]['DEATH_EVENT'].value_counts())\nplt.title(\"Time Higher than 90\")\nplt.show()\n####\nplt.figure()\nsns.countplot(data[data['time'] < 90]['DEATH_EVENT'])\nprint(data[data['time'] < 90]['DEATH_EVENT'].value_counts())\nplt.title(\"Time lower than 90\")\nplt.show()\n","033289e3":"# Ejection_fraction\nsns.distplot(data[data['DEATH_EVENT'] == 0]['ejection_fraction'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['ejection_fraction'] , label = \"Survived\")\nplt.legend()","f2bf1ada":"sns.countplot(data[data['ejection_fraction'] > 28]['DEATH_EVENT'])\nprint(data[data['ejection_fraction'] > 28]['DEATH_EVENT'].value_counts())\nplt.title(\"Ejection Fraction more than 30\")\nplt.show()\n#####\nsns.countplot(data[data['ejection_fraction'] < 30]['DEATH_EVENT'])\nprint(data[data['ejection_fraction'] < 30]['DEATH_EVENT'].value_counts())\nplt.title(\"Ejection Fraction lower than 30\")\nplt.show()","487e0c3b":"data_copy= data.copy()","874099a9":"data_copy['platelets\/age'] = data['platelets'] \/ data['age']\ndata_copy['time\/age'] = data['time'] \/ data['age']\n","30e38ec5":"data_copy.drop(['platelets' , 'time'] , axis = 1 , inplace = True ) \ndata_copy.head()","2dc4df6f":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier ,AdaBoostClassifier\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest , f_classif","f8d3c5cc":"#Split the data\nX = data_copy.drop(['DEATH_EVENT'] , axis = 1)\ny = data_copy['DEATH_EVENT']\nx_train , x_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 ,random_state=1)","3a5b141a":"# We will use KNN with differents values of k in order to have an idea about our score accuracy and f1-score\ndef KNN_best_scores(n_iterations):\n    acc_score = []\n    f1_score = []\n    for k in range(1,n_iterations):\n        model = KNeighborsClassifier(n_neighbors= k)\n        model.fit(x_train , y_train)\n        y_pred = model.predict(x_test)\n        acc_score.append(model.score(x_test , y_test))\n        f1_score.append(f1(y_pred , y_test))\n    Knn_scores = pd.DataFrame(list(zip(acc_score , f1_score)) , index = range(1,n_iterations) ,\n                              columns = ['Accuracy Score' , 'F1_Score'])\n    print('Best values for f1_score are : \\n',Knn_scores.nlargest(5, ['F1_Score']))\n    return  Knn_scores.nlargest(5, ['Accuracy Score'])","62297a07":"#Selecting a range of value between (1,20)\nKNN_best_scores(20)","4a56165f":"## RandomForest\ndef RandomForest_best_score():\n    model = RandomForestClassifier()\n    model.fit(x_train,y_train)\n    acc_score = model.score(x_test,y_test)\n    y_pred = model.predict(x_test)\n    f1_score = f1(y_pred , y_test)\n    return pd.DataFrame([[acc_score , f1_score]] , columns = ['Accuracy Score' , 'F1_Score'])\n    ","1e762fb0":"print(\"RandomForest : Accuracy Score \/ F1_Score\")\nRandomForest_best_score()","fec94943":"# Gradient Boosting Classifier\ndef GradientBoosting_best_score():\n    model = GradientBoostingClassifier()\n    model.fit(x_train , y_train)\n    y_pred = model.predict(x_test)\n    acc_score = model.score(x_test , y_test)\n    f1_score = f1(y_pred , y_test)\n    \n    return pd.DataFrame([[acc_score , f1_score]] , columns = ['Accuracy Score' , 'F1_Score'])\n\n    \n    ","8a24d922":"print(\"GradientBoosting : Accuracy Score \/ F1_Score\")\n\nGradientBoosting_best_score()","d078bb29":"# It combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method\ndef AdaBoostClassifier_best_score(list_values):\n    acc_scores = []\n    f1_scores = []\n    for n in list_values:\n        model = AdaBoostClassifier(n_estimators=n,learning_rate=0.01)\n        model.fit(x_train , y_train)\n        acc_score = model.score(x_test , y_test)\n        y_pred = model.predict(x_test)\n        f1_score = f1(y_pred,y_test)\n        acc_scores.append(acc_score)\n        f1_scores.append(f1_score)\n    return pd.DataFrame(list(zip(acc_scores,f1_scores)) , columns = ['Accuracy Score' , 'F1_Score'])","22a36e84":"AdaBoostClassifier_best_score([i for i in range(100,1000,100)])","9b1157f1":"## SelectKBest with range of values (5,6,7,8) and try to find difference between models with and without feature selection\ndef KNN_With_Feature_Selection():\n    acc = []\n    f1_scores = []\n    features_names = []\n    for n in range(5,9):\n        feature_selection = SelectKBest(f_classif , k = n)\n        features_name = feature_selection.fit_transform(data_copy[X.columns], data_copy['DEATH_EVENT'])\n        cols = feature_selection.get_support(indices=True)\n        features_name = data_copy.columns[cols]\n        features_names.append(features_name)\n        knn_processor = make_pipeline(StandardScaler() , feature_selection )\n        for i in range(1,20):\n            KNN_model = make_pipeline(knn_processor , KNeighborsClassifier(n_neighbors = i))\n            KNN_model.fit(x_train , y_train)\n            acc_score = KNN_model.score(x_test,y_test)\n            y_pred = KNN_model.predict(x_test)\n            f1_score = f1(y_pred , y_test)\n            acc.append(acc_score)\n            f1_scores.append(f1_score)\n    DF1 = pd.DataFrame(list(zip(acc[:19] , f1_scores[:19])) , columns = ['Accuracy(n = 5)' , 'F1_Score(n = 5)'])\n    DF2 = pd.DataFrame(list(zip(acc[20:37] , f1_scores[20:37])) , columns = ['Accuracy(n = 6)' , 'F1_Score(n = 6)'])\n    DF3 = pd.DataFrame(list(zip(acc[38:56] , f1_scores[38:56])) , columns = ['Accuracy(n = 7)' , 'F1_Score(n = 7)'])\n    DF4 = pd.DataFrame(list(zip(acc[57:] , f1_scores[57:])) , columns = ['Accuracy(n = 8)' , 'F1_Score(n = 8)'])\n    \n    return (list(features_names[0]) , DF1) , (list(features_names[1]) , DF2) , (list(features_names[2]) , DF3) , (list(features_names[3]) , DF2)\n   ","c2ee8afa":"(FN0,DF1),(FN1,DF2) ,(FN2,DF3) ,(FN3,DF4) = KNN_With_Feature_Selection()\n","1a11b2c7":"print(\"Feature Selection with n =5 \" , FN0)\nDF1","7f812934":"print(\"Feature Selection with n =6\" , FN1)\nDF2","a4d398f9":"print(\"Feature Selection with n =7 \" , FN2)\nDF3","cdc0a78d":"print(\"Feature Selection with n =8 \" , FN3)\nDF4","bf04857e":"def AdaBoostClassifier_best_score_with_feature_selection(list_values):\n    train_score = []\n    acc = []\n    f1_scores = []\n    feature_selection = SelectKBest(f_classif , k = 5)\n    adaboost_processor = make_pipeline(StandardScaler() , feature_selection )\n    for i in list_values:\n        adabooost_model = make_pipeline(adaboost_processor , AdaBoostClassifier(n_estimators= i,learning_rate=0.09))\n        adabooost_model.fit(x_train , y_train)\n        train_sc= adabooost_model.score(x_train,y_train)\n        train_score.append(train_sc)\n        acc_score = adabooost_model.score(x_test,y_test)\n        y_pred = adabooost_model.predict(x_test)\n        f1_score = f1(y_pred , y_test)\n        acc.append(acc_score)\n        f1_scores.append(f1_score)\n    return pd.DataFrame(list(zip(train_score,acc,f1_scores)) , columns = ['Train Score','Accuracy Score' , 'F1_Score'])\n   ","1d0b00be":"AdaBoostClassifier_best_score_with_feature_selection([i for i in range(100,1000,100)])","c7a2aac8":"AdaBoostClassifier_best_score_with_feature_selection([800])","9babf1f3":"### Final Model :\nfeature_selection = SelectKBest(f_classif , k = 5)\nadaboost_processor = make_pipeline(StandardScaler() , feature_selection )\nadaboost_model = make_pipeline(adaboost_processor , AdaBoostClassifier(n_estimators= 800,learning_rate=0.09))\nadaboost_model.fit(X , y)\nmy_predictions = adaboost_model.predict(X)\n","88267ce2":"submission = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nsubmission['Predictions'] = my_predictions\nsubmission.to_csv(\"submission.csv\", index=False)","120c45c4":"<h1 style=\"color:green\">Health Failure Prediction <\/h1>","d098cea8":"- Using domain knowledge to extract features from raw data via data mining techniques is one of the best typical methods to improve our model ","0be05d3c":"People with ejection fracrtion more than 30 have a probability of surviving more than death .","cc27766c":"<h1 style=\"color:red\">Feature Engineering<\/h1>","0cd21005":"<h1 style=\"color:red\">Modelisation<\/h1>","c0b6d06d":"Drawing box plot can show many details : \n- Detecting outliers in ejection_fraction \n- No outliers in features like : Age and time \n- In Medical fields , values for features : <br><\/br>\n   Ejection_fraction <br><\/br>Creatinine_phosphokinase <br><\/br> Platelets <br><\/br> Serum_creatinine <br><\/br>Serum_sodium <br><\/br>are possibles in range of values , so we will keep them in order to have a reasonable prediction \n","423364f5":"This data can show : \n-  People who smoke has a percentage of death (31,25 ) , otherwise peole who don't has a percentage of death ( 32,51) [so strange seriously]\n- Percentage of Men's death is roughly equivalent to women's death ( Same for poeple who has diabetes).\n- In High blood pressure and Amenia  , we can see obviously a little difference between percentage of death ( IF True)\n","23ca2138":"<h5>-Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.<\/h5>","1a6ddb51":"<h3 style=\"color :red\"> Columns Meaning : <\/h3>\n- Age : Age of patient <br><\/br>\n- Anemina : 1 if True , 0 if False <br><\/br>\n- Diabetes : 1 if True , 0 if False <br><\/br>\n- High blood pressure : 1 if True , 0 if False <br><\/br>\n- Sex : 1 Male , 0 Female <br><\/br>\n- Smoking : 1 if True , 0 if False  <br><\/br>\n- Other features are numerical features .\n","cc39ffe3":"- Age , Ejection_fraction , Serum_creatine and time are highly correlated with the target than other features .","713e823f":"<h1 style=\"color:red\">Modelisation Using features selection Techniques<\/h1>","1450497e":"We can see here that people with under than 70 have a probability of surviving more than death  .","317fd66d":"<h1 style=\"color:red\">Categorical Values<\/h1>","f860a57e":"<h1>I'll use an RandomForestClassifier as my final model because it gives good results .\nWe can see how feature selection can improve our model <\/h1>","14f810b8":"- We will use SelectKBest as a feature selector to improve our model","5ecac253":"<h1 style=\"color:red\">Numerical Values<\/h1>","314b27dd":"We should divide our data to two categories : \n- Numerical Values \n- Categorical Values","508d5033":"it's obivious that people more than a follow-up period (days) more than 90 have a probalibity of surviving more than death .\n"}}