{"cell_type":{"69e6b937":"code","0c037074":"code","775e02d0":"code","70c53f03":"code","6d608db5":"code","31780c7f":"code","b67e3b74":"code","e49df668":"code","2ab20fba":"code","80fb42e2":"code","513b6aac":"code","9ac6b59b":"code","a0f04f29":"code","4684174a":"code","34652011":"code","bd13d8b6":"markdown","69c67018":"markdown","fafc9ffb":"markdown","fb96351f":"markdown","f287c129":"markdown","b02f6b26":"markdown","cbd32a4a":"markdown","22d2ad4f":"markdown","202fa4c7":"markdown","3ec76d72":"markdown","d8b204dd":"markdown","4408001e":"markdown","790f81c0":"markdown","3099609c":"markdown","e7cbe2f5":"markdown"},"source":{"69e6b937":"!pip install transformers","0c037074":"from transformers import pipeline, set_seed","775e02d0":"generator = pipeline('text-generation', model='gpt2')","70c53f03":"set_seed(42)\n","6d608db5":"generator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)","31780c7f":"generator(\"The Inidan man worked as a\", max_length=10, num_return_sequences=5)","b67e3b74":"generator(\"She is so beautiful\", max_length=10, num_return_sequences=5)","e49df668":"# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')","2ab20fba":"# Allocate a pipeline for question-answering\nquestion_answerer = pipeline('question-answering')\nquestion_answerer({\n    'question': 'What is the name of the repository ?',\n    'context': 'Pipeline have been included in the huggingface\/transformers repository'})","80fb42e2":"nlp = pipeline(\"question-answering\")\n\ncontext = r\"\"\"\nMicorsoft was founded by Bill gates and Paul allen on 1975.\nThe property of being prime (or not) is called primality.\nA simple but slow method of verifying the primality of a given number n is known as trial division.\nIt consists of testing whether n is a multiple of any integer between 2 and itself.\nAlgorithms much more efficient than trial division have been devised to test the primality of large numbers.\nThese include the Miller\u2013Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\nParticularly fast methods are available for numbers of special forms, such as Mersenne numbers.\nAs of January 2016, the largest known prime number has 22,338,618 decimal digits.\n\"\"\"\n\n#Question 1\nresult = nlp(question=\"What is a simple method to verify primality?\", context=context)\n\nprint(f\"Answer 1: '{result['answer']}'\")\n\n#Question 2\nresult = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n\nprint(f\"Answer 2: '{result['answer']}'\")","513b6aac":"from transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-base-cased')\nunmasker(\"Hello I'm a [MASK] person.\")","9ac6b59b":"#Summarization is currently supported by Bart and T5.\n\nsummarizer = pipeline(\"summarization\")\n\nARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\nFirst conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\nApollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. \nProject Mercury was followed by the two-man Project Gemini (1962\u201366). \nThe first manned flight of Apollo was in 1968.\nApollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. \nGemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\nApollo used Saturn family rockets as launch vehicles. \nApollo\/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973\u201374, and the Apollo\u2013Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n \"\"\"\n\nsummary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\n\nprint(summary['summary_text'])","a0f04f29":"translator = pipeline(\"translation_en_to_de\")\n\nprint(translator(\"A great obstacle to happiness is to expect too much happiness.\", max_length=40)[0]['translation_text'])","4684174a":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft\/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft\/DialoGPT-medium\")\n\n# Let's chat for 5 lines\nfor step in range(5):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n\n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","34652011":"nlp_token_class = pipeline('ner')\nnlp_token_class('Hugging Face is a French company based in New-York.')","bd13d8b6":"### I'll be updating this notbook with new examples","69c67018":"### Named Entity Recognition","fafc9ffb":"### Text generation","fb96351f":"### Conversation","f287c129":"# This kernel demonstrates how to use Hugging Face's transformers package","b02f6b26":"### GPT2\n\n#### Model description\n\n**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n\n","cbd32a4a":"### Question Answering","22d2ad4f":"### Text prediction","202fa4c7":"#### The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library in PyTorch and TensorFlow in a transparent and interchangeable way.","3ec76d72":"### Install transformer","d8b204dd":"### BERT\n\nThe BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\u2019s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nThe abstract from the paper is the following:\n\n> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.`\n> \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","4408001e":"![Screenshot-2019-09-30-17.43.59.png](attachment:Screenshot-2019-09-30-17.43.59.png)","790f81c0":"### English to German translation","3099609c":"### Text Summarization","e7cbe2f5":"### Sentiment analysis"}}