{"cell_type":{"88cfb11b":"code","9c912328":"code","776b212d":"code","616c2ff1":"code","68a05b61":"code","ffad6df9":"code","a5e14bb6":"code","1a23eb3c":"code","1c2a2105":"code","e8bfe215":"code","e22c4497":"code","ad6fac9d":"code","0590a43b":"code","9294c4e1":"code","6229c09b":"code","8dd94630":"code","aa70cca8":"code","74306c51":"code","bd1fd159":"code","ea24a3ca":"code","135e2f3e":"code","b03bf5e4":"code","da657ba2":"code","2109f2b1":"code","b7b2f642":"code","87e303c1":"code","692b3f17":"code","185c5026":"code","d0695013":"code","7856548d":"code","150efae3":"code","3566ec6f":"code","4cdff0e6":"code","5f9a8178":"code","e3f88578":"code","5cb1b5f5":"code","762878e0":"code","08403394":"code","98b5652b":"code","9ce7822d":"code","41ef6ec0":"code","d3c6ecff":"code","51289f1a":"code","1d402ba5":"code","6facfd94":"code","04cde5da":"code","b0fb8927":"code","fad6ece6":"code","7ea4451c":"code","b30133d5":"code","e1aa2beb":"code","c5590512":"code","6de795e5":"code","5c3022b5":"code","5dd21bde":"code","8b3328e7":"code","54f5ad5b":"code","4a3dd020":"code","18669d93":"code","ae9ff4b9":"code","3e7f650f":"code","839c9bd9":"code","5ea0dd52":"code","42aae38b":"code","1c3096fc":"code","e1e08698":"code","3f70f6d1":"code","cd214373":"code","db736dc0":"code","9a01c0cb":"code","35c0a0b4":"code","9c7f7d9f":"code","917060b9":"code","75f3cf45":"code","cf02caca":"code","beec0796":"code","4f950b26":"code","e07fb9ba":"code","0e6b35fa":"code","b16dd1a4":"code","f95d2a16":"code","40f22373":"code","cecf399b":"code","b529c8ee":"code","c16b7b13":"code","0864b88e":"code","041350d3":"code","e91d7a2c":"code","7d3c8d25":"code","fe4e134d":"code","771e2b39":"code","0fd99c4c":"code","718e1d2d":"code","ea182049":"code","93a339ea":"code","4549f0b3":"code","7758c131":"code","f0adc4e9":"code","60d8a8f0":"code","65fc84bc":"code","e7a1b260":"code","1e484b80":"code","bafa8dab":"code","c3c8fc4d":"code","f9073506":"code","ccb9b3ee":"code","5307e464":"code","96b1cfb7":"code","3674933a":"code","2bbb7017":"code","0bf81c67":"code","f1da28fd":"code","89c98bac":"code","0a6ecb49":"code","5cd7b9d9":"code","f0ce2422":"code","b1d45c6a":"code","6d79e420":"code","ec530bcb":"code","4bdae02a":"code","4944d968":"code","bac90c65":"code","60fd29bd":"code","2360fa10":"code","d64f2e71":"code","0db256f7":"code","a6f865b2":"code","0c5dc8c9":"code","7c497e89":"code","a1e6b13e":"markdown","4b1b5e83":"markdown","64f7b71f":"markdown","d29f8520":"markdown","207b8897":"markdown","9f823b97":"markdown","5ab6d8a0":"markdown","a590124e":"markdown","80c068e3":"markdown","d2101444":"markdown","ead7e321":"markdown","f25dfbe3":"markdown"},"source":{"88cfb11b":"import warnings  \nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, date, time, timedelta\nimport math\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import recall_score, precision_score, roc_auc_score\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dropout, Dense, InputLayer\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers\nfrom keras import backend as K\nimport h5py\nimport math","9c912328":"%cd input\n%ls","776b212d":"#fichero='query (14).csv' #USA\nfichero='\/kaggle\/input\/query20190601.csv'\n#fichero='queryCuadrado.csv'\nearthquake =pd.read_csv(fichero, sep=',')\nearthquake.head(1)","616c2ff1":"print (\"n\u00famero de terremotos:\", earthquake.count()[0])","68a05b61":"print('Fecha inicial: ', earthquake['time'].min())\nprint('Fecha final: ', earthquake['time'].max())","ffad6df9":"earthquake['latitude'].describe()","a5e14bb6":"earthquake = earthquake.sort_values(by='time').reindex()","1a23eb3c":"plt.figure(figsize=(10,3))\nearthquake[\"mag\"].hist() ##bins=4\nplt.title('Magnitud de terremotos')","1c2a2105":"earthquake[\"mag\"].describe()","e8bfe215":"# Separamos la variable \"time\" (YYYY-MM-DDThh:mm:ss.s) en \"date\" en formato YYYY-MM-DD\n# y \"time\" sobreescrita en formato hh:mm:ss.s\nearthquake[\"date\"] = earthquake[\"time\"].apply(lambda x: x.split(\"T\")[0])\nearthquake[\"time\"] = earthquake[\"time\"].apply(lambda x: x.split(\"Z\")[0].split(\"T\")[1])\nearthquake[[\"date\",\"time\"]].head(1)","e22c4497":"#Vnw, Vsw, Vne, Vse\nla_min = earthquake[\"latitude\"].min()\nprint(\"Minimum Latitude: \",la_min)\nla_max = earthquake[\"latitude\"].max()\nprint(\"Maximum Latitude: \",la_max)\nlo_min = earthquake[\"longitude\"].min()\nprint(\"Minimum Longitude: \",lo_min)\nlo_max = earthquake[\"longitude\"].max()\nprint(\"Maximum Longitude: \",lo_max)","ad6fac9d":"#Vectores de posici\u00f3n que limitan el \u00e1rea de estudio: Vnw, Vsw, Vne, Vse\nVnw = (lo_min, la_max) #Northwest\nVsw = (lo_min, la_min) #Southwest\nVne = (lo_max, la_max) #Northeast\nVse = (lo_max, la_min) #Southeast\n#Vnw, Vsw, Vne, Vse\nprint('Vnw: ',Vnw)\nprint('Vsw: ',Vsw)\nprint('Vne: ',Vne)\nprint('Vse: ',Vse)\n# N\u00famero de subdivisiones por base y altura:\nmb = 3\nma = 3\n#Base y altura de las sub\u00e1reas de estudio:\nb = round(abs(lo_max - lo_min)\/mb,4)\na = round(abs(la_max - la_min)\/ma,4)\nprint(\"Altura: \", a, \"Base: \", b)","0590a43b":"earthquake[\"i\"] = earthquake[\"latitude\"].apply(lambda x: (x-la_min)\/a)\nearthquake[\"j\"] = earthquake[\"longitude\"].apply(lambda x: (x-lo_min)\/b)\n        \n# earthquake[\"int_i\"] = np.ceil(earthquake[\"i\"])\n# earthquake[\"int_j\"] = np.ceil(earthquake[\"j\"])","9294c4e1":"#plt.plot(earthquake[\"int_i\"],earthquake[\"int_j\"])\nplt.figure(figsize=(20,20))\n# plt.scatter(earthquake[\"int_i\"],earthquake[\"int_j\"])\nplt.scatter(earthquake[\"i\"],earthquake[\"j\"])\nplt.grid(True)\nplt.show()","6229c09b":"earthquake[\"k\"] = 0.0\nearthquake.loc[(earthquake.i < 1.0) & (earthquake.j < 1.0), 'k'] = 1.0\nearthquake.loc[(earthquake.i < 1.0) & (1.0 <= earthquake.j) & (earthquake.j < 2.0), 'k'] = 2.0\nearthquake.loc[(earthquake.i < 1.0) & (2.0 <= earthquake.j) & (earthquake.j < 3.0), 'k'] = 3.0\nearthquake.loc[(1.0 <= earthquake.i) & (earthquake.i < 2.0) & (earthquake.j < 1.0), 'k'] = 4.0\nearthquake.loc[(1.0 <= earthquake.i) & (earthquake.i < 2.0) & (1.0 <=earthquake.j) & (earthquake.j< 2.0), 'k'] = 5.0\nearthquake.loc[(1.0 <= earthquake.i) & (earthquake.i < 2.0) & (2.0 <=earthquake.j) & (earthquake.j < 3.0), 'k'] = 6.0\nearthquake.loc[(2.0 <= earthquake.i) & (earthquake.i < 3.0) & (earthquake.j < 1.0), 'k'] = 7.0\nearthquake.loc[(2.0 <= earthquake.i) & (earthquake.i < 3.0) & (1.0 <=earthquake.j) & (earthquake.j < 2.0), 'k'] = 8.0\nearthquake.loc[(2.0 <= earthquake.i) & (earthquake.i < 3.0) & (2.0 <=earthquake.j) & (earthquake.j <3.0), 'k'] = 9.0\n","8dd94630":"earthquake","aa70cca8":"max_date = datetime.strptime(earthquake[\"date\"].max(), '%Y-%m-%d')\nmin_date = datetime.strptime(earthquake[\"date\"].min(), '%Y-%m-%d')\nprint(max_date)\nprint(min_date)","74306c51":"def diff_month(d1, d2):\n    return np.ceil((d1 - d2).days \/ 14)","bd1fd159":"earthquake[\"quincena\"] = earthquake.date.apply(lambda d: diff_month(datetime.strptime(d, '%Y-%m-%d'), min_date))","ea24a3ca":"earthquake","135e2f3e":"vectors = pd.DataFrame()\nvectors = pd.crosstab(earthquake[\"k\"], earthquake[\"quincena\"]) #C\u00e1lculo de frecuencias\nvectors","b03bf5e4":"d = pd.DataFrame(np.zeros((1,int(earthquake[\"quincena\"].max()))))\nvectors = vectors.append(d)\nvectors =  vectors.fillna(0.0)\nvectors = vectors.T\nvectors = vectors.iloc[:,1:10]\nvectors","da657ba2":"region = vectors.iloc[:,4]","2109f2b1":"print('queryCuadrado.csv - 0.0',region[region == 0.0].count())\nprint('queryCuadrado.csv - 1.0',region[region == 1.0].count())","b7b2f642":"porcentaje_no_terremotos = region[region == 0.0].count() * 100 \/ region.count()\nprint('porcentaje de terremotos: {:.4f}'.format(100 - porcentaje_no_terremotos))","87e303c1":"hoy = datetime.today().strftime('%Y%m%d')\n# nombreFichero = 'vectors_Lorca3x3-{}.nb.csv'.format(hoy)\n# #nombreFichero = 'vectors_mediterraneo3x3-20190608.csv'\n# vectors.to_csv(path_or_buf=nombreFichero, index=False)","692b3f17":"vectors2 = vectors\npd.DataFrame(vectors2.iloc[879:902,4].apply(lambda x: 1 if x>0 else 0))","185c5026":"zona = 4\nregion = vectors2.iloc[:,zona]\nregion.describe()","d0695013":"metrics = dict()","7856548d":"region_central = vectors2.iloc[:,zona]\ntrain_size = int(len(region_central) * 0.7)\ntrain = region_central[0:train_size]\ntest_size = int(len(region_central) * 0.1) + train_size\ntest = region_central[train_size:test_size]\nval = region_central[test_size:len(region_central)]","150efae3":"print('train_size: ', len(train))\nprint('test_size: ', len(test))\nprint('val_size: ', len(val))\n\n\nlen(train) + len(test) + len(val) == len(region_central)","3566ec6f":"def create_dataset(dataset, window_size = 1):\n    data_X, data_Y = [], []\n    #scaler = MinMaxScaler()\n    for i in range(len(dataset) - window_size - 1):\n        a = dataset.values[i:i + window_size]\n        data_X.append(a)       \n        data_Y.append(dataset.iloc[i + window_size])\n    X = np.array(data_X)\n    #X = scaler.fit_transform(X)\n    Y = np.array(data_Y)\n    Y[Y > 1] = 1\n    return X,Y","4cdff0e6":"def DrawGraphcs(model):\n    plt.figure(figsize=(20,5))\n    plt.subplot(1, 2, 1)\n    plt.plot(model.history.history['accuracy'])\n    plt.plot(model.history.history['val_accuracy'])\n    plt.subplot(1, 2, 2)\n    plt.plot(model.history.history['loss'])\n    plt.plot(model.history.history['val_loss'])","5f9a8178":"def Metrics_LR(train_X, train_Y, test_X, test_Y, name, window_size):\n    lr_train_X, lr_train_Y = train_X, train_Y\n    lr_test_X, lr_test_Y = test_X, test_Y\n\n    clf = LogisticRegression()\n    clf.fit(lr_train_X, lr_train_Y)\n\n    lr_y_pred = clf.predict(lr_test_X)\n\n    acc = clf.score(lr_test_X, lr_test_Y)\n    print('Accuracy of logistic regression classifier on test set: {:.4f}'.format(acc))\n    precision = precision_score(lr_test_Y, lr_y_pred)\n    print('Precision of logistic regression classifier on test set: {:.4f}'.format(precision))\n    recall = recall_score(lr_test_Y, lr_y_pred)\n    print('Recall of logistic regression classifier on test set: {:.4f}'.format(recall))\n    roc = roc_auc_score(lr_test_Y, lr_y_pred)\n    print('Roc of logistic regression classifier on test set: {:.4f}'.format(roc))\n    print('Training data shape:{}'.format(train_X.shape))\n    print('Testing data shape:{}'.format(test_X.shape))\n    metrics[name] = [window_size, train_X.shape, test_X.shape, acc, precision, recall, roc]","e3f88578":"def Metrics_NN(model, model_name, test_X, test_Y, window_size):\n    y_pred = model.predict(test_X).round()\n    score, acc = model.evaluate(test_X, test_Y, batch_size=window_size)\n    print('score: ', score)\n    print('acc: ', acc)\n    precision = precision_score(test_Y, y_pred)\n    print('Precision: {:.4f}'.format(precision))\n    recall = recall_score(test_Y, y_pred)\n    print('Recall: {:.4f}'.format(recall))\n    roc = roc_auc_score(test_Y, y_pred)\n    print('ROC: {:.4f}'.format(roc))\n    print('Training data shape:{}'.format(train_X.shape))\n    print('Testing data shape:{}'.format(test_X.shape))\n    metrics[model_name] = [model.count_params(), train_X.shape, test_X.shape, acc, precision, recall, roc]","5cb1b5f5":"def Save_model(model, model_name):\n    hoy = datetime.today().strftime('%Y%m%d')\n    model.name = model_name\n    model.save('{}_{}.h5'.format(model.name,hoy))\n    with open('{}_{}_history.json'.format(model.name,hoy), 'w') as f:\n        json.dump(model.history.history, f)","762878e0":"# Create test and training sets for one-step-ahead regression.\nwindow_size = 1\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nval_X, val_Y = create_dataset(val, window_size)\n\nprint(\"Original training data shape:\")\nprint(train_X.shape)\n","08403394":"Metrics_LR(train_X, train_Y, test_X, test_Y, \"lr_1\", window_size)","98b5652b":"def fit_model_mlp (X_train, y_train, window_size = 1, X_val = None, y_val = None, val = False):\n    model = Sequential()\n    model.add(Dense(4, bias_initializer='ones', input_dim=window_size, activation='tanh', name = 'Dense6'))\n    model.add(Dense(1, bias_initializer = 'ones', activation='sigmoid', name = 'DenseOutput'))\n    model.compile(loss = \"binary_crossentropy\", \n                  optimizer = \"adam\", metrics=[\"accuracy\"])\n    \n    print(model.summary())\n    \n    earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n    \n    if val:\n            model.fit(X_train, \n              y_train, \n              epochs = 100, \n              batch_size = window_size, \n              verbose = 2,\n              validation_data=(X_val, y_val),\n              callbacks = [earlystopper])\n    else:\n        model.fit(X_train, \n                  y_train, \n                  epochs = 100, \n                  batch_size = window_size, \n                  verbose = 1,\n                  callbacks = [earlystopper])\n    \n    #print(model.summary())\n    return(model)\n    \n    ","9ce7822d":"mlp1 = fit_model_mlp(train_X, train_Y, window_size, val_X, val_Y, True)","41ef6ec0":"DrawGraphcs(mlp1)","d3c6ecff":"Metrics_NN(mlp1, \"mlp_1\", test_X, test_Y, window_size)","51289f1a":"models = dict()\nmodels[\"mlp1\"] = mlp1","1d402ba5":"# Reshape the input data into appropriate form for Keras.\ntrain_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\nval_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\nprint(\"New training data shape:\")\nprint(train_X.shape)\nprint(train_X[0])","6facfd94":"def fit_model(X_train, y_train, window_size = 1, X_val = None, y_val = None, val = False):\n    model = Sequential()\n    \n#     model.add(LSTM(1,\n#                    bias_initializer='ones',\n#                    input_shape = (1, window_size),\n#                    return_sequences=True,\n#                    name='InputLayer')\n#                    )\n    \n    model.add(LSTM(units=4, return_sequences=True, bias_initializer='ones', input_shape = (1, window_size), name = 'HiddenLayer1'))\n    #model.add(Dropout(0.33))\n    model.add(LSTM(units=2, return_sequences=True, bias_initializer='ones', name = 'HiddenLayer2'))\n    #model.add(Dropout(0.33))\n    model.add(LSTM(units=1, activation='softmax', bias_initializer='ones', name='OutputLayer'))\n    #model.add(Dense(1))\n    model.compile(loss = \"binary_crossentropy\", \n                  optimizer = \"adam\", metrics=[\"accuracy\"])\n    print(model.summary())\n    \n    earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n    \n    if val:\n            model.fit(X_train, \n              y_train, \n              epochs = 100, \n              batch_size = window_size, \n              verbose = 2,\n              validation_data=(X_val, y_val),\n              callbacks = [earlystopper])\n    else:\n        model.fit(X_train, \n                  y_train, \n                  epochs = 100, \n                  batch_size = window_size, \n                  verbose = 1,\n                  callbacks = [earlystopper])\n    \n    return(model)","04cde5da":"# Fit the first model.\n#model1 = fit_model(train_X, train_Y, window_size)\nlstm1 = fit_model(train_X, train_Y, window_size, val_X, val_Y, True)","b0fb8927":"DrawGraphcs(lstm1)","fad6ece6":"Metrics_NN(lstm1, \"lstm_1\", test_X, test_Y, window_size)","7ea4451c":"metrics","b30133d5":"window_size = 2\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nval_X, val_Y = create_dataset(val, window_size)","e1aa2beb":"Metrics_LR(train_X, train_Y, test_X, test_Y, \"lr_2\", window_size)","c5590512":"mlp2 = fit_model_mlp(train_X, train_Y, window_size, val_X, val_Y, True)","6de795e5":"DrawGraphcs(mlp2)","5c3022b5":"Metrics_NN(mlp2, \"mlp_2\", test_X, test_Y, window_size)","5dd21bde":"train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\nval_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))","8b3328e7":"lstm2 = fit_model(train_X, train_Y, window_size, val_X, val_Y, True)","54f5ad5b":"DrawGraphcs(lstm2)","4a3dd020":"Metrics_NN(lstm2, \"lstm_2\", test_X, test_Y, window_size)","18669d93":"metrics","ae9ff4b9":"window_size = 3\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nval_X, val_Y = create_dataset(val, window_size)\n\n","3e7f650f":"Metrics_LR(train_X, train_Y, test_X, test_Y, \"lr_3\", window_size)","839c9bd9":"mlp3 = fit_model_mlp(train_X, train_Y, window_size, val_X, val_Y, True)","5ea0dd52":"DrawGraphcs(mlp3)","42aae38b":"Metrics_NN(mlp3, \"mlp_3\", test_X, test_Y, window_size)","1c3096fc":"train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\nval_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))","e1e08698":"lstm3 = fit_model(train_X, train_Y, window_size, val_X, val_Y, True)","3f70f6d1":"DrawGraphcs(lstm3)","cd214373":"Metrics_NN(lstm3, \"lstm_3\", test_X, test_Y, window_size)","db736dc0":"metrics","9a01c0cb":"window_size = 4\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nval_X, val_Y = create_dataset(val, window_size)","35c0a0b4":"Metrics_LR(train_X, train_Y, test_X, test_Y, \"lr_4\", window_size)","9c7f7d9f":"mlp4 = fit_model_mlp(train_X, train_Y, window_size, val_X, val_Y, True)","917060b9":"DrawGraphcs(mlp4)","75f3cf45":"Metrics_NN(mlp4, \"mlp_4\", test_X, test_Y, window_size)","cf02caca":"train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\nval_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))","beec0796":"lstm4 = fit_model(train_X, train_Y, window_size, val_X, val_Y, True)","4f950b26":"DrawGraphcs(lstm4)","e07fb9ba":"Metrics_NN(lstm4, \"lstm_4\", test_X, test_Y, window_size)","0e6b35fa":"metrics","b16dd1a4":"region_total = vectors2\ntrain_size = int(len(region_total) * 0.7)\ntrain9 = region_total[0:train_size]\ntest_size = int(len(region_total) * 0.1) + train_size\ntest9 = region_total[train_size:test_size]\nval9 = region_total[test_size:len(region_total)]","f95d2a16":"def create_dataset(dataset, window_size = 1):\n    data_X, data_Y = [], []\n    for i in range(len(dataset) - window_size - 1):\n        a = dataset.values[i:i + window_size,:]\n        data_X.append(a)\n        data_Y.append(dataset.iloc[i + window_size,4])\n    X = np.array(data_X)\n    Y = np.array(data_Y)\n    Y[Y>1] = 1\n    return X,Y","40f22373":"def Metrics_LR_9(train_X, train_Y, test_X, test_Y, name, window_size):\n    lr_train_X, lr_train_Y = train_X, train_Y\n    lr_test_X, lr_test_Y = test_X, test_Y\n    \n    lr_train_X9 = np.reshape(train_X, (train_X.shape[0], train_X.shape[1] * 9))\n    lr_train_Y9 = train_Y\n    lr_test_X9 = np.reshape(test_X, (test_X.shape[0], test_X.shape[1] * 9))\n    lr_test_Y9 = test_Y\n\n    clf = LogisticRegression()\n    clf.fit(lr_train_X9, lr_train_Y9)\n\n    lr_y_pred = clf.predict(lr_test_X9)\n\n    acc = clf.score(lr_test_X9, lr_test_Y9)\n    print('Accuracy of logistic regression classifier on test set: {:.4f}'.format(acc))\n    precision = precision_score(lr_test_Y9, lr_y_pred)\n    print('Precision of logistic regression classifier on test set: {:.4f}'.format(precision))\n    recall = recall_score(lr_test_Y9, lr_y_pred)\n    print('Recall of logistic regression classifier on test set: {:.4f}'.format(recall))\n    roc = roc_auc_score(lr_test_Y9, lr_y_pred)\n    print('Roc of logistic regression classifier on test set: {:.4f}'.format(roc))\n    print('Training data shape:{}'.format(train_X.shape))\n    print('Testing data shape:{}'.format(test_X.shape))\n    metrics[name] = [window_size, train_X.shape, test_X.shape, acc, precision, recall, roc]","cecf399b":"def Metrics_NN_9(model, model_name, trainX, trainY, testX, testY, window_size):\n    y_pred = model.predict(testX).round()\n    score, acc = model.evaluate(testX, testY, batch_size=window_size)\n    print('score: ', score)\n    print('acc: ', acc)\n    precision = precision_score(testY, y_pred)\n    print('Precision: {:.4f}'.format(precision))\n    recall = recall_score(testY, y_pred)\n    print('Recall: {:.4f}'.format(recall))\n    roc = roc_auc_score(testY, y_pred)\n    print('ROC: {:.4f}'.format(roc))\n    print('Training data shape:{}'.format(trainX.shape))\n    print('Testing data shape:{}'.format(testX.shape))\n    metrics[model_name] = [model.count_params(), trainX.shape, testX.shape, acc, precision, recall, roc]","b529c8ee":"def Reshape_Tensor_9(data_train, data_test, data_val, isMlp = True):\n    \n    if isMlp:\n        data_train = np.reshape(data_train, (data_train.shape[0], data_train.shape[1] * data_train.shape[2]))\n        data_test = np.reshape(data_test, (data_test.shape[0], data_test.shape[1] * data_test.shape[2])) \n        data_val = np.reshape(data_val, (data_val.shape[0], data_val.shape[1] *  data_val.shape[2]))\n    else:        \n        data_train = np.reshape(data_train, (data_train.shape[0], 1, data_train.shape[1] * data_train.shape[2]))\n        data_test = np.reshape(data_test, (data_test.shape[0], 1, data_test.shape[1] * data_test.shape[2])) \n        data_val = np.reshape(data_val, (data_val.shape[0], 1, data_val.shape[1] *  data_val.shape[2]))\n    return data_train, data_test, data_val","c16b7b13":"def fit_model_mlp_9 (X_train, y_train, window_size = 1, X_val = None, y_val = None, val = False):\n    model = Sequential()\n    model.add(Dense(4, bias_initializer='ones', input_dim=window_size*9, activation='tanh', name = 'Dense6'))\n    model.add(Dense(1, bias_initializer = 'ones', activation='sigmoid', name = 'DenseOutput'))\n    model.compile(loss = \"binary_crossentropy\", \n                  optimizer = \"adam\", metrics=[\"accuracy\"])\n    \n    print(model.summary())\n    \n    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    \n    if val:\n            model.fit(X_train, \n              y_train, \n              epochs = 100, \n              batch_size = window_size, \n              verbose = 2,\n              validation_data=(X_val, y_val),\n              callbacks = [earlystopper])\n    else:\n        model.fit(X_train, \n                  y_train, \n                  epochs = 100, \n                  batch_size = window_size, \n                  verbose = 1,\n                  callbacks = [earlystopper])\n    \n    #print(model.summary())\n    return(model)","0864b88e":"def fit_model(X_train, y_train, window_size = 1, X_val = None, y_val = None, val = False):\n    model = Sequential()\n    \n#     model.add(LSTM(1,\n#                    bias_initializer='ones',\n#                    input_shape = (1, window_size * 9),\n#                    return_sequences=True,\n#                    name='InputLayer')\n#                    )\n    model.add(LSTM(units=4, return_sequences=True, bias_initializer='ones', input_shape = (1, window_size * 9),\n                   name = 'HiddenLayer1'))\n    model.add(Dropout(0.5))\n    model.add(LSTM(units=3, return_sequences=True, bias_initializer='ones', name = 'HiddenLayer2'))\n    model.add(Dropout(0.4))\n    model.add(LSTM(units=1, activation='softmax', bias_initializer = 'ones', name='OutputLayer'))\n    model.compile(loss = \"binary_crossentropy\", \n                  optimizer = \"adam\", metrics=[\"accuracy\"])\n    print(model.summary())\n    \n    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    if val:\n            model.fit(X_train, \n              y_train, \n              epochs = 200, \n              batch_size = window_size, \n              verbose = 2,\n              validation_data=(X_val, y_val),\n              callbacks=[earlystopper])\n    else:\n        model.fit(X_train, \n                  y_train, \n                  epochs = 200, \n                  batch_size = window_size, \n                  verbose = 1,\n                  callbacks=[earlystopper])\n    \n    return(model)","041350d3":"# Create test and training sets for one-step-ahead regression.\nwindow_size_sp = 1\notrain_X9, otrain_Y9 = create_dataset(train9, window_size_sp)\notest_X9, otest_Y9 = create_dataset(test9, window_size_sp)\noval_X9, oval_Y9 = create_dataset(val9, window_size_sp)","e91d7a2c":"Metrics_LR_9(otrain_X9, otrain_Y9, otest_X9, otest_Y9, \"lr_1_9\", window_size_sp)","7d3c8d25":"mtrain_X9, mtest_X9, mval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9)","fe4e134d":"mlp1_9 = fit_model_mlp_9(mtrain_X9, otrain_Y9, window_size_sp, mval_X9, oval_Y9, True)","771e2b39":"DrawGraphcs(mlp1_9)","0fd99c4c":"Metrics_NN_9(mlp1_9, \"mlp_1_9\", mtrain_X9, otrain_Y9,mtest_X9, otest_Y9, window_size_sp)","718e1d2d":"ltrain_X9, ltest_X9, lval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9, False)","ea182049":"lstm_1_9 = fit_model(ltrain_X9, otrain_Y9, window_size_sp, lval_X9, oval_Y9, True)","93a339ea":"DrawGraphcs(lstm_1_9)","4549f0b3":"Metrics_NN_9(lstm_1_9, \"lstm_1_9\", ltrain_X9, otrain_Y9, ltest_X9, otest_Y9, window_size_sp)","7758c131":"# Create test and training sets for one-step-ahead regression.\nwindow_size_sp = 2\notrain_X9, otrain_Y9 = create_dataset(train9, window_size_sp)\notest_X9, otest_Y9 = create_dataset(test9, window_size_sp)\noval_X9, oval_Y9 = create_dataset(val9, window_size_sp)","f0adc4e9":"Metrics_LR_9(otrain_X9, otrain_Y9, otest_X9, otest_Y9, \"lr_2_9\", window_size_sp)","60d8a8f0":"mtrain_X9, mtest_X9, mval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9)","65fc84bc":"mlp2_9 = fit_model_mlp_9(mtrain_X9, otrain_Y9, window_size_sp, mval_X9, oval_Y9, True)","e7a1b260":"DrawGraphcs(mlp2_9)","1e484b80":"Metrics_NN_9(mlp2_9, \"mlp_2_9\", mtrain_X9, otrain_Y9, mtest_X9, otest_Y9, window_size_sp)","bafa8dab":"ltrain_X9, ltest_X9, lval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9, False)","c3c8fc4d":"lstm_2_9 = fit_model(ltrain_X9, otrain_Y9, window_size_sp, lval_X9, oval_Y9, True)","f9073506":"DrawGraphcs(lstm_2_9)","ccb9b3ee":"Metrics_NN_9(lstm_2_9, \"lstm_2_9\", ltrain_X9, otrain_Y9, ltest_X9, otest_Y9, window_size_sp)","5307e464":"# Create test and training sets for one-step-ahead regression.\nwindow_size_sp = 3\notrain_X9, otrain_Y9 = create_dataset(train9, window_size_sp)\notest_X9, otest_Y9 = create_dataset(test9, window_size_sp)\noval_X9, oval_Y9 = create_dataset(val9, window_size_sp)","96b1cfb7":"Metrics_LR_9(otrain_X9, otrain_Y9, otest_X9, otest_Y9, \"lr_3_9\", window_size_sp)","3674933a":"mtrain_X9, mtest_X9, mval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9)","2bbb7017":"mlp3_9 = fit_model_mlp_9(mtrain_X9, otrain_Y9, window_size_sp, mval_X9, oval_Y9, True)","0bf81c67":"DrawGraphcs(mlp3_9)","f1da28fd":"Metrics_NN_9(mlp3_9, \"mlp_3_9\", mtrain_X9, otrain_Y9, mtest_X9, otest_Y9, window_size_sp)","89c98bac":"ltrain_X9, ltest_X9, lval_X9 = Reshape_Tensor_9(otrain_X9, otest_X9, oval_X9, False)","0a6ecb49":"lstm_3_9 = fit_model(ltrain_X9, otrain_Y9, window_size_sp, lval_X9, oval_Y9, True)","5cd7b9d9":"DrawGraphcs(lstm_3_9)","f0ce2422":"Metrics_NN_9(lstm_3_9, \"lstm_3_9\", ltrain_X9, otrain_Y9, ltest_X9, otest_Y9, window_size_sp)","b1d45c6a":"def LoadHistoryFromJSON(filename):\n    dic = dict()\n    with open(filename, \"r\") as json_file:\n        dic = json.load(json_file)\n    return dic","6d79e420":"model_names = [\"mlp_1\", \"lstm_1\",\"mlp_2\", \"lstm_2\",\"mlp_3\", \"lstm_3\", \"mlp_4\", \"lstm_4\",\"mlp_1_9\", \"lstm_1_9\",\"mlp_2_9\", \"lstm_2_9\",\"mlp_3_9\", \"lstm_3_9\"]\n\nhistories = {}\nfor model_name in model_names:\n    name = model_name+'_20190911_history.json'\n    histories[model_name] = LoadHistoryFromJSON(name)","ec530bcb":"data = pd.DataFrame.from_dict(metrics, orient='index')\ndata.columns = ['nparams', 'train_shape', 'test_shape','acc','precision','recall','roc_auc']\ndata['name'] = data.index.values\ndata","4bdae02a":"# data.to_csv('Metricas.csv')","4944d968":"data = pd.read_csv('Metricas.csv', index_col=0)","bac90c65":"def GraficoBarrasH(serie, title):\n    bar_colors = ['yellowgreen', 'olivedrab','darkolivegreen'] * 1000\n    ax = serie.plot(kind='barh', figsize=(12,6), color=bar_colors, fontsize=13, title=title)\n           \n    # set individual bar lables using above list\n    for i in ax.patches:\n        # get_width pulls left or right; get_y pushes up or down\n        ax.text(i.get_width()+.005, i.get_y()+0.5, \\\n              str(round((i.get_width()), 3)), fontsize=10, color='black')\n    \n    # invert for largest on top \n    ax.invert_yaxis()","60fd29bd":"GraficoBarrasH(data['acc'],'Accuracy')","2360fa10":"GraficoBarrasH(data['precision'], 'Precision')","d64f2e71":"GraficoBarrasH(data['recall'], 'Recall')","0db256f7":"GraficoBarrasH(data['roc_auc'], 'Roc')","a6f865b2":"def DrawValLossFromHistories(histories, representacion):\n    plt.figure(figsize=(20,8))\n    legend1, legend2, legend3, legend4 = [], [], [], []\n\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.3)\n    for key in histories.keys():\n        if \"_1\" in key:\n            plt.subplot(221)\n            plt.plot(histories[key][representacion])\n            legend1.append(key)\n            plt.legend(legend1)\n            plt.title('Modelos con ventana $\\it{m}$=1')\n\n        if \"_2\" in key:\n            plt.subplot(222)\n            plt.plot(histories[key][representacion])\n            legend2.append(key)\n            plt.legend(legend2)\n            plt.title('Modelos con ventana $\\it{m}$=2')\n\n        if \"_3\" in key:\n            plt.subplot(223)\n            plt.plot(histories[key][representacion])\n            legend3.append(key)\n            plt.legend(legend3)\n            plt.title('Modelos con ventana $\\it{m}$=3')\n\n        if \"_4\" in key:\n            plt.subplot(224)\n            plt.plot(histories[key][representacion])\n            legend4.append(key)\n            plt.legend(legend4)\n            plt.title('Modelos con ventana $\\it{m}$=4')\n\n        plt.xlabel('epochs')\n        plt.ylabel(representacion)\n        plt.grid(True)","0c5dc8c9":"DrawValLossFromHistories(histories, \"val_loss\")","7c497e89":"DrawValLossFromHistories(histories, \"val_acc\")","a1e6b13e":"### Estudio region central con ventana temporal de 2 quicena, incluyendo en el dataset las regiones adyacentes","4b1b5e83":"### Creamos un diccionario con clave nombre del modelo y valor las m\u00e9tricas del modelo","64f7b71f":"### Estudio region central con ventana temporal de 4 quincenas\n","d29f8520":"### Estudio region central con ventana temporal de 1 quincena","207b8897":"### Estudio region central con ventana temporal de 1 quicena, incluyendo en el dataset las regiones adyacentes","9f823b97":"## Carga de modelos e histories","5ab6d8a0":"### Estudio region central con ventana temporal de 2 quincenas","a590124e":"### Estudio region central con ventana temporal de 3 quicenas, incluyendo en el dataset las regiones adyacentes","80c068e3":"## Modelo espacio-temporal","d2101444":"## Comparativa de las m\u00e9tricas de todos los modelos","ead7e321":"## Comparativa de los modelos LSTM","f25dfbe3":"### Estudio region central con ventana temporal de 3 quincenas"}}