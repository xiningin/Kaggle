{"cell_type":{"5a0da341":"code","35430061":"code","1a714c57":"code","f3d7915a":"code","6d65e093":"code","92024109":"code","63c7c532":"code","b238cd1d":"code","3217a383":"code","ed95e6ab":"code","6c07f5d3":"code","3e2eeb7c":"code","fa59629f":"code","a5f59fb2":"code","13cc59f5":"code","b2b332f4":"code","d0bb95e9":"code","7cf2193c":"code","6d64863a":"code","cca3e456":"code","adcd20f2":"code","ecbb2f30":"code","ebc1ba67":"code","6dae2273":"code","0c4827dc":"markdown","3e3c7504":"markdown","d4b9f1b4":"markdown","0a6a0ca7":"markdown","be26b5ef":"markdown","11c3c3ff":"markdown","fd42425e":"markdown","89bb40d0":"markdown","1eb3df6b":"markdown","9a4b82ac":"markdown","614eda42":"markdown","140a6249":"markdown","d9a66ff9":"markdown","fd07e09f":"markdown"},"source":{"5a0da341":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35430061":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\n\nimport xgboost","1a714c57":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","f3d7915a":"train_cp = train.copy()\ncabin_letter = [x[0] for x in train_cp.dropna().Cabin.to_list()]\nPclass_clean = train_cp.dropna().Pclass.to_list()\n\nfig = go.Figure(\n    data = go.Bar(\n        x = cabin_letter,\n        y = Pclass_clean\n    )\n)\n\nfig.update_layout(\n    title = 'Cabin Class (1st letter) vs PClass. Each small bar represents a PClass (1, 2, or 3). <br>Some cabins appears exclusively to have a certain Pclass (e.g. G, F) while some are a mixture of different Pclass (e.g. D)'\n)","6d65e093":"# a table reprenstation of the graph above. From the table we can calculate probabilities of a certain Pclass being assigned to a cabin class. \ntrain_cp_notnull = train_cp.dropna()\ntrain_cp_notnull['cabin_letter'] = cabin_letter\ntrain_cp_notnull[['cabin_letter', 'Pclass', 'PassengerId']].dropna().groupby(['cabin_letter', 'Pclass']).agg(['count'])\n","92024109":"# we will use the probability to fill in the gaps\ncabin_class = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']\np_class = [1, 2, 3]\n\npclass1_cabin_prob = np.array([12\/158, 43\/158, 51\/158, 27\/158, 24\/158, 0, 0, 1\/158]) # these numbers come from the table above\npclass2_cabin_prob = np.array([0, 0, 0, 4\/15, 3\/15, 8\/15, 0, 0])\npclass3_cabin_prob = np.array([0, 0, 0, 0, 3\/10, 3\/10, 4\/10, 0])","63c7c532":"# fill cabin info in the train set using random draw with respectivie probabilities \nnp.random.seed(12345)\ncabin_class_lst = []\nfor idx, row in train.iterrows():\n    if not pd.isnull(row['Cabin']):\n        cabin_class_lst.append(row['Cabin'][0])\n    else:\n        p = row['Pclass']\n        if p == 1:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass1_cabin_prob))\n        elif p == 2:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass2_cabin_prob))\n        else:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass3_cabin_prob))\n\ntrain['cabin_class'] = cabin_class_lst\ntrain","b238cd1d":"# fill cabin info the test set using random draw with respectivie probabilities \nnp.random.seed(12345)\ncabin_class_lst = []\nfor idx, row in test.iterrows():\n    if not pd.isnull(row['Cabin']):\n        cabin_class_lst.append(row['Cabin'][0])\n    else:\n        p = row['Pclass']\n        if p == 1:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass1_cabin_prob))\n        elif p == 2:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass2_cabin_prob))\n        else:\n            cabin_class_lst.append(np.random.choice(cabin_class, p = pclass3_cabin_prob))\n\ntest['cabin_class'] = cabin_class_lst\ntest\n","3217a383":"X_train = train.drop(columns=['Name', 'Ticket', 'Survived', 'PassengerId', 'Cabin'])\nX_test = test.drop(columns=['Name', 'Ticket', 'PassengerId', 'Cabin',])\n\ny_train = train['Survived']","ed95e6ab":"X_train['total_people'] = X_train.SibSp + X_train.Parch + 1 # add self\nX_train['fare_per_person'] = X_train['Fare']\/X_train['total_people']\n\nX_test['total_people'] = X_test.SibSp + X_test.Parch + 1 # add self\nX_test['fare_per_person'] = X_test['Fare']\/X_test['total_people']","6c07f5d3":"num_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'total_people', 'fare_per_person']\ncat_cols = ['Sex', 'Embarked', 'cabin_class']\n\n# imputers\nmedian_imputer = SimpleImputer(strategy='median')\nfrequent_imputer = SimpleImputer(strategy='most_frequent')\n\n#Convert categoric values to one-hot\nonehot_encoder = OneHotEncoder()\n\n# scalers\nstd_scaler = StandardScaler()\n\n# pipeline\nnum_pip = Pipeline([\n    ('imputer', median_imputer),\n    ('scaler', std_scaler)\n])\ncat_pip = Pipeline([\n    ('imputer', frequent_imputer),\n    ('encoder', onehot_encoder)\n])\n\n# column trasnformer\ncol_transformer = ColumnTransformer([\n    ('num', num_pip, num_cols),\n    ('cat', cat_pip, cat_cols),\n])\n\n# transform X_train, X_test\nX_train_tf = col_transformer.fit_transform(X_train)\nX_test_tf = col_transformer.transform(X_test)\nprint(X_train_tf.shape, X_test_tf.shape)","3e2eeb7c":"X_train_tf_partial, X_valid, y_train_partial, y_valid = train_test_split(X_train_tf, y_train, shuffle=True)","fa59629f":"svc_grid_params = {\n    'kernel': ['rbf', 'linear'],\n    'gamma': [1.0, 10.0, 20],\n\n}\nsvc_cv = GridSearchCV(\n    SVC(),\n    param_grid=svc_grid_params,\n    cv = 5,\n    n_jobs=-1\n)\nsvc_cv.fit(X_train_tf_partial, y_train_partial)","a5f59fb2":"svc_model = svc_cv.best_estimator_\nprint('SVC Classifier CV:')\nprint(svc_cv.best_params_)\nprint(svc_cv.best_score_)\nprint(accuracy_score(y_valid, svc_model.predict(X_valid)))","13cc59f5":"xgboost_grid_params = {\n    'n_estimators':[500, 1000],\n    'gamma': [0.001, 0.005, 0.01, 0.1],\n    'learning_rate':[0.005, 0.01, 0.1],\n}\nxgboost_cv = GridSearchCV(\n    xgboost.XGBClassifier(),\n    param_grid=xgboost_grid_params,\n    cv = 5,\n    n_jobs=-1\n)\nxgboost_cv.fit(X_train_tf_partial, y_train_partial)","b2b332f4":"xgboost_model = xgboost_cv.best_estimator_\nprint('XGBoost Classifier CV:')\nprint(xgboost_cv.best_params_)\nprint(xgboost_cv.best_score_)","d0bb95e9":"log_model = LogisticRegression(C=10).fit(X_train_tf, y_train)\nprint(accuracy_score(y_valid, log_model.predict(X_valid)))","7cf2193c":"et_grid_params = {\n    'max_features': [10, 15, 20],\n    'n_estimators': [100, 300],\n    'min_samples_split':[0.01, 0.1, 0.5],\n    'min_samples_leaf':[1,3, 5,7, 10],\n\n}\net_cv = GridSearchCV(\n    ExtraTreesClassifier(),\n    param_grid=et_grid_params,\n    cv = 5,\n    n_jobs=-1\n)\net_cv.fit(X_train_tf_partial, y_train_partial)\n","6d64863a":"et_model = et_cv.best_estimator_\net_cv.best_params_","cca3e456":"# I manually tweaked some of these parameters to see which one produces a better score on validaiton set. \n# The best params provided by gridsearch isn't necessarily the set that works best on the validation\/test set. \n# if you use this model alone you might get around 0.78 accuracy on the test set. \net_model_best = ExtraTreesClassifier(\n    max_features= 20, \n    min_samples_leaf=3,\n    min_samples_split=0.01,\n    n_estimators=300,\n\n\n)\net_model_best.fit(X_train_tf_partial, y_train_partial)\naccuracy_score(y_valid, et_model_best.predict(X_valid))\n","adcd20f2":"tree_grid_params = {\n    'max_features': [10, 15, 20],\n    'min_samples_split':[0.01, 0.1, 0.5],\n    'min_samples_leaf':[1,3, 5,7, 10],\n\n}\ntree_cv = GridSearchCV(\n    DecisionTreeClassifier(),\n    param_grid=tree_grid_params,\n    cv = 5,\n    n_jobs=-1\n)\n\ntree_cv.fit(X_train_tf_partial, y_train_partial)","ecbb2f30":"tree_model = tree_cv.best_estimator_\naccuracy_score(y_valid, et_model_best.predict(X_valid))","ebc1ba67":"voting_clf = VotingClassifier(\n    estimators=[\n        ('log', log_model),\n        ('tree', tree_model),\n        ('svc', SVC(kernel='rbf',probability=True)), # linear and RBF both looks OK \n        ('sgboost', xgboost_model),\n        ('et', et_model_best)\n    ],\n    voting = 'soft', # soft voting\n    weights=[0.1, 0.1, 0.1,0.1, 0.6] # weights are determined by how they perform on the actual test set. \n\n)\nvoting_clf.fit(X_train_tf_partial, y_train_partial)","6dae2273":"accuracy_score(y_valid, voting_clf.predict(X_valid))","0c4827dc":"# Titanic with extra random forest and voting classifier\n\n1. Create some classifiers (SVC, random forest, extra random forest...) using GridSearch. Test set always scored somwhere around 0.77-0.78 at best.\n2. Extra random forest alone scores around 0.78 for test set\n3. I combined some of the classifiers into a voting classifer, with weights, and that gave score of ~ 0.79. \n4. A lot of these classifiers use random_state. If you don't set random_state, your scores will jump a bit, from around 0.78 - 0.79. You can see similar things happening to validation test. ","3e3c7504":"## Fill cabin info\nThis `Cabin` column has lots of missing data, but it seems to be somewhat related to the `PClass` columns. E.g. a ticket of certain class will be assigned to a certain cabin. Some cabins appear to be exclusively for upper class people, some are a mixture of the lower classes. The way I filled the missing data for `Cabin` is to extract the 1st letter of the cabin class, calculate proabibilty of a given PClass will be assigned to a Cabin class (only the 1st letter), then randomly fill out the missing columns with this probability","d4b9f1b4":"## Some new features related to family size and fare\/person","0a6a0ca7":"## Decision Tree (accuracy = 0.81 on validation set)","be26b5ef":"## Load data","11c3c3ff":"# Preprocessing","fd42425e":"## Define X_train, X_test, y_train","89bb40d0":"## Voting Classifier (accuracy = 0.83 on validation set)","1eb3df6b":"## Logistic (Accuracy = 0.80 on validation set)","9a4b82ac":"# Modeling","614eda42":"## Extreme random (accuracy = 0.81 on validation set)\nIf you use this model alone, you might get around 0.78 for the actual test set","140a6249":"## SVC (accuracy = 0.79 on validation set)","d9a66ff9":"## XGBoost (accuracy = 0.82 on validation set)","fd07e09f":"## Preprocessing pipeline"}}