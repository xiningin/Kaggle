{"cell_type":{"cfd61df9":"code","06b6bf30":"code","ec00e418":"code","091c9e53":"code","e03ba564":"code","cbc576d9":"code","946045ac":"code","2ce436d4":"code","d4e26059":"code","df98f97c":"code","06357c09":"code","84a42013":"code","065d5acf":"code","8aab7bb3":"code","80a9b211":"code","149320f5":"code","83cff3fa":"code","c5e34f6d":"code","c75c72a3":"code","86b200d3":"code","e7b704f8":"code","9bdf3cfa":"code","2858658e":"code","ea601bc2":"code","43e6abca":"code","7427beff":"code","6d62e606":"code","66c992e0":"code","81d83531":"code","baeb8519":"code","c119c876":"code","13a0cdfa":"code","ae73a238":"code","d85ce985":"markdown","91967dca":"markdown","9e10848f":"markdown","4711f3b6":"markdown","73bfb7d6":"markdown","6989f60e":"markdown","9c3fff8f":"markdown","ac76ebe3":"markdown","416c4e15":"markdown","5539352b":"markdown","a0f3bacc":"markdown","c8fbaa41":"markdown","1a0ff503":"markdown","72aab4f4":"markdown"},"source":{"cfd61df9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06b6bf30":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV","ec00e418":"# read in data\ndir = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\ntrain_df = pd.read_csv(dir + 'train.csv')\ntest_df = pd.read_csv(dir + 'test.csv')\ntrain_df.head()","091c9e53":"# save and drop the Id column from the data\ntrain_Id = train_df['Id']\ntest_Id = test_df['Id']\ntrain_df = train_df.drop('Id', axis=1)\ntest_df = test_df.drop('Id', axis=1)\ntrain_df.head()","e03ba564":"# plot SalePrice distribution\nsns.histplot(train_df, x='SalePrice', kde=True)\nplt.show()\nstats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","cbc576d9":"# correlation matrix\ncorr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nf, ax = plt.subplots(figsize=(13, 10))\nsns.heatmap(corr, cmap=cmap, mask=mask, center=0.00, square=True) ","946045ac":"cols = ['OverallQual','GrLivArea','GarageCars','GarageArea','1stFlrSF','TotalBsmtSF','TotRmsAbvGrd','YearBuilt','GarageYrBlt','SalePrice']\nmask = np.triu(np.ones_like(corr.loc[cols,cols], dtype=bool))\nsns.heatmap(corr.loc[cols,cols], cmap=cmap, center=0.00, mask=mask, square=True, annot=True, fmt='.2f', annot_kws={'size':9.5})","2ce436d4":"# plot SalePrice against GrLivArea            (maybe develop a better way of detecting outliers)\nfig, ax = plt.subplots()\nax.scatter(train_df['GrLivArea'], train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","d4e26059":"# concatenate train and test set\nall_data = pd.concat([train_df.drop('SalePrice', axis=1), test_df], ignore_index=True)","df98f97c":"# replace all NAs in features where NA is a category with 'None' (this assumes none of the NAs actually represent a missing value but we'll correct this next)\ncols = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\nall_data[cols] = all_data[cols].fillna('None')\n# examine missing data\ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.loc[missing_data['Total'] > 0]","06357c09":"# check for contradictions in categorical Garage variables\nrows = ((all_data['GarageType'] == 'None') | (all_data['GarageFinish'] == 'None') | (all_data['GarageQual'] == 'None') | (all_data['GarageCond'] == 'None')) \\\n    & ~((all_data['GarageType'] == all_data['GarageFinish']) & (all_data['GarageFinish'] == all_data['GarageQual']) & (all_data['GarageQual'] == all_data['GarageCond']))\nall_data.loc[rows, 'GarageType':'GarageCond']","84a42013":"# impute missing data for 1st entry\nall_data.loc[2126,'GarageFinish'] = all_data['GarageFinish'].value_counts().index[0]\nall_data.loc[2126,'GarageQual'] = all_data['GarageQual'].value_counts().index[0]\nall_data.loc[2126,'GarageCond'] = all_data['GarageCond'].value_counts().index[0]\n# impute missing data for 2nd entry\nall_data.loc[2576,'GarageType'] = 'None'\nall_data.loc[2576,'GarageCars'] = 0\nall_data.loc[2576,'GarageArea'] = 0\n# impute missing values for GarageYrBlt\nall_data.loc[all_data['GarageType'] == 'None', 'GarageYrBlt'] = 0\nall_data.loc[all_data['GarageType'] != 'None', 'GarageYrBlt'] = all_data.loc[all_data['GarageType'] != 'None', 'YearBuilt']","065d5acf":"# check for contradictions in categorical Bsmt variables\nrows = ((all_data['BsmtQual'] == 'None') | (all_data['BsmtCond'] == 'None') | (all_data['BsmtExposure'] == 'None') | (all_data['BsmtFinType1'] == 'None') | (all_data['BsmtFinType2'] == 'None')) \\\n    & ~((all_data['BsmtQual'] == all_data['BsmtCond']) & (all_data['BsmtCond'] == all_data['BsmtExposure']) & (all_data['BsmtExposure'] == all_data['BsmtFinType1']) & (all_data['BsmtFinType1'] == all_data['BsmtFinType2']))\nall_data.loc[rows, 'BsmtQual':'BsmtFinSF2']","8aab7bb3":"# impute missing values\nall_data.loc[332,'BsmtFinType2'] = all_data['BsmtFinType2'].value_counts().index[0]\nall_data.loc[[948,1487,2348],'BsmtExposure'] = all_data['BsmtExposure'].value_counts().index[0]\nall_data.loc[[2040,2185,2524],'BsmtCond'] = all_data['BsmtCond'].value_counts().index[0]\nall_data.loc[[2217,2218],'BsmtQual'] = all_data['BsmtQual'].value_counts().index[0]","80a9b211":"# examine missing entries for BsmtHalfBath\nall_data.loc[all_data[all_data['BsmtHalfBath'].isnull()].index.tolist(),['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','BsmtQual']]","149320f5":"# impute missing data\ncols = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\nall_data[cols] = all_data[cols].fillna(0)","83cff3fa":"# examine missing entries for MasVnrType\nall_data.loc[all_data[all_data['MasVnrType'].isnull()].index.tolist(),'MasVnrType':'MasVnrArea']","c5e34f6d":"# impute missing data\nall_data.loc[2610,'MasVnrType'] = all_data['MasVnrType'].value_counts().index[1]\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')","c75c72a3":"# examine entries for MSZoning, Electrical, Functional and SaleType\nprint(all_data['MSZoning'].value_counts())\nprint('\\n')\nprint(all_data['Electrical'].value_counts())\nprint('\\n')\nprint(all_data['Functional'].value_counts())\nprint('\\n')\nprint(all_data['SaleType'].value_counts())","86b200d3":"# impute missing data\nall_data['MSZoning'] = all_data['MSZoning'].fillna('RL')\nall_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\nall_data['SaleType'] = all_data['SaleType'].fillna('WD')","e7b704f8":"# examine entries for Utilities\nprint('Train set:')\nprint(train_df['Utilities'].value_counts())\nprint('Number of missing values:')\nprint(sum(train_df['Utilities'].isnull()))\nprint('\\nTest set:')\nprint(test_df['Utilities'].value_counts())\nprint('Number of missing values:')\nprint(sum(test_df['Utilities'].isnull()))","9bdf3cfa":"# drop Utilities\nall_data = all_data.drop(['Utilities'], axis=1)","2858658e":"# impute missing data\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","ea601bc2":"# examine missing data\ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.loc[missing_data['Total'] > 0]","43e6abca":"# impute missing data\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].value_counts().index[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].value_counts().index[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].value_counts().index[0])","7427beff":"# transform MSSubClass\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)","6d62e606":"# plot SalePrice against YrSold\nsns.boxplot(x='YrSold',y='SalePrice',data=train_df)","66c992e0":"# transform YrSold\nall_data['YrSold'] = all_data['YrSold'].astype(str)","81d83531":"# feature engineering\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['GrLivArea']\nall_data['TotalBath'] = all_data['FullBath'] + all_data['BsmtFullBath'] + 0.5 * (all_data['BsmtHalfBath'] + all_data['HalfBath'])\n# transform the cyclical feature MoSold (http:\/\/blog.davidkaleko.com\/feature-engineering-cyclical-features.html)\n# note: not sure if this method is appropriate for a random forest model since only one of the two transformed features will be present for each split decision. \n# but it improves the accuracy so...\nall_data['SinMoSold'] = np.sin(2 * np.pi * all_data['MoSold'] \/ 12)\nall_data['CosMoSold'] = np.cos(2 * np.pi * all_data['MoSold'] \/ 12)\nall_data = all_data.drop('MoSold', axis=1)\n# convert categorical variables to dummy variables\nall_data = pd.get_dummies(all_data)","baeb8519":"# feature importance\nntrain = train_df.shape[0]\nX = all_data[:ntrain]\ny = train_df['SalePrice']\nrf = RandomForestRegressor(random_state=42, n_estimators=360).fit(X, y)\nfeature_importances = pd.DataFrame(rf.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)\nfeature_importances['cumulative_importance'] = feature_importances['importance'].cumsum()\nfeature_importances.head(20)","c119c876":"# drop 218 least important features (this number was found using a semi-exhaustive search, this whole feature selection process needs improving). \ncols = feature_importances.tail(218).index.values.tolist()\nall_data = all_data.drop(cols, axis=1)","13a0cdfa":"# 5-fold cross-validation\nX = all_data[:ntrain]\nrf = RandomForestRegressor(random_state=42, n_estimators=360).fit(X, y)\nscores = -1 * cross_val_score(rf, X, y, cv=5, scoring='neg_root_mean_squared_error')\nprint(\"Average RMSE score (across experiments):\")\nprint(scores.mean())\n# 28698.17 dropping 218 features","ae73a238":"# create submission             \nrf = RandomForestRegressor(random_state=42, n_estimators=360).fit(X, y)\ntest_df = all_data[ntrain:]\nrf.fit(X,y)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_Id\nsubmission['SalePrice'] = rf.predict(test_df)\nsubmission.to_csv('submission.csv',index=False)","d85ce985":"We can consider keeping YearBuilt and dropping GarageYrBlt as the former is more correlated with SalePrice. Using the same logic, we might also consider dropping TotRmsAbvGrd and GarageArea. 1stFlrSF and TotalBsmtSF have the same correlation with SalePrice so we could consider combining them to create a new feature.","91967dca":"Utilities in the test set consists entirely of 'AllPub' and has 2 missing values. Imputing 'AllPub' for these missing values would mean all houses in the test set have 'AllPub' making the variable unnecessary for prediction, so we can drop it. ","9e10848f":"The target variable SalePrice does not satisfy normality, it is right skewed. If we were looking to fit a linear regression model we might choose to log transform the SalePrice, but with a Random Forest model this will not affect the accuracy. ","4711f3b6":"All these variables have a category which occurs much more frequently than the others, so we'll replace the missing values with those. ","73bfb7d6":"Clearly, all the 'None' values in each entry actually represent a missing value rather than 'No Basement' since all other Bsmt variables imply there is a basement. We'll impute the missing values with the mode. ","6989f60e":"All houses with missing entries for MasVnrType also have missing entries for MasVnrArea and vice-versa, with one exception. If a house has a masonry veneer area, it must have a masonry veneer type, so for this particular entry, we'll set the type as the mode (excluding 'None'). We'll assume all the other missing entries have no masonry veneer and thus 0 MasVnrArea.","9c3fff8f":"For the 1st entry, there are values for GarageType, GarageCars and GarageArea so we'll assume the other Garage variables with 'None' actually mean missing rather than 'No Garage'. We'll impute them with the mode. The 2nd entry happens to be responsible for the single missing entry in GarageArea and GarageCars. The GarageType is 'detached', but all other Garage variables imply the house has no garage. We'll assume the GarageType is erroneous and that there is no garage, imputing the missing values accordingly. We can now also set all missing values for GarageYrBlt to be 0 if there is no garage, and if there is a garage, we'll assume it's equal to the YearBuilt. ","ac76ebe3":"The remaining variables which have missing values are all categorical and only have 1 NA so we'll just set those as the mode. ","416c4e15":"These two houses are responsible for all the missing entries in the numeric Bsmt variables. Since both houses have no basement as BsmtQual = 'None', we can assume all the missing values are 0. ","5539352b":"We'll assume that houses in the same neighbourhood will have similar lot frontage and impute the missing LotFrontage values using the median LotFrontage of the neighbourhood. We could improve this by training a separate model on our data to predict the missing LotFrontage values","a0f3bacc":"Looking at the data description, the numbers in MSSubClass don't seem to represent any sort of ordering so we'll treat it as categorical rather than numeric. ","c8fbaa41":"There doesn't really appear to be any trend so we'll treat YrSold as categorical rather than numeric. ","1a0ff503":"There appear to be two outliers at the bottom right.","72aab4f4":"OverallQual and GrLivArea are highly correlated with SalePrice.\nPossible evidence of multicollinearity: GarageYrBlt & YearBuilt, 1stFlrSF & TotalBsmtSF, TotRmsAbvGrd & GrLivArea, GarageArea & GarageCars."}}