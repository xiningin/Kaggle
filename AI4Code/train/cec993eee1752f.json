{"cell_type":{"25d1ea1b":"code","8f7809c9":"code","ad122381":"code","b2adc59f":"code","6f663019":"code","2e78a279":"code","35056359":"code","0133cc4e":"code","3fc2f366":"code","2ef176a1":"code","196484ff":"code","b9dd0116":"code","77f58415":"code","2186ae10":"code","cd1b5a8c":"code","7f49743b":"code","327cebc6":"code","a4d1402f":"code","79e60a40":"code","c601bf9c":"code","e5bea235":"code","f8b55a1d":"code","2e186438":"code","c646aa20":"code","73fd1167":"code","c24b4cce":"code","25619073":"code","4af99238":"code","39ff7685":"code","062edf26":"code","632d44d1":"code","24275908":"code","0a761e14":"code","99df82f4":"code","69c63619":"code","66ba8254":"code","fd81281d":"code","0288e24b":"code","ef2817fa":"code","b695bde4":"markdown","e9dc3825":"markdown","b5129ed3":"markdown","54f79af6":"markdown","1490c4ac":"markdown","3ad7bf76":"markdown","3a970613":"markdown","5454306a":"markdown","455e6c26":"markdown","d484c15b":"markdown","a03b7cfe":"markdown","8224434e":"markdown","cbf3e770":"markdown","73594d5c":"markdown","9b8b182b":"markdown","f30c3d7e":"markdown","96485efb":"markdown","3e611846":"markdown","924d4d4a":"markdown","f521c49f":"markdown","096825fa":"markdown","abd91af3":"markdown","fb67eea4":"markdown","9373a371":"markdown","c60f527b":"markdown","07443f43":"markdown","d3ae05aa":"markdown","a40a2bc8":"markdown","557d4cbd":"markdown","dc7bc34c":"markdown","a772125f":"markdown","9ff698cb":"markdown","fb2238ef":"markdown","5952815e":"markdown","a56f6103":"markdown","cc3912ee":"markdown","471bef8e":"markdown","e98dd384":"markdown"},"source":{"25d1ea1b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport collections\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectPercentile , chi2\nfrom sklearn.naive_bayes import MultinomialNB","8f7809c9":"data = pd.read_csv('..\/input\/spooky-author-identification\/train\/train.csv')  \nprint(f'Data Shape is {data.shape}')\ndata.head()","ad122381":"def show_details() : \n    global data\n    for col in data.columns : \n        print(f'for feature : {col}')\n        print(f'Number of Nulls is   {data[col].isna().sum()}')\n        print(f'Number of Unique values is   {len(data[col].unique())}')\n        print(f'random Value {data[col][0]}')\n        print(f'random Value {data[col][10]}')\n        print(f'random Value {data[col][20]}')\n        print('--------------------------')\n\ndef CountWords(text) :  \n    \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n\n    print(f'Total words are {len(all_words)} words')   \n    print('')\n    print(f'Total unique words are {len(set(all_words))} words')   \n    \ndef CommonWords(text ,show = True , kk=10) : \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n    common_words = collections.Counter(all_words).most_common()\n    k=0\n    word_list =[]\n    for word, i in common_words : \n        if not word.lower() in  nlp.Defaults.stop_words :\n            if show : \n                print(f'The word is   {word}   repeated   {i}  times')\n            word_list.append(word)\n            k+=1\n        if k==kk : \n            break\n            \n    return word_list\n\ndef SelectedData(feature , value , operation, selected_feature ):\n    global data\n    if operation==0 : \n        result = data[data[feature]==value][selected_feature]\n    elif operation==1 : \n        result = data[data[feature] > value][selected_feature]\n    elif operation==2 : \n        result = data[data[feature]< value][selected_feature]\n    \n    return result \n\ndef LowerCase(feature , newfeature) : \n    global data\n    def ApplyLower(text) : \n        return text.lower()\n    data[newfeature] = data[feature].apply(ApplyLower)\n    \ndef Drop(feature) :\n    global data\n    data.drop([feature],axis=1, inplace=True)\n    data.head()\ndef Unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')\n    \ndef Encoder(feature , new_feature, drop = False) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n        \ndef MakeCloud(text , title = 'Word Clouds' , w = 15 , h = 15):\n    plt.figure(figsize=(w,h))\n    plt.imshow(WordCloud(background_color=\"white\",stopwords=set(stopwords.words('english')))\n               .generate(\" \".join([i for i in text.str.lower()])))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\ndef BPlot(feature_1,feature_2) :\n    global data\n    sns.barplot(x=feature_1, y=feature_2 , data=data)\n    \ndef CPlot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n","b2adc59f":"Drop('id')","6f663019":"LowerCase('text' , 'lower text')\nDrop('text')","2e78a279":"data.head(20)","35056359":"show_details()","0133cc4e":"Unique('author')","3fc2f366":"Encoder('author' , 'author code')\ndata.head()","2ef176a1":"CountWords(data['lower text'])","196484ff":"BPlot(data['author'].value_counts().index , data['author'].value_counts().values )","b9dd0116":"AllCommon = CommonWords(data['lower text'])","77f58415":"MakeCloud(data['lower text'] , 'All Words')","2186ae10":"ECommon = CommonWords(SelectedData('author','EAP',0,'lower text'))","cd1b5a8c":"MakeCloud(SelectedData('author','EAP',0,'lower text') , 'EAP Words')","7f49743b":"HCommon = CommonWords(SelectedData('author','HPL',0,'lower text'))","327cebc6":"MakeCloud(SelectedData('author','HPL',0,'lower text') , 'HPL Words')","a4d1402f":"MCommon = CommonWords(SelectedData('author','MWS',0,'lower text'))","79e60a40":"MakeCloud(SelectedData('author','MWS',0,'lower text') , 'MWS Words')","c601bf9c":"data['number of words'] = data['lower text'].apply(lambda x : len(x.split()))\nprint('mean words for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of words').mean())  \nprint('mean words for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of words').mean())  \nprint('mean words for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of words').mean())  ","e5bea235":"data['number of chars'] = data['lower text'].apply(lambda x : len(x))\nprint('mean chars for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of chars').mean())  \nprint('mean chars for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of chars').mean())  \nprint('mean chars for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of chars').mean())  ","f8b55a1d":"data['number of punctuations'] = data['lower text'].apply(lambda x : len([k for k in  x if k in r'.,;:!?|\\#$%^&*\/']))\nprint('mean punctuations for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of punctuations').mean())  \nprint('mean punctuations for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of punctuations').mean())  \nprint('mean punctuations for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of punctuations').mean())  ","2e186438":"data['number of stop'] = data['lower text'].apply(lambda x : len([k for k in  x if k in nlp.Defaults.stop_words ]))\nprint('mean stop for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of stop').mean())  \nprint('mean stop for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of stop').mean())  \nprint('mean stop for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of stop').mean())  ","c646aa20":"data.head()","73fd1167":"X = data['lower text']\ny = data['author code']","c24b4cce":"VecModel = TfidfVectorizer()\nXVec = VecModel.fit_transform(X)\n\nprint(f'The new shape for X is {XVec.shape}')","25619073":"FeatureSelection = SelectPercentile(score_func = chi2, percentile=50)\nX_data = FeatureSelection.fit_transform(XVec, y)\n\nprint('X Shape is ' , X_data.shape)","4af99238":"X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.33, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","39ff7685":"MultinomialNBModel = MultinomialNB(alpha=0.05)\nMultinomialNBModel.fit(X_train, y_train)\n\nprint('MultinomialNBModel Train Score is : ' , MultinomialNBModel.score(X_train, y_train))\nprint('MultinomialNBModel Test Score is : ' , MultinomialNBModel.score(X_test, y_test))","062edf26":"data = pd.read_csv('..\/input\/spooky-author-identification\/test\/test.csv')  \nprint(f'Test data Shape is {data.shape}')\ndata.head()","632d44d1":"LowerCase('text' , 'lower text')\nDrop('text')\ndata.head()","24275908":"X = data['lower text']","0a761e14":"XVec = VecModel.transform(X)\nprint(f'The new shape for X is {XVec.shape}')","99df82f4":"X_data = FeatureSelection.transform(XVec)\nprint('X Shape is ' , X_data.shape)","69c63619":"y_pred = MultinomialNBModel.predict(X_data)\ny_pred_prob = MultinomialNBModel.predict_proba(X_data)\nprint('Predicted Value for MultinomialNBModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for MultinomialNBModel is : ' , y_pred_prob[:10])","66ba8254":"data = pd.read_csv('..\/input\/spooky-author-identification\/sample_submission\/sample_submission.csv')  \nprint(f'Test data Shape is {data.shape}')\ndata.head()","fd81281d":"idd = data['id']\nFinalResults = pd.DataFrame(y_pred_prob  ,columns= ['EAP','HPL','MWS'])\nFinalResults.insert(0,'id',idd)","0288e24b":"FinalResults.head()","ef2817fa":"FinalResults.to_csv(\"sample_submission.csv\",index=False)","b695bde4":"then to lowercase all texts , & to drop original text feature","e9dc3825":"hope you enjoyed it !\n","b5129ed3":"HPL prefer to write more letters , but still close to each other \n\nso how about number of punctuations ? ","54f79af6":"here they don't need the argmax prediction , but the probabilities , so we'll use predict probability method from same model , then transform it into dataframe , & concatenate to the original id feature","1490c4ac":"now to apply Vectorizing Model to it , & it have bring the same 25K features","3ad7bf76":"great , now to open the submission file , to insert the answer","3a970613":"let's be sure no Nulls exists","5454306a":"about 25K features . . \n\nhow about reducing them using SelectPercentile from sklearn , to its half , using chi2 function","455e6c26":"then to read the training data","d484c15b":"also we can make it in cloud form ","a03b7cfe":"we have to lower case it as well ","8224434e":"ok , the 3 means are very close to each other \n\nhow about the number of charachters","cbf3e770":"# Spooky Classification for 3 Authors\nBy : Hesham Asem\n\n_____\n\nhere we have 3 kinds of phrases , for 3 fammous authors (Edgar Allan Poe  , HP Lovecraft , Mary Wollstonecraft Shelley) , & we need to build a model which is able to know the author depend on the phrase \n\nlet's first import needed libraries","73594d5c":"then define X","9b8b182b":"now it's ready for predicting","f30c3d7e":"then repeat same process for HPL","96485efb":"how about common words in phrases written by EAP ? ","3e611846":"let's define X & y , since we'll not use any of those new features ","924d4d4a":"perfect  . again we have to apply SelectPercentile Model to it , to select same half features","f521c49f":"great, now submission ","096825fa":"great , balanced amount\n\n_____\n\n# Common Words\n\n\nlet's have a look to most common used words ( stopwords will be excluded automatically from this function)","abd91af3":"who are authors we have here ? ","fb67eea4":"____\n\n# Text Processing\n\nso let's start make basic processes in our Text here \n\nfirst to drop the feature id","9373a371":"well , 87% accuracy in test score is not the best but it's acceptable , may be we can increase it by more tuning for the parameters\n\n_____\n\n# Predicting Test Data\n\nnow let's moving to predicting test data","c60f527b":"now to define needed functions","07443f43":"then we'll have to vectorize the text & check its shape","d3ae05aa":"& it will be better to make wordcloud for all words","a40a2bc8":"still not a big difference . so how about number of stop words ? ","557d4cbd":"about half million word , which depend on about 45K unique words\n\n____\n\n\nand we need to be sure that we have balances about of phrases for each category in the output\n\n","dc7bc34c":"ok looks fine , not to split it into training & testing data","a772125f":"almost equal each other . . \n\nso it might not be helpful to use any of those features\n\n_____\n\n# Training the Data\n\nso let's start prepare our data to be ready for training ","9ff698cb":"how it looks now ? ","fb2238ef":"now to use MultiNomial Naive Bayes model for classification , with alpha only 0.05 to get the best score","5952815e":"now how data looks like","a56f6103":"____\n\n# More Data\n\nwe might need to know more features about these phrases , which might be helpful in our training\n\nlet's make a new feature about number of words in each phrase , & check if it will be a helpful feature","cc3912ee":"so we need to labelencode our authors here , so EAP will be 0 , HPL will be 1 & MWS will be 2 , we need to memorize these numbers to use in predicting test data","471bef8e":"now how many words we have in all phrases  ? ","e98dd384":"and for MWS"}}