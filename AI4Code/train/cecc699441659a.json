{"cell_type":{"855e6f7b":"code","7be1356a":"code","12cd543f":"code","1a1e32d3":"code","a4d58e0d":"code","cd032daa":"code","e46d8331":"code","1ade147e":"code","95a45138":"code","957ab243":"code","c59c8433":"code","0bfe26d4":"code","7394df78":"code","4f9d7eb6":"code","6e3694a4":"code","77e118e0":"code","1e7221a2":"code","0b41a4a9":"code","7c94eee6":"code","3f64c5aa":"code","a983788f":"code","842862d9":"code","ba1d28a6":"code","de49b005":"code","bad6e2f0":"code","baa4aada":"code","68c682cd":"code","fa75baa3":"code","5bf51262":"code","1db7077e":"code","bbee05cc":"code","17177401":"code","3ce380e4":"code","c6530401":"code","6d63b5e1":"code","636a7d5d":"code","6c06fa69":"code","7829afcc":"code","04f17e46":"code","8c779fd2":"code","8b2be446":"code","6f0b3760":"code","b742e554":"code","6497135a":"code","6d8301f1":"code","d50dc73f":"code","1b20bccd":"markdown","5c4ce705":"markdown","c5f13f8b":"markdown","74181da3":"markdown","1757ae44":"markdown","f0a2d8e6":"markdown","94e9cf6c":"markdown","2422cc1c":"markdown","156da29f":"markdown"},"source":{"855e6f7b":"import os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm","7be1356a":"torch.cuda.empty_cache()\nif torch.cuda.is_available():\n    print(\"GPU available\")\n    device = \"cuda:0\"\nelse:\n    print(\"CPU available\")\n    device = \"cpu\"\n    \ndevice = torch.device(device)\nprint(device)\n\n\n","12cd543f":"pwd","1a1e32d3":"data_dir = os.path.join(\"\/kaggle\",\"input\",\"cityscapes-image-pairs\",\"cityscapes_data\")\ntrain_dir= os.path.join(data_dir,\"train\")\nval_dir  = os.path.join(data_dir,\"val\")\ntrain_lst= os.listdir(train_dir)\nval_lst  = os.listdir(val_dir)","a4d58e0d":"print(len(train_lst),len(val_lst))\nprint(train_lst[0])","cd032daa":"sample_image_os = os.path.join(train_dir, train_lst[0]) \nprint(type(sample_image_os)) # string \nprint(sample_image_os)# total path\n","e46d8331":"\n\n\nsample_image = Image.open(sample_image_os)\nplt.imshow(sample_image)","1ade147e":"def split_image(image):\n    img_np = np.array(image)\n    cityscape = img_np[:, :256, :]\n    label     = img_np[:, 256: ,:]\n    return cityscape, label","95a45138":"sample_image_np = np.array(sample_image)\nprint(sample_image_np.shape)\ncityscape, label = split_image(sample_image_np)\ncityscape_PIL = Image.fromarray(cityscape)\nlabel_PIL = Image.fromarray(label)\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(cityscape_PIL)\naxes[1].imshow(label_PIL)","957ab243":"sample_image_torch = torch.tensor(sample_image_np)\nprint(sample_image_torch.shape)\nprint(sample_image_np.shape)\nplt.imshow(sample_image_torch)","c59c8433":"num_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3) # 1000 * 3 \nprint(color_array.shape)\n# jusat for clarification\nprint(color_array[:5, :])\nprint(color_array.min())\nprint(color_array.max())","0bfe26d4":"# clarrification\nnum_classes = 10\nlabel_model2 = KMeans(n_clusters=num_classes)\nlabel_model2.fit(color_array)# 2D array\nprint(type(label_model2))","7394df78":"label_model2.labels_\n# 1D array","4f9d7eb6":"len(label_model2.labels_)","6e3694a4":"label_model2.predict(color_array[:7])","77e118e0":"\"\"\"\"\ncityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)\nprint(label.shape)\nprint(label_class.shape)\nprint(label_class[0][0])\nprint(label_class[5][150])\n\"\"\"","1e7221a2":"num_classes = 10\nlabel_model = KMeans(n_clusters=num_classes)\nlabel_model.fit(label.reshape(-1,3))# 2D array","0b41a4a9":"cityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)\nprint(label.shape)\nprint(label_class.shape)\nprint(label_class[0][0])\nprint(label_class[5][150])\nlabel_class2 = torch.tensor(label_class)\nprint(type(label_class2))\nprint(label_class[5][150])\nprint(type(torch.max(label_class2)))\nlabel_class2 = label_class2.long()\nprint(label_class[5][150])\nprint(type(torch.max(label_class2)))","7c94eee6":"class Cityscape_dataset(Dataset):\n    def __init__(self, image_dir, label_model):\n        self.image_dir = image_dir\n        self.image_lst = os.listdir(image_dir)\n        self.label_model = label_model\n    \n    def __len__(self):\n        return len(self.image_lst)\n    \n    def __getitem__(self, idx):\n        image_name       = self.image_lst[idx]\n        image_path       = os.path.join(self.image_dir, image_name)\n        image_colored    = Image.open(image_path).convert('RGB')    \n        cityscape, label = split_image(image_colored)\n        label_class      = self.label_model.predict(label.reshape(-1,3)).reshape(256,256)\n        cityscape        = self.transform(cityscape)\n        label_class      = torch.tensor(label_class).long()\n        return cityscape, label_class\n    \n    def split_image(self, img):\n        image = np.array(image)\n        cityscape, label = image[:, :256, :], image[:, 256:, :]\n        return cityscape, label\n    \n    def transform(self, img):\n        Transform = transforms.Compose([\n            transforms.ToTensor(),\n             transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return Transform(img)","3f64c5aa":"dataset = Cityscape_dataset(train_dir, label_model)\n","a983788f":"class DoubleConv(nn.Module):\n  def __init__(self, in_c, out_c):\n    super().__init__() # call the constractor of the inherted class to be implemnted in the child class\n    self.conv = nn.Sequential(\n        # The First Conv\n        nn.Conv2d(in_c, out_c, 3, 1, 1, bias = False), # bias = False cause we will use BatchNorm next\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace = True),\n        # The Second Conv \n        nn.Conv2d(out_c, out_c, 3, 1, 1, bias = False), # bias = False cause we will use BatchNorm next\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace = True)\n    )\n  \n  def forward(self, x_img):\n    return self.conv(x_img)\n\n","842862d9":"num_of_classes = 10\nclass UNET(nn.Module):\n  def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512],):\n    super().__init__()\n    self.ups  = nn.ModuleList() # Holds submodules in a list\n    self.downs= nn.ModuleList()\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    \n    # down part of UNET\n    for feature in features:\n      self.downs.append(DoubleConv(in_channels, feature))\n      in_channels = feature\n\n    # Up part of UNET\n    # double the height and width of the image\n    for feature in reversed(features):\n      self.ups.append(\n          nn.ConvTranspose2d(\n              feature*2,\n              feature,\n              kernel_size=2,\n              stride =2\n          )\n      )\n      self.ups.append(\n          DoubleConv(\n              feature*2,\n              feature\n          )\n      )\n      # End Of Loop\n      \n   #bottleneck of UNET\n    self.bottleneck = DoubleConv(\n        features[-1],\n        features[-1]*2\n        )\n   \n  \n  #Final Conv (last layer)\n    self.final_conv = nn.Conv2d(\n        features[0],\n        out_channels,\n        kernel_size = 1\n    )\n\n  # Forward\n  def forward(self,x_img):\n    y = x_img\n    #Skip Connection\n    skip_connections = []\n    # Down\n    for down in self.downs:\n      x_img = down(x_img)\n      skip_connections.append(x_img)\n      x_img = self.pool(x_img)\n    \n    x_img = self.bottleneck(x_img)\n    # reverse skip connection to be from down to up\n    skip_connections = skip_connections[::-1]\n    # Up\n    for idx in range(0, len(self.ups), 2): # Up Then Double Conv\n      x_img = self.ups[idx](x_img)\n      skip_connection = skip_connections[idx \/\/ 2]\n      # Crop to match size \n      # x_img has to be greater than or equal to skip_connection because of flooring that happend during maxpool if W,h are not even in any level\n      # print(x_img.shape)\n      # print(skip_connection.shape)\n      if x_img.shape != skip_connection.shape:\n        # skip_connection = TF.resize(skip_connection, size=x_img.shape[2:]) # H and W\n           x_img = TF.resize(x_img, size=skip_connection.shape[2:])\n      concat_skip = torch.cat((skip_connection, x_img), dim = 1) # torch tensor [BatchSize, Channels, H, W], so it will add them across channels\n      #Double Conv\n      x_img = self.ups[idx+1](concat_skip)\n\n    #Final Conv, (1*1) convolution\n    return self.final_conv(x_img)","ba1d28a6":"batch_size = 16\nepochs = 30\nlr = 0.001","de49b005":"dataset = Cityscape_dataset(train_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=batch_size)\n\nmodel = UNET(in_channels=3, out_channels=10).to(device)\nprint(model)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","bad6e2f0":"step_losses = []\nepoch_losses = []","baa4aada":"\n\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    print(epoch_loss)\n    epoch_losses.append(epoch_loss\/len(data_loader))","68c682cd":"# version one\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","fa75baa3":"# version 2\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","5bf51262":"# version 3\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","1db7077e":"# version 4\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","bbee05cc":"# version 5\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","17177401":"# version 6\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","3ce380e4":"# version 7\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","c6530401":"model_name = \"U-Net.pth7\"\ntorch.save(model.state_dict(), model_name)\nmodel_path = \"\/kaggle\/working\/U-Net.pth7\"\n","6d63b5e1":"model_ = UNET(in_channels=3, out_channels=10).to(device)\nmodel_.load_state_dict(torch.load(model_path))","636a7d5d":"test_batch_size = 8\ndataset = Cityscape_dataset(val_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=test_batch_size)\n\nX, Y = next(iter(data_loader))\nX, Y = X.to(device), Y.to(device)\nY_pred = model_(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred, dim=1)\nprint(Y_pred.shape)\n\ninverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","6c06fa69":"fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\niou_scores = []\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    # IOU score\n    intersection = np.logical_and(label_class, label_class_predicted)\n    union = np.logical_or(label_class, label_class_predicted)\n    iou_score = np.sum(intersection) \/ np.sum(union)\n    iou_scores.append(iou_score)\n\n    axes[i, 0].imshow(landscape)\n    axes[i, 0].set_title(\"Landscape\")\n    axes[i, 1].imshow(label_class)\n    axes[i, 1].set_title(\"Label Class\")\n    axes[i, 2].imshow(label_class_predicted)\n    axes[i, 2].set_title(\"Label Class - Predicted\")\n\nprint(\"got accurancy = \",sum(iou_scores) \/ len(iou_scores))\n","7829afcc":"pwd","04f17e46":"%cd \/kaggle\/working","8c779fd2":"from IPython.display import FileLink \nFileLink(r'.\/U-Net.pth')","8b2be446":"import cv2 \n\nvideo_name = input(\"Enter video name and format\\n\")\n\ncap = cv2.VideoCapture(\"{}\".format(video_name))\n\nwhile(True):\n    ret, frame = cap.read()\n    Y_pred = model_(frame)\n    Y_pred = torch.argmax(Y_pred, dim=1)\n    landscape = inverse_transform(frame[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n\n    font = cv2.FONT_HERSHEY_SIMPLEX \n    org = (10, 10) \n    fontScale = 1\n    color = (255, 0, 0) \n    thickness = 2\n    image = cv2.putText(Y_pred, 'OpenCV', org, font,  \n                   fontScale, color, thickness, cv2.LINE_AA) \n    \n    cv2.imshow(\"SemanticSegmentation\", image)\n    ","6f0b3760":"a, b = dataset[0]","b742e554":"a, b = dataset[0]\nprint(a.shape)\nprint(b.shape)","6497135a":"plt.imshow(cityscape.permute(1, 2, 0))","6d8301f1":"cityscape2 = cityscape.numpy()\nplt.imshow(np.transpose(cityscape2, (1, 2, 0)), interpolation='nearest')","d50dc73f":"model2_ = UNET(in_channels=3, out_channels=10).to(device)\nmodel2_.load_state_dict(torch.load(model_path))","1b20bccd":"# PyTorch Tensors (\"Image tensors\") are channel first","5c4ce705":"np.random.choice(x,y)\\\nx --> range of numbers [ 0 : x - 1 ]\\\ny --> number of requierd generated numbers","c5f13f8b":"# Download the File who is generated \n## U-Net.pth (which carry parametrs)]","74181da3":"# Clustring ","1757ae44":" cityscape_np, label_np = splite_image(Combined_Image_np)","f0a2d8e6":"os.path.join --> Path Combination \\\nos.listdir   --> Converting all fiels in specifed path into list\n","94e9cf6c":"\nPIL = Image.fromarray(np)","2422cc1c":"PIL image = Image.open(path)","156da29f":"# Define DataSet"}}