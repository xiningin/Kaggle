{"cell_type":{"35aa7481":"code","eb50ee86":"code","5f13126f":"code","d48eed76":"code","ef66f9c1":"code","d6429284":"code","4f791c89":"code","36959e75":"code","6fe9321c":"code","d314ec73":"code","a416c8df":"code","373ce313":"code","c80f8a56":"code","1b816c81":"code","c5cc2b9f":"code","2bd1f047":"code","2d167274":"code","4539f665":"code","48976a8f":"code","c17b63bb":"code","6bcdbb0c":"code","3b5ac2df":"code","f83bf26c":"code","18f8ae24":"code","c139ecb8":"code","984767c7":"code","8eacab4a":"code","46e81c6f":"code","f3171bc1":"code","4f08e3ca":"code","7b0d8f3b":"code","a70d5ae4":"code","0dc74e28":"code","6d850617":"code","739688fe":"code","028e66b8":"code","088b06cf":"code","5ba7c632":"code","693b7cce":"code","ba192a7d":"code","44b9f815":"code","574cd2dd":"code","630b11f5":"code","781b5aca":"code","72045d6d":"code","1dbefa38":"code","b38039ab":"code","d34ac64e":"markdown","da497981":"markdown","b7024a49":"markdown","aeac1557":"markdown","2e10aaf9":"markdown","e8484be8":"markdown","aec78181":"markdown","e4b31f36":"markdown","6474744b":"markdown","3015d3e3":"markdown","9e17170f":"markdown","7e29eda5":"markdown","33d9c336":"markdown","bfeb6f8d":"markdown","f73d5da8":"markdown","07b22bda":"markdown","6c75c0b3":"markdown","2da24779":"markdown","068ae2ef":"markdown","79c1e12c":"markdown","c8fb93a5":"markdown","595cd40a":"markdown","6f189fdb":"markdown","a2fbf5fd":"markdown","208c1b10":"markdown"},"source":{"35aa7481":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb50ee86":"import matplotlib.pyplot as plt\nimport seaborn as sns","5f13126f":"# Load Data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d48eed76":"#top five row of data\ntrain.head()","ef66f9c1":"test.head()","d6429284":"#dimension of train data\ntrain.shape","4f791c89":"#dimension of test data\ntest.shape","36959e75":"train.info()","6fe9321c":"sns.distplot(train['SalePrice']);","d314ec73":"#skewness\nprint(\"Skewness: %f\" % train['SalePrice'].skew())","a416c8df":"#To remove the skewness we use the log function\nSalePriceLog = np.log(train['SalePrice'])\nSalePriceLog.skew()","373ce313":"#Plot after adjusted skewness\nsns.distplot(SalePriceLog);","c80f8a56":"SalePrice = SalePriceLog","1b816c81":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","c5cc2b9f":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(50,20))\nsns.heatmap(corrmat, vmax=0.9, square=True, annot=True)","2bd1f047":"Num=corrmat['SalePrice'].sort_values(ascending=False).head(10).to_frame()\n\nNum","2d167274":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\nmissing_data = pd.concat([total], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","4539f665":"#missing data\ntotal = test.isnull().sum().sort_values(ascending=False)\nmissing_data = pd.concat([total], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","48976a8f":"#visulize missing value using sns plot\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=total.index, y=total)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","c17b63bb":"train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','MasVnrType'], axis=1 ,inplace=True)\ntest.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','MasVnrType'], axis=1 ,inplace=True)","6bcdbb0c":"# missing value treatment for continuous variable\nfor col in ('LotFrontage','GarageYrBlt','GarageCars','BsmtFinSF1','TotalBsmtSF','GarageArea','BsmtFinSF2','BsmtUnfSF','LotFrontage','GarageYrBlt','BsmtFullBath','BsmtHalfBath'):\n    train[col]=train[col].fillna(train[col].mean())\n    test[col]=test[col].fillna(test[col].mean())","3b5ac2df":"# missing value treatment for categorical variable\nfor col in ('BsmtQual','BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrArea', 'Electrical','Exterior2nd','Exterior1st','KitchenQual','Functional','SaleType','Utilities','MSZoning','BsmtQual','BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrArea', 'Electrical'):\n    test[col]=test[col].fillna(test[col].mode()[0])\n    train[col]=train[col].fillna(train[col].mode()[0])","f83bf26c":"# checking if is there any missing variable left\ntrain.isnull().sum().max()","18f8ae24":"# checking if is there any missing variable left\ntest.isnull().sum().max()","c139ecb8":"list_of_numerics=train.select_dtypes(include=['float','int']).columns\ntypes= train.dtypes\n\noutliers= train.apply(lambda x: sum(\n                                 (x<(x.quantile(0.25)-1.5*(x.quantile(0.75)-x.quantile(0.25))))|\n                                 (x>(x.quantile(0.75)+1.5*(x.quantile(0.75)-x.quantile(0.25))))\n                                 if x.name in list_of_numerics else ''))\n\n\nexplo = pd.DataFrame({'Types': types,\n                      'Outliers': outliers}).sort_values(by=['Types'],ascending=False)\nexplo.transpose()","984767c7":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nax1= sns.scatterplot(x='GrLivArea', y='SalePrice', data= train,ax=axes[0])\nax2= sns.boxplot(x='GrLivArea', data= train,ax=axes[1])","8eacab4a":"#removing outliers recomended by author\ntrain= train[train['GrLivArea']<4000]\n#test= test[test['GrLivArea']<4000]","46e81c6f":"train['MSSubClass'] = train['MSSubClass'].apply(str)\ntrain['YrSold'] = train['YrSold'].astype(str)\n\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\ntest['YrSold'] = test['YrSold'].astype(str)","f3171bc1":"categorial_features_train = train.select_dtypes(include=[np.object])\ncategorial_features_train.head(2)","4f08e3ca":"categorial_features_test = test.select_dtypes(include=[np.object])\ncategorial_features_test.head(2)","7b0d8f3b":"##Label Encoding\nfrom sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\n\nlabel_encoders = {}\nfor column in categorial_features_train:\n    label_encoders[column] = LabelEncoder()\n    train[column] = label_encoders[column].fit_transform(train[column]) ","a70d5ae4":"##Label Encoding\nfrom sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\n\nlabel_encoders = {}\nfor column in categorial_features_test:\n    label_encoders[column] = LabelEncoder()\n    test[column] = label_encoders[column].fit_transform(test[column]) ","0dc74e28":"# dividing into dependent and independent variable data set\nxtrain = train.drop('SalePrice', axis = 1)\nytrain = train['SalePrice']","6d850617":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nimport xgboost as xgb","739688fe":"model1 = LinearRegression()\nmodel1.fit(xtrain, ytrain)","028e66b8":"# score the model\nmodel1.score(xtrain,ytrain)","088b06cf":"model2 = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","5ba7c632":"model2.fit(xtrain,ytrain)","693b7cce":"model2.score(xtrain,ytrain)","ba192a7d":"model3 = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","44b9f815":"model3.fit(xtrain, ytrain)","574cd2dd":"model3.score(xtrain, ytrain)","630b11f5":"pred_1=model1.predict(test)\npred_2=model2.predict(test)\npred_3=model3.predict(test)","781b5aca":"final_pred = (pred_1+pred_2+pred_3)\/3","72045d6d":"final_pred","1dbefa38":"sample_sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsample_sub.head()","b38039ab":"sample_sub['SalePrice'] = final_pred\nsample_sub.to_csv('final_submission1.csv', index=False)","d34ac64e":"# Visualize the Data","da497981":"1. **Linear Regression**","b7024a49":"Hyper Tunnning","aeac1557":"More than 50% of data are missing for PoolQC, MiscFeature, Alley, Fence. So, we can drop the dataframe.\nFew data frame is uncorrelated, so we can drop them too.\n","2e10aaf9":"As you might know by now, we can\u2019t have text in our data if we\u2019re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.\n","e8484be8":"From the above you can see that Sales Price has Positive Skewness","aec78181":"Extracting the categorical column from train and test data.","e4b31f36":"2. **Gradient Boost**","6474744b":"Replacing other missing data frame into median for continous variable and mode for categorical variable","3015d3e3":"As you can see dimension of test and train data is different, you can not merge the data, data exploration should be done indivisually","9e17170f":"Label Encoding","7e29eda5":"Creating Submission file","33d9c336":"# Importing Library","bfeb6f8d":"# Variable Transformation","f73d5da8":"# Analyse the target variable (Univariate Analysis) ","07b22bda":"# Outlier","6c75c0b3":"Numerical variable which are actually categorical","2da24779":"# *If you like my kernel please upvote :) *","068ae2ef":"# Missing Variable Treatment","79c1e12c":"# Modelling","c8fb93a5":"3. **XGBoost**","595cd40a":"Prediction on test data of different model","6f189fdb":"Hyper Tunning ","a2fbf5fd":"# Bivariate Analysis","208c1b10":"we take an average of predictions from all the models and use it to make the final prediction"}}