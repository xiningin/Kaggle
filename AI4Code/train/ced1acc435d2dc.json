{"cell_type":{"8bb7cab4":"code","4e5184fd":"code","91a74d51":"code","c310323d":"code","af8f17f8":"code","281f3e59":"code","29025091":"code","937df8e4":"code","519c186c":"code","9d7ed7dc":"code","2ba93f08":"code","defc1822":"code","9d9fbd9e":"code","a5d01583":"code","8986da39":"code","19ec8c9e":"code","7d15fecb":"code","5f5ade34":"code","90aea9b7":"code","f3200f0c":"code","8672c94c":"code","24b537ca":"code","5a7a1c50":"code","71a93a4f":"code","88a0ad30":"code","fd971313":"code","e5176e5a":"code","770f7fc2":"code","c9e0e1d9":"code","16687b20":"code","cd08097d":"code","654fa924":"code","08ae9bd5":"code","aea4f70d":"code","d36342c5":"code","728b0ba1":"code","0bb3018a":"code","e5292fb3":"code","58e85cc4":"markdown","7fd3e6b8":"markdown","7ea1f35b":"markdown","e074d602":"markdown","bfc0890d":"markdown","40518359":"markdown","255280d3":"markdown","158b36fb":"markdown","75a29794":"markdown","faaa71b7":"markdown","5d6eab92":"markdown","35e98381":"markdown","20aeec41":"markdown","af02fafa":"markdown","3a6d6097":"markdown","8bf0eb18":"markdown","4c61baa6":"markdown","d70739d0":"markdown","58e451b0":"markdown","82f0c66d":"markdown","3b053b8b":"markdown","02c40fdc":"markdown","f70bb8d0":"markdown","d893b636":"markdown","c4b6a40e":"markdown","b10a84e8":"markdown","166890a6":"markdown","d6c505c0":"markdown"},"source":{"8bb7cab4":"from pathlib import Path\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\n# from kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","4e5184fd":"!nvidia-smi","91a74d51":"# Not using TPU\n# try:\n#     raise ValueError('disable TPU')\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     print('...fail...')\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)\n\nstrategy = tf.distribute.get_strategy()\nAUTOTUNE = tf.data.experimental.AUTOTUNE","c310323d":"tf.__version__","af8f17f8":"img_height, img_width = 256, 256\nchannels = 3","281f3e59":"def decode_img(image):\n    # convert to 3-channel\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [img_height, img_width])\n    # scale to <-1,1>\n    image = (tf.cast(image, tf.float32) \/ 127.5) - 1\n    return image\n\ndef process_path(file_path):\n    # does not work when using TPU\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img","29025091":"list_ds = tf.data.Dataset.list_files('..\/input\/imagenetsketch\/sketch\/*\/*.JPEG', shuffle=True)\n# batch images so they have the right shape for neural net\nstyled_ds = list_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1)","937df8e4":"def plot(image, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    ax.imshow(image * 0.5 + 0.5)","519c186c":"fix, axs = plt.subplots(1, 5, figsize=(17,17))\nfor batch, ax in zip(styled_ds.take(5), axs):\n    plot(batch[0], ax)\nplt.show()","9d7ed7dc":"list_ds = tf.data.Dataset.list_files('..\/input\/gan-getting-started\/photo_jpg\/*.jpg', shuffle=True)\nphoto_ds = list_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1)","2ba93f08":"fix, axs = plt.subplots(1, 5, figsize=(17,17))\nfor batch, ax in zip(photo_ds.take(5), axs):\n    plot(batch[0], ax)\nplt.show()","defc1822":"OUTPUT_CHANNELS = 3\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result","9d9fbd9e":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","a5d01583":"def Generator():\n    inputs = layers.Input(shape=[img_height,img_width,3])\n\n    # bs = batch size, assuming (h, w) = (256, 256)\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","8986da39":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","19ec8c9e":"with strategy.scope():\n    styled_generator = Generator() # transforms photos to drawings\n    photo_generator = Generator() # transforms drawings to photos\n\n    styled_discriminator = Discriminator() # differentiates real and generated drawings\n    photo_discriminator = Discriminator() # differentiates real and generated photos","7d15fecb":"example = next(iter(photo_ds))","5f5ade34":"example.shape","90aea9b7":"styled = styled_generator(example)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original\")\nplot(example[0])\nplt.show()\n\nplt.subplot(1, 2, 2)\nplt.title(\"Generated\")\nplot(styled[0])\nplt.show()","f3200f0c":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        styled_generator,\n        photo_generator,\n        styled_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.s_gen = styled_generator\n        self.p_gen = photo_generator\n        self.s_disc = styled_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        s_gen_optimizer,\n        p_gen_optimizer,\n        s_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.s_gen_optimizer = s_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.s_disc_optimizer = s_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        # @TODO: rename\n        real_styled, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to styled back to photo\n            fake_styled = self.s_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_styled, training=True)\n\n            # styled to photo back to styled\n            fake_photo = self.p_gen(real_styled, training=True)\n            cycled_styled = self.s_gen(fake_photo, training=True)\n\n            # generating itself\n            same_styled = self.s_gen(real_styled, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_styled = self.s_disc(real_styled, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_styled = self.s_disc(fake_styled, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            styled_gen_loss = self.gen_loss_fn(disc_fake_styled)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_styled, cycled_styled, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_styled_gen_loss = styled_gen_loss + total_cycle_loss + self.identity_loss_fn(real_styled, same_styled, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            styled_disc_loss = self.disc_loss_fn(disc_real_styled, disc_fake_styled)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        styled_generator_gradients = tape.gradient(total_styled_gen_loss,\n                                                  self.s_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        styled_discriminator_gradients = tape.gradient(styled_disc_loss,\n                                                      self.s_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.s_gen_optimizer.apply_gradients(zip(styled_generator_gradients,\n                                                 self.s_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.s_disc_optimizer.apply_gradients(zip(styled_discriminator_gradients,\n                                                  self.s_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"styled_gen_loss\": total_styled_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"styled_disc_loss\": styled_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","8672c94c":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","24b537ca":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","5a7a1c50":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","71a93a4f":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","88a0ad30":"with strategy.scope():\n    styled_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    styled_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","fd971313":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        styled_generator, photo_generator, styled_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        s_gen_optimizer = styled_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        s_disc_optimizer = styled_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","e5176e5a":"len(photo_ds), len(styled_ds)","770f7fc2":"# in general yo uwant to batch after shuffling but it doesn't matter for batch_size=1\nds_A = tf.data.Dataset.from_tensor_slices(['a', 'b', 'c']).batch(1).shuffle(3, reshuffle_each_iteration=True).repeat()\nds_B = tf.data.Dataset.range(5).batch(1).shuffle(5, reshuffle_each_iteration=True).repeat()\nds_zipped_dummy = tf.data.Dataset.zip((ds_A, ds_B))\n\nlist(ds_zipped_dummy.take(10).as_numpy_iterator())","c9e0e1d9":"class ShowSamplesCallback(keras.callbacks.Callback):\n    def __init__(self, save_to_dir: str):\n        self.save_to_dir = save_to_dir\n    \n    def on_train_begin(self, logs=None):\n        self.original = []\n        self.styled = []\n    \n    def on_train_end(self, logs=None):\n        if self.save_to_dir:\n            dir_ = Path(self.save_to_dir)\n            dir_.mkdir(parents=True, exist_ok=True)\n            for i, (original, styled) in enumerate(zip(self.original, self.styled)):\n                tf.keras.preprocessing.image.save_img(dir_ \/ f'{i + 1}_original.jpg', original)\n                tf.keras.preprocessing.image.save_img(dir_ \/ f'{i + 1}_styled.jpg', styled)\n    \n    def on_epoch_end(self, epoch, logs=None):\n        styled_generator = self.model.s_gen\n        # sample predict\n        img_batch = next(iter(photo_ds.take(1)))\n        prediction = styled_generator(img_batch, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        img = img_batch[0]\n        img = (img * 127.5 + 127.5).numpy().astype(np.uint8)\n        self.original.append(img)\n        self.styled.append(prediction)\n        \n        # plot\n        fig, axs = plt.subplots(1, 2, figsize=(6,6), squeeze=True)\n        axs[0].imshow(img)\n        axs[1].imshow(prediction)\n        axs[0].set_title(\"Input Photo [{:02d}]\".format(epoch + 1))\n        axs[1].set_title(\"Generated [{:02d}]\".format(epoch + 1))\n        axs[0].axis(\"off\")\n        axs[1].axis(\"off\")\n        plt.show()","16687b20":"class SaveGeneratorCallback(keras.callbacks.Callback):\n    def __init__(self, epochs: int, path: str):\n        self.epochs = epochs\n        self.dir_ = Path(path)\n    \n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % self.epochs == 0:\n            styled_generator = self.model.s_gen\n            path = self.dir_ \/ f'weights.{epoch + 1}.hdf5'\n            print(f'[SaveGeneratorCallback] Saving weights to {path}.')\n            styled_generator.save_weights(path)","cd08097d":"HOUR_TO_SECONDS = 3600","654fa924":"_buffer_size = 3000\n\nhistory = cycle_gan_model.fit(\n    # zipped dataset - infinitely repeating\n    tf.data.Dataset.zip((\n        # buffer_size should be at least the size of the dataset for uniform shuffling\n        # - https:\/\/stackoverflow.com\/a\/47025850\/3936732\n        # - however the dataset is too large for that so we limit the size\n        styled_ds.shuffle(_buffer_size, reshuffle_each_iteration=True).repeat(),\n        photo_ds.shuffle(_buffer_size, reshuffle_each_iteration=True).repeat(),\n    )),\n    \n    steps_per_epoch=1000, # batches per epoch\n    epochs=100,\n    # --- callbacks ---\n    callbacks=[\n        tfa.callbacks.TimeStopping(seconds=HOUR_TO_SECONDS * 6, verbose=1),\n        ShowSamplesCallback('\/kaggle\/working\/imgs'),\n        SaveGeneratorCallback(50, '\/kaggle\/working'),\n              ],\n)","08ae9bd5":"_, ax = plt.subplots(2, 5, figsize=(16, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = styled_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[0, i].imshow(img)\n    ax[1, i].imshow(prediction)\n    ax[0, i].set_title(\"Input Photo\")\n    ax[1, i].set_title(\"Generated\")\n    ax[0, i].axis(\"off\")\n    ax[1, i].axis(\"off\")\nplt.tight_layout()\nplt.show()","aea4f70d":"_TEST = False","d36342c5":"if _TEST:\n    generator = Generator()\n    generator.load_weights('\/kaggle\/working\/weights.1.hdf5')\n    sample_photo = next(iter(photo_ds))\n    prediction = generator(sample_photo)\n    plot(sample_photo[0])\n    plt.show()\n    plot(prediction[0])\n    plt.show()","728b0ba1":"!mkdir -p \/kaggle\/working\/saved_model\nstyled_generator.save('\/kaggle\/working\/saved_model\/styled_generator')","0bb3018a":"loaded_styled_generator = tf.keras.models.load_model('\/kaggle\/working\/saved_model\/styled_generator')","e5292fb3":"n_rows = 3 * 2\nn_cols = 5\n_, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*4))\nrow = 0\ni = 0\nfor img in photo_ds.take(n_rows\/\/2*n_cols):\n    prediction = loaded_styled_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n    \n    ax[row + 0, i].imshow(img)\n    ax[row + 1, i].imshow(prediction)\n    ax[row + 0, i].set_title(\"Input Photo\")\n    ax[row + 1, i].set_title(\"Generated\")\n    ax[row + 0, i].axis(\"off\")\n    ax[row + 1, i].axis(\"off\")\n    \n    i += 1\n    if i >= n_cols:\n        i = 0\n        row += 2\n    \nplt.tight_layout()\nplt.show()","58e85cc4":"## CycleGAN","7fd3e6b8":"### Generator\n\nThe first loss tells us how well the generator tricked the discriminator. Ideally the discriminator should output only 1s for the generated images.","7ea1f35b":"Dataset is from https:\/\/github.com\/HaohanWang\/ImageNet-Sketch.\n\nMost images have 3 channels, some have 1. Sizes vary with median size of 570x600.","e074d602":"## Building Blocks","bfc0890d":"## Loss functions","40518359":"# Data","255280d3":"### Preparing zipped dataset\n\nWe need to combine the two datasets so we can provide images in pairs. Datasets also have different sizes and we need to deal with that.","158b36fb":"# Visualize","75a29794":"Enable TPU if needed.","faaa71b7":"## Custom Callbacks","5d6eab92":"# Setup","35e98381":"## Photo Dataset\n\nFrom https:\/\/www.kaggle.com\/c\/gan-getting-started.","20aeec41":"### Training","af02fafa":" The identity loss compares the input with the output of the generator.","3a6d6097":"### Discriminator\n\nDiscriminator should predict 1s for real images and 0s for fake images. The discriminator loss is the average of the real and generated loss.","8bf0eb18":"## Generator\n\nThe generator downsamples and then upsamples the image while using skip connections (UNET architecture).","4c61baa6":"# Intro\n\nThe goal of this project is to drawings\/sketches from photos. This type of task is often called style transfer and popular approach for this task is to use GANs ([review](https:\/\/arxiv.org\/abs\/1705.04058)).\n\nI've decided to start with [CycleGAN](https:\/\/arxiv.org\/abs\/1703.10593). CycleGAN allows for image-to-image translation without paired examples. This makes it fairly easy to use, as we do not need to get\/create both the reference output for each image and instead our input are only images to be translated and images showing the desired style.\n\nOther interesting options are [ArtPDGAN](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-030-50436-6_21) and [Im2Pencil](https:\/\/arxiv.org\/pdf\/1903.08682.pdf), both of which are directly focused on drawings specifically but require paired images (original and reference).\n\n## Datasets & Implementation\n\nDatasets are from [\"I\u2019m Something of a Painter Myself\"](https:\/\/www.kaggle.com\/c\/gan-getting-started) Kaggle competition and [ImageNet-Sketch](https:\/\/github.com\/HaohanWang\/ImageNet-Sketch).\n\nThe implementation of CycleGAN is based on the \"[Monet CycleGAN Tutorial](https:\/\/www.kaggle.com\/amyjang\/monet-cyclegan-tutorial)\".\n\n\n### Preprocessing\n\nImages are **resized** to 256x256 and **scaled** to <-1;1>. All images are converted to 3 channels for consistency.\n\n## Setup & Running\n\nThe notebook can be run directly on Kaggle without any setup. You can reference Kaggle's [Dockerfile](https:\/\/github.com\/Kaggle\/docker-python\/blob\/main\/Dockerfile.tmpl) for local running. The main dependency is `Tensorflow 2.6.0`.","d70739d0":"## Loading weights only (saved with callback)","58e451b0":"#### Dataset combinaton example","82f0c66d":"## Final Model","3b053b8b":"# Model Saving\/Loading","02c40fdc":"I'll be using CycleGAN with the UNET architecture. The networks are composed of `downsample` and `upsamble` blocks. Instance normalization is used which is included in Tensorflow Addons.","f70bb8d0":"The original and twice transformed photos should be similar. We calculate the the average of their (absolute) difference as cycle consistency loss. \n","d893b636":"## Discriminator\n\nDiscriminator classifies the image as real or generated. The discriminator outputs a smaller 2D image with higher values indicating a real image.","c4b6a40e":"# Training","b10a84e8":"### Sample Run","166890a6":"## Sketch Dataset","d6c505c0":"# Architecture"}}