{"cell_type":{"09ab7019":"code","1d8b4558":"code","e3eea1c0":"code","5cf48e4d":"code","46d60dc2":"code","ee66cf32":"code","4ba1a828":"code","33ec23ab":"code","571bb700":"code","4a24e014":"code","51bcf5aa":"code","1f340c7c":"code","cfa24232":"code","fd592f7a":"code","a6f2aa3c":"code","a85bec5f":"code","1a2bf4f9":"code","d7d09ec2":"code","a5842ae5":"markdown","8f129d94":"markdown","bc9c6115":"markdown","c5ece4dd":"markdown","440168f2":"markdown","e9b91ce7":"markdown","fcb17db8":"markdown"},"source":{"09ab7019":"!pip uninstall efficientnet\n!pip install -U git+https:\/\/github.com\/qubvel\/efficientnet","1d8b4558":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom tensorflow.keras.models import Model\nfrom PIL import Image\nimport efficientnet.tfkeras as efn\nfrom efficientnet.tfkeras import preprocess_input\nimport math\nfrom tqdm import tqdm\nimport random\nimport matplotlib.pyplot as plt","e3eea1c0":"def calculate_size(current_size, target_size):\n    \"\"\"\n    Calculates the size of the image when it is resized to target_size while keeping the aspect ratio.\n    Params:\n    current_size (tuple): Size of the current image\n    target_size (tuple):  Desired size of the image\n    Returns:\n    A calculated size which is most closest to target_size when aspect ratio is kept.\n    \"\"\"\n    w_ratio = target_size[0] \/ current_size[0]\n    h_ratio = target_size[1] \/ current_size[1]\n    scale = min(w_ratio, h_ratio)\n    return (int(scale * current_size[0]), int(scale * current_size[1]))\n\n\ndef pad_to_n(arr, new_dim):\n    \"\"\"\n    Apply a 2d padding on array to match its size to a new dimension.\n    Params:\n    arr (ndarray): Array which padding will be applied.\n    new_dim (tuple): Dimension after the padding.\n    Returns:\n    Padded input array\n    \"\"\"\n    if len(arr.shape)== 3:\n        h,w,c = arr.shape\n        x_new = np.zeros((new_dim[0], new_dim[1], c))\n        x_new[:h, :w, :c] = arr.copy()\n    else:\n        h,w = arr.shape\n        x_new = np.zeros((new_dim[0], new_dim[1]))\n        x_new[:h, :w] = arr.copy()\n    return x_new\n\ndef load_img(img_path, preprocess=None, target_size=None, same_aspect=False):\n    \"\"\"\n    Loads image file in given target size and applies preprocess to loaded file.\n    Params:\n    img_path: Path of the image file to be loaded.\n    preprocess: Preprocess function to apply the images. If passed as None then\n    no preprocess will be applied. \n    target_size: Size of the image to be loaded in (width, height). If passed \n    as None then the image will be loaded in its orginal size\n    same_aspect: Whether to keep the same aspect ratio while resizing.\n    If not none then target_size should provided.\n    Returns:\n    Preprocess applied loaded image in target size \n    \"\"\"\n    img = Image.open(img_path)\n    org_img_size = img.size\n  \n    if target_size is not None:\n        if same_aspect:\n            # When aspect ratio kept same, it may not be possbile to \n            # resize to a target size.\n            possible_size = calculate_size(org_img_size, target_size)\n            img = img.resize(possible_size)\n            img = np.array(img)\n            new_target_size = (target_size[1], target_size[0])\n            img = pad_to_n(img,  new_target_size).astype('uint8')\n        else:\n            img = img.resize(target_size)\n            img = np.array(img)\n    \n    if preprocess is not None:\n        img = preprocess(img)\n\n    return img","5cf48e4d":"model = efn.EfficientNetB2(weights='imagenet')\nfeature_layer = model.get_layer('avg_pool')\nfeature_extractor = Model(model.input, feature_layer.output)","46d60dc2":"train_imgs_dir = '\/kaggle\/input\/petfinder-pawpularity-score\/train\/'\ntrain_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/train.csv')\nN_EXAMPLES = len(train_df)","ee66cf32":"features = []\npaw_scores = []\nimg_paths = []\nimage_ids = []\nbatch_size = 24\niter_num = math.ceil(len(train_df) \/ batch_size)","4ba1a828":"for i in tqdm(range(iter_num)) :\n    batch_df = train_df.iloc[i*batch_size:(i+1)*batch_size]\n    batch_paths = [os.path.join(train_imgs_dir, pid + '.jpg') for pid in batch_df['Id']]\n    batch_paws = batch_df['Pawpularity'].tolist()\n    batch_ids = batch_df['Id'].tolist()\n    batch_imgs = np.array([load_img(img_path, preprocess=preprocess_input, target_size=(260, 260), same_aspect=False) for img_path in batch_paths])\n    \n    batch_features = feature_extractor.predict(batch_imgs)\n    features.append(batch_features)\n    paw_scores.extend(batch_paws)\n    img_paths.extend(batch_paths)\n    image_ids.extend(batch_ids)","33ec23ab":"features = np.array(features)\n# Rearange batch dimension\nnp_features = features.reshape(features.shape[0] * features.shape[1], features.shape[2])\npaw_scores = np.array(paw_scores)\nimg_paths = np.array(img_paths)","571bb700":"def plot_similar_pairs(xs, ys):\n    \"\"\"\n    Plots similar pairs given their index lists\n    Params:\n        xs: Similar item index list\n        ys: Similar item index list. Each item must be correspond to its pair in parameter xs\n    \"\"\"    \n    for x,y in zip(xs,ys):\n        img_path_1 = img_paths[x]\n        img_path_2 = img_paths[y]\n        img1 = load_img(img_path_1, preprocess=None, target_size=None, same_aspect=False)\n        img2 = load_img(img_path_2, preprocess=None, target_size=None, same_aspect=False)\n        paw_score_1 = paw_scores[x]\n        paw_score_2 = paw_scores[y]\n\n        fig, ax = plt.subplots(1, 2)\n        ax[0].imshow(img1)\n        ax[0].set_title(\"Paw Score: {:.2f}\".format(paw_score_1))\n        ax[1].imshow(img2)\n        ax[1].set_title(\"Paw Score: {:.2f}\".format(paw_score_2))\n\n        for j in range(2):\n            ax[j].set_xticks([])\n            ax[j].set_yticks([])\n\n        plt.show()    ","4a24e014":"from sklearn.metrics.pairwise import cosine_similarity\n\ncos_similarity = cosine_similarity(np_features)","51bcf5aa":"# Diagonal entries are similarity scores between same images\n# Make them 0 to not include them later\ncos_similarity[np.eye(N_EXAMPLES, dtype='bool')] = 0","1f340c7c":"SIM_THRES = 0.9\nsim_mask = np.where(cos_similarity > SIM_THRES, True, False)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","cfa24232":"plot_similar_pairs(xs, ys)","fd592f7a":"UP_SIM_THRES = 0.9\nLOW_SIM_THRES = 0.85\nsim_mask_low = np.where(cos_similarity > LOW_SIM_THRES, True, False)\nsim_mask_up = np.where(cos_similarity < UP_SIM_THRES, True, False)\nsim_mask = np.logical_and(sim_mask_low, sim_mask_up)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","a6f2aa3c":"plot_similar_pairs(xs, ys)","a85bec5f":"UP_SIM_THRES = 1.01\nLOW_SIM_THRES = 0.77\nsim_mask_low = np.where(cos_similarity > LOW_SIM_THRES, True, False)\nsim_mask_up = np.where(cos_similarity < UP_SIM_THRES, True, False)\nsim_mask = np.logical_and(sim_mask_low, sim_mask_up)\n# Consider upper half of the similarity mask to remove duplicate similar pairs\nhalf_sim_mask = np.triu(sim_mask)\nxs, ys = np.where(half_sim_mask)","1a2bf4f9":"to_delete_indx = np.unique(np.concatenate([xs, ys]))\nprint(\"There are {} images to be deleted\".format(len(to_delete_indx)))\nto_delete_img_ids = [image_ids[idx] for idx in to_delete_indx]","d7d09ec2":"cleaned_df = train_df[train_df['Id'].apply(lambda x: x not in to_delete_img_ids)]\ncleaned_df.to_csv('duplicate_removed_train.csv', index=False)","a5842ae5":"# Findings\n\n* 37 images are exatcly same. This number is obtained when cosine similarity threshold is set to 0.9\n* Between 0.8-0.9 similarity threshold values, images from the same animal are captured. In this range images are very similar yet paw score of them differ much\n* Above showed that, scaling, rotating or applying similar augmentations may yield different paw scores. In my opinion, we should either discard one of these images or change our augmentation methods","8f129d94":"# Samples With Similarity Between 0.85-0.9","bc9c6115":"# Samples With Similarity > 0.9","c5ece4dd":"Inspired from [schulta's](https:\/\/www.kaggle.com\/schulta) [work on identifying duplicates](https:\/\/www.kaggle.com\/schulta\/petfinder-identify-duplicates-and-share-findings\/notebook), I tried to find the duplicate images with a pretrained CNN rather than image hashing. The reason for that is, CNNs are better at identifying same images when one of them is rotated, translated, etc.\n\n","440168f2":"# Method\n\nI used a pretrained EfficientnetB2 model with Imagenet weights as a feature extractor (I consider the features just after the Global Average Pooling layer). \n\nThen I calculated the cosine similarity between each images and visualize them.","e9b91ce7":"# Model Creation","fcb17db8":"# **PetFinder-Finding Duplicates With CNN**"}}