{"cell_type":{"17bf2280":"code","4828fcfd":"code","c21d7e1e":"code","c71ed97a":"code","5fb889c7":"code","f77a99ab":"code","63c16356":"code","f6c9e42e":"code","fc330398":"code","23c2b3e2":"code","d49b3df2":"code","635dc151":"code","31e80260":"code","d6e46726":"code","553d98b8":"code","81e05008":"code","db26ffcf":"code","b5710442":"code","50870ef7":"code","ed8b62f1":"code","bb584fe4":"code","c279ca54":"code","02f31e53":"code","a25d83ed":"code","693623d0":"markdown","98185f8a":"markdown","31034a8c":"markdown","56ab8daa":"markdown","db1238de":"markdown","7fd11adf":"markdown","2c0ddfef":"markdown","ef6779cc":"markdown","d523a9bc":"markdown","3dd7510c":"markdown","c89502d5":"markdown","b55e8c97":"markdown","4ea4e179":"markdown","cbb02eab":"markdown","4ec4d2fa":"markdown","a8f80a4f":"markdown","0059cd0b":"markdown","7154d320":"markdown","d0215ef1":"markdown","b6f635a1":"markdown","e90d5f64":"markdown","3830120c":"markdown","7a41ea71":"markdown","46e6f7f2":"markdown","85a717cc":"markdown","dc31097d":"markdown","3175ebda":"markdown","488c5eb8":"markdown","4100384b":"markdown","a9c8b7a4":"markdown"},"source":{"17bf2280":"# Pandas library is used for handling tabular data\nimport pandas as pd\n\n# NumPy is used for handling numerical series operations (addition, multiplication, and ...)\nimport numpy as np\n\n# Sklearn library contains all the machine learning packages we need to digest and extract patterns from the data\nfrom sklearn import linear_model, model_selection, metrics\nfrom sklearn.model_selection import train_test_split","4828fcfd":"!pip install openpyxl","c21d7e1e":"launch_data = pd.read_excel('..\/input\/rocket-launch-data\/RocketLaunchDataCompleted.xlsx')\nlaunch_data.head()","c71ed97a":"launch_data.shape","5fb889c7":"launch_data.columns","f77a99ab":"launch_data.info()","63c16356":"## To handle missing values, we will fill the missing values with appropriate values \nlaunch_data['Launched?'].fillna('N',inplace=True)\nlaunch_data['Crewed or Uncrewed'].fillna('Uncrewed',inplace=True)\nlaunch_data['Wind Direction'].fillna('unknown',inplace=True)\nlaunch_data['Condition'].fillna('Fair',inplace=True)\nlaunch_data.fillna(0,inplace=True)\nlaunch_data.head()","f6c9e42e":"launch_data['Launched?'] = launch_data['Launched?'].map({'N':0, 'Y':1})\nlaunch_data['Crewed or Uncrewed'] = launch_data['Crewed or Uncrewed'].map({'Uncrewed':0, 'Crewed':1})","fc330398":"# First, we save the output we are interested in. In this case, \"launch\" yes and no's go into the output variable.\ny = launch_data['Launched?']\n\n# Removing the columns we are not interested in\nlaunch_data.drop(['Name','Date','Time (East Coast)','Location','Launched?','Wind Direction', 'Hist Ave Sea Level Pressure','Sea Level Pressure','Day Length','Notes','Hist Ave Visibility', 'Condition', 'Hist Ave Max Wind Speed'],axis=1, inplace=True)","23c2b3e2":"# Saving the rest of the data as input data\nX = pd.get_dummies(launch_data, drop_first=True)","d49b3df2":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = ss.fit_transform(X)","635dc151":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)","31e80260":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nlog.fit(X_train, y_train)\npred_y = log.predict(X_test)","d6e46726":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, pred_y)","553d98b8":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, pred_y))","81e05008":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(log, X_test, y_test, display_labels=['N', 'Y'])","db26ffcf":"tn, fp, fn, tp = confusion_matrix(y_test, pred_y).ravel()\ntn, fp, fn, tp","b5710442":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_y))","50870ef7":"from sklearn.metrics import f1_score\nprint(f1_score(y_test, pred_y))","ed8b62f1":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, pred_y)\nfpr","bb584fe4":"tpr","c279ca54":"thresholds","02f31e53":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(log, X_test, y_test)","a25d83ed":"from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test, pred_y))","693623d0":"# Confusion Matrix","98185f8a":"The array values reflect false postive rate, true positive rate for different values of the thresholds.","31034a8c":"For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:","56ab8daa":"Encoding 'Launched?' and 'Crewed or Uncrewed' columns to integers to ease further process.","db1238de":"## Handling Missing Values","7fd11adf":"We'll use the plot_roc_curve to plt the curve.","2c0ddfef":"## Model","ef6779cc":"![Image](https:\/\/miro.medium.com\/max\/1400\/1*pOtBHai4jFd-ujaNXPilRg.png)","d523a9bc":"The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.","3dd7510c":"## Train, Test, Split","c89502d5":"\u201cA ***Receiver Operating Characteristic (ROC)***, or simply ***ROC curve***, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\u201d","b55e8c97":"The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class (Wikipedia and other references may use different convention for axes).\n\nBy definition, entry  in a confusion matrix is the number of observations actually in group <i>i<\/i>, but predicted to be in group <i>j<\/i>. Here is an example:","4ea4e179":"Our model has worked fine. We see that it has not misclassified any instance. Still, for the sake of exploring various classification metrics we'll see others as well.","cbb02eab":"Let's use the plot_confusion_matrix() which helps represent the confusion matrix visually.","4ec4d2fa":"The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom target_names and inferred labels:","a8f80a4f":"We'll look at various performance metrics used for scoring accuracy in Binary Classification problem.","0059cd0b":"# Receiver operating characteristic (ROC)","7154d320":"# Classification Report","d0215ef1":"The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number.","b6f635a1":"# F1 score\nF1 score also known as balanced F-score or F-measure, can be interpreted as a weighted harmonic mean of the precision and recall. It provides a balance between precision and recall.","e90d5f64":"## Imports","3830120c":"Our primary goal is to choose a machine learning algorithm to predict Rocket Launch success. Here, we have records of 300 rows of weather data representing 60 rocket launches plus the couple of days before and after a launch. We have weather information such as Conditions (cloudy, partly cloudy, fair, rain, thunder, heavy storm), Temperature, Humidity, Wind speed, Wind direction, Precipitation,\nVisibility, Sea level, Pressure, etc.\n<br><br>\nData used for this notebook is obtained from [Here](https:\/\/docs.microsoft.com\/en-us\/learn\/paths\/machine-learning-predict-launch-delay-nasa\/).\n<br><br>\n\nWe'll try to employ some simple models as the size of this particular dataset is small we do not want to use overly complex classification models. We'll also explore some of the **Classification Metrics** to score the performance of our binary classification model. ","7a41ea71":"![F1_score](https:\/\/miro.medium.com\/max\/353\/1*T6kVUKxG_Z4V5Fm1UXhEIw.png)","46e6f7f2":"# Accuracy Score","85a717cc":"Accuracy score is one of the straightforward and simple metric. However, it fails to give any information about how the labels are predicted i.e. false positives, true positives, false negatives, true negatives.","dc31097d":"# Thank You!","3175ebda":"We have 'Conditions' which gives information about the day's climate. We'll use dummy variables to handle the feature.","488c5eb8":"# Metrics for measuring model performance","4100384b":"Let's take a refresher of what measures the classification report presents.","a9c8b7a4":"<h1 align = \"center\">Rocket Launch Prediction & Classification Metrics<\/h1>"}}