{"cell_type":{"3998307b":"code","ef73b429":"code","66fbd469":"code","6497ec70":"code","f4947def":"code","56a35a9a":"code","ea1354f0":"code","fdae5ca8":"code","b4e74fb4":"code","c3aa5490":"code","2d2abfda":"code","9e900303":"code","fa890af4":"code","da746d17":"code","e1ecb0af":"code","424f82cd":"code","e14ae604":"code","806b3602":"code","3e935f87":"code","63ef046e":"code","2f0d50a1":"code","1c8bb6a2":"code","83334442":"code","a1bd7681":"code","8bfda86b":"code","3a080021":"code","45fd35e3":"code","ae8c645f":"code","e9a53130":"code","8070d32f":"code","923ced49":"code","daff5371":"code","7dad0dfc":"code","e79b63bb":"code","41f4849b":"code","a373d26f":"code","e14fa74a":"code","eabafd07":"code","635e14f1":"code","fc12853d":"code","e269b0c7":"code","13568f08":"code","2ea582b0":"code","c3de2c01":"code","61761ca0":"code","dd89e8e0":"code","d31cb695":"markdown","b5d37dbe":"markdown","d9a229a1":"markdown","9aed2dd1":"markdown","2e3e246e":"markdown","7bb58c70":"markdown","2710b60e":"markdown","06b663a7":"markdown","1dce98e4":"markdown","7631bf75":"markdown"},"source":{"3998307b":"# importing packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.datasets","ef73b429":"# reading Breast Cancer Dataset\nbreast_cancer = sklearn.datasets.load_breast_cancer()","66fbd469":"x = breast_cancer.data\ny = breast_cancer.target","6497ec70":"print(x) # contains the series of data items(each such rows contains features)","f4947def":"print(y) # contians 0s and 1s(labels)","56a35a9a":"print(x.shape, y.shape)","ea1354f0":"data = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)","fdae5ca8":"data['class'] = breast_cancer.target","b4e74fb4":"data.head()","c3aa5490":"data.describe()","2d2abfda":"print(data['class'].value_counts())","9e900303":"print(breast_cancer.target_names)","fa890af4":"data.groupby('class').mean()","da746d17":"from sklearn.model_selection import train_test_split","e1ecb0af":"x = data.drop('class', axis=1)\ny = data['class']","424f82cd":"type(x)","e14ae604":"X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1)","806b3602":"print(x.shape, X_train.shape, X_test.shape)","3e935f87":"print(y.shape, Y_train.shape, Y_test.shape)","63ef046e":"print(y.mean(), Y_train.mean(), Y_test.mean())","2f0d50a1":"X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1, stratify = y)","1c8bb6a2":"print(y.mean(), Y_train.mean(), Y_test.mean()) # now ratio is maintained of malignant and benign","83334442":"print(x.mean(), X_train.mean(), X_test.mean())","a1bd7681":"X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1, stratify = y, random_state=1)","8bfda86b":"print(X_train.mean(), X_test.mean(),x.mean())","3a080021":"import matplotlib.pyplot as plt","45fd35e3":"plt.plot(X_train.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","ae8c645f":"x_binarised_3_train = X_train['mean area'].map(lambda x: 0 if x < 1000 else 1)","e9a53130":"plt.plot(x_binarised_3_train, '*')","8070d32f":"x_binarised_train = X_train.apply(pd.cut, bins=2, labels=[1,0])","923ced49":"plt.plot(x_binarised_train.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","daff5371":"x_binarised_test = X_test.apply(pd.cut, bins=2, labels=[1,0])","7dad0dfc":"type(x_binarised_test)","e79b63bb":"x_binarised_test = x_binarised_test.values\nx_binarised_train = x_binarised_train.values","41f4849b":"type(x_binarised_train)","a373d26f":"from random import randint","e14fa74a":"b = 3\n\ni = randint(0, x_binarised_train.shape[0])\n\nprint(\"For row\", i)\n\nif (np.sum(x_binarised_train[100, :]) >= b):\n    print(\"MP Neuron inference is malignant\")\nelse:\n    print(\"MP Neuron inference is benign\")\n\nif (Y_train[i] == 1):\n    print(\"Ground Truth is malignant\")\nelse:\n    print(\"Ground truth is benign\")","eabafd07":"b = 3\n\nY_pred_train = []\naccurate_rows = 0\n\nfor x, y in zip(x_binarised_train, Y_train):\n    y_pred = (np.sum(x) >= b)\n    Y_pred_train.append(y_pred)\n    accurate_rows += (y == y_pred)\n\nprint(accurate_rows, accurate_rows\/x_binarised_train.shape[0])","635e14f1":"for b in range(x_binarised_train.shape[1] + 1):\n    Y_pred_train = []\n    accurate_rows = 0\n\n    for x, y in zip(x_binarised_train, Y_train):\n        y_pred = (np.sum(x) >= b)\n        Y_pred_train.append(y_pred)\n        accurate_rows += (y == y_pred)\n\n    print(b, accurate_rows, accurate_rows\/x_binarised_train.shape[0])","fc12853d":"# Accuracy seems has to be most at b = 28","e269b0c7":"from sklearn.metrics import accuracy_score","13568f08":"b = 28 \n\nY_pred_test = []\n\nfor x in x_binarised_test:\n    y_pred = (np.sum(x) >= b)\n    Y_pred_test.append(y_pred)\n    \naccuracy = accuracy_score(Y_pred_test, Y_test)\nprint(b,accuracy)","2ea582b0":"class MPNeuron:\n    \n    def __init__(self):\n        self.b = None\n    \n    def model(self, x):\n        return(sum(x) >= self.b)\n    \n    def predict(self, X):\n        Y = []\n        for x in X:\n            result = self.model(x)\n            Y.append(result)\n        return np.array(Y)\n    \n    def fit(self, X, Y):\n        accuracy = {}\n        \n        for b in range(X.shape[1] + 1):\n            self.b = b\n            Y_pred = self.predict(X)\n            accuracy[b] = accuracy_score(Y_pred, Y)\n        \n        best_b = max(accuracy,key = accuracy.get)\n        self.b = best_b\n        \n        print('Optimal Value of  is', best_b)\n        print('Highest accuracy is',accuracy[best_b])","c3de2c01":"mp_neuron = MPNeuron()\nmp_neuron.fit(x_binarised_train, Y_train)","61761ca0":"Y_test_pred = mp_neuron.predict(x_binarised_test)\naccuracy_test = accuracy_score(Y_test_pred, Y_test)","dd89e8e0":"print(accuracy_test)","d31cb695":"### MP neuron model","b5d37dbe":"Remember from our previous discussion, MP Neuron takes only binary values as the input. So we need to convert the continuous features into binary format. To achieve this, we will use pandas.cut function to split all the features into 0 or 1 in one single shot. Once we are ready with the inputs we need to build the model, train it on the training data and evaluate the model performance on the test data.\n\nTo create a MP Neuron Model we will create a class and inside this class, we will have three different functions:\n\n* model function\u200a\u2014\u200ato calculate the summation of the Binarized inputs.\n* predict function\u200a\u2014\u200ato predict the outcome for every observation in the data.\n* fit function\u200a\u2014\u200athe fit function iterates over all the possible values of the threshold b and find the best value of b, such that the loss will be minimum","d9a229a1":"### Binarisation of input","9aed2dd1":"After building the model, test the model performance on the testing data and check the training data accuracy as well as the testing data accuracy.\n\n### Problems with MP Neuron Model\n* Boolean Inputs.\n* Boolean Outputs.\n* Threshold b can take only a few possible values.\n* All the inputs to the model have equal weights.\n\n### Summary\nWe looked at the working of MP Neuron Model and its analogy towards biological neuron. In the end, we also saw the implementation of MP Neuron Model using python and a real word breast cancer data set.","2e3e246e":"* malignant has 357 entries\n* benign has 212 entries\n* With cancer has more than without cancer","7bb58c70":"* X as set of features and y as class label\n* In this case we looking for primary classification problem wheather a particular tissue is malignant and benign\n    * Malignant means it has cancerous nature\n    * Benign means it does't has cancer","2710b60e":"### MP Neuron Class","06b663a7":"### Train Test Split","1dce98e4":"# McCulloch Pitts Neuron - Deep Learning Building Blocks\n\n   The fundamental block of deep learning is artificial neuron i.e. it takes a weighted aggregate of inputs, applies a function and gives an output. The very first step towards the artificial neuron was taken by Warren McCulloch and Walter Pitts in 1943 inspired by neurobiology, created a model known as McCulloch-Pitts Neuron.\n\n**Disclaimer**: The content and the structure of this article is based on the deep learning lectures from One-Fourth Labs\u200a\u2014\u200a[Padhai](https:\/\/padhai.onefourthlabs.in\/).\n\n**Motivation\u200a\u2014\u200aBiological Neuron**\n\nThe inspiration for the creation of an artificial neuron comes from the biological neuron.\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL1.png?raw=true'>\n\n                            Fig\u200a\u2014\u200a1 Biological Neuron\u200a\u2014\u200aPadhai Deep Learning\nIn a simplistic view, neurons receive signals and produce a response. The general structure of a neuron is shown in the Fig-1. Dendrites are the transmission channels to bring inputs from another neuron or another organ. Synapse\u200a\u2014\u200aGoverns the strength of the interaction between the neurons, consider it like weights we use in neural networks. Soma\u200a\u2014\u200aThe processing unit of the neuron.\n\nAt the higher level, neuron takes a signal input through the dendrites, process it in the soma and passes the output through the axon (the brown color cable-like structure in the Fig-1).\n\n### McCulloch-Pitts Neuron Model\n\nMP Neuron Model introduced by Warren McCulloch and Walter Pitts in 1943. MP neuron model is also known as linear threshold gate model.\n\n#### Mathematical Model\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL2.png?raw=true'>\n                        Fig\u200a\u2014\u200a2 Simple representation of MP Neuron Model\n                        \nThe function (soma) is actually split into two parts: **g**\u200a\u2014\u200aThe aggregates the inputs to a single numeric value and the function **f** produces the output of this neuron by taking the output of the **g** as the input i,e.. a single value as its argument. The function **f** will output the value 1 if the aggregation performed by the function **g** is greater than some threshold else it will return 0.\n\nThe inputs x1, x2, \u2026.. xn for the MP Neuron can only take boolean values and the inputs can be inhibitory or excitatory. Inhibitory inputs can have maximum effect on the decision-making process of the model. In some cases, inhibitory inputs can influence the final outcome of the model.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL3.png?raw=true'>\n\n                                Fig\u200a\u2014\u200a3 Mathematical representation\nFor example, I can predict my own decision of whether I would like to watch a movie in a nearby IMAX theater tomorrow or not using an MP Neuron Model. All the inputs to the model are boolean i.e., [0,1] and from the Fig\u200a\u2014\u200a2 we can see that output from the model will also be boolean. (0\u200a\u2014\u200aNot going to movie, 1\u200a\u2014\u200agoing to the movie)\n\nInputs for the above problem could be\n\n* x1\u200a\u2014\u200aIsRainingTomorrow (Whether it's going to rain tomorrow or not)\n* x2\u200a\u2014\u200aIsScifiMovie (I like science fiction movies)\n* x3\u200a\u2014\u200aIsSickTomorrow (Whether I am going to be sick tomorrow or not depends on any symptoms, eg: fever)\n* x4\u200a\u2014\u200aIsDirectedByNolan (Movie directed by Christopher Nolan or not.) etc\u2026.\n\nIn this scenario, if x3\u200a\u2014\u200aIsSickTomorrow is equal to 1, then the output will always be 0. If I am not feeling well on the day of the movie then no matter whoever is the actor or director of the movie, I wouldn\u2019t be going for a movie.\n\n### Loss Function\nLet's take an example of buying a phone based on some features of the features in the binary format. { y\u200a\u2014\u200a0: Not buying a phone and y\u200a\u2014\u200a1: buying a phone}\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL4.png?raw=true'>\n\n                                        Fig\u200a\u2014\u200a4: Buying a phone\nFor each particular phone (observation) with a certain threshold value **b**, using the MP Neuron Model, we can predict the outcome using a condition that the summation of the inputs is greater than b then the predicted value will be 1 or else it will be 0. The loss for the particular observation will be squared difference between the Yactual and Ypredicted.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL5.png?raw=true'>\n\n                            Fig\u200a\u2014\u200a5: MP Neuron Model for Buying a phone\nSimilarly, for all the observations, calculate the summation of the squared difference between the Yactual and Ypredicted to get the total loss of the model for a particular threshold value **b**.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL6.png?raw=true'>\n\n                                Fig\u200a\u2014\u200a6: Loss of the Model\n### Learning Algorithm\n\nThe purpose of the learning algorithm is to find out the best value for the parameter **b** so that the loss of the model will be minimum. In the ideal scenario, the loss of the model for the best value of **b** would be zero.\n\nFor n features in the data, the summation we are computing in Fig\u200a\u2014\u200a5 can take only values between 0 and n because all of our inputs are binary (0 or 1). 0\u200a\u2014\u200aindicates all the features described in the Fig\u200a\u2014\u200a4 are off and 1\u200a\u2014\u200aindicates all the features are on. Therefore the different values the threshold b can take will also vary from 0 to n. As we have only one parameter with a range of values 0 to n, we can use the brute force approach to find the best value of b.\n\nInitialize the b with a random integer [0,n]\nFor each observation\nFind the predicted outcome, by using the formula in Fig\u200a\u2014\u200a5\nCalculate the summation of inputs and check whether its greater than or equal to b. If its greater than or equal to b, then the predicted outcome will be 1 or else it will be 0.\n\nAfter finding the predicting outcome compute the loss for each observation.\nFinally, compute the total loss of the model by summing up all the individual losses.\nSimilarly, we can iterate over all the possible values of b and find the total loss of the model. Then we can choose the value of b, such that the loss is minimum.\n\n### Model Evaluation\n\nAfter finding the best threshold value b from the learning algorithm, we can evaluate the model on the test data by comparing the predicted outcome and the actual outcome.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL7.png?raw=true'>\n\n                    Fig\u200a\u2014\u200a7: Predictions on the test data for b = 5\nFor evaluation, we will calculate the accuracy score of the model.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/DL8.png?raw=true'>\n\n                            Fig\u200a\u2014\u200a8: Accuracy metric\nFor the above-shown test data, the accuracy of the MP neuron model = 75%.\n\n### Python Implementation of MP Neuron Model\n\nIn this section, we will see how to implement MP neuron model using python. The data set we will be using is breast cancer data set from sklearn. Before start building the MP Neuron Model. We will start by loading the data and separating the response and target variables.\n\nOnce we load the data, we can use the sklearn\u2019s train_test_split function to split the data into two parts\u200a\u2014\u200atraining and testing in the ratio of 80:20.","7631bf75":"IMPORTANT:\n\n1. There are two alternate ways to writing the perceptron function:\n    w2x2 + w1x1 >= b\n    w2x2 + w1x1 + w0 >=0\n\n2. By comparing the two expressions, it's easy to see that wo=-b\n\n3. We will now focus on the second form w2x2 + w1x1 + w0 >=0 or w2x2 +w1x1 + w0x0 >=0 where x0=1\n\n4. Where y_pred=0 and y=1, the update rule is \n\n    w = w+x, which expands to \n    w2 = w2 + x2\n    w1 = w1 + x1\n    w0 = w0 + x0 ==> w0 = w0 + 1 ==> -b = -b + 1 ==> b = b - 1\n   \n5. When y_pred = 1 and y = 0, the update rule is \n    w = w - x, which expands to \n    w2 = w2 - x2\n    w1= w1 - x1\n    w0 = w0 - x- ==> w0 = w0 - 1 ==> -b = -b - 1 ==> b = b + 1\n  \n6. Note the difference in signs for the updates of w(w1, w2) and b.\n"}}