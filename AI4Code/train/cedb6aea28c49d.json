{"cell_type":{"7f29d452":"code","b6f353c7":"code","5358de02":"code","4591b11b":"code","2209ad63":"code","8cf94a03":"code","3edf159e":"code","5d8990ca":"code","eaf9fb3c":"code","9e13f025":"code","b11d1621":"code","b9989ba8":"code","fa2c4a5a":"code","c84374bd":"code","801823cb":"code","ff25874e":"code","d92ee44d":"code","21be0ff2":"code","3db265e1":"code","067388ab":"code","8c891d7f":"code","bdf5a01d":"code","00b82b45":"code","afe264e0":"code","3d0c3894":"code","15d8dac3":"code","f5f5d646":"code","d3142f9a":"code","4be75e6b":"code","ab56850a":"code","b10f21a2":"code","ab79d28d":"code","706421cb":"code","b96d3a1d":"code","4d4c43aa":"code","25baea71":"code","751a722c":"code","fc5f2582":"code","7338146f":"code","fb38406c":"code","7e2fcd2f":"code","ca4e0406":"code","509db459":"code","494c6d42":"code","eef1382a":"code","35977137":"code","361c80ac":"code","f7fce0d0":"code","cbb796b0":"code","81f98fa2":"code","d16d39fb":"code","ec00d9ac":"markdown","eca1b841":"markdown","d27399e8":"markdown","7fa7c2e2":"markdown","effd8b52":"markdown","fbf44b79":"markdown","a21084c0":"markdown","1ccb5f20":"markdown","2fba4d72":"markdown","3e4e989f":"markdown","5fe397f6":"markdown","5c5e81fd":"markdown","ff8904a8":"markdown","aa16af34":"markdown","0814cb09":"markdown","8d4d3080":"markdown","34372542":"markdown"},"source":{"7f29d452":"import tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport shutil\nimport stat\nimport collections\nfrom collections import defaultdict\n\nfrom ipywidgets import interact, interactive, fixed\nimport ipywidgets as widgets\nimport random\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2","b6f353c7":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","5358de02":"%cd \/kaggle\/input\/food41\/","4591b11b":"# Helper function to download data and extract\ndef get_data_extract():\n  if \"food41\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http:\/\/data.vision.ee.ethz.ch\/cvl\/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")","2209ad63":"# Check the extracted dataset folder\n!ls \/kaggle\/input\/food41","8cf94a03":"path = '\/kaggle\/input\/food41\/images'\nos.listdir(path)","3edf159e":"os.listdir('\/kaggle\/input\/food41\/meta\/meta')","5d8990ca":"!head \/kaggle\/input\/food41\/meta\/meta\/train.txt","eaf9fb3c":"!head \/kaggle\/input\/food41\/meta\/meta\/classes.txt","9e13f025":"root_dir = '\/kaggle\/input\/food41\/images\/'\nrows = 17\ncols = 6\nfig, ax = plt.subplots(rows, cols, frameon=False, figsize=(15, 25))\nfig.suptitle('Random Image from Each Food Class', fontsize=20)\nsorted_food_dirs = sorted(os.listdir(root_dir))\nfor i in range(rows):\n    for j in range(cols):\n        try:\n            food_dir = sorted_food_dirs[i*cols + j]\n        except:\n            break\n        if food_dir == '.DS_Store':\n            continue\n        all_files = os.listdir(os.path.join(root_dir, food_dir))\n        rand_img = np.random.choice(all_files)\n        img = plt.imread(os.path.join(root_dir, food_dir, rand_img))\n        ax[i][j].imshow(img)\n        ec = (0, .6, .1)\n        fc = (0, .7, .2)\n        ax[i][j].text(0, -20, food_dir, size=10, rotation=0,\n                ha=\"left\", va=\"top\", \n                bbox=dict(boxstyle=\"round\", ec=ec, fc=fc))\nplt.setp(ax, xticks=[], yticks=[])\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","b11d1621":"#setup multiprocessing tool -> to accelerate image augmentation during training.\nimport multiprocessing as mp\n\nnum_processes = 6\npool = mp.Pool(processes=num_processes)","b9989ba8":"#map from class to index and vice versa, for proper label encoding\nclass_to_ix = {}\nix_to_class = {}\nwith open('\/kaggle\/input\/food41\/meta\/meta\/classes.txt', 'r') as txt:\n    classes = [l.strip() for l in txt.readlines()]\n    class_to_ix = dict(zip(classes, range(len(classes))))\n    ix_to_class = dict(zip(range(len(classes)), classes))\n    class_to_ix = {v: k for k, v in ix_to_class.items()}\nsorted_class_to_ix = collections.OrderedDict(sorted(class_to_ix.items()))","fa2c4a5a":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('\/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]: \n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","c84374bd":"# Prepare train dataset by copying images from food-101\/images to food-101\/train using the file train.txt\n%cd \/\nprint(\"Creating train data...\")\nprepare_data('\/kaggle\/input\/food41\/meta\/meta\/train.txt', '\/kaggle\/input\/food41\/images', 'train')","801823cb":"# Prepare test data by copying images from food-101\/images to food-101\/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('\/kaggle\/input\/food41\/meta\/meta\/test.txt', '\/kaggle\/input\/food41\/images', 'test')","ff25874e":"# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c    ","d92ee44d":"# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","21be0ff2":"# List of all 101 types of foods(sorted alphabetically)\n# remove .DS_Store from the list\ndel sorted_food_dirs[0]","3db265e1":"sorted_food_dirs","067388ab":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest)\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ","8c891d7f":"# picking 10 food items and generating separate data folders for the same\nfood_list = ['apple_pie','chicken_curry', 'chicken_wings', 'omelette', 'poutine',\n             'grilled_salmon','tiramisu','spring_rolls','steak','strawberry_shortcake',\n             'sushi','tacos', 'red_velvet_cake','risotto','samosa', 'sashimi','bibimbap',\n             'breakfast_burrito','caesar_salad', 'pizza']\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'","bdf5a01d":"print(\"Creating train data folder with new classes\")\ndataset_mini(food_list, src_train, dest_train)","00b82b45":"print(\"Total number of samples in mini train folder\")\n!find train_mini -type d -or -type f -printf '.' | wc -c","afe264e0":"print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)","3d0c3894":"print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c","15d8dac3":"labels = os.listdir('train_mini')\nlen(labels)","f5f5d646":"#IMAGE AUGMENTATION\nK.clear_session()\nn_classes = 101\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train'\nvalidation_data_dir = 'test'\nnb_train_samples = 75750\nnb_validation_samples = 25250\nbatch_size = 64\n\ntrain_datagen = ImageDataGenerator(\n                 samplewise_center=False,\n                 featurewise_std_normalization=False,\n                 samplewise_std_normalization=False,\n                 zca_whitening=False,\n                 rotation_range=10,\n                 width_shift_range=0.05,\n                 height_shift_range=0.05,\n                 shear_range=0.1,\n                 zoom_range=0.2,\n                 channel_shift_range=0.,\n                 fill_mode='nearest',\n                 cval=0.,\n                 horizontal_flip=True,\n                 vertical_flip=False,\n                 rescale=1\/255)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","d3142f9a":"#MODEL TRAINING\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(101,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=32,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_3class.hdf5')\n","4be75e6b":"model.summary() ","ab56850a":"class_map_101 = train_generator.class_indices \nclass_map_101","b10f21a2":"#print(\" loss: 0.4156 , acc: 0.8762 , val_loss: 0.2803 , val_acc: 0.9267\")","ab79d28d":"#Visualize the accuracy and loss plots\ndef plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","706421cb":"plot_accuracy(history,'FOOD101-Inceptionv3')\nplot_loss(history,'FOOD101-Inceptionv3')","b96d3a1d":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_3class.hdf5',compile = False)","4d4c43aa":"# save the model to disk\nimport pickle\nfilename = '.\/finalized_model.save'\npickle.dump(model, open(filename, 'wb')) ","25baea71":"# save the best model to disk\nfilenamebest = 'finalized_bestmodel.sav'\npickle.dump(model_best, open(filenamebest, 'wb'))","751a722c":"def predict_class(model_best, img, show = True):\n#   for img in images:\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255.                                      \n\n    pred = model_best.predict(img)\n    index = np.argmax(pred)\n    food_list.sort()\n    pred_value = food_list[index]\n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.title(pred_value, fontsize=20, backgroundcolor='green', color=\"white\")\n        plt.show()\n        \n    return pred_value","fc5f2582":"\"\"\"\n# Downloading images from internet using the URLs\n!wget -O spring_rolls.jpg https:\/\/c.ndtvimg.com\/2018-11\/olkdj03_spring-rolls_625x300_05_November_18.jpg\n!wget -O apple_pie.jpg https:\/\/www.biggerbolderbaking.com\/wp-content\/uploads\/2018\/11\/Apple-Pie-Thumbnail.png\n!wget -O tacos.jpg https:\/\/images-gmi-pmc.edge-generalmills.com\/e59f255c-7498-4b84-9c9d-e578bf5d88fc.jpg\n!wget -O sushi.jpg https:\/\/media-cdn.tripadvisor.com\/media\/photo-s\/19\/3b\/00\/06\/sushi-place.jpg\n\"\"\"","7338146f":"# Make a list of downloaded images and test the trained model\n\"\"\"\nimages = []\n\nimages.append('spring_rolls.jpg')\nimages.append('apple_pie.jpg')\nimages.append('tacos.jpg')\nimages.append('sushi.jpg')\n\npredict_class(model_best, images, True)\n\"\"\"","fb38406c":"#TESTING ON PIKKY IMAGE DATA\n!wget -O FD_73.jpg https:\/\/pikky-phase-1.s3.ap-south-1.amazonaws.com\/dish_images\/FD_73.jpg\n!wget -O FD_17.jpg https:\/\/pikky-phase-1.s3.ap-south-1.amazonaws.com\/dish_images\/FD_17.jpg\n!wget -O FD_38.jpg https:\/\/pikky-phase-1.s3.ap-south-1.amazonaws.com\/dish_images\/FD_38.jpg\n!wget -O FD_105.jpg https:\/\/pikky-phase-1.s3.ap-south-1.amazonaws.com\/dish_images\/FD_105.jpg\n!wget -O FD_75.jpg https:\/\/pikky-phase-1.s3.ap-south-1.amazonaws.com\/dish_images\/FD_75.jpg","7e2fcd2f":"# Make a list of downloaded images and test the trained model\nimages = []\n\nimages.append('FD_73.jpg')\nimages.append('FD_17.jpg')\nimages.append('FD_38.jpg')\nimages.append('FD_105.jpg')\nimages.append('FD_75.jpg')\n\nresult=[]\n\nfor i in images:\n    r = predict_class(model_best, i, True)\n    result.append(r)\n\nprint(result)","ca4e0406":"import pandas as pd\nsense_data = pd.read_excel('..\/input\/dish-profiles\/prototype_dishes_ingredients.xlsx')\nsense_data.head(15)","509db459":"sense_data.columns","494c6d42":"# Image Link and Ingredient\/Spices columns are not useful for this particular work, thus dropping them\n#sense_data = sense_data.drop(['ingredient\/Spices'], axis = 1)","eef1382a":"# Removing empty rows\nsense_data = sense_data[sense_data['dish_id'].notna()]\nsense_data = sense_data.reset_index(drop=True)","35977137":"# concatenate function to be used in groupby\ndef str_cat(x):\n    return x.str.cat(sep=', ')","361c80ac":"sense_data.columns","f7fce0d0":"df1 = sense_data.groupby(['dish_id', 'ingredient type']).agg({ 'taste': str_cat, 'smell': str_cat,\n                                                   'texture': str_cat, 'color': str_cat})\ndf1","cbb796b0":"# cleaning taste and smell column\n# making tags lowercase for keyword matching in database\n# extracting unique tags\n# final output is a list of tags\n\ndef clean_taste_smell():\n    column = ['taste', 'smell']\n    for col in column:\n        for i in range(0,len(df1[col])):\n            a = list(filter(None, [x.strip().lower() for x in df1[col][i].split(',')]))\n            df1[col][i] = pd.unique(a).tolist()","81f98fa2":"clean_taste_smell()","d16d39fb":"df1","ec00d9ac":"### **Understand dataset structure and files**","eca1b841":"Pikky Images Testing","d27399e8":"**meta** folder contains the text files - train.txt and test.txt  \n**train.txt** contains the list of images that belong to training set  \n**test.txt** contains the list of images that belong to test set  \n**classes.txt** contains the list of all classes of food","7fa7c2e2":"**Interactive Classification: Predicting classes for new images from internet using the best trained model**","effd8b52":"**The dataset being used is [Food 101](https:\/\/www.vision.ee.ethz.ch\/datasets_extra\/food-101\/)**\n* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**\n* **Each type of food has 750 training samples and 250 test samples**","fbf44b79":"### **Split the image data into train and test using train.txt and test.txt**","a21084c0":"### **Download and extract Food 101 Dataset**","1ccb5f20":"* Keras and other Deep Learning libraries provide pretrained models  \n* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n* This helps in faster convergance and saves time and computation when compared to models trained from scratch","2fba4d72":"### **Model Evalutaion and Testing**","3e4e989f":"### **Visualize random image from each of the 101 classes**","5fe397f6":"### **Fine tune Inception Pretrained model using Food 101 dataset**","5c5e81fd":"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**\n* **Validation accuracy has been on the higher side than training accuracy for many epochs**\n* **This could be for several reasons:**\n  * We used a pretrained model trained on ImageNet which contains data from a variety of classes\n  * Using dropout can lead to a higher validation accuracy\n\n \n","ff8904a8":"Google images Testing","aa16af34":"#### Color Detection of image","0814cb09":"####-----Obtaining Aromatic - Taste Tags of Dishes Identified-------####","8d4d3080":"food - non food  -> fine tuning","34372542":"**images** folder contains 101 folders with 1000 images  each  \nEach folder contains images of a specific food class"}}