{"cell_type":{"6838268c":"code","c7f921fb":"code","3ee12c7a":"code","c3f37016":"code","0b3310e4":"code","fb74c547":"code","a84d6bc2":"code","99eb3cd5":"code","23d41a94":"code","23cf4396":"code","a38f8c1c":"code","be47b924":"code","099013f9":"code","9660aac2":"code","3b38474a":"code","8b8b5b4a":"code","17d19f38":"code","9342d47a":"code","3c298961":"code","46160379":"code","99bf8c6a":"code","f1862429":"code","952f8f97":"code","a4d84c2a":"markdown","81ef2403":"markdown","fc37688a":"markdown","7de0d72e":"markdown","51fdc5af":"markdown","73b707ce":"markdown","84c4784f":"markdown","45d6f1f8":"markdown"},"source":{"6838268c":"import numpy as np\nimport pandas as pd\npd.set_option(\"max_colwidth\", 80)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","c7f921fb":"def count_duplicates(data: pd.Series) -> pd.DataFrame:\n    \"\"\" Get information about duplicates. \"\"\"\n    result = data.value_counts()\n    \n    result = pd.DataFrame({'count': result.values,\n                           'text': result.index})\n\n    result.index += 1\n    \n    return result\n\n\ndef toxic_rate(string: str) -> float:\n    \"\"\" Get a text toxicity score. \"\"\"\n    result = 0\n    \n    sid_score = sid.polarity_scores(string)\n    \n    neg_value = sid_score.get('neg')\n    \n    if neg_value:\n        result = neg_value\n    \n    return result","3ee12c7a":"validation_data_path = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\nvalid_data = pd.read_csv(validation_data_path)\nvalid_data.shape","c3f37016":"is_duplicate = valid_data.duplicated(subset=['less_toxic', 'more_toxic'])\n\nprint(\"Duplicates: {} %\".format(round(is_duplicate.mean() * 100, 2)))","0b3310e4":"print(\"With duplicates:   \", valid_data.shape[0])\nprint(\"Count duplicates:  \", valid_data[is_duplicate].shape[0])\nprint(\"Without duplicates:\", valid_data[~is_duplicate].shape[0])","fb74c547":"valid_data[is_duplicate].head(14)","a84d6bc2":"valid_data[is_duplicate &\n           valid_data['less_toxic'] \\\n                .str.contains('Straw poll being conducted on Catholic Church')]","99eb3cd5":"valid_data[~is_duplicate &\n           valid_data['less_toxic'] \\\n                .str.contains('Straw poll being conducted on Catholic Church')]","23d41a94":"n_joined = count_duplicates(valid_data['less_toxic'] \\\n                            + \" \"\n                            + valid_data['more_toxic'])\nn_joined","23cf4396":"n_joined['count'].value_counts()","a38f8c1c":"print(\"Duplicates: {} %\".format(\n                round(valid_data['less_toxic'].duplicated() \\\n                          .mean() * 100, 2)))","be47b924":"n_less_toxic = count_duplicates(valid_data['less_toxic'])\nn_less_toxic","099013f9":"valid_data[valid_data['less_toxic'].str.contains('How many sockpuppets do you have?')]","9660aac2":"n_less_toxic['count'].hist(bins=13, figsize=(12, 5));","3b38474a":"n_less_toxic['count'].value_counts(normalize=True) \\\n                        .mul(100).round(2).map(\"{} %\".format)","8b8b5b4a":"print(\"Duplicates: {} %\".format(\n                round(valid_data['more_toxic'].duplicated() \\\n                          .mean() * 100, 2)))","17d19f38":"n_more_toxic = count_duplicates(valid_data['more_toxic'])\nn_more_toxic","9342d47a":"valid_data[valid_data['more_toxic'].str.contains('ALL NIGHT BITCH')]","3c298961":"n_more_toxic['count'].hist(bins=14, figsize=(12, 5));","46160379":"n_more_toxic['count'].value_counts(normalize=True) \\\n                        .mul(100).round(2).map(\"{} %\".format)","99bf8c6a":"sid = SentimentIntensityAnalyzer()\nsid.polarity_scores(\"How many sockpuppets do you have?\")","f1862429":"%%time\ntoxic_data = valid_data.copy()\n\ntoxic_data['less_toxic'] = toxic_data['less_toxic'].apply(toxic_rate)\ntoxic_data['more_toxic'] = toxic_data['more_toxic'].apply(toxic_rate)\n\ntoxic_score = toxic_data.eval('less_toxic < more_toxic').mean()\n\nprint(\"\\nSCORE: {}\\n\".format(round(toxic_score, 5)))","952f8f97":"%%time\ntoxic_data = valid_data[~is_duplicate].copy()\n\ntoxic_data['less_toxic'] = toxic_data['less_toxic'].apply(toxic_rate)\ntoxic_data['more_toxic'] = toxic_data['more_toxic'].apply(toxic_rate)\n\ntoxic_score = toxic_data.eval('less_toxic < more_toxic').mean()\n\nprint(\"\\nSCORE: {}\\n\".format(round(toxic_score, 5)))","a4d84c2a":"# 1. Duplicates in rows","81ef2403":"## 2.2. Column 'more_toxic'","fc37688a":"# 2. Duplicates in columns","7de0d72e":"# 3. Check score with\/without duplicates\n\n### Sample usage for sentiment\n\n> from nltk.sentiment.vader import SentimentIntensityAnalyzer  \n\nhttps:\/\/www.nltk.org\/howto\/sentiment.html","51fdc5af":"# Import & Def & Load","73b707ce":"## 3.1. Score with duplicates","84c4784f":"## 2.1. Column 'less_toxic'","45d6f1f8":"## 3.2. Score without duplicates"}}