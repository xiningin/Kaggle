{"cell_type":{"46a8ded2":"code","e10a2852":"code","8b09418c":"code","01922c45":"code","f402b3e0":"code","6f5f35fa":"code","9f7a2137":"code","092671f3":"code","9b95d920":"code","c1f77948":"code","c977d5a9":"code","79fb4350":"code","9c10ae74":"code","a82f41c2":"code","df93c1ef":"code","69024001":"code","7fc713ae":"code","36bb23a4":"code","918af0da":"code","43fce9ef":"code","9b9aae57":"code","6cb5a718":"code","af718d4a":"code","72fe4102":"code","a9ad5df5":"code","fe1a89b0":"code","1f8e2055":"code","639cf257":"code","ca1717a1":"code","705d7c5d":"code","207c6c5a":"code","959dac92":"code","6ff0a8f1":"markdown","235863ce":"markdown","fb117fa6":"markdown","f8e0a7d0":"markdown","bedbd55a":"markdown","48629df4":"markdown","f4edbb93":"markdown","a9e920c9":"markdown","4efead0c":"markdown","caccf589":"markdown","b9de5ee9":"markdown","76178536":"markdown","576da8da":"markdown","0e73b671":"markdown","a4eb5263":"markdown","95a1db51":"markdown","23701038":"markdown","00c0bfe7":"markdown","019b8a41":"markdown"},"source":{"46a8ded2":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, GridSearchCV, train_test_split\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e10a2852":"train = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\", low_memory=False)#, keep_default_na=False)\ntest = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\", low_memory=False)\ntrain.info()","8b09418c":"test.info()","01922c45":"train.head()","f402b3e0":"train.describe()","6f5f35fa":"test.describe()","9f7a2137":"fig, ax = plt.subplots(figsize=(16, 8))\nbars = ax.bar(train.isna().sum().index, train.isna().sum().values,\n              color=\"cornflowerblue\", edgecolor=\"black\")\nax.set_title(\"Missing values in the whole train dataset\", fontsize=20, pad=15)\nax.set_xlabel(\"Feature names\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values missing\", fontsize=15, labelpad=15)\nax.set_yticks(np.arange(0, 1600, 100))\nax.set_xticks(ax.get_xticks())\nax.set_xticklabels(train.isna().sum().index, fontsize=10, rotation = 90)\nax.tick_params(axis=\"y\", labelsize=15)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","092671f3":"fig, ax = plt.subplots(figsize=(16, 8))\nbars = ax.bar(test.isna().sum().index, test.isna().sum().values,\n              color=\"mediumpurple\", edgecolor=\"black\")\nax.set_title(\"Missing values in the whole test dataset\", fontsize=20, pad=15)\nax.set_xlabel(\"Feature names\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values missing\", fontsize=15, labelpad=15)\nax.set_yticks(np.arange(0, 1600, 100))\nax.set_xticks(ax.get_xticks())\nax.set_xticklabels(test.isna().sum().index, fontsize=10, rotation = 90)\nax.tick_params(axis=\"y\", labelsize=15)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","9b95d920":"fig, ax = plt.subplots(figsize=(16, 8))\nbars = ax.bar(train.loc[:, train.isna().sum()>0].isna().sum().index,\n              train.loc[:, train.isna().sum()>0].isna().sum().values,\n              color=\"lightsalmon\", edgecolor=\"black\")\nax.set_title(\"Features with missing values in the train dataset\", fontsize=20, pad=15)\nax.set_xlabel(\"Feature with missing values names\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values missing\", fontsize=15, labelpad=15)\nax.set_yticks(np.arange(0, 1600, 100))\nax.set_xticks(ax.get_xticks())\nax.set_xticklabels(train.loc[:, train.isna().sum()>0].isna().sum().index,\n                   fontsize=12, rotation = 60, ha=\"right\", rotation_mode='anchor')\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","c1f77948":"fig, ax = plt.subplots(figsize=(16, 8))\nbars = ax.bar(test.loc[:, test.isna().sum()>0].isna().sum().index,\n              test.loc[:, test.isna().sum()>0].isna().sum().values,\n              color=\"mediumturquoise\", edgecolor=\"black\")\nax.set_title(\"Features with missing values in the test dataset\", fontsize=20, pad=15)\nax.set_xlabel(\"Feature with missing values names\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values missing\", fontsize=15, labelpad=15)\nax.set_yticks(np.arange(0, 1600, 100))\nax.set_xticks(ax.get_xticks())\nax.set_xticklabels(test.loc[:, test.isna().sum()>0].isna().sum().index,\n                   fontsize=12, rotation = 60, ha=\"right\", rotation_mode='anchor')\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","c977d5a9":"# Checking for NaN values in both datasets\npd.concat([train.drop(\"SalePrice\", axis=1).isna().sum(), test.isna().sum()], axis=1).T","79fb4350":"x = -1*np.arange(len(test.drop([\"Id\"], axis=1).columns))\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars = ax.barh(x+0.2, train.drop([\"Id\", \"SalePrice\"], axis=1).nunique().values,\n               height=0.4, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.barh(x-0.2, test.drop([\"Id\"], axis=1).nunique().values,\n                height=0.4, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Unique values in train and test datasets\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Amount of unique values\", fontsize=20, labelpad=15)\nax.set_xticks(np.arange(0, 1600, 100))\nax.set_yticks(x)\nax.set_yticklabels(list(test.drop([\"Id\"], axis=1).columns.values))\nax.tick_params(axis=\"x\", labelsize=15)\nax.tick_params(axis=\"y\", labelsize=14)\nax.grid(axis=\"x\")\nax.legend(fontsize=15)\nax2 = ax.secondary_xaxis('top')\nax2.set_xticks(np.arange(0, 1600, 100))\nax2.set_xlabel(\"Amount of unique values\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.05, 0.01)","9c10ae74":"# A list of features which I think are categorical\ncat_features = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LotShape\", \"LandContour\",\n                \"Utilities\", \"LotConfig\", \"LandSlope\", \"Neighborhood\", \"Condition1\",\n                \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"Exterior1st\",\n                \"Exterior2nd\", \"MasVnrType\", \"ExterQual\", \"ExterCond\", \"Foundation\",\n                \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n                \"Heating\", \"HeatingQC\", \"CentralAir\", \"Electrical\", \"KitchenQual\",\n                \"Functional\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\",\n                \"GarageCond\", \"PavedDrive\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MoSold\", \n                \"YrSold\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"SaleType\",\n                \"SaleCondition\", \"OverallQual\", \"OverallCond\", \"RoofMatl\"]\n\n# A list of features which I think are numerical\nnum_features = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\",\n                \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\",\n                \"LowQualFinSF\",\"GrLivArea\", \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\",\n                \"HalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n                \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n                \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\"]\n\n# A list of features which will have their NaN values replaced by \"NA\" string\nfill_NA = [\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtFinType1\", \n           \"BsmtFinType2\", \"BsmtExposure\", \"BsmtQual\", \"BsmtCond\", \"FireplaceQu\",\n           \"Alley\", \"MasVnrType\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MSZoning\",\n           \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"KitchenQual\", \"Functional\",\n           \"SaleType\"]\n\n# A list of features which will have their NaN values replaced by zeros\nfill_zeros = [\"GarageYrBlt\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n              \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"GarageCars\", \"GarageArea\"]","a82f41c2":"cols = 3\nrows = len(cat_features) \/\/ cols + 1\n\n# Decades intervals and corresponding labels for year related features \ndecades = [-np.inf, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, np.inf]\ndecades_labels = [\"pre 1900\", \"1900s\", \"1910s\", \"1920s\", \"1930s\", \"1940s\", \"1950s\",\n                  \"1960s\", \"1970s\", \"1980s\", \"1990s\", \"2000s\",]\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,110), sharex=False)\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\ni=0\n\ndf = pd.concat([train.drop(\"SalePrice\", axis=1), test], axis=0)\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features):\n            axs[r, c].set_visible(False)\n        else:\n            # Not a year related feature\n            if cat_features[i] not in [\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"]:\n                values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n                bars_pos = np.arange(0, len(values))\n                if len(values)<4:\n                    height=0.1\n                else:\n                    height=0.3\n\n                bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                       [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                       height=height,\n                                       color=\"teal\",\n                                       edgecolor=\"black\",\n                                       label=\"Train Dataset\")\n                bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                       [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                       height=height,\n                                       color=\"salmon\",\n                                       edgecolor=\"black\",\n                                       label=\"Test Dataset\")\n                y_labels = [str(x) for x in values]\n            # For \"YearBuilt\", \"YearRemodAdd\" and \"GarageYrBlt\" features I need to group \n            # years into decades in order to make plots more readable\n            else: \n                height=0.3\n                bars_pos = np.arange(0, len(decades_labels))\n                bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                       pd.cut(train[cat_features[i]], decades, right=False).value_counts().sort_index(ascending=True).values,\n                                       height=height,\n                                       color=\"teal\",\n                                       edgecolor=\"black\",\n                                       label=\"Train Dataset\")\n                bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                       pd.cut(test[cat_features[i]], decades, right=False).value_counts().sort_index(ascending=True).values,\n                                       height=height,\n                                       color=\"salmon\",\n                                       edgecolor=\"black\",\n                                       label=\"Test Dataset\")  \n                y_labels = decades_labels\n                \n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"Id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical values distribution in both datasets\", y=0.99)\nplt.show();","df93c1ef":"cols = 3\nrows = len(num_features) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,50), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\ndf = pd.concat([train.drop(\"SalePrice\", axis=1), test], axis=0)\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(num_features):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[num_features[i]].values,\n                                   range=(df[num_features[i]].min(),\n                                          df[num_features[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[num_features[i]].values,\n                                   range=(df[num_features[i]].min(),\n                                          df[num_features[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(num_features[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\nplt.show();\n# plt.suptitle(\"Numerical values distribution in both datasets\", y=0.99)","69024001":"test[test[\"GarageYrBlt\"] > 2010][\"GarageYrBlt\"]","7fc713ae":"test.loc[test[\"GarageYrBlt\"] > 2010, \"GarageYrBlt\"] = 2007","36bb23a4":"cols = 3\nrows = len(cat_features) \/\/ cols + 1\n\ndecades = [-np.inf, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, np.inf]\ndecades_labels = [\"pre 1900\", \"1900s\", \"1910s\", \"1920s\", \"1930s\", \"1940s\", \"1950s\",\n                  \"1960s\", \"1970s\", \"1980s\", \"1990s\", \"2000s\",]\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(20,110), sharex=False)\nplt.subplots_adjust(hspace = 0.15)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features):\n            axs[r, c].set_visible(False)\n        else:\n            # Not a year related feature\n            if cat_features[i] not in [\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"]:\n                bars_pos = np.arange(0, train[cat_features[i]].nunique())\n                if len(bars_pos)<4:\n                    height=0.1\n                else:\n                    height=0.3            \n            \n                bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                       train.groupby(cat_features[i])[\"SalePrice\"].mean(),\n                                       height=height,\n                                       color=\"darkorange\",\n                                       edgecolor=\"black\",\n                                       label=\"Mean price\")\n                bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                       train.groupby(cat_features[i])[\"SalePrice\"].median(),\n                                       height=height,\n                                       color=\"steelblue\",\n                                       edgecolor=\"black\",\n                                       label=\"Median price\")\n                y_labels = [str(x) for x in train.groupby(cat_features[i])[\"SalePrice\"].mean().index]\n                \n            # For \"YearBuilt\", \"YearRemodAdd\" and \"GarageYrBlt\" features I need to group \n            # years into decades in order to make plots more readable            \n            else:\n                height=0.3\n                bars_pos = np.arange(0, len(decades_labels))\n                values = pd.concat([pd.cut(train[cat_features[i]], decades, right=False), train[\"SalePrice\"]], axis=1).groupby(cat_features[i])[\"SalePrice\"]\n                bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                       values.mean(),\n                                       height=height,\n                                       color=\"darkorange\",\n                                       edgecolor=\"black\",\n                                       label=\"Mean price\")\n                bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                       values.median(),\n                                       height=height,\n                                       color=\"steelblue\",\n                                       edgecolor=\"black\",\n                                       label=\"Median price\")                \n                \n                y_labels = decades_labels\n                \n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].set_xticks(axs[r, c].get_xticks())\n            axs[r, c].set_xticklabels([str(int(x\/1000))+\"k\" for x in axs[r, c].get_xticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=13)\n            axs[r, c].margins(0.1, 0.02)\n\n        i+=1\n\n# plt.suptitle(\"Mean and medean prices distribution per categorical feature\", y=0.99)","918af0da":"cols = 3\nrows = len(num_features) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,50), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(num_features):\n            axs[r, c].set_visible(False)\n        else:\n            axs[r, c].scatter(train[num_features[i]].values,\n                              train[\"SalePrice\"],\n                              color=\"mediumpurple\")\n            axs[r, c].set_title(num_features[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(x\/1000))+\"k\" for x in axs[r, c].get_yticks()])\n            axs[r, c].grid(axis=\"y\")\n                                  \n        i+=1\nplt.show();\n# plt.suptitle(\"Numerical features correlation with sale price\", y=0.99)","43fce9ef":"# Train and test data preparation functions\n\ndef prepare_train_data(df, cat_features, num_features, fill_NA, fill_zeros, to_drop):\n    \n    df = df.drop(to_drop, axis=1)\n        \n    # Filling NaN values with \"NA\" string\n    for column in fill_NA:\n        df[column].fillna(\"NA\", inplace=True)\n        \n    # Filling NaN values with zeros\n    for column in fill_zeros:\n        df[column].fillna(value=0, inplace=True)\n    \n    # Filling NaN values with different values basing on feature description \n    df[\"Electrical\"].fillna(value=\"SBrkr\", inplace=True)\n    df[\"LotFrontage\"].fillna(value=df[\"LotFrontage\"].median(), inplace=True)\n    \n    # Replacing for unification\n    df[\"MasVnrType\"].replace(\"None\", \"NA\", inplace=True)\n    \n    # Adding additional features\n    df[\"Was_remodeled\"] = df[\"YearBuilt\"]!=df[\"YearRemodAdd\"]\n    df[\"House_age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n    df[\"House_remodel_age\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    df[\"Garage_age\"] = df[\"YrSold\"] - df[\"GarageYrBlt\"]\n    \n    # Adding two new features with amount of NaN and zero values per row\n#     df[\"NA_count\"] = (df==\"NA\").sum(axis=1)\n#     df[\"Zeroes_count\"] = (df==0).sum(axis=1)\n    \n    # Combining date features into 10-year intervals. It helped to improve the score.\n    decades = [-np.inf, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, np.inf]\n    for column in [\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"]:\n        df[column] = pd.cut(df[column], decades, right=False)\n    \n    # Encoding categorical features and adding each trained encoder to a list so it could be used to transform test data   \n    encoders = []\n    for column in cat_features:\n        encoder = LabelEncoder()\n        df[column] = encoder.fit_transform(df[column])\n        encoders.append(encoder)\n    \n    # Scaling numerical features and adding each trained scaler to a list so it could be used to transform test data   \n    scalers = []\n    for column in num_features:\n        scaler = MinMaxScaler()\n        df[column] = scaler.fit_transform(np.array(df[column]).reshape(-1, 1))\n        scalers.append(scaler)\n        \n    return df, encoders, scalers\n        \n        \ndef prepare_test_data(df, cat_features, num_features, fill_NA, \n                      encoders, scalers, fill_zeros, to_drop):\n    \n    df.drop(to_drop, axis=1, inplace=True)\n\n    # Filling NaN values with \"NA\" string    \n    for column in fill_NA:\n        df[column].fillna(\"NA\", inplace=True)\n        \n    # Filling NaN values with zeros\n    for column in fill_zeros:\n        df[column].fillna(value=0, inplace=True)\n        \n    # Filling NaN values with different values basing on feature description         \n    df[\"Electrical\"].fillna(value=\"SBrkr\", inplace=True)\n    df[\"LotFrontage\"].fillna(value=df[\"LotFrontage\"].median(), inplace=True)\n    \n    # Replacing for unification\n    df[\"MasVnrType\"].replace(\"None\", \"NA\", inplace=True)   \n    \n    # Adding additional features\n    df[\"Was_remodeled\"] = df[\"YearBuilt\"]!=df[\"YearRemodAdd\"]\n    df[\"House_age\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n    df[\"House_remodel_age\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n    df[\"Garage_age\"] = df[\"YrSold\"] - df[\"GarageYrBlt\"]\n    \n    # Adding two new features with amount of NaN and zero values per row\n#     df[\"NA_count\"] = (df==\"NA\").sum(axis=1)\n#     df[\"Zeroes_count\"] = (df==0).sum(axis=1)\n    \n    # Combining date features into 10-year intervals. It helped to improve the score.\n    decades = [-np.inf, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, np.inf]\n    for column in [\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"]:\n        df[column] = pd.cut(df[column], decades, right=False)    \n        \n    for i, column in enumerate(cat_features):\n        \n        # Getting an encoder from trained encoders list\n        encoder = encoders[i]\n        \n        # Creating a temporary encoder and fitting it to the test data \n        temp_encoder = LabelEncoder()\n        temp_encoder.fit(df[column])\n        # Checking if test data has some categories missing in train data\n        for element in temp_encoder.classes_:\n            if element not in encoder.classes_:\n                # Adding previously unseen categories to a trained encoder dictionary\n                # in order to avoid errors during transformation\n                encoder.classes_ = np.append(encoder.classes_, element)\n        \n        # Transforming test data using an encoder with updated categories dictionary\n        df[column] = encoder.transform(df[column])\n        \n    # Getting trained scalers from a list and using it to transform test data     \n    for i, column in enumerate(num_features):\n        scaler = scalers[i]\n        df[column] = scaler.transform(np.array(df[column]).reshape(-1, 1))\n        \n    return df","9b9aae57":"# Adding names of new features to categorical and numerical features lists\ncat_features.append(\"Was_remodeled\")\nnum_features.append(\"House_age\")\nnum_features.append(\"House_remodel_age\")\nnum_features.append(\"Garage_age\")\n# num_features.append(\"NA_count\")\n# num_features.append(\"Zeroes_count\")\n\n# A list of features to be dropped from the datasets because they have too many NaN values. It helped to imporve the score. \nto_drop = [\"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n\nprint(f\"Cat_features length before dropping features: {len(cat_features)}\")\nprint(f\"Num_features length before dropping features: {len(num_features)}\")\nprint(f\"Fill_zeros length before dropping features: {len(fill_zeros)}\")\nprint(f\"Fill_NA length before dropping features: {len(fill_NA)}\")\n\n# Removing feature names from the feature lists\nfor column in to_drop:\n    if column in cat_features:\n        cat_features.remove(column)\n    else:\n        num_features.remove(column)\n    if column in fill_zeros:\n        fill_zeros.remove(column)\n    elif column in fill_NA:\n        fill_NA.remove(column)\n\nprint(f\"Cat_features length after dropping features: {len(cat_features)}\")\nprint(f\"Num_features length after dropping features: {len(num_features)}\")\nprint(f\"Fill_zeros length after dropping features: {len(fill_zeros)}\")\nprint(f\"Fill_NA length after dropping features: {len(fill_NA)}\")","6cb5a718":"# Calling the functions to transform train and test data sets\ntrain_data, encoders, scalers = prepare_train_data(train.drop([\"SalePrice\", \"Id\"], axis=1),\n                                                   cat_features, num_features, fill_NA,\n                                                   fill_zeros, to_drop)\n\ndisplay(train_data.head())\n\ntest_data = prepare_test_data(test.drop(\"Id\", axis=1), cat_features,\n                              num_features, fill_NA, encoders, scalers, fill_zeros, to_drop)","af718d4a":"# Checking for NaN values in both datasets after transformation\ndf = pd.concat([train_data.isna().sum(), test_data.isna().sum()], axis=1)\ndf.sum()","72fe4102":"X_train = train_data\ny_train = train[\"SalePrice\"]","a9ad5df5":"# %%time\n\n# # Cross validation grid search to find best hyperparameters combination\n# model = RandomForestRegressor()\n\n# param_grid = {\n#               \"n_estimators\": [10, 50, 100, 250, 500, 1000],\n#               \"criterion\": [\"mse\"],\n#               \"max_depth\": [None, 2, 5, 10, 20],\n#               \"min_samples_split\": [1, 2, 4, 8],\n#               \"min_samples_leaf\": [1, 2, 4, 8],\n#               \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n#               \"n_jobs\": [1],\n#               \"random_state\": [42]\n#                 }\n\n# grid = GridSearchCV(model, param_grid, cv=4, n_jobs=-1, verbose=1,\n#                     scoring=\"neg_root_mean_squared_error\")\n# grid.fit(X_train, y_train)\n\n# print(f\"Best score is {-1*grid.best_score_}\")\n# print(f\"Mean fit time is {np.mean(grid.cv_results_['mean_fit_time']):.2f} sec\")\n# print(f\"Best parameters combination is:\\n{grid.best_params_}\")","fe1a89b0":"# %%time\n\n# # Cross validation grid search to find best hyperparameters combination\n# model = CatBoostRegressor()\n\n# # CatBoostRegressor has a lot of parameters. Below are only some of them. \n# param_grid = {\n#               \"iterations\": [200, 500, 1000],\n#               \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1],\n#               \"min_data_in_leaf\": [1, 3, 5],\n#               \"thread_count\": [1],\n#               \"max_ctr_complexity\": [1, 3, 5],\n#               \"depth\": [2, 4, 6, 8], \n#               \"verbose\": [0]\n#              }\n\n# grid = GridSearchCV(model, param_grid, cv=4, n_jobs=-1, verbose=1,\n#                     scoring=\"neg_root_mean_squared_error\")\n# grid.fit(X_train, y_train)\n\n# print(f\"Best score is {-1*grid.best_score_}\")\n# print(f\"Mean fit time is {np.mean(grid.cv_results_['mean_fit_time']):.2f} sec\")\n# print(f\"Best parameters combination is:\\n{grid.best_params_}\")","1f8e2055":"# A function to train all base-models and the blender\ndef train_estimators(models, blender, X_train, y_train):\n    trained_models = []\n    cols = [type(x).__name__ for x in models]\n    preds = pd.DataFrame(columns=cols)\n    \n    for model in models:\n        model.fit(X_train, y_train)\n        preds[str(type(model).__name__)] = model.predict(X_train)\n        trained_models.append(model)\n    \n    \n    blender.fit(preds, y_train)\n    preds[\"blender\"] = blender.predict(preds)\n    \n    return trained_models, blender\n\n# A function to get predictions by the base-models and the blender\ndef get_predictions(X_test, trained_models, blender):\n    cols = [type(x).__name__ for x in models]\n    preds = pd.DataFrame(columns=cols)\n    \n    for model in models:\n        preds[str(type(model).__name__)] = model.predict(X_test)\n    preds[\"blender\"] = blender.predict(preds)   \n\n    return preds[\"blender\"]","639cf257":"# Base-models that will be trained using the train dataset. \n# The hyperparameters were taken from GridSearchCV (RandomForest) and Optuna (CatBoost) above.\n\nmodels = [\n          RandomForestRegressor(criterion='mse',\n                                max_depth=20,\n                                max_features='sqrt',\n                                min_samples_leaf=1,\n                                min_samples_split=4,\n                                n_estimators=250,\n                                n_jobs=-1,\n                                random_state=42),\n          CatBoostRegressor(depth=4,\n                            iterations=1000,\n                            learning_rate=0.1,\n                            max_ctr_complexity=1,\n                            min_data_in_leaf=1,\n                            thread_count=4,\n                            verbose=0)\n          ]\n\n# A meta-model which will be trained on base-models' predictions. \nblender = Ridge(random_state=42)\n\nmodels, blender = train_estimators(models, blender, X_train, y_train)\n\npreds = get_predictions(test_data, models, blender)","ca1717a1":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = train_data.columns\ndf[\"Importance\"] = models[0].feature_importances_\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(9, 7))\nbars = ax.barh(df[\"Feature\"][:10], df[\"Importance\"][:10],\n               color=\"lightcoral\", edgecolor=\"black\")\nax.set_title(\"Top 10 important features of RandomForest model\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels(df[\"Feature\"][:10], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nplt.margins(0.04, 0.04)\nplt.gca().invert_yaxis()","705d7c5d":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = train_data.columns\ndf[\"Importance\"] = models[1].feature_importances_ \/ 100\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(9, 7))\nbars = ax.barh(df[\"Feature\"][:10], df[\"Importance\"][:10],\n               color=\"lightskyblue\", edgecolor=\"black\")\nax.set_title(\"Top 10 important features of CatBoost model\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels(df[\"Feature\"][:10], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nplt.margins(0.04, 0.04)\nplt.gca().invert_yaxis()\n","207c6c5a":"df = pd.DataFrame(columns=[\"Feature\", \"Importance_rf\", \"Importance_cb\"])\ndf[\"Feature\"] = train_data.columns\ndf[\"Importance_rf\"] = models[0].feature_importances_\ndf[\"Importance_cb\"] = models[1].feature_importances_ \/ 100\ndf.sort_values(\"Importance_rf\", axis=0, ascending=False, inplace=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(18, 30))\nbars1 = ax.barh(x-height\/2, df[\"Importance_rf\"], height=height,\n                color=\"cornflowerblue\", edgecolor=\"black\", label=\"RandomForest\")\nbars2 = ax.barh(x+height\/2, df[\"Importance_cb\"], height=height,\n                color=\"palevioletred\", edgecolor=\"black\", label=\"CatBoost\")\nax.set_title(\"Feature importances of RandomForest and CatBoost models\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\n# ax.set_xlim(0, 0.25)\n# ax.set_xticks(np.arange(0, 0.275, 0.025))\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n# ax2.set_xlim(0, 0.25)\n# ax2.set_xticks(np.arange(0, 0.275, 0.025))\nax2.tick_params(axis=\"x\", labelsize=15)\nax.legend(fontsize=15, loc=1, bbox_to_anchor=(0, 0, 1, 0.97))\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()\n","959dac92":"predictions = pd.DataFrame()\npredictions[\"Id\"] = test[\"Id\"]\npredictions[\"SalePrice\"] = preds\npredictions.to_csv('submission.csv', index=False,\n                   header=[\"Id\", \"SalePrice\"],\n                   columns=[\"Id\", \"SalePrice\"])\npredictions.head()","6ff0a8f1":"Let's see how many unique values each feature has.","235863ce":"# **Data Import**","fb117fa6":"The code below is commented in order to save runtime.","f8e0a7d0":"## CatBoostRegressor hyperparameters tuning","bedbd55a":"# **Machine Learning**","48629df4":"## Blending ensemble","f4edbb93":"Let's cut features without any missing values.","a9e920c9":"There should probably be 2007 value instead of 2207. Lets fix it.","4efead0c":"So the test dataset has more missing values than the train one. The other way to quickly check the difference in missing values between two datasets is shown below.","caccf589":"# **Data Analysis**","b9de5ee9":"As you can see on a last plot we have a home with a garage built in the future in our test dataset. Nice.","76178536":"The code below is commented in order to save runtime.","576da8da":"# **Data Preparation**","0e73b671":"Lets check if there is any observable corellation between sale price and categorical values.","a4eb5263":"Exploring each feature individually I devided them into categorical and numerical. It also helped me to determine which values I should use to replace missing values. Below are lists showing results of that.","95a1db51":"Let's check missing values first in the both train and test datasets.","23701038":"## RandomForestRegressor hyperparameters tuning","00c0bfe7":"Below there are two functions defined to prepare train and test data to be used by estimators. For as small dataset as this you can encode, scale transform both datasets together (i.e. work with preliminarly combined dataset). However working with large datasets this way concidering 16 GB Keggle RAM limit may be a problem. And this is where separate handling of train and test data sets comes in handy.","019b8a41":"Below are plots showing values distribution for categorical and numerical features."}}