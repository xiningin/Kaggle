{"cell_type":{"d5ed7b95":"code","f09a9e27":"code","f629c7b9":"code","6c20e037":"code","21ef22cb":"code","60a00209":"code","10639487":"code","b73c58cf":"code","118f341b":"code","08548d41":"code","f0113e78":"code","c17e4499":"markdown","0029a7b5":"markdown","ecec8b23":"markdown","b6064606":"markdown","fbfa11b1":"markdown","e435de0b":"markdown","ee10bac6":"markdown","78a3560f":"markdown","de19e3dc":"markdown","bb0491ab":"markdown","4fb61fff":"markdown","4bceac75":"markdown","1fcc474b":"markdown","56096baf":"markdown"},"source":{"d5ed7b95":"import os\nos.environ['DGLBACKEND'] = 'tensorflow'\n\n!pip -q install dgl-cu101\n!pip -q install tfdlpack-gpu  # when using tensorflow gpu version\n!export TF_FORCE_GPU_ALLOW_GROWTH=true # and add this to your .bashrc\/.zshrc file if needed\n\nimport dgl\nprint(dgl.backend.backend_name)\n\n","f09a9e27":"# common imports\nimport time\nimport numpy as np\nimport networkx as nx\nimport requests\n\n# tf part\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# dgl part\nfrom dgl import DGLGraph\nfrom dgl.data import register_data_args, load_data","f629c7b9":"class available_datasets_already_available_with_dgl:\n    def __init__(self, dataset_name):\n        self.dataset = dataset_name\n\ndata = load_data(available_datasets_already_available_with_dgl('citeseer'))","6c20e037":"features = tf.convert_to_tensor(data.features, dtype=tf.float32)\nlabels = tf.convert_to_tensor(data.labels, dtype=tf.int64)\ntrain_mask = tf.convert_to_tensor(data.train_mask, dtype=tf.bool)\nval_mask = tf.convert_to_tensor(data.val_mask, dtype=tf.bool)\ntest_mask = tf.convert_to_tensor(data.test_mask, dtype=tf.bool)\n\nin_feats = features.shape[1]\nn_classes = data.num_labels\nn_edges = data.graph.number_of_edges()\n\n\nprint(\"\"\"----Data statistics------'\n#Edges %d\n#Classes %d\n#Train samples %d\n#Val samples %d\n#Test samples %d\"\"\" %\n      (n_edges, n_classes,\n       train_mask.numpy().sum(),\n       val_mask.numpy().sum(),\n       test_mask.numpy().sum()))","21ef22cb":"# graph preprocess and calculate normalization factor\ng = data.graph\ng.remove_edges_from(nx.selfloop_edges(g))\ng.add_edges_from(zip(g.nodes(), g.nodes()))\n\ng = DGLGraph(g)\n\n\n# nx.draw(g.to_networkx())  # you can draw it better than me. I am useless","60a00209":"\nn_edges = g.number_of_edges()\n\ndegs = tf.cast(tf.identity(g.in_degrees()), dtype=tf.float32)\nnorm = tf.math.pow(degs, -0.5)\nnorm = tf.where(tf.math.is_inf(norm), tf.zeros_like(norm), norm)\ng.ndata['norm'] = tf.expand_dims(norm, -1)\n","10639487":"class GraphConv(layers.Layer):\n    def __init__(self,\n                out_feats):\n        super(GraphConv, self).__init__()\n        \n        # a very simple dense layer takes node features as input and outputs a lower dimension representation\n        self.denselayer = layers.Dense(out_feats, use_bias=False)\n\n    def call(self, graph, feat):\n        # make a local copy of the graph -- something related to not changing the global variable graph, I dont really care...\n        graph = graph.local_var()\n\n        feat = self.denselayer(feat)\n        graph.ndata['h'] = feat\n        graph.update_all(dgl.function.copy_src(src='h', out='m'),\n                         dgl.function.sum(msg='m', out='h'))\n        rst = graph.ndata['h']\n\n        return rst\n","b73c58cf":"class GCN(tf.keras.Model):\n    def __init__(self, g,in_feats,n_hidden,n_classes):\n        super(GCN, self).__init__()\n        self.g = g\n        \n        self.input_layer = GraphConv(n_hidden)\n        self.hidden_layer = GraphConv(n_hidden)  # create more if you like\n        self.output_layer = GraphConv(n_classes)\n    \n    def call(self, features):\n        h = features\n        h = self.input_layer(self.g, h)\n        h = self.hidden_layer(self.g, h)\n        h = self.output_layer(self.g, h)\n        return h\n        ","118f341b":"# create GCN model\n\nmodel = GCN(g,\n           in_feats = in_feats,\n           n_hidden=16,\n           n_classes=n_classes)\n\nloss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2, epsilon=1e-8)\n","08548d41":"# put it in a loop for ten times you will get 79% accuracy. WoW~!\nfor x in range (100):\n    with tf.GradientTape() as tape:\n        logits = model(features)\n        loss_value = loss_func(labels[train_mask], logits[train_mask])\n\n        grads = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n\n\n    # evaluate\n    logits_test = model(features, training = False)\n\n    logits_val = logits[val_mask]\n    labels_val = labels[val_mask]\n    indices = tf.math.argmax(logits_val, axis = 1)\n    acc = tf.reduce_mean(tf.cast(indices == labels_val, dtype = tf.float32))\n\n    print(acc.numpy().round(2))","f0113e78":"model.summary()","c17e4499":"# Step 7 -> Do some normalization on the graph\n\nConnect each node to itself ... why ? I assume that if I am affected by others in the network, I must be affected by myself in the first place, huh ?\n\nDo some square root stuff... just normalization .... not a big deal ... like scaling columns we used to do with pandas dataframes...huh ?","0029a7b5":"# Step 3 -> Import everything you need","ecec8b23":"You can try with any of the three datasets available from dgl\n (available dataset: \"cora\", \"citeseer\", \"pubmed\")\n## Cora dataset\n\n\n## Citeseer dataset\n\n\n    CiteSeer for Document Classification\n        The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0\/1-valued word vector indicating the absence\/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words. The README file in the dataset provides more details.\n        Download link:\n            https:\/\/linqs-data.soe.ucsc.edu\/public\/lbc\/citeseer.tgz  \n        Related papers:\n            Qing Lu, and Lise Getoor. \"Link-based classification.\" ICML, 2003.\n            Prithviraj Sen, et al. \"Collective classification in network data.\" AI Magazine, 2008.  source -> https:\/\/linqs.soe.ucsc.edu\/data\n\n\n## Pubmed dataset\n\n    PubMed Diabetes\n        The Pubmed Diabetes dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF\/IDF weighted word vector from a dictionary which consists of 500 unique words. The README file in the dataset provides more details.\n        Download Link:\n            https:\/\/linqs-data.soe.ucsc.edu\/public\/Pubmed-Diabetes.tgz\n        Related Papers:\n            Galileo Namata, et. al. \"Query-driven Active Surveying for Collective Classification.\" MLG. 2012. source -> https:\/\/linqs.soe.ucsc.edu\/data\n","b6064606":"# Step 8 -> Define the layers you will need\n\nRegardless of what I am going to do , I will need to define my own layers (Although the GraphConv layer is predefined in dgl, I prefered to write it on my own).\nIgnore normalization and baises for now.\n\nTo simplify,\n\n1. Remember the most simple classification problem you ever learnt.\n2. You have some users for each user you know [age, gender, height, weight, education, etc] and you want to predict his salary.\n3. Another example, Imaging some users on twitter writing tweets and you would like to predict if they are happy or not.\n\n4. Imagine the these users are connected in a network\n5. Forget about the network and think about the users\n6. Remember the very simple dense layer that you always do for any classification problem \n7. You will have input_features (age, gender, height,education) and output_features (salary)\n8. Make a simple dense layer that take input_features of each node  and output the output_features\n\nAll the above is the same for any network\n\n9. But, \n10. But, this is a network\n11. In short, exchange the features between the nodes\nCalculate the output features of each node, and, and, and, give each node the summation of the output features of all nodes around it.\n\n        graph.ndata['h'] = feat\n        graph.update_all(dgl.function.copy_src(src='h', out='m'),\n                             dgl.function.sum(msg='m', out='h'))\n        rst = graph.ndata['h']\n\n","fbfa11b1":"# Step 6 -> Extract the graph from the data and put it into a DGL graph \n\nDGL graph is just a simple graph, but it works well with tensorflow.","e435de0b":"# Step 9 -> describe the GCN (Graph convolution network) model\n\nSimilar to any linear model that we write as a class\n\nyou will define the layers in the init method and use them in the call method.","ee10bac6":"# Step 11 -> Train the model","78a3560f":"# Fast approximate convolutions on graphs\n\nGCN is just a layer similar to Dense, Conv2D,...layers. It has an activation function and some inputs, and output. This is the GCN formula.\n![gcn.png](attachment:gcn.png)\n\n\n![desc.png](attachment:desc.png) [reference](https:\/\/docs.dgl.ai\/tutorials\/models\/1_gnn\/1_gcn.html)\n\nIn the equation, there are A (adjacency matrix), squareroot of D (D to -1\/2) , W and H.\n1. Adjacency matrix is a binary matrix; It compares each node with all other nodes and if they are connected they score 1 else 0\n\n![amatrix.png](attachment:amatrix.png)\n\n\n2. Degree Matrix (D) is a diagonal matrix where each number refers to the number of degrees (edges; lines connected to) of each node. For example, here node(6) had only one edge while node(1) has 4 (Two other nodes plus two for itself (how many lines touch the circle around node (1) ? Four))\n\n![degree_matrix.jpg](attachment:degree_matrix.jpg)\n\n\n3. H(l) is the input which is a matrix of nodes as rows and each column is a feature e.g. if nodes represent users; features could include user age, country, etc. If there are no features we can the index number of each node as its sole feature. \n\n4. H(l+1) is computed from the equation above, and can be used as input again.","de19e3dc":"# Step 4 -> Load the data\n\nFor a very secret reason, I will not say why I loaded the data this way. \n\nHere I loaded the cora dataset. (Google it for more information )","bb0491ab":"# Step 2 -> Decide which library to use \n\nI will use the dgl library (similar to keras) with tensorflow as backend\n\ndgl is responsible for the  message passing  part and everything else is conducted by tensorflow","4fb61fff":"# Step 5 -> Extract needed information from the dataset","4bceac75":"# Step 10 -> Build the model","1fcc474b":"# Step 12 -> Learn more about the model","56096baf":"# Graph deep learning with keras \n\nI will discuss Graph deep learning in depth. I will focus in each kernel on one topic.\nIn this kernel, I will take screenshots from [Kipf paper](https:\/\/arxiv.org\/pdf\/1609.02907.pdf) and also from the [dgl library](https:\/\/github.com\/dmlc\/dgl\/tree\/master\/examples\/tensorflow\/gcn)\n\n## Motivation\n> Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural network models to such structured datasets.\nIn the last couple of years, a number of papers re-visited this problem of generalizing neural networks to work on arbitrarily structured graphs (Bruna et al., ICLR 2014; Henaff et al., 2015; Duvenaud et al., NIPS 2015; Li et al., ICLR 2016; Defferrard et al., NIPS 2016; Kipf & Welling, ICLR 2017), some of them now achieving very promising results in domains that have previously been dominated by, e.g., kernel-based methods, graph-based regularization techniques and others.   WRITTEN BY [KIPF](https:\/\/tkipf.github.io\/graph-convolutional-networks\/) \n\n\n## GCN vs RGCN\nThis tutorial is for GCN. I wrote another [tutorial on RCGN.](https:\/\/www.kaggle.com\/elmahy\/relational-graph-convolution-networks-explained)\n\n|  | <font style=\"color:red\">Graph convolution networks<\/font> | <font style=\"color:red\"> Relational Graph convolution networks<\/font> |\n| - | - | - |\n| Type of netwok that it works on | Simple just some nodes connected with edges | A directed labeled network  |\n| Example | papers citing each others | knowledge graph (google it)\n| Weights | Just make a dense layer for each node and pass messages from each node to the other | make a dense layer for each relation type since we have many types of relation e.g. ahmed employes(relation type) mohamed, John visits (relation type) [Dahab](https:\/\/www.youtube.com\/watch?v=CheLNATs4IA&t=33s) because it's awesome and cheap and the author can invite you for dinner there."}}