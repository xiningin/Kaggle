{"cell_type":{"d9081192":"code","d4be5231":"code","2a6cef62":"code","ff8c1d74":"code","ec3f4acb":"code","c1f99dce":"code","77e89d09":"code","faf1b063":"code","474c54a1":"code","cc20355f":"code","7b229000":"code","fe859eeb":"code","63649a92":"code","5b80f902":"code","b400719b":"code","978f7924":"code","2dd15fcf":"code","5672230c":"code","08aa4459":"markdown","fc121177":"markdown","30e5536b":"markdown","c8ab1cdb":"markdown","092486ee":"markdown","e1ffa406":"markdown","46091509":"markdown","3b68bb32":"markdown","093f2354":"markdown","7bbd023d":"markdown","4e448960":"markdown","e852291a":"markdown","967ff270":"markdown","105f2f03":"markdown","bf61482c":"markdown","82a6cbc7":"markdown","ce4bb887":"markdown","9385b508":"markdown","73b7c646":"markdown","7e8a56bd":"markdown","45332c0b":"markdown","fab5229f":"markdown","98cb1f18":"markdown","5063fe20":"markdown","ff480bd2":"markdown","a2126768":"markdown","d3d853e3":"markdown","28cc201d":"markdown","079a465d":"markdown","ed4651c7":"markdown","63b06de3":"markdown","2ff0f073":"markdown","91a0a326":"markdown","b6f998c6":"markdown","2b680964":"markdown","90bec8a9":"markdown","edb6aca5":"markdown","2b315092":"markdown","751fdd3b":"markdown","8bbd9116":"markdown","99092854":"markdown","e00bc5da":"markdown"},"source":{"d9081192":"import os\nimport gc\nimport re\nimport folium\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport pycountry\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport nltk\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nimport requests\nfrom IPython.display import HTML","d4be5231":"import seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\ntqdm.pandas()\nnp.random.seed(0)\n%env PYTHONHASHSEED=0\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2a6cef62":"DATA_PATH = \"..\/input\/CORD-19-research-challenge\/\"\nCLEAN_DATA_PATH = \"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/\"\n\npmc_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_pmc.csv\")\nbiorxiv_df = pd.read_csv(CLEAN_DATA_PATH + \"biorxiv_clean.csv\")\ncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_comm_use.csv\")\nnoncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_noncomm_use.csv\")\n\npapers_df = pd.concat([pmc_df,\n                       biorxiv_df,\n                       comm_use_df,\n                       noncomm_use_df], axis=0).reset_index(drop=True)","ff8c1d74":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\npapers_df[\"abstract_words\"] = papers_df[\"abstract\"].apply(new_len)\nnums = papers_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All abstracts\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Abstract words\", xaxis_title=\"Abstract words\", template=\"simple_white\", showlegend=False)\nfig.show()","ec3f4acb":"biorxiv_df[\"abstract_words\"] = biorxiv_df[\"abstract\"].apply(new_len)\nnums_1 = biorxiv_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\npmc_df[\"abstract_words\"] = pmc_df[\"abstract\"].apply(new_len)\nnums_2 = pmc_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\ncomm_use_df[\"abstract_words\"] = comm_use_df[\"abstract\"].apply(new_len)\nnums_3 = comm_use_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nnoncomm_use_df[\"abstract_words\"] = noncomm_use_df[\"abstract\"].apply(new_len)\nnums_4 = noncomm_use_df.query(\"abstract_words != 0 and abstract_words < 500\")[\"abstract_words\"]\nfig = ff.create_distplot(hist_data=[nums_1, nums_2, nums_3, nums_4],\n                         group_labels=[\"Biorxiv\", \"PMC\", \"Commerical\", \"Non-commercial\"],\n                         colors=px.colors.qualitative.Plotly[4:], show_hist=False)\n\nfig.update_layout(title_text=\"Abstract words vs. Paper type\", xaxis_title=\"Abstract words\", template=\"plotly_white\")\nfig.show()","c1f99dce":"def nonan(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \"\")\n    else:\n        return \"\"\n\ntext = ' '.join([nonan(abstract) for abstract in papers_df[\"abstract\"]])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Common words in abstracts')","77e89d09":"def nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            lemmatized_sentence.append(word)\n        else:\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n\n    return \" \".join(lemmatized_sentence)\n\ndef clean_text(abstract):\n    abstract = abstract.replace(\". \", \" \").replace(\", \", \" \").replace(\"! \", \" \")\\\n                       .replace(\"? \", \" \").replace(\": \", \" \").replace(\"; \", \" \")\\\n                       .replace(\"( \", \" \").replace(\") \", \" \").replace(\"| \", \" \").replace(\"\/ \", \" \")\n    if \".\" in abstract or \",\" in abstract or \"!\" in abstract or \"?\" in abstract or \":\" in abstract or \";\" in abstract or \"(\" in abstract or \")\" in abstract or \"|\" in abstract or \"\/\" in abstract:\n        abstract = abstract.replace(\".\", \" \").replace(\",\", \" \").replace(\"!\", \" \")\\\n                           .replace(\"?\", \" \").replace(\":\", \" \").replace(\";\", \" \")\\\n                           .replace(\"(\", \" \").replace(\")\", \" \").replace(\"|\", \" \").replace(\"\/\", \" \")\n    abstract = abstract.replace(\"  \", \" \")\n    \n    for word in list(set(stopwords.words(\"english\"))):\n        abstract = abstract.replace(\" \" + word + \" \", \" \")\n\n    return lemmatize_sentence(abstract).lower()\n\ndef get_similar_words(word, num):\n    vec = model_wv_df[word].T\n    distances = np.linalg.norm(model_wv_df.subtract(model_wv_df[word], \n                                                    axis=0).values, axis=0)\n\n    indices = np.argsort(distances)\n    top_distances = distances[indices[1:num+1]]\n    top_words = model_wv_vocab[indices[1:num+1]]\n    return top_words\n\ndef visualize_word_list(color, word):\n    top_words = get_similar_words(word, num=6)\n    relevant_words = [get_similar_words(word, num=8) for word in top_words]\n    fig = make_subplots(rows=3, cols=2, subplot_titles=tuple(top_words), vertical_spacing=0.05)\n    for idx, word_list in enumerate(relevant_words):\n        words = [word for word in word_list if word in model_wv_vocab]\n        X = model_wv_df[words].T\n        pca = PCA(n_components=2)\n        result = pca.fit_transform(X)\n        df = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\n        df[\"Word\"] = word_list\n        word_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\n        df[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\n        plot = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=color, size=\"Distance\")\n        plot.layout.title = top_words[idx]\n        plot.update_traces(textposition='top center')\n        plot.layout.xaxis.autorange = True\n        plot.data[0].marker.line.width = 1\n        plot.data[0].marker.line.color = 'rgb(0, 0, 0)'\n        fig.add_trace(plot.data[0], row=(idx\/\/2)+1, col=(idx%2)+1)\n    fig.layout.coloraxis.showscale = False\n    fig.update_layout(height=1400, title_text=\"2D PCA of words related to {}\".format(word), paper_bgcolor=\"#f0f0f0\", template=\"plotly_white\")\n    return fig\n\ndef visualize_word(color, word):\n    top_words = get_similar_words(word, num=20)\n    words = [word for word in top_words if word in model_wv_vocab]\n    X = model_wv_df[words].T\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n    df = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\n    df[\"Word\"] = top_words\n    if word == \"antimalarial\":\n        df = df.query(\"Word != 'anti-malarial' and Word != 'anthelmintic'\")\n    if word == \"doxorubicin\":\n        df = df.query(\"Word != 'anti-rotavirus'\")\n    word_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\n    df[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\n    fig = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=color, size=\"Distance\")\n    fig.layout.title = word\n    fig.update_traces(textposition='top center')\n    fig.layout.xaxis.autorange = True\n    fig.layout.coloraxis.showscale = True\n    fig.data[0].marker.line.width = 1\n    fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n    fig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(word), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n    fig.show()","faf1b063":"#lemmatizer = WordNetLemmatizer()\n\n#def get_words(abstract):\n#    return clean_text(nonan(abstract)).split(\" \")\n\n#words = papers_df[\"abstract\"].progress_apply(get_words)\n#model = Word2Vec(words, size=200, sg=1, min_count=1, window=8, hs=0, negative=15, workers=1)\n\nmodel = Word2Vec.load(\"..\/input\/covid19-challenge-trained-w2v-model\/covid.w2v\")\n#model_wv_vocab = pd.read_csv(\"..\/input\/word2vec-results-1\/vocab.csv\").values[:, 0]\n#model_wv_df = pd.DataFrame(np.transpose(model_wv), columns=model_wv_vocab)","474c54a1":"keywords = [\"infection\", \"cell\", \"protein\", \"virus\",\\\n            \"disease\", \"respiratory\", \"influenza\", \"viral\",\\\n            \"rna\", \"patient\", \"pathogen\", \"human\", \"medicine\",\\\n            \"cov\", \"antiviral\"]\n\nprint(\"Most similar words to keywords\")\nprint(\"\")\n\ntop_words_list = []\nfor word in keywords:\n    print(word, model.wv.most_similar(word, topn=5))","cc20355f":"words = [word for word in keywords if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = keywords\ndf[\"Distance\"] = np.sqrt(df[\"Component 1\"]**2 + df[\"Component 2\"]**2)\nfig = px.scatter(df, x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\",size=\"Distance\")\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of Word2Vec embeddings\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","7b229000":"words = [word for word in keywords if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=3)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\", \"Component 3\"])\ndf[\"Word\"] = keywords\ndf[\"Distance\"] = np.sqrt(df[\"Component 1\"]**2 + df[\"Component 2\"]**2 + df[\"Component 3\"]**2)\nfig = px.scatter_3d(df, x=\"Component 1\", y=\"Component 2\", z=\"Component 3\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\")\nfig.update_traces(textposition='top left')\nfig.layout.coloraxis.showscale = False\nfig.layout.xaxis.autorange = True\nfig.update_layout(height=800, title_text=\"3D PCA of Word2Vec embeddings\", template=\"plotly\")\nfig.show()","fe859eeb":"words = [word for word in top_words_list[6] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[6]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df.query(\"Word != 'uenza'\"), x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"aggrnyl\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Green\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[6]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","63649a92":"words = [word for word in top_words_list[8] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[8]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:].query(\"Word != 'abstractrna'\"), x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"agsunset\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"MediumPurple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[8]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","5b80f902":"words = [word for word in top_words_list[-2] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[-2]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"oryel\",size=\"Distance\")\n\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Orange\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[-2]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","b400719b":"words = [word for word in top_words_list[3] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[3]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[1:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"bluered\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Purple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[3]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","978f7924":"words = [word for word in top_words_list[-1] if word in model_wv_vocab]\nX = model_wv_df[words].T\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\ndf = pd.DataFrame(result, columns=[\"Component 1\", \"Component 2\"])\ndf[\"Word\"] = top_words_list[-1]\nword_emb = df[[\"Component 1\", \"Component 2\"]].loc[0]\ndf[\"Distance\"] = np.sqrt((df[\"Component 1\"] - word_emb[0])**2 + (df[\"Component 2\"] - word_emb[1])**2)\nfig = px.scatter(df[2:], x=\"Component 1\", y=\"Component 2\", text=\"Word\", color=\"Distance\", color_continuous_scale=\"viridis\",size=\"Distance\")\n\n\"\"\"for row in range(len(df)):\n    fig.add_shape(\n                type=\"line\",\n                x0=word_emb[0],\n                y0=word_emb[1],\n                x1=df[\"Component 1\"][row],\n                y1=df[\"Component 2\"][row],\n                line=dict(\n                    color=\"Purple\",\n                    width=0.75,\n                    dash=\"dot\"\n                )\n    )\"\"\"\n\nfig.update_traces(textposition='top center')\nfig.layout.xaxis.autorange = True\nfig.data[0].marker.line.width = 1\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.update_layout(height=800, title_text=\"2D PCA of words related to {}\".format(keywords[-1]), template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\nfig.show()","2dd15fcf":"fig = visualize_word_list('agsunset', 'antiviral')\nfig.update_layout(colorscale=dict(diverging=px.colors.diverging.Tealrose))","5672230c":"visualize_word('plotly3', 'antimalarial')","08aa4459":"In the above plot, we can see the 2D PCA of the keywords' vectors.\n\n1. The words \"virus\", \"viral\", and \"CoV\" form a cluster in the bottom-right part of the plot, indicating that they have similar meanings. This makes sense because CoV is a virus.\n2. The words \"medicine\" and \"patient\" are both on the far left end of the image because these words are used together very frequently.\n3. The \"pathogen\", \"influenza\", and \"respiratory\" form a cluster in the bottom-left part of the plot, indicating that they have similar meanings. This makes sense because influenza is a repsiratory disease.\n\nThese abstract linguistic relationships are successfully represented by word vectors.","fc121177":"In the above distribution plot, we can see that the abstract length has a roughly normal distribution with several minor peaks on either side of the mean. The probability density peaks at around 200 words, indicating that this is the most plausible value.","30e5536b":"# Takeaways <a id=\"4\"><\/a>\n\n1. Several antimalarial drugs such as hydroxychloroquine might be potential drugs to cure COVID-19. Antimalarial drugs have been successfully tested on COVID-19 patients in certain countries.","c8ab1cdb":"### Visualize most similar words to keywords","092486ee":"### 2D PCA of words similar to words similar to antiviral","e1ffa406":"In the above image, we can see that word vectors can reflect relationships such as \"King is to Queen as Man is to Woman\" or \"Italy is to Rome\" as \"Germany is to Berlin\". These vectors can be also be used to find unknown relationships between words. These unknown relationships may help us find latent knowledge in research papers and find drugs that can possibly cure COVID_19!","46091509":"In the diagram above, we can see that the authors found two levels of words similar to \"thermoelectric\" in a heirarchical manner. The second order similar words contained compounds like Li<sub>2<\/sub>CuSb, Cu<sub>7<\/sub>Te<sub>5<\/sub>, and CsAgGa<sub>2<\/sub>Se<sub>4<\/sub>, which turned out to be very good thermoelectric materials in real life.","3b68bb32":"### 2D PCA of words similar to RNA","093f2354":"## Abstracts <a id=\"1.3\"><\/a>\n\nEvery research paper has an abstract at the start, which briefly summarizes the contents and ideas presented in the paper. These abstracts can be a great source of insights and solutions (as we will see later). First, I will do some basic visualization of the abstracts in the dataset.","7bbd023d":"### 2D PCA of keyword vectors","4e448960":"### Load pretrained Word2Vec model (200D vectors) \/ Train on Corpus","e852291a":"I have plotted the 2D PCA of the words most similar to RNA above. We cannot see an clear clustering in the plot above, but we can see that few words similar to RNA appear in the graph. For example, the words \"ssRNA\" (single-stranded RNA) and \"vRNA\" (viral RNA), which are types of RNA (ribonucleic acid). We also see words like \"negative-strand\" and \"negative-sense\", When we put all these terms together, it makes sense because they are deeply related. The genome of the influenza virus is in fact composed of eight negative-strand vRNA!","967ff270":"First, we need to find the most common words in the corpus to continue our analysis. From the word cloud above, we can see that \"infection\", \"cell\", \"virus\", and \"protein\" are among the most common words in COVID-19 research paper abstracts. These words will form our \"keyword\" list.","105f2f03":"I have plotted the 2D PCA of the words most similar to influenza above.\n\n1. The words \"H2N2\", \"PDM\", \"PDM2009\", \"H7N7\", and \"swine origin\" form a very dense cluster in the bottom-left corner of the plot. This makes sense because H2N2 and H7N7 are both subtypes of Influenza and they have their origin in swines. Note that \"PDM\" stands for pandemic.\n2. The remaining words are very far away from this cluster. For example, the word \"flu\" is far away from this cluster because it is a general term which is not equivalent to any specific type of flu or influenza.","bf61482c":"### Word cloud of abstracts","82a6cbc7":"### 2D PCA of words related to keywords\n\nNow, I will pick up a few keywords and analyze the PCA of words similar to them, making conclusions and inferences as I go.","ce4bb887":"### 2D PCA of words similar to influenza","9385b508":"### 2D PCA of words similar to CoV","73b7c646":"I have plotted the 2D PCA of the words most similar to virus above. We cannot see any clear clustering, but we do see many types of viruses, such as \"pneumovirus\", \"lyssavirus\", \"pox\", \"CPIV\", and \"HHV\", appearing in the plot.\n\nNow since we have visualized the PCA of words most similar to certain keywords, let us use the same strategy to find a possible medicine for COVID-19.","7e8a56bd":"## Unsupervised NLP and Word2Vec <a id=\"2.1\"><\/a>\n\nUnsupervised NLP involves the analysis of unlabeled language data. Certain techniques can be used to derive insights from a large corpus of text. One such method is called **Word2Vec**. Word2Vec is a neural network architecture trained on thousands of sentences of text. After training, the neural network finds the **optimal vector representation** of each word in the corpus. These vectors are meant to reflect the meaning of the word. Words with similar meanings have similar vectors. ","45332c0b":"In the plot above, we can see the words most similar to antimalarial. These are different drugs and medicines that are used to combat malaria, which may work for COVID-19, such as \"amodiaquine\", \"hydroxychloroquine\", and \"nitazoxanide\".","fab5229f":"## Using Word2Vec to find cures <a id=\"2.2\"><\/a>\n\nWe can take advantage of these intricate relationships between word vectors to find cures for COVID-19. The steps are as follows:\n\n1. Find common related to the study of COVID-19, such as \"infection\", \"CoV\", \"viral\", etc.\n2. Find the words with lowest Euclidean distance to these words (most similar words).\n3. Finally, find the words most similar to these words (second order similarity). These words will hopefully contain potential COVID-19 cures.\n\nNote that the similarity between two Word2Vec vectors is calculated using the formula below (where *u* and *v* are the word vectors).\n\n<center><img src=\"https:\/\/i.imgur.com\/wBuMMS9.png\" width=\"450px\"><\/center>","98cb1f18":"I have plotted the 3D PCA above. The clustering seems to be very similar to that in 2D PCA. More dimensions usually ensure better clustering and word representation, but it comes at the cost of higher dimensionality and less intuitive visualization.","5063fe20":"### Load data","ff480bd2":"### Second-order word similarities\n\nNow, I will look at the words similar to the words found above (second order similarity) to hopefully, find potential cures for COVID-19.","a2126768":"### PCA\n\nPCA is a dimensionality reduction method which takes vectors with several dimensions and compresses it into a smaller vector (with 2 or 3 dimensions) while preserving most of the information in the original vector (using some linear algebra). PCA makes visualization easier while dealing with high-dimensional data, such as Word2Vec vectors.\n\n<center><img src=\"https:\/\/i.imgur.com\/CKWFUyd.png\" width=\"400px\"><\/center>","d3d853e3":"The approach detailed above is actually inspired by a research paper called [\"Unsupervised word embeddings capture latent knowledge from materials science literature\"](https:\/\/www.nature.com\/articles\/s41586-019-1335-8), where the authors find new materials with desirable properties (such as thermoelectricity) solely based on a large corpus materials science literature. These materials were never used for these purposes before, but they outperform old materials by a large margin. I hope to emulate the same method to look for COVID-19 cures. The diagram below illustrates what the authors did in their research.\n\n<center><img src=\"https:\/\/i.imgur.com\/TjXOhuJ.png\" width=\"400px\"><\/center>","28cc201d":"### Abstract words distribution","079a465d":"### 2D PCA of words related to antiviral","ed4651c7":"This plot shows the abstract length distribution for different research paper types (BiorXiv, PMC, Commercial, and Non-commercial). The abstract of commerical papers seem to longest on average, followed by non-commercial, BiorXiv, and PMC (in descending order).","63b06de3":"### 2D PCA of words related to virus","2ff0f073":"The entire process can be summarized with the flowchart below. (the same steps as given above)\n\n<center><img src=\"https:\/\/i.imgur.com\/l8b6enq.png\" width=\"450px\"><\/center>","91a0a326":"I have plotted the 2D PCA of the words most similar to CoV (stands for **CO**rona**V**irus) above.\n\n1. We can see few words like \"coronavirus\", \"SARS-CoV\", and \"coronaviral\" which are almost synonymal with CoV. These words are surprisingly very close to \"CoV\" in the vector space.\n2. We can also see a clear cluster in the bottom-left corner of the plot, and these words are also closely linked with the word \"CoV\".","b6f998c6":"<center><img src=\"https:\/\/i.imgur.com\/JHCOaan.png\" width=\"800px\"><\/center>","2b680964":"These words will form the next batch of words, which we will analyze to find cures to COVID-19.","90bec8a9":"### 2D PCA of words similar to words similar to antimalarial","edb6aca5":"I have plotted the 2D PCA of the words most similar to antiviral above. We can see a lot of different types of antivirals and other drugs in the plot, such as \"saracatinib\", an anti-malarial and anti-HIV drug. The list also includes \"antiparasitic\", \"ant-HBV\", and \"anti-EV71\".","2b315092":"As I stated earlier, each word is associated with a vector. Amazingly, these vectors can also encode relationships and analogies between words. The diagram below iillustrates some examples of linear vector relationships representing the relationships between words.","751fdd3b":"We can see some amazing patterns in the plots above. We see certain drugs and chemicals that keep repeating, including \"anti-malarial\", \"hydroxychloroquine\", and \"doxorubicin\". It is amazing that these drugs have actually been successfully applied on COVID-19 patients across the world. There are cases of anti-malarial drugs working for COVID-19!\n\n**The most common result above, \"hydroxychloroquine\", might just be the cure for COVID-19!**[](http:\/\/)","8bbd9116":"<center><img src=\"https:\/\/i.imgur.com\/sZP4N8S.png\" width=\"800px\"><\/center>","99092854":"### 3D PCA of keyword vectors","e00bc5da":"# Finding cures for COVID-19 <a id=\"2\"><\/a>\n\nNow, I will leverage the power of unsupervised machine learning to try and find possible cures (medicines and drugs) to COVID-19."}}