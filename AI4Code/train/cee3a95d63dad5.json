{"cell_type":{"8c6b247b":"code","f43c39ab":"code","feaa8bfe":"code","8b0fa6c4":"code","c9661b7f":"code","c3dd6c06":"code","f985f74d":"code","22d069ec":"code","a689784e":"code","26312820":"code","8d97f866":"code","eca7c62e":"code","00ff06a7":"code","5928f679":"code","b181ca9e":"code","a03fea4c":"code","ce32c6fa":"code","d4949dca":"code","42736d76":"code","71136f45":"code","641ee70c":"markdown","3989e15e":"markdown","d04bc768":"markdown","0c4c9978":"markdown","d4874f4d":"markdown","cc415a84":"markdown","8402191c":"markdown","b0acef32":"markdown","50ebf993":"markdown","686167ab":"markdown","fb9aa4bd":"markdown","6b920672":"markdown","9061a2ab":"markdown","701de852":"markdown","8a5706ec":"markdown","652b8806":"markdown","67f6c01c":"markdown","0d702d1f":"markdown","a9493acd":"markdown","6c3cdc74":"markdown"},"source":{"8c6b247b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f43c39ab":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nimport tensorflow as tf","feaa8bfe":"training = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\ntesting = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')","8b0fa6c4":"num_rows, num_cols = len(training), len(training.columns)\nprint('Check for missing values: ')\nprint(max(pd.isna(training).sum()))\nprint('No missing values!')\n","c9661b7f":"features = training.iloc[:,:num_cols-2]\nlabels = training.iloc[:, num_cols-1]\nids = testing.id\nfeatures = features.drop(['id'], axis=1)\ntesting = testing.drop(['id', 'pubchem_id'], axis=1)\nprint(ids)\nprint(testing)\nprint(features.describe())\nprint(features.head())\nprint(labels)\nprint('Average value of labels: ', sum(labels)\/len(labels))","c3dd6c06":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)\n","f985f74d":"def plot_data(data):\n    s = pd.Series(data.history['loss'])\n    print(s.describe())\n    plt.bar(range(data.params['epochs']), data.history['loss'], log=True)\n    plt.show()","22d069ec":"input_shape = len(features.columns)\nmodels = []\nepochs = 20","a689784e":"best_model1, best_score1, num_neurons, to_plot = None, None, None, None\nopt = tf.keras.optimizers.RMSprop()\nfor i in range(10, 1, -1):\n    print('max neuron count: ', 2**i)\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(2**i, input_shape=(input_shape,), activation='relu'))\n    for j in range(i, 0, -2):\n        model.add(tf.keras.layers.Dense(2**j, activation='relu'))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer = opt, loss = 'mse')\n    data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n    score = model.evaluate(X_valid, y_valid)\n    if not best_model1 or score < best_score1:\n        best_model1, best_score1, num_neurons, to_plot = model, score, i, data\nprint(best_model1, best_score1, num_neurons)\nprint(data)","26312820":"model = best_model1\nbest_model1, best_score1, to_plot = None, None, None\nlearning_rates = [0.01, 0.005, 0.001]\nrhos = [0.9, 0.99, 0.8, 0.75]\nepsilons = [1e-05, 1e-06, 1e-07, 1]\niterations = len(learning_rates) * len(rhos) * len(epsilons)\ncounter = 0\nfor learning_rate in learning_rates:\n    for rho in rhos:\n        for epsilon in epsilons:\n            print(f'iteration: {counter} out of {iterations}. Current iteration - learning rate: {learning_rate}, rho: {rho}, epsilon: {epsilon}')\n            counter += 1\n            model = tf.keras.Sequential()\n            model.add(tf.keras.layers.Dense(2**num_neurons, input_shape=(input_shape,), activation='relu'))\n            for i in range(num_neurons, 0, -2):\n                model.add(tf.keras.layers.Dense(2**i, activation='relu'))\n            model.add(tf.keras.layers.Dense(1))\n            opt = tf.keras.optimizers.RMSprop(learning_rate = learning_rate, rho = rho)\n            model.compile(optimizer = opt, loss='mse')\n            data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n            score = model.evaluate(X_valid, y_valid)\n            if not best_model1 or score < best_score1:\n                best_model1, best_score1, to_plot = model, score, data\nprint('best score: ', best_score1)\nmodel1_data = to_plot\nmodels.append((best_model1, best_score1))","8d97f866":"best_model2, best_score2, num_neurons, to_plot = None, None, None, None\nopt = tf.keras.optimizers.Adadelta()\nfor i in range(9, 1, -1):\n    print('max neuron count: ', 2**i)\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(2**i, input_shape=(input_shape,), activation='relu', kernel_regularizer='l2'))\n    for j in range(i, 0, -2):\n        model.add(tf.keras.layers.Dense(2**j, activation='relu', kernel_regularizer='l2'))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer = opt, loss = 'mse')\n    data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n    score = model.evaluate(X_valid, y_valid)\n    if not score:\n        continue\n    if not best_model2 or score < best_score2:\n        best_model2, best_score2, num_neurons, to_plot = model, score, i, data\nprint(f'best score: {best_score2}, max number of neurons: {2**num_neurons}')\n            ","eca7c62e":"model = best_model2\nbest_model2, best_score2, to_plot = None, None, None\nlearning_rates = [0.1, 0.025, 0.01, 0.005, 0.001]\nrhos = [0.95, 0.96, 0.98, 0.99]\nepsilons = [1e-05, 1e-06, 1e-07, 1e-08]\niterations = len(learning_rates) * len(rhos) * len(epsilons)\ncounter = 0\nfor learning_rate in learning_rates:\n    for rho in rhos:\n        for epsilon in epsilons:\n            print(f'iteration: {counter} out of {iterations}. Current iteration - learning rate: {learning_rate}, rho: {rho}, epsilon: {epsilon}')\n            counter += 1\n            model = tf.keras.Sequential()\n            model.add(tf.keras.layers.Dense(2**num_neurons, input_shape=(input_shape,), activation='relu', kernel_regularizer='l2'))\n            for i in range(num_neurons, 0, -2):\n                model.add(tf.keras.layers.Dense(2**i, activation='relu', kernel_regularizer='l2'))\n            model.add(tf.keras.layers.Dense(1))\n            opt = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, rho=rho, epsilon=epsilon)\n            model.compile(optimizer = opt, loss='mse')\n            data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n            score = model.evaluate(X_valid, y_valid)\n            if not best_model2 or score < best_score2:\n                best_model2, best_score2, to_plot = model, score, data\nprint('best score: ', best_score2)\nmodel2_data = to_plot\nmodels.append((best_model2, best_score2))","00ff06a7":"best_model3, best_score3, to_plot = None, None, None\nopt = tf.keras.optimizers.Adam()\nfor i in range(10, 1, -1):\n    print('max neuron count: ', 2**i)\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(2**i, input_shape=(input_shape,), activation='relu', kernel_regularizer='l2'))\n    for j in range(i, 0, -2):\n        model.add(tf.keras.layers.Dense(2**j, activation='relu', kernel_regularizer='l2'))\n    model.add(tf.keras.layers.Dense(1))\n    model.compile(optimizer = opt, loss = 'mse')\n    data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n    score = model.evaluate(X_valid, y_valid)\n    if not best_model3 or score < best_score3:\n        best_model3, best_score3, num_neurons, to_plot = model, score, i, data\nprint(f'best score: {best_score3}, max number of neurons: {2**num_neurons}')","5928f679":"model = best_model3\nbest_model3, best_score3, to_plot = None, None, None\nlearning_rates = [0.1, 0.025, 0.01, 0.005, 0.001]\nbeta1s = [0.8, 0.9, 0.95]\nepsilons = [1e-05, 1e-06, 1e-07, 1e-08]\niterations = len(learning_rates) * len(beta1s) * len(epsilons)\ncounter = 0\nfor learning_rate in learning_rates:\n    for beta1 in beta1s:\n        for epsilon in epsilons:\n            print(f'iteration: {counter} out of {iterations}. Current iteration - learning rate: {learning_rate}, beta1: {beta1}, epsilon: {epsilon}')\n            counter += 1\n            model = tf.keras.Sequential()\n            model.add(tf.keras.layers.Dense(2**num_neurons, input_shape=(input_shape,), activation='relu', kernel_regularizer='l2'))\n            for i in range(num_neurons, 0, -2):\n                model.add(tf.keras.layers.Dense(2**i, activation='relu', kernel_regularizer='l2'))\n            model.add(tf.keras.layers.Dense(1))\n            opt = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = beta1, epsilon = epsilon)\n            model.compile(optimizer = opt, loss='mse')\n            data = model.fit(X_train, y_train, epochs = epochs, verbose=0)\n            score = model.evaluate(X_valid, y_valid)\n            if not best_model3 or score < best_score3:\n                best_model3, best_score3, to_plot = model, score, data\nprint('best score: ', best_score3)\nmodel3_data = to_plot\nmodels.append((best_model3, best_score3))","b181ca9e":"best_model4, best_score4, to_plot = None, None, None\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(16, input_shape=(input_shape,), activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(4, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer = 'adam', loss = 'mse')\ndata = model.fit(X_train, y_train, epochs = epochs * 3, verbose = 0)\nscore = model.evaluate(X_valid, y_valid)\nif not best_model4 or score < best_score4:\n    best_model4, best_score4, to_plot = model, score, data\nprint('best score: ', best_score4)\nmodel4_data = to_plot\nmodels.append((best_model4, best_score4))","a03fea4c":"print(i[1] for i in models)\nprint(models)\nbest_overall_model, best_overall_score = None, None\nfor model in models:\n    if not best_overall_model or model[1] < best_overall_score:\n        best_overall_model, best_overall_score = model[0], model[1]\nprint(best_overall_model, best_overall_score)","ce32c6fa":"print('model 1: ')\nplot_data(model1_data)\nprint('model 2: ')\nplot_data(model2_data)\nprint('model 3: ')\nplot_data(model3_data)\nprint('model 4: ')\nplot_data(model4_data)","d4949dca":"best_overall_model.fit(X_train, y_train, epochs = epochs * 10)","42736d76":"score_valid = best_overall_model.evaluate(X_valid, y_valid)\nscore_test = best_overall_model.evaluate(X_test, y_test)\n\nprint(score_valid**0.5, score_test**0.5)","71136f45":"predict_x = best_overall_model.predict(testing)\narr = [predict_x[i][0] for i in range(len(predict_x))]\nprint('mean: ', sum(predict_x)\/len(predict_x))\noutput = pd.DataFrame({'id': ids, 'Eat': arr})\nprint(output.to_string())\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","641ee70c":"## Tune shape of network (number of layers and number of neurons in each layer)","3989e15e":"# Function for creating validation distribution","d04bc768":"# Attempt to find missing values","0c4c9978":"# Compare model performances","d4874f4d":"## Tuning Hyperparameters to find best performing model","cc415a84":"## Tune shape of network (number of layers and number of neurons in each layer)","8402191c":"# Create Train, Validation, and Test sets\n## This is done by using train_test_split twice. Once to create the Train\/Test set, and then a second time where we split the Test set to create the Validation set.","b0acef32":"# Global Variable Setting","50ebf993":"### Best overall model is determined by score. The score of each variation of every model is compared before selecting the best overall model. This includes the models created just for determining the best number of layers and neurons.","686167ab":"# Train, Evaluate, and Submit the best overall model","fb9aa4bd":"# Model 4 - Adam Without Tuning","6b920672":"## Tune shape of network (number of layers and number of neurons in each layer)","9061a2ab":"## Distribution of validation scores on the best performing variation of each model","701de852":"# Load CSVs","8a5706ec":"## Tuning Hyperparameters to find best performing model","652b8806":"# Model 3 - Adam Optimizer","67f6c01c":"# Model 1 - RMSprop Optimizer","0d702d1f":"# Data transformation\n## IDs seemingly have no effect on the outcome of the label. Since I need the testing IDs to create a submission, I first store the ids in a separate variable, and then drop it from the test set. I also remove pubchem_id from both. ","a9493acd":"# Model 2 - Adadelta Optimizer","6c3cdc74":"## Tuning Hyperparameters to find best performing model"}}