{"cell_type":{"a1e8f64d":"code","b1f03d3e":"code","5a965b59":"code","18b1ecbf":"code","05b6ec20":"code","c52f3795":"code","12de334a":"markdown","c343fb7d":"markdown","fdd931c9":"markdown","d57d3524":"markdown","46710616":"markdown","cad32e1b":"markdown","6d024166":"markdown","8ff83c71":"markdown","03a26f56":"markdown","a5248632":"markdown","1e9a57f6":"markdown","71d6808d":"markdown","294514bc":"markdown","5824b9de":"markdown","3bba6b2b":"markdown","1dcbe95a":"markdown","b4366f73":"markdown","9df85600":"markdown","76a7055c":"markdown","9adf3fd2":"markdown","1d95f596":"markdown","dfb536b0":"markdown","6e4f944a":"markdown"},"source":{"a1e8f64d":"import csv\nimport os\nimport numpy as np\n# For creating the heatmaps\nimport matplotlib.pyplot as plt\nimport math\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session ","b1f03d3e":"file_name = \"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"\nwith open(file_name,'r') as csvfile:\n    reader = csv.DictReader(csvfile, restkey = None, restval = None, dialect = 'excel')    \n    # reader is a reader object and hence to convert into iterable have to convert to list.\n    data = list(reader)\n    \nlist_len = len(data)\nprint('Size of total data : {}'.format(list_len))\n\n# Feature matrix\nX = np.zeros((list_len,11), dtype = float)\n# Target vector\nY = np.zeros((list_len,1), dtype = float)\n\n# Transfer data into respective feature vectors\nfor i in range(0,list_len):\n    # data is a list of dictionaries. data[i] is a dictionary. data[i].values provides the values to the corresponding keys. list(data[i].values) converts it into a list\n    values = list(data[i].values())\n\n    X[i,0] = values[0]\n    X[i,1] = values[1]\n    X[i,2] = values[2]\n    X[i,3] = values[3]\n    X[i,4] = values[4]\n    X[i,5] = values[5]\n    X[i,6] = values[6]\n    X[i,7] = values[7]\n    X[i,8] = values[8]\n    X[i,9] = values[9]\n    X[i,10] = values[10]\n    Y[i,0] = values[11]\n\nX_Labels = [\"Fixed Acidity\", \"Volatile Acidity\", \"Citric Acid\", \"Residual Sugar\", \"Chlorides\", \"Free sulphur Dioxide\", \"Total sulphur Dioxide\", \"Density\", \"pH\" ,\"Sulphates\", \"Alcohol\"]\nLabels = [\"Fixed Acidity\", \"Volatile Acidity\", \"Citric Acid\", \"Residual Sugar\", \"Chlorides\", \"Free sulphur Dioxide\", \"Total sulphur Dioxide\", \"Density\", \"pH\" ,\"Sulphates\", \"Alcohol\",\"Quality\"]\n\nprint(X_Labels)\n\nprint(\"Data head :\")\nprint(X[0:5,:])","5a965b59":"# Concatenating the features and target\nfeat_mat = np.concatenate((X,Y), axis = 1)\n\n# Pearsons correlation coeff array\nr = np.zeros((12,12))\n\n# We will be using a three nested for loop to create the correlation heatmap\n# Extracting first feature\nfor i in range(0,12):\n    feature1 = feat_mat[:,i]\n    # Extracting second feature\n    for j in range(0,12):\n        feature2 = feat_mat[:,j]\n        check = np.array_equal(feature1,feature2)\n        # If arrays are equal no need to check for correlation\n        if check == True:\n            r[i,j] = 1\n            continue\n        # Finding the means of the features\n        mean_feature1 = sum(feature1)\/list_len\n        mean_feature2 = sum(feature2)\/list_len\n        # Initializing the num, den1, den2 variables\n        num = 0\n        den1 = 0\n        den2 = 0\n        # Calculating the Pearsons correlation coeff\n        for k in range(0,list_len):\n            num = num + (feature1[k] - mean_feature1)*(feature2[k] - mean_feature2)\n            den1 = den1 + (feature1[k] - mean_feature1)**2\n            den2 = den2 + (feature2[k] - mean_feature2)**2\n        den1 = den1**0.5\n        den2 = den2**0.5\n        den = den1*den2\n        coeff = num\/den\n        r[i,j] = coeff\n\n# Rounding the numbers in the correlation matrix to 2 decimal places\nr = np.round(r,decimals=3)\n\n# Creating the heatmap\nfig, ax = plt.subplots(figsize=(15,15))\nim = ax.imshow(r, vmin=-1, vmax=1)\n\n# Create colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Setting the ticks on X axis\nax.set_xticks(np.arange(len(Labels)))\nax.set_yticks(np.arange(len(Labels)))\n# Label them with repsective entries\nax.set_xticklabels(Labels)\nax.set_yticklabels(Labels)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",rotation_mode=\"anchor\")\n\nfor i in range(0,12):\n    for j in range(0,12):\n        text = ax.text(i,j,r[i,j], ha = \"center\", va = \"center\", color = \"w\")\n\nplt.show()\n    \n    ","18b1ecbf":"# Lets draw samples from a standard normal distribution and plot as histogram\nmean = 0\nstd = 1\n# Drawing 2000 samples from a gaussian distribution of mean 0 and std 1.\na_norm = np.random.normal(loc=mean,scale=std,size=2000)\n# Setting density equal to 1 so that area under histogram sums to 1 (normalizing the counts) and can be compared with std normal distribution curve.\n# bins defines the floating point numbers for which we are calculating the count\n# X axis of plot : floating point numbers \n# Y axis of plot : counts\/frequency of floating point numbers\ncount, bins, ignored = plt.hist(a_norm,200,density=1)\n# Lets compare the histogram with the equation of the standard normal distribution curve by plotting over it.\nplt.plot(bins,1\/(np.sqrt(2*np.pi))*np.exp(-(bins**2)\/2), linewidth = 2, color = 'r')\nplt.title('Standard normal distribution curve')\nplt.show()\n\nb_norm = np.random.normal(loc=mean,scale=std,size=2000)\nc_norm = np.random.normal(loc=mean,scale=std,size=2000)\nd_norm = np.random.normal(loc=mean,scale=std,size=2000)\ne_norm = np.random.normal(loc=mean,scale=std,size=2000)\n\n# Now lets plot the chi squared distribution for 1, 3 and 5 degrees of freedom\na_norm2 = a_norm**2\nb_norm2 = b_norm**2\nc_norm2 = c_norm**2\nd_norm2 = d_norm**2\ne_norm2 = e_norm**2\n\ncount1, bins1, ignored1 = plt.hist(a_norm2,200,density=1)\nbins1 = bins1[bins1>=10**(-6)]\n# X axis of plot : squared std normal variable\n# Y axis of plot : counts\/frequency of floating point numbers\nplt.plot(bins1,bins1**(1\/2-1)*np.exp(-bins1\/2)\/(math.gamma(1\/2)*2**(1\/2)),linewidth=2,color='r')\nplt.axis([0,10,0,1.5])\nplt.title('Chi2 distribution with 1 dof')\nplt.show()\n\ncount3, bins3, ignored3 = plt.hist(a_norm2 + b_norm2 + c_norm2,200,density=1)\nbins3 = bins3[bins3>=10**(-6)]\nplt.plot(bins3,bins3**(3\/2-1)*np.exp(-bins3\/2)\/(math.gamma(3\/2)*2**(3\/2)),linewidth=2,color='r')\nplt.axis([0,10,0,1.5])\nplt.title('Chi2 distribution with 3 dof')\nplt.show()\n                           \ncount5, bins5, ignored5 = plt.hist(a_norm2 + b_norm2 + c_norm2 + d_norm2 + e_norm2,200,density=1)\nbins5 = bins5[bins5>=10**(-6)]\nplt.plot(bins5,bins5**(5\/2-1)*np.exp(-bins5\/2)\/(math.gamma(5\/2)*2**(5\/2)),linewidth=2,color='r')\nplt.axis([0,10,0,1.5])\nplt.title('Chi2 distribution with 5 dof')\nplt.show()","05b6ec20":"# Feature vector - med - low alcohol content\nalcohol = X[:,10].reshape(list_len,1)\n# less than 11.55 % - 1 (medium low), greater than 11.55 % - 0 (medium high)\nalcohol = np.where(alcohol<=11.55,1,0)\n\n# Feature vector - low volatile acidity content\nVA = X[:,2].reshape(list_len,1)\nVA = np.where(VA<=0.7,1,0)\n\nchi2_feat_mat = np.append(alcohol,VA,axis=1)\n\n# Target vector - quality. If quality is greater than 7 then we call it as good quality wine(1). If quality is less than 7 we call it as bad quality wine (0)\nquality = np.where(Y>=7,1,0)\n# We are generating two class labels : Bad quality wine and Good quality wine.\nquality = np.append(1-quality,quality,axis=1)\n\n# Observed values are calcualted by taking the dot product of class labels with the feature.\n# This is the same as counting how many alcohol samples with medium low alcohol content and low VA \n# belong to bad and good quality wines. \n# Output : [[Observed number of bad quality wine with med-low alc samples, Observed number of bad quality wine with low VA],[Observed number of good quality wine with med-low alc samples, Observed number of good quality wine with low VA]]\nobserved = np.dot(quality.T,chi2_feat_mat)\nprint(\"Observed values : \")\nprint(\"\\n\")\nprint(observed)\nprint(\"\\n\")\n\n# Now we move onto to calculate the expected values.\n\n# Calculating the number of samples with medium low alcohol content and number of samples with low VA content\n# Output : [number of med-low alcohol samples,  number of low VA samples]\nfeature_count = chi2_feat_mat.sum(axis=0).reshape(1,-1)\n# Calculating probability of bad and good quality wine\n# Output : [percent bad quality, percent good quality]\nclass_prob = quality.mean(axis=0).reshape(1,-1)\n\n# Calculating expected values\n# Output : [[ Expected percent bad quality wine with med-low alc samples, Expected percent bad quality wine with low VA],[Expected percent good quality wine with med-low alc samples, Expected percent good quality wine with low VA]]\nexpected = np.dot(class_prob.T,feature_count)\nprint(\"Expected values : \")\nprint(\"\\n\")\nprint(expected)\n\n# Calculating chi square statistic\nchisq = ((observed - expected)**2)\/expected\nchisq = chisq.sum(axis=0)\n\nprint(\"\\n\")\nprint(\"Chi square statistic for alcohol is : \")\nprint(round(chisq[0],3))\nprint(\"\\n\")\nprint(\"Chi square statistic for VA is : \")\nprint(round(chisq[1],3))","c52f3795":"print(\"Number of degrees of freedom is (r - 1)*(c - 1): \")\nprint((2-1)*(2-1))","12de334a":"We can see that there is some positive as well as negative correlation among the feature vectors.\nPositive Pearson Correlation coefficient\n1. Citric Acid vs Fixed Acidity :  0.6717\n2. Density vs Fixed Acidity : 0.668\n3. Total Sulphur dioxide vs Free Sulphur dioxide : 0.668\n\nNegative Pearson Correlation Coefficient\n1. pH vs Fixed Acidity : -0.683\n2. pH vs Citric Acid : -0.5419\n2. Citric Acid vs Volatile Acidity : -0.5525\n\nAlcohol seems to be the most correlated with quality (r = 0.4762) compared to other features.\n\nThe features most correlated with quality are alcohol, sulphates and volatile acidity.","c343fb7d":"# Extracting the features from the csv file","fdd931c9":"# 1.3 Embedded Method","d57d3524":"Importing required libraries ...","46710616":"# What are features ?","cad32e1b":"Lets calculate the number of degrees of freedom","6d024166":"# 1.1 Filter Method\n\n![Filter methods](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/2c\/Filter_Methode.png)\n\n> Reference : https:\/\/en.wikipedia.org\/wiki\/Feature_selection\n\nFilter methods stand apart in the sense that they select the features independent of the model. They are selected based on the correlation with the target variable. The features that contribute the least to the target variable prediction are suppressed and the ones that pass are selected to be passed on to a classification or a regression model.\n\nThey are effective in computation time and robust to overfitting.\n\nThere are two types of filter methods:\n\n1. Univariate filter methods : evaluate and rank single features. Each feature is treated independent of the feature space.\n\n2. Mulitivariate filter methods : These methods evaluate the entire feature space. They take into account the relationships between the features.","8ff83c71":"Features are the input data which a machine learning algorithm trains on to predict an output. For example a machine learning algorithm which predicts price of a house could take features such as number of bedrooms, size of house, number of ammenities etc as input.","03a26f56":"# 1.1.3 Statistical and Ranking Filter Methods","a5248632":"# 1.2 Wrapper Method","1e9a57f6":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bf\/Feature_selection_Embedded_Method.png)\n\n> Reference : https:\/\/en.wikipedia.org\/wiki\/Feature_selection\n\nIn embedded methods the feature selction process is added within the construction of the machine learning algorithm. They complete the feature selection process during the model training. The learning algorithm takes advantage of its variable selection process and performs feature selection and classificaton\/regression at the same time.","71d6808d":"#### CHI SQUARE TEST\n\nThe chi-square test refers to a class of statistical tests in which the sampling distribution is a chi-square distribution. The term usually refers to Pearson's chi-square test. \n\nThe chi squared test can be used to test for the probability of independence between two datasets and for performing a goodness-of-fit test. Here we will be using it for testing the probability of independence between two datasets. It will not tell any details about the relationship between them. For instance, in a survey conducted in which the ages of participants are recorded, a chi squared test can be used to determine if age affects the survey responses, or if the two are independent. \n\nChi-square score is commonly used for testing relationships between categorical variables and like any statistical hypothesis test, the chi square test has both a null hypothesis and an alternative hypothesis.\n\nThe null hypothesis states that there is no relationship between the categorical variables while the alternative hypothesis states that there is a relationship between the variables.\n\nThe Chi-square test of independence works by comparing the distribution that you observe to the distribution that you expect if there is no relationship between the categorical variables. In the Chi-square context, the word \u201cexpected\u201d is equivalent to what you\u2019d expect if the null hypothesis is true. If your observed distribution is sufficiently different than the expected distribution (no relationship), you can reject the null hypothesis and infer that the variables are related.\n\nThe chi squared statistic for interdependence is defined by \n\n### $$\\chi^2 = \\sum \\dfrac{(Observed - Expected)^2}{Expected}$$\n\nBefore we start calculating the chi squared score lets look at how a typical contingency table would look like\n\n<table style=\"width:100%; border:1px solid black\">\n    \n<tr>\n<th rowspan=\"2\" style=\"border:1px solid black; text-align:center\"> Features <\/th> \n<th colspan=\"4\" style=\"border:1px solid black; text-align:center\"> Class Labels <\/th>\n<\/tr>\n    \n<tr>\n<td style=\"border:1px solid black; text-align:center\"> Class 1 <\/td> \n<td style=\"border:1px solid black; text-align:center\"> Class 2 <\/td>\n<td style=\"border:1px solid black; text-align:center\"> ....... <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Class n <\/td>\n<\/tr>\n    \n<tr>\n<td style=\"border:1px solid black; text-align:center\"> Feature 1 <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{1,1} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{1,2} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> ....... <\/td>\n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{1,n} $$ <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Row 1 total <\/td>\n<\/tr>\n    \n<tr>\n<td style=\"border:1px solid black; text-align:center\"> Feature 2 <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{2,1} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{2,2} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> ....... <\/td>\n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{2,n} $$ <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Row 2 total <\/td>\n<\/tr>\n    \n<tr>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<td style=\"border:1px solid black; text-align:center\"> : <\/td>\n<\/tr>\n    \n<tr>\n<td style=\"border:1px solid black; text-align:center\"> Feature n <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{n,1} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{n,2} $$ <\/td> \n<td style=\"border:1px solid black; text-align:center\"> ....... <\/td>\n<td style=\"border:1px solid black; text-align:center\"> $$ Observation_{n,n} $$ <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Row n total <\/td>\n<\/tr>\n    \n<tr>\n<td> <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Column 1 total <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Column 2 total <\/td>\n<td style=\"border:1px solid black; text-align:center\"> ....... <\/td>\n<td style=\"border:1px solid black; text-align:center\"> Column n total <\/td>\n<td style=\"border:1px solid black; text-align:center\"> <b> N <\/b> <\/td>\n<\/tr>\n    \n<\/table>\n\n<br>\n\nThe chi square test for independence proceeds as follows :\n\n1. Calculate the chi-squred statistic $\\chi^2$ by using the formula shown above.\n\n2. Determine number of degrees of freedom  $df = (r-1)(c-1)$\n\n3. As an independence test, the usual confidence level is p = 0.95. If the chi-squared statistic exceeds the critical value under these conditions, the independence assumption can be rejected, and the two data sets are unlikely to be independent.\n\n### EXAMPLE\n\nLets take alcohol content and volatile acidity and see how well they are correlated with wine quality.\n\n#### Feature 1 : Alcohol content\n\nThe alcohol is measured as percent alcohol content of wine. In the data we have the maximum alcohol content is 14.9. According to the classification done by winefolly.\n\n<table>\n<tr>\n    <td style=\"border:1px solid black; text-align:center\"> Low Alcohol Content Wines <\/td>\n    <td style=\"border:1px solid black; text-align:center\"> 10 % <\/td>\n<\/tr>\n    \n<tr>\n    <td style=\"border:1px solid black; text-align:center\"> Medium Low Alcohol Content Wines <\/td>\n    <td style=\"border:1px solid black; text-align:center\"> 10-11.55 % <\/td>\n<\/tr>\n    \n<tr> \n    <td style=\"border:1px solid black; text-align:center\"> Medium Alcohol Content Wines <\/td>\n    <td style=\"border:1px solid black; text-align:center\"> 11.55-13.5 % <\/td>\n<\/tr>\n    \n<tr>\n    <td style=\"border:1px solid black; text-align:center\"> Medium High Alcohol Content Wines <\/td>\n    <td style=\"border:1px solid black; text-align:center\"> 13.5-15 % <\/td>\n<\/tr>\n    \n<tr>\n    <td style=\"border:1px solid black; text-align:center\"> High Alcohol Content Wines <\/td>\n    <td style=\"border:1px solid black; text-align:center\"> Over 15 % <\/td>\n<\/tr>\n<\/table>\n\n<br>\n\nIn our dataset most of the wines have alcohol content less than 11.55 % alcohol content. Lets split the alcohol feature vector such that values less than 11.55 % alcohol content we will regard it as medium low alcohol content wine (1) and greater than 11.55 %  as medium high alcohol content wines (0). I have chosen this format as most of the alcohol datapoints lie in the medium-low alchol content range so we can see if medium-low alcohol content correlates well with quality.\n\n#### Feature 2 : Volatile acidity\n\nVolatile acidity (VA) is a measure of the wine's gaseous acids that contributes to the smell and taste of vinegar in wine. Volatile acidity at too high levels can lead to an unpleasant, vinegar taste.\n\nVolatile acidity is measured as $ \\dfrac{g(acetic acid)}{L^{3}} $. Volatile acidity concentration is regulated by the Federal Tax and Trade Bureau, and allowable levels for various wine styles can be found in the Code of Federal Regualtions (CFR). The mentioned regulations for red and white wines are 1.4 and 1.2 g(acetic acid)\/L respectively.\n\nLets split the VA feaure vector such that values less than 0.7 we will regard it as low VA (1) and greater than 0.7 we will regard it as high VA(0). I have chosen this format as most of the VA acid datapoints lie in the low-VA range so we can see if low-VA correlates well with quality.","294514bc":"![](http:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/04\/Feature_selection_Wrapper_Method.png)\n\n> Reference : https:\/\/en.wikipedia.org\/wiki\/Feature_selection\n\nWrapper methods are based on greedy search algorithms as they evaluate all possible combinations of the features and select the combination that produces the best result for a specific machine learning algorithm. Some disadvantages associated with wrapper methods are :\n\n1. Testing all possible combinations of the features can be computationally expensive if the features set is too large.\n\n2. When number of observations is insufficient there is increased risk of overfitting.\n\nWrapper methods for feature selection can be divided into three categories:\n\n### 1. Step forward feature selection \/ Forward Feature Selection\n\nIn the first step, the performance of the classifier is evaluated with respect to each feature. The feature that performs the best is selected out of all the features. Selected feature depends on the evaluation criteria (prediction accuracy, RMSE etc). Next all possible combinations of the selected feature and subsequent feature are evaluated and second feature is selected. This process is continued until the required pre-defined number of features is selected.\n\n### 2. Step backward feature selection \/ Backward Feature Elimination\n\nHere we start with all the features in the dataset, and then we evaluate the performance of the machine learning algorithm. After that, at each iteration, backward feature elimination removes one feature at a time. It continues removing features until a certain criterion is satified.\n\n### 3. Exhaustive feature selection\n\nThe performance of the machine learning algorithm is evaluated against all possible subsets of features in the dataset. You can define the minimum and maximum number of features to have in a subset. The feature subset that yields best performance is selected. A downside to exhaustive feature selction is that it can be slower than compared to step forward or step backward feature selections.\n\n**As a rule of thumb, if the dataset is small, exhaustive feature selection method should be the choice, however, in case of large datasets, step forward or step backward feature selection methods should be preferred.**","5824b9de":"Mutual information (MI) (also known as Information gain) between two random variables is a non negative value, which measures the dependency between the variables. It is equal to 0 if and only if two random variables are independent, and higher values mean higher dependency. It can be used for univariate feature selection.\n\nThe input to a MI estimator is a list of (x,y) data points, whose underlying probability distribution $ \\mu(x,y) $ we can only guess based on how the data points are clustered. \n\nIt relies on non parametric methods based on entropy estimation from k-nearest neigbours\n\nDiscrete probability functions are functions that take discrete number of values and are referred to as probability mass functions.\nContinuous probability functions are defined for an infinite number of points over a continuous interval and are referred to as probability density functions.\n\nsklearn implementation:\n\nsklearn.feature_selection.mutual_info_classif(X,y,*,discrete_features = 'auto', n_neighbours=3,copy=True,random_state=None)\n\n1. X : feature matrix\n2. y : target vector\n3. discrete_features : default'auto'. If auto, it is assigned False for dense X and to True for sparse X. A sparse matrix is a matrix in which most of the elements are non zero. By contrast if most of the elements are nonzero then the matrix is called a dense matrix.\n4. n_neighbours : Number of neighbours to use for MI estimation for continuous variables.\n5. copy : bool, default True\n6. random_state : For adding small noise to continuous variables to remove repeated values. Default None\n\nMaths behind MI:\n\nSince our matrix is a dense matrix we will see the formula for mutual information for continuous distributions \n\n### $$ I(X;Y) = \\int_y\\int_x p_{(X,Y)}(x,y)\\log{\\dfrac{p_{(X,Y)}(x,y)}{p_X(x)p_Y(y)}}dxdy$$\n\n$p_{X,Y}$ is the joint probability density function of X and Y.\n$p_X$ and $p_Y$ are the marginal probability density functions of X and Y respectively.\n\nJoint Probability Distribution (Discrete case)\n\nX and Y are two discrete random variables. X takes values {x1, x2 .. xn} (can take 'n' values) and Y takes values {y1,y2 .. ym} (can take 'm' values). The ordered pair takes values in the product {(x1,y1),(x2,y2), .. (xn,yn)}. The joint probability mass function (joint pmf) of X and Y is the function p(xi,yj) giving the probability of the joint outcome X = xi and Y = yj.\n\nThis is organized in a joint probability table as follows :\n\n\n\n> Reference : https:\/\/ocw.mit.edu\/courses\/mathematics\/18-05-introduction-to-probability-and-statistics-spring-2014\/readings\/MIT18_05S14_Reading7a.pdf\n\nJoint Probability Distribution (Continuous case)\n\nIn the continuous case discrete sets are replaced with continuous intervals and the joint probability mass function (joint pmf) is replaced with joint probability density functions (joint pdf).\n\nIf X takes values in [a,b] and Y takes values in [c,d] then the pair (X,Y) takes values in the product [a,b] x [c,d]. The joint probability density function is the function f(x,y) giving the probability density at (x,y). That is, the probability that (X, Y) is in a small rectangle of width dx and height dy around (x, y) is f(x,y)dx dy \n\n\n\n> Reference : https:\/\/ocw.mit.edu\/courses\/mathematics\/18-05-introduction-to-probability-and-statistics-spring-2014\/readings\/MIT18_05S14_Reading7a.pdf\n\nNote : probability density functions can take values greater than 1 as they are probability densities and not probabilities.\n\nMarginal Probability Distribution\n\nThe individual probability distribution of a random variable is reffered to as its marginal probability distribution.\n\nWhen the joint probability function concides with the product of the marginal probability functions that is when X and Y are independent (and hence observing Y tells you nothing about X).\n","3bba6b2b":"# 1. Feature Selection","1dcbe95a":"# 1.1.1 Basic Filter Methods\n\nThese are methods to drop features which have for example constant values, a value which occupies a majority of the dataset and features which have duplicate values.","b4366f73":"## The number of features and the variance in each feature are important factors to consider before building the model.\n\n* If too many features are fed into the model it may overfit and lose its ability to generalize. \n* If there isn't much variance in your features then that feature will likely not influence the target varable and hence can be eliminated.\n\nIn this tutorial I will be going over the maths of some the algorithms in feature selection and feature elimination and using these algorithms in a red wine quality dataset.\n\nIn feature selection, you have a set of features and you want to select a subset from those features which influence the target variable the greatest.\n\nAdvantanges of using feature selection are :\n\n* It allows models to be simplified making them easier to be interpreted and leads to better generalization.\n* Shorter training times\n\nFeature selection process is a combinatorial optimization problem. A combinatorial optimization problem aims to use combinatorial techniques to solve discrete optimization problems. A discrete optimization problem seeks to find the best possible solution from a finite set of possibilities. To overcome this problem it is frequently assumed either that features independently influence the class variable or do so only involving pairwise feature interaction.\n\nThree main categories of feature selection algorithms are wrappers, filters and embedded methods.","9df85600":"# 1.1.2 Correlation Filter Methods\n\nA simple definition for what correlation means is the relationship between two variables. These methods try to measure the correlation between the feature and its target and select the ones which are highly correlated with the target. If two variables are highly correlated with each other then we require only one variable to predict the target and the other becomes redundant and can be eliminated (can be measured using multivariate methods).\n\nPearson's correlation coefficient (PCC) is the most common linear coefficient measuring the degree of correlation between two variables and is denoted by *r*.\n\n*r* ranges between +1 and -1 and a value of +1 means that there is a pefect positive relation between the variables and are related by an increasing linear function. A value of -1 means that there is a perfect negative relation between the variables and are related by a decreasing function. A value of 0 means no predictive strength between the variables.\n\n### $$r = \\dfrac{\\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\mu_y)^2}}$$\n\nThe above formula can also be written as:\n\n### $$r = \\dfrac{Cov(x,y)}{\\sigma_x\\sigma_y}$$\n\nwhere,\n\n### $$Cov(x,y) = \\dfrac{\\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)}{n-1}$$","76a7055c":"\n\nFor classification sklearn has three algorithms. All of these are for univariate feature selection.\n\n1. chi2 (Chi squared stats of non-negative features for classification tasks)\n2. f_classif (ANOVA F-value between label\/feature for classification tasks)\n3. mutual_info_classif (Estimate mutual information for a discrete target variable)\n\nThe statistical feature selection methods can be divided into two classes : univariate and multivariate. As the name suggests univariate considers only a single feature and sees how well it correlates with the target variable. A disadvantage over here is that it discards features when taken as an aggregate would have contributed to the target variable. In order to overcome this multivariate feature selection processes use multivariate statistics where they consider dependencies between features when calculating scores for features\n\n#### CHI SQUARED DISTRIBUTION\n\nA standard normal distribution is a normal distribution with mean 0 and standard deviation of 1. It is given by the following probability density function.\n\n### $$ P_r(x) = \\dfrac{e^{-x^2\/2}}{\\sqrt{2\\pi}} $$\n\nConider a random variable 'Z' sampled from the standard normal distribution and is squared to give random variable Q. ($ Q = Z^2 $). The distribution of the random variable Q is an example of chi2 distribution. Since we are only squaring a single normal random variable it produces a chi2 distribution of degree 1 however if you have two standard normal distributions and sample and square the random variables and then add them together you will get a chi2 distribution of degree 2. In the below code you can see how to get a chi2 distribution of degree 3 and 5.\n\nThe probability density function for the chi2 distribution with 'r' degrees of freedom is given by\n\n### $$ P_r(x) = \\dfrac{x^{r\/2-1}e^{-x\/2}}{\\Gamma(r\/2)2^{r\/2}} $$","9adf3fd2":"# 3. References \n\nList of feature selection methods :\n1. https:\/\/heartbeat.fritz.ai\/hands-on-with-feature-selection-techniques-filter-methods-f248e0436ce5\n\nLinks for infromation on Correlation :\n1. https:\/\/brilliant.org\/wiki\/correlation\/\n\nLinks for Chi squared test : \n1. https:\/\/brilliant.org\/wiki\/chi-squared-test\/\n2. https:\/\/www.investopedia.com\/terms\/c\/chi-square-statistic.asp\n3. https:\/\/statisticsbyjim.com\/hypothesis-testing\/chi-square-test-independence-example\/\n\nContingency Table\n1. https:\/\/math.hws.edu\/javamath\/ryan\/ChiSquare.html\n2. https:\/\/medium.com\/analytics-vidhya\/pearsons-chi-squared-test-from-scratch-with-python-ba9e14d336c\n3. https:\/\/www.kaggle.com\/kuldeepnpatel\/chi-square-test-of-independence\n4. https:\/\/machinelearningmastery.com\/chi-squared-test-for-machine-learning\/\n\n\nLinks for Mutual information :\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif\n2. https:\/\/en.wikipedia.org\/wiki\/Mutual_information\n3. https:\/\/en.wikipedia.org\/wiki\/Joint_probability_distribution\n4. https:\/\/ocw.mit.edu\/courses\/mathematics\/18-05-introduction-to-probability-and-statistics-spring-2014\/readings\/MIT18_05S14_Reading7a.pdf\n5. https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3929353\/\n\nWrapper Methods :\n1. https:\/\/stackabuse.com\/applying-wrapper-methods-in-python-for-feature-selection\/\n\nEmbedded Methods : \n1. https:\/\/heartbeat.fritz.ai\/hands-on-with-feature-selection-techniques-embedded-methods-84747e814dab\n\nSymbols used for Latex equation :\n1. https:\/\/www.calvin.edu\/~rpruim\/courses\/s341\/S17\/from-class\/MathinRmd.html\n\nWine Quality :\n1. https:\/\/winefolly.com\/tips\/the-lightest-to-the-strongest-wine\/\n2. https:\/\/extension.psu.edu\/volatile-acidity-in-wine\n\n\n\n","1d95f596":"Considering p-value of 0.95 and 1 degree of freedom lets find out the critical values of the chi2 distribution.\n\n![Chi2 critical values](https:\/\/faculty.elgin.edu\/dkernler\/statistics\/ch09\/images\/chi-square-table.gif)\n\n* We see that the critical value is 0.004 for dof = 1 and p-val of 0.95\n* The chi square statistics for alcohol and volatile acidity are both greater than the critical value and hence we can reject the null hypothesis.","dfb536b0":"# 2. Feature Extraction","6e4f944a":"# 1.1.4 MUTUAL INFORMATION "}}