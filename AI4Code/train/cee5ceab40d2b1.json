{"cell_type":{"07b667aa":"code","c9b5d3fb":"code","4017a4fb":"code","2dfec61c":"code","03a85ca1":"code","db6c9bbe":"code","e2108dde":"code","2c01bd75":"code","94e08cae":"code","d5525717":"code","1a28c889":"code","8e95c0f5":"code","f5c9c7b8":"code","d8a9df97":"code","f4c588c4":"code","f5d08e8a":"code","55bf75c7":"code","3de1056b":"code","db1ccfbb":"markdown","72a88937":"markdown","eb9c5564":"markdown","32456502":"markdown","cc735bb7":"markdown","3a5b3133":"markdown","40a664ad":"markdown","e1eedcc5":"markdown","5def6787":"markdown","08e118ed":"markdown","c63d124f":"markdown","22247efc":"markdown","c409c954":"markdown","15fc80e0":"markdown","78a0ebff":"markdown","c4855104":"markdown","8e29fff8":"markdown","1dae5411":"markdown","af61ccdc":"markdown"},"source":{"07b667aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\n\n#Setting the properties to personal preference\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.rcParams[\"figure.figsize\"] = (8,4)\nwarnings.filterwarnings(\"ignore\")    # No major warnings came out on first run. So, I am ignoring the \"deprecation\" warnings instead of showing them the first time to keep the code clean\n\nprint(os.listdir(\"..\/input\"))\ndf = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","c9b5d3fb":"df.columns","4017a4fb":"df.drop('Serial No.', inplace=True, axis=1)\ndf.rename({'Chance of Admit ': 'Chance of Admit', 'LOR ':'LOR'}, axis=1, inplace=True)","2dfec61c":"df.tail()","03a85ca1":"df.info()","db6c9bbe":"df.describe()","e2108dde":"for rating in sorted(df['University Rating'].unique()):\n    print(\"For University Rating: \", rating, \"\\n\")\n    print(df[df['University Rating']==rating].describe(), 2*\"\\n\")","2c01bd75":"for rating in sorted(df['University Rating'].unique()):\n    sns.jointplot(data=df[df['University Rating']==rating], x = 'GRE Score', y = 'Chance of Admit')\n    print(\"Jointplot for the University Rating: \", rating)\n    plt.show()","94e08cae":"for rating in sorted(df['University Rating'].unique()):\n    sns.distplot(df[df['University Rating']==rating]['GRE Score'], hist=False)\nplt.show()","d5525717":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(), annot=True)","1a28c889":"sns.pairplot(df)","8e95c0f5":"features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research']\ny = df['Chance of Admit']\nX = df[features]","f5c9c7b8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","d8a9df97":"y_train_label = [1 if each > 0.8 else 0 for each in y_train]\ny_test_label  = [1 if each > 0.8 else 0 for each in y_test]","f4c588c4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\n\n# Create a pipeline\npipe = Pipeline([('classifier', LogisticRegression())])\n\n# Create space of candidate learning algorithms and their hyperparameters\nsearch_space = [{'classifier': [LogisticRegression()]},\n        {'classifier': [SVC()]},\n        {'classifier': [KNeighborsClassifier(n_neighbors=5)]}]\n\n# Create grid search \nclf = GridSearchCV(pipe, search_space)\n\n# Fit grid search\nbest_model = clf.fit(X_train, y_train_label)\n# View best model\nprint(best_model.best_estimator_.get_params()['classifier'], \"\\n\")\nprint(\"Accuracy of our best model is\", clf.score(X_test, y_test_label)*100, \"%\", \"\\n\")\nprint(\"Classification Report:\", \"\\n\", classification_report(y_test_label, best_model.predict(X_test)))","f5d08e8a":"lg = LogisticRegression()\n\nlg.fit(X_train, y_train_label)\npredictions = lg.predict(X_test)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_label, predictions))","55bf75c7":"svmmodel = SVC()\nsvmmodel.fit(X_train,y_train_label)\ny_pred_svm = svmmodel.predict(X_test)\n\nprint(classification_report(y_test_label, y_pred_svm))","3de1056b":"knn = KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(X_train,y_train_label)\ny_pred_knn = knn.predict(X_test)\n\nprint(classification_report(y_test_label, y_pred_knn))","db1ccfbb":"> I am using 80% 'Chance of Admit' as my cut off point to change Chance of Admit from float to binary labels.","72a88937":"## Stats for each University Ranking (1-5)","eb9c5564":">Surprising to see that even at the highest rated universities the students with GRE score as low as 303 have 61% chance of admitting in.\nLet's do some more EDA to dissect the data.","32456502":"> 95% accuracy!!! Better than our Logistic Regression model. \nLR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM. That might be the reason why SVM is giving a better accuracy than Logistic Regression. ","cc735bb7":">'GRE Score' \"seems\" to be a pretty good indicator for Chances of getting in.\nLet's create a heatmap to confirm our hypothesis.","3a5b3133":"## Time to implement some Machine Learning to predict 'Chance of Admit'. ","40a664ad":">Data looks pretty clean and in the right data type format.","e1eedcc5":"> Top 3 factors that influence the Chances of Admit are **'CGPA', 'GRE Score', 'TOEFL Score'** in that order.","5def6787":"## 3. KNeighborsClassifier","08e118ed":"## Continously improving the notebook. Revisit later for more...","c63d124f":"> Looks like SVC gave the best accuracy (94.55 %) out of the three models.","22247efc":"## Exploratory Data Analysis (EDA)","c409c954":"## 2. Support Vector Machine (SVM)","15fc80e0":"# Individual Model Performance\n## 1. Logistic Regression","78a0ebff":"# GridSearch for the Best Model","c4855104":"> 93% accuracy gives it the second place behind SVM","8e29fff8":"> 92% overall accuracy seems fine. Let's try SVM now.","1dae5411":"Looking at the heatmap (above), it seems like there are multiple factors affecting the 'Chance of Admit'. So, let's start with Logistic Regression and compare the accuracy to that of SVM's","af61ccdc":">Lot of \"cool\" info from the .describe(). Let's dive deeper and see this info for each \"University Rating\""}}