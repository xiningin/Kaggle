{"cell_type":{"f085bd3c":"code","2bc8b88d":"code","790b43d5":"code","1d99db4d":"code","f2477a1a":"code","7aeab703":"code","84f1c44a":"code","5cff448b":"code","0d789b66":"code","d7fad5e6":"code","79259713":"code","27b8b256":"code","f8f34bdf":"code","d33bad49":"code","e416e7f3":"code","3843e863":"code","572f0792":"code","d1277b49":"code","6f967a66":"code","a978267c":"code","da446fe3":"code","8f36bc0c":"code","e8acc9a7":"code","411729c7":"code","4bc07857":"code","3f908fb6":"code","30707b52":"code","d57f7c0f":"code","3e8f9144":"code","c8724915":"code","028ac586":"code","781a0f3e":"code","b1f34f38":"code","31f17a6a":"markdown","d5e86a6e":"markdown","700987c3":"markdown","d8be4be6":"markdown","8ee6ec10":"markdown","cb30785d":"markdown","f4f5ff8a":"markdown","cce2b0ab":"markdown","913fd0e6":"markdown","b4f4e69e":"markdown","f896fe08":"markdown","a3073546":"markdown","0326c75a":"markdown","6e0d7500":"markdown","a9ae1550":"markdown","21171b4e":"markdown","b9565a47":"markdown","7a16044d":"markdown","857ca212":"markdown","9c5deb4e":"markdown","44b59e4d":"markdown","1268396b":"markdown","a898f03f":"markdown","67864ae3":"markdown","450ecd61":"markdown","9347743e":"markdown","cc1e67fb":"markdown","6de144ab":"markdown","9ebc20cb":"markdown","b1a4377e":"markdown","81e2382c":"markdown","cd8afe52":"markdown","17dbe855":"markdown","c1a1dc16":"markdown","bb4cd757":"markdown"},"source":{"f085bd3c":"import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom scipy import stats\nfrom scipy.stats.distributions import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2bc8b88d":"# Columns 3 and 4 have date information\ndf = pd.read_csv('\/kaggle\/input\/noshowappointments\/KaggleV2-May-2016.csv', parse_dates=[3,4])\nprint(df.info())\ndf.head()","790b43d5":"# There are some spelling mistakes and some columns have slightly\n# misleading names\ndf.rename(columns = {'Hipertension': 'Hypertension',\n                         'Handcap': 'Handicap',\n                        'ScheduledDay': 'AppointMade',\n                        'AppointmentDay': 'AppointFor'}, inplace = True)\n\n# Appointment ID is unique for each isntance and so can be used for the index\ndf.set_index('AppointmentID', inplace=True)\n# Replace M and F with 1 and 0 to make it easier to test with statistical models\ndf['Gender'].replace(('M', 'F'), (1, 0), inplace=True)\n\nprint(df.info())\n\n# No show is 'Yes' and 'No'. Making these into dummies gives us flexibility in\n# how we use the data for statistics\ndummies = pd.get_dummies(df['No-show'])\ndf = pd.concat((df, dummies), axis = 1)\n\n# The overall probability of no show\nnoshow_prob = df.Yes.sum() \/ (df.Yes.sum() + df.No.sum())\nprint(noshow_prob)","1d99db4d":"print(df.describe())","f2477a1a":"# Removing entries where the age is less than 0\ndf = df.loc[df['Age'] >= 0]\n# showing the smallest and largest age categories and how many are in each\nprint(df.groupby('Age')['PatientId'].count())\n\n# Creating a simple histogram to show distribution\ndf['Age'].hist(bins = 15)\nplt.title('The distribution of ages')\nplt.xlabel('Age')\nplt.ylabel('Number of appointments')\nplt.show()","7aeab703":"# GRouping all appointments by age and then calculating the probability of \n# no show for each age\ndf_age = pd.concat([df.groupby('Age')['No'].sum(), df.groupby('Age')['Yes'].sum()], axis=1)\ndf_age['Probability of no-show'] = df_age['Yes'] \/ (df_age['No'] + df_age['Yes'])\ndf_age.reset_index(inplace=True)\n\n# Plotting the probabiltiy of no-show by age\ndf_age.plot(kind='scatter', x='Age', y='Probability of no-show')\n# Creating a line at the overall probability\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('All appointments grouped into age categories')\nplt.show()\n\ndf_age.head()","84f1c44a":"# reducing all ages over 85 to 85\nmax_age = 85\ndf['Age'][df['Age'] >= max_age] = max_age\n\n# Calculating the new probabilities with all ages capped at 85\ndf_age = pd.concat([df.groupby('Age')['No'].sum(), df.groupby('Age')['Yes'].sum()], axis=1)\ndf_age['Probability of no-show'] = df_age['Yes'] \/ (df_age['No'] + df_age['Yes'])\ndf_age.reset_index(inplace=True)\n\n# PLotting the new probabilities with all ages capped at 85\ndf_age.plot(kind='scatter', x='Age', y='Probability of no-show')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('All ages over 84 grouped into one age group')\nplt.show()","5cff448b":"print('Correlation for Handicap is', df['Handicap'].corr(df['Yes']))\n\n# splitting the handicap feature into dummies and loking for a correlaion with no-show\n# for each of them in turn\ndummies_hand = pd.get_dummies(df['Handicap'])\nfor dum in dummies_hand.columns:\n    cor = dummies_hand[dum].corr(df['Yes'])\n    print(f'Correlation for handicap - {dum} is {cor}')\n\n# Grouping all appointments by patient handicap and calculating the probability\n# of no-show for each of them\ndf_hand = pd.concat([df.groupby('Handicap')['No'].sum(), df.groupby('Handicap')['Yes'].sum()], axis=1)\ndf_hand['Probability of no-show'] = df_hand['Yes'] \/ (df_hand['No'] + df_hand['Yes'])\ndf_hand.reset_index(inplace=True)\n\n# Plotting the probability of no-show for each of the 5 handicap categories\ndf_hand.plot(kind='bar', x='Handicap', y='Probability of no-show')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('Handicap split into its categories')\nplt.ylabel('Probability of no-show')\nplt.show()","0d789b66":"# This is a list of all the binary features\nbinary_cats = ['Gender', 'Scholarship', 'Hypertension', 'Diabetes', 'Alcoholism']\n\n# Calculating the probablity of no show for both categories in each binary feature\ndf_probs = pd.DataFrame()\nfor cat in binary_cats:\n    probs = []\n    for unique in df[cat].unique():\n        probs.append(df[(df[cat] == unique) & (df['Yes'] == 1)].shape[0] \/ df[df[cat] == unique].shape[0])\n    df_probs[cat] = probs\n\n# Altering the df to make it easy to plot. We need a row for each bar\n# We need a column with the probability, a column for the feature and a \n# column for the category\ndf_probs = df_probs.T\ndf_probs2 = pd.melt(df_probs.reset_index(), id_vars='index')\n\ng = sns.factorplot(x='index', y=\"value\", hue=\"variable\", data=df_probs2, size=4,\n                     aspect=3, kind=\"bar\", legend=False)\nplt.legend()\nplt.title('How do the categorical features affect probability of no-show')\nplt.ylabel('Probability of no-show')\nplt.xlabel('')\nplt.show()","d7fad5e6":"# Creating a Pearson correlation matrix to visualise correlations between features\ncorr_cats = ['Age', 'Scholarship', 'Hypertension', 'Diabetes', 'Handicap', 'SMS_received', 'Yes']\ndf_corr = df.loc[:, corr_cats]\ncorr = df_corr.corr()\nfig, ax = plt.subplots(dpi=200)\nsns.heatmap(corr, cmap = 'Wistia', annot= True, ax=ax, annot_kws={\"size\": 6})\nplt.show()","79259713":"# How many differnet neighbourhoods are there\nneigh_size = df['Neighbourhood'].unique().size\nneighbourhoods = df['Neighbourhood'].unique()\n\n# Visualising the neighbourhood feature and each of its categories\nprint(df['Neighbourhood'].unique())\nprint(f'\\nNumber of neighbourhoods - {neigh_size} \\n')\nprint(f'The overall probability of a no show is {noshow_prob}')\n\n# Calculating the probability of no-show for each neighbourhood\ndf_neigh = pd.concat([df.groupby('Neighbourhood')['No'].sum(), df.groupby('Neighbourhood')['Yes'].sum()], axis=1)\ndf_neigh['Total'] = df_neigh.sum(axis=1)\ndf_neigh['Probability of no-show'] = df_neigh['Yes'] \/ df_neigh['Total']\ndf_neigh.sort_values('Total', ascending=False, inplace=True)\nprint(df_neigh)","27b8b256":"# Setting our minimum sample size for a category within neighbourhood\nsmall_samp_size = 50\n\nprint(f'The number of districts with sample size above our threshold of {small_samp_size} is -')\nprint(df_neigh[df_neigh['Total'] >= small_samp_size].shape[0])\nprint(f'The number of districts with sample size below our threshold of {small_samp_size} is -')\nprint(df_neigh[df_neigh['Total'] < small_samp_size].shape[0])\nprint('The number of rejected samples is then')\nprint(df_neigh[df_neigh['Total'] < small_samp_size]['Total'].sum())\n\n# Removing instances that come from categories with not enough samples\ndf_neigh = df_neigh[df_neigh['Total'] >= small_samp_size]","f8f34bdf":"# Calculating the chi-squared for neighbourhood as a feature in its entirety\n# This is extracting the necessary data\nneigh_vals = df_neigh.loc[:, ['Yes', 'No']].values\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(neigh_vals)\nprint(f'Chi squared value is {chi2_stat} and the p-value is {p_val}')","d33bad49":"# Calculating the expected Yes and No values by the number of instances\n# within each category timesed by the overall probability of no-show\ndf_neigh['Exp_yes'] = df_neigh['Total'] * noshow_prob\ndf_neigh['Exp_no'] = df_neigh['Total'] - df_neigh['Exp_yes']\n# Reordering the column titles so it goes No then Yes and also Exp_no and then Exp_yes\ncolumns_titles = ['No', 'Yes', 'Probability of no-show', 'Exp_no','Exp_yes']\ndf_neigh=df_neigh.reindex(columns=columns_titles)\n\nprint(df_neigh)","e416e7f3":"def chi_squared(row):\n    # Extracting the observed and expected values form each row\n    observed = row[['No', 'Yes']].values\n    expected = row[['Exp_no', 'Exp_yes']].values\n    \n    # Chi squared on this 1x2 set of values\n    chi = expected - observed\n    chi = chi * chi\n    chi = chi \/ expected\n    chi = np.sum(chi)\n    \n    # calculating the p-value with 1 degree of freedom (dof)\n    pval = chi2.sf(chi,1)\n    \n    return pd.Series({'chi': chi, 'pvalue': pval})\n\n# Applying the chi squared function to each row of a matrix\nchi_results = df_neigh.apply(chi_squared, axis = 1)\nprint(chi_results)","3843e863":"# making a list of neighbourhoods with a chi squared value of under 0.05\nneigh_keep = chi_results[chi_results['pvalue'] <= 0.05].index.tolist()\nprint('Number of kept neighbourhoods with p-val under 0.05 is -', len(neigh_keep))\nprint(neigh_keep)\n\n# making a list of neighbourhoods with a chi squared value of under 0.01\nneigh_keep2 = chi_results[chi_results['pvalue'] <= 0.01].index.tolist()\nprint('Number of kept neighbourhoods with p-val under 0.01 is -', len(neigh_keep2))","572f0792":"# Turning the datetime informaiton into the correct form\n# We want the datetime the appointment was made so we can order the data like\n# that at some point\n# we want the Day datetime so we can calculate number of days between them easily\ndf['AppointMade'] = df['AppointMade'].values.astype('datetime64[s]')\ndf['AppointMadeD'] = df['AppointMade'].values.astype('datetime64[D]')\ndf['AppointFor'] = df['AppointFor'].values.astype('datetime64[D]')\n\n# grouping the appointments into the week of the year it was made and counting\n# the number of instances\ndf.groupby(df[\"AppointMade\"].dt.week)[\"AppointMade\"].count().plot(kind=\"bar\", color='b', alpha=0.3)\nplt.title('When the appointment was made')\nplt.xlabel('Week of the year')\nplt.ylabel('Number of appointments made')\nplt.show()\n\n# grouping the appointments into the week of the year it was made for and counting\n# the number of instances\ndf.groupby(df[\"AppointFor\"].dt.week)[\"AppointFor\"].count().plot(kind=\"bar\", color='r', alpha=0.3)\nplt.title('When the appointment was made for')\nplt.xlabel('Week of the year')\nplt.ylabel('Number of appointments')\nplt.show()\n\n# Calculating the number of days inbetween the appointment and when it was made\n# Have to convert it into an integer for later use\ndf['days_wait'] = (df['AppointFor'] - df['AppointMadeD'])  \/ np.timedelta64(1, 'D')\ndf['days_wait'] = df['days_wait'].astype(int)\nprint(df['days_wait'].describe())","d1277b49":"print('Number of days being cut as the appointment was before it was made... ', \n      df[df['days_wait'].astype(int) < 0].shape[0])\n# Removing all instances where the appointment was before the day it was made\n# These are obviously typos and should be removed\ndf = df[df['days_wait'] >= 0]\n\n# GRouping all appointments into how many days the patient had to wait\n# and calculating a probability of showing up for each 'number of days wait'\ndf_days = pd.concat([df.groupby('days_wait')['No'].sum(), df.groupby('days_wait')['Yes'].sum()], axis=1)\ndf_days['Total'] = df_days['No'] + df_days['Yes']\ndf_days['Probability of no-show'] = df_days['Yes'] \/ df_days['Total']\ndf_days.reset_index(inplace=True)\n\n# Plotting the probabilities of no-show for each days_waited value\ndf_days.plot(kind='scatter', x='days_wait', y='Probability of no-show')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('All appointments grouped into waiting time')\nplt.xlabel('Days between appointment and when it was made')\nplt.show()\n\n# For making a distribution of waiting times\ndf_days_plot = df.groupby(\"days_wait\")[\"days_wait\"].count()\n# Misses out the first data point as it is so much higher\n# and then shows every 5th or there are too many bars and can't read xaxis\ndf_days_plot.iloc[1::5].plot(kind=\"bar\", color='b')\nplt.ylim(0,6000)\nplt.title('Distribution of waiting times')\nplt.xlabel('Days between appointment and when it was made')\nplt.ylabel('Total appointments')\nplt.show()","6f967a66":"# Creating a feature for if the appointment was made on the same day\ndf['same_day'] = np.NaN\ndf.loc[df['days_wait'] == 0, 'same_day'] = 1\ndf['same_day'].fillna(0, inplace=True)\n\n# capping the wait time at 75. above that there aren't enough samples for\n# reliable data\nmax_days = 75\ndf['days_wait'][df['days_wait'] >= max_days] = max_days\n\n# Grouping all appointments into number of days waiting and caluclaing the \n# probability of no-show\ndf_days = pd.concat([df.groupby('days_wait')['No'].sum(), df.groupby('days_wait')['Yes'].sum()], axis=1)\ndf_days['Total'] = df_days['No'] + df_days['Yes']\ndf_days['Probability of no-show'] = df_days['Yes'] \/ df_days['Total']\ndf_days.reset_index(inplace=True)\n\n# Plotting the probabilities\ndf_days.plot(kind='scatter', x='days_wait', y='Probability of no-show')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('All appointments grouped into waiting time')\nplt.xlabel('Days between appointment and when it was made')\nplt.show()","a978267c":"# Extracting the hour of the day form the appointment made time\ndf['hour_made'] = df['AppointMade'].dt.hour\n\n# GRouping all appointments into the time of day it was made and \n# calculating probability of no-show for each value\ndf_hour = pd.concat([df.groupby('hour_made')['No'].sum(), df.groupby('hour_made')['Yes'].sum()], axis=1)\ndf_hour['Total'] = df_days['No'] + df_hour['Yes']\ndf_hour['Probability of no-show'] = df_hour['Yes'] \/ df_hour['Total']\ndf_hour.reset_index(inplace=True)\n\ndf_hour.plot(kind='scatter', x='hour_made', y='Probability of no-show')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.legend()\nplt.title('All appointments grouped into hour appointment was made')\nplt.show()","da446fe3":"# To replace 0,2,3,4,5 with the actual days of the week\n# there are no appointments on or made on sunday\nweek_days = ['Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat']\n\n#  GEtting the day of he week the appointment was made form the date\ndf['dow_made'] = df['AppointMade'].dt.dayofweek\n# Grouping all appointments into the day of the week it was made and \n# calculating probability of no-show for each value\ndf_dow = pd.concat((df.groupby('dow_made')['Yes'].sum(), df.groupby('dow_made')['No'].sum()), axis=1)\ndf_dow['Probability of no-show'] = df_dow['Yes'] \/ (df_dow['Yes'] + df_dow['No'])\ndf_dow.index = week_days\n\nprint(df_dow.head(7))\ndf_dow['Probability of no-show'].plot(kind='bar')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.xlabel('Days of the week')\nplt.ylabel('Probability of no-show')\nplt.title('When the appointment was made')\nplt.show()\n\n# Grouping all appointments into the day of the week it was made FOR and \n# calculating probability of no-show for each value\ndf['dow_for'] = df['AppointFor'].dt.dayofweek\ndf_dowf = pd.concat((df.groupby('dow_for')['Yes'].sum(), df.groupby('dow_for')['No'].sum()), axis=1)\ndf_dowf['Probability of no-show'] = df_dowf['Yes'] \/ (df_dowf['Yes'] + df_dowf['No'])\ndf_dowf.index = week_days\n\nprint(df_dowf.head(7))\ndf_dowf['Probability of no-show'].plot(kind='bar')\nplt.axhline(noshow_prob, c='r', label='Overall probability')\nplt.xlabel('Days of the week')\nplt.ylabel('Probability of no-show')\nplt.title('The day of the appointment')\nplt.show()","8f36bc0c":"# This function looks at all appointments made before the current appointFor date\n# and where the patient number is the same as at the current row\n# and count the number of missed appointments\ndef count_missed_apts_before_now(row, df):\n    subdf = df.query(\"AppointFor<@row.AppointMade and\\\n                     `No-show`=='Yes' and PatientId==@row.PatientId\")\n    return len(subdf)\n\n# Sorting by appointment made date so we can get rolling counts\ndf.sort_values(by='AppointMade', inplace=True)\n# Calculating how many time the patient has made an appointment before\ndf['book_count'] = df.groupby('PatientId').cumcount()\n# Calculating the number of times a patient has missed an appointment before making the current one\nt3 = time.time()\ndf['miss_count'] = df.apply(count_missed_apts_before_now, axis=1, args = (df,))\nt4 = time.time()\nmiss_count_t = t4-t3\nprint(f'miss count column calculated in {miss_count_t}')","e8acc9a7":"# These additional date categories are of no use after processing the info we need\ndf.drop(['dow_made', 'dow_for', 'AppointFor', 'AppointMade', 'AppointMadeD'], \n        axis=1, inplace=True)\n# These are the results categories but in the wrong form\ndf.drop(['No', 'No-show'], axis=1, inplace=True)\n# These two categories showed no correlation with showing up for an appointment\ndf.drop(['Gender', 'Alcoholism'], inplace=True, axis=1)\n# handicap may have some vlaue and would need to be treated as dummy cols\n# but there may be some cross-correlation with age\ndf.drop('Handicap', axis=1, inplace=True)\n# These categories showed a weak correlation so could tell us something, but there\n# was considerable cross correlation with Age\ndf.drop(['Scholarship', 'Hypertension', 'Diabetes'], inplace=True, axis = 1)\n# No longer need Patient ID as we have all the information on repeat patients\ndf.drop('PatientId', inplace=True, axis=1)\n\ndf['Age^2'] = df['Age'] ** 2\ndf['Age^3'] = df['Age'] ** 3\ndf['hour_made^2'] = df['hour_made'] ** 2\ndf['hour_made^3'] = df['hour_made'] ** 3\n# Only going up to squared for days_wait as the relationship seemed to be more simple\ndf['days_wait^2'] = df['days_wait'] ** 2\n\ndf.head()","411729c7":"# These will be added to the dataframe during feature engineering\ndum_neigh = pd.get_dummies(df['Neighbourhood'])\n# neigh_keep is my list of statistically significant neighbourhoods\ndum_neigh = dum_neigh[neigh_keep]\n\ndf = pd.concat((df, dum_neigh), axis = 1)\ndf.drop('Neighbourhood', axis=1, inplace=True)\nprint(df.columns.tolist())\n\ndum_neigh.head()","4bc07857":"# Splitting the dataframe into features and the target\nX = df.loc[:, df.columns != 'Yes']\ny = df.loc[:, df.columns == 'Yes']\n\n# Splitting the dataframe into a train dataset and a test dataset\n# random state used so its the same split each time. this aids with comparisons\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nfeatures = X_train.columns","3f908fb6":"# substantiating an isntance of the SMOTE object\nos = SMOTE(random_state=0)\n\n# Adding a lot of new dummy data for no-show = Yes to even out the Yes and No\nos_data_X, os_data_y = os.fit_sample(X_train, y_train)\n# Giving the new dataframes the old names\nos_data_X = pd.DataFrame(data=os_data_X, columns=features )\nos_data_y = pd.DataFrame(data=os_data_y, columns=['Yes'])\n\nprint('old number of instances is ', X_train.shape[0])\nprint('old number of no shows is ', y_train['Yes'].sum())\nprint('proportions of no shows is ', y_train['Yes'].sum() \/ X_train.shape[0], '\\n\\n')\n\nprint('new number of instances is ', os_data_X.shape[0])\nprint('new number of no shows is ', os_data_y['Yes'].sum())\nprint('proportions of no shows is ', os_data_y['Yes'].sum() \/ os_data_X.shape[0])","30707b52":"# These are all fetures which go way above 1. They need to be brough back to a \n# similar scale as our binary features\nto_normalize = ['Age', 'Age^2', 'Age^3', 'hour_made', 'hour_made^2', 'hour_made^3', \n                'days_wait', 'days_wait^2', 'book_count', 'miss_count']\n\n# Normalizing very simply by just dividing be the range\nfor cat in to_normalize:\n    os_data_X[cat] = os_data_X[cat] \/ (os_data_X[cat].max() - os_data_X[cat].min())\n    # Done separately so there is no cross-contamination between test and train\n    X_test[cat] = X_test[cat] \/ (X_test[cat].max() - X_test[cat].min())\n\nprint(os_data_X.describe())","d57f7c0f":"# A logisitc regression object\nlogreg = LogisticRegression()\n# The recursive feature elimination object\nrfe = RFE(logreg, 20)\n\nX = os_data_X.values\ny = os_data_y.values\n\n# This is our list of features before feature filtering\nstart_feats = X_train.columns.tolist()\n\n# Using the log regression object to test the importance of each feature in turn\nrfe = rfe.fit(X, y)\nfor ii, cat in enumerate(start_feats):\n    print(cat, ' - ', rfe.support_[ii])","3e8f9144":"# Tried to remove all features which do not have such a strong influence but ended up\n# making the model worse. We will check the statistical signigicance of\n# each feature now\n#feats2 = [x for x, y in zip(start_feats, rfe.support_) if y == True]\nfeats2 = start_feats\nprint(feats2)\n\nX = os_data_X.loc[:, os_data_X.columns.isin(feats2)].values\ny = os_data_y.values\n\n# Looking at the statistical significance of each of our features in a log model\nlogit_model=sm.Logit(y,X)\nresult=logit_model.fit()\nprint(result.summary2())","c8724915":"# Removing the 'hour_made^2' feature\nfeats2 = [x for x in feats2 if x != 'hour_made^2']\n\nX = os_data_X.loc[:, os_data_X.columns.isin(feats2)].values\ny = os_data_y.values\n\n# The object we will use for our final model\nlogreg = LogisticRegression()\n# Making the model\nlogreg.fit(X, y)\n\n# Extracting only our new filtered feature dataset\nX_test = X_test[feats2]\n# Making our predictions on the test dataset\ny_pred = logreg.predict(X_test)","028ac586":"# This is how well the model predicts the 'Yes' column whether there was a no-show\nscore = logreg.score(X_test, y_test)\nprint('Accuracy when tested on the test set - ', score)\n\n# This is to show how often we got true\/negative, true\/positive, false\/negative and false\/positive\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","781a0f3e":"print('ratio of predicted noshows to show-ups -',y_pred.sum() \/ len(y_pred))\nprint(classification_report(y_test, y_pred))","b1f34f38":"# Calculating the false positive and true positive rates\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'Logistic Regression (area = {logit_roc_auc:.2f})')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","31f17a6a":"The 0 days (or same day appointment) instances clearly do not fit the trent of the rest of the wait times. It is shifted down and so can be captured with a binary category. 0 will represent the rest of the wait times and 1 will represent the 0 days wait time. The 1 will recieve a waiting that will shift the 0 days predction down by the appropriate amount.","d5e86a6e":"# Model evaluation","700987c3":"I tested dropping the features that the RFE method suggested were less important but got a significantly worse end model.\n\nThe logmodel test for significant features gives a small p-value for all by the 'hour_made^2' feature.","d8be4be6":"So there is a relationship here and it will certainly need polynomial terms to capture it. There is a clear correlation with the work day here, with even a dip during lunch time. This suggests to me that people who make appointments during work hours are probably less likely to be working and could therefore be a little more unreliable. This might mean we need to treat weekdays and weekends separately.","8ee6ec10":"This shows that there is certainly a statistical significance in the neghbourhood category and that it could give us some indication of likelihood of a no-show. The p-value is very small, way below any statistical limits (0.01 or 0.05).","cb30785d":"We can see that at least for 'Scholarship', Hypertension' and 'Diabetes' there is some relationship with no-show probability. 'Gender' and 'Alcoholism' clearly have almost no link.","f4f5ff8a":"### Individual patient records\nInvestigating patients that have multiple appointments.","cce2b0ab":"We can see that age goes up to a maximum of 115, although many of the ages at the upper end have very few samples.","913fd0e6":"There doesn't seem to be any parameters which have a strong correlation with showing up for an appointment. For many of these features, the correlation is stronger with age than with no-show. I would be wary of using these as parameters for the logistic regression. However, there is a clear correlation with receiving an SMS.","b4f4e69e":"# Exploratory data analysis\n\n**PatientId** - An individual ID number for each patient. Patients who make multiple appointments will retain the same ID number\n\n**AppointmentId** - This in a unique ID number for each appointment, it is not replacated.\n\n**Gender** - Whether the patient is male or female. There are no other possibilities here (no 'non-binary' or 'prefer not to say')\n\n**ScheduledDay** - The date the appointment was made including the time up to seconds.\n\n**AppointmentDay** - The date the appointment is for, not including time.\n\n**Age** - The age of the patient in years.\n\n**Neighbourhood** - The district name that the appointment takes place.\n\n**Scholarship** - Whether the patient qualifies for financial aid. These patients will likely be poor and will be to some degree financially supported by the government.\n\n**Hypertension** - Whether the patient has high blood pressure.\n\n**Diabetes** - Whether the patient has diabetes.\n\n**Alcoholism** - Whether the patient is an alcoholic.\n\n**Handicap** - The degree to which the patient is physically handicapped.\n\n**SMS_received** - Whether the patient has received one or more SMS messages, reminding them of their appointment.\n\n**No-show** - Whether the patient showed up for their appointment. No means they showed up, Yes means they didn't show up.","f896fe08":"We are left with 30 neighbourhoods which have a reasonable sample size and have a statistical significance (p-value under 0.05. There are 22 with the more stringent p-value under 0.01 requiremnet. We may return to this if we find we have too many features.\n\nWe will add these neighbourhoods as features using dummy columns but just yet. The number of new columns would make the df a little bulky for visualising.","a3073546":"Simply splitting the data into a test and train so that we can make sure our model doesn't over-paramterise on the train dataset. There is no point in it predicting the train data set perfectly if it causes inaccuracies predicting new data.","0326c75a":"### Appointment dates and time","6e0d7500":"Our classes are imbalanced with a 20.2% chance of no show and 79.8% chance to show up.","a9ae1550":"We can use this function to give us the statistical significance of each neighbourhood individually. The chisquared method tests the null hypothesis 'there is no relationship with this neighbourhood and the chance of showing up for an appointment'. A very low score means we can reject it and accept the alternative hypothesis 'there is a relationship with this neighbourhood and showing up for a medical appointment'. Values of 0.05 and 0.01 depending on how strict you want to be are often chosen as being low enough to reject the null hypothesis\n\nWe want to find find which are the most statistically significant neighbourhoods and only include them in the model to keep things as simple as possible without losing important information.","21171b4e":"Here we have used the list of statistically significant neighbourhoods we previously calculated. I have added the dummy columns to the df at this point (and not earlier) as we drop some columns between then so we want the dummies to match with the df.","b9565a47":"# Feature selection and engineering\n\nLets start by clearing out some columns we wont be using and creating those polynomial features.","7a16044d":"# Age","857ca212":"### Neighbourhood\n\nThe first thing to note in the neighbourhood category is that it has a large number of different categories. Secondly, some of the smaller categories have very few samples in them. We will have to deal with these two things if we want to extract meaningful information from this feature.","9c5deb4e":"This goes through our list of features and drops one each time to test it's significance. It can be used to drop features which have less significance. True should be important and False less important.","44b59e4d":"We can see that our data comees from quite a restricted time-span. We have all the appointments for seven weeks and when they were made. Many were made some time before the seven week window but most were made during it (short waiting times). This means that in many cases, by the time the appointment was made, there may have already been data on that patient. How many other appointments they have made and how many they have missed.","1268396b":"It looks like there is a polynomial relationship with age and probability of showing up for a medical appointment. To me it looks like we would be better using polynomial terms rather than bins for the 'Age' feature and we just reduce all values above 85 down to 85. It is around 85 that there are too few data points per year to give accurate probabilities.","a898f03f":"Here we have padded our data with new 'no-shows' in order to balance the data. This is done by creating more instances with similar featue values as the ones we already have. It is necessary to balance the data before we start the model or the model will struggle to predict the category with fewer instances.\n\nNext we do a simple normalization of the features which have a wide range to aid conversion. It helps the model for all features to be in a similar range. We do this after the test\/train split in order to make sure no information from the test dataset leaks into the train dataset.","67864ae3":"The ROC curve moves away from the central line to the top left a reasonable amount indicating a good model.\n\nOverall the model seems to be reasonably effective but still lacking. Probably there are some other features available but none that I could find in the data given. Certainly trying some other algorithms might be effective but I wanted to test logistic regression here.","450ecd61":"These are two features which are a little hard to justify for a normal kaggle\/machine learning project. Firstly because the data is only between a small timing window, i doubt these features will work particularly effectively. We will have many patients that have had multiple booked and missed appointments but which fall outside of our range. This will affect the chance of no-show but won't be captured by the model. Secondly, in order for them to work at all, I will have to include some cross contamination between the test and train datasets. Without doing this, the test set would be too small for miss_count to build up any at all. However, I chose to include them because if this was a real world scenario and not a kaggle dataset, I definitely would include it. It would be key information so I have decided to build a model that I would be happy to use in a real world situation (rather than one which outperforms on this problem set).\n\nIt should be noted that I can find no way to vectorise the miss_count column and so it takes quite some time. This would not, in its current form, be scalable to much larger datasets.","9347743e":"This overall accuracy is not high. In fact it would be higher (~80%) if we simply predicted every instance to be a show-up. This would however, give us no ability to predict no-shows.\n\nThe confusion matrix tells us: \n - we have correctly predicted a show-up 12556 times (true positive)\n - we have correctly predicted a no-show 2310 times (true negative)\n - we have failed to predict a show-up 5131 times (false negative)\n - we have failed to predict a no-show 2108 times (false positive)","cc1e67fb":"There seems to be a polynomial relationship here but it is made not clear by the high wait time data points as there are not enough samples at these points. We will combine all values above 75 at 75.\n\nWe also see that 0 days has a clearly different relationship than the rest and so this deserves its own weighting. Giving instances a 1 for same day appointment and 0 for not same day will introduce a bias for the same day instances.","6de144ab":"# Categorical features","9ebc20cb":"# Medical Appointment No Shows\n### Why do 20% of patients miss their scheduled appointments?\n\nThis is a exploratory data analysis of why people don't show up for their medical appointments. I believe this could be useful to know for a number of reasons. Firstly, those that are more likely to miss their appointments could be sent extra reminders. Secondly, as doctors often overrun on appointments, spacing out the more likely to no-show patients could give the doctors more time to catch up. This as apposed to having a morning packed with patients and running late and then an afternoon where multiple no-shows happen in a row causing the doctor to have wasted time.\n\nIn order to make predections I have made a logistic regression model using the data I have been given. This includes: \n - Finding which parts of the data are most relevant to probability of showing up.\n - Altering the data to find new featurs which have a correlation to showing up.\n - Filtering all these features into just the ones that are most important for the model.\n - Balancing the data as the data currently has many more show-ups than no-shows.\n - Creating the logistic regression model.\n - Testing the model.","b1a4377e":"Logistic regression is used to create a model which can predict if a patient will show up for a medical appointmnet or not. y_pred is my predicitons for the test dataset","81e2382c":"The precision is $\\frac{true_{pos}}{true_{pos} + false_{pos}}$ or the ability of the model to not miss any of this category.\n\nThe recall is $\\frac{true_{pos}}{true_{pos} + false_{neg}}$ or the ability of the model to accurately get all of the positives for this category\n\nCategory 0 is a 'show up'\nCategory 1 is a 'no-show'\n\nA recall of 0.5 for '1' means that we can predict no-shows with 52% accuracy which is very useful relative to how we started, only being able to predict all are no-shows giving an overall accuracy of ~20%. The model is obviously useless for 'predicting a show up' as we could just assume they will all show up and get it right 100% of the time (with an overall accuracy of ~80%) and my model predicts this only 71% of the time. But predicting them to show up isn't interesting. We want to find the people who wont show up.\n\nA model which does not adress the imbalanced nature of the model will simply predict all show-ups. This will give very good metrics for show ups and terrible for no-shows.","cd8afe52":"We have so few weekend bookings and appointments that we don't need to worry about it. This means time of day will entirely capture whether it is during work time or not and we don't need to separate the time for weekend and weekday.","17dbe855":"The smallest expected frequency should be above 5. At 20% chance of no-show there should be at least 25 from each district to expect 5 no-shows. We will double this just to be safe. The number of districts cut is not large at onle 5 and the number of samples wihtout neighbourhood information is only 56.","c1a1dc16":"The minimum age is -1 and the maximum is 115. The maximum being that high probably means there are a number of ages with very little sample size so we will likely put the age feature into bins at a later point. But for now lets remove and ages below 0","bb4cd757":"There isn't an overall trend, so to keep this feature intact would not work for a linear relationship. Maybe the best way to look at the effect of handicap on no-show probability would be to only include dummy columns for handicap 1, 3 and 4."}}