{"cell_type":{"8da707c3":"code","45f42e17":"code","9d4ab5b2":"code","3382052c":"code","c4e5a92e":"code","179c47f3":"code","3706cb16":"code","af4f2270":"code","83afcff2":"code","fc593f90":"code","6b06d548":"code","5f9aaccd":"code","ae9b9bf2":"code","8d387384":"code","4d8dbc0a":"code","e280ce1f":"code","3809b511":"code","05cf539e":"code","45eb7798":"code","2987eadf":"code","183bd7fb":"code","42bf0237":"code","dc34b695":"code","51675141":"code","01cdccd4":"code","790f3906":"code","6dce5b0e":"code","245bcdd4":"code","4fc34535":"code","c1125c30":"code","c2c5f152":"code","2ec53ca7":"code","4a7ecef2":"code","a722d6c8":"code","382e34ad":"code","1b099f15":"code","ec89aa22":"code","bb75f1dc":"code","003a49fd":"code","bd09977d":"code","ab961560":"code","1a6318eb":"code","4ec53a9c":"code","ae48588a":"code","adc70548":"code","0601667c":"code","e4946f36":"code","674ceefd":"code","f7c1ce55":"code","e73334bd":"code","5873d57c":"code","5e0af3e9":"code","ba9420f9":"code","191bc934":"code","479b5ddc":"code","fdd81e91":"code","46258f49":"code","7ba5ca05":"code","fcf6cb1e":"code","a9c02aa1":"code","eeb04c39":"code","2e5ca289":"code","245dc1a4":"code","d57967f9":"code","8f89c77e":"code","31a2f360":"code","d82e24f6":"markdown","bb395984":"markdown","44627029":"markdown","9748cff8":"markdown","18ac2b92":"markdown","9f5835a6":"markdown","05d846ed":"markdown","760ca9e7":"markdown","14cabbfd":"markdown","4309437d":"markdown","00ea744e":"markdown","cae64d62":"markdown","750215ee":"markdown","c8a95406":"markdown","12f61b0a":"markdown","38ed7e46":"markdown","a09fdda6":"markdown","b83a1785":"markdown","3ebc749c":"markdown","e1cd1157":"markdown","d4a3add4":"markdown","1f20284f":"markdown","e78fe486":"markdown","274d0f07":"markdown","09faf3e0":"markdown","ff0f3b11":"markdown","6dbcff76":"markdown","421c5cbe":"markdown","2e1bd052":"markdown","528ad485":"markdown","effae48f":"markdown","d215e5d4":"markdown","a9a1c5b3":"markdown","981e94c7":"markdown","ab59edea":"markdown"},"source":{"8da707c3":"import numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport pandas as pd \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\ndata1 = pd.read_csv(r'\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')\ndata2 = pd.read_csv(r'\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","45f42e17":"print(\"original shape is:\",data2.shape)\ndata2.head()","9d4ab5b2":"print(data1.shape)\ndata1.head()","3382052c":"data2.isnull().sum()","c4e5a92e":"data2.describe()","179c47f3":"korelasyon=data2.corr()\nfigure, axis=plt.subplots(figsize=(10,10))\nsns.heatmap(korelasyon, annot=True)","3706cb16":"sns.pairplot(data2, hue=\"output\")","af4f2270":"categorical = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'thall', 'caa', 'slp']\ncontinous = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nprint('Categorical Variables :', ', '.join(categorical))\nprint('Continous Variables :', ', '.join(continous))","83afcff2":"import scipy.stats as ss   #Statistic \n\ndef cramers_corrected_stat(x, y):\n\n    result = -1\n    conf_matrix = pd.crosstab(x, y)\n    if conf_matrix.shape[0] == 2:\n        correct = False\n    else:\n        correct = True\n\n    chi2, p = ss.chi2_contingency(conf_matrix, correction=correct)[0:2]\n\n    n = sum(conf_matrix.sum())\n    phi2 = chi2\/n\n    r, k = conf_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    result = np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))\n    return round(result, 6), round(p, 6)\n\n\nfor var in categorical:\n    \n    x = data2[var]\n    y = data2['output']\n    cramersV, p = cramers_corrected_stat(x, y)\n    print(f'For variable {var}, Cramer\\'s V: {cramersV} and p value: {p}')","fc593f90":"for var in continous:\n    gp = data2[[var, 'output']].groupby(['output'])\n    gp_array = [group[var].to_numpy() for name, group in gp]\n    kstat, p = ss.kruskal(*gp_array)\n    kstat, p = round(kstat, 6), round(p, 6)\n    print(f'For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}')\n    ","6b06d548":"age= data2.iloc[:,:1]\n\nsns.distplot(data2['age'])","5f9aaccd":"bins = [-np.inf, 40, 50, 60, 70, np.inf]\nlabels = [\"Young\", \"Grown\",\"Mature\",\"Senior\",\"Elder\"]\nage['binned age'] = pd.cut(age['age'], bins = bins, labels = labels)\nprint(age.isnull().sum())\nage.head()\n","ae9b9bf2":"sns.catplot(x=\"output\", y=\"age\", data=data2, kind=\"box\")","8d387384":"from sklearn import preprocessing\nohe= preprocessing.OneHotEncoder()\nagge=age\nagge = agge.iloc[:,:1]\nage = age.iloc[:,1:]\nage_hot = ohe.fit_transform(age).toarray()\ndfage =pd.DataFrame(data=age_hot, index=range(age.shape[0]))\ndfage.head()","4d8dbc0a":"agge.head()","e280ce1f":"sns.distplot(data2['sex'])\ndfsex = data2.iloc[:,1:2]","3809b511":"dfsex.head()","05cf539e":"sns.distplot(data2['cp'])\ndfc = data2.iloc[:,2:3]","45eb7798":"dfc.head()","2987eadf":"ohe2= preprocessing.OneHotEncoder()\nc_hot = ohe2.fit_transform(dfc).toarray()\ndfcp =pd.DataFrame(data=c_hot, index=range(dfc.shape[0]))\ndfcp.head()","183bd7fb":"sns.distplot(data2['trtbps'])\n","42bf0237":"dftrtbps = data2.iloc[:,3:4]\ndftrtbps.tail()\n","dc34b695":"sns.catplot(x=\"output\", y=\"trtbps\", data=data2, kind=\"box\")","51675141":"sns.distplot(data2['chol'])","01cdccd4":"dfchol= data2.iloc[:,4:5]\ndfchol.tail()","790f3906":"sns.catplot(x=\"output\", y=\"chol\", data=data2, kind=\"box\")","6dce5b0e":"sns.distplot(data2['fbs'])","245bcdd4":"dffbs= data2.iloc[:,5:6]\ndffbs.tail()","4fc34535":"sns.distplot(data2['restecg'])","c1125c30":"restecg = data2.iloc[:,6:7]\nrestecg.head()","c2c5f152":"ohe3= preprocessing.OneHotEncoder()\nc_res = ohe2.fit_transform(restecg).toarray()\ndfres =pd.DataFrame(data=c_res, index=range(restecg.shape[0]))\ndfres.head()","2ec53ca7":"sns.catplot(x=\"output\", y=\"restecg\", data=data2, kind=\"box\")","4a7ecef2":"sns.distplot(data2['thalachh'])","a722d6c8":"dfftha= data2.iloc[:,7:8]\ndfftha.tail()","382e34ad":"sns.distplot(data2['exng'])","1b099f15":"dfex= data2.iloc[:,8:9]\ndfex.tail()","ec89aa22":"sns.distplot(data2['oldpeak'])\n","bb75f1dc":"dfold= data2.iloc[:,9:10]\ndfold.head()","003a49fd":"sns.catplot(x=\"output\", y=\"oldpeak\", data=data2, kind=\"box\")","bd09977d":"sns.distplot(data2['slp'])\n","ab961560":"dfslp= data2.iloc[:,10:11]\ndfslp.head()","1a6318eb":"sns.catplot(x=\"output\", y=\"slp\", data=data2, kind=\"box\")","4ec53a9c":"sns.distplot(data2['caa'])","ae48588a":"dfcaa= data2.iloc[:,11:12]\ndfcaa.head()","adc70548":"sns.distplot(data2['thall'])","0601667c":"dfthall= data2.iloc[:,12:13]\ndfthall.describe()","e4946f36":"sns.distplot(data2['output'])","674ceefd":"y= data2.iloc[:,13:14]","f7c1ce55":"x=pd.concat(   [ agge, dfsex , dfcp,  dftrtbps,dfchol,dffbs,dfres,dfftha,dfex,dfold, dfslp, dfcaa, dfthall ], axis=1)\nx.head(10)","e73334bd":"dataframe=pd.concat([x,y],axis=1)\nnumlist=dataframe.loc[:,dataframe.nunique()>5].columns\nnumlist\n","5873d57c":"def clean_outliers(df, features):\n    for i in features:\n        Q1=df[i].quantile(0.25)\n        Q3=df[i].quantile(0.75)\n        IQR= (Q3-Q1)\n        print(\"Feature {} has min value: {} max value: {}\".format(i, Q1-IQR*1.5,Q3+IQR*1.5))\n        df=df[((df[i]>(Q1-IQR*1.5))&(df[i]<(Q3+IQR*1.5)))]\n    return df","5e0af3e9":"df_clean=clean_outliers(dataframe, numlist)\nprint(\"New shape: \",df_clean.shape)\ndf_clean.head()","ba9420f9":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score as ass\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Linear Discriminant Analysis k\u00fct\u00fcphaneleri\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.covariance import LedoitWolf\nfrom sklearn.covariance import MinCovDet\nfrom sklearn.covariance import OAS\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier","191bc934":"#x=x.iloc[:,:-2]\n#x.drop(columns=['sex'],inplace=True,axis=1)\ndf_clean.drop(columns=['fbs'],inplace=True,axis=1)\n#x.drop(columns=['restecg'],inplace=True,axis=1)\n#x.drop(columns=['exng'],inplace=True,axis=1)\ndf_clean.head()","479b5ddc":"x=df_clean.iloc[:,:-1]\ny=df_clean.iloc[:,-1:]\nprint(\"x shape: \", x.shape)\nprint(\"y shape: \", y.shape)\nx.head()","fdd81e91":"from imblearn.over_sampling import SMOTENC\nxv=x.values\nyv=y.values\nsmote_nc = SMOTENC(categorical_features=[dataframe['output'].min(), dataframe['output'].max()], random_state=0)\nx, y = smote_nc.fit_resample(xv, yv)\nx = pd.DataFrame(data=x, index=range(x.shape[0]))\ny= pd.DataFrame(data=y, index=range(y.shape[0]))\nprint(x.shape)\nprint(y.shape)","46258f49":"k_nn=KNeighborsClassifier(n_neighbors=8, metric=\"chebyshev\")\nlogi = LogisticRegression(random_state=5)\nDT = DecisionTreeClassifier(max_features=\"sqrt\")\nSDF = SGDClassifier(penalty=\"l2\", random_state=10)\nS_VC= SVC(degree=4,C=20, kernel=\"poly\")\nRF= RandomForestClassifier(n_estimators=108, criterion= \"entropy\") # criterion = \"gini\" or \"entropy\"\nBayes=  GaussianNB()\nMBayes = MultinomialNB()\nBBayes = BernoulliNB()\nLDA = LinearDiscriminantAnalysis(solver=\"eigen\")    #solver= \u2018svd\u2019, \u2018lsqr\u2019, \u2018eigen\u2019\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nResult =[]","7ba5ca05":"cv_sonuc= cross_validate(clf, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of GradientBoost: \", res*100, \"%\")\nResult.append( \"GradientBoost :\")\nResult.append(res)","fcf6cb1e":"cv_sonuc= cross_validate(k_nn, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of KNN: \", res*100, \"%\")\nResult.append( \"KNN :\")\nResult.append( res)","a9c02aa1":"cv_sonuc= cross_validate(SDF, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of SDG: \", res*100, \"%\")\n\nResult = []\nResult.append( \"SDG :\")\nResult.append( res)","eeb04c39":"cv_sonuc= cross_validate(logi, x, y, cv=10 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Logistic Regression: \", res*100, \"%\")\nResult.append( \"LR :\")\nResult.append( res)","2e5ca289":"cv_sonuc= cross_validate(DT, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\n\nprint(\"Accuracy of Decision Tree: \", res*100, \"%\")\nResult.append( \"DT :\")\nResult.append( res)","245dc1a4":"cv_sonuc= cross_validate(S_VC, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Support Vector Classifier: \", res*100, \"%\")\nResult.append( \"SVC :\")\nResult.append( res)","d57967f9":"cv_sonuc= cross_validate(RF, x, y, cv=20 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Random Forest: \", res*100, \"%\")\nResult.append( \"RF :\")\nResult.append( res)","8f89c77e":"cv_sonuc= cross_validate(Bayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Naive Bayes: \", res*100, \"%\")\nResult.append( \"NB :\")\nResult.append( res)","31a2f360":"print(Result)","d82e24f6":"**$\\color{orange}{\\text{2.2.1 Cramer\u2019s V  }}$**\n\nCramer\u2019s V  is a number between 0 and 1 that indicates how strongly two categorical variables are associated. If we'd like to know if 2 categorical variables are associated, our first option is the chi-square independence test. A p-value close to zero means that our variables are very unlikely to be completely unassociated in some population. However, this does not mean the variables are strongly associated; a weak association in a large sample size may also result in p = 0.000.","bb395984":"# $\\color{purple}{\\text{Feature 6: fbs : (fasting blood sugar > 120 mg\/dl)  }}$","44627029":"# $\\color{Blue}{\\text{4. Results and Discussion  }}$\n","9748cff8":"Seemly, there are not highly correleated features in our data set. ","18ac2b92":"About this dataset\n\n1. Age : Age of the patient  --> *Discrite*\n\n2. Sex : Sex of the patient --> *Categorical*\n\n\n\n3. cp : Chest Pain type chest pain type --> *Categorical*\n\n    * Value 1: typical angina\n    * Value 2: atypical angina\n    * Value 3: non-anginal pain\n    * Value 4: asymptomatic\n    \n\n4. trtbps : resting blood pressure (in mm Hg) --> *Numerical (real)*\n\n5. chol : cholestoral in mg\/dl fetched via BMI sensor --> *Numeric (real)*\n\n6. fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) --> *Categorical*\n\n7. rest_ecg : resting electrocardiographic results --> *Categorical*\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n    \n8. thalach : maximum heart rate achieved --> *Numerical*\n\n9. exang: exercise induced angina (1 = yes; 0 = no) --> *Categorical*\n\n10. ca: number of major vessels (0-3) --> *Categorical*\n\n\n\n11. output - target : 0= less chance of heart attack 1= more chance of heart attack --> *Categorical*","9f5835a6":"We investigate all features one by one. Especially in continous features, the distrubition of the data tells us there might be some outliers. We drew catplots of those features and investigate if there are outliers. This approach is so connected with statistcs. Inter-Quartile Range (IQR) Method is used for that.","05d846ed":"**24 outlier data is removed from the dataset. Removing the outliers increased the accuracy by 2% !!**","760ca9e7":"# $\\color{purple}{\\text{Feature 13: thall  }}$","14cabbfd":"# $\\color{purple}{\\text{Feature 8: thalachh : maximum heart rate achieved  }}$","4309437d":"**$\\color{orange}{\\text{2.2.2 Kruskal-Wallis H-test }}$**\n\n\nThe Kruskal-Wallis H test is also called the \"one-way ANOVA on ranks\". It is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable.","00ea744e":"**SMOTE to deal with imbalanced data**\n\nBy SMOTE data set has been increased from 279 to 316 sythetically.","cae64d62":"# $\\color{purple}{\\text{Target: Heart Attack  }}$","750215ee":"Numarical variables seem ok.","c8a95406":"# $\\color{purple}{\\text{Feature 7: Resting Electrocardiographic Results }}$\n","12f61b0a":"# $\\color{purple}{\\text{Feature 9: exang: exercise induced angina (1 = yes; 0 = no)  }}$","38ed7e46":"# $\\color{purple}{\\text{Feature 3: Chest Pain type chest pain - cp }}$\n\n* Value 1: typical angina\n* Value 2: atypical angina\n* Value 3: non-anginal pain\n* Value 4: asymptomatic","a09fdda6":"# $\\color{purple}{\\text{Feature 4: trtbps : resting blood pressure (in mm Hg)s }}$\n","b83a1785":"**Data Cleaning and Nan-Null checking**","3ebc749c":"# $\\color{purple}{\\text{Feature 2: Sexual Identity }}$","e1cd1157":"\n\nFor outlier detection, the best practice is to use IQR method. This method helps us to illustrate the data distrubution as a box plot. The difference between Q3 (75%) and Q1 (25%) is called the Inter-Quartile Range It tells about the distribution characteristics of the data. It gives data's range, boundries and  skewness. As can be seen the figure below that a box plot enables us to draw inference from it for an ordered data. \n\n> IQR = Q3 - Q1\n> Lower Bound: (Q1 - 1.5 * IQR)\n> Upper Bound: (Q3 + 1.5 * IQR)\n\n* * The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).\n* * Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n* * Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n\n![Outlier](https:\/\/miro.medium.com\/max\/1246\/1*0MvBAT8zFSOt6sfEojbI-A.png)\n\n\n\nQuantile function return svalues at the given quantile over requested axis in a pandas dataframe.\n> DataFrame.quantile(q=0.5, axis=0, numeric_only=True, interpolation='linear')\n\n\n\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.quantile.html","d4a3add4":"# $\\color{purple}{\\text{Outlier Cleaning and Dataframe Setting }}$","1f20284f":"# **$\\color{orange}{\\text{2.1 Multi-Collinearity Check }}$**\n\nIn statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.","e78fe486":"# $\\color{Blue}{\\text{3. Machine Learning Models: Classification Problem  }}$","274d0f07":"# $\\color{purple}{\\text{Feature 1: Age }}$\n\nAge has min 29 and max 77. So that we can dicritize it into 5 groups as\n* 29 - 40\n* 40 - 50\n* 50 - 60\n* 60 - 70\n* 70 - 77\n","09faf3e0":"# $\\color{purple}{\\text{Feature 11: slp\t  }}$","ff0f3b11":"# $\\color{Blue}{\\text{2. Exploratory Data Anaylsis  }}$\n\nWe will check :\n* Multi collinearity\n* P-values\n* Outlier Clear by Feature deep-dive & IQR Method\n","6dbcff76":"# $\\color{purple}{\\text{Feature 12: caa  }}$","421c5cbe":"# $\\color{blue}{\\text{1. Introduction  }}$","2e1bd052":"# $\\color{purple}{\\text{Feature 10: oldpeak  }}$","528ad485":"# $\\color{purple}{\\text{Heart Attack EDA and Prediction with Cross Validation)  }}$","effae48f":"# **$\\color{orange}{\\text{2.3 Feature Deep-dive }}$**","d215e5d4":"One hot encoder works better for embeding which contain more than 2 categories.","a9a1c5b3":"fps has a large p-value that have the hypothesis collapsed. So that we will drop it when we will build our machine learning models.","981e94c7":"# $\\color{purple}{\\text{Feature 5: chol : cholestoral in mg\/dl fetched via BMI sensor }}$","ab59edea":"# **$\\color{orange}{\\text{2.2 P Value check for Categorical features and Numerical Values  }}$**\n\nIn statistics, the p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. The p-value is used as an alternative to rejection points to provide the smallest level of significance at which the null hypothesis would be rejected. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\n\n* A small p (\u2264 0.05), reject the null hypothesis. This is strong evidence that the null hypothesis is invalid.\n* A large p (> 0.05) means the alternate hypothesis is weak, so you do not reject the null.\n\nWe will use two different methods to estimate p-values since we both have numerical and categorical features. For Categorical values Cramer\u2019s V will be used. For numerical values Kruskal-Wallis H-test will be used. After that insight, we will investigate each feature one by one to be sure that if they are worth or not.\n\nFor making this analysis, first we will divide our data set into as categorical and numerical (continous). We will use the help of pandas dataframe features for that.\n\n* sex, cp, fbs, restecg, exng, thall, caa, slp are the features which are categorical.\n* age, trtbps, chol, thalachh, oldpeak are the features which are numerical or continous.\n\n\nNote that age can be a contionus or categorical. It should be excavated seriously. We can try both and decide but first i will make it discritize by bining, we will see."}}