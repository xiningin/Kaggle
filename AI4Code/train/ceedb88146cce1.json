{"cell_type":{"b244a29a":"code","38d2b71b":"code","24793ced":"code","2c2ba29b":"code","5df1f0be":"code","447aad76":"code","70573894":"code","72bef29a":"code","f6c32d9c":"code","7278ab1c":"code","50f3a3ce":"code","01846aa4":"code","5bb507f7":"code","a5db21b8":"code","9714668d":"code","dbd499fd":"markdown","94eeca6d":"markdown","0d5c9b60":"markdown","cfede870":"markdown","47c6b526":"markdown","f389f865":"markdown","c358844c":"markdown","e1ecbc5e":"markdown","402ce77c":"markdown","72971301":"markdown","757a8100":"markdown","1f570e0f":"markdown"},"source":{"b244a29a":"#import section\nimport numpy as np # linear algebra\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport xgboost as xgb\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import skew\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n\n","38d2b71b":"#import and understand data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n","24793ced":"#concatenate both train and test data\nall_data = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)\n#\"SalePrice\" is the target value. We don't include it in data. We don't want \"id\" affecting our model. Hence remove it.\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data = all_data.drop([\"Id\"], axis=1)","2c2ba29b":"cols_with_missing = [col for col in all_data.columns \n                                 if all_data[col].isnull().any()]\n#You can print the cols_with_missing to get better understanding of the columns with missing values\ncols_with_missing","5df1f0be":"#Handle missing values one by one\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].mode()[0])\nall_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(all_data[\"KitchenQual\"].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data[\"Functional\"].mode()[0])\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType','GarageFinish','GarageQual','GarageCond'):\n    all_data[col] = all_data[col].fillna(\"None\")\nfor col in ('GarageYrBlt','GarageCars','GarageArea'):\n    all_data[col] = all_data[col].fillna(0)\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\n\n#Now check again if there are anymore columns with missing values.\ncols_with_missing = [col for col in all_data.columns \n                                 if all_data[col].isnull().any()]\nlen(cols_with_missing)","447aad76":"#\"SalePrice\" is skewed. This isn't good. It is better to apply log transformation.\nsns.distplot(df_train['SalePrice']);\n","70573894":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\nsns.distplot(df_train['SalePrice']);\n","72bef29a":"numerical_features = all_data.select_dtypes(exclude = [\"object\"]).columns\nprint(\"Number of numerical features:\" + str(len(numerical_features)))\n\n#log transform numerical features\nskewness = all_data[numerical_features].apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.7]\nskewed_features = skewness.index\nall_data[skewed_features] = np.log1p(all_data[skewed_features])","f6c32d9c":"categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\nprint(\"Number of categorical features:\" + str(len(categorical_features)))\n\n#getdummies for categorical features\n#Create a dataFrame with dummy categorical values\ndummy_all_data = pd.get_dummies(all_data[categorical_features])\n#Remove categorical features from original data, which leaves original data with only numerical featues\nall_data.drop(categorical_features, axis=1, inplace=True)\n#Concatenate the numerical features in original data and categorical features with dummies\nall_data = pd.concat([all_data, dummy_all_data], axis=1)\n#print(all_data.shape)","7278ab1c":"#Separate training and given test data\nX = all_data[:df_train.shape[0]]\ntest_data = all_data[df_train.shape[0]:]\ny = df_train[\"SalePrice\"]","50f3a3ce":"def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5))    #squared mean error\n    return(rmse)","01846aa4":"m_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X, y)\nrmse_cv(m_lasso).mean()   ","5bb507f7":"m_xgb = xgb.XGBRegressor(n_estimators=10000, max_depth=5,learning_rate=0.07)\nm_xgb.fit(X, y)","a5db21b8":"p_xgb = np.expm1(m_xgb.predict(test_data))\np_lasso = np.expm1(m_lasso.predict(test_data))\npredicted_prices = 0.75*p_lasso + 0.25*p_xgb\nprint(predicted_prices)","9714668d":"my_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","dbd499fd":"**Submit data**","94eeca6d":"**Apply XGB Regression**","0d5c9b60":"This is a very simple solution just to get started with House Regression problem.\n\nFollowing are the steps:\n\n1. Import data. Combine training data and test data.\n2. Handle missing data\n3. Apply log transformation on numerical features\n4. Get dummies for the categorical features\n5. Separate training data and test data before fitting the model\n6. Apply Lasso Regression\n7. Apply XGB Regression\n8. Ensemble both models and predict\n9. Submit data\n\nDetailed explanation of the steps are given at the respective places. \n","cfede870":"**Import data. Combine training data and test data**\n\nTraining data and given test data are combined so that \n* Missing values are handled together\n* Getting dummy values for categorical features (Features with object-string datatype) can be easily handled [To know more about dummy values read, https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding]\n\nLater, we separate both of them while training.","47c6b526":"Also for novices like me, you can browse through these simple notes I prepared for scikit learn and ML algos.\n- [Practical scikit Basics](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-practical-scikit-basics-part-1)\n- [ML Algos and tips Part 1](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-1)\n- [ML Algos and tips Part 2](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-2)\n- [ML Algos and tips Part 3](https:\/\/www.kaggle.com\/nee2shaji\/for-novices-ml-algos-how-to-use-them-part-3)","f389f865":"**Separate training data and test data before fitting the model**\n\nWe have to fit the model only to training data. Hence now we separate them.","c358844c":"**Get dummies for the categorical features**\n\nYou will get an error if you try to plug these object values into most machine learning models in Python without \"encoding\" them first. \nYou can read about it further here: https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding\n\nAnd, then we concatenate numerical features in original data and categorical features with dummies","e1ecbc5e":"**Handle missing data**\n\nNow we find columns with missing values and handle each of those. We look into the data description and apply some sensible way to handle the missing values","402ce77c":"**Ensemble both models and predict**","72971301":"**Apply Lasso Regression**","757a8100":"**Apply log transformation on numerical features**\n\nSkewed numerical features isn't good while training. \nYou can read about it further here: https:\/\/medium.com\/@TheDataGyan\/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55","1f570e0f":"**References**:\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\nhttps:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nP.S: I'm a beginner myself. Kindly let me know if there are any mistakes."}}