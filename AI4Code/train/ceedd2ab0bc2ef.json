{"cell_type":{"cdc0f8f5":"code","998d9ee9":"code","d89280b2":"code","bf2643f4":"code","0756297a":"code","2f9904c4":"code","8850ede5":"code","ca818173":"code","34e67e24":"code","aee14ad6":"code","53014457":"code","e91c759f":"code","2d610926":"code","4549a60d":"code","1b5ec889":"markdown","d9565366":"markdown"},"source":{"cdc0f8f5":"!pip install pmdarima","998d9ee9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d89280b2":"from numpy.random import seed\nseed(1)\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom pmdarima import auto_arima\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, BatchNormalization, Dropout, GaussianNoise\nfrom keras.layers import Input, multiply\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import initializers\nfrom datetime import datetime,timedelta\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline\nnp.random.seed(0)","bf2643f4":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\ntrain.head()","0756297a":"no_countr = train['Country_Region'].nunique()\nno_province = train['Province_State'].nunique()\nno_countr_with_prov = len(train[train['Province_State'].isna()==False]['Country_Region'].unique())\ntotal_forecasting_number = no_province + no_countr - no_countr_with_prov+2\nno_days = train['Date'].nunique()\nprint('there are ', no_countr, 'unique Countries\/Regionions, each with ', no_days, 'days of data, all of them having the same dates. There are also ',no_province, 'Provinces\/States which can be found on ', no_countr_with_prov, 'countries\/ regions.' )","2f9904c4":"from datetime import date\nmax_date = train['Date'].max()\nmin_date = train['Date'].min()\nprint('dates start on: ', train['Date'].min(), ' and end on: ',max_date)\nmax_year = pd.to_datetime(max_date).year\nmin_year = pd.to_datetime(min_date).year\nmax_month = pd.to_datetime(max_date).month\nmin_month = pd.to_datetime(min_date).month\nmax_day = pd.to_datetime(max_date).day\nmin_day = pd.to_datetime(min_date).day\nd0 = date(max_year, max_month, max_day)\nd1 = date(min_year, min_month, min_day)\ndata_days = (d0-d1).days + 1\nprint('There are:', data_days, 'days with data')\n\nmin_date = test['Date'].min()\nmax_year = pd.to_datetime(max_date).year\nmin_year = pd.to_datetime(min_date).year\nmax_month = pd.to_datetime(max_date).month\nmin_month = pd.to_datetime(min_date).month\nmax_day = pd.to_datetime(max_date).day\nmin_day = pd.to_datetime(min_date).day\nd0 = date(max_year, max_month, max_day)\nd1 = date(min_year, min_month, min_day)\nno_test_train_dates = (d0-d1).days + 1\nno_test_dates = test['Date'].nunique()\nno_test_days = no_test_dates - no_test_train_dates\nprint('there are ', no_test_train_dates, 'common test and train dates, ', no_test_dates, ' test dates, ', no_test_days, ' unique test dates ')","8850ede5":"def l_regr(x,y):\n    model = LinearRegression().fit(x, y)\n    return model","ca818173":"def predict_next(model, n, X, n_nodes):\n    series = [xi for xi in X]\n    pred_list = []\n    for i in range(n):\n        data = series_to_supervised(np.array(series), n_in = n_nodes-1)\n        data = data.reshape((data.shape[0], data.shape[1], 1))\n        pred = model.predict(data)\n        pred = pred[-1][0]\n        pred_list.append(pred)\n        series.append(pred)\n    return pred_list","34e67e24":"def series_to_supervised(data, n_in = 10, n_out=1):\n    df = pd.DataFrame(data)\n    cols = list()\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n\n    agg = pd.concat(cols, axis=1)\n\n    agg.dropna(inplace=True)\n    return agg.values","aee14ad6":"n_nodes = 10","53014457":"def train_model(train_x, train_y, n_nodes):\n    #     =================\n#     model = Sequential()\n#     model.add(Conv1D(filters=250, kernel_size=3, activation='relu', input_shape=(n_nodes,1), padding='same', kernel_initializer='truncated_normal'))\n#     model.add(LSTM(64, activation='relu', kernel_initializer='truncated_normal'))\n#     model.add(BatchNormalization(momentum =0.0)) <====\n#     model.add(Dropout(0.2))\n#     model.add(Dense(32, activation='relu', kernel_initializer='truncated_normal'))\n#     model.add(Dropout(0.2))\n#     model.add(Dense(1))\n#     model.compile(loss='mse', optimizer='adam')\n#     =================\n    np.random.seed(0)\n    model = Sequential()\n    layer_in = Input(shape=(n_nodes,1))\n    layer_regr = Conv1D(filters=250, kernel_size=3, activation='relu', padding='same', kernel_initializer='truncated_normal')(layer_in)\n    layer_regr = LSTM(64, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.3)(layer_regr)\n    layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n    layer_regr = Dropout(0.2)(layer_regr)\n    layer_out = Dense(1,)(layer_regr)\n    \n    model = Model(inputs=layer_in, outputs=layer_out)\n    \n    model.compile(loss='mse', optimizer='adam')\n#     =================\n#     model = Sequential()\n#     layer_in = Input(shape=(n_nodes,))\n#     layer_regr = Dense(64, activation='relu', kernel_initializer='truncated_normal')(layer_in)\n#     layer_regr = BatchNormalization()(layer_regr)\n#     layer_class = Dense(32, activation='softmax', kernel_initializer='truncated_normal')(layer_regr)\n#     layer_regr = Dense(32, activation='relu', kernel_initializer='truncated_normal')(layer_regr)\n\n#     layer_regr = multiply([layer_regr, layer_class])\n#     layer_out = Dense(1,)(layer_regr)\n#     model = Model(inputs=layer_in, outputs=layer_out)\n    \n#     model.compile(loss='mse', optimizer='adam')\n    for i in range(600):\n        model.fit(train_x, train_y, batch_size=len(train_x), epochs = 1, verbose = 0)\n        model.reset_states()\n    return model","e91c759f":"index = int(((len(test)\/no_test_dates)+1)\/2)\ncases_pred= []\nfatalities_pred = []\npbar = tqdm(total=((len(test)\/no_test_dates)))\nwhile index < ((len(test)\/no_test_dates)+1):\n    x = train['ConfirmedCases'].iloc[[i for i in range(no_days*(index-1),no_days*index)]].values\n    z = train['Fatalities'].iloc[[i for i in range(no_days*(index-1),no_days*index)]].values\n    \n    index += 1\n    \n    no_nul_cases = pd.DataFrame(x)\n\n    if(not no_nul_cases.empty):\n        X = [xi[0] for xi in no_nul_cases.values]\n        if (len(X) >= 30):\n            try:\n\n                new_pred = []\n                model_er = l_regr(np.array([i for i in range(len(X))]).reshape(-1, 1), np.log1p(np.array(X)).tolist())\n                er_pred = [(model_er.coef_*(len(X)+i) + model_er.intercept_).astype('int')[0] for i in range(no_test_days)]\n                er_pred = np.expm1(np.array(er_pred)).tolist()\n\n                train_set = series_to_supervised(X, n_in = n_nodes, n_out=1)\n                train_set = np.log1p(train_set)\n\n                train_x, train_y = train_set[:, :-1], train_set[:, -1]\n                train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n\n                for i in range(5):\n\n                    model = train_model(train_x, train_y, n_nodes)\n\n#                     plotme = series_to_supervised(X, n_in = n_nodes-1, n_out=1)\n#                     plotme = np.log1p(plotme)\n#                     plotme = plotme.reshape((plotme.shape[0], plotme.shape[1], 1))\n                    \n                    pred_list = predict_next(model, no_test_days, np.log1p(np.array(X)).tolist(), n_nodes)\n                    pred_list = np.expm1(np.array(pred_list)).tolist()\n                    \n#                     plt.plot([[float('NaN')] for s in range(n_nodes)] + np.expm1(model.predict(plotme)).tolist() + np.array(pred_list).reshape(-1,1).tolist(), color='blue', label='Model fit and prediction')\n#                     plt.plot(X, color='red', label='Train Data')\n#                     plt.legend()\n#                     plt.ylabel('no. of cases')\n#                     plt.xlabel('no. of days')\n#                     plt.show()\n                    \n                    new_pred += pred_list\n                pred = np.array(new_pred[:no_test_days])\n                for i in range(2, 6):\n                    pred = np.add( pred, np.array(new_pred[(i-1)*no_test_days:i*no_test_days]))\n                pred = pred\/5\n                pred = pred.tolist()\n                if (pred[-1] > 2*er_pred[-1]):\n                    pred = er_pred\n                    \n\n            except:\n                model = l_regr(np.array([i for i in range(len(X))]).reshape(-1, 1),X)\n                pred = [(model.coef_*(len(X)+i) + model.intercept_).astype('int')[0] for i in range(no_test_days)]\n        else:\n            try:\n                model = auto_arima(X,seasonal=True, m=12)\n                pred = model.predict(no_test_days)\n                pred = pred.astype(int)\n                pred = pred.tolist()\n            except:\n                model = l_regr(np.array([i for i in range(len(X))]).reshape(-1, 1),X)\n                pred = [(model.coef_*(len(X)+i) + model.intercept_).astype('int')[0] for i in range(no_test_days)]\n                \n    else:\n        pred = [0] * no_test_days\n    pred = x[-no_test_train_dates:].astype(int).tolist() + pred\n    cases_pred+=pred\n\n    no_nul_fatalities = pd.DataFrame(z)\n\n    if(not no_nul_fatalities.empty):\n        Z = [zi[0] for zi in no_nul_fatalities.values]\n        if (len(Z) >= 30):\n            try:\n                new_pred = []\n                model_er = l_regr(np.array([i for i in range(len(Z))]).reshape(-1, 1), np.log1p(np.array(Z)).tolist())\n                er_pred = [(model_er.coef_*(len(Z)+i) + model_er.intercept_).astype('int')[0] for i in range(no_test_days)]\n                er_pred = np.expm1(np.array(er_pred)).tolist()\n                train_set = series_to_supervised(Z, n_in = 10, n_out=1)\n                train_set = np.log1p(train_set)\n\n                train_x, train_y = train_set[:, :-1], train_set[:, -1]\n                train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n                \n                for i in range(5):\n\n                    model = train_model(train_x, train_y)\n                    \n                    pred_list = predict_next(model, no_test_days, np.log1p(np.array(Z)).tolist(), n_nodes)\n                    pred_list = np.expm1(np.array(pred_list)).tolist()\n\n                    new_pred += pred_list\n                pred = np.array(new_pred[:no_test_days])\n                for i in range(2, 6):\n                    pred = np.add( pred, np.array(new_pred[(i-1)*no_test_days:i*no_test_days]))\n                pred = pred\/5\n                pred = pred.tolist()\n\n            except:\n                model = l_regr(np.array([i for i in range(len(Z))]).reshape(-1, 1),Z)\n                pred = [(model.coef_*(len(Z)+i) + model.intercept_).astype('int')[0] for i in range(no_test_days)]\n        else:\n            try:\n                model = auto_arima(Z, seasonal=False, m=12)\n                pred = model.predict(no_test_days)\n                pred = pred.astype(int)\n                pred = pred.tolist()\n            except:\n                model = l_regr(np.array([i for i in range(len(Z))]).reshape(-1, 1),Z)\n                pred = [(model.coef_*(len(Z)+i) + model.intercept_).astype('int')[0][0] for i in range(1,32)]\n    else:\n        pred = [0] * no_test_days\n    pred = z[-no_test_train_dates:].astype(int).tolist() + pred\n    fatalities_pred+=pred\n    pbar.update(1)\npbar.close()","2d610926":"if(len(fatalities_pred) == len(test)):\n    print('the length of fatalities_pred and cases_pred is the same as the length of test')","4549a60d":"submission = pd.DataFrame({'ForecastId': [i for i in range(1,len(cases_pred)+1)] ,'ConfirmedCases': cases_pred, 'Fatalities': fatalities_pred})\nfilename = 'submission.csv'\nsubmission.to_csv(filename,index=False)","1b5ec889":"# COVID-19 pandemic forecast using CNN, LSTM, SARIMA and exponential regression","d9565366":"## GOALS OF THIS NOTEBOOK\n### **Forecast the next n days of cases\/ fatalities for every country by creating a neural-network model composed of:**\n<hr>\n* ### **A Neural Networks model that has:**\n     1. Convolutional Neural Networks ( for extracting features )\n     2. Long short-term memory Neural Networks ( for the regression characteristic )\n     3. Dense or multy-layer perceptons\n* ### **A SARIMA model that is calculated using the auto_sarima function.**\n* ### **An Exponential Regression model to be compared with the Neural Networks model.**\n<hr>"}}