{"cell_type":{"f61d297a":"code","0bb19155":"code","9d03aefd":"code","16cc21ec":"code","898c72f2":"code","1039f239":"code","21028a2a":"code","44b70a29":"code","e89c83e8":"code","dbb980a7":"code","96a4eea2":"code","9db4fb0b":"code","86c0d984":"code","0082d20d":"code","75cd1dab":"code","05fad30d":"code","b887aedf":"markdown","1dbfd0e6":"markdown","609f216c":"markdown","0a875423":"markdown","76ecf5f8":"markdown","f4edcf17":"markdown"},"source":{"f61d297a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\n\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom glob import glob\nimport multiprocessing\n\nimport os\n \n# Any results you write to the current directory are saved as output.","0bb19155":"whale = pd.read_csv('..\/input\/humpback-whale-identification\/train.csv')\nwhale.head()\nprint(len(whale))\nlen(np.unique(whale.Id))","9d03aefd":"unknown_whale = whale[whale.Id=='new_whale']\nunknown_whale.head()","16cc21ec":"train_path = '..\/input\/humpback-whale-identification\/train\/'\ntrain_images = unknown_whale.Image.values#os.listdir(train_path)\ntest_path = '..\/input\/humpback-whale-identification\/test\/'\n","898c72f2":"whale_dict = dict(zip(whale.Image, whale.Id))","1039f239":"layerNames = [\n\t\"feature_fusion\/Conv_7\/Sigmoid\",\n\t\"feature_fusion\/concat_3\"]\n","21028a2a":"!pip install imutils","44b70a29":"import time\nfrom imutils.object_detection import non_max_suppression\nnet = cv2.dnn.readNet('..\/input\/frozen-east-text-detection\/frozen_east_text_detection.pb')\n","e89c83e8":"WW = 320\nHH = 160\ndef get_images_with_text(path, with_class=True, WW=320, HH=160):\n\n    image_files = os.listdir(path)\n    FOUND  = []\n    new_whale_count = 0\n    for image_file in tqdm_notebook(image_files):\n\n        # load the input image and grab the image dimensions\n        image = cv2.imread(path + image_file)\n        orig = image.copy()\n        (H, W) = image.shape[:2]\n\n        # set the new width and height and then determine the ratio in change\n        # for both the width and height\n        (newW, newH) = (WW, HH)\n        rW = W \/ float(newW)\n        rH = H \/ float(newH)\n\n        # resize the image and grab the new image dimensions\n        image = cv2.resize(image, (newW, newH))\n        (H, W) = image.shape[:2]\n\n\n        # construct a blob from the image and then perform a forward pass of\n        # the model to obtain the two output layer sets\n        blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n            (123.68, 116.78, 103.94), swapRB=False, crop=False)\n        start = time.time()\n        net.setInput(blob)\n        (scores, geometry) = net.forward(layerNames)\n        end = time.time()\n\n        # show timing information on text prediction\n\n\n        (numRows, numCols) = scores.shape[2:4]\n        rects = []\n        confidences = []\n\n        text_found = 0\n        text_lines = 0\n        # loop over the number of rows\n        for y in range(0, numRows):\n\n            # extract the scores (probabilities), followed by the geometrical\n            # data used to derive potential bounding box coordinates that\n            # surround text\n            scoresData = scores[0, 0, y]\n            xData0 = geometry[0, 0, y]\n            xData1 = geometry[0, 1, y]\n            xData2 = geometry[0, 2, y]\n            xData3 = geometry[0, 3, y]\n            anglesData = geometry[0, 4, y]\n\n            # loop over the number of columns\n            found = False\n            for x in range(0, numCols):\n                # if our score does not have sufficient probability, ignore it\n                if scoresData[x] < 0.8:\n                    continue\n\n                # compute the offset factor as our resulting feature maps will\n                # be 4x smaller than the input image\n                (offsetX, offsetY) = (x * 4.0, y * 4.0)\n\n                if offsetY\/H < 0.80:\n                    continue\n\n\n                # extract the rotation angle for the prediction and then\n                # compute the sin and cosine\n                angle = anglesData[x]\n                cos = np.cos(angle)\n                sin = np.sin(angle)\n\n                # use the geometry volume to derive the width and height of\n                # the bounding box\n                h = xData0[x] + xData2[x]\n                w = xData1[x] + xData3[x]\n\n                # compute both the starting and ending (x, y)-coordinates for\n                # the text prediction bounding box\n                endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n                endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n                startX = int(endX - w)\n                startY = int(endY - h)\n\n                # add the bounding box coordinates and probability score to\n                # our respective lists\n                rects.append((startX, startY, endX, endY))\n                confidences.append(scoresData[x])\n                found = True\n\n            if found == True:\n                text_lines += 1\n\n\n        boxes = non_max_suppression(np.array(rects), probs=confidences)\n        #if len(boxes)>0:\n        #    print (image_file, text_lines)\n        # loop over the bounding boxes\n        for (startX, startY, endX, endY) in boxes:\n            # scale the bounding box coordinates based on the respective\n            # ratios\n            startX = int(startX * rW)\n            startY = int(startY * rH)\n            endX = int(endX * rW)\n            endY = int(endY * rH)\n\n        if len(boxes) > 0:\n            if with_class==True:\n                FOUND.append([image_file, whale_dict[image_file], text_lines])\n                if whale_dict[image_file] == 'new_whale':\n                    new_whale_count = new_whale_count + 1\n            else:\n                FOUND.append([image_file, text_lines])\n    return FOUND","dbb980a7":"df_train = get_images_with_text(train_path)\ndf_test = get_images_with_text(test_path, with_class=False)\n","96a4eea2":"df_train= pd.DataFrame(df_train)\ndf_train.columns = ['image', 'class', 'line_count']\ndf_train.head()","9db4fb0b":"df_test= pd.DataFrame(df_test)\ndf_test.columns = ['image', 'line_count']\ndf_test.head()","86c0d984":"df_train.to_csv('train_text.csv')\ndf_test.to_csv('train_text.csv')","0082d20d":"fig, axes = plt.subplots(5, 5)\n \nfig.set_figwidth(20)\nfig.set_figheight(20)\n\nfor i, row in df_train.iterrows():\n    if i >= 25:\n        break\n    img = cv2.imread(train_path + row['image'])\n    axes[int(i\/5), i%5].imshow(img)\n    axes[int(i\/5), i%5].set_title(row['image']  + '-' + str(whale_dict[row['image']]) + ' (' + str(row['line_count']) + ')')\n    axes[int(i\/5), i%5].axis('off')\n\nplt.show()","75cd1dab":"fig, axes = plt.subplots(5, 5)\n \nfig.set_figwidth(20)\nfig.set_figheight(20)\n\nfor i, row in df_test.iterrows():\n    if i >= 25:\n        break\n    img = cv2.imread(test_path + row['image'])\n    axes[int(i\/5), i%5].imshow(img)\n    axes[int(i\/5), i%5].set_title( row['image']  + '-' +  ' (' + str(row['line_count']) + ')')\n    axes[int(i\/5), i%5].axis('off')\n\nplt.show()","05fad30d":"df_train[df_train.line_count>3]","b887aedf":"In the pursuit of a nice juicy \"leakage\" I give you a simple and straightforward text detection facility using plain OpenCV (EAST text detection caffe model).It manages to automatically locate text in both train and test images. The majority of the code was taken from the great Pyimagesearh [OpenCV Text Detection (EAST text detector)](https:\/\/www.pyimagesearch.com\/2018\/08\/20\/opencv-text-detection-east-text-detector\/). No character\/word recognition yet. I believe there must be a correlation between text appearance and `new_whale` tag.\n\nTo reduce false positives we restrict our search to the the bottom area of each image (see `offsetY\/H < 0.80` below)","1dbfd0e6":"### 2. Text that starts with '#' followed by a four digit number is most probably new_whale (see  #3332, #0518 in training images above) ","609f216c":"## Some observations:\n### 1. By looking at the training images there seems to be great correlation between images with many lines of text  and identified whales","0a875423":"Let us plot some tet images along with the  number of lines. It seems that text detector is producing more false positives.","76ecf5f8":"Let us plot some training images along with class annotations and number of lines.","f4edcf17":"The architecture of EAST text detector is depicted in the image below. Among the outputs is a set of text  boxes. We loop through this test for increasing `y` values and we can get an estimate of the number of lines.  \n![image](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2018\/08\/opencv_text_detection_east.jpg)\n"}}