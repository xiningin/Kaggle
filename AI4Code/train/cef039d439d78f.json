{"cell_type":{"5426c424":"code","ea260ed4":"code","36af956e":"code","f9b13a66":"code","f3dcdad2":"code","ea09b2b2":"code","6481455b":"code","e29e7414":"code","345f45bd":"code","20d72e37":"code","2e2d00c4":"code","a6c26a13":"code","54a4d5bc":"code","a6ec54e3":"code","b73210df":"code","74c46ae4":"code","fd85572c":"code","6480f424":"code","fe1a9956":"code","c12169d6":"code","1f82bbe0":"code","0219e353":"code","a96d22e5":"code","c57c5fa5":"code","5d5cc3ec":"code","c320f110":"code","5d98dc71":"code","2d2fde0e":"code","0890cae7":"code","69b25784":"code","75d52c42":"code","15c5a6c6":"code","f1196c4a":"code","da23e4db":"code","ac307eeb":"code","7ac61471":"markdown","bc07ee69":"markdown","d7b22d9a":"markdown","0dae8b88":"markdown","bae499e8":"markdown","679917e6":"markdown","e473df9a":"markdown","052b399f":"markdown","04af9f1b":"markdown","fa4138de":"markdown","a1603da7":"markdown","7dd93ed4":"markdown","2973f1c1":"markdown","14db58b6":"markdown","d15edef4":"markdown","2401240e":"markdown","a8cceb2b":"markdown","d9e922a5":"markdown","08f67aac":"markdown","7b141243":"markdown","fe3ae75e":"markdown","d0f0c889":"markdown","5bb71023":"markdown","80584c73":"markdown","0a00c5dc":"markdown","ed5cde78":"markdown","c30ca956":"markdown","d15f4edb":"markdown","17d02a5b":"markdown","28e2aede":"markdown","5e549f13":"markdown","ee2d3c8f":"markdown","ff0c0344":"markdown"},"source":{"5426c424":"import os\nprint(os.listdir(\"..\/input\"))\nimport numpy as np \nimport pandas as pd \nimport json\nimport bq_helper\nfrom pandas.io.json import json_normalize\nimport warnings\nimport gc\ngc.enable()\nwarnings.filterwarnings(\"ignore\")\n","ea260ed4":"json_columns = ['device', 'geoNetwork','totals', 'trafficSource']\ndef load_dataframe(filename):\n    path = \"..\/input\/\" + filename\n    df = pd.read_csv(path, converters={column: json.loads for column in json_columns}, \n                     dtype={'fullVisitorId': 'str'})\n    \n    for column in json_columns:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","36af956e":"train = load_dataframe(\"ga-customer-revenue-prediction\/train.csv\")\nprint(train.shape)\ntrain.head()","f9b13a66":"test = load_dataframe(\"ga-customer-revenue-prediction\/test.csv\")\nprint(test.shape)\ntest.head()","f3dcdad2":"not_in_test_feature = [i for i in train.columns if i not in test.columns]\nprint (not_in_test_feature)","ea09b2b2":"train = train.drop(['trafficSource_campaignCode'], axis = 1)\ntrain.shape,test.shape","6481455b":"#import module of matplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\n%matplotlib inline","e29e7414":"#analysis count of each features\nunique_count_train = []\nfor i in train.columns:\n    unique_count_train.append(len(train[str(i)].value_counts()))\nunique_datafram_train = {\n    \"Feature\":train.columns,\n    \"unique_count\":unique_count_train\n}\nunique_train = pd.DataFrame(unique_datafram_train)\nunique_train\n#=========================or you can use unique to do this =================================#\n#train.nunique()","345f45bd":"const_columns_train = unique_train.loc[unique_train['unique_count'] == 1,\"Feature\"]\nconst_columns_train = list(const_columns_train)\n#=========================or you can use unique to do this =================================#\n#const_columns_train = [col for col in train.columns if train[col].nunique() == 1]\nprint (\"Shape before remove constant feature on train set = \" + str(train.shape))\ntrain = train.drop(const_columns_train,axis = 1)\nprint (\"Shape after remove constant feature on train set = \" + str(train.shape))","20d72e37":"#analysis count of each features\nunique_count_test = []\nfor i in test.columns:\n    unique_count_test.append(len(test[str(i)].value_counts()))\nunique_datafram_test = {\n    \"Feature\":test.columns,\n    \"unique_count\":unique_count_test\n}\nunique_test = pd.DataFrame(unique_datafram_test)\nunique_test\n#=========================or you can use unique to do this =================================#\n#test.nunique()","2e2d00c4":"const_columns_test = unique_test.loc[unique_test['unique_count'] == 1,\"Feature\"]\nconst_columns_test = list(const_columns_test)\n#=========================or you can use unique to do this =================================#\n#const_columns_test = [col for col in test.columns if test[col].nunique() == 1]\nprint (\"Shape before remove constant feature on train set = \" + str(test.shape))\ntest = test.drop(const_columns_test,axis = 1)\nprint (\"Shape after remove constant feature on train set = \" + str(test.shape))\n","a6c26a13":"def extract_new_feature(df): \n    print(\"Start extract date...\")\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['month'] = df['date'].dt.month\n    print(\"Finished extract date...\")\nextract_new_feature(train)\nextract_new_feature(test)","54a4d5bc":"t1 = train['day_of_week'].value_counts()\nt2 = train['hour'].value_counts()\nt3 = train['day'].value_counts()\nt4 = train['month'].value_counts()\n\nplt.subplots(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Visits by day of week',color='b',fontsize=12)\nplt.bar(t1.index,t1.values,color='orange')\nplt.xticks(t1.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,2)\nplt.title('Visits by hour',color='b',fontsize=12)\nplt.bar(t2.index,t2.values,color='pink')\nplt.xticks(t2.index,rotation='horizontal',fontsize=10)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,3)\nplt.title('Visits by day',color='b',fontsize=12)\nplt.bar(t3.index,t3.values,color='blue')\nplt.xticks(t3.index,rotation='horizontal',fontsize=10)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,4)\nplt.title('Visits by month',color='b',fontsize=12)\nplt.bar(t4.index,t4.values,color='purple')\nplt.xticks(t4.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)","a6ec54e3":"train['totals_transactionRevenue'] = train['totals_transactionRevenue'].astype('float64')\na = train[['day_of_week','totals_transactionRevenue']].groupby('day_of_week').mean().reset_index()\na1 = train[['hour','totals_transactionRevenue']].groupby('hour').mean().reset_index()\na2 = train[['day','totals_transactionRevenue']].groupby('day').mean().reset_index()\na3 = train[['month','totals_transactionRevenue']].groupby('month').mean().reset_index()\n\nplt.subplots(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Total transactionRevenue mean by day of week',color='b',fontsize=12)\nplt.bar(a['day_of_week'],a['totals_transactionRevenue'],color='orange')\nplt.xticks(a['day_of_week'],rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,2)\nplt.title('Total transactionRevenue mean by hour',color='b',fontsize=12)\nplt.bar(a1['hour'],a1['totals_transactionRevenue'],color='pink')\nplt.xticks(a1['hour'],rotation='horizontal',fontsize=10)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,3)\nplt.title('Total transactionRevenue mean by day',color='b',fontsize=12)\nplt.bar(a2['day'],a2['totals_transactionRevenue'],color='blue')\nplt.xticks(a2['day'],rotation='horizontal',fontsize=10)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,2,4)\nplt.title('Total transactionRevenue mean by month',color='b',fontsize=12)\nplt.bar(a3['month'],a3['totals_transactionRevenue'],color='purple')\nplt.xticks(a3['month'],rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)","b73210df":"def add_time_period_of_same_ID(df): \n    print(\"Start add time period feature...\")\n    df.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\n    df['next_revisit_time'] = (\n        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    df['prev_revisit_time'] = (\n        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    print(\"Finished time periodfeature...\")\nadd_time_period_of_same_ID(train)\nadd_time_period_of_same_ID(test)","74c46ae4":"b = train[['next_revisit_time','totals_transactionRevenue']].groupby('next_revisit_time').mean().reset_index()\nb[b['next_revisit_time']== -2562048] = 0 #Process no revisit ID to 0\nb.sort_values(by = \"next_revisit_time\",ascending=False).fillna(0)\nb2 = train[['prev_revisit_time','totals_transactionRevenue']].groupby('prev_revisit_time').mean().reset_index()\nb2[b2['prev_revisit_time']== -2562048] = 0\nb2.sort_values(by = \"prev_revisit_time\",ascending=False).fillna(0)\n\nplt.subplots(figsize=(15,10))\nplt.subplot(2,1,1)\nplt.title('Total transactionRevenue mean by sort of next revisit time',color='b',fontsize=12)\nplt.scatter(b['next_revisit_time'],b['totals_transactionRevenue'],color='orange')\n#plt.xticks(b.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.subplot(2,1,2)\nplt.title('Total transactionRevenue mean by sort of previous revisit time',color='b',fontsize=12)\nplt.scatter(b2['prev_revisit_time'],b2['totals_transactionRevenue'],color='orange')\n#plt.xticks(b.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)","fd85572c":"c = train[['fullVisitorId','totals_transactionRevenue']]\nc['counts'] = c['fullVisitorId']\nc = c.groupby(['fullVisitorId']).agg({'totals_transactionRevenue':'mean','counts':'count'}).reset_index()\nc = c.sort_values(by='counts').fillna(0).groupby('counts').mean().reset_index()\nlines = c.plot.scatter(x='counts', y='totals_transactionRevenue',figsize=(15,5))","6480f424":"categorical_features_train = train.select_dtypes(include=[np.object])\ncategorical_features_test = test.select_dtypes(include=[np.object])\ncategorical_features_train.columns","fe1a9956":"Nan_value_train = categorical_features_train.isnull().sum().sort_values(ascending=False)\nPercent_Nan_value_train = Nan_value_train\/categorical_features_train.shape[0]\nNan_train_data = pd.concat([Nan_value_train,Percent_Nan_value_train], axis= 1,keys = ['Missing value count','Percent'])\nNan_train_data.head(10)","c12169d6":"Nan_value_test = categorical_features_test.isnull().sum().sort_values(ascending=False)\nPercent_Nan_value_test = Nan_value_test\/categorical_features_test.shape[0]\nNan_test_data = pd.concat([Nan_value_test,Percent_Nan_value_test], axis= 1,keys = ['Missing value count','Percent'])\nNan_test_data.head(10)","1f82bbe0":"Totals_features = [i for i in train.columns if i.find(\"totals\")>=0 ]\nTotals_features","0219e353":"agg_dict = {}\nfor col in Totals_features:\n    train[col] = train[col].astype('float')\n    if col == 'totals_transactionRevenue':\n        agg_dict[col] = 'mean'\n    agg_dict[col] = \"sum\"\ntmp = train.groupby(\"fullVisitorId\").agg(agg_dict).reset_index()","a96d22e5":"plt.subplots(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.title('Revenue mean by totals hits',color='b',fontsize=12)\nplt.scatter(tmp['totals_hits'],tmp['totals_transactionRevenue'],color='b')\n#plt.xticks(b.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.subplot(1,2,2)\nplt.title('Revenue mean by totals pageviews',color='b',fontsize=12)\nplt.scatter(tmp['totals_pageviews'],tmp['totals_transactionRevenue'],color='b')\n#plt.xticks(b.index,rotation='horizontal',fontsize=12)\nplt.yticks(fontsize=12)","c57c5fa5":"train['totals_pageviews']=train['totals_pageviews'].astype('float64')\ntrain['totals_hits']=train['totals_hits'].astype('float64')\ntest['totals_pageviews']=test['totals_pageviews'].astype('float64')\ntest['totals_hits']=test['totals_hits'].astype('float64')","5d5cc3ec":"categorical_unique = train[list(categorical_features_test)].nunique().reset_index()\ncategorical_unique.columns = ['features','unique_count']\ncategorical_unique_less_15 = categorical_unique.loc[categorical_unique['unique_count'] < 15,\"features\"]\ncategorical_unique_less_15 = list(categorical_unique_less_15)\ncategorical_unique_less_15","c320f110":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime']\ncategorical_larger_15_feature = [i for i in categorical_features_train if i not in categorical_unique_less_15 and i not in excluded_features]\nprint(categorical_larger_15_feature)","5d98dc71":"geoNetwork_feature = [i for i in categorical_larger_15_feature if i.find('geoNetwork')>=0]\nprint(geoNetwork_feature)","2d2fde0e":"plt.subplots(figsize=(20,60))\nfor k,i in enumerate(geoNetwork_feature): \n    train_geoNetwork = train[[i]+[\"totals_transactionRevenue\"]]\n    train_geoNetwork['totals_transactionRevenue']=train_geoNetwork['totals_transactionRevenue'].fillna(0).astype('float64')\n    train_geoNetwork[i+'_counts'] = train_geoNetwork[i].fillna('others')\n    train_geoNetwork = train_geoNetwork.groupby(i).agg({'totals_transactionRevenue':'mean',i+'_counts':'count'}).reset_index()\n    train_geoNetwork = train_geoNetwork.sort_values(by='totals_transactionRevenue',ascending=False)\n    \n\n    plt.subplot(len(geoNetwork_feature),2,(2*k)+1)\n    plt.title('mean revenue by '+i,color='b',fontsize=12)\n    plt.xlabel(str(i),color='b',fontsize=12)\n    plt.barh(range(10),train_geoNetwork.totals_transactionRevenue[:10],color='grey')\n    plt.yticks(range(10),train_geoNetwork[i][:10],rotation='horizontal',fontsize=10)\n    plt.xticks(fontsize=12)\n    \n    plt.subplot(len(geoNetwork_feature),2,(2*k)+2)\n    plt.title('Count by '+i,color='b',fontsize=12)\n    plt.xlabel(str(i),color='b',fontsize=12)\n    plt.barh(range(10),train_geoNetwork[i+'_counts'][:10],color='orange')\n    plt.yticks(range(10),train_geoNetwork[i][:10],rotation='horizontal',fontsize=10)\n    plt.xticks(fontsize=12)\n    \nplt.show()","0890cae7":"device_feature = [i for i in categorical_larger_15_feature if i.find('device')>=0]\nprint(device_feature)","69b25784":"plt.subplots(figsize=(20,40))\nfor k,i in enumerate(device_feature): \n    train_device = train[[i]+[\"totals_transactionRevenue\"]]\n    train_device['totals_transactionRevenue']=train_device['totals_transactionRevenue'].fillna(0).astype('float64')\n    train_device[i+'_counts'] = train_device[i].fillna('others')\n    train_device = train_device.groupby(i).agg({'totals_transactionRevenue':'mean',i+'_counts':'count'}).reset_index()\n    train_device = train_device.sort_values(by='totals_transactionRevenue',ascending=False)\n    \n\n    plt.subplot(len(geoNetwork_feature),2,(2*k)+1)\n    plt.title('mean revenue by '+i,color='b',fontsize=12)\n    plt.xlabel(str(i),color='b',fontsize=12)\n    plt.barh(range(10),train_device.totals_transactionRevenue[:10],color='grey')\n    plt.yticks(range(10),train_device[i][:10],rotation='horizontal',fontsize=10)\n    plt.xticks(fontsize=12)\n    \n    plt.subplot(len(geoNetwork_feature),2,(2*k)+2)\n    plt.title('Count by '+i,color='b',fontsize=12)\n    plt.xlabel(str(i),color='b',fontsize=12)\n    plt.barh(range(10),train_device[i+'_counts'][:10],color='orange')\n    plt.yticks(range(10),train_device[i][:10],rotation='horizontal',fontsize=10)\n    plt.xticks(fontsize=12)\n    \nplt.show()","75d52c42":"Traffic_feature = [i for i in categorical_larger_15_feature if i.find('traffic')>=0 and i !='trafficSource_adwordsClickInfo.gclId' and i!='trafficSource_referralPath']\nprint(Traffic_feature)","15c5a6c6":"plt.subplots(figsize=(20,60))\nfor k,i in enumerate(Traffic_feature): \n    if i !='trafficSource_adwordsClickInfo.gclId' and i!='trafficSource_referralPath':\n        train_traffic = train[[i]+[\"totals_transactionRevenue\"]]\n        train_traffic['totals_transactionRevenue']=train_traffic['totals_transactionRevenue'].fillna(0).astype('float64')\n        train_traffic[i+'_counts'] = train_traffic[i].fillna('others')\n        train_traffic = train_traffic.groupby(i).agg({'totals_transactionRevenue':'mean',i+'_counts':'count'}).reset_index()\n        train_traffic = train_traffic.sort_values(by='totals_transactionRevenue',ascending=False)\n\n\n        plt.subplot(len(geoNetwork_feature),2,(2*k)+1)\n        plt.title('mean revenue by '+i,color='b',fontsize=12)\n        plt.xlabel(str(i),color='b',fontsize=12)\n        plt.barh(range(10),train_traffic.totals_transactionRevenue[:10],color='grey')\n        plt.yticks(range(10),train_traffic[i][:10],rotation='horizontal',fontsize=10)\n        plt.xticks(fontsize=12)\n\n        plt.subplot(len(geoNetwork_feature),2,(2*k)+2)\n        plt.title('Count by '+i,color='b',fontsize=12)\n        plt.xlabel(str(i),color='b',fontsize=12)\n        plt.barh(range(10),train_traffic[i+'_counts'][:10],color='orange')\n        plt.yticks(range(10),train_traffic[i][:10],rotation='horizontal',fontsize=10)\n        plt.xticks(fontsize=12)\n    \nplt.show()","f1196c4a":"Revenue_assist_count = train[\"totals_transactionRevenue\"].count()\nRevenue_nonassist_count = train.shape[0]-Revenue_assist_count\nlabel_name = [\"Revenue_assist_count\",\"Revenue_nonassist_count\"]\n\nRevenue_assist_list ={\n    \"Revenue\":label_name,\n    \"Revenue_count\":[Revenue_assist_count,Revenue_nonassist_count]\n}\n\nRevenue_assist_dataframe = pd.DataFrame(Revenue_assist_list)\n\n#Revenue_non_zero = train[train[\"totals_transactionRevenue\"]>0][\"totals_transactionRevenue\"]\nRevenue_non_zero = train[\"totals_transactionRevenue\"].fillna(0).reset_index().astype('float64')\nRevenue_non_zero = Revenue_non_zero[Revenue_non_zero[\"totals_transactionRevenue\"]>0][\"totals_transactionRevenue\"]\n\n\nplt.figure(figsize=(18,18))\nax1=plt.subplot(221)\nRevenue_assist_dataframe.plot(kind='pie', y = 'Revenue_count', ax=ax1,autopct='%1.1f%%',\n startangle=90, shadow=False, labels=Revenue_assist_dataframe['Revenue'], legend = False, fontsize=14)\nplt.subplot(222)\nsns.distplot(Revenue_non_zero)\nplt.title(\"Distribution of Non-Zero Total Revenue\")\nplt.xlabel('Total TransactionRevenue')\nplt.subplot(212)\nsns.distplot(np.log1p(Revenue_non_zero))\nplt.title(\"Natural Log Distribution of Non Zero Total Transactions\");\nplt.xlabel(\"Natural Log - Total Transactions\");\n","da23e4db":"train.head()","ac307eeb":"test.head()","7ac61471":"## Analysis unique count of each feature and remove only one unique feature\n* There are some features that contains only one value \"not available in demo dataset\" as constant values\n* constant value dont have any effective on model prdictions, let's remove it.","bc07ee69":"* Plot the epriod time that revisit cause totals_transactionRevenue\n* We found that the totals_transactionRevenue are locate at small revisit time portion\n### We keep the feature of next and previous revisit time","d7b22d9a":"## Analysis Revenue \n* First we analysis the assist value  of Transations Revenue\n* Plot Distribution of total Revenue\n* Add visiter level Revenue","0dae8b88":"## Analysis Total feature\n* Analysis total feature by bropuby full ID","bae499e8":"## Introduction\n\n#### Analysis feature on this competitions \n* Analysis all feature that used and do data visualize\n* Hep to find the key features on this competitions","679917e6":"### Analysis Revene correlation on geoNetwork features\n* Plot the Count \/ Revenue correlation for all geoNetwork features","e473df9a":"## Analysis Date feature\n* Extracting new feature from date\n* Analysis new feature ","052b399f":"## Align test and train features\n* We found train set have 2 more feature than test set\n* 1 is target featrue, find another feature that not include in test set and remove it on train set","04af9f1b":"### Analysis Revene correlation on device and traffic features\n* Plot the Count \/ Revenue correlation for device and traffic features","fa4138de":"## Find the categorical feature and process data \n* We can see there are missing value  \"NaN\" on the data\n* Find the categorical feature and visulize the data to analysis\n* Analysis Unique count < 15, and unique count>15","a1603da7":"### Analysis missing values for all categorical features ","7dd93ed4":"### Plot total trascations revenue of extracted date feature","2973f1c1":"### Plot scatter to see the distribution\n* Looks the totals feature don't need to do any process, fillna(0)","14db58b6":"* Test Set","d15edef4":"### Show the flatten data","2401240e":"### Remove constant features","a8cceb2b":"### Find the categorical feature","d9e922a5":"### Plot Visit of extracted date feature","08f67aac":"* Train Set","7b141243":"## Load data\n* import module that data analysis need to used\n* load original data to see feature and data type\n\n\u2605There are some feature have json object","fe3ae75e":"* Device features","d0f0c889":"## Analysis the feature that have unique count >= 15 \n* Anaysis the revenue of these feature\n* Process different type feature to be training feature","5bb71023":"### Find the time period of same ID revisit \nidea comes from Aahish kernel :  https:\/\/www.kaggle.com\/ashishpatel26\/future-is-here","80584c73":"* Training Set","0a00c5dc":"## Analysis Visit count and Revenue relation\n* Result show the ditribution have no any relation with Revenue, No need to add this feature","ed5cde78":"## Flatten feature with json object\n* There are json objects in some features.\n* We used this function written by julian in his kernel to flat these features.\n\nhttps:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook.","c30ca956":"* Test Set","d15f4edb":"# Future Plan\n* findout the key feature on training (you can refer the upper record of different feature)\n* doing esemble learning","17d02a5b":"### Version 1 ( LB = 1.5057)\n* Process Date feature (add day, week, hour, \"revisit time\")\n* Totals feature don't do anything, only fillna(0)\n* Do one hot encoding for categorical feature that have unique value <15\n* Do ranking encoding for categorical feature that have unique value >15","28e2aede":"**Data Fields**\n* fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n* channelGrouping - The channel via which the user came to the Store.\n* date - The date on which the user visited the Store.\n* device - The specifications for the device used to access the Store.\n* geoNetwork - This section contains information about the geography of the user.\n* sessionId - A unique identifier for this visit to the store.\n* socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n* totals - This section contains aggregate values across the session.\n* trafficSource - This section contains information about the Traffic Source from which the session originated.\n* visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, we should use a combination of fullVisitorId and visitId.\n* visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n* visitStartTime - The timestamp (expressed as POSIX time).","5e549f13":"## Analysis unique count <15 on categorical features\n* Cause we have different unique count of test set and train set, lets concate it to do one hot encoding","ee2d3c8f":"## Training  kernel ==> https:\/\/www.kaggle.com\/super13579\/revisit-one-hot-ranking","ff0c0344":"* Traffic features\nRemove path and gclId feature"}}