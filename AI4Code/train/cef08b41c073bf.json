{"cell_type":{"c20ca600":"code","7812aa91":"code","da0c47b0":"code","3d688599":"code","b046852f":"code","3ecb7928":"code","e1ebd956":"code","84051ac1":"code","efc8fa6f":"code","88de3ebd":"code","f64bacf6":"code","2ff574c2":"code","e894d86e":"code","76ba8b31":"code","380014ec":"code","9804adc5":"code","1a7d447c":"code","d46fa413":"code","f1fad610":"code","83d14e96":"code","3666e8d5":"code","24ec3d62":"code","b0749841":"code","c4dcd9c4":"code","3fbd7d36":"code","4b79cc7d":"code","6c384f09":"code","c65a6d9b":"code","3dbf1769":"code","9fa8feec":"code","4ca1a91a":"code","669e119f":"code","14d5375d":"code","7463092b":"code","e477967c":"code","1764a679":"code","8cfb67f3":"code","97f9681c":"code","87241761":"code","b59518f6":"code","fce13713":"code","2ef41068":"code","c9ea00a8":"code","7fd05dfc":"code","5994e6a8":"code","99019713":"code","4d392136":"code","6af5b785":"code","eed88b99":"code","ee93dd81":"code","720ab337":"code","0b002204":"code","b9669d36":"code","33bb8a64":"code","9afa0a1b":"code","715c7771":"code","4d3b54c6":"code","7e0b1cb4":"code","069d4d83":"code","ab9e0dea":"code","71e15eba":"code","48a78c4e":"code","a2549122":"code","8e847dea":"code","4b86c89f":"code","4885a0e5":"code","a326baf7":"code","d2eddea5":"code","0161b730":"code","58caafce":"code","656e0464":"code","6dc4fc5c":"code","4e7cdb68":"markdown","7ab2ea93":"markdown","8abdab57":"markdown","486a18db":"markdown","09fe2e28":"markdown","a29c75a6":"markdown","daf677a3":"markdown","8efdcee9":"markdown","69730768":"markdown","f806501e":"markdown","cd8f49de":"markdown","81855d31":"markdown","632405ac":"markdown","0fd1b613":"markdown","7ce7b782":"markdown","a7b7ee71":"markdown","1065db1a":"markdown","2f988a83":"markdown","980fa135":"markdown","63a00875":"markdown","1f24f95c":"markdown","d0630f53":"markdown","6738cf52":"markdown","d7e5a994":"markdown","40025c9d":"markdown","665bf9c1":"markdown","98f23497":"markdown","d6d0db40":"markdown","4dcd8de9":"markdown","5a21129e":"markdown","3dfa52dc":"markdown","b6331553":"markdown","4997e803":"markdown","339d16ae":"markdown","9ff50278":"markdown","72026374":"markdown","6d52c2d5":"markdown","b1c57d02":"markdown","6276d7d4":"markdown","6467730a":"markdown","702f5669":"markdown","4a1ebe96":"markdown","31897c67":"markdown","944fc669":"markdown","7df8565d":"markdown","aceae664":"markdown","b9010636":"markdown","2ba2713f":"markdown","6fe9b72f":"markdown","abe0df5c":"markdown","d476a84d":"markdown","fa718cb9":"markdown","592134b0":"markdown","4f4fccf8":"markdown","3a4bd422":"markdown"},"source":{"c20ca600":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7812aa91":"# Importing required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\npd.set_option(\"display.max_columns\", None) # to see all columns ","da0c47b0":"data = pd.read_csv('\/kaggle\/input\/censusincome\/adult.csv')\ndata.shape","3d688599":"# understanding type of values in each column of our dataset\ndata.info()","b046852f":"# understanding data, how it look like default is 5 rows\n\ndata.head()","3ecb7928":"# rows with missing values represented as'?'.\ndata_missing = data[data.workclass == '?']\n\n# printing only thouse rows which have missing values in them\ndata_missing.head()","e1ebd956":"# shape of data_missing\ndata_missing.shape","84051ac1":"data_missing.shape[0]","efc8fa6f":"# finding for % of missing values \n(data_missing.shape[0]\/data.shape[0])*100","88de3ebd":"# dropping rows having missing values in 'workclass'\ndata = data.drop(data_missing.index,axis=0) # axis 0 mean rows i.e remove thouse row index which have missing values\n\n#Print (first 5) rows of dataframe after dropping\ndata.head()","f64bacf6":"# selecting all categorical variables\nonly_categorical_columns_of_data = data.select_dtypes(include=['object'])\n\n# checking whether any other columns contains \"?\"\n(only_categorical_columns_of_data == '?').sum()","2ff574c2":"# dropping \"?\"s one by one\nonly_categorical_columns_of_data = data[data.occupation=='?']\ndata = data.drop(only_categorical_columns_of_data.index,axis=0) \n\nonly_categorical_columns_of_data = data[data['native.country']=='?']  # this method can be used to update both column at once\ndata = data.drop(only_categorical_columns_of_data.index,axis=0)","e894d86e":"# see nan if any\n(data == '?').sum()","76ba8b31":"data.info()","380014ec":"# selecting all categorical variables\ncat_col = [col for col in data.columns if data[col].dtype == 'O']\ndata_categorical = data[cat_col]\n\n# first 5 rows of categorical columns\ndata_categorical.head()","9804adc5":"from sklearn.preprocessing import LabelEncoder\n\n\n# initialising LabelEncoder function\nLE = LabelEncoder()\n\n# applying encoding on data_categorical\nfor col in cat_col:\n    data_categorical[col] = LE.fit_transform(data_categorical[col])\n\n\n# printing data_categoricall first 5 rows\ndata_categorical.head()","1a7d447c":"# droping all categorical columns from data \ndata.drop(cat_col,axis=1,inplace=True)\n\n#concating data_categorical to data(original)\ndata = pd.concat([data,data_categorical],axis=1)\n# printing last 5 rows of data\ndata.tail()","d46fa413":"# looking at column types in data\ndata.info()","f1fad610":"# converting target variable income to categorical\ndata['income'] = data['income'].astype('category')","83d14e96":"# importing train-test-split \n\nfrom sklearn.model_selection import train_test_split","3666e8d5":"# putting feature variable to X\nX = data.drop('income',axis=1)\n\n# putting response or target variable to y\ny = data.income","24ec3d62":"# splitting data into train and test with test size as 30%, and random state as 108\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)","b0749841":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","c4dcd9c4":"# importing decision tree classifier from sklearn library\n\nfrom sklearn.tree import DecisionTreeClassifier","3fbd7d36":"# initialising decision tree with default hyperparameters, apart from max_depth which is 5 so that we can plot and read tree\nDT_default = DecisionTreeClassifier(max_depth=5)\n\n# fitting descision tree on train & test data\nmodel_DT = DT_default.fit(X_train,y_train)","4b79cc7d":"# importing classification report,confusion matrix and accuracy_score from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","6c384f09":"# predicting\ny_pred_test_default = model_DT.predict(X_test)\n\n# printing classification report\nprint(classification_report(y_test,y_pred_test_default))","c65a6d9b":"# printing confusion matrix \nprint(f' Confusion_Matrix: \\n {confusion_matrix(y_test,y_pred_test_default)}','\\n')\n\n# Printing accuracy\nprint(f' Accuracy_Score:\\n {accuracy_score(y_test,y_pred_test_default)}')","3dbf1769":"# import plot_confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix","9fa8feec":"# plot confusion matrix\nplot_confusion_matrix(model_DT,X_test,y_test,cmap='rainbow')","4ca1a91a":"# putting all feature names in a list\nfeatures_name = list(data.columns)\n\n# putting all classes in list i.e >=50 and <=50\nclasses = ['>=50', '<=50']","669e119f":"# see features_name\nfeatures_name","14d5375d":"# see classes\nclasses","7463092b":"# importing required packages for visualization\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.tree import export_text","e477967c":"plt.figure(figsize=(25,10))\n\nplot_tree(model_DT,filled=True) # filled=True to fill colour\n \nplt.show()","1764a679":"### saving tree figure to analysie further\n###fig.savefig('tree.png',format='png')","8cfb67f3":"# printig textual representation of a tree \ntext_representation = export_text(model_DT)\n\n# printing text_representation\nprint(text_representation)","97f9681c":"# you can see and read about every possible hyperparameter and there working\nhelp(DecisionTreeClassifier)","87241761":"# all parameters of decision tree classifier\nDecisionTreeClassifier.get_params(DecisionTreeClassifier).keys()","b59518f6":"# ImportING Kfold and GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","fce13713":"# specifying number of folds for k-fold CV (taking 5)\nn_folds = 10\n# parameters to build model on (max_depth from range 1 to 50)\nparameters = {'max_depth':range(1,50)}\n\n# instantiating model (DecisionTreeClassifier) with criteria gini and random_state as 108\ndtree = DecisionTreeClassifier(criterion='gini',random_state=108)\n\n# instantiating GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true\nGScv = GridSearchCV(estimator=dtree,param_grid=parameters,cv=n_folds,scoring='accuracy',return_train_score=True)\n#fiting tree on training data\nGScv.fit(X_train,y_train)","2ef41068":"# scores of GridSearchCV\nscores_GScv = GScv.cv_results_\n\n#Checking scores in a dataframe\nscores_GScv_df = pd.DataFrame(scores_GScv)\nscores_GScv_df.head()","c9ea00a8":"# plotting accuracies with max_depth vs mean train and test scores\n\nplt.figure(figsize=(16,5))\n\nplt.plot(scores_GScv_df['mean_train_score'],label=\"training accuracy\",marker='o',linestyle='dashed')\nplt.plot(scores_GScv_df['mean_test_score'],label=\"test accuracy\",marker='o',linestyle='dashed')\n\nplt.legend(loc=\"upper left\", prop={'size':15})\nplt.xlabel(\"max_depth\", size=15) \nplt.ylabel(\"Accuracy\", size=15)\n\nplt.show()","7fd05dfc":"# specifying number of folds for k-fold CV (here 5)\nn_folds = 10\n# parameters to build  model on(min_samples_leaf with range 5 to 200 and with step of 20)\nparameters = {'min_samples_leaf':range(5,200,20)}\n\n# instantiating model (DecisionTreeClassifier)\ndtree = DecisionTreeClassifier(criterion='gini',random_state=108)\n\n# instantiating GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true\nGScv = GridSearchCV(estimator=dtree,param_grid=parameters,scoring='accuracy',cv=n_folds,return_train_score=True)\n#fitting above tree on training data\nGScv.fit(X_train,y_train)","5994e6a8":"# scores of GridSearchCV\nscores_GScv = GScv.cv_results_\n\n#Checking scores in a dataframe\nscores_GScv_df = pd.DataFrame(scores_GScv)\nscores_GScv_df.head()","99019713":"# plotting accuracies with min_samples_leaf vs mean train and test accuracy\n\nplt.figure(figsize=(16,5))\n\nplt.plot(scores_GScv_df['mean_train_score'],label=\"training accuracy\",marker='o',linestyle='dashed')\nplt.plot(scores_GScv_df['mean_test_score'],label=\"test accuracy\",marker='o',linestyle='dashed')\n\nplt.legend(loc=\"best\", prop={'size':12}) # loc = best,upper right,upper left,lower left,lower right,right,center left,center right,lower center,upper center,center\nplt.xlabel(\"min_samples_leaf\", size=15) \nplt.ylabel(\"Accuracy\", size=15)\nplt.xticks(range(0,10,1))\n\nplt.show()","4d392136":"# specifying number of folds for k-fold CV\nn_folds = 10\n\n# parameters to build the model on (min_samples_split with range 5 to 200 with step size as 20)\nparameters = {\"min_samples_split\":range(5,200,20)}\n\n# instantiating model(DecisionTreeClassifier)\ndtree = DecisionTreeClassifier(criterion='gini',random_state=108)\n\n# instantiating GridSearchCV with above 3 parameters and scoring will be accuracy, while return_train_score as true\nGScv = GridSearchCV(dtree,parameters,cv=n_folds,scoring='accuracy',return_train_score=True)\n\n#fitting the above tree on training data\nGScv.fit(X_train,y_train)","6af5b785":"# scores of GridSearchCV\nscores_GScv = GScv.cv_results_\n\n#Checking scores in a dataframe\nscores_GScv_df = pd.DataFrame(scores_GScv)\nscores_GScv_df.head()","eed88b99":"# plotting accuracies with min_samples_split vs mean train and test accuracy\nplt.figure(figsize=(16,5))\n\nplt.plot(scores_GScv_df['mean_train_score'],label=\"training accuracy\",marker='o',linestyle='dashed')\nplt.plot(scores_GScv_df['mean_test_score'],label=\"test accuracy\",marker='o',linestyle='dashed')\n\nplt.legend(loc=\"best\", prop={'size':12}) # loc = best,upper right,upper left,lower left,lower right,right,center left,center right,lower center,upper center,center\nplt.xlabel(\"min_samples_split\",size=15) \nplt.ylabel(\"Accuracy\",size=15)\nplt.xticks(range(0,10,1))\n\nplt.show()","ee93dd81":"# creating parameter grid\nparam_grid = {\n    'max_depth': range(5,15,5),\n    'min_samples_leaf': range(50,150,50),\n    'min_samples_split': range(50,150,50),\n    'criterion': [\"entropy\",\"gini\"]}\n\n# number of folds\nn_folds = 10\n# instantiating grid search model with default parameters value\ndtree = DecisionTreeClassifier(random_state=108)\n\n\n# instantiating GridSearchCV with above 3 parameters and verbose as 1\ngrid_search = GridSearchCV(estimator=dtree,param_grid=param_grid,cv=n_folds,verbose=1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","720ab337":"# cross validation results\ncv_results = grid_search.cv_results_\n\n# printing CV_result\npd.DataFrame(cv_results).head()","0b002204":"# printing\nprint(f' Best Accuracy you can get:\\n {grid_search.best_score_}\\n','***'*15)\n\n# printing best hyperparameters parameters & its values\nprint(f' Best Hyperparameters Parameters & there Values: \\n{grid_search.best_params_}')","b9669d36":"# model with optimal hyperparameters also add random state as 108\ndt_with_optimal_hyperparameters = DecisionTreeClassifier(criterion='entropy',max_depth=10,min_samples_leaf=50,\n                                  min_samples_split=50,random_state=108)\n\n\n# fitting above model with training data\nmodel_dt = dt_with_optimal_hyperparameters.fit(X_train,y_train)\nmodel_dt","33bb8a64":"from sklearn import metrics","9afa0a1b":"# printing model score\ny_pred_test = model_dt.predict(X_test)\nmetrics.accuracy_score(y_test,y_pred_test)","715c7771":"# plotting tree\nfig = plt.figure(figsize=(100,30))\nplot_tree(model_dt,filled=True,fontsize=20)\nplt.show()\n\n### saving tree figure to analysie further\n###fig.savefig('tree.png',format='png')","4d3b54c6":"print(export_text(model_dt))","7e0b1cb4":"# tree with max_depth = 3\ndt_with_max_depth_optimal_hyperparameters =  DecisionTreeClassifier(criterion='entropy',max_depth=3,min_samples_leaf=50,\n                                   min_samples_split=50,random_state=108)\n\n\n# fitting model with training data\nmodel_dt = dt_with_max_depth_optimal_hyperparameters.fit(X_train,y_train)\nmodel_dt","069d4d83":"# printing model score\ny_pred_test = model_dt.predict(X_test)\nmetrics.accuracy_score(y_test,y_pred_test)","ab9e0dea":"# plotting tree for max_depth=3\n\nplt.figure(figsize=(20,6))\n\nplot_tree(model_dt,filled=True,fontsize=10)\n\nplt.show()","71e15eba":"print(export_text(model_dt))","48a78c4e":"# printing matrix of classification_report\nprint(metrics.classification_report(y_test,y_pred_test))","a2549122":"# printing confusion matrix \nprint(metrics.confusion_matrix(y_test,y_pred_test),'\\n','***'*10)\n\n# ploting confusion matrix\nplot_confusion_matrix(model_dt,X_test,y_test)\n\nplt.show()","8e847dea":"# model built with proper hyperparameter\nmodel_dt","4b86c89f":"# initialising cost_complexity_pruning_path with training data\npruning_path = model_dt.cost_complexity_pruning_path(X_train,y_train)\n\n# checling out ccp_alphas and impurities\nalphas, impurities = pruning_path.ccp_alphas,pruning_path.impurities","4885a0e5":"# all alpha value\nalphas","a326baf7":"# all impurities value\nimpurities","d2eddea5":"# initialising empty arrays for storing train and test accuracy\ntrain_accuracy, test_accuracy = [],[]\n\n# iterating over ccp_alpha\nfor ccp_alpha in alphas:\n    # instantiating DecisionTreeClassifier with random_state=108 and ccp_alpha as alpha iterated value\n    dt_clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha,random_state=108)\n    # fitting model on training data\n    dt_clf.fit(X_train,y_train)\n    \n    # predicting on train data\n    y_pred_train= dt_clf.predict(X_train)\n    # predicting on test data\n    y_pred_test= dt_clf.predict(X_test)\n    \n    # appending training accuracy \n    train_accu = metrics.accuracy_score(y_pred_train,y_train)\n    train_accuracy.append(train_accu)\n    # appending test accuracy \n    test_accu = metrics.accuracy_score(y_pred_test,y_test)\n    test_accuracy.append(test_accu)","0161b730":"# plotting all alphas vs Train and test accuracy scores\nplt.figure(figsize=(16,6))\n\nplt.plot(alphas,train_accuracy,label='train',marker='o',linestyle='dashed')\nplt.plot(alphas,test_accuracy,label='test',marker='o',linestyle='dashed')\n\nplt.legend(loc=\"best\",prop={'size':10})\nplt.xlabel('alpha',size=15)\nplt.ylabel('accuracy',size=15)\n\nplt.show()","58caafce":"# will add ccp_alpha value to it\nmodel_dt","656e0464":"'''Seting '0.01'  as ccp_alpha value in our final deciscion tree'''\n\n# instantiating DecisionTreeClassifier\nfinal_dt_clf = DecisionTreeClassifier(ccp_alpha=0.01,random_state=108)\n\n# fitting model on training data\nfinal_model = final_dt_clf.fit(X_train,y_train)\n\n# predicting on train data\ny_pred_train = final_model.predict(X_train)\n\n# predicting on test data\ny_pred_test = final_model.predict(X_test)\n\n# printing final test and train accuracy\nfinal_train_accuracy = metrics.accuracy_score(y_pred_train,y_train)\nfinal_test_accuracy = metrics.accuracy_score(y_pred_test,y_test)\n\nprint(f' Final_train_accuracy\\n {final_train_accuracy}','\\n','***'*6)\nprint(f' Final_test_accuracy\\n {final_test_accuracy}')","6dc4fc5c":"# plotting tree  \nplt.figure(figsize=(25,10))\n\nplot_tree(final_model,filled=True)\n\nplt.show()","4e7cdb68":"**Now I am trying to set these values of `alpha` and `pass them to ccp_alpha parameter of our DecisionTreeClassifier` as by looping over `alphas array` I will find `accuracy on both Train and Test parts of this dataset`**","7ab2ea93":"Observe above model is not much better than default model\n\nStill its fine not that bad. Atleast now tree is not much complex to understand","8abdab57":"![image.png](attachment:8adc9cab-2338-4150-a7e9-bd6d7d52d469.png)","486a18db":"accuracy_score is decreasing","09fe2e28":"**`GridSearchCV to find optimal max_depth`**","a29c75a6":"\n**`Encoding Categorical variables using LabelEncoder()`**","daf677a3":"# Hyperparameter DT\nTry to Understand parameters in a Decision Tree\n**`Default Tree is quite complex and one need to simplify it by tuning Hyperparameters`**\n* `criterion`\n    * defines function to measure `quality of a split`\n    * Sklearn supports `gini` criteria for `Gini Index`\n    * `entropy` for `Information Gain`\n    * By default it takes the value `gini`\n* `splitter`\n    * defines strategy to choose split at each node\n    * supports `best value` to choose `best split` & `random` to choose `best random split`\n    * By default it takes `best` value\n* `max_features`\n    * nunmber of features to consider when looking for best split\n    * one can input integer, float, string & None value\n        * If an integer is inputted then it considers that value as max features at each split\n        * If float value is taken then it shows percentage of features at each split\n        * If `auto` or `sqrt` is taken then `max_features = sqrt(n_features)`\n        * If `log2` is taken then `max_features = log2(n_features)`\n        * If `None` then `max_features=n_features`\n        * By default it takes `None` value\n* `max_depth`\n    * maximum depth of tree\n    * It can take any integer value or None\n    * If `None` then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n    * By default it takes `None` value\n* `min_samples_split`\n    * minimum number of samples requires to split an internal node\n    * If `integer value` is taken then consider `min_samples_split` as minimum no\n    * If `float value` is taken then it shows `percentage`\n    * By default it takes `2` value \n* `min_samples_leaf`\n    * minimum number of samples required to be at a leaf node\n    * If `integer value` is taken then consider - -`min_samples_leaf` as minimum no\n    * If `float value` is taken then it `shows percentage` \n    * By default it takes `1` value\n* `max_leaf_nodes`\n    * defines maximum number of possible leaf nodes \n    * If `None` then it takes an unlimited number of leaf nodes\n    * By default it takes `None` value\n* `min_impurity_split`\n    * defines threshold for early stopping tree growth\n    * A node will split if its impurity is above threshold otherwise it is a leaf","8efdcee9":"---\n# `Problem Statement` : Income Prediction\nI am trying to build a Decision Tree to predict(target) `income` of a given population, which is labelled as `<=$50K and >$50K`\n\n`Attributes` (predictors) are `age`, `working class type`, `marital status`, `gender`, `race` and other.\n\n---","69730768":"# 2. Tuning `min_samples_leaf`\n`min_samples_leaf` =>  indicates minimum number of samples required to be at a leaf <br>\n\nIf values of `min_samples_leaf is less` say >5, then will be constructed even if a leaf has 5, 6 etc observations (and is likely to overfit)\n\n\n\n* `Let's see what will be the optimum value for min_samples_leaf`","f806501e":"Observe the difference\n","cd8f49de":"Still `native.counter` columns have 556 say kind of missing values along with `occupation` column","81855d31":"# Plotting Decision Tree\n* [All DT Doc](https:\/\/scikit-learn.org\/stable\/modules\/tree.html)","632405ac":"`Observe above tree looks bit more clear and understanding`\n\nSo can say after loosing accuracy of almost 1% we have got loat of interpretability for out DT model","0fd1b613":"# Pruning Descision Tree\nSee Pruning as a factor which `protects DT to Overfit`\n* It involves selective removal of certain parts of a tree\n* It makes DT versatile so that it can adapt if we feed any new kind of data to it, thereby fixing problem of overfitting oveo\n* `Reduces size of a Decision Tree` which might slightly `increasev training error` but `drastically decrease testing error`, hence making it more adaptable\n\n---\n`Minimal Cost-Complexity` pruning is one of the types of Pruning of Decision Trees \n* This algorithm is parameterized by `\u03b1(\u22650)` known as `complexity parameter`\n* [Plot_Cost_Complexity_Pruning](https:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_cost_complexity_pruning.html)\n\n\nDecisionTree in sklearn has a function called `cost_complexity_pruning_path` it:\n* `gives effective alphas` of subtrees during pruning and also `corresponding impurities`\n    * these values of `alpha can be used to prune our Decision Tree`","7ce7b782":"___\n___\n___\n# If you find this notebook helpfull then I have treasure for you [My Other Notebook INDEX](https:\/\/www.kaggle.com\/mukeshmanral\/index-machinelearning-deeplearning\/edit\/run\/78049910)\n___\n___\n___","a7b7ee71":"# <h1><center>Decision Tree <\/center><\/hd>","1065db1a":"# Model Building\n>`Step-1`. I'll build a Decision Tree with default Hyperparameters <br>\n`Step-2`.  I'll use Cross Validation to tune them","2f988a83":"Now '?' value have been removed.\n* One can also handle it while loading the dataframe, I am doing this as to make sure one who is new can undestand it.","980fa135":"Observe that as min_sample_split increases tree overfits lesser since the model is less complex","63a00875":"# Work Flow\n`1. Data Cleaning and Preparation`\n\n`2. Building a Decision Tree with default Hyperparameters`\n\n`3. Understanding of all hyperparameters of DT and then tuning them`\n\n`4. Finally choosing optimal hyperparameters using cross-validations`\n\n`5. Understanding and working on Pruning of`","1f24f95c":"**`concating data_categorical(encoded) with original data`**","d0630f53":"* Observe above data columns and you will see `workclass` and `occupation` consist of missing values which are represented as `?` in dataframe\n    * Keep in mind Missing value can be `0` or `?` or `Nan` etc\n* you will also find that whenever `workclass` column is having a missing value, `occupation` column is also having missing in that row\n    * Time to check how may rows are missing","6738cf52":"---\n---\n# `I will add more to this notebook in future, for now happy lurning'","d7e5a994":" # Hyperparameter Tuning\n This is how one tune above given hyperparameters\n1. `KFold Cross Validation`\n2. `GridSearch Cross Validation`\n3. `RandomSerch Cross Validation`","40025c9d":"As value of max_dept increase, both training and test score increases about maybee max-depth = 6, after which test score gradually reduces <br> \n`Note:`<br> \n* Scores are `average accuracies` across 7-folds\n* It is clear that `model is overfitting training data` if max_depth is too high Next\n\n\n#### Time to see how Model behaves with Other Hyperparameters","665bf9c1":"![image.png](attachment:0f1283f1-7ad2-4784-92c0-72f6dffb53f4.png)","98f23497":"`Model Results seems very much trustworthy now`","d6d0db40":"**`Let's see whether any other columns contain a \"?\". Since \"?\" is a string`** <br>\nas '?' is a string, it will be good to use only Categorical columns","4dcd8de9":"Observe see that `between first and second alpha values we get maximum test accuracy` although our train accuracy has decreased to 0.8\n* `this model is now more generalized and it will perform better on unseen data`\n\nNow I will `add ccp_alpha as another hyperparameter along with grid searched parameters in this deciscion tree model`","5a21129e":"**`GridSearchCV to find optimal min_samples_leaf`**","3dfa52dc":"**`Observe how our default model is performing.To check progress lets know what is type 1 and type 2 error in above case`**\n1. `Type 1 Error(False +ve)` => Rejecting null hypothesis [322]\n    * Falsely infers existence of a somthing that does not exist\n    * Example : Telling a man he is pregnant\n2. `Type 2 Error(False -ve)` => Fail to reject null hypothesis [1062]\n    * Falsely infers non existence of a somthing that does exist\n    * Example : Telling a pregnant woment she is not pregnent","b6331553":"* Total 1836 rows with missing values, which is about 6% of the total data\n    * I want to choose to simply drop these rows, since dropping just 5% data won't make any big impact","4997e803":"`As all Categorical variables are suitably encoded now building the model`","339d16ae":"___\n___\n___\n# If you find this notebook helpfull then I have treasure for you [My Other Notebook INDEX](https:\/\/www.kaggle.com\/mukeshmanral\/index-machinelearning-deeplearning\/edit\/run\/78049910)\n___\n___\n___","9ff50278":"# Hyperparameter Decision Tree\n`Observe Default Tree it is quite complex and we need to simplify it by tuning its hyperparameters`","72026374":"# Tree Jargons\n* `Root Nodes\/Parent Nodes` => very first node in a tree (best node in the tree)\n* `Internal Nodes` => nodes which are connected to deeper nodes\n* `Leaf Nodes\/Decision Nodes` => node which are not connected to deeper nodes, but have an internal node connected to it\n* `Child Nodes` => nodes which are below\n","6d52c2d5":"# Visualizing how train and test score changes with max_depth","b1c57d02":"# Aplying Pruning on Last model","6276d7d4":"# 1. Tuning `max_depth`\nFirst I am trying to find optimum values for `max_depth` and understand `how value of max_depth affects Decision Tree??`\n\nI am trying to create a dataframe with `max_depth` in `range(1to80)` and checking `accuracy_score` corresponding to `each max_depth` \n\n**`GridSearch consists of`**:\n* an `estimator` (classifier such as SVC() or decision tree)\n* a `parameter space`\n* a `method for searching` or `sampling candidates`(optional) \n* a `cross-validation scheme`\n* a `score function`(accuracy, roc_auc etc)","6467730a":"# Hyperparameter Tuning\nHyperparameter can be tuned using:\n* `KFold Cross Validation`\n* `GridSearch Cross Validation`\n* `RandomSerch Cross Validation`","702f5669":"Lets dive in","4a1ebe96":"Observe that this tree is too complex to understand\n\n**`Let's try reducing just `max_depth` and see how tree looks`**","31897c67":"**`Checking Evaluation Metrics of DT default model`**\n* [Classification_Report Doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html)\n* [Confusion_Matrix Doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n* [Plot_Confusion_Matrix Doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_confusion_matrix.html)","944fc669":"# Data Preparation\nThere are n number of preprocessing steps one need to do before building the model ,totaly depends upon data\n\n**`Why Data Preparation?`**\nSimple example is `ADD 2 + your name + kaggle`, what is the output, nothing same goes with model too, note that we have both categorical(object) and numeric features(int or float) as predictors, processing both is hard for model as for you, `but This IS DT` `it can process categorical variables easily`, but but but `I still need to encode Categorical Variables into a standard format so that sklearn can understand them and build the tree out of them` to rescue us here comes:\n* `LabelEncoder()` class => It comes with `sklearn.preprocessing` => [Documentation](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n\nSteps:","7df8565d":"# Tuning `min_samples_split`\n`This shoews minimum no. of samples required to split an internal node`\n* `default value` => `2`\n    * it means that even if a node is having 2 samples it can be furthur divided into leaf nodes","aceae664":"* [DecisionTree Classifier Docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","b9010636":"* Decision Trees can be used in `Supervised or Unsupervised contexts`, for `both Classification and Regression`\n* It can be seen as if-else condition\n\n**`NOTE`**<br>\n* Decision Trees are highly flexible and intuitive but not most accurate. However, they are foundation of other incredibly powerful algorithms like Bagging,Boosting,Classification and Regression Trees (CART)\n    * Classification and Regression Trees (CART) is one of most used algorithms in Machine Learning, as it appears in Gradient Boosting\n    * This means that most popular packages like XGBoost and LightGBM are using CART to build trees\n\n---\n\n* Decision Tree or Classification Tree or Regression Tree is a generic term and they can be implemented in many ways \u2013 don't get terms mixed\n    * A Decision Tree is not necessarily a Classification Tree, it could also be a Regression Tree","2ba2713f":"**`apply LabelEncoder() to data_categorical`**","6fe9b72f":"Observe for low values of min_samples_leaf tree gets a bit overfitted, min_sample_leaf value is good after may be 5","abe0df5c":"# Jargons\n1. `Gini Impurity` for (Internal nodes & Leaf) => `helps in measuring quality of a split`\n    * A Classification metric that measures how we should create internal nodes and leaf nodes\n    * This metric is different from `Gini Index and Information Gain(Impurity Improvement)`, but similar to `Gini Index and Information Gain(Impurity Improvement)`\n2. `Entropy` \n    * maximize `Information Gain` to reduce `entropy`\n3. `Pruning`\n    * technique used to overcome problem of Overfitting \n    * pruning involves selective removal of certain parts of a tree to protect it from overfitting\n    \n**`What Pruning does to our Decision Trees??`**\n* It makes DT versatile so that it can adapt if we feed new kind of data to it, thereby fixing problem of overfitting\n* Reduces size of a DT which might slightly increase training error but drastically decrease testing error hence making it more adaptable <br>\n**`Minimal Cost-Complexity Pruning`** is one of the types of Pruning of Decision Trees\n* This algorithm is `parameterized by \u03b1(\u22650)` known as `complexity parameter`\n\n**DecisionTree in sklearn has a function called `cost_complexity_pruning_path`**<br>\n* it gives effective alphas of subtrees during pruning and also corresponding impurities\n    * `In other words one can use these values of alpha to prune the Decision Tree`","d476a84d":"Observe income column now it is 0 or 1","fa718cb9":"**`printing Optimal Accuracy Score`**","592134b0":"`income` is the target variable but it is in `int`, which should be in categorical data type ","4f4fccf8":"# Finding Optimal Hyperparameters(GridSearchCV)\nUsing GridSearchCV to find multiple optimal hyperparameters together \n* also specify criterion (gini\/entropy or IG)","3a4bd422":"**`running model with best parameters obtained from GridSearchCV`**"}}