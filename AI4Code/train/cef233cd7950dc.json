{"cell_type":{"79826c57":"code","a756023b":"code","b9f0472b":"code","b011d9df":"code","2c055dc9":"code","b0249d27":"code","d24e8ab7":"code","f6ce8486":"code","1f18de1c":"code","dec98f00":"code","d1ad6a75":"code","bcc3c96c":"markdown","238b06fa":"markdown","df9e6b00":"markdown","f40afc53":"markdown","58d9ad35":"markdown"},"source":{"79826c57":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport transformers\nfrom transformers import AlbertTokenizer, AlbertModel#Bert\u304c\u91cd\u304b\u3063\u305f\u306e\u3067\u3001albert\u306b\u5909\u66f4\nfrom tqdm import tqdm\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nimport umap#\nimport matplotlib.pyplot as plt\nimport time\ntqdm.pandas()","a756023b":"device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\ntrain = pd.read_csv(\"..\/input\/data-science-winter-osaka2\/train.csv\")\ntest = pd.read_csv(\"..\/input\/data-science-winter-osaka2\/test.csv\")","b9f0472b":"#\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u4ed8\u3051\u66ff\u3048\ntrain[\"user_reviews_int\"] = train[\"user_reviews\"].map({'c0':0, 'c1':1, 'c2':2})\ntrain['description'] = train['description'].fillna(\"NaN\")\ntest['description'] = test['description'].fillna(\"NaN\")\n\nprint(train.shape)\nprint(test.shape)","b011d9df":"#\u63a8\u8ad6\u7528\u306edataloader\nclass Description_dataset_valid(Dataset):\n    def __init__(self, df, \n                 features=\"description\",\n                 model_name = 'albert-base-v1',\n                 max_len=512#\u9577\u6587\u306e\u7d39\u4ecb\u3082\u591a\u304b\u3063\u305f\u306e\u3067\u3001\u30e2\u30c7\u30eb\u306e\u6700\u5927\u307e\u3067token\u306e\u9577\u3055\u3092\u62e1\u5f35(\u3057\u305f\u305f\u3081\u306b\u30e1\u30e2\u30ea\u306e\u6d88\u8cbb\u304c\u6fc0\u3057\u304f\u306a\u308a\u307e\u3057\u305f\u3002)\n                ):\n        self.features_values = df[features].values\n        self.tokenizer = AlbertTokenizer.from_pretrained(model_name)\n        self.max_len = max_len\n\n    # len()\u3092\u4f7f\u7528\u3059\u308b\u3068\u547c\u3070\u308c\u308b\n    def __len__(self):\n        return len(self.features_values)\n\n    # \u8981\u7d20\u3092\u53c2\u7167\u3059\u308b\u3068\u547c\u3070\u308c\u308b\u95a2\u6570    \n    def __getitem__(self, index):\n        text = self.features_values[index]#text\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\n        inputs = self.tokenizer.encode_plus(\n              text,\n              add_special_tokens=True,\n              max_length=self.max_len,\n              padding='max_length',\n              truncation=True\n            )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n          'ids': torch.LongTensor(ids),\n          'mask': torch.LongTensor(mask)\n        }","2c055dc9":"model_name = 'albert-base-v1'\npretrained_bert_model = transformers.AlbertModel.from_pretrained(model_name).to(device)\n\ndataset_valid = Description_dataset_valid(train)\nvalid_loader = DataLoader(dataset_valid, batch_size=8, shuffle=False)\n\n\nembedings = []\nfor batch in tqdm(valid_loader):\n    bert_out = pretrained_bert_model(batch[\"ids\"].to(device), \n                                     batch[\"mask\"].to(device))\n    res = bert_out[\"last_hidden_state\"][:, 0, :] #\u5404\u30b5\u30f3\u30d7\u30eb\u306e[CLS] token\u306e\u307f\u3092\u53d6\u308a\u51fa\u3059(dim:768)\n    if torch.cuda.is_available():    \n        res = res.cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n    else:\n        res =  res.detach().numpy()\n    embedings.append(res)\n    \nres = np.concatenate(embedings)\nnp.save('train_embed_pretrain.npy', res)","b0249d27":"dataset_valid_test = Description_dataset_valid(test)\nvalid_loader_test = DataLoader(dataset_valid_test, batch_size=8, shuffle=False)\n\n\nembedings_test = []\nfor batch in tqdm(valid_loader_test):\n    bert_out = pretrained_bert_model(batch[\"ids\"].to(device), \n                                     batch[\"mask\"].to(device))\n    res_test = bert_out[\"last_hidden_state\"][:, 0, :] #\u5404\u30b5\u30f3\u30d7\u30eb\u306e[CLS] token\u306e\u307f\u3092\u53d6\u308a\u51fa\u3059(dim:768)\n    if torch.cuda.is_available():    \n        res_test = res_test.cpu().detach().numpy() # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf\n    else:\n        res_test =  res_test.detach().numpy()\n    embedings_test.append(res_test)\n    \nres_test = np.concatenate(embedings_test)\nnp.save('test_embed_pretrain.npy', res_test)","d24e8ab7":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u307f\u3067mapper\u3092\u4f5c\u6210\nmapper = umap.UMAP(random_state=0)\nembedding_train = mapper.fit_transform(res)\nembedding_train = pd.DataFrame(embedding_train)\nembedding_train[\"grade\"] = train[\"user_reviews_int\"]","f6ce8486":"embedding_test = mapper.transform(res_test)","1f18de1c":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u5168\u3066\nplt.figure(figsize=[10, 10])\nplt.scatter(embedding_train.iloc[:,0], \n            embedding_train.iloc[:,1], \n            c=embedding_train.iloc[:,2], \n            alpha=0.3, s=3)\nplt.colorbar()","dec98f00":"#\u30ec\u30d3\u30e5\u30fc\u304c\u3064\u3044\u305f\u30c7\u30fc\u30bf\u306e\u307f\nembedding_labeled = embedding_train[embedding_train.iloc[:,2]!=2]\nplt.figure(figsize=[10, 10])\nplt.scatter(embedding_labeled.iloc[:,0], \n            embedding_labeled.iloc[:,1], \n            c=embedding_labeled.iloc[:,2], \n            alpha=0.3, s=3)\nplt.colorbar()","d1ad6a75":"plt.figure(figsize=[10, 10])\nplt.scatter(embedding_train.iloc[:,0], \n            embedding_train.iloc[:,1], \n            alpha=0.3, s=3, label=\"train\")\nplt.scatter(embedding_test[:,0], \n            embedding_test[:,1], \n            alpha=0.3, s=3, label=\"test\")\nplt.legend()","bcc3c96c":"\u4ee5\u4e0b\u306eNotebook\u306b\u3066\u3001\u8a13\u7df4\u6e08\u307fbert\u3092\u7528\u3044\u305fembedding\u304c\u5b9f\u65bd\u3055\u308c\u3066\u304a\u308a\u307e\u3057\u305f\u306e\u3067\u3001\u53c2\u8003\u306b\u3055\u305b\u3066\u9802\u304d\u307e\u3057\u305f\u3002\n\nhttps:\/\/www.kaggle.com\/hiroshioshio\/bert-text-description\n\n\n\u30d0\u30c3\u30c1\u51e6\u7406\u3055\u305b\u308c\u3070\u9ad8\u901f\u5316\u3067\u304d\u308b\u3001\u3068\u306e\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3054\u3056\u3044\u307e\u3057\u305f\u306e\u3067\u3001\u79c1\u306e\u52c9\u5f37\u306e\u305f\u3081\u306b\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002","238b06fa":"\u5b66\u7fd2\u6e08\u30e2\u30c7\u30eb\u306b\u3088\u308bembed\u3067\u306f\u3001\u5404\u30af\u30e9\u30b9\u304c\u304b\u306a\u308a\u30aa\u30fc\u30d0\u30e9\u30c3\u30d7\u3057\u3066\u3044\u308b\u3002\n* \u305d\u306e\u307e\u307e\u3067\u306f\u3042\u307e\u308a\u6709\u7528\u306a\u7279\u5fb4\u3068\u306f\u306a\u3089\u306a\u305d\u3046...\u3002\n* Bert\u306eFinetune\u306b\u3088\u308a\u3001\u6539\u5584\u3055\u308c\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u306e\u3067\u3001next_step\u3068\u3057\u3066FineTune\u3057\u305f\u91cd\u307f\u306b\u3088\u308bembed\u3092\u5b9f\u88c5\u3057\u305f\u3044\u3002","df9e6b00":"\u793e\u5916\u306eKaggle\u521d\u5fc3\u8005\u3067\u3059\u304c\u3001\u53c2\u52a0\u3055\u305b\u3066\u9802\u3051\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u30b9\u30b3\u30a2\u3067\u306f\u8ca2\u732e\u3067\u304d\u306a\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001Notebook\u306a\u3069\u3067(\u5c11\u306a\u3044)\u77e5\u898b\u3092\u3054\u5171\u6709\u3059\u308b\u3053\u3068\u3067\u3001\u5c11\u3057\u3067\u3082\u8ca2\u732e\u3067\u304d\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002","f40afc53":"# batch\u51e6\u7406\u306b\u3088\u308bdescription embedding\u306e\u5b9f\u88c5","58d9ad35":"### embed\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u308b(by umap)"}}