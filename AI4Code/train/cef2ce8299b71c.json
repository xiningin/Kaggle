{"cell_type":{"46b81c76":"code","f31647c3":"code","4adf146e":"code","2011fd34":"code","2b24a2fa":"code","12abe659":"code","61c6211e":"code","e6d2c611":"code","ca1faa64":"code","e6e53e54":"code","19578f73":"code","193fb798":"code","df001479":"code","e846c88d":"code","87e4e125":"code","7409b52a":"code","49b9b65b":"code","4796e5e3":"code","98d65486":"code","31749092":"code","0b364df5":"code","728baceb":"code","ff4cc1fe":"code","9185efd6":"code","93dd9ea9":"code","83316897":"code","ae6ea4f1":"code","f19979ba":"code","8d8c44af":"code","5e904a78":"code","dd53f936":"code","eb8b3085":"code","53db42a1":"code","bbe0ef83":"code","6cd545d7":"code","3207aa73":"code","0ca67784":"code","e398091f":"code","4cf180f5":"code","c719b5b7":"code","989c418f":"code","347668c9":"code","c5a30c74":"code","abf2255f":"code","5636e700":"code","9acaba51":"code","c7566eac":"code","018ed3e3":"code","066d3e95":"code","43e15198":"code","15ebec68":"code","fd969ebf":"code","93154a12":"code","8af9ae17":"code","d5677e7d":"code","76748bae":"code","9568b7f0":"markdown","ae2a4ac2":"markdown","8a99486c":"markdown","fc63fe9a":"markdown","789ad738":"markdown","b6929027":"markdown","93c85ff4":"markdown","0c683754":"markdown","c2d7247e":"markdown","f226e0ca":"markdown","ec214a7d":"markdown","ff8cbde2":"markdown","54e2383c":"markdown","6bd88aa8":"markdown","6349ac26":"markdown","8003c8a2":"markdown","3b3f7ae0":"markdown","36a67b4c":"markdown","be5e1b5b":"markdown","48c3a021":"markdown","e9631e70":"markdown","c5000dae":"markdown","c1e8d431":"markdown","82b2e537":"markdown"},"source":{"46b81c76":"# Import EDA libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f31647c3":"# Load data\ntrain = pd.read_csv(\"..\/input\/train.csv\", infer_datetime_format=True)","4adf146e":"# Brief visualization\ntrain.head()","2011fd34":"# Brief infomatin\ntrain.info()","2b24a2fa":"# Brief description\ntrain.describe()","12abe659":"# Features\ntrain.columns","61c6211e":"# Size\ntrain.shape","e6d2c611":"# Look for duplicates\ntrain.duplicated().sum()","ca1faa64":"# Look for missing data\ntrain.isna().sum().sum()","e6e53e54":"msno.matrix(train)","19578f73":"miss_count = train.isna().sum()\nmiss_count = miss_count[miss_count > 0]\nmiss_count","193fb798":"# Shows which missings values occurs simultaneously on both columns\nax = msno.heatmap(train, cmap='RdBu')","df001479":"# Show unique values in missing columns\ndef get_unique_values_col(df):\n    data = {}\n    for col, s in df.iteritems():\n        data[col] = s.unique()\n    return pd.Series(data)\nunique_mc = get_unique_values_col(train[miss_count.index])\nunique_mc","e846c88d":"# Cleaned data\nc_train = train.copy()\n\n# Imput categorical columns\nmiss_cat_cols = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu',\n                 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                 'PoolQC', 'Fence', 'MiscFeature']\nc_train[miss_cat_cols] = train[miss_cat_cols].fillna('None')\n\n# Tranform numerical variables in categorical variables\ncat_feat = ['MSSubClass', 'OverallQual', 'OverallCond'] + list(train.select_dtypes('object').columns)\nc_train[cat_feat] = c_train[cat_feat].astype('category')\n\nprint(f\"Empty entries: {c_train[miss_cat_cols].isna().sum().sum()}\")\nc_train[miss_cat_cols].head()","87e4e125":"from sklearn.preprocessing import Imputer\n\n# Imput numeric columns\nc_train['LotFrontage'] = train.LotFrontage.fillna(train.LotFrontage.median())\nc_train['MasVnrArea'] = train.MasVnrArea.fillna(0)\nc_train['GarageYrBlt'] = train.GarageYrBlt.fillna(0)\n\nprint(f\"Empty entries: {c_train.isna().sum().sum()}\")\nc_train.head()","7409b52a":"c_train.describe()","49b9b65b":"# Look at Target variable\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.distplot(c_train.SalePrice, ax=ax)","4796e5e3":"c_train.drop('SalePrice', axis=1).hist(bins=40, figsize=(20, 16))\nplt.tight_layout()","98d65486":"# Some of the main categorical features\nfig, ax = plt.subplots(figsize=(12,10))\nsns.violinplot(x='OverallQual', y='SalePrice', data=c_train, ax=ax)","31749092":"# Some of the main categorical features\nfig, ax = plt.subplots(figsize=(12,10))\nsns.violinplot(x='OverallCond', y='SalePrice', data=c_train, ax=ax)","0b364df5":"# Some of the main categorical features\nfig, ax = plt.subplots(figsize=(12,10))\nsns.violinplot(x='MSSubClass', y='SalePrice', data=c_train, ax=ax)","728baceb":"num_corr_matrix = c_train.corr()\nnum_corr_target = num_corr_matrix['SalePrice'].sort_values(ascending=False)","ff4cc1fe":"num_corr_target.plot.barh(color='steelblue', figsize=(20, 12));\nplt.title('Correlation with Sale Price')","9185efd6":"fig, ax = plt.subplots(figsize=(22, 12))\nsns.heatmap(num_corr_matrix, ax=ax, annot=False, cmap='coolwarm', vmin=-1., vmax=1.)","93dd9ea9":"# Correlation ratio definition\ndef correlation_ratio(categories, measurements):\n    \"\"\"\n    Calculate the correlation ratio between the categoryes and the measurement\n    \n    Args: \n        categories: Iterable with categories\n        measurements: Iterable with the measurements\n    \n    Return:\n        eta: Correlation ratio\n    \"\"\"\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = numerator\/denominator\n    return eta","83316897":"# Categorical feature correlated with numeric target\ncat_cols = c_train.select_dtypes('category').columns\ncat_corr_target = pd.Series([correlation_ratio(c_train[c], c_train['SalePrice']) for c in cat_cols], index=cat_cols)\n\ncat_corr_target.sort_values(ascending=False).plot.barh(color='steelblue', figsize=(20, 12));\nplt.title('Correlation with Sale Price')","ae6ea4f1":"# Removendo Vari\u00e1veis de baixa correla\u00e7\u00e3o com o alvo\nthres_t = 0.1\nnum_corr_matrix = c_train.corr()\ncols_target = num_corr_matrix['SalePrice'][(num_corr_matrix['SalePrice'] > thres_t) |\n                                           (num_corr_matrix['SalePrice'] < -thres_t)].sort_values(ascending=True).index\ncols_target","f19979ba":"new_num_corr_matrix = c_train[cols_target].corr()","8d8c44af":"# Removing numeric variable highly correlated\nthres_i = 0.8\ncols_corr = {}\nfor c in cols_target:\n    series = new_num_corr_matrix[c].drop(c)\n    if np.any([np.any(series > thres_i), np.any(series < -thres_i)]):\n        cols_corr[c] = series.idxmax()\n# Select just one of the pair with highest correlation\nremove_cols = set([k if new_num_corr_matrix['SalePrice'][k] < new_num_corr_matrix['SalePrice'][v] else v for k, v in cols_corr.items() ])\nn_cols = [c for c in cols_target if c not in remove_cols]\nn_cols","5e904a78":"# Categorical columns\nthres_c = 0.1\nc_cols = [c for c, v in cat_corr_target.iteritems() if v > thres_c]\nc_cols","dd53f936":"# Filters data\nf_cols = c_cols + n_cols\nf_train = c_train[f_cols]\nf_train.head()","eb8b3085":"# Check new correlations\nf_num_corr_matrix = f_train[n_cols].corr()\nfig, ax = plt.subplots(figsize=(22, 12))\nsns.heatmap(f_num_corr_matrix, ax=ax, annot=True, cmap='coolwarm', vmin=-1., vmax=1.)","53db42a1":"# Filtered results\nf_cat_corr_target = pd.Series([correlation_ratio(f_train[c], f_train['SalePrice']) for c in c_cols], index=c_cols)\n\nf_cat_corr_target.sort_values(ascending=False).plot.barh(color='steelblue', figsize=(20, 12));\nplt.title('Correlation with Sale Price')","bbe0ef83":"# Dynamic plot\nfrom bokeh.plotting import output_file, figure, show\nfrom bokeh.models.tools import HoverTool\nfrom bokeh.io import output_notebook\n\n# Notebook mode\noutput_notebook()\n\n# New sample for manipulated df\nx = f_train['GrLivArea']\ny = f_train['SalePrice']\nTOOLS = \"hover,pan,wheel_zoom,box_zoom,reset,save\"\np = figure(title=\"Ground Living Area X Sale Price\", tools=TOOLS,\n           y_range=(y.min(), y.max()), x_range=(x.min(), x.max()))\n\np.circle('GrLivArea', 'SalePrice', source=f_train)\n# Linear Regression\nslope, intercept = np.polyfit(x, y, 1)\nreg_x = np.linspace(x.min(), x.max())\np.line(reg_x, reg_x*slope+intercept, color='red')\np.xaxis[0].axis_label = 'Square Feet (ft\u00b2)'\np.yaxis[0].axis_label = 'Price ($)'\n\np.hover.tooltips = [\n    # add to this\n    (\"(Ground Living Area, Sale Price)\", \"($x, $y)\"),\n]\nshow(p)\n\n# Check correlation between variables\n# sns.scatterplot(x='distance', y='fare_amount', data=f_train[columns].sample(100000))","6cd545d7":"# Generate dynamic boxplot\ny = f_train[\"SalePrice\"].copy()\nx = f_train[\"OverallQual\"].copy()\n# Boxplot of categorical feature\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.boxenplot(x, y, color='steelblue', ax=ax)\nax.set_title(\"Overall Quality x Price\")","3207aa73":"# Recap of data types\nf_train.info()","0ca67784":"# Recap of data format\nprint(f_train.shape)\nf_train.head()","e398091f":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split","4cf180f5":"# Managing categorical variables\ncat_cols = f_train.select_dtypes('category').columns\ndf_dummies = pd.get_dummies(f_train[cat_cols], prefix=cat_cols, drop_first=True)\nX_cat = df_dummies\nX_cat.head()","c719b5b7":"# Scaling numeric variables\nscaler = RobustScaler()\nscale_features = list(f_train.select_dtypes(exclude='category').columns)\nscale_features.remove('SalePrice')\nX_num = f_train[scale_features].copy()\nX_num[scale_features] = scaler.fit_transform(f_train[scale_features].values)\nX_num.head()","989c418f":"# Features and target\nX = pd.concat([X_num, X_cat], axis=1)\ny = f_train['SalePrice']\nX.head()","347668c9":"print(X.shape)\nprint(y.shape)","c5a30c74":"# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)","abf2255f":"print(X_train.shape)\nprint(y_train.shape)","5636e700":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","9acaba51":"# SKlearn Models\nlin_reg = LinearRegression()\ntree_reg = DecisionTreeRegressor()\nforest_reg = RandomForestRegressor()\n\n# Scores with x-validation\nlin_scores = cross_val_score(lin_reg, \n                             X_train, \n                             y_train,\n                             scoring = \"neg_mean_squared_error\", \n                             cv = 10)\n\ndecision_scores = cross_val_score(tree_reg,\n                                  X_train, \n                                  y_train,\n                                  scoring = \"neg_mean_squared_error\", \n                                  cv = 10)\n\nforest_scores = cross_val_score(forest_reg,\n                                X_train, \n                                y_train,\n                                scoring = \"neg_mean_squared_error\", \n                                cv = 10)","c7566eac":"# Boost Models\nxgb_reg = xgb.XGBRegressor(n_jobs=12).fit(X_train, y_train)\n\n# Scores with x-validation\nxgb_scores = cross_val_score(xgb_reg, \n                             X_train, \n                             y_train,\n                             scoring = \"neg_mean_squared_error\", \n                             cv = 10)","018ed3e3":"# Test Results\nlin_rmse_scores = np.sqrt(-lin_scores)\ndecision_rmse_scores = np.sqrt(-decision_scores)\nforest_rmse_scores = np.sqrt(-forest_scores)\nxgb_rmse_scores = np.sqrt(-xgb_scores)\n# decision_rmse_test = mean_squared_error(y_test, tree_reg.predict(X_test))\n# forest_rmse_test = mean_squared_error(y_test, forest_reg.predict(X_test))\n# xgb_rmse_test = mean_squared_error(y_test, xgb_reg.predict(X_test))\n\n# Results\nprint(\"Linear Regression Results:\")\n# print(f\"Test MSE: {lin_rmse_test:.2f}\")\nprint(f\"CV RMSE: {lin_rmse_scores.mean():.2f} +\/- {lin_rmse_scores.std() * 2:.2f}\\n\")\nprint(\"Decision Tree Regressor Results:\")\n# print(f\"Test MSE: {decision_rmse_test:.2f}\")\nprint(f\"CV RMSE: {decision_rmse_scores.mean():.2f} +\/- {decision_rmse_scores.std() * 2:.2f}\\n\")\nprint(\"Random Forest Regressor Results:\")\n# print(f\"Test MSE: {forest_rmse_test:.2f}\")\nprint(f\"CV RMSE: {forest_rmse_scores.mean():.2f} +\/- {forest_rmse_scores.std() * 2:.2f}\\n\")\nprint(\"X-Gradient Boosting Results:\")\n# print(f\"Test MSE: {xgb_rmse_test:.2f}\")\nprint(f\"CV RMSE: {xgb_rmse_scores.mean():.2f} +\/- {xgb_rmse_scores.std() * 2:.2f}\\n\")","066d3e95":"from sklearn.model_selection import GridSearchCV","43e15198":"y_train.shape","15ebec68":"# Grid of parameters\nparameters = {'max_depth': [3, 6],\n              'learning_rate': [0.15, 0.75],\n              'n_estimators': [250, 300],\n              }\nfit_parameters = {'early_stopping_rounds':range(4, 12, 2),\n                  'eval_set': (X_test, y_test)\n                  }\ngrid_reg = GridSearchCV(xgb_reg,\n                        parameters,\n                        scoring='neg_mean_squared_error',\n                        cv= 10,\n                        n_jobs=12)\ngrid_reg.fit(X_train, y_train)","fd969ebf":"xgb_rmse_test = mean_squared_error(y_test, grid_reg.best_estimator_.predict(X_test))\n\n# Results\nprint(f\"Best Parameters: {grid_reg.best_params_}\")\nprint(f\"Best Mean CV Score: {np.sqrt(-grid_reg.best_score_):.2f}\")\nprint(f\"Test MSE: {np.sqrt(xgb_rmse_test):.2f}\")\n","93154a12":"modelos = [\"CV Decision Tree Regressor\", \"CV Random Forest Regressor\", \"CV Linear Regression\",\n           \"CV XGBoost Regressor\", \"CV Tunned XGBoost Regressor\"]\nmses = [decision_rmse_scores.mean(), forest_rmse_scores.mean(), lin_rmse_scores.mean(),\n        xgb_rmse_scores.mean(), np.sqrt(-grid_reg.best_score_)]\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_xlabel(\"MSE\")\nax.set_ylabel(\"Model\")\nax.set_title(\"MSE Error - Lower Better\")\nsns.barplot(y=modelos, x=mses, color=\"steelblue\", ax=ax)\n","8af9ae17":"afig, ax = plt.subplots(figsize=(16, 16))\nxgb.plot_importance(xgb_reg, height=.5, ax=ax)","d5677e7d":"# Save model\nimport pickle\npickle.dump(grid_reg.best_estimator_, open(\"model_house.pt\", \"wb\"))","76748bae":"# Load model\nimport pickle\nbest_model = pickle.load(open(\"model_house.pt\", \"rb\"))","9568b7f0":"## Procurar por Correla\u00e7\u00f5es\n\nVerificando as correla\u00e7\u00f5es das vari\u00e1veis com a nossa vari\u00e1vel alvo, podemos tirar conclus\u00f5es sobre as vari\u00e1veis mais **importantes** ou poss\u00edvelmente **redudantes**.\n### Vari\u00e1veis Num\u00e9ricas","ae2a4ac2":"# Save Model\n\nSubmiss\u00e3o no Kaggle compara resultados.","8a99486c":"# EDA\nAgora que os dados est\u00e3o dispon\u00edveis, podemos explor\u00e1-los. Nessa etapa, os dados ser\u00e3o analisados e visualizados com o intuito de se obter insights e informa\u00e7\u00f5es adicionais.\n\nInicialmente, vamos d\u00e1 uma olhada superficial na descri\u00e7\u00e3o dispon\u00edvel dos dados. No pr\u00f3rprio site da competi\u00e7\u00e3o do [Kaggle](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) pode-se notar que os dados correspondem a **81 colunas e 1460 linhas**.\n","fc63fe9a":"# Machine Learning - Cria\u00e7\u00e3o de Modelos\n","789ad738":"## Vari\u00e1veis Categ\u00f3ricas","b6929027":"Temos dados muitos ricos, por\u00e9m escassos. Nesse caso, remover observa\u00e7\u00f5es com valores faltando \u00e9 inaceit\u00e1vel.\n\nPode-se notar que os conjuntos de colunas abaixo possuem dados faltando em simultaneamente:\n\n- `MasVnrType` e `MasVnrArea`: **8**.\n- `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1` e `BsmtFinType2`: **37\/38**.\n- `GarageType`, `GarageYrBlt`, `GarageFinish`, `GarageQual` e `GarageCond`: **81**.\n\nTemos que lidar com os dados faltando de cada tipo feature individualmente. As abordagens usadas foram:\n\n- **Categ\u00f3ricas**: Olhando as descri\u00e7\u00e3o das features, vemos que, com excep\u00e7\u00e3o da `MasVnrType`, as vari\u00e1veis `NaN` significa a falta da feature. Ent\u00e3o, vamos adicionar uma categoria `None` para os dados faltando, e assumir que os dados faltando na categoria `MasVnrType` seja a aus\u00eancia da feature. Al\u00e9m disso, vamos definir as vari\u00e1veis num\u00e9ricas `MSSubClass`, `OverallQual` e `OverallCond` como categ\u00f3ricas, conforme a sua descri\u00e7\u00e3o.\n\n- **Num\u00e9ricas**: Imputar os valores que fa\u00e7am sentido com o que a vari\u00e1vel representa.\n  - `LotFrontage`: Vamos substituir os valores faltando pela **mediana** da coluna.\n  - `MasVnrArea `: Como assumimos que os valores faltando s\u00e3o aus\u00eancia da feature, substitu\u00edmos por **0**.\n  - `GarageYrBlt`: Como nesses casos n\u00e3o existe garagem, vamos subtitu\u00ed-lo por um valor imposs\u00edvel que lhe d\u00ea destaque, nesse caso **0**.","93c85ff4":"## Testando Modelos\n\nCom os dados prontos para os modelos, podemos test\u00e1-los aplicando v\u00e1rios modelos. Inicialmente, os seguintes modelos foram testados:\n- Regress\u00e3o Linear: Modelo linear simples.\n- \u00c1rvore de Decis\u00e3o: Modelo de \u00e1rvore simples.\n- Random Forest: Conjunto de modelo de \u00e1rvore.\n- Boosting Algorithms: Algoritmos amplamente usados, similar ao random forest, mas com regulariza\u00e7\u00e3o.","0c683754":"Com os dados limpos e modificados, podemos prepar\u00e1-los para modelos de ML\/DL. ","c2d7247e":"## Visualizar os Dados\n\nAgora que s\u00f3 temos dados completos e n\u00e3o duplicados, vamos verificar a consist\u00eancia dos valores das colunas. Para isso, iremos plotar algumas visualiza\u00e7\u00f5es.","f226e0ca":"Vamos observar algumas estat\u00edsticas das colunas.","ec214a7d":"## Sintonizando Hyper-par\u00e2metros\n\nVamos testar v\u00e1rios valores de hyper-parametros para encontrarmos o melhor resultado poss\u00edvel do modelo.","ff8cbde2":"## Limpeza\n\nAgora que temos uma no\u00e7\u00e3o geral dos dados, vamos realizar algumas opera\u00e7\u00f5es para verificar se se h\u00e1 dados faltando, duplicados ou outras inconsist\u00eancias.","54e2383c":"## Resultados\n\nAqui est\u00e3o os resultados obtidos dos modelos testados.","6bd88aa8":"## Vari\u00e1veis Num\u00e9ricas","6349ac26":"## Features (Colunas)\n\nAnalisa algumas das **Features** de cada observa\u00e7\u00e3o e suas respectivas descri\u00e7\u00f5es.\n\n- **Id**: ID \u00fanica para identificar do im\u00f3vel - \ud83c\udd94\n- **SalePrice** : Pre\u00e7o de venda  - \ud83c\udfaf\n- **MSSubClass**: Classifica\u00e7\u00e3o do tipo de constru\u00e7\u00e3o.\n- **MSZoning**: Classifica\u00e7\u00e3o geral de zona.\n- **LotFrontage**: Distancia em p\u00e9s de rua conectada a propriedade.\n- **LotArea**: Tamanho da propriedade dem p\u00e9s quadrados.\n- **Street** : Tipo de acesso pela estrada.\n- **Alley** : Tipo de acesso por becos.\n- **LotShape**: Planicidade de propriedade.\n- **LandCountour** : Tipo de acesso pela estrada.\n- **Utilities** : Tipo de servi\u00e7os dispon\u00edveis.\n- **LotConfig**: Configura\u00e7\u00e3o do lote.\n- **LandSlope** : Inclina\u00e7\u00e3o da propriedade.\n- ...","8003c8a2":"## Poss\u00edveis Melhorias\n\n- Utilizar mais dados\n- Continuar procurando melhores hyper-par\u00e2metros\n- Aplicar Deep Learning\n","3b3f7ae0":"### Sele\u00e7\u00e3o de Features\n\nVendo o **heatmap** anterior podemos perceber que algumas features apresentam pouqu\u00edssima correla\u00e7\u00e3o com as vari\u00e1veis alvo. Adicionalmente, fica claro que algumas possuem uma correla\u00e7\u00e3o muito pr\u00f3xima e acabam sendo informa\u00e7\u00e3o redudante.\n\n- Remover features com correla\u00e7\u00e3o absoluta com a vari\u00e1vel alvo menor que **0.1**.\n- Remover uma das features com correla\u00e7\u00e3o absoluta entre si maior que **0.8**.\n","36a67b4c":"Nesse caso, considerando os dados de fontes confi\u00e1veis, remover outlier pode ser um problema.\n\nIsso fica claro quando verificamos que boa parte dos valores que parecem ser outliers s\u00e3o representados dessa forma devido a n\u00e3o exist\u00eancia da feature mostrado pelo alto pico em 0.\n\n**Assim, n\u00e3o haver\u00e1 a remo\u00e7\u00e3o de outliers.**","be5e1b5b":"## Feature Engineering\n\nAqui vamos tentar combinar\/manipular as colunas para que as informa\u00e7\u00f5es presentes nelas e a correla\u00e7\u00e3o com a vari\u00e1vel alvo fiquem o mais claro poss\u00edvel.\n","48c3a021":"## Preparando as Vari\u00e1veis para os Modelos\nAgora, vamos manipular as vari\u00e1veis para que elas reflitam corretamente o seu significado no modelo.\n\n1. **Removendo Vari\u00e1veis de Identifica\u00e7\u00e3o**: Nesse caso, isso j\u00e1 foi feito durante o **Feature Engineering**.\n2. **Modificando Vari\u00e1veis Categ\u00f3ricas**: Todas as vari\u00e1veis categ\u00f3ricas devem ser mapeadas usando o t\u00e9cnica de *One-hot encoding*.\n3. **Preparando Vari\u00e1veis C\u00edclicas**: N\u00e3o h\u00e1 veri\u00e1veis c\u00edclicas.\n4. **Ajustando a Magnitude das Vari\u00e1veis Num\u00e9ricas**: Outra etapa muito importante que tem o prop\u00f3sito de ignorar a intensidade absoluta das vari\u00e1veis. Assim, dando \u00eanfase as varia\u00e7\u00f5es relativas.\n5. **Separando a Vari\u00e1vel Alvo**: Nesse caso, iremos separa a vari\u00e1vel `SalePrice` do restante do dataset.\n6. **Dividindo o Dataset em Treino\/Teste**: Usaremos uma propor\u00e7\u00e3o de 80% treino 20% teste.\n","e9631e70":"Podemos perceber algumas coisas:\n- A vari\u00e1vel num\u00e9rica de maior correla\u00e7\u00e3o \u00e9 `GrLivArea`, \u00e1re de habita\u00e7\u00e3o no t\u00e9rreo.\n- A vari\u00e1vel categ\u00f3rica de maior correla\u00e7\u00e3o \u00e9 `OverallQual`, qualidade geral dos materiais e acabamento.\n- Correla\u00e7\u00f5es entre vari\u00e1veis mostrando redund\u00e2ncia, como `GarageCars` e `GarageArea`.\n- Vari\u00e1veis com pouca influ\u00eancia na vari\u00e1vel alvo.\n\nVamos tentar sanar alguns desses problemas no pr\u00f3ximo t\u00f3pico.","c5000dae":"**Assim, com as features filtradas, resta apenas os dados mais relacionados com a nossa vari\u00e1vel alvo.**\n\nVisualizar a vari\u00e1vel alvo com as mais correlacionadas.","c1e8d431":"### Vari\u00e1veis Categ\u00f3ricas\nNese caso, usaremos o [Correlation Ratio](https:\/\/en.wikipedia.org\/wiki\/Correlation_ratio) para criar um valor num\u00e9rico que indique a influ\u00eancia da categoria no nosso valor num\u00e9rico.","82b2e537":"De fato, podemos ver uma clara rela\u00e7\u00e3o enetre as vari\u00e1veis nesse gr\u00e1ficos."}}