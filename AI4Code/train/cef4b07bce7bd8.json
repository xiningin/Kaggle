{"cell_type":{"e59830cb":"code","89817245":"code","7f8244c3":"code","d27e902f":"code","6e03108e":"code","8b594506":"code","89493ed4":"code","aaeb3018":"code","b4224c92":"code","568d87f9":"code","42e53055":"code","9ec558af":"code","6c5e328c":"code","509ddc33":"code","9c3bcd05":"code","63651dc5":"code","fdb7a686":"code","77f6e077":"code","21192b83":"code","a2be2978":"code","cf318553":"code","6f531076":"markdown","19d78a1e":"markdown","29ccfcfe":"markdown","3059e173":"markdown","22c7384d":"markdown","cf30af94":"markdown","c9c62da7":"markdown","c4740470":"markdown","35b7c4f8":"markdown","b29b4a67":"markdown","73856f73":"markdown","bf518ac1":"markdown","61a2b20a":"markdown","ba80ea1e":"markdown"},"source":{"e59830cb":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","89817245":"# Read train and test files\ntrain_df = pd.read_csv('..\\train.csv')\ntest_df = pd.read_csv('..\\test.csv')","7f8244c3":"train_df.head()","d27e902f":"test_df.head()","6e03108e":"train_df.info()","8b594506":"test_df.info()","89493ed4":"#### Check if there are any NULL values in Train Data\nprint(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","aaeb3018":"#### Check if there are any NULL values in Test Data\nprint(\"Total Test Features with NaN Values = \" + str(test_df.columns[test_df.isnull().sum() != 0].size))\nif (test_df.columns[test_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test_df.columns[test_df.isnull().sum() != 0])))\n    test_df[test_df.columns[test_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","b4224c92":"# check and remove constant columns\ncolsToRemove = []\nfor col in train_df.columns:\n    if col != 'ID' and col != 'target':\n        if train_df[col].std() == 0: \n            colsToRemove.append(col)\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)        \n# remove constant columns in the training set\ntrain_df.drop(colsToRemove, axis=1, inplace=True)\nfor col in test_df.columns:\n    if col != 'ID' and col != 'target':\n        if test_df[col].std() == 0: \n            colsToRemove.append(col)\n# remove constant columns in the test set\ntest_df.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","568d87f9":"def drop_sparse(train, test):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    for f in flist:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n    return train, test","42e53055":"%%time\ntrain_df, test_df = drop_sparse(train_df, test_df)","9ec558af":"gc.collect()\nprint(\"Train set size: {}\".format(train_df.shape))\nprint(\"Test set size: {}\".format(test_df.shape))","6c5e328c":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","509ddc33":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","9c3bcd05":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","63651dc5":"%%time\n# Training LGB\npred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","fdb7a686":"# feature importance\nprint(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:50])","77f6e077":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","21192b83":"%%time\n# Training XGB\npred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"XGB Training Completed...\")","a2be2978":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)","cf318553":"%%time\ncb_model.fit(dev_X, dev_y,\n             eval_set=(val_X, val_y),\n             use_best_model=True,\n             verbose=50)","6f531076":"## Load Train and Test Data","19d78a1e":"So there are a total of 4993 columns out of which 1845 are of type float64, 3147 are int64 and 1 is object (ID is the object column)","29ccfcfe":"So there are a total of 4992 columns in the test set out of which 4991 are of type float64 and 1 is object (ID is the object column)","3059e173":"## Load Required Libraries","22c7384d":"## Drop Sparse Data","cf30af94":"## Catboost","c9c62da7":"## XGB Modeling","c4740470":"## Check and Remove Constant Features","35b7c4f8":"### Test Data","b29b4a67":"## Check for Missing Values","73856f73":"### Train Data","bf518ac1":"## Build Train and Test Data for Modeling","61a2b20a":"## Train and Test Data Info","ba80ea1e":"## LightGBM"}}