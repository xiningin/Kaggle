{"cell_type":{"6a2e6498":"code","b2264b1c":"code","0977bfbd":"code","172e7ac8":"code","3e35d6e8":"code","a6d6d998":"code","f0d0e529":"code","ebc721df":"code","40fa1d60":"code","b5a93fe7":"code","8760845a":"code","b3910f21":"code","fd240dec":"code","c8dbbe71":"code","1381a618":"code","963c607f":"code","6a34cbf5":"code","9ad2cc4a":"code","8a781b51":"code","f8d5dba4":"code","f1470798":"code","913e338c":"code","45fa3f70":"code","8e7ec198":"code","90c446d4":"code","07a6129f":"code","8469e855":"code","ee1312eb":"code","273512bf":"code","e53de911":"code","6fe803f1":"code","1d1feab0":"code","1bdaec7c":"code","458e42a2":"code","7ab69f4f":"code","adddc7ca":"code","279558d1":"code","cfcbb3cd":"code","69d4f91c":"code","1d9b79f1":"code","222845b1":"code","25b4a97e":"code","8e05d374":"code","5052abae":"code","38b9aee3":"code","65422b27":"code","97b2997b":"code","1c735a29":"code","5fe52e86":"code","05ce4036":"code","8d33c5ac":"code","6ecbcd38":"markdown","d3dec4b7":"markdown","e6dcbf10":"markdown","e355a0e7":"markdown","bff6fccf":"markdown","17830dd7":"markdown","54be8f83":"markdown","084f2c17":"markdown","3b67c1b1":"markdown","529cd31a":"markdown","5f71aa8f":"markdown","3a560549":"markdown","fca04fd1":"markdown","4cbc3209":"markdown","c1805732":"markdown","a42cc591":"markdown"},"source":{"6a2e6498":"#import files that are required for reading the data. \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n#plt.figure(figsize=(16,5))\n\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nimport os\n#print(os.listdir('..\/input'))","b2264b1c":"# create datafile\n\ndf= pd.read_csv('..\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')\ndf.head()\n","0977bfbd":"df.info()","172e7ac8":"# all the columns shall be converted into float32 to reduce the file size. This increase the performance speed. Moreover the \n# accuracy beyond 7 digits is not critical. Atleast for prototype testing!\n\ncolumns = list(df.columns[:-1])\n\nfor n in columns:\n    df[n]= df[n].astype(np.float32)\n    \n\n# df['ambient']=df['ambient'].astype(np.float32)\ndf.info()","3e35d6e8":"# first lets test on one profile id. Lets pick profile id == 4\n\ndef profile_id_df(dataframe, prof_id):\n    '''\n    Input:\n    dataframe = Pandas dataframe \n    profile id = # profile id number out of df['profile_id'].unique()\n    \n    Output:\n    filtered dataframe for a given profile id\n    '''\n       \n    \n    return dataframe.loc[dataframe['profile_id'] == prof_id]","a6d6d998":"#lets pick profile id 4 and carry out analysis on this\n\ndf_4 = profile_id_df(df,4)\ndf_4.shape","f0d0e529":"sns.jointplot(x='motor_speed', y='torque', data= df_4)\n\n# looking at the plot there are lot junk data points. (Torque is directly proportional to motor speed) At zero motor speed\n# torque cant increase to 2. Hence the data requires lot of cleaning! Also at various speed levels torque cant be zero. \n# So no clear explaintion provided along with the dataset. Hence we leave this as is and continue our journey of analysis. ","ebc721df":"sns.jointplot(x='motor_speed', y='pm', data= df_4)","40fa1d60":"# dataframe split into x and y data\nX = df_4.drop(['pm','profile_id'], axis = 1)\ny = df_4['pm'] # rotor temperature 'pm'","b5a93fe7":"# import sklearn \nfrom sklearn.linear_model import Ridge, LinearRegression, Lasso, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression","8760845a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","b3910f21":"# Feature selection from selectKbest \"Mutual info regression\" is applicable for continous data type. Most of the other functions\n# are for classification problems. \n\nmethod = SelectKBest(score_func= mutual_info_regression, k = 'all')\n\nmethod.fit_transform(X_train, y_train)","fd240dec":"# amazing results with straight correlation fit, i am unable to get the values greater than 0.5 correlation. \n# in this case the correlation you see is greater than 0.4 for some features. \n\ncorrelation_matrix = X_train.corr(method= 'pearson').abs()\n#print(correlation_matrix)","c8dbbe71":"upper_corr_matrix = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))","1381a618":"#print(upper_corr_matrix)\nplt.figure(figsize=(16,5))\nsns.heatmap(data = upper_corr_matrix , cmap= 'YlGnBu', annot= True)","963c607f":"# filter the columns which have greater than 0.5 correlation !\n\nto_filter = [column for column in upper_corr_matrix.columns if any (upper_corr_matrix[column] > 0.70)]","6a34cbf5":"to_filter","9ad2cc4a":"# new reduced x input. \nX_new = df_4[to_filter]","8a781b51":"from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score, max_error,median_absolute_error, mean_squared_log_error\n","f8d5dba4":"# function to evaulate performance of the regressor. \n\ndef evaulation(model, y_pred, y_true):\n    \n    '''\n    Input:- model = string (Name of the regressor)\n    y_pred= model prediction\n    y_true = actual labels. \n    \n    Output:\n    Dataframe with evaulation matrix. \n    \n    '''\n    \n    # create data output frame for the evaluation. \n    data = [explained_variance_score(y_true,y_pred), \n            max_error(y_true,y_pred),\n            mean_squared_error(y_true,y_pred),\n            mean_absolute_error(y_true,y_pred),\n            r2_score(y_true,y_pred, multioutput='uniform_average'),\n            median_absolute_error(y_true,y_pred)           \n            ]\n    row_index = ['Exp_Var_Score', 'Max_Error','MSE','MAE','R2_Score', 'Median_Abs_Error']\n    \n    df = pd.DataFrame(data, columns= [model], index= row_index)\n    \n    return df","f1470798":"# Step1 Train test split\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state = 0)\n\n# Step2 Initiate linear regressor\nlr = LinearRegression()\n\n# step3 fit the data\nlr.fit(X_train, y_train)\n\n# predict the test data\ny_pred_lr = lr.predict(X_test)\n\n\n#evaulation of the lr   \n\nprint('Intercept:', lr.intercept_)\nprint('Coefficients:', lr.coef_)\n\n","913e338c":"# Linear regressor evaulation parameters\ndf_linear = evaulation('linear', y_pred_lr, y_test)\ndf_linear","45fa3f70":"# Ridge regressor\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.linear_model import Ridge\nridge = Ridge()\n\nparams = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5]}\n\nridge_reg = GridSearchCV(ridge, params, scoring = 'neg_mean_squared_error', cv =5)\n\nridge_reg.fit(X_train, y_train)\nridge_alpha = ridge_reg.best_params_\n\nprint(ridge_alpha['alpha'])\nprint(ridge_reg.best_score_)\n\n\n#Output:- \n#0.01 --> Alpha\n#-0.06259961947798728 --> Best_score\n    ","8e7ec198":"# Displays various tests scores for each alpha value. Refer Rank_test_score to find out alpha = 0.01 is the best answer. \nridge_reg.cv_results_","90c446d4":"# now we got the optimum alpha value. Next step is to perform Ridge regression. \nridge_reg_model= Ridge(alpha= 0.01)\n\nridge_reg_model.fit(X_train, y_train)\n\ny_pred_ridge = ridge_reg.predict(X_test)\n\n\n#evaulation of the Ridge  \nprint('Intercept:', ridge_reg_model.intercept_)\nprint('Coefficients:', ridge_reg_model.coef_)\n\n","07a6129f":"# Linear regressor evaulation parameters\ndf_ridge = evaulation('ridge', y_pred_ridge, y_test)\ndf_ridge","8469e855":"from sklearn.metrics import r2_score\n# default parameters before running gridsearch. \nsvr = SVR(C=1, epsilon=0.2, kernel='rbf', gamma= 'scale', tol = 1e-6)\n\n\n\npipe = Pipeline( steps = [('Standardscaler', StandardScaler()), \n                          ('SVR', svr)])\n\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n\n\n\n#evaulation of the Ridge  \nsvr_reg = np.mean((y_pred - y_test)**2)\nsvr_reg_max = np.max((y_pred - y_test)**2)\nsvr_reg_min = np.min((y_pred - y_test)**2)\n\nprint('MSE:', svr_reg)\nprint('MSE Max:', svr_reg_max)\nprint('MSE Min:', svr_reg_min)\nprint('R2_score:', r2_score(y_test, y_pred))\n\n# intercept and coefficients are available only for kernel = 'linear'\n","ee1312eb":"from sklearn.metrics import r2_score\n\n\n# after performing gridsearch, following parameters yielded optiminum results. \n\nsvr = SVR(C=80, epsilon=0.005, kernel='rbf', gamma=3, tol = .001, verbose = 0)\n\n\n\npipe = Pipeline( steps = [('Standardscaler', StandardScaler()), \n                          ('SVR', svr)])\n\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n# intercept and coefficients are available only for kernel = 'linear'\n","273512bf":"df_svr = evaulation('SVR', y_pred, y_test)\ndf_svr","e53de911":"# the cell is commented, if you need to optimize further uncomment and modify the C, epsilon and gamma parameters. \n# the operation will take several hours to run. \n\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.svm import SVR\n# from sklearn.metrics import r2_score\n\n\n\n# gsc = GridSearchCV(\n#         estimator=SVR(kernel='rbf'),\n#         param_grid={\n#             'C': [80, 100, 120],\n#             'epsilon': [0.001, 0.005],\n#             'gamma': [3, 4, 5]\n#         },\n#         cv=20, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n\n\n# grid_result = gsc.fit(X_test, y_test)\n\n# best_parms = grid_result.best_params_\n\n","6fe803f1":"# best_svr = SVR(kernel='rbf', C=best_parms[\"C\"], epsilon=best_parms[\"epsilon\"], gamma=best_parms[\"gamma\"],\n#                coef0=0.1, shrinking=True, tol=0.001, cache_size=200, \n#                from sklearn.metrics import r2_score\n\n\n# print(best_svr)\n\n# output:\n#     SVR(C=80, coef0=0.1, epsilon=0.005, gamma=3)","1d1feab0":"# from sklearn.metrics import r2_score\n\n# kernel = ['rbf', 'poly', 'sigmoid', 'precomputed']\n\n# svr = SVR(C=1, epsilon=0.2, kernel='poly', degree = 5)\n\n# pipe = Pipeline( steps = [('Standardscaler', StandardScaler()), \n#                           ('SVR', svr)])\n\n# pipe.fit(X_train, y_train)\n\n# y_pred = pipe.predict(X_test)\n\n\n\n# #evaulation of the Ridge  \n# svr_reg = np.mean((y_pred - y_test)**2)\n# svr_reg_max = np.max((y_pred - y_test)**2)\n# svr_reg_min = np.min((y_pred - y_test)**2)\n\n# print('MSE:', svr_reg)\n# print('MSE Max:', svr_reg_max)\n# print('MSE Min:', svr_reg_min)\n# print('R2_score:', r2_score(y_test, y_pred))\n\n# # intercept and coefficients are available only for kernel = 'linear'\n\n\n# output:- \n# MSE: 0.03343396604718888\n# MSE Max: 44.990648440689434\n# MSE Min: 6.1491123972214586e-09\n# R2_score: 0.9667921373579083\n","1bdaec7c":"# Lasso Reg\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n\n# Grid search analysis was done, 0.01 was the best alpha\n#params = {'alpha': [.01]}\n\nlasso = Lasso(alpha = .01)\n\nlasso.fit(X_test, y_test)\ny_pred_lasso = lasso.predict(X_test)\n#pipe = Pipeline( steps = [('Standardscaler', StandardScaler()),('Lasso', lasso)])\n\n#pipe.fit(X_train, y_train)\n\n#y_pred_lasso = pipe.predict(X_test)\n\nprint('Coefficients',lasso.coef_)\nprint('Intercepts', lasso.intercept_)\n#print('feature_name', X_train.columns)","458e42a2":"df_lasso = evaulation('lasso', y_pred_lasso, y_test)\ndf_lasso","7ab69f4f":"from sklearn.linear_model import ElasticNetCV, ElasticNet\nelastic = ElasticNet(alpha = 0.01, l1_ratio = 0.5) # parameters were selected based on grid search \n\n\nelastic_score = elastic.fit(X_train, y_train)\n\ny_pred_elastic = elastic_score.predict(X_test)\n\n\n# evaulation\nprint('Intercept', elastic.intercept_)\nprint('Coefficients', elastic.coef_)\n","adddc7ca":"df_Elastic = evaulation('ElasticNet', y_pred_elastic, y_test)\ndf_Elastic","279558d1":"df_summary= pd.concat([df_Elastic, df_lasso, df_svr, df_ridge, df_linear], axis=1, sort=False)\ndf_summary","cfcbb3cd":"import tensorflow as tf\nfrom tensorflow import keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense","69d4f91c":"# split the dataset into train, test, and validation sets\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X_new, y, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n\n\n# scale the datasets\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(X_test)\n","1d9b79f1":"def build_model():\n    model = keras.models.Sequential([\n        keras.layers.Dense(64, activation=\"relu\", input_shape=X_train.shape[1:]),\n        keras.layers.Dense(64, activation=\"relu\"),\n        keras.layers.Dense(1, activation = 'linear')\n        ])\n    optimizer = tf.keras.optimizers.Adam(lr = 1e-5)\n    \n    model.compile(loss= 'mean_squared_error', optimizer= optimizer, metrics = ['mse'])\n    \n    return model\n","222845b1":"model = build_model()","25b4a97e":"model.summary()","8e05d374":"EPOCHS = 100\nhistory = model.fit(X_train, y_train, batch_size= 50, epochs= EPOCHS, verbose=0)","5052abae":"history_vald= model.fit(X_valid, y_valid, batch_size=32, epochs=EPOCHS, verbose = 0)","38b9aee3":"plt.plot(pd.DataFrame(history.history))\nplt.plot(pd.DataFrame(history_vald.history))\nplt.grid(True)\nplt.gca().set_ylim(0,0.5)\nplt.show()","65422b27":"y_pred_ANN = model.predict(X_test, batch_size=32, verbose= 0)","97b2997b":"#y_pred_ANN\n#y_test.values\nmse_ANN = np.mean((y_pred_ANN - y_test.values)**2)\nprint(mse_ANN)","1c735a29":"((y_pred_ANN - y_test.values)**2).max()","5fe52e86":"(y_pred_ANN - y_test.values).min()","05ce4036":"y_pred_ANN","8d33c5ac":"#plt.plot(y_test.values)\n#plt.plot(y_pred_ANN)\nplt.grid(True)\nplt.scatter(y_test.values, y_pred_ANN)","6ecbcd38":"## Data Exploration and cleaning","d3dec4b7":"## SVR ","e6dcbf10":"## Regression analysis for profile id = 4\n## Steps\n\n1. Feature selection \n2. Regression analysis\n    2.1 LR\n    2.2 Ridge\n    2.3 Lasso\n    2.4 ElasticNet\n    2.5 ANN\n3. Comparision matrix. \n\nObjective of analysis:- \n1. What are the features that are important for regression analysis \n2. R2 score and MSE is used for selecting the model. \n3. Upon selection of model. Similar analysis can be carried out for other profile id's and torque\n\n","e355a0e7":"> ## Analysis Objective","bff6fccf":"## ElasticNet","17830dd7":"## Visualisation","54be8f83":"## Conclusions","084f2c17":"Output from above cells. \n\nC = 100, epsilon = .001, gamma = 3. ","3b67c1b1":"Reference\nhttps:\/\/medium.com\/datadriveninvestor\/an-introduction-to-grid-search-ff57adcc0998","529cd31a":"## Lasso Regressor","5f71aa8f":"## Linear Regression\n","3a560549":"# Context\nThe dataset comprises several sensor data collected from a permanent magnet synchronous motor (PMSM) deployed on a test bench. The PMSM represents a german OEM's prototype model. Test bench measurements were collected by the LEA department at Paderborn University. This dataset is mildly anonymized.\nContent\nAll recordings are sampled at 2 Hz. The dataset consists of multiple measurement sessions, which can be distinguished from each other by column \"profile_id\". A measurement session can be between one and six hours long.\nThe motor is excited by hand-designed driving cycles denoting a reference motor speed and a reference torque. Currents in d\/q-coordinates (columns \"i_d\" and i_q\") and voltages in d\/q-coordinates (columns \"u_d\" and \"u_q\") are a result of a standard control strategy trying to follow the reference speed and torque. Columns \"motor_speed\" and \"torque\" are the resulting quantities achieved by that strategy, derived from set currents and voltages.\nMost driving cycles denote random walks in the speed-torque-plane in order to imitate real world driving cycles to a more accurate degree than constant excitations and ramp-ups and -downs would.\nAcknowledgements\nSeveral publications leveraged the setup of the PMSM in the Paderborn University Lab:\n\n\nInspiration\nThe most interesting target features are rotor temperature (\"pm\"), stator temperatures (\"stator_*\") and torque. Especially rotor temperature and torque are not reliably and economically measurable in a commercial vehicle.\nBeing able to have strong estimators for the rotor temperature helps the automotive industry to manufacture motors with less material and enables control strategies to utilize the motor to its maximum capability. A precise torque estimate leads to more accurate and adequate control of the motor, reducing power losses and eventually heat build-up.\n\n(https:\/\/www.kaggle.com\/wkirgsn\/electric-motor-temperature)","fca04fd1":"## ANN","4cbc3209":"Objective of analysis:- \n1. What are the features that are important to predict 'rotor temperature (pm)'? \n2. Evaulate various regressor models and filter them  based on R2 score and MSE. Select the model \n3. Upon selection of model. Similar analysis can be carried out for other profile id's and torque","c1805732":"## Ridge Regression","a42cc591":"1. SVR analysis yields better result in most of the evaulation parameters except the max error (could be due to outlier).\nAll other parameters are at optmimum level. Hence SVR model is selected for further analysis. \n\n2. ANN did not yield great result. the MSE was higher than SVR model. \n\n2. Ensamble methods of regression analysis were not used mainly due to 'risk of overfitting the data'. Since most regression \nyielded good R2 score, the next level of analysis looking to ensamble methods were not considered. \n\nFurther analysis. \n1. Perform similar analysis for 'torque'. \n2. Run classification model to identify the 'profile id'"}}