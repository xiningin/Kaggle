{"cell_type":{"acfbe01d":"code","fef981c9":"code","c2f38d50":"code","0fd17118":"code","d04e2aec":"code","ca5fc134":"code","ce6d98bc":"code","4403ce43":"code","34d17245":"code","2e304442":"code","39c264e8":"code","12b4bb11":"code","632f8d71":"code","ab59bb4d":"code","38d44a27":"code","25b76328":"code","2172d1ee":"code","3bcf75f9":"code","c6444c8a":"code","558be5d4":"code","07f8af03":"markdown","4aabca41":"markdown","9473f130":"markdown","a779f78d":"markdown","d5122ac1":"markdown","97c80812":"markdown","aead04cd":"markdown","5d4f5b10":"markdown"},"source":{"acfbe01d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport matplotlib.pyplot as plt # drawing and visualizing data\nimport seaborn as sns # nicer plotting above matplotlib\nimport nltk # NLP library\nfrom nltk.tokenize import word_tokenize\nfrom tqdm.autonotebook import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fef981c9":"conversations = pd.read_csv(\n    \"\/kaggle\/input\/movie-dialog-corpus\/movie_conversations.tsv\", \n    sep='\\t', \n    encoding='ISO-8859-2',\n    names = ['charID_1', 'charID_2', 'movieID', 'conversation']\n)\n\n\nlines = pd.read_csv(\n    \"\/kaggle\/input\/movie-dialog-corpus\/movie_lines.tsv\", \n    encoding='utf-8-sig', \n    sep='\\t', \n    error_bad_lines=False, \n    header = None,\n    names = ['lineID', 'charID', 'movieID', 'charName', 'text'],\n    index_col=['lineID']\n)\n\ncharacters = pd.read_csv(\n    \"\/kaggle\/input\/movie-dialog-corpus\/movie_characters_metadata.tsv\", \n    sep='\\t', \n    header = None,\n    error_bad_lines=False,\n    names = ['charID','charName','movieID','movieName','gender','score'],\n    index_col=['charID']\n)\n\ntitles = pd.read_csv(\n    \"\/kaggle\/input\/movie-dialog-corpus\/movie_titles_metadata.tsv\",\n    sep='\\t',\n    header=None,\n    error_bad_lines=False,\n    names=['movieID', 'title', 'year', 'ratingIMDB', 'votes', 'genresIMDB'],\n    index_col=['movieID']\n)","c2f38d50":"conversations","0fd17118":"lines","d04e2aec":"characters","ca5fc134":"titles","ce6d98bc":"conversations['conversation'] = conversations['conversation'].map(lambda x: re.findall(r\"\\w+\", x))\n\n# I could not find another way to filter based on the length of the conversation other than creating another column\nconversations['length'] = conversations['conversation'].apply(lambda x: len(x))\n\nconversations.sort_values(by=['length'], ascending=False)","4403ce43":"print(conversations.length.describe())\nfig, ax = plt.subplots(figsize=(15,5))\nsns.violinplot(ax=ax, x=conversations.length, inner=None)","34d17245":"# conversation here should be one row of the conversations df. \n# TODO: should probably change this to allow different arguments\ndef view_convo(conversation, characters, movies, lines):\n    charID_1, charID_2, movieID = conversation['charID_1'], conversation['charID_2'], conversation['movieID']\n    \n    char1 = characters.loc[charID_1].charName\n    char2 = characters.loc[charID_2].charName\n    movie = movies.loc[movieID].title\n    \n    convo_header = f\"This conversation was between {char1} and {char2}, from movie : {movie}.\"\n    print(convo_header)\n    print(f\"{'-' * len(convo_header)}\")\n\n    for lineID in conversation.conversation:\n        line = lines.loc[lineID]\n        print(f\"{line.charName} : {line.text}\")","2e304442":"view_convo(conversations.iloc[0], characters, titles, lines)","39c264e8":"# this is to identify all the lines that do not have text\nlines[\"type\"] = lines[\"text\"].apply(lambda x: type(x))\n\n# we create our malformed dictionary with the correct lineID as the key, and the value being the full string from which we will correctly\n# populate \nmalformed_index_dict = {line.split('\\t')[0]: line for line in lines[lines.type == type(float(1.))].index}\n\n# the malformed dict has both correctly parsed but empty lines and broken lines that are all stored in lineID. Lets remove the empty lines\nempty_lines = []\nfor k, v in malformed_index_dict.items():\n    if k == v:\n        empty_lines.append(k)\n\n# we remove the empty lines and now \nfor empty_k in empty_lines:\n    malformed_index_dict.pop(empty_k, None)\n    lines.loc[empty_k]['text'] = \" \"\n    lines.loc[empty_k]['type'] = type(\"a\")\n    \n# lets fix the indices first, then loop over again and update the values \nlines.rename(index={v: k for k, v in malformed_index_dict.items()}, inplace=True)\n\n##################################### \n\nfixed = []\n\n# first, lets go ahead and append the lines where the lineID contains all the data for a single line, and in a bad array, store the lineIDs \n# that contain the information for multiple lines\nbad = []\nfor idx, bad_idx in malformed_index_dict.items():\n    if len(bad_idx.split('\\t')[1:] + [type(str)]) > 5:\n        bad.append(idx)\n    else:\n        fixed.append(bad_idx.split('\\t') + [type(\"a\")])\n\n# now fix and append the lineIDs that contain all the data for multiple lines into fixed     \nfor val in bad:\n    for line in malformed_index_dict[val].split(\"\\n\"):\n        if len(line.split('\\t')[1:] + [type(\"a\")]) > 5:\n            fixed.append(line.split('\\t')[:4] + [\"\".join(line.split('\\t')[4:])] + [type(\"a\")])\n        else:\n            fixed.append(line.split('\\t') + [type(\"a\")])\n\n            \n# so now lets make a dataframe out of fixed, and update lines with it\ndf_fixed = pd.DataFrame(fixed, columns=['lineID', 'charID', 'movieID', 'charName', 'text', 'type'])\ndf_fixed.set_index(['lineID'], inplace=True)\n\nlines = pd.concat([df_fixed, lines])\nlines = lines.groupby(lines.index).first().drop(columns=['type'])\n\n# lines = pd.concat([df_fixed, lines]).drop_duplicates(subset=['lineID'])\n# https:\/\/stackoverflow.com\/questions\/63842185\/how-to-update-one-pandas-dataframe-with-another-dataframe-update-the-old-data-a","12b4bb11":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different processed titles together.\nlong_string = ','.join(list(lines.text.values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","632f8d71":"def process_line(line):\n    line = re.sub(r'[,!?;-]', '.', line)\n    return line.lower()\n    # line = word_tokenize(line)\n    # return line\n\n# total_words = [process_line(line) for line in tqdm(lines.text)]\n# print(len(total_words))","ab59bb4d":"lines['pText'] = lines['text'].apply(process_line)","38d44a27":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA","25b76328":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_data = count_vectorizer.fit_transform(lines['pText'])","2172d1ee":"lda = LDA(n_components=5)\nlda.fit(count_data)","3bcf75f9":"def print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, 10)","c6444c8a":"# each_word = sum(total_words, [])","558be5d4":"# freq = nltk.FreqDist(each_word)","07f8af03":"Do we care about the following?\n1. Getting rid of stop words? --- Probably not? \n2. Stemming our words? --- Probably not?\n3. Getting rid of rare words? --- Probably not?\n4. Lower casing everything? --- Yes\n5. Punctuation type? --- Probably not? Just make all the punctuation \".\" for now","4aabca41":"# Fixing some import issues with movie_lines.tsv\n1. Some lines have no text, but thats ok, we will treat them as a mute reply\n2. Some lines were parsed in such a way that they either did not get tab seperated and the whole row is actually stored in the lineID column\n3. To further complicate \\#2, we have some lineIDs that do not only contain the whole row info, but contain 100+ seperate lines.\n\nTo fix all three (#3 being an extension of \\#2) We will build a malformed dictionary, and loop through this dictionary, fixing the lines DataFrame.\n","9473f130":"We have an array of tokens per line. We can choose to append this to the DataFrame for easier access","a779f78d":"# Conversations\nWe need to modify the conversation column to be a proper list, to allow us to query these conversations much easier","d5122ac1":"Goal for this notebook is to pull out topics from the conversations in movies","97c80812":"# Building a vocabulary","aead04cd":"Obligatory word cloud using every single line","5d4f5b10":"We can see the longest conversation was an 89 line interaction between two characters, but what about the distribution of interactions between characters?"}}