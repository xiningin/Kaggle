{"cell_type":{"00086cfb":"code","2ba388ff":"code","8164b51f":"code","6398f149":"code","abd5383c":"code","dd7bd2ee":"code","073bd70a":"code","a29861b8":"code","774ac85d":"code","4c587f36":"code","28d0c188":"code","f0d9363e":"code","2bb404f3":"code","a5fd356c":"code","7c593f41":"code","e310e09d":"code","1eed5966":"code","6c566a54":"code","641ab316":"markdown","db4ef8db":"markdown","93df1fe5":"markdown","a56b4581":"markdown","9de9dce7":"markdown","c9a4be67":"markdown","ee3e936a":"markdown","fd7bb9c5":"markdown","b5c1a319":"markdown","ab1c4eea":"markdown","f6b27e40":"markdown"},"source":{"00086cfb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ba388ff":"import pandas as pd\nimport numpy as np\ntrain_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\",index_col=\"id\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",index_col=\"id\")\ndf = pd.concat([train_df, test_df])\ntrain_df.shape, test_df.shape, df.shape","8164b51f":"test_rows = test_df.shape[0]\ndfs = [train_df,test_df]\nfor df_ in dfs:\n    temp_df = {\"col\":[],\"count_unique\":[],\"unique\":[],\"null\":[]}\n    for col in df_.columns:\n        temp_df[\"col\"].append(col)\n        temp_df[\"count_unique\"].append(df_[col].nunique())\n        temp_df[\"unique\"].append(df_[col].unique())\n        temp_df[\"null\"].append(df_[col].isnull().sum())\n    display(pd.DataFrame(temp_df))","6398f149":"# look into excerpt data with null and notnul in license\nfrom termcolor import colored\ntemp_df = df[df.license.notnull()].head(1)\nfor col in temp_df.columns:\n    print(f\"{colored(col,'red')}: {temp_df[col].values[0]}\")\n\nprint()\n\ntemp_df = df[df.license.isnull()].head(1)\nfor col in temp_df.columns:\n    print(f\"{colored(col,'red')}: {temp_df[col].values[0]}\")","abd5383c":"from nltk.tokenize import sent_tokenize\ndf[\"noof_sentences\"] = df.excerpt.apply(lambda x: len(sent_tokenize(x)))\n\nfrom nltk.tokenize import word_tokenize\ndf[\"noof_words\"] = df.excerpt.apply(lambda x: len(word_tokenize(x)))\n\nfrom nltk.corpus import stopwords\ndef get_stopwords(text):\n    words = word_tokenize(text)\n    return len([word for word in words if word in set(stopwords.words(\"english\"))])\ndf[\"noof_stopwords\"] = df.excerpt.apply(get_stopwords)\n\ndf[\"noof_chars\"] = df.excerpt.apply(lambda x: len(x))\ndf[\"noof_space\"] = df.excerpt.apply(lambda x: x.count(\" \"))\ndf[\"islegal\"] = df.url_legal.apply(lambda x: False if pd.isnull(x) else True).astype(\"int\")\ndf[\"islicence\"] = df.license.apply(lambda x: False if pd.isnull(x) else True).astype(\"int\")","dd7bd2ee":"'''\nfrom nltk.corpus import wordnet\ndef get_wrongwords(text):\n    wrongwords = []\n    for i in \"? , . ! ( ) - ` ;\".split():\n        text = text.replace(i, \" \")\n    for word in word_tokenize(text.rstrip()):\n        if not wordnet.synsets(word):\n            if word not in set(stopwords.words(\"english\")):\n                if not word[0].isupper():\n                    if not word.isdigit():\n                        wrongwords.append(word)\n    return len(wrongwords)\ndf[\"noof_wrongwords\"] = df.excerpt.apply(get_wrongwords)\ndf.noof_wrongwords.mean()\n'''","073bd70a":"from itertools import chain\n#list(chain.from_iterable(df.noof_wrongwords.values))","a29861b8":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nnumeric_columns = df.select_dtypes(exclude=\"object\").columns\ndata = {\"col\":[],\"corr\":[]}\nfor col in numeric_columns:\n    data[\"col\"].append(col)\n    data[\"corr\"].append(abs(df[:-test_rows].target.corr(df[:-test_rows][col])))\npd.Series(data[\"corr\"],index=data[\"col\"]).sort_values().plot(kind=\"barh\",title=\"Correlation\")","774ac85d":"plt.figure(figsize=(10,30))\nfor i, col in enumerate(numeric_columns):\n    plt.subplot(len(numeric_columns),2,i*2+1)\n    sns.boxplot(x=df[:-test_rows][col])\n    plt.subplot(len(numeric_columns),2,i*2+2)\n    sns.lineplot(x=df[:-test_rows][col],y=df[:-test_rows].target)\nsns.despine()","4c587f36":"def outlair(x):\n    if x > upper_lim:\n        return None\n    elif x < lower_lim:\n        return None\n    else:\n        return x\nplt.figure(figsize=(10, 30))  \nfor i, col in enumerate(numeric_columns):\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3 - q1\n    upper_lim = q3 + 1.5 * iqr\n    lower_lim = q1 - 1.5 * iqr\n    indexes = df[:-test_rows].index\n    df.loc[indexes,col] = df[:-test_rows][col].apply(outlair)\n    \n    # drop outlier\n    df2 = df[:-test_rows].dropna(subset=[col],axis=0)\n    df = pd.concat([df2,df[-test_rows:]])\n    # visualise changes\n    plt.subplot(len(numeric_columns),2,i*2+1)\n    sns.boxplot(x=df[col]) \n    plt.subplot(len(numeric_columns),2,i*2+2)\n    sns.lineplot(x=df[col],y=df.target)\nprint(len(train_df)-len(df[:-test_rows]),\"rows dropped!\")\nsns.despine()","28d0c188":"df.isnull().sum(), df.shape, df2.shape","f0d9363e":"# for col in list(set(numeric_columns) - set(\"islegal islicence\".split())):\n#     mean_, std_ = df[col].mean(), df[col].std()\n#     df[col] = df[col].apply(lambda x: (x-mean_)\/std_)","2bb404f3":"# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom termcolor import colored\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=Warning)\n\ndef evaluate_models(X,y):\n    models = [\n        (\"LIR\",LinearRegression()),\n#         (\"LOR\",LogisticRegression()),\n        (\"RFR\",RandomForestRegressor(n_estimators=100)),\n        (\"XGB\",XGBRegressor(learning_rate=0.01,max_depth=4,eval_metric=\"error\",n_estimators=100,use_label_encoder=False)),\n        (\"SVR\",SVR(gamma=0.1)),\n        (\"GBR\",GradientBoostingRegressor(n_estimators=100)),\n        (\"DTR\",DecisionTreeRegressor(max_leaf_nodes=500)),\n#         (\"LDA\",LinearDiscriminantAnalysis()),\n        (\"KNR\",KNeighborsRegressor(leaf_size=13,n_neighbors=29,p=1)),\n#         (\"GNB\",GaussianNB(var_smoothing=0.0533669923120631)),\n        (\"ETR\",ExtraTreesRegressor(max_depth=4,n_estimators=100)),\n        (\"BGR\",BaggingRegressor(max_samples=0.2,n_estimators=100)),\n    ]\n    for name,model in models:\n        results,times =[],5\n        for i in range(times):\n            train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=0.2,train_size=0.8,random_state=666)\n            model.fit(train_X,train_y)\n            pred = model.predict(test_X) #.round(decimals=0)\n            results.append(np.sqrt(mean_squared_error(pred,test_y)))\n        print(colored(f\"{name} Model accuracy:\",\"blue\"),sum(results)\/times)\n        #print(colored(\"Results:\",\"red\"),results)","a5fd356c":"X.head(1)","7c593f41":"numeric_columns = df.select_dtypes(exclude=\"object\").columns.drop(\"target standard_error\".split())\n#numeric_columns= \"noof_sentences noof_chars noof_wrongwords\".split()\nX = df[:-test_rows][numeric_columns]\ny = df[:-test_rows][\"standard_error\"]\n\nevaluate_models(X,y)","e310e09d":"#model = LinearRegression()\nmodel = RandomForestRegressor(n_estimators=100,random_state=0)\nmodel.fit(X,y)\npred = model.predict(df[-test_rows:][numeric_columns])\nindexes = df[-test_rows:].index\ndf.loc[indexes,\"standard_error\"] = pred","1eed5966":"numeric_columns = df.select_dtypes(exclude=\"object\").columns.drop(\"target\".split())\nX = df[:-test_rows][numeric_columns]\ny = df[:-test_rows][\"target\"]\nevaluate_models(X,y)","6c566a54":"#model = LinearRegression()\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel = BaggingRegressor(max_samples=0.2,n_estimators=100)\nmodel.fit(X,y)\npred = model.predict(df[-test_rows:][numeric_columns])\noutput = pd.DataFrame({\"id\":df[-test_rows:].index,\"target\":pred})\noutput.to_csv(\"submission.csv\", index=False)\npd.read_csv(\".\/submission.csv\").transpose()","641ab316":"# Features scaling     X = (Xi - Xmean)\/std","db4ef8db":"# Evaluate models (Predict standard_error)","93df1fe5":"# Transform and create new Features","a56b4581":"# Fill standard_error into test dataset","9de9dce7":"**Incorroect words check count**","c9a4be67":"# Replace outlier","ee3e936a":"# Data visualisation","fd7bb9c5":"# Load and read dataset","b5c1a319":"> Replace outlier tested not eorks and Drop outlier do improve model little","ab1c4eea":"# Evaluate models (Predict target)","f6b27e40":"**Incorrext grammer check count**"}}