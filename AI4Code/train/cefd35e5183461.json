{"cell_type":{"992502ff":"code","e2eef2c3":"code","5621f060":"code","af8f007c":"code","012972e8":"code","380fe7f3":"code","55a5c161":"code","1a915050":"code","ab6bda82":"code","82028a6c":"code","b739be52":"code","5906ecd8":"code","2fc9dcc9":"code","74063012":"code","e0c58291":"code","eb124efb":"code","bb26b2e7":"code","b517de10":"code","8c6e06ba":"code","e546e39a":"code","9599b2d7":"code","9d3f7467":"code","8ab6e164":"code","63b1d7f9":"code","dce8556e":"code","9a99b1fc":"code","77c549b2":"code","64df6cb4":"code","c114f00a":"code","63d08ed1":"code","ab3ffc65":"code","0c0301b4":"code","90fb9491":"code","f69c89bc":"code","33e82b99":"code","13b92249":"code","73ff56f7":"code","046ec533":"code","0e432e40":"code","95ebb2d3":"code","5cb8449e":"code","0a43a9c3":"code","45b5bad6":"code","adf08718":"code","d1310aaf":"markdown","0d2c1797":"markdown","50073dbb":"markdown","16d4abc9":"markdown","03b870a1":"markdown","b76cbe03":"markdown","dd18a59f":"markdown","7301dc5f":"markdown","3464e6fa":"markdown","5ffee76e":"markdown","06683edb":"markdown","3d925b1d":"markdown","8b10135c":"markdown","22155766":"markdown","8d092786":"markdown","ab5510df":"markdown"},"source":{"992502ff":"import numpy as np \nimport pandas as pd \nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport eli5\n\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_columns',100)\npd.set_option('max_rows',100)","e2eef2c3":"# Thanks to : https:\/\/www.kaggle.com\/aantonova\/some-new-risk-and-clusters-features\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5621f060":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain = reduce_mem_usage(train)","af8f007c":"train.head(3)","012972e8":"train.info()","380fe7f3":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest = reduce_mem_usage(test)","55a5c161":"test.head(3)","1a915050":"test.info()","ab6bda82":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub.head(3)","82028a6c":"train.describe()","b739be52":"test.describe()","5906ecd8":"#pp.ProfileReport(train)","2fc9dcc9":"#pp.ProfileReport(test)","74063012":"numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\n\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","e0c58291":"target = train['SalePrice']\ndel train['SalePrice']","eb124efb":"train = reduce_mem_usage(train)","bb26b2e7":"train.info()","b517de10":"train = train.fillna(-1)\ntest = test.fillna(-1)","8c6e06ba":"X = train\nz = target","e546e39a":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","9599b2d7":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 45,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=5000,\n                   early_stopping_rounds=50,verbose_eval=50, valid_sets=valid_set)","9d3f7467":"pred_lgb = modelL.predict(test)","8ab6e164":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","63b1d7f9":"feature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","dce8556e":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","9a99b1fc":"parms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","77c549b2":"pred_xgb = modelx.predict(xgb.DMatrix(test))","64df6cb4":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","c114f00a":"feature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","63d08ed1":"# Standardization for regression models\ntrain2 = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns,\n    index=train.index\n)","ab3ffc65":"test2 = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(test),\n    columns=test.columns,\n    index=test.index\n)","0c0301b4":"# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train2, target)\ncoeff_linreg = pd.DataFrame(train.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","90fb9491":"pred_lin = linreg.predict(test2)","f69c89bc":"len(coeff_linreg)","33e82b99":"# Eli5 visualization\neli5.show_weights(linreg)","13b92249":"# the level of importance of features is not associated with the sign\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\n\n# add result\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","73ff56f7":"#Thanks to https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 7))","046ec533":"feature_score.sort_values('mean', ascending=False)","0e432e40":"# Create total column with different weights\nweight = [0.55, 0.35]\nweight.append(1-weight[0]-weight[1])\nprint(weight)\n\nfeature_score['total'] = weight[0]*feature_score['score_lgb'] + weight[1]*feature_score['score_xgb'] + weight[2]*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score_sort = feature_score.sort_values('total', ascending=False)\nfeature_score_sort[:29].plot(kind='bar', figsize=(20, 10))","95ebb2d3":"feature_score.sort_values('total', ascending=False)","5cb8449e":"feature_score_sort = feature_score.sort_values('total', ascending=False)\nfeature_score_sort['features'] = feature_score_sort.index\nfeature_score_sort.columns = ['score_lgb', 'score_xgb', 'score_linreg', 'mean', 'total', 'feature']\nfeature_score_sort = feature_score_sort[['feature', 'score_lgb', 'score_xgb', 'score_linreg', 'mean', 'total']]\nfeature_score_sort","0a43a9c3":"feature_score_sort.to_csv('feature_score_sort.csv', index=False)","45b5bad6":"sub['SalePrice'] = pred_lgb\nsub.to_csv('submission.csv', index=False )","adf08718":"sns.set()\nplt.hist(sub['SalePrice'],bins=25)\nplt.show()","d1310aaf":"Your comments and feedback are most welcome.","0d2c1797":"## 5. Tuning models and building the feature importance diagrams<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","50073dbb":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","16d4abc9":"## 3. EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","03b870a1":"I hope you find this kernel useful and enjoyable.","b76cbe03":"### 6. Comparison of the all feature importance diagrams <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","dd18a59f":"[Go to Top](#0)","7301dc5f":"### 5.3 Linear Regression <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","3464e6fa":"# [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)","5ffee76e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [FE & EDA](#3)\n1. [Preparing to modeling](#4)\n1. [Tuning models and building the feature importance diagrams](#5)\n    -  [LGBM](#5.1)\n    -  [XGB](#5.2)\n    -  [Linear Regression](#5.3)\n1. [Comparison of the all feature importance diagrams](#6)\n1. [Submission](#7)","06683edb":"### 5.2 XGB<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3d925b1d":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","8b10135c":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# Universal EDA & FE on basic kernels:\n* [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)\n* [Feature importance - xgb, lgbm, logreg, linreg](https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg)","22155766":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","8d092786":"### 7. Submission <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","ab5510df":"### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}