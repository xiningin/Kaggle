{"cell_type":{"e5eac90e":"code","5a981a89":"code","6e3ec65a":"code","11878bdf":"code","a4477f62":"code","10cb498a":"code","63909d7f":"code","104602ec":"code","166b52a3":"code","1f8acec8":"code","3218135d":"code","71b98a33":"code","1084c56f":"code","b99a95cf":"markdown","07f44b0d":"markdown","8a5d3fef":"markdown","70acf180":"markdown","e9dcc537":"markdown","ec25173b":"markdown","57e4c7ed":"markdown","01c37d63":"markdown","b43df6a7":"markdown","f9ecaa2c":"markdown"},"source":{"e5eac90e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a981a89":"import matplotlib.pyplot as plt\n\ndata = '..\/input\/cryptocurrencypricehistory\/coin_Bitcoin.csv'\ndataset = pd.read_csv(data)\nchosen_col = 'Close'","6e3ec65a":"print(len(dataset))\ndataset.head()","11878bdf":"split_row = len(dataset) - int(0.2 * len(dataset))\ntrain_data = dataset.iloc[:split_row]\ntest_data = dataset.iloc[split_row:]","a4477f62":"fig, ax = plt.subplots(1, figsize=(13, 7))\nax.plot(train_data[chosen_col], label='Train', linewidth=2)\nax.plot(test_data[chosen_col], label='Test', linewidth=2)\nax.set_ylabel('Price USD', fontsize=14)\nax.set_title('', fontsize=16)\nax.legend(loc='best', fontsize=16)","10cb498a":"from sklearn.preprocessing import MinMaxScaler\n\ndatacol = dataset.iloc[:, 7:8].values\ntraincol = train_data.iloc[:, 7:8].values\ntestcol = test_data.iloc[:, 7:8].values\n\nshaped_data = np.reshape(datacol, (-1,1))\ntrain_shaped = np.reshape(traincol, (-1,1))\ntest_shaped = np.reshape(testcol, (-1,1))\n\nsc = MinMaxScaler(feature_range=(0,1))\nsc.fit(shaped_data)\n\ntrain_scaled_data = sc.transform(train_shaped)\ntest_scaled_data = sc.transform(test_shaped)","63909d7f":"X = []\ny = []\ntime_steps = 50\n\nfor i in range(time_steps, len(train_data)):\n    X.append(train_scaled_data[i-time_steps:i,0])\n    y.append(train_scaled_data[i, 0])\n\nX, y = np.array(X), np.array(y)\nX = np.reshape(X, (X.shape[0], X.shape[1], 1))","104602ec":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","166b52a3":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Activation\n\nmodel = Sequential()\nmodel.add(LSTM(units=100, input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation('linear'))\nmodel.compile(optimizer='adam', loss='mse')","1f8acec8":"from keras.callbacks import EarlyStopping\n\ncallback = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1, callbacks=[callback], shuffle=True)","3218135d":"model.summary()","71b98a33":"from keras.metrics import mean_absolute_error\n\nX_testing = []\ny_testing = []\n\nfor i in range(time_steps, len(test_data)):\n    X_testing.append(test_scaled_data[i-time_steps:i,0])\n    y_testing.append(test_scaled_data[i, 0])\n\nX_testing, y_testing = np.array(X_testing), np.array(y_testing)\nX_testing = np.reshape(X_testing, (X_testing.shape[0], X_testing.shape[1], 1))\n\npredicted_price = model.predict(X_testing)\ninv_price = sc.inverse_transform(predicted_price)\nreal_price = np.reshape(y_testing, (-1,1))\nreal_price = sc.inverse_transform(real_price)","1084c56f":"fig, ax = plt.subplots(1, figsize=(13, 7))\nax.plot(real_price, label='Real', linewidth=2)\nax.plot(inv_price, label='Pred', linewidth=2)\nax.set_ylabel('Price USD', fontsize=14)\nax.set_title('', fontsize=16)\nax.legend(loc='best', fontsize=16)","b99a95cf":"Now we implement our actual model. We start with an LSTM input layer with 100 hidden units. We add a dropout of 0.2 before our Dense output layer with a linear activation and a shape of 1 (as we are outputting our expected price). We are using mean squared error to calculate our loss and adam as our optimizer.","07f44b0d":"Lets begin by importing our bit coin price data.\n\nchosen_col variable can be changed here to alter what recorded price to use.","8a5d3fef":"And then taking a look at the first few columns. ","70acf180":"Fit our model now on our X and y training\/validation data. We take advantage of keras' early stopping class so that once we are no longer recieving improvements the model will take its best weights and stop.","e9dcc537":"Now we seperate our training data into our inputs and our outputs in time steps of time_steps. Where we will look at time_steps amount of data before we make our prediction of what the output for y will be.","ec25173b":"Now we are breaking our testing data up into time steps and splitting it again into our inputs and our expected outputs. \n\nWe then predict on the inputs and then scale both the inputs and outputs back up, now were ready to see how we did!","57e4c7ed":"Splitting our data into what will be our training set VS the test set so we can visualize it.","01c37d63":"Splitting our training data into training and validation.","b43df6a7":"We can see that there are some pretty wild fluctuations at the end, lets leave this as it is for the time being and see what happens.\n\nContinue by normalizing the values to floats between 0-1. We are using sklearn's MinMaxScaler. We need to be careful to fit the scaler on our entire data range or else we will end up with a messed up scale between our test data and our train data. After we fit it we then transform both our test and training data.","f9ecaa2c":"It looks ok! Considering this was a very large spike![image.png](attachment:c4a3c754-0ea3-4289-95af-8ed9a3e8eff5.png)\n\nThe orange in the orignal graph is the testing data we are using here so you can see that the price spiked from 10k USD all the way to 60k and back down to 35k.\n\nThe largest problem I see with the predictions is that on the larger changes it lags a bit behind the actual prices, while far from an expert, I assume this comes from the model still being innacurate when it comes to such violent volatility like was shown in the data."}}