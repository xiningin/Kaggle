{"cell_type":{"ec3e5f6d":"code","44f961aa":"code","c4bb2a8a":"code","36d2cdf3":"code","d4e85f35":"code","208ef90b":"code","a4ea6a4e":"code","32f2eb7d":"code","72d5ace4":"code","7cbc2fe0":"code","19f499e9":"code","ea3c889d":"code","958e3272":"code","147c16ea":"code","5fe86f94":"code","19952a00":"code","dc263a16":"code","c4eab322":"code","3ae6b0a5":"code","1e3a4131":"code","7744c794":"code","8b965770":"code","70939a5e":"code","81ff239d":"code","711a2ec7":"code","564579f8":"code","1e6e1fd7":"code","e24c4cf4":"code","a8e56151":"code","341efefd":"code","f300d5f0":"code","1a65ab84":"code","6c311ce7":"code","60a1da30":"code","fbcb324d":"code","f1e8e14a":"code","a11ccfca":"code","ec75f96d":"code","fa316fd8":"markdown","5ba6db73":"markdown","f6446b0e":"markdown","ffaf0798":"markdown","e91c8086":"markdown","65a06429":"markdown","b51f7c23":"markdown","c3d12d79":"markdown","9d29e16a":"markdown","d07c2d5a":"markdown","a16f233b":"markdown","2d3ded1f":"markdown","5ee41b38":"markdown","18e76f1b":"markdown","c142099e":"markdown","f13a61c4":"markdown","d1d6e86a":"markdown","fecb71e8":"markdown","2a9c1dcb":"markdown","26a5bebc":"markdown","2f2b7d5d":"markdown","8bf4b608":"markdown","b37432f4":"markdown","b216d969":"markdown","1e6deebd":"markdown","3776f8bf":"markdown","1ab7360a":"markdown","e026dbfe":"markdown","0bc10de9":"markdown","2e8d84ee":"markdown","bbd48dbb":"markdown","20fc29a6":"markdown","61bc1487":"markdown","56662d5a":"markdown"},"source":{"ec3e5f6d":"import os\nimport re\nimport gc\nimport sys\nimport math\nimport glob\nimport json\nimport spacy\nimport pandas\nimport random\nimport warnings\nimport numpy as np\nimport cufflinks as cf\n\nfrom spacy import displacy\nfrom tqdm.notebook import tqdm\nfrom collections import Counter \nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom IPython.core.display import HTML, Markdown\n\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# defining some options for displacy\ncolors = {\"KP\": \"#2fbbab\"}\noptions = {\"ents\": [\"KP\"], \"colors\": colors}\n\n# defining some options for pandas\npandas.set_option('display.max_rows', 5)\npandas.set_option('display.max_columns', None)\n\n# defining some options for cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","44f961aa":"v7_path = '\/kaggle\/input\/CORD-19-research-challenge'\ndf = pandas.read_csv(f'{v7_path}\/metadata.csv', \n    usecols=['cord_uid', 'sha', 'pmcid', 'title', 'abstract', 'publish_time', 'authors'],\n    dtype={'cord_uid' : str, 'sha': str, 'pmcid': str, 'title': str, 'abstract': str, 'authors': str},\n    parse_dates=['publish_time'], \n    keep_default_na=False\n)","c4bb2a8a":"def code_we_used_on_our_server() :\n    pdf_json_dict = dict()\n    for filename in tqdm(glob.glob(f'{v7_path}\/**\/pdf_json\/*.json', recursive=True), leave = False): \n        pdf_json_dict[json.load(open(filename, 'rb'))['paper_id']] = filename\n    print(len(pdf_json_dict), \"papers from PDF parsing\")\n\n    xml_json_dict = dict()\n    for filename in tqdm(glob.glob(f'{v7_path}\/**\/pmc_json\/*.json', recursive=True), leave = False): \n        xml_json_dict[json.load(open(filename, 'rb'))['paper_id']] = filename\n    print(len(xml_json_dict), \"papers from XML parsing\")","36d2cdf3":"def code_we_used_on_our_server() :\n    tqdm.pandas()\n    df[\"body_text\"] = df.apply(lambda x: [], axis=1)\n    df[\"sha\"] = df[\"sha\"] = df[\"sha\"].apply(lambda x: x.split(\"; \")[0])\n    for index, meta_data in tqdm(df.iterrows(), total = len(df.index)) :\n        if meta_data[\"sha\"] != \"\" and meta_data[\"sha\"] in pdf_json_dict :\n            file = json.load(open(pdf_json_dict[meta_data[\"sha\"]], 'rb'))\n            if(file['body_text'] != []) :\n                df.at[(df[df['sha'] == meta_data[\"sha\"]].index)[0], 'body_text'] = file['body_text']\n\n        if meta_data[\"pmcid\"] != \"\" and meta_data[\"pmcid\"] in xml_json_dict :\n            file = json.load(open(xml_json_dict[meta_data[\"pmcid\"]], 'rb'))\n            if(file['body_text'] != []) :\n                df.at[(df[df['pmcid'] == meta_data[\"pmcid\"]].index)[0], 'body_text'] = file['body_text']  ","d4e85f35":"print(\"For the\",len(df), \"papers in the dataset (v7)\")\nprint(\"-\", len(df[df[\"abstract\"] != \"\"]), \"papers with abstract\")","208ef90b":"df['abstract'] = df['abstract'].apply(lambda x: re.sub('(\\\\n)+', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('[a|A]bstract( [0-9]+)*', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('ABSTRACT( [0-9]+)*', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('[b|B]ackground(: )*', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('BACKGROUND(: )*', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('^[\\s]*$', ' ', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub(r\"http\\S+\", '', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('\"', '', x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub(\"'\", '', x))","a4ea6a4e":"df['abstract'].replace(\"\", np.nan,inplace=True)\ndf.dropna(subset=['abstract'], inplace=True)\nprint(\"There are\",len(df),\"articles after removing missing values.\")","32f2eb7d":"df.drop_duplicates(subset=['abstract'], inplace=True)\nprint(\"There are\",len(df),\"articles after removing duplicate abstracts.\")","72d5ace4":"df.drop(df.index[(df.abstract.str.len() < 100)],inplace=True)\nprint(\"There are\",len(df),\"articles after removing abstracts with few characters.\")","7cbc2fe0":"display(df)","19f499e9":"!pip install --user kleis-keyphrase-extraction","ea3c889d":"import kleis.resources.dataset as kl\nfrom kleis.config.config import SEMEVAL2017","958e3272":"# load semeval 2017 dataset\ndataset = kl.load_corpus(name=SEMEVAL2017)\n\n# recomended options\ndataset.training(features_method=\"simple-posseq\", filter_min_count=16, tagging_notation=\"BILOU\")","147c16ea":"text_sample = df.sample()['abstract'].values[0]\nkeyphrases_sample = dataset.label_text(text_sample)\ndisplacy.render({\n        \"text\": text_sample,\n        # keyphrases are in brat-like format (we use only the span)\n        \"ents\": [{\"start\": start, \"end\": end, \"label\": \"KP\"}  for _, (_, (start, end)), _ in keyphrases_sample],\n        \"title\": None\n}, style=\"ent\", options=options, manual=True)","5fe86f94":"special_chars = re.compile(r\"\\W\")\nunderscores = re.compile(r\"[_]+\") \n\ndef normalize_term(t):\n    t_normalized, _ = re.subn(special_chars, \"_\", t.lower())\n    t_normalized, _ = re.subn(underscores, \"_\", t_normalized)\n    return t_normalized","19952a00":"def extract_abstract_kps(x) :\n    return [(normalize_term(kptext), (start, end)) for _, (_, (start, end)), kptext in dataset.label_text(x)]","dc263a16":"tqdm.pandas()\ndf[\"abstract_kps\"] = df[\"abstract\"].progress_apply(extract_abstract_kps)","c4eab322":"display(df)","3ae6b0a5":"def get_kptext(list_kps):\n    \"\"\"Return keyphrase text\"\"\"\n    return [ktext for kps in list_kps for ktext, _ in kps]\n\n# initialize\ndf[\"tf\"] = df[\"cord_uid\"].apply(lambda x : {}) # it is later turned into a list\n\n# keyphrase occurrences per doc\nkps_docs_count = Counter() # |{d: k \\in d}|\nkeyphrases = Counter()\nfor index, row in tqdm(df.iterrows(), total = len(df.index)):\n    \n    # list of keyphrases per document\n    current_kps_count = Counter(get_kptext([row[\"abstract_kps\"]]))\n    \n    # all keyphrases\n    keyphrases.update(current_kps_count)\n    \n    # +1 for each keyphrase in the document\n    kps_docs_count.update(current_kps_count.keys()) # |{d: k \\in d}|\n    \n    # Keyphrase frequency # TF\n    df.at[(df[df['cord_uid'] == row['cord_uid']].index)[0], 'tf'] = current_kps_count # Cnt","1e3a4131":"# keep cnt(keyphrase) > 1\nkeyphrases = {kp: cnt for kp, cnt in keyphrases.items() if cnt > 1}\n# remove single symbols normalized\nif \"_\" in keyphrases:\n    del keyphrases[\"_\"]\n\ndef get_tf(kps):\n    total_count = sum(kps.values())\n    return [(kp, cnt\/total_count) for kp, cnt in kps.items() if kp in keyphrases]\n\ntqdm.pandas()\ndf[\"tf\"] = df[\"tf\"].progress_apply(get_tf) # TF","7744c794":"N = len(df) # number of documents\ntqdm.pandas()\ndf[\"idf\"] = df[\"tf\"].progress_apply(lambda x : [(kp, np.log(N\/kps_docs_count[kp])) for kp, _ in x]) # IDF","8b965770":"def get_tf_idf(e):\n    tf, idf = e\n    return [(kp, ktf*idf[i][1]) for i, (kp, ktf) in enumerate(tf)]\n\ntqdm.pandas()\ndf[\"tf-idf\"] = df[['tf','idf']].progress_apply(lambda x: get_tf_idf(x), axis=1)","70939a5e":"MAX_KEYPHRASES = 15\n\ndef rank_keyphrases(e):\n    tf_idf, title, abstract = e\n    # normalized title and abstract\n    # underscores are added to search each _keyphrase_ as an single entity \n    # considering if it is the first or last word\n    title = \"_\" + normalize_term(title) + \"_\" \n    abstract = \"_\" + normalize_term(abstract) + \"_\"\n    # check if keyphrase is in title or abstract\n    tf_idf = map(lambda e: (e[0], e[1] + (int(title.find(\"_\" + e[0] + \"_\") >= 0) + int(abstract.find(\"_\" + e[0] + \"_\") >= 0))\/2 ), tf_idf)\n    \n    return sorted(tf_idf, key=lambda e: e[1], reverse=True)[:MAX_KEYPHRASES]\n    \ntqdm.pandas()\ndf[\"ranked_kps\"] = df[[\"tf-idf\", \"title\", \"abstract\"]].progress_apply(lambda x: rank_keyphrases(x), axis=1) # rank by tf-idf + title + abstract","81ff239d":"tqdm.pandas()\ndf[\"keyphrases\"] = df[\"ranked_kps\"].progress_apply(lambda x: set(kp for kp, _ in x))","711a2ec7":"example = df.iloc[random.randrange(len(df))]\nkps, tfidf = zip(*(example['ranked_kps']))\npandas.DataFrame({'ranked_kps': tfidf}, index=kps).iplot(kind='bar', title=example['title'])","564579f8":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\nnlp.pipe_names","1e6e1fd7":"MAX_SENTS = 5\n\ndf[\"ranked_sentences\"] = df[\"cord_uid\"].apply(lambda x : [])\ndf[\"doc_score_sents\"] = df[\"cord_uid\"].apply(lambda x : 0.0)\nfor index, row in tqdm(df.iterrows(), total = len(df.index)):\n    \n    # list of keyphrases per document\n    paragraphs_texts = nlp.pipe(map(lambda par: par[\"text\"], [{\"text\": row[\"abstract\"]}]))\n    \n    # merge all keyphrases per paragraph\n    full_kps = [row['abstract_kps']]\n   \n    # sum_sent_scores = 0.0\n    sentences_count = 0\n    scored_sentences = []\n    # for each paragraph\n    for i_par, par in enumerate(paragraphs_texts):\n        \n        # norm L1 of ranked keyphrases \n        l1_norm_rkps = sum(map(lambda x: x[1], row['ranked_kps'])) \n        \n        # keyphrases in the curent paragraph\n        paragraph_kps = full_kps[i_par]\n\n        for s in par.sents:\n            sentences_count += 1\n            sent_start, sent_end = s.start_char, s.end_char\n            s_text = s.text\n            s_score = 0.0 # sentence score\n            s_spans = []\n\n            for rkp, kscore in row['ranked_kps']:\n                # spans of current keyphrase\n                kspans = [pkp_spans for pkp_text, pkp_spans in paragraph_kps if pkp_text == rkp]\n\n                for kstart, kend in kspans:\n                    if kstart >= sent_start and kstart <= sent_end:\n                        s_score += (kscore\/l1_norm_rkps)**2 # sum(X^2)\n                        s_spans.append((kstart - sent_start, kend - sent_start))\n\n            if s_score > 0.0:\n                s_score = np.sqrt(s_score) # sqrt(sum(X^2)) # norm L2 normalized\n                # sum_sent_scores += s_score\n\n            scored_sentences.append((i_par, s_score, s_text, s_spans))\n            \n    # save ranked sentences\n    ranked_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:MAX_SENTS]\n    df.at[(df[df['cord_uid'] == row['cord_uid']].index)[0], 'ranked_sentences'] = ranked_sentences\n    df.at[(df[df['cord_uid'] == row['cord_uid']].index)[0], 'doc_score_sents'] = sum([s_score for _, s_score, _, _ in ranked_sentences])\/(sentences_count if sentences_count > 0 else 0.0)","e24c4cf4":"# df[\"score\"] = df.apply(lambda x: x['sum_sent_scores']\/x['sentences_count'])\ndocs_avgsize = df['tf'].apply(lambda x: len(x)).sum()\/len(df)\n\ndef okapi_bm25(d):\n    \"\"\"OKAPI BM25 over keyphrases\"\"\"\n    # default values in elastic search\n    okapi_b = 0.75\n    okapi_k1 = 1.2\n\n    kps, tf, idf = d\n    tf = dict(tf)\n    idf = dict(idf)\n    \n    doc_size = len(tf)\n    okapi_denominator_part = okapi_k1 * (1 - okapi_b + okapi_b * doc_size\/docs_avgsize)\n    \n    return sum([idf[kp] * (tf[kp] * (okapi_k1 + 1)) \/ (tf[kp] + okapi_denominator_part) for kp in kps])\n\ntqdm.pandas()\ndf['bm25'] = df[['keyphrases', 'tf', 'idf']].progress_apply(okapi_bm25, axis=1)","a8e56151":"del nlp\ndf.drop(columns=['abstract_kps'], inplace=True)","341efefd":"docs_avgsize = df['tf'].apply(lambda x: len(x)).sum()\/len(df)\n\ndef okapi_bm25(d):\n    \"\"\"OKAPI BM25 over keyphrases\"\"\"\n    # default values in elastic search\n    okapi_b = 0.75\n    okapi_k1 = 1.2\n\n    kps, tf, idf = d\n    kps = set(kp for kp, _ in kps)\n    tf = dict(tf)\n    idf = dict(idf)\n    \n    doc_size = len(tf)\n    okapi_denominator_part = okapi_k1 * (1 - okapi_b + okapi_b * doc_size\/docs_avgsize)\n    \n    return sum([idf[kp] * (tf[kp] * (okapi_k1 + 1)) \/ (tf[kp] + okapi_denominator_part) for kp in kps])\n\ntqdm.pandas()\ndf['score'] = df[['ranked_kps', 'tf', 'idf']].progress_apply(okapi_bm25, axis=1)","f300d5f0":"df.drop(columns=['tf'], inplace=True)\ndf.drop(columns=['idf'], inplace=True)","1a65ab84":"phi = 1\/3\nbm25_max = df['score'].max()\nscore_sents_max = df['doc_score_sents'].max()\ncnt_sents_max = MAX_SENTS\n\ndef get_doc_rank(e):\n    bm25_score, score_sents, rsents = e\n    return phi*(bm25_score\/bm25_max + score_sents\/score_sents_max + len(rsents)\/cnt_sents_max)\n\ntqdm.pandas()\ndf['score'] = df[['score', 'doc_score_sents', 'ranked_sentences']].apply(get_doc_rank, axis=1)","6c311ce7":"df.drop(columns=['doc_score_sents'], inplace=True)","60a1da30":"df_nlargest = df.nlargest(10, 'score')\n\nfor index, row in tqdm(df_nlargest.iterrows(), total = len(df_nlargest.index)):\n    print(\"\\nDoc score: %0.4f\" % row['score'])\n    print(\"\\n  Title: \" + row['title'])\n    for s in row['ranked_sentences']:\n        i_par, s_score, text, spans = s\n        print(\"\\n[+] Sentence score: %0.4f\" % s_score)\n        displacy.render({\n            \"text\": text,\n            # keyphrases are in brat-like format (we use only the span)\n            \"ents\": [{\"start\": start, \"end\": end, \"label\": \"KP\"}  for start,end in spans],\n            \"title\": None\n        }, style=\"ent\", options=options, manual=True)","fbcb324d":"df.sort_values(by='score', ascending=False, inplace=True)","f1e8e14a":"df.drop(df.loc[df['score']<=0.44].index, inplace = True)\ndf.reset_index(drop=True, inplace = True)","a11ccfca":"display(df)","ec75f96d":"def code_we_used_on_our_server() :\n    model_path = \"\/path_to\/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n    model = sent2vec.Sent2vecModel()\n    try:\n        model.load_model(model_path)\n    except Exception as e:\n        print(e)\n    print('model successfully loaded')\n    \n    stop_words = set(stopwords.words('english'))\n    \n    def preprocess_sentence(x):\n        x = x.replace('\/', ' \/ ')\n        x = x.replace('.-', ' .- ')\n        x = x.replace('.', ' . ')\n        x = x.replace('\\'', ' \\' ')\n        x = x.replace('[', ' [ ')\n        x = x.replace(']', ' ] ')\n        x = x.replace('(', ' ( ')\n        x = x.replace(')', ' ) ')\n        x = x.replace('%', ' % ')\n        x = x.replace('\"', ' \" ')\n        x = x.lower()\n\n        return \" \".join([token for token in word_tokenize(x) if token not in punctuation and token not in stop_words])\n    \n    tqdm.pandas()\n    df_json['abstract_tokenised'] = df_json['abstract'].progress_apply(preprocess_sentence)\n    df_json[\"abstract_vector\"] = df_json[\"abstract_tokenised\"].progress_apply(lambda x : model.embed_sentence(x))\n    \n    with open(\"\/calcul\/kaggle_challenge\/CORD-19-research-challenge_v7\/CORD-19_v7_final_data_0.36_vectors.tsv\", 'w+') as tensors:\n        with open(\"\/calcul\/kaggle_challenge\/CORD-19-research-challenge_v7\/CORD-19_v7_final_data_0.36_metadata.tsv\", 'w+') as metadata:\n            metadata.write(\"Index\\tTitle\\n\")\n            for index, row in tqdm(df_json.iterrows(), total = len(df_json.index)) :\n                metadata.write(\"%d\\t%s\\n\" % (index, row[\"title\"].encode('utf8')))\n                vector_row = '\\t'.join(map(str, row[\"abstract_vector\"][0]))\n                tensors.write(vector_row + \"\\n\")\n    \n    def most_similar_sentence(vec, num):\n        sims = empty(len(df_json), dtype=float)\n        vec_len = np_norm(vec)\n        for idx, row in df_json.iterrows() :\n            vec2 = row[\"abstract_vector\"]\n            vec2_len = np_norm(vec2)\n            sims[idx] = np.dot(vec[0],vec2[0]) \/ vec_len \/ vec2_len\n        nearest = []\n        topN = argsort(sims)[::-1]\n        display(topN)\n        for top_sent in topN:\n            if(idx != top_sent):\n                nearest.append((top_sent,float(sims[top_sent])))\n                if len(nearest) == num: break\n        return nearest\n    \n    def apply_features(x):\n        return most_similar_sentence(x, 10)\n\n    df_json[\"KNN\"] = df_json[\"abstract_vector\"].parallel_apply(apply_features)","fa316fd8":"### So Let's see our data now :","5ba6db73":"# Part 2 - Cleaning the text from the abstract\n- Even if we tried to get a maximum of missing data, we still missed some, so we deleted those and the duplicate in the cleaning step.","f6446b0e":"An example of ranked keyphrases in a document.","ffaf0798":"#### Scoring documents using BM25 and keyphrases.\n\nWe use BM25 to score documents by using their keyphrases as queries for themselves.","e91c8086":"## How to rank sentences and documents using keyphrases ?\n\nTo rank documents and sentences, first, we need to rank keyphrases.\n\n### The simplest way to rank keyphrases => TF-IDF\n\nUsing TF-IDF (term\/keyphrase frequency - inverse document frequency) is an inmediate alternative to rank keyphrases.\n\n#### TF\n\nFirst, we get count each keyphrase and the number of documents containing each keyphrase.","65a06429":"### Then we merged the data from the metadata and from the json files\n- So we used the metadata file as the source and we augmented it with all the json data ;\n- For the body text, due to the PDF parsing errors, we prioritized the xml json over the pdf json;","b51f7c23":"### Ranking sentences and documents\n\nLoading spacy model to manipulate text easily.","c3d12d79":"#### Instaling kleis\n*kleis* is available in pypi over the name `kleis-keyphrase-extraction`","9d29e16a":"*Note:* Ignore the warning related to the corpus. The package is going to use the pre-trained models.","d07c2d5a":"Now, we obtain the keyphrase frequency excluding keyphrases occurring only one time.","a16f233b":"#### IDF\n\nWe save the IDF for each keyphrase in the documents. \n\n\nNote: It is later used to rank documents.","2d3ded1f":"#### TF-IDF\n\nNow, we obtain the TF-IDF","5ee41b38":"# Part 1 - Loading the dataset","18e76f1b":"### Example of ranked documents and sentences\n\nThe example below shows ten ranked documents including their ranked sentences.","c142099e":"### How to load the json\n- we use this code on our server to extract the keyphrases from the body text\n- unfortunately, the limited RAM on kaggle prevent us of doing it here but our results will be available on our tool\n- we made a dictionnary of the path of each file\n- \"sha\" as key for the papers from PDF parsing\n- \"pmcid\" as key for the papers from XML parsing","f13a61c4":"### Ranking keyphrases (tf-idf + title + abstract)\n\nKeyphrases present in the title or abstract are more important than the rest. \n\nWe add +1 to the keyphrase's tf-idf if it is in both, title and abstract, 0.5 if it is in only one of them and 0 otherwise.\n\n**Note:** Use the variable to control the max number of retrieved keyphrases.\n\n```python\n# default\nMAX_KEYPHRASES = 15\n```","d1d6e86a":"## Let's look at the data (version 7)\n- there is a lot of missing data in each column, we forced the type of some column to avoir parsing errors later\n- new in v6 and v7 there is a json file from XML parse and PDF parse.\n- that's actually a really good thing, because we found a lot of PDF parsing errors in the dataset (see our [previous notebook](https:\/\/www.kaggle.com\/pierreholat\/keyphrases-ranking-of-data-supplemented-by-api))\n\n### Let's load the metadata","fecb71e8":"\n![Explorer_1.png](attachment:Explorer_1.png)\n\nWith this notebook, we wanted to share our work on the exploration of the scientific literature given in the dataset of this challenge. Indeed, reading more than 50k papers is harldy feasible, so the community needs a way the easily select the more relevant.\n\n# Ranking papers based on their Keyphrases \n\nPeople have tried to fulfill the tasks (or help others to fulfill them) in this Kaggle Challenge in quite a lot of ways. Some created search engines ([jdparsons\/biobert-corex-topic-search](https:\/\/www.kaggle.com\/jdparsons\/biobert-corex-topic-search\/notebook), [discussion\/138026](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/138026)), others proposed tools ([ajrwhite\/covid19-tools](https:\/\/www.kaggle.com\/ajrwhite\/covid19-tools\/notebook), [discussion\/138250](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/138250), [discussion\/139106](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139106)), others used knowledge graphs ([group16\/covid-19-knowledge-graph-embeddings](https:\/\/www.kaggle.com\/group16\/covid-19-knowledge-graph-embeddings), [sandyvarma\/covid-19-bert-mesh-enabled-knowledge-graph](https:\/\/www.kaggle.com\/sandyvarma\/covid-19-bert-mesh-enabled-knowledge-graph)), some used KeyPhrases [hamid3731\/keyphrase-extraction-and-graph-analysis](https:\/\/www.kaggle.com\/hamid3731\/keyphrase-extraction-and-graph-analysis), etc.  \n\nFor our part, we decided to propose a way to rank papers and sentences based on their extracted keyphrases. Some already have proposed scoring functions like [discussion\/140726](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/140726). Others like [discussion\/137558](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137558) proposed to base the ranking on the study design (with this discussion [discussion\/139355](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139355) directly linked to it).\n\nTo rank papers, we scored them using an heuristic, in which the main component is the Okapi BM25 algorithm based on keyphrases. Our keyphrases lack of a direct scoring, so we scored them, then we could make *extrative summarization* (it is similar to the method originally used with TextRank, for example, [mobassir\/mining-covid-19-scientific-papers](https:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers\/notebook)). \n\n![ranked.png](attachment:ranked.png)\n\nNote : We published an [other notebook](https:\/\/www.kaggle.com\/pierreholat\/keyphrases-ranking-of-data-supplemented-by-api), a few weeks ago, about augmenting the data with public API and extracting the keyphrases on it. But as the v7 of the dataset is way cleaner, we decided to publish this notebook with a focus on keyphrases and visualisation.\n\n# Exploration of the data\n\nFrom the ranked papers, we selected a subset with the highest scores to show in our visualisation tool, including some handy functionnalities, like a search engine and the selection of the most similar papers. To compute the position in the 3D space and the similarity between papers, we  used the advanced [BioSentVec model](https:\/\/github.com\/ncbi-nlp\/BioSentVec) [1] [2] to get each abstract embeddings. This model is the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database.\n\nOur choice was based on two things :\t\n- Given the biomedical data of this challenge, the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives.\n- Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially with advanced deep learning methods.\n\n\nOur main source of inspiration for our visualisation tool is the [Embedding Projector](https:\/\/projector.tensorflow.org\/), but we wanted an easier way to share a final model to the community (without the limit of a 10 000 maximum sample of the Embedding Projector).\n\nWe used the [Three.js library](https:\/\/threejs.org\/) to build the 3D exploration canvas. Given the little time we had to develop this tool, you might encounter some unexpected bugs, please post them here so we can fix them if possible. \n\nWith our ranking method of papers, our clustering of paper based on the abstract embeddings and our visualisation tools, one can easily find the best articles to answer his query. As shown in this example for the task \"What is known about transmission, incubation, and environmental stability?\" and the sub tasks \"Specifically, we want to know what the literature reports about: Disease models, including animal models for infection, disease and transmission\".\n\n![Explorer_search_models.png](attachment:Explorer_search_models.png)\n\n\n### Our tool is [available here](http:\/\/holat.fr)","2a9c1dcb":"# Part 3 - Ranking important sentences and documents using extracted keyphrases (keywords).","26a5bebc":"# We now have all our papers ranked\n## So let's select only the best paper to display in our tools\n- indeed, displaying 44000 papers is irrelevant","2f2b7d5d":"We free some memory","8bf4b608":"### We used the [BioSentVec model](https:\/\/github.com\/ncbi-nlp\/BioSentVec) to get each abstract embeddings for :\n- projecting in our 3D exploration tool;\n- computing similarity between abstract;\n","b37432f4":"#### How to rank a sentence using the weights from keyphrases?\n\nThe simplest way is to represent each sentence as an unit vector of keyphrases' weights (e.g., TF-IDF). From the resulting vector, the norm L2 can be used to obtain an scalar weight of the sentence.\n\n**Note:** Use the variable to control the max number of retrieved sentences per document.\n\n```python\n# default\nMAX_SENTS = 5\n```","b216d969":"### Again, the kaggle notebook limitation prevent us to replicate this in here\n- But let's share our code anyway","1e6deebd":"#### Scoring documents using BM25 and keyphrases.\n\nWe use BM25 to score documents by using their keyphrases as queries for themselves.","3776f8bf":"We free some memory","1ab7360a":"### Now we proceed to extract all the keyphrases\n- We applied the code from the example above to each abstract and body text. We saved only the spans of the keyphrases.","e026dbfe":"We free some memory","0bc10de9":"We saved a simple list of the keyphrases.","2e8d84ee":"#### Kleis: Extracting keyphrases from a random file\nOpenning random file to exctract keyphraes and testing keyphrase extraction over from an abstract.","bbd48dbb":"# For the visualisation of those results, we made our tool available [here](http:\/\/holat.fr)\n## Have fun (we had a lot working on this).","20fc29a6":"## Why to use keyphrases?\n\n**Keyphrases** (or keywords or keyterms) are the most representative segments of text within the body of a document. **Extracted keyphrases** can be used for extactive summarization, by ranking sentences. They can be used to score documents and select their most representative sentences helping to retrieve important information.\n\n### Motivation: \n\n*TextRank* is a graph-based ranking model for unsupervised automatic keyphrase extraction. In the paper [TextRank: Bringing Order into Text](https:\/\/www.aclweb.org\/anthology\/W04-3252\/), it is proposed as an useful method for extractive summarization, since it produces ranked keyphrases. An implementation of this method is available for [Spacy](https:\/\/spacy.io\/) in the universe modules [pyTextRank](https:\/\/spacy.io\/universe\/project\/spacy-pytextrank). However, as mentioned in the challenge kernel [KeyPhrase Extraction and Graph analysis](https:\/\/www.kaggle.com\/hamid3731\/keyphrase-extraction-and-graph-analysis), TextRank is not the best available method. \n\nInstead of using TextRank to extract keyphrases, we propose to use a supervised CRF-based method that *filters candidates of keyphrases* using their *PoS tag sequences* [**LIPN** at SemEval 2017 Task 10](https:\/\/www.aclweb.org\/anthology\/S17-2174\/). Several members of our team participating in the current Kaggle challenge are former integrants of the LIPN team who presented that work. \n\nTo consider about *kleis*:\n*     It is a non-rank based method to extract keyphrases\n*     It is a supervised method\n*     It is not trained over medical data\n\nWe used an improved version of the method available as the python package [kleis](https:\/\/pypi.org\/project\/kleis-keyphrase-extraction\/). The latest version of this package achieves a **F1 score of 0.405 (40.5%)** on the dataset *SemEval 2017 Task 10* for the subtask of keyphrase identification. \n\n### Our approach\n\nOn resume:\n\n1.     We extracted keyphrases from all papers (using a supervised non-rank based method) \n2.     We rank keyphrases with TF-IDF and metadata information (location of the keyphrase in the paper)\n3.     We selected 15 keyphrases to represent each document (maximum)\n4.     We scored sentences depending on the presence of the 15 keyphrases (like TextRank)\n5.     We retrieve the 5 highest scored sentences (maximum)\n6.     We scored each paper averaging three scores:\n       * Okapi BM25 using papers' keyphrases as queries\n       * Average of the scores of the retrieved sentences\n       * Penalization of very small texts: cnt(retrieved_sents)\/max_num_sents\n\n\n\n[1] Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n\n[2] Chen Q, Peng Y, Lu Z. BioSentVec: creating sentence embeddings for biomedical texts. The 7th IEEE International Conference on Healthcare Informatics. 2019. Bio","61bc1487":"#### Ranking documents (BM25 + sentence scoring)\n\nNow, we use sentence and BM25 scorings to rank each document.","56662d5a":"#### Configuring and importing kleis\n\nLoading SemEval2017 model."}}