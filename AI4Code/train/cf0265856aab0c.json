{"cell_type":{"00272678":"code","a254da7c":"code","2112aef6":"code","03d0d45e":"code","c13b7639":"code","97f4d29d":"code","6e0b33ed":"code","6ab75355":"code","2658d630":"code","dbc59380":"code","1dc00fd4":"code","ebf8a9ab":"code","865d6b8f":"code","4dc3803f":"code","378cd543":"markdown","2434aab5":"markdown","abf5e62a":"markdown","470db71f":"markdown","f990847e":"markdown","2d115428":"markdown","11c500f8":"markdown","50f98370":"markdown","af0add43":"markdown","8e41dd3e":"markdown","1e4fe97c":"markdown","b82124f3":"markdown","e3e5ebdd":"markdown"},"source":{"00272678":"# Imports\n# Basic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, random, math\nfrom sklearn.model_selection import train_test_split\n\n# DL\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import Model\nfrom keras.layers import Input, Embedding, Dropout, SpatialDropout1D, GlobalAveragePooling1D\nfrom keras.layers import GlobalMaxPooling1D, Bidirectional, GRU, CuDNNGRU, Activation, Dense\nfrom keras.layers import Dot, Reshape, TimeDistributed, concatenate, BatchNormalization\nfrom keras.optimizers import Adam\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport seaborn as sns\nsns.set()\n\n# GLOBAL VARIABLES\nEMB_SIZE = 300","a254da7c":"## Utils functions\n\ndef prepare_data():\n    dataset = pd.read_csv(\"..\/input\/emotion.data\")\n    input_sentences = [text.split(\" \") for text in dataset[\"text\"].values.tolist()]\n    labels = dataset[\"emotions\"].values.tolist()\n        \n    # Initialize word2id and label2id dictionaries that will be used to encode words and labels\n    word2id = dict()\n    label2id = dict()\n\n    max_words = 0 # maximum number of words in a sentence\n\n    # Construction of word2id dict\n    for sentence in input_sentences:\n        for word in sentence:\n            # Add words to word2id dict if not exist\n            if word not in word2id:\n                word2id[word] = len(word2id)\n        # If length of the sentence is greater than max_words, update max_words\n        if len(sentence) > max_words:\n            max_words = len(sentence) # Number of words is set to the longest text\n\n    \n    # Construction of label2id, id2label and id2word dicts\n    label2id = {l: i for i, l in enumerate(set(labels))}\n    id2label = {v: k for k, v in label2id.items()}\n    id2word = {v: k for k, v in word2id.items()}\n    \n    # Encode input words and labels\n    X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n    Y = [label2id[label] for label in labels]\n\n    # Apply Padding to X\n    X = pad_sequences(X, max_words)\n\n    # Convert Y to numpy array\n    Y = keras.utils.to_categorical(Y, num_classes=len(label2id), dtype='float32')\n\n    # Print shapes\n    print(\"Shape of X: {}\".format(X.shape))\n    print(\"Shape of Y: {}\".format(Y.shape))\n    \n    X_tra, X_te, Y_tra, Y_te = train_test_split(X,Y, stratify=Y, test_size = 0.3)\n    \n    return X_tra, X_te, Y_tra, Y_te,word2id,id2word,label2id,id2label\n\ndef make_plot(data, metric=\"loss\"):\n    # Data for plotting\n    data1 = data[0]\n    data2 = data[1]\n    t = np.arange(1,len(data1)+1,1)\n    plt.figure(figsize=(10,5))\n    plt.plot(t, data1)\n    plt.plot(t, data2)\n    plt.xlabel('epoch')\n    plt.ylabel(metric)\n    plt.title('Train vs Val ' + metric)\n    plt.grid()\n    plt.legend(['train','val'], ncol=2, loc='upper right');\n    plt.savefig(\"train_val_\"+metric+\".png\", dpi=300)\n    plt.show()\n\ndef focal_loss(target, input):\n    gamma = 2.\n    input = tf.cast(input, tf.float32)\n\n    max_val = K.clip(-input, 0, 1)\n    loss = input - input * target + max_val + K.log(K.exp(-max_val) + K.exp(-input - max_val))\n    invprobs = tf.log_sigmoid(-input * (target * 2.0 - 1.0))\n    loss = K.exp(invprobs * gamma) * loss\n\n    return K.mean(K.sum(loss, axis=1))\n\ndef rgb_to_hex(rgb):\n    return '#%02x%02x%02x' % rgb\n    \ndef attention2color(attention_score):\n    r = 255 - int(attention_score * 255)\n    color = rgb_to_hex((255, r, r))\n    return str(color)\n\ndef visualize_attention():\n    # Make new model for output predictions and attentions\n    model_att = keras.Model(inputs=model.input, \\\n                            outputs=[model.output, model.get_layer('attention_vec').output])\n    idx = np.random.randint(low = 0, high=X_te.shape[0]) # Get a random test\n    tokenized_sample = np.trim_zeros(X_te[idx]) # Get the tokenized text\n    label_probs, attentions = model_att.predict(X_te[idx:idx+1]) # Perform the prediction\n\n    # Get decoded text and labels\n    decoded_text = [id2word[word] for word in tokenized_sample] \n    label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(),label_probs[0])}\n\n    # Get word attentions using attenion vector\n    token_attention_dic = {}\n    max_score = 0.0\n    min_score = 0.0\n    for token, attention_score in zip(decoded_text, attentions[0][-len(tokenized_sample):]):\n        token_attention_dic[token] = math.sqrt(attention_score)\n\n    # Build HTML String to viualize attentions\n    html_text = \"<hr><p style='font-size: large'><b>Text:  <\/b>\"\n    for token, attention in token_attention_dic.items():\n        html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention),\n                                                                            token)\n    html_text += \"<\/p>\"\n    # Display text enriched with attention scores \n    display(HTML(html_text))\n\n    # PLOT EMOTION SCORES\n    emotions = [label for label, _ in label_probs.items()]\n    scores = [score for _, score in label_probs.items()]\n    plt.figure(figsize=(5,2))\n    plt.bar(np.arange(len(emotions)), scores, align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan', \"purple\"])\n    plt.xticks(np.arange(len(emotions)), emotions)\n    plt.ylabel('Scores')\n    plt.show()","2112aef6":"X_tra, X_te, Y_tra, Y_te,word2id,id2word,label2id,id2label = prepare_data()","03d0d45e":"def get_model():\n    \n    inp = Input(shape=(X_tra.shape[1],))\n    x = Embedding(X_tra.shape[1], EMB_SIZE)(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n\n    dense = Dense(len(label2id))(conc)\n    bn = BatchNormalization()(dense)\n    outp = Activation(\"softmax\")(bn)\n    \n    opt = Adam(1e-3)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss=focal_loss,\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    # Print model summary\n    # model.summary()\n    \n    return model","c13b7639":"model = get_model()","97f4d29d":"model.summary()","6e0b33ed":"# Train model\nhist = model.fit(X_tra, Y_tra, validation_data=(X_te, Y_te), epochs=3, batch_size=32, validation_split=0.3, shuffle=True)\nval_loss = hist.history['val_loss'];val_acc = hist.history['val_acc']\nloss = hist.history['loss'];acc = hist.history['acc']","6ab75355":"make_plot([loss, val_loss]);make_plot([acc,val_acc], metric=\"acc\")","2658d630":"def get_model():\n    \n    # Define input tensor\n    sequence_input = Input(shape=(X_tra.shape[1],), dtype='int32')\n\n    # Word embedding layer\n    embedded_inputs = Embedding(len(word2id) + 1,\n                                            EMB_SIZE,\n                                            input_length=X_tra.shape[1])(sequence_input)\n\n    # Apply dropout to prevent overfitting\n    embedded_inputs = SpatialDropout1D(0.2)(embedded_inputs)\n\n    # Apply Bidirectional LSTM over embedded inputs\n    lstm_outs = Bidirectional(\n        CuDNNGRU(40, return_sequences=True)\n    )(embedded_inputs)\n\n    # Apply dropout to LSTM outputs to prevent overfitting\n    lstm_outs = Dropout(0.2)(lstm_outs)\n\n    # Attention Mechanism - Generate attention vectors\n    attention_vector = TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n    attention_vector = Reshape((X_tra.shape[1],))(attention_vector)\n    attention_vector = Activation('softmax', name='attention_vec')(attention_vector)\n    attention_output = Dot(axes=1)([lstm_outs, attention_vector])\n\n    # Last layer: fully connected with softmax activation\n    fc = Dense(EMB_SIZE, activation='relu')(attention_output)\n    output = Dense(len(label2id), activation='softmax')(fc)\n\n    # Finally building model\n    model = Model(inputs=sequence_input, outputs=output)\n    model.compile(loss=focal_loss, metrics=[\"accuracy\"], optimizer='adam')\n\n    # Print model summary\n    # model.summary()\n    \n    return model","dbc59380":"model = get_model()","1dc00fd4":"model.summary()","ebf8a9ab":"# Train model\nhist = model.fit(X_tra, Y_tra, validation_data=(X_te, Y_te), epochs=3, batch_size=32)\nval_loss = hist.history['val_loss'];val_acc = hist.history['val_acc']\nloss = hist.history['loss'];acc = hist.history['acc']","865d6b8f":"make_plot([loss, val_loss]);make_plot([acc, val_acc], metric=\"acc\")","4dc3803f":"visualize_attention()","378cd543":"![MonoBiGruAtt](https:\/\/i.imgur.com\/uShjISA.png)","2434aab5":"## <a name=\"bigru_multiemb\"><\/a>Multi-class BiGru with Multi Embeddings\n\n- Input\n    - Words\n    - Word embeddings 1\n    - Word embeddings ...\n    \n![img](https:\/\/i.imgur.com\/v6Uxi3T.png)","abf5e62a":"### Performance","470db71f":"# Update: Multi-class document classification\n\nNotebook with the progress accomplished for the multi-class document classification problem.\n\n*Note: Everything is implemented, some models are not showcased here (just the model architecture) because of the data available on this kernel.*\n\n## Outline of this Notebook\n\n1. [Task and dataset description](#task)\n2. [Multi-class BiGru](#bigru_multi)\n3. [Multi-class BiGru with Attention](#bigru_att)\n    * [Visualize attention](#att)\n4. [Multi-class BiGru with Categorical Embeddings](#bigru_cat)\n5. [Multi-class BiGru with Multi Embeddings](#bigru_multiemb)","f990847e":"![img](https:\/\/i.imgur.com\/diVlwwj.png)","2d115428":"### <a name=\"att\"><\/a>Visualize attention","11c500f8":"## <a name=\"task\"><\/a>Task and dataset description\n\nThe task and dataset are analogous to the CorpusV. Just a little bit simpler.\n\n**Sentiment analysis:** the aim is to classify short texts according to the main emotion reflected on it.\n\n- Multi-class classification\n    - 6 classes\n- Only text feature\n    - 74 words max length","50f98370":"## <a name=\"bigru_multi\"><\/a>Multi-class BiGru","af0add43":"## Future\n\n- Architectures \n    - CNN models\n    - Capsule Network models\n- Embeddings\n    - Contextual embeddings (Elmo, Bert...)\n- Multi-Task","8e41dd3e":"![MonoBiGru](https:\/\/i.imgur.com\/A6oVHEl.png)","1e4fe97c":"### Performance","b82124f3":"## <a name=\"bigru_cat\"><\/a>Multi-class BiGru with Categorical Embeddings\n\n- Input variables\n    - Words\n    - Categorical feature","e3e5ebdd":"## <a name=\"bigru_att\"><\/a>Multi-class BiGru with Attention"}}