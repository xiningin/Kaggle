{"cell_type":{"22cf94fc":"code","6104d644":"code","f5f2079c":"code","0aae1ffa":"code","d0a859e1":"code","c392d5b2":"code","be681336":"code","3c5d4b64":"code","c806b580":"code","14683a1d":"code","c9f62bce":"code","e3f10d57":"code","9a81975a":"code","6bc5e911":"code","978cfe48":"code","9029097e":"code","05783ad1":"code","52fe8100":"code","10acc645":"code","9ca50110":"code","418924f6":"code","c13408cb":"code","a612afa6":"code","beaea134":"markdown","5036b5f1":"markdown","40732faa":"markdown","2d11ac3d":"markdown","bbcc290e":"markdown","a80c0285":"markdown","22c78789":"markdown","a8f1cb0d":"markdown","b4a66db1":"markdown","46bc0b75":"markdown","6ce0359d":"markdown","da9b3794":"markdown","4d445df3":"markdown","115e7700":"markdown","4b74db4e":"markdown","cf09f6e5":"markdown","56a55f19":"markdown","c5a28b33":"markdown","77cc8509":"markdown","fcfeb1ce":"markdown","d02edf7c":"markdown","c28ad0be":"markdown","4b70b9ce":"markdown","2a5dfe88":"markdown","81e91e6e":"markdown","af184a29":"markdown","da83cd22":"markdown","1ce9d289":"markdown","3bdcab4d":"markdown","ee3f9a59":"markdown","84fa32f7":"markdown","a804d0ea":"markdown","43fb9f96":"markdown","a0c17027":"markdown","83c6deca":"markdown","274b1031":"markdown","8f449ed5":"markdown","b8b8ca1e":"markdown","0aa79970":"markdown","43085dfc":"markdown","0cbd4adc":"markdown","3d8aaa2d":"markdown","aa21036f":"markdown","f5e87180":"markdown","c5948a3a":"markdown","ac046c66":"markdown","a4f66728":"markdown"},"source":{"22cf94fc":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import PorterStemmer\nimport nltk","6104d644":"data = pd.read_csv(\"..\/input\/youtubevideodataset\/Youtube Video Dataset.csv\")\ndata.head(10)","f5f2079c":"print(data.shape)\nprint(data.isnull().values.any())","0aae1ffa":"data=data.dropna()\nprint(data.isnull().values.any())","d0a859e1":"Category=data['Category'].value_counts()\nprint(Category.shape)\nprint(Category)","c392d5b2":"# loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n#Stemmering the word\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n\ndef preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        #Removing link\n        url_pattern = r'((http|ftp|https):\\\/\\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:\/~\\+#]*[\\w\\-\\@?^=%&amp;\/~\\+#])?'\n        total_text = re.sub(url_pattern, ' ', total_text)\n        #removing email address\n        email_pattern =r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+'\n        total_text = re.sub(email_pattern, ' ', total_text)\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                word=(sno.stem(word))\n                string += word + \" \"\n        \n        data[column][index] = string","be681336":"#text processing stage.\nstart_time = time.clock()\nfor index, row in data.iterrows():\n    if type(row['Description']) is str:\n        preprocessing(row['Description'], index, 'Description')\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","3c5d4b64":"data.head(10)","c806b580":"y_true = data['Category'].values\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.2)\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","14683a1d":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cv data:', cv_df.shape[0])","c9f62bce":"X_trainCategory=train_df['Category'].value_counts()\nprint(X_trainCategory)\nprint('Distribution of y in train')\nplt.figure(figsize=(8,8))\nsns.barplot(X_trainCategory.index, X_trainCategory.values, alpha=0.8)\nplt.title('Distribution of y in train')\nplt.ylabel('data point per class')\nplt.xlabel('Class')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\ntest_dfCategory=test_df['Category'].value_counts()\nprint(test_dfCategory)\nprint('Distribution of y in test')\nplt.figure(figsize=(8,8))\nsns.barplot(test_dfCategory.index,test_dfCategory.values, alpha=0.8)\nplt.title('Distribution of y in test')\nplt.ylabel('data point per class')\nplt.xlabel('Class')\nplt.xticks(rotation=90)\nplt.show()\n\ncv_dfCategory=cv_df['Category'].value_counts()\nprint(cv_dfCategory)\nprint('Distribution of y in cv')\nplt.figure(figsize=(8,8))\nsns.barplot(cv_dfCategory.index,cv_dfCategory.values, alpha=0.8)\nplt.title('Distribution of y in cv')\nplt.ylabel('data point per class')\nplt.xlabel('Class')\nplt.xticks(rotation=90)\nplt.show()\n","e3f10d57":"x_tr=train_df['Description']\nx_test=test_df['Description']\nx_cv=cv_df['Description']","9a81975a":"bow = CountVectorizer()\nx_tr_uni = bow.fit_transform(x_tr)\nx_test_uni= bow.transform(x_test)\nx_cv_uni= bow.transform(x_cv)","6bc5e911":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nx_tr_tfidf = tf_idf_vect.fit_transform(x_tr)\nx_test_tfidf = tf_idf_vect.transform(x_test)\nx_cv_tfidf = tf_idf_vect.transform(x_cv)","978cfe48":"def plotPrecisionRecall(y_test,y_pred):\n    C = confusion_matrix(y_test, y_pred)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    B =(C\/C.sum(axis=0))\n    labels = ['Art&Music','Food','History','Sci&Tech','Manu','TravelBlog']\n\n\n    print(\"-\"*20, \"Precision matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True,annot_kws={\"size\": 16}, fmt='g', xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True,annot_kws={\"size\": 16}, fmt='g', xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","9029097e":"clf = SGDClassifier(loss = 'hinge', alpha = 0.01, class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_uni,y_train)\ny_pred = clf.predict(x_test_uni)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\nmatrix=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(matrix)\nsns.set(font_scale=1.4)#for label size\nlabels = ['Art&Music','Food','History','Sci&Tech','Manu','TravelBlog']\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()\nplotPrecisionRecall(y_test,y_pred)","05783ad1":"clf = SGDClassifier(loss = 'hinge', alpha =0.0001, class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_tfidf,y_train)\ny_pred = clf.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(6),range(6))\nsns.set(font_scale=1.4)#for label size\nlabels = ['Art&Music','Food','History','Sci&Tech','Manu','TravelBlog']\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()\nplotPrecisionRecall(y_test,y_pred)","52fe8100":"y_true = data['Category'].values\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.2)","10acc645":"x_tr=X_train['Description']\nx_test=test_df['Description']","9ca50110":"bow = CountVectorizer()\nx_tr_uni = bow.fit_transform(x_tr)\nx_test_uni= bow.transform(x_test)","418924f6":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nx_tr_tfidf = tf_idf_vect.fit_transform(x_tr)\nx_test_tfidf = tf_idf_vect.transform(x_test)","c13408cb":"RF= RandomForestClassifier(n_estimators=16,max_depth=130)\nRF.fit(x_tr_uni,y_train)\ny_pred =RF.predict(x_test_uni)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(6),range(6))\nsns.set(font_scale=1.4)#for label size\nlabels = ['Art&Music','Food','History','Sci&Tech','Manu','TravelBlog']\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()\nplotPrecisionRecall(y_test,y_pred)","a612afa6":"RF= RandomForestClassifier(n_estimators=20,max_depth=190)\nRF.fit(x_tr_tfidf,y_train)\ny_pred =RF.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(6),range(6))\nsns.set(font_scale=1.4)#for label size\nlabels = ['Art&Music','Food','History','Sci&Tech','Manu','TravelBlog']\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()\n\nplotPrecisionRecall(y_test,y_pred)","beaea134":"<h2>4. .Text vectorization[BOW,TF-IDF]<\/h2>","5036b5f1":"<h3>Conclusion<\/h3>\n* Travel Blog-407 are correctly classified out of 440 Precision of 91.4% and Recall of 88.981<br>\n* Science and Technology-377 are correctly classified out of 415 Precision of 96.2% and Recall of 90.9%<br>\n* Food-333 are correctly classified out of 366 Precision of 82.4 % and Recall of 89.9%<br>\n* Manufacturing-310 are correctly classified out of 340 Precision of 95.9%  and Recall of 90.8% <br>\n* Art & Music-229 are correctly classified out of 336 Precision of 88.0% and Recall of  91.1%<br>\n* History-296 are correctly classified out of 329 Precision of 90.6% and Recall of 92.5%<br>","40732faa":"<h3>2.1.2. Example Data Point<\/h3>","2d11ac3d":"<h1>2. Machine Learning Problem Formulation<\/h1>","bbcc290e":"<h3>Conclusion<\/h3>\n* Travel Blog-393 are correctly classified out of 440 Precision of 89.3% and Recall of 89.3%<br>\n* Science and Technology-368 are correctly classified out of 415 Precision of 92% and Recall of 88.6%<br>\n* Food-324 are correctly classified out of 366 Precision of 93% and Recall of 88.5%<br>\n* Manufacturing-298 are correctly classified out of 340 Precision of 78.2%  and Recall of 87.6% <br>\n* Art & Music-276 are correctly classified out of 336 Precision of 83.8% and Recall of  82.1%<br>\n* History-258 are correctly classified out of 329 Precision of 83.2% and Recall of 78.4%<br>","a80c0285":"<h3>3.1. Reading Youtube Data<\/h3>","22c78789":"<h3>Conclusion<\/h3>\n* Travel Blog-416 are correctly classified out of 440 Precision of 90.9% and Recall of 89.2%<br>\n* Science and Technology-384 are correctly classified out of 415 Precision of 96.3% and Recall of 92.8%<br>\n* Food-340 are correctly classified out of 366 Precision of 86.6 % and Recall of 90.8%<br>\n* Manufacturing-312 are correctly classified out of 340 Precision of 95.5%  and Recall of 92.5% <br>\n* Art & Music-300 are correctly classified out of 336 Precision of 90.9% and Recall of  91.7%<br>\n* History-299 are correctly classified out of 329 Precision of 91.8% and Recall of 94.5%<br>","a8f1cb0d":"|Model |n_estimators|Max Depth|F1-score cv|F1-score test|Precision test|Recall test|Accuracy Test|\n|------|------|------|------|------|------|------|------|\n|unigram|16|130|0.868|0.858|0.860|0.858|86.119%|\n|TF-IDF|20|190|0.872|0.849|0.854|0.848|85.085%|","b4a66db1":"<h4>5.2.2.1 BOW<\/h4>","46bc0b75":"<h4>5.2.1. Splitting data into train,test (80:20)<\/h4>","6ce0359d":"|Model |hyper parameter|F1-score cv|F1-score test|Precision test|Recall test|Accuracy Test|\n|------|------|------|------|------|------|------|\n|unigram  | 0.01|0.927|0.907|0.908|0.907|90.836%|\n| TF-IDF | 0.0001|0.922|0.920|0.920|0.920|92.138%|","da9b3794":"<h4>5.1.3. LinearSVM Conclusion<\/h4>","4d445df3":"<h4>5.2.2.1 TF-IDF<\/h4>","115e7700":"# 6.Final Conclusion\n<p style=\"font-size:24px;text-align:center\">Support Vector Machine<\/p>\n\n|Model |hyper parameter|F1-score cv|F1-score test|Precision test|Recall test|Accuracy Test|\n|------|------|------|------|------|------|------|\n|unigram  | 0.01|0.927|0.907|0.908|0.907|90.836%|\n| TF-IDF | 0.0001|0.922|0.920|0.920|0.920|92.138%|\n\n<p style=\"font-size:24px;text-align:center\">Random Forest<\/p>\n\n|Model |n_estimators|Max Depth|F1-score cv|F1-score test|Precision test|Recall test|Accuracy Test|\n|------|------|------|------|------|------|------|------|\n|unigram|16|130|0.868|0.858|0.860|0.858|86.119%|\n|TF-IDF|20|190|0.872|0.849|0.854|0.848|85.085%|\n","4b74db4e":"<h4>4.1 Bag of Words (BoW)<\/h4>","cf09f6e5":"<h2>5.2.4 TF-IDF<\/h2>","56a55f19":"<h1>1. Business Problem<\/h1>","c5a28b33":"<h3>3.3. Test, Train and Cross Validation Split<\/h3>","77cc8509":"<h4>3.3.2. Distribution of y_i's in Train, Test and Cross Validation datasets<\/h4>","fcfeb1ce":"<h3>2.2.2. Performance Metric<\/h3>","d02edf7c":"<h1>3. Reading Data<\/h1>","c28ad0be":"<h2>5.2.3 BOW<\/h2>","4b70b9ce":"<h4>5.1.1. Unigram(BOW)<\/h4>","2a5dfe88":"<h3>3.2. Preprocessing of Description<\/h3>","81e91e6e":"<h3>2.1.1. Data Overview<\/h3>","af184a29":"* low-latency requirement.\n* Interpretability is not important.","da83cd22":"<h3> Problem statement : <\/h3>\n<p>Given Description,Title of Youtube Video predict category<\/p>","1ce9d289":"<h4>5.1.2. TF-IDF<\/h4>","3bdcab4d":"<h3>2.2.1. Type of Machine Learning Problem<\/h3>","ee3f9a59":"* F1-Score \n* recall \n* precision","84fa32f7":"<h3>Conclusion<\/h3>\n* Travel Blog-396 are correctly classified out of 440 Precision of 80.6% and Recall of 90.0%<br>\n* Science and Technology-354 are correctly classified out of 415 Precision of 92.6% and Recall of 85.3%<br>\n* Food-315 are correctly classified out of 366 Precision of 95.4% and Recall of 86.0%<br>\n* Manufacturing-305 are correctly classified out of 340 Precision of 79.0%  and Recall of 89.7% <br>\n* Art & Music-265 are correctly classified out of 336 Precision of 81.5% and Recall of  78.8%<br>\n* History-259 are correctly classified out of 329 Precision of 83.1% and Recall of 78.7%<br>","a804d0ea":"<h2>5. Machine Learning<\/h2>","43fb9f96":"<p style=\"font-size:36px;text-align:center\"> <b>Youtube category prediction<\/b> <\/p>","a0c17027":"<h4>4.2. TF-IDF<\/h4>","83c6deca":"<h2>2.1. Data<\/h2>","274b1031":"- Source: collect using web scraper\nhttps:\/\/github.com\/rahulanand1103\/Web-Scraping-Youtube\/blob\/master\/YoutubeDataSet.csv\n- <p> \n    Data file's information:\n    <ul> \n        <li>\n        Youtube(Title,Videourl,Category,Description)\n        <\/li>\n    <\/ul>\n    <ul> \n        <li>\n        Title-Name of youtube video\n        <\/li>\n        <li>\n        Videourl-Video Url\n        <\/li>\n        <li>\n        desciption-Information about video\n        <\/li>\n        <li>\n        category-There are 6 Category<br>\n            1-Art and Music<br>\n            2-manufacturing<br>\n            3-Food<br>\n            4-History<br>\n            5-travelblog<br>\n            6-Science And Technology<br>\n        <\/li>\n    <\/ul>\n<\/p>","8f449ed5":"<p>\nThere are six different classes of youtube video so it is a Multi class classification problem\n<\/p>","b8b8ca1e":"<h3>5.1 Linear SVM<\/h3>","0aa79970":"<h4>5.2.2. BOW,TF-IDF<\/h4>","43085dfc":"<h2>1.1. Description<\/h2>","0cbd4adc":"<h2>2.2. Mapping the real-world problem to an ML problem<\/h2>","3d8aaa2d":"\n| Title|Videourl|Descption|category|\n|------|------|------|------|\n|RcmrbNRK-jY|200 Days - A Trip Around the World Travel Film|This film is the story of our incredible trip|Travel|\n|...|...|...|...|","aa21036f":"<h4>3.3.1. Splitting data into train, test,cv (64:16:20)<\/h4><br>","f5e87180":"<h3>5.2 Bagging(Random Forest)<\/h3>","c5948a3a":"<h2>1.2 Real-world\/Business objectives and constraints.<\/h2>","ac046c66":"<h2>5.2.4 Random Forest Conclusion <\/h2>","a4f66728":"<h3> Preprocessing<\/h3>\n* Remove url\n* Remove email address\n* remove special character\n* word Stemming"}}