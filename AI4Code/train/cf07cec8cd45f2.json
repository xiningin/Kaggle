{"cell_type":{"49076271":"code","a8ff9f53":"code","e9420672":"code","c307f0f3":"code","e0273882":"code","27843ab8":"code","bca59ca0":"code","3074a005":"code","f2c52d7d":"code","4025eb9e":"code","79a068ef":"code","01f5333d":"code","97a99dce":"code","c842f45c":"code","76568c44":"code","7e674830":"code","68e66211":"code","aa5ca86f":"code","9884a4c4":"code","2d89aeef":"code","2c12c726":"code","d1f2659a":"code","0930d2fe":"code","7f31d463":"code","aae84763":"code","10c6eb22":"code","778e1a47":"code","15f9238b":"code","80f04cbc":"code","e5284d9f":"code","7ebe5ebd":"code","af56c886":"code","04c05d1b":"code","4045d139":"code","187aa207":"code","1d12ae49":"code","5737c4e2":"code","92bc159d":"code","a2e8e045":"code","70230c9e":"code","37ce6423":"code","851223f7":"code","379881a6":"code","ffce54bb":"code","f849e5a2":"code","46d9d747":"code","67a115e3":"code","f23a6437":"code","58f29cf0":"code","de54f2f6":"code","6a8b97c1":"code","3621c9c3":"code","f8c205c4":"code","eeb4e90a":"code","dde467fb":"markdown","337560c0":"markdown","b4fc10e1":"markdown","0b1e1336":"markdown","b614ef03":"markdown","41df9b5b":"markdown","0d11bdb5":"markdown","fca3245d":"markdown","c59ddd1a":"markdown","804f7e89":"markdown","b38d2d07":"markdown","c4e00553":"markdown","f65a5b3e":"markdown","ee15db11":"markdown"},"source":{"49076271":"# Import Libraries\nimport gc\nimport glob\nimport os\nimport random\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import Input\nfrom keras import Model\nfrom keras.activations import elu, softmax\nfrom keras.applications import VGG16\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import BatchNormalization, Activation\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers.core import Flatten\nfrom keras.losses import categorical_crossentropy\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD,Adam\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","a8ff9f53":"## Hypermeters & Constants\nkaggleDir = '\/kaggle\/input\/state-farm-distracted-driver-detection\/'\ntrain_img_dir = 'train\/'\ntest_img_dir = 'test\/'\nCLASSES = {\"c0\": \"safe driving\", \"c1\": \"texting - right\", \"c2\": \"talking on the phone - right\", \"c3\": \"texting - left\",\n           \"c4\": \"talking on the phone - left\", \"c5\": \"operating the radio\", \"c6\": \"drinking\", \"c7\": \"reaching behind\",\n           \"c8\": \"hair and makeup\", \"c9\": \" talking to passenger\"}\nIMG_DIM = 299\nCHANNEL_SIZE = 3\nSEED_VAL = 41\nBATCH_SIZE = 28\nEPOCHS = 150  # Total Number of epoch","e9420672":"# Set the seed value for repreducing the results\ntf.set_random_seed(SEED_VAL)\ngc.enable()\nnp.random.seed(SEED_VAL)\nrandom.seed(SEED_VAL)","c307f0f3":"# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","e0273882":"## Load Data\ndf_train = pd.read_csv(kaggleDir + 'driver_imgs_list.csv', low_memory=True)\nprint('Number of Samples in trainset : {}'.format(df_train.shape[0]))\nprint('Number Of districted Classes : {}'.format(len((df_train.classname).unique())))\n\ndf_train = shuffle(df_train)","27843ab8":"print(\"[INFO] : Load all the images.....\")\ntrainImgDir = os.path.join(kaggleDir, train_img_dir)\ntestImgDir = os.path.join(kaggleDir, test_img_dir)\ntrainImgs = glob.glob(trainImgDir + '*\/*.jpg')\ntestImgs = glob.glob(testImgDir + '*.jpg')\nlen(trainImgs), len(testImgs)","bca59ca0":"for x in trainImgs:\n    print(x)\n    break\n\nfor x in testImgs:\n    print(x)\n    break","3074a005":"# Display top five record in csv\ndf_train.head()","f2c52d7d":"# Dispaly Last five samples from CSV.\ndf_train.tail()","4025eb9e":"class_freq_count = df_train.classname.value_counts()\n\nclass_freq_count.plot(kind='bar', label='index')\nplt.title('Sample Per Class');\nplt.show()\n\nplt.pie(class_freq_count, autopct='%1.1f%%', shadow=True, labels=CLASSES.values())\nplt.title('Sample % per class');\nplt.show()","79a068ef":"imgPath = os.path.join(kaggleDir, train_img_dir, \"c6\/img_20687.jpg\")\nimg = load_img(imgPath)\nplt.suptitle(CLASSES['c6'])\nplt.imshow(img)","01f5333d":"def draw_driver(imgs, df, classId='c0'):\n    fig, axis = plt.subplots(2, 3, figsize=(20, 7))\n    for idnx, (idx, row) in enumerate(imgs.iterrows()):\n        imgPath = os.path.join(kaggleDir, train_img_dir, f\"{classId}\/{row['img']}\")\n        row = idnx \/\/ 3\n        col = idnx % 3 \n        img = load_img(imgPath)\n        #         img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        plt.imshow(img)\n        axis[row, col].imshow(img)\n    plt.suptitle(CLASSES[classId])\n    plt.show()","97a99dce":"draw_driver(df_train[df_train.classname == 'c0'].head(6), df_train, classId='c0')","c842f45c":"draw_driver(df_train[df_train.classname == 'c1'].head(6), df_train, classId='c1')","76568c44":"draw_driver(df_train[df_train.classname == 'c2'].head(6), df_train, classId='c2')","7e674830":"draw_driver(df_train[df_train.classname == 'c3'].head(6), df_train, classId='c3')","68e66211":"\ndraw_driver(df_train[df_train.classname == 'c4'].head(6), df_train, classId='c4')","aa5ca86f":"draw_driver(df_train[df_train.classname == 'c5'].head(6), df_train, classId='c5')","9884a4c4":"draw_driver(df_train[df_train.classname == 'c6'].head(6), df_train, classId='c6')","2d89aeef":"draw_driver(df_train[df_train.classname == 'c7'].head(6), df_train, classId='c7')","2c12c726":"draw_driver(df_train[df_train.classname == 'c8'].head(6), df_train, classId='c8')","d1f2659a":"draw_driver(df_train[df_train.classname == 'c9'].head(6), df_train, classId='c9')","0930d2fe":"dfY = df_train.classname\nx_train, x_test, y_train, y_test = train_test_split(df_train, dfY, test_size=0.15, stratify=dfY)\nprint('Number of Samples in XTrain : {} Ytrain: {}'.format(x_train.shape[0], y_train.shape[0]))\nprint('Number of Samples in Xtest : {} Ytest: {}'.format(x_test.shape[0], y_test.shape[0]))","7f31d463":"df_train.head()","aae84763":"df_train['file_name']=df_train.img.apply(lambda  x:x[:-4])","10c6eb22":"df_train.head()","778e1a47":"class SimplePreprocessor:\n    def __init__(self, width, height, inter=cv2.INTER_AREA):\n        #         print(\"[INFO] : Simple PreProcessor invoked...!\")\n        self.width = width\n        self.height = height\n        self.inter = inter\n\n    def preprocess(self, image):\n        #         print(\"[INFO] : Prepossess Resizing invoked...!\")\n        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)\n","15f9238b":"class ImageToArrayPreprocessor:\n    def __init__(self, dataFormat=None):\n        self.dataFormat = dataFormat\n\n    def preprocess(self, image):\n        return img_to_array(image, data_format=self.dataFormat)","80f04cbc":"class SimpleDataLoader:\n    def __init__(self, preprocessors=None):\n        self.preprocessors = preprocessors\n        if self.preprocessors is None:\n            self.preprocessors = []\n\n    def load(self, trainImgs, verbose=-1):\n        imgData = []\n        imgLabels = []\n        for (idx, imgPath) in enumerate(trainImgs):\n            tmpImg = cv2.imread(imgPath)\n            classLabel = imgPath.split(os.path.sep)[-2]\n\n            if self.preprocessors is not None:\n                for preprocesor in self.preprocessors:\n                    img = preprocesor.preprocess(tmpImg)\n                    gc.collect()\n                imgData.append(tmpImg)\n                imgLabels.append(imgLabels)\n\n            if verbose > 0 and idx > 0 and (idx + 1) % verbose == 0:\n                print('[INFO]: Processed {}\/{}'.format((idx + 1), len(trainImgs)))\n        print(len(imgData), len(imgLabels))\n        return np.array(imgData), np.array(imgLabels)","e5284d9f":"print(\"[INFO] : Loading data from desk and scale the raw pixel intensities to the range [0,1] ....!\")\n# sp=SimplePreprocessor(IMG_DIM,IMG_DIM)\n# iap=ImageToArrayPreprocessor()\n# loader=SimpleDataLoader(preprocessors=[sp,iap])\n# (data,labels)=loader.load(trainImgs, verbose=5000)\n# # Re-scale the image","7ebe5ebd":"# data = data.astype('float') \/ 255.0\n# xtrain,xtest,ytrain,ytest= train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)\n# print(\"[INFO] : Split dataset....!\")\n# labelBi = LabelBinarizer()\n# trainY = labelBi.fit_transform(ytrain)\n# testY = labelBi.fit_transform(ytest)","af56c886":"imgPath = os.path.join(kaggleDir, train_img_dir, \"c6\/img_20687.jpg\")\nimage=load_img(imgPath)\nimage=img_to_array(image)\nimage=np.expand_dims(image, axis=0)\ngenerator = ImageDataGenerator(rotation_range=30,\n                               height_shift_range=0.1,\n                               width_shift_range=0.1,\n                               shear_range=0.2,\n                               zoom_range=0.2,\n#                                horizontal_flip=True,\n                               fill_mode='nearest') \nimageGen=generator.flow(image,batch_size=1)","04c05d1b":"for i in range(6):\n    nextImg=imageGen.next()\n    plt.subplot(230 + 1 + i)\n    image = nextImg[0].astype('uint8')\n    plt.imshow(image)\nplt.show()","4045d139":"generator = ImageDataGenerator(rescale=1 \/ 255.0,\n                               zoom_range=30,\n                               samplewise_center=True,\n                               height_shift_range=0.2,\n                               width_shift_range=0.2,\n                               shear_range=0.2, \n                               fill_mode='nearest',\n                               validation_split=0.15)","187aa207":"\ntrain_generator = generator.flow_from_directory(directory=os.path.join(kaggleDir, train_img_dir),\n                                                classes=CLASSES.keys(),\n                                                class_mode='categorical',\n                                                color_mode=\"rgb\",\n                                                target_size=(IMG_DIM, IMG_DIM),\n                                                shuffle=True,\n                                                seed=SEED_VAL,\n                                                subset='training')\nvalid_generator = generator.flow_from_directory(directory=os.path.join(kaggleDir, train_img_dir),\n                                                classes=CLASSES.keys(),\n                                                class_mode='categorical',\n                                                color_mode=\"rgb\",\n                                                target_size=(IMG_DIM, IMG_DIM),\n                                                shuffle=True,\n                                                seed=SEED_VAL,\n                                                subset='validation')\ntrain_generator.class_indices\ngc.collect()","1d12ae49":"train_generator.class_indices,valid_generator.samples","5737c4e2":"trainImgs[:5]","92bc159d":"earlyStop = EarlyStopping(monitor='val_loss', mode='min', patience=8, verbose=1, min_delta=0.0000001)\nreduceRL = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=7, factor=0.001, min_delta=0.0001, verbose=1,\n                             min_lr=1e-6)\ncallbacks = [reduceRL, earlyStop]","a2e8e045":"\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=[IMG_DIM, IMG_DIM, 1], activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=256, kernel_size=(3, 3), activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=512, kernel_size=(3, 3), activation=elu))\nmodel.add(Activation(activation=elu))\nmodel.add(MaxPooling2D())\nmodel.add(BatchNormalization())\n\n# model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=elu))\n# model.add(Activation(activation=elu))\n# model.add(MaxPooling2D())\n# model.add(BatchNormalization())\n\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(3000))\nmodel.add(Activation(activation=elu))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(2000))\nmodel.add(Activation(activation=elu))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(len(CLASSES)))\nmodel.add(Activation(activation=softmax))\nmodel.summary()","70230c9e":"# opt = SGD()#lr=0.0001\nfrom keras.optimizers import adam\nopt=adam()\nmodel.compile(optimizer=opt, loss=categorical_crossentropy, metrics=['acc'])\n","37ce6423":"# History = model.fit_generator(train_generator,\n#                               steps_per_epoch=train_generator.samples \/\/ BATCH_SIZE,\n#                               validation_data=valid_generator,\n#                               validation_steps=valid_generator.samples \/\/ BATCH_SIZE,\n#                               epochs=EPOCHS,\n#                               verbose=1).history  # , callbacks=callbacks","851223f7":"# eval_loss, eval_acc = model.evaluate_generator(valid_generator, steps=valid_generator.samples \/ BATCH_SIZE);\n# print('[INFO] : Evaluation Accuracy : {:.2f}%'.format(eval_acc * 100))\n# print('[INFO] : Evaluation Loss : {}'.format(eval_loss))","379881a6":"# History.keys()","ffce54bb":"# plt.style.use('ggplot')\n# plt.figure()\n# plt.plot(np.arange(0, EPOCHS), History['acc'], label='Train_Acc')\n\n# plt.plot(np.arange(0, EPOCHS), History['val_acc'], label='Valid_Acc')\n# plt.plot(np.arange(0, EPOCHS), History['val_loss'], label='Valid_Loss')\n# plt.plot(np.arange(0, EPOCHS), History['loss'], label='Train_Loss')\n# plt.xlabel('Epochs#')\n# plt.ylabel('Accuracy and Loss#')\n# plt.title(\"Loss and Accuracy\")\n# plt.legend()\n# plt.show()","f849e5a2":"class FCHeadNet:\n    @staticmethod\n    def build(baseModel, classes, D):\n        # initialize the head model that will be placed on top of the base then ad a Fully Connected Layer\n        headModel = baseModel.output\n        headModel = Flatten(name='flatten')(headModel)\n        headModel = Dense(D, activation='elu')(headModel)\n        headModel = Dropout(0.2)(headModel) \n\n        # add Softmax Layer\n        headModel = Dense(classes, activation='softmax')(headModel)\n\n        # Return the model\n        return headModel","46d9d747":"baseModel = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(IMG_DIM, IMG_DIM,3)))","67a115e3":"# initialize the new head of the network, a set of FC layers followed by a softmax classifier\nheadModel = FCHeadNet.build(baseModel, len(CLASSES), 10000)","f23a6437":"# place the head FC model on top of the base model -- this will become the actual model we will train\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n# loop over all layers in the base model and freeze them so they will *not* be updated during the training process\nfor layer in baseModel.layers:\n    layer.trainable = False\n\n# compile our model (this needs to be done after our setting our  layers to being non-trainable\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=0.001)\nmodel.compile(optimizer=opt, loss=['categorical_crossentropy'], metrics=['acc'])\n\n# train the head  of the network for a  few epochs (all other layers are frozen) -- this will allow to the new FC\n# layers to start to become initialized with actual  'learned ' values versus pure random\n","58f29cf0":"model.summary()","de54f2f6":"# loop over the layers in the network and display them to the console\nfor (idx, layer) in enumerate(model.layers):\n    print('[INFO] : {} \\t {}'.format(idx, layer.__class__.__name__))\n","6a8b97c1":"History = model.fit_generator(train_generator,\n                              steps_per_epoch=train_generator.samples \/\/ BATCH_SIZE,\n                              validation_data=valid_generator,\n                              validation_steps=valid_generator.samples \/\/ BATCH_SIZE,\n                              epochs=EPOCHS,\n                              verbose=1 , callbacks=callbacks).history  #","3621c9c3":"eval_loss, eval_acc = model.evaluate_generator(valid_generator, steps=valid_generator.samples \/ BATCH_SIZE);\nprint('[INFO] : Evaluation Accuracy : {:.2f}%'.format(eval_acc * 100))\nprint('[INFO] : Evaluation Loss : {}'.format(eval_loss))","f8c205c4":"plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), History['acc'], label='Train_Acc')\n\nplt.plot(np.arange(0, EPOCHS), History['val_acc'], label='Valid_Acc')\nplt.plot(np.arange(0, EPOCHS), History['val_loss'], label='Valid_Loss')\nplt.plot(np.arange(0, EPOCHS), History['loss'], label='Train_Loss')\nplt.xlabel('Epochs#')\nplt.ylabel('Accuracy and Loss#')\nplt.title(\"Loss and Accuracy\")\nplt.legend()\nplt.show()","eeb4e90a":"from keras.applications import DenseNet201\n\nmodel=DenseNet201(include_top=True, weights='imagenet')","dde467fb":"# Data Augmentation\n----\n-  Data Augmentation is the technique to generate new sample from the existing sample. So, you can reduce generalization error. It will genrerate natrual sample. There are number of features, which can help you in data agumentation. \n\n* **rotation_range :**  is a value in degrees (0-180), a range within which to randomly rotate pictures.\n* **height_shift_range :**   Constructor control the amount of horizontal and vertical shift respectively.\n* **width_shift_range :**  Constructor control the amount of horizontal and vertical shift respectively.\n* **shear_range :** Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n* **zoom_range :** Range for random zoom\n* **horizontal_flip :** Randomly filp of input image in horizontally. *But we can't use in our case.* It can chane the class of images.\n* **fill_mode :** Points outside the boundaries of the input image are filled according to the given mode. (default Nearest)\n ","337560c0":"Data is balanced ","b4fc10e1":"# Transfer Learning\n\nTransfer Learning is nothing it just using the existing pre-trained model and train on your data. \nTransfer Learning can be used two ways....\n- Extract Features by passing samples to pre-train model\n- Train model with your data from last layer and fine-tune hyperparameter\n\n![TransferLearning.PNG](attachment:TransferLearning.PNG)","0b1e1336":"In this Section, I am using **glob** python pakage. Which is using python shell script to fill all the files path. On the bases of regex matches. \n\nLoad all the images form directory and check the number of images per directory. ","b614ef03":"* https:\/\/mc.ai\/tutorial-on-keras-flow_from_dataframe\/\n* https:\/\/theailearner.com\/2019\/07\/06\/imagedatagenerator-flow_from_directory-method\/\n* https:\/\/pythontic.com\/visualization\/charts\/piechart\n* https:\/\/www.kdnuggets.com\/2019\/08\/keras-callbacks-explained-three-minutes.html\n* https:\/\/www.pyimagesearch.com\/2019\/07\/08\/keras-imagedatagenerator-and-data-augmentation\/","41df9b5b":"# Smart System for Distraction Detection and Monitoring\n---\n\n**\"Detecting distraction and monitoring smartly can save other and you life. So driver Safly \"**\n\nThis is a deep leanring computer vision problem. We need to implment the system, which will detetct the distraction and monitor the driver\n\n**Distracted driving refers to the act of driving while engaging in other activities which distract the driver's attention away from the road. Distractions are shown to compromise the safety of the driver, passengers, pedestrians, and people in other vehicles.**\n\n![Distracted-Driving.jpg](attachment:Distracted-Driving.jpg)\n![download.jfif](attachment:download.jfif)\n\nThis Kernel is for the detection and classification the driver behavior. We will try to understand the traning data and behaviour of data. I will do image pre-processing and use Deep learning CNN for identifying the behavior of the driver. Which is necessary for handling the unexpected road accident. \u00a0\n\nNowadays vehicles are increasing exponentially. We need to be extra careful about the road incident. As there are a couple of things around you, which can distract the driving. We can use this model and implement a smart system, which will monitor driver activities at the time of driving and can alert him about the activities. It will reduce the number of roads accidents and will increase road safety.\n\n\n\nState Farm Distracted Driver Detection has divided driving behavior into 10 classes.\u00a0 \n\n\n| CLASS        | Description          |  \n| :------------- |:-------------|  \n| C0     | Safe Driving | \n| C1     | Texting - Right      |\n| C2     | Talking On The Phone - Right    |\n| C3     | Texting- left | \n| C4     | Talking on the Phone- left | \n| C5     | Operating The Radio   |\n| C6     | Drinking| \n| C7     |  Reaching Behind     |\n| C8     | Hair And Makeup   |\n| C9     | Talking To Passenger   |\n\n\nWe will implement a deep learning model, which will predict the driver activities at the time of driving. \n\nKindly ****upvote**** or comment if you like this kernel.\u00a0","0d11bdb5":"# Training Accurcy \/ Loss ","fca3245d":"# Exploratory Data Analysis\n---\nIn this section, we will do the image data analysis for driver pose and behavior.\n\n\n---\nWe have data in couple for file \n1. **CSV(driver_imgs_list) :** Contianing list of drivers and there activites images. \n1. **images (train, tes) :** These two directory is containing the drivers images. ","c59ddd1a":"# BarPlot and PieChart\n---\n- A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.4-\n\n- A pie chart is a circular statistical diagram. The area of the whole chart represents 100% or the whole of the data. The areas of the pies present in the Pie chart represent the percentage of parts of data. \u00a0The area of the wedges determines the relative quantum or percentage of a part with respect to a whole. Pie charts are frequently used in business presentations as they give a quick summary of the business activities like sales, operations and so on.\n\n\nPie charts are also used heavily in survey results, news articles, resource usage diagrams like disk and memory.\u00a0","804f7e89":"**ReduceLROnPlateau:** \n\u00a0 \u00a0It's for reducing the learning rate, if after specified epoch monitoring matrices(Val_loss, val_acc ) stopped improving. \n\n* **monitor:** quantity matrices to be monitored.\n* **factor:** factor by which the learning rate will be reduced. new_lr = lr * factor\n* **patience:** number of epochs with no improvement after which learning rate will be reduced.\n* **min_lr:** lower bound on the learning rate.\n* **min_delta:** threshold for measuring the new optimum, to only focus on significant changes.","b38d2d07":"# Neural Network Architecture\n---\nKeras define neural network as sequence of layers. \n* **Sequential :**   Sequential API used for stacking the neural Network layers.\n\n","c4e00553":"**EarlyStopping:** \u00a0This is a callback function, which helps in handling the over-fitting. It can terminate the process of training for avoiding the over-fitting. It has number of parameters, those can help you, at which stage training need to be stopped.\n\n* **monitor:** Specify the matrix for monitoring e.g val_loss, Val_acc etc\n* **min_delta:** minimum change in the monitored value. For example, min_delta=1 means that the training process will be stopped if the absolute change of the monitored value is less than 1\n* **patience:** Stop training if after specified epoch,if no improvement in loss or accuracy. \n* **mode:** It depend on monitor, e.g if you are monitoring loss it should be min and if you are monitoring accuracy it should be max.\u00a0","f65a5b3e":"# Image Pre-Processing\n---","ee15db11":"# Keras Callbacks Functions\n--- \nI can say callbacks function are helper functions. They will help you in understanding overfitting and underfitting at early stage, you don't need to wait till the end of train.  \n\nThey Returns training details loss, accuracy from algorithm, after each epoch and you can use that details to analysis you training after each iteration.\n> **A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.**\n\nKeras has multiple callback functions.  \n* **BaseLogger and History :** By Default \n* **ModelCheckpoint :** Save the model best weights\n* **EarlyStopping :** Stop training if it's not getting improve after defined epochs\n* **ReduceLROnPlateau :** Reduce Learning Rate if  accuracy is not getting improve after defined epochs.\n\nHere we are using \n\n**EarlyStopping** & **ReduceLROnPlateau**\n"}}