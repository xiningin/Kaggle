{"cell_type":{"be216290":"code","7f2195e0":"code","fbd17895":"code","fbaef69b":"code","5d16d9b1":"code","1153b08d":"code","e8bcc2b1":"code","d7796f85":"code","079c0b36":"code","20ff531d":"code","21b32f71":"code","8805c539":"code","7e612137":"code","14e9bbf2":"code","c99e17e4":"code","29c0e869":"code","f2afde60":"code","a9e150b6":"code","fff395fc":"code","cbc96dd6":"code","0331f55c":"code","048f08c5":"code","e1c3600d":"code","4030087c":"code","7dfc6976":"code","99fa5f7a":"code","2d506aad":"code","6ad65277":"code","9a4ce2ec":"code","49ae0250":"code","11766868":"code","84834993":"code","71dec790":"code","11de330a":"code","c5961269":"code","31c04cf4":"code","e0715ca6":"code","a1410228":"code","cbcb8aa7":"code","009227bb":"code","ac66cccf":"code","28b50686":"code","b7e61cc1":"code","a25e0d88":"code","37e46451":"code","7d44d9a2":"code","89f22cd7":"code","ef08d572":"code","49311a27":"code","692aafca":"code","cb30c6dc":"code","c2aea8c9":"code","406be78c":"code","5a125e8b":"code","fd94a939":"code","aa38966f":"code","09409e91":"code","39c5cf12":"code","98cdcbf8":"code","7d256956":"code","442dac4d":"code","8de534e0":"code","029695ef":"markdown","c5a60f3e":"markdown","0a10e474":"markdown","cb71c004":"markdown","e66f4a9f":"markdown","9f1be9ec":"markdown","b1bcf80f":"markdown","8fa2c004":"markdown","91897af3":"markdown","910f264d":"markdown","686ecc48":"markdown","44d9b673":"markdown","4ab14bfb":"markdown","a24dc247":"markdown","02465e74":"markdown","d618ec5b":"markdown","827d0cb7":"markdown","be216c46":"markdown","40f7ca4f":"markdown","1b8bae56":"markdown","5463e57f":"markdown","9715d59e":"markdown","81eb2c20":"markdown","acb19e71":"markdown","d5fca7f3":"markdown","f0184bd7":"markdown","7e66d84c":"markdown"},"source":{"be216290":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nimport skimage\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D,\\\nZeroPadding2D, Convolution2D, ZeroPadding2D,AveragePooling2D,Input, GlobalMaxPooling2D, Conv2DTranspose, Reshape\nfrom keras import models\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras.regularizers import l1,l2,L1L2\nfrom tensorflow.keras import regularizers\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","7f2195e0":"Main_Path = Path(\"..\/input\/garbage-collective-data-for-nature-conservation\/Garbage_Collective_Data\")","fbd17895":"JPG_Path = list(Main_Path.glob(r\"*\/*.jpg\"))","fbaef69b":"JPG_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],JPG_Path))","5d16d9b1":"JPG_Path_Series = pd.Series(JPG_Path,name=\"JPG\").astype(str)\nJPG_Labels_Series = pd.Series(JPG_Labels,name=\"CATEGORY\")","1153b08d":"Main_Data = pd.concat([JPG_Path_Series,JPG_Labels_Series],axis=1)","e8bcc2b1":"print(Main_Data.head(-1))","d7796f85":"Main_Data = Main_Data.sample(frac=1).reset_index(drop=True)","079c0b36":"print(Main_Data.head(-1))","20ff531d":"Recy_List = []\nfor image_search in Main_Data[\"CATEGORY\"].values:\n    if image_search == \"paper\":\n        Recy_List.append(\"Y\")\n    elif image_search == \"metal\":\n        Recy_List.append(\"Y\")\n    elif image_search == \"plastic\":\n        Recy_List.append(\"Y\")\n    else:\n        Recy_List.append(\"N\")\n        \n# WE WILL USE IT FOR MULTI-OUTPUT","21b32f71":"Main_Data[\"RECYCLING\"] = Recy_List\n\n# TO DEFINE A NEW COLUMN","8805c539":"print(Main_Data.head(-1))","7e612137":"def reading_image(image):\n    \n    Picking_Image = image\n    Reading_Image = cv2.cvtColor(cv2.imread(Picking_Image),cv2.COLOR_BGR2RGB)\n    \n    return Reading_Image","14e9bbf2":"figure = plt.figure(figsize=(10,10))\n\nrandom_image = reading_image(Main_Data[\"JPG\"][4])\nplt.xlabel(random_image.shape)\nplt.ylabel(random_image.size)\nplt.title(Main_Data[\"CATEGORY\"][4])\nplt.imshow(random_image)","c99e17e4":"figure = plt.figure(figsize=(10,10))\n\nrandom_image = reading_image(Main_Data[\"JPG\"][400])\nplt.xlabel(random_image.shape)\nplt.ylabel(random_image.size)\nplt.title(Main_Data[\"CATEGORY\"][400])\nplt.imshow(random_image)","29c0e869":"figure = plt.figure(figsize=(10,10))\n\nrandom_image = reading_image(Main_Data[\"JPG\"][320])\nplt.xlabel(random_image.shape)\nplt.ylabel(random_image.size)\nplt.title(Main_Data[\"CATEGORY\"][320])\nplt.imshow(random_image)","f2afde60":"figure = plt.figure(figsize=(10,10))\n\nrandom_image = reading_image(Main_Data[\"JPG\"][11320])\nplt.xlabel(random_image.shape)\nplt.ylabel(random_image.size)\nplt.title(Main_Data[\"CATEGORY\"][11320])\nplt.imshow(random_image)","a9e150b6":"figure,axis = plt.subplots(5,5,figsize=(10,10))\n\nfor indexing,operation in enumerate(axis.flat):\n    \n    Image_Random = reading_image(Main_Data[\"JPG\"][indexing])\n    operation.set_xlabel(Image_Random.shape)\n    operation.set_ylabel(Image_Random.size)\n    operation.set_title(Main_Data[\"CATEGORY\"][indexing])\n    operation.imshow(Image_Random)\n    \nplt.tight_layout()\nplt.show()","fff395fc":"X_Train,X_Test = train_test_split(Main_Data,train_size=0.9,random_state=123,shuffle=True)","cbc96dd6":"print(X_Train.head(-1))","0331f55c":"print(X_Train.head(-1))","048f08c5":"X_Validation = X_Train[0:1000]\nX_Train = X_Train[3000:7001]","e1c3600d":"X_Train = X_Train.reset_index()\nX_Test = X_Test.reset_index()\nX_Validation = X_Validation.reset_index()\n\n# TO RESET INDEX FOR ORIGINAL SAMPLES","4030087c":"print(X_Train.shape)\nprint(X_Test.shape)\nprint(X_Validation.shape)","7dfc6976":"print(X_Train.head(-1))","99fa5f7a":"print(X_Test.head(-1))","2d506aad":"print(X_Validation.head(-1))","6ad65277":"X_Test = X_Test.drop(\"index\",axis=1)\nX_Validation = X_Validation.drop(\"index\",axis=1)\nX_Train = X_Train.drop(\"index\",axis=1)\n\n# WE DON'T NEED \"index\" COLUMN","9a4ce2ec":"print(X_Test.columns)\nprint(X_Validation.columns)\nprint(X_Train.columns)","49ae0250":"print(X_Train.shape)\nprint(X_Test.shape)\nprint(X_Validation.shape)","11766868":"Encoder_Function = LabelEncoder()","84834993":"X_Train[\"CATEGORY\"] = Encoder_Function.fit_transform(X_Train[\"CATEGORY\"])\nX_Train[\"RECYCLING\"] = Encoder_Function.fit_transform(X_Train[\"RECYCLING\"])\n\n# TO TRANSFORM FOR MODEL PROCESS","71dec790":"print(X_Train[\"CATEGORY\"].value_counts())","11de330a":"print(X_Train[\"RECYCLING\"].value_counts())","c5961269":"X_Test[\"CATEGORY\"] = Encoder_Function.fit_transform(X_Test[\"CATEGORY\"])\nX_Test[\"RECYCLING\"] = Encoder_Function.fit_transform(X_Test[\"RECYCLING\"])\n\n# TO TRANSFORM FOR MODEL PROCESS","31c04cf4":"print(X_Test[\"CATEGORY\"].value_counts())","e0715ca6":"print(X_Test[\"RECYCLING\"].value_counts())","a1410228":"Transformated_Train_JPG = []\nTransformated_Train_Label = []\nTransformated_Train_Recycling = []\n\nfor image_X,label_X,recycling_X in zip(X_Train.JPG,X_Train.CATEGORY,X_Train.RECYCLING):\n    \n    Image_Picking = reading_image(image_X)\n    Image_Resize = cv2.resize(Image_Picking,(180,180))\n    Image_Reduce = Image_Resize \/ 255.0\n    \n    Transformated_Train_JPG.append(Image_Reduce)\n    Transformated_Train_Label.append(label_X)\n    Transformated_Train_Recycling.append(recycling_X)","cbcb8aa7":"Train_JPG = np.array(Transformated_Train_JPG,dtype=\"float32\")\nTrain_Label = to_categorical(Transformated_Train_Label)\nTrain_Recycling = to_categorical(Transformated_Train_Recycling)\n\n# TO TRANSFORM FOR MODEL INPUT AND OUTPUT","009227bb":"print(Train_JPG.shape)\nprint(Train_Label.shape)\nprint(Train_Recycling.shape)","ac66cccf":"Transformated_Test_JPG = []\nTransformated_Test_Label = []\nTransformated_Test_Recycling = []\n\nfor image_X,label_X,recycling_X in zip(X_Test.JPG,X_Test.CATEGORY,X_Test.RECYCLING):\n    \n    Image_Picking = reading_image(image_X)\n    Image_Resize = cv2.resize(Image_Picking,(180,180))\n    Image_Reduce = Image_Resize \/ 255.0\n    \n    Transformated_Test_JPG.append(Image_Reduce)\n    Transformated_Test_Label.append(label_X)\n    Transformated_Test_Recycling.append(recycling_X)","28b50686":"Test_JPG = np.array(Transformated_Test_JPG,dtype=\"float32\")\nTest_Label = to_categorical(Transformated_Test_Label)\nTest_Recycling = to_categorical(Transformated_Test_Recycling)\n\n# TO TRANSFORM FOR MODEL INPUT AND OUTPUT","b7e61cc1":"print(Test_JPG.shape)\nprint(Test_Label.shape)\nprint(Test_Recycling.shape)","a25e0d88":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")\nReduce_Model = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n                                                   factor=0.1,\n                                                   patience=5)","37e46451":"COMPILE_METRICS = [\"accuracy\"]\nCOMPILE_CLASS_PREDICTION = \"categorical_crossentropy\"\nCOMPILE_RECYCLING_PREDICTION = \"categorical_crossentropy\"\nCOMPILE_OPTIMIZER = \"rmsprop\"\nINPUT_SHAPE = (180,180,3)\nCLASS_OUTPUT = 12\nRECYCLING_OUTPUT = 2\nEPOCH_PERIOD = 50\nBATCH_SIZE = 64","7d44d9a2":"Input_Layer = tf.keras.Input(shape=INPUT_SHAPE)\n#\nx = Conv2D(64,(3,3),activation=\"relu\",padding=\"same\",use_bias = True)(Input_Layer)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(128,(3,3),activation=\"relu\",padding=\"same\",use_bias = True)(x)\nx = Dropout(0.3)(x)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(128,(3,3),activation=\"relu\",padding=\"same\",use_bias = True)(x)\nx = Dropout(0.3)(x)\nx = MaxPooling2D((2,2))(x)\nx = Conv2D(256,(3,3),activation=\"relu\",padding=\"same\",use_bias = True)(x)\nx = Dropout(0.3)(x)\nx = MaxPooling2D((2,2))(x)\nx = GlobalMaxPooling2D()(x)\nx = Dense(256,activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nclass_prediction_layer = Dense(CLASS_OUTPUT,activation=\"softmax\",name=\"CLASS_PREDICTION\")(x)\nrecycling_prediction_layer = Dense(RECYCLING_OUTPUT,activation=\"softmax\",name=\"RECYCLING_PREDICTION\")(x)","89f22cd7":"Model_Multi_Layer = Model(Input_Layer,[class_prediction_layer,recycling_prediction_layer])","ef08d572":"print(Model_Multi_Layer.summary())","49311a27":"plot_model(Model_Multi_Layer, to_file='Model.png', show_shapes=True, show_layer_names=True)","692aafca":"Model_Multi_Layer.compile(optimizer=COMPILE_OPTIMIZER,loss={\"CLASS_PREDICTION\":COMPILE_CLASS_PREDICTION,\n                                                           \"RECYCLING_PREDICTION\":COMPILE_RECYCLING_PREDICTION},\n                         loss_weights=[0.20,1.],\n                         metrics=COMPILE_METRICS)","cb30c6dc":"Main_Model = Model_Multi_Layer.fit(Train_JPG,{\"CLASS_PREDICTION\":Train_Label,\n                                             \"RECYCLING_PREDICTION\":Train_Recycling},\n                                  epochs=EPOCH_PERIOD,\n                                  batch_size=BATCH_SIZE,\n                                  callbacks=[Early_Stopper,Checkpoint_Model,Reduce_Model])","c2aea8c9":"plt.style.use(\"dark_background\")","406be78c":"Grap_Data = pd.DataFrame(Main_Model.history)\nGrap_Data.plot()","5a125e8b":"plt.plot(Main_Model.history[\"CLASS_PREDICTION_loss\"])\nplt.plot(Main_Model.history[\"RECYCLING_PREDICTION_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","fd94a939":"plt.plot(Main_Model.history[\"CLASS_PREDICTION_accuracy\"])\nplt.plot(Main_Model.history[\"RECYCLING_PREDICTION_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","aa38966f":"print(Test_JPG.shape)\nprint(Test_Label.shape)\nprint(Test_Recycling.shape)","09409e91":"Model_Results = Model_Multi_Layer.evaluate(Test_JPG,[Test_Label,Test_Recycling])","39c5cf12":"Test_Prediction_Class = Model_Multi_Layer.predict(Test_JPG[0:10])[0] # this is class(label) prediction","98cdcbf8":"print(Test_Prediction_Class.argmax(axis=-1))","7d256956":"Test_Prediction_Recycling = Model_Multi_Layer.predict(Test_JPG[0:60])[1] # this is Recycling prediction","442dac4d":"print(Test_Prediction_Recycling.argmax(axis=-1))","8de534e0":"fig, axes = plt.subplots(nrows=2,\n                         ncols=5,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(Test_JPG[i])\n    ax.set_title(f\"PREDICTION_CLASS:{Test_Prediction_Class.argmax(axis=-1)[i]}\")\n    ax.set_xlabel(f\"PREDICTION_RECYCLING:{Test_Prediction_Recycling.argmax(axis=-1)[i]}\")\nplt.tight_layout()\nplt.show()","029695ef":"# VISION","c5a60f3e":"#### COMPILE","0a10e474":"#### PREDICTION PROCESS","cb71c004":"#### TRAINING","e66f4a9f":"![](https:\/\/images.theconversation.com\/files\/45737\/original\/k5nfjgm7-1396845167.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=926&fit=clip)","9f1be9ec":"# MODEL","b1bcf80f":"#### CHECKING","8fa2c004":"#### END OF THE PROJECT","91897af3":"#### TO EVALUATE","910f264d":"#### JPG LABELS","686ecc48":"#### TO DEFINE","44d9b673":"#### TO SHUFFLE","4ab14bfb":"# PATH, LABEL, TRANSFORMATION","a24dc247":"# PACKAGES AND LIBRARIES","02465e74":"#### MAIN PATH","d618ec5b":"#### TO DATAFRAME","827d0cb7":"#### PARAMETERS","be216c46":"#### CALLBACK","40f7ca4f":"#### TO SERIES","1b8bae56":"# SPLITTING DATA","5463e57f":"#### STRUCTURE","9715d59e":"#### TO CREATE A NEW COLUMN","81eb2c20":"# USAGE GUIDE\n\n* PACKAGES & LIBRARIES\n* PATH, LABEL & TRANSFORMATION PROCESS\n* VISION FOR EXAMPLE\n* SPLITTING PROCESS\n* DATA TRANSFORMATION FOR MODEL PROCESS\n* MULTI-OUTPUT MODEL PROCESS","acb19e71":"# TRANSFORMATION DATA","d5fca7f3":"# HISTORY\n\n* With the increase in population and urbanization, pollution in nature is also increasing. This dataset was created to raise awareness.\n* Everyone needs to put a stop to this pollution. With Artificial Intelligence models, we can detect this pollution and ensure that it is cleaned.\n\n12 CLASSES\n\n* battery\n\n* biological\n\n* brown_glass\n\n* cardboard\n\n* clothes\n\n* green_glass\n\n* metal\n\n* paper\n\n* plastic\n\n* shoes\n\n* trash\n\n* white_glass","f0184bd7":"#### JPG PATH","7e66d84c":"#### MODEL INFORMATION"}}