{"cell_type":{"126bc59d":"code","cfbddfa9":"code","475ab5af":"code","4ef76c23":"code","f8c84742":"code","801489f7":"code","27255ae7":"code","75035b97":"code","6b1db749":"code","473b9635":"code","c78508b3":"code","dbd3614e":"code","2544d822":"code","f60e0a58":"code","c17053ce":"code","b234a6cd":"code","aceac461":"code","11e42e08":"code","230a84bc":"code","474d6e57":"code","8368dde8":"code","14601fe0":"code","e9d2ebb3":"code","27395773":"code","c95f0da0":"code","6e437461":"code","6e4130e6":"code","0588de5e":"code","7c031ca7":"markdown","88c8de36":"markdown","87b5b5a5":"markdown","eb948c06":"markdown","fc56c049":"markdown","b4ea5440":"markdown","96ab4309":"markdown","d8f82efc":"markdown","04ce34db":"markdown","a1e23a46":"markdown","d42e07d1":"markdown","ad7a46fe":"markdown","bbb81900":"markdown"},"source":{"126bc59d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cfbddfa9":"seed_value= 30\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)","475ab5af":"with open(\"..\/input\/description.txt\", \"r\") as f:\n    print(*f.readlines(), sep=\"\")","4ef76c23":"import pandas as pd\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(train.shape)\nprint(test.shape)","f8c84742":"X_train = train.iloc[:, 1:-1].astype(np.float64)\ny = train.iloc[:, -1]\nX_test = test.iloc[:, 1:].astype(np.float64)","801489f7":"# Fill Inf and NaN\nX_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.mean())\nX_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.mean())","27255ae7":"all_data = pd.concat([X_train, X_test])","75035b97":"categorical = [\n    'cp',\n    'sex',\n    'fbs',\n    'restecg',\n    'exang',\n    'slope',\n    'thal'\n]\nprint(\"--Categorical Features--\")\nfor c in categorical:\n    print(c, set(X_train[c]))\nnumerical = X_train.columns[np.logical_not(X_train.columns.isin(categorical))]\nprint(\"--Numerical Features--\")\nprint(*numerical, sep=\"\\n\")","6b1db749":"# Onehot encoding\n\nall_data = pd.get_dummies(data = all_data, columns = categorical)\nall_data.head()","473b9635":"# Create new features\nall_data['age_thalach'] = all_data['thalach']*(220-all_data['age'])","c78508b3":"# Skewness\nskewed = [\n    'chol', \n    'oldpeak',\n    'thalach'\n]\nc = 1\nfor feature in skewed:\n    if feature in numerical:\n        all_data[feature] = np.log(all_data[feature]+c)","dbd3614e":"X_train = all_data[:X_train.shape[0]]\nX_test = all_data[X_train.shape[0]:]","2544d822":"corr_data = pd.concat([X_train, y], axis=1)\nprint(corr_data.columns)\nplt.figure(figsize=(30,10))\nsns.heatmap(corr_data.corr(),cbar=True,fmt =' .2f', annot=True, cmap='coolwarm')","f60e0a58":"# Drop useless features\n\ndrop_features = [\n    'fbs_0.0',\n    'fbs_1.0'\n]\n\nfor feature in drop_features:\n    X_train.drop(columns=[feature], inplace=True)\n    X_test.drop(columns=[feature], inplace=True)","c17053ce":"X_train.head()","b234a6cd":"sns.countplot(y)\nplt.show()\ncnt_target = y.value_counts()\nclf_thres = cnt_target[1]\/(cnt_target[0]+cnt_target[1])\n# clf_thres = 0.5\nprint(clf_thres)","aceac461":"def clf_result(y_proba, thres=clf_thres):\n    return np.where(y_proba[:,1] > thres, 1, 0)","11e42e08":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, y, test_size=0.2, stratify=y)","230a84bc":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\n\nx_train[:] = scaler.fit_transform(x_train[:])\nx_valid[:] = scaler.transform(x_valid[:])\n\nprint(x_train.shape)\nprint(X_test.shape)\nx_train.head()","474d6e57":"X_test.head()","8368dde8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report\n\nclf = LogisticRegression(C=0.3)\nclf.fit(x_train, y_train)\n\nprint(classification_report(clf_result(clf.predict_proba(x_valid)),y_valid))\n\npd.DataFrame(pd.Series(sorted(clf.coef_.transpose().reshape(-1).tolist(), key=abs, reverse=True), index=X_train.columns))","14601fe0":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nK = 5\n\nmodels = []\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\nC = [\n    0.05,\n    0.3, \n    0.1, \n    0.5,\n    0.1\n]\n\naccs = []\nf1s = []\nweights = np.zeros((1,X_train.shape[1]), dtype=np.float64)\n\nfolds = StratifiedKFold(n_splits = K, shuffle = True, random_state=seed_value)\n\nfor trainIdx, validIdx in folds.split(X_train, y):\n    x_train, x_valid, y_train, y_valid = X_train.iloc[trainIdx], X_train.iloc[validIdx], y.iloc[trainIdx], y.iloc[validIdx]\n    \n    # Normalizing:\n    x_train[:] = scaler.fit_transform(x_train[:])\n    x_valid[:] = scaler.transform(x_valid[:])\n    \n    # Training:\n    clf = LogisticRegression(C=C[len(models)])\n    clf.fit(x_train, y_train)\n    models.append(clf)\n    print(\"\\t\\t\\t--Fold %d evaluation--\"%(len(models)))\n    print(\"\\t\\t\\t\\tC =\",C[len(models)-1])\n    print(classification_report(clf_result(clf.predict_proba(x_valid)),y_valid))\n    accs.append(accuracy_score(clf_result(clf.predict_proba(x_valid)), y_valid))\n    f1s.append(f1_score(clf_result(clf.predict_proba(x_valid)), y_valid))\n    weights = weights + clf.coef_\nweights = (weights\/K)\nprint(\"Average valid accuracy:\", sum(accs)\/K)\nprint(\"Average valid f1 score:\", sum(f1s)\/K)","e9d2ebb3":"pd.DataFrame(pd.Series(sorted(weights.transpose().reshape(-1).tolist(), key=abs, reverse=True), index=X_train.columns), columns=['Coefficient'])","27395773":"# Normalizing test set:\nscaler.fit(X_train[:])\nX_test[:] = scaler.transform(X_test[:])","c95f0da0":"X_test.head()","6e437461":"# Final result:\nfinal_result = sum(model.predict_proba(X_test) for model in models)\/K\nfinal_result = clf_result(final_result)\nsubmit = pd.DataFrame()\nsubmit['ID'] = test['ID']\nsubmit['target'] = final_result.astype(np.int64)\nsns.countplot(x='target', data=submit, palette=\"bwr\")\nplt.show()","6e4130e6":"cnt_pred_target = submit['target'].value_counts()\nprint(cnt_pred_target[1]\/(cnt_pred_target[0]+cnt_pred_target[1]))","0588de5e":"submit.to_csv('VuongLeMinhNguyen.csv', index=False)","7c031ca7":"# Description and Dataset","88c8de36":"# Modeling","87b5b5a5":"## Normalizing","eb948c06":"# Feature Engineering","fc56c049":"## Train test split","b4ea5440":"## Logistic Regression","96ab4309":"## Correlation","d8f82efc":"## Numerical Features","04ce34db":"# Submission","a1e23a46":"## Classification Threshold","d42e07d1":"### Bagging","ad7a46fe":"# Ensemble Modeling","bbb81900":"## Categorical Features"}}