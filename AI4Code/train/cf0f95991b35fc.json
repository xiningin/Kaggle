{"cell_type":{"997d0b2b":"code","8fd00132":"code","5d7d6a1d":"code","4e5e8647":"code","93fdea11":"code","9286d9c9":"code","cb1c5220":"code","a3bb8a04":"code","ed7e2e80":"code","78a069a0":"code","739ad665":"code","ad67b41f":"code","8ba41969":"code","7011ed0e":"code","ea65ac82":"code","63b56e0a":"code","7ce9d3f9":"code","7d4a6dd5":"code","be75612a":"code","40c4a0a7":"code","75789c1d":"code","a6e1a602":"code","6f03210e":"code","16d4e041":"code","2afbe11a":"code","722a7a67":"code","a5d4375c":"code","74cbb2a7":"code","4c24d997":"code","e37c03dd":"code","4bc25ef1":"code","10b34ee0":"code","6bc37c27":"code","4ee5f8b5":"code","99610272":"code","e336f50f":"code","dfc4e864":"code","4be20312":"code","da3550bd":"code","4c3e5d2e":"code","7eaef090":"markdown","01c7fc9f":"markdown","4054c113":"markdown","2a012f2f":"markdown","32d60764":"markdown","64b2f673":"markdown"},"source":{"997d0b2b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8fd00132":"import os\nimport math\n\nimport torch\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig,XLNetForSequenceClassification\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport matplotlib.pyplot as plt\nimport pickle\n%matplotlib inline","5d7d6a1d":"from collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score,accuracy_score","4e5e8647":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","93fdea11":"data = pd.read_csv(\"\/kaggle\/input\/dbpedia-classes\/DBP_wiki_data.csv\")\n#train = pd.read_csv(\"\/content\/drive\/My Drive\/Colab Notebooks\/dbpedia-classes\/DBPEDIA_train.csv\")\n#validation = pd.read_csv(\"\/content\/drive\/My Drive\/Colab Notebooks\/dbpedia-classes\/DBPEDIA_val.csv\")\n#test = pd.read_csv(\"\/content\/drive\/My Drive\/Colab Notebooks\/dbpedia-classes\/DBPEDIA_test.csv\")","9286d9c9":"le = LabelEncoder()\nle.fit(data[\"l1\"])\ndata[\"target\"] = le.transform(data['l1'])","cb1c5220":"device","a3bb8a04":"train , test = train_test_split(data,test_size=0.25,shuffle=True,random_state=42,stratify=data[\"target\"])\ntrain , val = train_test_split(train,test_size=0.1,shuffle=True,random_state=42,stratify=train[\"target\"])","ed7e2e80":"print(len(train),len(test),len(val))\nprint(\"Train \",Counter(train['target']))\nprint(\"Test \",Counter(test['target']))\nprint(\"Validation \",Counter(val['target']))","78a069a0":"#config = XLNetConfig()\n#config.from_pretrained = 'xlnet-large-cased'\n#config.output_hidden_states = True\n#config.output_attentions = False\n#config.summary_type = 'mean'","739ad665":"#tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n#model = XLNetForSequenceClassification(config)","ad67b41f":"#model.config","8ba41969":"#Input_ids = tokenizer.encode_plus(\"Hello, my dog is cute\", add_special_tokens=True) # Batch size 1","7011ed0e":"#input_ids , atten_mask , token_type_ids, labels  = torch.tensor(Input_ids[\"input_ids\"]).unsqueeze(0), torch.tensor(Input_ids[\"attention_mask\"]).unsqueeze(0),torch.tensor(Input_ids[\"token_type_ids\"]).unsqueeze(0),torch.tensor([1]).unsqueeze(0) ","ea65ac82":"#outputs = model(input_ids=input_ids,attention_mask=atten_mask,token_type_ids=token_type_ids,labels=labels)","63b56e0a":"#len(outputs)","7ce9d3f9":"#outputs[2][0].shape","7d4a6dd5":"def CreateData(tokenizer,data):\n    inp_ids = []\n    tok_type_ids = []\n    atten_mask  = []\n    labels = []\n    for i in range(len(data)):\n      text = data.iloc[i][\"text\"]\n      temp = tokenizer.encode_plus(text,max_length=100,pad_to_max_length = True)\n      inp_ids.append(temp[\"input_ids\"])\n      tok_type_ids.append(temp[\"token_type_ids\"])\n      atten_mask.append(temp[\"attention_mask\"])\n      labels.append([data.iloc[i][\"target\"]])\n    \n    input_ids = torch.tensor(inp_ids,dtype=torch.long)\n    attention_mask = torch.tensor(atten_mask,dtype=torch.long)\n    token_type_ids = torch.tensor(tok_type_ids,dtype=torch.long)\n    labels = torch.tensor(labels,dtype=torch.long)\n\n    dataset = TensorDataset(input_ids, attention_mask,token_type_ids,labels)\n    return dataset","be75612a":"def make_weight(data):\n  #data = data.copy()\n  counter = Counter(data[\"target\"].values)\n  print(counter)\n  data[\"Weight\"] = data[\"target\"].apply(lambda x:counter[x])\n  data[\"Weight\"] = 1.0 \/ data[\"Weight\"]\n  return data","40c4a0a7":"#data_v = make_weight(train)","75789c1d":"def train_engine(train_data,val_data,model,batch_sz,lr,epochs,device):\n\n  tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n  train_data = make_weight(train_data)\n  train_dataset = CreateData(tokenizer,train_data)\n  val_dataset = CreateData(tokenizer,val_data)\n  #pickle.dump(open(\"\/content\/drive\/My Drive\/Colab Notebooks\/train_dataset_tokenised.p\",train_dataset))\n  #pickle.dump(open(\"\/content\/drive\/My Drive\/Colab Notebooks\/val_dataset_tokenised.p\",val_dataset))\n  optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-1, correct_bias=False)\n  #scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)\n  weight_sampler = WeightedRandomSampler(weights=train_data[\"Weight\"].values,num_samples=len(train_data[\"Weight\"].values),replacement=True)\n  train_loader = DataLoader(train_dataset,batch_size=batch_sz,sampler=weight_sampler)\n  train_loss = []\n  val_loss = []\n  best_loss = math.inf\n  for epoch in range(epochs):\n    epoch_train_loss = 0\n    model.train()\n    for batch_id,batch in enumerate(train_loader):\n      if batch_id % 7000 == 0:\n          print(epoch,batch_id)\n      optimizer.zero_grad()\n      inputs = {\"input_ids\": batch[0].to(device), \"attention_mask\": batch[1].to(device),  \"token_type_ids\": batch[2].to(device),\"labels\": batch[3].to(device)}\n      #lb = inputs[\"labels\"].squeeze(1).numpy().tolist()\n      #print(Counter(lb),len(Counter(lb)))\n      loss , logits = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'],token_type_ids=inputs['token_type_ids'],labels=inputs['labels'])\n      _ , out_preds = torch.max(logits,axis=1)\n      epoch_train_loss = epoch_train_loss + loss.item()\n      loss.backward()\n      optimizer.step()\n\n    epoch_train_loss = epoch_train_loss \/ (1.0 * len(train_loader))\n    train_loss.append(epoch_train_loss)\n    model.eval()\n    with torch.no_grad():\n      y_true_val = []\n      y_pred_val = []\n      epoch_val_loss = 0.0\n      epoch_val_acc = 0.0 \n      val_loader = DataLoader(val_dataset,batch_size=32)\n      for batch_id,batch in enumerate(val_loader):\n        inputs = {\"input_ids\": batch[0].to(device), \"attention_mask\": batch[1].to(device),  \"token_type_ids\": batch[2].to(device),\"labels\": batch[3].to(device)}\n        loss , logits = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'],token_type_ids=inputs['token_type_ids'],labels=inputs['labels'])\n        epoch_val_loss = epoch_val_loss + loss.item()\n        _ , out_preds = torch.max(logits,axis=1)\n        epoch_val_acc =  epoch_val_acc + torch.eq(out_preds,inputs['labels'].squeeze(1)).sum().item()\n        #print(\"Validation id \",batch_id,batch[3].size(),torch.eq(out_preds,inputs['labels']).sum().item())\n        y_pred_val.extend(out_preds.detach().cpu().numpy().tolist())\n        y_true_val.extend(inputs[\"labels\"].squeeze(1).detach().cpu().numpy().tolist())\n\n      epoch_val_loss = epoch_val_loss \/ (len(val_loader)*1.0)\n      epoch_val_acc = epoch_val_acc \/ len(val_data)\n      val_loss.append(epoch_val_loss)\n      if best_loss > epoch_val_loss :\n        best_loss = epoch_val_loss\n        torch.save({\n                'model_state_dict':model.state_dict(),\n                'optimizer_state_dict':optimizer.state_dict(),\n                'loss':best_loss,},'\/kaggle\/saved_modelv1.pth')\n        \n    target_name = list(le.classes_)\n    print(\"*****************************************************************\")\n    print(\"Validation Report\")\n    print(\"*****************************************************************\")\n    print(classification_report(y_true_val,y_pred_val,target_names=target_name))\n    print(\"*****************************************************************\")\n  \n    print(epoch,train_loss[-1],val_loss[-1],epoch_val_acc,f1_score(y_true_val,y_pred_val,average='weighted'),accuracy_score(y_true_val,y_pred_val))  \n        \n  return model          ","a6e1a602":"def test_engine(model,test_data):\n  tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n  preds_all = []\n  true_all = []\n  test_loss = 0.0\n  test_acc = 0.0 \n  test_dataset = CreateData(tokenizer,test_data)\n  model.eval()\n  with torch.no_grad():\n      test_loader = DataLoader(test_dataset,batch_size=32)\n      for batch_id,batch in enumerate(test_loader):\n        inputs = {\"input_ids\": batch[0].to(device), \"attention_mask\": batch[1].to(device),  \"token_type_ids\": batch[2].to(device),\"labels\": batch[3].to(device)}\n        loss , logits = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'],token_type_ids=inputs['token_type_ids'],labels=inputs['labels'])\n        test_loss = test_loss + loss.item()\n        _ , out_preds = torch.max(logits,axis=1)\n        test_acc =  test_acc + torch.eq(out_preds,inputs['labels'].squeeze(1)).sum().item()\n        preds_all.extend(out_preds.detach().cpu().numpy().tolist())\n        true_all.extend(inputs['labels'].squeeze(1).detach().cpu().numpy().tolist())\n\n      test_acc = test_acc \/ (len(test_data)*1.0)\n      tes_loss = test_loss \/ (1.0*len(test_loader))\n  return preds_all,true_all,test_loss,test_acc","6f03210e":"'''\nseq_len = []\nfor i in range(len(train)):\n  text = train.iloc[i][\"text\"]\n  temp = tokenizer.encode_plus(text)\n  seq_len.append(len(temp[\"input_ids\"]))\n\nprint(np.mean(seq_len))\n'''","16d4e041":"class XLNetClassifier(torch.nn.Module):\n  def __init__(self,labels):\n    super(XLNetClassifier,self).__init__()\n    #self.num_labels = labels\n    #self.config = config\n    self.xlnet_encoder = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased',max_length=100,output_hidden_states=True,summary_type = \"mean\",_num_labels=labels)\n  \n  def forward(self,input_ids,token_type_ids,attention_mask,labels):\n    out = self.xlnet_encoder(input_ids=input_ids,token_type_ids=token_type_ids,\n                             attention_mask=attention_mask,labels=labels)\n    loss , logits = out[:2]\n    return (loss,logits)","2afbe11a":"def main():\n  BATCH_SIZE = 16\n  LR = 2e-5\n  EPOCHS = 1\n  num_labels = len(Counter(train.target.values))\n  model = XLNetClassifier(num_labels)\n  #print(model.config)\n  model = train_engine(train,val,model.to(device),BATCH_SIZE,LR,EPOCHS,device)\n  model_best = XLNetClassifier(num_labels)\n  checkpoint = torch.load(\"\/kaggle\/saved_modelv1.pth\")\n  model_best.load_state_dict(checkpoint[\"model_state_dict\"])\n  model_best.to(device)\n  preds_all,true_all,test_loss,test_acc = test_engine(model_best,test)\n  target_name = list(le.classes_)\n  print(test_acc)\n  print(classification_report(true_all,preds_all,target_names=target_name))\n  pickle.dump(true_all,open(\"Test_True.p\",'wb'))\n  pickle.dump(preds_all,open(\"Test_Preds.p\",'wb'))","722a7a67":"torch.cuda.empty_cache()\n","a5d4375c":"main()","74cbb2a7":"# Some Basic Tests\nz = torch.tensor([[1,4],[3,7],[2,5]])\nz1 = torch.tensor([1,1,0])\nz = z.to(device)\n_ , preds = torch.max(z,axis=1)\nprint(z)\nprint(preds)\nprint(preds.detach().cpu())\nprint(preds)\ntorch.eq(preds,z1.to(device)).sum().item()","4c24d997":"\n1 2.218397746202519 1.7787151649263537 0.11122064440159676 2.2414355885923207 0.2325870646766169 0.05242849427672362","e37c03dd":"config = XLNetConfig()\nconfig.from_pretrained = 'xlnet-base-cased'\nconfig.output_hidden_states = True\nconfig.output_attentions = False\nconfig.summary_type = 'mean'\n#config.n_layer = 12\n#config.n_head = 8\nconfig._num_labels = len(Counter(train.target.values))\nmodel = XLNetClassifier()\nmodel.xlnet_encoder.config.output_hidden_states = True\nmodel.xlnet_encoder.config.summary_type = \"mean\"","4bc25ef1":"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')","10b34ee0":"Input_ids = tokenizer.encode_plus(\"Hello, my dog is cute\", add_special_tokens=True)  # Batch size 1\ninput_ids , atten_mask , token_type_ids, labels  = torch.tensor(Input_ids[\"input_ids\"]).unsqueeze(0), torch.tensor(Input_ids[\"attention_mask\"]).unsqueeze(0),torch.tensor(Input_ids[\"token_type_ids\"]).unsqueeze(0),torch.tensor([1]).unsqueeze(0) ","6bc37c27":"outputs = model(input_ids=input_ids,attention_mask=atten_mask,token_type_ids=token_type_ids,labels=labels)","4ee5f8b5":"for z in model.xlnet_encoder.modules():\n  print(z)","99610272":"t1= torch.tensor([0,1,3,2])\nt2 = torch.tensor([1,0,3,2])","e336f50f":"t2 = t2.view(4,-1)","dfc4e864":"t2.shape","4be20312":"torch.eq(t1,t2.squeeze(1))","da3550bd":"val_dataset = CreateData(tokenizer,val)\nval_loader = DataLoader(val_dataset,batch_size=32)","4c3e5d2e":"(torch.tensor([0,1,1,0,1],dtype=torch.long).sum()\/5.0).item()","7eaef090":"## Starter Code","01c7fc9f":"## Define the Model","4054c113":"## Model Building with Dev Set","2a012f2f":"# Starting Model Building From Here !","32d60764":"## Tester Code","64b2f673":"### Check Sequence Length for Choosing Seq Length"}}