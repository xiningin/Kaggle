{"cell_type":{"047607f3":"code","f5437f69":"code","6ac57407":"code","9329e590":"code","e63b96f4":"code","9af03b8b":"code","548ee65b":"code","9c39d79e":"code","47071afa":"code","eaed40bf":"code","25438c3f":"code","8fe2bf25":"code","a33f5a0f":"code","000e525d":"code","74fa5913":"code","10d80e40":"markdown","283ee721":"markdown","73ffffdf":"markdown","9b570cd2":"markdown","ad37fbdb":"markdown","e74c20bc":"markdown","a0a621aa":"markdown","d241ad87":"markdown","5a43b66a":"markdown","918cfc66":"markdown"},"source":{"047607f3":"!pip install scikit-learn==1.0.2","f5437f69":"!pip install xlrd --ignore-installed --no-deps","6ac57407":"!pip install featurewiz --ignore-installed --no-deps","9329e590":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport pickle\nimport itertools\nimport gc\nimport math\nfrom typing import Tuple, List, Dict\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nimport dateutil.easter as easter\n","e63b96f4":"import featurewiz as FW\nimport xgboost","9af03b8b":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","548ee65b":"%%time\n# read data\nin_kaggle = True\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '..\/input\/tabular-playground-series-jan-2022\/train.csv'\n        test_path = '..\/input\/tabular-playground-series-jan-2022\/test.csv'\n        sample_submission_path = '..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data\/train.csv'\n        test_path = 'data\/train.csv'\n        sample_submission_path = 'data\/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path\n\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\noriginal_train_df = pd.read_csv(train_set_path)\noriginal_test_df = pd.read_csv(test_set_path)\nsubm = pd.read_csv(sample_subm_path)\nprint(original_train_df.shape, original_test_df.shape)\n","9c39d79e":"original_train_df.head()","47071afa":"original_test_df.head()","eaed40bf":"%%time\ndef smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200","25438c3f":"%%time\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)","8fe2bf25":"%%time\n# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    new_df = pd.DataFrame({'year': df.date.dt.year, # This feature makes it possible to fit an annual growth rate\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                           'dec29': (df.date.dt.month == 12) & (df.date.dt.day == 29), # end-of-year peak\n                           'dec30': (df.date.dt.month == 12) & (df.date.dt.day == 30),\n                          })\n\n    # Easter\n    new_df['easter_week'] = False\n    for year in range(2015, 2020):\n        easter_date = easter.easter(year)\n        easter_diff = df.date - np.datetime64(easter_date)\n        new_df['easter_week'] = new_df['easter_week'] | (easter_diff > np.timedelta64(0, \"D\")) & (easter_diff < np.timedelta64(8, \"D\"))\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * df.date.dt.year\n        \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 100): # 100\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\ntest_df.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n\ntest_df['date'] = original_test_df.date\ntest_df['num_sold'] = 0 # An artificial step \n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\n","a33f5a0f":"%%time\ntarget = 'num_sold'\ntrain_best, test_best = FW.featurewiz(train_df, target, corr_limit=0.70, verbose=2, sep=',', \n        header=0, test_data=test_df,feature_engg='', category_encoders='', dask_xgboost_flag=True, nrows=train_df.shape[0])\n","000e525d":"print(test_best.columns.tolist())","74fa5913":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","10d80e40":"# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">Introduction<\/div>\n\nThis notebook is dedicated to detecting the feature importance for this contest with *FeatureWiz*. *FeatureWiz* will leverage its power to effectively select the subset of decisive features to properly interact with the target variable in a matter of minutes.\n\nWhile doing the feature importance case study, we are going to take the inspiration\/insights from @ambrosm who contributed with\n\n- the nice EDA for this dataset conveyed per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n- the feature engineering routines invented per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model","283ee721":"\n# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">Understanding FeatureWiz<\/div>\n\nFeaturewiz is an open-source python library that is an efficient and fast way to find out important variables from a dataset with respect to the target variable. It works on two different techniques which collectively helps in finding out the best features. These techniques are \n\n- SULOV\n- recuirsive XGBoost training\n\nThe subsections below will explain these techniques in more details.\n\n## <div style=\"font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%\">SULOV<\/div>\n\nSearching for the uncorrelated list of variables, this method finds out the pair of variables that are crossing a correlation threshold externally passed and thus are called highly correlated. After finding the pairs it calculates their MIS(Mutual Information Score) which is a quantity that measures the amount of information one can obtain from one random variable given another.\n\nAfter that, it takes into consideration the pair of variables that have the least correlation and highest MIS scores. Which are further processed.\n\n## <div style=\"font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%\">Recursive XGBoost<\/div>\n\nThe variables selected from SULOV are recursively passed through XGboost which helps in determining the best features according to the target variable bypassing the data into smaller datasets that are generated from the whole dataset.\n\nIn such a mannner, it selects the best feature variables from the dataset and that too in few lines of code only.","73ffffdf":"# <div style=\"color:white;background-color:#1d1545;padding:5%;font-size:1.3em;text-align:center;text-shadow:2px 2px 4px black\"> Feature Wizard in Action <\/div>","9b570cd2":"![18xp-wizard-01-superJumbo.jpg](attachment:9e6c5c79-4b5d-4a6c-ace4-642c7c414de3.jpg)","ad37fbdb":"# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">What Does Make FeatureWiz to Be Really Fast and Efficient?<\/div>\n\nIf you review the source code of the latest version of *FeatureWiz*, you will see a lot of features to optimize the product for the speed of processing your data while still focusing on the high accuracy \/ relevance of the feature importance scores. These are\n\n- using dask for parallel processing the dataframes (especially powerful on large datasets where pandas hits its limits)\n- using a number of memory optimization techniques\n- reusing a bunch of AutoML innovations delivered by Alex Lekov (https:\/\/github.com\/Alex-Lekov\/AutoML_Alex\/tree\/master\/automl_alex) - that is, DataBunch and encoders modules which are used by the tool (although with some modifications).\n- using Category Encoders library in Python (https:\/\/contrib.scikit-learn.org\/category_encoders\/index.html)\n\nSpeed of processing, combined with the relevance of the feature importance scores calculated, makes *FeatureWiz* a unique tool in the industrial setup where the time to market with the discoveries is as important as the relevance of the findings\/insights you as a Data Scientist come up with.\n\n**Note:** Someone could claim the size of the dataset for this competition is not that big, and therefore the *FeatureWiz*'s speed of processing (doing the feature importance scoring in less then 5 min) is not indicative. In such a case, I would like to mention my benchmarking for a way bigger dataset for Dec 2021 TPC. For that dataset, the feature importance calculations facilitated by *FeatureWiz* took about 22 min whereas the alternative feature importance  experiment (permutative feature importance with ELI5, per https:\/\/www.kaggle.com\/gvyshnya\/eli5-perm-fi-with-additional-fe-dec-21-tpc) took about 3.5 h to go through.","e74c20bc":"\n# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">Installation of FeatureWiz<\/div>\n\n*FeatureWiz* is not the part of the standard Python packages set enabled in the Docker container used by Kaggle notebooks. Therefore we will have to install it into the notebook interactively.\n\n**Note:** Due to the the recuorsive dependencies loop in the recent pip versions, we are going to specifically install some important dependencies manually.","a0a621aa":"# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">References<\/div>\n\n\n- Github repo of FeatureWiz: https:\/\/github.com\/AutoViML\/featurewiz\n- the nice EDA for this dataset conveyed per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n- the feature engineering routines invented per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\n- the cover image reused from https:\/\/www.nytimes.com\/2021\/10\/18\/world\/australia\/christchurch-new-zealand-wizard.html","d241ad87":"# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">Running Feature Selection Experiment<\/div>","5a43b66a":"As we can see, the 'magic of FeatureWiz' resulted in\n\n- dropping the highly correlated features ('dayofyear', 'sin1', 'cos1', 'sin12', 'date_is_festive', 'date_is_warm', 'date_year', 'date_dayofyear', 'date_weekofyear') from the original list of 608 features we engineered for our dataset with SULOV\n- identifying the subset of 198 important features (out of 696 features remained after the SULOV probes stage)\n\nNow we are going to review the list of features suggested to be important by *FeatureWiz*.","918cfc66":"# <div style=\"color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center\">FeatureWiz Takeaways<\/div>\n\n*FeatureWiz* was designed for selecting High Performance variables with the fewest steps.\n\nIn most cases, *FeatureWiz* builds models with 20%-99% fewer features than your original data set with nearly the same or slightly lower performance.\n\nIn essence, you can thing of *FeatureWiz* to be a feature wizard that will:\n\n- **Automatically pre-process data**: you can send in your entire dataframe \"as is\" and featurewiz will classify and change\/label encode categorical variables changes to help XGBoost processing. It classifies variables as numeric or categorical or NLP or date-time variables automatically so it can use them correctly to model.\n- **Perform feature engineering automatically**: The ability to create \"interaction\" variables or adding \"group-by\" features or \"target-encoding\" categorical variables is difficult and sifting through those hundreds of new features is painstaking and left only to \"experts\". Now, with featurewiz you can create hundreds or even thousands of new features with the click of a mouse. This is very helpful when you have a small number of features to start with. However, be careful with this option. You can very easily create a monster with this option.\n- **Perform feature reduction automatically**. When you have small data sets and you know your domain well, it is easy to perhaps do EDA and identify which variables are important. But when you have a very large data set with hundreds if not thousands of variables, selecting the best features from your model can mean the difference between a bloated and highly complex model or a simple model with the fewest and most information-rich features. *FeatureWiz* uses XGBoost repeatedly to perform feature selection. You must try it on your large data sets and compare!\n- **Explain SULOV method graphically** using networkx library so you can see which variables are highly correlated to which ones and which of those have high or low mutual information scores automatically. Just set verbose = 2 to see the graph.\n- **Build a fast LightGBM model** using the features selected by featurewiz. There is a function called \"simple_lightgbm_model\" which you can use to build a fast model (note that the latter capability has been demonstrated in this notebook)"}}