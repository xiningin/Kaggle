{"cell_type":{"df3e7e13":"code","8676f025":"code","1823f573":"code","9e79e378":"code","e7a99a2d":"code","7b8978f3":"code","4c2cefbd":"code","9ce5bae7":"code","9a88c37a":"code","675ca524":"code","24538a32":"code","d685627a":"code","f3f02f79":"code","a2ca2a64":"code","af86df6e":"code","8fb1e8e1":"code","b56add7f":"code","c2276219":"code","04719d50":"code","a978bf48":"code","a9cfe1f4":"code","69e70d8d":"code","705851e2":"code","8e979b77":"code","18fac9d9":"code","126baed5":"code","79b0b2b9":"code","7bcd8943":"code","6e265791":"code","f9bf1832":"code","03099111":"code","6ec18b34":"code","d0174cfb":"code","d6e00d0c":"code","ea78595d":"code","b807035b":"markdown","38c7f17c":"markdown","53f1b372":"markdown","d805854a":"markdown","0282fe55":"markdown","21659001":"markdown","593431a4":"markdown","a213e74b":"markdown","b51f8801":"markdown","edb5c513":"markdown","dff6e969":"markdown","03800bf6":"markdown","647a522d":"markdown","4be7f9ae":"markdown","51002068":"markdown","1202b20e":"markdown","8265b032":"markdown","98410dcc":"markdown"},"source":{"df3e7e13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8676f025":"# !pip uninstall scikit-learn \n!pip install scikit-learn==0.24.1","1823f573":"import sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))","9e79e378":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport theano\nimport tensorflow\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\nfrom sklearn.experimental import enable_halving_search_cv  \nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.model_selection import HalvingRandomSearchCV","e7a99a2d":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv\")","7b8978f3":"summary = pd.DataFrame(train_df.dtypes, columns = [\"Dtypes\"])\nsummary[\"Null\"] = train_df.isnull().sum()\nsummary[\"first\"] = train_df.iloc[0]\nsummary[\"second\"] = train_df.iloc[1]\nsummary[\"third\"] = train_df.iloc[2]\nsummary[\"unique\"] = train_df.nunique()\nprint(len(train_df))\nprint(len(test_df))","4c2cefbd":"train_df","9ce5bae7":"summary","9a88c37a":"train_df.columns","675ca524":"train_df['Survived'].value_counts()\n# To get the idea how the data is dustributed. what if it's unbalanced.","24538a32":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# To get the idea of how the chance to survive is related to the sibsp, lower the number of sibsp higher the chance of survival but not really\n\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# To get the chance of survival based on the sex, females have comparatively higher chances than men.\n\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# To check the correlation between the parents\/children to chances of survival.\n\ntrain_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# To check the correlation between the Embarked and chances of survival.\n","d685627a":"train_copy = train_df.copy()# made a copy of train and test sets\ntest_copy = test_df.copy()\n\ntrain = train_df.drop(['PassengerId', 'Ticket'], axis = 1) # Dropping the unncessary column\ntest = test_df.drop(['PassengerId', 'Ticket',], axis = 1)\n\ncombine = [train, test] # combining both training and test set.\n","f3f02f79":"train['Cabin'].fillna('U', inplace=True) # filling the null values with U i,e undefined\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0]) # filling all the values with the first alphabet that appeared!\n\ntest['Cabin'].fillna('U', inplace=True)\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\n\ntrain['Cabin'].unique()\nfor dataset in combine:\n    dataset['Cabin'] = dataset['Cabin'].fillna('U')\n    dataset['Cabin'] = dataset['Cabin'].apply(lambda x: x[0])","a2ca2a64":"pd.crosstab(train['Cabin'], train['Survived'])\n# To get the frequency between cabin and survived\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = False).mean().sort_values(by = 'Survived', ascending = True)\n# Get the correlarion between both and cabin and Survived columns\ncabin_mapping = {\"T\": 0, \"U\": 1, \"A\": 2, \"G\": 3, \"C\": 4, \"F\": 5, \"B\": 6, \"E\": 7, \"D\": 8}\n\nfor dataset in combine:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n    dataset['Cabin'] = dataset['Cabin'].fillna(0)","af86df6e":"\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\n\npd.crosstab(train['Title'], train['Sex'])\n  \nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","8fb1e8e1":"train.isnull().sum()\ntest.isnull().sum()","b56add7f":"train['Age'].fillna(train['Age'].dropna().median(), inplace=True) # With median\ntest['Age'].fillna(train['Age'].mean(), inplace = True)# With mean\n\ntest['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)# With median\ntrain['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\n\ntrain['Embarked'].fillna('C', inplace = True)\ntest['Embarked'].fillna('C', inplace = True)\n\ntrain.drop('Title',axis=1,inplace=True) # dropping the title column\ntest.drop('Title',axis=1,inplace=True) ","c2276219":"train.isnull().sum()\ntest.isnull().sum()","04719d50":"train['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","a978bf48":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 0","a9cfe1f4":"dataset.Age.unique()","69e70d8d":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","705851e2":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","8e979b77":"train['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain = train.drop(['FareBand'], axis=1)\ncombine = [train, test]","18fac9d9":"train = train.drop(['AgeBand', 'Name', 'SibSp', 'Parch' ], axis = 1)\ntest = test.drop(['Name', 'SibSp', 'Parch'], axis = 1)","126baed5":"x_train = train.drop('Survived', axis = 1)\ny_train = train.Survived\n\nx_test = test","79b0b2b9":"horizontal_stack = pd.concat([x_train, y_train], axis=1)\nhorizontal_stack.index+=1\ntrain_df=horizontal_stack\n","7bcd8943":"x_test.index+=100000\ntest_df=x_test","6e265791":"test_df","f9bf1832":"y=train_df['Survived']\nX=train_df.drop('Survived', axis =1)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=101, test_size=0.25)","03099111":"def build_classifier(optimizer):\n    classifier = Sequential()\n    \n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=7))\n    classifier.add(Dropout(rate = 0.2))\n    \n    classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n    \n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    \n    #Optimizer and Compilation\n    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    \n    return classifier","6ec18b34":"%%time\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(optimizer = ['Adam'],\n                  epochs=[50,100,200],\n                  batch_size=[16,32,64])\ngrid = HalvingRandomSearchCV(classifier, param_grid, scoring='accuracy', cv=10, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\nbest_parameters = grid.best_params_\nbest_accuracy = grid.best_score_","d0174cfb":"print(grid.best_params_)\nprint(grid.best_score_)","d6e00d0c":"classifier = KerasClassifier(build_fn = build_classifier,\n                             optimizer=best_parameters['optimizer'],\n                             batch_size=best_parameters['batch_size'],\n                             epochs=best_parameters['epochs'])\n\nclassifier.fit(X_train,y_train)\npreds = classifier.predict(test_df)","ea78595d":"y_final = (preds > 0.5).astype(int).reshape(test_df.shape[0])\n\noutput = pd.DataFrame({'PassengerId': test_df.index, 'Survived': y_final})\noutput.to_csv('pred_ann.csv', index=False)","b807035b":"### Sanity check","38c7f17c":"# Model Building","53f1b372":"Description of Data\n\n1. <b>survival: Survival 0 = No, 1 = Yes\n1. <b>  pclass: Ticket class1 = 1st, 2 = 2nd, 3 = 3rd\n1. <b>  sex: Sex\n1. <b>  Age: Age in years\n1. <b> sibsp: # of siblings \/ spouses aboard the Titanic\n1. <b>  parch: # of parents \/ children aboard the Titanic\n1. <b>  ticket: Ticket number\n1. <b>  fare: Passenger fare\n1. <b> cabin: Cabin number\n1. <b> embarked: Port of Embarkation <\/b>","d805854a":"### Filling Null values and modifying values","0282fe55":"# Reading Data","21659001":"### Getting target variable and splitting the data","593431a4":"### Checking the correlation","a213e74b":"<b>After so much of things that we did, at this point it is okay to get lost of what are we trying to acheive here, so I tried my way to describe what all we have done through up until now.\n\n<b> I hope this is helpful to you as well.\n\n1. So at first we look at the files we have at our disposal.\n1. Then we imported all the libraries up until now.\n1. We read the data\n1. We summarized the data to get more familiar with data, with how many null values are there and what data types of values are there, we also looked at the number of unique values of each column.\n1. Then we went through the steps of Feature Engineering where in \n1. We first had a look at whether our training var is skewed or normalized.\n2. We checked the correlation between few columns to get more familiar with them.\n3. We then made the copy of our original dataset.\n4. We dropped unnecessary columns, i.e passengerId and Ticket.\n5. We then imputed the null values with U for cabin and also modified its values.\n6. Later we then checked the correlation of cabin with survived column.\n7. The reason why we have a combine dataset because it will make things easier if we want to change something which will hold true for both Train and Test set.\n8. We then mapped the name column with its first name and did the change for both train and test set.\n9. We looked the frequency table between newly created Title column which holds the first name of the Name col and then look at the correlation between it and Sex column which did tell much information.\n10. Categorised the Sex column\n11. Imputed missing values for all the columns which has missing values. \n12. Dropped the Title column.\n13. Discretized into bins for column Age and created a new column, AgeBand and checked its corerlation\n14. Added a new feature name FamilySize and checked its correlation.\n15. Categorised the Embarked feature.\n16. Discretized the Fare feature and checked its correlation and dropped the Fareband feature created while discretizing.\n17. Dropped the Name, Sibsp and Parch feature.\n18. Get the target feature and remove it from train data.\n19. Get the horizontal_Stack and then get the test_df \n20. Split the data","b51f8801":"### Discretizing into bins","edb5c513":"### Categorising the Data","dff6e969":"# Imputing Missing Values\n","03800bf6":"### Creating frequency table for title and sex column","647a522d":"# Feature Engineering","4be7f9ae":"### Checking correlation and creating frequency table","51002068":"We can see that there are total of 100k rows in both train and test data.","1202b20e":"### Making copy of original dataset and dropping the unnecessary columns.","8265b032":"There are few null values in Ticket till Embarked column. There are names which are repetetive.","98410dcc":"This kernel is just more explanied version of the kernel https:\/\/www.kaggle.com\/pranjalverma08\/tps-april-21-the-ann-approach-score-80-782 .\n\nFor me I liked this kernel and wanted to get the feel of what is going on."}}