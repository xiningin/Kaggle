{"cell_type":{"9a4cf16c":"code","5d33029a":"code","a344c9c1":"code","23720feb":"code","f5fb343f":"code","5c1d535d":"code","7e97ca30":"code","0168304f":"code","20d01250":"code","f9eeb272":"code","6323dafe":"code","cd6eb267":"code","01f3aeb9":"code","535984db":"code","156f6fac":"code","bfa4ae09":"code","da92260d":"markdown","a09185b6":"markdown","ae354063":"markdown","9d68ba17":"markdown","7d28c707":"markdown","9a13dc5c":"markdown"},"source":{"9a4cf16c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport IPython.display as ipd","5d33029a":"# First we are going to read the train and validation scp files\ntrain_file_path = \"..\/input\/covid\/train\/wav.scp\"\nvalidation_file_path = \"..\/input\/covid\/valid\/wav.scp\"\ntest_file_path = \"..\/input\/covid\/test\/wav.scp\"","a344c9c1":"train_data = []\n\nwith open(train_file_path) as f:\n    \n    lines = f.read().splitlines()\n    \n    for line in tqdm(lines):\n        info = line.split(\" \")\n        label = info[0][-3:]\n        wav_path = info[1]\n        \n        if \"cough\" in wav_path or \"coughs\" in wav_path: #coughs\n            info_name = \"Cough\"\n        elif \"breath\" in wav_path or \"breaths\" in wav_path or \"breathing\" in wav_path: # breath, breaths, breathing\n            info_name = \"Breath\"\n        elif \"speech\" in wav_path:  # speech \n            info_name = \"Speech\"\n        else:\n            info_name = \"Unknown\"\n        \n        x , sr = librosa.load(\"..\/input\/covid\/wavs16k\/\"+wav_path)\n        duration = len(x)\/sr\n        \n        train_data.append([\"Train\",label,info_name,wav_path,sr,duration])","23720feb":"validation_data = []\n\nwith open(validation_file_path) as f:\n    \n    lines = f.read().splitlines()\n    \n    for line in tqdm(lines):\n        info = line.split(\" \")\n        label = info[0][-3:]\n        wav_path = info[1]\n        \n        if \"cough\" in wav_path or \"coughs\" in wav_path: #coughs\n            info_name = \"Cough\"\n        elif \"breath\" in wav_path or \"breaths\" in wav_path or \"breathing\" in wav_path: # breath, breaths, breathing\n            info_name = \"Breath\"\n        elif \"speech\" in wav_path:  # speech \n            info_name = \"Speech\"\n        else:\n            info_name = \"Unknown\"\n            \n        x , sr = librosa.load(\"..\/input\/covid\/wavs16k\/\"+wav_path)\n        duration = len(x)\/sr\n        \n        validation_data.append([\"Validation\",label,info_name,wav_path,sr,duration])","f5fb343f":"dataset = train_data + validation_data\ndata_df = pd.DataFrame(data=dataset, columns=['Set','Label','Type','Wav_path','Sample rate','Duration'])\ndisplay(data_df.head())","5c1d535d":"data_df.info()","7e97ca30":"sns.displot(data=data_df, x=\"Label\", hue=\"Set\", multiple=\"stack\", shrink=.8)","0168304f":"sns.violinplot(data=data_df, x='Label', y='Duration', hue='Set', split=True)","20d01250":"sns.displot(data=data_df, x=\"Type\", hue=\"Set\", multiple=\"stack\", shrink=.8)","f9eeb272":"sns.displot(data=data_df, x=\"Type\", hue=\"Label\", multiple=\"stack\", shrink=.8)","6323dafe":"sns.violinplot(data=data_df, x='Type', y='Duration', hue='Set', split=True)","cd6eb267":"# Select one random pos sample\nsample = data_df[data_df['Label']=='pos'].sample()\nsample_audio_path = sample.iloc[0]['Wav_path']\ndisplay(sample)\n\nx , sr = librosa.load(\"..\/input\/covid\/wavs16k\/\"+sample_audio_path)\nlibrosa.display.waveplot(x, sr=sr)\n\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\nipd.Audio(\"..\/input\/covid\/wavs16k\/\"+sample_audio_path)\n","01f3aeb9":"# Select one random neg sample\nsample = data_df[data_df['Label']=='neg'].sample()\nsample_audio_path = sample.iloc[0]['Wav_path']\nsr = sample.iloc[0]['Sample rate']\ndisplay(sample)\n\nx , sr = librosa.load(\"..\/input\/covid\/wavs16k\/\"+sample_audio_path)\nlibrosa.display.waveplot(x, sr=sr)\n\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\nipd.Audio(\"..\/input\/covid\/wavs16k\/\"+sample_audio_path)\n","535984db":"test_data = []\n\nwith open(test_file_path) as f:\n    \n    lines = f.read().splitlines()\n    \n    for line in tqdm(lines):\n        info = line.split(\" \")\n        wav_path = info[1]\n        \n        if \"cough\" in wav_path or \"coughs\" in wav_path: #coughs\n            info_name = \"Cough\"\n        elif \"breath\" in wav_path or \"breaths\" in wav_path or \"breathing\" in wav_path: # breath, breaths, breathing\n            info_name = \"Breath\"\n        elif \"speech\" in wav_path:  # speech \n            info_name = \"Speech\"\n        else:\n            info_name = \"Unknown\"\n        \n        x , sr = librosa.load(\"..\/input\/covid\/wavs16k\/\"+wav_path)\n        duration = len(x)\/sr\n        \n        test_data.append([\"Test\",info_name,wav_path,sr,duration])","156f6fac":"test_dataset = test_data\ndata_df = pd.DataFrame(data=test_dataset, columns=['Set','Type','Wav_path','Sample rate','Duration'])\ndisplay(data_df.head())","bfa4ae09":"sns.displot(data=data_df, x=\"Type\", hue=\"Set\", multiple=\"stack\", shrink=.8)","da92260d":"# Exploratory data analysis\n\nThe competition database is a balanced selection of recordings from 3 different databases:\n\n- Coswara\n- Cambridge\n- Coughvid\n\nInput data files are available in the read-only \"..\/input\/\" directory. The samples are stored in wav16 folder while the splits of the data are stored in train\/validation\/test folders inside a *.scp files. The labels of the validation and train datasets are stored in text files inside train\/validation folders.\n\n## Loading the data\n\nThe first step is loading the data:","a09185b6":"## Checking the test set","ae354063":"## Visualizing the data\n\nNow that we have the data loaded, we can start to extract some insights from the dataset.\n\n### Data by label","9d68ba17":"## Listening to the samples\n\nWe can check positive and negative random samples of the different sets of the dataset to manually inspect the data. After listening to several samples I have noticed that a lot of samples are not clean: speech dataset samples have a lot of background audio, breathing samples have a lot of noise if the person makes a deep breath near the microphone...\n\nThe types of samples are very different between them. Perhaps a good approach to improve the results is to have an ensemble of models, one for each type of sample.","7d28c707":"### Data by sample type","9a13dc5c":"Although some samples have relevant information in their name: type and subtype of the sample, sample hash, sample number etc not all the samples follow the same structure. A simple way to retrieve the sample type (speech, cough, breathing...) is to check if certain keywords are in the audio file name. Running these cells can take a little time, around 7 minutes for the training set and 2 minutes for the validation set."}}