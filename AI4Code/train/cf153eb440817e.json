{"cell_type":{"fed9fbd8":"code","07431ed0":"code","2236a5bf":"code","887f85ea":"code","3e21f903":"code","e1bb2615":"code","22904183":"code","2d93a16b":"code","1e310e64":"code","8bde33e1":"code","7e7ae4b4":"code","0fb1dacd":"code","b2da37ae":"code","7dae5b9d":"code","5ecb1156":"code","bdfc46a9":"code","790d9a96":"code","45e8e0ea":"code","20d0d33e":"code","034a40f6":"code","acbc402d":"code","87a0d544":"code","cac5e478":"code","9a18e1ee":"code","e5a09e3e":"code","f1040dc3":"code","238dc6cd":"code","3d1c343d":"code","928463ff":"code","441e1f68":"code","714e776f":"code","b4dae876":"code","e39f6177":"code","6b773ffd":"code","b6c7455d":"code","09053673":"code","0eabc9d6":"code","4de29f84":"markdown","b02a854b":"markdown","b5e150d0":"markdown","fc62598f":"markdown"},"source":{"fed9fbd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07431ed0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n#plotly\nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px","2236a5bf":"df=pd.read_csv('..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndf.head().style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen',\n                           'border-color': 'white'})","887f85ea":"df.describe().T.style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen',\n                           'border-color': 'white'})","3e21f903":"msno.bar(df)","e1bb2615":"dat=ff.create_table(df.head(10))\npy.iplot(dat)","22904183":"plt.rcParams['figure.figsize'] = (9, 9)\nlabels=['Yes', 'No']\nplt.pie(df['default.payment.next.month'].value_counts(), explode=(0,0.1), labels=labels,autopct='%1.1f%%', shadow=True)\nplt.title('Default Payement ', fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","2d93a16b":"col = \"default.payment.next.month\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\nlayout = go.Layout(title=\"\", height=600, legend=dict(x=0.1, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","1e310e64":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, precision_recall_curve","8bde33e1":"X=df.drop(['default.payment.next.month'], axis=1)\ny=df['default.payment.next.month']","7e7ae4b4":"X_train, X_test, y_train, y_test=train_test_split(X , y, test_size=0.3, random_state=7)\n","0fb1dacd":"lr=LogisticRegression()\nlr.fit(X,y)","b2da37ae":"y_pred=lr.predict(X_train)\n","7dae5b9d":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","5ecb1156":"# Predicting the Test data with model \ny_test_pred=lr.predict(X_test)","bdfc46a9":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","790d9a96":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]\n\nprint(\"Confusion Matrix\", cfm)\nprint(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","45e8e0ea":"print(\"correct prediction\", \n      round((trueNegative+truePositive)\/len(y_test_pred)*100, 1),'%')","20d0d33e":"cfm_df=pd.DataFrame(cfm, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(cfm_df, cmap='Reds', annot=True)\nplt.show()","034a40f6":"disp = ConfusionMatrixDisplay(confusion_matrix=cfm,display_labels=lr.classes_)\n\ndisp.plot()","acbc402d":"print(classification_report(y_test, y_test_pred))\n","87a0d544":"y_test_pred_prob=lr.predict_proba(X_test)[:,1]\n","cac5e478":"y_test_pred_prob\n","9a18e1ee":"metrics.roc_auc_score(y_test, y_test_pred_prob)\n","e5a09e3e":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\n","f1040dc3":"plt.figure(figsize=(15,10),facecolor='grey')\nplt.plot([0,1],[0,1],'k--')\nax = plt.axes()\n  \n# Setting the background color of the plot \n# using set_facecolor() method\nax.set_facecolor(\"grey\")\nplt.plot(fpr, tpr, label='Logistic Regression',linestyle='--')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","238dc6cd":"plt.figure(figsize=(15,10),facecolor='grey')\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(X_test)[:,1]\nplt.figure(figsize=(10,8))\nax = plt.axes()\n  \n# Setting the background color of the plot \n# using set_facecolor() method\nax.set_facecolor(\"grey\")\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.grid()\n\nplt.show()","3d1c343d":"from sklearn.metrics import log_loss\nLogLoss = log_loss(y_train, y_pred, eps = 1e-15,\n    normalize = True, sample_weight = None, labels = None)","928463ff":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n","441e1f68":"df.columns","714e776f":"df=df.rename(columns={'default.payment.next.month':'default'})","b4dae876":"df.columns","e39f6177":"formula='default ~ LIMIT_BAL+SEX+EDUCATION+PAY_0+PAY_2+PAY_3+PAY_4+PAY_5+PAY_6+BILL_AMT1+BILL_AMT2+BILL_AMT3+BILL_AMT4+BILL_AMT5+BILL_AMT6'","6b773ffd":"model = smf.glm(formula = formula, data=df, family=sm.families.Binomial())\nresult = model.fit()\nprint(result.summary())","b6c7455d":"predictions = result.predict()\nprint(predictions[0:10])","09053673":"prediction=pd.DataFrame(predictions)","0eabc9d6":"prediction.head(100).plot()","4de29f84":"# glm()\n* The glm() function fits generalized linear models, a class of models that includes logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family=sm.families.Binomial() in order to tell python to run a logistic regression rather than some other type of generalized linear model.","b02a854b":"* The predict() function can be used to predict the probability that the market will go down, given values of the predictors. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model.","b5e150d0":"# Spliting the dataset into train and test model","fc62598f":"# Logistic Regression"}}