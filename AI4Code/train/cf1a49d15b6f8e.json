{"cell_type":{"02e30d04":"code","9c76e1c8":"code","1efd588c":"code","1f4a7f3a":"code","14e11deb":"code","7d33c4d2":"code","8378961a":"code","aa1642a0":"code","6f067fb5":"code","45b1c5a6":"code","6d58c467":"code","244c1151":"code","35885dca":"code","74fad1a6":"code","d4aa9365":"code","2c45750c":"code","71d4979a":"code","2d4a9a3b":"code","214d1079":"code","43072f04":"code","696d5af0":"code","0e609214":"code","99e6b219":"code","a1db2434":"code","ba2a4da0":"code","a608aa18":"code","95770409":"code","42bc9a94":"code","b2fa5ab9":"code","5b78dc58":"code","61f68c19":"code","8e94c830":"code","014932a3":"code","3ff2ee32":"code","99da3632":"code","ca47721a":"code","4140aa34":"markdown","724125c7":"markdown","d86b8657":"markdown","a8019ce0":"markdown","b5ac6bc9":"markdown","c3fdbd08":"markdown","340fdc57":"markdown","e4e77123":"markdown","35a9df68":"markdown","e799e656":"markdown","fa04b3de":"markdown","c45a7df8":"markdown","ed447ce6":"markdown","f8421e1a":"markdown","55dcd595":"markdown","4fd0e755":"markdown","bb075555":"markdown","ab9eb85a":"markdown","0d52b3d5":"markdown","c4f1a4d2":"markdown","32196b69":"markdown","1b8f3871":"markdown","6fef0e36":"markdown","1502bb50":"markdown","30034a8c":"markdown","03540ede":"markdown","2f4961e6":"markdown","e0a89903":"markdown","e93001db":"markdown","4ed454a0":"markdown","5766f26c":"markdown","c655bbba":"markdown","77968eb2":"markdown","d96ca055":"markdown","ac81fbb8":"markdown","ffc49e97":"markdown"},"source":{"02e30d04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom sklearn.metrics import f1_score\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import *\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.set_option('display.max.rows',5000)\npd.set_option('display.max.columns',5000)\n\n# Any results you write to the current directory are saved as output.","9c76e1c8":"os.listdir('..\/input\/')","1efd588c":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","1f4a7f3a":"train.head()","14e11deb":"test.head()","7d33c4d2":"train.drop(columns=['loan_id'],inplace=True)\ntest.drop(columns=['loan_id'],inplace=True)","8378961a":"train.isnull().sum()","aa1642a0":"test.isnull().sum()","6f067fb5":"wedge = [train['m13'].value_counts()[0],train['m13'].value_counts()[1]]\n\nperc = [train['m13'].value_counts()[0]\/len(train),\n        train['m13'].value_counts()[1]\/len(train)\n       ]\nplt.pie(wedge,labels=['Normal - '+ format(perc[0]*100, '.2f') + '%','Anomaly - '],\n        shadow=True,radius = 2.0)","45b1c5a6":"def origin_dateYear(x):\n    year = x.split('-')[0]\n    return year\n\ndef origin_dateMonth(x):\n    month = x.split('-')[1]\n    return month\n\ndef origin_dateDay(x):\n    day = x.split('-')[2]\n    return day\n\ndef first_paymentMonth(x):\n    month = x.split('\/')[0]\n    return month\n\ndef first_paymentYear(x):\n    year = x.split('\/')[1]\n    return year\n\ndef number_of_borrowers(x):\n    if(x==1):\n        return \"One\"\n    else:\n        return \"Two\"\n\n\ndef origin_dateYearTest(x):\n    year = x.split('\/')[0]\n    return year\n\ndef origin_dateMonthTest(x):\n    month = x.split('\/')[1]\n    return month\n\ndef origin_dateDayTest(x):\n    day = x.split('\/')[2]\n    return day\n\ndef first_paymentMonthTest(x):\n    month = x.split('-')[0]\n    return month\n\ndef first_paymentYearTest(x):\n    year = x.split('-')[1]\n    return year\n\ndef number_of_borrowersTest(x):\n    if(x==1):\n        return \"One\"\n    else:\n        return \"Two\"\n    \n\ntrain['origin_Year'] = train['origination_date'].apply(origin_dateYear)\ntrain['origin_Month'] = train['origination_date'].apply(origin_dateMonth)\ntrain['origin_Day'] = train['origination_date'].apply(origin_dateDay)\n\ntrain['firstPayMonth'] = train['first_payment_date'].apply(first_paymentMonth)\ntrain['firstPayYear'] = train['first_payment_date'].apply(first_paymentYear)\n\ntrain['numBorrowers'] = train['number_of_borrowers'].apply(number_of_borrowers)\n\ntest['origin_Year'] = test['origination_date'].apply(origin_dateYear)\ntest['origin_Month'] = test['origination_date'].apply(origin_dateMonthTest)\ntest['origin_Day'] = test['origination_date'].apply(origin_dateDayTest)\n\ntest['firstPayMonth'] = test['first_payment_date'].apply(first_paymentMonthTest)\ntest['firstPayYear'] = test['first_payment_date'].apply(first_paymentYearTest)\n\ntest['numBorrowers'] = test['number_of_borrowers'].apply(number_of_borrowersTest)","6d58c467":"train['months'] = train['loan_term']\/30\ntest['months'] = test['loan_term']\/30\n\ntrain['unpaid_per_month'] = train['unpaid_principal_bal']\/train['months']\ntest['unpaid_per_month'] = test['unpaid_principal_bal']\/test['months']\n\ntrain['approxIncome'] = train['unpaid_principal_bal']\/train['debt_to_income_ratio']\ntest['approxIncome'] = test['unpaid_principal_bal']\/test['debt_to_income_ratio']\n\ntrain['approxValue'] = train['unpaid_principal_bal']\/train['loan_to_value']\ntest['approxValue'] = test['unpaid_principal_bal']\/test['loan_to_value']\n\ntrain['payPerPerson'] = train['unpaid_principal_bal']\/train['number_of_borrowers']\ntest['payPerPerson'] = test['unpaid_principal_bal']\/test['number_of_borrowers']\n\ntrain['selfPercent'] = 100 - train['insurance_percent']\ntest['selfPercent'] = 100 - test['insurance_percent']\n\ntrain['uncoveredLoan'] = (train['selfPercent']\/100) * train['unpaid_principal_bal']\ntest['uncoveredLoan'] = (test['selfPercent']\/100) * test['unpaid_principal_bal']\n\ntrain['totalDelinquency'] = train['m1'] + train['m2'] + train['m3'] + train['m4'] + train['m5'] + train['m6'] + train['m7'] + train['m8'] + train['m9'] + train['m10'] + train['m11'] + train['m12'] \ntest['totalDelinquency'] = test['m1'] + test['m2'] + test['m3'] + test['m4'] + test['m5'] + test['m6'] + test['m7'] + test['m8'] + test['m9'] + test['m10'] + test['m11'] + test['m12'] \n\ntrain['delinqProb'] = train['totalDelinquency']\/12\ntest['delinqProb'] = test['totalDelinquency']\/12","244c1151":"def convert(x):\n    if(x=='Apr'):\n        return 4\n    elif(x=='Mar'):\n        return 3\n    elif(x=='May'):\n        return 5\n    else:\n        return 2\n\ntest['firstPayMonth'] = test['firstPayMonth'].apply(convert) ","35885dca":"train['intMulUnpaid'] = train['interest_rate'] * train['unpaid_principal_bal']\ntest['intMulUnpaid'] = test['interest_rate'] * test['unpaid_principal_bal']\n\ntrain.drop(columns=['origination_date','first_payment_date'],inplace=True)\ntest.drop(columns=['origination_date','first_payment_date'],inplace=True)\n\ntrain['lag'] = train['firstPayMonth'].astype('int') - train['origin_Month'].astype('int')\ntest['lag'] = test['firstPayMonth'].astype('int') - test['origin_Month'].astype('int')\n\ntrain['capabilityRatio'] = train['debt_to_income_ratio']\/train['loan_to_value']\ntest['capabilityRatio'] = test['debt_to_income_ratio']\/test['loan_to_value']\n\ntrain['sumCreditScore'] = train['borrower_credit_score'] + train['co-borrower_credit_score']\ntest['sumCreditScore'] = test['borrower_credit_score'] + test['co-borrower_credit_score']\n\ntrain['sumCreditScore'] = train['sumCreditScore']\/train['number_of_borrowers']\ntest['sumCreditScore'] = test['sumCreditScore']\/test['number_of_borrowers']\n\ndef CIBIL_trend(x):\n    a=''\n    if(x<250):\n        a='Insufficent Information'\n    elif((x>=250) and (x<=550)):\n        a='Poor'\n    elif((x>550) and (x<=650)):\n        a='Fair'\n    elif((x>650) and (x<=790)):\n        a='Good'\n    elif((x>790) and (x<=900)):\n        a='Excellent'\n    else:\n        a='Others'\n    return a\n\ntrain.loc[70926,'sumCreditScore'] = train.loc[70926,'sumCreditScore']\/2\n\ntrain['remark'] = train['sumCreditScore'].apply(CIBIL_trend)\ntest['remark'] = test['sumCreditScore'].apply(CIBIL_trend)","74fad1a6":"categorical = ['source','financial_institution','loan_purpose','insurance_type','origin_Month',\n               'firstPayMonth','numBorrowers','remark','origin_Year','origin_Day','firstPayYear']","d4aa9365":"powers = set(list(test.columns)) - set(categorical) - set(['m1','m2','m3','m4','m5','m6','m7','m8','m9','m10','m11','m12'])\n\nfor i in powers:\n    print(i)\n    train['inverse'+i]=1\/(train[i]+0.01)\n    train['square'+i] = np.square(train[i])\n    train['cube'+i] = np.power(train[i],3)\n    train['sqrt'+i] = np.sqrt(train[i])\n    train['cbrt'+i] = np.power(train[i],(1\/3))\n    \n    test['inverse'+i]=1\/(test[i]+0.01)\n    test['square'+i] = np.square(test[i])\n    test['cube'+i] = np.power(test[i],3)\n    test['sqrt'+i] = np.sqrt(test[i])\n    test['cbrt'+i] = np.power(test[i],(1\/3))","2c45750c":"!pip install catboost\n\nfrom catboost import CatBoostClassifier\n\n\nf1 = 0\nprobasPred = np.zeros(len(test))\nprobasPredTrain = np.zeros(len(train))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(train.drop(columns=['m13']), train['m13'].values))):\n    X_train = (train.drop(columns=['m13'])).iloc[trn_idx]\n    X_test = (train.drop(columns=['m13'])).iloc[val_idx]\n    y_train = train['m13'].iloc[trn_idx]\n    y_test = train['m13'].iloc[val_idx]\n    clf = CatBoostClassifier(class_weights=[1,1.5],cat_features = categorical,silent=True,iterations=100)\n    #Iterations have been set to 100 to make commiting easy. It was set to 1000 while actual submission.\n    clf.fit(X_train,y_train)\n    preds = clf.predict(X_test)\n    f1 += f1_score(y_test,preds)\n    print('Current f1 is:' + str(f1\/(fold_+1)))\n    probasPredTrain += clf.predict_proba((train.drop(columns=['m13'])))[:,1]\n    probasPred += clf.predict_proba(test)[:,1]","71d4979a":"probasPredTrain = probasPredTrain\/5\nprobasPred = probasPred\/5\n\nthreshF1 = []\nthreshVal = []\n\nfor i in np.linspace(0.05,0.95,100):\n    threshF1.append(f1_score(train['m13'],(probasPredTrain>i)*1))\n    threshVal.append(i)\n\nplt.plot(threshVal,threshF1)\n\nnp.array(threshF1).argmax()\n\nlabels = (probasPred>threshVal[(np.array(threshF1).argmax())])*1","2d4a9a3b":"data = pd.concat([train.drop(columns=['m13']),test])\n\ndata.head()\n\ndata = pd.get_dummies(data,columns=categorical,prefix='dum')\n\ntrain_new = data.iloc[:len(train)]\ntest_new = data.iloc[len(train):]\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain_new = sc.fit_transform(train_new)\ntest_new = sc.transform(test_new)\n\n# from sklearn.decomposition import PCA\n# pca = PCA(n_components = 180)\n# train_new = pca.fit_transform(train_new)\n# test_new = pca.transform(test_new)","214d1079":"from xgboost import XGBClassifier\nfrom imblearn.under_sampling import TomekLinks","43072f04":"f1 = 0\nprobasPred = np.zeros(len(test))\nprobasPredTrain = np.zeros(len(train))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(train_new, train['m13'].values))):\n    X_train = (train_new)[trn_idx]\n    X_test = (train_new)[val_idx]\n    y_train = train['m13'][trn_idx]\n    y_test = train['m13'][val_idx]\n    clf = XGBClassifier(colsample_bytree=0.57574225920725,learning_rate=0.12524888976943854,max_delta_step=0.23218201667538563,\n             max_depth=15,min_child_weight=11.522073006203208,reg_alpha=6.779851915666066,\n              reg_lambda=6.682134255966473,scale_pos_weight=2.556586526453567,subsample=0.7805883424057105,\n            eval_metric='auc')\n    clf.fit(X_train,y_train)\n    preds = clf.predict(X_test)\n    f1 += f1_score(y_test,preds)\n    print('Current f1 is:' + str(f1\/(fold_+1)))\n    #probasPredTrain += clf.predict_proba((train_new))[:,1]\n    probasPredTrain[val_idx] = clf.predict_proba(X_test)[:,1]\n    probasPred += clf.predict_proba(test_new)[:,1]","696d5af0":"probasPred = probasPred\/5","0e609214":"threshF1 = []\nthreshVal = []\n\nfor i in np.linspace(0.05,0.95,100):\n    threshF1.append(f1_score(train['m13'],(probasPredTrain>i)*1))\n    threshVal.append(i)\n\nplt.plot(threshVal,threshF1)","99e6b219":"print(np.array(threshF1).max())\nprint(np.array(threshF1).argmax())\nlabels = (probasPred>threshVal[(np.array(threshF1).argmax())])*1","a1db2434":"f1 = 0\nprobasPred = np.zeros(len(test))\nprobasPredTrain = np.zeros(len(train))\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(train_new, train['m13'].values))):\n    X_train = (train_new)[trn_idx]\n    X_test = (train_new)[val_idx]\n    y_train = train['m13'][trn_idx]\n    y_test = train['m13'][val_idx]\n    clf = LGBMClassifier(scale_pos_weight=10.12,learning_rate=0.1322,max_depth=36,\n                     min_data_in_leaf=213,reg_alpha=11.01,reg_lambda=9.281,num_leaves=996)\n    clf.fit(X_train,y_train)\n    preds = clf.predict(X_test)\n    f1 += f1_score(y_test,preds)\n    print('Current f1 is:' + str(f1\/(fold_+1)))\n    probasPredTrain[val_idx] = clf.predict_proba(X_test)[:,1]\n    probasPred += clf.predict_proba(test_new)[:,1]","ba2a4da0":"probasPred = probasPred\/5","a608aa18":"threshF1 = []\nthreshVal = []\n\nfor i in np.linspace(0.05,0.95,100):\n    threshF1.append(f1_score(train['m13'],(probasPredTrain>i)*1))\n    threshVal.append(i)\n\nplt.plot(threshVal,threshF1)","95770409":"print(np.array(threshF1).max())\nprint(np.array(threshF1).argmax())\nlabels = (probasPred>threshVal[(np.array(threshF1).argmax())])*1","42bc9a94":"# f1 = 0\n# probasPred = np.zeros(len(test))\n# probasPredTrain = np.zeros(len(train))\n\n# folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(train_new, train['m13'].values))):\n#     X_train = (train_new)[trn_idx]\n#     X_test = (train_new)[val_idx]\n#     y_train = train['m13'][trn_idx]\n#     y_test = train['m13'][val_idx]\n#     tk = TomekLinks()\n#     X_train_res, y_train_res = tk.fit_resample(X_train, y_train.ravel())\n#     clf = LGBMClassifier(scale_pos_weight=10.12,learning_rate=0.1322,max_depth=36,\n#                      min_data_in_leaf=213,reg_alpha=11.01,reg_lambda=9.281,num_leaves=996)\n#     clf.fit(X_train_res,y_train_res)\n#     preds = clf.predict(X_test)\n#     f1 += f1_score(y_test,preds)\n#     print('Current f1 is:' + str(f1\/(fold_+1)))\n#     probasPredTrain[val_idx] = clf.predict_proba(X_test)[:,1]\n#     probasPred.append(clf.predict(test_new))","b2fa5ab9":"# probasPred = probasPred\/5","5b78dc58":"# threshF1 = []\n# threshVal = []\n\n# for i in np.linspace(0.05,0.95,100):\n#     threshF1.append(f1_score(train['m13'],(probasPredTrain>i)*1))\n#     threshVal.append(i)\n\n# plt.plot(threshVal,threshF1)","61f68c19":"# print(np.array(threshF1).max())\n# print(np.array(threshF1).argmax())\n# labels = (probasPred>threshVal[(np.array(threshF1).argmax())])*1","8e94c830":"# f1 = 0\n# probasPred = np.zeros(len(test))\n# probasPredTrain = np.zeros(len(train))\n\n# folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(train_new, train['m13'].values))):\n#     X_train = (train_new)[trn_idx]\n#     X_test = (train_new)[val_idx]\n#     y_train = train['m13'][trn_idx]\n#     y_test = train['m13'][val_idx]\n#     tk = TomekLinks()\n#     X_train_res, y_train_res = tk.fit_resample(X_train, y_train.ravel())\n#     clf = XGBClassifier(colsample_bytree=0.5126,learning_rate=0.02512,max_delta_step=5.431,max_depth=15,min_child_weight=4.108,reg_alpha=6.16,\n#                         reg_lambda=4.111,scale_pos_weight=3.9,subsample=0.5776,eval_metric='auc',tree_method='gpu_hist')\n#     clf.fit(X_train_res,y_train_res)\n#     preds = clf.predict(X_test)\n#     f1 += f1_score(y_test,preds)\n#     print('Current f1 is:' + str(f1\/(fold_+1)))\n#     probasPredTrain[val_idx] = clf.predict_proba(X_test)[:,1]\n#     probasPred.append(clf.predict(test_new))","014932a3":"# probasPred = probasPred\/5","3ff2ee32":"# threshF1 = []\n# threshVal = []\n\n# for i in np.linspace(0.05,0.95,100):\n#     threshF1.append(f1_score(train['m13'],(probasPredTrain>i)*1))\n#     threshVal.append(i)\n\n# plt.plot(threshVal,threshF1)","99da3632":"# print(np.array(threshF1).max())\n# print(np.array(threshF1).argmax())\n# labels = (probasPred>threshVal[(np.array(threshF1).argmax())])*1","ca47721a":"tester = pd.read_csv('..\/input\/test.csv')\n\nsubmission = pd.DataFrame({'loan_id':tester['loan_id'],'m13':labels})\n\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"subm.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submission)","4140aa34":"Things which I tried but didnt work out for this problem - \n\n1.**Dense Neural Networks**\n\n2.**Fast AI Tabular Learner**\n\n3.**Applying PCA on the Dataset** - This approach failed miserably, hinting towards a possibility of the train and the test set having different distributions, due to which most of the data from the test set is lost on applying PCA which is trained on the train set.\n","724125c7":"I have commented out the codes for Tomek Link implementation as the Tomek Link process being extremely slow, it takes a lot of time to run and it exceeds the time limit for commiting to Kaggle notebooks.\n\nBut the entire working code has been attached below.","d86b8657":"**Applying LightBoost**","a8019ce0":"Making a list of Categorical Variables for future use.","b5ac6bc9":"Dropping the 'loan_id' column as we aren't supposed to use that and it wont be of much help anyway, being unique for every row.","c3fdbd08":"**APPROACH**","340fdc57":"As we  can see, the dataset has no Null value at all.\n\nOnly possible thing which might be there in the dataset is the presence of outliers.\n\nBut looking at the sheer volume of the dataset, we can safely assume that the models would not be affected much by the outliers, if sufficient and appropriate regularization is applied.","e4e77123":"**FEATURE ENGINEERING**","35a9df68":"**Class Distribution**","e799e656":"Let's check for some NaN \/ Null values present in the dataset","fa04b3de":"**INTRODUCTION**","c45a7df8":"**HERE WE HAVE OUR SECOND MODEL SUBMISSION FROM XGBOOST**","ed447ce6":"**Data Pre-Processing**","f8421e1a":"Stacking on these 5 models further gives the score a push. \n\nThe models are summarised again below - \n\n1.CatBoost on Normal Data.\n\n2.XGBoost on One Hot Encoded, Scaled Data.\n\n3.LGBM on One Hot Encoded, Scaled Data.\n\n4.XGBoost on Tomek Links Undersampled Data.\n\n5.LGBM on Tomek Links Undersampled Data.","55dcd595":"That's all from the Feature Engineering part of the dataset. \n\nLets proceed to the modelling part now.","4fd0e755":"![](https:\/\/datahack.analyticsvidhya.com\/media\/__sized__\/contest_cover\/1920x480_fiqz1Xl-thumbnail-1200x1200.png)","bb075555":"**HERE WE HAVE THE FIFTH MODEL SUBMISSION FROM TOMEK LINKS AND XGBOOST**","ab9eb85a":"**UNDERSAMPLING TO HANDLE THE IMBALANCE IN THE DATASET**","0d52b3d5":"**HERE WE HAVE OUR FIRST MODEL SUBMISSION FROM CATBOOST**","c4f1a4d2":"Before proceeding to the code, I present a brief summary of my approach - \n\nThe dataset provided is **heavily imbalanced** ( around **99.45%:0.55%** for the classes ), hinting towards the possibility of dealing with this problem as a **Anomaly detection** as well as a classification problem.\n\nIn order to handle the class imbalance, I used the \"**Scale_pos_weight**\" parameter present in most of the models and also used Oversampling and Undersampling on the dataset.\n\nFinally, Undersampling using **Tomek Links** aced all other oversampling and undersampling methods, so, i have presented that method in the notebook.\n\nAlso, Many new features were designed which have been explained along with the logic behind them, below.\n\nI trained 5 different models trained on 2 types of datasets in order to have a low correlation between the predictions of the model, so that Stacking would be effective.\n\nThe 5 models trained were finally stacked and submitted.\n\n**Thresholding** was also used for all the Individual Models as well as the Stacking model so as to give the score a push.\n\nOne observation about the dataset was that PCA performed really poor on it, hinting towards the possibility of the train and the test set having different distributions.","32196b69":"* **One-Hot Encoding Trials for XGB and LGBM**\n","1b8f3871":"Hello Everyone !\n\nThis kernel consists of my work for the India ML Hiring Hackathon on Analytics Vidhya in which we were supposed to predict whether the borrower would default in loan payment in a particular instalment.\n\nIt was essentially a binary classification problem.\n\nThe Kernel got a highest Public Leaderboard F1 score of **0.336**.\n\nAnd the highest Cross Validation F1 score achieved is **0.543**.\n\nThe highest score achieved on the **Private Leaderboard** is **0.583**\n\nThe kernel explains the different steps and decisions I took during the training of the model and the reason behind them too.","6fef0e36":"A brief explanation of all the new features made - \n\n1.Extracted **Year**, **Month** and **Day** from Origin Date.\n\n2.Extracted **Year**, **Month** of First repayment.\n\n3.**Number of Borrowers** - Made the number of Borrowers categorical.\n\n4.**Number of Months** - Assuming 30 days month, got the number of months from the Loan Term.","1502bb50":"**HERE WE HAVE OUR THIRD MODEL SUBMISSION FROM LIGHTBOOST**","30034a8c":"As we can see, the dataset is **HEAVILY IMBALANCED** and hence, we need to keep this in mind as this might make models to be biased.\n\nAlso, with such heavy imbalance in the dataset, our metric cannot be something like accuracy as the model can learn absolutely nothing at all and predict all test values as normal and still get a very high accuracy (~99%). \n\nTherefore, we use a better balanced metric like **F1 Score** for rating the model's performance.","03540ede":"Unpaid Per Month - Unpaid balance divided by the number of months\n\nApproximate Income - This feature was designed using the unpaid principal balance and the debt to income ratio.\n\nApproximate Value - This feature was designed using the unpaid principal balance and loan to value ratio.\n\nPayment Amount Per Person - In case there are 2 borrowers, we find the amount each borrower needs to pay.\n\nSelf Percent - The percentage of amount of the loan to be paid by the borrower ( considering the insurance percentage )\n\nUncovered Loan - The amount of unpaid balance not covered by Insurance.\n\nTotal Delinquency and Delinquency Probability- The sum of the m1 to m12 columns and the probability of delinqueny in a particular month.\n\nTotal Interest to be paid\n\nTime Lag - The Time gap between loan origination date and first repayment date.\n\nCapability Ratio - This is the ratio of debt-income to loan-value, essentially the ratio of debt to value.\n\nCredit Score Sum and Binning - to give a better idea of how good a borrower is in repayment.","2f4961e6":"Taking the Inverse, Square, Cube, Square Root and Cube Root of the numerical columns because the payment default might depend on a form of these variables.","e0a89903":"Here is the link of the final CSV generated from the notebook post stacking - \n\nhttps:\/\/drive.google.com\/open?id=15wbLEbMv3xUWBCMqndzNZeK-mowAHFut","e93001db":"**->** **CatBoost Method**","4ed454a0":"Bayesian Optimization was used for Tuning the models and getting the optimal hyper-parameters for the models used and also for stacking with optimal weights.\n","5766f26c":"Undersampling is done using **Tomek Links**","c655bbba":"**Reading and visualizing the dataset.**","77968eb2":"\n**Now that we have reached the end of the kernel, I am assuming you liked the kernel, since you didnt close it mid-way.**\n\n**If you did like it, please UPVOTE the kernel. That keeps me going !\n**\n\n**Any suggestions and criticism are welcome.**\n\n**Cheers !**","d96ca055":"**WE HAVE OUR FOURTH MODEL SUBMISSION FROM TOMEK UNDERSAMPLING AND LIGHTBOOST**","ac81fbb8":"**Moving on to making new features ...**","ffc49e97":"**Applying Various Models**"}}