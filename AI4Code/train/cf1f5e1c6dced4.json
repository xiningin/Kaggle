{"cell_type":{"1deda2a1":"code","4b4dfdc0":"code","d3bfaa3d":"code","1b68e386":"code","0f0e01fa":"code","dd44b869":"code","0c47b1f6":"code","156905d2":"code","971e6334":"code","3e34b7e7":"code","00761df1":"code","8fa273bf":"code","edee333f":"code","0dd010f6":"code","f4ce1ae3":"code","f9a64b77":"code","264350f3":"code","edb40577":"code","2fb38bcd":"code","34410cbe":"code","b612d694":"code","8982773c":"code","19e8a7fb":"code","13eb4e8a":"code","84695ec3":"code","72c1e361":"code","dfc0fd0c":"code","ab024060":"code","37eca579":"code","e00949f7":"code","2453aeb6":"code","e08f8625":"markdown","a8beeb98":"markdown"},"source":{"1deda2a1":"!pip install mtcnn\nimport cv2 \nimport mtcnn\nfrom mtcnn.mtcnn import MTCNN\ndetector = MTCNN()","4b4dfdc0":"import os\nfrom tqdm import tqdm\ntrain_dir='..\/input\/trainset\/trainset'\n","d3bfaa3d":"from keras.models import Sequential\n#turning sequential layers into keras model\n\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n#conv2D=spatial convolution over images\n#zerpPadding2D = This layer can add rows,columns of zeros at all side of an image tensor\n#Activation=  using activation function\n#Input=  is used to instantiate a Keras tensor with arguments\n#concatenate=concatenation of all inputs\n\nfrom keras.models import Model\n#Model =groups layers into an object with training and inference features\n\nfrom keras.layers.normalization import BatchNormalization\n#Normalize the activations of the previous layer for each given example in a batch independently,\n#rather than across a batch like Batch Normalizationn\n\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\n#Max pooling operation for 2D spatial data\n#AveragePooling2d for the Images\n\nfrom keras.layers.merge import Concatenate\n# Concatenate the layers\n\nfrom keras.layers.core import Lambda, Flatten, Dense\n#The Lambda layer exists so that arbitrary TensorFlow functions \n#can be used when constructing Sequential and Functional API models\n#Flatten the Input dose not affect the batch size\n#Dense implements the operation: output = activation(dot(input, kernel) + bias) \n#where activation is the element-wise activation function passed as the activation\n#argument, kernel is a weights matrix created by the layer, and bias is a bias \n#vector created by the layer (only applicable if use_bias is True).\n\nfrom keras.initializers import glorot_uniform\n#random initialization of weight\n\nfrom keras.engine.topology import Layer\n#creating layers like classes\n\nfrom keras import backend as K\n#using tensorflow as backend \n\nK.set_image_data_format('channels_first')\n\nimport cv2\nimport matplotlib.pyplot as plt\n\n#open cv for python\n\n\n#os libarary for python\n\nimport numpy as np\n#numpy library\n\nfrom numpy import genfromtxt\n# converts each line of the file in a sequence of strings\n\nimport pandas as pd\n#pandas library\n\nimport tensorflow as tf\n#since the session variable isint supported in tensorflow 2 installing \n#importing tensorflow\n\n\n\n\n\ndef triplet_loss(y_true, y_pred, alpha = 0.2):\n        \n    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n    ### START CODE HERE ### (\u2248 4 lines)\n    # Step 1: Compute the (encoding) distance between the anchor and the positive\n    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1)\n    # Step 2: Compute the (encoding) distance between the anchor and the negative\n    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1)\n    # Step 3: subtract the two previous distances and add alpha.\n    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n    ### END CODE HERE ###\n    \n    return loss\n\ndef who_is_it(image_path, database, model):\n    \"\"\"\n    Implements face recognition for the office by finding who is the person on the image_path image.\n    \n    Arguments:\n    image_path -- path to an image\n    database -- database containing image encodings along with the name of the person on the image\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n    identity -- string, the name prediction for the person on image_path\n    \"\"\"\n    \n    ### START CODE HERE ### \n    \n    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (\u2248 1 line)\n    encoding = img_to_encoding(image_path, model)\n    \n    ## Step 2: Find the closest encoding ##\n    \n    # Initialize \"min_dist\" to a large value, say 100 (\u22481 line)\n    min_dist = 100\n    \n    # Loop over the database dictionary's names and encodings.\n    for (name, db_enc) in database.items():\n        \n        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (\u2248 1 line)\n        dist = np.linalg.norm((encoding - db_enc))\n\n        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (\u2248 3 lines)\n        if dist < min_dist:\n            min_dist = dist\n            identity = name\n\n    ### END CODE HERE ###\n    \n    if min_dist > 0.7:\n        print(\"Not in the database.\")\n    else:\n        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n        \n    return min_dist, identity\n","1b68e386":"#importing frutils\nimport requests\nurl = 'https:\/\/raw.githubusercontent.com\/05rs\/Deep-Learning\/master\/Convolutional%20Neural%20Networks\/week4\/Face%20Recognition\/fr_utils.py'\nr = requests.get(url, allow_redirects=True)\nopen('fr_utils.py', 'wb').write(r.content)\nfrom fr_utils import *","0f0e01fa":"#importing incpetion_block_v2\nimport requests\nurl = 'https:\/\/raw.githubusercontent.com\/05rs\/Deep-Learning\/master\/Convolutional%20Neural%20Networks\/week4\/Face%20Recognition\/inception_blocks_v2.py'\nr = requests.get(url, allow_redirects=True)\nopen('inception_blocks_v2.py', 'wb').write(r.content)\nfrom inception_blocks_v2 import *","dd44b869":"FRmodel = faceRecoModel(input_shape=(3, 96, 96))\nprint(\"Total Params:\", FRmodel.count_params())","0c47b1f6":"FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\nload_weights_from_FaceNet(FRmodel)","156905d2":"\ndef verify(image_paths, identity, database, model):\n    \"\"\"\n    Function that verifies if the person on the \"image_path\" image is \"identity\".\n    \n    Arguments:\n    image_path -- path to an image\n    identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    dist -- distance between the image_path and the image of \"identity\" in the database.\n    door_open -- True, if the door should open. False otherwise.\n    \"\"\"\n    ### START CODE HERE ###\n    \n    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (\u2248 1 line)\n    enc=[]\n    dis=[]\n    i=0\n#     base=os.path.basename(identity)\n#     name=os.path.splitext(base)[0]\n    if image_paths == None:\n        return (0.5,'nomatch')\n    for image_path in image_paths:\n        encoding = img_to_encoding(image_path, model)\n        \n        dist = np.linalg.norm((encoding - database[identity]))\n\n        if dist < 0.8:\n            return (dist,'match')\n        \n\n        dis.append(dist)  \n    \n    \n    return (min(dis),'nomatch')\n\n  ","971e6334":"cwd = '\/kaggle\/working\/Images\/'\nos.mkdir(cwd)\ndef detect_face(path,debug=False):\n    base=os.path.basename(path)\n    i = plt.imread(path)\n    err_id=0\n    try:\n        faces = detector.detect_faces(i)\n        fp=[]\n        idx=0\n        for face in faces:\n#             print(i)\n            idx=idx+1\n            (x,y,w,h) =face['box']\n            roi_color = i[y:y+h, x:x+w]\n            img=cv2.resize(roi_color,(96,96))\n\n            cv2.imwrite(cwd + os.path.splitext(base)[0]+'-'+str(idx)+ '.png',img)\n    #         plt.imshow(img)\n            fp.append(cwd + os.path.splitext(base)[0]+'-'+str(idx)+'.png')\n\n            return (fp )\n    except:\n        err_id=err_id+1\n        if debug:\n            print('face detection error!: [PATH]: {}'.format(path))\n        return None\n","3e34b7e7":"detect_face(\"..\/input\/trainset\/trainset\/0010\/0010_0001595\/0010_0001595_script.jpg\",debug=True)","00761df1":"database = {}\ndef fub (selfie_path, passport_path):\n    try:\n        passport_paths = detect_face(passport_path)[0]\n\n        selfie_paths = detect_face(selfie_path)\n\n        base=os.path.basename(passport_path)\n        name=os.path.splitext(base)[0]\n\n        database[name] = img_to_encoding(passport_paths, FRmodel)\n\n        return verify(selfie_paths, name, database, FRmodel)\n    except:\n        return \n \n    \n  ","8fa273bf":"p='..\/input\/trainset\/trainset\/0001\/0001_0000268\/0001_0000268_script.jpg'\ns='..\/input\/trainset\/trainset\/0001\/0001_0000268\/0000000.jpg'","edee333f":"fub(s,p)","0dd010f6":"img_pas = plt.imread(s)\nimg_p = plt.imread(p)\nplt.imshow(img_pas)\n\n# plt.imshow(img_p)\n# plt.show()\nimg_pas.shape","f4ce1ae3":"# def train(path='..\/input\/trainset\/trainset'):\n#     metrics={}\n#     i_set=0\n#     for train_set in (os.listdir(train_dir)):\n#         train_set_path=os.path.join(train_dir,train_set)\n#         i_set=i_set+1\n#         i_subset=0\n#         for subset in tqdm(os.listdir(train_set_path)):\n            \n#             i_subset=i_subset+1\n#             train_set_subset_path=os.path.join(train_set_path,subset)\n#     #         print(subset)\n\n#             self_pics=[]\n#             pass_pics=[]\n#             unq=[]\n#             for file in (os.listdir(train_set_subset_path)):\n#                 file_path=os.path.join(train_set_subset_path,file)\n#                 if file not in unq:\n#     #                 print(file)\n#                     unq.append(file)\n#                     if 'script' in file:\n#                         pass_pics.append(file_path)\n\n#                     else:\n#                         self_pics.append(file_path)\n#                 try :      \n#                     for p in pass_pics:\n#                         for s in self_pics:\n#                             (diss,a)=fub(s,p)\n#                             if a =='match':\n#                                 metrics[s]=[diss,1] \n#                             else:\n#                                 metrics[s]=[diss,0] \n#                 except:\n#                     pass\n                    \n#         if i_set>2:\n#             break\n        \n\n# #         if i_set==3:\n# #             break\n#     #     print((os.listdir(train_set_path)))\n#     del unq,self_pics,pass_pics\n#     avg_acc=sum([v[1] for v in metrics.values()])\/len([v[1] for v in metrics.values()])\n#     avg_dist=sum([v[0] for v in metrics.values()])\/len([v[0] for v in metrics.values()])\n    \n#     print('Average accuracy: ',avg_acc)\n#     print('Average distance: ',avg_dist)\n#     print('Total Files Processed: ',len(metrics.values()))\n  \n#     return metrics,avg_acc,avg_dist","f9a64b77":"# met,avg_acc,avg_dist=train()","264350f3":"# import pandas as pd\n# metrics=met\n# his = pd.DataFrame({\"accuracy\":[ v[1] for v in metrics.values()],\"distance\":[ v[0] for v in metrics.values()]})\n# data=his[his['distance']<1]\n# data.describe()\n# print(data['accuracy'].mean())","edb40577":"# p='..\/input\/trainset\/trainset\/0001\/0001_0000268\/0001_0000268_script.jpg'\n# s='..\/input\/trainset\/trainset\/0001\/0001_0000268\/0000000.jpg'","2fb38bcd":"# def train_ds(path='..\/input\/trainset\/trainset'):\n#     dicx={}\n#     dicy={}\n#     for train_set in os.listdir(train_dir):\n#         train_set_path=os.path.join(train_dir,train_set)\n#         for subset in os.listdir(train_set_path):\n#             train_set_subset_path=os.path.join(train_set_path,subset)\n#     #         print(subset)\n\n#             self_pics=[]\n#             pass_pics=[]\n#             unq=[]\n#             for file in (os.listdir(train_set_subset_path)):\n#                 file_path=os.path.join(train_set_subset_path,file)\n#                 if file not in unq:\n#     #                 print(file)\n#                     unq.append(file)\n#                     if 'script' in file:\n#                         pass_pics.append(file_path)\n\n#                     else:\n#                         self_pics.append(file_path)\n#             dicx[subset]=pass_pics\n#             dicy[subset]=self_pics\n\n\n#     #     print((os.listdir(train_set_path)))\n#     del unq,self_pics,pass_pics   \n#     return dicx,dicy","34410cbe":"# dicx,dicy=train_ds()","b612d694":"# databases=encode_ds(dicx,dicy,debug=True)a","8982773c":"\n# def verify_ds(dicx,dicy,data=databases,debug=False):\n#     metrics={}\n    \n#     Y={k: dicy[k] for k in databases.keys()}\n    \n#     i=0\n#     err_id=0\n#     err=[]\n   \n#     for i_subset,idx in tqdm(enumerate (Y.keys())):\n        \n                \n#         itr=0\n#         for idy in (dicy[idx]):\n#             itr=itr+1\n#             i+=1\n#             acc=0\n#             diss=0\n            \n# #             print(idy)\n#             selfie_paths = detect_face(idy)\n# #             print(selfie_paths)\n#             try:\n#                 if selfie_paths == None:\n#                     break\n#                 for selfie_path in selfie_paths:\n#     #                 print(selfie_path)\n#                         pass_enc = databases[idx]\n\n#                         encoding = img_to_encoding(selfie_path, FRmodel)\n\n#                         dist = np.linalg.norm((encoding - pass_enc))\n\n\n\n#                         if dist <0.84:\n#                             acc=1\n#                             diss=dist\n#                 if acc==1:\n                    \n#                     metrics[idy]=[diss,acc]  \n#                 else:\n#                     metrics[idy]=[dist,acc]           \n                    \n#             except:\n#                 err_id+=1\n#                 err.append(idy)\n                \n\n#     avg_acc=sum([v[1] for v in metrics.values()])\/len([v[1] for v in metrics.values()])\n#     avg_dist=sum([v[0] for v in metrics.values()])\/len([v[0] for v in metrics.values()])\n#     print('Average accuracy: ',avg_acc)\n#     print('Average distance: ',avg_dist)\n#     print('Total Files Processed: ',len(metrics.values()))\n#     if debug:\n      \n#         if err_id>0:\n#             print('Found ({}) corrupt files!'.format(err_id))\n#     return metrics,avg_acc,avg_dist     \n        \n        \n  ","19e8a7fb":"# metrics,acc,dist = verify_ds(dicx,dicy,databases,debug=True)\n# #  ","13eb4e8a":"# metrics=met","84695ec3":"# import pandas as pd\n# his = pd.DataFrame({\"accuracy\":[ v[1] for v in metrics.values()],\"distance\":[ v[0] for v in metrics.values()]})\n# data=his[his['distance']<1]\n# data.describe()\n# import math","72c1e361":"# his['distance'].mean()\n# # ","dfc0fd0c":"# def histogram(data,nbins=10, cumulative=False, x_label = \"\", y_label = \"\", title = \"\"):\n# #     nbins=int(math.sqrt((data.cou nt())))\n#     plt.style.use('ggplot')\n#     _, ax = plt.subplots()\n#     ax.hist(data, bins=nbins,cumulative = cumulative, color = '#539caf')\n#     ax.set_ylabel(y_label)\n#     ax.set_xlabel(x_label)\n#     ax.set_title(title)","ab024060":"# data['distance'].describe()","37eca579":"# histogram(data['distance'],nbins=70)","e00949f7":"# histogram(his[his['distance']<1]['distance'],nbins=14)","2453aeb6":"# a=his[his['distance']<1]\n# a.describe()","e08f8625":"### References:\n\n- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https:\/\/arxiv.org\/pdf\/1503.03832.pdf)\n- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https:\/\/research.fb.com\/wp-content\/uploads\/2016\/11\/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https:\/\/github.com\/iwantooxxoox\/Keras-OpenFace.\n- Our implementation also took a lot of inspiration from the official FaceNet github repository: https:\/\/github.com\/davidsandberg\/facenet \n","a8beeb98":"# fub() is function which inputs selfie_path and pass_port_path and returns distance and name.\n\n### distance parameter measures corelation b\/w the two and is threshold at 0.7\n### **lower the distance** maximum is **confidance for Positive** ('MATCH')\n### **bigger the distance** maximum is **confidance for Negative** ('NOMATCH')"}}