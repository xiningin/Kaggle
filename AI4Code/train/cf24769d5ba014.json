{"cell_type":{"6563bef1":"code","625d2a55":"code","891409d2":"code","8eef2144":"code","82be540d":"code","2da101bb":"code","02f8ae75":"code","61919192":"code","3ea94cc7":"code","da8ba406":"markdown"},"source":{"6563bef1":"import pandas as pd\nimport numpy as np\nwine = pd.read_csv(\"..\/input\/wine-dataset\/wine.csv\")\nwine.head()","625d2a55":"wine.shape","891409d2":"wine.describe()","8eef2144":"from matplotlib import pyplot as plt\nwine['Class'].value_counts().plot.bar(title=\"Distribution of Wine Classes\")","82be540d":"#remove the class column from wine, and save as label\nlabel = wine.pop('Class')","2da101bb":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(wine)\n#the results will show that PC1 dominates for the original data\nprint(\"variance explained\", pca.explained_variance_ratio_, \"singular_values\", pca.singular_values_)\n#transform the data according to the PCA results\nwine_transformed = pca.transform(wine)\n#wine_transformed = pd.DataFrame.from_records(pca.transform(wine))\n#wine_transformed['label'] = label","02f8ae75":"#apply standardscaler to scale the data\nfrom sklearn.preprocessing import StandardScaler\nwine_scaled = StandardScaler().fit_transform(wine)\nfirst_column = np.array(wine_scaled[:, 0])\nprint(first_column[:10], \"...\")\nprint(\"mean %.2f\" % first_column.mean())\nprint(\"var %.2f\" % first_column.var())","61919192":"#apply PCA to scaled data\npca = PCA(n_components=2)\npca.fit(wine_scaled)\nprint(\"variance explained\", pca.explained_variance_ratio_, \"singular_values\", pca.singular_values_)\nprint(pca)\nwine_scaled_transformed = pca.transform(wine_scaled)","3ea94cc7":"# comparison of scatter plots of standarized and original dataset after dimension reduction\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\nfor l, c, m in zip(range(1, 4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n    ax1.scatter(wine_transformed[label == l, 0],\n                wine_transformed[label == l, 1],\n                color=c,\n                label='class %s' % l,\n                alpha=0.5,\n                marker=m\n                )\nfor l, c, m in zip(range(1, 4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n    ax2.scatter(wine_scaled_transformed[label == l, 0],\n                wine_scaled_transformed[label == l, 1],\n                color=c,\n                label='class %s' % l,\n                alpha=0.5,\n                marker=m\n                )\nax1.set_title('original wine dataset')\nax2.set_title('standardized wine dataset')\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.legend(loc='upper right')\n    ax.grid()\n\nplt.tight_layout()\nplt.show()","da8ba406":"## This demo shows\n- Data standardization using sklearn.preprocessing\n- Dimension reduction by Principle Component Analysis (PCA) using sklearn.decomposition\n- Note: sklearn.decomposition.PCA do NOT standardize the input matrix\n- Using UCI Wine Data Set (converted into .csv file)\n- Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n- While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of PCA as being a prime example of when normalization is important, as in PCA we are interested in the components that maximize the variance, which could be inflated because of the different scales. \n- [ref](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)"}}