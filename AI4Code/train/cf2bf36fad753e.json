{"cell_type":{"9fe495bb":"code","6affa922":"code","d211c68b":"code","808d8a00":"code","3d20436b":"code","7b6b1fd6":"code","6fdfaf35":"code","0e23783e":"code","9a4b94b3":"code","952834b9":"code","34786db4":"code","841f0621":"code","2546f3e2":"code","2c03d819":"code","456cd3d7":"code","79c8af30":"code","126b9dbc":"code","9df0bbf1":"code","05dd0354":"code","877a456a":"code","b0e1efe9":"code","7f216b5d":"code","f28e6709":"code","2fa3947e":"code","a0c29c6f":"code","e6f349da":"code","1a62d31f":"code","1e817d36":"code","2de9ae47":"code","17693c4b":"code","a5a918c7":"code","63f3c85b":"markdown","edfa707f":"markdown","5ad9e75f":"markdown"},"source":{"9fe495bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6affa922":"import re\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nfrom keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Concatenate\nfrom keras.models import Model","d211c68b":"df = pd.read_csv(\"..\/input\/simple-dialogs-for-chatbot\/dialogs.txt\",names=[\"question\",\"answer\"],sep='\\t')\ndf.head()","808d8a00":"print(df.shape)\nprint(\"=\"*60)\nprint(df.info())\nprint(\"=\"*60)\ndf.describe()","3d20436b":"df.drop_duplicates(subset=['question'],inplace=True)\ndf.describe()","7b6b1fd6":"print(df.isnull().sum())\nprint('\\n')\nprint(df.isnull().any())","6fdfaf35":"df.loc[55:60,:]","0e23783e":"stop_words = set(stopwords.words('english'))\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\n#remove_stop = True for questions\n#remove_stop = False for answers\ndef preprocess_text(text,remove_stop):\n    text = text.lower()\n    text = ' '.join([contractions[word] if word in contractions else word for word in text.split()])\n    text = re.sub(r'[^a-zA-Z0-9]',' ',text)\n    if remove_stop == True:\n        text = ' '.join([word for word in text.split() if word not in stop_words])\n    text = ' '.join([word for word in text.split()])\n    return text","9a4b94b3":"df.question = df.question.apply(preprocess_text,remove_stop=False)\ndf.answer = df.answer.apply(preprocess_text,remove_stop=False)\ndf.loc[55:60,:]","952834b9":"df['decoder_input'] = df.answer.apply(lambda x: 'sos '+x)\ndf['decoder_label'] = df.answer.apply(lambda x: x+' eo>')\ndf.head()","34786db4":"encoder_input = np.array(df.question)\ndecoder_input = np.array(df.decoder_input)\ndecoder_label = np.array(df.decoder_label)\n\nn_rows = df.shape[0]\nprint(f\"{n_rows} rows\")\n\nindices = np.arange(n_rows)\nnp.random.shuffle(indices)\n\nencoder_input = encoder_input[indices]\ndecoder_input = decoder_input[indices]\ndecoder_label = decoder_label[indices]\n\ntrain_size = 0.9\n\ntrain_encoder_input = encoder_input[:int(n_rows*train_size)]\ntrain_decoder_input = decoder_input[:int(n_rows*train_size)]\ntrain_decoder_label = decoder_label[:int(n_rows*train_size)]\n\ntest_encoder_input = encoder_input[int(n_rows*train_size):]\ntest_decoder_input = decoder_input[int(n_rows*train_size):]\ntest_decoder_label = decoder_label[int(n_rows*train_size):]\n\nprint(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape)\n\nprint(test_encoder_input.shape)\nprint(test_decoder_input.shape)\nprint(test_decoder_label.shape)","841f0621":"q_tok = Tokenizer()\nq_tok.fit_on_texts(train_encoder_input)\nprint(len(q_tok.word_counts))\n\na_tok = Tokenizer()\na_tok.fit_on_texts(train_decoder_input)\na_tok.fit_on_texts(train_decoder_label)\nprint(len(a_tok.word_counts))\n\ntrain_encoder_input = q_tok.texts_to_sequences(train_encoder_input)\ntest_encoder_input = q_tok.texts_to_sequences(test_encoder_input)\n\ntrain_decoder_input = a_tok.texts_to_sequences(train_decoder_input)\ntest_decoder_input = a_tok.texts_to_sequences(test_decoder_input)\n\ntrain_decoder_label = a_tok.texts_to_sequences(train_decoder_label)\ntest_decoder_label = a_tok.texts_to_sequences(test_decoder_label)\n\nquestion_word_size = len(q_tok.word_counts)\nquestion_vocab_size = question_word_size+1\n\nanswer_word_size = len(a_tok.word_counts)\nanswer_vocab_size = answer_word_size+1","2546f3e2":"question_lengths = [len(s) for s in train_encoder_input]\nprint(f\"maximum question sequence length >> {np.max(question_lengths)}\")\n\nanswer_lengths = [len(s) for s in train_decoder_input]\nprint(f\"maximum answer sequence length >> {np.max(answer_lengths)}\")\n\nplt.subplot(2,1,1)\nplt.hist(question_lengths,bins=50)\nplt.show()\n\nplt.subplot(2,1,2)\nplt.hist(answer_lengths,bins=50)\nplt.show()","2c03d819":"train_encoder_input = pad_sequences(train_encoder_input,padding='post')\ntrain_decoder_input = pad_sequences(train_decoder_input,padding='post')\ntrain_decoder_label = pad_sequences(train_decoder_label,padding='post')\n\nprint(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape)\n\nquestion_sequence_size = train_encoder_input.shape[1]\nanswer_sequence_size = train_decoder_input.shape[1]\n\ntest_encoder_input = pad_sequences(test_encoder_input,padding='post',maxlen=question_sequence_size)\ntest_decoder_input = pad_sequences(test_decoder_input,padding='post',maxlen=answer_sequence_size)\ntest_decoder_label = pad_sequences(test_decoder_label,padding='post',maxlen=answer_sequence_size)\n\nprint(test_encoder_input.shape)\nprint(test_decoder_input.shape)\nprint(test_decoder_label.shape)","456cd3d7":"import os\n\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","79c8af30":"# embedding_dict = dict()\n\n# f = open(os.path.join('glove.6B.300d.txt'),encoding='utf-8')\n\n# for line in f:\n#     tokens = line.split()\n#     word = tokens[0]\n#     vector = tokens[1:]\n#     vector =  np.asarray(vector,dtype='float32')\n#     embedding_dict[word] = vector\n    \n# f.close()\n\n# embedding_size = len(embedding_dict['world'])\n# print(f\"There are {len(embedding_dict)} embedding vectors in total\")\n# print(f\"The size of embedding vector here >> {embedding_size}\")\n\n# question_embedding_matrix =  np.zeros((question_vocab_size,embedding_size))\n# for word,idx in q_tok.word_index.items():\n#     if idx <= question_word_size:\n#         vector = embedding_dict.get(word)\n#         if vector is not None:\n#             question_embedding_matrix[idx] = np.asarray(vector,dtype='float32')\n\n# answer_embedding_matrix =  np.zeros((answer_vocab_size,embedding_size))\n# for word,idx in q_tok.word_index.items():\n#     if idx <= answer_word_size:\n#         vector = embedding_dict.get(word)\n#         if vector is not None:\n#             answer_embedding_matrix[idx] = np.asarray(vector,dtype='float32')\n\nembedding_size=256","126b9dbc":"#trainer model\n\nhidden_size = 256\n\nencoder_input = Input(shape=[question_sequence_size])\n# encoder_embedding = Embedding(question_vocab_size,embedding_size,mask_zero=True,trainable=False,weights=[question_embedding_matrix])\nencoder_embedding = Embedding(question_vocab_size,embedding_size,mask_zero=True)\nencoder_embedded = encoder_embedding(encoder_input)\n\nlstm1 = Bidirectional(LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3))\nencoder_output1,_,_,_,_ = lstm1(encoder_embedded)\nlstm2 = Bidirectional(LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3))\nencoder_output2,_,_,_,_ = lstm2(encoder_output1)\nlstm3 = Bidirectional(LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3))\nencoder_output3,encoder_forward_h3,encoder_forward_c3,encoder_backward_h3,encoder_backward_c3= lstm3(encoder_output2)\n\nencoder_h3 = Concatenate(axis=-1)([encoder_forward_h3,encoder_backward_h3])\nencoder_dense_h = Dense(hidden_size)\nencoder_h3 = encoder_dense_h(encoder_h3)\n\nencoder_c3 = Concatenate(axis=-1)([encoder_forward_c3,encoder_backward_c3])\nencoder_dense_c = Dense(hidden_size)\nencoder_c3 = encoder_dense_c(encoder_c3)\n\ndecoder_input = Input(shape=(None,))\n# decoder_embedding = Embedding(answer_vocab_size,embedding_size,mask_zero=True,trainable=False,weights=[answer_embedding_matrix])\ndecoder_embedding = Embedding(answer_vocab_size,embedding_size,mask_zero=True)\ndecoder_embedded = decoder_embedding(decoder_input)\n\ndecoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True)\ndecoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n\ndense1 = Dense(512,activation='relu')\ndecoder_output = dense1(decoder_output)\n\ndecoder_softmax = Dense(answer_vocab_size,activation='softmax')\ndecoder_output = decoder_softmax(decoder_output)\n\ntrainer = Model([encoder_input,decoder_input],decoder_output)\ntrainer.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","9df0bbf1":"train_hist = trainer.fit([train_encoder_input,train_decoder_input],train_decoder_label,epochs=45,validation_split=0.1,batch_size=32)","05dd0354":"from keras.utils import plot_model\n\n#generator model\ngen_encoder = Model(encoder_input,[encoder_h3,encoder_c3])\n\ngen_decoder_h_input = Input(shape=[hidden_size])\ngen_decoder_c_input = Input(shape=[hidden_size])\n\ngen_decoder_embedded = decoder_embedding(decoder_input)\ngen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\ngen_decoder_output = dense1(gen_decoder_output)\ngen_decoder_output = decoder_softmax(gen_decoder_output)\n\ngen_decoder = Model([decoder_input]+[gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output]+[gen_decoder_h,gen_decoder_c])\n\nplot_model(gen_encoder)","877a456a":"plot_model(gen_decoder)","b0e1efe9":"def generate_from_input(encoder_input):\n    h,c = gen_encoder.predict(encoder_input)\n    \n    decoder_seq = np.zeros((1,1))\n    decoder_seq[0,0] = a_tok.word_index['sos']\n    \n    generated_sent = ''\n    stop_condition= False\n    \n    while not stop_condition:\n        output,h_state,c_state = gen_decoder.predict([decoder_seq]+[h,c])\n        sampled_index = np.argmax(output[0,-1,:])\n        sampled_word = a_tok.index_word[sampled_index]\n        \n        if sampled_word != 'eos':\n            generated_sent = generated_sent + sampled_word + ' '\n        \n        if sampled_word == 'eos' or len(generated_sent) >= answer_sequence_size:\n            stop_condition=True\n        \n        decoder_seq = np.zeros((1,1))\n        decoder_seq[0,0] = sampled_index\n        h,c = h_state,c_state\n        \n    return generated_sent.strip()\n\nprint(generate_from_input(train_encoder_input[0].reshape(1,question_sequence_size,-1)))\nprint(generate_from_input(train_encoder_input[100].reshape(1,question_sequence_size,-1)))\nprint(generate_from_input(train_encoder_input[2050].reshape(1,question_sequence_size,-1)))","7f216b5d":"def seq2question(encoder_input):\n    ret=[q_tok.index_word[idx] for idx in encoder_input if idx != 0]\n    return ' '.join(ret)\n\ndef seq2answer(decoder_input):\n    ret = []\n    for idx in decoder_input:\n        if idx != 0:\n            if a_tok.index_word[idx] != 'sos' and a_tok.index_word[idx] != 'eos':\n                ret.append(a_tok.index_word[idx])\n                \n    return ' '.join(ret)","f28e6709":"#results on train dataset\nsample_indices= [5,26,7,11,735,662,115,321]\n\nfor idx in sample_indices:\n    generated_sent = generate_from_input(train_encoder_input[idx:idx+1])\n    print(f\"Question >> {seq2question(train_encoder_input[idx])}\")\n    print(f\"Answer(Generated) >> {generated_sent}\")\n    print(f\"Answer(Label) >> {seq2answer(train_decoder_input[idx])}\")\n    print(\"=\"*45)\n    print(\"\\n\")\n    ","2fa3947e":"#results on test dataset\nsample_indices= [159,29,44,33]\n\nfor idx in sample_indices:\n    generated_sent = generate_from_input(test_encoder_input[idx:idx+1])\n    print(f\"Question >> {seq2question(test_encoder_input[idx])}\")\n    print(f\"Answer(Generated) >> {generated_sent}\")\n    print(f\"Answer(Label) >> {seq2answer(test_decoder_input[idx])}\")\n    print(\"=\"*45)\n    print(\"\\n\")\n    ","a0c29c6f":"import tensorflow\nfrom tensorflow.keras.layers import Attention","e6f349da":"#trainer model\n\nhidden_size = 256\n\n#encoder part start\nencoder_input = Input(shape=[question_sequence_size])\n# encoder_embedding = Embedding(question_vocab_size,embedding_size,mask_zero=True,trainable=False,weights=[question_embedding_matrix])\nencoder_embedding = Embedding(question_vocab_size,embedding_size,mask_zero=True)\nencoder_embedded = encoder_embedding(encoder_input)\n\nlstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3)\nencoder_output1,_,_= lstm1(encoder_embedded)\nlstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3)\nencoder_output2,_,_ = lstm2(encoder_output1)\nlstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.3,recurrent_dropout=0.3)\nencoder_output3,encoder_h3,encoder_c3= lstm3(encoder_output2)\n#encoder part done\n\n#decoder part start\ndecoder_input = Input(shape=(None,))\n# decoder_embedding = Embedding(answer_vocab_size,embedding_size,mask_zero=True,trainable=False,weights=[answer_embedding_matrix])\ndecoder_embedding = Embedding(answer_vocab_size,embedding_size,mask_zero=True)\ndecoder_embedded = decoder_embedding(decoder_input)\n\ndecoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True)\ndecoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n\n#Attention layer (query,value)\nattn_layer = Attention()\nattention_context = attn_layer([decoder_output,encoder_output3])\n\ndecoder_output = Concatenate(axis=-1)([decoder_output,attention_context])\n\ndense1 = Dense(512,activation='relu')\ndecoder_output = dense1(decoder_output)\n\ndecoder_softmax = Dense(answer_vocab_size,activation='softmax')\ndecoder_output = decoder_softmax(decoder_output)\n\ntrainer = Model([encoder_input,decoder_input],decoder_output)\nloss = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\ntrainer.compile(loss=loss,optimizer='adam',metrics=['accuracy'])","1a62d31f":"train_hist = trainer.fit([train_encoder_input,train_decoder_input],train_decoder_label,epochs=45,validation_split=0.1,batch_size=32)","1e817d36":"#generator model\ngen_encoder = Model(encoder_input,[encoder_output3,encoder_h3,encoder_c3])\n\ngen_decoder_value_input = Input(shape=(question_sequence_size,hidden_size))\ngen_decoder_h_input = Input(shape=[hidden_size])\ngen_decoder_c_input = Input(shape=[hidden_size])\n\ngen_decoder_embedded = decoder_embedding(decoder_input)\ngen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n\n#attention([querys,values])\ngen_attention_context = attn_layer([gen_decoder_output,gen_decoder_value_input])\ngen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,gen_attention_context])\n\ngen_decoder_output = dense1(gen_decoder_output)\ngen_decoder_output = decoder_softmax(gen_decoder_output)\n\ngen_decoder = Model([decoder_input]+[gen_decoder_value_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output]+[gen_decoder_h,gen_decoder_c])\n\nplot_model(gen_encoder)\nplot_model(gen_decoder)","2de9ae47":"def generate_from_input(encoder_input):\n    values,h,c = gen_encoder.predict(encoder_input)\n    \n    decoder_seq = np.zeros((1,1))\n    decoder_seq[0,0] = a_tok.word_index['sos']\n    \n    generated_sent = ''\n    stop_condition= False\n    \n    while not stop_condition:\n        output,h_state,c_state = gen_decoder.predict([decoder_seq]+[values,h,c])\n        sampled_index = np.argmax(output[0,-1,:])\n        sampled_word = a_tok.index_word[sampled_index]\n        \n        if sampled_word != 'eos':\n            generated_sent = generated_sent + sampled_word + ' '\n        \n        if sampled_word == 'eos' or len(generated_sent) >= answer_sequence_size:\n            stop_condition=True\n        \n        decoder_seq = np.zeros((1,1))\n        decoder_seq[0,0] = sampled_index\n        h,c = h_state,c_state\n        \n    return generated_sent.strip()\n\nprint(generate_from_input(train_encoder_input[24].reshape(1,question_sequence_size,-1)))\nprint(generate_from_input(train_encoder_input[1021].reshape(1,question_sequence_size,-1)))\nprint(generate_from_input(train_encoder_input[3001].reshape(1,question_sequence_size,-1)))","17693c4b":"#results on train dataset\nsample_indices= [5,26,7,11,735,662,115,321]\n\nfor idx in sample_indices:\n    generated_sent = generate_from_input(train_encoder_input[idx:idx+1])\n    print(f\"Question >> {seq2question(train_encoder_input[idx])}\")\n    print(f\"Answer(Generated) >> {generated_sent}\")\n    print(f\"Answer(Label) >> {seq2answer(train_decoder_input[idx])}\")\n    print(\"=\"*45)\n    print(\"\\n\")","a5a918c7":"#results on test dataset\nsample_indices= [159,29,44,33]\n\nfor idx in sample_indices:\n    generated_sent = generate_from_input(test_encoder_input[idx:idx+1])\n    print(f\"Question >> {seq2question(test_encoder_input[idx])}\")\n    print(f\"Answer(Generated) >> {generated_sent}\")\n    print(f\"Answer(Label) >> {seq2answer(test_decoder_input[idx])}\")\n    print(\"=\"*45)\n    print(\"\\n\")","63f3c85b":"**[With Attention]**\n","edfa707f":"**[Encoder-Decoder Model without Attention]**\n* Use Glove Embedding Vectors -> Not using pre trained Glove Embedding Vectors","5ad9e75f":"**[Text Preprocessing]**\n* lowercase\n* remove characters which are not english or numbers\n"}}