{"cell_type":{"baef6c7b":"code","17c59e4f":"code","c2265477":"code","e24d6128":"code","79d120d9":"code","d6ef99ae":"code","d41d0cce":"code","43781a72":"code","e4ba3ee7":"code","cce2d4de":"code","a7d48110":"code","787a0ee6":"code","ef21b401":"code","8d86e7b2":"code","2b946bb3":"code","19febe50":"code","169602f4":"code","b1e749b0":"code","8f4f6f56":"markdown","410308fc":"markdown","dbcee14f":"markdown","6b2d093e":"markdown","96f54670":"markdown","878ad4f7":"markdown","2298e05b":"markdown","446b769d":"markdown","90a28c1f":"markdown","1d6667d8":"markdown","7afb40eb":"markdown","82a09222":"markdown","df174dcb":"markdown","79130037":"markdown"},"source":{"baef6c7b":"import gc\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom kaggle.competitions import twosigmanews\npd.set_option('max_columns', 50)","17c59e4f":"env = twosigmanews.make_env()\nmarket_train, news_train = env.get_training_data()","c2265477":"start = datetime(2009, 1, 1, 0, 0, 0).date()\nmarket_train = market_train.loc[market_train['time'].dt.date >= start].reset_index(drop=True)\nnews_train = news_train.loc[news_train['time'].dt.date >= start].reset_index(drop=True)","e24d6128":"market_train.head(3)","79d120d9":"news_train.head(3)","d6ef99ae":"def preprocess_news(news_train):\n    drop_list = [\n        'audiences', 'subjects', 'assetName',\n        'headline', 'firstCreated', 'sourceTimestamp',\n    ]\n    news_train.drop(drop_list, axis=1, inplace=True)\n    \n    # Factorize categorical columns\n    for col in ['headlineTag', 'provider', 'sourceId']:\n        news_train[col], uniques = pd.factorize(news_train[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_train = preprocess_news(news_train)","d41d0cce":"def unstack_asset_codes(news_train):\n    codes = []\n    indexes = []\n    for i, values in news_train['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train)\nindex_df.head()","43781a72":"def merge_news_on_index(news_train, index_df):\n    news_train['news_index'] = news_train.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train, index_df)\ndel news_train, index_df\ngc.collect()\nnews_unstack.head(3)","e4ba3ee7":"def group_news(news_frame):\n    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack; gc.collect()\nnews_agg.head(3)","cce2d4de":"market_train['date'] = market_train.time.dt.date\ndf = market_train.merge(news_agg, how='left', on=['assetCode', 'date'])\ndel market_train, news_agg\ngc.collect()\ndf.head(3)","a7d48110":"def custom_metric(date, pred_proba, num_target, universe):\n    y = pred_proba*2 - 1\n    r = num_target.clip(-1,1) # get rid of outliers\n    x = y * r * universe\n    result = pd.DataFrame({'day' : date, 'x' : x})\n    x_t = result.groupby('day').sum().values\n    return np.mean(x_t) \/ np.std(x_t)","787a0ee6":"date = df.date\nnum_target = df.returnsOpenNextMktres10.astype('float32')\nbin_target = (df.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = df.universe.astype('int8')\n# Drop columns that are not features\ndf.drop(['returnsOpenNextMktres10', 'date', 'universe', 'assetCode', 'assetName', 'time'], \n        axis=1, inplace=True)\ndf = df.astype('float32')  # Set all remaining columns to float32 datatype\ngc.collect()","ef21b401":"train_index, test_index = train_test_split(df.index.values, test_size=0.1, shuffle=False)","8d86e7b2":"def evaluate_model(df, target, train_index, test_index, params):\n    params['n_jobs'] = 2  # Use 2 cores\/threads\n    #model = XGBClassifier(**params)\n    model = LGBMClassifier(**params)\n    model.fit(df.iloc[train_index], target.iloc[train_index])\n    return log_loss(target.iloc[test_index], model.predict_proba(df.iloc[test_index]))","2b946bb3":"param_grid = {\n    'learning_rate': [0.15, 0.1, 0.05, 0.02, 0.01],\n    'num_leaves': [i for i in range(12, 90, 6)],\n    'n_estimators': [50, 200, 400, 600, 800],\n    'min_child_samples': [i for i in range(10, 100, 10)],\n    'colsample_bytree': [0.8, 0.9, 0.95, 1],\n    'subsample': [0.8, 0.9, 0.95, 1],\n    'reg_alpha': [0.1, 0.2, 0.4, 0.6, 0.8],\n    'reg_lambda': [0.1, 0.2, 0.4, 0.6, 0.8],\n}\n\nbest_eval_score = 0\nfor i in range(100):  # Hundred runs\n    params = {k: np.random.choice(v) for k, v in param_grid.items()}\n    score = evaluate_model(df, bin_target, train_index, test_index, params)\n    if score < best_eval_score or best_eval_score == 0:\n        best_eval_score = score\n        best_params = params\nprint(\"Best evaluation logloss\", best_eval_score)","19febe50":"# Train model with full data\nclf = LGBMClassifier(**best_params)\nclf.fit(df, bin_target)","169602f4":"def write_submission(model, env):\n    days = env.get_prediction_days()\n    for (market_obs_df, news_obs_df, predictions_template_df) in days:\n        news_obs_df = preprocess_news(news_obs_df)\n        # Unstack news\n        index_df = unstack_asset_codes(news_obs_df)\n        news_unstack = merge_news_on_index(news_obs_df, index_df)\n        # Group and and get aggregations (mean)\n        news_obs_agg = group_news(news_unstack)\n\n        # Join market and news frames\n        market_obs_df['date'] = market_obs_df.time.dt.date\n        obs_df = market_obs_df.merge(news_obs_agg, how='left', on=['assetCode', 'date'])\n        del market_obs_df, news_obs_agg, news_obs_df, news_unstack, index_df\n        gc.collect()\n        obs_df = obs_df[obs_df.assetCode.isin(predictions_template_df.assetCode)]\n        \n        # Drop cols that are not features\n        feats = [c for c in obs_df.columns if c not in ['date', 'assetCode', 'assetName', 'time']]\n\n        preds = model.predict_proba(obs_df[feats])[:, 1] * 2 - 1\n        sub = pd.DataFrame({'assetCode': obs_df['assetCode'], 'confidence': preds})\n        predictions_template_df = predictions_template_df.merge(sub, how='left').drop(\n            'confidenceValue', axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n        \n        env.predict(predictions_template_df)\n        del obs_df, predictions_template_df, preds, sub\n        gc.collect()\n    env.write_submission_file()\n    \nwrite_submission(clf, env)","b1e749b0":"feat_importance = pd.DataFrame()\nfeat_importance[\"feature\"] = df.columns\nfeat_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\nfeat_importance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(8,10))\nax = sns.barplot(y=\"feature\", x=\"gain\", data=feat_importance)","8f4f6f56":"We can use the last 10% of data (aprox one year) to validate our model. You have to be careful when using different validation techniques like KFold since the time is important here.","410308fc":"Now we can merge the news on this frame:","dbcee14f":"<h2>2. Preprocessing News<\/h2>\n\nWe are going to remove some columns for now and apply label encoding to a few others:","6b2d093e":"<h2>1. Introduction and loading data<\/h2>\n\nThere are two sets of data in this 'kernels only' competition: News and Prices\/Returns. The ideia is to use both sets to predict the movement of a given financial asset in the next 10 days. We have data from 2007 to 2017 for training and must predict the movement of assets from Jan 2017 to July 2019.\n\nTwo Sigma and Kaggle created a custom package for this competition:","96f54670":"Drop columns that we don't need and set type to float32:","878ad4f7":"<h2>5. Merge on Market data<\/h2>\n\nThe final preprocessing step is to merge news data with market data:","2298e05b":"<h2>6. Train GBM model <\/h2>\n\nThis competition has a custom metric (check the evaluation tab). The following function returns the custom metric from the probability of each example being positive (or 1).","446b769d":"<h2>8. Feature importance<\/h2>\n\nWe can use Seaborn to plot the feature importance (with gain or number of splits criteria):","90a28c1f":"TODO\n\n* Use custom metric\n* Improve aggregations\n\nThanks for reading! Please upvote if you find usefull.","1d6667d8":"Let's remove data before 2009 (optional):","7afb40eb":"We can use a simple random search to find some hyperparameters:","82a09222":"<h2>4. Group by date and asset<\/h2>\n\nThere can be many News for a single date and asset, so we need to group this data. I'll be using a simple mean, but you can use more intelligent features.","df174dcb":"<h2>3. Unstacking news<\/h2>\n\nAssets are actually a list of codes in the news frame, but we need to merge with market data which has individual asset codes. Therefore, we are going to unstack each asset code and save the original index with the following function. This is probably not the best way of doing that, but it is simple:","79130037":"<h2>7. Make predictions and submit<\/h2>\n\nIn order to make predictions for the test set we must use the *env.predict* function and for our final submission *env.write_submission_file*. Otherwise, the script will fail in the second stage, when Kaggle will replace the \"fake\" data for 2019 market data and run our scripts again."}}