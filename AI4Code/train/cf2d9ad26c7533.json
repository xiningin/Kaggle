{"cell_type":{"1a789ca3":"code","d73919e3":"code","ebeb685d":"code","ea733ad4":"code","e7481d56":"code","fe8ee96f":"code","8ab7899d":"code","555f38ff":"code","2d255214":"code","8a8917ec":"code","553162c0":"code","458b74e4":"code","a2b7c984":"code","4c66989e":"code","94e2d685":"code","a5bb75cc":"code","b0ce534c":"code","e0054ae0":"markdown","50e9a58b":"markdown","90a445d8":"markdown","15780600":"markdown","078dcf07":"markdown","08e0a691":"markdown","bd6599ef":"markdown","e9474f82":"markdown","60721c53":"markdown","fa32273d":"markdown","94e0ab4a":"markdown","c6f8303e":"markdown","21d5f7e9":"markdown","f479c378":"markdown","10469a81":"markdown","e702654c":"markdown","5ac3c9b2":"markdown","91cafe3c":"markdown","c3c9952e":"markdown","bd6a63d1":"markdown"},"source":{"1a789ca3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, skew #for some statistics\n\n# importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.metrics import mean_squared_error #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","d73919e3":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\ntrain_data.head(5)","ebeb685d":"fig, axarr = plt.subplots(2, 2, figsize = (12, 8))\ntrain_data.plot.scatter(\n    x=\"GrLivArea\", \n    y=\"SalePrice\", \n    ax=axarr[0][0]\n)\ntrain_data.plot.scatter(\n    x=\"BsmtFinSF1\", \n    y=\"SalePrice\", \n    ax=axarr[0][1]\n)\ntrain_data.plot.scatter(\n    x=\"LotArea\", \n    y=\"SalePrice\", \n    ax=axarr[1][0]\n)\ntrain_data.plot.scatter(\n    x=\"GarageArea\", \n    y=\"SalePrice\", \n    ax=axarr[1][1]\n)","ea733ad4":"# drop outliers\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)\n# train_data = train_data.drop(train_data[(train_data['BsmtFinSF1']>3000)].index)\ntrain_data = train_data.drop(train_data[(train_data['LotArea']>150000)].index)\ntrain_data = train_data.drop(train_data[(train_data['GarageArea']>1200) & (train_data['SalePrice']<300000)].index)\nfig, axarr = plt.subplots(2, 2, figsize = (12, 8))\ntrain_data.plot.scatter(\n    x=\"GrLivArea\", \n    y=\"SalePrice\", \n    ax=axarr[0][0]\n)\ntrain_data.plot.scatter(\n    x=\"BsmtFinSF1\", \n    y=\"SalePrice\", \n    ax=axarr[0][1]\n)\ntrain_data.plot.scatter(\n    x=\"LotArea\", \n    y=\"SalePrice\", \n    ax=axarr[1][0]\n)\ntrain_data.plot.scatter(\n    x=\"GarageArea\", \n    y=\"SalePrice\", \n    ax=axarr[1][1]\n)","e7481d56":"sns.distplot(train_data['SalePrice']);","fe8ee96f":"train_data['SalePrice'] = np.log1p(train_data['SalePrice'])\nsns.distplot(train_data['SalePrice']);","8ab7899d":"# data preprocessing\nId = test_data['Id']\ntrain_y = train_data.SalePrice.values\n# print(train_y)\nall_data = pd.concat((train_data, test_data), sort=False).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\nall_data.head(5)","555f38ff":"# drop id\nall_data = all_data.drop('Id', axis=1)\n\n# drop NAN that missing ratio is above a certain threshold\nmissing_data = all_data.isnull().sum()\nmissing_data = missing_data.drop(missing_data[missing_data == 0].index)\nmissing_ratio = missing_data \/ len(all_data) * 100\n# print(missing_ratio)\nall_data = all_data.drop(missing_ratio[missing_ratio.values > 20].index, axis=1)\n# all_data = all_data.drop(missing_data[missing_data.iloc[:] > 0].index, axis=1)\nall_data.head(5)","2d255214":"missing_data = all_data.isnull().sum()\nmissing_data = missing_data.drop(missing_data[missing_data == 0].index)\nmissing_ratio = missing_data \/ len(all_data) * 100\nprint(missing_ratio)\nall_data[missing_ratio.index].head(5)","8a8917ec":"# LotFrontage has more missing value, thus we consider it more delicately\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n# These features are useless, drop them\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\n\n# These features, we just fill them with common case\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna(all_data['MasVnrType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n#in these features, NAN means none\nall_data['BsmtQual'] = all_data['BsmtQual'].fillna('None')\nall_data['BsmtCond'] = all_data['BsmtCond'].fillna('None')\nall_data['BsmtExposure'] = all_data['BsmtExposure'].fillna('None')\nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].fillna('None')\nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].fillna('None')\nall_data['GarageType'] = all_data['GarageType'].fillna('None')\nall_data['GarageFinish'] = all_data['GarageFinish'].fillna('None')\nall_data['GarageQual'] = all_data['GarageQual'].fillna('None')\nall_data['GarageCond'] = all_data['GarageCond'].fillna('None')\n\n#in these features, NAN means 0\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0)\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0)\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0)\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\nall_data['GarageCars'] = all_data['GarageCars'].fillna(0)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(0)\n\n# all_data = all_data.drop(missing_ratio[missing_ratio.values > 0].index, axis=1)\n# missing_data = all_data.isnull().sum()\n# missing_data = missing_data.drop(missing_data[missing_data == 0].index)\n# missing_ratio = missing_data \/ len(all_data) * 100\n# print(missing_ratio)\n# all_data[missing_ratio.index].head(5)","553162c0":"all_data = pd.get_dummies(all_data)\nall_data.head(5)","458b74e4":"ntrain = train_data.shape[0]\nntest = test_data.shape[0]\n# train, test = train_test_split(all_data, test_size=0.4998)\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\ntrain_x = train\nprint(train_x.shape[0], train_y.shape[0])\n# train_y\n","a2b7c984":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, train_y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","4c66989e":"model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","94e2d685":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","a5bb75cc":"model.fit(train_x, train_y)\ntrain_prediction = model.predict(train)\nprediction = np.expm1(model.predict(test.values))\nprint(rmsle(train_y, train_prediction))\n# print(prediction)","b0ce534c":"submission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('submission.csv', index=False)","e0054ae0":"By simply adding a log transformation, my place in competition jumed almost 2000 forward!","50e9a58b":"**Handling missing value :**","90a445d8":"Now let's concat train data and test data, and save a copy of SalePrice and Id :","15780600":"saleprice :","078dcf07":"**Select an algorithm :**","08e0a691":"2. deal with the rest of the missing value :","bd6599ef":"**Cross validation :**","e9474f82":"# **SalePrice prediction :**","60721c53":"Some basic setup :","fa32273d":"Done. Submit the answer.","94e0ab4a":"**Mean square error validation :**","c6f8303e":"Apply **Log transfomation** to SalePrice:","21d5f7e9":"At my first attempt, I dropped all the columns that contain missing value. That's one way.\nThe next attemt I tried to simply fill them with either some common value, or 0, or None. That's improves my placement in competition by another 1000! ","f479c378":"**Train the selected model :**","10469a81":"1. drop columns that missing percent is too high or unnecessary :","e702654c":"I picked some of the features that seem to be most affected to the salePrice,\nand then plot the pictures:","5ac3c9b2":"**Split to train and test data :**","91cafe3c":"As we can see, some of the data is terribly strange, thus I remove them.\nBut sometimes it'll be worse if we done this too much, like 'yearBuilt', I thought the two dots at the top would be outliers, but it turns out that removing them just makes the result worse.","c3c9952e":"Input data :","bd6a63d1":"**Remove Outliers :**"}}