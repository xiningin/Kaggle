{"cell_type":{"b1868268":"code","4282c3f7":"code","3157a448":"code","d1ff05a9":"code","592ec8f0":"code","099d3c97":"code","e598a9d9":"code","a147f923":"code","a14e1a74":"code","d150b8fe":"markdown","51ab52bd":"markdown"},"source":{"b1868268":"import copy\nimport os\nimport time\nimport traceback\nfrom contextlib import contextmanager\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\n!pip install nyaggle\nfrom nyaggle.validation import StratifiedGroupKFold\nfrom sklearn.cluster import AgglomerativeClustering\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","4282c3f7":"TOKEN2INT = {x: i for i, x in enumerate('().ACGUBEHIMSX')}\nPRED_COLS_SCORED = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\nPRED_COLS = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\nDATA_DIR = \"..\/input\/stanford-covid-vaccine\/\"\nREPLACE_DATA_PATH = \"..\/input\/eternafold\/eternafold_mfe.csv\"\nPRIMARY_BPPS_DIR = \"..\/input\/eternafold\/bpps\/\"\nSECONDARY_BPPS_DIR = \"..\/input\/bpps-by-viennat70\/\"\nNFOLDS = 7\nBATCH_SIZE = 64\nTRAIN_EPOCHS = 140","3157a448":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","d1ff05a9":"class SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel \/\/ reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel \/\/ reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1)\n        return x * y.expand_as(x)\n\n\nclass Conv(nn.Module):\n    def __init__(self, d_in, d_out, kernel_size, dropout=0.1):\n        super().__init__()\n        self.conv = nn.Conv1d(d_in, d_out, kernel_size=kernel_size, padding=kernel_size \/\/ 2)\n        self.bn = nn.BatchNorm1d(d_out)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        return self.dropout(self.relu(self.bn(self.conv(src))))\n\n\nclass ResidualGraphAttention(nn.Module):\n    def __init__(self, d_model, kernel_size, dropout):\n        super().__init__()\n        self.conv1 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.conv2 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, src, attn):\n        h = self.conv2(self.conv1(torch.bmm(src, attn)))\n        return self.relu(src + h)\n    \n\nclass SEResidual(nn.Module):\n    def __init__(self, d_model, kernel_size, dropout):\n        super().__init__()\n        self.conv1 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.conv2 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.relu = nn.ReLU()\n        self.se = SELayer(d_model)\n\n    def forward(self, src):\n        h = self.conv2(self.conv1(src))\n        return self.se(self.relu(src + h))\n\n\nclass FusedEmbedding(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.emb = nn.Embedding(len(TOKEN2INT), n_emb)\n        self.n_emb = n_emb\n\n    def forward(self, src, se):\n        # src: [batch, seq, feature]\n        # se: [batch, seq]\n        embed = self.emb(src)\n        embed = embed.reshape((-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        embed = torch.cat((embed, se), 2)\n\n        return embed\n\n    @property\n    def d_out(self):\n        d_emb = 3 * self.n_emb\n        d_feat = 2 * 5  # max, sum, 2nd, 3rd, nb_count\n        return d_emb + d_feat\n    \n\nclass ConvModel(nn.Module):\n    def __init__(self, d_emb=50, d_model=256, dropout=0.6, dropout_res=0.4, dropout_emb=0.0,\n                 kernel_size_conv=7, kernel_size_gc=7):\n        super().__init__()\n\n        self.embedding = FusedEmbedding(d_emb)\n        self.dropout = nn.Dropout(dropout_emb)\n        self.conv = Conv(self.embedding.d_out, d_model, kernel_size=3, dropout=dropout)\n\n        self.block1 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block2 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block3 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block4 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block5 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n\n        self.attn1 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn2 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn3 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn4 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n\n        self.linear = nn.Linear(d_model, len(PRED_COLS))\n\n    def forward(self, \n                src: torch.Tensor, \n                features: torch.Tensor, \n                bpps: torch.Tensor, \n                adj: torch.Tensor):\n        # src: [batch, seq, 3]\n        # features: [batch, seq, 10]\n        # bpps: [batch, seq, seq, 2]\n        # adj: [batch, seq, seq]\n        \n        x = self.dropout(self.embedding(src, features))\n        x = x.permute([0, 2, 1])  # [batch, d-emb, seq]\n        \n        x = self.conv(x)\n        x = self.block1(x)\n        x = self.attn1(x, adj)\n        x = self.block2(x)\n        x = self.attn2(x, adj)\n        x = self.block3(x)\n        x = self.attn3(x, bpps[:, :, :, 0])\n        x = self.attn4(x, bpps[:, :, :, 1])\n        x = self.block4(x)\n        x = self.block5(x)\n\n        x = x.permute([0, 2, 1])  # [batch, seq, features]\n        out = self.linear(x)\n\n        out = torch.clamp(out, -0.5, 1e8)\n\n        return out\n    \n\n\nclass WRMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, yhat, y, sample_weight=None):\n        l = (yhat - y) ** 2\n\n        if sample_weight is not None:\n            l = l * sample_weight.unsqueeze(dim=1)\n\n        return torch.sqrt(torch.mean(l))\n\n\nclass ColWiseLoss(nn.Module):\n    def __init__(self, base_loss):\n        super().__init__()\n        self.base_loss = base_loss\n        self.len_scored = 68\n\n    def forward(self, yhat, y, column_weight=None, sample_weight=None):\n        score = 0\n        for i in range(len(PRED_COLS)):\n            s = self.base_loss(\n                yhat[:, :self.len_scored, i], \n                y[:, :self.len_scored, i], \n                sample_weight\n            ) \/ len(PRED_COLS)\n            \n            if column_weight is not None:\n                s *= column_weight[i]\n                \n            score += s\n        return score\n\n\nclass MCRMSELoss(ColWiseLoss):\n    def __init__(self):\n        super().__init__(WRMSELoss())","592ec8f0":"@contextmanager\ndef timer(name):\n    s = time.time()\n    yield\n    print(f\"{name}: {time.time() - s:.3f}sec\")\n\n\ndef pandas_list_to_array(df: pd.DataFrame) -> np.ndarray:\n    return np.transpose(\n        np.array(\n            df.values\n                .tolist()\n        ),\n        (0, 2, 1)\n    )\n\n\ndef preprocess_inputs(df: pd.DataFrame) -> np.ndarray:\n    return pandas_list_to_array(\n        df[['sequence', 'structure', 'predicted_loop_type']]\n            .applymap(lambda seq: [TOKEN2INT[x] for x in seq])\n    )\n\n\ndef build_adj_matrix(src_df: pd.DataFrame, normalize: bool = True) -> np.ndarray:\n    n = len(src_df['structure'].iloc[0])\n    mat = np.zeros((len(src_df), n, n))\n    start_token_indices = []\n\n    for r, structure in tqdm(enumerate(src_df['structure'])):\n        for i, token in enumerate(structure):\n            if token == \"(\":\n                start_token_indices.append(i)\n            elif token == \")\":\n                j = start_token_indices.pop()\n                mat[r, i, j] = 1\n                mat[r, j, i] = 1\n\n    assert len(start_token_indices) == 0\n\n    if normalize:\n        mat = mat \/ (mat.sum(axis=2, keepdims=True) + 1e-8)\n\n    return mat\n\n\ndef replace_data(train_df: pd.DataFrame, test_df: pd.DataFrame, replace_data_dir: str):\n    print(f\"using data from {replace_data_dir}\")\n\n    aux = pd.read_csv(replace_data_dir)\n    del train_df['structure']\n    del train_df['predicted_loop_type']\n    del test_df['structure']\n    del test_df['predicted_loop_type']\n    train_df = pd.merge(train_df, aux, on='id', how='left')\n    test_df = pd.merge(test_df, aux, on='id', how='left')\n    assert len(train_df) == 2400\n    assert len(test_df) == 3634\n    assert train_df['structure'].isnull().sum() == 0\n    assert train_df['predicted_loop_type'].isnull().sum() == 0\n    assert test_df['structure'].isnull().sum() == 0\n    assert test_df['predicted_loop_type'].isnull().sum() == 0\n    return train_df, test_df\n\n\ndef load_bpps(df: pd.DataFrame, data_dir: str) -> np.ndarray:\n    return np.array([np.load(f'{data_dir}bpps\/{did}.npy') for did in df.id])\n\n\ndef make_bpps_features(bpps_list: List[np.ndarray]) -> np.ndarray:\n    ar = []\n\n    for b in bpps_list:\n        ar.append(b.sum(axis=2))\n\n        # max, 2ndmax, 3rdmax\n        bpps_sorted = np.sort(b, axis=2)[:, :, ::-1]\n        ar.append(bpps_sorted[:, :, 0])\n        ar.append(bpps_sorted[:, :, 1])\n        ar.append(bpps_sorted[:, :, 2])\n\n        # number of nonzero\n        bpps_nb_mean = 0.077522  # mean of bpps_nb across all training data\n        bpps_nb_std = 0.08914  # std of bpps_nb across all training data\n        nb = (b > 0).sum(axis=2)\n        nb = (nb - bpps_nb_mean) \/ bpps_nb_std\n        ar.append(nb)\n\n    return np.transpose(np.array(ar), (1, 2, 0))\n\n\ndef make_dataset(device, x: np.ndarray, y: np.ndarray,\n                 bpps_primary: np.ndarray,\n                 bpps_secondary: np.ndarray,\n                 adj_matrix: np.ndarray,\n                 prediction_mask: np.ndarray,\n                 signal_to_noise=None):\n    x = copy.deepcopy(x)\n    if y is not None:\n        y = copy.deepcopy(y)\n    bpps_primary = copy.deepcopy(bpps_primary)\n    bpps_secondary = copy.deepcopy(bpps_secondary)\n    bpps = np.concatenate([\n        bpps_primary[:, :, :, np.newaxis],\n        bpps_secondary[:, :, :, np.newaxis]\n    ], axis=-1)\n\n    adj_matrix = copy.deepcopy(adj_matrix)\n    prediction_mask = copy.deepcopy(prediction_mask)\n\n    if y is not None:\n        y = np.clip(y, -0.5, 10)\n        mask = np.abs(y).max(axis=(1, 2)) < 10\n    else:\n        mask = [True] * len(x)\n\n    tensors = [\n        torch.LongTensor(x[mask]),\n        torch.Tensor(make_bpps_features([bpps_primary[mask], bpps_secondary[mask]])),\n        torch.Tensor(bpps[mask]),\n        torch.Tensor(adj_matrix[mask]),\n        torch.Tensor(prediction_mask[mask])\n    ]\n\n    if y is not None:\n        tensors.append(torch.Tensor(y[mask]))\n        \n        sample_weight = np.clip(np.log(signal_to_noise[mask] + 1.1) \/ 2, 0, 100)\n        tensors.append(torch.Tensor(sample_weight))\n\n    return torch.utils.data.TensorDataset(*[t.to(device) for t in tensors])\n\n\ndef make_dataset_from_df(device, df: pd.DataFrame, bpps_dir: str, secondary_bpps_dir: str):\n    assert df['seq_scored'].nunique() == 1\n\n    inputs = preprocess_inputs(df)\n    bpps = load_bpps(df, bpps_dir)\n    adj = build_adj_matrix(df)\n    secondary_bpps = load_bpps(df, secondary_bpps_dir)\n\n    mask = np.zeros((len(df), len(df['sequence'].iloc[0]), len(PRED_COLS)))\n    mask[:, :df['seq_scored'].iloc[0], :] = 1\n\n    return make_dataset(device, inputs, None, bpps, secondary_bpps, adj, mask)\n\n\ndef dist(s1: str, s2: str) -> int:\n    return sum([c1 != c2 for c1, c2 in zip(s1, s2)])\n\n\ndef get_distance_matrix(s: pd.Series) -> np.ndarray:\n    mat = np.zeros((len(s), len(s)))\n\n    for i in tqdm(range(len(s))):\n        for j in range(i + 1, len(s)):\n            mat[i, j] = mat[j, i] = dist(s[i], s[j])\n    return mat\n\n\ndef batch_predict(model: nn.Module, loader: DataLoader) -> np.ndarray:\n    y_preds = np.zeros((len(loader.dataset), loader.dataset[0][0].shape[0], len(PRED_COLS)))\n\n    for i, (x_batch, x_se, x_bpps, x_adj, y_mask) in enumerate(loader):\n        y_pred = model(x_batch, x_se, x_bpps, x_adj).detach() * y_mask\n        y_preds[i * loader.batch_size:(i + 1) * loader.batch_size, :, :] = y_pred.cpu().numpy()\n        \n    return y_preds\n\n\ndef calc_loss(y_true: np.ndarray, y_pred: np.ndarray):\n    err_w_valid = [1 if s in PRED_COLS_SCORED else 0 for s in PRED_COLS]\n    raw = MCRMSELoss()(torch.Tensor(y_pred), torch.Tensor(y_true), err_w_valid).item()\n    \n    return raw * len(PRED_COLS) \/ len(PRED_COLS_SCORED)","099d3c97":"def train_model(model, train_loader, valid_loader, y_valid,\n                train_epochs, train_loss, verbose=True,\n                model_path='model'):\n    params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f'number of params: {params}')\n\n    err_w_train_1 = [1 if s in PRED_COLS_SCORED else 1.0 for s in PRED_COLS]\n    err_w_train_2 = [1 if s in PRED_COLS_SCORED else 0.01 for s in PRED_COLS]\n\n    criterion_train = train_loss\n    optimizer = torch.optim.Adam(model.parameters())\n\n    losses = []\n    val_losses = []\n    y_preds_best = None\n\n    for epoch in range(train_epochs):\n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        for x_batch, x_se, x_bpps, x_adj, y_mask, y_batch, sample_weight in tqdm(train_loader, disable=True):\n            y_pred = model(x_batch, x_se, x_bpps, x_adj) * y_mask\n            \n            # use 5 columns for the first 30 epoch\n            w = err_w_train_1 if epoch < 30 else err_w_train_2\n            \n            loss = criterion_train(y_pred, y_batch, w, sample_weight)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n\n        model.eval()\n        y_preds = batch_predict(model, valid_loader)\n        mcloss = calc_loss(y_valid, y_preds)\n        val_losses.append(mcloss)\n        \n        s = f\"{epoch:03d}: trn:{avg_loss:.4f}, clean={mcloss:.4f}, {time.time() - start_time:.2f}s\"\n\n        losses.append(avg_loss)\n\n        if np.min(val_losses) == mcloss:\n            y_preds_best = y_preds\n            torch.save(model.state_dict(), model_path)\n\n        if (isinstance(verbose, bool) and verbose) or (verbose > 0 and (epoch % verbose == 0)):\n            print(s)\n\n    print(f'min val_loss: {np.min(val_losses):.4f} at {np.argmin(val_losses) + 1} epoch')\n\n    # recover best weight\n    model.load_state_dict(torch.load(model_path))\n\n    if not verbose:\n        return np.min(val_losses)\n\n    fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n\n    ax[0].plot(np.arange(1, len(losses) + 1), losses)\n    ax[1].plot(np.arange(1, len(val_losses) + 1), val_losses)\n\n    for i, p in enumerate(PRED_COLS):\n        ax[2].scatter(y_valid[:, :, i].flatten(), y_preds_best[:, :, i].flatten(), alpha=0.5)\n\n    ax[0].legend(['train'])\n    ax[1].legend(['valid(clean)'])\n    ax[2].legend(PRED_COLS)\n    ax[2].set_xlabel('y_true')\n    ax[2].set_ylabel('y_predicted')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[2].set_xlabel('y_true(clean)')\n    ax[2].set_ylabel('y_predicted(clean)')\n    plt.show()\n\n    return np.min(val_losses)","e598a9d9":"with timer(\"load data\"):\n    train_df = pd.read_json(DATA_DIR + 'train.json', lines=True)\n    test_df = pd.read_json(DATA_DIR + 'test.json', lines=True)\n    sample_df = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n    train_df, test_df = replace_data(train_df, test_df, REPLACE_DATA_PATH)\n\nwith timer(\"clustering\"):\n    # use clustering based on edit distance\n    seq_dist = get_distance_matrix(train_df['sequence'])\n    clf = AgglomerativeClustering(n_clusters=None, \n                                  distance_threshold=20, \n                                  affinity='precomputed',\n                                  linkage='average')\n    group_index = clf.fit_predict(seq_dist)","a147f923":"with timer(\"preprocess\"):\n    public_df = test_df.query(\"seq_length != 130\")\n    private_df = test_df.query(\"seq_length == 130\")\n\n    x = preprocess_inputs(train_df)\n    y = pandas_list_to_array(train_df[PRED_COLS])\n\n    label_mask = np.ones_like(y)\n    pad = np.zeros((y.shape[0], x.shape[1] - y.shape[1], y.shape[2]))\n    y = np.concatenate((y, pad), axis=1)\n    label_mask = np.concatenate((label_mask, pad), axis=1)\n\n    assert x.shape[1] == y.shape[1]\n\n    train_adj = build_adj_matrix(train_df)\n    primary_bpps = load_bpps(train_df, PRIMARY_BPPS_DIR)    \n    secondary_bpps = load_bpps(train_df, SECONDARY_BPPS_DIR)\n\n    public_data = make_dataset_from_df(device, public_df, PRIMARY_BPPS_DIR, SECONDARY_BPPS_DIR)\n    private_data = make_dataset_from_df(device, private_df, PRIMARY_BPPS_DIR, SECONDARY_BPPS_DIR)","a14e1a74":"kf = StratifiedGroupKFold(NFOLDS, random_state=42, shuffle=True)\n\npred_oof = np.zeros_like(y)\npred_public = np.zeros((len(public_data), len(public_df['sequence'].iloc[0]), len(PRED_COLS)))\npred_private = np.zeros((len(private_data), len(private_df['sequence'].iloc[0]), len(PRED_COLS)))\n\npublic_loader = DataLoader(public_data, batch_size=128, shuffle=False)\nprivate_loader = DataLoader(private_data, batch_size=128, shuffle=False)\n\nclean_idx = [i for i in range(len(train_df)) if train_df['SN_filter'].iloc[i]]\nsn_mask = train_df['SN_filter'] == 1\n\ncriterion_train = MCRMSELoss()\nmodel_path = \"model_fold{}\"\n\nlosses = []\n\nfor i, (train_index, valid_index) in enumerate(kf.split(x, train_df['SN_filter'], groups=group_index)):\n    print(f'fold {i}')\n    model = ConvModel().to(device)\n    s = time.time()\n\n    train_data = make_dataset(device, x[train_index], y[train_index],\n                              primary_bpps[train_index], secondary_bpps[train_index],\n                              train_adj[train_index],\n                              label_mask[train_index],\n                              signal_to_noise=train_df['signal_to_noise'][train_index].values)\n\n    valid_index_c = [v for v in valid_index if v in clean_idx]\n    valid_data_clean = make_dataset(device, x[valid_index_c], None,\n                                    primary_bpps[valid_index_c],\n                                    secondary_bpps[valid_index_c],\n                                    train_adj[valid_index_c],\n                                    label_mask[valid_index_c])\n    valid_data_noisy = make_dataset(device, x[valid_index], None,\n                                  primary_bpps[valid_index],\n                                  secondary_bpps[valid_index],\n                                  train_adj[valid_index],\n                                  label_mask[valid_index])\n\n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    valid_loader_clean = DataLoader(valid_data_clean, batch_size=128, shuffle=False)\n    valid_loader_noisy = DataLoader(valid_data_noisy, batch_size=128, shuffle=False)\n\n    loss = train_model(model, \n                       train_loader, \n                       valid_loader_clean, \n                       y[valid_index_c], \n                       TRAIN_EPOCHS, \n                       criterion_train,\n                       verbose=5, \n                       model_path=model_path.format(i))\n\n    losses.append(loss)\n\n    # predict\n    pred_oof[valid_index] = batch_predict(model, valid_loader_noisy)\n    pred_public += batch_predict(model, public_loader) \/ NFOLDS\n    pred_private += batch_predict(model, private_loader) \/ NFOLDS\n\n    print(f'elapsed: {time.time() - s:.1f}sec')\n\noof_score = calc_loss(y, pred_oof)\nprint(f'oof(all): {oof_score: .4f}')\n\noof_score = calc_loss(y[sn_mask], pred_oof[sn_mask])\nprint(f'oof(clean): {oof_score: .4f}')\n\n# make submission and oof\npreds_ls = []\n\nfor df, preds in [(public_df, pred_public), (private_df, pred_private)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=PRED_COLS)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()\n\nsubmission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)\n\nnp.save('oof', pred_oof)\nnp.save('public', pred_public)\nnp.save('private', pred_private)\n\nprint(losses)","d150b8fe":"## Load data","51ab52bd":"## Training"}}