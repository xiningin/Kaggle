{"cell_type":{"c50c525f":"code","2c37c04d":"code","fc51d522":"code","20a418b8":"code","63ea8039":"code","bb4ab868":"code","d85832af":"code","12445c6d":"code","d506419a":"code","75dca742":"code","fd0a0fe7":"code","92553d75":"code","9bd936eb":"code","8c3beabc":"code","98accf3b":"code","381b1d7a":"code","82fc77f9":"code","abb87b0b":"code","8e378478":"code","b696b541":"code","60d40966":"code","688bbc12":"code","a12c039d":"code","39ab3a5e":"code","36fc0e47":"code","0e334236":"code","53ef3c8f":"code","46ed6a24":"code","3166e40b":"code","1d9e93be":"code","7ab4080e":"code","04c054dd":"code","3cd51717":"code","0127a3ba":"code","8cb071ba":"code","92b0b641":"code","7fdad4ea":"code","0123d9da":"code","667892ee":"markdown","0773b9b0":"markdown","ef33c127":"markdown","9ebbd0de":"markdown","4b2e8cf2":"markdown","ed23e94c":"markdown","c3dbb27a":"markdown","9e7e86a1":"markdown","dbe246ff":"markdown","36586b06":"markdown","f15a315a":"markdown","e06567b5":"markdown","421a9a2a":"markdown","4c527100":"markdown","f4bc871e":"markdown","ce558de4":"markdown","e7743aa5":"markdown","2c6934fa":"markdown","5d1ebecf":"markdown","849344cc":"markdown","fc87dd37":"markdown","8d15be8a":"markdown","83c59717":"markdown","cb4c4a31":"markdown","cb03595a":"markdown","6e84a765":"markdown","a28742e9":"markdown","519a3f7c":"markdown"},"source":{"c50c525f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c37c04d":"!pip install imutils","fc51d522":"import numpy as np\n#import pandas as pd\nimport tensorflow as tf\nimport cv2\n#import os\nfrom imutils import paths\npath = list(paths.list_images(\"..\/input\/cat-and-dog\/training_set\/training_set\"))\n","20a418b8":"path[1]","63ea8039":"len(path)","bb4ab868":"path[1].split(\"\/\")","d85832af":"(cv2.imread(path[3])).shape","12445c6d":"X_data = []\nY_labels = []\n\ny_data = ['cats','dogs']\n\nfor i in range(0,len(path)):\n    label = path[i].split(\"\/\")[-2]\n    if label not in y_data:\n        continue\n    else:\n        image = cv2.imread(path[i])#IMAGE WILL BE SAVED IN ARRAY FORMAT \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)#CONVERT ING INTO RGB FORMAT\n        image = cv2.resize(image, (124,124))#APPLY FIX SIZE TO ALL IMAGES \n        X_data.append(image)\n        Y_labels.append(label)","d506419a":"x_array = np.array(X_data)\ny_array = np.array(Y_labels)","75dca742":"x_array.shape,y_array.shape","fd0a0fe7":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.imshow(x_array[1000])","92553d75":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nLE = LabelEncoder()\nLE.fit(y_array)\nY_Encoded = LE.transform(y_array)","9bd936eb":"x_train, x_test, y_train, y_test = train_test_split(x_array, Y_Encoded, test_size = 0.2,random_state=42)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape ","8c3beabc":"x_train_norm = x_train\/255.\nx_test_norm = x_test\/255.","98accf3b":"model=tf.keras.Sequential()\n\n\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel.add(tf.keras.layers.Dense(500, activation='relu'))\n\nmodel.add(tf.keras.layers.Dense(200, activation='relu'))\n\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\n\nmodel.add(tf.keras.layers.Dense(60, activation='relu'))\n\nmodel.add(tf.keras.layers.Dense(30, activation='relu'))\n\n\n#Output layer\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))","381b1d7a":"\nmodel.compile(optimizer='Adam', \n              loss='binary_crossentropy', metrics=['accuracy'])","82fc77f9":"model.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","abb87b0b":"model_1=tf.keras.Sequential()\n\nmodel_1.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_1.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_1.add(tf.keras.layers.BatchNormalization())\nmodel_1.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_1.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_1.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_1.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_1.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_1.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_1.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_1.add(tf.keras.layers.Dense(1, activation='sigmoid'))","8e378478":"model_1.compile(optimizer='SGD', \n              loss='binary_crossentropy', metrics=['accuracy'])","b696b541":"model_1.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","60d40966":"model_2=tf.keras.Sequential()\n\nmodel_2.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_2.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_2.add(tf.keras.layers.BatchNormalization())\nmodel_2.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_2.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_2.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_2.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_2.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_2.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_2.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_2.add(tf.keras.layers.Dense(1, activation='sigmoid'))","688bbc12":"model_2.compile(optimizer='RMSprop', \n              loss='binary_crossentropy', metrics=['accuracy'])","a12c039d":"model_2.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","39ab3a5e":"model_3=tf.keras.Sequential()\n\nmodel_3.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_3.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_3.add(tf.keras.layers.BatchNormalization())\nmodel_3.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_3.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_3.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_3.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_3.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_3.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_3.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_3.add(tf.keras.layers.Dense(1, activation='sigmoid'))","36fc0e47":"model_3.compile(optimizer='Adadelta', \n              loss='binary_crossentropy', metrics=['accuracy'])","0e334236":"model_3.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","53ef3c8f":"model_4=tf.keras.Sequential()\n\nmodel_4.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_4.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_4.add(tf.keras.layers.BatchNormalization())\nmodel_4.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_4.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_4.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_4.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_4.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_4.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_4.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_4.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n\n\nmodel_4.compile(optimizer='Adagrad', \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_4.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","46ed6a24":"model_5=tf.keras.Sequential()\n\nmodel_5.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_5.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_5.add(tf.keras.layers.BatchNormalization())\nmodel_5.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_5.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_5.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_5.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_5.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_5.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_5.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_5.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n\n\nmodel_5.compile(optimizer='Adamax', \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_5.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","3166e40b":"model_6=tf.keras.Sequential()\n\nmodel_6.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_6.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_6.add(tf.keras.layers.BatchNormalization())\nmodel_6.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_6.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_6.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_6.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_6.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_6.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_6.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_6.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n\n\nmodel_6.compile(optimizer='Nadam', \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_6.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)\n","1d9e93be":"model_7=tf.keras.Sequential()\n\nmodel_7.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_7.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_7.add(tf.keras.layers.BatchNormalization())\nmodel_7.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_7.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_7.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_7.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_7.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_7.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_7.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_7.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#By default learning rate 0.01\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nmodel_7.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_7.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)\n","7ab4080e":"model_8=tf.keras.Sequential()\n\nmodel_8.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_8.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_8.add(tf.keras.layers.BatchNormalization())\nmodel_8.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_8.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_8.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_8.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_8.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_8.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_8.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_8.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.01\nopt =tf.keras.optimizers.SGD(learning_rate=0.001)\n\n\nmodel_8.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_8.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","04c054dd":"model_9=tf.keras.Sequential()\n\nmodel_9.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_9.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_9.add(tf.keras.layers.BatchNormalization())\nmodel_9.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_9.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_9.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_9.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_9.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_9.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_9.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_9.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.001\n\n\nopt =tf.keras.optimizers.RMSprop(learning_rate=0.01)\n\n\nmodel_9.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_9.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","3cd51717":"model_10=tf.keras.Sequential()\n\nmodel_10.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_10.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_10.add(tf.keras.layers.BatchNormalization())\nmodel_10.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_10.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_10.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_10.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_10.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_10.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_10.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_10.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.001\n\nopt =tf.keras.optimizers.Adadelta(learning_rate=0.01)\n\n\n\nmodel_10.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_10.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","0127a3ba":"model_11=tf.keras.Sequential()\n\nmodel_11.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_11.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_11.add(tf.keras.layers.BatchNormalization())\nmodel_11.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_11.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_11.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_11.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_11.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_11.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_11.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_11.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.001\n\nopt =tf.keras.optimizers.Adagrad(learning_rate=0.01)\n\n\n\nmodel_11.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_11.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","8cb071ba":"model_12=tf.keras.Sequential()\n\nmodel_12.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_12.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_12.add(tf.keras.layers.BatchNormalization())\nmodel_12.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_12.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_12.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_12.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_12.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_12.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_12.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_12.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.001\n\nopt =tf.keras.optimizers.Adamax(learning_rate=0.01)\n\n\nmodel_12.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_12.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","92b0b641":"model_13=tf.keras.Sequential()\n\nmodel_13.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3),input_shape=(124,124,3),activation='relu'))\nmodel_13.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel_13.add(tf.keras.layers.BatchNormalization())\nmodel_13.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#Flatten the output\nmodel_13.add(tf.keras.layers.Flatten())\n\n#Dense layer\nmodel_13.add(tf.keras.layers.Dense(500, activation='relu'))\nmodel_13.add(tf.keras.layers.Dense(200, activation='relu'))\nmodel_13.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel_13.add(tf.keras.layers.Dense(60, activation='relu'))\nmodel_13.add(tf.keras.layers.Dense(30, activation='relu'))\n\n#Add another dropout layer\n#model.add(tf.keras.layers.Dropout(0.25))\n\n#Output layer\nmodel_13.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n#by default learning rate 0.001\n\nopt =tf.keras.optimizers.Nadam(learning_rate=0.01)\n\n\nmodel_13.compile(optimizer=opt, \n              loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel_13.fit(x_train_norm,y_train,          \n          validation_data=(x_test_norm,y_test),\n          epochs=7)","7fdad4ea":"df1 = pd.DataFrame()\ndf1['Names'] = ['SGD','RMSprop','Adam','Adadelta','Adagrad','Adamax','Nadam']\n\ndf1['TRAING_ACC_DEF_LR'] =[0.9981,0.9764,0.9781,0.8632,0.9939,0.9998,0.9858]\ndf1['TESTING_ACC_DEF_LR']=[0.7108,0.6889,0.6814,0.6571,0.6477,0.7189,0.7071]\ndf1['TRAINING_ACC_EXP_LR']=[0.9513,0.5030,0.9849,0.9789,0.9941,0.8874,0.4884]\ndf1['TESTING_ACC_EXP_LR']=[0.6921,0.5122,0.6971,0.6939,0.7139,0.6290,0.4897]\ndf1","0123d9da":"import seaborn as sns\nsns.set_theme()\nCM =sns.color_palette(\"light:b\", as_cmap=True)\ndf1.style.background_gradient(cmap=CM)","667892ee":"**OPTIMIZER(SGD WITH EXP LEARNING RATE)**","0773b9b0":"**OPTIMIZER(ADAM WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9781\n* TESTING_ACCURACY:0.6814","ef33c127":"**OPTIMIZER(Nadam WITH DEFAULT LEARNING RATE)**","9ebbd0de":"**OPTIMIZER(Adagrad WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9941\n* TESTING_ACCURACY:0.7139","4b2e8cf2":"**OPTIMIZER(Adamax WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9998\n* TESTING_ACCURACY:0.7189","ed23e94c":"**OPTIMIZER(Adamax WITH DEFAULT LEARNING RATE)**","c3dbb27a":"**OPTIMIZER(SGD WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9981\n* TESTING_ACCURACY:0.7108","9e7e86a1":"**OPTIMIZER(Adagrad WITH EXP LEARNING RATE)**","dbe246ff":"**OPTIMIZER(Adagrad WITH DEFAULT LEARNING RATE)**","36586b06":"**OPTIMIZER(RMSprop WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9764\n* TESTING_ACCURACY:0.6889","f15a315a":"**OPTIMIZER(RMSprop WITH DEFAULT EXP RATE) ACCURACY**\n* TRAINING_ACCURACY:0.5030\n* TESTING_ACCURACY:0.5122","e06567b5":"**OPTIMIZER(Adadelta WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.8632\n* TESTING_ACCURACY:0.6571","421a9a2a":"**OPTIMIZER(Adadelta WITH EXP LEARNING RATE)**","4c527100":"**OPTIMIZER(SGD WITH DEFAULT LEARNING RATE)**","f4bc871e":"**OPTIMIZER(Adamax WITH EXP LEARNING RATE)**","ce558de4":"**OPTIMIZER(Adamax WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.8874\n* TESTING_ACCURACY:0.6290","e7743aa5":"**OPTIMIZER(Nadam WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.4884\n* TESTING_ACCURACY:0.4897","2c6934fa":"**OPTIMIZER(Adadelta WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9789\n* TESTING_ACCURACY:0.6939","5d1ebecf":"**OPTIMIZER(RMSprop WITH DEFAULT LEARNING RATE)**","849344cc":"**OPTIMIZER(RMSprop WITH EXP LEARNING RATE)**","fc87dd37":"**OPTIMIZER(Adam WITH EXP LEARNING RATE)**","8d15be8a":"**OPTIMIZER(Adadelta WITH DEFAULT LEARNING RATE)**","83c59717":"**OPTIMIZER(ADAM WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9849\n* TESTING_ACCURACY:0.6971","cb4c4a31":"**OPTIMIZER(Nadam WITH EXP LEARNING RATE)**","cb03595a":"**OPTIMIZER(SGD WITH EXP LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9513\n* TESTING_ACCURACY:0.6921","6e84a765":"**OPTIMIZER(ADAM WITH DEFAULT LEARNING RATE)**","a28742e9":"**OPTIMIZER(Nadam WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9858\n* TESTING_ACCURACY:0.7071","519a3f7c":"**OPTIMIZER(Adagrad WITH DEFAULT LEARNING RATE) ACCURACY**\n* TRAINING_ACCURACY:0.9939\n* TESTING_ACCURACY:0.6477"}}