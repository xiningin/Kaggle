{"cell_type":{"c36a9f4e":"code","f4df2440":"code","6068e192":"code","3378bfba":"code","03c15e4c":"code","c4b366f6":"code","b8f1d7b6":"code","34eb2c95":"code","d36ba33e":"code","f0ddfcc0":"code","ac3f5180":"code","3c326adf":"code","56148c5f":"code","fa64b277":"code","92a51e34":"code","cd9b3613":"code","7416d7b1":"code","8cc7f368":"code","e005d447":"code","75ebab2a":"code","34d30fd5":"code","cb7dffb6":"code","96c531bf":"code","2f981697":"code","8abea639":"code","4f888c9c":"code","49f9edf3":"code","a0cc9bec":"markdown","16b337fb":"markdown","4fbb7604":"markdown","6e5b8ecc":"markdown","2054e946":"markdown","74b3d2f2":"markdown","3b63f127":"markdown","0d26698e":"markdown","f21d926c":"markdown","2ba49098":"markdown"},"source":{"c36a9f4e":"# Update to transformers 2.8.0\n!pip install -q transformers --upgrade\n!pip show transformers","f4df2440":"import os\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, average_precision_score, roc_auc_score\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel, TFElectraModel, ElectraTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","6068e192":"tqdm.pandas()","3378bfba":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512, enable_padding=False):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \n    ---\n    \n    Inputs:\n        tokenizer: the `fast_tokenizer` that we imported from the tokenizers library\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    if enable_padding:\n        tokenizer.enable_padding(max_length=maxlen)\n    \n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","03c15e4c":"def combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=512):\n    \"\"\"\n    Given two arrays of IDs (questions and answers) created by\n    `fast_encode`, we combine and pad them.\n    Inputs:\n        tokenizer: The original tokenizer (not the fast_tokenizer)\n    \"\"\"\n    combined_ids = []\n\n    for i in tqdm(range(q_ids.shape[0])):\n        ids = []\n        ids.append(tokenizer.cls_token_id)\n        ids.extend(q_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend(a_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend([tokenizer.pad_token_id] * (maxlen - len(ids)))\n\n        combined_ids.append(ids)\n    \n    return np.array(combined_ids)","c4b366f6":"def encode_qa(questions, answers, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(questions), chunk_size)):\n        q_chunk = questions[i:i+chunk_size].tolist()\n        a_chunk = answers[i:i+chunk_size].tolist()\n        text_chunk = list(zip(q_chunk, a_chunk))\n        \n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","b8f1d7b6":"def truncate_text(text, tokenizer, chunk_size=256, maxlen=256):\n    \"\"\"\n    Ensure that the text does not have more than maxlen tokens\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    all_norm_str = []\n    \n    for i in tqdm(range(0, len(text), chunk_size)):\n        chunk = text[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(chunk)\n        all_norm_str.extend([str(enc.normalized_str) for enc in encs])\n    \n    return all_norm_str","34eb2c95":"def build_model(transformer, max_len=None):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = L.Input(shape=(max_len, ), dtype=tf.int32)\n    \n    x = transformer(input_ids)[0]\n    x = x[:, 0, :]\n    x = L.Dense(1, activation='sigmoid', name='sigmoid')(x)\n    \n    # BUILD AND COMPILE MODEL\n    model = Model(inputs=input_ids, outputs=x)\n    model.compile(\n        loss='binary_crossentropy', \n        metrics=['accuracy'], \n        optimizer=Adam(lr=1e-5)\n    )\n    \n    return model","d36ba33e":"def save_model(model, sigmoid_dir='transformer', transformer_dir='transformer'):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    os.makedirs(transformer_dir, exist_ok=True)\n    os.makedirs(sigmoid_dir, exist_ok=True)\n    \n    transformer = model.layers[1]\n    transformer.save_pretrained(transformer_dir)\n    \n    sigmoid_path = os.path.join(sigmoid_dir,'sigmoid.pickle')\n    sigmoid = model.get_layer('sigmoid').get_weights()\n    pickle.dump(sigmoid, open(sigmoid_path, 'wb'))","f0ddfcc0":"    \ndef load_model(sigmoid_dir='transformer', transformer_dir='transformer', \n               architecture=\"electra\", max_len=None):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    sigmoid_path = os.path.join(sigmoid_dir,'sigmoid.pickle')\n    \n    if architecture == 'electra':\n        transformer = TFElectraModel.from_pretrained(transformer_dir)\n    else:\n        transformer = TFAutoModel.from_pretrained(transformer_dir)\n    model = build_model(transformer, max_len=max_len)\n    \n    sigmoid = pickle.load(open(sigmoid_path, 'rb'))\n    model.get_layer('sigmoid').set_weights(sigmoid)\n    \n    return model","ac3f5180":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","3c326adf":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 8\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 512\nMODEL = 'google\/electra-small-discriminator'","56148c5f":"df = pd.concat([\n    pd.read_csv(f'\/kaggle\/input\/stackexchange-qa-pairs\/pre_covid\/{group}.csv')\n    for group in ['expert', 'biomedical', 'general']\n]).reset_index(drop=True)\n\ndf.head()","fa64b277":"questions = df.title + \"[SEP]\" + df.question","92a51e34":"# First load the real tokenizer\ntokenizer = ElectraTokenizer.from_pretrained(MODEL)\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True, add_special_tokens=False)\nfast_tokenizer","cd9b3613":"# q_ids = fast_encode(questions.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n# a_ids = fast_encode(df.answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n# wa_ids = fast_encode(df.wrong_answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n\n# correct_ids = combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=MAX_LEN)\n# wrong_ids = combine_qa_ids(q_ids, wa_ids, tokenizer, maxlen=MAX_LEN)","7416d7b1":"correct_ids = np.load('\/kaggle\/input\/stackexchange-encode-for-electra\/correct_ids.npy')\nwrong_ids = np.load('\/kaggle\/input\/stackexchange-encode-for-electra\/wrong_ids.npy')","8cc7f368":"input_ids = np.concatenate([correct_ids, wrong_ids])\n\nlabels = np.concatenate([\n    np.ones(correct_ids.shape[0]),\n    np.zeros(wrong_ids.shape[0])\n]).astype(np.int32)","e005d447":"train_idx, test_idx = train_test_split(\n    np.arange(input_ids.shape[0]), \n    test_size=0.3, \n    random_state=0\n)\n\nvalid_idx, test_idx = train_test_split(\n    test_idx, \n    test_size=0.5, \n    random_state=1\n)","75ebab2a":"train_ids = input_ids[train_idx]\nvalid_ids = input_ids[valid_idx]\ntest_ids = input_ids[test_idx]\n\ntrain_labels = labels[train_idx]\nvalid_labels = labels[valid_idx]\ntest_labels = labels[test_idx]","34d30fd5":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_ids, train_labels))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_ids, valid_labels))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_ids)\n    .batch(BATCH_SIZE)\n)","cb7dffb6":"%%time\nwith strategy.scope():\n    transformer_layer = TFElectraModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","96c531bf":"n_steps = train_labels.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","2f981697":"save_model(model)","8abea639":"hist_df = pd.DataFrame(train_history.history)\nhist_df.to_csv('train_history.csv')\nhist_df","4f888c9c":"with strategy.scope():\n    model = load_model(max_len=MAX_LEN)","49f9edf3":"y_score = model.predict(test_dataset, verbose=1).squeeze()\ny_pred = y_score.round().astype(int)\nprint(\"AP:\", average_precision_score(test_labels, y_score))\nprint(\"ROC AUC:\", roc_auc_score(test_labels, y_score))\nprint(classification_report(test_labels, y_pred))","a0cc9bec":"## Eval","16b337fb":"## Load data","4fbb7604":"## Modeling","6e5b8ecc":"## Helper functions","2054e946":"## Convert text to matrices\n\nCaveat: Since a lot of the questions on stackexchange goes over 256, characters, we end up truncating a large part (if not all) of the answers. Thus, we need to \"pre\" truncate them by separately encode the questions and answers, and use a functions to combine them again.\n\nNote: Here we are not actually encoding it, instead we load the encoded q&a pairs from another notebook, in order to limit memory consumption.","74b3d2f2":"### Train model","3b63f127":"## Bert tokenizer","0d26698e":"## TPU Configs","f21d926c":"## Build datasets objects","2ba49098":"## Train test split"}}