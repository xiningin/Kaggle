{"cell_type":{"5d12a2a6":"code","e2815070":"code","cafee7b9":"code","4b073e92":"code","a63e396c":"code","1f87dad1":"code","e1a334d9":"code","bca28038":"code","5db6c40a":"code","77632259":"code","abae4722":"code","68924fb9":"code","91c246c4":"code","bf902489":"code","ed661a93":"code","6b3eb73b":"code","70900886":"code","b7158fba":"code","d937e0cc":"code","1869d2b9":"code","eb1db4c3":"code","2187fbb7":"code","669fd8ae":"code","d656de58":"code","b631180c":"code","c2727d63":"code","4510777f":"code","067a1d0d":"code","46814e90":"code","7aa82023":"code","b63fec36":"code","a926c277":"code","ed9c053f":"code","e85dc76b":"code","0581dd44":"code","ace5573c":"code","df6d9206":"code","38fa9c4f":"code","e213cea7":"code","79a24ef0":"code","8d22cfbb":"code","4088f3df":"code","55dc1671":"code","88f11ec0":"code","8b9c8b14":"code","8c2be78d":"code","5ab79629":"code","fa140373":"code","f67a484c":"code","3704398b":"code","ea86b3e2":"code","7d397f05":"code","d6f23196":"code","00f57aa0":"code","b39e0f5f":"code","c7b26b02":"code","15f6dc41":"code","3f7324aa":"code","224e4750":"code","bfc67cd6":"code","c9a80d2f":"code","24ba3606":"code","eed62e3f":"code","2111d3cb":"code","1613e46a":"code","e1c16b4f":"code","beb1af9c":"code","c72595c2":"code","e9fdd2ac":"markdown","bf1adffc":"markdown","8190c504":"markdown","04d67f23":"markdown","a94594cc":"markdown","ad197a99":"markdown","7684b662":"markdown","6f88dc3f":"markdown","047ec14f":"markdown","838d4047":"markdown","788bc4df":"markdown","997334df":"markdown","6c468d28":"markdown","08f87c84":"markdown","8e6ab9f8":"markdown","c14b9a2f":"markdown","ac57b881":"markdown","600d7aba":"markdown","f8e39b12":"markdown","6d5538b0":"markdown","0b290d34":"markdown","a068f952":"markdown","3186de7d":"markdown","b27e93c5":"markdown","de7aed93":"markdown","902abe5c":"markdown","189ce6e6":"markdown","a749c742":"markdown","b2f8d125":"markdown","c5d39a9b":"markdown","c8138b95":"markdown","1fc324f1":"markdown","8af07409":"markdown","b24dcc09":"markdown","07476c9a":"markdown","d78a9d60":"markdown","40198970":"markdown","04742026":"markdown","d6bccd4e":"markdown","19d52b75":"markdown","dd8dec60":"markdown","81c4cf2c":"markdown","e31e5d8a":"markdown","9efc28c0":"markdown","5cb186ad":"markdown","5906bd49":"markdown","3ca4d251":"markdown","b71da098":"markdown"},"source":{"5d12a2a6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_curve\nimport seaborn as sns\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import StandardScaler\nimport missingno as msno\nfrom sklearn.impute import KNNImputer\nfrom imblearn.over_sampling import SMOTE\nimport gc\nimport warnings\nfrom bayes_opt import BayesianOptimization\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport contextlib\nimport sys\nfrom io import StringIO","e2815070":"%matplotlib inline\nmpl.rcParams['figure.figsize'] = (10, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","cafee7b9":"pd.set_option('display.float_format', lambda x: '%.3f' % x)","4b073e92":"train_dataset_path = '..\/input\/malicious-server-hack\/Train.csv'\ntest_dataset_path = '..\/input\/malicious-server-hack\/Test.csv'","a63e396c":"raw_train_df = pd.read_csv(train_dataset_path)\nraw_train_df.head()","1f87dad1":"raw_test_df = pd.read_csv(test_dataset_path)\nraw_test_df.head()","e1a334d9":"# Assign class column name to a variable \nclass_variable = \"MALICIOUS_OFFENSE\"","bca28038":"### Check data types of the column\nraw_train_df.dtypes","5db6c40a":"raw_train_df['DATE'] = pd.to_datetime(raw_train_df['DATE'])\nraw_train_df['day'] = raw_train_df['DATE'].dt.day\nraw_train_df['month'] = raw_train_df['DATE'].dt.month","77632259":"# Drop DATE Column\nraw_train_df.drop(columns=[\"DATE\"],inplace=True)","abae4722":"# Set INCIDENT_ID to index\nraw_train_df= raw_train_df.set_index('INCIDENT_ID')","68924fb9":"raw_train_df.describe()","91c246c4":"def missing_values_table(input_df):\n    \"\"\"\n    Returns the number of missing values in each column (if it has any missing values) and percentage of missing values.\n\n    Parameters\n    ----------\n    input_df: pd.DataFrame\n        The dataframe that whose missing data information is required \n\n    Returns\n    -------\n    mis_val_table_ren_columns: pd.DataFrame\n        Returns a dataframe containing columns and missing data information\n\n    \"\"\"\n    # Total missing values\n    mis_val = input_df.isnull().sum()\n\n    # Percentage of missing values\n    mis_val_percent = 100 * input_df.isnull().sum() \/ len(input_df)\n\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n\n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Values Missing'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Values Missing', ascending=False).round(1)\n\n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(input_df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","bf902489":"train_missing= missing_values_table(raw_train_df)\ntrain_missing","ed661a93":"test_missing= missing_values_table(raw_test_df)\ntest_missing","6b3eb73b":"msno.bar(raw_train_df)","70900886":"#sorted by X_12\nsorted_df = raw_train_df.sort_values('X_12')\nmsno.matrix(sorted_df)","b7158fba":"#sorted by X_12\nsorted_test_df = raw_test_df.sort_values('X_12')\nmsno.matrix(sorted_test_df)","d937e0cc":"def impute_missing_data(input_df, columns):\n    \"\"\"\n    Imputes the missing data in given column\n    \n    Parameters\n    ----------\n    input_df: pd.DataFrame\n        The dataframe that whose column is to be imputed\n\n    columns: list\n        List containing names of the columns that needs to be imputed\n\n\n    Returns\n    -------\n    result_df: pd.DataFrame\n        Returns the dataframe with imputed values.\n    \"\"\"\n    knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n    for column_name in columns:\n        input_df[column_name] = knn_imputer.fit_transform(input_df[[column_name]])\n    \n    result_df = input_df.copy()\n\n    return result_df","1869d2b9":"## We have missing data in X_12\ntrain_df = impute_missing_data(raw_train_df, ['X_12'])","eb1db4c3":"def scale_dataframe(input_df, columns):\n    \"\"\"\n        Scales the given columns of input dataframe\n        \n        Parameters\n        ----------\n        input_df: pd.DataFrame\n            The dataframe that has to be scaled\n        \n        columns: list\n            List containing names of the columns that needs to be scaled\n            \n        \n        Returns\n        -------\n        result_df: pd.DataFrame\n            Returns the normalized dataframe.\n    \"\"\"\n    scaler = StandardScaler()\n    non_scale_columns = list(filter(lambda col : col not in columns, input_df.columns))\n    normalized_df = pd.DataFrame(scaler.fit_transform(input_df[columns]), columns=columns)\n    result_df = pd.concat(normalized_df, input_df[non_scale_columns])\n    \n    return result_df","2187fbb7":"# Correlation with all variables with MULTIPLE_OFFENSE\ntrain_df.corr().nlargest(18, 'MALICIOUS_OFFENSE')['MALICIOUS_OFFENSE'].index","669fd8ae":"f,ax = plt.subplots(figsize=(16, 16))\nhigh_to_low_col_index = train_df.corr().nlargest(19, class_variable)[class_variable].index\nsns.heatmap(train_df[high_to_low_col_index].corr(), annot=True, linewidths=.5, fmt= '.3f',ax=ax)","d656de58":"neg, pos = np.bincount(train_df[class_variable])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n    Negative: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total, neg, 100*neg\/total))","b631180c":"X = train_df.loc[:, train_df.columns != class_variable]\nY = train_df.loc[:, train_df.columns == class_variable]\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.33,random_state=8)\n\nprint('Training labels shape:', X_train.shape)\nprint('Test labels shape:', X_test.shape)\nprint('Training features shape:', y_train.shape)\nprint('Test features shape:', y_test.shape)","c2727d63":"X_resample,y_resample=SMOTE(sampling_strategy=0.85).fit_sample(X_train, y_train)","4510777f":"X_resample.head()","067a1d0d":"y_resample.head()","46814e90":"print('Resampled Training features shape:', X_resample.shape)\nprint('Resampled Training labels shape:', y_resample.shape)","7aa82023":"def generate_metrics(labels, predictions):\n    \"\"\"\n        Calculates the metrics like accuracy, weighted recall, weighted precision\n        and F1 score.\n        \n        Parameters\n        ----------\n        labels: 1d-array\n            True values of the class variable\n        \n        predictions: 1d-array\n            Predictions by the model\n            \n        \n        Returns\n        -------\n        Does not return anything\n    \n    \"\"\"\n    ac = accuracy_score(labels,predictions)\n    f_score = f1_score(labels,predictions)\n    recall = recall_score(labels, predictions, average='weighted')\n    precision = precision_score(labels, predictions, average='weighted')\n    print('Accuracy is: ', ac)\n    print('Recall is:', recall )\n    print('Precision is:', precision)\n    print('F1 score is: ', f_score)\n    \ndef plot_roc(name, labels, predictions, **kwargs):\n    \"\"\"\n        This helper function plots the receiver operating characteristic curve. One of the best metrics \n        to evaluate a model other than F1 score and Kappa score.\n        \n        Parameters\n        ----------\n        labels: 1d-array\n            True values of the class variable\n        \n        predictions: 1d-array\n            Predictions by the model\n            \n        \n        Returns\n        -------\n        Does not return anything\n    \n    \"\"\"\n    fp, tp, _ = roc_curve(labels, predictions)\n    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n    plt.xlabel('False positives [%]')\n    plt.ylabel('True positives [%]')\n    plt.xlim([-0.5,20])\n    plt.ylim([80,100.5])\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')\n\ndef plot_cm(labels, predictions, p=0.5):\n    \"\"\"\n        This helper function plots the confusion matrix\n        \n        Parameters\n        ----------\n        labels: 1d-array\n            True values of the class variable\n        \n        predictions: 1d-array\n            Predictions by the model\n        \n        p: Float\n            The thresold value\n            \n        \n        Returns\n        -------\n        Does not return anything\n    \n    \"\"\"\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n    print('Total Fraudulent Transactions: ', np.sum(cm[1]))","b63fec36":"if globals().get(\"XG_base\",None) is not None: \n    del XG_base\n    gc.collect()\n    \nXG_base = XGBClassifier(random_state=17)\nXG_base.fit(X_resample, y_resample.values.ravel())\n","a926c277":"baseline_model_xg = XG_base.fit(X_resample, y_resample.values.ravel())","ed9c053f":"baseline_test_predictions = baseline_model_xg.predict(X_test)","e85dc76b":"baseline_train_predictions = baseline_model_xg.predict(X_resample)","0581dd44":"generate_metrics(y_test, baseline_test_predictions)","ace5573c":"plot_cm(y_test,baseline_test_predictions)","df6d9206":"plot_roc(\"Baseline Train\", y_resample, baseline_train_predictions, color=colors[1])\nplot_roc(\"Baseline Test\", y_test, baseline_test_predictions, color=colors[1], linestyle='--')\nplt.legend(loc='lower right')\n","38fa9c4f":"# save base model\nbaseline_model_xg.save_model(\"baseline_model\")","e213cea7":"@contextlib.contextmanager\ndef capture():\n    \"\"\"\n    Captures the output and writes to logfile\n    \"\"\"\n    olderr, oldout = sys.stderr, sys.stdout\n    try:\n        out=[StringIO(), StringIO()]\n        sys.stderr,sys.stdout = out\n        yield out\n    finally:\n        sys.stderr,sys.stdout = olderr,oldout\n        out[0] = out[0].getvalue().splitlines()\n        out[1] = out[1].getvalue().splitlines()","79a24ef0":"def load_data():\n    \"\"\"\n    Loads a copy of train and test data.\n    \"\"\"\n    train = X_resample.copy()\n    train_labels = y_resample.copy()\n    print('\\n Shape of raw train data:', train.shape)\n\n    return train, train_labels","8d22cfbb":"def XGB_CV(\n          max_depth,\n          gamma,\n          min_child_weight,\n          max_delta_step,\n          subsample,\n          colsample_bytree,\n          n_estimators\n         ):\n    \"\"\"\n    This is the Cross-validation function with given parameters. \n    We will optimize this function using Bayesian Optimization\n    \n    Parameters\n    ----------\n    The parameters of the XGBoost that I want to optimize.\n    \n    Returns\n    ----------\n    Returns cv_score to caller function\n    \"\"\"\n\n    global AUCPRbest\n    global ITERbest\n\n#\n# Define all XGboost parameters\n#\n\n    paramt = {\n              'booster' : 'gbtree',\n              'max_depth' : int(max_depth),\n              'gamma' : gamma,\n              'eta' : 0.1,\n              'objective' : 'binary:logistic',\n              'nthread' : 4,\n              'silent' : True,\n              'eval_metric': 'aucpr',\n              'subsample' : max(min(subsample, 1), 0),\n              'colsample_bytree' : max(min(colsample_bytree, 1), 0),\n              'min_child_weight' : min_child_weight,\n              'max_delta_step' : int(max_delta_step),\n              'seed' : 1001,\n              'n_estimators' : int(n_estimators),\n              'random_state': 17\n              }\n\n    folds = 5\n    cv_score = 0\n\n    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file )\n    log_file.flush()\n\n    xgbc = xgb.cv(\n                    paramt,\n                    dtrain,\n                    num_boost_round = 20000,\n                    stratified = True,\n                    nfold = folds,\n                    early_stopping_rounds = 100,\n                    metrics = 'aucpr',\n                    show_stdv = True\n               )\n\n\n    with capture() as result:\n        warnings.filterwarnings('ignore')\n        val_score = xgbc['test-aucpr-mean'].iloc[-1]\n        train_score = xgbc['train-aucpr-mean'].iloc[-1]\n        print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n    (val_score*2-1)) )\n        if ( val_score > AUCPRbest ):\n            AUCPRbest = val_score\n            ITERbest = len(xgbc)\n\n    return (val_score*2) - 1","4088f3df":"log_file = open('AUCPR-5fold-XGB-run-01-v1-full.log', 'a')\nAUCPRbest = -1.\nITERbest = 0\n\n# Load data set and target values\ntrain, target = load_data()\n","55dc1671":"dtrain = xgb.DMatrix(train, label = target)","88f11ec0":"#set the lower and upper searching bounds\nbounds = {'max_depth': (2, 12), 'gamma': (0.001, 10.0), 'min_child_weight': (0, 20),\n          'max_delta_step': (0, 10),'subsample': (0.4, 1.0),'colsample_bytree' :(0.4, 1.0),'n_estimators': (20,300)\n                                    }\nXGB_BO = BayesianOptimization(XGB_CV, bounds )","8b9c8b14":"# This might take a couple of minutes to run\n\nprint('-'*130)\nprint('-'*130, file=log_file)\nlog_file.flush()\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    XGB_BO.maximize(init_points=2, n_iter=7, acq='ei', xi=0.0)","8c2be78d":"best_cv_score = XGB_BO.max","5ab79629":"best_cv_score","fa140373":"print(best_cv_score)","f67a484c":"optimal_params = best_cv_score[\"params\"]\noptimal_params[\"max_depth\"] = int(optimal_params[\"max_depth\"])\noptimal_params[\"n_estimators\"] = int(optimal_params[\"n_estimators\"])","3704398b":"if globals().get(\"XG_optimal\",None) is not None: \n    del XG_optimal\n    \nXG_optimal = XGBClassifier(random_state=17,**optimal_params)\nXG_optimal.fit(X_resample, y_resample.values.ravel())","ea86b3e2":"optimal_model_xg = XG_optimal.fit(X_resample, y_resample.values.ravel())","7d397f05":"optimal_test_predictions = optimal_model_xg.predict(X_test)\noptimal_train_predictions = optimal_model_xg.predict(X_resample)\ngenerate_metrics(y_test, optimal_test_predictions)\n","d6f23196":"plot_cm(y_test,optimal_test_predictions)","00f57aa0":"plot_roc(\"Optimal Train\", y_resample, optimal_train_predictions, color=colors[1])\nplot_roc(\"Optimal Test\", y_test, optimal_test_predictions, color=colors[1], linestyle='--')\nplt.legend(loc='lower right')","b39e0f5f":"# save optimal model\nXG_optimal.save_model(\"BO_optimal_model\")","c7b26b02":"#This might take a couple of minutes to run\n\nrfecv_xg = RFECV(estimator=XG_optimal, step=1, cv=6, scoring='recall_weighted')\nrfecv_xg = rfecv_xg.fit(X_resample, y_resample.values.ravel())\nprint('Optimal number of features :', rfecv_xg.n_features_)\nbest_features = X_resample.columns[rfecv_xg.support_].tolist()\nprint('Best features :', best_features)","15f6dc41":"sorted(rfecv_xg.grid_scores_,reverse=True)","3f7324aa":"plt.figure(figsize=(15,15))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv_xg.grid_scores_) + 1), rfecv_xg.grid_scores_)\nplt.grid()\nplt.show()","224e4750":"x_train_rfecv_xg = rfecv_xg.transform(X_resample)\nx_test_rfecv_xg = rfecv_xg.transform(X_test)","bfc67cd6":"x_train_rfecv_xg.shape","c9a80d2f":"#Fitting rfecv or XG_optimal is same, one can verify by using 'get_params()' attribute in both.\nif globals().get(\"XG_optimal\",None) is not None: \n    del XG_optimal\n    \nXG_optimal = XGBClassifier(random_state=17,**optimal_params)\nrfecv_model_xg = XG_optimal.fit(x_train_rfecv_xg, y_resample.values.ravel())","24ba3606":"rfecv_test_predictions = rfecv_model_xg.predict(x_test_rfecv_xg)\nrfecv_train_predictions = rfecv_model_xg.predict(x_train_rfecv_xg)\ngenerate_metrics(y_test, rfecv_test_predictions)\n\n\n\n\n","eed62e3f":"plot_cm(y_test,rfecv_test_predictions)","2111d3cb":"\nplot_roc(\"Optimal Train\", y_resample, rfecv_train_predictions, color=colors[1])\nplot_roc(\"Optimal Test\", y_test, rfecv_test_predictions, color=colors[1], linestyle='--')\nplt.legend(loc='lower right')","1613e46a":"# Save Final Model\nrfecv_model_xg.save_model(\"final_model\")","e1c16b4f":"def inference(saved_model_name, features):\n    \"\"\"\n    This function runs the inference code\n    \n    Parameters\n    ------------\n    saved_model_name: XGBoost Model\n        Name of the saved model\n    features: list \n        List of the features selected by RFECV\n    \n    Returns\n    -----------\n    submission_df: pd.DataFrame\n        Returns the dataframe with predicted values\n    \"\"\"\n    # Load testdata\n    submission_df = pd.read_csv('..\/input\/malicious-server-hack\/Test.csv')\n    #df_test.fillna('0',inplace=True)\n\n    # Pre-process Data\n    \n    # Create Month and Date Columns\n    submission_df['DATE'] = pd.to_datetime(submission_df['DATE'])\n    submission_df['day'] = submission_df['DATE'].dt.day\n    submission_df['month'] = submission_df['DATE'].dt.month\n\n    # Set \"INCIDENT ID\" as index\n    submission_df.set_index('INCIDENT_ID', inplace= True)\n\n    #Select features based on RFECV \n    submission_df_ = submission_df[features].copy()\n\n    #Impute the missing values in column X_12\n    if 'X_12' in submission_df.columns:\n        submission_df_ = impute_missing_data(submission_df_, ['X_12'])\n    # Load Model\n    inference_model = XGBClassifier()\n    inference_model.load_model(saved_model_name)\n    \n    # Prediction\n    predictions = inference_model.predict(submission_df_.to_numpy())\n    prediction_df=pd.DataFrame(predictions,columns=[class_variable])\n    \n    # Create Submission Dataframe    \n    index_df =pd.DataFrame(submission_df.index)\n    submission_df = pd.concat([index_df,prediction_df], axis=1)\n    \n    return submission_df","beb1af9c":"# Save to submission.csv\nfinal_submission_df = inference(\"final_model\", best_features)\nfinal_submission_df.to_csv(\"final_model_submission.csv\",index=False)","c72595c2":"print(final_submission_df.MALICIOUS_OFFENSE.value_counts())\nprint(final_submission_df.MALICIOUS_OFFENSE.value_counts()\/len(final_submission_df))","e9fdd2ac":"## Helper Functions\n* I have written some helpers functions to visualize the results of model which will help in model evaluation.","bf1adffc":"## SMOTE <a name=\"smote\"><\/a>","8190c504":"### Import libraries <a name=\"import\"><\/a>","04d67f23":"### Insights\n* The bar chart above gives a quick graphical overview of the completeness of the dataset. We can see that X_12 column has missing values. Next,it would make sense to find out the locations of the missing data.\n","a94594cc":"## Classification Pipeline <a name=\"classification\"><\/a>","ad197a99":"# Bayesian Optimization <a name=\"bayes\"><\/a>\n* I used Bayesian Optimization for hyperparameter searching as it is more comprehensive and time-efficient. I used [Bayesian Optimization](https:\/\/github.com\/fmfn\/BayesianOptimization) library.\n* This library will provide different solutions in different runs.","7684b662":"You cab explore more about [Bayesian Optimization](https:\/\/github.com\/fmfn\/BayesianOptimization) and try other optimization approaches in [Scikit-Optimize](https:\/\/scikit-optimize.github.io\/stable\/)","6f88dc3f":"### Insights\n* We can see that the column has missing values towards higher values. Let's check what is the case with Test set.","047ec14f":"\n## Insights\n* We can see that the scores have increased even better now.\n","838d4047":"### Class imbalance can be handled in three ways:\n* Class Weight: Some algorithms like random forest, neural network etc. allow us to have a variable to assign weights to the classes to tell the model to focus more one or more classes. I used Sklearn's [**compute_class_weight**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.class_weight.compute_class_weight.html) to assign class weights while training neural network (using Tensorflow 2.)\n* Undersampling: We can under sample the majority class to get equal number of positive and negative classes. But this might result in information loss. Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n* Oversampling: We can oversample the minority class to get equal number of positive and negative classes. Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with. Here we do not necessarily make the ratio between minority and majority classes one.\n\n### I decided to use Upsampling to avoid information loss as I have only around 24k data points.","788bc4df":"### Insights\n\n* The standard deviation of most of the variables is close to eachother and is not high. \n* Scaling is required since the variables are not of same scale.","997334df":"## Note: \n* I am not using scaling in this pipeline since XGBoost does not require scaling. \n* Since Decision Trees do not require normalization of their inputs; and XGBoost is essentially an ensemble algorithm comprised of Decision Trees, it does not require normalization for the inputs either.","6c468d28":"## Insights\n* The metric scores are really good even with the baseline model with default parameters. \n* The ROC Curve for both train set and test set looks almost perfect.","08f87c84":"### Read both datasets <a name=\"read\" ><\/a>","8e6ab9f8":"### Covert \"Date\" column to datetime type and further extract the day and month. \n\n* **Intution:** The number of hacks might be more on pay days or specific (bonus) months. ","c14b9a2f":"### Insights\n* **Since the X_12 column has missing values in test, we cannot simply remove the missing rows in train set and continue modeling. We will have to deal with missing data in test too.**","ac57b881":"### Insights\n* X_11 is most positvely correlated and X_10 is most neagatively correlated with MALICIOUS_OFFENSE respectively.\n* Clearly X_2 and X_3 are almost same column.\n* Furthuremore, (X_10, X_12) and (X_6, X_7) are also highly correlated respectively.","600d7aba":"# Inference Pipeline <a name=\"infp\"><\/a>","f8e39b12":"## Handle Missing Data <a name=\"miss\"><\/a>","6d5538b0":"* A technique for upsampling is to create synthetic samples. Here we will use imblearn\u2019s [SMOTE](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html) or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n* It\u2019s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.\n* Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.\n","0b290d34":"# Baseline model <a name=\"base\" ><\/a>","a068f952":"## Note:\n* **The results obtained might slightly differ everytime because of non-deterministic optimization behavior of bayesian optimization**","3186de7d":"## Insights\n* There is clear case of class imbalance.\n","b27e93c5":"### Insights\n* Test set also have similar missing values, so we can handle missing data in both in same way.","de7aed93":"## Solution Approach <a name=\"solution_approach\"><\/a>\n* This was an Imbalanced Classification problem, I used [SMOTE](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html) for minority class upsampling.\n* I have tried using Neural Networks, Random Forest and XGBoost classifiers both with and without SMOTE and finally ended up using (XGBoost with SMOTE) because this received the best score.\n* This notebook only contains the XGBoost implementation. ","902abe5c":"# This notebook has my solution for task [Predict if server will be hacked](https:\/\/www.kaggle.com\/lplenka\/malicious-server-hack\/tasks?taskId=2168)\n\n* Author: [Lalu Prasad Lenka](https:\/\/www.linkedin.com\/in\/lplenka\/)","189ce6e6":"## Insights\n* We can see that the scores have increased slightly. Note: It improves in most of the cases but in some cases the scores are not better.\n\n* Let's try feature elimination and see if we can better score.","a749c742":"### Let's Detect missing data visually using [Missingno](https:\/\/github.com\/ResidentMario\/missingno) library","b2f8d125":"## Note:\n* Decision trees are by nature immune to multi-collinearity. For example, if you have 2 features which are 99% correlated, when deciding upon a split the tree will choose only one of them. Other models such as Logistic regression would use both the features.Since boosted trees use individual decision trees, they also are unaffected by multi-collinearity.\n* I did remove correlated columns using [Variation Inflation Factor](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.stats.outliers_influence.variance_inflation_factor.html) but didn't see any good improvement in recall_score so decided not to handle multicollinearity, XGBoost is doing that for me.","c5d39a9b":"## Check Class Imbalance <a name=\"imbalance\"><\/a>","c8138b95":"## That was my approach to this Imbalanced dataset. Do upvote if you liked my approach and this Notebook. Try out your approach and suggest improvements","1fc324f1":"## Model with Optimal Params <a name=\"opt\"><\/a>","8af07409":"# Scaling Function <a name=\"scale\"><\/a>\n\n* Normalize the input features using the sklearn StandardScaler. This will set the mean to 0 and standard deviation to 1.","b24dcc09":"## Feature Selection using Sklearn's Recursive Feature elimination <a name=rfecv><\/a>\n* This library will not only find best features but also find how many features do we need for best accuracy.\n* I have used **recall_weighted** for scoring because decreasing False Negative is the intention here.","07476c9a":"## Note:\n* Since these feature selection is dependent on XG_Optimal estimator, these might change when Bayes Optimization gives different optimal params.","d78a9d60":"## Note:\n* Class imbalance is a serious problem for many classifiers since most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.\n* Though ensemble tree models perform decently even when there is class imbalance I will be handling class imbalance here especially because this dataset is highly imbalanced.","40198970":"## Final Model <a name=\"fmodel\"><\/a>","04742026":"# Table of contents\n1. [Introduction](#Introduction-)\n2. [Solution Approach](#Solution-Approach-)\n3. [Classification Pipeline](#Classification-Pipeline-)\n    1. [Import Libraries](#Import-Libraries-)\n    2. [Read Dataset](#Read-Dataset-)\n    3. [Exploratory Data Analysis](#Exploratory-Data-Analysis-)\n        1. [Scaling](#Scaling-)\n        2. [Missing Data Handling](#Missing-Data-Handling-)\n        3. [Correlation Plot](#Correlation-Plot-)\n        4. [Check Class Imbalance](#Check-Class-Imbalance-)\n    4. [Prepare Test and Train Data](#Prepare-Test-and-Train-Data-)\n    5. [Handle Class Imbalance](#Handle-Class-Imbalance-)\n    6. [SMOTE](#SMOTE-)\n    7. [Baseline Model](#Baseline-Model-)\n    8. [Bayesian Optimization](#Bayesian-Optimization-)\n    9. [Model with Optimal Params](#Model-with-Optimal-Params-)\n    9. [Recursive Feature elimination](#Recursive-Feature-elimination-)\n    10. [Final Model](#Final-Model-)\n4. [Inference Pipeline](#Inference-Pipeline-)","d6bccd4e":"### To make the display of number for readable","19d52b75":"## Handling Class Imbalance <a name=\"handle\"><\/a>","dd8dec60":"### Let's look at the variation of data across columns","81c4cf2c":"## Preparing Test and Train data <a name=\"split\"><\/a>","e31e5d8a":"## Exploratory Data Analysis <a name=eda><\/a>","9efc28c0":"## Correlation Plot <a name=\"corr\"><\/a>","5cb186ad":"### Printing to get the idea of distribution of classes in the predictions\n* It should be close to the distribution classes in train set. \n","5906bd49":"\n### K-Nearest Neighbor Imputation\n* The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.","3ca4d251":"## Note:\n* Looks like the score is already good but to get better score I decided to optimize the parameters of XGBoost model.","b71da098":"## Introduction <a name=\"introduction\"><\/a>\n* The [task](https:\/\/www.kaggle.com\/lplenka\/malicious-server-hack\/tasks?taskId=2168) was pretty straigforward, we had a binary classification problem where we had to predict whether a server  will be hacked or not based on some numerical features.\n* The Target column was \"MALICIOUS_OFFENSE\"\n* I have used [**recall_weighted**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html) as the metric to evaluate my model since False Negatives (predicting a server was not hacked when it actually was hacked) are more important to reduce in this problem statement."}}