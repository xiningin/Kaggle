{"cell_type":{"e80d02b1":"code","b224bcb7":"code","e7956143":"code","28b8e72a":"code","93226150":"code","ebc3c429":"code","8a5caa8d":"code","60dfaf1f":"code","c4a98f1c":"code","466f9db0":"code","5d19e1e0":"code","bc8e42b6":"code","d781bc54":"code","96942dec":"code","c545f6e5":"code","25eb498c":"code","262cdc0f":"code","7023c0b2":"code","d170ab51":"code","71e754ed":"code","6fb6737e":"code","c4fc42a7":"code","38b161cf":"code","3cfc8351":"code","f8afa153":"code","c96728ce":"code","aee3ca5e":"code","6a361de9":"code","c608285c":"code","15a635e8":"code","82a3eb4e":"code","3ed422fd":"code","63d91543":"code","51729dd9":"code","b4beaf8f":"code","7803a548":"code","7409e04c":"code","e5df5333":"code","6269b068":"code","1a715ec0":"markdown","50b95270":"markdown","230a3e36":"markdown","b47336dc":"markdown","f555264c":"markdown","762f5400":"markdown","893fce57":"markdown","00df75a6":"markdown","43c3d347":"markdown","543a5c48":"markdown","36567ad0":"markdown","4a6c3191":"markdown","74461dca":"markdown","d2830274":"markdown","b4a6c316":"markdown","4473292f":"markdown"},"source":{"e80d02b1":"from IPython.display import clear_output\n!pip install imutils\nclear_output()","b224bcb7":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123","e7956143":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO\n!tree -d","28b8e72a":"IMG_PATH = '..\/input\/mybraintumor\/brain_tumor_dataset\/'\n# split the data by train\/val\/test\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '\/' + FILE_NAME\n            if n < 50:\n                shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n            elif n < 0.8*IMG_NUM:\n                shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","93226150":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","ebc3c429":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","8a5caa8d":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['E\u011fitim Verisi', 'Do\u011frulama Verisi', 'Test Verisi'],\n    y=y[0],\n    name='Hay\u0131r',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['E\u011fitim Verisi', 'Do\u011frulama Verisi', 'Test Verisi'],\n    y=y[1],\n    name='Evet',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Her Setteki S\u0131n\u0131flar\u0131n Say\u0131s\u0131',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Say\u0131'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","60dfaf1f":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","c4a98f1c":"plot_samples(X_train, y_train, labels, 30)","466f9db0":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('G\u00f6r\u00fcnt\u00fc Oranlar\u0131n\u0131n Da\u011f\u0131l\u0131m\u0131')\nplt.xlabel('Oran De\u011feri')\nplt.ylabel('Say\u0131')\nplt.show()","5d19e1e0":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","bc8e42b6":"img = cv2.imread('..\/input\/mybraintumor\/brain_tumor_dataset\/yes\/Y1714.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","d781bc54":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('1.Ad\u0131m Orjinal Resmi Se\u00e7in')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('2.Ad\u0131m En B\u00fcy\u00fck Alan\u0131 Se\u00e7in')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('3.Ad\u0131m U\u00e7 Noktalar\u0131 Bulun ')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('4.Ad\u0131m Resmi K\u0131rp\u0131n')\nplt.show()","96942dec":"# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","c545f6e5":"plot_samples(X_train_crop, y_train, labels, 30)","25eb498c":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","262cdc0f":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')","7023c0b2":"def preprocess_imgs(set_name, img_size):\n    \"\"\"\n    Resize and apply VGG-15 preprocessing\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","d170ab51":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","71e754ed":"# plot_samples(X_train_prep, y_train, labels, 30)","6fb6737e":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1.\/255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)","c4fc42a7":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","38b161cf":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Orjinal Resim')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview\/'):\n    img = cv2.cv2.imread('preview\/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Artt\u0131r\u0131lm\u0131\u015f Resimler')\nplt.show()","3cfc8351":"!rm -rf preview\/","f8afa153":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","c96728ce":"# load base model\nvgg16_weight_path = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nbase_model = VGG16(\n    weights=vgg16_weight_path,\n    include_top=False, \n    input_shape=IMG_SIZE + (3,)\n)","aee3ca5e":"NUM_CLASSES = 1\n\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nmodel.layers[0].trainable = False\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(lr=1e-4),\n    metrics=['accuracy']\n)\n\nmodel.summary()","6a361de9":"EPOCHS = 120\nes = EarlyStopping(\n    monitor='val_acc', \n    mode='max',\n    patience=6\n)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=25,#TrainData\/batchsize\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=10,#ValidationData\/batchsize\/2\n    callbacks=[es],\n    verbose=1\n)","c608285c":"# validate on val set\npredictions = model.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\n_, train_acc = model.evaluate(X_val_prep, y_val, verbose=0)\n_, test_acc = model.evaluate(X_test_prep, y_test, verbose=0)","15a635e8":"history_1= history","82a3eb4e":"import sys\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\npyplot.figure(figsize=(12,12))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Vgg16 Kay\u0131p')\npyplot.plot(history.history['loss'], label='E\u011fitim')\npyplot.plot(history.history['val_loss'], label='Do\u011frulama')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Vgg16 Do\u011fruluk')\npyplot.plot(history.history['accuracy'], label='E\u011fitim')\npyplot.plot(history.history['val_accuracy'], label='Do\u011frulama')\npyplot.legend()\npyplot.show()","3ed422fd":"print('E\u011fitim Verisi: %.3f, Test Verisi: %.3f' % (train_acc, test_acc))","63d91543":"from sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, predictions)\nprint('Do\u011fruluk: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(y_test, predictions)\nprint('Kesinlik: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(y_test, predictions)\nprint('Duyarl\u0131l\u0131k: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(y_test, predictions)\nprint('F1 Skoru: %f' % f1)","51729dd9":"kappa = cohen_kappa_score(y_test, predictions)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, predictions)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, predictions)\nprint(matrix)","b4beaf8f":"# plot model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(1, len(history.epoch) + 1)\n\nplt.figure(figsize=(20,10))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='E\u011fitim Verisi')\nplt.plot(epochs_range, val_acc, label='Do\u011frulama Verisi')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs \"(D\u00f6nemler)\"')\nplt.ylabel('Accuracy \"(Do\u011fruluk)\"')\nplt.title('Modelin Do\u011frulu\u011fu')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='E\u011fitim Verisi')\nplt.plot(epochs_range, val_loss, label='Do\u011frulama Verisi')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs \"(D\u00f6nemler)\"')\nplt.ylabel('Loss \"(Kay\u0131p)\"')\nplt.title('Modelin Kayb\u0131')\n\nplt.tight_layout()\nplt.show()","7803a548":"# validate on val set\npredictions = model.predict(X_val_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Do\u011frulama Verisi  Accuracy\"(Do\u011frulu\u011fu)\" = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","7409e04c":"# validate on test set\npredictions = model.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Verisi  Accuracy\"(Do\u011frulu\u011fu)\" = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","e5df5333":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nif ind_list.size == 0:\n    print('Yanl\u0131\u015f S\u0131n\u0131fland\u0131r\u0131lm\u0131\u015f Resim Yok.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Ger\u00e7ek S\u0131n\u0131f\u0131: {y_val[i]}\\n\u00d6ng\u00f6r\u00fclm\u00fc\u015f S\u0131n\u0131f\u0131: {predictions[i]}')\n        plt.show()","6269b068":"# olu\u015fturulan dosyalar\u0131n silinmesi\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n# Modelin kaydedilmesi\nmodel.save('VGG_model.h5')","1a715ec0":"**<center><font size=5>VGG-16 Modelini Kullanarak Beyin T\u00fcm\u00f6r\u00fc Hesaplama<\/font><\/center>**\n***\n","50b95270":"Yukar\u0131daki fonksiyonun neler yapaca\u011f\u0131 ile ilgili bir \u00f6rnek","230a3e36":"### <a id='apply'>4.1.2. K\u0131rpma \u0130\u015flemlerinin Uygulanmas\u0131<\/a>","b47336dc":"# <a id='intro'>1. Projeye Genel Bak\u0131\u015f ve Hedefler<\/a>\n\nProjenin genel amac\u0131 MR g\u00f6r\u00fcnt\u00fclerini kullanarak t\u00fcm\u00f6r var veya t\u00fcm\u00f6r yok olarak mr g\u00f6r\u00fct\u00fclerinin s\u0131n\u0131fland\u0131r\u0131lmas\u0131n\u0131 sa\u011flamak. kullan\u0131lan model [VGG-16](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection) VGG-16 \u00f6nceden e\u011fitilmi\u015f model kullan\u0131larak T\u00fcm\u00f6r alg\u0131lama CNN modelimiz e\u011fitilir.. \n\n$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n\nFinal Sonu\u00e7lar\u0131\n| Set | Accuracy |\n|:-:|:-:|\n| Validation Set* | ~90% |\n| Test Set* | ~94% |\n<br>\n\n\n## <a id='dataset'>1.1. Veri Seti Hakk\u0131ndaki A\u00e7\u0131klamalar<\/a>\n\nVeri Setini bu verisetini  [Brain MRI Images for Brain Tumor Detection](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection). ve [Brain Tumor Classification (MRI)](https:\/\/www.kaggle.com\/sartajbhuvaji\/brain-tumor-classification-mri\/activity) verisetlerindeki benzer resimleri kullanarak yeni bir veri seti olu\u015fturdum\n\n* `NO` -  `0` olarak kodland\u0131 (T\u00fcm\u00f6r Yok)\n* `YES` -  `1` olarak kodland\u0131 (T\u00fcm\u00f6r Var\n\nMR g\u00f6r\u00fcnt\u00fcleri hakk\u0131nda kullan\u0131lan verisetinde yeterli bilgiler mevcut de\u011fildir.\n\n## <a id='tumor'>1.2. Beyin T\u00fcm\u00f6r\u00fc Nedir?<\/a>\n\n> Beyinde anormal h\u00fccreler olu\u015ftu\u011funda bir beyin t\u00fcm\u00f6r\u00fc olu\u015fur. \u0130ki ana t\u00fcm\u00f6r t\u00fcr\u00fc vard\u0131r: kanserli (k\u00f6t\u00fc huylu) t\u00fcm\u00f6rler ve iyi huylu t\u00fcm\u00f6rler. Kanserli t\u00fcm\u00f6rler, beyinde ba\u015flayan birincil t\u00fcm\u00f6rler ve beyin metastaz t\u00fcm\u00f6rleri olarak bilinen ba\u015fka yerlerden yay\u0131lan ikincil t\u00fcm\u00f6rler olarak ikiye ayr\u0131labilir. T\u00fcm beyin t\u00fcm\u00f6r\u00fc t\u00fcrleri, beynin ilgili b\u00f6l\u00fcm\u00fcne ba\u011fl\u0131 olarak de\u011fi\u015fen semptomlar \u00fcretebilir. Bu semptomlar ba\u015f a\u011fr\u0131s\u0131, n\u00f6betler, g\u00f6rme sorunlar\u0131, kusma ve zihinsel de\u011fi\u015fiklikleri i\u00e7erebilir. Ba\u015f a\u011fr\u0131s\u0131 klasik olarak sabahlar\u0131 daha k\u00f6t\u00fcd\u00fcr ve kusmayla ge\u00e7er. Di\u011fer semptomlar aras\u0131nda y\u00fcr\u00fcme, konu\u015fma veya duyularda zorluk olabilir. Hastal\u0131k ilerledik\u00e7e bilin\u00e7 kayb\u0131 meydana gelebilir.\n>\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5f\/Hirnmetastase_MRT-T1_KM.jpg)\n>\n> *Manyetik rezonans g\u00f6r\u00fcnt\u00fclemede g\u00f6sterilen akci\u011fer kanserinden sa\u011f serebral hemisferdeki beyin metastaz\u0131.*\n\n","f555264c":"B\u00fct\u00fcn g\u00f6r\u00fcnt\u00fc dosyalar\u0131m\u0131z bir klas\u00f6r\u00fcn i\u00e7inde bu klas\u00f6r\u00fcn i\u00e7inde  `yes` ve `no` adl\u0131 iki alt klas\u00f6r bulunuyor. Bu klas\u00f6rlerdeki verileri kullanarak `train`, `val` ve `test` klas\u00f6rlerini olu\u015fturaca\u011f\u0131m","762f5400":"\u015eimdi test setinden yanl\u0131\u015f s\u0131n\u0131fland\u0131r\u0131lan MR g\u00f6r\u00fcnt\u00fclerine bakal\u0131m","893fce57":"## <a id='perf'>4.4. Model Performans\u0131<\/a>","00df75a6":"# <a id='cnn'>4. CNN Model<\/a>\n\n Temel model olarak VGG-16 mimarisi ve a\u011f\u0131rl\u0131klar\u0131 ile [Transfer Learning](https:\/\/towardsdatascience.com\/keras-transfer-learning-for-beginners-6c9b8b7143e) mimarisini kulland\u0131m .\n\n## <a id='aug'>4.1. Data Augmentation<\/a>\n\nK\u00fc\u00e7\u00fck veri k\u00fcmem oldu\u011fu i\u00e7in  [Data Augmentation] ad\u0131 verilen tekni\u011fi kulland\u0131m(https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html) E\u011fitim setinin boyutunu 'b\u00fcy\u00fctmeye' yard\u0131mc\u0131 olur.\n\n### <a id='demo'>4.1.1. \u00d6rnek<\/a>\n\nBu, artt\u0131r\u0131lm\u0131\u015f g\u00f6r\u00fcnt\u00fclerin nas\u0131l g\u00f6r\u00fcnd\u00fc\u011f\u00fc hakk\u0131ndaki \u00f6rnek g\u00f6r\u00fcnt\u00fcd\u00fcr.","43c3d347":" ## <a id='build'>4.3. Metriklerin Hesaplanmas\u0131<\/a>","543a5c48":"G\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi, resimler farkl\u0131 'geni\u015flik' ve 'y\u00fckseklik' e ve farkl\u0131 'siyah k\u00f6\u015felere' sahiptir. VGG-16 imput katman\u0131 i\u00e7in g\u00f6r\u00fcnt\u00fc boyutu `(224,224) '' oldu\u011fundan, baz\u0131 geni\u015f g\u00f6r\u00fcnt\u00fcler yeniden boyutland\u0131r\u0131ld\u0131ktan sonra tuhaf g\u00f6r\u00fcnebilir. Oran da\u011f\u0131l\u0131mlar\u0131n\u0131n histogram\u0131 ('oran = geni\u015flik \/ y\u00fckseklik`):","36567ad0":"Bir sonraki ad\u0131m, g\u00f6r\u00fcnt\u00fcleri '' (224,224) '' olarak yeniden boyutland\u0131rmak ve VGG-16 model giri\u015fi i\u00e7in gereken \u00f6n i\u015flemeyi uygulamak olacakt\u0131r.","4a6c3191":"# <a id='import'>3. Verilerin Y\u00fcklenmesi ve \u00d6ni\u015flemlerin Uygulanmas\u0131<\/a>","74461dca":"# <a id='concl'>5. Modelin Kaydedilmesi<\/a>\n\n","d2830274":"# <a id='env'>2. Gerekli Ortamlar\u0131n Kurulmas\u0131<\/a>","b4a6c316":"Normalle\u015ftirmenin ilk ad\u0131m\u0131 beyni g\u00f6r\u00fcnt\u00fclerden ay\u0131rmak olacakt\u0131r.Normalle\u015ftirme i\u00e7in a\u015fa\u011f\u0131daki tekni\u011fi uygulad\u0131m [pyimagesearch](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/).","4473292f":"## <a id='build'>4.2. VGG-16 Modelinin Kurulmas\u0131<\/a>"}}