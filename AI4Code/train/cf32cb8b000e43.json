{"cell_type":{"9baac133":"code","cb09d82b":"code","f5e42cd7":"code","dbb32c2e":"code","81b09a0b":"code","e9c2f7eb":"code","f346d5d7":"code","0d7a0c0b":"code","3f6a749d":"code","4178ee26":"code","bcb119b2":"code","a3ec059a":"code","b50501c5":"code","c8292cf3":"code","98aded2d":"code","67a97beb":"code","c91ac286":"code","41fcf89d":"code","6974b7b7":"code","0d00cb4c":"code","11f053b6":"code","bfbc03f8":"code","2bad34c9":"code","f00a0925":"code","64931b0a":"code","8fab0ad4":"code","f82184d1":"code","2420d3e5":"code","41c776a8":"code","b026b521":"code","d0656ab2":"markdown","acee851d":"markdown","530b0728":"markdown","e7d5ea1f":"markdown","c9bedfd3":"markdown","ce7747eb":"markdown","9b5924db":"markdown","bf8becfb":"markdown","dc58b60b":"markdown","b32ba2c2":"markdown","b4aeaeb6":"markdown","260d484f":"markdown","3812a0ee":"markdown","fd8bba9d":"markdown","c019c65a":"markdown","86d15612":"markdown","e38163f3":"markdown","f3ba695b":"markdown","c8a7b638":"markdown","4dfeb4f9":"markdown","0b7aca30":"markdown"},"source":{"9baac133":"'''Import basic modules'''\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom datetime import datetime    \nfrom pandas import Series \nimport statsmodels.api as sm\n\n'''import visualization'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n%matplotlib inline\nimport altair as alt # visualization\n\n'''Display markdown formatted output like bold, italic bold etc.'''\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))\n    \n'''Ignore deprecation and future, and user warnings.'''   \nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning)","cb09d82b":"\"\"\"Let's look on the Bajaj-Auto stok price dataset\"\"\"\nstock = pd.read_csv(\"..\/input\/nifty50-stock-market-data\/BAJAJ-AUTO.csv\")\nstock.head()","f5e42cd7":"\"\"\"Let's look on the data info\"\"\"\nstock.info()","dbb32c2e":"stock.Date = pd.to_datetime(stock.Date, format=\"%Y-%m-%d\")\nstock[\"month\"] = stock.Date.dt.month\nstock[\"week\"] = stock.Date.dt.week\nstock[\"day\"] = stock.Date.dt.day\nstock[\"day_of_week\"] = stock.Date.dt.dayofweek\nstock.fillna(stock.mean(), inplace=True)\n\nstock.set_index(\"Date\", drop=False, inplace=True)\nstock.head()","81b09a0b":"bars = alt.Chart(stock).mark_trail(color='orange').encode(\n    x = 'Date:T',\n    y = 'VWAP:Q',\n).properties(\n    title={\n    \"text\":['Volume Weighted Average Price (VWAP)'],\n    \"subtitle\":['There is a continuos increase in the VWAP price till 2018 and a certain dip in 2019'],\n    \"fontSize\":15,\n    \"fontWeight\": 'bold',\n    \"font\":'Courier New',\n    }\n)\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n(bars + text).properties( height=300, width=600)","e9c2f7eb":"vwap_df = stock[['VWAP']]\nstart_date = datetime(2017,1,1)\nend_date = datetime(2018,12,1)\ntemp = vwap_df[(start_date <=vwap_df.index) & (end_date <=vwap_df.index)].reset_index()\nbars = alt.Chart(temp).mark_trail(color='orange').encode(\n    x = 'Date:T',\n    y = 'VWAP:Q',\n).properties(\n    title={\n    \"text\":['Trend of VWAP in 2019  '],\n    \"subtitle\":['There is a continuos increase in the VWAP price in 2019'],\n    \"fontSize\":15,\n    \"fontWeight\": 'bold',\n    \"font\":'Courier New',\n    }\n)\n\ntext = bars.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n(bars + text).properties( height=300, width=600)","f346d5d7":"# set base for creat custom legend and plots\nbase = alt.Chart(stock).transform_calculate(\nlegend1=\"'Close prices of stocks'\",\nlegend2=\"'Open price of stock'\",\n\n)\nscale = alt.Scale(domain=[\"Close prices of stocks\", \"Open price of stock\"], range=['blue', 'violet', ])\n\n# timeseries plot of close prices of stocks in blue colour\nline1 = base.mark_line(color='blue').encode(\nx = 'Date:T',\ny = 'Close:Q',\ncolor=alt.Color('legend1:N', scale=scale, title=''),\n)\n\n# timeseries plot of open prices of stocks in blue colour\nline2 = base.mark_line(color='violet').encode(\nx = 'Date:T',\ny = 'Open:Q',\ncolor=alt.Color('legend2:N', scale=scale, title='')\n)\n\ntext = line1.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n\n(line1 + line2 + text).properties(\n    title={\"text\":['Timeseries Plot of Close and Open Price of Stock Over Year'],\n           \"fontSize\":15,\n           \"fontWeight\": 'bold',\n           \"font\":'Courier New',},\n    height=500, width=600\n).interactive()","0d7a0c0b":"# set base for creat custom legend and plots\nbase = alt.Chart(stock).transform_calculate(\nlegend1=\"'High prices of stocks'\",\nlegend2=\"'Low price of stock'\",\n\n)\nscale = alt.Scale(domain=[\"High prices of stocks\", \"Low price of stock\"], range=['red', 'green', ])\n\n# timeseries plot of High prices of stocks red colour\nline1 = base.mark_line(color='red').encode(\nx = 'Date:T',\ny = 'High:Q',\ncolor=alt.Color('legend1:N', scale=scale, title=''),\n)\n\n# timeseries between low prices of stocks green colour\n\nline2 = base.mark_line(color='green').encode(\nx = 'Date:T',\ny = 'Low:Q',\ncolor=alt.Color('legend2:N', scale=scale, title='')\n)\n\ntext = line1.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n\n(line1 + line2 + text).properties(\n    title= {\"text\":['Timeseries Plot of High and Price of Stock Over Year'],\n                   \"fontSize\":15,\n                   \"fontWeight\": 'bold',\n                   \"font\":'Courier New',},\n    height=500, width=600\n).interactive()","3f6a749d":"# dataframe Moving Average for weeks 4, 16, 28, 40, 52 \n\ndf_ma = pd.DataFrame()\n# Grouping the data week by week by taking its average.So there will be total 52 rows in the final list\ndf_ma['Close'] = stock['Close'].resample('W').mean()\n\n# calculating moving averge\ndf_ma['weeks_4'] = df_ma['Close'].rolling(window = 4, min_periods = 1).mean()\ndf_ma['weeks_16'] = df_ma['Close'].rolling(window = 16, min_periods = 1).mean()\ndf_ma['weeks_28'] = df_ma['Close'].rolling(window = 28, min_periods = 1).mean()\ndf_ma['weeks_40'] = df_ma['Close'].rolling(window = 40, min_periods = 1).mean()\ndf_ma['weeks_52'] = df_ma['Close'].rolling(window = 52, min_periods = 1).mean()\n\ndf_ma","4178ee26":"# set base for creat custom legend and plots\nbase = alt.Chart(df_ma.reset_index()).transform_calculate(\nlegend1=\"'Close price'\",\nlegend2=\"'MA of weeks 4'\",\nlegend3=\"'MA of weeks 16'\",\nlegend4=\"'MA of weeks 28'\",\nlegend5=\"'MA of weeks 40'\",\nlegend6=\"'MA of weeks 52'\"\n)\nscale = alt.Scale(domain=[\"Close price\", \n                          \"MA of weeks 4\",\n                          \"MA of weeks 16\",\n                          \"MA of weeks 28\",\n                          \"MA of weeks 40\",\n                          \"MA of weeks 52\"], \n                  range=['blue', \n                         'gold', \n                         'darkgreen', \n                         'slategray', \n                         'deeppink',\n                         'firebrick'])\n\nline1 = base.mark_line(color='blue').encode(\nx = 'Date:T',\ny = 'Close:Q',\ncolor=alt.Color('legend1:N', scale=scale, title=''),\n)\n\nline2 = base.mark_line(color='gold').encode(\nx = 'Date:T',\ny = 'weeks_4:Q',\ncolor=alt.Color('legend2:N', scale=scale, title='')\n)\n\nline3 = base.mark_line(color='darkgreen').encode(\nx = 'Date:T',\ny = 'weeks_16:Q',\ncolor=alt.Color('legend3:N', scale=scale, title='')\n)\n\nline4 = base.mark_line(color='slategray').encode(\nx = 'Date:T',\ny = 'weeks_28:Q',\ncolor=alt.Color('legend4:N', scale=scale, title='')\n)\n\nline5 = base.mark_line(color='deeppink').encode(\nx = 'Date:T',\ny = 'weeks_40:Q',\ncolor=alt.Color('legend5:N', scale=scale, title='')\n)\n\nline6 = base.mark_line(color='firebrick').encode(\nx = 'Date:T',\ny = 'weeks_52:Q',\ncolor=alt.Color('legend6:N', scale=scale, title='')\n)\n\ntext = line1.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n\n(line1 + line2 + line3 + line4 + line5 + line6 +text).properties(\n    title={\"text\":['Moving Average Plot of Stock Over the Weeks'],\n           \"fontSize\":15,\n           \"fontWeight\": 'bold',\n           \"font\":'Courier New',},\n    height=500, width=600\n).interactive()","bcb119b2":"# we will here use resample.Resampler.asfreq() function. Because it also provide us option of padding (backwardfill\/forwardfill missing values \"not NANs\" ). \n# source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.asfreq.html We are using this, because on saturdays and sundays, market remains closed, so friday's close price could be forwarded in closing days.\nstock_day = stock.asfreq('D', method ='pad')\n\ndf_rw = pd.DataFrame()\ndf_rw['Close'] = stock['Close']\ndf_rw['day_10'] = df_rw['Close'].rolling(window = 10, min_periods = 1).mean()\ndf_rw['day_50'] = df_rw['Close'].rolling(window = 50, min_periods = 1).mean()\ndf_rw","a3ec059a":"# set base for creat custom legend and plots\nbase = alt.Chart(df_ma.reset_index()).transform_calculate(\nlegend1=\"'Close price'\",\nlegend2=\"'MA of day 10'\",\nlegend3=\"'MA of day 50'\",\n\n)\nscale = alt.Scale(domain=[\"Close price\", \n                          \"MA of day 10\",\n                          \"MA of day 50\",], \n                  range=['blue',   \n                         'pink', \n                         'firebrick'])\n\nline1 = base.mark_line(color='blue').encode(\nx = 'Date:T',\ny = 'Close:Q',\ncolor=alt.Color('legend1:N', scale=scale, title=''),\n)\n\nline2 = base.mark_line(color='pink').encode(\nx = 'Date:T',\ny = 'weeks_4:Q',\ncolor=alt.Color('legend2:N', scale=scale, title='')\n)\n\nline3 = base.mark_line(color='firebrick').encode(\nx = 'Date:T',\ny = 'weeks_16:Q',\ncolor=alt.Color('legend3:N', scale=scale, title='')\n)\n\n\ntext = line1.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)   \n\n\n(line1 + line2 + line3  +text).properties(\n    title={\"text\":['Moving Average Plot of Stock Over the Days'],\n           \"fontSize\":15,\n           \"fontWeight\": 'bold',\n           \"font\":'Courier New',},\n    height=500, width=600\n).interactive()","b50501c5":"from statsmodels.tsa.stattools import acf, pacf\n# data for partial autocorrelation plot\nlags = 50\nsource = pd.DataFrame({\n    'lags': list(range(lags+1)),\n    'PACF': pacf(stock[\"Close\"], nlags=lags)\n})\n\n# ploting partial autocorrelation plot https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.graphics.tsaplots.plot_pacf.html )\nbar = alt.Chart(source).mark_bar().encode(\n    x='lags:Q',\n    y='PACF:Q',\n\n)\ncircle = alt.Chart(source).mark_circle(size = 50, color='red').encode(\n    x='lags:Q',\n    y='PACF:Q',\n\n)\ntext = bar.mark_text(\n    align='left',\n    baseline='middle',\n    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n)  \n\n(bar + circle + text).properties(\n    title={\"text\":['Partial AutoCorrelation Plot With 50 Lags of Stocks'],\n                   \"fontSize\":15,\n                   \"fontWeight\": 'bold',\n                   \"font\":'Courier New',},\n    height=500, width=600\n)","c8292cf3":"df_train = stock[stock.Date < \"2020\"]\ndf_valid = stock[stock.Date >= \"2020\"]\n\nfrom fbprophet import Prophet\nmodel = Prophet()\nmodel.fit(df_train[[\"Date\", \"VWAP\"]].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast = model.predict(df_valid[[\"Date\", \"VWAP\"]].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))","98aded2d":"model.plot_components(forecast)","67a97beb":"model.plot(forecast)\nplt.title('Volume Weighted Average Price (VWAP) With Predicted Values', fontsize=15)\nplt.show()","c91ac286":"# train\/test split\ndef train_data(data):\n    x = data['Close']\n    X_train = []\n    y_train = []\n    for i in range(60, 2870):\n        X_train.append(x[i-60:i])\n        y_train.append(x[i])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    return X_train, y_train\n\ndef test_data(data):\n    x = data['Close'][len(data['Close'])-182:]\n    X_test = []\n    y_test = []\n    for i in range(60, 182):\n        X_test.append(x[i-60:i])\n        y_test.append(x[i])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    return X_test, y_test","41fcf89d":"# performing \nX_train, y_train = train_data(stock_day)\n\nX_test, y_test = test_data(stock_day)\n\n# shape of the input and output\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","6974b7b7":"# train test split plot\ntrain_line = alt.Chart(stock[stock.Date < \"2020\"]['Close'].reset_index()).mark_line(color='blue').encode(\nx = 'Date:T',\ny = 'Close:Q',\n)\n\ntest_line = alt.Chart(stock[stock.Date >= \"2020\"]['Close'].reset_index()).mark_line(color='orange').encode(\nx = 'Date:T',\ny = 'Close:Q',\n)\n\n(train_line + test_line ).properties(\n        title='TRAIN\/TEST SPLIT',\n        height=200, width=600\n    ) ","0d00cb4c":"import xgboost\nfrom hyperopt import hp\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nxgb = xgboost.XGBRegressor(random_state = 101, tree_method='gpu_hist')","11f053b6":"# Traing and  Evaluate model on train\/test set\nxgb.fit(X_train, y_train)\nprediction = xgb.predict(X_test)\nmae = np.round(mean_absolute_error(prediction, y_test), 5)\nmse = np.round(mean_squared_error(prediction, y_test), 5)\nprint(' ')\nprint('Mean Absolute Error :',mae)\nprint('Mean Absolute Error :',mse)","bfbc03f8":"# kfold split\nkfold = KFold(n_splits=10, shuffle=True)\nval_score = cross_val_score(xgb, X_train, y_train, cv=kfold, n_jobs= -1, scoring = 'neg_mean_absolute_error')\nval_score = (-1 * val_score)\nval_score = np.round(val_score.mean(), 5)\nprint('CV Mean Absolute Error :', val_score)","2bad34c9":"## Create a function to tune hyperparameters of the selected models.\ndef tune_hyperparameters(model, params, X_train, y_train):\n    global best_params, best_score #if you want to know best parametes and best score\n## Construct grid search object with 3 fold cross validation\n    kfold = KFold(n_splits=5, shuffle=True)\n    regcv = RandomizedSearchCV(estimator=xgb_opt, \n                               param_distributions=params, \n                               cv = kfold, \n                               verbose = 1, \n                               scoring = 'neg_mean_absolute_error', \n                               n_jobs = -1)\n    regcv.fit(X_train, y_train)\n    best_params = regcv.best_params_ \n    best_score = np.round((-1 * regcv.best_score_), 5)\n    return best_params, best_score","f00a0925":"# Difine hyperparameters of Xgboost\nparams = {\n        'n_estimators': randint(1000, 2000),\n        \"learning_rate\": uniform(0.01, 0.06),\n        'max_depth': [5, 10, 15, 20],\n        'min_child_weight': [1, 5, 10, 15],\n        'subsample': [0.7, 0.05, 0.1],\n        'gamma': [0.1, 0.5, 0.05],\n        'colsample_bytree': [0.1, 0.7, 0.05],\n        'alpha' : [0.5, 1, 5],\n        'lambda': [0.1, 1, 3],\n}\n\n# Define the model \nxgb_opt = xgboost.XGBRegressor(random_state = 101, tree_method='gpu_hist')\n\n# Turning for jio data\ntune_hyperparameters(xgb_opt, params, X_train, y_train)\nbest_params, best_score = best_params, best_score\nprint('best params:{} & best_mae_score:{:0.5f}' .format(best_params, best_score))","64931b0a":"# Prepare the data for predicting the Close Value on 01-07-2020.\n\njul_x = stock_day['Close'][len(stock_day['Close'])-60:]\njul_X_test = []\njul_X_test.append(jul_x[0:])\njul_X_test = np.array(jul_X_test)\njul_X_test","8fab0ad4":"# fitting and preding on optimize param\nxgb_opt = xgboost.XGBRegressor(**best_params)\n\nxgb_opt.fit(X_train, y_train)\n# predicting the Closing Values for the Last 6 month of 2020\npred = xgb_opt.predict(X_test)\n# predicting the closing price of 1 july 2020\njul_pred = xgb_opt.predict(jul_X_test)\nprint('')\nprint('Prediction of close value of JIO for 1st july 2020:', prediction)","f82184d1":"pred.shape","2420d3e5":"stock_day['Close'][len(stock_day['Close'])-122:].shape","41c776a8":"# Creating dataframe of y_test, prediction values Last 30 Days of 2015, and prediction values 1st jan 2016 ,\n\ntest_pred_df = stock_day['Close'][len(stock_day['Close'])-122:].reset_index()\n\n# prediction value\ntest_pred_df['prediction'] = pred\n\n# prediction value of 1 jan 2016\nnew_row = {'Date': pd.to_datetime('2020-07-01'), 'jul_pred': jul_pred}\ntest_pred_df = test_pred_df.append(new_row, ignore_index=True)","b026b521":"base = alt.Chart(test_pred_df).transform_calculate(\nlegend1=\"'Actual price of last 6 months'\",\nlegend2=\"'Predicted price of last 6 months'\",\nlegend3=\"'First July 2020 Close price Prediction'\"\n)\nscale = alt.Scale(domain=['Actual price of last 6 months', 'Predicted price of last 6 months', 'First July 2020 Close price Prediction'], range=['blue', 'red', 'black'], zero=False)\n\n# timeseries plot of close prices of stocks\/indices in blue colour\nline1 = base.mark_line(color='blue').encode(\nx = 'Date:T',\ny = 'Close:Q',\ncolor=alt.Color('legend1:N', scale=scale, title=''),\n)\n\n# timeseries between two volume shocks in a different color (Red)\n# Since the difference value is too big, in order to visualize all plots properly,\n# we are taking the percentage change of Volumes.\nline2 = base.mark_line(color='red').encode(\nx = 'Date:T',\ny = 'prediction:Q',\ncolor=alt.Color('legend2:N', scale=scale, title='')\n)\n\n\ncircle = base.mark_point(color='black',size=50).encode(\nx = 'Date:T',\ny = 'jul_pred:Q',\ncolor=alt.Color('legend3:N', scale=scale, title='')\n)\n\n(line1 + line2 + circle).properties(\n    title={\"text\":['Fisrt July Prediction'],\n                   \"fontSize\":15,\n                   \"fontWeight\": 'bold',\n                   \"font\":'Courier New',},\n    height=200, width=650\n).interactive()","d0656ab2":"## <font color=\"teal\">Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated.<\/font>","acee851d":"#### MA of stock over the Weeks","530b0728":"mean absolute squared error of model for both the data set has decreaded from previous train\/test split","e7d5ea1f":"<h1 style=\"color:royalblue;\">Introduction<\/h1>\nTime series analysis is a statistical technique that deals with time series data, or trend analysis.  Time series data means that data is in a series of  particular time periods or intervals.  The data is considered in three types:\n\n* **Time series data:** A set of observations on the values that a variable takes at different times.\n\n* **Cross-sectional data:** Data of one or more variables, collected at the same point in time.\n\n* **Pooled data:** A combination of time series data and cross-sectional data.\n\n<i style=\"color:royalblue;\">Time Series Analysis is used for many applications such as:<\/i>\n * Economic Forecasting\n * Sales Forecasting\n * Budgetary Analysis\n * Stock Market Analysis\n * Yield Projections\n * Process and Quality Control\n * Inventory Studies\n * Workload Projections\n * Utility Studies\n * Census Analysis\n     \n<h2 style=\"color:royalblue;\">Components of Time Series<\/h2>\nTime series data consist of four components:\n\n * Trend Component: This is a variation that moves up or down in a reasonably predictable pattern over a long period.\n\n * Seasonality Component: is the variation that is regular and periodic and repeats itself over a specific period such as a day, week, month, season, etc.,\n\n * Cyclical Component: is the variation that corresponds with business or economic 'boom-bust' cycles or follows their own peculiar cycles, and\n\n * Random Component: is the variation that is erratic or residual and does not fall under any of the above three classifications.\n\nTo make this concept more clear here is a visual interpretation of the various components of the Time Series. You can view the original diagram with its context, [here](https:\/\/www.atap.gov.au\/tools-techniques\/travel-demand-modelling\/6-forecasting-evaluation).\n\n![](https:\/\/kite.com\/wp-content\/uploads\/2019\/08\/variations-of-time-series.jpg )\n<h3 style=\"color:royalblue;\">Traditional Techniques:<\/h3> The fitting of time series models can be an ambitious undertaking. There are many methods of model fitting including the following:\n * Box-Jenkins ARIMA models\n * Box-Jenkins Multivariate Models\n * Holt-Winters Exponential Smoothing (single, double, triple)\nThe user's application and preference will decide the selection of the appropriate technique. It is beyond the realm and intention of the authors of this handbook to cover all these methods. The overview presented here will start by looking at some basic smoothing techniques:\n * Averaging Methods\n * Exponential Smoothing Techniques.\n \n<h3 style=\"color:royalblue;\">Modern Technique:<\/h3>\nAll these techniques tutorial are mention in this [notebook](https:\/\/www.kaggle.com\/rohanrao\/a-modern-time-series-tutorial) given by @rohanrao\n* Auto ARIMAX\n* Facebook Prophet\n* LightGBM\n* LSTM \n\n<h1 style=\"color:royalblue;\">NIFTY 50<\/h1>\nThe NIFTY 50 index National Stock Exchange of India's benchmark broad based stock market index for the Indian equity market. Full form of NIFTY is National Index Fifty. It represents the weighted average of 50 Indian company stocks in 13 sectors and is one of the two main stock indices used in India, the other being the BSE Sensex.\n\n<p>Nifty is owned and managed by India Index Services and Products (IISL), which is a wholly owned subsidiary of the NSE Strategic Investment Corporation Limited. IISL had a marketing and licensing agreement with Standard and Poor's for co-branding equity indices until 2013. The Nifty 50 was launched 1 April 1996, and is one of the many stock indices of Nifty.<\/p>\n**Source:**https:\/\/en.wikipedia.org\/wiki\/NIFTY_50\n\n<h1 style=\"color:royalblue;\">BAJAJ-AUTO<\/h1>\n<h4 style=\"color:darkblue;\">The Company<\/h4>\nThe Bajaj Group is amongst the top 10 business houses in India. Its footprint stretches over a wide range of industries, spanning automobiles (two wheelers manufacturer and three wheelers manufacturer), home appliances, lighting, iron and steel, insurance, travel and finance. The group's flagship company, Bajaj Auto, is ranked as the world's fourth largest three and two wheeler manufacturer and the Bajaj brand is well-known across several countries in Latin America, Africa, Middle East, South and South East Asia. Founded in 1926, at the height of India's movement for independence from the British, the group has an illustrious history. The integrity, dedication, resourcefulness and determination to succeed which are characteristic of the group today, are often traced back to its birth during those days of relentless devotion to a common cause. Jamnalal Bajaj, founder of the group, was a close confidant and disciple of Mahatma Gandhi. In fact, Gandhiji had adopted him as his son.\n<p>In 2007, Bajaj Auto acquired a 14% stake in KTM that has since grown to 48%. This partnership catalysed Bajaj Auto\u2019s endeavour to democratise motorcycle racing in India. Bajaj Auto today exclusively manufactures Duke range of KTM bikes and exports them worldwide. In FY2018, KTM was the fastest growing motorcycle brand in the country<\/p>\n\n**Source: https:\/\/www.bajajauto.com\/**\n<h4 style=\"color:orange;\">Before starting, I would like to thanks @parulpandey and @rohanrao for some very amazing inspiration.<\/h4>\n<h4> Kernel Inspiration:<\/h4>\n\n* https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-time-series-using-pandas\n\n* https:\/\/www.kaggle.com\/rohanrao\/a-modern-time-series-tutorial","c9bedfd3":"# <font color=\"brown\">Moving Average (MA)<\/font>\nThe moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks or any time period the trader chooses. [source](https:\/\/www.investopedia.com\/articles\/active-trading\/052014\/how-use-moving-average-buy-stocks.asp#:~:text=The%20moving%20average%20(MA)%20is,time%20period%20the%20trader%20chooses.)","ce7747eb":"#### MA of stock over the Days","9b5924db":"# <font color=\"brown\">XGBoost Modeling and Forecasting<\/font>\n### <font color=\"dodgerblue\">Predecting next day stock price<\/font>\n\nPreparing Data for the Regression.The model takes first sixty values as input and predict the next value. For that We have to prepare the data with sixty previous values as 'X' and current value as 'Y'.\n\nWe are taking last 6 month for Testing from 01-01-2020 to 30-06-2020 and remaining for the training 2870 days.\n\nThen, we are try to predict next day that is 01-07-2020. ","bf8becfb":"<h1 style=\"color:orange;font-family:courier\" align=\"center\">Time Series Analysis And Prediction<\/h1>\n<h2 style=\"color:orange;font-family:verdana;\" align=\"center\">NIFTY-50 Stock Market<\/h2>\n<h3 style=\"color:orange;\" align=\"center\">BAJAJ-AUTO<\/h3>","dc58b60b":"# Facebook Prophet\n\nProphet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods.\n\nThe input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast.\n\nSplitting the data into train and validation along with features.\n\n* **train:** Data from 26th May, 2008 to 31st December, 2019.\n* **valid:** Data from 1st January, 2019 to 31st December, 2020.\n\n```Note that the default parameters are used for Prophet. They can be tuned to improve the results.```","b32ba2c2":"Now that our data has been converted into the desired format, let\u2019s take a look at its various columns for further analysis.\n\n* **The Open and Close columns** indicate the opening and closing price of the stocks on a particular day.\n* **The High and Low columns** provide the highest and the lowest price for the stock on a particular day, respectively.\n* **The Volume column** tells us the total volume of stocks traded on a particular day.\nThe **volume weighted average price (VWAP)** is a trading benchmark used by traders that gives the average price a security has traded at throughout the day, based on both volume and price. It is important because it provides traders with insight into both the trend and value of a security.[source](https:\/\/www.investopedia.com\/terms\/v\/vwap.asp).","b4aeaeb6":"# Retrain and Predict Using Best Hyperparameters","260d484f":"# Data Prearation","3812a0ee":"# <font color=\"brown\">Creating Model<\/font>","fd8bba9d":"# <font color=\"brown\">Autocorrelation<\/font>\nAutocorrelation is a mathematical representation of the degree of similarity between a given time series and a lagged version of itself over successive time intervals. It is the same as calculating the correlation between two different time series, except autocorrelation uses the same time series twice: once in its original form and once lagged one or more time periods. [source](https:\/\/www.investopedia.com\/terms\/a\/autocorrelation.asp) ","c019c65a":"**We have successfully predicted the next day stock price.**","86d15612":"## <font color=\"brown\">Open and Close Stock Price<\/font>","e38163f3":"# Optimization of Hyperparameters\n\nLet's start optimizing Hyperparameters of the models.\n\nFor optimization we used Random Search to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out.","f3ba695b":"Being mean absolute squared error and mean squared error , smaller is better. Look like model not perform well. However, train_test split has its drawbacks. Because this approach introduces bias as we are not using all of our observations for testing and also we're reducing the train data size. To overcome this we can use a technique called cross validation where all the data is used for training and testing periodically. Thus we may reduce the bias introduced by train_test_split. From different cross validation methods, we would use k-fold cross validation. In sklearn we have a method cross_val_score for calculating k-fold cross validation score.","c8a7b638":"# Cross Validation","4dfeb4f9":"# Exploratory Data Analysis\nLet's explore the data and look at details at year, month and day level\n\n# <font color=\"brown\">Volume Weighted Average Price<\/font>","0b7aca30":"# 1. Importing Packages and Collecting Data"}}