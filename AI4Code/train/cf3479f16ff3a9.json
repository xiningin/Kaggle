{"cell_type":{"4e81560d":"code","fe02160e":"code","dc6771f5":"code","7a0e2637":"code","b919b15c":"code","c9aab559":"code","612910cc":"code","7ef13804":"code","ba82fd4f":"code","c0b60aff":"code","2d9aee6c":"code","087b2f4d":"code","80acb20b":"code","be816601":"code","4e6854f4":"code","830f4105":"code","6dbc149b":"code","3db4d868":"code","ca7bb84a":"code","8c75ac11":"code","8d708463":"code","913aef35":"code","82c65edd":"code","a1e0ef1e":"code","b51d517d":"code","af7783bd":"code","04dda952":"markdown","59d4ed72":"markdown","2836ddb8":"markdown","34a520f5":"markdown","0b489256":"markdown","9191f71f":"markdown","a8a1a708":"markdown","ee9a5eae":"markdown","7dc6a3bb":"markdown","13dc83e5":"markdown","76b104e8":"markdown","c095a414":"markdown","4eab072f":"markdown","0b5f07a3":"markdown","b9f2e1a0":"markdown","25fda98d":"markdown","7873c7c6":"markdown","c3ec19a7":"markdown","9c300a98":"markdown","1b5fec06":"markdown","50271ee5":"markdown","c2c70fbd":"markdown","75563d23":"markdown","e16addd1":"markdown"},"source":{"4e81560d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","fe02160e":"input_dir = \"..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\"\n \n\n# HYPERPARAMETERS\nRANDOM_STATE = 12\nTRAIN_SPLIT = 0.80\nOOV_TOK = \"<OOV>\"\nVOCAB_SIZE = 10000\nEMBEDDING_DIM = 16\nMAX_LENGTH = 120\nBATCH_SIZE=128\nEPOCHS=25\nEARLY_STOPPING_CRITERIA=3\nDROPOUT_P=0.4\nLEARNING_RATE = 0.01\nMOMENTUM =0.9\nMAX_ITER = 10000\nN_JOBS = 4\n\n","dc6771f5":"data = pd.read_csv(input_dir)\ndata.head()","7a0e2637":"def clean_data(data, column_text='review_comment_message', \n               column_score='review_score', \n               points_cut = [0, 2, 5], \n               classes = [0, 1]):\n    \n    df_bin = data\n    df_bin = df_bin.dropna(subset=[column_text])\n    df_bin['label'] = pd.cut(df_bin[column_score], bins=points_cut, labels=classes)\n    df_bin = df_bin.rename(columns={column_text: 'text'})\n    df_bin = df_bin[['text','label']]\n    \n    df_cat = data\n    df_cat = df_cat.dropna(subset=[column_text])\n    df_cat = df_cat.rename(columns={column_text: 'text' , column_score: 'label'})\n    df_cat = df_cat[['text','label']]\n    return df_bin ,df_cat\n\n\ndata_bin , data_cat = clean_data(data)","b919b15c":"fig = px.bar(x = np.unique(data_cat[\"label\"]),\n             y = [list(data_cat[\"label\"]).count(i) for i in np.unique(data_cat[\"label\"])] , \n             color = np.unique(data_cat[\"label\"]) ,\n             color_continuous_scale=\"Emrld\") \nfig.update_xaxes(title=\"Ratings\")\nfig.update_yaxes(title = \"Number of Reviews\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Rating vs No of reviews ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","c9aab559":"fig = px.bar(x = np.unique(data_bin[\"label\"]), \n             y = [list(data_bin[\"label\"]).count(i) for i in np.unique(data_bin[\"label\"])] , \n             color=np.unique(data_bin[\"label\"]) ,\n             color_continuous_scale=\"Emrld\")  \nfig.update_xaxes(title=\"Ratings\")\nfig.update_yaxes(title = \"Number of Reviews\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Rating vs No of reviews ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","612910cc":"stop_words = stopwords.words('portuguese')\nwordcloud = WordCloud(stopwords=stop_words,\n                      background_color=\"black\",\n                      width=1600, height=800).generate(' '.join(data_bin[\"text\"]))\nfig, ax = plt.subplots(figsize=(12,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud);","7ef13804":"def split_test_train(data, split_train=TRAIN_SPLIT, random_state=RANDOM_STATE):\n    df_train = data.sample(frac = split_train, random_state = random_state)\n    df_test = data.drop(df_train.index)\n\n    X_train = []\n    y_train = []\n    X_test = []\n    y_test = []\n    \n    for index, train in df_train.iterrows():\n        X_train.append(str(train['text']))\n        y_train.append(train['label'])\n    \n    for index, test in df_test.iterrows():\n        X_test.append(str(test['text']))\n        y_test.append(test['label'])\n        \n    y_train = np.array(y_train)\n    y_test = np.array(y_test) \n    \n    return X_train , y_train , X_test , y_test\n\n\nX_train , y_train , X_test , y_test = split_test_train(data_bin, split_train=TRAIN_SPLIT, random_state=RANDOM_STATE)","ba82fd4f":"tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token=OOV_TOK)\ntokenizer.fit_on_texts(X_train)\n\ndef preprocess(X_train, X_test, max_length, vocab_size, trunc_type='post', oov_tok = \"<OOV>\"):\n\n    training_sequences = tokenizer.texts_to_sequences(X_train)\n    X_train_padded = pad_sequences(training_sequences,maxlen=max_length, truncating=trunc_type)\n    \n    testing_sequences = tokenizer.texts_to_sequences(X_test)\n    X_test_padded = pad_sequences(testing_sequences,maxlen=max_length)\n    \n    return X_train_padded, X_test_padded\n\nX_train, X_test = preprocess(X_train, X_test, MAX_LENGTH, VOCAB_SIZE)","c0b60aff":"def create_model():\n    \n    input = tf.keras.Input(shape=(MAX_LENGTH))\n    \n    x = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)(input)\n    x = tf.keras.layers.LSTM(32, return_sequences=True)(x)\n    x = tf.keras.layers.LSTM(32)(x)\n    x = tf.keras.layers.Dropout(DROPOUT_P)(x)\n    \n    x = tf.keras.layers.Dense(800, activation='relu')(x)\n    x = tf.keras.layers.Dropout(DROPOUT_P)(x)\n    x = tf.keras.layers.Dense(400, activation='relu')(x)\n    x = tf.keras.layers.Dropout(DROPOUT_P)(x)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(input, output)\n\n    return model","2d9aee6c":"earlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                         patience=EARLY_STOPPING_CRITERIA,\n                                                         verbose= 1 ,\n                                                         restore_best_weights=True\n                                                        )\n\nmodel = create_model()\n\nmodel.compile(\n  loss = tf.keras.losses.BinaryCrossentropy(),\n  optimizer= tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE , momentum=MOMENTUM),\n  metrics=['accuracy']\n)\n\nmodel.summary()","087b2f4d":"history = model.fit(x = X_train,\n                    y = y_train, \n                    validation_data=(X_test, y_test),\n                    epochs=EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    callbacks= [earlyStoppingCallback]\n)\nhistory = pd.DataFrame(history.history)","80acb20b":"x = px.line(data_frame= history , y= [\"accuracy\" , \"val_accuracy\"] , \n            markers = True )\nx.update_xaxes(title=\"Number of Epochs\",\n              rangeslider_visible = True)\nx.update_yaxes(title = \"Accuracy\")\nx.update_layout(showlegend = True,\n    title = {\n        'text': 'Accuracy vs Number of Epochs',\n        'y':0.94,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nx.show()","be816601":"x = px.line(data_frame= history , \n            y= [\"loss\" , \"val_loss\"] , \n            markers = True )\nx.update_xaxes(title=\"Number of Epochs\",\n              rangeslider_visible = True)\nx.update_yaxes(title = \"Loss\")\nx.update_layout(showlegend = True,\n    title = {\n        'text': 'Loss vs Number of Epochs',\n        'y':0.94,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nx.show()","4e6854f4":"y_preds_prob = model.predict(X_test, verbose=0)\ny_preds = np.where(y_preds_prob> 0.5, 1, 0)\ny_preds_prob = y_preds_prob[:, 0]\ny_preds = y_preds[:, 0]\n\n[loss,accuracy] = model.evaluate(X_test, y_test ,verbose = 0 )\nprint(\"Accuracy on Test Data :\", accuracy*100 ,\"%\")\nprint(classification_report(y_test ,y_preds))","830f4105":"cm_data = confusion_matrix(y_test , y_preds)\ncm = pd.DataFrame(cm_data, columns=[0,1], index = [0,1])\ncm.index.name = 'Actual'\ncm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nplt.title('Confusion Matrix', fontsize = 20)\n\nsns.set(font_scale=1.4)\nax = sns.heatmap(cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g')","6dbc149b":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds  = roc_curve(y_test, y_preds_prob)\nauc = roc_auc_score(y_test, y_preds_prob)\nplt.figure(figsize = (10,7))\nplt.plot(fpr,tpr,color='darkred', lw=2, label=\"LSTM , auc=\"+str(round(auc,2)))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.title('ROC AUC CURVE- LSTM', fontsize = 16)\nplt.legend(loc=4)\nplt.show()","3db4d868":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","ca7bb84a":"lr = LogisticRegression(max_iter= MAX_ITER,random_state=RANDOM_STATE, n_jobs= N_JOBS)\nlr.fit(X_train,y_train)\nlr_preds = lr.predict(X_test)\nprint(\"Model score on Training Data = \", lr.score(X_train,y_train)*100,\"%\")\nprint(\"Model score on Testing Data = \", lr.score(X_test,y_test)*100,\"%\")","8c75ac11":"nb = MultinomialNB()\nnb.fit(X_train,y_train)\nnb_preds = nb.predict(X_test)\nprint(\"Model score on Training Data = \", nb.score(X_train,y_train)*100 ,\"%\")\nprint(\"Model score on Testing Data = \", nb.score(X_test,y_test)*100 ,\"%\")","8d708463":"svm = SVC(random_state=RANDOM_STATE , verbose = True)\nsvm.fit(X_train,y_train)\nsv_preds = svm.predict(X_test)\nprint(\"Model score on Training Data = \", svm.score(X_train,y_train)*100 ,\"%\")\nprint(\"Model score on Testing Data = \", svm.score(X_test,y_test)*100 ,\"%\")","913aef35":"rfc = RandomForestClassifier(n_estimators = 100, random_state =RANDOM_STATE , n_jobs = N_JOBS)\nrfc.fit(X_train,y_train)\nrf_preds = rfc.predict(X_test)\nprint(\"Model score on Training Data = \", rfc.score(X_train,y_train)*100 ,\"%\")\nprint(\"Model score on Testing Data = \", rfc.score(X_test,y_test)*100 ,\"%\")","82c65edd":"xgb = XGBClassifier(objective='binary:logistic' , \n                    use_label_encoder=False,  \n                    random_state = RANDOM_STATE,\n                    eval_metric='mlogloss')\nxgb.fit(X_train,y_train)\nxg_preds = xgb.predict(X_test)\nprint(\"Model score on Training Data = \", xgb.score(X_train,y_train)*100 ,\"%\")\nprint(\"Model score on Testing Data = \", xgb.score(X_test,y_test)*100 ,\"%\")","a1e0ef1e":"results = pd.DataFrame(data  = [accuracy*100] , columns =[\"NN-Lstm\"] , index = [\"Test Accuracy\"])\nresults['Logistic Regression'] = lr.score(X_test,y_test)*100\nresults['Naive Bayes'] = nb.score(X_test,y_test)*100\nresults['Support Vector Machine (SVM)'] = svm.score(X_test,y_test)*100\nresults['RandomForest Classifier'] = rfc.score(X_test,y_test)*100\nresults['XGBoost Classifier'] = xgb.score(X_test,y_test)*100\nresults = results.transpose()\nx = px.line(data_frame= results, y = [\"Test Accuracy\"] ,markers = True  )\nx.update_xaxes(title=\"Model Name\")\nx.update_yaxes(title = \"Test Accuracy\")\nx.update_layout(showlegend = True,\n    title = {\n        'text': 'Test Accuracy comparison of all 6 models',\n        'y':0.94,'x':0.5,'xanchor': 'center','yanchor': 'top'})\nx.show()","b51d517d":"lr_cm=confusion_matrix(y_test, lr_preds )\nnb_cm=confusion_matrix(y_test, nb_preds)\nsvm_cm=confusion_matrix(y_test, sv_preds)\nnn_cm= confusion_matrix(y_test,y_preds)\nrf_cm = confusion_matrix(y_test,rf_preds)\nxg_cm = confusion_matrix(y_test,xg_preds)\n\nplt.figure(figsize=(20,15))\nplt.suptitle(\"Confusion Matrices\",fontsize=24)\n\nplt.subplot(2,3,1)\nplt.title(\"Neural Network\")\nsns.heatmap(nn_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\n\nplt.subplot(2,3,2)\nplt.title(\"Naive Bayes\")\nsns.heatmap(nb_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine (SVM)\")\nsns.heatmap(svm_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\nplt.subplot(2,3,4)\nplt.title(\"Logistic Regression\")\nsns.heatmap(lr_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Classifier\")\nsns.heatmap(rf_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\nplt.subplot(2,3,6)\nplt.title(\"XGBosst Classifier\")\nsns.heatmap(xg_cm, cbar=False, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='g');\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)","af7783bd":"from sklearn.metrics import auc\npred_lr = lr.predict_proba(X_test)[:,1]\nfpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\nroc_auc_lr = auc(fpr_lr,tpr_lr)\n\npred_nb = nb.predict_proba(X_test)[:,1]\nfpr_nb,tpr_nb,_ = roc_curve(y_test,pred_nb)\nroc_auc_nb = auc(fpr_nb,tpr_nb)\n\npred_svm = svm.decision_function(X_test)\nfpr_svm,tpr_svm,_ = roc_curve(y_test,pred_svm)\nroc_auc_svm = auc(fpr_svm,tpr_svm)\n\nfpr_nn, tpr_nn, _  = roc_curve(y_test, y_preds_prob)\nroc_auc_nn = auc(fpr_nn,tpr_nn)\n\npred_rf = rfc.predict_proba(X_test)[:,1]\nfpr_rf,tpr_rf,_ = roc_curve(y_test,pred_rf)\nroc_auc_rf = auc(fpr_rf,tpr_rf)\n\npred_xg = xgb.predict_proba(X_test)[:,1]\nfpr_xg,tpr_xg,_ = roc_curve(y_test,pred_xg)\nroc_auc_xg = auc(fpr_xg,tpr_xg)\n\nf, axes = plt.subplots(2,3,figsize=(20,15))\naxes[0,0].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\naxes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'LSTM')\naxes[0,0].legend(loc='lower right', fontsize=13);\n\n\naxes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\naxes[0,1].legend(loc='lower right', fontsize=13)\n\naxes[0,2].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[0,2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,2].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,2].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\naxes[0,2].legend(loc='lower right', fontsize=13)\n\naxes[1,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes[1,0].legend(loc='lower right', fontsize=13)\n\n\naxes[1,1].plot(fpr_rf, tpr_rf, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_rf))\naxes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Random Forest Classifier')\naxes[1,1].legend(loc='lower right', fontsize=13);\n\n\naxes[1,2].plot(fpr_xg, tpr_xg, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_xg))\naxes[1,2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,2].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,2].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'XGB Classifier')\naxes[1,2].legend(loc='lower right', fontsize=13);\n\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\n","04dda952":"## Data Distribution among classes","59d4ed72":"## ROC-AUC Curves Comparison","2836ddb8":"## Evaluating LSTM model","34a520f5":"<a id=\"hp\"><\/a>\n# <center>HYPERPARAMETRERS AND DIRECTORIES<\/center>","0b489256":"<a id=\"im\"><\/a>\n# <center>IMPORTING LIBRARIES<\/center> ","9191f71f":"## Test Accuracies Comparison","a8a1a708":"<a id=\"com\"><\/a>\n# <center> COMPARISON WITH OTHER MODELS <\/center> ","ee9a5eae":"## Confusion Matrix Comparison","7dc6a3bb":"## [1. Imports](#im) ##\n## [2. HyperParameters](#hp) ##\n## [3. Data Loading and Preprocessing](#data) ##\n## [4. NN-LSTM Model](#lstm) ##\n## [5. Comparison with other Models](#com) ##\n###    [5.1 Logistic Regression](#lr) ###\n###    [5.2 Naive Bayes](#nb) ###\n###    [5.3 Support Vector Classifier(SVC)](#sv) ###\n###    [5.4 Random Forest Classifier](#rf) ###\n###    [5.5 XGB Classifier](#xg) ###\n## [6. Performance Metrics Comparision](#per) ##\n","13dc83e5":"# <center>If you find this notebook useful, support with an upvote\ud83d\udc4d<\/center>","76b104e8":"## Training LSTM Model","c095a414":"<a id=\"nb\"><\/a>\n# NAIVE BAYES","4eab072f":"### WORDCLOUD","0b5f07a3":"Due to data imbalanced in 5 different rating categories , we will work on binary classification problem instead. The output will be 0 and 1 representing NOT HAPPY and HAPPY respectively.","b9f2e1a0":"<a id=\"sv\"><\/a>\n# SUPPORT VECTOR CLASSIFIER (SVC)","25fda98d":"<a id=\"lstm\"><\/a>\n# <center> NN-LSTM MODEL <\/center>","7873c7c6":"<a id=\"xg\"><\/a>\n# XGBOOST CLASSIFIER","c3ec19a7":"**Created by Sanskar Hasija**\n\n**Feedback Based Rating Predictions**\n\n**18 OCTOBER 2021**\n","9c300a98":"<a id=\"data\"><\/a>\n# <center> DATA LOADING AND PRE-PROCESSING<\/center>","1b5fec06":"<a id=\"lr\"><\/a>\n# LOGISTIC REGRESSION","50271ee5":"<a id=\"per\"><\/a>\n# <center> PERFORMANCE METRICS COMPARISON <\/center> ","c2c70fbd":"<a id=\"rf\"><\/a>\n#  RANDOM FOREST CLASSIFIER","75563d23":"# <center> FEEDBACK BASED RATING PREDICTIONS <\/center>","e16addd1":"## Visualizing Training Results"}}