{"cell_type":{"62cfd33d":"code","30b48cbb":"code","9fc99257":"code","20fa816f":"code","d3a7b85e":"code","42958678":"code","88a1e635":"code","85798e56":"code","00ee5e58":"code","fc1c7fcf":"code","1ba2f20e":"code","ceeebaeb":"code","673ca85b":"code","4ebebc7d":"code","20d2fd58":"code","b95b0d8b":"code","f64142fd":"code","2b524f74":"code","69037a71":"code","96c7eb73":"code","21d8681a":"code","913a9726":"code","2b481cbd":"markdown","355c4515":"markdown","82c1cade":"markdown","697f53f3":"markdown","4c9d4fc6":"markdown","1180eb63":"markdown","2667a0bf":"markdown"},"source":{"62cfd33d":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))","30b48cbb":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nX = np.array(train.drop([\"label\"], axis=1)) \/ 255.\ny = np.array(train[\"label\"])\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y)\n\nX_test = np.array(test) \/ 255.\n\nprint(X.shape, y.shape, X_test.shape)","9fc99257":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","20fa816f":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2D, BatchNormalization, MaxPooling2D\nfrom tensorflow.keras.models import Model","d3a7b85e":"inp = Input(shape=(X_train.shape[1], ))\nreshape = Reshape(target_shape=(28, 28, 1))(inp)\nconv_1 = Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(reshape)\nconv_1 = BatchNormalization()(conv_1)\npool_1 = MaxPooling2D(pool_size=(2,2))(conv_1)\nconv_2 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_1)\nconv_2 = BatchNormalization()(conv_2)\npool_2 = MaxPooling2D(pool_size=(2,2))(conv_2)\nconv_3 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_2)\nconv_3 = BatchNormalization()(conv_3)\npool_3 = MaxPooling2D(pool_size=(2,2))(conv_3)\nflatten = Flatten()(pool_3)\nfc = Dense(128, activation=\"relu\")(flatten)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(fc)\nmodel_nodrop = Model(inp, outp)\n\nmodel_nodrop.summary()\n\nmodel_nodrop.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","42958678":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint('nodrop.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_nodrop.fit(X_train, \n                        y_train, \n                        verbose=0,\n                        batch_size=512, \n                        epochs=50, \n                        validation_data=(X_val, y_val), \n                        callbacks=[checkpoint])","88a1e635":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","85798e56":"max(val_acc)","00ee5e58":"inp = Input(shape=(X_train.shape[1], ))\nreshape = Reshape(target_shape=(28, 28, 1))(inp)\nconv_1 = Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(reshape)\nconv_1 = BatchNormalization()(conv_1)\npool_1 = MaxPooling2D(pool_size=(2,2))(conv_1)\nconv_2 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_1)\nconv_2 = BatchNormalization()(conv_2)\npool_2 = MaxPooling2D(pool_size=(2,2))(conv_2)\nconv_3 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_2)\nconv_3 = BatchNormalization()(conv_3)\npool_3 = MaxPooling2D(pool_size=(2,2))(conv_3)\nflatten = Flatten()(pool_3)\nfc = Dense(128, activation=\"relu\")(flatten)\nfc = Dropout(0.5)(fc)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(fc)\n\nmodel_drop = Model(inp, outp)\n\nmodel_drop.summary()\n\nmodel_drop.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","fc1c7fcf":"checkpoint = ModelCheckpoint('dropout.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_drop.fit(X_train, \n                      y_train,\n                      verbose=0,\n                      batch_size=64, \n                      epochs=50, \n                      validation_data=(X_val, y_val),\n                      callbacks=[checkpoint])","1ba2f20e":"acc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","ceeebaeb":"max(val_acc)","673ca85b":"# https:\/\/github.com\/andry9454\/KerasDropconnect\/blob\/master\/ddrop\/layers.py\n\nfrom tensorflow.keras.layers import Wrapper\nimport tensorflow.keras.backend as K\n\nclass DropConnect(Wrapper):\n    def __init__(self, layer, prob=1., **kwargs):\n        self.prob = prob\n        self.layer = layer\n        super(DropConnect, self).__init__(layer, **kwargs)\n        if 0. < self.prob < 1.:\n            self.uses_learning_phase = True\n\n    def build(self, input_shape):\n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = True\n        super(DropConnect, self).build()\n\n    def compute_output_shape(self, input_shape):\n        return self.layer.compute_output_shape(input_shape)\n\n    def call(self, x):\n        if 0. < self.prob < 1.:\n            self.layer.kernel = K.in_train_phase(K.dropout(self.layer.kernel, self.prob), self.layer.kernel)\n            self.layer.bias = K.in_train_phase(K.dropout(self.layer.bias, self.prob), self.layer.bias)\n        return self.layer.call(x)","4ebebc7d":"inp = Input(shape=(X_train.shape[1], ))\nreshape = Reshape(target_shape=(28, 28, 1))(inp)\nconv_1 = Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(reshape)\nconv_1 = BatchNormalization()(conv_1)\npool_1 = MaxPooling2D(pool_size=(2,2))(conv_1)\nconv_2 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_1)\nconv_2 = BatchNormalization()(conv_2)\npool_2 = MaxPooling2D(pool_size=(2,2))(conv_2)\nconv_3 = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(pool_2)\nconv_3 = BatchNormalization()(conv_3)\npool_3 = MaxPooling2D(pool_size=(2,2))(conv_3)\nflatten = Flatten()(pool_3)\nfc = DropConnect(Dense(128, activation=\"relu\"), prob=0.5)(flatten)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(fc)\n\nmodel_dropconn = Model(inp, outp)\n\nmodel_dropconn.summary()\n\nmodel_dropconn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","20d2fd58":"checkpoint = ModelCheckpoint('dropconn.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_dropconn.fit(X_train, \n                          y_train, \n                          verbose=0,\n                          batch_size=64, \n                          epochs=50, \n                          validation_data=(X_val, y_val),\n                          callbacks=[checkpoint])","b95b0d8b":"acc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","f64142fd":"max(val_acc)","2b524f74":"sub_nodrop = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub_drop = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub_dropconn = pd.read_csv(\"..\/input\/sample_submission.csv\")","69037a71":"model_nodrop.load_weights(\"nodrop.h5\")\nmodel_drop.load_weights(\"dropout.h5\")\nmodel_dropconn.load_weights(\"dropconn.h5\")","96c7eb73":"y_test = model_nodrop.predict(X_test, batch_size=1024, verbose=0)\nsub_nodrop.Label = np.argmax(y_test, axis=1)\nsub_nodrop.to_csv(\"submission_nodrop.csv\", index=False)","21d8681a":"y_test = model_drop.predict(X_test, batch_size=1024, verbose=0)\nsub_drop.Label = np.argmax(y_test, axis=1)\nsub_drop.to_csv(\"submission_drop.csv\", index=False)","913a9726":"y_test = model_dropconn.predict(X_test, batch_size=1024, verbose=0)\nsub_dropconn.Label = np.argmax(y_test, axis=1)\nsub_dropconn.to_csv(\"submission_dropconn.csv\", index=False)","2b481cbd":"# 3. DropConnect","355c4515":"# 2. Dropout","82c1cade":"# predict and submit","697f53f3":"# 1.  No drop","4c9d4fc6":"Hi, Kagglers.\n\nIn this kernel, I am trying to test [DropConnect](https:\/\/cs.nyu.edu\/~wanli\/dropc\/) with three Conv2D layers.","1180eb63":"# load dataset","2667a0bf":"# train test split"}}