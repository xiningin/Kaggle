{"cell_type":{"ae5b5253":"code","32a1fc68":"code","e610f907":"code","e3a01fe8":"code","87d89e9c":"code","7b23b069":"code","850a626f":"code","c8b06538":"code","4be4e7cb":"code","8488aa67":"code","4833cc52":"code","8add1e06":"code","58d3e0c6":"code","41310f08":"code","b6c1536b":"code","4d508258":"code","ef384e59":"code","2f74fd5f":"code","94dd82ae":"code","ef4e4a20":"code","c691f6a2":"code","fff31c3f":"code","25fd9b55":"code","3b579dc0":"code","47c33bdf":"code","22b6e453":"code","cf02d641":"code","42b00adc":"code","51069cd8":"code","f111a399":"code","2cd06042":"code","fbbfa71f":"code","3cc1f8ae":"code","7f0f779a":"markdown","3739f033":"markdown","e9340603":"markdown","7f5efe58":"markdown","55439a84":"markdown","70c90232":"markdown","128d635f":"markdown"},"source":{"ae5b5253":"import os\nfrom keras.applications.xception import Xception\nfrom keras.models import Model\nfrom keras.layers import Input, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\n\nfrom keras.utils import np_utils\n\nfrom keras.models import load_model\n\nimport cv2\n\nfrom keras import backend as K\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport gc","32a1fc68":"img = np.loadtxt(\"..\/input\/aptos2019-blindness-detection\/train.csv\",       # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\n                  delimiter=\",\",    # \u30d5\u30a1\u30a4\u30eb\u306e\u533a\u5207\u308a\u6587\u5b57\n                  skiprows=1,    # \u5148\u982d\u306e\u4f55\u884c\u3092\u7121\u8996\u3059\u308b\u304b\uff08\u6307\u5b9a\u3057\u305f\u884c\u6570\u307e\u3067\u306f\u8aad\u307f\u8fbc\u307e\u306a\u3044\uff09\n                  usecols=(0), # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u5217\u756a\u53f7\n                  dtype = \"str\"\n                 )\nimg","e610f907":"label = np.loadtxt(\"..\/input\/aptos2019-blindness-detection\/train.csv\",       # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\n                  delimiter=\",\",    # \u30d5\u30a1\u30a4\u30eb\u306e\u533a\u5207\u308a\u6587\u5b57\n                  skiprows=1,    # \u5148\u982d\u306e\u4f55\u884c\u3092\u7121\u8996\u3059\u308b\u304b\uff08\u6307\u5b9a\u3057\u305f\u884c\u6570\u307e\u3067\u306f\u8aad\u307f\u8fbc\u307e\u306a\u3044\uff09\n                  usecols=(1), # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u5217\u756a\u53f7\n                  dtype = \"int\"\n                 )\nlabel","e3a01fe8":"img_label_trains = []\nimg_label_validations = []\n\nfor i in range(4):\n    data_train, data_test, labels_train, labels_test = train_test_split(img, label, train_size=0.85,random_state=i*5,stratify=label)\n    \n    img_label_train = np.stack([data_train, labels_train],axis=1)\n    img_label_validation = np.stack([data_test, labels_test],axis=1)\n    \n    img_label_trains.append(img_label_train)\n    img_label_validations.append(img_label_validation)","87d89e9c":"#confirm dataset count(train)\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"0\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"1\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"2\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"3\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"4\"))\n#confirm dataset count\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"0\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"1\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"2\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"3\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"4\"))","7b23b069":"len(img_label_trains[0])","850a626f":"len(img_label_validations[0])","c8b06538":"def vertical_flip(image, rate=0.5):\n    if np.random.rand() < rate:\n        image = image[::-1, :, :]\n    return image\n\ndef horizontal_flip(image):\n    image = image[:, ::-1, :]\n    return image\n\ndef image_translation(img):\n    params = np.random.randint(-50, 51)\n    if not isinstance(params, list):\n        params = [params, params]\n    rows, cols, ch = img.shape\n\n    M = np.float32([[1, 0, params[0]], [0, 1, params[1]]])\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_shear(img):\n    params = np.random.randint(-20, 21)*0.01\n    rows, cols, ch = img.shape\n    factor = params*(-1.0)\n    M = np.float32([[1, factor, 0], [0, 1, 0]])\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_rotation(img):\n    params = np.random.randint(-30, 31)\n    rows, cols, ch = img.shape\n    M = cv2.getRotationMatrix2D((cols\/2, rows\/2), params, 1)\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_contrast(img):\n    params = np.random.randint(7, 10)*0.1\n    alpha = params\n    new_img = cv2.multiply(img, np.array([alpha]))                    # mul_img = img*alpha\n    #new_img = cv2.add(mul_img, beta)                                  # new_img = img*alpha + beta\n  \n    return new_img\n\ndef image_brightness2(img):\n    params = np.random.randint(-21, 22)\n    beta = params\n    b, g, r = cv2.split(img)\n    b = cv2.add(b, beta)\n    g = cv2.add(g, beta)\n    r = cv2.add(r, beta)\n    new_img = cv2.merge((b, g, r))\n    return new_img\n\ndef pca_color_augmentation_modify(image_array_input):\n    assert image_array_input.ndim == 3 and image_array_input.shape[2] == 3\n    assert image_array_input.dtype == np.uint8\n\n    img = image_array_input.reshape(-1, 3).astype(np.float32)\n    # \u5206\u6563\u3092\u8a08\u7b97\n    ch_var = np.var(img, axis=0)\n    # \u5206\u6563\u306e\u5408\u8a08\u304c3\u306b\u306a\u308b\u3088\u3046\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n    scaling_factor = np.sqrt(3.0 \/ sum(ch_var))\n    # \u5e73\u5747\u3067\u5f15\u3044\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n    img = (img - np.mean(img, axis=0)) * scaling_factor\n\n    cov = np.cov(img, rowvar=False)\n    lambd_eigen_value, p_eigen_vector = np.linalg.eig(cov)\n\n    rand = np.random.randn(3) * 0.1\n    delta = np.dot(p_eigen_vector, rand*lambd_eigen_value)\n    delta = (delta * 255.0).astype(np.int32)[np.newaxis, np.newaxis, :]\n\n    img_out = np.clip(image_array_input + delta, 0, 255).astype(np.uint8)\n    return img_out","4be4e7cb":"def get_random_data(image_lines_1, abs_path, img_width, img_height, data_aug):\n    image_file = abs_path + image_lines_1[0] + \".png\"\n    label = np.eye(5)[int(image_lines_1[1])]\n    \n    seed_image = cv2.imread(image_file)\n    seed_image = cv2.cvtColor(seed_image, cv2.COLOR_BGR2RGB)\n    seed_image = cv2.resize(seed_image, dsize=(img_width, img_height))\n    \n    if data_aug:\n        \n        r = np.random.rand()\n        \n        if r >= 0.5:\n    \n            seed_image = vertical_flip(seed_image)\n            seed_image = horizontal_flip(seed_image)\n            seed_image = image_shear(seed_image)\n            seed_image = image_rotation(seed_image)\n            seed_image = pca_color_augmentation_modify(seed_image)\n    \n    seed_image = seed_image \/ 255\n    \n    return seed_image, label","8488aa67":"def data_generator(image_lines, batch_size, abs_path, img_width, img_height, data_aug):\n    '''data generator for fit_generator'''\n    n = len(image_lines)\n    i = 0\n    while True:\n        image_data = []\n        label_data = []\n        for b in range(batch_size):\n            if i==0:\n                np.random.shuffle(image_lines)\n            image, label = get_random_data(image_lines[i], abs_path, img_width, img_height, data_aug)\n            image_data.append(image)\n            label_data.append(label)\n            i = (i+1) % n\n        image_data = np.array(image_data)\n        label_data = np.array(label_data)\n        yield image_data, label_data\n\ndef data_generator_wrapper(image_lines, batch_size, abs_path, img_width, img_height, data_aug):\n    n = len(image_lines)\n    if n==0 or batch_size<=0: return None\n    return data_generator(image_lines, batch_size, abs_path, img_width, img_height, data_aug)","4833cc52":"img_width, img_height = 449, 449\nnum_train = len(img_label_trains[0])\nnum_val = len(img_label_validations[0])\nbatch_size = 4\nprint(num_train, num_val)\nabs_path = \"..\/input\/aptos2019-blindness-detection\/train_images\/\"","8add1e06":"models = []\n\nfor i in range(4):\n\n    input_tensor = Input(shape=(img_height, img_width, 3))\n\n    xception_model = Xception(include_top=False, weights=None, input_tensor=input_tensor)\n\n    xception_model.load_weights(\"..\/input\/keras-pretrained-models\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n\n    x = xception_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=xception_model.input, outputs=outputs)\n    \n    model.compile(optimizer=optimizers.SGD(lr=0.001,momentum=0.9),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n    model.summary()\n    \n    models.append(model)","58d3e0c6":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","41310f08":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","b6c1536b":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","4d508258":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","ef384e59":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","2f74fd5f":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","94dd82ae":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","ef4e4a20":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","c691f6a2":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","fff31c3f":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","25fd9b55":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","3b579dc0":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","47c33bdf":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","22b6e453":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","cf02d641":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","42b00adc":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train\/\/batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val\/\/batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","51069cd8":"img_test = np.loadtxt(\"..\/input\/aptos2019-blindness-detection\/test.csv\",       # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\n                  delimiter=\",\",    # \u30d5\u30a1\u30a4\u30eb\u306e\u533a\u5207\u308a\u6587\u5b57\n                  skiprows=1,    # \u5148\u982d\u306e\u4f55\u884c\u3092\u7121\u8996\u3059\u308b\u304b\uff08\u6307\u5b9a\u3057\u305f\u884c\u6570\u307e\u3067\u306f\u8aad\u307f\u8fbc\u307e\u306a\u3044\uff09\n#                  usecols=(1), # \u8aad\u307f\u8fbc\u307f\u305f\u3044\u5217\u756a\u53f7\n                  dtype = \"str\"\n                 )","f111a399":"test_abs_path = \"..\/input\/aptos2019-blindness-detection\/test_images\/\"\n\ndata = []\nfor i in range(len(img_test)):\n    image_file = test_abs_path + img_test[i] + \".png\"\n    seed_image = cv2.imread(image_file)\n    seed_image = cv2.cvtColor(seed_image, cv2.COLOR_BGR2RGB)\n    seed_image = cv2.resize(seed_image, dsize=(img_width, img_height))\n    seed_image = np.expand_dims(seed_image, axis=0)\n    seed_image = seed_image \/ 255\n    predict1 = models[0].predict(seed_image)\n    predict2 = models[1].predict(seed_image)\n    predict3 = models[2].predict(seed_image)\n    predict4 = models[3].predict(seed_image)\n    predict_mean = (predict1+predict2+predict3+predict4)\/4\n    x = np.array([img_test[i], np.argmax(predict_mean)])\n    data.append(x)\n    \ndata = np.array(data)","2cd06042":"columns = ['id_code', 'diagnosis']\nname = 'sample'\n\nd = pd.DataFrame(data=data, columns=columns, dtype='str')","fbbfa71f":"d.to_csv(\"submission.csv\",index=False)","3cc1f8ae":"df = pd.read_csv(\"submission.csv\")\nprint(df)","7f0f779a":"# load image and label","3739f033":"# data augmatation","e9340603":"# make unsamble dataset","7f5efe58":"# make model","55439a84":"# import lib","70c90232":"# data generator","128d635f":"# train"}}