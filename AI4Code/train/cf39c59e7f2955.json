{"cell_type":{"b65b45e7":"code","8e28a5a4":"code","bdd2d05e":"code","ba4882a6":"code","d2e837f9":"code","94083b7a":"code","1147b9b7":"code","00201d96":"code","9fbbda71":"code","ac2dea22":"code","86a2ebe2":"code","2e568b92":"code","ef01ade1":"code","78e58d53":"code","610e1017":"code","2734e1b5":"code","e5fbfe8b":"code","a7530045":"markdown"},"source":{"b65b45e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib as plty\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sklearn\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport plotly.graph_objs as go\nfrom sklearn import preprocessing\nEncode = preprocessing.LabelEncoder()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfid = TfidfVectorizer()\nvect = CountVectorizer()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.manifold import TSNE\nNB = MultinomialNB()\n\nimport nltk\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words(\"english\")\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","8e28a5a4":"bot = pd.read_csv(\"..\/input\/deepnlp\/Sheet_1.csv\",usecols=['response_id','class','response_text'],encoding='latin-1')\nresume = pd.read_csv(\"..\/input\/deepnlp\/Sheet_2.csv\",encoding='latin-1')","bdd2d05e":"bot.head()","ba4882a6":"resume.head()","d2e837f9":"def cloud(text):\n    wordcloud = WordCloud(background_color=\"red\",stopwords=stop).generate(\" \".join([i for i in text.str.upper()]))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"Bot Response\")\ncloud(bot['response_text'])","94083b7a":"import pandas as pd\n# df = pd.read_csv(\"\")\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer #nltk k\u00fct\u00fcphanesinde haz\u0131r bir sentiment Intensity Analizi var\nimport nltk\nimport re\nfrom textblob import TextBlob # metnin olumlu-olumsuz durumuna g\u00f6re size 0-1 aral\u0131\u011f\u0131nda bir de\u011fer d\u00f6nmektedir. TextBlob ile amac\u0131m\u0131z yaz\u0131n\u0131n olumlu mu olumsuz mu i\u00e7erik i\u00e7erdi\u011fini anlamakt\u0131r.\nnltk.downloader.download('vader_lexicon')\n","1147b9b7":"bot['label'] = Encode.fit_transform(bot['class'])\n","00201d96":"x = bot.response_text\ny = bot.label\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=34)\nx_train_dtm = vect.fit_transform(x_train)\nx_test_dtm = vect.transform(x_test)\nNB.fit(x_train_dtm,y_train)\ny_predict = NB.predict(x_test_dtm)\nmetrics.accuracy_score(y_test,y_predict)","9fbbda71":"rf = RandomForestClassifier(max_depth=10,max_features=10)\nrf.fit(x_train_dtm,y_train)\nrf_predict = rf.predict(x_test_dtm)\nmetrics.accuracy_score(y_test,rf_predict)\n\n","ac2dea22":"x_test_dtm","86a2ebe2":"text = bot[\"response_text\"]\n\n\nTf_idf = CountVectorizer(max_features=256).fit_transform(text.values)\n","2e568b92":"tsne = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=34,\n    method='barnes_hut',\n    n_iter=250,\n    verbose=2,\n    angle=0.5\n).fit_transform(Tf_idf.toarray())","ef01ade1":"bot.replace(('flagged','not_flagged'),(1,0),inplace=True)\nbot.head()","78e58d53":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsentences=bot['label'].tolist()\nlabels=bot['class'].tolist()","610e1017":"training_size = int(len(sentences) * 0.8)\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]\n\n# Make labels into numpy arrays for use with the network later\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","2734e1b5":"vocab_size = 300\nembedding_dim = 16\nmax_length = 25\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_sentences = [str (item) for item in training_sentences]\ntesting_sentences = [str (item) for item in testing_sentences]\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","e5fbfe8b":"num_epochs = 30\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))","a7530045":"With RandomForest, we increased with almost %10.."}}