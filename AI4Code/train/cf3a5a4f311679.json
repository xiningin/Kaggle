{"cell_type":{"b4d143c9":"code","02f81283":"code","26958fa4":"code","0380d4bb":"code","82f168e5":"code","b334c838":"code","9b996e5c":"code","f210d901":"code","11be8ad1":"code","e842d71d":"code","7f6034e4":"code","51d055ce":"code","3b87335c":"code","f83a90d1":"code","ce8ab799":"code","7c67beda":"code","c3fd9e0b":"code","34ec80ab":"code","7b1322cd":"code","dd6b0ec0":"code","5ebdd917":"code","6c44268b":"code","5acd747b":"code","22a620a4":"code","841556bf":"code","c372a629":"code","789846fb":"code","99df403e":"code","d1569001":"code","e6fb46ff":"code","c7ef84d3":"code","882792ec":"markdown","84872ba8":"markdown","e5cbccab":"markdown","dbe7f1c2":"markdown","e8311d5a":"markdown","3705ca17":"markdown","8ccbf4a0":"markdown","264ecb2e":"markdown","685cf6bf":"markdown","8b78fb2d":"markdown","49285580":"markdown","342b9024":"markdown","d894b729":"markdown","6d8ed204":"markdown","7e3a4ea3":"markdown","c3359e31":"markdown","27a9c45b":"markdown","d49bb53c":"markdown","71d644e9":"markdown","cf4b0360":"markdown","77e32f0a":"markdown"},"source":{"b4d143c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02f81283":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","26958fa4":"# import datasets\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","0380d4bb":"train_df.describe()","82f168e5":"test_df.describe()","b334c838":"missing_train_df = pd.DataFrame(train_df.isna().sum(axis=0))\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['count_percent'] = missing_train_df['count']\/train_df.shape[0]\n\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['count_percent'] = missing_test_df['count']\/test_df.shape[0]","9b996e5c":"missing_train_row = train_df.drop(['id', 'claim'], axis=1).isna().sum(axis=1)\nmissing_train_feature_numbers = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_feature_numbers.columns = ['no_of_feature', 'count_percent']\n\nmissing_test_row = test_df.drop(['id'], axis=1).isna().sum(axis=1)\nmissing_test_feature_numbers = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_feature_numbers.columns = ['no_of_feature', 'count_percent']","f210d901":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","11be8ad1":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_feature_numbers['no_of_feature'], x=missing_train_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","e842d71d":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","7f6034e4":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_feature_numbers['no_of_feature'], x=missing_test_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","51d055ce":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","3b87335c":"train_df['num_nulls'].corr(train_df['claim'])","f83a90d1":"train_df.claim.value_counts()","ce8ab799":"# import packages\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer, RobustScaler\nfrom sklearn.impute import SimpleImputer","7c67beda":"%%time\n\nfeatures = [col for col in train_df.columns if col not in ['claim', 'id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\ntrain_df[features] = pipe.fit_transform(train_df[features])\ntest_df[features] = pipe.transform(test_df[features])","c3fd9e0b":"N_SPLITS = 5\nEARLY_STOPPING_ROUNDS = 200\nSEED = 42\nTRAINING_METHODS = {\n    'XGB' : True,\n    'LGDM' : True,\n    'CAT' : True\n}","34ec80ab":"def cross_validate(\n    model,\n    train_df,\n    test_df,\n    early_stopping='True'\n):\n    train_oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n    for fold, (train_idx, valid_idx) in tqdm(enumerate(skf.split(train_df.drop(['claim', 'id'],axis=1), train_df['claim']))):\n        X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n        y_train = X_train['claim']\n        y_valid = X_valid['claim']\n        X_train = X_train.drop(['claim', 'id'], axis=1)\n        X_valid = X_valid.drop(['claim', 'id'], axis=1)\n        \n        if early_stopping:\n            model.fit(\n                X_train, \n                y_train,\n                eval_set=[(X_valid, y_valid)],\n                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                verbose=0\n            )\n        else:\n            model.fit(\n                X_train, \n                y_train\n            )\n         \n        temp_oof = model.predict_proba(X_valid)[:, 1]\n        train_oof[valid_idx] = temp_oof\n        print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n        predictions += model.predict_proba(test_df)[:, -1] \/ N_SPLITS\n\n    print(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))\n    \n    return train_oof, predictions, model    ","7b1322cd":"test_df = test_df.drop('id', axis=1)","dd6b0ec0":"xgb_params = {\n        'n_estimators': 16939, \n        'learning_rate': 0.1876042995729744, \n        'subsample': 0.9947704250490819, \n        'colsample_bytree': 0.714913373260802, \n        'max_depth': 1, \n        'min_child_weight': 300, \n        'reg_lambda': 2.520228860596293e-05, \n        'reg_alpha': 0.00045044167069949973,\n        'tree_method': 'gpu_hist'\n}","5ebdd917":"%%time\nif TRAINING_METHODS['XGB']:\n    xg_train_oof, xg_predictions, model = cross_validate(\n                                            XGBClassifier(**xgb_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    model.save_model('xgb_model')\n    np.save('xg_train_oof', xg_train_oof)\n    np.save('xg_predictions', xg_predictions)","6c44268b":"lgb_params = {\n            'n_estimators': 12000, \n            'learning_rate': 0.027934730713420564, \n            'reg_alpha': 1.1799328678792862e-05, \n            'reg_lambda': 0.38585046073832296, \n            'num_leaves': 23, \n            'feature_fraction': 0.5301717514985537, \n            'bagging_fraction': 0.7745063435612487, \n            'bagging_freq': 6, \n            'min_child_samples': 19, \n            'min_child_weight': 193, \n            'colsample_bytree': 0.5145963018815463,\n            \"objective\": \"binary\",\n            \"metric\": \"binary_logloss\",\n            \"boosting_type\": \"gbdt\",\n            \"device_type\" : \"gpu\"\n}","5acd747b":"%%time\nif TRAINING_METHODS['LGDM']:\n    lgd_train_oof, lgd_predictions, model = cross_validate(\n                                            LGBMClassifier(**lgb_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    np.save('lgd_train_oof', lgd_train_oof)\n    np.save('lgd_predictions', lgd_predictions)\n    model.booster_.save_model('lgdm_model', num_iteration=model.best_iteration_)","22a620a4":"cat_params = {\n        'iterations': 12000, \n        'objective': 'Logloss', \n        'bootstrap_type': 'Bayesian', \n        'od_wait': 1491, \n        'learning_rate': 0.07733510576652604, \n        'reg_lambda': 6.067283648607877, \n        'random_strength': 19.03761597798964, \n        'depth': 4, \n        'min_data_in_leaf': 17, \n        'leaf_estimation_iterations': 8, \n        'bagging_temperature': 0.7761781866167776,\n        'task_type' : 'GPU'\n}","841556bf":"%%time\nif TRAINING_METHODS['LGDM']:\n    cat_train_oof, cat_predictions, model = cross_validate(\n                                            CatBoostClassifier(**cat_params),\n                                        train_df,\n                                        test_df,\n                                    )\n    np.save('cat_train_oof', cat_train_oof)\n    np.save('cat_predictions', cat_predictions)\n    model.save_model('cat_boost')\n","c372a629":"cols = [\"lgb\", \"xgb\", \"cat\"]\ndf_oof = pd.DataFrame(\n    dict(\n        zip(cols, [lgd_train_oof, xg_train_oof, cat_train_oof])\n    )\n)\ndf_pred = pd.DataFrame(\n    dict(\n        zip(cols, [lgd_predictions, xg_predictions, cat_predictions])\n    )\n)","789846fb":"df_oof['claim'] = train_df['claim']\ndf_oof['id'] = df_oof.index","99df403e":"# params = {\"objective\": \"binary\", \"metric\": \"binary_logloss\", \"random_state\": SEED, \"device_type\" : \"gpu\", 'verbose':0, \"n_estimators\" : 2000} \n# oof_lgb2, pred_lgb2, _ = cross_validate(\n#     LGBMClassifier(**params),\n#     df_oof,\n#     df_pred\n# )\n\n# params = {\"objective\": \"binary:logistic\", \"random_state\": SEED, 'tree_method': 'gpu_hist', 'verbose':0, 'n_estimators': 2000}\n# oof_xgb2, pred_xgb2, _ = cross_validate(\n#     XGBClassifier(**params),\n#     df_oof,\n#     df_pred\n# )\n\n# params = {\"random_state\": SEED, 'task_type': 'GPU', 'verbose':0, 'iterations': 2000}\n# oof_cat2, pred_cat2, _ = cross_validate(\n#     CatBoostClassifier(**params),\n#     df_oof, \n#     df_pred\n# )\n\nparams = {\"random_state\": SEED, 'n_jobs': -1 , 'C':1000, 'max_iter':1000}\noof_log2, pred_log2, _ = cross_validate(\n    LogisticRegression(**params), \n    df_oof,\n    df_pred,\n    early_stopping=False\n)\n","d1569001":"# ensemble_predictions = np.array([pred_lgb2, pred_xgb2, pred_cat2, pred_log2]).mean(axis=0)","e6fb46ff":"# ensemble_predictions","c7ef84d3":"submission['claim'] = pred_log2.tolist()\nsubmission.to_csv('submission.csv', index=False)","882792ec":"# Lets stack them for Ensemble modelling","84872ba8":"# Lets Begin the training","e5cbccab":"Lets check same for test","dbe7f1c2":"Damn!! thats a large correlation . Need to keep this factor. ","e8311d5a":"# Lets Define the training code","3705ca17":"Create a dataframe for final ensemble input","8ccbf4a0":"# Lets see the visual tables quickly ","264ecb2e":"# XGBOOST","685cf6bf":"# So now the work remaining is the removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","8b78fb2d":"Hi,\\\n**Welcome to my Notebook !!**.\\\nThe following notebook deals with basic EDA and model training.\n\n**Please leave an upvote or feedback, if you like or use at part of it. \\\nWould encourage me more to share.**\n\nI have obtained the optimised params with optuna using from the following notebook:-\\\nhttps:\/\/www.kaggle.com\/skiller\/best-params-detection-optuna \n\n\\\nBonus Notebooks\\\nIf you have interest in **NLP** do checkout my notebook on NER and text classification :-\n* https:\/\/www.kaggle.com\/skiller\/yes-torch-bert-finetuning-top-5\n* https:\/\/www.kaggle.com\/skiller\/ner-pytorch-masked-accuracy-and-loss\n\n\nWarm Regards","49285580":"Lets see why people are obsessed with the null counts","342b9024":"# Lets see data","d894b729":"# Load the Data","6d8ed204":"Well can't throw the null data more than 37% ","7e3a4ea3":"# Train the Multiple ensemble model","c3359e31":"# CATBOOST","27a9c45b":"# A lot of missing values. Lets see the correlation","d49bb53c":"# Lets also look if we have imbalance case","71d644e9":"# LGDM CLASSIFIER","cf4b0360":"Good to go. No Imbalance Class","77e32f0a":"See all less than 2%"}}