{"cell_type":{"9a54db04":"code","099e5d25":"code","7daf34f2":"code","92920655":"code","a8febda0":"code","94da24ab":"code","b4df5f16":"code","22126d8f":"code","61ef9ef7":"code","f29284cb":"code","61f20cd2":"code","428040bb":"code","86682c3a":"code","a59094db":"code","e2df72b2":"code","f48e15dc":"code","1e532d61":"code","de01f36d":"code","f2b646a2":"code","fcc6f4be":"code","34765d41":"code","76958545":"code","0e0cf709":"code","e0378275":"code","780fadfc":"code","01485a6c":"code","0e75da07":"code","53f1505a":"code","90be2f99":"code","64aa2e1b":"code","bace8f6f":"code","00821224":"code","e60f889e":"code","56da0852":"code","34cba10d":"code","813a4088":"code","08d0aa23":"code","681ea5ea":"code","c8a508f8":"code","c8595c39":"code","76a6679f":"code","95bc3e07":"code","c0afc323":"code","0fd8aede":"code","96342e5a":"code","566c85a8":"code","908378ab":"code","1d12871b":"code","cb66302d":"code","5bd2a2e5":"code","dc306452":"code","08d796c0":"code","162c3278":"code","3c23582b":"code","ce443644":"code","c37cb2e5":"code","7ca52c90":"code","98b94e74":"code","7aac699c":"code","c7904852":"code","42284fff":"code","b5af1f80":"code","32298bcf":"code","7c6146de":"code","5ea8126d":"code","f3790c3f":"code","749eb134":"code","7d470abe":"code","779741a5":"code","d5cfc49b":"code","4e76d725":"code","13b4fcb7":"code","01ad9748":"code","2b3279c6":"code","ee0ec1dd":"code","acacf323":"code","29fda957":"code","bb3e3e4a":"code","632c0ce9":"code","f8e198c4":"code","f8d38dd8":"code","889c8bf5":"code","bb405da5":"code","ca95c174":"code","1126d400":"code","639c544e":"code","49f8a654":"code","0eed397d":"code","f277aae0":"code","ef8e3d19":"code","1a169278":"code","f2956e7b":"code","b1a7077b":"code","7e4a84fd":"code","317971ad":"code","8e21486e":"code","506d4fd6":"code","35fc7a5c":"code","10b07d70":"code","23619151":"code","414d5b97":"code","f8624a96":"code","456af76a":"code","8a99c1a5":"code","b5100422":"code","b95953c0":"code","65fd1060":"code","9352b615":"code","020f6af5":"code","b93bc986":"code","fd39fe20":"code","c74ac8be":"code","0a639a5b":"code","57d9801e":"code","81d6e84c":"code","57f78d08":"code","e9084637":"code","5599ba26":"code","b89583e7":"code","f1fb17d8":"code","bfc6d32f":"code","a9d1297b":"code","4528ed11":"code","960889af":"code","ffce1015":"code","6518a926":"code","5c004e64":"code","75aaa9a4":"code","5ed030e1":"code","1ffdc22a":"code","2660355a":"code","4b1e4d0a":"markdown","da64d4be":"markdown","33a2fc77":"markdown","8f83c585":"markdown","25260ce2":"markdown","d586b2da":"markdown","26fe4a86":"markdown","d625a048":"markdown","3927cc5e":"markdown","029f1a9c":"markdown","8ff5dfca":"markdown","acb7f6df":"markdown","4ab40ecd":"markdown","9e44e843":"markdown","6c87c0c0":"markdown","26118a13":"markdown","bf3beb31":"markdown","5ffe0699":"markdown","1217283f":"markdown","f9f888e5":"markdown","b8d513d0":"markdown","f0467052":"markdown","be53a065":"markdown","bbdc0e26":"markdown","7aab921c":"markdown","dc8fd5e4":"markdown","8d7a9441":"markdown","77dc6483":"markdown","f617697d":"markdown","9eaa6970":"markdown","7455bc1d":"markdown","c7f3b8ac":"markdown","e9b13c93":"markdown","a4d3e2f0":"markdown","63beb707":"markdown","067de1d7":"markdown","b4511a60":"markdown","f41b38f4":"markdown"},"source":{"9a54db04":"print('Hello World')","099e5d25":"# data manipulation\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data visualization\nimport seaborn as sns\n\n# handling missing values\nimport missingno\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\n# machine learning regression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# machine learning classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# evaluating ML models\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV","7daf34f2":"# setup figure sizes\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 5]","92920655":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col=[0])\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col=[0])","a8febda0":"df_train.head()","94da24ab":"df_test.head()","b4df5f16":"df_train.plot(kind='hist', y='Survived')","22126d8f":"df_train.loc[df_train['Survived'] == 1].plot(kind='kde', y='Age')","61ef9ef7":"df_train.loc[df_train['Survived'] == 0].plot(kind='kde', y='Age')","f29284cb":"help(sns.barplot)","61f20cd2":"sns.distplot(df_train['Se', y='Sex')","428040bb":"sns.barplot(data=df_train, y='Sex', hue='Survival')","86682c3a":"df_train.columns","a59094db":"df_train.info()","e2df72b2":"df_train.info()","f48e15dc":"# determine which columns should be categorical\ncols_cat = []\nfor col in df_train: \n    num_unique_vals = df_train[col].unique().size\n    if num_unique_vals < 15:\n        print(col, df_train[col].dtype, num_unique_vals, df_train[col].unique()) \n        cols_cat.append(col)\n    else:\n        print(col, df_train[col].dtype, num_unique_vals)\n        ","1e532d61":"cols_cat","de01f36d":"cols_cat.remove('Survived')","f2b646a2":"cols_cat","fcc6f4be":"# convert features on train and test set to category\ndf_train[cols_cat] = df_train[cols_cat].astype('category')\ndf_test[cols_cat] = df_test[cols_cat].astype('category')","34765d41":"df_train.info()","76958545":"df_test.info()","0e0cf709":"df_train.select_dtypes('object')","e0378275":"df_train.loc[df_train.Cabin.isnull()]","780fadfc":"df_train.select_dtypes('object')","01485a6c":"df_train.Cabin","0e75da07":"df_train.Cabin.str.replace('[^a-zA-Z]', '').value_counts()","53f1505a":"df_train.Cabin.str.replace('[^a-zA-Z]', '').apply(lambda x: ', '.join(set(x)) if not pd.isnull(x) else np.nan).value_counts()\n# I can handle this using one-hot encoding... if sample contains F or G or E... set column value to 1 else 0.","90be2f99":"df_train['cabin_letter'] = df_train.Cabin.str.replace('[^a-zA-Z]', '').apply(lambda x: ', '.join(set(x)) if not pd.isnull(x) else np.nan)\ndf_train['cabin_letter'] = df_train['cabin_letter'].astype('category')","64aa2e1b":"df_train","bace8f6f":"df_test['cabin_letter'] = df_test.Cabin.str.replace('[^a-zA-Z]', '').apply(lambda x: ', '.join(set(x)) if not pd.isnull(x) else np.nan)\ndf_test['cabin_letter'] = df_test['cabin_letter'].astype('category')","00821224":"df_train.info()","e60f889e":"df_test.info()","56da0852":"sns.distplot(df_train['Age'])","34cba10d":"df_train['Age'].mean()\n","813a4088":"df_train['Age'].median()","08d0aa23":"df_train.info()","681ea5ea":"df_train.info()","c8a508f8":"df_train.select_dtypes(['category', np.number]).info()","c8595c39":"df_train_1hot = pd.get_dummies(df_train.select_dtypes(['category', np.number]))","76a6679f":"df_test_1hot = pd.get_dummies(df_test.select_dtypes(['category', np.number]))","95bc3e07":"pd.get_dummies(df_train)","c0afc323":"df_train_1hot.head()","0fd8aede":"df_test_1hot.head()","96342e5a":"# find missing columns\nset(df_train_1hot.columns) ^ set(df_test_1hot.columns)","566c85a8":"# make sure columns align\ndf_train_1hot, df_test_1hot = df_train_1hot.align(df_test_1hot,\n                                                  join='left', \n                                                  axis=1)","908378ab":"# check if columns of train and test are the same\ndf_train_1hot.columns == df_test_1hot.columns","1d12871b":"df_train_1hot.head()","cb66302d":"df_test_1hot.head()","5bd2a2e5":"df_test_1hot['cabin_letter_T'] = 0","dc306452":"df_train_1hot['Age']","08d796c0":"# split data into train and test\nidx_missing = df_train_1hot['Age'].isnull()","162c3278":"# split data into train and missing\nX_train = df_train_1hot.drop('Age', axis=1).loc[~idx_missing]\nX_missing = df_train_1hot.drop('Age', axis=1).loc[idx_missing]\ny_train = df_train_1hot['Age'].loc[~idx_missing]\nprint(X_train.shape)\nprint(X_missing.shape)\nprint(y_train.shape)\n","3c23582b":"X_train.info()","ce443644":"?RandomForestRegressor","c37cb2e5":"# train classifier\nreg = RandomForestRegressor(random_state=42)\nreg.fit(X_train, y_train)","7ca52c90":"y_pred = reg.predict(X_missing)","98b94e74":"y_pred","7aac699c":"y_pred = y_pred.round()","c7904852":"# compare ages to predictions\nsns.distplot(df_train_1hot['Age'].loc[~idx_missing])","42284fff":"sns.distplot(y_pred)","b5af1f80":"df_train_1hot['Age'].loc[idx_missing] = y_pred","32298bcf":"df_train_1hot","7c6146de":"df_test_1hot.loc[df_test_1hot['Fare'].isnull()]","5ea8126d":"df_test_1hot.loc[df_test_1hot['Fare'].isnull()] = df_test_1hot['Fare'].mean()","f3790c3f":"# fill in age in df_test\nidx_missing = df_test_1hot['Age'].isnull()\nX_train = df_test_1hot.drop(['Survived', 'Age'], axis=1).loc[~idx_missing]\nX_missing = df_test_1hot.drop(['Survived', 'Age'], axis=1).loc[idx_missing]\ny_train = df_test_1hot['Age'].loc[~idx_missing]\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_missing).round()\ndf_test_1hot['Age'].loc[idx_missing] = y_pred","749eb134":"df_train_1hot.info()","7d470abe":"df_test_1hot.info()","779741a5":"X = df_train_1hot.drop('Survived', axis=1).values\nX_test = df_test_1hot.drop('Survived', axis=1).values\ny = df_train_1hot['Survived'].values","d5cfc49b":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","4e76d725":"clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_val)","13b4fcb7":"accuracy_score(y_val, y_pred)","01ad9748":"print(classification_report(y_val, y_pred))","2b3279c6":"clf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_val)","ee0ec1dd":"accuracy_score(y_val, y_pred)","acacf323":"print(classification_report(y_val, y_pred))","29fda957":"# Create a StratifiedKFold object\nstr_kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Loop through each split\nfold = 0\nfor idx_train, idx_val in str_kf.split(X, y):\n    # Obtain training and testing folds\n    X_train, X_val = X[idx_train], X[idx_val]\n    y_train, y_val = y[idx_train], y[idx_val]\n    print('Fold: {}'.format(fold))\n    print('CV train shape: {}'.format(X_train.shape))\n    print('Number of survivors: {}\\n'.format(sum(y_train == 1)))\n    fold += 1","bb3e3e4a":"df_train['Survived'].value_counts()","632c0ce9":"# Loop through each split\nfold = 0\nlist_accuracy = []\nfor idx_train, idx_val in str_kf.split(X, y):\n    # Obtain training and testing folds\n    X_train, X_val = X[idx_train], X[idx_val]\n    y_train, y_val = y[idx_train], y[idx_val]\n    print('Fold: {}'.format(fold))\n    print('CV train shape: {}'.format(X_train.shape))\n    print('Number of survivors: {}\\n'.format(sum(y_train == 1)))\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_val)\n    list_accuracy.append(accuracy_score(y_val, y_pred))\n    print(classification_report(y_val, y_pred))\n    fold += 1","f8e198c4":"list_accuracy","f8d38dd8":"cross_val_score(clf, X, y, cv=str_kf, scoring='accuracy')","889c8bf5":"from sklearn.gaussian_process import GaussianProcessClassifier\nclf = GaussianProcessClassifier()\ncross_val_score(clf, X, y, cv=str_kf, scoring='accuracy')","bb405da5":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\ncross_val_score(clf, X, y, cv=str_kf, scoring='accuracy')","ca95c174":"from sklearn.svm import SVC\nclf = SVC(kernel='linear')\ncross_val_score(clf, X, y, cv=str_kf, scoring='accuracy')","1126d400":"import xgboost as xgb\nparams = {'objective': 'reg:linear',\n          'max_depth': 5,\n          'silent': 1}\n# Loop through each split\nfold = 0\nlist_accuracy = []\nfor idx_train, idx_val in str_kf.split(X, y):\n    # Obtain training and testing folds\n    X_train, X_val = X[idx_train], X[idx_val]\n    y_train, y_val = y[idx_train], y[idx_val]\n    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n    dval = xgb.DMatrix(data=X_val)\n    print('Fold: {}'.format(fold))\n    print('CV train shape: {}'.format(X_train.shape))\n    print('Number of survivors: {}\\n'.format(sum(y_train == 1)))\n    clf = xgb.train(params=params, dtrain=dtrain)\n    y_pred = clf.predict(dval).round()\n    list_accuracy.append(accuracy_score(y_val, y_pred))\n    print(classification_report(y_val, y_pred))\n    fold += 1","639c544e":"list_accuracy","49f8a654":"# predict on test data\nclf = SVC(kernel='linear')\nclf.fit(X, y)\ny_pred = clf.predict(X_test)","0eed397d":"df_pred = pd.DataFrame(y_pred, index=df_test.index, columns=['Survived'])","f277aae0":"df_pred","ef8e3d19":"df_pred.to_csv('predictions_SVC.csv')","1a169278":"help(LogisticRegression)","f2956e7b":"help(GridSearchCV)","b1a7077b":"param_grid = {\n    'solver': ['newton-cg', 'liblinear'],\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'C': [0.5, 1, 10],\n    'random_state': [42],\n    'max_iter': [500, 1000, 4000]\n}\ngrid_search = GridSearchCV(LogisticRegression(), param_grid)","7e4a84fd":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)","317971ad":"grid_search.fit(X_train, y_train)","8e21486e":"grid_search.best_params_","506d4fd6":"cross_val_score(LogisticRegression(max_iter=300), X, y, cv=str_kf, scoring='accuracy')","35fc7a5c":"clf = LogisticRegression(**grid_search.best_params_)\ncross_val_score(clf, X, y, cv=str_kf, scoring='accuracy')","10b07d70":"clf.fit(X, y)\ny_pred = clf.predict(X_test)","23619151":"df_pred = pd.DataFrame(y_pred, index=df_test.index, columns=['Survived'])","414d5b97":"df_pred","f8624a96":"df_pred.to_csv('predictions_hyperparameter_tuning_grid_search.csv')","456af76a":"df_train","8a99c1a5":"sns.countplot(data=df_train, x='Sex', hue='Survived')","b5100422":"sns.countplot(data=df_train, x='Pclass', hue='Survived')","b95953c0":"cols_cabin = [i for i in df_train_1hot.columns if i.startswith('cabin_letter')]","65fd1060":"df_train_1hot = df_train_1hot.loc[df_train.cabin_letter.isnull()].drop(cols_cabin, axis=1)\ndf_test_1hot = df_test_1hot.loc[df_test.cabin_letter.isnull()].drop(cols_cabin, axis=1)","9352b615":"# add cabin_letter back into data\ndf_train_1hot['cabin_letter'] = df_train['cabin_letter']","020f6af5":"# predict cabin letter\nX_train = df_train_1hot.loc[df_train['cabin_letter']]\nclf = RandomForestClassifier()\nclf.fit","b93bc986":"df_train.shape","fd39fe20":"df_train_1hot.shape","c74ac8be":"df_train['cabin_letter'].value_counts()","0a639a5b":"# split data into train and test\nidx_missing = df_train_1hot['cabin_letter'].isnull()","57d9801e":"# split data into train and missing\nX_train = df_train_1hot.drop('cabin_letter', axis=1).loc[~idx_missing]\nX_missing = df_train_1hot.drop('cabin_letter', axis=1).loc[idx_missing]\ny_train = df_train_1hot['cabin_letter'].loc[~idx_missing]\nprint(X_train.shape)\nprint(X_missing.shape)\nprint(y_train.shape)\n","81d6e84c":"X_train.info()","57f78d08":"# train classifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)","e9084637":"y_pred = reg.predict(X_missing)","5599ba26":"y_pred.round()","b89583e7":"# compare ages to predictions\nsns.distplot(df_train_1hot['Age'].loc[~idx_missing])","f1fb17d8":"sns.distplot(y_pred)","bfc6d32f":"df_train_1hot['Age'].loc[idx_missing] = y_pred","a9d1297b":"df_train_1hot.info()","4528ed11":"ticket_num = pd.Series([i[-1] for i in df_train.Ticket.str.split()], index=df_train.index, name='Ticket number')","960889af":"ticket_num","ffce1015":"df_train.loc[~ticket_num.str.isnumeric()]","6518a926":"ticket_num.loc[~ticket_num.str.isnumeric()] ","5c004e64":"ticket_num.loc[~ticket_num.str.isnumeric()] = np.NaN","75aaa9a4":"ticket_num = ticket_num.astype(float)","5ed030e1":"sns.distplot(ticket_num)","1ffdc22a":"df_train['ticket_number'] = ticket_num","2660355a":"# do the same for df_test\nticket_num_test = pd.Series([i[-1] for i in df_test.Ticket.str.split()], index=df_test.index, name='Ticket number')\nticket_num_test.loc[~ticket_num_test.str.isnumeric()] = np.NaN\nticket_num_test = ticket_num_test.astype(float)\ndf_test['ticket_number'] = ticket_num_test","4b1e4d0a":"### SVC","da64d4be":"**What might influence survival?**\n- Having parents or children\n- Which level of the boat you are in\n- How close you are to the impact with iceberg\n- How close you are to flotation devices\/boats","33a2fc77":"# EDA","8f83c585":"![decks](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/Titanic_cutaway_diagram.png\/440px-Titanic_cutaway_diagram.png)","25260ce2":"### Nearest neighbors\n- \"Curse of dimensionality\"","d586b2da":"# Categorizing and handling missing data on train and test sets\n\nThe purpose when addressing missing data is to correctly reproduce the variance\/covariance matrix we would have observed had our data not had any missing information.\n\nThere are three categories of missing data:\n1. Missing completely at random (MCAR): the reason for missingness has nothing to do with other variables.\n    - We can impute these values with test statistics.\n    - We can predict these values.\n2. Missing at random (MAR): the reason for missingness has to do with one or more variables\n3. Missing not at random (MNAR)\n![handling missing data](https:\/\/miro.medium.com\/max\/1400\/1*_RA3mCS30Pr0vUxbp25Yxw.png)\nUseful links:\n- [Sklearn docs](https:\/\/scikit-learn.org\/stable\/modules\/impute.html)\n- [Blog post](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4)\n- [Book: Applied missing data analysis](http:\/\/hsta559s12.pbworks.com\/w\/file\/fetch\/52112520\/enders.applied)","26fe4a86":"### XGBoost","d625a048":"Random forest --> non-linear","3927cc5e":"## Questions\n- Age\n- Sex\n- Cabin\n- Pclass","029f1a9c":"### Random forest classifier","8ff5dfca":"### Cabin","acb7f6df":"# Improving our score\n- More feature engineering.\n    - Extracting information from Name and Ticket variables.\n    - Use a \"family\" category.\n- Different ways to fill in missing values.\n    - Flag missing cabins as \"Missing\" value.\n    - Target encoding.\n- Hyperparameter tuning.\n- Ensemble models.","4ab40ecd":"## Ticket number","9e44e843":"# Predicting survival with a baseline model\nSklearn has many classifiers: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html","6c87c0c0":"### Other classifiers...","26118a13":"### Observations\n**Missing values**\nAge and Cabin have many missing values. Embarked has a couple of missing values... why?\n\n**Which variables should be categorical?**\n- Survived is a yes or no. \n- Pclass contains three classes. \n- Name is text... can info be extracted from it?\n- Sex has two. \n- SibSp has six... what is SibSp? \n- Parch has seven values, what is it?\n- Ticket is text... can info be extracted from it?\n- Cabin is text... can info be extracted from it?\n- Embarked has three values\nSo, categorical variables are: Pclass, Sex, SibSp, Parch, and Embarked.\n\n**Information in names**\n- Master means boy\/baby boy\n\n","bf3beb31":"## Handling missing data\n- Three different kinds of missing data.\n- Strategies for handling missing data are:\n    - Complete-case analysis.\n    - Fill missing values with mean, median, mode, or another constant value \"single imputation\".\n    - Stochastic imputation.\n    - Machine learning to predict missing values.\n    - Forward\/back fill if time series","5ffe0699":"# Load data","1217283f":"# Workflow:\n1. Load data.\n2. Exploratory data analysis.\n3. Clean data.\n    1. Check if data types are correct.\n    2. Feature engineering.\n    3. One-hot encoding of categorical features.\n4. Train\/test and cross validation.\n5. Training and predicting ML models.\n6. Submitting predictions.\n7. Further improvements.\n8. Hyperparameter tuning.\n","f9f888e5":"<div class=\"alert alert-info\">\n    <h1>Before creating a model, we need to transform categorical values and handle missing values.<\/h1>\n    <p>What should we do with cabin_letter?<\/p>\n<\/div>","b8d513d0":"### GaussianProcessClassifier","f0467052":"### Ticket","be53a065":"## Go back and redo Cabin values\nWe never imputed values for cabin. Instead, we one-hot encoded our data, which set missing cabin_letter values to 0. Could we improve our predictions if we go back and imputed cabins after we replaced the missing values in the Age column?","bbdc0e26":"## One-hot encoding categorical variables\n\nUseful links\n- [Kaggle's guide](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding)","7aab921c":"# Goal:\n### Use a machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","dc8fd5e4":"# Other possible feature engineering steps","8d7a9441":"### Name","77dc6483":"# Submitting predictions","f617697d":"## _What do these letters mean?_","9eaa6970":"# Hyperparameter tuning","7455bc1d":"### Fill in missing values in Age column","c7f3b8ac":"# Clean data","e9b13c93":"# Some notes","a4d3e2f0":"## Feature engineering\n- Married?\n- Has family?\n- Big family?\n- No family?\n- Maiden name?\n- Number of Mrs.\n- Is dependent?\n- How to find family: same cabin, same last name, SibSp and Parch add up, same Ticket...","63beb707":"## What does each column mean?","067de1d7":"## Check data types","b4511a60":"## Cross validation\n![cross validation](https:\/\/ethen8181.github.io\/machine-learning\/model_selection\/img\/kfolds.png)","f41b38f4":"<div class=\"alert alert-info\">\n    <h1 class=\"text-left\">\n        What can we do with the remaining \"object\" features?\n    <\/h1>\n    <p class=\"lead\">\n        These text data are not in a format from which an algorithm can learn.\n    <\/p>\n<\/div>"}}