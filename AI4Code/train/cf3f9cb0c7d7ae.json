{"cell_type":{"b65685a0":"code","dd690aef":"code","f1b93e05":"code","e14d2016":"code","6484a767":"code","5cac17f6":"code","66f87871":"code","89ad5299":"code","e08b4fcd":"code","b84a259c":"code","7f7fbaa3":"code","86356847":"code","216a5eb4":"code","b99c69be":"code","6a064700":"code","45dac4af":"code","c4d52d11":"code","bb72a376":"code","82f9ef19":"code","d683942d":"code","413d1f07":"code","6dd78c75":"code","3f9e4ad0":"code","1bc37873":"code","f5abc886":"code","a0ac7ed4":"code","f8b3aac5":"code","6ca40728":"code","241e6670":"code","f8ec676e":"code","7e442be5":"code","01a50c99":"code","eb53b0ee":"code","819940e5":"code","2e3e3ea3":"code","933bea01":"code","9258891e":"code","16a06b42":"code","56757d8a":"code","e22f2ce5":"code","d53f5bbd":"code","afebf36c":"code","79fe91c8":"code","6754cb6e":"code","3019f25a":"code","59bf7f63":"code","7454f7b1":"code","2d9dca05":"code","4edd39e5":"code","a32c016d":"code","039e79c4":"code","bc3fe2a8":"code","90ed87f3":"code","890c7432":"code","c84f9b16":"code","8c94a1a4":"code","bb2048d7":"code","ca54bdf1":"code","d385027e":"code","291c8d19":"markdown","5b3018b8":"markdown","68097ae2":"markdown","3be31a39":"markdown","9fc456db":"markdown","e4261ef7":"markdown","e69f88b9":"markdown","e95127fa":"markdown","0ea4b0db":"markdown","2d694cde":"markdown","5eeeec85":"markdown","7180edb4":"markdown","9fbb5988":"markdown","d7ba1d2d":"markdown","4c54bfb4":"markdown"},"source":{"b65685a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","dd690aef":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression","f1b93e05":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')\n\ndf.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\ndf.totalcharges = df.totalcharges.fillna(0)\n\ndf.churn = (df.churn == 'yes').astype(int)","e14d2016":"df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.churn.values\ny_val = df_val.churn.values\ny_test = df_test.churn.values\n\ndel df_train['churn']\ndel df_val['churn']\ndel df_test['churn']","6484a767":"numerical = ['tenure', 'monthlycharges', 'totalcharges']\n\ncategorical = [\n    'gender',\n    'seniorcitizen',\n    'partner',\n    'dependents',\n    'phoneservice',\n    'multiplelines',\n    'internetservice',\n    'onlinesecurity',\n    'onlinebackup',\n    'deviceprotection',\n    'techsupport',\n    'streamingtv',\n    'streamingmovies',\n    'contract',\n    'paperlessbilling',\n    'paymentmethod',\n]","5cac17f6":"dv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train[categorical + numerical].to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","66f87871":"val_dict = df_val[categorical + numerical].to_dict(orient='records')\nX_val = dv.transform(val_dict)\n\ny_pred = model.predict_proba(X_val)[:, 1]\nchurn_decision = (y_pred >= 0.5)\n(y_val == churn_decision).mean()","89ad5299":"len(y_val)","e08b4fcd":"(y_val == churn_decision).mean()","b84a259c":"from sklearn.metrics import accuracy_score","7f7fbaa3":"accuracy_score(y_val, y_pred >= 0.5)","86356847":"# Check the accuracy with different threshold\nthresholds = np.linspace(0, 1, 21)\n\nscores = []\n\nfor t in thresholds:\n    score = accuracy_score(y_val, y_pred >= t)\n    print('%.2f %.3f' % (t, score))\n    scores.append(score)","216a5eb4":"# Plot these results\nplt.plot(thresholds, scores)","b99c69be":"from collections import Counter","6a064700":"Counter(y_pred >= 1.0)","45dac4af":"1 - y_val.mean()","c4d52d11":"y_val.mean()","bb72a376":"actual_positive = (y_val == 1)\nactual_negative = (y_val == 0)","82f9ef19":"t = 0.5\npredict_positive = (y_pred >= t)\npredict_negative = (y_pred < t)","d683942d":"tp = (predict_positive & actual_positive).sum()\ntn = (predict_negative & actual_negative).sum()\n\nfp = (predict_positive & actual_negative).sum()\nfn = (predict_negative & actual_positive).sum()","413d1f07":"confusion_matrix = np.array([\n    [tn, fp],\n    [fn, tp]\n])\nconfusion_matrix","6dd78c75":"# Confusion Matrix in percentage Form\n(confusion_matrix \/ confusion_matrix.sum()).round(2)","3f9e4ad0":"# Precision\np = tp \/ (tp + fp)\np","1bc37873":"# Recull\nr = tp \/ (tp + fn)\nr","f5abc886":"tp + fn","a0ac7ed4":"# TPR\ntpr = tp \/ (tp + fn)\ntpr","f8b3aac5":"# FPR\nfpr = fp \/ (fp + tn)\nfpr","6ca40728":"scores = []\n\nthresholds = np.linspace(0, 1, 101)\n\nfor t in thresholds:\n    actual_positive = (y_val == 1)\n    actual_negative = (y_val == 0)\n    \n    predict_positive = (y_pred >= t)\n    predict_negative = (y_pred < t)\n\n    tp = (predict_positive & actual_positive).sum()\n    tn = (predict_negative & actual_negative).sum()\n\n    fp = (predict_positive & actual_negative).sum()\n    fn = (predict_negative & actual_positive).sum()\n    \n    scores.append((t, tp, fp, fn, tn))","241e6670":"columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\ndf_scores = pd.DataFrame(scores, columns=columns)\n\ndf_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\ndf_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)","f8ec676e":"plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\nplt.legend()","7e442be5":"np.random.seed(1)\ny_rand = np.random.uniform(0, 1, size=len(y_val))\n((y_rand >= 0.5) == y_val).mean()","01a50c99":"def tpr_fpr_dataframe(y_val, y_pred):\n    scores = []\n\n    thresholds = np.linspace(0, 1, 101)\n\n    for t in thresholds:\n        actual_positive = (y_val == 1)\n        actual_negative = (y_val == 0)\n\n        predict_positive = (y_pred >= t)\n        predict_negative = (y_pred < t)\n\n        tp = (predict_positive & actual_positive).sum()\n        tn = (predict_negative & actual_negative).sum()\n\n        fp = (predict_positive & actual_negative).sum()\n        fn = (predict_negative & actual_positive).sum()\n\n        scores.append((t, tp, fp, fn, tn))\n\n    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n    df_scores = pd.DataFrame(scores, columns=columns)\n\n    df_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\n    df_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)\n    \n    return df_scores","eb53b0ee":"df_rand = tpr_fpr_dataframe(y_val, y_rand)\nplt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\nplt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\nplt.legend()","819940e5":"num_neg = (y_val == 0).sum()\nnum_pos = (y_val == 1).sum()\nnum_neg, num_pos","2e3e3ea3":"y_ideal = np.repeat([0, 1], [num_neg, num_pos])\ny_ideal_pred = np.linspace(0, 1, len(y_val))","933bea01":"1 - y_val.mean()","9258891e":"accuracy_score(y_ideal, y_ideal_pred >= 0.726)","16a06b42":"df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\ndf_ideal[::10]","56757d8a":"plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\nplt.legend()","e22f2ce5":"plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\n\nplt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\n\n# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\n# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\n\nplt.legend()","d53f5bbd":"plt.figure(figsize=(5, 5))\n\nplt.plot(df_scores.fpr, df_scores.tpr, label='Model')\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend()","afebf36c":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\nplt.figure(figsize=(5, 5))\n\nplt.plot(fpr, tpr, label='Model')\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend()","79fe91c8":"from sklearn.metrics import auc","6754cb6e":"auc(fpr, tpr) # Score from implementation\nauc(df_scores.fpr, df_scores.tpr) #score from sklearn model","3019f25a":"auc(df_ideal.fpr, df_ideal.tpr) # score from ideal model","59bf7f63":"fpr, tpr, thresholds = roc_curve(y_val, y_pred)\nauc(fpr, tpr) # Calculating auc from fpr and tpr generated from previous step","7454f7b1":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_val, y_pred) # Calculate area under curve directly","2d9dca05":"neg = y_pred[y_val == 0] # Score for negative exemples\npos = y_pred[y_val == 1] # Score for positive exemples","4edd39e5":"import random","a32c016d":"# Randomly selecting postive and negetive class and checking\n# If we get postive probability greater than negative probability we increment the score by 1\n\nn = 100000\nsuccess = 0 \n\nfor i in range(n):\n    pos_ind = random.randint(0, len(pos) - 1)\n    neg_ind = random.randint(0, len(neg) - 1)\n\n    if pos[pos_ind] > neg[neg_ind]:\n        success = success + 1\n\nsuccess \/ n","039e79c4":"n = 50000\n\nnp.random.seed(1)\npos_ind = np.random.randint(0, len(pos), size=n)\nneg_ind = np.random.randint(0, len(neg), size=n)\n\n(pos[pos_ind] > neg[neg_ind]).mean()","bc3fe2a8":"def train(df_train, y_train, C=1.0):\n    dicts = df_train[categorical + numerical].to_dict(orient='records')\n\n    dv = DictVectorizer(sparse=False)\n    X_train = dv.fit_transform(dicts)\n\n    model = LogisticRegression(C=C, max_iter=1000)\n    model.fit(X_train, y_train)\n    \n    return dv, model","90ed87f3":"dv, model = train(df_train, y_train, C=0.001)","890c7432":"def predict(df, dv, model):\n    dicts = df[categorical + numerical].to_dict(orient='records')\n\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:, 1]\n\n    return y_pred","c84f9b16":"y_pred = predict(df_val, dv, model)","8c94a1a4":"from sklearn.model_selection import KFold\nfrom tqdm.auto import tqdm","bb2048d7":"n_splits = 5\n\nfor C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    scores = []\n\n    for train_idx, val_idx in kfold.split(df_full_train):\n        df_train = df_full_train.iloc[train_idx]\n        df_val = df_full_train.iloc[val_idx]\n\n        y_train = df_train.churn.values\n        y_val = df_val.churn.values\n\n        dv, model = train(df_train, y_train, C=C)\n        y_pred = predict(df_val, dv, model)\n\n        auc = roc_auc_score(y_val, y_pred)\n        scores.append(auc)\n\n    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))","ca54bdf1":"scores","d385027e":"dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\ny_pred = predict(df_test, dv, model)\n\nauc = roc_auc_score(y_test, y_pred)\nauc","291c8d19":"## 8. ROC AUC\n* Area under the ROC curve - useful metric\n* Interpretation of AUC","5b3018b8":"### 7.2 Random model","68097ae2":"## 3. Data Preparation","3be31a39":"## 2. Importing Libraries","9fc456db":"## 9. Cross-Validation\n* Evaluating the same model on different subsets of data\n* Getting the average prediction and the spread within predictions","e4261ef7":"## 4. Accuracy and dummy model\n* Evaluate the model on different thresholds\n* Check the accuracy of dummy baselines","e69f88b9":"## Content in this Notebook\n## Notebook is a part of FREE ML course by Glexey Grigorev. [Link for the Course](https:\/\/github.com\/alexeygrigorev\/mlbookcamp-code\/tree\/master\/course-zoomcamp\/04-evaluation)\n## Evaluation Metrics for Classification\n","e95127fa":"## 5.  Confusion Table\n* Different types of errors and correct decisions\n* Arranging them in a table","0ea4b0db":"## 7. ROC Curves","2d694cde":"### We can see that when all the predictions are False and the accuracy of the model : 0.726 are not churning\n### and the actuel model : 0.273 are churning , this is because our data is not balanced.","5eeeec85":"## 6. Precision and Recall","7180edb4":"### 7.3 Ideal model","9fbb5988":"### 7.1 TPR and FRP","d7ba1d2d":"## 1. Introduction\n\n*Context*\n\n\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\" [IBM Sample Data Sets]\n\n*Content*\n\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\nThe data set includes information about:\n\n    - Customers who left within the last month \u2013 the column is called Churn\n    - Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n    - Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n    - Demographic info about customers \u2013 gender, age range, and if they have partners and dependents","4c54bfb4":"### 7.4 Putting everything together"}}