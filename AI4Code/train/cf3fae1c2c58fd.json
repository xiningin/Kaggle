{"cell_type":{"2339c92d":"code","8795c65b":"code","dabcb193":"code","7a613f66":"code","6b13083e":"code","bc399ab9":"code","44161f27":"code","d5fb50b7":"code","21f77c7b":"code","12b0b560":"code","06024b1b":"code","c168ef25":"code","1ccfc37b":"code","e889ea44":"code","4bcb07ff":"code","a23db9a4":"code","ac47c51a":"code","945aa2c7":"code","1cff3304":"code","5da7df89":"code","677eb1fa":"code","0dc94d25":"code","d6b4e6de":"code","d28f61ac":"code","34353f03":"code","37829b74":"code","cf2703eb":"code","52830bc5":"code","d248a9fe":"code","f01730cc":"code","d6cebc17":"code","395461c5":"code","b2c2ffb6":"code","9daede34":"code","6f2929ff":"code","78e9b815":"code","94ff6c71":"code","ac981c27":"code","9f2740dd":"code","92700d85":"code","e87efc66":"code","6476458c":"code","16407273":"code","85d4ed9a":"code","62477baf":"code","e70874e6":"code","abda8af0":"code","dc4711ed":"code","882a3ec1":"code","226d2ed4":"code","88735d38":"code","b6bbf575":"code","05b527d9":"code","72c03cf6":"code","5d586af5":"code","a37a6c74":"code","21d11f1e":"code","4a3d5741":"code","6813cd3a":"code","673cf90a":"code","3071306b":"code","a6b12f5a":"code","41decb1c":"code","46d4f4e1":"code","eca05314":"code","5ef69509":"code","6291d614":"code","f92c58c1":"code","63eca38a":"code","e217fcd4":"code","6411243f":"code","897b8dc2":"code","814ee87a":"code","7f67eee5":"code","9191d4cb":"code","d176cba3":"code","116f03ed":"code","94352f1f":"code","7daf5b5a":"code","87cb121e":"code","e76c2b0b":"code","f0bfff59":"code","9586df3a":"code","700d925a":"code","a76d4347":"code","f0653c32":"code","0d190591":"code","d4e4598e":"code","c682710d":"code","c6513623":"code","2cb864c2":"code","b1c49845":"code","d5719276":"code","8fee9fe9":"code","01f4363e":"code","7dc2883e":"code","7c3cecc6":"code","f5384dfa":"code","ad8557d4":"code","b2cb2468":"code","201e4b42":"code","96a51959":"code","5154e56b":"code","19bf7231":"code","8708f50f":"code","9944c0ac":"code","912765ce":"markdown","723b4540":"markdown","46829e93":"markdown","a142dba9":"markdown","5dde0019":"markdown","6ecabc52":"markdown","b4fa99ba":"markdown","02c9959e":"markdown","51a8376a":"markdown","d94fa44d":"markdown","4f9d4b7a":"markdown","546d00a8":"markdown","45eee3a9":"markdown","7035b221":"markdown","35548087":"markdown","0b9ff469":"markdown","a0ed8a58":"markdown","3bd04093":"markdown","5bc01928":"markdown","381d521d":"markdown","f1513649":"markdown","a4668b12":"markdown","2e238e10":"markdown","af09b374":"markdown","67f49204":"markdown","1dde15b4":"markdown","734b97f9":"markdown","a9628fd5":"markdown","4e93806b":"markdown","558b4fbc":"markdown","de22bc75":"markdown","3d4aa11e":"markdown","f300ce8d":"markdown","fb55339a":"markdown","3f1068cd":"markdown","7ff4f54d":"markdown","9a670ef5":"markdown","421b1473":"markdown","22b092c5":"markdown","79b63ad8":"markdown","c1a552ca":"markdown","98f148b5":"markdown","2ac7ea8e":"markdown","bfebd299":"markdown","20647811":"markdown","36a1fc02":"markdown","e60ac95d":"markdown","f3be1838":"markdown","15e2d0b3":"markdown","f780ce05":"markdown","5e7dff4f":"markdown","3272dfb5":"markdown","0bb65c49":"markdown","c08ecfe1":"markdown","795e68fe":"markdown","4bf7f55b":"markdown","e3b0345c":"markdown","0214b830":"markdown"},"source":{"2339c92d":"#import libraries \n\n#structures\nimport numpy as np\nimport pandas as pd\n\n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#get model duration\nimport time\nfrom datetime import date\n\n#analysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8795c65b":"#load dataset\ndata = '..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv'\ndataset = pd.read_csv(data)\ndataset.shape","dabcb193":"dataset.dtypes","7a613f66":"dataset.describe()","6b13083e":"#check for missing data\ndataset.isnull().any().any()","bc399ab9":"#check for unreasonable data\ndataset.applymap(np.isreal)","44161f27":"sns_plot = sns.pairplot(dataset)","d5fb50b7":"sns_plot = sns.distplot(dataset['quality'])","21f77c7b":"#create new column; \"quality_class\"\ndataset['quality_class'] = dataset['quality'].apply(lambda value: 1 if value < 5 else 2 if value < 7 else 3)","12b0b560":"#set x and y\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = dataset.iloc[:,0:11]\ny = dataset['quality_class']\n\n#stadardize data\nX_scaled = StandardScaler().fit_transform(X)\n\n#get feature names\nX_columns = dataset.columns[:11]\n\n#split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)","06024b1b":"dataset.head()","c168ef25":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","1ccfc37b":"pca = PCA(n_components=6)\npc_X = pca.fit_transform(X_scaled)\npc_columns = ['pc1','pc2','pc3','pc4','pc5','pc6']\nprint(pca.explained_variance_ratio_.sum())","e889ea44":"print(pca.explained_variance_ratio_)","4bcb07ff":"#split train and test data for pca\nXpc_train, Xpc_test, ypc_train, ypc_test = train_test_split(pc_X, y, random_state=42)","a23db9a4":"#get correlation map\ncorr_mat=dataset.corr()","ac47c51a":"#visualise data\nplt.figure(figsize=(13,5))\nsns_plot=sns.heatmap(data=corr_mat, annot=True, cmap='GnBu')\nplt.show()\n\n#save file\n#sns_plot.get_figure().savefig('corr_mat.jpg')","945aa2c7":"#check for highly correlated values to be removed\ntarget = 'quality'\ncandidates = corr_mat.index[\n    (corr_mat[target] > 0.5) | (corr_mat[target] < -0.5)\n].values\ncandidates = candidates[candidates != target]\nprint('Correlated to', target, ': ', candidates)","1cff3304":"from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split","5da7df89":"# import model\nfrom sklearn.linear_model import LinearRegression\n\n#instantiate\nlinReg = LinearRegression()\n\nstart_time = time.time()\n# fit out linear model to the train set data\nlinReg_model = linReg.fit(X_train, y_train)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","677eb1fa":"#get coefficient values\ncoeff_df = pd.DataFrame(linReg.coef_, X_columns, columns=['Coefficient'])  \ncoeff_df","0dc94d25":"#validate model\ny_pred = linReg.predict(X_test)\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(10)","d6b4e6de":"df1.plot(kind='bar',figsize=(5,5))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","d28f61ac":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","34353f03":"# print the intercept and coefficients\nprint('Intercept: ',linReg.intercept_)\nprint('r2 score: ',linReg.score(X_train, y_train))","37829b74":"sns_plot = sns.distplot(dataset['quality'])","cf2703eb":"#the dataset contains 6 unique values.\nlen(dataset['quality'].unique())","52830bc5":"from sklearn.linear_model import LogisticRegression","d248a9fe":"logReg=LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42)\n\nstart_time = time.time()\n# Building a Logistic Regression Model\nlogReg.fit(X_train, y_train)\n\n#print duration of model\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","f01730cc":"# Calculate Accuracy Score\ny_pred = logReg.predict(X_test)\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))","d6cebc17":"#Calculate Confusion Matrix\nprint('confusion matrix: ','\\n',confusion_matrix(y_test,y_pred, labels=[1,2,3]))","395461c5":"#apply pca\nstart_time = time.time()\n\n# Building a Logistic Regression Model\nlogReg.fit(Xpc_train, ypc_train)\n\n#print duration of model\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","b2c2ffb6":"# Calculate Accuracy Score\ny_pred = logReg.predict(Xpc_test)\nprint('Accuracy score with PCA applied: ', accuracy_score(ypc_test, y_pred))","9daede34":"# Calculate Confusion Matrix\nprint('confusion matrix: ','\\n',confusion_matrix(ypc_test,y_pred, labels=[1,2,3]))","6f2929ff":"from sklearn.neighbors import KNeighborsClassifier","78e9b815":"k_array = np.arange(1, 17, 2)\nfor k in k_array:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    y_pred=knn.predict(X_test)\n    ac = accuracy_score(y_test, y_pred)\n    print('n_neighbours: ',k)\n    print('accuracy score: ',ac)\n    print('confusion matrix: ','\\n',confusion_matrix(y_test, y_pred))\n    print('-------------------------------')","94ff6c71":"#apply pca\nk_array = np.arange(1, 17, 2)\nfor k in k_array:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(Xpc_train, ypc_train)\n    y_pred=knn.predict(Xpc_test)\n    ac = accuracy_score(ypc_test, y_pred)\n    print('n_neighbours: ',k)\n    print('accuracy score: ',ac)\n    print('confusion matrix: ','\\n',confusion_matrix(ypc_test, y_pred))\n    print('-------------------------------')","ac981c27":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()","9f2740dd":"#train model\nstart_time = time.time()\ndt.fit(X_train,y_train)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","92700d85":"# Calculate Accuracy Score\ndt_predict = dt.predict(X_test)\ndt_acc_score = accuracy_score(y_test, dt_predict)\nprint(dt_acc_score)","e87efc66":"# Calculate Confusion Matrix\ndt_conf_matrix = confusion_matrix(y_test, dt_predict)\nprint('confusion matrix: ','\\n',dt_conf_matrix)","6476458c":"#training with Gini\ndef decTreeScore2(crit = 'gini',  maxDepth = 2, minSamples = 1, minSplit = 2):\n    dect = DecisionTreeClassifier(criterion = crit, max_depth = maxDepth, min_samples_leaf = minSamples, \n                                 min_samples_split = minSplit, random_state= 42)\n    dect.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, dect.predict(X_test))\n    print(accuracy)\n    return accuracy","16407273":"start_time=time.time()\ndecTreeScore2()\ntoday=date.today()\nprint(\"---%s seconds---\"% (time.time()-start_time))","85d4ed9a":"decTreeScore2(crit = 'entropy')\n#if we use entropy to calculate infomation gain instead of gini score, the accuracy drops","62477baf":"# find the max allowed depth for the decision tree\nfor i in np.arange(1, 15, 1):\n    decTreeScore2(maxDepth = i)","e70874e6":"# find maximum_samples leaf of the tree\nfor i in np.arange(1, 10, 1):\n    decTreeScore2(minSamples = i)","abda8af0":"# find minimum_samples_split of the tree\nfor i in np.arange(2, 10,1):\n    decTreeScore2(minSplit = i)","dc4711ed":"# decision tree model\n# import graphviz and sklearn.tree\nfrom sklearn import tree\nimport graphviz\nfrom graphviz import Source","882a3ec1":"dot_data = tree.export_graphviz(dt, out_file=None, max_depth=2,class_names=True,feature_names= X_columns, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph","226d2ed4":"#apply pca\ndt = tree.DecisionTreeClassifier(max_depth=2)\ndt.fit(Xpc_train, ypc_train)","88735d38":"#training with Gini\ndef decTreeScore2(crit = 'gini',  maxDepth = 2, minSamples = 1, minSplit = 2):\n    dect = DecisionTreeClassifier(criterion = crit, max_depth = maxDepth, min_samples_leaf = minSamples, \n                                 min_samples_split = minSplit, random_state= 42)\n    dect.fit(Xpc_train, ypc_train)\n    accuracy = accuracy_score(ypc_test, dect.predict(Xpc_test))\n    print(accuracy)\n    return accuracy","b6bbf575":"start_time=time.time()\ndecTreeScore2()\ntoday=date.today()\nprint(\"---%s seconds---\"% (time.time()-start_time))","05b527d9":"decTreeScore2(crit = 'entropy')\n#if we use entropy to calculate infomation gain instead of gini score, the accuracy drops","72c03cf6":"# use different maximum depth of the tree\nfor i in np.arange(1, 15, 1):\n    decTreeScore2(maxDepth = i)","5d586af5":"# use different maximum_samples leaf of the tree\nfor i in np.arange(1, 10, 1):\n    decTreeScore2(minSamples = i)","a37a6c74":"dot_data = tree.export_graphviz(dt, out_file=None, max_depth=2,class_names=True, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph","21d11f1e":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","4a3d5741":"#In this step we will be importing and preparing dataset that is to be analyzed, in this case we will be using\n#\u2018winequality-red.csv\u2019 dataset. \n#dataset = pd.read_csv('winequality-red.csv',sep=';')\ndataset['quality_class'] = dataset['quality'].apply(lambda value: 1 if value < 5 else 2 if value < 7 else 3)\ndataset['quality_class'] = pd.Categorical(dataset['quality_class'], categories=[1,2,3])\ndataset['quality_class'] = dataset['quality_class'].astype(int)\ndataset.head()","6813cd3a":"quality_label_sums= dataset['quality_class'].value_counts()\nquality_label_percentage = quality_label_sums\/len('quality_class')\nprint(quality_label_sums)\nprint(quality_label_percentage)","673cf90a":"#visualize quality_class\nj = sns.countplot(x='quality_class', data=dataset)\nplt.show(j)","3071306b":"dataset['quality_class'] = dataset['quality_class'].astype(int)\ndataset = pd.get_dummies(dataset, columns=['quality_class'])\ndataset.head()","a6b12f5a":"Xn = dataset.iloc[:,0:11].values\nYn = dataset.iloc[:,12:].values\n\nXn = StandardScaler().fit_transform(Xn)\n\nXn_train, Xn_test, Yn_train, Yn_test = train_test_split(Xn, Yn,random_state=42)\n\nprint(Xn_train.shape, Yn_train.shape, Xn_test.shape, Yn_test.shape)","41decb1c":"model = Sequential()\nmodel.add(Dense(30, input_dim=11, activation='sigmoid'))\nmodel.add(Dense(50, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))","46d4f4e1":"model.summary()","eca05314":"model.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nstart_time = time.time()\n#train model\nhistory = model.fit(x = Xn_train, y = Yn_train,batch_size=128, epochs = 800,verbose=1,validation_data=(Xn_test, Yn_test))\n\n#get model training duration\ntoday= date.today()\nprint('---%s seconds---'%(time.time()-start_time))","5ef69509":"# Calculation of Loss and Accuracy metrics\nloss, accuracy = model.evaluate(Xn_test, Yn_test)\nprint('loss: ', loss, ', accuracy: ', accuracy)","6291d614":"predictions = model.predict(Xn_test)\nprint('\\nPrediction:')\nfor i in np.arange(len(predictions)):\n    print('Actual: ', Yn_test[i], ', Predicted: ', predictions[i])\n\npredictions=np.argmax(predictions, axis=1)\nYn_test = np.argmax(Yn_test, axis=1)","f92c58c1":"# Training History - Model Accuracy\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","63eca38a":"# Training History - Loss Accuracy\nprint(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","e217fcd4":"#Calculation of confusion matrix\n#from sklearn.metrics import confusion_matrix\nconfusion_matrix(Yn_test, predictions)","6411243f":"Y = dataset.iloc[:,12:].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(pc_X, Y, random_state=42)\n\nprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)","897b8dc2":"model = Sequential()\nmodel.add(Dense(30, input_dim=6, activation='sigmoid'))\nmodel.add(Dense(50, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))","814ee87a":"\nmodel.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nstart_time = time.time()\nhistory = model.fit(x = X_train, y = Y_train,batch_size=128, epochs = 800,verbose=1,validation_data=(X_test, Y_test))\n\ntoday= date.today()\nprint('---%s seconds---'%(time.time()-start_time))","7f67eee5":"# Calculation of Loss and Accuracy metrics\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint('loss: ', loss, ', accuracy: ', accuracy)","9191d4cb":"predictions = model.predict(X_test)\nprint('\\nPrediction:')\nfor i in np.arange(len(predictions)):\n    print('Actual: ', Y_test[i], ', Predicted: ', predictions[i])\n    \npredictions=np.argmax(predictions, axis=1)\nY_test = np.argmax(Y_test, axis=1)","d176cba3":"# Training History - Model Accuracy\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","116f03ed":"# Training History - Loss Accuracy\nprint(history.history.keys())\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","94352f1f":"#Calculation of confusion matrix\nconfusion_matrix(Y_test, predictions)","7daf5b5a":"#import libraries\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans","87cb121e":"#try to find optimal k using the elbow method\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\nf3, ax = plt.subplots(figsize=(8, 6))\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","e76c2b0b":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","f0bfff59":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(X_scaled[clusters==i,0],\n               X_scaled[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","9586df3a":"# Visualise the clusterds considerig fixed acidity, residual sugar, and alcohol\nfig = plt.figure(figsize=(20, 15))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=15, azim=40)\n\nax.scatter(X_scaled[:,0], X_scaled[:,3], X_scaled[:,10],c=y, edgecolor='k')\nax.set_xlabel('Acidity')\nax.set_ylabel('Sugar')\nax.set_zlabel('Alcohol')\nax.set_title('K=2: Acidity, Sugar, Alcohol', size=22)","700d925a":"#evaluate model\nfrom sklearn.metrics import pairwise_distances\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","a76d4347":"kmeans.inertia_","f0653c32":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","0d190591":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(pc_X[clusters==i,0],\n               pc_X[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","d4e4598e":"#evaluate model\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","c682710d":"kmeans.inertia_","c6513623":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import dendrogram, linkage","2cb864c2":"#plot dendrogram to determine number of clusters\nplt.figure(figsize=(25, 10))\nplt.title('Dendrogram')\nplt.xlabel('Wine Details')\nplt.ylabel('Euclidean distances')\n\ndendrogram (\n    linkage(X_scaled, 'ward')  # generate the linkage matrix\n    ,leaf_font_size=8 # font size for the x axis labels\n)\nplt.axhline(y=8)\nplt.show()","b1c49845":"clustering = AgglomerativeClustering(linkage=\"ward\", n_clusters=3)\n#train model\nstart_time = time.time()\nclustering.fit(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","d5719276":"#visualize clustering\ncolors = 'rgbkcmy'\n\nfor i in np.unique(clustering.labels_):\n    plt.scatter(X_scaled[clustering.labels_ == i, 0], X_scaled[clustering.labels_ == i, 1],\n                color=colors[i], label='Cluster ' + str(i + 1))\n\nplt.legend()\nplt.title('Hierarchical Clustering')\nplt.xlabel(X_columns[1])\nplt.ylabel(X_columns[2])\nplt.show()","8fee9fe9":"#evaluate model\nlabels = clustering.labels_\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","01f4363e":"clustering = AgglomerativeClustering(linkage=\"ward\", n_clusters=3)\nstart_time = time.time()\nclustering.fit(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","7dc2883e":"#visualize clustering\ncolors = 'rgbkcmy'\n\nfor i in np.unique(clustering.labels_):\n    plt.scatter(pc_X[clustering.labels_ == i, 0], \n                pc_X[clustering.labels_ == i, 1],\n                color=colors[i], label='Cluster ' + str(i + 1))\n\nplt.legend()\n\nplt.title('Hierarchical Clustering')\nplt.xlabel(pc_columns[0])\nplt.ylabel(pc_columns[1])\nplt.show()","7c3cecc6":"#evaluate model\nlabels = clustering.labels_\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","f5384dfa":"from sklearn.cluster import DBSCAN","ad8557d4":"dbscan = DBSCAN(eps=2, min_samples=7)\nstart_time = time.time()\nclusters= dbscan.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","b2cb2468":"np.unique(clusters)","201e4b42":"colors = 'rgbkcmy'\nax = plt.axes(projection='3d')\n\nfor i in np.unique(clusters):\n    label = 'Outlier' if i == -1 else 'Cluster ' + str(i + 1)\n    ax.scatter3D(X_scaled[clusters==i,0], X_scaled[clusters==i,1],X_scaled[clusters==i,4],\n                #color=colors[i], \n                 label=label)\n\nplt.legend()\nplt.show()","96a51959":"#evaluate model\nlabels = dbscan.labels_\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","5154e56b":"dbscan = DBSCAN(eps=2, min_samples=7)\nstart_time = time.time()\nclusters= dbscan.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","19bf7231":"np.unique(clusters)","8708f50f":"ax = plt.axes(projection='3d')\n\nfor i in np.unique(clusters):\n    label = 'Outlier' if i == -1 else 'Cluster ' + str(i + 1)\n    ax.scatter3D(pc_X[clusters==i,0], \n                 pc_X[clusters==i,1],\n                 pc_X[clusters==i,2],\n                 label=label)\n\nplt.legend()\nplt.show()","9944c0ac":"#evaluate model\nlabels = dbscan.labels_\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","912765ce":"It can be seen that clusters are not well separated. Some members of Cluster 2 can be seen in Cluster 1 and vice versa.","723b4540":"Higher min_samples or lower eps indicate higher density necessary to form a cluster.","46829e93":"## 2.1 Agglormerative Clustering with PCA","a142dba9":"## 2.4.1 Neural Network with PCA","5dde0019":"## 1.1 K-Means with PCA","6ecabc52":"Training Time \u2013 0.062 seconds","b4fa99ba":"## 3.1 Dbscan with PCA","02c9959e":"From the graph above, we can tell that that clusters not clearly defined. Lets explore Agglormerative Clustering agin with PCA.","51a8376a":"## 1. K-Means","d94fa44d":"Now, the silhouette score of the model will be measured. The silhouette score ranges from -1 to +1. <br>\nThe high silhouette score indicates that the objects are well matched to its own cluster and not to its neighbouring clusters. <br>\n(The higher the silhouette score \u2013 the better the clustering)","4f9d4b7a":"# Data visualisation","546d00a8":"From the heatmap, it can be seen that most features are weakly correlated to the quality of wine the exception of alcohol (0.48) which is a moderate correlation.","45eee3a9":"\u201cK\u201d value of 2 will be used as a dip can be seen around 2 which is our elbow in a graph above. <br> <br> <br>\n\n\nFirst, clustering will be performed with K-Means on dataset without applying principle component analysis (PCA). Note that the total dimension of dataset is 11.","7035b221":"## 2. Classification Models","35548087":"We will apply 2 clustering ML models to the dataset to try uncover possible clusters.\n\n1. K-Means (centriod based)\n2. Hierarchical Agglomerative Clustering (similarity based)\n3. Dbscan (density based)","0b9ff469":"Next we are going to determine the input and output variable of our dataset. We will also be doing scaling of feature using StandardScaler() function in sklearn library to ensure that our data is arranged in a standard normal distribution with mean of 0 and standard deviation of 1.","a0ed8a58":"1. Regression Model <br>\n1.1 Linear Regression <br>\n2. Classification Models <br>\n2.1 Logistic Regression <br>\n2.2 K-NN <br>\n2.3 Decision Tree <br>\n2.4 Neural Network","3bd04093":"## 2.3 Decision Tree","5bc01928":"In this model, the entire dataset has been used as a training data. <br>\nThen an elbow method will be used to find out an optimal number of \u201cK\u201d clusters.","381d521d":"Note: This Red-Wine Analysis using both Supervised Learning & Unsupervised Learning models is a group project which I had done together with my team-mates. ","f1513649":"The red wine data consists of 1599 rows and 12 columns.","a4668b12":"Neural Network also known as Deep Learning is a type of machine learning with a series of algorithms used to identify relationship in a given data set. For this assignment we will be using Karen library to construct a Neural Network used to estimate the quality of wine in our chosen data set.","2e238e10":"After implementing PCA, it can be seen that clustering is improved. So it is expected to see a higher silhouette score.","af09b374":"# Data Cleaning","67f49204":"Training time \u2013 0.05 seconds Training time is observed to have reduced slightly.","1dde15b4":"## 2.4 Neural Network","734b97f9":"## 2.3.1 Decision Tree with PCA","a9628fd5":"# 1. Principal component analysis","4e93806b":"An accuracy score of 84.5% looks good enough for Logistic Regression model as a classification technique. Out of 400 testing samples used, 338 are correctly predicted and 62 are classified wrongly. When PCA is applied to reduce the dimensions of the dataset, the accuracy score did not improve but decreased marginally to 82.75% and there were 69 classification errors using the test data. <br> <br>\n\nThe advantages of using Logistic Regression are high efficiency, does not require much computational resources, highly interpretable and it can produce predicted probabilities of possible outcomes. <br> <br>\n\nFeature Engineering is important to Logistic Regression in order to apply it. Each sample must belong in one of the categories and the categories must be mutually exclusive. There must be no missing values in the dataset. For Logistic Regression to work better, the independent variables should not be correlated with each other (i.e. no multi-collinearity). <br> <br>\n\nThe disadvantages of Logistic Regression are that it requires large amounts of samples and it cannot be used to predict continuous values. It can only be used to predict a categorical outcome.","558b4fbc":"The mean square error for the model (0.30) is rather low and indicative of high prediction accuracy. However, this could possibly mean that the model is overfitting. <br> <br>\n\nThe root mean squared error (0.55) is slightly less than 10% of the mean wine quality (5.63), this asserts that the model can make reasonable predictions although not entirely accurate. <br> <br>\n\nHowever, we have to keep in mind that the correlations in the model is rather low.","de22bc75":"Apply agglomerative clustering to pick the best number of clusters, we need to draw the dendrogram graph","3d4aa11e":"## 1.1 Linear Regression","f300ce8d":"# Description of Data","fb55339a":"## 2.2.1 K-NN with PCA","3f1068cd":"1. Feature extraction: Principal component analysis\n2. Feature selection: Pearson's correlation","7ff4f54d":"The silhouette score obtained is considered low. It means clusters are neither dense nor well separated. <br>\nNext, let\u2019s measure the inertia value.","9a670ef5":"Using a correlation of 0.6 to -0.5 as benchmark, a correlation matrix has been created to sieve out features that are highly correlated to the quality of red wine. Our results show that all features are within the acceptable range of 0.6 to -0.5.","421b1473":"From the dengrogram above we can see that the features after the 3rd branch are very similar to each other (i.e. shorter in height). The dataset should optimally have are 3 clusters; where the distance between the clusters are the highest.","22b092c5":"# Supervised Machine Learning","79b63ad8":"All features seem to have little effect on the wine quality. <br> <br>\n\nThe coefficient scores suggest that for a unit increase in any feature, there is less than 0.12 units increase\/decrease in the wine [\u201cquality_class\u201d]. Similarly, for wine [\u201cquality\u201d], although coefficient scores are higher, they remain low with alcohol having the highest coefficient score of 0.3.","c1a552ca":"## 3. Dbscan","98f148b5":"Our purpose of applying principal component analysis is to reduce dimension. <br>\nIn this dataset, we reduced the 11-dimensional data to 6-dimensional data during PCA.","2ac7ea8e":"## 2.1 Logistics Regression","bfebd299":"# Unsupervised Machine Learning","20647811":"**Direction of relationship** <br>\nAcidity (-0.39), chlorides (-0.13), free sulfur dioxide (-0.051), total sulfur dioxide (-0.19), density (-0.17) and PH (-0.058) are negatively correlated to the quality of wine; as these variables decrease, the quality of wine will increase vice versa. <br> <br>\n\n\nConversely, fixed acidity (0.12), citric acid, residual sugar (0.014), sulphates (0.25) and alcohol (0.48) are positively correlated to the quality of wine; as these variables increase, the quality of wine improves.","36a1fc02":"An extremely high inertia value of 14330.119 was obtained. It is an indicative of the \u201ccurse of dimensionality\u201d. <br>\nWe are using 11 dimensions of data in this model. <br>\nIn this case, we will explore the model again using PCA (principle component analysis).","e60ac95d":"## 2. Agglormerative Clustering","f3be1838":"The inertia value is also decreased but still extremely high. <br> <br>\n\nK-means clustering has poor clustering result for high dimensional data. Even with the implementation of PCA, the silhouette score can only be improved to some extent but is considered low. Also the inertia value is observed to be extremely high. In an ideal situation, the inertia value should be as low as possible. Hence, we can conclude that this is not a good model fit to the data.","15e2d0b3":"## 2.1.1 Logistics Regression with PCA","f780ce05":"# 2. Pearson's Correlation","5e7dff4f":"# Feature Engineering","3272dfb5":"Although the clusters are not entirely segreggated, they appear to be clearer after applying PCA.","0bb65c49":"## 2.2 K-NN","c08ecfe1":"After preparing our dataset, we will be moving on to create our Neural Network model using keras library that will be used to determine wine quality.\n\nwe are going to use Sequential class from keras.models to allow us to define all of the layer in constructor.\nwe are going to use Dense from keras.layers, to allow us to run our model operation.","795e68fe":"As expected, we can see an improvement in the silhouette score. But it is still considered low which means there are still some overlapping of clusters or incorrect grouping. <br> <br>\n\nAlthough the silhouette score increased with PCA, it still low; clusters are overlapping or incorrectly grouped.","4bf7f55b":"# Pre-processing","e3b0345c":"The R2 (0.22) score is small (i.e. the residuals are big); only 22% of the variance in wine quality can be explained by the variables.","0214b830":"## 1. Regression Model"}}