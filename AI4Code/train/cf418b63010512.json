{"cell_type":{"362c6143":"code","ff85aaa3":"code","31e35742":"code","036292b6":"code","c81c57a7":"code","0131bdcc":"code","d0bd4f09":"code","d7379d51":"code","4dc2e69c":"code","d3f42a70":"code","49b524cd":"code","69e746ee":"code","dc4370e4":"code","81b15fce":"code","ee572511":"code","d034ec87":"code","fd8c8013":"code","b3337c0a":"code","3095c968":"code","e35d1b88":"code","1fad2de9":"code","60c58c46":"code","5978416f":"code","deb0f2b7":"code","eaa89fbd":"code","c888f40c":"code","0508c840":"code","7c455bc2":"code","343538bf":"code","f8a8a065":"code","45c2de09":"code","132844e0":"code","7058ee1b":"code","e51a69c3":"code","71d55c80":"code","e9d3c431":"markdown","dd346399":"markdown","4e2e8667":"markdown","3693f1ea":"markdown","1e5001e6":"markdown","0e4bc8ca":"markdown","8bd5522e":"markdown","9012daed":"markdown","0749e85f":"markdown","e85ff222":"markdown","77cb7357":"markdown","b20b5c49":"markdown","1a6522c5":"markdown","3b3bbe54":"markdown","df6e35fa":"markdown","58f832f9":"markdown"},"source":{"362c6143":"## Lets import the library avaliable to analysis\n\nimport pandas as pd # data processing, pd.read_csv\nimport numpy as np # algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolor=sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment=None\npd.options.display.max_columns = 999\n","ff85aaa3":"ls - all","31e35742":"train_df=pd.read_csv(\"..\/input\/train_2016_v2.csv\",parse_dates=['transactiondate'])\n\n# train_df=pd.read_csv(\"C:\/Users\/raundral\/MyPythonLab\/Case study\/Zillow price estimates\/all\/train_2016_v2.csv\",parse_dates=['transactiondate'])\n","036292b6":"train_df.head(10)\ntrain_df.dtypes\ntrain_df.shape","c81c57a7":"train_df.head(10)","0131bdcc":"# our target variable is logerror , let analise the bit more\n\n# plt.figure(figsize=(10,10))\n\nplt.scatter(range(train_df.shape[0]),np.sort(train_df.logerror.values))\n\nplt.xlabel('Index',fontsize=12)\nplt.ylabel('Logerr', fontsize=12)\n\nplt.show()\n","d0bd4f09":"ulimit=np.percentile(train_df.logerror.values,99)\nllimit=np.percentile(train_df.logerror.values,1)\n\ntrain_df['logerror'].loc[train_df['logerror']>ulimit]=ulimit\ntrain_df['logerror'].loc[train_df['logerror']<llimit]=llimit\n\n#plt.figure(figsize=(20,10))\n\nsns.distplot(train_df.logerror.values,bins=50,kde=False)\n\nplt.xlabel('Logerror',fontsize=20)\n\nplt.show()\n\n","d7379d51":"## Understand how TransactionDate on this \n\ntrain_df['transactiondate_month']=train_df['transactiondate'].dt.month\n\ncnt_srs=train_df['transactiondate_month'].value_counts()\n\nsns.barplot(cnt_srs.index,cnt_srs.values)\n\nplt.xticks(rotation='vertical')\n\nplt.xlabel('Month of Transaction', fontsize=12)\nplt.ylabel('Number of occures',fontsize=12)\n\nplt.show()","4dc2e69c":"(train_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()","d3f42a70":"prop_df=pd.read_csv('..\/input\/properties_2016.csv')","49b524cd":"prop_df.shape","69e746ee":"prop_df.head(5)","dc4370e4":"missing_df=prop_df.isnull().sum(axis=0).reset_index()","81b15fce":"missing_df.columns=['column_name','missing_count']","ee572511":"missing_df=missing_df.loc[missing_df['missing_count']>0] \nmissing_df=missing_df.sort_values(by='missing_count')\n\nind=missing_df.shape[0]\n\n\n","d034ec87":"\n\nfig, ax = plt.subplots(figsize=(12,18))\n\nax.barh(np.arange(ind),missing_df.missing_count.values)\n\nax.set_yticks(np.arange(ind))\nax.set_yticklabels(missing_df.missing_count.values,rotation='horizontal')\nax.set_xlabel(\"Number of missing values\")\nax.set_title(\"Number of missing values in each columns\")\n\nplt.show()","fd8c8013":"sns.jointplot(x=prop_df.latitude.values,y=prop_df.longitude.values)\n\nplt.xlabel('Longitude',fontsize=12)\nplt.ylabel('Latitude',fontsize=12)\nplt.show()","b3337c0a":"train_df=pd.merge(train_df,prop_df,on='parcelid',how='left')\n\ntrain_df.shape","3095c968":"train_df.shape","e35d1b88":"pd.options.display.max_rows=50\n\ndtype_df=train_df.dtypes.reset_index()\n\ndtype_df.columns=['Count','Column_type']\n\ndtype_df","1fad2de9":"dtype_df.groupby('Column_type').aggregate('count').reset_index()","60c58c46":"missing_df=train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns=['column_name', 'missing_count']\n\nmissing_df['missing_ratio']=missing_df['missing_count'] \/ train_df.shape[0]\n\nmissing_df.loc[missing_df['missing_count']>0.999]\n\n","5978416f":"train_df_mean=train_df.mean(axis=0)\n\ntrain_df_new=train_df.fillna(train_df_mean)\n\n# Now look at how correlation Coff correlated with of each of these variables\n\n# x_cols=[col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n\nx_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n\nlabels=[]\nvalues=[]\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(train_df_new[col].values,train_df_new.logerror.values)[0,1])\ncorr_df=pd.DataFrame({'col_labels':labels,'corr_values':values})\ncorr_df=corr_df.sort_values(by='corr_values')\n\nind=np.arange(len(labels))\n\nfig, ax = plt.subplots(figsize=(12,20))\n\nax.barh(ind,np.array(corr_df.corr_values.values),color='y')\n\nax.set_yticks(ind)\n\nax.set_yticklabels(corr_df.col_labels.values,rotation='horizontal')\n\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\n#autolabel(rects)\nplt.show()","deb0f2b7":"corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\nfor col in corr_zero_cols:\n    print(col, len(train_df_new[col].unique()))","eaa89fbd":"corr_df_sel=corr_df.loc[(corr_df['corr_values']>0.02) | (corr_df['corr_values']<-0.01)]\n\ncorr_df_sel","c888f40c":"cols_to_use=corr_df_sel.col_labels.tolist()\n\ntem_df = train_df[cols_to_use]\n\ncorrmat=tem_df.corr(method='spearman')\n\nfig,ax=plt.subplots(figsize=(8,8))\n\nsns.heatmap(corrmat,vmax=1.,square=True)\n\nplt.title(\"Important variable corrlationMap\",fontsize=20)\nplt.show()\n","0508c840":"col = \"finishedsquarefeet12\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.finishedsquarefeet12.values, y=train_df.logerror.values, size=10, color=color[4])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('Finished Square Feet 12', fontsize=12)\nplt.title(\"Finished square feet 12 Vs Log error\", fontsize=15)\nplt.show()","7c455bc2":"col = \"calculatedfinishedsquarefeet\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=train_df.finishedsquarefeet12.values, y=train_df.logerror.values, size=10, color=color[4])\nplt.ylabel('Log Error', fontsize=12)\nplt.xlabel('calculatedfinishedsquarefeet', fontsize=12)\nplt.title(\"calculatedfinishedsquarefeet Vs Log error\", fontsize=15)\nplt.show()","343538bf":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"bathroomcnt\", data=train_df)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Bathroom', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of Bathroom count\", fontsize=15)\nplt.show()","f8a8a065":"plt.figure(figsize=(12,8))\nsns.boxplot(x=\"bathroomcnt\", y=\"logerror\", data=train_df)\nplt.ylabel('Log error', fontsize=12)\nplt.xlabel('Bathroom Count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"How log error changes with bathroom count?\", fontsize=15)\nplt.show()","45c2de09":"train_df_latest=train_df_new","132844e0":"train_df_new.columns","7058ee1b":"train_y=train_df_new['logerror'].values\n\ncat_cols=[\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n\ntrain_df=train_df_new.drop(['parcelid', 'logerror', 'transactiondate', 'transactiondate_month'] + cat_cols,axis=1)\n\nfeat_names=train_df.columns.values","e51a69c3":"from sklearn import ensemble\n\nmodel=ensemble.ExtraTreesRegressor(n_estimators=25,max_depth=30,max_features=0.3,n_jobs=-1,random_state=0)\n\nmodel.fit(train_df,train_y)\n\n\n# plotting the importance features\n\nimportance = model.feature_importances_\nstd=np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\nindices=np.argsort(importance)[::-1][:20]\n\nplt.bar(range(len(indices)), importance[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()\n","71d55c80":"import xgboost as xgb\nxgb_params={\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\n\ndtrain=xgb.DMatrix(train_df,train_y,feature_names=train_df.columns.values)\n\nmodel=xgb.train(dict(xgb_params,silent=0),dtrain,num_boost_round=50)\n\n# plotting\n\nfig,ax=plt.subplots(figsize=(20,20))\nxgb.plot_importance(model,max_num_features=50,height=.8,ax=ax)\n\nplt.show()\n","e9d3c431":"Seems \"tax amount\" is the most importanct variable followed by \"structure tax value dollar count\" and \"land tax value dollor count\"","dd346399":"Important variables themself have highly corrlated !! Lets understand each variable and analyse","4e2e8667":"From the data page, *we are provided with a full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.*\n\nWe have about 90,811 rows in train but we have about 2,985,217 rows in properties file. So let us merge the two files and then carry out our analysis. ","3693f1ea":"Checking data types different types od data","1e5001e6":"##### listing out avaliable files under all folder","0e4bc8ca":"We had an understanding of important variables from the univariate analysis. But this is on a stand alone basis and also we have linearity assumption. Now let us build a non-linear model to get the important variables by building Extra Trees model.","8bd5522e":"* Nice Normal distribution","9012daed":"### FinishedSquarefeet12","0749e85f":"Let us take high correlated values and do some analysis","e85ff222":"We have more float type columns so lets look into how these are correlated to target variables (logerror )","77cb7357":"Four columns has 99.9% of missing Values ","b20b5c49":"Seems the range of logerror narrows down with increase in finished square feet 12 variable. Probably larger houses are easy to predict?\n\n**Calculated finished square feet:**","1a6522c5":"Let us seee how the finished square feet 12 varies with the log error.","3b3bbe54":"## properties 2016","df6e35fa":"Lets understand the missing values in merged dataframe","58f832f9":"Let impute the missing values using mean for missing values"}}