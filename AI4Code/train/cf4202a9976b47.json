{"cell_type":{"d1fad67d":"code","0bb54874":"code","81071316":"code","39725066":"code","1ecfc45d":"code","db3e5bc4":"code","d5140be9":"code","3e849ddc":"code","135586c9":"code","13bdc6ef":"code","a2ee13d9":"code","7a2506d4":"code","aaabe287":"code","f0db1fe6":"code","06006d20":"code","87f4ed3b":"code","5c8ea176":"code","1e16ba89":"code","ac632e11":"code","6325dbaf":"code","4cc8cb16":"code","1242c22f":"code","d3fa8665":"code","9308a0fc":"code","99241170":"code","aef06acf":"code","72494701":"code","aaadb80a":"code","e8033026":"code","5a093805":"code","ff97c36d":"code","294df1fa":"code","2f71640d":"code","4c8cb330":"code","aee43f83":"code","b2accad0":"code","a09e709c":"code","382607e6":"code","d0a397f6":"code","4b42ca0d":"code","86d49dce":"code","71eabbf1":"code","2e991645":"code","751f069d":"code","86014f1e":"code","98b9d96a":"code","0e84449a":"code","6bf0e6da":"code","fac4632e":"code","4b55966a":"code","a5625c08":"code","cb318a7e":"code","11cd603c":"code","b65cb677":"markdown","fb63cc3d":"markdown","f0fae76d":"markdown","e334fb93":"markdown","649d4fdd":"markdown","eb63e157":"markdown","133cad76":"markdown","166eea0d":"markdown","b4dd9aa7":"markdown","773cb54b":"markdown","118cf8aa":"markdown","a3964563":"markdown","3e99d4b6":"markdown","2cf6e0b0":"markdown","171ea34a":"markdown","bb807479":"markdown","913c8baf":"markdown","dec661f9":"markdown","44915e8a":"markdown","f220161a":"markdown","e3eff202":"markdown","8011c914":"markdown","f09d5826":"markdown","99ae0a23":"markdown","6ef31cc4":"markdown","4ae2e0d1":"markdown","e2bf8c8f":"markdown","e7ffbc39":"markdown","cbf80996":"markdown","d9edb420":"markdown","134f02e4":"markdown","396b0ded":"markdown","c04f1b02":"markdown","b4828c29":"markdown","8dd90046":"markdown","0459725c":"markdown","da49da63":"markdown","b7660b6e":"markdown","b5587473":"markdown","ab470481":"markdown","182f8c94":"markdown","5143d2b6":"markdown","ca98e9c3":"markdown","40c97fc1":"markdown","df844f14":"markdown","2c2038a4":"markdown","6a37eb64":"markdown","c04c7260":"markdown","98dbd57b":"markdown","48ed77cf":"markdown","4d41d8bc":"markdown","5d55db4c":"markdown","7904482e":"markdown","95651ec6":"markdown","88ada1e1":"markdown","f9f3c478":"markdown","2db8a485":"markdown","2b105e1a":"markdown","066e2bbc":"markdown"},"source":{"d1fad67d":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\n\nHTML('<iframe width=\"1106\" height=\"622\" src=\"https:\/\/www.youtube.com\/embed\/NZyQu1u3N9Y\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","0bb54874":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\n\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nfrom scipy import stats\nfrom scipy.stats import skew\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 100)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os,random, math, psutil, pickle\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\n","81071316":"%%time\nroot = '..\/input\/ashrae-energy-prediction\/'\ntrain_df = pd.read_csv(root + 'train.csv')\ntrain_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n\nweather_train_df = pd.read_csv(root + 'weather_train.csv')\ntest_df = pd.read_csv(root + 'test.csv')\nweather_test_df = pd.read_csv(root + 'weather_test.csv')\nbuilding_meta_df = pd.read_csv(root + 'building_metadata.csv')\nsample_submission = pd.read_csv(root + 'sample_submission.csv')\n","39725066":"print('Size of train_df data', train_df.shape)\nprint('Size of weather_train_df data', weather_train_df.shape)\nprint('Size of weather_test_df data', weather_test_df.shape)\nprint('Size of building_meta_df data', building_meta_df.shape)","1ecfc45d":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","db3e5bc4":"## REducing memory\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\n\nweather_train_df = reduce_mem_usage(weather_train_df)\nweather_test_df = reduce_mem_usage(weather_test_df)\nbuilding_meta_df = reduce_mem_usage(building_meta_df)","d5140be9":"train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\ntest_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\nweather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])\nweather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'])\n\nbuilding_meta_df['primary_use'] = building_meta_df['primary_use'].astype('category')","3e849ddc":"temp_df = train_df[['building_id']]\ntemp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')\ndel temp_df['building_id']\ntrain_df = pd.concat([train_df, temp_df], axis=1)\n\ntemp_df = test_df[['building_id']]\ntemp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')\n\ndel temp_df['building_id']\ntest_df = pd.concat([test_df, temp_df], axis=1)\ndel temp_df, building_meta_df\n","135586c9":"temp_df = train_df[['site_id','timestamp']]\ntemp_df = temp_df.merge(weather_train_df, on=['site_id','timestamp'], how='left')\n\ndel temp_df['site_id'], temp_df['timestamp']\ntrain_df = pd.concat([train_df, temp_df], axis=1)\n\ntemp_df = test_df[['site_id','timestamp']]\ntemp_df = temp_df.merge(weather_test_df, on=['site_id','timestamp'], how='left')\n\ndel temp_df['site_id'], temp_df['timestamp']\ntest_df = pd.concat([test_df, temp_df], axis=1)\n\ndel temp_df, weather_train_df, weather_test_df\n","13bdc6ef":"train_df.to_pickle('train_df.pkl')\ntest_df.to_pickle('test_df.pkl')\n   \ndel train_df, test_df\ngc.collect()","a2ee13d9":"train_df = pd.read_pickle('train_df.pkl')\ntest_df = pd.read_pickle('test_df.pkl')","7a2506d4":"# Convert wind direction into categorical feature. We can split 360 degrees into 16-wind compass rose. \n# See this: https:\/\/en.wikipedia.org\/wiki\/Points_of_the_compass#16-wind_compass_rose\ndef degToCompass(num):\n    val=int((num\/22.5))\n    arr=[i for i in range(0,16)]\n    return arr[(val % 16)]","aaabe287":"le = LabelEncoder()\n\ntrain_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)\n\ntest_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)","f0db1fe6":"train_df['age'] = train_df['year_built'].max() - train_df['year_built'] + 1\ntest_df['age'] = test_df['year_built'].max() - test_df['year_built'] + 1","06006d20":"def average_imputation(df, column_name):\n    imputation = df.groupby(['timestamp'])[column_name].mean()\n    \n    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n    del imputation\n    return df","87f4ed3b":"train_df = average_imputation(train_df, 'wind_speed')\ntrain_df = average_imputation(train_df, 'wind_direction')\n\nbeaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\nfor item in beaufort:\n    train_df.loc[(train_df['wind_speed']>=item[1]) & (train_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n\ntrain_df['wind_direction'] = train_df['wind_direction'].apply(degToCompass)\ntrain_df['beaufort_scale'] = train_df['beaufort_scale'].astype(np.uint8)\ntrain_df[\"wind_direction\"] = train_df['wind_direction'].astype(np.uint8)\ntrain_df[\"meter\"] = train_df['meter'].astype(np.uint8)\ntrain_df[\"site_id\"] = train_df['site_id'].astype(np.uint8)\n    \n    \ntest_df = average_imputation(test_df, 'wind_speed')\ntest_df = average_imputation(test_df, 'wind_direction')\n\nfor item in beaufort:\n    test_df.loc[(test_df['wind_speed']>=item[1]) & (test_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n\ntest_df['wind_direction'] = test_df['wind_direction'].apply(degToCompass)\ntest_df['wind_direction'] = test_df['wind_direction'].apply(degToCompass)\ntest_df['beaufort_scale'] = test_df['beaufort_scale'].astype(np.uint8)\ntest_df[\"wind_direction\"] = test_df['wind_direction'].astype(np.uint8)\ntest_df[\"meter\"] = test_df['meter'].astype(np.uint8)\ntest_df[\"site_id\"] = test_df['site_id'].astype(np.uint8)","5c8ea176":"train_df['month_datetime'] = train_df['timestamp'].dt.month.astype(np.int8)\ntrain_df['weekofyear_datetime'] = train_df['timestamp'].dt.weekofyear.astype(np.int8)\ntrain_df['dayofyear_datetime'] = train_df['timestamp'].dt.dayofyear.astype(np.int16)\n    \ntrain_df['hour_datetime'] = train_df['timestamp'].dt.hour.astype(np.int8)  \ntrain_df['day_week'] = train_df['timestamp'].dt.dayofweek.astype(np.int8)\ntrain_df['day_month_datetime'] = train_df['timestamp'].dt.day.astype(np.int8)\ntrain_df['week_month_datetime'] = train_df['timestamp'].dt.day\/7\ntrain_df['week_month_datetime'] = train_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)\n    \n    \ntest_df['month_datetime'] = test_df['timestamp'].dt.month.astype(np.int8)\ntest_df['weekofyear_datetime'] = test_df['timestamp'].dt.weekofyear.astype(np.int8)\ntest_df['dayofyear_datetime'] = test_df['timestamp'].dt.dayofyear.astype(np.int16)\n    \ntest_df['hour_datetime'] = test_df['timestamp'].dt.hour.astype(np.int8)\ntest_df['day_week'] = test_df['timestamp'].dt.dayofweek.astype(np.int8)\ntest_df['day_month_datetime'] = test_df['timestamp'].dt.day.astype(np.int8)\ntest_df['week_month_datetime'] = test_df['timestamp'].dt.day\/7\ntest_df['week_month_datetime'] = test_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)\n    ","1e16ba89":"train_df.head()","ac632e11":"train_df.columns.values","6325dbaf":"plt.figure(figsize = (15,5))\ntrain_df['meter_reading'].plot()","4cc8cb16":"train_df['meter_reading'].plot(kind='hist',\n                            bins=25,\n                            figsize=(15, 5),\n                           title='Distribution of Target Variable (meter_reading)')\nplt.show()","1242c22f":"ax = np.log1p(train_df['meter_reading']).hist()\nax.set_yscale('log')\ntrain_df.meter_reading.describe()","d3fa8665":"# checking missing data\ntotal = train_df.isnull().sum().sort_values(ascending = False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","9308a0fc":"# Number of each type of column\ntrain_df.dtypes.value_counts()","99241170":"# Number of unique classes in each object column\ntrain_df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","aef06acf":"import missingno as msno","72494701":"msno.matrix(train_df.head(20000))","aaadb80a":"a = msno.heatmap(train_df, sort='ascending')","e8033026":"train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)\ntest_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)\n\ntrain_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)\ntest_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)\n\ntrain_df['age'] = train_df['age'].fillna(-999).astype(np.int16)\ntest_df['age'] = test_df['age'].fillna(-999).astype(np.int16)\n\ntrain_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)\ntest_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) ","5a093805":"energy_types_dict = {0: \"electricity\", 1: \"chilledwater\", 2: \"steam\", 3: \"hotwater\"}\nenergy_types      = ['electricity', 'chilledwater', 'steam', 'hotwater']\n\nplt.figure(figsize=(16,7))\ntmp_df = train_df.meter.value_counts()\ntmp_df.index = energy_types\ntmp_df.sort_values().plot(kind=\"barh\")\nplt.title(f\"Most readings measure electricity\")\nplt.xlabel(\"Count of measurements\")\nplt.ylabel(f\"Meter type\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5))\ntmp_df = train_df.groupby(\"meter\").meter_reading.sum()\ntmp_df.index = energy_types\ntmp_df.sort_values().plot(kind=\"barh\")\nplt.title(f\"Generating steam consumes most energy\")\nplt.xlabel(\"Sum of consumed energy\")\nplt.ylabel(f\"Type of energy\")\nplt.tight_layout()\nplt.show()","ff97c36d":"plt.figure(figsize=(16,5))\nsns.distplot(train_df.meter_reading, hist=False)\nplt.title(f\"Target variable meter_reading is highly skewed\")\nplt.ylabel(\"Count of readings\")\nplt.xlabel(f\"Measured consumption\")\nplt.xlim(0, train_df.meter_reading.max() + 100_000)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5))\nsns.distplot(np.log1p(train_df.meter_reading))\nplt.title(f\"After log transform, meter readings look more workable but still skewed\")\nplt.ylabel(\"Count of readings\")\nplt.xlabel(f\"Measured consumption\")\nplt.xlim(0, 12)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5))\nfor idx in range(0,4):\n    sns.distplot(np.log1p(train_df[train_df.meter==idx].meter_reading), hist=False, label=energy_types[idx])\nplt.title(f\"After log transform, distributions of energy types look comparably skewed\")\nplt.ylabel(\"Count of readings\")\nplt.xlabel(f\"Measured consumption\")\nplt.legend()\nplt.xlim(0, 12)\nplt.tight_layout()\nplt.show()","294df1fa":"plt.figure(figsize=(16,7))\n_ = stats.probplot(train_df['meter_reading'], plot=plt)\nplt.title(\"Probability plot for meter_reading shows extreme skewness\")\nplt.show()","2f71640d":"for bldg_id in [0, 1, 2, 2, 4,5, 6,7,8,9,10]:\n    plt.figure(figsize=(16,5))\n    tmp_df = train_df[train_df.building_id == bldg_id].copy()\n    tmp_df.set_index(\"timestamp\", inplace=True)\n    tmp_df.resample(\"D\").meter_reading.sum().plot()\n    plt.title(f\"Meter readings for building #{bldg_id} \")\n    plt.xlabel(\"Sum of readings\")\n    plt.tight_layout()\n    plt.show()","4c8cb330":"temp_df = train_df.groupby(\"primary_use\").meter_reading.sum().sort_values()\n\nplt.figure(figsize=(16,9))\ntemp_df.plot(kind=\"barh\")\nplt.title(f\"Education buildings consume by far most of energy\")\nplt.xlabel(\"Sum of readings\")\nplt.ylabel(f\"Primary use\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,9))\ntemp_df[:-1].plot(kind=\"barh\")\nplt.title(f\"Among other types, office buildings consume most energy\")\nplt.xlabel(\"Sum of readings\")\nplt.ylabel(f\"Primary use w\/o \u00abEducation\u00bb\")\nplt.tight_layout()\nplt.show()","aee43f83":"y_mean_time = train_df.groupby('timestamp').meter_reading.mean()\ny_mean_time.plot(figsize=(20, 8))","b2accad0":"y_mean_time.rolling(window=10).std().plot(figsize=(20, 8))\nplt.axhline(y=0.009, color='red')\nplt.axvspan(0, 905, color='green', alpha=0.1)\nplt.axvspan(906, 1505, color='red', alpha=0.1)\n","a09e709c":"daily_train = train_df\ndaily_train['date'] = daily_train['timestamp'].dt.date\ndaily_train = daily_train.groupby(['date', 'building_id', 'meter']).sum()\ndaily_train\n\ndaily_train_agg = daily_train.groupby(['date', 'meter']).agg(['sum', 'mean', 'idxmax', 'max'])\ndaily_train_agg = daily_train_agg.reset_index()\nlevel_0 = daily_train_agg.columns.droplevel(0)\nlevel_1 = daily_train_agg.columns.droplevel(1)\nlevel_0 = ['' if x == '' else '-' + x for x in level_0]\ndaily_train_agg.columns = level_1 + level_0\ndaily_train_agg.rename_axis(None, axis=1)\ndaily_train_agg.head()","382607e6":"daily_train_agg = daily_train.groupby(['date', 'meter']).agg(['sum', 'mean', 'idxmax', 'max'])\ndaily_train_agg = daily_train_agg.reset_index()\nlevel_0 = daily_train_agg.columns.droplevel(0)\nlevel_1 = daily_train_agg.columns.droplevel(1)\nlevel_0 = ['' if x == '' else '-' + x for x in level_0]\ndaily_train_agg.columns = level_1 + level_0\ndaily_train_agg.rename_axis(None, axis=1)\ndaily_train_agg.head()","d0a397f6":"fig_total = px.line(daily_train_agg, x='date', y='meter_reading-sum', color='meter', render_mode='svg')\nfig_total.update_layout(title='Total kWh per energy aspect')\nfig_total.show()","4b42ca0d":"fig_maximum = px.line(daily_train_agg, x='date', y='meter_reading-max', color='meter', render_mode='svg')\nfig_maximum.update_layout(title='Maximum kWh value per energy aspect')\nfig_maximum.show()","86d49dce":"daily_train_agg['building_id_max'] = [x[1] for x in daily_train_agg['meter_reading-idxmax']]\ndaily_train_agg.head()","71eabbf1":"del train_df[\"timestamp\"], test_df[\"timestamp\"]","2e991645":"categoricals = [\"site_id\", \"building_id\", \"primary_use\",  \"meter\",  \"wind_direction\"] #\"hour\", \"weekday\",","751f069d":"drop_cols = [\"sea_level_pressure\", \"wind_speed\"]\n\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\", 'precip_depth_1_hr', 'floor_count', 'beaufort_scale']\n\nfeat_cols = categoricals + numericals","86014f1e":"target = np.log1p(train_df[\"meter_reading\"])\n\ndel train_df[\"meter_reading\"] \n\ntrain_df = train_df.drop(drop_cols, axis = 1)","98b9d96a":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'subsample_freq': 1,\n            'learning_rate': 0.3,\n            'bagging_freq': 5,\n            'num_leaves': 330,\n            'feature_fraction': 0.9,\n            'lambda_l1': 1,  \n            'lambda_l2': 1\n            }\n\nfolds = 5\nseed = 666\nshuffle = False\nkf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n\nmodels = []\nfor train_index, val_index in kf.split(train_df[feat_cols], train_df['building_id']):\n    train_X = train_df[feat_cols].iloc[train_index]\n    val_X = train_df[feat_cols].iloc[val_index]\n    train_y = target.iloc[train_index]\n    val_y = target.iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=(lgb_train, lgb_eval),\n                early_stopping_rounds=50,\n                verbose_eval = 50)\n    models.append(gbm)","0e84449a":"import gc\ndel train_df #, train_X, val_X, lgb_train, lgb_eval, train_y, val_y, target\ngc.collect()","6bf0e6da":"test_df = test_df[feat_cols]","fac4632e":"i=0\nres=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df.shape[0]\/50000)))):\n    res.append(np.expm1(sum([model.predict(test_df.iloc[i:i+step_size]) for model in models])\/folds))\n    i+=step_size","4b55966a":"res = np.concatenate(res)","a5625c08":"sample_submission['meter_reading'] = res\nsample_submission.loc[sample_submission['meter_reading']<0, 'meter_reading'] = 0\nsample_submission.to_csv('submission.csv', index=False)\n","cb318a7e":"sample_submission.head()","11cd603c":"HTML('<iframe width=\"829\" height=\"622\" src=\"https:\/\/www.youtube.com\/embed\/ABAR8TIwce4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","b65cb677":"**Heatmap**\n\nThe missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:","fb63cc3d":"# <a id='10'>10. Baseline<\/a>\n<a href='#1'>Top<\/a>","f0fae76d":"# <a id='6-1'>6.1 Examine the Distribution of the Target Column<\/a>\n<a href='#1'>Top<\/a>\n\n\nThe target is meter_reading - Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.","e334fb93":"### <a href='#0'> In this kernel I want to show a Exhaustive Analysis of ASHRAE - Great Energy Predictor III, with:<\/a>\n\n\n- <a href='#1'>1. Glimpse of Data<\/a>\n- <a href='#2'>2. Reducing Memory Size<\/a>\n- <a href='#3'>3. ASHRAE - Data minification<\/a>\n- <a href='#4'>4. Encoding Variables<\/a>\n- <a href='#5'>5. Feature Engineering<\/a>\n- <a href='#6'>6. Exploratory Data Analysis<\/a>\n    - <a href='#6-1'>6.1 Examine the Distribution of the Target Column<\/a>\n    - <a href='#6-2'>6.2 Examine Missing Values<\/a>\n- <a href='#7'>7. Handling missing values<\/a>\n- <a href='#8'>8. Outlier Analysis<\/a>\n    - <a href='#8-1'>8.1. Outlier Distribution<\/a>\n- <a href='#9'>9. Plotings<\/a>\n- <a href='#10'>10. Baseline<\/a>\n- <a href='#11'>11. Blend<\/a>\n- <a href='#12'>12. Submission<\/a>\n- <a href='#13'>13. ASHRAE Energy prediction - summary<\/a>\n\n","649d4fdd":"# \ud83d\udd13MEMORY USAGE AFTER COMPLETION:\n\nMem. usage decreased to : 289.19 Mb (53.1% reduction)\n\nMem. usage decreased to : 6.08 Mb (68.1% reduction)\n\nMem. usage decreased to : 0.03 Mb (60.3% reduction)","eb63e157":"## Datetime Features","133cad76":"Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as [LightGBM](http:\/\/https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n\nYou can see [this](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables):\n- ## Label Encoding:\n\nLabel encoding assigns each unique value to a different integer.\n![](https:\/\/raw.githubusercontent.com\/WillKoehrsen\/Machine-Learning-Projects\/master\/label_encoding.png)\n\nThis approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n\nThis assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.\n","166eea0d":"> #### train_df data","b4dd9aa7":"\n\n> - Convert timestamp \n> - Convert Strings to category\n","773cb54b":"> ### Meter readings for first 10 buildings [ 1,2,3,4,5,6,7,8,9,10]","118cf8aa":"## Read in Data \n\nFirst, we can list all the available data files. There are a total of 6 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 4 other files containing additional information about energy types based on historic usage rates and observed weather. . ","a3964563":"#### Still in progress","3e99d4b6":"# What is K-Fold Cross Validation?","2cf6e0b0":"- meter=0    **Eletricity**\n\n- meter=1    **Chilledwater**\n\n- meter=2    **Steam**\n\n- meter=3    **Hotwater**","171ea34a":"\n> Use can use train_df.pkl, test_df.pkl for FE, FS for your baseline_predict\n","bb807479":"# <a id='2'>2. Reducing Memory Size<\/a>\n<a href='#1'>Top<\/a>\n> It is necessary that after using this code, carefully check the output results for each column.","913c8baf":"> ## Building DF merge through concat \n","dec661f9":"# In This kernel [\ud83d\udd0cASHRAE -Start Here: A GENTLE Introduction](https:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction), We saw a gentle introduction of the data. \n","44915e8a":"Manually dealing with missing values will often improve model performance. \n\nOur approach we input fillNaN = -999 just for the 4 features with most missing values.\n","f220161a":"# <a id='8-1'>8.1  Outlier Distribution<\/a>\n<a href='#1'>Top<\/a>\n","e3eff202":"See this, [simple FE](https:\/\/www.kaggle.com\/isaienkov\/lightgbm-fe-1-19): ","8011c914":"## Imports\n\n> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. ","f09d5826":"> ### Target's log-log histogram:","99ae0a23":"### Points of the compass\nCompass points\n\nThe names of the compass point directions follow these rules:\n> #### 16-wind compass rose\n   - The eight half-winds are the direction points obtained by bisecting the angles between the principal winds. The half-winds are north-northeast (NNE), east-northeast (ENE), east-southeast (ESE), south-southeast (SSE), south-southwest (SSW), west-southwest (WSW), west-northwest (WNW) and north-northwest (NNW). The name of each half-wind is constructed by combining the names of the principal winds to either side, with the cardinal wind coming first and the intercardinal wind second.\n   - The eight principal winds and the eight half-winds together form the 16-wind compass rose, with each compass point at a \u200b22 1\u20442\u00b0 angle from its two neighbours.","6ef31cc4":"## ASHRAE : Lgbm Simple FE WITH PYTHON\n[Crisl\u00e2nio Mac\u00eado](https:\/\/medium.com\/sapere-aude-tech) -  December, 31th, 2019\n\nEDA \u26a1\ud83d\udd0cASHRAE : Gentle Introduction: [\u26a1\ud83d\udd0cStart Here: A Gentle Introduction](https:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction)\n\n----------\n----------","4ae2e0d1":"# <a id='8'>8.  Outlier Analysis<\/a>\n<a href='#1'>Top<\/a>","e2bf8c8f":"# <a id='11'>11. Submission<\/a>\n<a href='#1'>Top<\/a>","e7ffbc39":"**Nullity Matrix**\n\nThe msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion. \n","cbf80996":"# <a id='6'>6. Exploratory Data Analysis<\/a>\n<a href='#1'>Top<\/a>\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. ","d9edb420":"# <a id='5'>5. Feature Engineering<\/a>\n<a href='#1'>Top<\/a>","134f02e4":"## Column Types\n\nLet's look at the number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https:\/\/stats.stackexchange.com\/questions\/206\/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/what-are-categorical-discrete-and-continuous-variables\/) . ","396b0ded":"train_df.columns.values","c04f1b02":"\n<html>\n<body>\n\n<p><font size=\"4\" color=\"Gray\">If you find this kernel useful or interesting, please don't forget to upvote the kernel =)\n\n<\/body>\n<\/html>\n\n","b4828c29":"\n**K-Fold CV[](https:\/\/medium.com\/datadriveninvestor\/k-fold-cross-validation-6b8518070833)** is where a given data set is split into a K number of sections\/folds where each fold is used as a testing set at some point. Lets take the scenario of 5-Fold cross validation(K=5). Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.\n![5-Fold Cross Validation](https:\/\/miro.medium.com\/max\/1509\/1*IjKy-Zc9zVOHFzMw2GXaQw.png)","8dd90046":"> Modeling simple LGBM","0459725c":"\n<html>\n<body>\n\n<p><font size=\"3\" color=\"Gray\">\u26a1 Please, you can use parts of this notebook in your own scripts or kernels, no problem, but please give credit (for example link back to this, see this...)<\/font><\/p>\n\n<\/body>\n<\/html>\n\n","da49da63":"See this: [ASHRAE: EDA and Visualization (wip)](https:\/\/www.kaggle.com\/chmaxx\/ashrae-eda-and-visualization-wip) (upvote this !) Not only useful but also valuable","b7660b6e":"Package called missingno (https:\/\/github.com\/ResidentMario\/missingno)\n\n     !pip install quilt","b5587473":"> - Drop collumns","ab470481":"## \u26a1\ud83d\udd0cAggregate the data for buildings","182f8c94":"> ## General findings\n\n\nPublished Articles on Energy Consumtion Prediction\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113080#latest-665324\n\nGeo location:\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/115040#latest-667889\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/115698#latest-667385\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/115698#latest-667385\n\nOutliers:\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113254\n\nHolidays:\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113286\n\nMetric:\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113064#latest-663076\n\nMemory Storage:\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/112917#latest-663094\n\n- https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/114754#latest-660345\n\n","5143d2b6":"The sum for each energy aspect, shows some aberrant values.","ca98e9c3":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Blue\">Remember the upvote button is next to the fork button, and it's free too! ;)<\/font><\/p>\n<p><font size=\"4\" color=\"Purple\">Don't hesitate to give your suggestions in the comment section<\/font><\/p>\n\n<\/body>\n<\/html>\n","40c97fc1":"# <a id='6-2'>6.2 Examine Missing Values<\/a>\n<a href='#1'>Top<\/a>\n\n\nNext we can look at the number and percentage of missing values in each column. ","df844f14":"# Feature Selection\n     \n#### Find the optimal feature subset using an evaluation measure. The choice of evaluation metric distinguish the three main strategies of feature selection algorithms: the wrapper strategy, the filter strategy, and the embedded strategy.\n\n    Filter methods:\n        information gain\n        chi-square test\n        correlation coefficient\n        variance threshold\n    Wrapper methods:\n        recursive feature elimination\n        sequential feature selection algorithms\n    Embedded methods:\n        L1 (LASSO) regularization\n        decision tree\n        \nIn our case, we remove some useless, redundant variables. ","2c2038a4":"# <a id='13'>13. ASHRAE Energy prediction - summary<\/a>\n<a href='#1'>Top<\/a>\n","6a37eb64":"sample_submission.head()","c04c7260":"# <a id='7'>7. Handling missing values<\/a>\n<a href='#1'>Top<\/a>","98dbd57b":"[ASHAE Outliers: by juanmah](https:\/\/www.kaggle.com\/juanmah\/ashrae-outliers) (upvoted this)","48ed77cf":"# <a id='4'>4. Encoding Variables<\/a>\n<a href='#1'>Top<\/a>","4d41d8bc":"A period of baseline energy consumption is used to create a machine learning prediction model to\nevaluate how much energy a building would use in a status quo baseline mode. An energy conservation measure (ECM) is installed and the difference between the baseline is the avoided energy consumption or demand. This process is crucial for the implementation of energy savings implementations as it gives building owners, designers and contractors a means of evaluating the success of such measures.\n\nFor further details, I recommend reading [this paper by Clayton Miller ](https:\/\/www.mdpi.com\/2504-4990\/1\/3\/56)\n\n![](https:\/\/www.mdpi.com\/make\/make-01-00056\/article_deploy\/html\/images\/make-01-00056-g001-550.jpg)","5d55db4c":"![image](https:\/\/www.shell.com\/energy-and-innovation\/the-energy-future\/scenarios\/shell-scenario-sky\/_jcr_content\/pageCarouselImage.img.960.jpeg\/1521983923121\/future-of-energy-homepage-r.jpeg?imwidth=960)\n\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Blue\">\u26a1\ud83d\udd0cASHRAE - LigthGBM simple FE <\/p>\n\n<\/body>\n<\/html>\n\n","7904482e":"# <a id='3'>3. ASHRAE - Data minification<\/a>\n<a href='#1'>Top<\/a>","95651ec6":"# Final","88ada1e1":"> ## Weather DF merge over concat\n","f9f3c478":"<html>\n<body>\n\n<p><font size=\"4\" color=\"Blue\"> ASHRAE 90.1-2016, Energy Standard for Buildings - Review of Changes<\/font><\/p>\n<\/body>\n<\/html>\n","2db8a485":"<html>\n<body>\n\n<p><font size=\"4\" color=\"Blue\"> ASHRAE Standard 90.1 2010, Part III -- HVAC Provisions<\/font><\/p>\n\n<\/body>\n<\/html>\n","2b105e1a":"# Some 'Anomalies' - max meter reading","066e2bbc":"# <a id='1'>1. Glimpse of Data<\/a>\n<a href='#0'>Top<\/a>"}}