{"cell_type":{"fe2de775":"code","61bbc0ad":"code","1150e54c":"code","4f4167ca":"code","f23bcc05":"code","db2f52f8":"code","c7a22ebf":"code","4295a282":"code","015f7ab4":"code","b8315281":"code","28be267a":"code","375493b9":"code","67555fd8":"code","ec157f6a":"code","f32d7529":"code","0bbe7474":"code","412bd93b":"code","f2abdcfb":"code","753cf8dd":"code","2eb9d813":"code","59def162":"code","3226a80b":"code","4a8f2cab":"code","ef652ad3":"code","35eee290":"code","6c0743a1":"code","5d5fa327":"code","9e1cd920":"code","7ad64056":"code","de11b866":"code","cce0b8d3":"markdown","7d1bcffd":"markdown","f50a790c":"markdown","ac46a658":"markdown","51a87569":"markdown","7f3320ef":"markdown","f98f14da":"markdown","7219a6a0":"markdown","68fd91ad":"markdown","24bc73d6":"markdown","0aba1e1e":"markdown"},"source":{"fe2de775":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61bbc0ad":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import tokenize,stem\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nimport lightgbm as lgb\nimport nltk\nfrom nltk.util import ngrams\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud, STOPWORDS\nimport shap\nshap.initjs()","1150e54c":"df = pd.read_csv(\"\/kaggle\/input\/fruit-consumer-survey\/Fruit_Consumer_Survey.csv\")\ndf.head()","4f4167ca":"df.isnull().sum()","f23bcc05":"df.rename(columns={'Age range':'age_range', 'Retail Brand':'retail_brand', 'Online channel':'online_channel'}, inplace=True)\ndf.head(3)","db2f52f8":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\ngender_cnt = np.round(df['Gender'].value_counts(normalize=True) * 100)\nhv.Bars(gender_cnt).opts(title=\"Gender Count\", color=\"green\", xlabel=\"Gender\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","c7a22ebf":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nFrequence_cnt = np.round(df['Frequence'].value_counts(normalize=True) * 100)\nhv.Bars(Frequence_cnt).opts(title=\"Frequence Count\", color=\"green\", xlabel=\"Frequencies\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n            * hv.Text('Twice a week', 15, f\"{int(Frequence_cnt.loc['Twice a week'])}%\")\\\n            * hv.Text('Not regularly', 15, f\"{int(Frequence_cnt.loc['Not regularly'])}%\")\\\n            * hv.Text('Once a week', 15, f\"{int(Frequence_cnt.loc['Once a week'])}%\")\\\n            * hv.Text('Everyday', 15, f\"{int(Frequence_cnt.loc['Everyday'])}%\")\\\n            * hv.Text('Twice a month', 15, f\"{int(Frequence_cnt.loc['Twice a month'])}%\")\\\n            * hv.Text('Other', 15, f\"{int(Frequence_cnt.loc['Other'])}%\")","4295a282":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\narea_cnt = np.round(df['Area'].value_counts(normalize=True) * 100)\nhv.Bars(area_cnt).opts(title=\"Area Count\", color=\"green\", xlabel=\"Locals\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True))","015f7ab4":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nAREA_cnt = np.round(df['Area'].value_counts(normalize=True) * 100)\nhv.Bars(AREA_cnt).opts(title=\"Area Percents\", color=\"yellow\", xlabel=\"Areas\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n            * hv.Text('Northwest', 15, f\"{int(AREA_cnt.loc['Northwest'])}%\")\\\n            * hv.Text('North', 15, f\"{int(AREA_cnt.loc['North'])}%\")\\\n            * hv.Text('Northeast', 15, f\"{int(AREA_cnt.loc['Northeast'])}%\")\\\n            * hv.Text('Southwest', 15, f\"{int(AREA_cnt.loc['Southwest'])}%\")\\\n            * hv.Text('South', 15, f\"{int(AREA_cnt.loc['South'])}%\")\\\n            * hv.Text('East', 15, f\"{int(AREA_cnt.loc['East'])}%\")","b8315281":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nage_cnt = np.round(df['age_range'].value_counts(normalize=True) * 100)\nhv.Bars(age_cnt).opts(title=\"Age Count\", color=\"green\", xlabel=\"Locals\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True))","28be267a":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nvalue_cnt = np.round(df['Value'].value_counts(normalize=True) * 100)\nhv.Bars(value_cnt).opts(title=\"Value Count\", color=\"red\", xlabel=\"Values\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True)).opts(xrotation=45)","375493b9":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nonline_cnt = np.round(df['online_channel'].value_counts(normalize=True) * 100)\nhv.Bars(online_cnt[::-1]).opts(title=\"Online Channel Count\", color=\"cyan\", xlabel=\"Online Channel\", ylabel=\"Percentage\", xformatter='%d%%')\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","67555fd8":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nsuper_cnt = np.round(df['Supermarket'].value_counts(normalize=True) * 100)\nhv.Bars(super_cnt[::-1]).opts(title=\"Supermarket Count\", color=\"purple\", xlabel=\"Supermarket\", ylabel=\"Percentage\", xformatter='%d%%')\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","ec157f6a":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\n#function to calculate ngram under several conditions\n\ndef ngram_func(ngram, trg='', trg_value=''):\n    #trg_value is list-object\n    if (trg == '') or (trg_value == ''):\n        string_filterd =  df['retail_brand'].sum().split()\n    else:\n        string_filterd =  df[df[trg].isin(trg_value)]['retail_brand'].sum().split()\n    dic = nltk.FreqDist(nltk.ngrams(string_filterd, ngram)).most_common(30)\n    ngram_df = pd.DataFrame(dic, columns=['ngram','count'])\n    ngram_df.index = [' '.join(i) for i in ngram_df.ngram]\n    ngram_df.drop('ngram',axis=1, inplace=True)\n    return ngram_df","f32d7529":"#Code by Kohei-mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\nhv.Bars(ngram_func(1)[::-1]).opts(title=\"Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","0bbe7474":"hv.Bars(ngram_func(2)[::-1]).opts(title=\"Bigram Count top-30\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","412bd93b":"hv.Bars(ngram_func(3)[::-1]).opts(title=\"Trigram Count top-30\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","f2abdcfb":"uni_ma=hv.Bars(ngram_func(1, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Unigram with Male\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nuni_fe=hv.Bars(ngram_func(1, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Unigram with Female\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\n\nbi_ma=hv.Bars(ngram_func(2, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Bigram with Male\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\nbi_fe=hv.Bars(ngram_func(2, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Bigram with Female\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\n\ntri_ma=hv.Bars(ngram_func(3, 'Gender', ['Male'])[0:15][::-1]).opts(title=\"Trigram with Male\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\ntri_fe=hv.Bars(ngram_func(3, 'Gender', ['Female'])[0:15][::-1]).opts(title=\"Trigram with Female\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n                \n\n(uni_ma + uni_fe + bi_ma + bi_fe + tri_ma + tri_fe).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False).cols(2)","753cf8dd":"\nwordcloud = WordCloud(width = 1500, height = 800, random_state=0, background_color='black', colormap='rainbow', \\\n                      min_font_size=5, max_words=300, collocations=False, min_word_length=3, stopwords = STOPWORDS).generate(\" \".join(df['retail_brand'].values))\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","2eb9d813":"feature_df = pd.DataFrame()\nfor i in [1,2,3]:\n    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))\n    X = vec_tfidf.fit_transform(df['retail_brand']).toarray()\n    tfs = pd.DataFrame(X, columns=[\"TFIDF_\" + n for n in vec_tfidf.get_feature_names()])\n    feature_df = pd.concat([feature_df, tfs], axis=1)\nfeature_df = pd.concat([df, feature_df], axis=1)\nfeature_df.head(3)","59def162":"feature_df['age_range'] = LabelEncoder().fit_transform(feature_df['age_range']).astype(np.int8)\nfeature_df['Gender'] = LabelEncoder().fit_transform(feature_df['Gender']).astype(np.int8)\nfeature_df['Area'] = LabelEncoder().fit_transform(feature_df['Area']).astype(np.int8)\nfeature_df['Frequence'] = LabelEncoder().fit_transform(feature_df['Frequence']).astype(np.int8)\nfeature_df['Place'] = LabelEncoder().fit_transform(feature_df['Place']).astype(np.int8)\nfeature_df['Value'] = LabelEncoder().fit_transform(feature_df['Value']).astype(np.int8)\nfeature_df['Type'] = LabelEncoder().fit_transform(feature_df['Type']).astype(np.int8)\nfeature_df['retail_brand'] = LabelEncoder().fit_transform(feature_df['retail_brand']).astype(np.int8)\nfeature_df['Supermarket'] = LabelEncoder().fit_transform(feature_df['Supermarket']).astype(np.int8)\nfeature_df['online_channel'] = LabelEncoder().fit_transform(feature_df['online_channel']).astype(np.int8)\nfeature_df.drop(['Id','retail_brand'],axis=1,inplace=True)\nfeature_df.head(3)","3226a80b":"y_series = feature_df['Gender']\nx_df = feature_df.drop(['Gender', 'online_channel'], axis=1) \nX_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y_series, test_size=0.2, random_state=0 )\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_valid = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)","4a8f2cab":"params = {\n    'task' : 'train',\n    'boosting' : 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 5,\n    'metric': 'multi_logloss',\n    'num_leaves': 200,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 5\n}\ngbm_ac = lgb.train(params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=lgb_valid,\n            early_stopping_rounds=100)","ef652ad3":"df[\"online_channel\"].value_counts()","35eee290":"ac_label = ['JD,Tmall', 'Tmall,Pinduoduo','JD', 'Meituan', 'None or Other', 'JD,Pinduoduo', 'Tmall,Suning', 'JD,Suning', 'JD,Meituan', 'Tmall', 'Pinduoduo', 'Pinduoduo,Meituan', 'Meituan,None or Other']\nexplainer = shap.TreeExplainer(model=gbm_ac)\nshap_values_ac = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_ac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=30, class_names=ac_label)","6c0743a1":"t = lgb.plot_tree(gbm_ac, figsize=(20, 20), precision=1, tree_index=0, show_info=['split_gain'])\nplt.title('Visulalization of Tree in Online Channel')\nplt.show()","5d5fa327":"f1 = lambda x : np.round(x\/len(df) * 100)\ngender_cnt = df.groupby(['Gender'])['age_range'].count().apply(f1)\ng = hv.Bars(pd.melt(gender_cnt.reset_index(), ['Gender']), ['Gender'], 'value').opts(opts.Bars(title=\"Gender Count\", color='green'))\n\nf2 = lambda x : np.round(x\/x.sum() * 100)\nac_gen = df.groupby(['Gender','age_range'])['age_range'].count().unstack().apply(f2, axis=1)\nac = hv.Bars(pd.melt(ac_gen.reset_index(), ['Gender']), ['Gender','age_range'], 'value').opts(opts.Bars(title=\"Age Range by Gender Count\"))\n\n(g + ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=45, ylabel=\"Percentage\", yformatter='%d%%', shared_axes=True))","9e1cd920":"uni=hv.Bars(ngram_func(1)[0:15][::-1]).opts(title=\"Unigram Count\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi=hv.Bars(ngram_func(2)[0:15][::-1]).opts(title=\"Bigram Count\", color=\"yellow\", xlabel=\"Bigrams\", ylabel=\"Count\")\ntri=hv.Bars(ngram_func(3)[0:15][::-1]).opts(title=\"Trigram Count\", color=\"blue\", xlabel=\"Trigrams\", ylabel=\"Count\")\n(uni + bi + tri).opts(opts.Bars(width=265, height=300,tools=['hover'],show_grid=True,invert_axes=True, shared_axes=False)).opts(shared_axes=False)","7ad64056":"shap.summary_plot(shap_values=shap_values_ac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=15, class_names=ac_label)\n#shap.summary_plot(shap_values=shap_values_pac, features=X_train, feature_names=X_train.columns, plot_type=\"bar\", max_display=15, class_names=pac_label)","de11b866":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Kohei-Mu, @koheimuramatsu for the code.' )","cce0b8d3":"#Code by Kohei-Mu https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\/notebook\n\n#Thanks to Des @desalegngeb for helping me with .opts(xrotation=45) The xticks Bokeh to avoid clumsy X axis. ","7d1bcffd":"#Train test split  had stratify =y\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y_series, test_size=0.2, random_state=0, stratify=y_series)\n\nI removed it to avoid error \"The least populated in y has only 1 member.","f50a790c":"#I had to change the features to avoid Error: \"label must be in [0,num_class] ","ac46a658":"#Pay attention to xrotation on the last line below. On the original code it was xrotation=0 since it was not clumsy. ","51a87569":"#Label Encoding","7f3320ef":"#Appling opts(xrotation=45)  the equivalent to xticks in Bokeh?","f98f14da":"![](https:\/\/www.fullstackpython.com\/img\/logos\/bokeh.jpg)fullstackpython","7219a6a0":"#Thanks to Des @desalegngeb  I found out how to make xticks in Bokeh\n\n(http:\/\/holoviews.org\/user_guide\/Customizing_Plots.html).","68fd91ad":"#I had to change Male\/Female to the online channels to avoid Error: list out of range. ","24bc73d6":"#out-of-focus blur\n\nBokeh is defined as \u201cthe effect of a soft out-of-focus background that you get when shooting a subject, using a fast lens, at the widest aperture, such as f\/2.8 or wider.\u201d Simply put, bokeh is the pleasing or aesthetic quality of out-of-focus blur in a photograph.\n\nhttps:\/\/www.nikonusa.com\/en\/learn-and-explore\/a\/tips-and-techniques\/bokeh-for-beginners.html","0aba1e1e":"#Feature Engineering"}}