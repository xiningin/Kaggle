{"cell_type":{"29d5c270":"code","3b4fec56":"code","ab8b5680":"code","50e085c1":"code","23dd0d28":"code","f0d4d4f5":"code","ca0d630e":"code","90509ce8":"code","a9b33dd1":"code","840d79e6":"code","d66fa911":"code","d1df1a5f":"code","4674f17c":"code","92e88b97":"code","c380c3c3":"code","15796010":"code","f479cdaa":"code","17e9f531":"code","116e61e7":"code","db4924d1":"code","31d7e555":"code","9b731d7d":"code","bd9136e8":"code","121cc7a9":"code","85634885":"code","41e6f613":"code","fc1bb6c1":"code","402294ea":"code","8dfed0c5":"code","367078ec":"code","b708cc73":"code","98711bda":"code","4e54f68e":"code","67313f5e":"code","5048ecc2":"code","302df545":"code","53e998ea":"code","10584629":"code","e59be404":"code","737d213c":"code","fbcd7508":"code","4003ec6a":"code","89408ba9":"code","bb374ce8":"code","67613c17":"code","c189f62d":"code","51f1b67c":"code","7874ee2a":"code","336b31af":"code","c71fd8a5":"code","b23fbb0a":"code","d6ed800f":"code","27f2fd94":"code","9c4d502e":"code","1a13b1d6":"code","4e469977":"code","f8dd22c5":"code","ab720b4d":"code","41cbf55f":"code","2af75155":"code","01d04c1d":"code","c0b4a2b7":"code","8403c439":"markdown","b1330b75":"markdown","0b4ee2e8":"markdown","3d67f786":"markdown","c1bd2a3b":"markdown","ca217f6e":"markdown","15b1e1fc":"markdown","47b0ac60":"markdown","f476b590":"markdown","19221fa5":"markdown","f4f4664f":"markdown","93470c68":"markdown","eb98721d":"markdown","51b2b400":"markdown","d88019b8":"markdown","23908ae0":"markdown","a44fd8ad":"markdown","d60b68bc":"markdown","33d53fac":"markdown","fadc588f":"markdown","f5a33fbb":"markdown","4d63ac0c":"markdown","856dc8b4":"markdown","9e565f6f":"markdown","830744e9":"markdown","ce521479":"markdown","1fe8ccda":"markdown","7c9268a9":"markdown","f7512fc5":"markdown","bf4cdef4":"markdown","5dacdb03":"markdown","4a5eb285":"markdown","382a78fa":"markdown","3896e3e0":"markdown","0e03b3ca":"markdown","72389b8b":"markdown","6c3c08b0":"markdown","0c48c424":"markdown","1971b5ad":"markdown","069eef39":"markdown"},"source":{"29d5c270":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","3b4fec56":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv')","ab8b5680":"train_df.head()","50e085c1":"test_df.head()","23dd0d28":"print('shape of training data',train_df.shape)\nprint('shape of test data',test_df.shape)","f0d4d4f5":"train_df.columns","ca0d630e":"train_df.describe()","90509ce8":"test_df.describe()","a9b33dd1":"train_df.info() #ticket","840d79e6":"train_df_without_survival=train_df.drop('Survived',axis=1)\ndf=pd.concat([train_df_without_survival,test_df],ignore_index=True)\ndf.shape","d66fa911":"df.head()","d1df1a5f":"#missing values\nprint(train_df.isnull().sum())\nprint(\"-\"*20)\nprint(test_df.isnull().sum())","4674f17c":"df.drop('Cabin',axis=1,inplace=True)\ndf.shape[1]","92e88b97":"df['Fare'].fillna(df['Fare'].mean(),inplace=True)\ndf.isnull().sum()","c380c3c3":"df['Embarked'].value_counts()","15796010":"df['Embarked'].fillna('S',inplace=True)\ndf.isnull().sum()","f479cdaa":"df[['Age','Sex','Pclass','Embarked']].groupby(['Sex','Pclass','Embarked']).agg(['std','median'])","17e9f531":"#to calculate the missing value of age\ndf[['Age','Sex','Pclass']].groupby(['Sex','Pclass']).agg(['std','median'])","116e61e7":"grouped=df[['Age','Sex','Pclass']].groupby(['Sex','Pclass']).median()\n\n\nfor i in df[df['Age'].isnull()].index:\n    loop_sex=df.loc[i,'Sex']\n    loop_pclass=df.loc[i,'Pclass']\n    corr_of_pclass=grouped.loc[str(loop_sex),:]\n    approx_age=corr_of_pclass.loc[int(loop_pclass),:]\n    df.loc[i,'Age']=float(approx_age)\ndf['Age'].isnull().sum()","db4924d1":"#checking the freatures of the dataset by corelating with target\ncor_pclass=train_df[['Pclass','Survived']].groupby('Pclass').mean()\ncor_pclass","31d7e555":"cor_pclass.plot(kind='barh',color='gold')","9b731d7d":"corr_sex=train_df[['Sex','Survived']].groupby('Sex').mean()\ncorr_sex","bd9136e8":"corr_sex.plot(kind='barh',color='lightgreen')","121cc7a9":"corr_embarked=train_df[['Embarked','Survived']].groupby('Embarked').mean()\ncorr_embarked","85634885":"corr_embarked.plot(kind='barh',color='orange')","41e6f613":" #to find the relation between several continuous features and target variable\ncont_columns=['Age','SibSp','Parch','Fare']\ncorelation_df=train_df[cont_columns+['Survived']]\ncorelation_df.corr()['Survived']\n#none of the features provide a significant corelation ","fc1bb6c1":"\n\n#turning out age to a categorical variable\ntrain_df['Age_Categ']=pd.cut(train_df['Age'],8)\ntrain_df[['Age_Categ','Survived']].groupby('Age_Categ').agg(['sum','count','mean'])","402294ea":"df['Age_Categ']=pd.cut(df['Age'],8)\ndf1=df['Age_Categ'].astype('str')\ndf['Age_Categ']=df1.map({'(0.0902, 10.149]': 1,\n        '(10.149, 20.128]': 2,\n        '(20.128, 30.106]': 2,\n        '(30.106, 40.085]': 3,\n        '(40.085, 50.064]': 3,\n        '(50.064, 60.043]': 3,\n        '(60.043, 70.021]': 4,\n        '(70.021, 80.0]': 4}).astype('category')\ntrain_df['age_categ']=df.loc[:891,'Age_Categ']","8dfed0c5":"df['Age_Categ']","367078ec":"corr_age=train_df[['age_categ','Survived']].groupby('age_categ').mean()\ncorr_age","b708cc73":"corr_age.plot(kind='bar',color='r')","98711bda":"#getting title from names\ndf['Name'].head()","4e54f68e":"title=df[['Name','Ticket']].set_index('Name')\nfor i in title.index:\n    rev=str(i)[::-1]\n    tit=rev[rev.index('.')+1:rev[rev.index('.'):].index(' ')+rev.index('.')]\n    tit=tit[::-1]\n    title.loc[i,'Ticket']=tit\ntitle.reset_index(inplace=True)\ndf['Title']=title['Ticket']","67313f5e":"df['Title'].value_counts()","5048ecc2":"df['Title'].replace(['Dr','Rev','Col','Major','Ms','Mlle','Lady','Sir','Mme','Countess','L',\n                     'Jonkheer','Don','Dona','Capt'],'other',inplace=True)","302df545":"train_df['Title']=df.loc[:891,'Title']\ntrain_df[['Title','Survived']].groupby('Title').agg(['mean','count'])","53e998ea":"corr_title=train_df[['Title','Survived']].groupby('Title').mean()\ncorr_title.plot(kind='bar',color='green')","10584629":"df['Title']=df['Title'].map({'Master':1,\n                'Miss':2,\n                'Mr':3,\n                'Mrs':4,\n                'other':5})","e59be404":"#using parch and sibsp to find whether the person is single or not\ndf['Family_Size']=df[['Parch','SibSp']].sum(axis=1)\ntrain_df['Family_Size']=df.loc[:891,'Family_Size']\ntrain_df[['Family_Size','Survived']].groupby('Family_Size').agg(['count','mean'])","737d213c":"def singleornot(famsize):\n    if famsize==0:\n        return 1\n    else:\n        return 0\ndf['Single']=df['Family_Size'].apply(singleornot)\ntrain_df['Single']=df.loc[:891,'Single']\ntrain_df[['Single','Survived']].groupby('Single').agg(['mean','count'])","fbcd7508":"corr_single=train_df[['Single','Survived']].groupby('Single').mean()\ncorr_single.plot(kind='bar',color='blue')","4003ec6a":"df['Fare_Categ']=pd.cut(df['Fare'],20)\ntrain_df['Fare_Categ']=df.loc[:891,'Fare_Categ']\ntrain_df[['Fare_Categ','Survived']].groupby('Fare_Categ').agg(['mean','count'])","89408ba9":"df['Fare_Categ']=df['Fare_Categ'].astype(str).map({'(-0.512, 25.616]':1,\n                                 '(25.616, 51.233]':2,\n                                 '(51.233, 76.849]':2,\n                                 '(76.849, 102.466]':3,\n                                 '(102.466, 128.082]':3,\n                                 '(128.082, 153.699]':3,\n                                 '(153.699, 179.315]':3,\n                                 '(179.315, 204.932]':3,\n                                 '(204.932, 230.548]':3,\n                                 '(230.548, 256.165]':3,\n                                 '(256.165, 281.781]':3,\n                                 '(281.781, 307.398]':3,\n                                 '(307.398, 333.014]':3,\n                                 '(333.014, 358.63]':3,\n                                 '(358.63, 384.247]':3,\n                                 '(384.247, 409.863]':3,\n                                 '(409.863, 435.48]':3,\n                                 '(435.48, 461.096]':3,\n                                 '(461.096, 486.713]':3,\n                                 '(486.713, 512.329]':3})\n","bb374ce8":"train_df['Fare_Categ']=df.loc[:891,'Fare_Categ']\ntrain_df[['Fare_Categ','Survived']].groupby('Fare_Categ').agg(['count','mean'])","67613c17":"corr_fare=train_df[['Fare_Categ','Survived']].groupby('Fare_Categ').mean()\ncorr_fare.plot(kind='bar',color='black')","c189f62d":"#ticket\ndf.drop('Ticket',axis=1,inplace=True)","51f1b67c":"#changing the categoricals sex and embarked to numericals \ndf['Sex']=df['Sex'].map({'male':0,'female':1})","7874ee2a":"df['Embarked']=df['Embarked'].map({'C':1,'Q':2,'S':3})","336b31af":"#droping the unwanted features\n\ndf.drop(['PassengerId','Name','Age','SibSp','Parch','Fare','Family_Size'],axis=1,inplace=True)","c71fd8a5":"final_testing_df=df[891:]\nfinal_training_x=df[:891]\nfinal_training_y=train_df.loc[:891,'Survived']\nfinal_training_df=pd.concat([df[:891],train_df['Survived']],axis=1)","b23fbb0a":"final_training_df","d6ed800f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,mean_absolute_error\nx_train,x_test,y_train,y_test=train_test_split(final_training_x,final_training_y,test_size=0.3)\n","27f2fd94":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlog_model=LogisticRegression()\nlog_model.fit(x_train,y_train)\nlog_yhat=log_model.predict(x_test)\nlog_acc=accuracy_score(log_yhat,y_test)\nprint('accuracy score',log_acc)","9c4d502e":"from sklearn.tree import DecisionTreeClassifier\ntree_model=DecisionTreeClassifier(max_depth=5)\ntree_model.fit(x_train,y_train)\ntree_yhat=tree_model.predict(x_test)\ntree_acc=accuracy_score(tree_yhat,y_test)\nprint('accuracy score',tree_acc)","1a13b1d6":"from sklearn.ensemble import RandomForestClassifier\nforest_model=RandomForestClassifier(n_estimators=100,max_depth=5)\nforest_model.fit(x_train,y_train)\nforest_yhat=forest_model.predict(x_test)\nforest_acc=accuracy_score(forest_yhat,y_test)\nprint('accuracy score',forest_acc)\n\n\n","4e469977":"from sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier(n_neighbors=8)\nknn_model.fit(x_train,y_train)\nknn_yhat=knn_model.predict(x_test)\nknn_acc=accuracy_score(knn_yhat,y_test)\nprint('accuracy score',knn_acc)","f8dd22c5":"from sklearn.naive_bayes import GaussianNB\nnaive_model=GaussianNB()\nnaive_model.fit(x_train,y_train)\nnaive_yhat=naive_model.predict(x_test)\nnaive_acc=accuracy_score(naive_yhat,y_test)\nprint('accuracy score',naive_acc)","ab720b4d":"from sklearn.svm import SVC\nsvc_model=SVC()\nsvc_model.fit(x_train,y_train)\nsvc_yhat=svc_model.predict(x_test)\nsvc_acc=accuracy_score(svc_yhat,y_test)\nprint('accuracy score',svc_acc)","41cbf55f":"#scochastic gradient descent\nfrom sklearn.linear_model import SGDClassifier\nsgd_model=SGDClassifier()\nsgd_model.fit(x_train,y_train)\nsgd_yhat=sgd_model.predict(x_test)\nsgd_acc=accuracy_score(sgd_yhat,y_test)\nprint('accuracy score',sgd_acc)","2af75155":"best_algorithm=pd.DataFrame({'algorithm':['LogisticRegression','DecisionTree','RandomForest','KNeighbors',\n                                          'GaussianNB','svm','SGD'],\n                             'accuracy':[log_acc,tree_acc,forest_acc,knn_acc,naive_acc,svc_acc,sgd_acc]})\nbest_algorithm","01d04c1d":"#decision tree is best to classify the dataset\nmodel=RandomForestClassifier(n_estimators=100,max_depth=5)\nmodel.fit(final_training_x,final_training_y)\nprediction=model.predict(final_testing_df)\nprediction","c0b4a2b7":"final_df=pd.DataFrame({'PassengerId':final_testing_df.index +1,'Survived':prediction},index=None)\n\nfinal_df.set_index('PassengerId',inplace=True)\nfinal_df.to_csv('.\/titanic_submission.csv')","8403c439":"***\n<h3> <font color='fluoroscent'>Concating test and train <\/font><\/h3>\n  <font color='grey'>  The two dataframes are concatenated to a single data frame ,so that every change that is made on training data is also applied on test data. this saves time to apply the same code on test data. <\/font><br> <br<\nNote : - Everytime while changing the data,we must do it with 'df' DataFrame,while analyzing the data,we must use 'train_df' DataFrame","b1330b75":"****\n### Correlating Features of the dataset to find the best and worst Features\n#### Corelating Categorical Features\n_While dealing with Categorical Features, if the Survival rate between several Categories significantly differ,then the Feature imporves our model!_","0b4ee2e8":"The above Features show only weak correlations, showing these features are not best for modeling.<br>Therefore Feature Engineering has to be done to improve these features ","3d67f786":"### Calculating the Missing Values\n- missing values can be filled with average,median,mode of the columns\n- taking the most reccurring variable and filling in the missing columns\n- considering the other most corelated features to the missing feature and the choosing the missing value.\n\nThird method will be more sophesticated.<br>\nThe most corelated features to 'Age' are \"Sex' ,'Embarked','Pclass' .<br>\nwhile adding 'Embarked' standard deviation of age for every 'Sex' and 'Pclass' increases, so 'Embarked is not added.","c1bd2a3b":"***\n_Importing the nessesary libraries_","ca217f6e":"<h1 align='center'> Survival Prediction of Titanic<\/h1>\n","15b1e1fc":"### Droping the 'Cabin' Column","47b0ac60":"***\n<h2 align='center'>Modeling<\/h2>\n","f476b590":"Replacing the Rare titles into a single category","19221fa5":"Using Family size of a passenger instead of Parch or Sipsp to improve the Correlation.\nIt is more logical, if a person has a family in Titanic,he has more Chance of Survival.","f4f4664f":"### Accuracy of Several Algorithms","93470c68":"_Splitting the data into Training and testing data ,whose accuracy will help to improve the model._","eb98721d":"#### Corelating Continuous Features with Survived Column\n<br>\n\nNote :   The corr() Function finds the linear Corelation between features and target. whenever the Corelation  coeffecient is near to 1,the feature has a strong positive correlation eith Target , and whenever it is near -1 it shows strong negative Correlation. If correlation is near zero, there is no or weak Correlation.\n","51b2b400":"Decision Tree and Random Forest are the highest accuracy models,\nI have Selected Random Forest [parameters are tuned on Trial and error Basis]","d88019b8":" With the help of 'Name' Feature ,we can create a new Feature the Title of the passenger which may increase the survival rate of a passenger","23908ae0":"\n***\n### Converting the solution to a csv file","a44fd8ad":"***\n<h1 align ='center'>Thank You <\/h1>","d60b68bc":"***\n### Converting Fare to a Categorical Feature\nFirst I have Converted Fare to 20 Categories(by trial and Error) to get significantly differable survival rates and then joined similar Categories to finally make 3 Categories:\n  - paid less than 25 - [low survival rate]\n  - paid 25 to 75 - [medium survival rate]\n  - paid greater than 75 - [very high survival rate]<br>\nAs expected Highly paid Passengers had more chance of Survival","33d53fac":"_Corelating Embarked with Survived Column_","fadc588f":"This loop finds the 'Sex' and 'Pclass' of every missing 'Age' row and fills accordingly","f5a33fbb":"In these lines of code I have Extracted the title of the passenger which comes after a space character and ends before a fullstop character.","4d63ac0c":"_Missing values in a dataset implies it is a untidy data. those mising values can destroy the modelling and force into decreased accuracy. So the missing data need to be replaced._\n\n  - the rows or columns containing the missing values can be droped , but we may miss some valuable information from the droped columns.\n      \n  - the missing data can be filled with average, median values of the column,or use any other methods to fill the missing values.\n    ","856dc8b4":"***\n## Feature Engineering","9e565f6f":"Correlating Age with Survived Column\n","830744e9":"### Droping Ticket as it does not provide any relation to survival","ce521479":"Reading the files  train and test from the directory","1fe8ccda":"### Converting other Categoricals to numerical values for modeling","7c9268a9":"_Corelating Sex with Survived Column_","f7512fc5":"columns containing minimum null values like 'Fare' ,'Embarked' can be filled with average if its numerical or with most reccuring if it is Categorical.","bf4cdef4":"Lower the Standard Deviation , the more accurate the filled value will be.<br>\nSo every missing 'Age' will be filled with the median according to the row's 'Sex' and 'Pclass'\n","5dacdb03":"<h3> Finding on the missing values -<\/h3>","4a5eb285":"Since the Cabin Column consists of a large number of null values, it can be droped as it could not provide any valid information.","382a78fa":"### Removing all features other than nessesary (Feature Engineered) Features\nagain spliting up into training and testing data and adding 'Survived' Column to the training data ","3896e3e0":"A view of the Data and its properties","0e03b3ca":"Converting categories to Numericals","72389b8b":"Classification can be done using several models :\n   - Logistic Regression\n   - Decision Tree Classifier\n   - Random Forest Classifier\n   - K Neighbors Classifier\n   - Naive Bayes Classifier \n   - Support Vector Machines\n   - Gradient Descent Classifier","6c3c08b0":"This notebook is created by Gopinath K S on how to deal on Titanic Dataset on a beginer's view     <br>\nAny Queires, contact me anywhere!                                                            <br>\nkaggle Id -@gopinath15                                                                       <br>\nlinkedin Id -www.linkedin.com\/in\/gopinath-k-s-98a48a1b3                                      <br>","0c48c424":"'Title' can be considered as a Good Feature as the survival rates among several Categories differ notably.\nConverting each category to a numerical for modeling","1971b5ad":"Creating a new feature Single which denotes a person is single or not in Titanic","069eef39":"if Continuous Features do not provide much Corelation, continuous features can be converted to a categorical feature,to provide better Correlation.<br><br><br>\nHear I have split the Age Feature to 8 Categories (purely be Trial and Error Method) so that the several categories significantly differ in rate of survival.<br>\nNote : - I have mixed the 2nd and 3th categories to a single category(as it provides same Survival Rates),mixed 4th to 6th to a single category and mixed 7th and 8th (as they are similar), leaving 1st category to achieve 4 categories \n  - kids less than 10  - [high rate of survival]\n  - Teen and young adults - [medium rate of survival]\n  - adults - [moderate rate of survival]\n  - senior citizens - [low rate of survival] <br>\nNote : - we should also look at the number of passengers in every category as count thereby avoiding stuck by survival rates"}}