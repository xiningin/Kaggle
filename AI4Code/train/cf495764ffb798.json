{"cell_type":{"9971212a":"code","ccd38eaf":"code","55358943":"code","aa73052b":"code","124583ff":"code","7b1a3507":"code","2238bdd4":"code","1ee0fb8c":"code","7a86ca0a":"code","38f4a27a":"code","9de1b385":"code","3ba7a558":"code","2f29d006":"code","81fe1ffd":"code","4d462a8e":"code","92fb37aa":"code","35ace3c6":"code","8610646a":"code","f51ff780":"code","c87fba37":"code","5518add6":"code","47e9a645":"code","6b480402":"code","7bdcc3d7":"code","83fcea09":"code","dc126202":"code","78d3e88b":"code","95b6ecf0":"code","4cfa78ad":"code","9ca1e699":"code","2d17748f":"code","310aae8d":"code","3362630c":"code","5e63b26c":"code","32859f20":"code","131de2e3":"code","5d08fb93":"code","7e8bdda1":"code","4c377a7d":"code","4445d2f8":"code","7db3aca6":"code","7858eecc":"code","81827e30":"code","8fa694e5":"code","59ac7f85":"code","160d72c6":"code","cf691cd3":"code","bb9c0103":"code","1948e067":"code","cd9454a6":"code","5b034056":"code","30a72f42":"code","669edd1a":"code","aec17f19":"code","7d50b3fe":"code","2c444215":"code","258b3f6f":"code","6603dcc5":"code","fbf2244c":"markdown","56db9210":"markdown","1b58c6c8":"markdown","18304179":"markdown","94d24571":"markdown","a8c9233d":"markdown","2107f1a7":"markdown","e2e70e1f":"markdown","58b11aef":"markdown","ef4592a0":"markdown","b44a43b6":"markdown","dacaedb5":"markdown","24a8a35b":"markdown","2325741f":"markdown","ab0cafe3":"markdown","81c920c3":"markdown","041ab418":"markdown","bef5e126":"markdown","1b724841":"markdown","6bc4546c":"markdown","431c7afd":"markdown","85f7f967":"markdown","214ea4c7":"markdown","1ea4385f":"markdown","979734d1":"markdown","0fbc03dc":"markdown","f1cf4763":"markdown","0826c265":"markdown","dfbe5bd5":"markdown","2f62cc55":"markdown","a5542848":"markdown","38e6bf1b":"markdown"},"source":{"9971212a":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #for visualisation\nimport seaborn as sns   #for visualisation\nsns.set(style=\"white\") #setting background of vizualisation as white \n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ccd38eaf":"%%time\n#using pandas to read the train and test file \ntrain= pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","55358943":"print(train.shape)\nprint(test.shape)","aa73052b":"train.shape, train.columns","124583ff":"train.head() #to understand the type of values in these columns","7b1a3507":"#Lets now understand our target variable that out 891 how many survived or died in this tragedy\nf,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Died vs Survived percentage')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train,ax=ax[1])\nax[1].set_title('Died vs Survived Count')\nax[1].set_xlabel('')\nplt.show()","2238bdd4":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex Percentage')\nsns.countplot(x='Survived',hue='Sex',data=train,ax=ax[1])\nax[1].set_title('Survived vs Sex')\nplt.legend(loc = 'top left')","1ee0fb8c":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Class Percentage')\nsns.countplot(x='Survived',hue='Pclass',data=train,ax=ax[1])\nax[1].set_title('Survived vs Class')\nplt.legend(loc = 'top left')","7a86ca0a":"sns.factorplot('Pclass','Survived',hue='Sex',data=train)#This clearly summarises our hypothesis","38f4a27a":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[0])#lets check using violin plot\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\n\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","9de1b385":"#Lets plot survival vs rest of the variable \nvariable = ['Embarked', 'Parch', 'SibSp']\n\nfig, axs = plt.subplots(ncols=1, nrows=3, figsize=(10, 8))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, v in enumerate(variable, 1):    \n    plt.subplot(1, 3, i)\n    sns.countplot(x=v, hue='Survived', data=train)\n    \n    plt.xlabel('{}'.format(v), size=10, labelpad=10)\n    plt.ylabel('Passenger Count', size=10, labelpad=10)    \n    plt.tick_params(axis='x', labelsize=10)\n    plt.tick_params(axis='y', labelsize=10)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 10})\n    plt.title('Count of Survival in {} Feature'.format(v), size=10, y=1.05)\n\nplt.show()","3ba7a558":"%%time\ntitanic_df = pd.concat([train, test])\nprint(titanic_df.shape) #lets check how concatenation worked\ntitanic_df.head()   #lets check the first 5 rows now\n","2f29d006":"titanic_df.dtypes #Lets check the type of data we are dealing with...","81fe1ffd":"titanic_df.describe() #summarizes all numerical columns","4d462a8e":"#lets check data has how many missing values\ntitanic_df.isnull().sum()\n","92fb37aa":"titanic_df['Title'] = titanic_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ntitanic_df.head()","35ace3c6":"titanic_df['Title'].value_counts()","8610646a":"mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\n           'Don': 'Mr', 'Mme': 'Mrs', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\n           'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ntitanic_df.replace({'Title': mapping}, inplace=True)\ntitanic_df['Title'].value_counts()#left with only 6 titles now","f51ff780":"# impute missing Age values using median of Title groups\ntitle_ages = dict(titanic_df.groupby('Title')['Age'].median())\n\n\ntitanic_df['age_med'] = titanic_df['Title'].apply(lambda x: title_ages[x])\n\n# replace all missing ages with the value in this column\ntitanic_df['Age'].fillna(titanic_df['age_med'], inplace=True, )\ndel titanic_df['age_med']\nsns.distplot(titanic_df['Age'])","c87fba37":"sns.barplot(x='Title', y='Age', data=titanic_df, estimator=np.median, ci=None, palette='Blues_d')\nplt.xticks(rotation=45)\nplt.show()","5518add6":"titanic_df.isnull().sum()","47e9a645":"%%time\n#from sklearn.preprocessing import Imputer\n#impute = Imputer(missing_values='NaN', strategy='mean', axis=0) #SKlearn Mean imputer\n#impute.fit(titanic_df['Age'])\n#titanic_df= impute.transform(titanic_df['age'])\n\n#from fancyimpute import KNN\n#imputer = KNN(k=2)\n#titanic_df['Age'] = imputer.fit_transform(titanic_df['Age'])","6b480402":"sns.heatmap(titanic_df.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \n#titanic_df.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","7bdcc3d7":"%%time\n# impute missing Fare values using median of Pclass groups\nclass_fares = dict(titanic_df.groupby('Pclass')['Fare'].median())\n\n# create a column of the average fares\ntitanic_df['fare_med'] = titanic_df['Pclass'].apply(lambda x: class_fares[x])\n\n# replace all missing fares with the value in this column\ntitanic_df['Fare'].fillna(titanic_df['fare_med'], inplace=True, )\ndel titanic_df['fare_med']\n","83fcea09":"#lets check data has how many missing values\ntitanic_df.isnull().sum()","dc126202":"titanic_df['Embarked'].value_counts()","78d3e88b":"#As Embarked is a categorical variable only 2 missing values so we will simply use backfill    \ntitanic_df['Embarked'].fillna(method='backfill', inplace=True)\ntitanic_df['Embarked'].isnull().sum()","95b6ecf0":"#we have number of parents and no.of siblings so we can calculate the size of the family \ntitanic_df['Family Size'] = titanic_df['Parch'] + titanic_df['SibSp']+1\ntitanic_df['IsAlone'] = 1 #initialize to yes\/1 is alone\ntitanic_df['IsAlone'].loc[titanic_df['Family Size'] > 1] = 0\ntitanic_df.columns\n","4cfa78ad":"#We will now drop column which have mostly Null and unimportant values. \ndrop = ['Cabin', 'Ticket','Name','PassengerId']\ntitanic_df.drop(drop, axis=1, inplace = True)\ntitanic_df.head()","9ca1e699":"titanic_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntitanic_df['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ntitanic_df['Title'].replace(['Mr','Mrs','Miss','Master','Dr','Rev'],[0,1,2,3,4,5],inplace=True)\n\n#you can also use get dummies or label encoder\n","2d17748f":"titanic_df.head()","310aae8d":"#Import all neccessary SKlearn libraries for KNN classifier\nfrom sklearn.preprocessing import MinMaxScaler#to make sure all variables are on the same scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics","3362630c":"y_train=titanic_df['Survived'].iloc[:891]#training target variable\nx_train=titanic_df.drop('Survived', axis=1)#dropping target variable\n#y_train.tail()\nx_train.tail()","5e63b26c":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x_train)\nx_scaled = pd.DataFrame(x_scaled, columns = x_train.columns)\nx_scaled.tail()#All variables are now in same scale of 0 to 1 ","32859f20":"from sklearn.model_selection import train_test_split\ntrain_scaled= x_scaled.iloc[:891] #train dataset\n#train_scaled.shape\ntest_scaled=x_scaled.iloc[891:]#final dataset\n#test_scaled.shape\n","131de2e3":"#Creating training and test dataset from from training dataset\ntrain_x,test_x,train_y,test_y = train_test_split(train_scaled,y_train, random_state = 56, stratify=y_train)","5d08fb93":"%%time\n# Creating instance of KNN\nclf = KNN(n_neighbors = 10)\n\n# Fitting the model\nclf.fit(train_x, train_y)\n\n# Predicting over the Train Set and calculating accuracy score\ntest_predict = clf.predict(test_x)\nprint('The accuracy of the KNN is',metrics.accuracy_score(test_predict,test_y))\nfinal_predict1=clf.predict(test_scaled)\nprint(final_predict1)","7e8bdda1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","4c377a7d":"#Running logistics Regression model\nlr = LogisticRegression()\nlr.fit(train_x,train_y)\n#making predicition on test set created from train_scaled data\nvalid1=lr.predict(test_x)\nprint(lr.score(test_x, test_y))\npred1=lr.predict(test_scaled)\npred1[:10]\n\n","4445d2f8":"#Running KNN model\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(train_x,train_y)\n#making predicition on test set created from train_scaled data\nvalid2=knn.predict(test_x)\nprint(knn.score(test_x, test_y))\npred2=knn.predict(test_scaled)\npred2[:10]","7db3aca6":"#Running Decision tree model\ndt = DecisionTreeClassifier(max_depth=5)\ndt.fit(train_x,train_y)\n#making predicition on test set created from train_scaled data\nvalid3=dt.predict(test_x)\nprint(dt.score(test_x, test_y))\npred3=dt.predict(test_scaled)\npred3[:10]","7858eecc":"from sklearn.metrics import accuracy_score\nfrom statistics import mode\nfinal_pred = np.array([])\nfor i in range(0,len(test)):\n    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))\n\nfinal_pred=final_pred.astype(int)\nfinal_pred.shape","81827e30":"#Creating a New train dataframe\ntrain_prediction = {\n              'LR': valid1,\n              'knn': valid2,\n              'DT': valid3\n              }\ntrain_predictions = pd.DataFrame(train_prediction)\ntrain_predictions.shape, test_y.shape","8fa694e5":"#Creating a New test dataframe\ntest_prediction = {\n              'LR': pred1,\n              'knn': pred2,\n              'DT': pred3\n              }\ntest_predictions = pd.DataFrame(test_prediction)\ntest_predictions.head()","59ac7f85":"# Stacking Model\nmodel = LogisticRegression()\nmodel.fit(train_predictions, test_y)\nfinal_pred=model.predict(test_predictions)\nfinal_pred=final_pred.astype(int)\nfinal_pred[:10],final_pred.dtype","160d72c6":"from sklearn.model_selection import KFold\ntrain_pred = np.empty((0,0) , int)\nskfold = KFold(10, random_state = 101)\n  \n#For every permutation of KFold\nfor i,j in skfold.split(train_x, train_y):\n    x_train, x_test = train_x.iloc[i], train_x.iloc[j]\n    y_train, y_test = train_y.iloc[i], train_y.iloc[j]\n\n#Everything else remains same as regular stacking\n\n#Running logistics Regression model\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n#making predicition on test set created from train_scaled data\nvalid1=lr.predict(x_test)\nprint(lr.score(x_test, y_test))\npred1=lr.predict(test_scaled)\n\n\n#Running KNN model\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(x_train,y_train)\n#making predicition on test set created from train_scaled data\nvalid2=knn.predict(x_test)\nprint(knn.score(x_test, y_test))\npred2=knn.predict(test_scaled)\n\n\n#Running Decision tree model\ndt = DecisionTreeClassifier(max_depth=5)\ndt.fit(x_train,y_train)\n#making predicition on test set created from train_scaled data\nvalid3=dt.predict(x_test)\nprint(dt.score(x_test, y_test))\npred3=dt.predict(test_scaled)\n\n\n#Creating a New train dataframe\ntrain_prediction = {\n              'LR': valid1,\n              'knn': valid2,\n              'DT': valid3\n              }\ntrain_predictions = pd.DataFrame(train_prediction)\n\n\n\n#Creating a New test dataframe\ntest_prediction = {\n              'LR': pred1,\n              'knn': pred2,\n              'DT': pred3\n              }\ntest_predictions = pd.DataFrame(test_prediction)\n\n\n\n# Stacking Model\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(train_predictions, y_test)\nfinal_pred=model.predict(test_predictions)\nfinal_pred=final_pred.astype(int)\nfinal_pred[:10],final_pred.dtype","cf691cd3":"#Importing random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n","bb9c0103":"train_accuracy = []\nvalidation_accuracy = []\nfor depth in range(1,10):\n    rfc_model = RandomForestClassifier(max_depth=depth, random_state=10)\n    rfc_model.fit(train_x, train_y)\n    train_accuracy.append(rfc_model.score(train_x, train_y))\n    validation_accuracy.append(rfc_model.score(test_x, test_y))","1948e067":"table = pd.DataFrame({'max_depth':range(1,10), 'train_acc':train_accuracy, 'test_acc':validation_accuracy})\ntable","cd9454a6":"plt.figure(figsize=(12,6))\nplt.plot(table['max_depth'], table['train_acc'], marker='o')\nplt.plot(table['max_depth'], table['test_acc'], marker='o')\nplt.xlabel('Depth of tree')\nplt.ylabel('performance')\nplt.legend()","5b034056":"#creating a random forest instance\nRFC = RandomForestClassifier(random_state=10,max_depth=7, max_leaf_nodes=25,n_estimators=12)\nRFC.fit(train_x,train_y)\npred4=dt.predict(test_scaled)\nfinal_pred=pred4.astype(int)\nfinal_pred[:10],final_pred.dtype","30a72f42":"#Importing GBDT Classifier \nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc= GradientBoostingClassifier(random_state=101)","669edd1a":"#putting some random values for hyperp optimisation\nparameter_grid = {\n    'max_depth' : [4,5,6,7,8],\n    'n_estimators': [100,150,200, 250],\n    'min_samples_split': [50,100,150,200]\n    }","aec17f19":"from sklearn.model_selection import GridSearchCV\ngridsearch = GridSearchCV(estimator=gbc, param_grid=parameter_grid, scoring='neg_mean_squared_error', cv=5)","7d50b3fe":"gridsearch.fit(train_x, train_y)\n","2c444215":"gridsearch.best_params_","258b3f6f":"#creating an Gradient boosting instance\ngbc= GradientBoostingClassifier(random_state=101, n_estimators=150,min_samples_split=100, max_depth=6)\ngbc.fit(train_x,train_y)\nprint(gbc.score(test_x, test_y))\npred5=gbc.predict(test_scaled)\nfinal_pred=pred5.astype(int)","6603dcc5":" #Output the predictions into a csv\n#output= pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived':final_pred})\n#output.to_csv('gender_submission.csv', index=False)","fbf2244c":"# **Boosting Technique- GBC**","56db9210":"**Data Cleaning\/ treatment**\n\nWe cannot expect data to be clean and readily available for modelling. Therefore, we will try to understand type of data and find if they have missing values","1b58c6c8":"# **Stacking Using Kfold**","18304179":"Lets now try to predict using logistic regression","94d24571":"**Data Mining**\n\nStarting from the available data, we should always keep looking features\/ data that can improve our prediction. I could not find much data which can influence our predictions. Therefore, we will work with available data. We have train and test files. ","a8c9233d":"We can now see that we have alot of rare titles. Lets club them","2107f1a7":"**Data Exploration**\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. We will now try to understand the train data by forming hypothesis such female and higher class passengers were most likely to survive the tragedy","e2e70e1f":"We can now see that \n\n![image.png](attachment:image.png)","58b11aef":"# ** Basic Stacking**","ef4592a0":"Lets check another hypothesis, was class a factor for children survival as well..","b44a43b6":"We can clearly see that almost 75% female survived the tragedy. Lets check if class had an impact on survival or not ","dacaedb5":"Noting down train and test rows to separate it easily when required","24a8a35b":"So, 61.6% passengers in train dataset survived and 38.3% died. We know from the movie that Female were given preference over male. Lets check..","2325741f":"# **MAX Vote**\n","ab0cafe3":"**Predictive Modelling**\n\nLets start building models to predict our target variable. We will start will KNN and gradually move towards more complex models ","81c920c3":"**Titanic prediction using data science lifecycle**\n\nWith this notebook, my objective is to follow a complete process of data science project and explain each step and rationale behind every decision we take during development of solution.","041ab418":"Lets now merge both train and test data to perform data preprocessing together. After preprocessing, we will separate the train and test file again. ","bef5e126":"We have 11 columns and 891 rows in our train data where 1 column is our target variable.","1b724841":"**Business Understanding**\n\nIt is the most important part of the data science cycle as more than 50% fail because they do not understand the problem properly. We are not subject matter expert so this part becomes even more important and chalenging. \n\n**Titanic: Machine learning from disaster** \n\nTitanic was one of the biggest ship to be built in 1912. \n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. It means 31.46 survival rate.\n- The shortage of lifeboats for the passengers and crew led to such loss of life. \n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n- Our aim is to predict which passengers survived the tragedy. ","6bc4546c":"Clearly buying higher class ticket had more than 60% survival chance and only 25% class 3 passengers could survive. Lets see what are the chance for female passenger of Pclass 1, probably 100%..Lets check on a factor plot..","431c7afd":"**Feature Engineering**- It is one of the key process and can become a differentiator for your model. Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.\nsource- Wikipedia","85f7f967":"We can see that we have Name column and it has salutation. It will be insteresting to impute on the basis of their salutation. ","214ea4c7":"Max vote takes prediction from multiple models and provide its prediction on the basis of maximum votes","1ea4385f":"Looks great...","979734d1":"survival rate for children under 10 looks irrespective of class but we not be certain as we have missing values in age column. Therefore, it requires further analysis.","0fbc03dc":"We can also impute mean using sklearn library","f1cf4763":"# **Bagging Ensemble model- Random Forest**","0826c265":"We can clear make out from the available data that Age column has missing values. The mean of the age is 29 but max age is upto 80 years. Similarly average fare is 33 and someone paid 512 as well. It also can be infered that the number sibling and parents are less and max is 8 and 6 repsectively.","dfbe5bd5":"![data%20science%20lifecycle.png](attachment:data%20science%20lifecycle.png)\n\nI found this amazing infographics in data science lifecycle. You can click on the reference link below to read more about it. \n\n\nReference: http:\/\/sudeep.co\/data-science\/Understanding-the-Data-Science-Lifecycle\/","2f62cc55":"We have string value in 3 columns and SKlearn doesnot support strings. Therefore, we can change them categorical values as 0,1,2 etc. or use onehot encoding ","a5542848":"**These are basic and easy to understand codes. Please Upvote if you found it useful. Please also feel free to comment**","38e6bf1b":"Therefore, age of people are missing along with cabin details. We will now try to handle these missing values. We can basically treat missing values in three ways. \n\n* 1. Remove rows or columns containing missing values\n* 2. Fill missing values with test statistics such as mean, median, mode etc.\n* 3. Predict Missing value with machine learning algorithm such as regression, KNN\n\nEach one has its own pros and cons. Type of data is one of the keys parameters of choosing the treatment methodology. We should always try different methodology and see if our accuracy improves. "}}