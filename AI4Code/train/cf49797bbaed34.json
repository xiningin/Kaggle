{"cell_type":{"9756ef93":"code","713b5193":"code","a9cfcf84":"code","a003aeaa":"code","708abbaf":"code","6064bf48":"code","0962d7f8":"code","bfc13dad":"code","d7796eb4":"code","d071876a":"markdown","ec7b336b":"markdown","a0eef94d":"markdown","25a2b9c3":"markdown","1cfc405c":"markdown","4c82c68f":"markdown"},"source":{"9756ef93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","713b5193":"import pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn import model_selection","a9cfcf84":"TRAINING_FILE = \"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\"\nTEST_FILE = \"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\"","a003aeaa":"df = pd.read_csv(TRAINING_FILE)\n\n# Transform \"target\" variable into integer\ndf[\"target\"] = df[\"target\"].map({'Class_1': 0, 'Class_2': 1, 'Class_3': 2, 'Class_4': 3})\n# Removing \"id\" column\ndf = df.drop(columns=[\"id\"])\n\n# Plotting distribution of target variable\nsns.countplot(x=df[\"target\"]);","708abbaf":"df[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.target.values)):\n    df.loc[val_, \"kfold\"] = fold","6064bf48":"# PREPARATION\n# Setting the model parameters\nlgbm_parameters = {\n    'boosting': 'gbdt',\n    'objective': 'multiclass',\n    'num_leaves': 280, \n    'learning_rate': 0.02, \n    'num_class': 4,\n    'metric': 'multi_logloss',\n}","0962d7f8":"# df = train file\n# df_test = test file\n\ndf_test_prep = pd.read_csv(TEST_FILE)\ndf_test = df_test_prep.drop([\"id\"], axis=1).values\n\n# Use all folds except \"0\" as train file = 80%\ndf_train = df[df.kfold != 0].reset_index(drop=True)\n# Use fold \"0\" as test file = 20% \ndf_valid = df[df.kfold == 0].reset_index(drop=True)\n       \nx_train = df_train.drop([\"target\", \"kfold\"], axis=1).values\ny_train = df_train.target.values\n    \nx_valid = df_valid.drop([\"target\", \"kfold\"], axis=1).values\ny_valid = df_valid.target.values","bfc13dad":"# TRAINING\ntrain_data = lgbm.Dataset(x_train, label=y_train, free_raw_data=False)\nvalid_data = lgbm.Dataset(x_valid, label=y_valid, reference=train_data ,free_raw_data=False)\n    \nclf = lgbm.train(lgbm_parameters, train_data, valid_sets=[valid_data], num_boost_round = 400, verbose_eval=100, early_stopping_rounds=50)\n\npred = clf.predict(df_test) \npred_test = pd.DataFrame(pred)\npred_test.head(3)\n\ny_test = pd.DataFrame(pred)\nsubmission = y_test.copy()\nsubmission.columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"]\nsubmission[\"id\"] = df_test_prep[\"id\"]","d7796eb4":"submission.head(10)","d071876a":"# 5. Preparing and training the model","ec7b336b":"## As we can see, Class 2 is overrepresented, while Class 1 is underrepresented. Therefore we apply stratification.","a0eef94d":"# 4. Creating folds with StratifiedKFold","25a2b9c3":"# 3. Very brief EDA (exploratory data analysis) ","1cfc405c":"# 2. Preparation for loading the data","4c82c68f":"# 1. Import the necessary libraries"}}