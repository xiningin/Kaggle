{"cell_type":{"8ae981e2":"code","ba170af5":"code","1c5c6036":"code","277a6e86":"code","e5196a9f":"code","3f2fef93":"code","19de88cb":"code","5f3101e4":"code","67b606f5":"code","bb118574":"code","e2e48128":"code","364a3dfe":"code","667a64da":"code","f9d98cf3":"code","c3162b69":"code","65ab9345":"code","ea605f77":"code","da4b24fd":"code","0897c61f":"code","e4836001":"code","9667c776":"code","146f738d":"code","2a9fe701":"code","aaed62c3":"markdown","3ffaedcf":"markdown","75ba210a":"markdown","87e067cc":"markdown","38bc2b23":"markdown","24c0644f":"markdown","b1d4971a":"markdown","4dd0d3fe":"markdown","1b272355":"markdown","aa4855ad":"markdown","ab16a62f":"markdown","bba688a0":"markdown","38e6d942":"markdown","0d3bf438":"markdown","f4893c34":"markdown","9be8809a":"markdown","35b95707":"markdown"},"source":{"8ae981e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba170af5":"df = pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\")","1c5c6036":"df.head()","277a6e86":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TreebankWordTokenizer","e5196a9f":"def tokenizer(text):\n    stop_words = set(stopwords.words('english')) \n    stop_words.add(\"@\")\n    tokenizer_obj = TreebankWordTokenizer()\n    word_list = tokenizer_obj.tokenize(text)\n    filtered_words = [w.lower() for w in word_list if w not in stop_words]\n    snow_stemmer = nltk.stem.SnowballStemmer(\"english\")\n    stemmed = [snow_stemmer.stem(w) for w in filtered_words]\n    return \" \".join(stemmed)\n    ","3f2fef93":"df[\"tokenized_text\"] = df[\"text\"].apply(tokenizer)","19de88cb":"df[\"tokenized_text\"].head()","5f3101e4":"df[\"text\"].head()","67b606f5":"from keras.preprocessing.text import Tokenizer","bb118574":"t = Tokenizer()\nt.fit_on_texts(df[\"tokenized_text\"])","e2e48128":"# summarize what was learned\n# print(t.word_counts)\n# print(t.document_count)\n# print(t.word_index)\n# print(t.word_docs)","364a3dfe":"vocab_size=len(t.word_index)+1 \nprint(vocab_size)","667a64da":"X=t.texts_to_sequences(df['tokenized_text'].values)\n    ","f9d98cf3":"max_len=max(len(row) for row in df['tokenized_text'].values)\nprint(max_len)","c3162b69":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nX=pad_sequences(X, maxlen=max_len)\nprint(X.shape)","65ab9345":"y=pd.get_dummies(df['airline_sentiment']).values\nprint(y[0])","ea605f77":"from keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.models import Sequential\nfrom keras.regularizers import l2","da4b24fd":"model=Sequential()\nmodel.add(Embedding(vocab_size, 50, input_length=max_len))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation=\"softmax\"))","0897c61f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","e4836001":"print(X_train.shape, y_train.shape, X_test.shape)","9667c776":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","146f738d":"model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)","2a9fe701":"acc = model.evaluate(X_test,y_test)\nprint(acc)","aaed62c3":"Summary on text","3ffaedcf":"Check the data","75ba210a":"Get the maximum length of tweet","87e067cc":"Change Y into one hot encoding","38bc2b23":"Apply the tokenizer on tweets","24c0644f":"# Evaluate the model","b1d4971a":"# Load the Twitter Airline Sentiment data","4dd0d3fe":"# Create sequential Model","1b272355":"Load the Keras Layers","aa4855ad":"Convert text to sequences ","ab16a62f":"Split the data into train and test","bba688a0":"Remove Stopwords.\nTokenize and Stemming the data","38e6d942":"Add the pad sequences before the integer sequences if length of tweet is less than no. of tokens","0d3bf438":"# # # Import Keras module for conversion from text to sequences","f4893c34":"Check the text after and before tokenization","9be8809a":"# # Load the NLTK module ","35b95707":"Fit the training data on model with batch size as 32"}}