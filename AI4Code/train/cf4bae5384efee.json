{"cell_type":{"d0cd4caa":"code","215e14e7":"code","a86c8ccb":"code","5d457004":"code","6e508362":"code","c2d6bb49":"code","88a76417":"code","8926b1b3":"code","41667ae5":"code","e8389997":"code","673c1d9c":"code","d25984b9":"code","bfa1f399":"code","676e4434":"code","b27f7787":"code","7aa40afe":"code","3e4b0783":"code","950a4454":"code","c80bee8b":"code","9b2ef780":"code","20b02526":"code","e2d9726f":"code","b24e449f":"code","22c3d412":"code","e94827d0":"code","9d7441be":"code","a5123ba1":"code","b57e7eb1":"code","56acd79f":"code","a1ac0638":"code","37a62135":"code","1b3f50e4":"code","e96d827b":"code","49ca527d":"code","ca4f13e9":"code","b673bdc8":"code","f6289f36":"code","9a3bc0ca":"code","0188007c":"code","eeb386a5":"code","e913ee9c":"code","bddf3208":"code","e0bf9b09":"code","efca8915":"code","57b97dc5":"code","1f12fb86":"code","ff73021c":"code","dfb1741c":"code","c195142d":"code","4f07eaf1":"markdown","7c57d9fc":"markdown","7fdbca27":"markdown","bf4fc589":"markdown","b7937156":"markdown","1747e03e":"markdown","d1a81bf2":"markdown","8300820a":"markdown","a6f5e5b4":"markdown","cca39e7d":"markdown","90c45101":"markdown","53cbbd1e":"markdown","376dea27":"markdown","506d6d30":"markdown","f354ca71":"markdown","b46d60bd":"markdown","eed4b84c":"markdown","4c4ef97e":"markdown","343bf3b1":"markdown","503df891":"markdown","faf4e603":"markdown","b67d6117":"markdown","b3495d26":"markdown","ef82ec72":"markdown","2fe1a4d9":"markdown","83f96f0f":"markdown","139ea6b5":"markdown","36dbf5d6":"markdown","fe7e24e5":"markdown","d5228013":"markdown","043c77ee":"markdown","5ab8f7e2":"markdown","cdee70c0":"markdown","95ff0989":"markdown","6f1b2053":"markdown","4ff150f9":"markdown","74aa1a02":"markdown","799d145a":"markdown"},"source":{"d0cd4caa":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","215e14e7":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a86c8ccb":"students_grades_df = pd.read_csv(\n    '..\/input\/student-performance-data-set\/student-por.csv')\n","5d457004":"def create_average_column():\n    columns = ['G1', 'G2', 'G3']\n    students_grades_df['annual_grades_avg'] = students_grades_df[columns].mean(\n        axis=1)\n","6e508362":"create_average_column()\n","c2d6bb49":"def five_level_classification():\n    bins = pd.IntervalIndex.from_tuples(\n        [(0, 9.5), (9.5, 11.5), (11.5, 13.5), (13.5, 15.5), (15.5, 20)], closed='right')\n\n    levels = ['fail', 'sufficient', 'satisfactory', 'good', 'excellent']\n\n    new_column = 'annual_grades_evaluation'\n    students_grades_df[new_column] = np.array(levels)[\n        pd.cut(students_grades_df['annual_grades_avg'], bins=bins).cat.codes]\n","88a76417":"five_level_classification()\n","8926b1b3":"print(f'data frame shape: {students_grades_df.shape}')\n","41667ae5":"students_grades_df.head()\n","e8389997":"students_grades_df.info()\n","673c1d9c":"students_grades_df.describe()\n","d25984b9":"sns.set_theme(style=\"darkgrid\")\n","bfa1f399":"def plot_corr_map(data, **kwargs):\n\n    _, ax = plt.subplots(figsize=(35, 35))\n    sns.heatmap(data=data.corr(), ax=ax, **kwargs)\n    ax.set_title('Correlation Heatmap')\n","676e4434":"params = {'annot': True, 'fmt': '.2g', 'cmap': 'YlGn',\n          'linewidths': 1, 'linecolor': 'black'}\n\n# We need to encode the string nominal columns to numbers\nencoded_df = students_grades_df.iloc[:, :-1].copy()\ncolumns = encoded_df.select_dtypes(include=['object']).columns\nordinal_encoder = OrdinalEncoder()\nencoded_df[columns] = ordinal_encoder.fit_transform(encoded_df[columns])\n\nplot_corr_map(encoded_df, **params)\n","b27f7787":"def plot_categorical_insight(categorical_columns):\n\n    nrows, ncols = categorical_columns.shape[1], 3\n    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(25, 85))\n\n    for idx, column in enumerate(categorical_columns):\n        ax = axes[idx]\n        sns.countplot(data=students_grades_df,\n                      x='annual_grades_evaluation', hue=column, ax=ax[0])\n\n        sns.countplot(data=students_grades_df, x=column, ax=ax[1])\n\n        sns.boxplot(data=students_grades_df, x=column,\n                    y='annual_grades_avg', ax=ax[2])\n","7aa40afe":"columns = students_grades_df.select_dtypes(include='object')\ncolumns = columns.drop('annual_grades_evaluation', axis=1)\nplot_categorical_insight(columns)\n","3e4b0783":"def plot_grades_to_self():\n    nrows, ncols = 1, 2\n    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 8))\n    sns.countplot(data=students_grades_df,\n                  x='annual_grades_evaluation', ax=axes[0])\n\n    sns.boxplot(data=students_grades_df,\n                x='annual_grades_evaluation', y='annual_grades_avg', ax=axes[1])\n","950a4454":"plot_grades_to_self()\n","c80bee8b":"def plot_non_categorical_insight(non_categorical_columns):\n\n    nrows, ncols = non_categorical_columns.shape[1], 2\n    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 100))\n\n    for idx, column in enumerate(non_categorical_columns):\n        ax = axes[idx]\n\n        sns.scatterplot(data=students_grades_df, x=column,\n                        y='annual_grades_avg', ax=ax[0], hue='annual_grades_evaluation')\n\n        ax[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1.07),\n                     ncol=5, fancybox=True, shadow=True)\n\n        sns.boxplot(data=students_grades_df, x=column,\n                    y='annual_grades_avg', ax=ax[1])\n","9b2ef780":"columns = students_grades_df.select_dtypes(include=['int64', 'float64'])\ncolumns = columns.drop('annual_grades_avg', axis=1)\nplot_non_categorical_insight(columns)\n","20b02526":"def boxplot_non_categorical(non_categorical_columns, colors):\n    nrows, ncols = 4, 4\n    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))\n\n    for idx, sliced_columns in enumerate(non_categorical_columns):\n        ax = axes[idx]\n\n        for col_idx, (column, color) in enumerate(zip(sliced_columns, colors)):\n            sns.boxplot(data=students_grades_df, x=column,\n                        ax=ax[col_idx], color=color)\n","e2d9726f":"size = 4\nsliced_columns = [column for column in columns]\nsliced_columns = [sliced_columns[sliced-4:sliced]\n                  for sliced in range(size, columns.shape[1] + 1, size)]\n\ncolors = ['#4c72b0', '#dd8452', '#55a868', '#c44e52']\nboxplot_non_categorical(sliced_columns, colors)\n","b24e449f":"students_grades_df['failures'].value_counts()\n","22c3d412":"sns.displot(data=students_grades_df['annual_grades_avg'],\n            element='step', kde=True, color='green')\n","e94827d0":"print(f\"skewness: {students_grades_df['annual_grades_avg'].skew()}\")\nprint(f\"kurtosis: {students_grades_df['annual_grades_avg'].kurt()}\")\n","9d7441be":"column_to_drop = ['G1', 'G2', 'G3']\nstudents_grades_df = students_grades_df.drop(column_to_drop, axis=1)\n","a5123ba1":"def detect_outliers(columns):\n    outlier_indices = []\n    for column in columns:\n        Q1 = students_grades_df[column].quantile(0.25)\n        Q3 = students_grades_df[column].quantile(0.75)\n        IQR = Q3 - Q1  # IQR is interquartile range.\n\n        mask = (students_grades_df[column] >= Q1 - 1.5 *\n                IQR) & (students_grades_df[column] <= Q3 + 1.5 * IQR)\n        mask = mask.to_numpy()\n        false_indices = np.argwhere(~mask)\n        outlier_indices.append(false_indices)\n    return np.unique(np.concatenate(outlier_indices).ravel())\n","b57e7eb1":"numerical_columns = ['age', 'absences', 'annual_grades_avg']\noutlier_indices = detect_outliers(numerical_columns)\n","56acd79f":"print(f'Number of outliers: {len(outlier_indices)}')\n","a1ac0638":"# Delete outliers\nstudents_grades_df = students_grades_df.drop(outlier_indices, axis=0)\n","37a62135":"# Split dataset\nX, y = students_grades_df.iloc[:, :-1], students_grades_df.iloc[:, -1]\n","1b3f50e4":"# Create train and test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","e96d827b":"# First we need to know which columns are binary, nominal and numerical\ndef get_columns_by_category():\n    categorical_mask = X.select_dtypes(\n        include=['object']).apply(pd.Series.nunique) == 2\n    numerical_mask = X.select_dtypes(\n        include=['int64', 'float64']).apply(pd.Series.nunique) > 5\n\n    binary_columns = X[categorical_mask.index[categorical_mask]].columns\n    nominal_columns = X[categorical_mask.index[~categorical_mask]].columns\n    numerical_columns = X[numerical_mask.index[numerical_mask]].columns\n\n    return binary_columns, nominal_columns, numerical_columns\n","49ca527d":"binary_columns, nominal_columns, numerical_columns = get_columns_by_category()\n","ca4f13e9":"# Now we can create a column transformer pipeline\ntransformers = [('binary', OrdinalEncoder(), binary_columns),\n                ('nominal', OneHotEncoder(), nominal_columns),\n                ('numerical', StandardScaler(), numerical_columns)]\n\ntransformer_pipeline = ColumnTransformer(transformers, remainder='passthrough')\n","b673bdc8":"# Starified k cross validation\nKfold = StratifiedKFold(n_splits=5)\n","f6289f36":"RANDOM_STATE = 42\n# I did tweak some of the hyperparameters with trial & error\nclassifiers = [LogisticRegression(max_iter=70, solver='sag', random_state=RANDOM_STATE),\n               DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE),\n               SVC(C=2, kernel='linear', random_state=RANDOM_STATE),\n               RandomForestClassifier(\n                   max_depth=7, min_samples_split=5, min_samples_leaf=5, random_state=RANDOM_STATE),\n               AdaBoostClassifier(DecisionTreeClassifier(\n                   max_depth=1, random_state=RANDOM_STATE), random_state=RANDOM_STATE),\n               GradientBoostingClassifier(\n                   learning_rate=0.005, n_estimators=30, random_state=RANDOM_STATE),\n               KNeighborsClassifier(),\n               GaussianNB(var_smoothing=1e-2)\n               ]\n","9a3bc0ca":"classifiers_names = ['Logistic Regression',\n                     'Decision Tree Classifier',\n                     'Support Vector Machine',\n                     'Random Forest Classifier',\n                     'AdaBoost Classifier',\n                     'Gradient Boosting Classifier',\n                     'K Neighbors Classifier',\n                     'Gaussian Naive Bayes'\n                     ]\n\npipelines = [Pipeline([('transformer', transformer_pipeline), (classifier_name, classifier)])\n             for classifier_name, classifier in zip(classifiers_names, classifiers)]\n","0188007c":"def cv_fit_models():\n    train_acc_results = []\n    cv_scores = {classifier_name: [] for classifier_name in classifiers_names}\n    for classifier_name, pipeline in zip(classifiers_names, pipelines):\n        cv_score = cross_validate(pipeline,\n                                  X_train,\n                                  y_train,\n                                  scoring=scoring,\n                                  cv=Kfold,\n                                  return_train_score=True,\n                                  return_estimator=True)\n\n        train_accuracy = cv_score['train_acc'].mean() * 100\n        \n        train_acc_results.append(train_accuracy)\n        cv_scores[classifier_name].append(cv_score)\n\n    return np.array(train_acc_results), cv_scores\n","eeb386a5":"scoring = {'acc': 'accuracy'}\n\nresults, folds_scores = cv_fit_models()\n","e913ee9c":"def pick_best_estimator():\n    best_estimators = {classifier_name: [] for classifier_name in classifiers_names}\n    for key, model in folds_scores.items():\n        best_acc_idx = np.argmax(model[0]['test_acc'])\n        best_model = model[0]['estimator'][best_acc_idx]\n        best_estimators[key].append(best_model)\n    return best_estimators\n","bddf3208":"best_estimators = pick_best_estimator()\n    ","e0bf9b09":"def gather_metrics_scores():\n    test_accs, precisions, recalls = [], [], []\n    for estimator_val in best_estimators.values():\n        estimator = estimator_val[0]\n        y_pred = estimator.predict(X_test)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        recall = recall_score(y_test, y_pred, average='weighted')\n        score = estimator.score(X_test, y_test)\n\n        test_accs.append(score)\n        precisions.append(precision)\n        recalls.append(recall)\n        \n    scores = {'test_acc': np.array(test_accs),\n              'precision': np.array(precisions),\n              'recall': np.array(recalls)}\n    \n    return scores","efca8915":"scores = gather_metrics_scores()","57b97dc5":"def plot_train_test_accuracy(df):\n    _, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n    sns.barplot(data=df, x='train_accuracy',\n                y='Model', orient='h', ax=ax[0])\n    ax[0].set_xlim([0, 100])\n    sns.barplot(data=df, x='test_accuracy',\n                y='Model', orient='h', ax=ax[1])\n    ax[1].set_xlim([0, 100])\n","1f12fb86":"results_df = pd.DataFrame({'Model': classifiers_names,\n                           'train_accuracy': results,\n                           'test_accuracy': scores['test_acc'] * 100,\n                           'test_precision': scores['precision'] * 100,\n                           'test_recall': scores['recall'] * 100})\n","ff73021c":"plot_train_test_accuracy(results_df)\n","dfb1741c":"def plot_precision_recall(df):\n    _, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n    sns.barplot(data=df, x='test_precision',\n                y='Model', orient='h', ax=ax[0])\n    ax[0].set_xlim([0, 100])\n    sns.barplot(data=df, x='test_recall',\n                y='Model', orient='h', ax=ax[1])\n    ax[1].set_xlim([0, 100])\n","c195142d":"plot_precision_recall(results_df)\n","4f07eaf1":"**And one final lookup:**","7c57d9fc":"*Cross Validation time:*","7fdbca27":"**So, what he have in our menu?**\n- For appetizers, we have a `Logistic Regression`, `Decision Tree Classifer` and `Support Vector Classifier`.\n- For the main course, you can choose between:\n    - `Random Forest Classifier`.\n    - `AdaBoost Classifier`.\n    - `Gradient Boosting Classifier`.\n- And finally for deserts, we have either `KNN` or `Gussian Naive Bayes`.\n","bf4fc589":"<a id=\"Outliers_Detection\"><\/a>\n# Outliers Detection","b7937156":"*First lets add the average column:*","1747e03e":"*But before we start, lets drop columns `G1`, `G2` & `G3`.*","d1a81bf2":"*Lets plot our results:*","8300820a":"<a id=\"Data_Scaling\"><\/a>\n## Data Scaling\n","a6f5e5b4":"*The column resembles a normal distribution, it has a low negative skewness which means the data is pretty much symetric.*\n\n*We can derive for the kurtosis value that the data has some sort of peakedness.*","cca39e7d":"*Seems like there is no missing data, which is good since the dataset is small to begin with.*","90c45101":"<a id=\"libraries\"><\/a>\n# Import Libraries","53cbbd1e":"<a id=\"Conclusions\"><\/a>\n# Conclusions\n- **Decision Tree Classifier & Gradient Boosting seem to give us the best results.**\n- **SVM probably overfits the data, we have only 649 rows (with outliers) and 31 columns, for this amount of features we really need way more 'meat' for our models to fit properly.**\n- **Distance based model? big NO.**\n\n## What can I try in the future?\n- **Maybe not drop the outliers, or change the criteria for the minimum amount needed to be considered as an 'outlier' (now it's on 1).**\n- **I doubt feature reduction will help here, but still can be given a shot.**\n- **GridSearchCV to tune our hyperparameters.**","376dea27":"*It seems like students just don't like school much thsese days & we have quite a few outliers below the `min` value of failures.*","506d6d30":"*First lets see which types of correlations we have*.","f354ca71":"### **The main points that we can see are:**\n- That 22 years old student seems like an outlier.\n- Students grades seems to be better if the level of education their parents have is higher (`Fedu` & `Medu`).\n- Interestingly enough, the closer the student lives to the school - the better the grades.\n- It's good to see that all excellent students `goout` in a certain level.\n- Seems like you can't have `good` grades when your `absences` is more than 30 - outliers(?).\n- As the year went along, more and more students failed in the exam & probably didn't even show up.\n- For some reason, the `failures` boxplot returns a single line, lets examine it below.","b46d60bd":"#### **Important note: all of the insights mentioned below should be taken in context to the amount we see of each label in the middle count plot.**\n\n\n### **The main points that we can see are:**\n- *One `school` comes out top in every metric (or comes out down in failures).*\n- *`Females` do fail more than `males` but still do better overall in the other levels.*\n- *Students living in an `urban `area perform better than those who live in a `rural` are.*\n- *The `reputation` seems to be the main factor to choose a 'good' school.*\n- *Students who engage in extracurricular `activities` have better grades than those who not.*","eed4b84c":"# Table Of Contents\n* [Introduction](#intro)\n* [Import Libraries](#libraries)\n* [Dataset Files](#Dataset_Files)\n    - [Load Dataset](#Load_Dataset)\n* [5-Level classification modification](#5_Level_classification_modification)\n* [Initial Observation](#Initial_Observation)\n* [Visualization](#Visualization)\n    - [Correlation Analysis](#Correlation_Analysis)\n    - [Categorical Quantitative Insights](#Categorical_Quantitative_Insights)\n    - [Non-Categorical Quantitative Insights](#Non_Categorical_Quantitative_Insights)\n    - [Skewness & Kurtosis](#Skewness_Kurtosis)\n* [Outliers Detection](#Outliers_Detection)\n* [Classification Modeling](#Classification_Modeling)\n    - [Data Scaling](#Data_Scaling)\n* [Conclusions](#Conclusions)\n","4c4ef97e":"![](https:\/\/i.imgur.com\/iWtMWh7.png)","343bf3b1":"*Pick the best fold for each model according to the highest test accuracy:*","503df891":"<a id=\"Categorical_Quantitative_Insights\"><\/a>\n## Categorical Quantitative Insights","faf4e603":"<a id=\"Non_Categorical_Quantitative_Insights\"><\/a>\n## Non-Categorical Quantitative Insights","b67d6117":"*We can see that many features like `address`, `famsize`, `studytime` & other ones got a more meaningful data labels (e.g. `address` is not the actual address, but a binary label for the area the students live in) and I'm guessing the model will have an easier time to understand the data this way.*","b3495d26":"*Now lets classifiy by the above table criteria but since we are going to use it to classifiy averages, we will modify it by defining that everything below & including .5 will be labeled as the left level, above will be labeled as the right level.*\n\n**The modifed ranges are:**\n\n`(0, 9.5], (9.5, 11.5], (11.5, 13.5], (13.5, 15.5], (15.5, 20]`","ef82ec72":"<a id=\"Correlation_Analysis\"><\/a>\n## Correlation Analysis","2fe1a4d9":"*So because we have only 3 distinct values where one of them is way more frequent than the others, it is expected to happen.*","83f96f0f":"*Now we finally can get the accuracy scores of each best fold and at the same time get their precision & recall scores:*","139ea6b5":"<a id=\"Initial_Observation\"><\/a>\n# Initial Observation","36dbf5d6":"## Attribute Information: (taken from [archive.ics.uci.edu](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Student+Performance))\n### Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n#### 1. ***school*** - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n#### 2. ***sex*** - student's sex (binary: 'F' - female or 'M' - male)\n#### 3. ***age*** - student's age (numeric: from 15 to 22)\n#### 4. ***address*** - student's home address type (binary: 'U' - urban or 'R' - rural)\n#### 5. ***famsize*** - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n#### 6. ***Pstatus*** - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n#### 7. ***Medu*** - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n#### 8. ***Fedu*** - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n#### 9. ***Mjob*** - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n#### 10. ***Fjob*** - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n#### 11. ***reason*** - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n#### 12. ***guardian*** - student's guardian (nominal: 'mother', 'father' or 'other')\n#### 13. ***traveltime*** - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n#### 14. ***studytime*** - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n#### 15. ***failures*** - number of past class failures (numeric: n if 1<=n<3, else 4)\n#### 16. ***schoolsup*** - extra educational support (binary: yes or no)\n#### 17. ***famsup*** - family educational support (binary: yes or no)\n#### 18. ***paid*** - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n#### 19. ***activities*** - extra-curricular activities (binary: yes or no)\n#### 20. ***nursery*** - attended nursery school (binary: yes or no)\n#### 21. ***higher*** - wants to take higher education (binary: yes or no)\n#### 22. ***internet*** - Internet access at home (binary: yes or no)\n#### 23. ***romantic*** - with a romantic relationship (binary: yes or no)\n#### 24. ***famrel*** - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n#### 25. ***freetime*** - free time after school (numeric: from 1 - very low to 5 - very high)\n#### 26. ***goout*** - going out with friends (numeric: from 1 - very low to 5 - very high)\n#### 27. ***Dalc*** - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n#### 28. ***Walc*** - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n#### 29. ***health*** - current health status (numeric: from 1 - very bad to 5 - very good)\n#### 30. ***absences*** - number of school absences (numeric: from 0 to 93)\n\n## these grades are related with the course subject, Math or Portuguese:\n#### 31. ***G1*** - first period grade (numeric: from 0 to 20)\n#### 32. ***G2*** - second period grade (numeric: from 0 to 20)\n#### 33. ***G3*** - final grade (numeric: from 0 to 20, output target)\n\n","fe7e24e5":"**So what we actually see here?**\n\n- No one likes `school` - it has a low negative correlation with `address` & `annual_grades_avg`.\n\n- `sex` has low positive correlation with `Walc`.\n\n- `address` has low negative correlation with `traveltime`.\n\n- Each `grade` is positively highly correlated with each other (and obviously wtih `annual_grades_avg` since that feature linearly dependent on those 3 **G1, G2 and G3 will be dropped before modeling**).\n\n- `Medu` & `Fedu` have a medium positive correlation between them.\n\n- `Medu` has a low positive correlation with `Mjob`.\n\n- `age` has a low positive correlation with `failure`.\n\n- `studytime` has a low positive correlation with each `grade`.\n\n- `failrues` has a low negative correlation with `higher` & each `grade` (and the average).\n\n- `higher` has a low positive correlation with each `grade` (and the average).\n\n- Naturally, `freetime` has a low positive correlation with `goout`.\n\n- And if you already are going out, why not have a drink? there is a low positive correlation between `goout` with `Walc`.","d5228013":"<a id=\"Load_Dataset\"><\/a>\n# Load Dataset","043c77ee":"*Now lets get a better insight on our 5 level classification*","5ab8f7e2":"<a id=\"Dataset_Files\"><\/a>\n# Dataset Files","cdee70c0":"<a id=\"5_Level_classification_modification\"><\/a>\n# 5-Level classification modification\n\n![](https:\/\/i.imgur.com\/LULMfa5.png)\n\n> Source: P. Cortez and A. Silva. paper [which can be found here](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf)\n\n*Since we are going to classify our grades average by the above the table, we first need to add a column of grades average & label each grade.*","95ff0989":"<a id=\"intro\"><\/a>\n# Introduction\nBefore I even started with this kernel I saw that there this is an actual study (source can be found here [5-Level classification modification](#5_Level_classification_modification)) and I read the paper, the researcher had some ideas of what to actually learn from it, one of those ideas was the 5 level classification model, which **I took and added a little twist: instead of classifying the `G3` grade, I actually took the annual average and classified it according to that paper.**\n\nSince I didn't work that much with `Seaborn` until now, I wanted to see how much information I could extract from the data with the different types of plots, so there are a lot of those here.","6f1b2053":"<a id=\"Visualization\"><\/a>\n# Visualization","4ff150f9":"<a id=\"Classification_Modeling\"><\/a>\n# Classification Modeling","74aa1a02":"*Lets stack up our classifiers:*","799d145a":"<a id=\"Skewness_Kurtosis\"><\/a>\n## Skewness & Kurtosis"}}