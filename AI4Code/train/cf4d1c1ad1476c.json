{"cell_type":{"b0a36bb2":"code","e03f614c":"code","6a4e26ca":"code","4952c6b4":"code","9d4e9a66":"code","466b5e8e":"code","017b66b0":"code","6a5db4fd":"code","a3b0368c":"code","c152bc3a":"code","21d81d46":"code","87137ffb":"code","3eaf812f":"code","ff49b380":"code","2fabb147":"code","b21eb96f":"code","2e769c97":"code","93d671b7":"code","92dc4963":"code","22687387":"code","1b637b84":"code","cae147d7":"code","fa065d97":"code","18d7c3b2":"code","438dc333":"code","49b42b8b":"code","26dcb88f":"code","24dbbd54":"code","96890026":"code","60a02d09":"code","f7655607":"code","1fbdb4b8":"code","8f9c984d":"code","37c038f7":"code","21d89f7a":"code","ece1e9fa":"code","856ff3b3":"code","f6bf64a0":"code","376cb04e":"code","946736cc":"code","c05a51f6":"code","a47285ee":"code","63bfa45a":"markdown","e11841cb":"markdown","82917f1e":"markdown","acf29422":"markdown","160cfd73":"markdown","be45cc8f":"markdown","6fcb50a0":"markdown","d0c87e97":"markdown","89eabf52":"markdown","70348817":"markdown","54e5052b":"markdown","2d81f22b":"markdown","bd1566c8":"markdown","d9f08214":"markdown","cfb6b430":"markdown","08d4335a":"markdown","6f11d9e9":"markdown","6c2b1ad8":"markdown"},"source":{"b0a36bb2":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"JC84GCU7zqA\")","e03f614c":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"kBjYK3K3P6M\")","6a4e26ca":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer,BertTokenizer,TFBertModel\nfrom tqdm.notebook import tqdm\nimport plotly.express as px\nfrom collections import Counter\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom sklearn import model_selection","4952c6b4":"AUTO = tf.data.experimental.AUTOTUNE","9d4e9a66":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","466b5e8e":"# model_name = 'jplu\/tf-xlm-roberta-large'\nmodel_name = 'bert-base-multilingual-cased'\nn_epochs = 25\nmax_len = 100\n\n# Our batch size will depend on number of replicas\nbatch_size = 16 * strategy.num_replicas_in_sync\nprint(batch_size)","017b66b0":"train = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv\")","6a5db4fd":"print('Premise:', train.premise[0])\nprint('hypothesis:', train.hypothesis[0])\ndisplay(train.isnull().sum(axis = 0))\ndisplay(train.head())","a3b0368c":"def code_labels(label):\n    res = 'Entailment'\n    if label == 1:\n        res = 'Neutral'\n    elif label == 2:\n        res = 'Contradiction'\n    return (res)   ","c152bc3a":"train[\"Encoded\"] = train[\"label\"].apply(code_labels)","21d81d46":"encode = train[\"Encoded\"].value_counts()\nencode_df = pd.DataFrame({\"Encode\":encode.index,\"frequency\":encode.values})\nfig = px.bar(data_frame=encode_df,x=\"Encode\",y=\"frequency\",color=\"Encode\",text=\"frequency\",title=\"Target Column Distribution\",labels={\"Encode\":\"Type of Relationship\",\"frequency\":\"Counts\"})\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()\n","87137ffb":"language = train[\"language\"].value_counts()\nlanguage_df = pd.DataFrame({\"Languages\":language.index,\"frequency\":language.values})\nlanguage_df[\"count_percent\"] = language_df['frequency'].apply(lambda x: round(x*100\/language_df.frequency.sum(),2))\nfig = px.bar(data_frame=language_df,x=\"Languages\",y=\"frequency\",color=\"Languages\",title=\"Different Language Distribution\",text=\"frequency\")\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","3eaf812f":"test_language = test[\"language\"].value_counts()\ntest_language_df = pd.DataFrame({\"Languages\":test_language.index,\"frequency\":test_language.values})\nfig = px.bar(data_frame=test_language_df,x=\"Languages\",y=\"frequency\",color=\"Languages\",title=\"Different Language Distribution\",text=\"frequency\")\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","ff49b380":"english_text = train[train[\"language\"]==\"English\"]","2fabb147":"english_text.head()","b21eb96f":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","2e769c97":"def remove_stopword(x):\n    return [w for w in x if not w in stop]\n","93d671b7":"english_text['temp_list'] = english_text['premise'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in english_text['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","92dc4963":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', width=700, height=700,color='Common_words')\nfig.show()","22687387":"english_text['temp_list'] = english_text['temp_list'].apply(lambda x:remove_stopword(x))","1b637b84":"top = Counter([item for sublist in english_text['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')\n","cae147d7":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()\n","fa065d97":"# train[\"kfold\"] = -1\n# train = train.sample(frac=1).reset_index(drop=True)\n\n# y = train.label.values\n\n# kf =model_selection.StratifiedKFold(n_splits=5)\n\n# for f,(t_,v_) in enumerate(kf.split(X=train,y=y)):\n#     train.loc[v_,'kfold'] = f\n\n","18d7c3b2":"# tokenizer = AutoTokenizer.from_pretrained(model_name)","438dc333":"model_name = 'bert-base-multilingual-cased'\nmax_len = 80\ntokenizer = BertTokenizer.from_pretrained(model_name)","49b42b8b":"train.head()","26dcb88f":"# Convert the text so that we can feed it to `batch_encode_plus`\ntrain_text = train[['premise', 'hypothesis']].values.tolist()\ntest_text = test[['premise', 'hypothesis']].values.tolist()\n\n# Now, we use the tokenizer we loaded to encode the text\n# train_encoded = tokenizer.batch_encode_plus(\n#     train_text,\n#     pad_to_max_length=True,\n#     max_length=max_len\n# )\n\n# test_encoded = tokenizer.batch_encode_plus(\n#     test_text,\n#     pad_to_max_length=True,\n#     max_length=max_len\n# )","24dbbd54":"def quick_encode(values,maxlen):\n    tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)\n    return np.array(tokens['input_ids'])\n\nx_train = quick_encode(train_text,maxlen=max_len)\nx_test = quick_encode(test_text,maxlen=max_len)\ny_train = train.label.values\n    ","96890026":"# x_train = train_encoded[\"input_ids\"]\n# y_train = train.label.values\n# x_test = test_encoded['input_ids']","60a02d09":"# x_train, x_valid, y_train, y_valid = train_test_split(\n#     train_encoded['input_ids'], train.label.values, \n#     test_size=0.2, random_state=2020\n# )\n\n","f7655607":"# train_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_train, y_train))\n#     .repeat()\n#     .shuffle(2048)\n#     .batch(batch_size)\n#     .prefetch(AUTO)\n# )\n\n# valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_valid, y_valid))\n#     .batch(batch_size)\n#     .cache()\n#     .prefetch(AUTO)\n# )\n\n\n\ndef create_dist_dataset(X, y,val,batch_size=batch_size):\n    \n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n\n    \n    \n    return dataset\n\n\n\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","1fbdb4b8":"def build_lrfn(lr_start=0.00001, lr_max=0.00003, \n               lr_min=0.000001, lr_rampup_epochs=3, \n               lr_sustain_epochs=0, lr_exp_decay=.6):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n","8f9c984d":"lrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n","37c038f7":"def build_model(model_name):\n    # First load the transformer layer\n    transformer_encoder = TFBertModel.from_pretrained(model_name)\n\n    # This will be the input tokens \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Now, we encode the text using the transformers we just loaded\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Only extract the token used for classification, which is <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # Finally, pass it through a 3-way softmax, since there's 3 possible laels\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    return model","21d89f7a":"pred_test=np.zeros((test.shape[0],3))\nskf = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nval_score=[]\nhistory=[]\n\n\nfor fold,(train_ind,valid_ind) in enumerate(skf.split(x_train,y_train)):\n    \n    if fold < 4:\n    \n        print(\"fold\",fold+1)\n        \n       \n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        train_data = create_dist_dataset(x_train[train_ind],y_train[train_ind],val=False)\n        valid_data = create_dist_dataset(x_train[valid_ind],y_train[valid_ind],val=True)\n    \n        Checkpoint=tf.keras.callbacks.ModelCheckpoint(f\"bert-base-multilingual-cased.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min')\n        \n        with strategy.scope():\n#             transformer_layer = TFAutoModel.from_pretrained(MODEL)\n            model = build_model(model_name=model_name)\n            \n        \n\n        n_steps = len(train_ind)\/\/batch_size\n        print(\"training model {} \".format(fold+1))\n\n        train_history = model.fit(\n        train_data,\n        steps_per_epoch=n_steps,\n        validation_data=valid_data,\n        epochs=n_epochs,callbacks=[lr_schedule,Checkpoint],verbose=1)\n        \n        print(\"Loading model...\")\n        model.load_weights(f\"bert-base-multilingual-cased.h5\")\n        \n        \n\n        print(\"fold {} validation accuracy {}\".format(fold+1,np.mean(train_history.history['val_accuracy'])))\n        print(\"fold {} validation loss {}\".format(fold+1,np.mean(train_history.history['val_loss'])))\n        \n        val_score.append(train_history.history['val_accuracy'])\n        history.append(train_history)\n\n        val_score.append(np.mean(train_history.history['val_accuracy']))\n        \n        print('predict on test....')\n        preds=model.predict(test_dataset,verbose=1)\n\n        pred_test+=preds\/4\n        \n\n        \nprint(\"Mean Validation accuracy : \",np.mean(val_score))","ece1e9fa":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(n_epochs),hist.history['accuracy'],label='train accu')\n    plt.plot(np.arange(n_epochs),hist.history['val_accuracy'],label='validation acc')\n    plt.gca().title.set_text(f'Fold {i+1} accuracy curve')\n    plt.legend()\n","856ff3b3":"plt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(n_epochs),hist.history['loss'],label='train loss')\n    plt.plot(np.arange(n_epochs),hist.history['val_loss'],label='validation loss')\n    plt.gca().title.set_text(f'Fold {i+1} loss curve')\n    plt.legend()","f6bf64a0":"submission['prediction'] = np.argmax(pred_test,axis=1)\nsubmission.head()\nsubmission.to_csv(\"submission.csv\",index=False)","376cb04e":"\n# train_history = model.fit(\n#     train_dataset,\n#     steps_per_epoch=n_steps,\n#     validation_data=valid_dataset,\n#     epochs=n_epochs\n# )\n","946736cc":"# test_preds = model.predict(test_dataset, verbose=1)\n# submission['prediction'] = test_preds.argmax(axis=1)","c05a51f6":"# submission.to_csv('submission.csv', index=False)\n# submission.head()","a47285ee":"# hist = train_history.history\n# px.line(\n#     hist, x=range(1, len(hist['loss'])+1), y=['accuracy', 'val_accuracy'], \n#     title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n# )\n\n\n# px.line(\n#     hist, x=range(1, len(hist['loss'])+1), y=['loss', 'val_loss'], \n#     title='Model Loss', labels={'x': 'Epoch', 'value': 'Loss'}\n# )","63bfa45a":"## Train Data","e11841cb":"# TPU Configurations","82917f1e":"References\nhttps:\/\/www.kaggle.com\/xhlulu\/contradictory-watson-concise-keras-xlm-r-on-tpu","acf29422":"## Upcoming\n* More tuned Models \n* Creating Folds to train \n* More pre processing ","160cfd73":"![](https:\/\/lh3.googleusercontent.com\/pacQdCJFoCq5ME7h2FfKCTmd6HwoEnq38PzZZFpAIfuSs5kvL05luyNJo4BWQxHXBy2ij006yo_JPk2UGiZhuskcQDxX7xIqzEAZt0lLC9Kb6QQfR0_8aajJLRffpST4fPWGhsag)\n## Mixed precision floating point and bfloat16\n\nThe MXU computes matrix multiplications using bfloat16 inputs and float32 outputs. Intermediate accumulations are performed in float32 precision.\n\nNeural network training is typically resistant to the noise introduced by a reduced floating point precision. There are cases where noise even helps the optimizer converge. 16-bit floating point precision has traditionally been used to accelerate computations but float16 and float32 formats have very different ranges. Reducing the precision from float32 to float16 usually results in over and underflows. Solutions exist but additional work is typically required to make float16 work.\n\nThat is why Google introduced the bfloat16 format in TPUs. bfloat16 is a truncated float32 with exactly the same exponent bits and range as float32. This, added to the fact that TPUs compute matrix multiplications in mixed precision with bfloat16 inputs but float32 outputs, means that, typically, no code changes are necessary to benefit from the performance gains of reduced precision.\n\n    The use of bfloat16\/float32 mixed precision is the default on TPUs. No code changes are necessary in your Tensorflow code to enable it\n\n## Systolic arrays\n\nCPUs are made to run pretty much any calculation. Therefore, CPU store values in registers and a program sends a set of instructions to the Arithmetic Logic Unit to read a given register, perform an operation and register the output into the right register. This comes at some cost in terms of power and chip area.\n\nFor an MXU, matrix multiplication reuses both inputs many times,\nUnder the hood: XLA\n\nTensorflow programs define computation graphs. The TPU does not directly run Python code, it runs the computation graph defined by your Tensorflow program. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the Tensorflow graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. The compilation happens automatically as work is sent to the TPU. You do not have to include XLA in your build chain explicitly.\n## Using TPUs in Keras\n\nTPUs are supported through the Keras API as of Tensorflow 2.1. Keras support works on TPUs and TPU pods.\n\nDon't worry TPU is also supported in Pytorch, check out @abhishek, 4X Kaggle grandmaster's video on training BERT's in TPU\n\nDo check out System Architecture of TPU gives more detials of TPU configurations and various versions of TPU\n\nThis video explains in detail about main differences between TPUv2 and TPUv3","be45cc8f":"# training the Model","6fcb50a0":"# EDA","d0c87e97":"Target feature is equally distributed","89eabf52":"## What we found ?\n* There are a total of 15 languages\n\n* English is the most predominant language (56.68% from the total)\n\n* The rest of the languages have a similar distribution between 3.4% and 2.82% (the total of these languages is almost the other half of the dataset, a 55.68%)\n","70348817":"# Creating the model","54e5052b":"\n# What are TPU's?\n\nTPU's are holy grail of computers for any Machine Learning Practitioners! A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning. TPUs are hardware accelerators specialized in deep learning tasks. In this code lab, you will see how to use them with Keras and Tensorflow 2. Cloud TPUs are available in a base configuration with 8 cores and also in larger configurations called \"TPU pods\" of up to 2048 cores. The extra hardware can be used to accelerate training by increasing the training batch size.\nWhy TPUs?\n\nModern GPUs are organized around programmable \"cores\", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.\n\nThe following video from Kaggle explains the main components of TPU like systolic arrays and bfloat16 number formats, and how these two components of TPUs help reduce deep learning model training times","2d81f22b":"## MXU and VPU\n\nA TPU v2 core is made of a Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc. \nThe VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16-32 bit floating point format.","bd1566c8":"# TPU and [Resources](http:\/\/https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-tpu\/#2)","d9f08214":"# Splitting Train and Valid Dataset","cfb6b430":"# Encoding the text suitable to transformer Models","08d4335a":"## Creating a Tokenizer","6f11d9e9":"## Test Data","6c2b1ad8":"# Creating Train , valid and Test Data loader"}}