{"cell_type":{"79da2179":"code","461bb558":"code","2dab996b":"code","fa7a4327":"code","b7cd6976":"code","f7d078d2":"code","43b1d2d3":"code","0c84b539":"code","b8d04b65":"code","0840c4ce":"code","1a07da6e":"code","a9b9c785":"code","da3fe9f9":"code","2eb03c92":"code","cdcce008":"code","2c65166e":"code","f200db5e":"code","adcba8bd":"code","1c04d3b2":"code","989dca1b":"code","7e8d26d5":"code","b7de442b":"code","fb0f5bcd":"code","b92bf919":"code","936ca850":"markdown","63247c2d":"markdown","c0e01e22":"markdown","b0bbd624":"markdown","76129268":"markdown"},"source":{"79da2179":"!pip install nltk\nimport pandas as pd\nimport nltk\nimport re","461bb558":"messages = pd.read_csv(\"..\/input\/smsspamcollection\/SMSSpamCollection\",sep=\"\\t\",names=[\"label\",\"message\"])\nmessages.head()","2dab996b":"nltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nfrom nltk.stem import PorterStemmer\n#from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","fa7a4327":"#Stemming\nps = PorterStemmer()\n#wordnet = WordNetLemmatizer()\ncorpus = []\nfor i in range(0,len(messages)):\n    review = re.sub(\"[^a-zA-Z]\",\" \",messages[\"message\"][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\n    review = \" \".join(review)\n    corpus.append(review)","b7cd6976":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM","f7d078d2":"tf.__version__","43b1d2d3":"voc_size = 10000","0c84b539":"X = messages[\"message\"]\nY = messages[\"label\"]","b8d04b65":"encoded_output=pd.get_dummies(Y)\nencoded_output","0840c4ce":"encoded_output = encoded_output[\"spam\"]","1a07da6e":"#One hot encoding text data\nencoded_corpus = [one_hot(word,voc_size)for word in corpus]\nencoded_corpus","a9b9c785":"#finding max length of the sentence with max length from the corpus\nmax=len(encoded_corpus[i])\nfor i in range(0,len(encoded_corpus)-1):\n    if(len(encoded_corpus[i+1])>=max):\n        max=len(encoded_corpus[i+1])\n    else:\n        max=len(encoded_corpus[i])\nprint(max)","da3fe9f9":"sent_length = max","2eb03c92":"#Padding\npadded_corpus = pad_sequences(encoded_corpus,padding=\"pre\",maxlen=13)","cdcce008":"padded_corpus","2c65166e":"vector_features = 30\nmodel = Sequential()\nmodel.add(Embedding(voc_size,vector_features,input_length=sent_length))\nmodel.add(LSTM(50))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\nmodel.summary()","f200db5e":"import numpy as np\nX = np.array(padded_corpus)\nY = np.array(encoded_output)","adcba8bd":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)","1c04d3b2":"len(X_train)","989dca1b":"y_test","7e8d26d5":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=40)","b7de442b":"y_pred = model.predict_classes(X_test)","fb0f5bcd":"y_pred","b92bf919":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nprint(\"Accuracy score : \\n\",accuracy_score(y_test,y_pred))\nprint(\"Confusion matrix : \\n\",confusion_matrix(y_test,y_pred))\nprint(\"Classification report : \\n\",classification_report(y_test,y_pred))","936ca850":"# Reading Data","63247c2d":"# Checking Accuracy","c0e01e22":"# Creating Model","b0bbd624":"# Preprocessing","76129268":"# Model Fitting"}}