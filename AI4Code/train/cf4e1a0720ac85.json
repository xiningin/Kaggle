{"cell_type":{"f2725943":"code","515e5dd3":"code","a10b3526":"code","e6d23cde":"code","3c6a3dfc":"code","d605fb83":"code","63f04832":"code","06e46826":"code","7d9b0eb7":"code","dd0346c4":"code","2ba8e238":"code","3118f23f":"code","d57428b2":"code","17cafbfa":"code","e84b1f44":"code","1f723c7a":"code","c0d53055":"code","5b069829":"code","c32a36e0":"code","02d291a2":"code","0f3815d6":"code","64de36ef":"code","70267671":"code","c1229cc9":"code","76639f8b":"code","1eb439f6":"code","cf97db9b":"code","dbda736b":"code","bb2a0528":"code","6b44a930":"code","e276709a":"code","6dcd373f":"code","5f123475":"code","1aaebf0b":"code","5676a751":"code","37b87289":"code","623519c7":"code","e8e9c0c0":"code","9addcd06":"code","16a3877e":"code","a458a89f":"code","f15a5278":"code","139ae6a1":"code","37a1d414":"code","ba7480f5":"code","bd0af14a":"code","b1355e39":"code","5419185b":"code","0195060a":"code","86a71a88":"code","4b2fe4d0":"code","6090087c":"code","bc9d86e0":"code","5092f55e":"code","eef9b4a1":"code","d32833e3":"code","93e1c157":"code","bee9694c":"code","409c26eb":"code","fbc2b6d3":"code","6b543510":"code","3063576b":"code","a0a07723":"code","5bdbbec1":"code","e4280e2f":"code","be9742da":"markdown","8714f8b8":"markdown","9005e34a":"markdown","f4d0d69a":"markdown","9126014d":"markdown","d1155d52":"markdown","716d288a":"markdown","23e29e5b":"markdown","ef8d5b2e":"markdown","31a312d4":"markdown","fea4bea0":"markdown","3d89801a":"markdown","bb0d35ea":"markdown","7efc82c9":"markdown","a2ca525b":"markdown","039a254a":"markdown","a650c1bd":"markdown","489f50e3":"markdown","cfd966a6":"markdown","4ea0eb9e":"markdown","34f8a9a7":"markdown","580e2c90":"markdown","2feae1d1":"markdown","43ed1229":"markdown"},"source":{"f2725943":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport statsmodels.api as sm\nimport datetime\n\n%matplotlib inline\n\nimport seaborn as sns\n\nimport sklearn\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.feature_selection import RFE\n\npd.options.display.max_colwidth = 200\npd.options.display.max_rows=100\npd.options.display.max_columns=500\n\nimport warnings\nwarnings.filterwarnings('ignore')","515e5dd3":"data = pd.read_csv('..\/input\/surprise-housing-dataset\/train.csv')","a10b3526":"data.head()","e6d23cde":"data.shape","3c6a3dfc":"data.info()","d605fb83":"data.describe().T","63f04832":"# checking null values\nNA_col = data.isnull().sum()\n# find out columns which have nulls\nNA_col = NA_col[NA_col > 0]\n# % of columns missing\nprint(round(100*NA_col[NA_col > 0]\/len(data),2))","06e46826":"# Dropping columns which have high values missing + I\ndata.drop(['Id','LotFrontage','Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature', 'MoSold','Street', 'Utilities'], axis=1, inplace = True)","7d9b0eb7":"data.shape","dd0346c4":"# checking null values\nNA_col = data.isnull().sum()\n# find out columns which have nulls\nNA_col = NA_col[NA_col > 0]\n# % of columns missing\nprint(round(100*NA_col[NA_col > 0]\/len(data),2))","2ba8e238":"# Converting years to age\ndata['YearBuilt_Age'] = data['YearBuilt'].max() - data['YearBuilt']\ndata['YearRemodAdd_Age'] = data['YearRemodAdd'].max() - data['YearRemodAdd']\ndata['YrSold_Age'] = data['YrSold'].max() - data['YrSold']\ndata['GarageYrBlt_Age'] = data['GarageYrBlt'].max() - data['GarageYrBlt']\n\n# Dropping columns\ndata.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis=1, inplace = True)","3118f23f":"data[['YearBuilt_Age','YearRemodAdd_Age','YrSold_Age','GarageYrBlt_Age']].head(10)","d57428b2":"def treat_Missing_Values(df):    \n    \n    # checking null values\n    NA_col = df.isnull().sum()\n    # find out columns which have nulls\n    NA_col = NA_col[NA_col > 0]\n\n    for col in NA_col.index:\n        if df[col].dtype.name == 'object':\n            # impute mode\n            df[col].fillna(data[col].mode()[0], inplace=True)\n            \n        elif df[col].dtype.name == 'float64' or df[col].dtype.name == 'int64' or df[col].dtype.name == 'int32':\n            # impute median\n            df[col] = df[col].fillna((df[col].median()))\n            \n        else:\n            print('Unable to detect the datatype for col - ', col)\n            \n    return df","17cafbfa":"data = treat_Missing_Values(data)","e84b1f44":"# checking null values\nround(data.isnull().sum()\/len(data.index),2)[round(data.isnull().sum()\/ len(data.index),2).values>0.00]","1f723c7a":"f, ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(data.corr(), \n            xticklabels=data.corr().columns.values,\n            yticklabels=data.corr().columns.values,annot= True)\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\nplt.show()","c0d53055":"corr_val = data[list(data.dtypes[data.dtypes != 'object'].index)].corr()","5b069829":"corr_coef = corr_val[corr_val['SalePrice'] > 0.5]['SalePrice'].sort_values(ascending=False)\nprint(corr_coef[1:])\ncorr_coef_cols = [idx for idx in corr_coef.index]","c32a36e0":"# Draw Pair plot for the correlated features\nsns.pairplot(data, x_vars=corr_coef_cols[1:], y_vars=[corr_coef_cols[0]], kind=\"reg\" )","02d291a2":"# Let us check the SalePrice as well\nf, axes = plt.subplots(1, 2, figsize=(15,6))\nsns.boxplot(data['SalePrice'],  orient='v' , ax=axes[0])\nsns.distplot(data['SalePrice'], ax=axes[1])\nplt.show()","0f3815d6":"def label_encoding(colNames):\n    for colName in colNames:\n        unique_vals = data[colName].unique()\n        map_vals = {}\n        for idx, val in enumerate(unique_vals):\n            map_vals[val] = idx\n        data[colName] = data[colName].map(map_vals)","64de36ef":"cat_col_list = ['LandSlope', 'ExterQual', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'KitchenQual', \n                'GarageFinish', 'GarageQual', 'GarageCond', 'ExterCond', 'LotShape']\nlabel_encoding(cat_col_list)","70267671":"data[cat_col_list].head()","c1229cc9":"dummy_col_names = ['MSZoning','LandContour','LotConfig','Neighborhood','Condition1','Condition2','BldgType',\n             'HouseStyle','RoofStyle','RoofMatl','Exterior1st', 'Exterior2nd','MasVnrType','Foundation',\n             'Heating','Electrical','Functional','GarageType','PavedDrive','SaleType','SaleCondition']\ndummies = pd.get_dummies(data[dummy_col_names], drop_first = True)\ndata = pd.concat([data, dummies], axis = 1)\ndata.drop(dummy_col_names, axis = 1, inplace = True)","76639f8b":"data.head()","1eb439f6":"data.shape","cf97db9b":"# Transform SalePrice\ndata['SalePrice'] = np.log1p(data['SalePrice'])","dbda736b":"# Create train and test data\ndf_train, df_test = train_test_split(data, train_size=0.7, test_size=0.3, random_state=100)","bb2a0528":"# Scale data\nscaler_col = ['MSSubClass','LotArea','OverallQual','OverallCond', 'MasVnrArea','BsmtFinSF1',\n              'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF', 'LowQualFinSF',\n              'GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\n              'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF',\n              'OpenPorchSF','EnclosedPorch','3SsnPorch', 'ScreenPorch','PoolArea','MiscVal','SalePrice']\n\nscaler = StandardScaler()\ndf_train[scaler_col] = scaler.fit_transform(df_train[scaler_col])\ndf_test[scaler_col] = scaler.transform(df_test[scaler_col])","6b44a930":"df_train.head()","e276709a":"# Let us check if the target variable is normal in both train and test dataset\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(df_train.SalePrice)\nplt.subplot(122)\nsns.distplot(df_test.SalePrice)","6dcd373f":"# Create X and y\ny_train = df_train.pop('SalePrice')\nX_train = df_train\n\ny_test = df_test.pop('SalePrice')\nX_test = df_test","5f123475":"# RFE\nlm  = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm,50)\nrfe.fit(X_train,y_train)","1aaebf0b":"rfe_scores = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))","5676a751":"col = X_train.columns[rfe.support_]","37b87289":"col","623519c7":"# Modify the X_train and X_test\nX_train = X_train[col]\nX_test = X_test[col]","e8e9c0c0":"X_train.shape","9addcd06":"X_test.shape","16a3877e":"#Lasso\nlm = Lasso(alpha=0.001)\nlm.fit(X_train,y_train)\n\n# train score\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train,y_pred=y_train_pred))\n\n# test score\ny_test_pred  = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test,y_pred=y_test_pred))","a458a89f":"model_parameter = list(lm.coef_)\nmodel_parameter.insert(0,lm.intercept_)\nmodel_parameter = [round(x,3) for x in model_parameter]\ncol = X_train.columns\ncol = col.insert(0,'Constant')\nlist(zip(col,model_parameter))","f15a5278":"# Gridsearch Operation on Training data set\n# Objective - Find optimal value of alpha\n\nfolds = KFold(n_splits=10,shuffle=True,random_state=100)\n\nhyper_param = {'alpha':[0.001, 0.01, 0.1,1.0, 5.0, 10.0,20.0]}\n\nmodel = Lasso()\n\nmodel_cv = GridSearchCV(estimator = model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True\n                       )\n\nmodel_cv.fit(X_train,y_train)","139ae6a1":"cv_result_train_lasso = pd.DataFrame(model_cv.cv_results_)\ncv_result_train_lasso['param_alpha'] = cv_result_train_lasso['param_alpha'].astype('float32')\ncv_result_train_lasso.head()","37a1d414":"def r2_score(cv_result, is_log=False):\n    plt.figure(figsize=(12,6))\n    plt.plot(cv_result['param_alpha'], cv_result['mean_train_score'])\n    plt.plot(cv_result['param_alpha'], cv_result['mean_test_score'])\n    if is_log == True:\n        plt.xscale('log')\n    plt.ylabel('R2 Score')\n    plt.xlabel('Alpha')\n    plt.show()","ba7480f5":"r2_score(cv_result_train_lasso, True)","bd0af14a":"print('For Lasso, the Best Alpha value = ', model_cv.best_params_['alpha'])","b1355e39":"# Now that we have optimal value of alpha = 0.001, we can use this alpha to run the model again\n#Lasso\nlm = Lasso(alpha=0.001)\nlm.fit(X_train,y_train)\n\n# train score\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train,y_pred=y_train_pred))\n\n# test score\ny_test_pred  = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test,y_pred=y_test_pred))","5419185b":"#Ridge\nridge = Ridge(alpha=0.001)\nridge.fit(X_train,y_train)\n\n# train score\ny_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_train, y_train_pred))\n\n# test score\ny_test_pred = ridge.predict(X_test)\nprint(metrics.r2_score(y_test, y_test_pred))","0195060a":"# Gridsearch Operation on Training data set\n# Objective - Find optimal value of alpha\n\nfolds = KFold(n_splits=10,shuffle=True,random_state=100)\n\nhyper_param = {'alpha':[0.001, 0.01, 0.1,1.0, 5.0, 10.0,20.0]}\n\nmodel = Ridge()\n\nmodel_cv = GridSearchCV(estimator = model,\n                        param_grid=hyper_param,\n                        scoring='r2',\n                        cv=folds,\n                        verbose=1,\n                        return_train_score=True\n                       )\n\nmodel_cv.fit(X_train,y_train)","86a71a88":"cv_result_train_ridge = pd.DataFrame(model_cv.cv_results_)\ncv_result_train_ridge['param_alpha'] = cv_result_train_ridge['param_alpha'].astype('float32')\ncv_result_train_ridge.head()","4b2fe4d0":"# plot r2_score using the defined function for ridge\nr2_score(cv_result_train_ridge, True)","6090087c":"print('For Ridge, the Best Alpha value = ', model_cv.best_params_['alpha'])","bc9d86e0":"# Now that we have optimal value of alpha = 0.001, we can use this alpha to run the model again\n#Ridge\nridge = Ridge(alpha=10)\nridge.fit(X_train,y_train)\n\n# train score\ny_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_train, y_train_pred))\n\n# test score\ny_test_pred = ridge.predict(X_test)\nprint(metrics.r2_score(y_test, y_test_pred))","5092f55e":"# ridge coefficients\nmodel_parameter = list(ridge.coef_)\nmodel_parameter.insert(0,ridge.intercept_)\nmodel_parameter = [round(x,3) for x in model_parameter]\ncol = X_train.columns\ncol = col.insert(0,'Constant')\nlist(zip(col,model_parameter))","eef9b4a1":"def run_multiple_alphas(model,alphas):\n    \n    feature_ridge_df = pd.DataFrame(columns=['feature'], data=X_train.columns)\n    feature_lasso_df = pd.DataFrame(columns=['feature'], data=X_train.columns)\n    \n    for alpha in alphas:\n        if model == 'ridge':\n            ridge = Ridge(alpha=alpha)\n            ridge.fit(X_train, y_train)\n            # Creating feature\/coefficient map for future use\n            feature_ridge_df['Alpha: '+str(alpha)] = ridge.coef_\n        elif model == 'lasso':\n            lasso = Lasso(alpha=alpha)\n            lasso.fit(X_train, y_train)\n            # Creating feature\/coefficient map for future use\n            feature_lasso_df['Alpha: '+str(alpha)] = lasso.coef_\n    \n    if model == 'ridge':\n        return feature_ridge_df\n    else:\n        return feature_lasso_df","d32833e3":"lasso_df = run_multiple_alphas('lasso',[0.001,0.002,0.01,0.02,0.05,5])","93e1c157":"lasso_df.head()","bee9694c":"print(lasso_df[lasso_df['Alpha: 0.001'] == 0][['feature', 'Alpha: 0.001']].shape)\nprint(lasso_df[lasso_df['Alpha: 0.002'] == 0][['feature', 'Alpha: 0.002']].shape)\nprint(lasso_df[lasso_df['Alpha: 0.01'] == 0][['feature', 'Alpha: 0.01']].shape)\nprint(lasso_df[lasso_df['Alpha: 0.02'] == 0][['feature', 'Alpha: 0.02']].shape)\nprint(lasso_df[lasso_df['Alpha: 0.05'] == 0][['feature', 'Alpha: 0.05']].shape)\nprint(lasso_df[lasso_df['Alpha: 5'] == 0][['feature', 'Alpha: 5']].shape)","409c26eb":"# We know alpha = 0.001 is optimal value\nlasso_df = lasso_df[['feature','Alpha: 0.001', 'Alpha: 0.002']]\nlasso_df = lasso_df.reindex(lasso_df['Alpha: 0.002'].abs().sort_values(ascending=False).index)\nlasso_df['predictors'] = lasso_df['feature'].apply(lambda x:x.split('_')[0])","fbc2b6d3":"lasso_df.head(10)","6b543510":"x = lasso_df[['feature','Alpha: 0.002','predictors']]","3063576b":"ridge_df = run_multiple_alphas('ridge',[10,20])","a0a07723":"ridge_df.head()","5bdbbec1":"# We know alpha = 10 is optimal value\nridge_df = ridge_df[['feature','Alpha: 10','Alpha: 20']]\nridge_df = ridge_df.reindex(ridge_df['Alpha: 20'].abs().sort_values(ascending=False).index)\nridge_df['predictors'] = ridge_df['feature'].apply(lambda x:x.split('_')[0])","e4280e2f":"ridge_df.head(10)","be9742da":"### 4. EDA - find the variables which are significant to the target variable","8714f8b8":"#### Ridge alphas","9005e34a":"### 6. Model Building","f4d0d69a":"#### Target variable transformation","9126014d":"#### Heatmap","d1155d52":"Even though at this point we are not sure if the above variables are significant or not\nHowever We will need to treat the above columns","716d288a":"### 1. Read the Data","23e29e5b":"Observation\nBased on the above data, we can drop the following columns\n- LotFrontage\n- Alley\n- FireplaceQu\n- PoolQC\n- Fence\n- MiscFeature\n- Id (dropping this not because of count, irrelevant)\n- MoSold (dropping this not because of count, irrelevant)\n- Street (dropping this not because of count, irrelevant)\n- Utilities (dropping this not because of count, irrelevant)","ef8d5b2e":"### Ridge Regression Model","31a312d4":"#### Lasso alphas","fea4bea0":"We received the above from the training dataset.\nNow we need to run the model with alpha optimal = 0.001 test dataset and see the results","3d89801a":"The SalePrice is right skewed","bb0d35ea":"#### Label Encoding","7efc82c9":"#### Impute missing values","a2ca525b":"### Lasso Results on Test dataset\n    *** R2 score = 0.87 ***","039a254a":"### 2. See the info, stats to understand the data","a650c1bd":"### Conclusion - For Lasso, As we increase the alpha, more coefficients become zero ","489f50e3":"### 5. Feature engineering","cfd966a6":"# Advanced Regression Assignment\n\n## Niladri Banerjee\n## Date of Submission - July 20, 2020\n\n\n### Steps\n\n### 1. Read the Data\n### 2. See the info, stats to understand the data\n### 3. Data Treatment\n      - removing columns with high no of missing values \n      - missing value imputation\n      - derived features creation (convert the year columns into age cols)\n### 4. EDA - find the variables which are significant to the target variable\n      - pairplot  - numerical vars with target variable \n      - heatmap   - numerical variables - multicollinearity, target variable dependence\n      - goal    --> analyze which variables are significant visually\n### 5. Feature engineering\n      - label encoding \n      - one hot encoding\n      - target variable tarnsformation to normalize      \n### 6. Model Building\n      - Train-Test split (70%-30%)\n      - Scale data using Standard scaler\n      - Create X and y\n      - RFE [top 50 features]\n      - Build model for Lasso and Ridge\n      - Perform Gridsearch to get optimal values for lambda\n      - Re-run the model on test dataset using lambda optimal","4ea0eb9e":"Note\nThe SalePrice mean and median are not close and hence it should not ideally form a perfect normal distribution","34f8a9a7":"### Lasso Regression model","580e2c90":"### 3. Data Treatment","2feae1d1":"#### One Hot Encoding","43ed1229":"### Ridge Results on Test dataset\n    *** R2 score = 0.88 *** \n    - This is very close to Lasso result as well"}}