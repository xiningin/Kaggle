{"cell_type":{"3f8763af":"code","7547fa71":"code","ab7b38c0":"code","f4aa872b":"code","f42fd06b":"code","3ff95d84":"code","4aa1e932":"code","a60a2243":"code","124472b7":"code","1fe64bab":"code","e9729561":"code","e10db124":"code","9ba726c9":"code","bbe31965":"code","d2c48ee0":"code","a5258857":"code","483eeff1":"code","d51247cd":"code","f393b014":"code","ca94a1eb":"code","74d0036d":"code","9ca7ec6c":"code","a9155e04":"code","777471f7":"code","f4355269":"code","44357402":"code","d464a6b4":"code","089a22cc":"code","b0d3f3dd":"code","a6b0f3e5":"code","12f7f936":"code","f670ae69":"code","fa801f54":"code","1f00a96e":"markdown","3c269074":"markdown","c425db1b":"markdown","7d7abe69":"markdown","30b6f100":"markdown","6bc5f113":"markdown","a19ee085":"markdown","32e3fde1":"markdown","f8405804":"markdown","16505956":"markdown","c8f9f731":"markdown","3d33d720":"markdown","db826199":"markdown","712ceac3":"markdown","a58f107f":"markdown","70e0dbd3":"markdown","cb316830":"markdown","346b415a":"markdown","a1c230fa":"markdown","07c46cf5":"markdown","53b60872":"markdown","49c3f351":"markdown","d6dcfde0":"markdown","9623d299":"markdown","5c520bb3":"markdown","de166028":"markdown","87c99aef":"markdown","426aa444":"markdown","f041d2ca":"markdown","fd1c480d":"markdown","910d6357":"markdown"},"source":{"3f8763af":"from google.colab import drive\ndrive.mount('\/content\/gdrive\/')","7547fa71":"path = '\/content\/gdrive\/MyDrive\/kaggle\/'","ab7b38c0":"import os\nos.listdir(path)","f4aa872b":"import pandas as pd\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsubmission = pd.read_csv(path + 'sample_submission.csv')","f42fd06b":"print(train.shape)\nprint(test.shape)\nprint(submission.shape)","3ff95d84":"train.info()","4aa1e932":"train=train.iloc[:,1:]\ntest=test.iloc[:,1:]","a60a2243":"!pip install pycaret","124472b7":"from pycaret.regression import *","1fe64bab":"# Please put the name of column you want to predict in the target variable.\n\nreg_ = setup(data = train, target = 'target', session_id=123, n_jobs=-1, fold=3,\n                  normalize = True, transformation = True, transform_target = True, \n                  combine_rare_levels = True, rare_level_threshold = 0.05, \n                  remove_multicollinearity = True, multicollinearity_threshold = 0.95, \n                  use_gpu = True, log_experiment = True)","e9729561":"best_3_ = compare_models(sort = 'RMSE', n_select = 3)","e10db124":"lgbm = create_model('lightgbm', cross_validation = False)","9ba726c9":"gbr = create_model('gbr', cross_validation = False)","bbe31965":"rf = create_model('rf', cross_validation = False)","d2c48ee0":"tuned_lgbm = tune_model(lgbm, optimize = 'RMSE', n_iter = 100)","a5258857":"tuned_gbr = tune_model(gbr, optimize = 'RMSE', n_iter = 50)","483eeff1":"tuned_rf = tune_model(rf, optimize = 'RMSE', n_iter = 2)","d51247cd":"blender_specific = blend_models(estimator_list = [tuned_lgbm,tuned_gbr,tuned_rf], optimize = 'RMSE', fold = 5)","f393b014":"pred_holdout = predict_model(blender_specific)","ca94a1eb":"# residuals\n\nplot_model(blender_specific)","74d0036d":"# error\n\nplot_model(blender_specific, plot='error')","9ca7ec6c":"final_model = finalize_model(blender_specific)","a9155e04":"pred_holdout = predict_model(final_model, data = test)","777471f7":"pred_holdout","f4355269":"submission_blender = submission.copy()\nsubmission_blender.iloc[:,1] = pred_holdout.iloc[:,-1]\nsubmission_blender.to_csv('\/content\/gdrive\/MyDrive\/kaggle\/submission_blender_.csv', index = False)","44357402":"stacknet = stack_models(estimator_list = [tuned_gbr,tuned_rf,blender_specific], meta_model = tuned_lgbm, optimize = 'RMSE')","d464a6b4":"pred_holdout = predict_model(stacknet)","089a22cc":"# residuals\n\nplot_model(stacknet)","b0d3f3dd":"# error\n\nplot_model(stacknet, plot='error')","a6b0f3e5":"final_model = finalize_model(stacknet)","12f7f936":"pred_holdout = predict_model(final_model, data = test)","f670ae69":"pred_holdout","fa801f54":"submission_blender = submission.copy()\nsubmission_blender.iloc[:,1] = pred_holdout.iloc[:,-1]\nsubmission_blender.to_csv('\/content\/gdrive\/MyDrive\/kaggle\/submission_stacker__.csv', index = False)","1f00a96e":"## Read Data","3c269074":"## hecking the shapes of data","c425db1b":"## Predicting on test set for the competition","7d7abe69":"## Re-training the model on whole data","30b6f100":"## Define your path","6bc5f113":"## Setup the environment\n\n- In PyCaret you have to setup the environment before experimenting with the models. It can be done by using 'setup' method. \n- I also added a data refining option because this data is raw to fit in the model.\n- In setup stage, PyCaret automatically interprets column types of the given data and asks the user if it has intepreted it correctly. You can customize whether you want each columns to be interpreted differently by using the parameters in setup method. In this tutorial we will just go with the automatic interpretation by pressing 'enter'. \n- Also, it asks the ratio of dataset used to contruct train\/validation sets. We will use 100% of the dataset so just press 'enter' again. ","a19ee085":"## Hyperparameter Tuning\n\n- It automatically tunes the hyperparameters of the three selected models.","32e3fde1":"- The probability values are stored on 'target' column. So we will write them on our submission format and submit on Kaggle.\n- You will probabily get around 0.726 RMSE. Similarly, the actual public score is 0.72588.","f8405804":"- The probability values are stored on 'target' column. So we will write them on our submission format and submit on Kaggle.\n- You will probabily get around 0.7215 RMSE. Similarly, the actual public score is 0.72060.","16505956":"## Predicting on test set for the competition\n- We will now use the re-trained model on the test set for the competition","c8f9f731":"- We will now create a 'stacking model' using four models, including the previous mixed model. Stacking is a method for combining multiple classification or regression models.\n\n- In order to optimize the score for this competition we have to predict probabilities, we will stack predicted value of the three models(gbr, rf, blending model) and predict the data stacked with meta-model(lgbm). I use 'stack_models' method this time.","3d33d720":"## Model Stacking","db826199":"- Until now we have splitted the given train data into another train \/ validation sets to experiment. So the models are not trained on the full training data set.\n- We will train the model on the whole dataset for the most optimal performance. ","712ceac3":"# AutoML by PyCaret","a58f107f":"## Model Ensemble","70e0dbd3":"- Train\/Test Similar Distribution\n\n- Explanatory ability is very low because not all variables are numerical data.\n\n- Train\/Test both appear to need outliers removed.\n\n- It has data structures that violate regression assumptions such as linearity, equivariability, etc.","cb316830":"## Prediction\n\n- We will use the ensembled model on predicting unseen data.\n- There is already a hold-out set constucted on our environment so we will test on it to evaluate the performance.","346b415a":"\n- Reference\n    - https:\/\/pycaret.org\/regression1\/\n    - https:\/\/www.kaggle.com\/qkrwlsdn96\/automl-by-pycaret-base-pb-score-0-87669","a1c230fa":"## Import methods for regression problem","07c46cf5":"## Train models and compare\n\n- Now we have constructed the environment, we will now train and compare the default models provided in PyCaret\n- By using 'compare_models' method we can easily train and compare 16 default models provided in the package\n- We will select top 3 models in terms of RMSE, that is because the evaluation metric for this competition is RMSE","53b60872":"In this notebook I will use an AutoML package called PyCaret to enter '30 Days of ML' competitions with structured data. If you use this package, you can easily make prediction without any feature engineering and trained the models without model tuning. I just want to see which regression model learns better in this data. Then, I will use 'Bayesian Optimization' which is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. So, I expect better scores if we engineer additional feature and tune the models. \n\n\n","49c3f351":"- We will now use the re-trained model on the test set for the competition","d6dcfde0":"- We got a pretty decent model with RMSE of 0.726","9623d299":"I'm not used to kaggle yet, so you can't see the results in this notebook. I'm sorry, but if you come into my github, you can see my notebook with the results of the code.\n\nhttps:\/\/github.com\/wjdghwo\/Kaggle-30-Days-of-ML\/blob\/main\/%5B30%20Days%20of%20ML%5D%20Competition_If%20machine%20learning%20is%20hard%2C%20try%20using%20AutoML.ipynb","5c520bb3":"- In conclusion, AutoML can easily and quickly use to check data, select models, and tune models with simple codes. It also applied complex models such as blending and stacking model. \n\n- However, the hyperparameter tuning and model selection required a lot of time and RAM to get the higher score.\n\n- Also, the package is not free to use, which seems to difficult higher scores than current performance. \n\n- Therefore, it seems useful for identifying data and setting target scores, selecting model simply.","de166028":"\n- We will now ensemble the three models. In order to optimize the score for this competition we have to predict probabilities, we we will ensemble the three models using 'blend_models' method.\n","87c99aef":"- Train\/Test Similar Distribution\n\n- It is relatively evenly distributed rather than concentrated on a particular forecast than on the previous ensemble graph.\n\n- But it seems necessary to deal with anomalies.\n\n- The RMSE was better than the previous blending model.","426aa444":"- Light Gradient Boosting Machine, Gradient Boosting Regressor, and Random Forest Regressor\tare the best 3 models.\n- The boosting model had good performance.","f041d2ca":"## Install PyCaret","fd1c480d":"## Prediction\n\n- We will use the stacking model on predicting unseen data.\n- There is already a hold-out set constucted on our environment so we will test on it to evaluate the performance.","910d6357":"## Re-training the model on whole data\n- Until now we have splitted the given train data into another train \/ validation sets to experiment. So the models are not trained on the full training data set.\n- We will train the model on the whole dataset for the most optimal performance. "}}