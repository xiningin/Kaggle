{"cell_type":{"5a46043e":"code","0bc392a1":"code","4ec92661":"code","97b9478a":"code","03ec7860":"code","2f1e395e":"code","f4e7f26f":"code","fb50885f":"code","71381a2c":"code","4b34ffd4":"code","9a4d3563":"code","e9a152ba":"code","7aac3825":"code","243aec05":"code","42b27489":"code","4fdec879":"code","3731ba92":"code","bc7f9af1":"code","dfa7d6bd":"code","6aedd8f1":"code","4b17cf05":"code","98be8d43":"code","fc9ee91d":"code","067d09ad":"code","701db0bb":"code","3d0b1d60":"code","a80c9757":"code","1a4c115d":"code","4b132160":"code","223f599b":"code","447b3f20":"code","b10aca55":"code","5cd96910":"code","42c3ade7":"code","83ea3b01":"code","7249dacb":"code","ddcf8575":"code","181a6535":"code","a62c8c25":"code","3afecc0f":"code","b84ed5d7":"code","da0c02c5":"markdown","b2a7d1b6":"markdown","4c49f07e":"markdown","77c16e78":"markdown","c4460375":"markdown","d0272e84":"markdown","d1b8cd48":"markdown","e16bc490":"markdown","d7676867":"markdown","db0c8835":"markdown","29aba2c6":"markdown","3bbf4dd6":"markdown","cbdb752a":"markdown","6c63b320":"markdown","b988f30c":"markdown","81a76e12":"markdown","70a9db2a":"markdown","9923f4a6":"markdown","669d835e":"markdown","38830383":"markdown","b2479f9c":"markdown","4893e0f1":"markdown","8c5ad5bc":"markdown","e476f3d2":"markdown","5ebdcd4c":"markdown","1d2ef23b":"markdown","b35a0000":"markdown","2cda537a":"markdown","434a62b3":"markdown","ca67faa2":"markdown","c2f6da44":"markdown","6657ecca":"markdown","6cd8c2a5":"markdown","f91be6ae":"markdown"},"source":{"5a46043e":"import random\nimport time\nimport sys\nimport gc\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\nfrom tensorflow.keras.models import Model\nimport pickle\nfrom tqdm import tqdm\nimport transformers","0bc392a1":"import warnings\nwarnings.filterwarnings('ignore')","4ec92661":"df_movie_title_filtered=pd.read_pickle('..\/input\/10-netflix-movie-recommender-part-3\/df_movie_titles_filtered.pkl')\ndf_train_filtered=pd.read_pickle('..\/input\/10-netflix-movie-recommender-part-3\/df_train_filtered.pkl')\ndf_val_filtered=pd.read_pickle('..\/input\/10-netflix-movie-recommender-part-3\/df_val_filtered.pkl')\ndf_test_filtered=pd.read_pickle('..\/input\/10-netflix-movie-recommender-part-3\/df_test_filtered.pkl')","97b9478a":"def create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in docs:\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = [all_ids, all_masks]\n    \n    return encoded","03ec7860":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","2f1e395e":"df_movie_title_filtered['Movie']=df_movie_title_filtered.index","f4e7f26f":"MAX_SEQ_LENGTH = 300 # slightly larger than 274\n\nfeature_id_dic={}\nfeature_mask_dic={}\n\nfor i in tqdm(range(df_movie_title_filtered.shape[0])):\n    movie=df_movie_title_filtered.iloc[i, 3]\n    overview=df_movie_title_filtered.iloc[i, 2]\n    temp=create_bert_input_features(tokenizer, [overview], max_seq_length=MAX_SEQ_LENGTH)\n    feature_id_dic[movie] = temp[0][0]\n    feature_mask_dic[movie] = temp[1][0]","fb50885f":"inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\ninp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\ninputs = [inp_id, inp_mask]\n\nlayer=transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\nlayer.trainable=False\nhidden_state = layer(inputs)[0]\nprint(hidden_state.shape)\noutput = hidden_state[:, 0]\n\nmodel = Model(inputs=[inp_id, inp_mask], outputs=output)\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5))","71381a2c":"bert_dic={}\n\nfor movie in feature_id_dic:\n    inp_id=np.array([feature_id_dic[movie]])\n    inp_mask=np.array([feature_mask_dic[movie]])\n    bert_dic[movie]=list(model.predict([inp_id, inp_mask])[0])","4b34ffd4":"l1=list(df_train_filtered['User'].unique())\nl2=list(df_val_filtered['User'].unique())\nl3=list(df_test_filtered['User'].unique())\nlen(l1), len(l2), len(l3)","9a4d3563":"check=True\nl1_dic={}\n\nfor ele in l1:\n    l1_dic[ele]=True\n    \nfor ele in l2:\n    if ele not in l1_dic:\n        check=False\n        break\n        \nfor ele in l3:\n    if ele not in l1_dic:\n        check=False\n        break\n\nif True:\n    print('no problem')\nelse:\n    print('error')","e9a152ba":"l1=list(df_train_filtered['Movie'].unique())\nl2=list(df_val_filtered['Movie'].unique())\nl3=list(df_test_filtered['Movie'].unique())\nlen(l1), len(l2), len(l3)","7aac3825":"check=True\nl1_dic={}\n\nfor ele in l1:\n    l1_dic[ele]=True\n    \nfor ele in l2:\n    if ele not in l1_dic:\n        check=False\n        break\n        \nfor ele in l3:\n    if ele not in l1_dic:\n        check=False\n        break\n\nif True:\n    print('no problem')\nelse:\n    print('error')\n\ndel l1, l2, l3","243aec05":"user_id_mapping = {id:i for i, id in enumerate(df_train_filtered['User'].unique())}","42b27489":"train_user=df_train_filtered['User'].map(user_id_mapping).values\nval_user=df_val_filtered['User'].map(user_id_mapping).values\ntest_user=df_test_filtered['User'].map(user_id_mapping).values","4fdec879":"train_movie=df_train_filtered['Movie'].values\nval_movie=df_val_filtered['Movie'].values\ntest_movie=df_test_filtered['Movie'].values","3731ba92":"# Number of minimum votes to be considered\nm = 1000\n\n# Mean rating for all movies\nC = df_train_filtered['Rating'].mean()\n\n# Mean rating for all movies separately\nR = df_train_filtered.groupby(['Movie']).mean()['Rating'].values\n\n# Rating freqency for all movies separately\nv = df_train_filtered.groupby(['Movie']).count()['Rating'].values","bc7f9af1":"# Weighted formula to compute the weighted rating\nweighted_score = v\/(v+m)*R+m\/(v+m)*C\ndel m, C, R, v\n_=gc.collect()","dfa7d6bd":"index=np.sort(df_train_filtered['Movie'].unique())","6aedd8f1":"ranked=[]\nfor i, ele in enumerate(index):\n    ranked.append([ele, weighted_score[i]])\n\nranked.sort(key=lambda x : x[1], reverse=True)","4b17cf05":"from collections import OrderedDict\n\nrank_dic=OrderedDict()\nfor ele in ranked:\n    rank_dic[ele[0]]=ele[1]","98be8d43":"train_Y=df_train_filtered['Rating'].values.copy()\nval_Y=df_val_filtered['Rating'].values.copy()\ntest_Y=df_test_filtered['Rating'].values.copy()","fc9ee91d":"train_Y-=df_train_filtered['Movie'].map(rank_dic).values\nval_Y-=df_val_filtered['Movie'].map(rank_dic).values\ntest_Y-=df_test_filtered['Movie'].map(rank_dic).values","067d09ad":"def batch_generator(X0, X1, Y, batch_size): \n    number_of_batches = X0.shape[0]\/\/batch_size\n    number_of_batches = 1000\n    counter=0\n    shuffle_index = np.arange(np.shape(Y)[0])\n    np.random.shuffle(shuffle_index)\n    X0 =  X0[shuffle_index]\n    X1 =  X1[shuffle_index]\n    Y =  Y[shuffle_index]\n    while 1:\n        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n        X0_batch = X0[index_batch]\n        X1_batch = X1[index_batch]\n        X1_batch = np.array(pd.Series(X1_batch).map(bert_dic).tolist())\n        Y_batch = Y[index_batch]\n        counter += 1\n        yield [X0_batch, X1_batch], Y_batch\n        if (counter >= number_of_batches):\n            np.random.shuffle(shuffle_index)\n            counter=0","701db0bb":"# reserve some spaces for new users, for users who haven't rated any movies, recommend weighted average\nusers=int(len(user_id_mapping)*1.1)\nembedding_size = 100\n\n# use Input() to create tensors for - 'user' and 'movie'\nuser_id_input = Input(shape=(1,), name='user')\n\n# Create embedding layer for users \nuser_embedding = Embedding(output_dim=embedding_size, \n                           input_dim=users,\n                           input_length=1, \n                           embeddings_regularizer=tf.keras.regularizers.l2(0.0001),\n                           name='user_embedding')(user_id_input)\n\nuser_vector = Reshape([embedding_size])(user_embedding)\n#################################################################################################\n\ninp_bert = Input(shape=(768,), name='movie')\nmovie_vector = Dense(embedding_size)(inp_bert)\n\n################################################################################################\n\noutput = Dot(1, normalize=False)([user_vector, movie_vector])\n\nmodel = Model(inputs=[user_id_input, inp_bert], outputs=output)\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4))","3d0b1d60":"val_bert=np.array(pd.Series(val_movie).map(bert_dic).tolist())","a80c9757":"batch_size=1024*16\nnb_epoch=3\nsteps_per_epoch=train_user.shape[0]\/\/batch_size\n\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                      patience=1,\n                                      restore_best_weights=True,\n                                      verbose=1)\n\nmodel.fit_generator(generator=batch_generator(train_user, train_movie, train_Y, batch_size),\n                    epochs=nb_epoch,\n                    callbacks=[es],\n                    validation_data=([val_user, val_bert], val_Y),\n                    steps_per_epoch=steps_per_epoch)","1a4c115d":"# model = keras.models.load_model('.\/content_based.h5')","4b132160":"model.save('content_based.h5')","223f599b":"def make_pred(user_id, movie_id, model, rank_dic):\n    if type(user_id)!=type('asdf'):\n        print('please enter a string for user id')\n        return None\n    if movie_id not in rank_dic:\n        print('movie id non-existent')\n        \n    if user_id not in user_id_mapping:\n        user=users-1\n    else:\n        user=user_id_mapping[user_id]\n        \n    movie=bert_dic[movie_id]\n    pred=model.predict([np.array([user]), np.array([movie])])[0,0]\n    pred+=rank_dic[movie_id]\n    if pred<1: pred=1\n    elif pred>5: pred=5\n    return pred","447b3f20":"movie_id=list(rank_dic.keys())[13]\nmake_pred('asdf', movie_id, model, rank_dic), rank_dic[movie_id]","b10aca55":"movie_id=list(rank_dic.keys())[13]\nuser_id=list(df_test_filtered['User'].unique())[7]\nmake_pred(user_id, movie_id, model, rank_dic), rank_dic[movie_id]","5cd96910":"test_bert=np.array(pd.Series(test_movie).map(bert_dic).tolist())\n\ny_pred=model.predict([test_user, test_bert]).flatten()\n\n# add back the weighted score\ny_pred+=df_test_filtered['Movie'].map(rank_dic).values\n\n# clip the predicted score that's lower than 1 or larger than 5\ny_pred = np.array(list(map(lambda x: 1.0 if x < 1 else 5.0 if x > 5.0 else x, y_pred)))\n\ny_true = test_Y.copy()\ny_true+=df_test_filtered['Movie'].map(rank_dic).values\n\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\nmae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\nprint(\"The RMSE Value for the content-based recommender:\", rmse)\nprint(\"The MAE Value for the content-based recommender:\", mae)","42c3ade7":"y_pred = df_test_filtered['Movie'].map(rank_dic).values\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\nmae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\nprint(\"The RMSE Value for the global rated average recommender:\", rmse)\nprint(\"The MAE Value for the global rated average recommender:\", mae)","83ea3b01":"test_user_ids=list(df_test_filtered['User'].unique())\n\ntemp={}\nfor ele in test_user_ids:\n    temp[ele]=True\ntest_user_ids=temp\n\ndel temp\n\nlen(test_user_ids)","7249dacb":"# {user_id: [movie_id_1, movie_id_2, ...], ...}\ntrain_movie_ids={}\n# {user_id: average_rating, ...}\ntrain_movie_average_ratings={}\nct=0\n\nfor i in range(df_train_filtered.shape[0]):\n    if i % (df_train_filtered.shape[0]\/\/100)==0: \n        print(str(ct)+' percent of job done')\n        ct+=1\n        \n    user = df_train_filtered.iloc[i, 0]\n    if user in test_user_ids:\n        try:\n            train_movie_ids[user].append(df_train_filtered.iloc[i,2])\n            train_movie_average_ratings[user]+=df_train_filtered.iloc[i,1]\n        except:\n            train_movie_ids[user]=[df_train_filtered.iloc[i,2]]\n            train_movie_average_ratings[user]=df_train_filtered.iloc[i,1]\n\ndel user","ddcf8575":"for key in test_user_ids:\n    train_movie_average_ratings[key]\/=len(train_movie_ids[key])","181a6535":"# {user_id: [movie_id_1, movie_id_2, ...], ...}\ntest_movie_ids={}\n\nfor i in range(df_test_filtered.shape[0]):\n    user=df_test_filtered.iloc[i,0]\n    if df_test_filtered.iloc[i,1]>train_movie_average_ratings[user]:\n        try:\n            test_movie_ids[user].append(df_test_filtered.iloc[i,2])\n        except:\n            test_movie_ids[user]=[df_test_filtered.iloc[i,2]]","a62c8c25":"def average_precision_at_k(rel: [int], pred: [int], k: int) -> float:\n    # this function works only for a single user\n    # rel is an list of movie id's for all relevant movies in the test set \n    # pred is the prediction of the model excluding those ratings already in the training set\n    # pred is a list of movie id's whose scores are ranked from high to low\n    # len(pred) should be large enough for k\n    # k is the cutoff\n    temp=0\n    true_positive=0\n    for i in range(min(k,len(pred))):\n        if pred[i] in rel:\n            true_positive+=1\n            temp+=true_positive\/(i+1)\n    return temp\/len(rel)","3afecc0f":"result1=[]\nresult2=[]\nuser_list=list(test_movie_ids.keys())\n\nmile=len(user_list)\/\/100\nct=0\n\nfor i in range(len(user_list)):\n    \n    if ct%mile==1: \n        print(ct\/\/mile)\n        print('content-based method', sum(result1)\/len(result1))\n        print('global method ', sum(result2)\/len(result2))\n    ct+=1\n\n    user=user_list[i]\n\n    rank_dic_copy=rank_dic.copy()\n\n    rel=test_movie_ids[user]\n\n    pred=[]\n    # list of movies already rated in train set\n    already=train_movie_ids[user]\n\n    # remove movies that are already in the train set\n    for ele in already:\n        rank_dic_copy.pop(ele)\n\n    # save a copy of movie ids\n    aaa=list(rank_dic_copy.keys())\n    \n    # map movie ids to movie vocabulary number\n    X_bert=np.array(pd.Series(np.array(aaa)).map(bert_dic).tolist())\n    # map user ids to user vocabulary number\n    X_user=pd.Series(np.array([user for i in range(X_bert.shape[0])])).map(user_id_mapping).values\n\n    Y=model.predict([X_user, X_bert])\n    Y=Y[:,0]\n    Y=list(Y)\n\n    pred=[]\n    for iii, y in enumerate(Y):\n        pred.append([aaa[iii], y+rank_dic[aaa[iii]]])\n\n    # sort by score from high to low\n    pred.sort(key=lambda x : x[1], reverse=True)\n    pred=np.array(pred)\n    pred=pred[:,0]\n    pred=list(pred)\n    result1.append(average_precision_at_k(rel, pred, 100000))\n    \n    rank_dic_copy=rank_dic.copy()\n    for ele in train_movie_ids[user]:\n        rank_dic_copy.pop(ele)\n    pred=list(rank_dic_copy.keys())\n    result2.append(average_precision_at_k(rel, pred, 100000))","b84ed5d7":"print('mean average precision for content-based method is {}'.format(sum(result1)\/len(result1)))\nprint('mean average precision for global method is {}'.format(sum(result2)\/len(result2)))","da0c02c5":"### define a function that makes prediction","b2a7d1b6":"### import dependencies","4c49f07e":"**map movie ids to movie overview embeddings**","77c16e78":"## import data","c4460375":"**list of unique users in the test**","d0272e84":"### MAP@K Evaluation","d1b8cd48":"**define a function that calculates map@k**","e16bc490":"**user_id_mapping**","d7676867":"**create a function that makes BERT input features from overview text** <br>\n**the function is copied from: https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\/blob\/master\/notebooks\/6%20-%20Transformers%20-%20DistilBERT.ipynb**","db0c8835":"**our dataset will be too large in the original form of shape=(m, 768), so we need to define a batch_generator**","29aba2c6":"**create dictionaries that store movie id's and the average ratings of users in the train set**","3bbf4dd6":"## prepare tokens and masks BERT model","cbdb752a":"**as you can see, for new users, we simply predict weighted average ratings of movies**","6c63b320":"## model evaluation","b988f30c":"# In Part 3 of this project, we build a content-based movie recommender. For each movie, input its overview (a short description of the movie) to distilled BERT for document embedding (a 768 dimensional vector). Learn a user-embedding whose dot product with the document embedding yields the predicted movie rating. Eventually we will evaluate the model performace by calculating RMSE, MAE and Mean Average Precision and compare the result with the global movie recommender baseline.","81a76e12":"**convert user_id using user_id_mapping**","70a9db2a":"**bert_dic is the actual dictionary that maps movie ids to movie overview embeddings**","9923f4a6":"**compare that with global recommender, we can see that content-based method is better**","669d835e":"### MAE and RMSE evaluation","38830383":"**calculate the weighted mean, don't use the result from part 1 becasue C=mean rating for all movies in Part 3 is different from Part 1 and Part 2**","b2479f9c":"## use a BERT model to do paragraph embedding. We won't train this model","4893e0f1":"**create movie id feature**","8c5ad5bc":"**divide the sum of ratings by the number of movies to get average rating for each user**","e476f3d2":"**make sure all users in val and test sets are included in train set**","5ebdcd4c":"**test_movie_ids is a dicitionary that stores positively rated movies in the test sets for each user in the test sets**","1d2ef23b":"**remove useless warnings**","b35a0000":"**make sure all movies in val and test sets are included in train set**","2cda537a":"**interestingly, the mean average precision of the content-based method is worse than the global method**","434a62b3":"**create labels**","ca67faa2":"**loop through users in test set and apply the function we defined**","c2f6da44":"**subtract the weighted average score from the label**","6657ecca":"0<br>\ncontent-based method 0.008928571428571428<br>\nglobal method  0.010101010101010102<br>\n1<br>\ncontent-based method 0.04016841820328236<br>\nglobal method  0.04021255361364549<br>\n2<br>\ncontent-based method 0.0366935745946676<br>\nglobal method  0.03933852433696072<br>\n3<br>\ncontent-based method 0.03789360292137612<br>\nglobal method  0.03884620077499362<br>\n4<br>\ncontent-based method 0.03616059140682855<br>\nglobal method  0.03879868984411381<br>\n5<br>\ncontent-based method 0.03745153140086199<br>\nglobal method  0.039678011828391826<br>\n...","6cd8c2a5":"## model building","f91be6ae":"The result of the traning is lost. But we did train the model. Improvement after 1 epoch is not significant"}}