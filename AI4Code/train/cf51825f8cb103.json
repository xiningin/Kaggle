{"cell_type":{"9f1d515a":"code","25bca810":"code","0cc01794":"code","7167b8be":"code","721d44a0":"code","a3e32cd8":"code","24708f4c":"code","ff8b46aa":"code","39554a1b":"code","d5fdbdff":"code","4227f291":"code","fd257b26":"code","7a909db1":"code","8e44ab03":"code","3e14fa3c":"code","c58c6815":"markdown","70a7f96e":"markdown","29bce169":"markdown","3957ae5e":"markdown","90bb052e":"markdown","3136ba1d":"markdown","1442ad00":"markdown","63b2c831":"markdown","93b78e29":"markdown","c6a5714f":"markdown","08e1d0d4":"markdown","6bbe0003":"markdown","97088da9":"markdown","566fff3d":"markdown","8138c93f":"markdown"},"source":{"9f1d515a":"import struct\nimport random\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom array import array\nfrom os.path  import join\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator","25bca810":"X_train = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")\n\n#Extracting the labels from the data\ny_train = X_train[\"label\"]\ny_test = X_test[\"label\"]\n\n#Dropping the labels from the X_train and X_test dataframe\nX_train.drop(columns = [\"label\"], inplace = True)\nX_test.drop(columns = [\"label\"], inplace = True)\n\n#Checking the shapes of the dataframes\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","0cc01794":"X_train = X_train\/255\nX_test = X_test\/255","7167b8be":"#Reshaping the data from (m,784) to (m,28,28)\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)","721d44a0":"plt.figure(figsize = (2,2))\nplt.imshow(X_train[108].reshape(28, 28) , cmap = \"gray\");","a3e32cd8":"#Augmenting the image data to prevent overfitting\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","24708f4c":"plt.figure(figsize = (12,5))\nsns.countplot(y_train);","ff8b46aa":"def getCNNModel(shape):\n    \n    model = Sequential([\n        Conv2D(64, (3,3), strides = 1, padding = \"same\", input_shape = shape, activation = \"relu\"),\n        BatchNormalization(),\n        MaxPool2D((2,2), strides =2, padding = \"same\"),\n        Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n        Dropout(0.2),\n        BatchNormalization(),\n        MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n        Conv2D(16 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'),\n        BatchNormalization(),\n        MaxPool2D((2,2) , strides = 2 , padding = 'same'),\n        Flatten(),\n        Dense(512 , activation = 'relu'),\n        Dropout(0.3),\n        Dense(units = 25 , activation = 'softmax')\n    ])\n    \n    return model","39554a1b":"model = getCNNModel(X_train[0].shape)\nmodel.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","d5fdbdff":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.5, min_lr=0.00001)","4227f291":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","fd257b26":"history = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_val, y_val) , \n                    callbacks = [learning_rate_reduction])","7a909db1":"print(\"Test Accuracy \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","8e44ab03":"y_pred = model.predict_classes(X_test)","3e14fa3c":"dfConfMat = pd.DataFrame(confusion_matrix(y_test,y_pred))\nplt.figure(figsize = (15,15))\nsns.heatmap(dfConfMat,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='');","c58c6815":"# Model Evaluation","70a7f96e":"While loading the data, we can see from the description that the imported data contains both the predictors and the labels. We hence separate the two(predictors and labels) into different dataframes. Next we drop the labels from the predictor dataframe and check the shapes of the 4 dataframes:\n\n**X_train, X_test, y_train, y_test**","29bce169":"We will also be using a callback so that during the model fitting stage, if our validation set accuracy does not increase even after 2 (patience level) consecutive epochs then we will alter our learning rate by a factor of 0.5.","3957ae5e":"# Importing libraries and Loading data","90bb052e":"# Conclusion\n\nHence, this is how we have achieved, a pretty good accuracy by using 3 sequential combinations of Convolution and Max pooling layers interspersed with Dropout layers and ultimately followed by Dense layers.\n\nI hope that this notebook gave you an insight into incorporating Data Augmentation into your projects. Do give it a try in your notebooks and observe the accuracy shoot up. Moreover, do write back in the comments if you have any doubts in this kernel and would want me to explain something in more detail.\n\nLast but not the least, credit goes to [this](https:\/\/www.kaggle.com\/madz2000\/cnn-using-keras-100-accuracy) notebook for helping out with the data augmentation section.","3136ba1d":"# Train Validation Split","1442ad00":"# Data Augmentation\n\n![aug.JPG](attachment:aug.JPG)","63b2c831":"# Model Building\n\nThe next step is to build models using a combination of Convolution and Max pooling layers in Keras. We will also be using Batch Normalization and dropout layers to prevent the model from overfitting the data.","93b78e29":"Data augmention involves taking a particular image and generating more images out of it by one of the following methods:\n1. **Zooming in**\n2. **Zooming out**\n3. **Sensible Cropping**\n4. **Rotation by a small angle**\n5. **Mirroring the image about horizontal or vertical axis**\n\nThe effect of augmentation is that we have more variants of the existing training examples and the models generalizes much better over the unseen examples as it has been trained on a quite a variety of input examples. Till date, image augmentation remains one of the best methods to boost the accuracy of any model and hence has widespread applications in image classification problems.\n\nIn this particular problem, since we have seen in the introduction section that some images when flipped over in the sign language alphabet become completely different alphabet, *hence to avoid misclassifications, we will not be flipping our images horizontally or vertically*.","c6a5714f":"# Introduction\n\n![signLang.JPG](attachment:signLang.JPG)\n\nHello Kagglers!\n\nThe below kernel proposes a solution to the popular Sign Language dataset which aims to classify the **American Sign language** images of 24 out of 26 alphabets of English language, using a **Convolutional Neural Network(CNN)** model. The solution uses Keras library of Python to build the model and I have tried to make the notebook as much self explanatory as possible by providing simple codes and easy to implement functions.\n\nA brief description of the problem and the dataset can be found below:\n> The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).\n> The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), along with 5% random pixelation, +\/- 15% brightness\/contrast, and finally 3 degrees rotation. Because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.\n","08e1d0d4":"# Class Imbalance Detection\n\nOnce the input X data is ready in the desired format, we will move our analysis towards the Y labels and check for class imabalance (*if at all it is present*) and treat it accordingly.\n\nHere, we have done a count plot of the 24 classses in our data and find that the data is mostly balanced for all classes.","6bbe0003":"# Model fitting","97088da9":"It might be a good idea to check a random image after all the operations on our input data. In the below line, we are checking the grayscaled image for 108th training example.","566fff3d":"# Data Normalization and Reshaping\n\nA significant step while working with pixeled data is to normalize the dataset so that the original individual values are converted from the range of **[0,255] to [0,1]**. This is a proven step to allow **faster convergence to the global minimum** of the objective function in gradient descent algorithms.\n","8138c93f":"While working with CNN models, we need to ensure that the (input) image data is in the form of (n,n) pixels per channel where the channels represent the RGB channels of a colour filter. Typically, a single channel means a grayscaled image.\n\nIn this particular problem, each of the 27,455 images of the input have 784 columns. To feed this data into a CNN model, we will have to *convert the single dimensional data into the CNN accepted data format of 3 dimensions*. Thus, we accomplish this by converting each row of **784 columns into (28,28,1) matrix**.\n\n\n![reshape.jpg](attachment:reshape.jpg)"}}