{"cell_type":{"da58426f":"code","9a43c18f":"code","2136bb7c":"code","bb20655d":"code","0cb60163":"code","a83acf2f":"code","59188fb9":"code","c0a79e5a":"code","8284272c":"code","f298b86b":"code","f80f8664":"code","3a4347dd":"code","a9b4626c":"code","827c9a79":"code","d89b327f":"code","0790c10b":"code","be244cd1":"code","92a835f0":"markdown"},"source":{"da58426f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9a43c18f":"import pandas as pd\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndata = pd.read_csv(\"..\/input\/titanic\/train.csv\")","2136bb7c":"data_train_x = ['Sex']\n\ndata_train = pd.get_dummies(data[data_train_x])","bb20655d":"fig, ax = plt.subplots(1, 2, figsize = (14, 5))\ndata[\"Sex\"].value_counts().plot.bar(color = \"skyblue\", ax = ax[0])\nax[0].set_title(\"Number Of Passengers By Sex\")\nax[0].set_ylabel(\"Population\")\nsns.countplot(\"Sex\", hue = \"Survived\", data = data, ax = ax[1])\nax[1].set_title(\"Sex: Survived vs Dead\")\nplt.show()","0cb60163":"#setting the matrixes\nX = data_train.iloc[:].values","a83acf2f":"noOfTrainEx = X.shape[0] # no of training examples\nprint(\"noOfTrainEx: \",noOfTrainEx)\nnoOfWeights = X.shape[1]+1 # no of features+1 => weights\nprint(\"noOfWeights: \", noOfWeights)","59188fb9":"ones = np.ones([noOfTrainEx, 1]) # create a array containing only ones \nX = np.concatenate([ones, X],1) # cocatenate the ones to X matrix\ntheta = np.ones((1, noOfWeights)) #np.array([[1.0, 1.0]])","c0a79e5a":"y = data['Survived'].values.reshape(-1,1) # create the y matrix","8284272c":"print(X.shape)\nprint(theta.shape)\nprint(y.shape)","f298b86b":"def sigmoid(z):\n        return 1 \/ (1 + np.exp(-z))\ndef computeCost(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","f80f8664":"#set hyper parameters\nalpha = 0.01\niters = 3000","3a4347dd":"## Gradient Descent funtion\ndef gradientDescent(X, y, theta, alpha, iters):\n    cost = np.zeros(iters)\n    for i in range(iters):\n        z = X @ theta.T\n        h = sigmoid(z)\n        theta = theta - (alpha\/len(X)) * np.sum((h - y) * X, axis=0)\n        cost[i] = computeCost(h, y)\n        if i % 100 == 0: # just look at cost every ten loops for debugging\n            print(i, 'iteration, cost:', cost[i])\n    return (theta, cost)","a9b4626c":"g, cost = gradientDescent(X, y, theta, alpha, iters)  ","827c9a79":"print(g)","d89b327f":"#plot the cost\nfig, ax = plt.subplots()  \nax.plot(np.arange(iters), cost, 'r')  \nax.set_xlabel('Iterations')  \nax.set_ylabel('Cost')  \nax.set_title('Error vs. Training Epoch')","0790c10b":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=1e20)\n%time model.fit(X, y)\n\npreds = model.predict(X)\n# accuracy\nprint (\"Accuracy: \", (preds == y).mean())","be244cd1":"model.intercept_, model.coef_","92a835f0":"## SkLearn"}}