{"cell_type":{"726cf5c5":"code","5ab5e160":"code","9e8060ca":"code","43d3076f":"code","aeeb65c2":"code","54c24946":"code","c2ebd90d":"code","e6d1c9ee":"code","ff0d98b0":"code","4854bb80":"code","d1df0af2":"code","9a7f6a56":"code","7b4acaae":"code","30db766a":"code","d6ca22a0":"code","08d524ef":"code","745468c4":"code","d20ab04e":"code","da32e83f":"code","c24eeb80":"code","d31d0f87":"code","28da0d26":"code","8097bbd7":"code","bbc7ec32":"code","79a0731c":"markdown","30f932cf":"markdown","dda722b9":"markdown","ddd3a37e":"markdown","ca4229a2":"markdown","24446baf":"markdown","3b1414ed":"markdown","39db2b26":"markdown","0e6be4d7":"markdown","08905f0e":"markdown","cac5bf2a":"markdown","46cef54d":"markdown","12190f4b":"markdown","68eb7026":"markdown"},"source":{"726cf5c5":"import optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings","5ab5e160":"DEBUG = True\n\nif ~DEBUG:\n    warnings.filterwarnings(\"ignore\")\n\ndict_types = {\n'id': np.int32,\n'breath_id': np.int32,\n'R': np.int8,\n'C': np.int8,\n'time_step': np.float32,\n'u_in': np.float32,\n'u_out': np.int8, #np.bool ?\n'pressure': np.float32,\n} \n\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv', dtype=dict_types)\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv', dtype=dict_types)\n\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\nall_pressure = np.sort(train.pressure.unique())\nPRESSURE_MIN = all_pressure[0]\nPRESSURE_MAX = all_pressure[-1]\nPRESSURE_STEP = (all_pressure[1] - all_pressure[0])\n\nif DEBUG:\n    train = train[:80*1000]","9e8060ca":"n_train = int(train.shape[0]\/80)\ntrain['time_id'] = [i for j in range(n_train) for i in range(80)]\n\nn_test = int(test.shape[0]\/80)\ntest['time_id'] = [i for j in range(n_test) for i in range(80)]\n\ntrain_pivot = train.pivot(index='breath_id', columns='time_id', values='u_in')\ntest_pivot = test.pivot(index='breath_id', columns='time_id', values='u_in')\n\ntrain_pivot_1000 = train_pivot[:999]\ntest_pivot_1000 = test_pivot[:999]","43d3076f":"ts1 = train_pivot.iloc[0]\nts2 = train_pivot.iloc[1]","aeeb65c2":"def euclid_dist(t1,t2):\n    return np.sqrt(sum((t1-t2)**2))\n\nprint(euclid_dist(ts1,ts2))","54c24946":"print(euclid_dist(ts1,ts1))\nprint(euclid_dist(ts1,ts1.shift().fillna(0)))\nprint(euclid_dist(ts1,ts1.shift().shift().fillna(0)))","c2ebd90d":"%%time\n\nfor i in range(1000):\n    euclid_dist(ts1,ts2)","e6d1c9ee":"def DTWDistance(s1, s2):\n    DTW={}\n    for i in range(len(s1)):\n        DTW[(i, -1)] = float('inf')\n    for i in range(len(s2)):\n        DTW[(-1, i)] = float('inf')\n    DTW[(-1, -1)] = 0\n\n    for i in range(len(s1)):\n        for j in range(len(s2)):\n            dist= (s1[i]-s2[j])**2\n            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n\nprint(DTWDistance(ts1,ts2))","ff0d98b0":"print(DTWDistance(ts1,ts1))\nprint(DTWDistance(ts1,ts1.shift().fillna(0)))\nprint(DTWDistance(ts1,ts1.shift().shift().fillna(0)))","4854bb80":"%%time\n\nfor i in range(1000):\n    DTWDistance(ts1,ts2)","d1df0af2":"def DTWDistance(s1, s2,w=10):\n    DTW={}\n    w = max(w, abs(len(s1)-len(s2)))\n    for i in range(-1,len(s1)):\n        for j in range(-1,len(s2)):\n            DTW[(i, j)] = float('inf')\n    DTW[(-1, -1)] = 0\n    for i in range(len(s1)):\n        for j in range(max(0, i-w), min(len(s2), i+w)):\n            dist = (s1[i]-s2[j])**2\n            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n\nDTWDistance(ts1,ts2)","9a7f6a56":"%%time\n\nfor i in range(1000):\n    DTWDistance(ts1,ts2, 5)","7b4acaae":"def LB_Keogh(s1,s2,r):\n    LB_sum=0\n    for ind,i in enumerate(s1):\n        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n        if i>upper_bound:\n            LB_sum=LB_sum+(i-upper_bound)**2\n        elif i<lower_bound:\n            LB_sum=LB_sum+(i-lower_bound)**2\n    return np.sqrt(LB_sum)\n\nprint(LB_Keogh(ts1,ts2,5))","30db766a":"%%time\n\nfor i in range(1000):\n    LB_Keogh(ts1.values,ts2.values, 5)","d6ca22a0":"from sklearn.metrics import classification_report\n\ndef knn(train,test,w):\n    preds=[]\n    for ind,i in enumerate(test):\n        min_dist=float('inf')\n        closest_seq=[]\n        #print ind\n        for indj, j in enumerate(train):\n            if LB_Keogh(i[:-1],j[:-1],w)<min_dist:\n                dist=DTWDistance(i[:-1],j[:-1],w)\n                if dist<min_dist:\n                    min_dist=dist\n                    closest_seq=indj\n        preds.append(closest_seq)\n    return preds","08d524ef":"%%time\nclosest = knn(train_pivot_1000.values,test_pivot[:10].values,5)","745468c4":"import random\n\ndef k_means_clust(data,num_clust,num_iter,w=5):\n    centroids=random.sample(data,num_clust)\n    counter=0\n    for n in range(num_iter):\n        counter+=1\n        print(counter)\n        assignments={}\n        #assign data points to clusters\n        for ind,i in enumerate(data):\n            min_dist=float('inf')\n            closest_clust=None\n            for c_ind,j in enumerate(centroids):\n                if LB_Keogh(i,j,5)<min_dist:\n                    cur_dist=DTWDistance(i,j,w)\n                    if cur_dist<min_dist:\n                        min_dist=cur_dist\n                        closest_clust=c_ind\n            if closest_clust in assignments:\n                assignments[closest_clust].append(ind)\n            else:\n                assignments[closest_clust]=[]\n    \n        #recalculate centroids of clusters\n        for key in assignments:\n            clust_sum=0\n            for k in assignments[key]:\n                clust_sum=clust_sum+np.array(data[k])\n            centroids[key]=[m\/len(assignments[key]) for m in clust_sum]\n    \n    return centroids, assignments","d20ab04e":"%%time\ncentroids, assignments = k_means_clust(train_pivot_1000.values.tolist(),num_clust=10,num_iter=5,w=5)","da32e83f":"plt.plot(np.array(centroids).transpose());","c24eeb80":"def get_nearest_centroid(ts):\n    cluster = -1\n    dist = np.inf\n    for i in range(len(centroids)):\n        if LB_Keogh(ts,centroids[i],5)<dist:\n            dist_c = DTWDistance(ts,centroids[i],5)\n            if dist_c < dist:\n                dist = dist_c.copy()\n                cluster = i\n    return cluster","d31d0f87":"%%time\n\ntest_assignements = test_pivot_1000.transpose().apply(get_nearest_centroid)","28da0d26":"with open('train_DTW_clust.pkl', 'wb') as handle:\n    pickle.dump(assignments, handle)\n    \nwith open('test_DTW_clust.pkl', 'wb') as handle:\n    pickle.dump(test_assignements, handle)","8097bbd7":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=10, random_state=0).fit(train_pivot)\n\npreds = kmeans.transform(test_pivot)\n\nplt.plot(kmeans.cluster_centers_.transpose());","bbc7ec32":"with open('train_kmeans_clust.pkl', 'wb') as handle:\n    pickle.dump(kmeans.labels_, handle)\n    \nwith open('test_kmeans_clust.pkl', 'wb') as handle:\n    pickle.dump(preds, handle)","79a0731c":"# Classification and NN Feature Engineering\n\nShort implementation of 1NN for demonstration. Might be better not to perform the full $n^2$ comparisons. This would allow for Classification (outputing the nearest neighbor prediction) or Feature engineering (outputing feature of the nearest neighbor).","30f932cf":"# Dynamic Time Warping Distance\n\nDTW account for shift (but is way slower)","dda722b9":"This seems too slow for the current competition. I am currently looking for faster implementations.","ddd3a37e":"# Speeding up DTW\n\nDTW need to be speeded up to be exploitable. One way to do this is limit the delay between series to avoid comparison with data point that are too far apart. This can be achieved with a windows w.","ca4229a2":"# Clustering with DTW\n\nSimilarly short implementation for demonstration. Clustering has a lower complexity (number of time series x number of clusters). So it might directly be usefull.","24446baf":"Standard distance, doesn't account for time shift, but is rather fast.","3b1414ed":"# Dynamic Time Warping for Classification, Feature Engineering and Clustering\n\nCalculation of distance between time series is often difficult as time series might be similar but shifted one from the other. Dynamic time warping aims to correct that. The general idea is that, instead of calculating the euclidian distance on vertical differences, the difference are calculated along inclined lines, as shown below (image from wikipedia).\n\n<p><a href=\"https:\/\/commons.wikimedia.org\/wiki\/File:Dynamic_time_warping.png#\/media\/File:Dynamic_time_warping.png\"><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/ab\/Dynamic_time_warping.png\" alt=\"Dynamic time warping.png\"><\/a><br>\n\nA lot of the code come from Alex Minnaar [Blog Post](http:\/\/alexminnaar.com\/2014\/04\/16\/Time-Series-Classification-and-Clustering-with-Python.html). I've done some rework, some adaptation to better match the competition data and some benchmarking to better illustrate the interest of Dynamic Time Warping.","39db2b26":"# Data Preparation","0e6be4d7":"# Comparison with kmeans","08905f0e":"So we are down to around 1ms for two time series comparison.","cac5bf2a":"# Keogh Lower Bound\n\nThe DTW calculation being $O(n^2)$ in complexity, a better approach is to consider an approximation. A lower bound for the DTW distance was discovered by Dr. Aemon Keogh. This lower bound is $O(n)$ in complexity and thus help avoid a ton of $O(n^2)$. Say you want to find the closest time serie to one instance. You can loop trough all candidates and check the lower bound. If the lower bound for a new candidate isn't below the lowest distance for your current best you can skip the whole DTW calculation.","46cef54d":"# Imports","12190f4b":"# Euclidian distance","68eb7026":"We can look at centroids:"}}