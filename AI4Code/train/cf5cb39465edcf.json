{"cell_type":{"93a3c254":"code","725be1fa":"code","b22e031b":"code","4bbe91bb":"code","602b6a48":"code","1ebef694":"code","5b5739de":"code","3ef04eb7":"code","8738746d":"code","1578fdba":"code","4fa08157":"code","6df0f0f6":"code","0bfb4b21":"code","ae6e5abe":"code","3e417d03":"code","56992762":"code","ce0fea90":"code","bc4c8af8":"code","0e1fe30e":"code","3dbfe3ad":"code","46bc1ad7":"code","45379a13":"code","7124e829":"code","ca530d61":"code","88a9c02a":"code","0c7ada66":"code","40e275ec":"code","cbc0b790":"code","a4f6630b":"code","a65e4de9":"code","f3586e35":"markdown","579c80ff":"markdown","bd50d16a":"markdown","82aa8c72":"markdown","c3f64956":"markdown","d15e6bf8":"markdown","efa51f23":"markdown","ee3c315d":"markdown","8f4e7d58":"markdown","4274b391":"markdown","44b97310":"markdown","005cb635":"markdown","2cf1ae19":"markdown","1486a15b":"markdown","d8706a52":"markdown","f9bc1f58":"markdown","26c24e88":"markdown","9d5e3bea":"markdown","0e9cc18b":"markdown","08e85f59":"markdown","2117cb9a":"markdown","2483f446":"markdown","9bcad3b1":"markdown","9cfd0393":"markdown","43cd55d4":"markdown","85592acf":"markdown","62591c7e":"markdown","dfe5e6c8":"markdown","0274fd01":"markdown","ece0eb4b":"markdown","e6f59965":"markdown","f2f3c24e":"markdown","4ca823a5":"markdown","9925e275":"markdown"},"source":{"93a3c254":"!pip install pycaret==1.0.0","725be1fa":"import numpy as np \nimport pandas as pd \nimport sklearn\nfrom sklearn.model_selection import train_test_split","b22e031b":"dataset=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndataset.head()","4bbe91bb":"dataset.isna().sum()","602b6a48":"train,test=train_test_split(dataset,test_size=0.2,random_state=42)","1ebef694":"print('train shape:',train.shape ,'test shape:',test.shape)","5b5739de":"from pycaret.classification import *","3ef04eb7":"train.head()","8738746d":"train.describe()","1578fdba":"clf=setup(data=train,\n         target='Outcome',\n        numeric_imputation = 'mean',\n         silent = True)","4fa08157":"compare_models()","6df0f0f6":"lightgbm  = create_model('lightgbm')\n\n## Here this model automatically use 10-Fold CV.","0bfb4b21":"tuned_lightgbm = tune_model('lightgbm')","ae6e5abe":"evaluate_model(tuned_lightgbm)","3e417d03":"plot_model(estimator = tuned_lightgbm, plot = 'learning')","56992762":"plot_model(estimator = tuned_lightgbm, plot = 'auc')","ce0fea90":"plot_model(estimator = tuned_lightgbm, plot = 'confusion_matrix')","bc4c8af8":"plot_model(estimator = tuned_lightgbm, plot = 'feature')","0e1fe30e":"interpret_model(tuned_lightgbm)","3dbfe3ad":"logr  = create_model('lr');      \nxgb   = create_model('xgboost');            \n\n#blending 3 models\nblend = blend_models(estimator_list=[tuned_lightgbm,logr,xgb])","46bc1ad7":"from pycaret.regression import *","45379a13":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","7124e829":"train.head()","ca530d61":"train.describe()","88a9c02a":"categorical_col_list=[i for i in train.columns if train[i].dtypes =='object']\ncategorical_col_list","0c7ada66":"reg = setup(data = train, \n             target = 'SalePrice',\n             numeric_imputation = 'mean',\n             categorical_features = categorical_col_list, \n             ignore_features = ['Alley','PoolQC','MiscFeature','Fence','FireplaceQu','Utilities'],\n             normalize = True,\n             silent = True)","40e275ec":"compare_models()","cbc0b790":"cb = create_model('catboost')","a4f6630b":"tuned_cb = tune_model('catboost')","a65e4de9":"interpret_model(tuned_cb)","f3586e35":"## Extra: Blending made easy!","579c80ff":"## Interpretation","bd50d16a":"* ## AUC Curve","82aa8c72":"<a id='11'><\/a><br>\n## lets hypertune CatBoost Regressor.","c3f64956":"> **Note:** Here i got some issue related visuallizing metrics and curves of **CatBoost Classifier** so that why we are not cosidering that but for performance wise catboost is best for this dataset. so to neglate above issue we are going with  **Light Gradient Boosting**","d15e6bf8":"# Introduction to PyCaret - An open source low-code ML library\n\n## This notebook consists 2 parts\n*         Classification part using Titanic DataSet\n*         Regression part using House Price Regression DataSet\n\n![image.png](attachment:image.png)\n\n> You can reach pycaret website and documentation from https:\/\/pycaret.org\n\nPyCaret is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within seconds in your choice of notebook environment.\n\nPyCaret being a low-code library makes you more productive. With less time spent coding, you and your team can now focus on business problems.\n\nPyCaret is simple and easy to use machine learning library that will help you to perform end-to-end ML experiments with less lines of code.\n\nPyCaret is a business ready solution. It allows you to do prototyping quickly and efficiently from your choice of notebook environment.\n\n\n> ### Index :\n\n1. [Installing Pycaret Library](#1)\n2. [Classification Task](#2)\n3. [Lets split our data into train and test set](#3)\n4. [Import whole classification from Pycaret](#4)\n5. [Lets Comapre all Classification models ](#5)\n6. [Let's create Individual Catboost Classifier](#6)\n7. [Let's Hypertune Tune model.](#7)\n8. [Plotting various Curves and Matrixs for classification and Regression](#8)\n9. [Part2 - Regression](#9)\n10. [let's do Catboost Regressor ](#10)\n11. [lets hypertune CatBoost Regressor.](#11)\n","efa51f23":"> **print the shape of train set and test set.**","ee3c315d":"## lets check for null values or missing values","8f4e7d58":"## Import Whole Regression","4274b391":"## lets Compare scores of different Regression models","44b97310":"<a id=\"1\"><\/a><br>\n## 1. let's install pycaret !","005cb635":"<a id='2'> <\/a><br>\n\n## Classification Task\n\n> import Necessary data for classification task \n> Here we are going to use this dataset for classification **pima-indians-diabetes-database** ","2cf1ae19":"<a id='7'><\/a><br>\n## Let's Hypertune Tune model.","1486a15b":"## lets see what type of data we are dealing with.","d8706a52":"<a id='5'><\/a> <br>\n\n# Lets Comapre all Classification models ","f9bc1f58":"<a id='3'><\/a><br>\n## Lets split our data into train and test set","26c24e88":"* ## Feature Importance","9d5e3bea":"* ## Confusion Matrix","0e9cc18b":"> Note: all plotting terms are similar in both regression and classification task hence i am not going to repeat that if need some help check out here :    \nhttps:\/\/pycaret.org\/plot-model\/","08e85f59":"<a id='6'><\/a><br>\n## Let's create Individual Catboost Classifier","2117cb9a":"* ## everything in one place isn't it Awasome!! :)) ","2483f446":"> see it short name from above table and copy it for future use.","9bcad3b1":"## SHAP Values (impact on model output)","9cfd0393":"### here we make all categorical column list ","43cd55d4":"<a id='10'><\/a><br>\n## let's do Catboost Regressor ","85592acf":"* ## Learning curve plotting","62591c7e":"> Here no null or missing values is present in this dataset. hence we move forward ","dfe5e6c8":"<a id='4'><\/a><br>\n# Import whole classification from Pycaret","0274fd01":"# We start by loading the libraries","ece0eb4b":"from above chart we can clearly see that train set has no missing values and some columns may required normalization but for this notebook we can ignore it and move forward to our main goal","e6f59965":"# thank you very much for checking my notebook!","f2f3c24e":"<a id='8'><\/a><br>\n# Plotting various Curves and Matrixs for classification and Regression","4ca823a5":"<a id='9'><\/a><br>\n\n## Part2 - Regression","9925e275":"from above comare_model function we can clearly see that **CatBoost Classifier** out performing hence we can select that using it name.\nBut to select specific model we have to look for its shaort name which can be see here \n\n### Classification\n* ID\tName\n* \u2018lr\u2019\tLogistic Regression\n* \u2018knn\u2019\tK Nearest Neighbour\n* \u2018nb\u2019\tNaives Bayes\n* \u2018dt\u2019\tDecision Tree Classifier\n* \u2018svm\u2019\tSVM \u2013 Linear Kernel\n* \u2018rbfsvm\u2019\tSVM \u2013 Radial Kernel\n* \u2018gpc\u2019\tGaussian Process Classifier\n* \u2018mlp\u2019\tMulti Level Perceptron\n* \u2018ridge\u2019\tRidge Classifier\n* \u2018rf\u2019\tRandom Forest Classifier\n* \u2018qda\u2019\tQuadratic Discriminant Analysis\n* \u2018ada\u2019\tAda Boost Classifier\n* \u2018gbc\u2019\tGradient Boosting Classifier\n* \u2018lda\u2019\tLinear Discriminant Analysis\n* \u2018et\u2019\tExtra Trees Classifier\n* \u2018xgboost\u2019\tExtreme Gradient Boosting\n* \u2018lightgbm\u2019\tLight Gradient Boosting\n* \u2018catboost\u2019\tCatBoost Classifier"}}