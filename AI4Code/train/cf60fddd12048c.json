{"cell_type":{"2c83bd6e":"code","80ea4b01":"code","a627537c":"code","12ccd0cc":"code","2047baec":"code","6024e9f2":"code","02bf57e2":"code","891f807a":"code","d44973ce":"code","7358280f":"code","f2ed08ad":"markdown","b1943fc6":"markdown","905d4b96":"markdown","928bb6e0":"markdown","83b17758":"markdown"},"source":{"2c83bd6e":"import numpy as np\nimport os\nfrom tqdm import tqdm_notebook","80ea4b01":"!wget http:\/\/nlp.stanford.edu\/data\/wordvecs\/glove.6B.zip","a627537c":"!unzip glove.6B.zip","12ccd0cc":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r') as f:\n        words = set()\n        word_to_vec_map = {}\n        \n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n            \n    return words, word_to_vec_map","2047baec":"word,word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')","6024e9f2":"def cosine_similarity(u,v):\n    dot = np.dot(u,v)\n    norm_u = np.sqrt(np.sum(np.dot(u.T,u)))\n    norm_v = np.sqrt(np.sum(np.dot(v.T,v)))\n    cosine_similarity = dot\/(norm_u*norm_v)\n    return cosine_similarity","02bf57e2":"man = word_to_vec_map['man']\nking = word_to_vec_map['king']\nwoman = word_to_vec_map['woman']\nqueen = word_to_vec_map['queen']\nprint(f'cosine similarity between man and king: {cosine_similarity(man,king)}')\nprint(f'cosine similarity between woman and queen: {cosine_similarity(woman,queen)}')","891f807a":"def complete_analogy(word_a,word_b,word_c,word_to_vec_map):\n    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n    e_a,e_b,e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]\n    words = word_to_vec_map.keys()\n    \n    max_cosine_similarity = -100\n    best_word = None\n    \n    input_word_set = set([word_a,word_b,word_c])\n    \n    for w in tqdm_notebook(words):\n        if w in input_word_set:\n            continue\n        \n        cosine_sim = cosine_similarity(e_b-e_a,word_to_vec_map[w]-e_c)\n        \n        if max_cosine_similarity<cosine_sim:\n            max_cosine_similarity = cosine_sim\n            best_word = w\n        \n    return best_word","d44973ce":"triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\nfor triad in tqdm_notebook(triads_to_try):\n    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))","7358280f":"ans = complete_analogy('man','doctor','woman',word_to_vec_map)\nprint('{}->{} :: {}->{}'.format('man','doctor','woman',ans))","f2ed08ad":"## In this kernel I will explain GloVe.\n(You can read about GloVe from this paper https:\/\/nlp.stanford.edu\/pubs\/glove.pdf )\n\n## Please UPVOTE if you like this kernel","b1943fc6":"## Cosine similarity\n\nTo measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n\n$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)\u00a0\\tag{1}$$\n\nThe norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$","905d4b96":"## Debiasing word vectors\n### You can can read this paper https:\/\/arxiv.org\/pdf\/1607.06520.pdf","928bb6e0":"### Every thing was ok with this approach but the problem arises when it was observed that this word embedding was biased\n\n#### Let's understand this statement with an example\n##### When we ask Man->computerProgrammer :: woman->? it answers **Homemaker**\n##### When we ask Man->Doctor :: Woman->? it answers **Nurse**\n##### These answers were very biased.But we can solve this problem easily","83b17758":"### Let's see how this calculation is done and how our model will predict. \n\n#### Suppose our task is to find answer of the following question:\n\n#### Man->King then Woman->?"}}