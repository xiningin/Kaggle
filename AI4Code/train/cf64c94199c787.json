{"cell_type":{"8453d6c7":"code","022625c5":"code","2bb94060":"code","6393dde1":"code","c0520b22":"code","007dc781":"code","d9af8e65":"code","b73961f7":"code","e86875f2":"code","6d0371a7":"code","14269647":"code","8bbd5b4f":"code","45572df0":"code","d6d02881":"code","bb361fb7":"code","32e78e00":"code","140606ed":"code","41cde8d8":"code","f8f25c6d":"code","4e8fb12c":"code","2ab11656":"code","3dda9edb":"code","6fa0d894":"code","16ac5366":"code","afcda7bc":"code","e5d3c8f8":"code","74568592":"code","0c1885d6":"markdown","2b5cbc9d":"markdown","91adea74":"markdown","c55c5da3":"markdown","3cd4b614":"markdown","d58efb6c":"markdown","e75174d0":"markdown","c3548fd1":"markdown","d263f392":"markdown"},"source":{"8453d6c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","022625c5":"from sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import preprocessing\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport seaborn as sns","2bb94060":"# load the dataset\ndf= pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint(\"The dataset has %d rows and %d columns.\" % df.shape)","6393dde1":"percent_missing = df.isnull().sum() * 100 \/ len(df)\npercent_missing","c0520b22":"df.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)","007dc781":"df['Attrition'].value_counts(normalize=True)","d9af8e65":"df.head()","b73961f7":"df.describe()","e86875f2":"fig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Age'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['DailyRate'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['DistanceFromHome'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Education'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['EnvironmentSatisfaction'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['HourlyRate'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Age'], hue=df['Attrition'])","6d0371a7":"print(\"------  Data Types  ----- \\n\",df.dtypes)","14269647":"import seaborn as sns\ncorr = df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr,cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},cmap='YlGnBu')","8bbd5b4f":"#Lets drop few variables which doesnt look helpful\ndf = df.drop(['EmployeeCount','EmployeeNumber'], axis=1)","45572df0":"dataset =  df.drop(['OverTime','MaritalStatus','JobRole','Gender','Department','EducationField','BusinessTravel','Over18'], axis=1)\nBusinessTravel = pd.get_dummies(df.BusinessTravel).iloc[:,1:]\nDepartment = pd.get_dummies(df.Department).iloc[:,1:]\nOverTime = pd.get_dummies(df.OverTime).iloc[:,1:]\nMaritalStatus = pd.get_dummies(df.MaritalStatus).iloc[:,1:]\nJobRole = pd.get_dummies(df.JobRole).iloc[:,1:]\nGender = pd.get_dummies(df.Gender).iloc[:,1:]\nEducationField = pd.get_dummies(df.EducationField).iloc[:,1:]\nOver18 = pd.get_dummies(df.Over18).iloc[:,1:]\ndataset = pd.concat([dataset,Over18,BusinessTravel,Department,OverTime,MaritalStatus,JobRole,Gender,], axis=1)\ndataset","d6d02881":"print(\"------  Data Types  ----- \\n\",dataset.dtypes)","bb361fb7":"X =  dataset.drop(['Attrition'], axis=1)\ny = dataset['Attrition']","32e78e00":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10,random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier","140606ed":"classifier =  RandomForestClassifier(n_estimators = 400,random_state = 42)\nclassifier.fit(X_train, y_train)  \npredictions = classifier.predict(X_test)\nfrom sklearn.metrics import classification_report, accuracy_score\nprint(classification_report(y_test,predictions ))  \nprint(accuracy_score(y_test, predictions ))","41cde8d8":"fig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nfeat_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","f8f25c6d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 70% training and 30% test","4e8fb12c":"import xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nxgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.08, objective= 'binary:logistic',n_jobs=-1).fit(X_train, y_train)\nprint('Accuracy of XGB classifier on training set: {:.2f}'\n       .format(xgb_model.score(X_train, y_train)))\nprint('Accuracy of XGB classifier on test set: {:.2f}'\n       .format(xgb_model.score(X_test[X_train.columns], y_test)))","2ab11656":"y_pred = xgb_model.predict(X_test)\nprint(classification_report(y_test, y_pred))","3dda9edb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=45)  # 80% training and 20% test","6fa0d894":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_test)#Import scikit-learn metrics module for accuracy calculation","16ac5366":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","afcda7bc":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nclassifier.fit(X_train, y_train)\n\nrf_predict_probabilities = classifier.predict_proba(X_test)[:,1]\ngb_predict_probabilities = gb.predict_proba(X_test)[:,1]\ny_predict_xgb = xgb_model.predict_proba(X_test)[:,1]\n\n\ngb_fpr, gb_tpr, _ = roc_curve(y_test, gb_predict_probabilities)\ngb_roc_auc = auc(gb_fpr, gb_tpr)\n\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_predict_probabilities)\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, y_predict_xgb)\nxgb_roc_auc = auc(xgb_fpr, xgb_tpr)\n\n\nplt.figure()\nplt.plot(gb_fpr, gb_tpr, color='darkorange',\n         lw=2, label='random Forrest (area = %0.2f)' % gb_roc_auc)\nplt.plot(rf_fpr, rf_tpr, color='darkgreen',\n         lw=2, label='Gradient Boosting (area = %0.2f)' % rf_roc_auc)\nplt.plot(xgb_fpr, xgb_tpr, color='black',\n         lw=2, label='XGBoost (area = %0.2f)' % xgb_roc_auc)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","e5d3c8f8":"pred_for_submission = xgb_model.predict(X_test).astype(int)\npred_for_submission","74568592":"xgb_probs = xgb_model.predict_proba(X_test)\nxgb_probs","0c1885d6":"Lets Check Missing values","2b5cbc9d":"RANDOM Forest Model","91adea74":"Target variables","c55c5da3":"XGBoost is clearly outperforming Random Forest. But none of these models are really good.  It can be imporved by hyper tuning the parametere or by doing resampling by SMOTE. I will add that later","3cd4b614":"XGBOOST Model","d58efb6c":"Lets Compare all the models we build","e75174d0":"Gradient Boosting","c3548fd1":"One hot encoding to fix the variables.","d263f392":"Target variable is imbalanced. We can try to use sampling to fix this but we will do this later."}}