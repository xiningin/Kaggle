{"cell_type":{"0207caee":"code","d5cfba83":"code","95007352":"code","3dc1bcac":"code","94662e01":"code","0697649f":"code","20d026d7":"code","c2a3afa0":"code","812f6080":"code","1843842c":"code","b9092f7b":"code","6b736856":"code","ca3405de":"code","5a45168a":"code","84cde0d4":"code","8559b7d1":"code","c8e7d54f":"code","41d2f9cd":"code","1922f1ed":"code","16678bce":"code","9307b1e5":"code","34a9b231":"code","e71210a7":"code","a7bb1422":"code","1a223d5e":"code","036c86a1":"code","74f6332e":"code","7a9e8c9b":"code","7566e0d6":"code","9df84c21":"code","1c3f2470":"code","c5bac611":"code","cd559460":"code","c29cf024":"code","ce0be939":"code","994cb456":"code","87dd7ea4":"code","c4c54b0f":"code","a1fdb74f":"code","b1f0bf41":"code","4be9939d":"code","83b72279":"code","6f67b7f6":"code","a613fde2":"code","b34ba4e2":"code","083e0609":"code","a110914c":"code","e4b1455c":"code","9e18d95d":"code","fa4e0d30":"code","6dfdbf9c":"code","1ff429e2":"code","5f9ea4c9":"code","4742fbe0":"code","78e03d5c":"code","a45ae657":"code","83be2a62":"code","e72c093b":"code","52166406":"code","6c90216b":"code","cf526ea0":"code","66ff8720":"code","a9a8bd6b":"code","b0816a40":"code","3c5e115e":"code","5cc65e2a":"code","ee70452b":"code","e0350fdf":"code","eecc36f2":"code","9311736e":"code","0fd852d3":"code","a3408888":"code","057785c8":"code","6c0017c8":"code","121369c4":"code","c6d613c4":"code","4cb77f37":"code","7a11dfd0":"code","29249be2":"code","ed3df2a6":"code","fd710538":"code","1e49d08f":"code","5d54f2b9":"code","25278996":"code","42baaa0c":"code","2dc61aa1":"code","8aaad8f7":"markdown","83a79781":"markdown","0909d5d6":"markdown","f77abb5f":"markdown","b3906028":"markdown","b3f9d113":"markdown","3169ddfc":"markdown","6a030e77":"markdown","85214f23":"markdown","16224095":"markdown","f5f8a34e":"markdown","15246061":"markdown","ced36d97":"markdown","c83555c2":"markdown","32384dbc":"markdown","1111adb9":"markdown","5a5ccecf":"markdown","e939c799":"markdown","cf109c50":"markdown","f48317b9":"markdown"},"source":{"0207caee":"!pip install torch==1.8.1\n!pip install torchtext==0.9.1","d5cfba83":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom tqdm.notebook import tqdm\nfrom time import time\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport time\nimport torch.nn.functional as F\nimport torchtext\nfrom torchtext.legacy import data\nfrom torchtext.legacy.data import Field, LabelField \nfrom torchtext.legacy.data import TabularDataset, BucketIterator\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","95007352":"df = pd.read_csv(\"..\/input\/news-category-dataset\/NewsCategorizer.csv\")\ndf.head()","3dc1bcac":"import missingno as msno\nmsno.bar(df, color=(0.1, 0.4, 0.5)) # There are null value in \"keywords\" column.","94662e01":"word_len=[]\nfor i in range(len(df)):\n    word_len.append(len(df.headline.values[i].split(' ')))  \n    \nplt.figure(figsize=(12,6))\nsns.set_style(\"darkgrid\")\nsns.countplot(word_len, color=\"darkblue\")\nplt.xlabel(\"Lengths\", fontsize = 17)\nplt.ylabel('Counts', fontsize = 17)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.title(\"The lengths of headlines\", fontsize = 22)\nplt.show()","0697649f":"word_len=[]\nfor i in range(len(df)):\n    word_len.append(len(df.short_description.values[i].split(' ')))  \n    \nplt.figure(figsize=(12,6))\nsns.set_style(\"darkgrid\")\nsns.histplot(word_len, color = \"red\")\nplt.xlabel(\"Lengths\", fontsize = 17)\nplt.ylabel('Counts', fontsize = 17)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.title(\"The lengths of short descriptions\", fontsize = 22)\nplt.show()","20d026d7":"# Let's check the rows which have the min length in description column\nminindex = min(word_len)\nminindex = word_len.index(minindex)\nprint(minindex)","c2a3afa0":"# min\ndf[5812:5813]","812f6080":"# TWO WAYS to remove emoji\ndef Emoji_remove1(text):\n    text = re.sub(r'[^a-zA-z]+', \" \", text)\n    return text \n\ndef Emoji_remove2(text):\n    return text.encode('ascii', 'ignore').decode('ascii')\n\nprint(Emoji_remove1(\"\ud83d\udca9 emoji\"))\nprint(Emoji_remove2(\"\ud83d\udca9 emoji\"))","1843842c":"pd.set_option('display.max_colwidth', 250)\ndf = df[[\"category\", \"headline\", \"short_description\", \"keywords\"]]\ndf.head(2)","b9092f7b":"len(df)","6b736856":"df.category.value_counts() # so balanced dataset!!!!","ca3405de":"possible_labels = df.category.unique()","5a45168a":"label_dict = {}\n\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index","84cde0d4":"label_dict","8559b7d1":"df[\"label\"] = df.category.replace(label_dict)\ndf.head(2)","c8e7d54f":"#tokenizer\nfrom nltk.tokenize import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\n#stopwords\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n#stemmer\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\n#lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()","41d2f9cd":"def text_tokenization(text): \n    text = text.replace(\"n't\", ' not')\n    text = text.replace(\"'ve\", ' have')\n    text = text.replace(\"'m\", ' am')\n    text = text.replace(\"\u2019s\", ' ')\n    text = text.replace(\"'s\", ' ')\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    text = re.sub(\"[..,,\u318d\\'\\\"\u2019\u2018\u201d\u201c!?\\\\\u2018|\\(\\)\\[\\]\\<\\>`\\'\u25c7\u2026]\", \"\", text)\n    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n    text = re.sub(\" +\", \" \", text) #remove multi-space\n    text = text.lower()\n    return \" \".join([stemmer.stem(word) for word in str(text).split() \n                     if word not in stop_words])\n\ndef cnn_tokenization(text):\n    text = text.replace(\"n't\", ' not')\n    text = text.replace(\"'ve\", ' have')\n    text = text.replace(\"'m\", ' am')\n    text = text.replace(\"\u2019s\", ' ')\n    text = text.replace(\"'s\", ' ')\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    text = re.sub(\"[..,,\u318d\\'\\\"\u2019\u2018\u201d\u201c!?\\\\\u2018|\\(\\)\\[\\]\\<\\>`\\'\u25c7\u2026]\", \"\", text)\n    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n    text = re.sub(\" +\", \" \", text) #remove multi-space\n    text = text.lower()\n    return [stemmer.stem(word) for word in str(text).split() \n                     if word not in stop_words]\n\ndef only_lemmatization(text):\n    text = text.replace(\"n't\", ' not')\n    text = text.replace(\"'ve\", ' have')\n    text = text.replace(\"'m\", ' am')\n    text = text.replace(\"\u2019s\", ' ')\n    text = text.replace(\"'s\", ' ')\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    text = re.sub(\"[..,,\u318d\\'\\\"\u2019\u2018\u201d\u201c!?\\\\\u2018|\\(\\)\\[\\]\\<\\>`\\'\u25c7\u2026]\", \"\", text)\n    text = re.sub(r'[^a-zA-z]+', \" \", text) # only english remain\n    text = re.sub(\" +\", \" \", text) #remove multi-space\n    text = text.lower()\n    text = \" \".join([lemma.lemmatize(word, pos='v') for word in text.split() if word not in stop_words])\n    return text","1922f1ed":"df_final = df[[\"category\", \"label\", \"keywords\"]]\ndf_final[\"cleaned_headline\"] = df[\"headline\"].apply(text_tokenization)\ndf_final[\"cleaned_description\"] = df[\"short_description\"].apply(text_tokenization)","16678bce":"df_final[\"lemma_description\"] = df[\"short_description\"].apply(only_lemmatization)","9307b1e5":"df_final.head(2)","34a9b231":"df_wellness = df_final[df_final[\"label\"]==0]\ndf_politics = df_final[df_final[\"label\"]==1]\ndf_entertainment = df_final[df_final[\"label\"]==2]\ndf_travel = df_final[df_final[\"label\"]==3]\ndf_stylenbeauty = df_final[df_final[\"label\"]==4]\ndf_parenting = df_final[df_final[\"label\"]==5]\ndf_foodndrink = df_final[df_final[\"label\"]==6]\ndf_worldnews = df_final[df_final[\"label\"]==7]\ndf_business = df_final[df_final[\"label\"]==8]\ndf_sports = df_final[df_final[\"label\"]==9]","e71210a7":"# check the keywords in wellness category\ndf_wellness[\"keywords\"] = df_wellness[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_wellness[\"keywords\"]","a7bb1422":"# check the top five keywords in categories\nwellness = df_wellness[\"keywords\"].value_counts().head()\nwellness","1a223d5e":"fig = plt.figure(figsize=(5,5))\ncolors = [\"powderblue\", \"lightskyblue\", \"deepskyblue\", \"dodgerblue\", \"steelblue\"]\n\nax = fig.add_subplot()\npie = ax.pie(wellness, textprops={'size': 12}, counterclock=False, startangle=90, \n             labels=wellness.index, autopct='%.1f%%', wedgeprops=dict(width=0.6),\n            shadow=True, colors = colors)\nplt.title(\"Wellness\", fontsize = 20)\nplt.show()","036c86a1":"# Let's do it to other categories\ndf_politics[\"keywords\"] = df_politics[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_entertainment[\"keywords\"] = df_entertainment[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_travel[\"keywords\"] = df_travel[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_stylenbeauty[\"keywords\"] = df_stylenbeauty[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_parenting[\"keywords\"] = df_parenting[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_foodndrink[\"keywords\"] = df_foodndrink[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_worldnews[\"keywords\"] = df_worldnews[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_business[\"keywords\"] = df_business[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)\ndf_sports[\"keywords\"] = df_sports[\"keywords\"].str.replace(pat=r'[-]', repl=r' ', regex=True)","74f6332e":"politics = df_politics[\"keywords\"].value_counts().head()\nentertainment = df_entertainment[\"keywords\"].value_counts().head()\ntravel = df_travel[\"keywords\"].value_counts().head() \nstylenbeauty = df_stylenbeauty[\"keywords\"].value_counts().head() \nparenting = df_parenting[\"keywords\"].value_counts().head()\nfoodndrink = df_foodndrink[\"keywords\"].value_counts().head() \nworldnews = df_worldnews[\"keywords\"].value_counts().head() \nbusiness = df_business[\"keywords\"].value_counts().head() \nsports = df_sports[\"keywords\"].value_counts().head() ","7a9e8c9b":"fig = plt.figure(figsize=(14,30))  \n\ncolors = [\"powderblue\", \"lightskyblue\", \"deepskyblue\", \"dodgerblue\", \"steelblue\"]\n\nax0 = fig.add_subplot(5,2,1) \npie0 = ax0.pie(labels = wellness.index, x=politics, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\n\nax1 = fig.add_subplot(5,2,2) \npie1 = ax1.pie(labels = politics.index, x=politics, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax2 = fig.add_subplot(5,2,3) \npie2 = ax2.pie(labels = entertainment.index, x=entertainment, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax3 = fig.add_subplot(5,2,4) \npie3 = ax3.pie(labels = travel.index, x=travel, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax4 = fig.add_subplot(5,2,5) \npie4 = ax4.pie(labels = stylenbeauty.index, x=stylenbeauty, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax5 = fig.add_subplot(5,2,6) \npie5 = ax5.pie(labels = parenting.index, x=parenting, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax6 = fig.add_subplot(5,2,7) \npie6 = ax6.pie(labels = foodndrink.index, x=foodndrink, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax7 = fig.add_subplot(5,2,8) \npie7 = ax7.pie(labels = worldnews.index, x=worldnews, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax8 = fig.add_subplot(5,2,9) \npie8 = ax8.pie(labels = business.index, x=business, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\nax9 = fig.add_subplot(5,2,10) \npie9 = ax9.pie(labels = sports.index, x=sports, colors = colors,\n       autopct=lambda p : '{:.2f}%'.format(p),wedgeprops=dict(width=0.6))\n\nax0.set_title(\"Wellness\", fontsize = 20)\nax1.set_title(\"Politics\", fontsize = 20)\nax2.set_title(\"Entertainment\", fontsize = 20)\nax3.set_title(\"Travel\", fontsize = 20)\nax4.set_title(\"Style & Beauty\", fontsize = 20)\nax5.set_title(\"Parenting\", fontsize = 20)\nax6.set_title(\"Food & Drink\", fontsize = 20)\nax7.set_title(\"World news\", fontsize = 20)\nax8.set_title(\"Business\", fontsize = 20)\nax9.set_title(\"Sports\", fontsize = 20)\n\nplt.show()","7566e0d6":"import itertools\nfrom itertools import chain, cycle\nfrom collections import Counter","9df84c21":"# make headline words to list\ndef make_list(processed_texts):\n    for processed_text in processed_texts:\n        global categorical_list\n        categorical_list = list(chain(*zip(processed_texts.split(), cycle(' '))))[:-1]\n    return categorical_list","1c3f2470":"def make_counter_df(categorical_list):\n# make counter list #use Counter()\n    categorical_list = Counter(categorical_list)\n    categorical_list = categorical_list.most_common()[1:20] # 0 is ' '\n# make dataframe       \n    for element in categorical_list:\n        element = pd.DataFrame(categorical_list, \n                              columns = [\"word_list\", \"count\"]) \n        return element","c5bac611":"wellness_list = df_wellness[\"cleaned_headline\"].apply(make_list)\nwellness_list = list(itertools.chain(*wellness_list))","cd559460":"wellness_df = make_counter_df(wellness_list)\nwellness_df.style.background_gradient(cmap='spring')","c29cf024":"plt.figure(figsize=[15, 5])\nax = sns.barplot(data = wellness_df, y=\"count\", x=\"word_list\", palette=\"spring\", edgecolor='black')\nax.set_xticklabels(ax.get_xticklabels(), fontsize = 19, rotation = 45)\nplt.yticks(size = 20)\nplt.xlabel(\"Words\", fontsize = 15)\nplt.ylabel(\"Count\", fontsize = 15)\nax.set_title(\"Most common words in Wellness news\", fontsize = 23)\nsns.set(style=\"whitegrid\")\nplt.show();","ce0be939":"# Let's do the process \npolitics_list = df_politics[\"cleaned_headline\"].apply(make_list)\npolitics_list = list(itertools.chain(*politics_list))\n\nentertainment_list = df_entertainment[\"cleaned_headline\"].apply(make_list)\nentertainment_list = list(itertools.chain(*entertainment_list))\n\ntravel_list = df_travel[\"cleaned_headline\"].apply(make_list)\ntravel_list = list(itertools.chain(*travel_list))\n\nstylenbeauty_list = df_stylenbeauty[\"cleaned_headline\"].apply(make_list)\nstylenbeauty_list = list(itertools.chain(*stylenbeauty_list))\n\nparenting_list = df_parenting[\"cleaned_headline\"].apply(make_list)\nparenting_list = list(itertools.chain(*parenting_list))\n\nfoodndrink_list = df_foodndrink[\"cleaned_headline\"].apply(make_list)\nfoodndrink_list = list(itertools.chain(*foodndrink_list))\n\nworldnews_list = df_worldnews[\"cleaned_headline\"].apply(make_list)\nworldnews_list = list(itertools.chain(*worldnews_list))\n\nbusiness_list = df_business[\"cleaned_headline\"].apply(make_list)\nbusiness_list = list(itertools.chain(*business_list))\n\nsports_list = df_sports[\"cleaned_headline\"].apply(make_list)\nsports_list = list(itertools.chain(*sports_list))","994cb456":"politics_df = make_counter_df(politics_list)\nentertainment_df = make_counter_df(entertainment_list)\ntravel_df = make_counter_df(travel_list)\nstylenbeauty_df = make_counter_df(stylenbeauty_list)\nparenting_df = make_counter_df(parenting_list)\nfoodndrink_df = make_counter_df(foodndrink_list)\nworldnews_df = make_counter_df(worldnews_list)\nbusiness_df = make_counter_df(business_list)\nsports_df = make_counter_df(sports_list)","87dd7ea4":"fig = plt.figure(figsize=(15, 60))\n\nax0 = fig.add_subplot(10,1,1)\nsns.barplot(data = wellness_df, y=\"count\", x=\"word_list\", ax=ax0,\n            palette=\"spring\", edgecolor='black')\nax1 = fig.add_subplot(10,1,2)\nsns.barplot(data = politics_df, y=\"count\", x=\"word_list\", ax=ax1,\n            palette=\"summer\", edgecolor='black')\nax2 = fig.add_subplot(10,1,3)\nsns.barplot(data = entertainment_df, y=\"count\", x=\"word_list\", ax=ax2,\n            palette=\"autumn\", edgecolor='black')\nax3 = fig.add_subplot(10,1,4)\nsns.barplot(data = travel_df, y=\"count\", x=\"word_list\", ax=ax3,\n            palette=\"winter\", edgecolor='black')\nax4 = fig.add_subplot(10,1,5)\nsns.barplot(data = stylenbeauty_df, y=\"count\", x=\"word_list\", ax=ax4,\n            palette=\"gist_rainbow\", edgecolor='black')\nax5 = fig.add_subplot(10,1,6)\nsns.barplot(data = parenting_df, y=\"count\", x=\"word_list\", ax=ax5,\n            palette=\"rainbow\", edgecolor='black')\nax6 = fig.add_subplot(10,1,7)\nsns.barplot(data = foodndrink_df, y=\"count\", x=\"word_list\", ax=ax6,\n            palette=\"cubehelix\", edgecolor='black')\nax7 = fig.add_subplot(10,1,8)\nsns.barplot(data = worldnews_df, y=\"count\", x=\"word_list\", ax=ax7,\n            palette=\"nipy_spectral\", edgecolor='black')\nax8 = fig.add_subplot(10,1,9)\nsns.barplot(data = business_df, y=\"count\", x=\"word_list\", ax=ax8,\n            palette=\"gist_ncar\", edgecolor='black')\nax9 = fig.add_subplot(10,1,10)\nsns.barplot(data = sports_df, y=\"count\", x=\"word_list\", ax=ax9,\n            palette=\"terrain\", edgecolor='black')\n\nax0.set(xlabel='')\nax1.set(xlabel='')\nax2.set(xlabel='')\nax3.set(xlabel='')\nax4.set(xlabel='')\nax5.set(xlabel='')\nax6.set(xlabel='')\nax7.set(xlabel='')\nax8.set(xlabel='')\nax9.set(xlabel='')\n\nax0.set_title(\"Most common words in Wellness news\", fontsize = 22)\nax1.set_title(\"Most common words in Politics news\", fontsize = 22)\nax2.set_title(\"Most common words in Entertainment news\", fontsize = 22)\nax3.set_title(\"Most common words in Travel news\", fontsize = 22)\nax4.set_title(\"Most common words in Style & Beauty news\", fontsize = 22)\nax5.set_title(\"Most common words in Parenting news\", fontsize = 22)\nax6.set_title(\"Most common words in Food & Drink news\", fontsize = 22)\nax7.set_title(\"Most common words in World-news news\", fontsize = 22)\nax8.set_title(\"Most common words in Business news\", fontsize = 22)\nax9.set_title(\"Most common words in Sports news\", fontsize = 22)\n\nplt.show()","c4c54b0f":"wellness_list = df_wellness[\"lemma_description\"].apply(make_list)\nwellness_list = list(itertools.chain(*wellness_list))\n\npolitics_list = df_politics[\"lemma_description\"].apply(make_list)\npolitics_list = list(itertools.chain(*politics_list))\n\nentertainment_list = df_entertainment[\"lemma_description\"].apply(make_list)\nentertainment_list = list(itertools.chain(*entertainment_list))\n\ntravel_list = df_travel[\"lemma_description\"].apply(make_list)\ntravel_list = list(itertools.chain(*travel_list))\n\nstylenbeauty_list = df_stylenbeauty[\"lemma_description\"].apply(make_list)\nstylenbeauty_list = list(itertools.chain(*stylenbeauty_list))\n\nparenting_list = df_parenting[\"lemma_description\"].apply(make_list)\nparenting_list = list(itertools.chain(*parenting_list))\n\nfoodndrink_list = df_foodndrink[\"lemma_description\"].apply(make_list)\nfoodndrink_list = list(itertools.chain(*foodndrink_list))\n\nworldnews_list = df_worldnews[\"lemma_description\"].apply(make_list)\nworldnews_list = list(itertools.chain(*worldnews_list))\n\nbusiness_list = df_business[\"lemma_description\"].apply(make_list)\nbusiness_list = list(itertools.chain(*business_list))\n\nsports_list = df_sports[\"lemma_description\"].apply(make_list)\nsports_list = list(itertools.chain(*sports_list))","a1fdb74f":"def make_counter_df2(categorical_list):\n# make counter list #use Counter()\n    categorical_list = Counter(categorical_list)\n    categorical_list = categorical_list.most_common()[1:14]\n# make dataframe       \n    for element in categorical_list:\n        element = pd.DataFrame(categorical_list, \n                              columns = [\"word_list\", \"count\"]) \n        return element","b1f0bf41":"wellness_df = make_counter_df2(wellness_list)\npolitics_df = make_counter_df2(politics_list)\nentertainment_df = make_counter_df2(entertainment_list)\ntravel_df = make_counter_df2(travel_list)\nstylenbeauty_df = make_counter_df2(stylenbeauty_list)\nparenting_df = make_counter_df2(parenting_list)\nfoodndrink_df = make_counter_df2(foodndrink_list)\nworldnews_df = make_counter_df2(worldnews_list)\nbusiness_df = make_counter_df2(business_list)\nsports_df = make_counter_df2(sports_list)","4be9939d":"wellness_df.style.background_gradient(cmap='Wistia')","83b72279":"# !pip install squarify\nimport squarify","6f67b7f6":"fig = plt.figure(figsize=(14, 28))\ncolor=[\"orangered\", \"tomato\",\"orange\", \"wheat\", \"gold\", \n       \"yellow\", \"cornsilk\", \"greenyellow\", \"lawngreen\", \"palegreen\",\n       \"lime\", \"aqua\", \"deepskyblue\"]\n\nax0 = fig.add_subplot(5,2,1)\nsquarify.plot(sizes=wellness_df['count'], label=wellness_df['word_list'], color = color, ax=ax0)\n\nax1 = fig.add_subplot(5,2,2)\nsquarify.plot(sizes=politics_df['count'], label=politics_df['word_list'], color = color, ax=ax1, alpha = 0.9)\n\nax2 = fig.add_subplot(5,2,3)\nsquarify.plot(sizes=entertainment_df['count'], label=entertainment_df['word_list'], color = color, ax=ax2, alpha = 0.8)\n\nax3 = fig.add_subplot(5,2,4)\nsquarify.plot(sizes=travel_df['count'], label=travel_df['word_list'], color = color, ax=ax3, alpha = 0.7)\n\nax4 = fig.add_subplot(5,2,5)\nsquarify.plot(sizes=stylenbeauty_df['count'], label=stylenbeauty_df['word_list'], color = color, ax=ax4, alpha = 0.6)\n\nax5 = fig.add_subplot(5,2,6)\nsquarify.plot(sizes=parenting_df['count'], label=parenting_df['word_list'], color = color, ax=ax5, alpha = 0.5)\n\nax6 = fig.add_subplot(5,2,7)\nsquarify.plot(sizes=foodndrink_df['count'], label=foodndrink_df['word_list'], color = color, ax=ax6, alpha = 0.4)\n\nax7 = fig.add_subplot(5,2,8)\nsquarify.plot(sizes=worldnews_df['count'], label=worldnews_df['word_list'], color = color, ax=ax7, alpha = 0.3)\n\nax8 = fig.add_subplot(5,2,9)\nsquarify.plot(sizes=business_df['count'], label=business_df['word_list'], color = color, ax=ax8, alpha = 0.2)\n\nax9 = fig.add_subplot(5,2,10)\nsquarify.plot(sizes=sports_df['count'], label=sports_df['word_list'], color = color, ax=ax9, alpha = 0.1)\n\nax0.set_title(\"Wellness news\", fontsize = 20)\nax1.set_title(\"Politics news\", fontsize = 20)\nax2.set_title(\"Entertainment news\", fontsize = 20)\nax3.set_title(\"Travel news\", fontsize = 20)\nax4.set_title(\"Style & Beauty news\", fontsize = 20)\nax5.set_title(\"Parenting news\", fontsize = 20)\nax6.set_title(\"Food & Drink news\", fontsize = 20)\nax7.set_title(\"World-news news\", fontsize = 20)\nax8.set_title(\"Business news\", fontsize = 20)\nax9.set_title(\"Sports news\", fontsize = 20)\n\nax0.axis('off')\nax1.axis('off')\nax2.axis('off')\nax3.axis('off')\nax4.axis('off')\nax5.axis('off')\nax6.axis('off')\nax7.axis('off')\nax8.axis('off')\nax9.axis('off')\n\nplt.show()","a613fde2":"# devide into train and test set\n\nSEED = 42\n\nfrom sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(df, test_size=0.2, shuffle=True, random_state=SEED)","b34ba4e2":"print(len(train_data))\nprint(len(test_data))","083e0609":"# save to csv file\ntrain_data.to_csv(\"train.csv\")\ntest_data.to_csv(\"test.csv\")","a110914c":"train_reading = pd.read_csv(\".\/train.csv\")\ntrain_reading.head(1) \n# An unnamed column was added. \n# I will remove this column in upcoming cell.","e4b1455c":"TEXT = Field(sequential=True, tokenize=cnn_tokenization, lower = True,\n             include_lengths=True)\nLABEL = LabelField(dtype=torch.long)","9e18d95d":"Raw_datafields = [(\"Unnamed:0\", None), (\"category\", LABEL),\n                 (\"headline\", None), (\"short_description\", TEXT), \n                 (\"keywords\", None), (\"label\", None)]\n\ntrain_data = data.TabularDataset(\n        path='.\/train.csv',\n        format='csv',\n        skip_header=True, \n        fields=Raw_datafields) \ntest_data = data.TabularDataset(\n        path='.\/test.csv',\n        format='csv',\n        skip_header=True,\n        fields=Raw_datafields)","fa4e0d30":"print(vars(train_data[0]))\nprint(vars(test_data[0]))","6dfdbf9c":"MAX_VOCAB_SIZE = 25000\n\nTEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\nLABEL.build_vocab(train_data)","1ff429e2":"print(TEXT.vocab.freqs.most_common(30))","5f9ea4c9":"print(f\"Unique tokens in TEXT vocabulary with <unk> and <pad>: {len(TEXT.vocab)}\") ","4742fbe0":"print(LABEL.vocab.stoi)","78e03d5c":"BATCH_SIZE = 32\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, test_data), sort = False, repeat = False,\n    sort_within_batch =False, batch_size = BATCH_SIZE, device = device)","a45ae657":"class CNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n                 dropout, pad_idx):\n        \n        super().__init__()\n                \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.convs = nn.ModuleList([\n                                    nn.Conv2d(in_channels = 1, \n                                              out_channels = n_filters, \n                                              kernel_size = (fs, embedding_dim)) \n                                    for fs in filter_sizes\n                                    ])\n        \n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        \n        embedded = self.embedding(text)\n        \n        embedded = embedded.permute(1, 0, 2)\n        \n        embedded = embedded.unsqueeze(1)\n        \n        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n                \n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n        \n        cat = self.dropout(torch.cat(pooled, dim = 1))\n            \n        return self.fc(cat)","83be2a62":"INPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nN_FILTERS = 100\nFILTER_SIZES = [3,4,5]\nOUTPUT_DIM = len(LABEL.vocab)\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\nN_EPOCHS = 15\nmodel = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)","e72c093b":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\nmodel = model.to(device)\ncriterion = criterion.to(device)","52166406":"def categorical_accuracy(preds, y):\n    top_pred = preds.argmax(1, keepdim = True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() \/ y.shape[0]\n    return acc","6c90216b":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        text, _ = batch.short_description\n        \n        predictions = model(text)\n        \n        loss = criterion(predictions, batch.category)\n        \n        acc = categorical_accuracy(predictions, batch.category)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","cf526ea0":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            text, _ = batch.short_description\n            \n            predictions = model(text)\n            \n            loss = criterion(predictions, batch.category)\n            \n            acc = categorical_accuracy(predictions, batch.category)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","66ff8720":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","a9a8bd6b":"N_EPOCHS = N_EPOCHS\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, \n                                  train_iterator, \n                                  optimizer, \n                                  criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    test_loss, test_accuracy = evaluate(model, test_iterator, criterion)\n    torch.save(model.state_dict(), 'cnn-model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_acc*100:.2f}%')\n    print(f'\\tTest Loss :{test_loss} | Test Accuracy : {test_accuracy}')","b0816a40":"train_df = pd.read_csv(\".\/train.csv\")\ntest_df = pd.read_csv(\".\/test.csv\")","3c5e115e":"train_df[\"cleaned\"] = train_df[\"short_description\"].apply(text_tokenization)\ntrain_df.head(2)","5cc65e2a":"from sklearn.feature_extraction.text import CountVectorizer","ee70452b":"bow_transformer = CountVectorizer().fit(train_df['cleaned'])","e0350fdf":"category_bow = bow_transformer.transform(train_df['cleaned'])","eecc36f2":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(category_bow)","9311736e":"category_tfidf=tfidf_transformer.transform(category_bow)\nprint(category_tfidf.shape)","0fd852d3":"# Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB \n\ncategory_detect_model = MultinomialNB().fit(category_tfidf, train_df['label'])","a3408888":"NB_predictions = category_detect_model.predict(category_tfidf)","057785c8":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(train_df['label'], NB_predictions))\nprint(confusion_matrix(train_df['label'], NB_predictions))","6c0017c8":"# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\ncategory_detect_model2 = LogisticRegression(solver='liblinear').fit(category_tfidf, train_df['label'])","121369c4":"LR_predictions = category_detect_model2.predict(category_tfidf)","c6d613c4":"print(classification_report(train_df['label'], LR_predictions))\nprint(confusion_matrix(train_df['label'], LR_predictions))","4cb77f37":"test_df[\"cleaned\"] = test_df[\"short_description\"].apply(text_tokenization)\ntest_df.head(2)","7a11dfd0":"from sklearn.pipeline import Pipeline\npipeline_NB = Pipeline([\n   ( 'bow',CountVectorizer()),\n    ('tfidf',TfidfTransformer()),\n    ('classifier',MultinomialNB()),\n])\npipeline_LR = Pipeline([\n   ( 'bow',CountVectorizer()),\n    ('tfidf',TfidfTransformer()),\n    ('classifier',LogisticRegression(solver='liblinear')),\n])","29249be2":"test_df[\"pred_NB\"] = \"\"\ntest_df[\"pred_LR\"] = \"\"","ed3df2a6":"train_TEXT = train_df[\"cleaned\"]\ninput_TEXT = test_df['cleaned']\n\nNB_train = train_df['label']\nNB_input = test_df['pred_NB']\n\nLR_train = train_df['label']\nLR_input = test_df['pred_LR']","fd710538":"pipeline_NB.fit(train_TEXT, NB_train)","1e49d08f":"pipeline_LR.fit(train_TEXT, LR_train)","5d54f2b9":"predictions_NB = pipeline_NB.predict(input_TEXT)\npredictions_LR = pipeline_LR.predict(input_TEXT)","25278996":"input_predictions_NB = pd.DataFrame(predictions_NB) \ninput_predictions_NB.columns = ['pred_NB']\ntest_df[\"pred_NB\"] = input_predictions_NB['pred_NB']\n\ninput_predictions_LR = pd.DataFrame(predictions_LR) \ninput_predictions_LR.columns = ['pred_LR']\ntest_df[\"pred_LR\"] = input_predictions_LR['pred_LR']","42baaa0c":"print(classification_report(test_df[\"label\"],input_predictions_NB))","2dc61aa1":"print(classification_report(test_df[\"label\"],input_predictions_LR))","8aaad8f7":"**News headlines contain essential information of the news contents. This is why I analyze the words in headlines!**","83a79781":"# **Table**\n\ud83e\udde11. Visualization\n\ud83d\udc992. Convolutional Neural Network\n\ud83d\udc9b3. Naive Bayes\n\ud83d\udc9a4. Logistic Regression","0909d5d6":"![Image Source:Markus Winkler](http:\/\/images.unsplash.com\/photo-1586339949916-3e9457bef6d3?ixid=MnwxMjA3fDB8MHxzZWFyY2h8M3x8bmV3c3xlbnwwfHwwfHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=600&q=60)","f77abb5f":"# \ud83d\udcc2<font color=\"Midnightblue\">Thank <font color=\"purple\">you <font color=\"red\">for <font color=\"orange\">reading <font color=\"greenyellow\">my <font color=\"blue\">notebook. ","b3906028":"# \ud83e\udde9 Tokenization","b3f9d113":"# \ud83c\udfaa Convolutional Neural Network model","3169ddfc":"# \ud83d\udea9Most common words in categories","6a030e77":"# \ud83d\udcde Import libraries","85214f23":"**OMG! THERE IS EMOJI IN NEWS SCRIPT...This is why the missingno graph could not find null valus in the column. OK, I will consider the emoji remover when I clean text.**","16224095":"# --\ud83d\udccc Within descriptions","f5f8a34e":"Reference to : https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/blob\/master\/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb","15246061":"**In keywords column, there are many null value. But I want to know the overview of the contents. In spite of null value, this helps me to understand information.**","ced36d97":"# \ud83e\uddf5 Machine learning","c83555c2":"# **\ud83e\uddf6<font color=\"red\">N<font color=\"orange\">e<font color=\"yellow\">w<font color=\"yellowgreen\">s <font color=\"lime\"> c<font color=\"green\">a<font color=\"darkgreen\">t<font color=\"skyblue\">e<font color=\"dodgerblue\">g<font color=\"blue\">o<font color=\"violet\">r<font color=\"purple\">i<font color=\"midnightblue\">e<font color=\"black\">s\ud83e\uddf6**","32384dbc":"# ---\ud83d\udc52 Within headlines","1111adb9":"# \ud83d\udd14Test set prediction","5a5ccecf":"# ---\ud83e\uddfe Within keywords","e939c799":"**In this part, I will used lemmatized texts because stemmed texts have been modified; some of them are far from the root words like the most common words in headlines. Description column has numerous characters and words so that this way looks better.**","cf109c50":"# --Building CNN ","f48317b9":"# \ud83e\uddf5 Explore data set"}}