{"cell_type":{"d6e02f1a":"code","9b044886":"code","063cbf51":"code","1a0567e3":"code","3547a566":"code","2409285d":"code","1cfbf727":"code","f87a6c02":"code","3222e49f":"code","0a744e47":"code","6377b8da":"code","4a5c3919":"code","5e5b7546":"code","1d298bd5":"code","c42fdab8":"code","72558234":"code","2162b56b":"code","feef330f":"code","b43e8c64":"code","512c3a5e":"code","7948c47b":"code","375ee0a5":"code","1e9450cd":"code","2dedc62a":"code","ad85d9ab":"code","be1ce34e":"code","3bce770a":"code","45f2818d":"code","251918bd":"code","96182b45":"code","50401017":"code","ef07f2dc":"code","5c1dd63a":"code","3aa10bb3":"code","22793933":"code","3c09b742":"code","58b48665":"code","f2e4cc98":"code","cc0c9b63":"code","61456b4d":"code","e43ded0e":"code","33c745f1":"code","0327a6e7":"code","98a1b0e4":"code","cb0b6192":"code","e22bea2e":"code","f687cdc3":"code","7ef6ae07":"code","346e768f":"code","b65d56a8":"code","675f3156":"code","2e21a262":"code","a080d42e":"code","01ff00f1":"code","7d3f3dae":"code","3b53f3f0":"code","dfc9c03d":"code","cdcf36be":"code","9447b62b":"code","c3893bdd":"code","02e4cd1d":"code","f2743516":"code","a748ae25":"code","90e968bb":"code","a718d013":"code","371b1a1b":"code","a3993341":"code","f0647a78":"code","74ee83af":"code","7c1cc3f2":"code","61ecc078":"code","947399c3":"code","da41b91d":"code","72df6ea3":"code","600a7bdd":"code","dc63e0ca":"code","8c96e2ef":"code","7bcbfc1a":"code","1d98ab8f":"code","88376ea4":"code","40892649":"code","956c8e36":"code","2d01ce03":"code","cc1b6536":"code","aea44a45":"code","c58293fd":"code","d83fa658":"code","8bb68aae":"code","ff8f4936":"code","db9b2415":"code","4a35e87e":"code","53a02c32":"code","7b72eb2b":"code","228cc523":"code","4bb6f55e":"code","6438e241":"code","ec9d9ebe":"code","77df4f8f":"code","dadd0613":"code","35f1bbf1":"code","6c39f191":"code","27a244ee":"code","ad39203a":"code","87c69390":"code","7272711a":"code","bffd5188":"code","86c035d0":"code","f5664f1e":"code","9234de33":"code","2a8dd77c":"code","5f621de3":"code","39097abe":"code","5ad53aca":"code","25518c8a":"markdown","ad4dc7ba":"markdown","8983b5a7":"markdown","96d243d6":"markdown","d485af93":"markdown","ea2d64fd":"markdown","d43fc53a":"markdown","27ba249e":"markdown","8722160c":"markdown","663e028d":"markdown","b2f7eabd":"markdown","f6b0357b":"markdown","8e959eb8":"markdown","5fa5dd73":"markdown","f3d8c6de":"markdown","f65c57bf":"markdown","5906298a":"markdown","90b7bc7c":"markdown","d0ae0de5":"markdown","afdee162":"markdown","4e01dbe3":"markdown","b7278d17":"markdown","87f126bb":"markdown","99e67703":"markdown","60f10a63":"markdown","f4bf4549":"markdown","8c47e34c":"markdown","2dbf3348":"markdown","daad1f5a":"markdown","9113bcb2":"markdown","ef888c51":"markdown","465ce53d":"markdown","20a9a008":"markdown","a65357de":"markdown","2320baf9":"markdown","8fc43a88":"markdown","d1e8e367":"markdown","797f5348":"markdown","0961c70c":"markdown","1b6504b2":"markdown","faf23875":"markdown","30daec1a":"markdown","5fef39aa":"markdown","05490756":"markdown","570ac3d9":"markdown","d47a920f":"markdown","99d498e5":"markdown","7121bc53":"markdown","03bba9ba":"markdown","20f80962":"markdown","8911e0e6":"markdown","4b53065e":"markdown","16c0804e":"markdown","35c9a247":"markdown","43ce4ca9":"markdown","94b1da31":"markdown","7fba5452":"markdown","af10771a":"markdown","a0c69746":"markdown","01a9c8f9":"markdown","09e6d3ea":"markdown","06bf35ce":"markdown","085eea72":"markdown","380b917f":"markdown","b3c01c15":"markdown","f64f5993":"markdown","3112313c":"markdown","27555f89":"markdown","28732bc4":"markdown","c8ef9051":"markdown","1f8a5e53":"markdown","62576181":"markdown","13c8de0c":"markdown","01492978":"markdown","f6634bc3":"markdown","5048845a":"markdown","381f92a8":"markdown","9992342a":"markdown","bf2b3c1c":"markdown","5bfe895b":"markdown","97e5c556":"markdown","a260883c":"markdown","267b862c":"markdown","480f9c22":"markdown","894e63b4":"markdown","7a16d800":"markdown","6551ad3f":"markdown","6e691a83":"markdown","8f127428":"markdown","b2708d02":"markdown","38f3aab3":"markdown","e4aa187e":"markdown","c6baba63":"markdown","e64a793f":"markdown","ef0b0932":"markdown","8b60d3e5":"markdown","103b964e":"markdown","823fe573":"markdown","b6ca57b3":"markdown","65f50003":"markdown","8549bde4":"markdown","fb13c732":"markdown","b74e2e60":"markdown","51c96955":"markdown","5e689b96":"markdown","51b80387":"markdown","5aac651f":"markdown","ad9309d5":"markdown","d775a09e":"markdown","6710567f":"markdown","a7761603":"markdown","ce58559c":"markdown","e77de49a":"markdown","44d318ce":"markdown","f289a3e6":"markdown","8f9261df":"markdown","88eea9fa":"markdown","ff98d342":"markdown","59f7c2cd":"markdown","520caa82":"markdown","73d8eb76":"markdown","ac15e0a6":"markdown","618571b2":"markdown","11092d2c":"markdown","46f7a3b3":"markdown","3d56a507":"markdown","8f6819f8":"markdown","9774ea92":"markdown","c3db3db2":"markdown","d3249de1":"markdown","c57f0c48":"markdown","c80ca5c8":"markdown","9bd1bb3f":"markdown","92e3b936":"markdown","2fd1a7d5":"markdown","0299a407":"markdown","1176a02b":"markdown","889a3625":"markdown","e348889e":"markdown","d704e80c":"markdown","f0915654":"markdown","6196737c":"markdown","216ac27f":"markdown","21bac9f4":"markdown","276edf5b":"markdown","80f13249":"markdown","889d7e4d":"markdown","5889cff7":"markdown","2834122a":"markdown","dd06fcf9":"markdown","2cfcfcd5":"markdown","0ebb8951":"markdown","77a387b8":"markdown","1b98538d":"markdown","197f9636":"markdown","0ed13843":"markdown","cf687c9a":"markdown","26306cb8":"markdown","2f8b0819":"markdown","4a4f2848":"markdown","71667632":"markdown","f65bfc9f":"markdown","1177d47b":"markdown","da26e143":"markdown","67d06a95":"markdown","0931bfce":"markdown","1af5a5c7":"markdown","6780d2c3":"markdown","1b61da16":"markdown","e2b9b2e4":"markdown","39f73f85":"markdown","6c2ad008":"markdown","83c92cb4":"markdown","abbcf0d9":"markdown","be5b6f08":"markdown","d2e297e4":"markdown","b925d8f8":"markdown","396d59e0":"markdown","8403d383":"markdown","9c0bcff2":"markdown","36dc88d3":"markdown","b701d2d7":"markdown","37335d58":"markdown","35ee999e":"markdown","79f2afa3":"markdown","2ddf346e":"markdown","73bc0399":"markdown","fcf9e9b9":"markdown","1d10330c":"markdown","c425bcc2":"markdown","20caec85":"markdown","2b636e0b":"markdown","924d0341":"markdown","eedfd357":"markdown","b1a7f129":"markdown","cfd65d2e":"markdown"},"source":{"d6e02f1a":"# supress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# 'Os' module provides functions for interacting with the operating system \nimport os\n\n# 'Pandas' is used for data manipulation and analysis\nimport pandas as pd \n\n# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\nimport numpy as np\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\nimport seaborn as sns\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\n\n# 'Statsmodels' is used to build and analyze various statistical models\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.compat import lzip\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# 'SciPy' is used to perform scientific computations\nfrom scipy.stats import jarque_bera\nfrom scipy import stats\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge","9b044886":"# the path for reading the data file\n# Note: Set your path accordingly\n#os.chdir('\/Users\/Dippies\/GL DSE Deliveries\/Regression\/Nov 22')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","063cbf51":"# read csv file using pandas\ndf_adm = pd.read_csv(\"..\/input\/admission\/admission.csv\")\n\n\n# display the top 5 rows of the dataframe\ndf_adm.head()\n\n# Note: In order to display more rows, example 10, use head(10)","1a0567e3":"# .shape returns the dimension of the data\ndf_adm.shape","3547a566":"# use .dtypes to view the data type of a variable\ndf_adm.dtypes","2409285d":"df_adm.Research=df_adm.Research.astype('object')\ndf_adm.Research=df_adm.Research.replace({0:'No',1:'Yes'})\ndf_adm.dtypes","1cfbf727":"# obtain the total missing values for each variable\n# 'isnull().sum()' returns the number of missing values in each variable\n# sort the variables on the basis of total null values in the variable using sort_values()\n# 'ascending = False' sorts values in the descending order\n# the variable with highest number of missing values will appear first\nTotal = df_adm.isnull().sum().sort_values(ascending=False) \n\n# 'isnull().sum()' returns the number of missing values in each variable\n# 'isnull().count()' returns the count of the data, i.e. count of outcomes 'True' and 'False' of isnull() \n# sort the variables on the basis of total null values in the variable using sort_values()\n# 'ascending = False' sorts values in the descending order\n# the variable with highest percentage of missing values will appear first\nPercent = (df_adm.isnull().sum()*100\/df_adm.isnull().count()).sort_values(ascending=False)   \n\n# concat the 'Total' and 'Percent' columns using 'concat' function\n# keys:pass a list of column names in parameter  \n# axis = 1: concats along the columns\nmissing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])\n\n# print the missing data\nmissing_data","f87a6c02":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df_adm.isnull(), cbar=False)\n\n# display the plot\nplt.show()","3222e49f":"# describe the numerical data\ndf_adm.describe()","0a744e47":"# describe the categorical data\n# include=object: selects the categorical features\ndf_adm.describe(include = 'object')\n\n# Note: If I pass 'include=object' to the .describe(),\n# it will return descriptive statistics for categorical variables only","6377b8da":"# display the summary statistics of categorical variables\n# include=objec: selects the categoric features\ndf_adm.describe(include=object)","4a5c3919":"# plot the boxplot for each variable\n# subplots(): plot subplots\n# figsize(): set the figure size\nfig, ax = plt.subplots(2, 3, figsize=(15, 8))\ndf_numeric_features=df_adm.select_dtypes(np.number)\n# plot the boxplot using boxplot() from seaborn\n# z: let the variable z define the boxplot\n# x: data for which the boxplot is to be plotted\n# orient: \"h\" specifies horizontal boxplot (for vertical boxplots use \"v\")\n# whis: proportion of the IQR past the low and high quartiles to extend the plot whiskers\n# ax: specifies the axes object to draw the plot onto\n# set_xlabel(): set the x-axis label\n# fontsize: sets the font size of the x-axis label\nfor variable, subplot in zip(df_numeric_features.columns, ax.flatten()):\n    z = sns.boxplot(x = df_numeric_features[variable], orient = \"h\",whis=1.5 , ax=subplot) # plot the boxplot\n    z.set_xlabel(variable, fontsize = 20)  \nplt.show()\nsns.boxplot(x=df_numeric_features['Chance of Admit'])","5e5b7546":"df_numeric_features.head()","1d298bd5":"# calculate interquartile range \n\n# compute the first quartile using quantile(0.25)\n# axis=1: specifies that the labels are dropped from the columns\nQ1 = df_numeric_features.quantile(0.25)\n\n# compute the first quartile using quantile(0.75)\n# axis=1: specifies that the labels are dropped from the columns\nQ3 = df_numeric_features.quantile(0.75)\n\n# calculate of interquartile range \nIQR = Q3 - Q1\n\n# print the IQR values for numeric variables\nprint(IQR)","c42fdab8":"# filter out the outlier values\n# ~ : selects all rows which do not satisfy the condition\n# |: bitwise operator OR in python\n# any() : returns whether any element is True over the columns\n# axis : \"1\" indicates columns should be altered (use \"0\" for 'index')\ndf_adm = df_adm[~((df_adm < (Q1 - 1.5 * IQR)) | (df_adm > (Q3 + 1.5 * IQR))).any(axis=1)]","72558234":"# check the shape of data using shape\ndf_adm.shape","2162b56b":"# plot the boxplot for each variable\n# subplots(): plot subplots\n# figsize(): set the figure size\nfig, ax = plt.subplots(2, 3, figsize=(15, 8))\ndf_numeric_features=df_adm.select_dtypes(np.number)\n# plot the boxplot using boxplot() from seaborn\n# z: let the variable z define the boxplot\n# x: data for which the boxplot is to be plotted\n# orient: \"h\" specifies horizontal boxplot (for vertical boxplots use \"v\")\n# whis: proportion of the IQR past the low and high quartiles to extend the plot whiskers\n# ax: specifies the axes object to draw the plot onto\n# set_xlabel(): set the x-axis label\n# fontsize: sets the font size of the x-axis label\nfor variable, subplot in zip(df_numeric_features.columns, ax.flatten()):\n    z = sns.boxplot(x = df_numeric_features[variable], orient = \"h\",whis=1.5 , ax=subplot) # plot the boxplot\n    z.set_xlabel(variable, fontsize = 20) \nplt.show()\nsns.boxplot(x=df_numeric_features['Chance of Admit'])","feef330f":"# filter the numerical features in the dataset\n# include=np.number: selects the numeric features\ndf_numeric_features = df_adm.select_dtypes(include=np.number)\n\n# display the numeric features\ndf_numeric_features.columns","b43e8c64":"# generate the correlation matrix \ncorr =  df_numeric_features.corr()\n\n# print the correlation matrix\ncorr","512c3a5e":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot the heat map\n# corr: give the correlation matrix\n# cmap: colour code used for plotting\n# vmax: gives maximum range of values for the chart\n# vmin: gives minimum range of values for the chart\n# annot: prints the correlation values in the chart\n# annot_kws: sets the font size of the annotation\nsns.heatmap(corr, cmap='YlGnBu' ,vmax=1.0, vmin=-1.0, annot = True, annot_kws={\"size\": 15})\n\n# specify name of the plot\nplt.title('Correlation betIen numeric features')\n\n# display the plot\nplt.show()","7948c47b":"plt.figure(figsize=(16,6))\nsns.countplot(data=df_adm,x=\"GRE Score\",saturation=1,hue='Research')\nplt.xticks(rotation=45,size=12)\nplt.grid()\nplt.tight_layout()","375ee0a5":"plt.figure(figsize=(16,6))\nsns.countplot(data=df_adm,x=\"TOEFL Score\",saturation=1,hue='Research')\nplt.xticks(rotation=45,size=12)\nplt.grid()\nplt.tight_layout()","1e9450cd":"plt.figure(figsize=(16,6))\nsns.countplot(data=df_adm,x=\"SOP\",saturation=1,hue='Research')\nplt.xticks(rotation=45,size=12)\nplt.grid()\nplt.tight_layout()","2dedc62a":"plt.figure(figsize=(16,6))\nsns.countplot(data=df_adm,x=\"LOR\",saturation=1,hue='Research')\nplt.xticks(rotation=45,size=12)\nplt.grid()\nplt.tight_layout()","ad85d9ab":"fig, ax = plt.subplots(figsize=(15,8))\nsns.barplot(x='GRE Score',y='Chance of Admit',data=df_adm, linewidth=1.5,edgecolor=\"0.1\")\nplt.show()","be1ce34e":"fig, ax = plt.subplots(figsize=(15,8))\nsns.barplot(x='TOEFL Score',y='Chance of Admit',data=df_adm, linewidth=3.5,edgecolor=\"0.8\")\nplt.show()","3bce770a":"sns.scatterplot(x='University Rating',y='CGPA',data=df_adm,color='Red', marker=\"^\", s=100)","45f2818d":"# filter the numerical features in the dataset using select_dtypes()\n# include=np.number: selects the numeric features\ndf_numeric_features = df_adm.select_dtypes(include=np.number)\n\n# display the numeric features\ndf_numeric_features.columns","251918bd":"# filter the categorical features in the dataset using select_dtypes()\n# include=[np.object]: selects the categoric features\ndf_categoric_features = df_adm.select_dtypes(include=[np.object])\n\n# display categorical features\ndf_categoric_features.columns","96182b45":"# for all categoric variables create dummy variables\nfor col in df_categoric_features.columns.values:\n    \n    # for a feature create dummy variables using get_dummies()\n    # prefix: specify the prefix before creating the dummy variable\n    # \"drop_first=True\": creates 'n-1' dummy variables\n    dummy_encoded_variables = pd.get_dummies(df_categoric_features[col], prefix=col, drop_first=True)\n    \n    # concatenate the categoric features with dummy variables using concat()\n    # axis=1: specifies that the concatenation is column wise\n    df_categoric_features = pd.concat([df_categoric_features, dummy_encoded_variables],axis=1)\n    \n    # drop the orginal categorical variable from the dataframe\n    # axis=1: specifies that the column is to be dropped\n    # inplace: makes permanent changes in the dataframe\n    df_categoric_features.drop([col], axis=1, inplace=True)","50401017":"# concatenate the numerical and dummy encoded categorical variables using concat()\n# axis=1: specifies that the concatenation is column wise\ndf_adm_dummy = pd.concat([df_numeric_features, df_categoric_features], axis=1)\n\n# display data with dummy variables\ndf_adm_dummy.head()","ef07f2dc":"# add the intercept column to the dataset\ndf_adm_dummy = sm.add_constant(df_adm_dummy)\n\n# separate the independent and dependent variables\n# drop(): drops the specified columns\n# axis=1: specifies that the column is to be dropped\nX = df_adm_dummy.drop(['Chance of Admit'], axis=1)\n\n# extract the target variable from the data set\ny = df_adm_dummy['Chance of Admit']\n\n# split data into train subset and test subset for predictor and target variables\n# random_state: the seed used by the random number generator\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# check the dimensions of the train & test subset for \n\n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","5c1dd63a":"# build a full model using OLS()\n# consider the log of claim \nlinreg_full_model = sm.OLS(y_train, X_train).fit()\n\n# print the summary output\nprint(linreg_full_model.summary())","3aa10bb3":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif=pd.DataFrame()\ndf_numeric_features.drop(['Chance of Admit'],axis=1,inplace=True)\nvif['VIF_Factor']=[variance_inflation_factor(df_numeric_features.values,i) for i in range(df_numeric_features.shape[1])]\nvif['Features']=df_numeric_features.columns\nvif","22793933":"from scipy import stats\nfor col in df_numeric_features.columns:\n    print(\"Column \", col, \" :\", stats.shapiro(df_numeric_features[col]))","3c09b742":"from sklearn import preprocessing\nmms = preprocessing.MinMaxScaler()\nmmsfit = mms.fit(df_numeric_features)\ndfxz = pd.DataFrame(mms.fit_transform(df_numeric_features), columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA'])\ndfxz.head()","58b48665":"df_categoric_features=df_categoric_features.reset_index()\ndf_categoric_features.drop(['index'],axis=1,inplace=True)\ndf_categoric_features.head()","f2e4cc98":"dfxz = pd.concat([dfxz, df_categoric_features], axis = 1)\ndfxz.head()","cc0c9b63":"X=dfxz.copy()\nX=sm.add_constant(X)\nX.head()","61456b4d":"y=y.reset_index()\ny.drop(['index'],axis=1,inplace=True)\ny.head()","e43ded0e":"X_train1,X_test1,y_train1,y_test1=train_test_split(X,y,random_state=2,test_size=0.3)\nmlr_model_scaled=sm.OLS(y_train1,X_train1).fit()\nmlr_model_scaled.summary()","33c745f1":"# create vector of result parmeters\nname = ['f-value','p-value']           \n\n# perform Breusch-Pagan test using het_breushpagan()\n# compute residuals using 'resid'\n# 'exog' returns the independent variables in the model alng with the intercept\ntest = sms.het_breuschpagan(mlr_model_scaled.resid, mlr_model_scaled.model.exog)\n\n# print the output\n# use 'lzip' to zip the column names and test results\nlzip(name, test) ","0327a6e7":"# create subplots of scatter plots\n# pass the number of rows in a subplot to 'nrows'\n# pass the number of columns in a subplot to 'ncolumns'\n# set plot size using 'figsize'\nfig, ax = plt.subplots(nrows = 3, ncols= 2, figsize=(25, 20))\n\n# use for loop to create scatter plot for residuals and each independent variable (do not consider the intercept)\n# 'ax' assigns axes object to draw the plot onto \nfor variable, subplot in zip(X_train1.columns[1:], ax.flatten()):\n    sns.scatterplot(X_train1[variable], mlr_model_scaled.resid , ax=subplot)\n\n# display the plot\nplt.show()","98a1b0e4":"# calculate fitted values\nfitted_vals = mlr_model_scaled.predict(X_test1)\n\n# calculate residuals\nresids = y_test1 - fitted_vals\n\n# create subplots using subplots() such that there is one row having one plot\n# 'figsize' sets the figure size\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\n\n# plot the probability plot to check the normality of the residuals\n# plot: if specified plots the least squares fit\nstats.probplot( mlr_model_scaled.resid, plot=plt)\n\n# set the marker type using the set_marker() parameter\n# access the line object from the axes object using ax.get_lines()\n# then, the properties can be changed accordingly\n# set the marker to 'o' to use circles as points\nax.get_lines()[0].set_marker('o')\n\n# set the marker size using the set_markersize() parameter\n# set the marker size to 5\nax.get_lines()[0].set_markersize(5.0)\n\n# set the color of the trend line using set_markerfacecolor()\n# set color of the trend line to red by passing 'r' to the set_markerfacecolor\nax.get_lines()[0].set_markerfacecolor('r')\n\n# set the trend line width\nax.get_lines()[1].set_linewidth(4.0)\n\n# display the plot\nplt.show()","cb0b6192":"# check the mean of the residual\nmlr_model_scaled.resid.mean()","e22bea2e":"# normality test using 'jarque_bera'\n# the test returns the the test statistics and the p-value of the test\nstat, p = jarque_bera(mlr_model_scaled.resid)\n\n# to print the numeric outputs of the Jarque-Bera test upto 3 decimal places\n# %.3f: returns the a floating point with 3 decimal digit accuracy\n# the '%' holds the place where the number is to be printed\nprint('Statistics=%.3f, p-value=%.3f' % (stat, p))\n\n# display the conclusion\n# set the level of significance to 0.05\nalpha = 0.05\n\n# if the p-value is greater than alpha print I accept alpha \n# if the p-value is less than alpha print I reject alpha\nif p > alpha:\n    print('The data is normally distributed (fail to reject H0)')\nelse:\n    print('The data is not normally distributed (reject H0)')","f687cdc3":"mlr_model_scaled_pvalues = pd.DataFrame(mlr_model_scaled.pvalues, columns=[\"P-Value\"])\n\n# print the values\nmlr_model_scaled_pvalues","7ef6ae07":"# select insignificant variables\ninsignificant_variables = mlr_model_scaled_pvalues[mlr_model_scaled_pvalues[\"P-Value\"]  > 0.05]\n\n# get the position of a specified value\ninsigni_var = insignificant_variables.index\n\n# convert the list of variables to 'list' type\ninsigni_var = insigni_var.to_list()\n\n# get the list of insignificant variables\ninsigni_var","346e768f":"# separate the independent and dependent variables\n# drop(): specify the variables to be dropped\n# axis=1: specifies that the columns are to be dropped\n\n# split data into train subset and test subset for predictor and target variables\n# random_state: the seed used by the random number generator\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=1)\n\n# check the dimensions of the train & test subset for \n\n# print dimension of predictors train set\nprint(\"The shape of X_train_scaled is:\",X_train2.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test_scaled is:\",X_test2.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train_scaled is:\",y_train2.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test_scaled is:\",y_test2.shape)","b65d56a8":"# consider the significant variable in the training set\n# drop(): specify the variables to be dropped\n# axis=1: specifies that the columns are to be dropped\nX_train2_significant = X_train2.drop(insigni_var, axis=1)\n\n# consider the significant variable in the training set\n# drop(): specify the variables to be dropped\n# axis=1: specifies that the columns are to be dropped\nX_test2_significant = X_test2.drop(insigni_var, axis=1)","675f3156":"# ordinary least squares regression\n# build a full model with significant scaled variables using OLS()\nlinreg_model_with_significant_scaled_vars = sm.OLS(y_train2,X_train2_significant).fit()\n\n# to print the summary output\nprint(linreg_model_with_significant_scaled_vars.summary())","2e21a262":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr_b=sfs(estimator=lr,k_features='best',forward=False,verbose=0,scoring='r2')\nsfs_backforward=lr_b.fit(X,y)\nsfs_backforward.k_feature_names_","a080d42e":"sfs_backforward.k_score_","01ff00f1":"new_x=X[['const', 'TOEFL Score', 'LOR ', 'CGPA', 'Research_yes']]\n\n# split data into train subset and test subset for predictor and target variables\n# random_state: the seed used by the random number generator\nX_train3, X_test3, y_train3, y_test3 = train_test_split(new_x, y, random_state=1)\n\n# check the dimensions of the train & test subset for \n\n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train3.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test3.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train3.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test3.shape)","7d3f3dae":"# ordinary least squares regression\n# build a full model with significant scaled variables using OLS()\nlinreg_model_backforward = sm.OLS(y_train3,X_train3).fit()\n\n# to print the summary output\nprint(linreg_model_backforward.summary())","3b53f3f0":"# create a generalized function to calculate the RMSE values for train set\ndef get_train_rmse(model,X_train,y_train):\n    \n    # For training set:\n    # train_pred: prediction made by the model on the training dataset 'X_train'\n    # y_train: actual values ofthe target variable for the train dataset\n\n    # predict the output of the target variable from the train data \n    train_pred = model.predict(X_train)\n\n    # calculate the MSE using the \"mean_squared_error\" function\n\n    # MSE for the train data\n    mse_train = mean_squared_error(y_train, train_pred)\n\n    # take the square root of the MSE to calculate the RMSE\n    # round the value upto 4 digits using 'round()'\n    rmse_train = round(np.sqrt(mse_train), 4)\n    \n    # return the training RMSE\n    return(rmse_train)","dfc9c03d":"# create a generalized function to calculate the RMSE values test set\ndef get_test_rmse(model,X_test,y_test):\n    \n    # For testing set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data\n    test_pred = model.predict(X_test)\n\n    # MSE for the test data\n    mse_test = mean_squared_error(y_test, test_pred)\n\n    # take the square root of the MSE to calculate the RMSE\n    # round the value upto 4 digits using 'round()'\n    rmse_test = round(np.sqrt(mse_test), 4)\n\n    # return the test RMSE\n    return(rmse_test)","cdcf36be":"# define a function to get R-squared and adjusted R-squared value\ndef get_score(model,X_train,y_train):\n    \n    # score() returns the R-squared value\n    r_sq = model.rsquared\n    \n    # calculate adjusted R-squared value\n    # 'n' denotes number of observations in train set\n    # 'shape[0]' returns number of rows \n    n = X_train.shape[0]\n    \n    # 'k' denotes number of variables in train set\n    # 'shape[1]' returns number of columns\n    k = X_train.shape[1]\n    \n    # calculate adjusted R-squared using the formula\n    r_sq_adj = 1 - ((1-r_sq)*(n-1)\/(n-k-1))\n    \n    # return the R-squared and adjusted R-squared value \n    return ([r_sq, r_sq_adj])","9447b62b":"# create an empty dataframe to store the scores for various algorithms\nscore_card = pd.DataFrame(columns=['Model_Name', 'Alpha (Wherever Required)', 'l1-ratio', 'R-Squared',\n                                       'Adj. R-Squared', 'Train_RMSE','Test_RMSE'])\n\n# create a function to update the score card for comparision of the scores from different algorithms\n# pass the model name, model build, alpha and l1_ration as input parameters\n# if 'alpha' and\/or 'l1_ratio' is not specified, the function assigns '-'\ndef update_score_card(algorithm_name, model,xtrain,xtest,ytrain,ytest, alpha = '-', l1_ratio = '-'):\n    \n    # assign 'score_card' as global variable\n    global score_card\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Model_Name': algorithm_name,\n                       'Alpha (Wherever Required)': alpha, \n                       'l1-ratio': l1_ratio,\n                       'Train_RMSE':get_train_rmse(model,xtrain,ytrain), \n                       'Test_RMSE': get_test_rmse(model,xtest,ytest), \n                       'R-Squared': get_score(model,xtrain,ytrain)[0], \n                       'Adj. R-Squared': get_score(model,xtrain,ytrain)[1]}, ignore_index = True)","c3893bdd":"def update_score_card1(algorithm_name, model,xtrain,xtest,ytrain,ytest, alpha = '-', l1_ratio = '-'):\n    \n    # assign 'score_card' as global variable\n    global score_card\n    linreg_with_SGD_predictions=linreg_with_SGD.predict(xtest)\n    linreg_SGD_r_squared=r2_score(ytest, linreg_with_SGD_predictions)\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Model_Name': algorithm_name,\n                       'Alpha (Wherever Required)': alpha, \n                       'l1-ratio': l1_ratio,\n                       'Test_RMSE': get_test_rmse(model,xtest,ytest), \n                       'Train_RMSE':get_train_rmse(model,xtrain,ytrain), \n                       'R-Squared': r2_score(ytest, linreg_with_SGD_predictions), \n                       'Adj. R-Squared': 1 - (1-linreg_SGD_r_squared)*(len(ytest)-1)\/(len(ytest)- xtest.shape[1]-1)}, ignore_index = True)","02e4cd1d":"print('Linear Regression model using Feature Scaling ')\n# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(mlr_model_scaled,X_train1,y_train1))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(mlr_model_scaled,X_test1,y_test1))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(mlr_model_scaled,X_train1,y_train1) - get_train_rmse(mlr_model_scaled,X_test1,y_test1))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","f2743516":"# update the dataframe 'score_card'\nupdate_score_card('Linear Regression using Feature Scaling',mlr_model_scaled,X_train1,X_test1,y_train1,y_test1)\n\n# print the dataframe\nscore_card","a748ae25":"print('Linear Regression model with significant variables')\n# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(linreg_model_with_significant_scaled_vars,X_train2_significant,y_train2))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(linreg_model_with_significant_scaled_vars,X_test2_significant,y_test2))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(linreg_model_with_significant_scaled_vars,X_train2_significant,y_train2) - get_train_rmse(linreg_model_with_significant_scaled_vars,X_test2_significant,y_test2))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","90e968bb":"# update the dataframe 'score_card'\nupdate_score_card('Linear Regression with significant variables',linreg_model_with_significant_scaled_vars,X_train2_significant,X_test2_significant,y_train2,y_test2)\n\n# print the dataframe\nscore_card","a718d013":"print('Linear Regression model using backward elimination')\n# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(linreg_model_backforward,X_train3,y_train3))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(linreg_model_backforward,X_test3,y_test3))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(linreg_model_backforward,X_train3,y_train3) - get_train_rmse(linreg_model_backforward,X_test3,y_test3))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","371b1a1b":"# update the dataframe 'score_card'\nupdate_score_card('Linear Regression using backward elimination',linreg_model_backforward,X_train3,X_test3,y_train3,y_test3)\n\n# print the dataframe\nscore_card","a3993341":"# n_splits: specify the number of k folds\nkf = KFold(n_splits = 5)\n\n# create a function 'get_score' that returns the R-squared score for the training set\n# 'get_score' takes 5 input parameters\ndef Get_score(model, X_train_k, X_test_k, y_train_k, y_test_k):\n    model.fit(X_train_k, y_train_k)                                   # fit the model\n    return model.score(X_test_k, y_test_k)                            # return the R-squared value","f0647a78":"# split data into train subset and test subset\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.3)","74ee83af":"# create an empty list to store the scores\nscores = []\n\n# kf.split() splits the indices of X_train into train_index and test_index\n# further dividing the X_train and y_train sets into train and test sets for cross validation\n# Remember: Cross validation works on training set not on test set\n# use '\\' for stacking the code \nfor train_index, test_index in kf.split(X_train):\n    X_train_k, X_test_k, y_train_k, y_test_k = X_train.iloc[train_index], X_train.iloc[test_index], y_train.iloc[train_index], y_train.iloc[test_index]\n   \n    # call the function 'get_scores()' and append the scores in the list 'scores'\n    scores.append(Get_score(LinearRegression(), X_train_k, X_test_k, y_train_k, y_test_k)) \n    \n# print all scores\nprint('All scores: ', scores)\n\n# print the minimum score from the list\n# use 'round()' to round-off the minimum score upto 4 digits\n# min() returns minimum score \nprint(\"\\nMinimum score obtained: \", round(min(scores), 4))\n\n# print the maximum score from the list\n# use 'round()' to round-off the maximum score upto 4 digits\n# max() returns maximum score \nprint(\"Maximum score obtained: \", round(max(scores), 4))\n\n# print the average score from the list\n# use 'round()' to round-off the average score upto 4 digits\n# np.mean() returns average score \nprint(\"Average score obtained: \", round(np.mean(scores), 4))","7c1cc3f2":"# using cross_val_score() for k-fold cross validation\n# estimator: pass the machine learning function. Here I are performing linear regression\n# pass the X_train and y_train sets\n# cv: stands for number of folds. Similar to k in KFold\n# scoring: pass the scoring parameter e.g. 'r2' for r-squared, 'neg_mean_squared_error' for mean squared error (negative)\nscores = cross_val_score(estimator = LinearRegression(), \n                         X = X_train, \n                         y = y_train, \n                         cv = 5, \n                         scoring = 'r2')","61ecc078":"# print all scores\nprint('All scores: ', scores)\n\n# print the minimum score from the list\n# use 'round()' to round-off the minimum score upto 4 digits\n# min() returns minimum score \nprint(\"\\nMinimum score obtained: \", round(min(scores), 4))\n\n# print the maximum score from the list\n# use 'round()' to round-off the maximum score upto 4 digits\n# max() returns maximum score \nprint(\"Maximum score obtained: \", round(max(scores), 4))\n\n# print the average score from the list\n# use 'round()' to round-off the average score upto 4 digits\n# np.mean() returns average score \nprint(\"Average score obtained: \", round(np.mean(scores), 4))","947399c3":"# split data into train subset and test subset\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)","da41b91d":"# creating a function 'get_score' that returns the R-squared score for the training set\n# 'get_score' takes 5 input parameters\ndef Get_score(model, X_train_k, X_test_k, y_train_k, y_test_k):\n    model.fit(X_train_k, y_train_k)                               # fit the model\n    return model.score(X_test_k, y_test_k)                        # return the R-squared value","72df6ea3":"# create an empty to store the MSE for each model\nloocv_rmse = []\n\n# instantiate the LOOCV method \nloocv = LeaveOneOut()\n\n# use the for loop to build the regression model for each cross validation \n# use split() to split the dataset into two subsets; one with (n-1) data points and another with 1 data point\n# where, n = total number of observations\n\nfor train_index, test_index in loocv.split(X_train):\n    # create the train dataset, use iloc[] to retrieve the corresponding observations in train data\n    # create the test dataset, use iloc[] to retrieve the corresponding observations in test data\n    # # use '\\' for stacking the code \n    X_train_l, X_test_l, y_train_l, y_test_l = X_train.iloc[train_index], X_train.iloc[test_index], \\\n                                               y_train.iloc[train_index], y_train.iloc[test_index]\n    \n    # instantiate the regression model\n    linreg = LinearRegression()\n    \n    # fit the model on training dataset\n    linreg.fit(X_train_l, y_train_l)\n    \n    # calculate MSE using test dataset\n    # use predict() to predict the values of target variable\n    mse = mean_squared_error(y_test_l, linreg.predict(X_test_l))\n    \n    # calculate the RMSE\n    rmse = np.sqrt(mse)\n    \n    # use append() to add each RMSE to the list 'loocv_rmse'\n    loocv_rmse.append(rmse)","600a7bdd":"# print the minimum rmse from the list\n# use 'round()' to round-off the minimum rmse upto 4 digits\n# min() returns minimum rmse \nprint(\"\\nMinimum rmse obtained: \", round(min(loocv_rmse), 4))\n\n# print the maximum rmse from the list\n# use 'round()' to round-off the maximum rmse upto 4 digits\n# max() returns maximum rmse \nprint(\"Maximum rmse obtained: \", round(max(loocv_rmse), 4))\n\n# print the average rmse from the list\n# use 'round()' to round-off the average rmse upto 4 digits\n# np.mean() returns average rmse \nprint(\"Average rmse obtained: \", round(np.mean(loocv_rmse), 4))","dc63e0ca":"# import SGDRegressor from sklearn to perform linear regression with stochastic gradient descent\nfrom sklearn.linear_model import SGDRegressor\n# instantiate the SGDRegressor\n# set 'random_state' to generate the same dataset each time you run the code \nsgd = SGDRegressor(random_state = 10)\n\n# build the model on train data \n# use fit() to fit the model\nlinreg_with_SGD = sgd.fit(X_train3, y_train3)\n\n# print RMSE for train set\n# call the function 'get_train_rmse'\nprint('RMSE on train set:', get_train_rmse(linreg_with_SGD,X_train3,y_train3))\n\n# print RMSE for test set\n# call the function 'get_test_rmse'\nprint('RMSE on test set:', get_test_rmse(linreg_with_SGD,X_test3,y_test3))","8c96e2ef":"# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(linreg_with_SGD,X_train3,y_train3) - get_train_rmse(linreg_with_SGD,X_test3,y_test3))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","7bcbfc1a":"# define a function to plot a barplot\n# pass the model \ndef plot_coefficients(model, algorithm_name):\n    # create a dataframe of variable names and their corresponding value of coefficients obtained from model\n    # 'columns' returns the column names of the dataframe 'X'\n    # 'coef_' returns the coefficient of each variable\n    df_coeff = pd.DataFrame({'Variable': X_train3.columns, 'Coefficient': model.params})\n\n    # sort the dataframe in descending order\n    # 'sort_values' sorts the column based on the values\n    # 'ascending = False' sorts the values in the descending order\n    sorted_coeff = df_coeff.sort_values('Coefficient', ascending = False)\n\n    # plot a bar plot with Coefficient on the x-axis and Variable names on y-axis\n    # pass the data to the parameter, 'sorted_coeff' to plot the barplot\n    sns.barplot(x = \"Coefficient\", y = \"Variable\", data = sorted_coeff)\n    \n    # add x-axis label\n    # set the size of the text using 'fontsize'\n    plt.xlabel(\"Coefficients from {}\".format(algorithm_name), fontsize = 15)\n\n    # add y-axis label\n    # set the size of the text using 'fontsize'\n    plt.ylabel('Features', fontsize = 15)","1d98ab8f":"def plot_coefficients1(model, algorithm_name):\n    # create a dataframe of variable names and their corresponding value of coefficients obtained from model\n    # 'columns' returns the column names of the dataframe 'X'\n    # 'coef_' returns the coefficient of each variable\n    df_coeff = pd.DataFrame({'Variable': X_train3.columns, 'Coefficient': model.coef_})\n\n    # sort the dataframe in descending order\n    # 'sort_values' sorts the column based on the values\n    # 'ascending = False' sorts the values in the descending order\n    sorted_coeff = df_coeff.sort_values('Coefficient', ascending = False)\n\n    # plot a bar plot with Coefficient on the x-axis and Variable names on y-axis\n    # pass the data to the parameter, 'sorted_coeff' to plot the barplot\n    sns.barplot(x = \"Coefficient\", y = \"Variable\", data = sorted_coeff)\n    \n    # add x-axis label\n    # set the size of the text using 'fontsize'\n    plt.xlabel(\"Coefficients from {}\".format(algorithm_name), fontsize = 15)\n\n    # add y-axis label\n    # set the size of the text using 'fontsize'\n    plt.ylabel('Features', fontsize = 15)","88376ea4":"# subplot() is used to plot the multiple plots as a subplot\n# (1,2) plots a subplot of one row and two columns\n# pass the index of the plot as the third parameter of subplot()\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_coefficients(linreg_model_backforward, 'Linear Regression (OLS)')\n\n# pass the index of the plot as the third parameter of subplot()\nplt.subplot(1,2,2)\nplot_coefficients1(linreg_with_SGD, 'Linear Regression (SGD)')\n\n# to adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()","40892649":"# update the dataframe 'score_card'\nupdate_score_card1('Linear Regression (using SGD)',linreg_with_SGD,X_train3,X_test3,y_train3,y_test3)\n\n# print the dataframe\nscore_card","956c8e36":"# compile the column names of the output dataframe\n# add the names of metrics used for comparing the models\n# these metrics are used for both lasso and ridge regression\n\n# create list of variable names\ncol = list(X_train3.columns)\n\n# add 'Sum of square of Residuals' to the list\ncol.append('ssr')\n\n# add 'R squared' to the list\ncol.append('R squared')\n\n# add 'Adj. R squared' to the list\ncol.append('Adj. R squared')\n\n# add 'Root mean Squared Error' to the list\ncol.append('RMSE')","2d01ce03":"# build a OLS model using OLS()\nridge_regression = sm.OLS(y_train3, X_train3)\n\n# the fitted ridge model\nresults_fu = ridge_regression.fit()","cc1b6536":"# create an empty list\nframes = []\n\n# a 'for' loop for values of alpha in a given range\n# the loop prepares a table of regression coefficients for different values of alpha\n# the values of alpha are given to be 0.0001, 0.1001, 0.2001, 0.3001, 0.4001, 0.5001 and 10.0001\nfor n in np.arange(0.0001,10.1, 0.1).tolist():\n    \n    # fit a rigde regression to the linear model built using OLS\n    # L1_wt=0: conducts ridge regression\n    # alpha: specifies the alpha value\n    # start_params: starting values for patameters\n    results_fr = ridge_regression.fit_regularized(L1_wt=0, alpha=n, start_params=results_fu.params)\n    \n    # fit the model\n    # model: specifies the regression model\n    # params: specifies the estimated parameters\n    # normalized_cov_params: specifies the normalized covariance parameters\n    results_fr_fit = sm.regression.linear_model.OLSResults(model=ridge_regression, \n                                             params=results_fr.params, \n                                             normalized_cov_params=ridge_regression.normalized_cov_params)\n    \n    # compute the rmse\n    # calling the test data\n    results_fr_fit_predictions = results_fr_fit.predict(X_test3)\n    \n    # obtain the rmse\n    mse_test=round(mean_squared_error(y_test3,results_fr_fit_predictions),4)\n    results_fr_fit_rmse=round(np.sqrt(mse_test),4)    \n  \n    # compile the necessary metrics used for comparing the models\n    list_metric = [results_fr_fit.ssr, results_fr_fit.rsquared, results_fr_fit.rsquared_adj, results_fr_fit_rmse]\n    \n    # append the empty list \n    frames.append(np.append(results_fr.params, list_metric))\n    \n    # converting the list to a dataframe.\n    df_params = pd.DataFrame(frames, columns= col)\n\n# add column names to the dataframe  \ndf_params.index=np.arange(0.0001, 10.1, 0.1).tolist()\n\n# add the first column name alpha to the data frame. \n# this column will hold the regularization parameter\u00a0value which can be anything\u00a0from zero to any positive number\ndf_params.index.name = 'alpha*'\n\n# display the output\ndf_params.head()","aea44a45":"# call the first two rows and last two rows\ndf_params.iloc[[0,1,-2,-1]]","c58293fd":"# find the alpha value for which RMSE is minimum\n# from the column EMSE of df_params obtain the minimum value of RMSE\n# .index.tolist(): gets the corresponding index value, i.e. the alpha value\nalpha = df_params.RMSE[df_params.RMSE == df_params.loc[:,'RMSE'].min()].index.tolist()\n\n# print the conclusion\nprint('The required alpha is %.4f ' % (alpha[0]))","d83fa658":"# check for zeros in the above coefficients table\n# axis=1: specifies that the apply() is to be applies column wise\ndf_params.apply(lambda x: sum(x.values==0),axis=1)","8bb68aae":"# build a ridge model for the desired alpha \n# L1_wt=0: conducts ridge regression\n# alpha: specifies the alpha value\n# start_params: starting values for patameters\nresults_fr = ridge_regression.fit_regularized(L1_wt=0, alpha=0.0001, start_params=results_fu.params)\n\n# fit the model \n# model: specifies the regression model\n# params: specifies the estimated parameters\n# normalized_cov_params: specifies the normalized covariance parameters\nridge_regression_best = sm.regression.linear_model.OLSResults(model = ridge_regression, \n                                        params=results_fr.params, \n                                        normalized_cov_params=ridge_regression.normalized_cov_params)\n\n# print the summary output \nprint (ridge_regression_best.summary())","ff8f4936":"# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(ridge_regression_best,X_train3,y_train3))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(ridge_regression_best,X_test3,y_test3))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(ridge_regression_best,X_train3,y_train3) - get_train_rmse(ridge_regression_best,X_test3,y_test3))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","db9b2415":"# define a function to plot a barplot\n# pass the model \ndef plot_coefficients(model, algorithm_name):\n    # create a dataframe of variable names and their corresponding value of coefficients obtained from model\n    # 'columns' returns the column names of the dataframe 'X'\n    # 'coef_' returns the coefficient of each variable\n    df_coeff = pd.DataFrame({'Variable': X_train3.columns, 'Coefficient': model.params})\n\n    # sort the dataframe in descending order\n    # 'sort_values' sorts the column based on the values\n    # 'ascending = False' sorts the values in the descending order\n    sorted_coeff = df_coeff.sort_values('Coefficient', ascending = False)\n\n    # plot a bar plot with Coefficient on the x-axis and Variable names on y-axis\n    # pass the data to the parameter, 'sorted_coeff' to plot the barplot\n    sns.barplot(x = \"Coefficient\", y = \"Variable\", data = sorted_coeff)\n    \n    # add x-axis label\n    # set the size of the text using 'fontsize'\n    plt.xlabel(\"Coefficients from {}\".format(algorithm_name), fontsize = 15)\n\n    # add y-axis label\n    # set the size of the text using 'fontsize'\n    plt.ylabel('Features', fontsize = 15)","4a35e87e":"# subplot() is used to plot the multiple plots as a subplot\n# (1,2) plots a subplot of one row and two columns\n# pass the index of the plot as the third parameter of subplot()\nplt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplot_coefficients(linreg_model_backforward, 'Linear Regression (OLS)')\n\n# pass the index of the plot as the third parameter of subplot()\nplt.subplot(1,2,2)\nplot_coefficients(ridge_regression_best, 'Ridge Regression (alpha = 0.0001)')\n\n# to adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()","53a02c32":"# update the dataframe 'score_card'\nupdate_score_card(xtrain=X_train3,ytrain=y_train3,xtest=X_test3,ytest=y_test3,algorithm_name = 'Ridge Regression', model = ridge_regression_best, alpha = '0.0001', l1_ratio = '0')\n\n# print the datarframe\nscore_card","7b72eb2b":"# I use the scaled data\n# build a OLS model using OLS()\nlasso_regression = sm.OLS(y_train3, X_train3)\n\n# the fitted lasso model\nresults_fu = lasso_regression.fit()","228cc523":"# create an empty list\nframes = []\n\n# a 'for' loop for values of alpha in a given range\n# the loop prepares a table of regression coefficients for different values of alpha\n# the values of alpha are given to be  0.0001, 0.0002 ,0.0003, 0.0004 ,0.0005 to 0.0199\nfor n in np.arange(0.0001, 0.02, 0.0001).tolist():\n    \n    # fit a lasso regression to the linear model built using OLS\n    # L1_wt=1: conducts lasso regression\n    # alpha: specifies the alpha value\n    # start_params: starting values for patameters\n    results_fr = lasso_regression.fit_regularized(L1_wt=1, alpha=n, start_params=results_fu.params)\n     \n    # fit the model \n    # model: specifies the regression model\n    # params: specifies the estimated parameters\n    # normalized_cov_params: specifies the normalized covariance parameters\n    results_fr_fit = sm.regression.linear_model.OLSResults(model=lasso_regression, \n                                                params=results_fr.params, \n                                                normalized_cov_params=lasso_regression.normalized_cov_params)\n    \n    # calculate the rmse\n    # predict the claim on test data\n    results_fr_fit_predictions = results_fr_fit.predict(X_test3)\n    \n    # obtain the rmse\n    # obtain the rmse\n    mse_test=round(mean_squared_error(y_test3,results_fr_fit_predictions),4)\n    results_fr_fit_rmse=round(np.sqrt(mse_test),4)  \n  \n    # compile the necessary metrics used for comparing the models\n    list_metric = [results_fr_fit.ssr, results_fr_fit.rsquared, results_fr_fit.rsquared_adj, results_fr_fit_rmse]\n    \n    # append the empty list \n    frames.append(np.append(results_fr.params, list_metric))\n    \n    # convert the list to a dataframe\n    df_params = pd.DataFrame(frames, columns= col)\n\n# add column names to the dataframe    \ndf_params.index=np.arange(0.0001, 0.02, 0.0001).tolist()\n\n# add the first column name alpha to the data frame\n# this column will hold the regularization parameter\u00a0value which can be anything\u00a0from zero to any positive number\ndf_params.index.name = 'alpha*'\n\n# display output\ndf_params.head()","4bb6f55e":"# call the first two rows and last two rows\ndf_params.iloc[[0,1,-2,-1]]","6438e241":"# find the alpha value for which RMSE is minimum\n# from the column EMSE of df_params obtain the minimum value of RMSE\n# .index.tolist(): gets the corresponding index value, i.e. the alpha value\nalpha = df_params.RMSE[df_params.RMSE == df_params.loc[:,'RMSE'].min()].index.tolist()\n\n# print the conclusion\nprint('The required alpha is %.4f ' % (alpha[0]))","ec9d9ebe":"# check for zeros in the above coefficients table\ndf_params.apply(lambda x: sum(x.values==0),axis=1)#.head()","77df4f8f":"# fit the lasso regression model\n# L1_wt=1: conducts lasso regression\n# alpha: specifies the alpha value\n# start_params: starting values for patameters\nresults_fr = lasso_regression.fit_regularized(L1_wt=1, alpha=0.0001, start_params=results_fu.params)\n\n# fit the lasso regression \n# model: specifies the regression model\n# params: specifies the estimated parameters\n# normalized_cov_params: specifies the normalized covariance parameters\nlasso_regression_best = sm.regression.linear_model.OLSResults(model=lasso_regression, \n                                              params=results_fr.params, \n                                              normalized_cov_params=lasso_regression.normalized_cov_params)\n\n# print the summary output\nprint (lasso_regression_best.summary())","dadd0613":"# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(lasso_regression_best,X_train3,y_train3))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(lasso_regression_best,X_test3,y_test3))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(lasso_regression_best,X_train3,y_train3) - get_train_rmse(lasso_regression_best,X_test3,y_test3))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","35f1bbf1":"# subplot() is used to plot the multiple plots as a subplot\n# (1,2) plots a subplot of one row and two columns\n# pass the index of the plot as the third parameter of subplot()\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_coefficients(linreg_model_backforward, 'Linear Regression (OLS)')\n\n# pass the index of the plot as the third parameter of subplot()\nplt.subplot(1,2,2)\nplot_coefficients(lasso_regression_best, 'Lasso Regression')\n\n# to adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()","6c39f191":"# update the dataframe 'score_card'\nupdate_score_card(xtrain=X_train3,ytrain=y_train3,xtest=X_test3,ytest=y_test3,algorithm_name = 'Lasso Regression', model = lasso_regression_best, alpha = '0.0001', l1_ratio = '1')\n\n# print the datarframe\nscore_card","27a244ee":"# build a OLS model using OLS()\nelastic_net_regression = sm.OLS(y_train3, X_train3)\n\n# the fitted elastic net model\nresults_fu = elastic_net_regression.fit()","ad39203a":"# normalize the data which is required for elastic net\nelastic = ElasticNet(normalize=True)\n\n# use gridsearchCV to find best penalty term \n# estimator: the method used to estimate the parameter of regression\n# param_grid: dictionary with parameters names (string) as keys and lists of parameter settings to try as values\n# scoring: a single string or a callable to evaluate the predictions on the test set\n# n_jobs: number of jobs to run in parallel\n# refit: refit an estimator using the best found parameters on the whole dataset\n# CV: determines the cross-validation splitting strategy\nsearch = GridSearchCV(estimator=elastic, \n                      param_grid={'l1_ratio':[0.0001, 0.0002, 0.001, 0.01,0.1,.2,.4,.6,.8]},\n                      scoring='neg_mean_squared_error', \n                      n_jobs=1, \n                      refit=True, \n                      cv=10)","87c69390":"# fit the model to get best parameter\nsearch.fit(X_train3, y_train3)\n\n# get best parameter\nsearch.best_params_","7272711a":"# create an empty list\nframes = []\n\n# a 'for' loop for values of alpha in a given range\n# the loop prepares a table of regression coefficients for different values of alpha\n# the values of alpha are given to be 0.0001, 0.0101, 0.0201 and 1.9901\nfor n in np.arange(0.0001, 1.5, 0.01).tolist():\n    \n    # fitting a elastic net regression to the elastic net model built using OLS \n    # the l1_ratio is the same as l1_wt in the below function\n    # L1_wt=0.8: conducts elastic net regression\n    # alpha: specifies the alpha value\n    # start_params: starting values for patameters\n    results_fr = elastic_net_regression.fit_regularized(method='elastic_net', \n                                                        L1_wt= 0.0001, \n                                                        alpha=n, \n                                                        start_params=results_fu.params)\n     \n    # obtaining the parameters of the fitted model \n    # model: specifies the regression model\n    # params: specifies the estimated parameters\n    # normalized_cov_params: specifies the normalized covariance parameters\n    results_fr_fit = sm.regression.linear_model.OLSResults(model=elastic_net_regression, \n                                    params=results_fr.params, \n                                    normalized_cov_params=elastic_net_regression.normalized_cov_params)\n    \n    # calculate rmse\n    # call the test data\n    results_fr_fit_predictions = results_fr_fit.predict(X_test3)\n    \n    # obtain the rmse\n    mse_test=round(mean_squared_error(y_test3,results_fr_fit_predictions),4)\n    results_fr_fit_rmse=round(np.sqrt(mse_test),4)     \n  \n    # compiling the necessary metrics used for comparing the models\n    list_metric = [results_fr_fit.ssr, results_fr_fit.rsquared, results_fr_fit.rsquared_adj, results_fr_fit_rmse]\n    \n    # appending the empty list \n    frames.append(np.append(results_fr.params, list_metric))\n    \n    # converting the list to a dataframe\n    df_params = pd.DataFrame(frames, columns= col)\n\n# add column names to the dataframe    \ndf_params.index=np.arange(0.0001, 1.5, 0.01).tolist()\n\n# add the first column name alpha to the data frame\n# this column will hold the regularization parameter\u00a0value which can be anything\u00a0from zero to any positive number\ndf_params.index.name = 'alpha*'\n\n# display output\ndf_params.head()","bffd5188":"# to call the first two rows and last two rows\ndf_params.iloc[[0,1,-2,-1]]","86c035d0":"# find the alpha value for which RMSE is minimum\n# from the column EMSE of df_params obtain the minimum value of RMSE\n# .index.tolist(): gets the corresponding index value, i.e. the alpha value\nalpha = df_params.RMSE[df_params.RMSE == df_params.loc[:,'RMSE'].min()].index.tolist()\n\n# print the conclusion\nprint('The required alpha is %.4f ' % (alpha[0]))","f5664f1e":"# check for zeros in the above coefficients table\ndf_params.apply(lambda x: sum(x.values==0),axis=1)","9234de33":"# fit the elastic net regression model\nresults_fr = elastic_net_regression.fit_regularized(method='elastic_net', \n                                                    L1_wt=0.0001, \n                                                    alpha= 0.0001, \n                                                    start_params=results_fu.params)\n\n# fit elastic net regression\n# model: specifies the regression model\n# params: specifies the estimated parameters\n# normalized_cov_params: specifies the normalized covariance parameters\nelastic_net_regression_best = sm.regression.linear_model.OLSResults(model=elastic_net_regression, \n                                              params=results_fr.params, \n                                              normalized_cov_params=elastic_net_regression.normalized_cov_params)\n\n# print the summary output\nprint (elastic_net_regression_best.summary())","2a8dd77c":"# print training RMSE\nprint('RMSE on train set: ', get_train_rmse(elastic_net_regression_best,X_train3,y_train3))\n\n# print training RMSE\nprint('RMSE on test set: ', get_test_rmse(elastic_net_regression_best,X_test3,y_test3))\n\n# calculate the difference betIen train and test set RMSE\ndifference = abs(get_test_rmse(elastic_net_regression_best,X_train3,y_train3) - get_train_rmse(elastic_net_regression_best,X_test3,y_test3))\n\n# print the difference betIen train and test set RMSE\nprint('Difference betIen RMSE on train and test set: ', difference)","5f621de3":"# subplot() is used to plot the multiple plots as a subplot\n# (1,2) plots a subplot of one row and two columns\n# pass the index of the plot as the third parameter of subplot()\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_coefficients(linreg_model_backforward, 'Linear Regression (OLS)')\n\n# pass the index of the plot as the third parameter of subplot()\nplt.subplot(1,2,2)\nplot_coefficients(elastic_net_regression_best, 'Elastic Net Regression')\n\n# to adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()","39097abe":"# update the dataframe 'score_card'\nupdate_score_card(xtrain=X_train3,ytrain=y_train3,xtest=X_test3,ytest=y_test3,algorithm_name = 'Elastic Net Regression', model = elastic_net_regression_best, alpha = '0.0001', l1_ratio = '0.0001')\n\n# print the datarframe\nscore_card","5ad53aca":"# view the result table\nscore_card","25518c8a":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Linear regression model using Feature Scaling is too simple.<br>\n                        Linear regression with significant variables and model using backward elimination are too complex.<br>\n                        But Linear regression model using backward elimination have less complexity compared to Linear regression model with significant variables. <br>\n                        \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","ad4dc7ba":"<a id='categorical'><\/a>\n### 4.5 Analyze Categorical Variables\n\nCategorical variables are those in which the values are labeled categories. The values, distribution, and dispersion of categorical variables are best understood with bar plots.","8983b5a7":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>The sum of squares of residual, increases with increase in alpha, this reduces model complexity. Compare the coefficients in the first to the second row of this table, there is a drastic change in the magnitude of coefficients. Similar change is seen on comparing rows 2 and 3. HoIver, there is not much change in rows 3 and 4. High alpha values can lead to significant underfitting. Note the rapid increase in residual sum of squares<br><br>\n                                         \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n\n\n\n","96d243d6":"**3. Concatenate numerical and label encoded categorical variables**","d485af93":"<a id='import_lib'><\/a>\n## 1. Import Libraries","ea2d64fd":"<a id='linearreg'><\/a>\n### 5.3 Linear Regression using Feature scaling","d43fc53a":"In this section I build a full model with linear regression using OLS (Ordinary Least Square) technique. By full model I indicate that I consider all the independent variables that are present in the dataset.","27ba249e":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/cdn1.iconfinder.com\/data\/icons\/e-commerce-31\/64\/website-analysis-web-rate-chart-512.png\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to get the count of missing values in each column, I use the in-built function .isnull().sum()\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","8722160c":"### K-Fold Cross Validation using `cross_val_score()`","663e028d":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"14%\">\n            <img src=\"https:\/\/cdn4.iconfinder.com\/data\/icons\/education-business-part-1\/513\/18-512.png\">\n        <\/td>\n        <td>\n            <div align=\"center\">\n                <font color=\"purple\" size=50%>\n                    <b>GRADUATE ADMISSION PREDICTION\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","b2f7eabd":"I may say the model obtained by alpha = 0.0001 is performing the best.","f6b0357b":"<a id='set_options'><\/a>\n## 2. Set Options","8e959eb8":"The Variance Inflation Factor (VIF) is used to detect the presence of multicollinearity betIen the features. The value of VIF equal to 1 indicates that no features are correlated. I calculate VIF of the numerical independent variables.   ","5fa5dd73":"**In ridge regression, coefficients may tend to zero yet are never zero. This can be checked by the follwing code**","f3d8c6de":"When the l1_ratio is set to 0 it is the same as ridge regression. When l1_ratio is set to 1 it is lasso. Elastic net is somewhere betIen 0 and 1 when setting the l1_ratio. Therefore, in our grid, I need to set several values of this argument. Below is the code.","f65c57bf":"<a id='Assumptions'><\/a>\n### 5.4 Check the Assumptions of the Linear Regression\n\nNow I perform test for checking presence of Autocorrelation and Heteroskedasticity.","5906298a":"<a id='correlation'><\/a>\n### 4.7 Correlation","90b7bc7c":"Let us compare the results for low and high values of alpha","d0ae0de5":"**7. Tabulate the results**","afdee162":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Let\u2019s start by creating box-and-whisker plots with seaborn\u2019s boxplot method:\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","4e01dbe3":"<a id='Data_Shape'><\/a>\n### 4.1. Data Dimension","b7278d17":"# Visualize Missing Values using Heatmap","87f126bb":"**6. Graph Representation**","99e67703":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Rsquare values for each fold are not that far away from each other so the model is able to generalize Ill<br>\n                        \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","60f10a63":"**1. Split the data in the form of train and test sets**","f4bf4549":"**3. Create a generalized function to create a dataframe containing the scores from all the models**","8c47e34c":"**1. Check data types**","2dbf3348":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>From the plots I see that none of the plots show a specific pattern. Hence, I may conclude that the variables are linearly related to the dependent variable.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","daad1f5a":"**4. Find the alpha for which RMSE is minimum**\n\nNow to know which model performs the best, I find the alpha value which has loIst root mean squared error.","9113bcb2":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.oOKPCkpApkNeJ-JdTl_R2wHaHa?w=207&h=206&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> In our dataset I have numerical and categorical variables. Now I check for summary statistics of all the variables<br>\n                        1. For numerical variables, I use .describe()<br>\n                        2. For categorical features, I use .describe(include=object) <br>\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","ef888c51":"**3.Adding to Result table**","465ce53d":"<a id='data_preparation'><\/a>\n## 4. Data Analysis and Preparation\n","20a9a008":"**1. Plot boxplot for numerical data**","a65357de":"**6. Compute model accuracy measures**\n\nNow I calculate accuray measures like Root-mean-square-error (RMSE), R-squared and Adjusted R-squared.","2320baf9":"<table align=\"left\">\n    <tr>\n        <td width=\"15%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.acy-BAOZJnWwVbs2n-75qAHaG6?w=204&h=191&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Data preparation is the process of cleaning and transforming raw data prior to building predictive models. <br><br>\n                        Here I will analyze and prepare data to perform regression analysis:<br>\n                        1. Check dimensions of the dataframe in terms of rows and columns <Br>\n                        2. Check the data types. Refer data definition to ensure your data types are correct  <br>\n                        3. If data types are not as per business context, change the data types as per requirement <br>\n                        4. Study summary statistics<br>\n                        5. Check for missing values<br>\n                        6. Study correlation<br>\n                        7. Analyze categorical variables<br>\n                        8. Analyze relationship betIen target variable and categorical variables<br>\n                        9. Perform feature engineering<br>\n                       10. Detect outliers<br>\n                       11. Recheck the correlation<br><br>\n                        Note: It is an art to explore data and one will need more and more practice to gain expertise in this area.\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","8fc43a88":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>To build the linear regression model using significant variable, I do the following: <br>\n                       1.find insignificant variables from pca model<br> \n                       2. Split the data into training and test sets<br>\n                       3. Consider only the significant variables<br>\n                       4. Build model using sm.OLS().fit()<br>\n                       <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n","d1e8e367":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Now I will perform regularization to check whether this technique performs better than our linear regression models without regularization<\/b>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n","797f5348":"**7 Tabulate the results**","0961c70c":"**1. Filter numerical and categorical variables**","1b6504b2":"**3. Build model using sm.OLS().fit()**","faf23875":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Notice that RMSE, R-squared and Adjusted R-squared values are the slightly different for ridge and lasso regression.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","30daec1a":"<a id='Heteroskedasticity'><\/a>\n### 5.4.2 Detecting Heteroskedasticity\nHeteroskedasticity is a systematic change in the spread of the residuals over the range of measured values. One of the assumptions of the linear regression is that there should not be Heteroskedasticity.\n\nBreusch-Pagan is the test for detecting heteroskedasticity:\n\nThe null and alternate hypothesis of Breusch-Pagan test is as follows:<BR>\n    \n    H0: The residuals are homoskedastic\n    H1: The residuals are not homoskedastic","5fef39aa":"**1. Fit a regression model using OLS method**","05490756":"**1. Filter out only the numeric variables from the dataframe**","570ac3d9":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Now I perform ridge regression. I start with our original data set gradually proceeding with our analysis<br><br>\n                        In order to perform ridge regression, I do the following:<br>\n                        1. Split the data in the form of train and test sets <br>\n                        2. For different values of alpha create a tabular representation of parameter estimates and accuracy meterics<br>\n                        3. Find the alpha for which RMSE is minimum<br>\n                        4. Fit a ridge model by substituting the alpha value obtained in step 3<br>\n                        5. Compute accuracy measures <br>\n                        6. Graph Representation <br>\n                        7. Tabulate the results <br>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","d47a920f":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Notice that R-squared and Adjusted R-squared values have decreased and there is no change in RMSE value.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","99d498e5":"Now, I will do MinMax scaling for our data","7121bc53":"<a id='Read_Data'><\/a>\n## 3. Read Data","03bba9ba":"**4. Plot boxplot to recheck for outliers**","20f80962":"**Analysis of relationship betIen TOEFL score and research**","8911e0e6":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Most of the variables are positively correlated. The target variable.i.e. Chance of Admit have strong correlation with all numeric variables.<br><br>\n                       <br><\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","4b53065e":"**Let us import the required libraries and functions**","16c0804e":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.PXpHr3duxoPzLQOYeEbEewHaHa?w=195&h=195&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Condition Number : One way to assess multicollinearity is to compute the condition number(CN). If CN is less than 100, there is no multicollinearity. If CN is betIen 100 and 1000, there is moderate multicollinearity and if CN is greater 1000 there is severe multicollinearity in the data <br><br>\n                        Durbin-Watson : The Durbin-Watson statistic will always have a value betIen 0 and 4. A value of 2.0 means that there is no autocorrelation detected in the sample. Values from 0 to less than 2 indicate positive autocorrelation and values from from 2 to 4 indicate negative autocorrelation      \n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","35c9a247":"**1. For numerical variables, I use .describe()**","43ce4ca9":"For all alpha values, the corresponding value is zero which impiles there are **no** coefficients with value zero.","94b1da31":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.PXpHr3duxoPzLQOYeEbEewHaHa?w=195&h=195&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>So, I can expect there may be a strong multicollinearity betIen the independent variables.<\/b> <br><br>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","7fba5452":"**8. Tabulate the results**","af10771a":"**1. Find insignifcant variables**","a0c69746":"<a id='Summary_Statistics'><\/a>\n### 4.4 Summary Statistics","01a9c8f9":"**1. Using Variance inflation Factor**","09e6d3ea":"<a id='model_perf'><\/a>\n## 6. Model Performance","06bf35ce":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to check the model performance, I do the following: <br>\n                       1. Create a generalized function to calculate the RMSE for train and test set.<br>\n                       2. Create a generalized function to calculate the R-Squared and Adjusted R- Squared<br>\n                       3. Create a generalized function to create a dataframe containing the scores from all the models<br>\n                      \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","085eea72":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>From the above plot, I notice that for the variable 'LOR','CGPA' and 'Chance of Admit' have outliers<br>\n\nLet us use the IQR method to remove the outliers<\/br><\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n","380b917f":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Now I perform Stochastic Gradient Descent. I start with our original data set gradually proceeding with our analysis<br><br>\n                        In order to perform Stochastic Gradient Descent, I do the following:<br>\n                        1. Fit the linear regression using the SGD <br>\n                        2. Create a generalized function to plot a barchart for the coefficients <br>\n                        3. Adding to Result table<br>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","b3c01c15":"**Analysis of SOP score based on research**","f64f5993":"<a id='Model_val'><\/a>\n## 7.1 Model Validation(Cross Validation)\n","3112313c":"**Analyze the relationship betIen University Rating and CGPA**","27555f89":"<a id=\"kfold\"><\/a>\n### 7.1.1 K-Fold Cross Validation\nK-Fold cross validation is where a given dataset is divided into `K` folds where at some stage each fold is used as a test set.","28732bc4":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>For all alpha values, the corresponding value is zero which implies there are no coefficients with value zero. This happened since the number of variables considered in the data are less. Also only significant variables are considered. HoIver, if I had considered a dataset with many variables (both significant and insignificant) then lasso regression and ridge regression would have shown significant performance.\n               \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","c8ef9051":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>The sum of squares of residual, increases with increase in alpha, this reduces model complexity.\nCompare the coefficients in the first and second row of this table, there is a drastic change in the magnitude of coefficients. Similar change is seen on comparing rows 2 and 3. HoIver, there is not much change in rows 3 and 4. High alpha values can lead to significant underfitting. Note the rapid increase in residual sum of squares.<br><br>\n                      \n                   \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","1f8a5e53":"**1. Find out most significant variables using backward elimination**","62576181":"I see the dataframe has 8 columns and 400 observations.","13c8de0c":"## Problem Statement\n\nStudents are often worried about their chances of admission in graduate school. The aim of this machine learning model is to help students in shortlisting universities with their profiles. The predicted output gives them a fair idea about their admission chances in a particular university. This will assist students to know in advance if they have a chance to get accepted.","01492978":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Notice that RMSE, R-squared and Adjusted R-squared values are very close for the ridge, lasso, and elastic net regression.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","f6634bc3":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>There is lot variation in the coefficients of linear regression model using SGD and coefficients of linear regression model.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","5048845a":"**2.  Call the corr() function which will return the correlation matrix of numeric variables**","381f92a8":"**2. Use GridsearchCV to find the best penalty term**","9992342a":"**5. Fit a elastic net model by substituting the alpha value obtained in step 4**","bf2b3c1c":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79% of the variation in dependent variable chance of admit. The Durbin-Watson test statistics is 1.778 and indicates that there is no autocorrelation. The Condition Number is 17.9 suggests that there is no collinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","5bfe895b":"I may say the model obtained by alpha = 0.0001 is performing the best.","97e5c556":"There is a single categorical variables. From the output I see that the variable 'Research' has 2 unique categories. \n\n","a260883c":"The above two graphs make it clear that higher the Scores better the Chance of admit","267b862c":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.PXpHr3duxoPzLQOYeEbEewHaHa?w=195&h=195&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>I can also find out the p-value associated with the Durbin-Watson statistic from Durbin_Watson table in order to reject or fail to reject the null hypothesis. Reference URL: <a href=\"https:\/\/bit.ly\/36DJDR8\">Durbin-Watson Table<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n","480f9c22":"<a id=\"loocv\"><\/a>\n### 7.1.2 Leave One Out Cross Validation (LOOCV)\nIt is a process in which the model is trained on the training dataset, with the exception of only one data point, which is used to test the model. This method increases the variance, as only one data point is used for testing. LOOCV can be time consuming as k is equal to the number of observations.\n\nIn LOOCV, the estimates from each fold are highly correlated and their average can have a high level of variance.","894e63b4":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Most of the students who has done research are those with good TOEFL score\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","7a16d800":"**3. Pass the correlation matrix to the heatmap() function of the seaborn library to plot the heatmap of the correlation matrix**","6551ad3f":"**2. Label encode the catergorical variables**","6e691a83":"#### Importance of detecting an outlier\nAn outlier is an observation that appears to deviate distinctly from other observations in the data. If the outliers are not removed, the model accuracy may decrease.\n\n**Recollect that one of the assumptions of Linear Regression is there should be no outliers present in the data**","8f127428":"In a for loop I pass different values of alpha. I tabulate all beta coeffients.","b2708d02":"<a id='Autocorrelation'><\/a>\n### 5.4.1 Detecting Autocorrelation\n\nAutocorrelation (also called serial correlation) refers to the degree of correlation betIen the values of the same variables across different observations in the data. One of the assumptions of the linear regression is that there should not be autocorrelation.\n\nThe Durbin Watson Test is a measure of autocorrelation in residuals from the regression analysis.\nAutocorrelation being present typically indicates that I are missing some information that should be captured by the model.\n\nThe null and alternate hypothesis of Durbin-Watson test is as follows: <br>\n        \n          H0: There is no autocorrelation in the residuals\n          H1: There is autocorrelation in the residuals\n\n**The test statistic is approximately equal to 2*(1-r) where r is the sample autocorrelation of the residuals. Thus, for r == 0, indicating no serial correlation, the test statistic equals 2. This statistic will always be betIen 0 and 4. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4, the more evidence for negative serial correlation**\n\n**From the summary output in section 5.3, I see that the Durbin-Watson static is  2.027**\n\n**Hence I can conclude that there is no autocorrelation**","38f3aab3":"<a id='Interaction'><\/a>\n### 5.6 Linear Regression using backward elimination\n","e4aa187e":"<a id='RemovingInsignificantVariable_scaleddata'><\/a>\n### 5.5 Linear Regression with significant Variable (OLS) ","c6baba63":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Apart from the expected inference of higher RMSE for higher alphas, I can see the many of the coefficients are zero even for very small values of alpha.<br>\n                    For all alpha values, the corresponding value is the number of coefficients which implies there are coefficients with value zero.\n               \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","e64a793f":"<a id='Data_Types'><\/a>\n### 4.2 Data Types\nData has a variety of data types. The main types stored in pandas dataframes are object, float, int64, bool and datetime64. In order to learn about each attribute, it is always good for us to know the data type of each column.","ef0b0932":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"solution.png\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>The students who have done research have good LOR score\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","8b60d3e5":"<table align='left'>\n    <tr>\n        <td width='8%'>\n            <img src='https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7'>\n        <\/td>\n        <td>\n            <div align='left', style='font-size:120%'>\n                <font color='#21618C'>\n                    <b>From the above output, I see that data types are as per the data definition. Now I can proceed with the analysis.<br><\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n\n\n","103b964e":"**The mean of the residuals always equals zero (assuming that your line is actually the line of \u201cbest fit\u201d)** ","823fe573":"**Analysis of LOR score based on research**","b6ca57b3":"**1. Split the data into training and test sets**","65f50003":"<table align=\"left\">\n    <tr>\n        <td width=\"12%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/R.0103a22d9cb9925546885e7a78407adf?rik=vDpxeHWfNfSDcA&riu=http%3a%2f%2f2.bp.blogspot.com%2f_rO4CAIA4y4A%2fRlsM85uFgQI%2fAAAAAAAAAn8%2fffCWeevI2h0%2fw1200-h630-p-nu%2foutlier.jpg&ehk=9ag2ovXEprCtga%2foRdoQFy75QHyv2ojKMJ2xMitqUz8%3d&risl=&pid=ImgRaw\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to detect outliers in numerical data, I perform the following:<br>\n                    1. Plot boxplot for numerical data<br>\n                    2. Note the variables for which outliers are present<br>\n                    3. Remove outliers by IQR method<br> \n                    4. Plot boxplot to recheck for outliers<\/b> <br><br>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","8549bde4":"**3. Find the alpha for which RMSE is minimum**\n\nNow to know which model performs the best, I find the alpha value which has loIst root mean squared error","fb13c732":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>So that the above output shows there is a reduction in the number of rows(from 400 to 396). Now, to confirm that the outliers have been removed let us visualize using a boxplot.<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","b74e2e60":"**Analysis of relationship betIen GRE score and research**","51c96955":"**2. Split the data into training and test sets**","5e689b96":"**2.Create a generalized function to calculate the R-Squared and Adjusted R- Squared**","51b80387":"**5. Compute model accuracy measures**\n\nNow I calculate accuray measures like Root-mean-square-error (RMSE), R-squared and Adjusted R-squared.","5aac651f":"<a id='LinearRegression'><\/a>\n## 5. Linear Regression (OLS)","ad9309d5":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"solution.png\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>The students who have done research have good SOP score.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","d775a09e":"**In lasso regression, coefficients may be zero. This can be checked by the following code**","6710567f":"Thus, I may say the model obtained by alpha = 0.0001 is performing the best since has the loIst root mean squared error.","a7761603":"First let us print the summary statistics of the categorical features.","ce58559c":"**6. Graph representation**","e77de49a":"<a id='model_opt'><\/a>\n## 7. Model Optimization ","44d318ce":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n<b>The above output illustrates the summary statistics of all the numeric variables like the mean, median(50%), minimum, and maximum values, along with the standard deviation.<br>\n    Note, the average GRE Score is 316.8,average TOEFL SCore is 107.4 and average CGPA is 8.59.<br><br>\n    If I observe the count of all the variables, there is equal count for all the variables. So I can say that there are no missing values in these variables. <\/b>     <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","f289a3e6":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>I observe that p-value is less than 0.05 and thus reject the null hypothesis. I conclude that there is heteroskedasticity present in the data. In real life it might not be possible to meet all the assumptions of linear regression.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","8f9261df":"<a id='Ridge_Regression'><\/a>\n### 7.3.2 Lasso Regression (OLS)\nLasso regression shrinks the less important variable's coefficient to zero. Thus, removing some variables completely. So, this works Ill for feature selection in case I have a large number of features.\nFor lasso regression, I follow a very similar process to ridge regression.","88eea9fa":"<a id='Ridge_Regression'><\/a>\n### 7.3.1 Ridge Regression (OLS)","ff98d342":"**2. Note the variables for which outliers are present**","59f7c2cd":" I have the dataset with no missing values.","520caa82":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79% of the variation in dependent variable chance of admission. The Durbin-Watson test statistics is  1.778 which indicates that there is no autocorrelation. The Condition Number is 17.9 suggests that there is no collinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","73d8eb76":"## Table of Contents\n\n1. **[Import Libraries](#import_lib)**\n2. **[Set Options](#set_options)**\n3. **[Read Data](#Read_Data)**\n4. **[Data Preprocessing](#data_preparation)**\n    - 4.1 - [Data Dimension](#Data_Shape)\n    - 4.2 - [Data Types](#Data_Types)\n    - 4.3 - [Missing Values](#Missing_Values)\n    - 4.4 - [Summary Statistics](#Summary_Statistics)\n    - 4.5 - [Analyze Categorical Variables](#categorical)\n    - 4.6 - [Discover Outliers](#outliers)\n    - 4.7 - [Correlation](#correlation)\n    - 4.8 - [Analyze Relationships BetIen Variables](#categorical_numerical)\n    - 4.9 - [Dummy Encoding of Categorical Variables](#dummy)\n5. **[Linear Regression (OLS)](#LinearRegression)**\n    - 5.1 - [Multiple Linear Regression - Full Model(OLS)](#withLog)\n    - 5.2 - [Dealing with Multicollinearity](#multideal)\n    - 5.3 - [Linear Regression Using Feature Scaling ](#linearreg)\n    - 5.4 - [Check the Assumptions of Linear Regression](#Assumptions)\n         - 5.4.1 - [Detecting Autocorrelation](#Autocorrelation)\n         - 5.4.2 - [Detecting Heteroskedasticity](#Heteroskedasticity)\n         - 5.4.3 - [Linearity of Residuals](#Linearity_of_Residuals)\n         - 5.4.4 - [Normality of Residuals](#Normality_of_Residuals)\n    - 5.5 - [Linear Regression With Significant Variable (OLS)](#RemovingInsignificantVariable_scaleddata)\n    - 5.6 - [Linear Regression Using Backward Elimination](#Interaction)\n6. **[Model Performance](#model_perf)**  \n7. **[Model Optimization](#model_opt)** \n    - 7.1 - [Model Validation (Cross Validation)](#Model_val)\n         - 7.1.1 - [K-Fold Cross Validation](#kfold)\n         - 7.1.2 - [Leave One Out Cross Validation (LOOCV)](#LOOCV)\n    - 7.2 - [Stochastic Gradient Descent - SGD](#sgd)\n    - 7.3   [Regularization (OLS)](#regularization)\n         - 7.3.1 - [Ridge Regression Model (OLS)](#Ridge_Regression)\n         - 7.3.2 - [Lasso Regression Model (OLS)](#Lasso_Regression)\n         - 7.3.3 - [Elastic Net Regression Model (OLS)](#Elastic_Net)\n8. **[Conclusion](#rmse_and_r-squared)**    ","ac15e0a6":"**3. Find the alpha for which RMSE is minimum**\n\nNow, to know which model performs the best, I find the alpha value which has loIst root mean squared error.","618571b2":"To build linear regression models I use OLS method. As seen before in section 4.1.2, it is seen that some variables are categorical. The OLS method fails to perform in presence of categorical variables. To overcome this I use label encoding.","11092d2c":"**2. For categorical features, I use .describe(include=object)**","46f7a3b3":"The mean of the residuals is very much closer to zero. Therefore, I can say that linearity is present.","3d56a507":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Most of the students who has done research are those with good GRE score.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","8f6819f8":"**4. Fit a lasso model by substituting the alpha value obtained in step 3**","9774ea92":"**2. Create a generalized function to plot a barchart for the coefficients**","c3db3db2":"**2. Build model using sm.OLS().fit()**","d3249de1":"<a id='withLog'><\/a>\n### 5.1 Multiple Linear Regression - Full Model (OLS)","c57f0c48":"**2. For different values of alpha create a tabular representation of parameter estimates and accuracy meterics**","c80ca5c8":"## Data Definition\n\n**GRE Score:** Graduate Record Exam (GRE) score. The score will be out of 340 points (numeric)\n\n**TOEFL Score:** Test of English as a Foreigner Language2 (TOEFL) score, which will be out of 120 points (numeric)\n\n**University Rating:** University Rating (Uni.Rating) that indicates the Bachelor University ranking among the other\n                       universities. The score will be out of 5 (numeric)\n\n**SOP:** Statement of purpose (SOP) which is a document written to show the candidate's life, ambitious and the motivations for\n         the chosen degree\/ university. The score will be out of 5 points (numeric)\n\n**LOR:** Letter of Recommendation Strength (LOR) which verifies the candidate professional experience, builds credibility,\n         boosts confidence and ensures your competency. The score is out of 5 points (numeric) \n\n**CGPA:** Undergraduate GPA (CGPA) out of 10 (numeric)\n\n**Research:** Research Experience that can support the application, such as publishing research papers in conferences, working               as research assistant with university professor (either yes or no) (categorical)\n\n**Chance of Admit:** One dependent variable can be predicted which is chance of admission, that is according to the input given                      will be ranging from 0 to 1 (numeric).","9bd1bb3f":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> The output shows all the variables have the highest VIF.i.e. all Features have VIF_Factor >10.<br><br>\n                        But I cannot drop this columns because this are the main attributes to predict the chance of admit         \n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","92e3b936":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> R_squared value is increased. Difference of RMSE is greater compare to linear regression model using backward elimination.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","2fd1a7d5":"<a id='Missing_Values'><\/a>\n### 4.3 Missing Values","0299a407":"<a id=\"sgd\"><\/a>\n### 7.2 Stochastic Gradient Descent - SGD","1176a02b":"**Analyze the relationship betIen GRE score and Chance of Admit**","889a3625":"<a id='dummy'><\/a>\n### 4.9 Dummy Encoding of Categorical Variables","e348889e":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to deal with multicollinearity, I do the following: <br>\n                       1. Using Variance inflation Factor<br>\n                       2. Feature Scaling<br>\n                       \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","d704e80c":"Observing the range of the boxplot, I say that the outliers are removed from the original data. The new 'outliers' that you see are moderate outliers that lie within the min\/max range before removing the actual outliers","f0915654":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79.1% of the variation in dependent variable chance of admit.The Durbin-Watson test statistics is  1.766 and indicates that the is no autocorrelation. The Condition Number is 17.6 suggests that there is no collinearity\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","6196737c":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"corr1.png\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to compute the correlation matrix, I perform the following:<br>\n                    1. Filter out only the numeric variables from the dataframe using select_dtypes() function<br>\n                    2. Call the corr() function which will return the correlation matrix of numeric variables <br>\n                    3. Pass the correlation matrix to the heatmap() function of the seaborn library to plot the heatmap of the correlation matrix\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","216ac27f":"<ul>\n    <li>Correlation is the extent of linear relationship among numeric variables<\/li>\n    <li>It indicates the extent to which two variables increase or decrease in parallel<\/li>\n    <li>The value of a correlation coefficient ranges betIen -1 and 1<\/li>\n    <li> Correlation among multiple variables can be represented in the form of a matrix. This allows us to see which pairs are correlated<\/li>\n    <\/ul>\n    ","21bac9f4":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to build the model, I do the following: <br>\n                       1. Find out most significant variables using backward elimination<br>\n                       2. Split the data into training and test sets<br>\n                       3. Build model using sm.OLS().fit()<br>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","276edf5b":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.Hbi99VKJepDAXpcppAOpXgHaF7?w=223&h=180&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> There is barely difference betIen the RMSE values of Elastic Net Regression model and Ridge Regression model. So, I conclude both these models are best.  \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","80f13249":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Now I perform lasso regression. I start with our original data set and gradually proceeding with our analysis<br><br>\n                        In order to perform lasso regression, I do the following:<br>\n                        1. Fit a regression model using OLS method <br>\n                        2. For different values of alpha create a tabular representation of parameter estimates and accuracy meterics<br>\n                        3. Find the alpha for which RMSE is minimum<br>\n                        4. Fit a lasso model by substituting the alpha value obtained in step 3<br>\n                        5. Compute accuracy measures <br>\n                        6. Graph Representation <br>\n                        7. Tabulate the results <br>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","889d7e4d":"**3. For different values of alpha create a tabular representation of parameter estimates and accuracy metrics**","5889cff7":"<a id='rmse_and_r-squared'><\/a>\n## 8. Conclusion ","2834122a":"I first create a list of all the variable names and accuracy metrics whose values I want.","dd06fcf9":"**2. Split the data into training and test sets**","2cfcfcd5":"Ratings of university increase with the increase in the CGPA","0ebb8951":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/cdn1.iconfinder.com\/data\/icons\/e-commerce-31\/64\/website-analysis-web-rate-chart-512.png\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> In this section I will: <br>  \n                        1. Check data types<br>\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","77a387b8":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Thus I have obtained the label coded variables. <br><br>\n                        Note: Now the categorical variable is dummy encoded and for each category, a unique number is assigned. \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","1b98538d":"To know the dimensions of the data:","197f9636":"The above result shows that all atributes are not normally distributed","0ed13843":"check whether all the attributes are normally distributed.","cf687c9a":"`cross_val_score()` also does the stratified sampling internally if required","26306cb8":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>In order to build the model, I do the following: <br>\n                       1. Split the data into training and test sets<br>\n                       2. Build model using sm.OLS().fit()<br>\n                       \n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","2f8b0819":"<a id='multideal'><\/a>\n### 5.2 Dealing with the multicollinearity","4a4f2848":"**Analyze the relationship betIen TOEFL score and Chance of Admit**","71667632":"**5. Compute model accuracy measures**\n\nNow I calculate accuray measures Root-mean-square-error (RMSE)","f65bfc9f":"**In elastic net regression, coefficients may be zero. This can be checked by the following code.**","1177d47b":"**2. For different values of alpha create a tabular representation of parameter estimates and accuracy metrics**","da26e143":"The gradient descent method considers all the data points to calculate the values of the parameters at each step. For a very large dataset, this method becomes computationally expensive. To avoid this problem, I use Stochastic Gradient Descent (SGD) which considers a single data point (sample) to perform each iteration. Each sample is randomly selected for performing the iteration.","67d06a95":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Using this plot, I can infer that the residuals do not come from a normal distribution. This is possible since our target variable is not normally distributed.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","0931bfce":"**4. Build model using sm.OLS().fit()**","1af5a5c7":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79% of the variation in dependent variable chance of admit.The Durbin-Watson test statistics is 1.777 and indicates that the is no autocorrelation. The Condition Number is 17.9 suggests that there is no collinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","6780d2c3":"To take the final conclusion, let us recall the result table again","1b61da16":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> Now I perform elastic net regression. I start with our original data set and gradually proceeding with our analysis<br><br>\n                        In order to perform elastic net regression, I do the following:<br>\n                        1. Fit a regression model using OLS method <br>\n                        2. Use GridsearchCV to find the best penalty term<br>\n                        3. For different values of alpha create a tabular representation of parameter estimates and accuracy meterics<br>\n                        4. Find the alpha for which RMSE is minimum<br>\n                        5. Fit a elastic net model by substituting the alpha value obtained in step 4<br>\n                        6. Compute accuracy measures <br>\n                        7. Graph Representation <br>\n                        8. Tabulate the results <br>\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","e2b9b2e4":"Let us compare the results for low and high values of alpha","39f73f85":"**3. Consider only the significant variables**","6c2ad008":"**1. Fit a regression model using OLS method**","83c92cb4":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>It is apparent that the p-value is less than 0.05. So I have enough evidence to reject the null hypothesis. It can be concluded that the residuals is not normally distributed.<br><br>\n                    <\/b> <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n","abbcf0d9":"A simple way to know whether the outliers have been removed or not is to check the dimensions of the data. If the dimensions are reduced that implies outliers are removed","be5b6f08":"I shall use the scaled data. The train test split is alread conducted for the scaled data.","d2e297e4":"The best l1_ratio is 0.0001.","b925d8f8":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79.5% of the variation in dependent variable Chance of Admit.The Durbin-Watson test statistics is 1.734 and indicates that there is no autocorrelation. The Condition Number 1.26e+04 suggests that there is strong multicollinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","396d59e0":"**2. Feature Scaling**","8403d383":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>Read and display data to get insights from the data<br> \n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","9c0bcff2":"**3. Remove outliers by IQR method**","36dc88d3":"**1.Fit the linear regression using the SGD**","b701d2d7":"<a id='outliers'><\/a>\n### 4.6 Discover Outliers","37335d58":"**1. Create a generalized function to calculate the RMSE for train and test set.**","35ee999e":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>The summary statistics for categorical variables can be interpreted as:<br><br>\n       count - Total number of observations<br>\n       unique - Number of unique classes in a variable<br>\n       top - The most repeated class<br>\n       freq - Frequency of most repeated class<br><br>\n       <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","79f2afa3":"**7. Graphs Representation**","2ddf346e":"**4. Fit a ridge model by substituting the alpha value obtained in step 3**\n\nI know that when aplha = 0.0001, the model performs better than other since it has the loIst RMSE and highest adjusted R- squared value. Let us now find its summary output.","73bc0399":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> This model explains 78.5% of the variation in dependent variable Chance of Admit.The Durbin-Watson test statistics is 2.027 and indicates that there is no autocorrelation. The Condition Number 21.9 suggests that there is no multicollinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","fcf9e9b9":"**Perform Jarque Bera test to check normality of the residuals**","1d10330c":"<a id='Normality_of_Residuals'><\/a>\n### 5.4.4 Normality of Residuals\n\nThe assumption of normality is an important assumption for many statistical tests. The normal Q-Q plot is one way to assess normality. This q-q or quantile-quantile is a scatter plot which helps us validate the assumption of normal distribution in a data set.","c425bcc2":"Fit a linear regression model by the OLS method.","20caec85":"<a id='categorical_numerical'><\/a>\n### 4.8 Analyze Relationship BetIen Variables\nThe box-and-whisker plot is commonly used for visualizing relationships betIen numerical variables and categorical variables.","2b636e0b":"<a id='regularization'><\/a>\n### 7.3  Regularization (OLS)\n\nOverfitting occurs when an algorithm fits the data too Ill.  Specifically, overfitting occurs if the algorithm shows low bias but high variance.  It is often a result of an excessively complicated model. This can be prevented by fitting multiple models and using cross-validation to compare their predictive accuracies on test data.\n\nUnderfitting occurs when an algorithm cannot capture the underlying trend of the data. Intuitively, it occurs when the algorithm does not fit the data Ill enough. Specifically, it occurs if the model or algorithm shows low variance but high bias. It is often a result of an excessively simple model\n\nOne of the challenge in machine learning is that our algorithm must perform Ill on new, previously unseen data - not just those on which our model was trained. This ability to perform Ill on previously unobserved data is called generalization. Generalization error is defined as the expected value of the error on a new data. Regularization is any modification I make to a learning algorithm that is intended to reduce it generalization error but not its training error. (Ref. Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville)","924d0341":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.zxGg6kvoeVuu1sQkdVHZRQHaE8?pid=ImgDet&rs=1\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b> I need to perform dummy encoding on our categorical variables before I proceed, since the method of OLS works only on numeric data <br><br>\n                    In order to label encode, I do the following: <br>\n                    1. Filter numerical and categorical variables<br>\n                    2. Dummy encode the catergorical variables<br>\n                    3. Concatenate numerical and dummy encoded categorical variables<br><\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","eedfd357":"<a id='Linearity_of_Residuals'><\/a>\n### 5.4.3 Linearity of Residuals\n\nMultiple linear regression requires the relationship betIen the independent and dependent variables to be linear, i.e. it should be linear in the parameters. The linearity assumption can best be tested with scatterplots.<br><br>The independent variables must have a linear relationship with the dependent variable.","b1a7f129":"<a id='Ridge_Regression'><\/a>\n### 7.3.3 Elastic net Regression (OLS)\nIn statistics, fitting of linear regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods. The model is trained using both L1 & L2 that allows learning of sparse model where few entries are zero similar to Lasso and also maintaining the regularization properties similar to ridge regression.","cfd65d2e":"<table align=\"left\">\n    <tr>\n        <td width=\"8%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/OIP.DwHGa2C61O_g8ZTTDrz6_gHaJ6?w=139&h=186&c=7&o=5&dpr=1.25&pid=1.7\">\n        <\/td>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>This model explains 79% of the variation in dependent variable chance of admit. The Durbin-Watson test statistics is 1.776 and indicates that there is no autocorrelation. The Condition Number is 17.9 suggests that there is no multicollinearity.\n<\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>"}}