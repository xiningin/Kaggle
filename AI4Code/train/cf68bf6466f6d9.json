{"cell_type":{"eb6bdf3e":"code","9aa4fe04":"code","2149e760":"code","a2033d37":"code","90dd7dff":"code","f1bda853":"code","bbecd010":"code","52de0f93":"code","220269af":"code","424eaf3e":"code","e28219c7":"code","0646d3c1":"code","bce5897c":"code","15dc2b69":"code","1b6517a9":"code","ab8e2c77":"code","55578385":"code","cc0bdeaa":"code","0976b37b":"code","09314336":"markdown","fa4caa39":"markdown","e5deed1a":"markdown","3b32bb45":"markdown"},"source":{"eb6bdf3e":"!pip install -q tensorflow_decision_forests","9aa4fe04":"import pandas as pd\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport keras_tuner as kt\nimport matplotlib.pyplot as plt","2149e760":"# Display settings\nplt.rcParams['figure.figsize'] = 12, 8\nplt.rcParams.update({'font.size': 11})\nplt.style.use('fivethirtyeight')","a2033d37":"# Original data\nTRAIN_PATH = '..\/input\/tabular-playground-series-sep-2021\/train.csv'\nTEST_PATH = '..\/input\/tabular-playground-series-sep-2021\/test.csv'\n\ntrain_data = pd.read_csv(TRAIN_PATH)\ntest_data = pd.read_csv(TEST_PATH)\nprint('Train data shape:', train_data.shape)\nprint('Test data shape:', test_data.shape)","90dd7dff":"def add_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Function adds new features based on missing values\n    (45% correlation with the target) and statistics for each row.\n    :param df: Original DataFrame\n    :return: Updated DataFrame\n    \"\"\"\n    df['n_nans'] = df[features].isnull().sum(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['var'] = df[features].var(axis=1)\n    return df","f1bda853":"# Original features (int and float values of various scale)\nfeatures = [f'f{i}' for i in range(1, 119)]\n\n# Add new features\ntrain_data = train_data.pipe(add_features)\ntest_data = test_data.pipe(add_features)\n\n# Update input features list\nfeatures += ['n_nans', 'std', 'var']\n\ntarget = 'claim'","bbecd010":"# Create TF Dataset from all labeled samples.\nn_samples = len(train_data)\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(\n    train_data[features + [target]], label=target).unbatch()\n\n# This method creates a batched Dataset with batch_size=64.\n# We unbatch it to be able to split into smaller subsets and use other batch size.","52de0f93":"# Small subsets of data to use in quick search for optimal hyperparameters.\ntrain_subset = 100_000  # Number of samples\nvalid_subset = 10_000\n\nbatch_size = 256\n\ntrain_small_ds = train_ds.take(train_subset).batch(batch_size)\nvalid_small_ds = train_ds.skip(train_subset).take(valid_subset).batch(batch_size)","220269af":"# Larger train and validation sets for retraining and evaluating\n# the model with optimal parameters selected.\nn_valid_samples = int(0.1 * n_samples)  # 10% of all train samples\n\nvalid_ds = train_ds.take(n_valid_samples).batch(batch_size)\ntrain_ds = train_ds.skip(n_valid_samples).batch(batch_size)","424eaf3e":"def build_model(hp):\n    \"\"\"Function initializes the model and defines search space.\n    :param hp: Hyperparameters\n    :return: Compiled GradientBoostedTreesModel model\n    \"\"\"\n    model = tfdf.keras.GradientBoostedTreesModel(\n        num_trees=hp.Int('num_trees', min_value=10, max_value=710, step=25),\n        growing_strategy=hp.Choice('growing_strategy', values=['BEST_FIRST_GLOBAL', 'LOCAL']),\n        max_depth=hp.Int('max_depth', min_value=3, max_value=16, step=1),\n        subsample=hp.Float('subsample', min_value=0.1, max_value=0.95, step=0.05),\n        num_threads=4,\n        missing_value_policy='GLOBAL_IMPUTATION')  # Default parameter,\n        # missing values are replaced by the mean or the most frequent value.\n\n    model.compile(metrics=['accuracy', tf.keras.metrics.AUC()])\n    return model","e28219c7":"# Keras tuner\ntuner = kt.BayesianOptimization(  # Or RandomSearch, or Hyperband\n    build_model,\n    objective=kt.Objective('val_auc', direction='max'),  # Or 'val_loss'\n    max_trials=20,\n    project_name='classifier')\n\n# Select the best parameters using a small subset of the train data.\ntuner.search(train_small_ds, epochs=1, validation_data=valid_small_ds)","0646d3c1":"# Display the results\ntuner.results_summary()","bce5897c":"# Best model trained on a small subset of the thain data\n# (could be used for predictions as is).\nbest_model = tuner.get_best_models(num_models=1)[0]","15dc2b69":"# Instantiate untrained model with the best parameters\n# and train on the larger training set.\nbest_hp = tuner.get_best_hyperparameters()[0]\nmodel = tuner.hypermodel.build(best_hp)\n\nhistory = model.fit(train_ds, validation_data=valid_ds,\n                    shuffle=False,\n                    workers=4, use_multiprocessing=True)","1b6517a9":"# Train metrics\ninspect = model.make_inspector()\ninspect.evaluation()","ab8e2c77":"# Visualize training progress\nlogs = inspect.training_logs()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot([log.num_trees for log in logs], \n         [log.evaluation.accuracy for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy (out-of-bag)')\nplt.subplot(1, 2, 2)\nplt.plot([log.num_trees for log in logs], \n         [log.evaluation.loss for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Logloss (out-of-bag)')\nplt.show()","55578385":"# Model accuracy on the validation set\nevaluation = model.evaluate(valid_ds, return_dict=True)\nfor name, value in evaluation.items():\n    print(f'{name}: {value:.4f}')","cc0bdeaa":"# Prediction on the test set\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(\n    test_data[features])\ntest_data['claim'] = model.predict(\n    test_ds, workers=4, use_multiprocessing=True)","0976b37b":"# Save predicted values for the test set\ntest_data[['id', 'claim']].to_csv('submission.csv', index=False)\ntest_data[['id', 'claim']].head()","09314336":"## AutoML with KerasTuner","fa4caa39":"# Auto-Tuning TensorFlow GradientBoostedTreesModel","e5deed1a":"## Data Processing","3b32bb45":"## Feature engineering"}}