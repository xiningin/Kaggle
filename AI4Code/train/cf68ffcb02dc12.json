{"cell_type":{"72363da8":"code","c0bb42f8":"code","05fa04e5":"code","893444af":"code","2bd5c308":"code","93a052b4":"code","a3f4e01d":"code","02c5fbf2":"code","f759431e":"code","29dc9968":"code","c89d50ba":"code","087b1c85":"code","bbafad50":"code","c0f5386e":"code","a98d5798":"code","91ab184c":"code","bdb11586":"code","9ebc3e9e":"code","ccf73ed3":"code","886b39a4":"code","969158a0":"code","657670e5":"code","f531a40b":"code","014bd148":"code","e13c4b79":"code","c91aa1e3":"code","b1d5e7b1":"code","0cb0ad40":"code","863d9a82":"code","98b7c271":"code","9740cf82":"code","34302fe9":"code","3b85078d":"code","84fd3c6b":"code","f40eb265":"code","6cdaa77c":"markdown","f4e74eaf":"markdown","5c9031bf":"markdown","d4a3606a":"markdown","aefdf2ad":"markdown","37f6b268":"markdown","cdd77dc4":"markdown","b1a3cee6":"markdown","f6fa1fa3":"markdown","a7a5bc52":"markdown","7cd81d78":"markdown","aab4097b":"markdown","0db7eb39":"markdown","2a5dfc7e":"markdown","a3b2b0f1":"markdown","07ad558e":"markdown","787c9a76":"markdown","28ac9acd":"markdown","790da4ae":"markdown","0365bfc6":"markdown","19dac56b":"markdown"},"source":{"72363da8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c0bb42f8":"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","05fa04e5":"train = pd.read_csv('..\/input\/Train.csv')\ntest = pd.read_csv('..\/input\/Test.csv')","893444af":"train['source'] = 'train'\ntest['source'] = 'test'\ntest['Item_Outlet_Sales'] = 0\ndata = pd.concat([train, test], sort = False)\nprint(train.shape, test.shape, data.shape)","2bd5c308":"data['Item_Outlet_Sales'].describe()","93a052b4":"sns.distplot(data['Item_Outlet_Sales'])","a3f4e01d":"print('Skewness: %f' % data['Item_Outlet_Sales'].skew())\nprint('Kurtsis: %f' %data['Item_Outlet_Sales'].kurt())","02c5fbf2":"categorial_features = data.select_dtypes(include=[np.object])\ncategorial_features.head(2)","f759431e":"numerical_features = data.select_dtypes(include=[np.number])\nnumerical_features.head(2)","29dc9968":"item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n\nmissing_values = data['Item_Weight'].isnull()\nprint('Missing values: %d' %sum(missing_values))\n\ndata.loc[missing_values,'Item_Weight']  = data.loc[missing_values,'Item_Identifier'].apply(lambda x: item_avg_weight.at[x,'Item_Weight'])\nprint('Missing values after immputation %d' %sum(data['Item_Weight'].isnull()))","c89d50ba":"#Import mode function:\nfrom scipy.stats import mode\n\n#Determing the mode for each\noutlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x.astype('str')).mode[0]))\nprint ('Mode for each Outlet_Type:')\nprint (outlet_size_mode)\n\n#Get a boolean variable specifying missing Item_Weight values\nmissing_values = data['Outlet_Size'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('\\nOrignal #missing: %d'% sum(missing_values))\ndata.loc[missing_values,'Outlet_Size'] = data.loc[missing_values,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\nprint (sum(data['Outlet_Size'].isnull()))","087b1c85":"#Determine average visibility of a product\nvisibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n\n#Impute 0 values with mean visibility of that product:\nmissing_values = (data['Item_Visibility'] == 0)\n\nprint ('Number of 0 values initially: %d'%sum(missing_values))\ndata.loc[missing_values,'Item_Visibility'] = data.loc[missing_values,'Item_Identifier'].apply(lambda x: visibility_avg.at[x, 'Item_Visibility'])\nprint ('Number of 0 values after modification: %d'%sum(data['Item_Visibility'] == 0))","bbafad50":"#Get the first two characters of ID:\ndata['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n#Rename them to more intuitive categories:\ndata['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n                                                             'NC':'Non-Consumable',\n                                                             'DR':'Drinks'})\ndata['Item_Type_Combined'].value_counts()","c0f5386e":"#Change categories of low fat:\nprint('Original Categories:')\nprint(data['Item_Fat_Content'].value_counts())\n\nprint('\\nModified Categories:')\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint(data['Item_Fat_Content'].value_counts())","a98d5798":"df = data.loc[:,['Item_Outlet_Sales']]\ndf.head(2)","91ab184c":"data['Outlet_Years'] = 2009 - data['Outlet_Establishment_Year']\ndata['Outlet_Years'].describe()","bdb11586":"data.index = data['Outlet_Establishment_Year']\ndf = data.loc[:,['Item_Outlet_Sales']]\nts = df['Item_Outlet_Sales']","9ebc3e9e":"temp_data = data.loc[data['Outlet_Establishment_Year'] == 1998]","ccf73ed3":"temp_data['Outlet_Type'].value_counts()","886b39a4":"test_temp_data = test.loc[test['Outlet_Establishment_Year'] == 1998]\ntest_temp_data['Outlet_Type'].value_counts()","969158a0":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\ndata['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\nvar_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet','Item_Identifier']\nle = LabelEncoder()\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","657670e5":"#One Hot Coding:\ndata = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',\n                              'Item_Type_Combined','Outlet'])","f531a40b":"#Drop the columns which have been converted to different types:\ndata.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n# data.to_csv(\"big_mart_sales.csv\")\n# #Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n# #Drop unnecessary columns:\ntest.drop(['source'],axis=1,inplace=True)\ntrain.drop(['source','Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)","014bd148":"from sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression","e13c4b79":"ss = MinMaxScaler()\nfor col in train.columns:\n    train[col] = ss.fit_transform(train[[col]])\ntrain.describe()","c91aa1e3":"plt.figure(figsize=(16,12))\nax = sns.heatmap(train.corr(), square=True, annot=True, fmt='.1f', linecolor='white',cmap='viridis')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","b1d5e7b1":"y = train.Item_Outlet_Sales\nX = train.drop('Item_Outlet_Sales',axis=1)","0cb0ad40":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1,shuffle=True)","863d9a82":"lr = LinearRegression()\nlr.fit(X_train,y_train)","98b7c271":"y_pred = lr.predict(X_test)","9740cf82":"print(\"Linear Regression Score : \",lr.score(X_test,y_test))\nprint(\"R2 Score : \",r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \",mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(y_test,y_pred)))\n","34302fe9":"print(lr.intercept_)\nprint(lr.coef_)","3b85078d":"df = train['Item_MRP']\ny = train.Item_Outlet_Sales\nX = pd.DataFrame(df,columns = ['Item_MRP'])\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1,shuffle=True)\nlr = LinearRegression()\nlr.fit(X_train,y_train)","84fd3c6b":"y_pred = lr.predict(X_test)","f40eb265":"print(\"Linear Regression Score : \",lr.score(X_test,y_test))\nprint(\"R2 Score : \",r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \",mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(y_test,y_pred)))","6cdaa77c":"### Observations :\n>> #### The above correlation matrix shows that Item_Outlet_Sales is highly dependent on Item_MRP with correlation coefficient of 0.6","f4e74eaf":"## NAME : JAYNIL GAGLANI\n## ROLL NO : 13\n## DATA WAREHOUSING AND MINING\n## EXPERIMENT NO 6","5c9031bf":"**Modify categories of Item_Fat_Content**\n\nWe found typos and difference in representation in categories of Item_Fat_Content variable. This can be corrected as:","d4a3606a":"**Data Cleaning and Imputing Missing Values**\n\nWe found two variables with missing values \u2013 Item_Weight and Outlet_Size. Lets impute the former by the average weight of the particular item. This can be done as:","aefdf2ad":"**Numerical and One-Hot Coding of Categorical variables**","37f6b268":"### Evaluation Metrics","cdd77dc4":"**BigMart Sales Prediction practice problem**\n\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). We need to predict the sales for test data set.\n\n\n* Item_Identifier: Unique product ID\n\n* Item_Weight: Weight of product\n\n* Item_Fat_Content: Whether the product is low fat or not\n\n* Item_Visibility: The % of total display area of all products in a store allocated to the particular product\n\n* Item_Type: The category to which the product belongs\n\n* Item_MRP: Maximum Retail Price (list price) of the product\n\n* Outlet_Identifier: Unique store ID\n\n* Outlet_Establishment_Year: The year in which store was established\n\n* Outlet_Size: The size of the store in terms of ground area covered\n\n* Outlet_Location_Type: The type of city in which the store is located\n\n* Outlet_Type: Whether the outlet is just a grocery store or some sort of supermarket\n\n* Item_Outlet_Sales: Sales of the product in the particulat store. This is the outcome variable to be predicted.","b1a3cee6":"**Exploratory Data Analysis**","f6fa1fa3":"### Feature Scaling","a7a5bc52":"**Finding Missing values**","7cd81d78":"### Evaluation Metrics","aab4097b":"### Simple Linear Regression","0db7eb39":"**Create a broad category of Type of Item**\n\nEarlier we saw that the Item_Type variable has 16 categories which might prove to be very useful in analysis. So its a good idea to combine them. One way could be to manually assign a new category to each. But there\u2019s a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. So I\u2019ve used the Item_Identifier variable to create a new column:","2a5dfc7e":"**Modify Item_Visibility**\n\nWe noticed that the minimum value here is 0, which makes no practical sense. Lets consider it like missing information and impute it with mean visibility of that product.","a3b2b0f1":"### Conclusion :\n>> #### The Multiple Linear Regression has average performance in case of multivariate data\n>> #### The Simple Linear Regression on one attribute has very low score of 0.32 ","07ad558e":"1. Deviate from the normal distribution.\n1. Have appreciable positive skewness.\n1. Show peakedness.","787c9a76":"**Data Mining**","28ac9acd":"**Determine the years of operation of a store**\nWe wanted to make a new column depicting the years of operation of a store. This can be done as:","790da4ae":"This confirms that the column has no missing values now. Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet.","0365bfc6":"**Load Libraries**","19dac56b":"### Multiple Linear Regression"}}