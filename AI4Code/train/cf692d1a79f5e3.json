{"cell_type":{"0a3040c8":"code","e0d700c7":"code","224d90e6":"code","0eef482f":"code","a0b52e7d":"code","d2d20b25":"code","9164bd52":"code","d5b9f630":"code","99afd3be":"code","420d35db":"code","c0074956":"code","530d9acd":"code","19dc6759":"code","48a4f239":"code","971cb830":"code","a23f4626":"code","602e202b":"code","68e52f4c":"code","7c94138b":"code","04a09ab5":"code","c2c056c8":"code","0732cfb2":"code","c86839f3":"code","cd316954":"code","a57578f5":"code","96358641":"code","e1aea7d0":"code","a943350c":"code","88bb1ea1":"code","438e2551":"code","839f18b5":"code","a46aa9e2":"code","a133b9be":"code","beda6f84":"code","bcf641b0":"code","b76b41cf":"code","803f2b95":"code","ac3b8f7e":"code","392c68aa":"code","162124c7":"code","e7f9f09b":"markdown","2a6975a5":"markdown","1dc25a43":"markdown","6d1a1305":"markdown","5874f5bb":"markdown","371f9b19":"markdown","ec299c0b":"markdown","ce3b93e2":"markdown","ae780dac":"markdown","f889bcc1":"markdown","40b1098d":"markdown","77091a85":"markdown","df1ce0ab":"markdown"},"source":{"0a3040c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0d700c7":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","224d90e6":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","0eef482f":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"Percentage of women who survived : \",rate_women)","a0b52e7d":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"Percentage of men who survived : \",rate_men)","d2d20b25":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.describe()","9164bd52":"test_data.describe()","d5b9f630":"train_data.dtypes","99afd3be":"test_data.dtypes","420d35db":"train_data.corr()","c0074956":"train_data[['Survived','Pclass']].groupby(['Pclass']).mean()","530d9acd":"train_data[['Survived','SibSp']].groupby(['SibSp']).mean()","19dc6759":"train_data[['Survived','Parch']].groupby(['Parch']).mean()","48a4f239":"train_data[['Survived','Age']].groupby(['Age']).mean()","971cb830":"import seaborn as sb\nsb.heatmap(train_data.isnull())","a23f4626":"train_data[['Survived','Fare']].groupby(['Fare']).mean()","602e202b":"train_data[['Survived','Embarked']].groupby(['Embarked']).mean()","68e52f4c":"train_data[['Survived','Ticket']].groupby(['Ticket']).mean()","7c94138b":"train_data[['Survived','Sex']].groupby(['Sex']).mean()","04a09ab5":"train_data[['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']].groupby(['Survived']).mean()","c2c056c8":"train_data=train_data.drop(['PassengerId','Name','Ticket','Cabin',],axis=1)","0732cfb2":"train_data.head()\ntrain_data.info()","c86839f3":"train_data.head()","cd316954":"train_data.Embarked=train_data.Embarked.fillna(train_data.Embarked.mode().iloc[0])","a57578f5":"train_data.info()","96358641":"import random\nmean_age=train_data.Age.mean()\nstdd_age=train_data.Age.std()\nfor i in range(0,len(train_data)-1):\n    if np.isnan(train_data.Age[i]):\n        train_data.Age[i]=random.uniform(mean_age,stdd_age)","e1aea7d0":"train_data.info()","a943350c":"for i in range(0, len(train_data)):\n    if train_data.Age[i]>= 0 and train_data.Age[i]< 20:\n        train_data.Age[i]=0\n    elif train_data.Age[i]>=20 and train_data.Age[i]< 40:\n        train_data.Age[i]=1\n    elif train_data.Age[i]>= 40 and train_data.Age[i]< 60:\n        train_data.Age[i]=2\n    elif train_data.Age[i]>= 60 and train_data.Age[i]< 80:\n        train_data.Age[i]=3 ","88bb1ea1":"train_data.head()","438e2551":"for i in range(0, len(train_data)):\n    if train_data.Fare[i]>= 0 and train_data.Fare[i]< 100:\n        train_data.Fare[i]=0\n    elif train_data.Fare[i]>= 100 and train_data.Fare[i]< 200:\n        train_data.Fare[i]=1\n    elif train_data.Fare[i]>= 200 and train_data.Fare[i]< 300:\n        train_data.Fare[i]=2\n    elif train_data.Fare[i]>= 300 and train_data.Fare[i]< 400:\n        train_data.Fare[i]=3    \n    elif train_data.Fare[i]>= 400 and train_data.Fare[i]< 515:\n        train_data.Fare[i]=4","839f18b5":"train_data.head()","a46aa9e2":"train_data['Sex'] = train_data['Sex'].map({'male':1, 'female':0})\ntrain_data['Embarked'] = train_data['Embarked'].map({'C':0, 'Q':1, 'S':2})","a133b9be":"train_data.head()","beda6f84":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\ntrain_input, test_input, train_output, test_output = train_test_split(train_data.drop(['Survived'],axis=1), train_data['Survived'],test_size=0.2)\ndtm = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=0)\ndtm.fit(train_input,train_output)\npredictions = dtm.predict(test_input)\naccuracy=accuracy_score(test_output,predictions)\naccuracy","bcf641b0":"import matplotlib.pyplot as plt\np_dtm = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dtm, filled=True)\n","b76b41cf":"from sklearn.model_selection import cross_val_score\ndt_cv = cross_val_score(dtm, train_input, train_output,cv=5)\ndt_cv.mean()","803f2b95":"from sklearn.ensemble import RandomForestClassifier\ndtm = RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=4, random_state=0)\ndtm.fit(train_input, train_output)\ndt_cv = cross_val_score(dtm, train_input, train_output,cv=5)\ndt_cv.mean()","ac3b8f7e":"from sklearn import svm\nSVM_L = svm.SVC(kernel='linear')\ndt_cv = cross_val_score(SVM_L, train_input, train_output,cv=5)\ndt_cv.mean()","392c68aa":"from sklearn import svm\nSVM_Q = svm.SVC(kernel='poly',degree=2)\ndt_cv = cross_val_score(SVM_Q, train_input, train_output,cv=5)\ndt_cv.mean()","162124c7":"from sklearn import svm\nSVM_RBF = svm.SVC(kernel='rbf')\ndt_cv = cross_val_score(SVM_RBF, train_input, train_output,cv=5)\ndt_cv.mean()","e7f9f09b":"***Dropping features with lesser relevance to survival***","2a6975a5":"***Categorizing Fare in Groups***","1dc25a43":"***1.1 Preprocessing the Titanic training data\n        a)Analysing correlation of features with Survival\n        b)Dropping features with lesser relevance to survival\n        c)Filling missing values\n        d)Converting to Ordinal Values***","6d1a1305":"***1.3 Plot : Decision Tree Model***","5874f5bb":"***1.7 As we do more and more splits in decision trees, the accuracy keeps on increasing till we might need to deal with overfitting if we don't know when to stop. But incase of random forest, we build multiple decision trees and then take voting to determine the final tree which reduces the overfitting issue.***","371f9b19":"***1.4 Five-fold cross validation : Decision tree learning model***","ec299c0b":"***Filling missing values in Age***","ce3b93e2":"***1.6 Random Forest algorithm is better as compared to Decision Tree algorithm as they have classification accuracies of 81.18% and 80.34% respectively***","ae780dac":"***Categorizing Age in Groups***","f889bcc1":"***1.2 The set of important features which are selected are Pclass, Sex, Age, Sibsp, Parch, Fare, Embarked for determining the Survival. As mentioned in the preprocessing step, we use correlation between the features and survival and select the features which highly impact on survival. We drop the features which are not relevant to determine the survival.***","40b1098d":"***Converting to Ordinal Values***","77091a85":"***1.5 Five-fold cross validation : Random Forest learning model***","df1ce0ab":"***Filling missing values in Embarked***"}}