{"cell_type":{"2bd9583e":"code","077a8a1b":"code","b00c6a0a":"code","27390797":"code","9acc1001":"code","9b4cfe41":"code","4527c507":"code","ad4a67ff":"code","7f3aa49a":"code","cc7df51b":"code","ec8e4c59":"code","39b5ae94":"code","9e13865a":"code","50e535a3":"code","404bf08b":"code","b3c02ab1":"code","6112d1ef":"code","22357f53":"code","8a920b3f":"code","79fa7f62":"code","c7e03492":"code","314ab023":"code","91e9b619":"code","9fed15a1":"code","145365b6":"code","d2ef233f":"code","e3d1d750":"code","3f998778":"code","7b854ad9":"code","36b68b8a":"code","4d87902e":"code","b08652ac":"code","3732af11":"code","1c07ae0a":"code","24267230":"code","963c18b9":"code","887478ff":"code","4ea50988":"code","3617b963":"code","989a960d":"code","00b80097":"code","1aa3d87d":"code","73b1057e":"code","ace07231":"code","b6574306":"code","96e7e567":"code","1b840ee8":"code","8c595795":"code","c5b2c0ae":"code","15692c2c":"code","6980ebfc":"code","3a6d1c61":"code","09d14d17":"markdown","c6bf8e5c":"markdown","a713b1aa":"markdown","fe7d3e45":"markdown","d0ecc013":"markdown","036e1360":"markdown","661e157d":"markdown","eb47f6db":"markdown","f001d716":"markdown","39b790fd":"markdown","f9c999d8":"markdown","f9a79b4c":"markdown","55ea5e06":"markdown","40401403":"markdown","b01265b0":"markdown","88a83d96":"markdown","7495a30c":"markdown","e354a787":"markdown","98b076a2":"markdown","598672a2":"markdown","3b220c19":"markdown","6601f0f4":"markdown","191849fc":"markdown","d5a4a214":"markdown","d5563d8c":"markdown","8ad4f57b":"markdown","86ccf454":"markdown","9614fbe8":"markdown","011513e7":"markdown","193b46d2":"markdown","6d040cc7":"markdown","15eddfbe":"markdown","2dda61e2":"markdown","91dd09ed":"markdown","b9c48a75":"markdown","683e8705":"markdown","efc51adc":"markdown"},"source":{"2bd9583e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","077a8a1b":"import matplotlib.pylab as plt","b00c6a0a":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])","27390797":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","9acc1001":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","9b4cfe41":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","4527c507":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","ad4a67ff":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","7f3aa49a":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","cc7df51b":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","ec8e4c59":"import numpy as np\n\n#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\nX.shape","39b5ae94":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')\n\nplt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')\n\n#plt.savefig(\"files\/ch07\/sample.png\",dpi=300, bbox_inches='tight')\n\nfrom sklearn import cluster\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)","9e13865a":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","50e535a3":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])","404bf08b":"plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","b3c02ab1":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","6112d1ef":"Z=clf.predict(data) # returns the labels of the data\nprint (Z)","22357f53":"#print(\"%.3f\" % metrics.completeness_score(data, Z))\n#print(\"%.3f\" % metrics.homogeneity_score(data, Z))\n#print(\"%.3f\" % metrics.v_measure_score(data, Z))","8a920b3f":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n","79fa7f62":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data","c7e03492":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n\nx = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\n","314ab023":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","91e9b619":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0,\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))\n\nclf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","9fed15a1":"import pandas as pd\nimport numpy as np\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('\/kaggle\/input\/ho-ict\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","145365b6":"edu.tail()","d2ef233f":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","e3d1d750":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","3f998778":"#Extract 2010 set of values\nedu2010=pivedu.loc[2010]\nedu2010.head()","7b854ad9":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\n\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head()","36b68b8a":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","4d87902e":"#Remove non info countries\nwrk_countries = nan_countries<4\nprint (wrk_countries)\neduclean=edu2010.loc[wrk_countries] #.ix - Construct an open mesh from multiple sequences.\nprint (educlean)\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\nprint (na_features)\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((8,4))","b08652ac":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","3732af11":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()\nprint(y_pred_fill)","1c07ae0a":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","24267230":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","963c18b9":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","887478ff":"print(y_pred_drop)\nprint(y_pred_drop+0.2*np.random.rand(35))\nplt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\n#plt.savefig(\"\/kaggle\/input\/lecture-2\/files\/ch07\/correlationkmeans.png\",dpi=300, bbox_inches='tight')","4ea50988":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","3617b963":"# OP & CC added :\nprint(\"Drop Silhouette Score %.3f\" % metrics.silhouette_score(X_train_drop, y_pred_drop))\nprint(\"Fill Silhouette Score %.3f\" % metrics.silhouette_score(X_train_drop, y_pred_fill))","989a960d":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()","00b80097":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[0]],'euclidean')\n#the distance of the elements of cluster 1 to the center of cluster 0 (changed)\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]#changed to list countries in cluster 1\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","1aa3d87d":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==1,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1)\n#plt.savefig(\"files\/ch07\/dist2cluster01.png\",dpi=300, bbox_inches='tight')","73b1057e":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","ace07231":"print(\"4-means Silhouette Score %.3f\" % metrics.silhouette_score(X_train, y_pred))","b6574306":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\n#plt.savefig(\"files\/ch07\/distances4clusters.png\",dpi=300, bbox_inches='tight')","96e7e567":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","1b840ee8":"from scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","8c595795":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","c5b2c0ae":"print(\"Spectral Clustering Silhouette Score %.3f\" % metrics.silhouette_score(edudrop.values, y_pred))","15692c2c":"X_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 3,labels = wrk_countries_names, leaf_font_size=20);\n\n#plt.savefig(\"files\/ch07\/ACCountires.png\",dpi=300, bbox_inches='tight')\nplt.show()\n\n#plt.tight_layout() # fixes margins","6980ebfc":"from sklearn.cluster import AgglomerativeClustering\n\n#create actual model for agglomerative clustering diagram above.\nagglom = cluster.AgglomerativeClustering(n_clusters=4, affinity=\"euclidean\")\nagglom.fit(edudrop.values)\n\ny_pred_aglom = agglom.labels_.astype(np.int)","3a6d1c61":"print(\"Agglomerate Silhouette Score %.3f\" % metrics.silhouette_score(edudrop.values, y_pred_aglom))","09d14d17":"The main pipeline of this part in which we were introduced to clustering through the K-Mean algorithm in which we created randomly a dataset is to initialize the number of centroids we desire, decide the centroids of the different clusters which is possible through different techniques. We first assign the label we know then we test the clustering with the same data.","c6bf8e5c":"**PIPELINE : 12**\nThis clarifies the previous guess :\nCluster 1\/method fill ~= Cluster 0\/method drop and vice versa.\n\nTo check which method is better, we can use the Silhouette score (code below). It reveals that the Drop method results in slightly better clustering.","a713b1aa":"**CASE STUDY EURODATA**","fe7d3e45":"**PIPELINE : 20**\n\nThis results in a heirarchy where the closest data points are associated with the next closest data point (or group of data points) and so on until all are associated at different levels. \n\nAn important bonus of the agglomerate clustering function is that we can clearly see each step in the decision majing process of the algorithm, which is not as easy for k-means or spectral.\n\nCreating the actual model and finding its silhouette score gives an identical score to the 4-means algorithm (note, both have been using non-scaled training data, which I think is probably a mistake).\n\nAs discussed previously, I am not sure why the spectral clustering results have a lower silhouette score than the 3-means models, and much lower than the 4-means model. My guess is that it is related to closer splitting of the clusters resulting in less distance between them. Depending on exactly what the aim of our clustering is, this could either suggest that the spectral clustering is not as effective for this data set, or that the silhouette score is not a good performance measure for the type of clustering we want to do.\n\nTo conclude, the outcomes of all the clustering we have done essentially divides countries based on their spending patterns on education. This type of clustering by itself is not particularly useful. What would be much more useful, would be if we included additional features, such as student performance in testing, economic indicators like actual GDP and unemployment rate, or even health and other types of indicators. \nThe beauty of clustering is that it allows the algorithm to find correlations and links that might be difficult to spot, and so by inlcuding some features related to (say) test scores, we may be able to observe a possible link between realtively higher primary education spending, and overall student performance.\n","d0ecc013":"**PIPELINE : 8**\n\nCountries are plotted by their cluster groupings. We can now see the results of the clustering into 3 different labels depending on their investment in education using the filled in data option.","036e1360":"**QUESTION : How many \u201cmisclusterings\u201d do we have?**\n\nANSWER : As we can see on the space partition we have 9 misclusterings. Most of the errors come from the blue partition.","661e157d":"**Question: Labelings that assign all classes members to the same clusters are: ___________, but not _____________:**\n\nANSWER : homogenous, but not complete (assuming there are multiple clusters for the same class.","eb47f6db":"**PIPELINE : 3**\n Simply the renaming of some of the more combersome country indexes.","f001d716":"We imported the raw dataset from europa eurostat. There are 12 indicators. By looking at the head of the dataframe we can see the shape of the dataset, which are four rows: time, region\/country, the inicator evaluated and then the value of this indicator.","39b790fd":"**PIPELINE : 2**\n\nThis statement selects the group of entries we have for the year 2010 and creates a new, smaller pivot table with this data.","f9c999d8":"**PIPELINE : 11**\n\nThis random value generation is simply to be able to view the relative numbers of countries located in each cluster. \n\nFrom this plot, we can see that the clusters are not hugely different using the two strategies. Cluster two remains relatively the same for both strategies (only two differing countries), and we can see that in the specific instances of each algorithm, cluster 0 in one method found the same cluster as was named cluster 1 in the other method (the actual numbering of clusters is of no importance).","f9a79b4c":"Let's first use the K-Mean clustering technique.","55ea5e06":"**PIPELINE : 6**\n\nTwo new dataframes are created to pursue two different strategies for dealing with the Nan values :\nedufill - an array where nan values in a feature are simply transplanted with the average of all the values for that feature. This runs the risk of inputting false information which may affect the algorithm later on.\n\nedudrop - a dataframe where any features with Nan values are removed entirely. This runs the risk of removing useful information from some countries.","40401403":"**PIPELINE : 15**\n\nAs for the previous block, this one was changed to focus on the Cluster 1 countries. It shows that, of the Cluster 1 countries, that Spain and Latvia are the closest to cluster 0, however Spain is significanty further from the centre of Cluster 1 than Latvia, and so is therefore the closest to changing cluster.","b01265b0":"Once we have removed the data  we can see that there are 4 features who have missing data in 2010. Let's check now how we can handle this data. Two options: filling the data with non-informative data  or drop these features.","88a83d96":"**PIPELINE : 16**\n\nThe clustering algorithm is run again using an additional cluster (and with a different initialisation generator randomstate=0).\n\nThis method appears to have split the 'high spending' previous cluster 0 into Clusters 1&3, with Cluster 0 taking the european averages and some additional countries like Spain (up from the low spending cluster) and the UK (down from the high spending cluster). \nThe low spending cluster (now cluster 2) reamins fairly similar to the previous result.\n\nWe can see that the addition of the 4th cluster significantly improves the Silouette Score below : \n\nNOTE : I think there is a mistake in this cell - the scaled values are not used :\n**X_train = edudrop.values** This also affects the silhouette score (0.3 if scaled values are used)","7495a30c":"The randscore is an interesting parameter that indicates the number of clusters that were established compared to the total clusters that could have been made.The inertia is about the coherence of the clusters. It is interesting to notice that for the best clustering is the one with less inertia meaning that the clusters are less coherent than for the kmean than for the random centroid research","e354a787":"**PIPELINE : 7**\n\nThe features are first all scaled, which is necessary so that features over a wider numerical range do not take up greater importance in the algorithm, which calculates error using distance.\n\nThe number of clusters is set arbitrarily to 3 (the algorthim does not decide on the best number of clusters, and we do not know what a good number would be initially. This could be optimised using methods such as Silhouette Coefficient because we do not have known correct labels for the data).\n\ny_pred_fill stores the cluster number in the order of countries.\nidx stores the countries ordered by cluster.","98b076a2":"**Question: Labelings that have pure clusters with members coming from the same classes are ________________ but un-necessary splits harm ____________________ and thus penalise V-measure as well:**\n\nANSWER : homogenous, completeness ","598672a2":"**PIPELINE : 1**\n\nA pandas pivot table is created using the dataframe 'edu'. By choosing 'TIME' and then 'GEO' as the indexes, all entries are first group by 'TIME', and then broken down into countries. INDIC_ED presents the results for these groups for the different expenditure indexes. NOTE: if there are erronous multiple entries for a country and a certain year, this method will automatically average the values and display the single result.","3b220c19":"**PIPELINE : 18**\n\nWhereas the normal k-means amgorthim automatically tends to create spherical clusters, SpectralClustering is allows clustering to be based more closely on measures of similarity between the data points, rather than their relative difference to the cluster centroid averages. This should allow for better division of clusters.\n\nNOTE : I think there is a mistake in this cell - the scaled values are not used :\n**distances = euclidean_distances(edudrop.values)**","6601f0f4":"**PIPELINE : 10**\n\nWe can see that some countries have changed cluster group compared to the 'fill NA' strategy.","191849fc":"In this part we saw the clustering technique. We gave as input the dataset, then the algorithm found the best parameters to draw the partitions of the different clusters. Yet we saw that the clustering isn't perfect, there are some misclustering that could be resolve perhabs by using a more complex clustering. ","d5a4a214":"**PIPELINE : 5[]**\n\nwrk_countries is a list of boolean values that identify that a country index has no more than 3 Nan values.\neduclean is then created as an array indexed by the country, but the column names have not been extracted.\n\nFrom this reduced set of countries, the number of Nan values per feature (ie. number of NAN for each INDIC_ED value)\n","d5563d8c":"The choice of centroids is essential to get good clustering results. Centroids are not necessary in the original datasets, it is important to initialize them well, kmeans++ proves to be more efficient than initiating them randomly.","8ad4f57b":"We can see that there are some missing values for some countries and some indicators while checking at the tail of the dataset.","86ccf454":"**PIPELINE : 9**\n\nSame as previous, but for the'drop NA features' strategy.","9614fbe8":"**PIPELINE : 14**\n\nIn our running of this code, for the 'drop-na' strategy, Spain is actually included in Cluster 1. We therefore changed the code in this block to show the distance from Cluster 1 countries to the centre point of Cluster 0. The plot clearly shows that Spain is one of the closest of the Cluster 1 countries to Cluster 0.","011513e7":"**PIPELINE : 13**\n\nContinuing on using the 'drop-na' strategy, this plot compares the average values of the expenditures for the countries grouped by cluster for each INDEC_ED. A general pattern is observed, which is that educational spending is generally highest in Cluster 0 and lowest in Cluster 1 across all indicators. It makes sense that Cluster 2 is in the middle, because Cluster 2 also consists of the european average entries.","193b46d2":"This first step was regarding the dataset treatment in order to analyze it better. We are now going to cluster this dataset.","6d040cc7":"**Question: Clusters that include samples from totally different classes totally destroy the _______________________  of the labelling, hence:**\n\nANSWER : homogneity, hence the v-measure also becomes zero.","15eddfbe":"Using dropped missing values data, the partition is drawn differently.Now for example most of the countries that were labelled 2 before are now 0.","2dda61e2":"We saw thaht the clustering with 3 partitions was borderline with the case of Spain,so let's redo the clustering but with K=4 that could give better results.","91dd09ed":"**PIPELINE : 4**\n\nThis creates a numpy list of the total count of Nan values of expenditure found for each country in the year 2010. It then displays these numbers by country in a bar plot.","b9c48a75":"In this case study we are studying the indicators of spending on education among EU countries","683e8705":"**PIPELINE : 19**\n\nWhile we would expect the Spectral Clustering to improve performance on the k-means clustering, it seems to have reduced the silhouette score from the simple '4-means' algorithm results. (I also checked this using scaled training data and it did not improve either) \nBased on the online literature, Silhouette score schould still be applicable for Spectral Clustering results. I suspect the reason for the lower score is the b-value being smaller, relating to less space between different clusters. Spectral clustering is able to create more complex clusters than k-means, and so it may be that while these clusters are the result of correct association of similarities, they have less distance between them, and so result in a lower silhouette score","efc51adc":"**PIPELINE : 17**\n\nConfirming previous guesses, the highest spending countries cluster has been divided into Clusters 1 and 3. The reasons for the split appear to be (1) Cluster 3 countries are consistantly higher spenders than Cluster 1, and (2)Cluster 1 countries have lower than average spending on primary education (indicator index 2 for edudrop). Cluster 0 (now holding the euro average values) is in the middle as expected. The lowest spending countries are now associated to Cluster 2."}}