{"cell_type":{"f6c3b22d":"code","c6053078":"code","c35a1472":"code","5c0ee681":"code","9e2750cd":"code","e366088d":"code","1c20708b":"code","80db6887":"code","3e98c0ef":"code","6633b53f":"code","e26a8521":"code","dbe50d08":"code","b725bc0d":"code","9971eec6":"code","fd6ff9ee":"code","b45fb567":"markdown","b3529f18":"markdown","e95a5582":"markdown","beea3eba":"markdown","4000fa78":"markdown"},"source":{"f6c3b22d":"print(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t\u2013 TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t\u2013 TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t\u2013 NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t\u2013 SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import GroupKFold, KFold;\n\n!pip install -q keras-cv-attention-models\nimport keras_cv_attention_models\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom multiprocessing import cpu_count\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\t\u2013 MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\n\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n    \nprint(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\nseed_it_all()","c6053078":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU\/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1\/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","c35a1472":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"\/kaggle\/input\/petfinder-pawpularity-score\"\n    save_locally = None\n    \nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n    \nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","5c0ee681":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","9e2750cd":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN DATAFRAME ..\\n\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\ntrain_df[\"img_path\"] = train_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"train\", x+\".jpg\"))\ntrain_df[\"Q\"] = pd.qcut(train_df['Pawpularity'], q = 15, labels = range(15))\ndisplay(train_df)\n\nprint(\"\\n... TEST DATAFRAME ..\\n\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"img_path\"] = test_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"test\", x+\".jpg\"))\n\ndisplay(test_df)\n\nprint(\"\\n... SAMPLE SUBMISSION DATAFRAME ..\\n\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)\n\n# Set Other Variables\nprint(\"\\n... SETTING OTHER VARIABLES ..\\n\")\n\nINPUT_SHAPE = (240, 240, 3)\nN_CLASSES = train_df.Pawpularity.nunique()\nREPLICA_BATCH_SIZE = 8\nOVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE * N_REPLICAS\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(\"\\n\\n... BASIC DATA SETUP FINISHING ...\\n\")","e366088d":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef tf_load_image(image, resize_to=INPUT_SHAPE):\n    image = tf.image.decode_jpeg(tf.io.read_file(image), channels=INPUT_SHAPE[-1])\n    image = tf.image.resize(image, size=resize_to[:-1])\n    return image\n\ndef rotate_and_crop(images):\n    \"\"\"Rotate the given image with the given rotation degree and crop for the black edges if necessary\n    Args:\n        image: A `Tensor` representing an image(s) of arbitrary size.\n    \n    Returns:\n        A rotated image.\n    \"\"\"\n    \n    \n    def _largest_rotated_rect(w, h, angle):\n        \"\"\"\n        \n        Given a rectangle of size wxh that has been rotated by 'angle' (in\n        radians), computes the width and height of the largest possible\n        axis-aligned rectangle within the rotated rectangle.\n        Original JS code by 'Andri' and Magnus Hoff from Stack Overflow\n        Converted to Python by Aaron Snoswell\n        \n        Source: http:\/\/stackoverflow.com\/questions\/16702966\/rotate-image-and-crop-out-black-borders\n        \n        \"\"\"\n        \n        quadrant = tf.cast(tf.math.floor(angle \/ (math.pi\/2)), dtype=tf.uint8)\n        quadrant = tf.bitwise.bitwise_and(quadrant, tf.constant(3, dtype=quadrant.dtype))\n        sign_alpha = tf.cond(tf.bitwise.bitwise_and(quadrant, tf.constant(1, dtype=quadrant.dtype))==tf.constant(0, dtype=quadrant.dtype), lambda: angle, lambda: math.pi-angle)\n        alpha = (sign_alpha % math.pi + math.pi) % math.pi\n\n        bb_w = w * tf.math.cos(alpha) + h * tf.math.sin(alpha)\n        bb_h = w * tf.math.sin(alpha) + h * tf.math.cos(alpha)\n\n        gamma = tf.cond(w<h, lambda: tf.math.atan2(bb_w, bb_w), lambda: tf.math.atan2(bb_w, bb_w))\n\n        delta = math.pi - alpha - gamma\n\n        length = tf.cond(w<h, lambda: h, lambda: w)\n\n        d = length * tf.math.cos(alpha)\n        a = d * tf.math.sin(alpha) \/ tf.math.sin(delta)\n\n        y = a * tf.math.cos(gamma)\n        x = y * tf.math.tan(gamma)\n\n        return (bb_w - 2 * x, bb_h - 2 * y)\n  \n    # Get desired output dimensions\n    output_height, output_width = tf.constant(INPUT_SHAPE[0], dtype=tf.float32), tf.constant(INPUT_SHAPE[1], dtype=tf.float32)\n\n    rotation_degree = (math.pi\/180)*tf.random.normal(shape=(), stddev=10)\n    images = tfa.image.rotate(images, rotation_degree, interpolation='BILINEAR')\n\n    # Center crop to ommit black noise on the edges\n    lrr_width, lrr_height = _largest_rotated_rect(output_height, output_width, rotation_degree)\n    lrr_offset_w, lrr_offset_h = tf.cast(tf.math.round((output_width-lrr_width)\/2), dtype=tf.int32), tf.cast(tf.math.round((output_height-lrr_height)\/2), dtype=tf.int32)\n    lrr_width, lrr_height = tf.cast(tf.math.round(lrr_width), dtype=tf.int32), tf.cast(tf.math.round(lrr_height), dtype=tf.int32)\n\n    images = tf.image.crop_to_bounding_box(images, lrr_offset_h, lrr_offset_w, target_height=lrr_height, target_width=lrr_width)\n    images = tf.image.resize(images, (output_height, output_width))\n    \n    return images\n\n\ndef simple_augmentation(images, labels):\n    # Random Horizontal Flip\n    images = tf.image.random_flip_left_right(images)\n    \n    # images = rotate_and_crop(images)\n    \n    images = tfa.image.random_cutout(images, mask_size=(2*(INPUT_SHAPE[0]\/\/30),2*(INPUT_SHAPE[0]\/\/30)))\n    \n    # Random Saturation\n    images = tf.image.random_saturation(images, 0.975, 1.025)\n\n    # Random Hue\n    images = tf.image.random_hue(images, 0.0125)\n    \n    # Random Brightness\n    images = tf.image.random_brightness(images, 0.125)\n    \n    return images, labels","1c20708b":"PLT_N = 4\nplt.figure(figsize=(20,12))\nfor i in range(PLT_N):\n    o_img = tf_load_image(train_df.sample(1).img_path.values[0])\n    a_img = simple_augmentation(tf.expand_dims(o_img, axis=0), None)[0][0]\n    plt.subplot(2,4,i+1)\n    plt.imshow(o_img\/255.)\n    plt.title(\"Original Image\", fontweight=\"bold\")\n    plt.axis(False)\n    \n    plt.subplot(2,4,i+5)\n    plt.imshow(a_img\/255.)\n    plt.title(\"Random Augmentation\", fontweight=\"bold\")\n    plt.axis(False)\n    \nplt.tight_layout()\nplt.show()","80db6887":"N_FOLDS = 8\nkf = KFold(n_splits=N_FOLDS)\n\ndataset_folds = {}\nfor i, (train_indices, val_indices) in enumerate(kf.split(X=train_df[\"img_path\"], y=train_df[\"Pawpularity\"])):\n    SUB_TRAIN_IMG_PATH_DS = train_df.iloc[train_indices].img_path.values\n    SUB_TRAIN_LABEL_DS = train_df.iloc[train_indices].Pawpularity.values.astype(np.float32)\n    \n    SUB_VAL_IMG_PATH_DS = train_df.iloc[val_indices].img_path.values\n    SUB_VAL_LABEL_DS = train_df.iloc[val_indices].Pawpularity.values.astype(np.float32)\n    \n    sub_train_img_ds = tf.data.Dataset.from_tensor_slices(SUB_TRAIN_IMG_PATH_DS)\n    sub_train_img_ds = sub_train_img_ds.map(tf_load_image, num_parallel_calls=tf.data.AUTOTUNE).cache()\n    sub_train_lbl_ds = tf.data.Dataset.from_tensor_slices(SUB_TRAIN_LABEL_DS)\n    sub_train_ds = tf.data.Dataset.zip((sub_train_img_ds, sub_train_lbl_ds))\n    sub_train_ds = sub_train_ds.shuffle((len(train_df)\/\/N_FOLDS)*(N_FOLDS-1)) \n    sub_train_ds = sub_train_ds.batch(OVERALL_BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n    sub_train_ds = sub_train_ds.map(simple_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    sub_val_img_ds = tf.data.Dataset.from_tensor_slices(SUB_VAL_IMG_PATH_DS)\n    sub_val_img_ds = sub_val_img_ds.map(tf_load_image, num_parallel_calls=tf.data.AUTOTUNE).cache()\n    sub_val_lbl_ds = tf.data.Dataset.from_tensor_slices(SUB_VAL_LABEL_DS)\n    sub_val_ds = tf.data.Dataset.zip((sub_val_img_ds, sub_val_lbl_ds))\n    sub_val_ds = sub_val_ds.shuffle(len(train_df)\/\/N_FOLDS) \n    sub_val_ds = sub_val_ds.batch(OVERALL_BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\n    dataset_folds[f\"fold_{i}\"] = {\"train_ds\":sub_train_ds, \"val_ds\":sub_val_ds}\n\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_df.img_path.values)\ntest_ds = test_img_ds.map(tf_load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(OVERALL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ndataset_folds","3e98c0ef":"def get_keras_cv_model(bb, pp_fn, make_trainable=False, input_shape=INPUT_SHAPE, head_size=64, dropout_rate=0.3, print_summary=True):\n    \"\"\"\n    \n    Function to get a custom model. This function allows for any model from the \n    keras applications library of models.\n    \n    Args:\n        bb (tf.keras.applications.<model> functional backbone):\n            The specific model backbone function to use (from tf.keras API)\n        pp_fn (tf.keras.applications.<model>.preprocess_input):\n            The specific preprocessing function to use (from tf.keras API)\n        make_trainable (optional, bool):\n            Whether or not to set the bulk of the model to be trainable\n        input_shape(optional, tuple of ints):\n            A tuple of integers describing the input shape the model should expect\n        head_size(optional, int): \n            The number of nodes in the final head FC layer\n        dropout_rate(optional, float): \n            The dropout to be applied between the final head layer and \n            the classification\/regression output layer\n        print_summary(optional, bool):\n            Whether or not to print the model summary\n        \n    Returns:\n        Custom model for petfinder\n    \n    \"\"\"\n    # Get the backbone\n    bb_submodel = bb(include_top=False, weights=\"imagenet\", input_shape=INPUT_SHAPE, pooling=\"avg\")\n    \n    # Set trainability\n    if not make_trainable:\n        bb_submodel.trainable=False\n    \n    # Define functional model\n    _inputs = tf.keras.layers.Input(input_shape)\n    preprocessed_inputs = pp_fn(_inputs)\n    \n    x = bb_submodel(preprocessed_inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(head_size, activation='relu')(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    _outputs = tf.keras.layers.Dense(1, activation=\"relu\")(x)\n    \n    _model = tf.keras.Model(inputs=_inputs, outputs=_outputs)\n    if print_summary:\n        print(_model.summary())\n    return _model","6633b53f":"N_EPOCHS=25\n\nMODELS_TO_TEST = [\n    tf.keras.applications.EfficientNetB1,\n    tf.keras.applications.MobileNetV2,\n    tf.keras.applications.ResNet50,\n    tf.keras.applications.DenseNet121\n]\nPP_TO_GO_W_TEST = [\n    tf.keras.applications.efficientnet.preprocess_input,\n    tf.keras.applications.mobilenet_v2.preprocess_input,\n    tf.keras.applications.resnet50.preprocess_input,\n    tf.keras.applications.densenet.preprocess_input,\n]\n\nhistories = []\nfor BB, PP_FN in zip(MODELS_TO_TEST, PP_TO_GO_W_TEST):    \n    print(f\"\\n\\n\\n... STARTING TRAINING FOR {BB._keras_api_names[-1].upper()} ...\\n\\n\")\n    with strategy.scope():\n        \n        callback_list = [\n            tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n            tf.keras.callbacks.ReduceLROnPlateau(factor=0.8, patience=2, min_lr=1e-9, verbose=1)\n        ]\n        \n        model = get_keras_cv_model(BB, PP_FN)\n        model.compile(\n            loss=\"mse\",\n            optimizer=tf.keras.optimizers.Adam(0.001),\n            metrics=[tf.keras.metrics.RootMeanSquaredError(),]\n        )\n        \n        histories.append(model.fit(dataset_folds[f\"fold_0\"][\"train_ds\"],\n                                   validation_data=dataset_folds[f\"fold_0\"][\"val_ds\"],\n                                   callbacks=callback_list, epochs=N_EPOCHS,))","e26a8521":"for metric_name in [\"loss\", \"root_mean_squared_error\"]:\n    fig = go.Figure()\n    for name, history, color in zip([\"EfficientNetB1\", \"MobileNetV2\", \"ResNet50\", \"DenseNet121\"], histories, [\"red\", \"blue\", \"orange\", \"green\"]):        \n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"{metric_name}\"],\n            mode='lines+markers',\n            legendgroup=name,\n            text=f\"<b>Training {name}<\/b>\",\n            legendgrouptitle_text=f\"<b>{name}<\/b>\",\n            visible=\"legendonly\",\n            name=f\"<b>Training {metric_name.replace('_', ' ').title()}<\/b>\",\n            marker=dict(color=f\"{color}\"),\n            line=dict(color=f\"{color}\")\n        ))\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"val_{metric_name}\"],\n            mode='lines+markers',\n            legendgroup=name,\n            text=f\"<b>Validation {name}<\/b>\",\n            legendgrouptitle_text=f\"<b>{name}<\/b>\",\n            name=f\"<b>Validation {metric_name.replace('_', ' ').title()}<\/b>\",\n            marker=dict(color=f\"dark{color}\"),\n            line=dict(color=f\"dark{color}\")\n        ))\n        fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.995),\n                          title=f\"{name} - {metric_name.replace('_', ' ').title()} Plot\",\n                          xaxis_title=\"Epoch Number\",\n                          yaxis_title=\"Mean Squared Error\" if metric_name==\"loss\" else \"Root Mean Squared Error\",\n                          legend_title=\"<b>LEGEND<\/b>\",)\n    fig.show()","dbe50d08":"for name, history in zip([\"EfficientNetB1\", \"MobileNetV2\", \"ResNet50\", \"DenseNet121\"], histories):\n    for metric_name in [\"loss\", \"root_mean_squared_error\"]:\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"{metric_name}\"],\n            mode='lines+markers',\n            name=f\"<b>Training {metric_name.replace('_', ' ').title()}<\/b>\"\n        ))\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"val_{metric_name}\"],\n            mode='lines+markers',\n            name=f\"<b>Validation {metric_name.replace('_', ' ').title()}<\/b>\"\n        ))\n        fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.995),\n                          title=f\"{name} - {metric_name.replace('_', ' ').title()} Plot\",\n                          xaxis_title=\"Epoch Number\",\n                          yaxis_title=\"Mean Squared Error\" if metric_name==\"loss\" else \"Root Mean Squared Error\",\n                          legend_title=\"<b>LEGEND<\/b>\",)\n        fig.show()","b725bc0d":"N_EPOCHS=25\n\nMODELS_TO_TEST = [\n    tf.keras.applications.EfficientNetB1,\n    tf.keras.applications.MobileNetV2,\n    tf.keras.applications.ResNet50,\n    tf.keras.applications.DenseNet121\n]\nPP_TO_GO_W_TEST = [\n    tf.keras.applications.efficientnet.preprocess_input,\n    tf.keras.applications.mobilenet_v2.preprocess_input,\n    tf.keras.applications.resnet50.preprocess_input,\n    tf.keras.applications.densenet.preprocess_input,\n]\n\nhistories = []\nfor BB, PP_FN in zip(MODELS_TO_TEST, PP_TO_GO_W_TEST):    \n    print(f\"\\n\\n\\n... STARTING TRAINING FOR {BB._keras_api_names[-1].upper()} ...\\n\\n\")\n    with strategy.scope():\n        \n        callback_list = [\n            tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n            tf.keras.callbacks.ReduceLROnPlateau(factor=0.8, patience=2, min_lr=1e-9, verbose=1)\n        ]\n        \n        model = get_keras_cv_model(BB, PP_FN)\n        model.compile(\n            loss=\"mse\",\n            optimizer=tf.keras.optimizers.Adam(0.001),\n            metrics=[tf.keras.metrics.RootMeanSquaredError(),]\n        )\n        \n        histories.append(model.fit(dataset_folds[f\"fold_5\"][\"train_ds\"],\n                                   validation_data=dataset_folds[f\"fold_5\"][\"val_ds\"],\n                                   callbacks=callback_list, epochs=N_EPOCHS,))","9971eec6":"for metric_name in [\"loss\", \"root_mean_squared_error\"]:\n    fig = go.Figure()\n    for name, history, color in zip([\"EfficientNetB1\", \"MobileNetV2\", \"ResNet50\", \"DenseNet121\"], histories, [\"red\", \"blue\", \"orange\", \"green\"]):        \n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"{metric_name}\"],\n            mode='lines+markers',\n            legendgroup=name,\n            text=f\"<b>Training {name}<\/b>\",\n            legendgrouptitle_text=f\"<b>{name}<\/b>\",\n            visible=\"legendonly\",\n            name=f\"<b>Training {metric_name.replace('_', ' ').title()}<\/b>\",\n            marker=dict(color=f\"{color}\"),\n            line=dict(color=f\"{color}\")\n        ))\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"val_{metric_name}\"],\n            mode='lines+markers',\n            legendgroup=name,\n            text=f\"<b>Validation {name}<\/b>\",\n            legendgrouptitle_text=f\"<b>{name}<\/b>\",\n            name=f\"<b>Validation {metric_name.replace('_', ' ').title()}<\/b>\",\n            marker=dict(color=f\"dark{color}\"),\n            line=dict(color=f\"dark{color}\")\n        ))\n        fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.995),\n                          title=f\"{name} - {metric_name.replace('_', ' ').title()} Plot\",\n                          xaxis_title=\"Epoch Number\",\n                          yaxis_title=\"Mean Squared Error\" if metric_name==\"loss\" else \"Root Mean Squared Error\",\n                          legend_title=\"<b>LEGEND<\/b>\",)\n    fig.show()","fd6ff9ee":"for name, history in zip([\"EfficientNetB1\", \"MobileNetV2\", \"ResNet50\", \"DenseNet121\"], histories):\n    for metric_name in [\"loss\", \"root_mean_squared_error\"]:\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"{metric_name}\"],\n            mode='lines+markers',\n            name=f\"<b>Training {metric_name.replace('_', ' ').title()}<\/b>\"\n        ))\n        fig.add_trace(go.Scatter(\n            x=np.arange(len(history.history[f\"{metric_name}\"])),\n            y=history.history[f\"val_{metric_name}\"],\n            mode='lines+markers',\n            name=f\"<b>Validation {metric_name.replace('_', ' ').title()}<\/b>\"\n        ))\n        fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.995),\n                          title=f\"{name} - {metric_name.replace('_', ' ').title()} Plot\",\n                          xaxis_title=\"Epoch Number\",\n                          yaxis_title=\"Mean Squared Error\" if metric_name==\"loss\" else \"Root Mean Squared Error\",\n                          legend_title=\"<b>LEGEND<\/b>\",)\n        fig.show()","b45fb567":"# Try With Another Fold","b3529f18":"# Try With Fold 0","e95a5582":"# Simple Augmentation","beea3eba":"# Function To Create Splits","4000fa78":"# Function To Grab A Keras Model"}}