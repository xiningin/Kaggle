{"cell_type":{"a8db8b7f":"code","aa76dfc6":"code","f5e9fee4":"code","da63df2e":"code","d25912b7":"code","2176cd60":"code","b9766483":"code","0ee35447":"code","38b652d8":"code","f3449e16":"code","f491a1f2":"code","250933f2":"code","30decf56":"code","a038d853":"code","121e94b9":"code","5855915b":"code","7bc5aa10":"code","10e34949":"code","ffee0b41":"code","fdcc0b4f":"code","47c6038d":"code","ea64bee9":"code","884ce6a1":"code","ce7ea200":"code","b70d7cb7":"markdown","9538ef8b":"markdown","72a23e8e":"markdown","9f04ad3c":"markdown","6b968fdb":"markdown","15482e48":"markdown","6f474b1c":"markdown","2e821aef":"markdown","7c50e649":"markdown","b7a75298":"markdown","a13bf680":"markdown"},"source":{"a8db8b7f":"!pip install -q tensorflow_decision_forests","aa76dfc6":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport os\nimport random\nimport warnings\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n\nwarnings.filterwarnings(\"ignore\")\nseed_everything(42)","f5e9fee4":"import tensorflow_decision_forests as tfdf","da63df2e":"tfdf.keras.get_all_models()","d25912b7":"# Reading the dataset\n\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","2176cd60":"df.head()","b9766483":"# Splitting the dataset\n\ntrain_df, test_df = train_test_split(\n    df, test_size=0.3, stratify=df[\"Outcome\"], random_state=42\n)","0ee35447":"# Convert the dataset into a TensorFlow dataset\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"Outcome\")\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"Outcome\")","38b652d8":"# Train a Random Forest model\n\nmodel_rf = tfdf.keras.RandomForestModel()\nmodel_rf.fit(train_ds)","f3449e16":"# Summary of the model structure\n\nmodel_rf.summary()","f491a1f2":"# Evaluate the model\n\npreds_rf = np.where(model_rf.predict(test_ds) < 0.5, 0, 1).ravel()\n\nacc_rf = accuracy_score(test_df[\"Outcome\"].values, preds_rf)\n\nprint(f\"Test set accuracy of Random Forest model is {acc_rf:.6f}\")","250933f2":"# Train a Gradient Boosted Trees Model\n\nmodel_gbt = tfdf.keras.GradientBoostedTreesModel()\nmodel_gbt.fit(train_ds)","30decf56":"# Summary of the model structure\n\nmodel_gbt.summary()","a038d853":"# Evaluate the model\n\npreds_gbt = np.where(model_gbt.predict(test_ds) < 0.5, 0, 1).ravel()\n\nacc_gbt = accuracy_score(test_df[\"Outcome\"].values, preds_gbt)\n\nprint(f\"Test set accuracy of Gradient Boosted Trees model is {acc_gbt:.6f}\")","121e94b9":"# Train a CART Model\n\nmodel_cart = tfdf.keras.CartModel()\nmodel_cart.fit(train_ds)","5855915b":"# Summary of the model structure\n\nmodel_cart.summary()","7bc5aa10":"# Evaluate the model\n\npreds_cart = np.where(model_cart.predict(test_ds) < 0.5, 0, 1).ravel()\n\nacc_cart = accuracy_score(test_df[\"Outcome\"].values, preds_cart)\n\nprint(f\"Test set accuracy of CART model is {acc_cart:.6f}\")","10e34949":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf_sk = RandomForestClassifier()\n\nmodel_rf_sk.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values)\n\nacc_rf_sk = accuracy_score(\n    test_df[\"Outcome\"].values, model_rf_sk.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of sklearn's Random Forest Classifier model is {acc_rf_sk:.6f}\"\n)","ffee0b41":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel_gbt_sk = GradientBoostingClassifier()\n\nmodel_gbt_sk.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values)\n\nacc_gbt_sk = accuracy_score(\n    test_df[\"Outcome\"].values, model_gbt_sk.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of sklearn's Random Forest Classifier model is {acc_gbt_sk:.6f}\"\n)","fdcc0b4f":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_dtc_sk = DecisionTreeClassifier()\n\nmodel_dtc_sk.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values)\n\nacc_dtc_sk = accuracy_score(\n    test_df[\"Outcome\"].values, model_dtc_sk.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of sklearn's Decision Tree Classifier model is {acc_dtc_sk:.6f}\"\n)","47c6038d":"from lightgbm import LGBMClassifier\n\nmodel_lgb = LGBMClassifier()\n\nmodel_lgb.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values)\n\nacc_lgb = accuracy_score(\n    test_df[\"Outcome\"].values, model_lgb.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of LightGBM's Classifier model is {acc_lgb:.6f}\"\n)","ea64bee9":"from catboost import CatBoostClassifier\n\nmodel_cb = CatBoostClassifier()\n\nmodel_cb.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values, silent=True)\n\nacc_cb = accuracy_score(\n    test_df[\"Outcome\"].values, model_cb.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of CatBoost's Classifier model is {acc_cb:.6f}\"\n)","884ce6a1":"from xgboost import XGBClassifier\n\nmodel_xgb = XGBClassifier()\n\nmodel_xgb.fit(train_df.drop([\"Outcome\"], axis=1), train_df[\"Outcome\"].values, verbose = 0)\n\nacc_xgb = accuracy_score(\n    test_df[\"Outcome\"].values, model_xgb.predict(test_df.drop([\"Outcome\"], axis=1))\n)\n\nprint(\n    f\"Test set accuracy of XGBoost's Classifier model is {acc_xgb:.6f}\"\n)","ce7ea200":"models = pd.DataFrame(\n    {\n        \"Model\": [\n            \"TF-DF Random Forest\",\n            \"TF-DF Gradient Boosted Trees\",\n            \"TF-DF CART\",\n            \"Sklearn Random Forest\",\n            \"Sklearn Gradient Boosted Trees\",\n            \"Sklearn Decision Tree\",\n            \"LightGBM Classifier\",\n            \"CatBoost Classifier\",\n            \"XGBoost Classifier\",\n        ],\n        \"Score\": [\n            acc_rf,\n            acc_gbt,\n            acc_cart,\n            acc_rf_sk,\n            acc_gbt_sk,\n            acc_dtc_sk,\n            acc_lgb,\n            acc_cb,\n            acc_xgb,\n        ],\n    }\n)\n\nmodels.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)","b70d7cb7":"## Scikit-learn Models","9538ef8b":"## TF-DF Gradient Boosted Trees Model","72a23e8e":"## PIMA Indians Diabetes Dataset\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. \n\nPredictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","9f04ad3c":"## XGBoost Classifier model","6b968fdb":"## Further reading\n\n- [TensorFlow (TF-DF) Decision Forest on Github](https:\/\/github.com\/tensorflow\/decision-forests)\n- [TensorFlow Decision Forests tutorials](https:\/\/www.tensorflow.org\/decision_forests\/tutorials)","15482e48":"## TF-DF Random Forest Model","6f474b1c":"## LightGBM Classifier model","2e821aef":"## TF-DF CART Model","7c50e649":"## CatBoost Classifier model","b7a75298":"## Accuracy Comparision","a13bf680":"## TensorFlow Decision Forests\n\nTensorFlow Decision Forests (TF-DF) is a collection of state-of-the-art algorithms for the training, serving and interpretation of Decision Forest models. The library is a collection of Keras models and supports classification, regression and ranking.\n\nTF-DF is a wrapper around the Yggdrasil Decision Forest C++ libraries. Models trained with TF-DF are compatible with Yggdrasil Decision Forests' models, and vice versa.\n\nIn this notebook we are going to compare TensorFlow Decision Forests models with Scikit-learn (sklearn) models, LightGBM, CatBoost and XGBoost Classifier models. All the models are run with default parameters and their Accuracy on test set is measured for comparision."}}