{"cell_type":{"25428022":"code","9006d9c7":"code","16982da3":"code","de0cbd02":"code","dec8a73b":"code","7583a90d":"code","589de70c":"code","4b3a9ce8":"code","3c87f9b7":"code","f7a2536c":"code","517f5d46":"code","2ec9794a":"code","3b447af6":"code","c987a439":"code","4d9f7558":"markdown","d2a1a716":"markdown","f5f109a7":"markdown","bf0f6094":"markdown","fcb0a628":"markdown","6143e818":"markdown","155847f5":"markdown","ee6b67fa":"markdown","cceae162":"markdown","342b1a73":"markdown","5ee5a0be":"markdown","9eac1eab":"markdown","42067530":"markdown","26f99380":"markdown","73a11969":"markdown"},"source":{"25428022":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nfrom scipy.stats import multivariate_normal\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\n\nimport matplotlib.pyplot as plt","9006d9c7":"sea.set_style(\"darkgrid\")","16982da3":"mean = [[ 4.0,  2.0],\n        [10.0,  7.5],\n        [ 2.0,  8.5],\n        [ 1.0, -3.5]]\n\ncovariance = [[[9,  0.1], [ 0.1, 1]],\n              [[2, -0.8], [-0.8, 3]],\n              [[1,  0.8], [ 0.8, 2]],\n              [[2,  0.0], [ 0.0, 2]]]\nsize = 1000\ndata = []\nlabel = []\nfor c in range(4):\n    mlt_nrm = multivariate_normal(mean[c], covariance[c])\n    data.extend(mlt_nrm.rvs(size = size, random_state = 0))\n    label.extend(c*np.ones(size))\ndata = np.array(data)\nlabel = np.array(label)","de0cbd02":"scaler = StandardScaler()\ndata_S = scaler.fit_transform(data)","dec8a73b":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nsea.scatterplot(x = data[:,0], y = data[:,1], hue = label,\n                palette = \"rocket\", ax=axes[0]);\nsea.scatterplot(x = data_S[:,0], y = data_S[:,1], hue = label,\n                palette = \"rocket\", ax=axes[1]);\n\naxes[0].set_title(\"Dataset\")\naxes[0].set_ylabel(\"Feature 1\")\naxes[0].set_xlabel(\"Feature 0\");\naxes[1].set_title(\"Standardized Dataset\")\naxes[1].set_ylabel(\"Feature 1\")\naxes[1].set_xlabel(\"Feature 0\");","7583a90d":"SC = []\nWCSS = []\n\nK_set = range(2,11)\nfor K in K_set:\n    kmeans = KMeans(n_clusters=K, random_state=1).fit(data_S)\n    SC.append(metrics.silhouette_score\n                 (data_S, kmeans.labels_, metric=\"euclidean\"))\n    WCSS.append(kmeans.inertia_)\n    \nfig, axes = plt.subplots(constrained_layout = True,\n                         nrows=1, ncols=2, figsize=(10, 5))\n\nsea.lineplot(x = K_set, y = SC, color = \"#111111\", ax=axes[0]);\nsea.lineplot(x = K_set, y = WCSS, color = \"#111111\", ax=axes[1]);\n\naxes[0].axvline(x=4, linestyle = \"--\", color =\"#111111\", alpha = 0.5)\naxes[1].axvline(x=4, linestyle = \"--\", color =\"#111111\", alpha = 0.5)\n\naxes[0].set_ylabel(\"SC\")\naxes[0].set_xlabel(\"K\");\naxes[1].set_ylabel(\"WCSS\")\naxes[1].set_xlabel(\"K\");","589de70c":"kmeans = KMeans(n_clusters=4, random_state=1).fit(data_S)\n\nplt.figure(figsize=(6,6))\nsea.scatterplot(x = data_S[:,0], y = data_S[:,1], hue = kmeans.labels_,\n                palette = \"rocket\");\nplt.legend().remove()\nplt.title(\"Clustered Data with K=4\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\");","4b3a9ce8":"SC = []\nlink = []\nn_clusters = []\n\nlinkage = [\"ward\", \"complete\", \"average\", \"single\"]\nfor l in linkage:\n    for n in range(2, 10, 1):\n        agg = AgglomerativeClustering(linkage=l,\n                                      n_clusters = n).fit(data_S)\n        SC.append(metrics.silhouette_score(data_S, agg.labels_,\n                                      metric=\"euclidean\"))\n        link.append(l)\n        n_clusters.append(n)","3c87f9b7":"print(\" linkage    n_clusters      SC\")\nprint(\"---------  ------------   ------\")\nfor i in np.array(SC).argsort()[::-1][:6]:\n    print(\" {:10}     {}         {:.4f}\".format(link[i],\n                                                n_clusters[i],\n                                                SC[i]))","f7a2536c":"fig, axes = plt.subplots(constrained_layout = True,\n                         nrows=3, ncols=2, figsize=(10, 12))\n\nfor f, i in enumerate(np.array(SC).argsort()[::-1][:6]):\n    agg = AgglomerativeClustering(linkage=link[i],\n                                  n_clusters = n_clusters[i]).fit(data_S)\n    sea.scatterplot(x = data_S[:,0], y = data_S[:,1],\n                    palette = \"rocket\", legend=\"full\",\n                    hue = agg.labels_, ax=axes[f\/\/2][f%2]);\n    axes[f\/\/2][f%2].set_title(\"linkage: {}, n_clusters: {},\\nSC: {:.4f}\". \\\n                              format(link[i],n_clusters[i],SC[i]))","517f5d46":"dbscan = DBSCAN().fit(data_S)\ndbscan.fit(data_S)\nplt.figure(figsize=(6,6))\nsea.scatterplot(x = data_S[:,0], y = data_S[:,1],\n                hue = dbscan.labels_, palette = \"rocket\");\nplt.title(\"DBCSAN with default parameters\");","2ec9794a":"SC = []\neps = []\nmin_samples = []\n\nfor d in np.arange(0.1, 0.2, 0.01):\n    for m in range(10, 30, 2):\n        dbscan = DBSCAN(eps=d, min_samples=m).fit(data_S)\n        SC.append(metrics.silhouette_score(data_S, dbscan.labels_,\n                                           metric=\"euclidean\"))\n        eps.append(d)\n        min_samples.append(m)","3b447af6":"print(\" eps      min_sample      SC\")\nprint(\"------   ------------   ------\")\nfor i in np.array(SC).argsort()[::-1][:6]:\n    print(\" {:.2f}         {}        {:.4f}\".format(eps[i],\n                                                    min_samples[i],\n                                                    SC[i]))","c987a439":"fig, axes = plt.subplots(constrained_layout = True,\n                         nrows=3, ncols=2, figsize=(10, 12))\n\nfor f, i in enumerate(np.array(SC).argsort()[::-1][:6]):\n    dbscan = DBSCAN(eps=eps[i], min_samples=min_samples[i]).fit(data_S)\n    sea.scatterplot(x = data_S[:,0], y = data_S[:,1],\n                    palette = \"rocket\", legend=\"full\",\n                    hue = dbscan.labels_, ax=axes[f\/\/2][f%2]);\n    noc = len(np.unique(dbscan.labels_)) - 1\n    axes[f\/\/2][f%2].set_title(\"eps: {:.2f}, min_samples: {},\\nSC: {:.4f},\"\n                              \" number of detected clusters: {}\". \\\n                              format(eps[i],min_samples[i],SC[i],noc))","4d9f7558":"## Hierarchical Clustering\n\nHierarchical Clustering is a family of methods building nested clusters. Agglomerative Clustering, as an example, is a bottom up approach. At the beginning, each sample is a cluster itself. Then, iteratively clusters are combined. In sklearn, there are 4 linkage strategy for agglomerative clustering.\n\n* Ward minimizes the variance of the clusters\n* Complete linkage minimizes the maximum distance between observations of 2 clusters\n* Average linkage minimizes the average of the distances between observations of 2 clusters\n* Single linkage minimizes the distance between the closest observations of 2 clusters.\n\nNumber of clusters can be given to algorithm using n_clusters. Also distance_threshold can be used to control merging process. The cluster pairs that has distance greater than distance_threshold are not merged. n_clusters must be None if distance_threshold is not None.\n\nWe can start trialling different values for linkage and n_clusters, compute SC for each combination and store parameters and SC in lists.","d2a1a716":"## DBSCAN\n\n**Density Based Spatial Clustering of Applications with Noise** finds core samples of high density and expands clusters from them. Core sample is defined as being a sample in the dataset such that there exist **min_samples** amount of other samples within a distance of **eps**, which are defined as neighbors of the core sample.\n\nDBSCAN does not have **n_clusters** parameter. Let's try DBSCAN with default eps and min_samples parameters.","f5f109a7":"As can be seen, the result is far from being optimal. Instead of default parameters, we can try different values for eps and min_samples, compute SC for each combination and store parameters and SC in lists.","bf0f6094":"Now, we can plot their graphs.","fcb0a628":"Note that, from this point on, data_S will be used without labels. Labels were created just for the purpose of visualization above.","6143e818":"Above plots say that when K=4, clustering performance is maximum for our data. Clustered data is plotted below for K=4.","155847f5":"## K-means\n\n**K-means** algorithm clusters data by minimizing the inertia or within cluster sum of squares (WCSS) iteratively. WCSS is computed by summing squared euclidean distances of all samples to cluster centroids.\n\nK denotes the number of clusters and is a hyperparameter of the method. Centroid of each cluster is the average of all sample coordinates in that cluster. The name K-means originates from this procedure.\n\nAlgorithm steps are:\n\n1. The value of K is determined and K centroids are initialized\n2. For each sample, distances to all centroids are computed\n3. Each sample is assigned to the closest centroid\n4. Centroids are computed again\n5. Steps 2, 3 and 4 are repeated until convergence\n\nK-means converges when centroids don't change or samples remain in the same cluster. Also, the number of iterations may be limited.\n\nPerformance of K-means is directly related to centroid initialization which is also named as seeding. With random initialization, a centroid may not coincide with a data sample, may be far away from true cluster centers or more than 1 centroid may be chosen from the same cluster while no centroid is assigned to some clusters.\n\nInstead of random initialization, a more effective method called K-means++ can be used. When compared with random initialization, K-means++ provides improvement on both speed and error. K-means++ chooses initial centroids from data points and away from each other, distributed over the dataset.\n\n1. First centroid is chosen randomly from data points\n2. Distances of each sample to previously set centroid (or centroids) are computed\n3. As the next centroid, the sample that has the maximum distance to its nearest centroid is chosen.\n4. Steps 2 and 3 are repeated until K centroids are initialized\n\nAfter initialization with K-means++, the rest is regular K-means clustering.\n\n**Silhouette Coefficient (SC)** and **WCSS** can be used to assess K-means performance. In SC computation, first, SC is computed for each sample. Mean distance \\\\(m_s\\\\) between the sample and all other points in the same cluster is computed. Mean distance \\\\(m_n\\\\) between the sample and all points in the next nearest cluster is computed. Using these two, SC for a sample is:\n\n\\begin{equation*}\nsc = \\frac{(m_n-m_s)}{max(m_n,m_s)}\n\\end{equation*}\n\n<br>\nSC for the dataset is the mean of coefficients for all samples. \n\nBelow, we try different K values, measure SC and WCSS for each case and plot its graph. We look for highest SC as the optimal point. For WCSS, elbow method is used to find optimal K. Elbow is the point where drop rate of WCSS changes drastically.","ee6b67fa":"In legends, -1 denotes unclustered noise (black samples). For each subplot, corresponding parameters, SC and number of detected clusters are written on top the subplot.","cceae162":"## Standardization","342b1a73":"**Clustering** is an **unsupervised** machine learning technique aiming to group unlabeled data points into clusters. After clustering, similar data points are expected to reside in the same cluster.\n\nOutline of the work is as follows:\n\n* Data Synthesis\n* Standardization\n* Visualization\n* K-means\n* Hierarchical Clustering\n* DBSCAN","5ee5a0be":"Let's take 6 parameter pairs giving the highest 6 SC.","9eac1eab":"## Data Synthesis\n\nSynthetic data will be used to illustrate the clustering process for different methods. The number of dimensions (features) is chosen to be 2 to ease the visualization. Sklearn sample generators like make_moons, make_blobs or make_circles could be used for this purpose. Also, we can write our own generator function using anisotropic Gaussians. 4 clusters are generated and each one has 1000 samples.","42067530":"## Visualization\n\nPlot data before and after standardization.","26f99380":"Let's take 6 parameter pairs giving the highest 6 SC. **argsort** makes sorting from lowest to highest, **[:,:,-1]** reverses (makes highest to lowest), **[:6]** gets the first 6.","73a11969":"Now, we can plot their graphs."}}