{"cell_type":{"3c9559d3":"code","b1ca8d4b":"code","4b0f7ecf":"code","28b6bda7":"code","bc1e502c":"code","51678fa9":"code","da7225e8":"code","5c4248a1":"code","036bf38f":"code","6b743799":"code","b13ff06c":"code","75c88ea4":"code","a357780c":"code","908a21ec":"code","8d35121b":"code","eb0046ee":"code","f488378d":"code","b35c7729":"code","cf4470dc":"code","3cddebc9":"code","33f1fd50":"code","5171bce9":"code","1eb1bfd3":"code","544a0a5a":"code","f0fedf5d":"code","a894ff73":"code","59a6edfc":"code","582eb828":"code","fedadfe2":"code","c9d15b79":"code","4634a02e":"code","e247dc23":"code","725eadac":"code","7b01eaea":"code","8ec8640c":"code","e4dcaeee":"code","3f08f353":"code","48657ef3":"code","198eec5a":"code","2d0e07bb":"code","18af253c":"code","5584e6d7":"code","475130ae":"code","d7bb4612":"code","f075ec56":"code","790292f4":"code","b6f83b9f":"code","f0ce553e":"code","22210b76":"code","b87e5685":"code","9a9b1020":"code","605c5f1c":"code","e334bde9":"code","7870ce86":"code","69142463":"code","c7df14e4":"code","66bab7d9":"code","979b9f5f":"code","c4b7e3ef":"code","5935f65e":"code","bef40ed0":"code","7c4b1eb1":"code","4abcf3c1":"code","a823f2bb":"code","c92d39d8":"code","d2e87ed7":"code","8dafe25e":"code","bc078f7d":"code","31099286":"code","d5ff7b5e":"code","4e108183":"code","35900d90":"code","a1a37979":"code","d630fd7c":"code","026bb477":"markdown","d4559f8c":"markdown","66ecf5ee":"markdown","6982d7c2":"markdown","b30c9506":"markdown","bf80146d":"markdown","352351e9":"markdown","ffcb3d86":"markdown","3d2ec32d":"markdown","a4e78c25":"markdown","7aa8459d":"markdown","faa6db62":"markdown","45d0becd":"markdown","f5f252ff":"markdown","2d7335ab":"markdown","930bcdbf":"markdown","1dddd139":"markdown","16e833be":"markdown","a70267aa":"markdown","e106911d":"markdown","31b7f15c":"markdown","2d78be79":"markdown","320bee0e":"markdown","3e87451f":"markdown","017e2d71":"markdown","f50bc8f2":"markdown","83ac1d02":"markdown"},"source":{"3c9559d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1ca8d4b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import Lasso #penalises features, helps in selecting features\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)","4b0f7ecf":"dataset=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n## print shape of dataset with rows and columns\nprint(dataset.shape)","28b6bda7":"dataset.head()","bc1e502c":"dataset.info()","51678fa9":"missing_count = dataset.isnull().sum() #This is a pandas Series data structure\n# As we have 81 feature, the names of all the features holding missing values are not clear","da7225e8":"features_with_missing_values = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1]\n\n# printing feature name along with its null value count\nfor feature in features_with_missing_values:\n    print(feature, \" has \", missing_count[feature], \" missing values\")","5c4248a1":"data = dataset.copy() # temporary dataset\n\nfor feature in features_with_missing_values:\n    data[feature] = np.where(data[feature].isnull(), \"missing value\", \"legit value\")\n\n    # mean of Sale Price, for each category, missing and legit\n    data.groupby(feature)['SalePrice'].mean().plot.bar()\n    plt.title(feature)\n    plt.show()\n\n","036bf38f":"# printing the dtype for each column\nfor feature in dataset.columns:\n    print(feature, \" -- \", dataset[feature].dtype)","6b743799":"numerical_features = [feature for feature in dataset.columns if dataset[feature].dtype in ['int64', 'float64']]\n\nprint('the number of numerical columns == ', len(numerical_features))","b13ff06c":"# list of variables that contain year information\nyear_numerical_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n\nyear_numerical_feature","75c88ea4":"# let's explore the unique contents of these year variables\nfor feature in year_numerical_feature:\n    print(feature, len(dataset[feature].unique()))\n    print(feature, dataset[feature].unique())\n    print()","a357780c":"# We will now find the difference between all year features with YrSold feature, and then compare it  with SalePrice \n# to find if any interesting pattern exists or not\nimport seaborn as sns\n\ndata = dataset.copy() #temporary dataset\n\nfor feature in year_numerical_feature:\n    if feature not in ['YrSold']:\n        data[feature] = data['YrSold'] - data[feature]\n        \n        plt.figure(figsize=(10,6))\n        \n        plt.subplot(1,2,1)\n        plt.scatter(data[feature], data['SalePrice'])\n\n        plt.subplot(1,2,2)\n        sns.histplot(data=data, x=feature, y='SalePrice', kde=False)\n        \n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","908a21ec":"discrete_numerical_features = [feature for feature in numerical_features if len(dataset[feature].unique()) < 25 and feature not in year_numerical_feature + ['Id']]\n\ndiscrete_numerical_features","8d35121b":"## Lets see if there is some realtionship between discrete numerical features and Sale PRice\ndata=dataset.copy()\n\nfor feature in discrete_numerical_features:    \n    data.groupby(feature)['SalePrice'].mean().plot.bar() #using mean of data \n    \n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()\n    \n# Observation: Clearly visible, that there exists relationship between discrete numerical variable and target SalePrice","eb0046ee":"continuous_numerical_feature = [feature for feature in numerical_features if feature not in discrete_numerical_features + year_numerical_feature + ['Id']]\nlen(continuous_numerical_feature)","f488378d":"## Lets analyse the continuous values by creating histograms to understand the distribution\ndata = dataset.copy()\n\nfor feature in continuous_numerical_feature:\n    data[feature].hist(bins=25)\n    \n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()\n","b35c7729":"for feature in continuous_numerical_feature:\n    dataset.boxplot(column = feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()\n        \n# clearly we can see presence of outliers for each continuous numerical feature  ","cf4470dc":"# let us use log transformation to get a better picture of outliers\ndata = dataset.copy()\n\nfor feature in continuous_numerical_feature:\n\n    if 0 in data[feature].unique():\n        continue #cause log tranformation of 0 will be infinite\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        \n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","3cddebc9":"# finding columns with categorical features\ncategorical_features = [feature for feature in dataset.columns if dataset[feature].dtypes in ['object']]\n\ncategorical_features","33f1fd50":"# finding how many categories each categorical feature holds\nfor feature in categorical_features:\n    print('The feature {} has {} many categories'.format(feature, len(dataset[feature].unique())))","5171bce9":"## visualizaiton\n# Finding out how the categories within each categorical feature is dependent on SalePrice\ndata = dataset.copy()\n\n# plotting barplots for mean SalePrice in each category\nfor feature in categorical_features:\n    data.groupby(feature)['SalePrice'].mean().plot.bar(rot=0)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()\n    \n# there is a clear relationship among categorical features and mean prices","1eb1bfd3":"# printing head of dataset\ndataset.head()","544a0a5a":"# splitting dependent and independent variables\nX = dataset.drop(['Id', 'SalePrice'], axis=1)\ny = dataset.SalePrice","f0fedf5d":"# Splitting the data into train and test data\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1, test_size=0.1)\n\nprint(x_train.shape)\nprint(x_val.shape)","a894ff73":"# Handling missing values for categorical features \n\ncat_features_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum() >= 1 and dataset[feature].dtypes in ['object']]\nx_train[cat_features_with_nan].isnull().sum()\n# replacing missing categorical features with 'Missing' string\n\ndef replace_cat_features(dataset, features_with_nan):\n    data = dataset.copy()\n    data[features_with_nan] = data[features_with_nan].fillna('Missing')\n    return data\n\nx_train = replace_cat_features(x_train, cat_features_with_nan)\nx_val = replace_cat_features(x_val, cat_features_with_nan)\n","59a6edfc":"x_train[cat_features_with_nan].isnull().sum()\n","582eb828":"x_val[cat_features_with_nan].isnull().sum()","fedadfe2":"# finding columns with numerical values and have missing values\n\nnumerical_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum() >= 1 and dataset[feature].dtypes in ['int64', 'float64']]\n\nlen(numerical_with_nan)","c9d15b79":"# any missing numerical values in train data?\ntrain_num_nan = x_train[numerical_with_nan].isnull().sum()\nfor col in train_num_nan.index:\n    print('{}    {}'.format(col, train_num_nan[col]))","4634a02e":"# Any missing numerical values in test data?\nval_num_nan = x_val[numerical_with_nan].isnull().sum()\nfor col in val_num_nan.index:\n    print('{}    {}'.format(col, val_num_nan[col]))","e247dc23":"# Replacing the numerical Missing values with \"median\", as all the numerical features have outliers.\n\nfor feature in numerical_with_nan:\n# creating a new feature, to capture the rows that have nan values. If nan, then we will assign 0\n    if train_num_nan[feature] > 0:\n#         x_train[feature+'nan'] = np.where(x_train[feature].isnull(), 0, 1)\n        x_train[feature].fillna(x_train[feature].median(), inplace=True)\n\n    if val_num_nan[feature] > 0:\n#         x_val[feature+'nan'] = np.where(x_val[feature].isnull(), 0, 1)\n        x_val[feature].fillna(x_val[feature].median(), inplace=True)","725eadac":"x_train[numerical_with_nan].isnull().sum()","7b01eaea":"x_val[numerical_with_nan].isnull().sum()","8ec8640c":"year_numerical_feature","e4dcaeee":"year_numerical_feature.remove('YrSold')\n\n# replacing temporal variable with time distance\nfor feature in year_numerical_feature:\n    x_train[feature] = x_train['YrSold'] - x_train[feature]\n    x_val[feature] = x_val['YrSold'] - x_val[feature]","3f08f353":"x_train[year_numerical_feature].head()","48657ef3":"continuous_numerical_feature","198eec5a":"### Log Transformation - Numerical Variables\n#### Since numerical variable are skewed we will perform log normal transformation\n\n# # let us use log transformation \n# continuous_numerical_feature.remove('SalePrice')\n# for feature in continuous_numerical_feature:\n#     # handling training data\n#     if 0 in x_train[feature].unique():\n#         continue #cause log tranformation of 0 will be infinite\n#     else:\n#         x_train[feature] = np.log(data[feature])\n    \n#     # handling test data\n#     if 0 in x_val[feature].unique():\n#         continue\n#     else:\n#         x_val[feature] = np.log(data[feature])","2d0e07bb":"x_train.head()","18af253c":"# we will discard categories not seen during fit\n# `handle_unknown` must be 'error' when the drop parameter is specified, as both would create categories that are all zero.\n\n# x_train_cat_encoded = pd.get_dummies(x_train, columns = categorical_features)\n# from sklearn.preprocessing import OneHotEncoder\n\n# Applying one-hot encoder to each categorical column\ncat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\ntrain_encoded = pd.DataFrame(cat_encoder.fit_transform(x_train[categorical_features]))\nval_encoded = pd.DataFrame(cat_encoder.transform(x_val[categorical_features]))\n\n\n# putting back the lost indexes due to one-hot encoding\ntrain_encoded.index = x_train.index\nval_encoded.index = x_val.index\n\n\n# removing categorical columns from original x_train and x_val\nnum_x_train = x_train.drop(categorical_features, axis=1)\nnum_x_val = x_val.drop(categorical_features, axis=1)\n\n\n# adding one-hot encoded columns and numerical features\nencoded_x_train = pd.concat([num_x_train, train_encoded], axis=1)\nencoded_x_val = pd.concat([num_x_val, val_encoded], axis=1)","5584e6d7":"encoded_x_train.head()","475130ae":"encoded_x_val.head()","d7bb4612":"encoded_x_train.shape","f075ec56":"encoded_x_val.shape","790292f4":"scaler_cols = encoded_x_train.columns","b6f83b9f":"# from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# transforming train data\nx_train_encoded_scaled = pd.DataFrame(scaler.fit_transform(encoded_x_train), columns = scaler_cols)\n\n# transforming val data\nx_val_encoded_scaled = pd.DataFrame(scaler.transform(encoded_x_val), columns = scaler_cols)","f0ce553e":"x_train_encoded_scaled.head()","22210b76":"from sklearn.linear_model import Lasso #penalises features, helps in selecting features\nfrom sklearn.feature_selection import SelectFromModel\n\n# feature selection from training data\nfeature_sel_x_train = SelectFromModel(Lasso(alpha = 0.01, random_state=1, max_iter=10000000)) # I had to increate the max_iter from 1000\nfeature_sel_x_train.fit(x_train_encoded_scaled, y_train)","b87e5685":"# a mask, or integer index, of the features selected\nfeature_sel_x_train.get_support()","9a9b1020":"# Let's print the selected features\nselected_features = x_train_encoded_scaled.columns[(feature_sel_x_train.get_support())]\nselected_features\n","605c5f1c":"# a comparison on features selected\nprint('total features: {}'.format(x_train_encoded_scaled.shape[1]))\nprint('selected features: {}'.format(len(selected_features)))","e334bde9":"x_train_final = x_train_encoded_scaled[selected_features]\nx_val_final = x_val_encoded_scaled[selected_features]","7870ce86":"from scipy.stats import uniform, randint\nimport xgboost as xgb\n\nparams = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n \"n_estimators\": [100, 120, 135, 150, 165, 200] }\n\nxgb_model = xgb.XGBRegressor()\n\n\n# from sklearn.model_selection import GridSearchCV\n\n# regressor = GridSearchCV(xgb_model, param_grid = params, n_jobs =-1)\n# Grid search CV was taking a lot of time\n\nfrom sklearn.model_selection import RandomizedSearchCV\nregressor = RandomizedSearchCV(xgb_model, param_distributions = params, random_state=1, n_iter=200, cv=3, verbose=1, \n                   n_jobs=1, return_train_score=True)\n\nregressor.fit(x_train_final, y_train)\n\nregressor.best_params_","69142463":"xgb_model = xgb.XGBRegressor(n_estimators = 200,\n min_child_weight = 5,\n max_depth = 5,\n learning_rate = 0.1,\n gamma = 0.2,\n colsample_bytree = 0.3)\n\nxgb_model.fit(x_train_final, y_train)\n\n# printing the metrics\nfrom sklearn.metrics import mean_absolute_error\n\ny_predict = xgb_model.predict(x_val_final)\nprint(mean_absolute_error(y_val, y_predict))","c7df14e4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","66bab7d9":"print('train-size = {} \\ntest-size = {}'.format(train.shape, test.shape))","979b9f5f":"# splitting train data into dependent and indenpendent features\nX = train.drop('SalePrice', axis=1)\ny = train.SalePrice","c4b7e3ef":"y.isnull().sum() #no missing values of target variable","5935f65e":"# Handling missing values for categorical features \n# replacing missing categorical features with 'Missing' string\ndef replace_cat_features(dataset):\n    cat_features_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum() >= 1 and dataset[feature].dtypes in ['object']]\n    \n    data = dataset.copy()\n    data[cat_features_with_nan] = data[cat_features_with_nan].fillna('Missing')\n    return data\n\nX = replace_cat_features(X)\ntest = replace_cat_features(test)","bef40ed0":"def replace_num_features(dataset):\n    numerical_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum() >= 1 and dataset[feature].dtypes in ['int64', 'float64']]\n    # Replacing the numerical Missing values with \"median\", as all the numerical features have outliers.\n    data = dataset.copy()\n    \n    for feature in numerical_with_nan:\n        data[feature].fillna(data[feature].median(), inplace=True)\n    \n    return data\n\nX = replace_num_features(X)\ntest = replace_num_features(test)\n","7c4b1eb1":"year_numerical_feature","4abcf3c1":"# replacing temporal variable with time distance\ndef replace_temporal_features(dataset):\n    data = dataset.copy()\n    \n    for feature in year_numerical_feature:\n        data[feature] = data['YrSold'] - data[feature]\n    \n    return data\n\nX = replace_temporal_features(X)\ntest = replace_temporal_features(test)","a823f2bb":"categorical_features = [feature for feature in train.columns if dataset[feature].dtypes in ['object']]\nlen(categorical_features)","c92d39d8":"# we will discard categories not seen during fit\n# `handle_unknown` must be 'error' when the drop parameter is specified, as both would create categories that are all zero.\n\n# Applying one-hot encoder to each categorical column\ncat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\ntrain_encoded = pd.DataFrame(cat_encoder.fit_transform(X[categorical_features]), index = train.index)\ntest_encoded = pd.DataFrame(cat_encoder.transform(test[categorical_features]), index = test.index)\n\n\n# removing categorical columns from original x_train and x_val\nnum_train = X.drop(categorical_features, axis=1)\nnum_test = test.drop(categorical_features, axis=1)\n\n\n# adding one-hot encoded columns and numerical features\nencoded_train = pd.concat([num_train, train_encoded], axis=1)\nencoded_test = pd.concat([num_test, test_encoded], axis=1)","d2e87ed7":"print(encoded_train.shape)\nprint(encoded_test.shape)","8dafe25e":"scaler_cols = encoded_train.columns\n\nscaler = MinMaxScaler()\n# transforming train data\ntrain_encoded_scaled = pd.DataFrame(scaler.fit_transform(encoded_train), columns = scaler_cols)\n\n# transforming test data\ntest_encoded_scaled = pd.DataFrame(scaler.transform(encoded_test), columns = scaler_cols)","bc078f7d":"# feature selection from training data\nfeature_sel_model = SelectFromModel(Lasso(alpha = 0.01, random_state=1, max_iter=10000000)) # I had to increate the max_iter from 1000\nfeature_sel_model.fit(train_encoded_scaled, y)","31099286":"# Let's print the selected features\nselected_features = train_encoded_scaled.columns[(feature_sel_model.get_support())]\nselected_features","d5ff7b5e":"# a comparison on features selected\nprint('total features: {}'.format(train_encoded_scaled.shape[1]))\nprint('selected features: {}'.format(len(selected_features)))","4e108183":"train_final = train_encoded_scaled[selected_features]\ntest_final = test_encoded_scaled[selected_features]","35900d90":"params = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n \"n_estimators\": [100, 120, 135, 150, 165, 200] }\n\nxgb_model = xgb.XGBRegressor()\n\n\nregressor = RandomizedSearchCV(xgb_model, param_distributions = params, random_state=1, n_iter=200, cv=3, verbose=1, \n                   n_jobs=1, return_train_score=True)\n\nregressor.fit(train_final, y)\n\nregressor.best_params_","a1a37979":"xgb_model_final = xgb.XGBRegressor(n_estimators = 165,\n min_child_weight = 1,\n max_depth = 4,\n learning_rate = 0.1,\n gamma = 0.1,\n colsample_bytree = 0.3)\n\nxgb_model_final.fit(train_final, y)\n","d630fd7c":"y_predict = xgb_model_final.predict(test_final)\n\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\noutput = pd.DataFrame({'Id': sample_submission.Id,'SalePrice': y_predict})\noutput.to_csv('.\/submission.csv', index=False)","026bb477":"### Continuous Numerical Features","d4559f8c":"#### Handling numeric missing values","66ecf5ee":"In Data Analysis\n* Missing Values\n* All The Numerical Variables\n* Distribution of the Numerical Variables\n* Categorical Variables\n* Cardinality of Categorical Variables\n* Outliers\n* Relationship between independent and dependent feature(SalePrice)","6982d7c2":"### Feature Scaling: using min_max_scaler (normalisation)","b30c9506":"#### Handing categorical missing values","bf80146d":"### Model Creation for training - validation data","352351e9":"# Categorical Features","ffcb3d86":"## Model Creation","3d2ec32d":"# Model builidng - Train and Test data","a4e78c25":"# Missing Values","7aa8459d":"# Numeric Variables","faa6db62":"### Handling Missing Values","45d0becd":"### Encoding Categorical Features","f5f252ff":"### Encoding the Categorical Features","2d7335ab":"#### Handling Temporal features (date - time variable)\n##### Measuring time distance ==> subtracting each temporal variable from \"YearSold\" feature","930bcdbf":"#### Handling numerical missing values","1dddd139":"### Temporal Variables (eg: Datetime Variable)\n#### we generally extract info like no. of years or no. of days. E.g., difference between year the house was sold and the house was built","16e833be":"### Finding relationship between columns with missing values and target variable (SalePrice).\n#### This will help us to discover if there is any relationship between missing values  and the SalePrice.\n#### Observation: For some features, the mean price for \"missing value\" is higher than \"legit values\"","a70267aa":"#### Handing categorical missing values","e106911d":"### Outliers in continuous numerical features","31b7f15c":"# Featue Engineering\n\n* Steps involved in  Feature Engineering\n1. Handling Missing values\n2. Handling Temporal variables\n3. Categorical feature encoding\n4. Feature Scaling: Standarise the values of the variables to the same range","2d78be79":"### Feature Scaling: using min-max scaler (Normalisation)","320bee0e":"#### Handling Temporal features (date - time variable)\n##### Measuring time distance ==> subtracting each temporal variable from \"YearSold\" feature","3e87451f":"### Feature Selection\n#### The bigger the alpha for Lasso, less features gets selected\n#### SelectFromModel selects features whose coefficients are non-zero","017e2d71":"### Discrete Numerical variables\n#### features which have less than 25 unique values, and feature is not in year_numerical features, and definitely not the Id feature","f50bc8f2":"### Feature Selection\n#### The bigger the alpha for Lasso, less features gets selected\n#### SelectFromModel selects features whose coefficients are non-zero","83ac1d02":"## Splitting the data beforehand to avoid any train-test data leak"}}