{"cell_type":{"7fa76d1e":"code","0da7c69c":"code","a86dc76b":"code","70b14f00":"code","c333f720":"code","26974e46":"code","c11452e5":"code","cc6a5e3c":"code","cd608eeb":"code","19fba7d3":"code","f3782757":"code","937b1ad5":"code","4cd03578":"markdown","53c0b9f4":"markdown","6f258507":"markdown","eab68e72":"markdown","fd1999ff":"markdown","a345269e":"markdown","ac267b61":"markdown","a4e4eac8":"markdown","1226a3a7":"markdown","cccd906f":"markdown","8a206166":"markdown"},"source":{"7fa76d1e":"DEBUG = False\nIMAGE_NUM = 1000 if DEBUG else 1_300_000\nEXAMPLE_NUM = 2 if DEBUG else 5\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport cv2\nimport imageio\nimport os\nimport sys\nimport re\nimport seaborn as sns\nimport time\nimport random\nimport pickle\n       \nSEED = round(time.time())\nprint(f'SEED: {SEED}')\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","0da7c69c":"# sorting again after sampling seems to make the file access slightly faster for large IMAGE_NUM\ntrain_ids = pd.read_csv('\/kaggle\/input\/bms-molecular-translation\/train_labels.csv', dtype={'image_id': 'string', 'InChI': 'string'}).sample(n=IMAGE_NUM).sort_values(by='image_id', ignore_index=True).image_id\ntest_ids = pd.read_csv('\/kaggle\/input\/bms-molecular-translation\/sample_submission.csv', usecols=['image_id'], dtype={'image_id': 'string'}).sample(n=IMAGE_NUM).sort_values(by='image_id', ignore_index=True).image_id","a86dc76b":"def crop(img, contour_min_size=2, small_stuff_size=2, small_stuff_dist=5, pad_pixels=1, debug=False, my_figsize=(12,6), horizontal=True):\n    \n    # idea: pad with contour_min_size pixels just in case we cut off\n    #       a small part of the structure that is separated by a missing pixel\n    \n    # rotate counter clockwise to get horizontal images\n    h, w = img.shape\n    if h > w:\n        img = np.rot90(img)\n    \n    if debug:\n        if horizontal:\n            fig, ax = plt.subplots(1,2, figsize=my_figsize)\n        else:\n            fig, ax = plt.subplots(2,1, figsize=my_figsize)\n        ax[0].imshow(img)\n        ax[0].set_title(f'original image, shape: {img.shape}', size=16)\n        \n    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n    \n    small_stuff = []\n    \n    x_min0, y_min0, x_max0, y_max0 = np.inf, np.inf, 0, 0\n    for cnt in contours:\n        if len(cnt) < contour_min_size:  # ignore contours under contour_min_size pixels\n            continue\n        x, y, w, h = cv2.boundingRect(cnt)\n        if w <= small_stuff_size and h <= small_stuff_size:  # collect position of small contours starting with contour_min_size pixels\n            small_stuff.append([x, y, x+w, y+h])\n            continue\n        x_min0 = min(x_min0, x)\n        y_min0 = min(y_min0, y)\n        x_max0 = max(x_max0, x + w)\n        y_max0 = max(y_max0, y + h)\n        \n    x_min, y_min, x_max, y_max = x_min0, y_min0, x_max0, y_max0\n    \n    # enlarge the found crop box if it cuts out small stuff that is very close by\n    for i in range(len(small_stuff)):\n        if small_stuff[i][0] < x_min0 and small_stuff[i][0] + small_stuff_dist >= x_min0:\n             x_min = small_stuff[i][0]\n        if small_stuff[i][1] < y_min0 and small_stuff[i][1] + small_stuff_dist >= y_min0:\n             y_min = small_stuff[i][1]\n        if small_stuff[i][2] > x_max0 and small_stuff[i][2] - small_stuff_dist <= x_max0:\n             x_max = small_stuff[i][2]\n        if small_stuff[i][3] > y_max0 and small_stuff[i][3] - small_stuff_dist <= y_max0:\n             y_max = small_stuff[i][3]\n                             \n    if pad_pixels > 0:  # make sure we get the crop within a valid range\n        y_min = max(0, y_min-pad_pixels)\n        y_max = min(img.shape[0], y_max+pad_pixels)\n        x_min = max(0, x_min-pad_pixels)\n        x_max = min(img.shape[1], x_max+pad_pixels)\n        \n    img_cropped = img[y_min:y_max, x_min:x_max]\n    \n    if debug:\n        ax[1].imshow(img_cropped)\n        ax[1].set_title(f'cropped image, shape: {img_cropped.shape}', size=16)\n        plt.show()\n    \n    return img_cropped","70b14f00":"def check_cropping(image_id, folder='train', my_figsize=(12,6), horizontal=True):\n    print(f'{folder}\/{image_id}')\n    file_path =  f'\/kaggle\/input\/bms-molecular-translation\/{folder}\/{image_id[0]}\/{image_id[1]}\/{image_id[2]}\/{image_id}.png'\n    img = 255 - cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n    img = crop(img, debug=True, my_figsize=my_figsize, horizontal=horizontal)","c333f720":"dummy = [check_cropping(image_id, folder='train') for image_id in train_ids[:EXAMPLE_NUM]]","26974e46":"dummy = [check_cropping(image_id, folder='test') for image_id in test_ids[:EXAMPLE_NUM]]","c11452e5":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\n\ndef analyse_img_sizes(image_ids, folder='train', plots=False, w_large=500, h_large=250, very_large_factor=1.5):\n    ws = []\n    hs = []\n    ws_c = []\n    hs_c = []\n    fs = []\n    for image_id in tqdm(image_ids):\n        file_path =  f'\/kaggle\/input\/bms-molecular-translation\/{folder}\/{image_id[0]}\/{image_id[1]}\/{image_id[2]}\/{image_id}.png'\n        file_size = os.path.getsize(file_path) \n        fs.append(file_size)\n        img = 255 - cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # '255 -' need for cropping to work\n\n        h, w = img.shape\n        if h > w:\n            h, w = w, h\n        ws.append(w)\n        hs.append(h)\n\n        img_cropped = crop(img)\n        h_c, w_c = img_cropped.shape\n        if h_c > w_c:\n            h_c, w_c = w_c, h_c\n        ws_c.append(w_c)\n        hs_c.append(h_c)\n\n    img_info = pd.DataFrame({'image_id': image_ids, 'file_size': fs, 'width': ws, 'width_crop': ws_c, 'height': hs, 'height_crop': hs_c})\n    \n    img_info['area'] = img_info.width * img_info.height\n    img_info['area_crop'] = img_info.width_crop * img_info.height_crop\n    img_info['ratio'] = img_info.width \/ img_info.height\n    img_info['ratio_crop'] = img_info.width_crop \/ img_info.height_crop\n        \n    img_info_large = img_info.loc[np.logical_or(img_info.width_crop > w_large, img_info.height_crop > h_large),:]\n    \n    img_info_very_large = img_info.loc[np.logical_or(img_info.width_crop > very_large_factor*w_large, img_info.height_crop > very_large_factor*h_large),:]\n        \n    print(f'statistics for all images')\n    display(img_info.describe())\n    print()\n    print(f\"statistics for 'large' images with cropped width > {w_large} or height > {h_large} ({len(img_info_large)\/len(img_info)*100:.3}%):\")\n    display(img_info_large.describe())\n    print()\n    print(f\"statistics for 'very large' images with cropped width > {very_large_factor*w_large} or height > {very_large_factor*h_large} ({len(img_info_very_large)\/len(img_info)*100:.3}%):\")\n    display(img_info_very_large.describe())\n    \n    if plots:\n        print()\n        print(f\"plots for 'large' and 'very large' images only\")\n        plot_info =  img_info_large\n        sns.jointplot(data=plot_info, x='width', y='height', kind='hist')\n        sns.jointplot(data=plot_info, x='file_size', y='area', kind='hist')\n        sns.jointplot(data=plot_info, x='width', y='width_crop', kind='hist')\n        sns.jointplot(data=plot_info, x='height', y='height_crop', kind='hist')\n        sns.jointplot(data=plot_info, x='width_crop', y='height_crop', kind='hist')\n        sns.jointplot(data=plot_info, x='ratio', y='ratio_crop', kind='hist')\n        sns.jointplot(data=plot_info, x='area_crop', y='ratio_crop', kind='hist')\n\n    return img_info","cc6a5e3c":"train_img_info = analyse_img_sizes(train_ids, folder='train', plots=not DEBUG)\n\nwith open('train_img_info.pkl', 'wb') as handle:\n    pickle.dump(train_img_info, handle)","cd608eeb":"test_img_info = analyse_img_sizes(test_ids, folder='test', plots=not DEBUG)\n\nwith open('test_img_info.pkl', 'wb') as handle:\n    pickle.dump(test_img_info, handle)","19fba7d3":"def plot_extreme_images(img_info, folder='train', my_figsize = (20, 10)):\n    img_info_width = img_info.sort_values(by='width_crop', ignore_index=True)[:EXAMPLE_NUM]\n    img_info_height = img_info.sort_values(by='height_crop', ignore_index=True)[:EXAMPLE_NUM]\n    \n    print('very low height images (after swapping if height > width)')\n    [check_cropping(image_id, folder=folder, my_figsize=my_figsize, horizontal=False) for image_id in img_info_height.image_id]\n    \n    print('very low width images (after swapping if height > width)')\n    [check_cropping(image_id, folder=folder, my_figsize=my_figsize) for image_id in img_info_width.image_id]\n\nplot_extreme_images(train_img_info)","f3782757":"plot_extreme_images(test_img_info, folder='test')","937b1ad5":"input_ratios = [1, 1.25, 1.5, 1.75, 1.9, 2, 2.1, 2.25, 2.5]\n\ndef get_res(pixels, ratio):\n    pixels = pixels**0.5\n    ratio = ratio**0.5\n    return (round(pixels*ratio), round(pixels\/ratio))\n\nbase_pixels = 320*320\ninput_sizes = [get_res(base_pixels, r) for r in input_ratios]\n\nbase_pixels = 448*256\ninput_sizes += [get_res(base_pixels, r) for r in input_ratios]\n\nbase_pixels = 512*256\ninput_sizes += [get_res(base_pixels, r) for r in input_ratios]\n\nbase_pixels = 384*384\ninput_sizes += [get_res(base_pixels, r) for r in input_ratios]\n\npixels = [w*h for w, h in input_sizes]\ninput_ratios = [w\/h for w, h in input_sizes]\n\ndef calc_shrink_factors(current_width, current_height, input_size):\n    if current_width < input_size[0] and current_height < input_size[1]:\n        return 1\n    else:\n        return max(input_size[0]\/current_width, input_size[1]\/current_height)\n\ndef check_resolutions(img_info):\n\n    mean_shrink_factors = []  # mean shrink factor (largest of the two factors to decrease image width and\/or height to fit the image into the input size, 1 if image fits already)\n    rms_shrink_factors = []  # root mean square \n    mean_shrink_factors_over_1 = []\n    rms_shrink_factors_over_1 = []\n    fraction_shrinked = []\n\n    for input_size in input_sizes:\n        shrink_factors = np.array([calc_shrink_factors(train_img_info.width_crop[i], train_img_info.height_crop[i], input_size) for i in range(len(train_img_info))])\n        mean_shrink_factors.append(np.mean(shrink_factors))\n        rms_shrink_factors.append(np.mean(shrink_factors**2)**0.5)\n        temp = shrink_factors>1\n        fraction_shrinked.append(np.mean(temp))\n        mean_shrink_factors_over_1.append(np.mean(shrink_factors[temp]))\n        rms_shrink_factors_over_1.append(np.mean(shrink_factors[temp]**2)**0.5)\n        \n    return(pd.DataFrame({'resolution': input_sizes, 'pixels': pixels, 'input_ratio': input_ratios, 'frac_shrinked': fraction_shrinked, \n                        'mean_shr_factor': mean_shrink_factors, 'rms_shr_factor': rms_shrink_factors, 'mean_shr_fac_over_1': mean_shrink_factors_over_1, \n                        'rms_shr_fac_over_1': rms_shrink_factors_over_1}))\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nprint('train images')\ndisplay(check_resolutions(train_img_info))\n\nprint('test images')\ndisplay(check_resolutions(test_img_info))","4cd03578":"# Train image statistics ","53c0b9f4":"# Find best input resolution","6f258507":"# Images with extremly low width or height\n\nThere are some images with extremly low height after cropping. Checking if crop function made a mistake... Seems legit.","eab68e72":"Hi everyone!\n\nI've seen different choices of the image resolution and w\/h ratio so far, some use squares, some rectangles. I did this analysis to learn more about the images we are given, especially after they are cropped. Note that image width and height are swapped if height > width, for orginal versions as well as cropped versions.\n\n**In the end you can find a summary with the fractions of images that need to be 'shrinked' after cropping for different input resolutions together with the mean 'shrink factor' and more statistics for each resolution.**\n\nIn this summary width \/ height ratios of around 2 seems to work best. What ratio and image resolution did you choose as input for your InChI prediction model? What were the reasons?\n\nFeel free to comment below and \/ or leave a vote if you find this kernel helpful :)","fd1999ff":"# Some statistics to choose the input resolution for your InChI predictor\n\nAnalysis of train and test images (random sample) in the [Bristol-Myers Squibb \u2013 Molecular Translation Competition](https:\/\/www.kaggle.com\/c\/bms-molecular-translation) to find a suitable input image ratio and resolution.\n\n**Credits: I adapted the crop function from https:\/\/www.kaggle.com\/markwijkhuizen\/advanced-image-cleaning-and-tfrecord-generation (great TFRecord kernel!)**","a345269e":"# Image analysis function\n\nImage width and height are swapped if height > width, for orginal versions as well as cropped versions.","ac267b61":"The crop function from the original source above was adapted to ignore noise pixels and thus crop the real molecule structure only without removing the noise first.","a4e4eac8":"# Test image statistics ","1226a3a7":"# Adapted crop function","cccd906f":"# Check cropped train images","8a206166":"# Check cropped test images"}}