{"cell_type":{"82c59aeb":"code","5a1fe668":"code","ae228c93":"code","b48ac458":"code","91991d7a":"code","9aa98c00":"code","d73d7eeb":"code","c566efe8":"code","c9afeb7e":"code","1336f40c":"code","02c027cb":"code","ae272ff2":"code","688f7e00":"code","d74f7597":"code","486cf1ff":"markdown","eb0ad284":"markdown","bec3e7c1":"markdown"},"source":{"82c59aeb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a1fe668":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\n\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","ae228c93":"train.head()","b48ac458":"x_train = train.drop(\"label\",axis=1)\ny_train = train[\"label\"]\n\nx_test = test","91991d7a":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,Dropout,BatchNormalization\nfrom tensorflow.keras import layers","9aa98c00":"x_train = np.array(x_train)\nx_test = np.array(x_test)\n\nX_train = x_train.reshape(len(x_train), 28,28,1)\nX_test = x_test.reshape(len(x_test), 28,28,1)\n","d73d7eeb":"y_train = tf.keras.utils.to_categorical(y_train)","c566efe8":"x_train = 1\/255.0","c9afeb7e":"model = Sequential()\nmodel.add(Conv2D(filters=16,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\n\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"softmax\"))","1336f40c":"model.compile(optimizer=\"adam\",metrics=[\"accuracy\"],loss=\"categorical_crossentropy\")\n\nhistory = model.fit( X_train, y_train, batch_size = 300  , epochs = 50)","02c027cb":"pred = model.predict(X_test)","ae272ff2":"res = np.argmax(pred, axis= 1)","688f7e00":"final_data = pd.Series(res, name=\"Label\")\nfinal_data.head()","d74f7597":"sub_file = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),final_data],axis = 1)\nsub_file.to_csv(\"submission.csv\", index=False)","486cf1ff":"# importing data using pandas\n","eb0ad284":"Normalising Data:","bec3e7c1":"Separate data as Training and Test"}}