{"cell_type":{"0d9d13d2":"code","eeb4db25":"code","70cee0db":"code","aa370097":"code","3de92d50":"code","b3333d91":"code","5c05f9c5":"code","d4835500":"code","adfaa194":"code","4dacc4b2":"code","18f24974":"code","0094d77f":"code","0e0d22c6":"code","853f5206":"code","527527f7":"code","c7d08a7a":"code","833fe09a":"code","66f471e1":"code","1e7a16d8":"code","d47c246a":"code","5f672387":"code","234d6a83":"code","3c24a1c8":"markdown","c464aaea":"markdown","56656521":"markdown"},"source":{"0d9d13d2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom tqdm import tqdm\n\nfrom keras.utils import to_categorical","eeb4db25":"dir_data = '..\/input\/flowers-recognition\/flowers'\n\nprint(os.listdir(dir_data)[1:])\ncategory = os.listdir(dir_data)[1:]\n","70cee0db":"for bunga in category:  \n    path = os.path.join(dir_data,bunga)\n    for img in os.listdir(path): \n        img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n        plt.imshow(img_array, cmap='gray')\n        plt.show()\n\n        break \n    break \n\nprint(img_array)","aa370097":"IMG_SIZE = 150\n\nnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\nplt.imshow(new_array, cmap='gray')\nplt.show()","3de92d50":"training_data = []\n\ndef create_training_data():\n    for bunga in category:  # do dogs and cats\n\n        path = os.path.join(dir_data,bunga) \n        class_num = category.index(bunga)\n        \n        for img in tqdm(os.listdir(path)): \n            try:\n                    img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE) \n                    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n                    training_data.append([new_array, class_num])\n            except Exception as e:  # in the interest in keeping the output clean...\n                pass\n        \ncreate_training_data()\n","b3333d91":"import random\n\nrandom.shuffle(training_data)\n\nfor sample in training_data[:10]:\n    print(sample[1])\n\nX = []\ny = []\n\nfor features,label in training_data:\n    X.append(features)\n    y.append(label)\n\n#print(X[:5])\n#print(y[:5])","5c05f9c5":"len(X)","d4835500":"X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny = np.array(y)","adfaa194":"X.shape","4dacc4b2":"y.shape","18f24974":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)\n","0094d77f":"x_train.shape","0e0d22c6":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train = x_train \/ 255.\nx_test = x_test \/ 255.","853f5206":"y_test.shape","527527f7":"y_trainfix = to_categorical(y_train)\ny_testfix = to_categorical(y_test)\nprint(y_trainfix[:5])\nprint(y_testfix[:5])","c7d08a7a":"import keras\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU","833fe09a":"batch_size = 1\nepochs = 20\nnum_classes = 5","66f471e1":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(150,150,1),padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D((2, 2),padding='same'))\nmodel.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='linear'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(Dense(num_classes, activation='softmax'))","1e7a16d8":"model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])","d47c246a":"model.summary()","5f672387":"x_train.shape, y_trainfix.shape","234d6a83":"trainpak = model.fit(x_train, y_trainfix, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_testfix))","3c24a1c8":"The images are of size 150 x 150. The images are converted to an array, rescale it between 0 and 1, reshape it so that it's of size 150 x 150 x 1, and feed this as an input to the network.","c464aaea":"start with the libs","56656521":"then, I trace manually the train data\nI see that the dir name has same name as the category, so I made it as a category "}}