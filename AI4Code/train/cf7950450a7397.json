{"cell_type":{"1f441485":"code","12c66641":"code","df862e4d":"code","6c8b6711":"code","3b62b5e2":"code","43f01458":"code","3b9e0235":"code","e8c0734f":"code","8fa3aab0":"code","8e440a38":"code","9679ca9b":"code","10c90eb4":"code","873fcba6":"code","be7dadbb":"code","4df5dc46":"code","712602fb":"code","51b99562":"code","1ada060c":"code","39b8d214":"code","33b56348":"code","40bcb67e":"code","3b2b30fd":"code","92375c8f":"code","dfa73b00":"code","cf477cb7":"code","5b981c05":"code","7c335656":"code","4f25a02b":"code","23d1f7b7":"code","93c6cfa4":"code","cda5b21e":"code","9e4802dc":"code","83bb8e4b":"code","263ce0bb":"code","3be32d52":"code","28aaa457":"code","a9ab21bb":"code","eb3acc18":"code","053daa43":"code","92ead43f":"code","93d54b85":"code","e9878788":"code","3a35e6da":"code","6d89071a":"code","e89aaed6":"markdown","9136a2a3":"markdown","f0ebc819":"markdown","4b5b5c56":"markdown","23be0d53":"markdown","ec61b560":"markdown","c692dfe9":"markdown","6b838bb1":"markdown","04c638e5":"markdown","508a130e":"markdown","f029e2e7":"markdown","4bb6c9d5":"markdown","d4e70a93":"markdown","3f4e8275":"markdown","fd58b31d":"markdown","ae29209e":"markdown","eabaa5cb":"markdown","28cd1261":"markdown","2dd5e178":"markdown","3f67ccc2":"markdown","75fcaeab":"markdown","0f6a4110":"markdown","1fa7c13f":"markdown","e2797e4f":"markdown","2e48ef8c":"markdown","10f27f7d":"markdown","efe74cea":"markdown","975e6a37":"markdown","4f03eaba":"markdown","14cf6840":"markdown","9baa5398":"markdown","592491ea":"markdown","e3588983":"markdown","aad95512":"markdown","ada02526":"markdown","198fedb0":"markdown","dc2397c4":"markdown","9ee65a22":"markdown","2c5a3b90":"markdown"},"source":{"1f441485":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport librosa\nimport librosa.display\nimport IPython.display as display\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12c66641":"path = '\/kaggle\/input\/birdclef-2021\/'\nos.listdir(path)","df862e4d":"# Helper Functions to Understand the Data\ndef read_ogg_file(path, file):\n    \"\"\" Read ogg audio file and return numpay array and samplerate\"\"\"\n    \n    data, samplerate = sf.read(path+file)\n    return data, samplerate\n\n\ndef plot_audio_file(data, samplerate):\n    \"\"\" Plot the audio data\"\"\"\n    \n    sr = samplerate\n    fig = plt.figure(figsize=(8, 4))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    plt.plot(x, y, color='red')\n    plt.legend(loc='upper center')\n    plt.grid()\n    \n    \ndef plot_spectrogram(data, samplerate):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    \n    sr = samplerate\n    spectrogram = librosa.feature.melspectrogram(data, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","6c8b6711":"train_labels = pd.read_csv(path+'train_soundscape_labels.csv')\ntrain_meta = pd.read_csv(path+'train_metadata.csv')\ntest_data = pd.read_csv(path+'test.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","3b62b5e2":"print('Number train label samples:', len(train_labels))\nprint('Number train meta samples:', len(train_meta))\nprint('Number train short folder:', len(os.listdir(path+'train_short_audio')))\nprint('Number train audios:', len(os.listdir(path+'train_soundscapes')))\nprint('Number test samples:', len(test_data))","43f01458":"os.listdir(path+'train_short_audio\/caltow')[:2]","3b9e0235":"train_labels.head()","e8c0734f":"train_meta.head()","8fa3aab0":"row = 0\ntrain_meta.iloc[row]","8e440a38":"label = train_meta.loc[row, 'primary_label']\nfilename = train_meta.loc[row, 'filename']\n\n# Check if the file is in the folder\nfilename in os.listdir(path+'train_short_audio\/'+label)","9679ca9b":"data, samplerate = sf.read(path+'train_short_audio\/'+label+'\/'+filename)\nprint(data[:8])\nprint(samplerate)","10c90eb4":"plot_audio_file(data, samplerate)","873fcba6":"plot_spectrogram(data, samplerate)","be7dadbb":"display.Audio(path+'train_short_audio\/'+label+'\/'+filename)","4df5dc46":"train_labels['audio_id'].unique()","712602fb":"train_labels.groupby(by=['audio_id']).count()['birds'][:4]","51b99562":"print('original label:', train_labels.loc[458, 'birds'])\nprint('split into list:', train_labels.loc[458, 'birds'].split(' '))","1ada060c":"labels = []\nfor row in train_labels.index:\n    labels.extend(train_labels.loc[row, 'birds'].split(' '))\nlabels = list(set(labels))\n\nprint('Number of unique bird labels:', len(labels))","39b8d214":"df_labels_train = pd.DataFrame(index=train_labels.index, columns=labels)\nfor row in train_labels.index:\n    birds = train_labels.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_train.loc[row, bird] = 1\ndf_labels_train.fillna(0, inplace=True)\n\n# We set a dummy value for the target label in the test data because we will need for the Data Generator\ntest_data['birds'] = 'nocall'\n\ndf_labels_test = pd.DataFrame(index=test_data.index, columns=labels)\nfor row in test_data.index:\n    birds = test_data.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_test.loc[row, bird] = 1\ndf_labels_test.fillna(0, inplace=True)","33b56348":"df_labels_train.sum().sort_values(ascending=False)[:10]","40bcb67e":"train_labels = pd.concat([train_labels, df_labels_train], axis=1)\ntest_data = pd.concat([test_data, df_labels_test], axis=1)","3b2b30fd":"file = os.listdir(path+'train_soundscapes')[0]\nfile","92375c8f":"data, samplerate = read_ogg_file(path+'train_soundscapes\/', file)","dfa73b00":"audio_id = file.split('_')[0]\nsite = file.split('_')[1]\nprint('audio_id:', audio_id, ', site:', site)","cf477cb7":"train_labels[(train_labels['audio_id']==int(audio_id)) & (train_labels['site']==site) & (train_labels['birds']!='nocall')]","5b981c05":"sub_data = data[int(455\/5)*160000:int(460\/5)*160000]","7c335656":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(sub_data, sr=samplerate)\nplt.grid()\nplt.show()","4f25a02b":"display.Audio(sub_data, rate=samplerate)","23d1f7b7":"data_lenght = 160000\naudio_lenght = 5\nnum_labels = len(labels)","93c6cfa4":"batch_size = 16","cda5b21e":"list_IDs_train, list_IDs_val = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\nlist_IDs_test = list(samp_subm.index)","9e4802dc":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, data, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.data = data\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        X = X.reshape((self.batch_size, 100, 1600\/\/2))\n        return X, y\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, data_lenght\/\/2))\n        y = np.zeros((self.batch_size, num_labels))\n        for i, ID in enumerate(list_IDs_temp):\n            prefix = str(self.data.loc[ID, 'audio_id'])+'_'+self.data.loc[ID, 'site']\n            file_list = [s for s in os.listdir(self.path) if prefix in s]\n            if len(file_list) == 0:\n                # Dummy for missing test audio files\n                audio_file_fft = np.zeros((data_lenght\/\/2))\n            else:\n                file = file_list[0]#[s for s in os.listdir(self.path) if prefix in s][0]\n                audio_file, audio_sr = read_ogg_file(self.path, file)\n                audio_file = audio_file[int((self.data.loc[ID, 'seconds']-5)\/audio_lenght)*data_lenght:int(self.data.loc[ID, 'seconds']\/audio_lenght)*data_lenght]\n                audio_file_fft = np.abs(np.fft.fft(audio_file)[: len(audio_file)\/\/2])\n                # scale data\n                audio_file_fft = (audio_file_fft-audio_file_fft.mean())\/audio_file_fft.std()\n            X[i, ] = audio_file_fft\n            y[i, ] = self.data.loc[ID, self.data.columns[5:]].values\n        return X, y","83bb8e4b":"train_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_train, train_labels, batch_size)\nval_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_val, train_labels, batch_size)\ntest_generator = DataGenerator(path+'test_soundscapes\/', list_IDs_test, test_data, batch_size)","263ce0bb":"epochs = 2\nlernrate = 2e-3","3be32d52":"model = Sequential()\nmodel.add(Conv1D(64, input_shape=(100, 1600\/\/2,), kernel_size=5, strides=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(num_labels, activation='sigmoid'))","28aaa457":"model.compile(optimizer = Adam(lr=lernrate),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])","a9ab21bb":"model.summary()","eb3acc18":"history = model.fit_generator(generator=train_generator, validation_data=val_generator, epochs = epochs, workers=4)","053daa43":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nfig.subplots_adjust(hspace = .2, wspace=.2)\naxs = axs.ravel()\nloss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\naxs[0].plot(epochs, loss, 'bo', label='loss_train')\naxs[0].plot(epochs, loss_val, 'ro', label='loss_val')\naxs[0].set_title('Value of the loss function')\naxs[0].set_xlabel('epochs')\naxs[0].set_ylabel('value of the loss function')\naxs[0].legend()\naxs[0].grid()\nacc = history.history['binary_accuracy']\nacc_val = history.history['val_binary_accuracy']\naxs[1].plot(epochs, acc, 'bo', label='accuracy_train')\naxs[1].plot(epochs, acc_val, 'ro', label='accuracy_val')\naxs[1].set_title('Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Value of accuracy')\naxs[1].legend()\naxs[1].grid()\nplt.show()","92ead43f":"y_pred = model.predict_generator(test_generator, verbose=1)","93d54b85":"y_test = np.where(y_pred > 0.5, 1, 0)","e9878788":"for row in samp_subm.index:\n    string = ''\n    for col in range(len(y_test[row])):\n        if y_test[row][col] == 1:\n            if string == '':\n                string += labels[col]\n            else:\n                string += ' ' + labels[col]\n    if string == '':\n        string = 'nocall'\n    samp_subm.loc[row, 'birds'] = string","3a35e6da":"output = samp_subm\noutput.to_csv('submission.csv', index=False)","6d89071a":"output","e89aaed6":"<center><h2>Understand The Competition<\/center><\/h2>\nThe LifeCLEF Bird Recognition Challenge (BirdCLEF) focuses on developing machine learning algorithms to identify avian vocalizations in continuous soundscape data to aid conservation efforts worldwide. Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity.","9136a2a3":"#### Analyse Training","f0ebc819":"#### Focus On Labels\n<b>The target label birds is a space delimited list of any bird songs present in the 5 second window. So we have to encode the labels. Therefor we look on an example with 3 different birds<\/b>","4b5b5c56":"<b>We encode the labels and write them into a data frame<\/b>","23be0d53":"Test the Data Generator","ec61b560":"Display the audio of the file:","c692dfe9":"### Data Details","6b838bb1":"We want to extract the first example with the id 1771. This bird we can here from 455 seconds to 460 seconds.  ","04c638e5":"For the Data Generator we want to define in the next step we need additional parameters:","508a130e":"Generate target label string:","f029e2e7":"Plot [spectrogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) with mel scaling:","4bb6c9d5":"### Load Data","d4e70a93":"#### Focus On Example\n<b>We focus on an example. The first audio file is named by<\/b>","3f4e8275":"So we have to split the long audio into 120 small audio.","fd58b31d":"<b>Listen to the bird<\/b>","ae29209e":"#### Export","eabaa5cb":"### Data Description\nYour challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations. This competition builds on the previous one by adding soundscapes from new locations, more bird species, richer metadata about the test set recordings, and soundscapes to the train set.\n<p><\/p>\n\n### Files\n<b>train_short_audio<\/b> - The bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org.\n<p><\/p>\n\n<b>train_soundscapes<\/b> - Audio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ogg format. The test set also has soundscapes from the two recording locations represented here.\n<p><\/p>\n<b>test_soundscapes<\/b> - When you submit a notebook, the test_soundscapes directory will be populated with approximately 80 recordings to be used for scoring. These will be roughly 10 minutes long and in ogg audio format. The file names include the date the recording was taken, which can be especially useful for identifying migratory birds.\n<p><\/p>\nThis folder also contains text files with the name and approximate coordinates of the recording location plus a csv with the set of dates the test set soundscapes were recorded.\n<p><\/p>\n<b>test.csv<\/b> - Only the first three rows are available for download; the full test.csv is in the hidden test set.\n\nrow_id: ID code for the row.\n\nsite: Site ID.\n\nseconds: the second ending the time window\n\naudio_id: ID code for the audio file.\n<p><\/p>\n\n<b>train_metadata.csv<\/b> - A wide range of metadata is provided for the training data. The most directly relevant fields are:\n\nebird_code: a code for the bird species. You can review detailed information about the bird codes by appending the code to https:\/\/ebird.org\/species\/, such as https:\/\/ebird.org\/species\/amecro for the American Crow.\n\nrecodist: the user who provided the recording.\n\nsite: where the recording was taken. Some bird species may have local call 'dialects,' so you may want to seek geographic diversity in your training data.\n\ndate: while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.\n\nfilename: the name of the associated audio file.\n<p><\/p>\n\n<b>train_soundscape_labels.csv -<\/b>\n\nrow_id: ID code for the row.\n\nsite: Site ID.\n\nseconds: the second ending the time window\n\naudio_id: ID code for the audio file.\n\nbirds: space delimited list of any bird songs present in the 5 second window. The label nocall means that no call occurred.\n<p><\/p>\n\n<b>sample_submission.csv -<\/b> A properly formed sample submission file. Only the first three rows are public, the remainder will be provided to your notebook as part of the hidden test set.\n\nrow_id\n\nbirds: space delimited list of any bird songs present in the 5 second window. If there are no bird calls, use the label nocall.","28cd1261":"#### References\n\nhttps:\/\/www.kaggle.com\/drcapa\/birdclef-2021-starter","2dd5e178":"<b>Finally we merge the labels with the original data<\/b>","3f67ccc2":"Load the data and samplerate:","75fcaeab":"<b>Each audio file consists of 120 birds with a lenth of 5 seconds<\/b>","0f6a4110":"We focus on the samples with the label birds unequal to nocall. There are 4 samples","1fa7c13f":"#### Train, Val And Test Data","e2797e4f":"<b>Plot the audio array<\/b>","2e48ef8c":"Set all values grather than 0.5 to 1:","10f27f7d":"<b>This representation of the labels we can use for further analysis. In instance for the distribution of the bird labels. We show the top 10 of the most observations<\/b>","efe74cea":"<h1><center>BirdCLEF 2021 - Birdcall Identification<\/h1>\n<h2><center>Identify bird calls in soundscape recordings<\/h2>\n\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25954\/logos\/header.png)\n\n","975e6a37":"We extract to features, the primary label which is the name of the folder where the audio file is stored and the filename:","4f03eaba":"#### Audio Data Generator\nWe use a Data Generator to load the data on demand.","14cf6840":"<h2><center>Data Analysis and Transformation<\/h2>\n<b>Our challenge is to identify which birds are calling in **long** recordings.There are 20 long audio files in the folder train_soundscapes. And there are also 20 unique audio ids<\/b>","9baa5398":"<b>Load the data and samplerate<\/b>","592491ea":"<h2><center>Model<\/h2>\nBased on the EDA we define some parameters:","e3588983":"#### Predict Test Data","aad95512":"## Model Defination","ada02526":"#### We extract all label of the train data","198fedb0":"<b>The numpy array has a lenght of 19,200,000. So every sample consists of 160,000 values. These 160,000 values describes 5 seconds of the audio file.<\/b>\n\n<b>We split the file name into the audio_id and site<\/b>","dc2397c4":"<center><h2>Competition Metric<\/center><\/h2>\nSubmissions will be evaluated based on their row-wise micro averaged <b>F1 score<\/b>.","9ee65a22":"### A Sample File\nWe focus on the sample in the first row of the train meta data.","2c5a3b90":"<h2><center>Exploratory Data Analysis<\/h2>"}}