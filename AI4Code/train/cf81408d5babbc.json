{"cell_type":{"73353f99":"code","4d45b9b4":"code","a524d45b":"code","ece7e066":"code","82ec9a2b":"code","adac4dff":"code","35c1b667":"code","33b4f8c0":"code","cd878328":"code","2a99be8f":"code","59937c8f":"code","5b136e60":"code","d0a3a903":"code","fa1f681f":"code","68859ba1":"code","e2f85535":"code","b3d36cd8":"code","9ea096ee":"code","48c60cb3":"code","7989b343":"code","96c215aa":"code","983c5c8a":"code","34a73551":"code","68bb3137":"code","11054688":"code","b1990df7":"code","97235e44":"code","583c5366":"code","22939ff9":"code","d16e3fc1":"code","631b5a45":"code","1b43df93":"code","95893b8a":"code","03cce075":"code","5203e6d9":"code","5ab439d3":"code","1c2a494d":"code","7498d6ed":"code","d5ffd155":"code","4d0fc97c":"code","e95d6df1":"markdown","8f0417b4":"markdown","57bca1f2":"markdown","83495efc":"markdown","85393d3d":"markdown","f0eee743":"markdown","f4fa76bf":"markdown","b51f101b":"markdown","b3041a78":"markdown"},"source":{"73353f99":"import pandas as pd\nfrom sklearn import preprocessing","4d45b9b4":"holidays_events = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv', index_col='date')\noil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv', index_col='date')\nstores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv', index_col='id')\ntrain = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv', index_col='id')\ntransactions = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')","a524d45b":"test.head()","ece7e066":"y = train.sales\ny = pd.DataFrame(y)\ntrain1 = train.drop(['sales'], axis=1)","82ec9a2b":"train1.head()","adac4dff":"y.head()","35c1b667":"stores.head()","33b4f8c0":"train2 = pd.merge(train1, stores, how=\"left\", on=\"store_nbr\")\ntest2 = pd.merge(test, stores, how=\"left\", on=\"store_nbr\")","cd878328":"train2.head()","2a99be8f":"#transactions.head()","59937c8f":"#train3 = pd.merge(train2, transactions, how=\"left\", on=\"store_nbr\")","5b136e60":"#train3.head()","d0a3a903":"def extract_weekday(s):\n    return s.dayofweek\n\ndef extract_monthday(s):\n    return s.day\n\ndef extract_month(s):\n    return s.month\n\ndef extract_year(s):\n    return s.year","fa1f681f":"train2['date'] = pd.to_datetime(train2['date'])\ntrain2['weekday'] = train2['date'].apply(extract_weekday)\ntrain2['extract_monthday'] = train2['date'].apply(extract_monthday)\ntrain2['year'] = train2['date'].apply(extract_year)\ntrain2['month'] = train2['date'].apply(extract_month)\n\ntest2['date'] = pd.to_datetime(test2['date'])\ntest2['weekday'] = test2['date'].apply(extract_weekday)\ntest2['extract_monthday'] = test2['date'].apply(extract_monthday)\ntest2['year'] = test2['date'].apply(extract_year)\ntest2['month'] = test2['date'].apply(extract_month)","68859ba1":"train2.head()","e2f85535":"train2.dtypes","b3d36cd8":"cat_cols = [cname for cname in train2.columns if\n                    train2[cname].dtype == 'object']","9ea096ee":"train2.nunique()","48c60cb3":"cat_cols","7989b343":"enc = preprocessing.LabelEncoder()\nfor col in cat_cols:\n    train2[col] = enc.fit_transform(train2[col])\n    test2[col] = enc.fit_transform(test2[col])","96c215aa":"train2","983c5c8a":"'''\nfor col in train2.columns:\n    if train2[col].dtype == 'float64':\n        train2[col] = train2[col].astype('float16')\n        test2[col] = test2[col].astype('float32')\n    if dataset_train[col].dtype == 'int64':\n        dataset_train[col] = dataset_train[col].astype('int8')\n        dataset_test1[col] = dataset_test1[col].astype('int8')\n'''","34a73551":"import tensorflow as tf","68bb3137":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","11054688":"train3 = train2.drop('date', axis=1)\ntest3 = test2.drop('date', axis=1)","b1990df7":"train3.head()","97235e44":"import numpy as np\ntrain_data = np.array(train3)\ntest_data = np.array(test3)","583c5366":"train_data","22939ff9":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(train_data)\ndataset_test_sc = x_scaler.transform(test_data)","d16e3fc1":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","631b5a45":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\"\"\"\n    inputA = keras.Input(shape=(11))\n    line = Reshape((11,1))(inputA)\n    line = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.1)(line)\n\n    line = Conv1D(filters=16, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.3)(line)\n    \n    line = Conv1D(filters=32, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Conv1D(filters=32, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    line = Conv1D(filters=64, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    line = Conv1D(filters=64, kernel_size=3, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Flatten()(line)\n    line = Dense(64, activation='relu')(line)\n    line = Dropout(0.5)(line)\n    #line = Dense(64, activation='relu')(line)\n    outputA = Dense(units=1)(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(loss = 'mse', optimizer = Adam(lr=lr), metrics=['mae'],)\n    return model","1b43df93":"lr=0.001\nwith strategy.scope():\n    model = model_builder(lr)","95893b8a":"model.summary()","03cce075":"checkpoint_filepath = 'best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_loss',\n    mode='min',\n    verbose=1,\n    save_best_only=True)","5203e6d9":"#model = load_model('..\/input\/tps1021\/best_85221.h5')","5ab439d3":"N_split = int(0.2 * len(dataset_train_sc))\ndataset_sc_TRAIN = dataset_train_sc[:-N_split, :]\ndataset_sc_VAL = dataset_train_sc[-N_split:, :]\ny_TRAIN = y[:-N_split]\ny_VAL = y[-N_split:]","1c2a494d":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001, verbose=1, mode='min')","7498d6ed":"EPOCHS = 200\n#EPOCHS = 1\nmodel.fit(\n    dataset_sc_TRAIN, y_TRAIN,\n    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=4096, shuffle=True)","d5ffd155":"model = load_model('best.h5')","4d0fc97c":"preds = model.predict(dataset_test_sc)\noutput = pd.DataFrame({'Id': test.index,'sales': preds[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","e95d6df1":"pseudo training","8f0417b4":"preds = model.predict(dataset_test_sc)","57bca1f2":"N = len(preds)\/(len(preds)+len(y_TRAIN))\nN","83495efc":"import numpy as np\nbig_dataset_X = np.concatenate([dataset_sc_TRAIN, dataset_test_sc], axis=0)\nlen(big_dataset_X)","85393d3d":"extract date features","f0eee743":"repeat few times 2 cells above","f4fa76bf":"real training","b51f101b":"big_dataset_y = np.concatenate([y_TRAIN, preds], axis=0)\nlen(big_dataset_y)","b3041a78":"EPOCHS = 50\n#EPOCHS = 1\nmodel.fit(\n    big_dataset_X, big_dataset_y,\n    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=512, shuffle=True)"}}