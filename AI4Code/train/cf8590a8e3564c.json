{"cell_type":{"7c0927a3":"code","6b934b47":"code","3ccd6e87":"code","9d2036b9":"code","5b7cea24":"code","ff1618a2":"code","766b3fd2":"code","4c73be8a":"code","56924aa2":"code","a6f095b2":"code","f9641361":"code","0b48e359":"code","92169eb8":"code","8511946a":"code","75c9f20a":"code","aee48c1f":"code","f36f100e":"code","f256ba7b":"code","18bc8333":"code","2d8d6e42":"code","3a5b412b":"code","dad09923":"code","8aacb80f":"code","93a55d3f":"code","dfaa3124":"code","871fdce8":"code","951b7a0e":"code","e21cafb1":"code","e123594d":"code","39d37d94":"code","e3a0eefd":"markdown","543352f5":"markdown","cf26ccf8":"markdown","86617d35":"markdown","06f45097":"markdown","cff532c4":"markdown","f887f106":"markdown","dbe9aadd":"markdown","cec48c8f":"markdown","924acd4e":"markdown","7af200c2":"markdown","7ca37782":"markdown","10115cb7":"markdown","01e15025":"markdown","ea60736e":"markdown","f744ee21":"markdown","8ee53022":"markdown","24e2b7a5":"markdown","e89c5027":"markdown","a99037f6":"markdown","6991601b":"markdown","f71a79b4":"markdown","9692cbc4":"markdown","85be2ca0":"markdown","961fabc6":"markdown","7a9aadd8":"markdown","aec814ea":"markdown","43d01179":"markdown","39ab5ff1":"markdown","c6cb8ebe":"markdown","7ef497c3":"markdown","c04f8f37":"markdown","78d1e4c8":"markdown","79f3c598":"markdown"},"source":{"7c0927a3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6b934b47":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')","3ccd6e87":"train_df.head()","9d2036b9":"train_df.info()","5b7cea24":"train_df.describe(include=['object'])","ff1618a2":"train_df[['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']].head()","766b3fd2":"train_df.isna().sum()","4c73be8a":"train_df = train_df.drop(['Ticket', 'Cabin', 'Embarked'], axis=1)","56924aa2":"train_df.head(10)","a6f095b2":"train_df['Title'] = train_df.Name.str.extract('([A-Za-z]+)\\.')\n\npd.crosstab(train_df['Title'], train_df['Sex'])","f9641361":"train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Ms', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","0b48e359":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\ntrain_df['Title'] = train_df['Title'].map(title_mapping)\ntrain_df['is_female'] = train_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","92169eb8":"train_df[['is_mr', 'is_miss', 'is_mrs', 'is_master', 'is_rare_title']] = pd.get_dummies(train_df[\"Title\"])","8511946a":"train_df.head()","75c9f20a":"train_df.drop(['Name', 'Sex', 'Title'] , axis=1, inplace = True)","aee48c1f":"train_df.head()","f36f100e":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby('FamilySize').mean()#.sort_values(by='Survived', ascending=False)","f256ba7b":"train_df['IsAlone'] = 0\ntrain_df.loc[train_df['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","18bc8333":"train_df.drop(['SibSp', 'Parch'] , axis=1, inplace = True)","2d8d6e42":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(train_df, title=\"Titanic Survival Report\", explorative=True)\nprofile.to_file(\"titanic_report.html\")","3a5b412b":"corr_matr = train_df.corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_matr, cmap = cmap)","dad09923":"train_df = train_df.drop(['PassengerId'], axis=1)","8aacb80f":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nX = train_df.loc[:, train_df.columns != 'Survived']\ny = train_df.loc[:, 'Survived']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)","93a55d3f":"X_train = X_train.fillna(X_train.mean())\nX_val= X_val.fillna(X_val.mean())","dfaa3124":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nknn.score(X_val, y_val)","871fdce8":"ss = StandardScaler()\n\nX_train_scaled = ss.fit_transform(X_train)\nX_val_scaled = ss.transform(X_val)\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train_scaled, y_train)\nknn.score(X_val_scaled, y_val)","951b7a0e":"sns.pairplot(train_df[['Fare', 'Age']])","e21cafb1":"rf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\nprint(f'accuracy score: {accuracy_score(rf.predict(X_val), y_val)}')\nprint(f'Feature importance: \\n {dict(zip(X.columns, rf.feature_importances_.round(2)))}')","e123594d":"rf.fit(X_train_scaled, y_train)\nprint(f'accuracy score: {accuracy_score(rf.predict(X_val_scaled), y_val)}')\nprint(f'Feature importance: \\n {dict(zip(X.columns, rf.feature_importances_.round(2)))}')","39d37d94":"corr_matr = train_df.corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_matr, cmap = cmap)","e3a0eefd":"In this section we will train and test one Knn and one Random forest classifier to predict survival rates. However, in our dataset there are still some missing values\n\n**Question 3)**\n\nWhat do we do with missing values?\n\na) We fill them with the column mean\/median \\\nb) We drop the column \\\nc) Depends on the algorithm that we use \\\nd) We fill them with an out-of-sample value (e.g. default_age = -10)","543352f5":"**Question 4)**\n\nWe are now going to train 4 different models, 2 Knn (one scaled, one not) and 2 Random Forest models (also, one scaled, one not). What to we expect from scaling the input data?\n\na) Knn score improves, Random forest remains (more or less) the same \\\nb) Both Knn and Random forest scores improve \\\nc) Both scores remain the same (more or less) \\\nd) Both scores get worse","cf26ccf8":"Notice that a feature being correlated to survival does not necessarily mean that the feature importance in a model is going to be high","86617d35":"Some important remarks:\n* Names are unique across the dataset (count=unique=891)\n* Sex variable has two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. In other words, several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).","06f45097":"From the competition description, we also know what each column represents:\n* survival -> Survival -> 0 = No, 1 = Yes\n* pclass -> Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n* sex -> Sex -> male\/female\n* Age -> Age in years\n* sibsp -> # of siblings \/ spouses aboard the Titanic\n* parch -> # of parents \/ children aboard the Titanic\n* ticket -> Ticket number\n* fare -> Passenger fare\n* cabin -> Cabin number\n* embarked -> Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton","cff532c4":"We can now move on to training the KNN model","f887f106":"Next, we examine the feature correlation, missing values and their most interesting statistical properties using the pandas profiler. It creates an interactive html page which can be downloaded","dbe9aadd":"**Conclusions from data analysis and feature extraction**\n\n**Dropped features**\n\n* Ticket feature was be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature was be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId was be dropped from training dataset as it not correlated to survival.\n\n\n**Created features**\n\n* Created a new feature called Family based on Parch and SibSp to get total count of family members on board.\n* Extracted Title from Name as a new feature. One-hot encoded it for modelling","cec48c8f":"# **Data exploration and cleaning**","924acd4e":"In this notebook we will go through some simple analysis and refer to the tools that you have seen in the Datacamp courses. We will train a couple of simple models to predict the survival rate with machine learning. First, let's list the data that are available on this kaggle notebook","7af200c2":"# KNN training and scoring","7ca37782":"# Random forest training and scoring","10115cb7":"On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.","01e15025":"# Model training and testing","ea60736e":"To get some info on non-numerical variables, we can still use the .describe() command but we need to pass the argument include=['object']","f744ee21":"**Question 2)**\n\nGiven the correlation matrix, which feature is likely going to be the **least** important for our modelling (choose one)?\n\na) is_female \\\nb) Fare \\\nc) PassengerId \\\nd) Not enough info \/ they are all very important","8ee53022":"With .info() we check numerical\/categorical columns","24e2b7a5":"**Creating new features from existing ones: Titles from Names**","e89c5027":"To simplify our analysis, we will drop the 'Ticket', 'Cabin' and 'Embarked' columns.","a99037f6":"There are a lot of good data analysis notebooks already available on kaggle. A good summary of the approaches can be found here:\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","6991601b":"In sklearn, both Knn and Random Forests do not handle missing values internally. Therefore, we need to impute the missing values before passing it through the models","f71a79b4":"From the plots, we have seen that the 'PassengerId' columns has almost no correlation with the other columns. Therefore, we will drop it from the training dataframe (we will keep in on the test_df because it is used for scoring our submission kaggle matches PassengerID to our answer)>","9692cbc4":"At this point, we can drop the 'Name', 'Sex' and 'Title' columnns","85be2ca0":"Let's created the proper train and validation set from the traind_df dataframe","961fabc6":"**Question 1)**\n\nAmong the categorical features listed above, and in absence of other information, which do you think are the least useful features for predicting survival (choose one):\n\na) Sex and Name \\\nb) Name and embarkation location \\\nc) Ticket and Cabin numbers \\\nd) Ticket number and Name ","7a9aadd8":"As a first step, we will explore the distribution of data, their correlation and type. First, let's import the main methods and load the datasets","aec814ea":"In the input folder we find the train and test data, plus an example of a submission file. This last file gives us an idea of how to structure our own submission file. ","43d01179":"Now that we have dealt with categorical columns in our dataframe, we can move on to analysing the numerical columns.\n\nContrarily to before, we will start with creating some new features that may help in our modelling. First, we associate for each passenger the number of family members","39ab5ff1":"Another similar feature we can extract is whether a person was alone or not","c6cb8ebe":"# Cleaning categorical features","7ef497c3":"It is often useful to encode categorical variables into one-hot encodings. ","c04f8f37":"In this example we have only used a standard scaler, but often other types of scaling yield better results. For example, some features may benefit from log scaling their values (typical for \"fat tails\")","78d1e4c8":"Encoding categorical variables into numbers can be often problematic. There are many ways to do this, which you will see in the next courses. Here, we will keep it simple and just assign to each string a number ","79f3c598":"# Cleaning Numerical features"}}