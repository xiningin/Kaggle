{"cell_type":{"e78e78b3":"code","dc59924b":"code","8c9d3eb4":"code","efbe318e":"code","4a677418":"code","72d07cba":"code","d65815f0":"code","5f64d0fd":"code","99e7acbe":"code","dc8fad63":"code","7ff0df5c":"code","27769f1d":"code","c232eff3":"code","1d0c8264":"code","121d8caf":"code","46a17114":"code","bc79c133":"code","a11a882e":"code","49e36205":"code","e31139f6":"code","fc0edfbd":"code","81922b2c":"code","90de822e":"code","1ed7b56a":"code","9b8efe9a":"code","bf50d24e":"code","d6069639":"code","a51c00d5":"code","0bbda364":"code","752d76f1":"code","4e80a1a6":"code","f8d24ebe":"code","62c7ae0f":"code","82422009":"code","691a1858":"code","ae0adeda":"code","94c6f734":"code","624273e1":"code","086f0580":"code","e49e2d9a":"code","b21f945a":"code","d36eedb9":"code","6ccc7dcb":"code","be10b358":"code","0d024f39":"code","1ec97781":"code","91c23fa9":"code","a2778740":"code","208713f2":"code","4cdacf2b":"code","417bf79a":"code","e70cb8ee":"code","de001ef5":"code","5be3fc1a":"code","ec953df4":"code","b03c695d":"code","ad795080":"code","ed4a9219":"code","a0cb7c5b":"code","07833ea4":"code","c1cc0489":"code","9305583b":"code","f0c90afa":"code","f1a46237":"code","7b96aed1":"code","b46e9a5b":"code","7c493cae":"code","16fdc842":"markdown","e90cb7c2":"markdown","87e1fa3c":"markdown","52be7194":"markdown","95f44675":"markdown","53cac79c":"markdown","ed5d8794":"markdown","5619c27e":"markdown","3c3601d3":"markdown"},"source":{"e78e78b3":"from fastai.vision import *\nimport gc","dc59924b":"datapath = Path(\"\/kaggle\/input\/bengaliai-cv19\/\")\ndatapath_feather = Path(\"\/kaggle\/input\/bengaliaicv19feather\/\")\nmodelspath = Path('\/kaggle\/input\/pytorch-pretrained-image-models')","8c9d3eb4":"def read_data(nf=1,pct=0.8):\n    \"\"\"Read a `nf` number of files and split with `pct` into `train_df` and valid_df`.\"\"\"\n    assert nf>=1 and nf<=4\n    train_df   = pd.DataFrame()\n    valid_df   = pd.DataFrame()\n\n    for i in np.arange(nf):\n        df_ = pd.read_feather(datapath_feather\/f'train_image_data_{i}.feather')\n        df_.drop('image_id',axis=1,inplace=True)\n        msk = np.random.rand(len(df_)) < pct\n        train_df = train_df.append(df_[msk],sort=False)\n        valid_df = valid_df.append(df_[~msk],sort=False)\n        del df_\n        gc.collect()\n    return train_df, valid_df","efbe318e":"train_df, valid_df = read_data(1,0.9)","4a677418":"train_df.head()","72d07cba":"labels = pd.read_csv(datapath\/'train.csv')\nlabels.head()","d65815f0":"train_lbls = labels.loc[train_df.index]\nvalid_lbls = labels.loc[valid_df.index]\nassert len(train_lbls)+len(valid_lbls)==len(train_df)+len(valid_df)","5f64d0fd":"train_lbls[(train_lbls.vowel_diacritic==0) & (train_lbls.consonant_diacritic==0)]","99e7acbe":"train_lbls.head()","dc8fad63":"lbltfm","7ff0df5c":"class LblTfm():\n    \"\"\"Transform that encodes\/decodes labels into\/from one-hot vector, suitable for multi-class classification.\"\"\"\n    def __init__(self,mapfile):\n        self.classmap = pd.read_csv(mapfile)\n        self.file = mapfile\n        self.c = len(self.classmap)\n        self.cd_max,self.gr_max,self.vd_max = self.classmap.loc[:,['component_type','label']].groupby('component_type').count().label.values\n    \n    def __len__(self): \n        \"\"\"Returns number of classes\"\"\"\n        return self.c\n    \n    def __call__(self,gr,vd=None,cd=None):\n        \"\"\"Encodes `gr` and optional `vd`,`cd` into a one-hot array\"\"\"\n        if gr is None and vd is None and cd is None: \n            print('Nones')\n            return\n        if gr is not None: assert (gr>=0 and gr<self.gr_max), f\"Argument `gr` must be in range 0-{self.gr_max-1}\"\n        if vd is not None: assert (vd>=0 and vd<self.gr_max), f\"Argument `vd` must be in range 0-{self.vd_max-1}\"\n        if cd is not None: assert (cd>=0 and cd<self.cd_max), f\"Argument `cd` must be in range 0-{self.cd_max-1}\"\n\n        gr_,vd_,cd_ = None,self.gr_max,self.gr_max+self.vd_max       \n        gr_,gr_char = gr, self.classmap.loc[0,'component'] #iloc[gr,(self.classmap.loc[:,'component_type']=='grapheme_root')&                         (self.classmap.loc[:,'label']==gr)].index.to_numpy().item()\n        if vd is not None: vd_,vd_char = vd+168, self.classmap.loc[vd+168,'component']  #self.classmap[(self.classmap.loc[:,'component_type']=='vowel_diacritic')&    (self.classmap.loc[:,'label']==vd)].index.to_numpy().item()\n        if cd is not None: cd_,cd_char = cd+179, self.classmap.loc[cd+179,'component']  #self.classmap[(self.classmap.loc[:,'component_type']=='consonant_diacritic')&(self.classmap.loc[:,'label']==cd)].index.to_numpy().item()\n        return MultiCategory(one_hot((gr_,vd_,cd_),self.c),[gr,vd,cd],[gr_char,vd_char,cd_char])\n        \n    def decode(self,oh):\n        \"\"\"Decodes a one-hot array into `gr`,`vd`,`cd`\"\"\"\n        assert len(oh) == self.c, f\"Argument's len = {len(onehot)}, yet must equal to {self.c+1}\"\n        return oh[:self.gr_max].argmax(), oh[self.gr_max:self.gr_max+self.vd_max].argmax(), oh[self.gr_max+self.vd_max:].argmax()\n    \n    def __repr__(self):\n        return f'Transformation based on `{self.file}\\n'","27769f1d":"lbltfm = LblTfm(datapath\/'class_map.csv')\nlbltfm","c232eff3":"l = lbltfm(1,2,0)\nl","1d0c8264":"l.data","121d8caf":"l.raw","46a17114":"l.obj","bc79c133":"[print(l[1]) for l in train_lbls.iterrows()]","a11a882e":"y = train_lbls[:10]","49e36205":"def lbl(o):\n    \"\"\"For iteration over DataFrame rows\"\"\"\n    return lbltfm(o.grapheme_root,o.vowel_diacritic,o.consonant_diacritic)","e31139f6":"ls = y.apply(lbl,axis=1)","fc0edfbd":"MultiCategoryList(ls,classes=['root','vd','cd'],one_hot=True)","81922b2c":"ls[0],y.iloc[0]","90de822e":"# Create a MultiCategory with:\n# `data` corresponds to the one-hot encoded labels,\n# `obj` that is a collection of labels, \n# `raw` is a list of associated string.\nl = MultiCategory(ls[0],[15,9,5],None)","1ed7b56a":"l","9b8efe9a":"mcl=MultiCategoryList(ls,classes=['gr','vd','cd'],one_hot=True)","bf50d24e":"mcl","d6069639":"class BengaliDS(Dataset):\n    \"\"\"Customized dataset\"\"\"\n    def __init__(self,x,y): #,transform=None):\n        super().__init__()\n        self.x,self.y = x,y\n        self.c = 3\n        #self.transform = transform\n\n    def __len__(self):\n        return len(self.x)\n    \n    # TODO: Add transforms, Categorized Labels\n    def __getitem__(self,idx):\n    #    if torch.is_tensor(idx):\n    #        idx = idx.tolist()\n        if isinstance(idx,int):\n            return (torch.tensor(self.x.iloc[idx].values.astype(np.uint8).reshape(-1,137,236))\/255.),\\\n                    self.y.iloc[idx].to_dict()\n#         if self.transform is not None:\n#             sample = self.transform(sample)\n#        return x_,y_","a51c00d5":"train_ds = BengaliDS(train_df,train_lbls)\nvalid_ds = BengaliDS(valid_df,valid_lbls)","0bbda364":"train_dl = DataLoader(train_ds,batch_size=32,shuffle=True)\nxb,yb = next(iter(train_dl))","752d76f1":"im = Image(xb[0])","4e80a1a6":"tfms = get_transforms()\ntfms[0]","f8d24ebe":"im.apply_tfms(RandTransform(tfm=TfmCrop (crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True))","62c7ae0f":"fig, axes = plt.subplots(3, 6, figsize=(18, 6))\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    im = Image(xb[i])\n    im.apply_tfms(RandTransform(tfm=TfmCrop(crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True))\n    im.show(ax,cmap=\"viridis_r\",title=yb['image_id'][i]+', '+str(yb['grapheme_root'][i].item())+', '+str(yb['vowel_diacritic'][i].item())+', '+str(yb['vowel_diacritic'][i].item()))","82422009":"valid_dl = DataLoader(valid_ds,batch_size=32,shuffle=False)","691a1858":"dbunch = ImageDataBunch(train_dl,valid_dl)","ae0adeda":"dbunch","94c6f734":"# model = torch.load(modelspath\/'resnet34.pth')","624273e1":"# model","086f0580":"# learn = cnn_learner(dbunch,models.resnet34,pretrained=False)","e49e2d9a":"# import albumentations as A","b21f945a":"# !pip install ..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/pretrainedmodels-0.7.4\/ > \/dev\/null # no output","d36eedb9":"# import pretrainedmodels","6ccc7dcb":"# model = pretrainedmodels.resnet34(pretrained=None)","be10b358":"# learn.fit_one_cycle(1)","0d024f39":"lt = LblTfm(datapath)","1ec97781":"assert lt.decode(lt(167,1,5)) == (167,1,5)\nassert lt(1).sum() == 3 # Empty vd and cd are nevertheless encoded as '1'\nassert lt(167)[167] == 1\nassert lt(0,0)[168] == 1\nassert lt(0,cd=0)[168+11] == 1\n","91c23fa9":"lt(0,vd=1)","a2778740":"lt.decode(np.zeros(186))","208713f2":"gr_root_cat = CategoryList(classmap[classmap.component_type=='grapheme_root'].label.values,      classmap[classmap.component_type=='grapheme_root'].component.values)\nvd_cat      = CategoryList(classmap[classmap.component_type=='vowel_diacritic'].label.values,    classmap[classmap.component_type=='vowel_diacritic'].component.values)\ncd_cat      = CategoryList(classmap[classmap.component_type=='consonant_diacritic'].label.values,classmap[classmap.component_type=='consonant_diacritic'].component.values)","4cdacf2b":"cd_cat","417bf79a":"train_lbls","e70cb8ee":"vd_cat","de001ef5":"cd_cat","5be3fc1a":"il = ItemLists(path='.',train=BengaliImageList(train_df),valid=BengaliImageList(valid_df))","ec953df4":"#from fastai.gen_doc.nbdoc import show_doc","b03c695d":"traindata = pd.read_csv(datapath\/'train.csv')\ntraindata.head()","ad795080":"traindata.grapheme_root.value_counts(sort=True).plot(kind='bar',title='grapheme_root values distribution\\nin train set',figsize=(32,4));","ed4a9219":"traindata.consonant_diacritic.value_counts(sort=False).plot(kind='bar',title='consonant_diacritic values distribution\\nin train set');","a0cb7c5b":"traindata.vowel_diacritic.value_counts(sort=False).plot(kind='bar',title='vowel_diacritic values distribution\\nin train set');","07833ea4":"testdata = pd.read_csv(datapath\/'test.csv')\ntestdata.head()","c1cc0489":"class BengaliImageList(ImageList):\n    def __init__(self,df:pd.core.frame.DataFrame):\n        super().__init__(self)\n        self.items = df\n        self.ignore_empty = True\n        \n    def __len__(self):\n        return len(self.items)\n    \n    def get(self,i:int):\n        return Image(torch.tensor(train_df.iloc[i].values.astype(np.uint8).reshape(1,137,236)))\n    #Image(torch.tensor(self.items.loc[i][1:].values).reshape(1,137,236))\n    #    self.sizes[i] = sys.getsizeof(res)\n    \n    def __getitem__(self,idxs)->Any:\n        \"returns a single item based if `idxs` is an integer or a new `ItemList` object if `idxs` is a range.\"\n        idxs = try_int(idxs)\n        if isinstance(idxs, int): return self.get(idxs)\n        else: return self.items.loc[idxs]#, inner_df=index_row(self.inner_df, idxs))\n        \n    def open(self, fn):\n        pass","9305583b":"tr_il = BengaliImageList(train_df)\n","f0c90afa":"type(tr_il)","f1a46237":"# TODO: label_from_func expects BengaliImageList.items as iterable, not a DataFrame\ntype(tr_il.items)","7b96aed1":"# data_src = (ImageItemList.from_df(df=df, path=parent_path, folder='train')\n#             .label_from_df(cols=['class1','class2','class3','class4'], label_cls=MultiCategoryList, one_hot=True, classes=['class1','class2','class3','class4']))","b46e9a5b":"LabelList(tr_il,)","7c493cae":"tr_il[2]","16fdc842":"## Phase 1. Data loading and visualization","e90cb7c2":"## Dataset","87e1fa3c":"Add [dataset in feather format](https:\/\/www.kaggle.com\/corochann\/bengaliaicv19feather)\nto your kernel to load data faster than from original parquet files.","52be7194":"1. `.obj` stores codes (indexes) of _grapheme root_, _vowel diacritic_ and _consonant diactitic_:","95f44675":"`.raw` stores characters of _grapheme root_, _vowel diacritic_ and _consonant diactitic_:","53cac79c":"![image.png](attachment:image.png)","ed5d8794":"`.data` stores one-hot array","5619c27e":"## Multilabel transformation","3c3601d3":"# Bengali Hand-written Grapheme Classification\n\nWith FastAI\n> Competition: https:\/\/www.kaggle.com\/c\/bengaliai-cv19\n"}}