{"cell_type":{"b66d899f":"code","717ad53d":"code","936a5094":"code","3c84f351":"code","2f99191f":"code","8ff2d88d":"code","9ecc459c":"code","ac3ed08f":"code","a8c051cd":"code","637bd4f9":"code","0be68eea":"code","4d365548":"code","2d161085":"code","f0c71695":"code","e4be55c8":"code","690a30c2":"code","046e6842":"code","80c5d96c":"code","2b19720b":"code","50c1f238":"code","2ef02fdc":"code","91b992a9":"code","3c9c7694":"code","2283a279":"code","71ab29ea":"code","dcadbeb5":"code","57233df5":"code","0e5960b6":"code","a11b2488":"code","1b140d18":"code","ba42f0ae":"code","07d85c9c":"markdown","605b3ba5":"markdown"},"source":{"b66d899f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","717ad53d":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras_preprocessing.image import ImageDataGenerator\n\nimport zipfile \n\nimport cv2\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras import models\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model","936a5094":"data = pd.read_csv('..\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']","3c84f351":"data.head()","2f99191f":"data.Usage.value_counts()","8ff2d88d":"def prepare_data(data):\n    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n    image_label = np.array(list(map(int, data['emotion'])))\n\n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48)) \n        image_array[i, :, :, 0] = image \/ 255\n\n    return image_array, image_label\n\n\n\ndef plot_examples(label):\n    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    for i in range(5):\n        idx = data[data['emotion']==label].index[i]\n        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n        axs[i].set_title(emotions[label])\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n\n        \n        \ndef plot_all_emotions():\n    N_train = train_labels.shape[0]\n\n    sel = np.random.choice(range(N_train), replace=False, size=16)\n\n    X_sel = train_images[sel, :, :, :]\n    y_sel = train_labels[sel]\n\n    plt.figure(figsize=[12,12])\n    for i in range(16):\n        plt.subplot(4,4,i+1)\n        plt.imshow(X_sel[i,:,:,0], cmap='binary_r')\n        plt.title(emotions[y_sel[i]])\n        plt.axis('off')\n    plt.show()\n        \ndef plot_image_and_emotion(test_image_array, test_image_label, pred_test_labels, image_number):\n    \"\"\" Function to plot the image and compare the prediction results with the label \"\"\"\n    \n    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n    \n    bar_label = emotions.values()\n    \n    axs[0].imshow(test_image_array[image_number], 'gray')\n    axs[0].set_title(emotions[test_image_label[image_number]])\n    \n    axs[1].bar(bar_label, pred_test_labels[image_number], color='orange', alpha=0.7)\n    axs[1].grid()\n    \n    plt.show()\n    \n\n    \ndef vis_training(hlist, start=1):\n    \n    loss = np.concatenate([h.history['loss'] for h in hlist])\n    val_loss = np.concatenate([h.history['val_loss'] for h in hlist])\n    acc = np.concatenate([h.history['accuracy'] for h in hlist])\n    val_acc = np.concatenate([h.history['val_accuracy'] for h in hlist])\n    \n    epoch_range = range(1,len(loss)+1)\n\n    plt.figure(figsize=[12,6])\n    plt.subplot(1,2,1)\n    plt.plot(epoch_range[start-1:], loss[start-1:], label='Training Loss')\n    plt.plot(epoch_range[start-1:], val_loss[start-1:], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(epoch_range[start-1:], acc[start-1:], label='Training Accuracy')\n    plt.plot(epoch_range[start-1:], val_acc[start-1:], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.show()","9ecc459c":"emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","ac3ed08f":"full_train_images, full_train_labels = prepare_data(data[data['Usage']=='Training'])\ntest_images, test_labels = prepare_data(data[data['Usage']!='Training'])\n\nprint(full_train_images.shape)\nprint(full_train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","a8c051cd":"train_images, valid_images, train_labels, valid_labels =\\\n    train_test_split(full_train_images, full_train_labels, test_size=0.2, random_state=1)\n\nprint(train_images.shape)\nprint(valid_images.shape)\nprint(train_labels.shape)\nprint(valid_labels.shape)","637bd4f9":"plot_all_emotions()","0be68eea":"plot_examples(label=0)","4d365548":"plot_examples(label=1)","2d161085":"plot_examples(label=2)","f0c71695":"plot_examples(label=3)","e4be55c8":"plot_examples(label=4)","690a30c2":"plot_examples(label=5)","046e6842":"plot_examples(label=6)","80c5d96c":"class_weight = dict(zip(range(0, 7), (((data[data['Usage']=='Training']['emotion'].value_counts()).sort_index())\/len(data[data['Usage']=='Training']['emotion'])).tolist()))","2b19720b":"class_weight","50c1f238":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nwith tpu_strategy.scope():\n    model = Sequential()\n\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(48,48,1)))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Flatten())\n\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(7, activation='softmax'))\n\n\n\n\n    opt = keras.optimizers.Adam(lr=0.001)\n    model.compile(loss='sparse_categorical_crossentropy',\n                      optimizer=opt, metrics=['accuracy'])","2ef02fdc":"model.summary()","91b992a9":"h1 = model.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, validation_data=(valid_images, valid_labels))","3c9c7694":"vis_training([h1])","2283a279":"keras.backend.set_value(model.optimizer.learning_rate, 0.00001)\n\nh2 = model.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","71ab29ea":"vis_training([h1, h2])","dcadbeb5":"test_prob = model.predict(test_images)\ntest_pred = np.argmax(test_prob, axis=1)\ntest_accuracy = np.mean(test_pred == test_labels)\n\nprint(test_accuracy)","57233df5":"conf_mat = confusion_matrix(test_labels, test_pred)\n\npd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())","0e5960b6":"fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n                                show_normed=True,\n                                show_absolute=False,\n                                class_names=emotions.values(),\n                                figsize=(8, 8))\nfig.show()","a11b2488":"print(classification_report(test_labels, test_pred, target_names=emotions.values()))","1b140d18":"model_json = model.to_json()\nwith open('model.json','w') as json_file:\n    json_file.write(model_json)","ba42f0ae":"model.save('final_model.h5')","07d85c9c":"## CNN Model","605b3ba5":"**Here we have *28709* Training dataset, *3589* Private Test and *3589* Public Test.**"}}