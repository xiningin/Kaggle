{"cell_type":{"f6402c2e":"code","6acfc7a7":"code","7aff97ff":"code","6f0ff212":"code","0dcdd5cc":"code","3d077ace":"code","3bf0acab":"code","1c2f2649":"code","f29a6ffa":"code","8c18b0e6":"code","e3520ed9":"code","644e44d2":"code","d752113f":"code","d9335e6e":"code","d09ad02b":"code","11c18dea":"code","fc4a2cad":"code","a50871dc":"code","76cb65c4":"code","03ab0b9d":"markdown","1b081001":"markdown","5c3bb6ea":"markdown","bba414fb":"markdown"},"source":{"f6402c2e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, OrthogonalMatchingPursuit\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsRegressor, KernelDensity, KDTree\nfrom sklearn.metrics import *\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython import display, utils\n\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 300)\npd.set_option('max_colwidth', 400)\n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","6acfc7a7":"train = pd.read_csv('..\/input\/covid19-hospital-treatment\/host_train.csv')\ntrain.head()","7aff97ff":"train.info()","6f0ff212":"train.describe()","0dcdd5cc":"missing = train.isnull().sum()\/len(train)\nmissing = missing[missing>0]\nplt.figure(figsize=(15, 11))\nmissing = missing.sort_values(ascending=False)\nplt.style.use('ggplot')\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 14}\nplt.rc('font', **font)\nplt.figure(figsize=(8, 5))\nmissing.plot.bar(color='teal')","3d077ace":"train.describe(include=['O'])","3bf0acab":"cats = [c for c in train.columns if train[c].dtypes=='object']\ncats","1c2f2649":"nums = ['Patient_Visitors', 'Admission_Deposit']\nnums","f29a6ffa":"true_cats = [c for c in train.columns if c not in nums]\ntrue_cats","8c18b0e6":"target = train.Stay_Days\n\n\n\nplt.figure(figsize=(20, 8))\nsns.countplot(target, palette='bone_r')","e3520ed9":"cats.remove('Stay_Days')","644e44d2":"\n\ndef analyse_cats(df, cat_cols):\n    d = pd.DataFrame()\n    cl = []\n    u = []\n    s =[]\n    nans =[]\n    for c in cat_cols:\n        #print(\"column:\" , c ,\"--Uniques:\" , train[c].unique(), \"--Cardinality:\", train[c].unique().size)\n        cl.append(c)\n        u.append(df[c].unique())\n        s.append(df[c].unique().size)\n        nans.append(df[c].isnull().sum())\n        \n       # plt.figure(figsize=(12, 5))\n        #sns.countplot(train[c], palette='bone');\n        \n    d['\"feat\"'] = cl\n    d[\"uniques\"] = u\n    d[\"cardinality\"] = s\n    d[\"nans\"] = nans\n\n    return d\n\ncatanadf = analyse_cats(train, cats)\ncatanadf","d752113f":"\nfor col in cats:\n    le = LabelEncoder() \n    train[col]  = le.fit_transform(train[col].astype(str)) \n    \nle = LabelEncoder()\ntrain.Stay_Days = le.fit_transform(train.Stay_Days.astype(str))\nle_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)\n\n","d9335e6e":"plt.style.use('seaborn-poster')\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 9}\nplt.rc('font', **font)\n\nplt.figure(figsize=(20, 12))\nsns.heatmap(train.corr(), annot=True, cmap='coolwarm')","d09ad02b":"target = train.pop('Stay_Days')","11c18dea":"from sklearn.impute import SimpleImputer\ncols = train.columns\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntrain = pd.DataFrame(imp_mean.fit_transform(train), columns= cols)\ntrain.isnull().sum()","fc4a2cad":"scores = []\noof = np.zeros(len(train))\ny_le = target.values\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, y_le)):\n    print('fold:', fold_)\n    X_tr, X_test = train.iloc[train_ind], train.iloc[val_ind]\n    y_tr, y_test = y_le[train_ind], y_le[val_ind]\n    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1,max_depth=5, max_features=0.8)\n    clf.fit(X_tr, y_tr)\n    oof[val_ind]= np.argmax(clf.predict_proba(X_test),axis=1) \n    y = np.argmax(clf.predict_proba(X_tr),axis=1) \n    print('train:',accuracy_score(y_tr, y),'val :' , accuracy_score(y_test, (oof[val_ind])))\n    print(20 * '-')\n    \n    scores.append(accuracy_score(y_test, oof[val_ind]))\n    \n    \n    \nprint('Random Forest accuracy=  ', np.mean(scores))\nnp.save('oof_rf', oof)\n","a50871dc":"params = {\n    \n    'objective': 'multiclass',\n    'boosting': 'rf',\n    'metric': 'multi_logloss',\n    'max_depth': -1,\n    'num_leaves': 12,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n       \n    'lambda_l2': 2.0,\n    'lambda_l1': 2.0,\n    'unbalanced': True,\n    'num_class': len(np.unique(target)),\n     }\n\nimport lightgbm as lgb\n\nscores = []\n\n\noof = np.zeros(len(train))\n\n\nfeature_importances_gain = pd.DataFrame()\nfeature_importances_gain['feature'] = train.columns\n\nfeature_importances_split = pd.DataFrame()\nfeature_importances_split['feature'] = train.columns\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4242)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print(\"fold :::::::: \" , fold_)\n    trn_data = lgb.Dataset(train.iloc[train_ind], target.iloc[train_ind])\n    val_data = lgb.Dataset(train.iloc[val_ind], target.iloc[val_ind])\n    \n    model = lgb.train(params, trn_data, valid_sets=(trn_data, val_data), num_boost_round=1000, verbose_eval=100, early_stopping_rounds=100)\n    oof[val_ind] = np.argmax(model.predict(train.iloc[val_ind], num_iteration=model.best_iteration), axis=1)\n    \n        \n    print('f1 :', f1_score(target.iloc[val_ind], oof[val_ind], average='micro'))\n    scores.append(f1_score(target.iloc[val_ind], oof[val_ind], average='micro'))\n    \n    feature_importances_gain['fold_{}'.format(fold_ + 1)] = model.feature_importance(importance_type='gain')\n    feature_importances_split['fold_{}'.format(fold_ + 1)] = model.feature_importance(importance_type='split')\n    \n    \n   \n    \nprint('mean f1: ', np.mean(scores))\n","76cb65c4":"feature_importances_gain['average'] = feature_importances_gain[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances_gain.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(15, 10))\nsns.barplot(data=feature_importances_gain.sort_values(by='average', ascending=False).head(100),palette='bone',  x='average', y='feature');\nplt.title('TOP feature importance over {} folds average'.format(folds.n_splits));","03ab0b9d":"- More stay days more visitor and better Ward_Type\n- As we expected Illness_Severity and Available_Extra_Rooms has negative correlation with the target\n- Different hospitals and Department operate almost equaly in treatment\n","1b081001":"*We also could use class weight in Random foredt to see the difference (imbalanced data)*","5c3bb6ea":"Numerics","bba414fb":"### LGB rf booster"}}