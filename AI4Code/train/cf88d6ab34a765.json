{"cell_type":{"31e41ecf":"code","7bf8475b":"code","ab48199a":"code","6b3a82b6":"code","25db2732":"code","ba41f5ff":"code","973d2766":"code","b53c254c":"code","da2c1b9c":"code","d0818f11":"code","d9c744bc":"code","09b89b52":"code","acb73024":"code","07d5ae88":"code","49fcfc40":"code","731dba44":"code","3075a9d9":"code","db2728ea":"code","1fb8a88d":"code","7d478817":"code","f4ebca2d":"code","416ee51d":"markdown"},"source":{"31e41ecf":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport requests\nfrom io import StringIO\nimport copy","7bf8475b":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","ab48199a":"imsize = 512 if torch.cuda.is_available() else 128\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),\n    transforms.ToTensor()\n])","6b3a82b6":"!wget https:\/\/thesource.com\/wp-content\/uploads\/2015\/02\/Pablo_Picasso1.jpg -O \/kaggle\/working\/style.jpg\n!wget http:\/\/mymasterpiecesmile.com\/wp-content\/uploads\/2018\/11\/Masterpiece-Smiles-Clinton-TN-Dentist.jpg -O \/kaggle\/working\/content.jpg","25db2732":"def im_crop_center(img):\n    img_width, img_height = img.size\n    w = h = min(img_width, img_height)\n    left, right = (img_width - w) \/ 2, (img_width + w) \/ 2\n    top, bottom = (img_height - h) \/ 2, (img_height + h) \/ 2\n    left, top = round(max(0, left)), round(max(0, top))\n    right, bottom = round(min(img_width - 0, right)), round(min(img_height - 0, bottom))\n    return img.crop((left, top, right, bottom))","ba41f5ff":"def image_loader(name):\n    image = Image.open(name)\n    image = im_crop_center(image,)\n    \n    image = loader(image).unsqueeze(0)\n    return image.to(device, dtype = torch.float)","973d2766":"unloader = transforms.ToPILImage()\nplt.ion()\n\ndef imshow(tensor, title = None):\n    image = tensor.cpu().clone()\n    image = image.squeeze(0)\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\n# plt.figure()\n# imshow(style_img)\n# plt.figure()\n# imshow(content_img)\n","b53c254c":"img = image_loader(\"\/kaggle\/working\/content.jpg\")\nimshow(img)\nimg1 = image_loader(\"\/kaggle\/working\/style.jpg\")\nimshow(img1)","da2c1b9c":"style_img = img1\ncontent_img = img","d0818f11":"assert style_img.size() == content_img.size()","d9c744bc":"class ContentLoss(nn.Module):\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input","09b89b52":"def gram_matrix(input):\n    a, b, c, d = input.size()\n    features = input.view(a*b, c*d)\n    g = torch.mm(features, features.t())\n    \n    return g.div(a*b*c*d)","acb73024":"class StyleLoss(nn.Module):\n    def __init__(self, target_features):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_features).detach()\n    def forward(self, input):\n        g = gram_matrix(input)\n        self.loss = F.mse_loss(g, self.target)\n        return input","07d5ae88":"cnn_norm_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_norm_std = torch.tensor([0.229, 0.224, 0.225]).to(device)","49fcfc40":"class Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n        \n    def forward(self, img):\n        return (img - self.mean) \/ self.std","731dba44":"cl_def = ['conv_4']\nsl_def = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, n_mean, n_std, st_img, ct_img, ctl = cl_def, stl = sl_def):\n    cnn = copy.deepcopy(cnn)\n    norm = Normalization(n_mean, n_std).to(device)\n    ct_losses = []\n    st_losses = []\n    \n    model = nn.Sequential(norm)\n    \n    i=0\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i+=1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            layer = nn.ReLU(inplace = False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer')\n            break\n        model.add_module(name, layer)\n        \n        if name in ctl:\n            target = model(ct_img).detach()\n            ct_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), ct_loss)\n            ct_losses.append(ct_loss)\n        \n        if name in stl:\n            target_st = model(st_img).detach()\n            st_loss = StyleLoss(target_st)\n            model.add_module(\"style_loss_{}\".format(i), st_loss)\n            st_losses.append(st_loss)\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, st_losses, ct_losses\n","3075a9d9":"input_img = img\nplt.figure()\nimshow(input_img)","db2728ea":"def get_input_optimizer(input_img):\n    # this line to show that input is a parameter that requires a gradient\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer","1fb8a88d":"def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=1000,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # correct the values of updated input image\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img","7d478817":"cnn = models.vgg19(pretrained=True).features.to(device).eval()","f4ebca2d":"output = run_style_transfer(cnn, cnn_norm_mean, cnn_norm_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nimshow(output)\n\n# sphinx_gallery_thumbnail_number = 4\nplt.ioff()\nplt.show()","416ee51d":"## Replica of the pytorch tutorial for Neuronal Style Transfer\n### I implemented this notebook to understand style transfer"}}