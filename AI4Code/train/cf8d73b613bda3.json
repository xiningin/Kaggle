{"cell_type":{"d8c6014e":"code","1ab0644f":"code","a57d5e42":"code","be56c329":"code","b81b67f6":"code","ecedc8a4":"code","831e3744":"code","cbf5e382":"code","daf24c65":"code","0db271bf":"code","ec592103":"code","6b9c8be6":"code","e0bdcbb2":"code","f34307b1":"code","96c9d7b7":"code","3d768681":"code","afa1a5b3":"code","5373cb8d":"code","d0a03c72":"code","173c7f17":"code","8f7b1d5c":"markdown","20c21355":"markdown","43be3156":"markdown","959224d9":"markdown","033b4664":"markdown","b518bb4d":"markdown","b3faa518":"markdown"},"source":{"d8c6014e":"import numpy as np\nimport pandas as pd\nimport random\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport urllib.request\nfrom PIL import Image\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR, LinearSVC\n\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport tools_tb\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","1ab0644f":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)","a57d5e42":"for column in df.columns:\n    sns.scatterplot(data = df, x = column, y = \"price\")\n    plt.show()","be56c329":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)\n\n#\u00a0Removing outliers\ndf = df[(df[\"depth\"] > 45) & (df[\"depth\"] < 75)]\ndf = df[(df[\"table\"] > 45) & (df[\"table\"] < 90)]\ndf = df[df[\"x\"] != 0]\ndf = df[df[\"y\"] < 10]\ndf = df[(df[\"z\"] > 2) & (df[\"z\"] < 7)]\n\n\n#\u00a0Let's start with the ordinal ones, to see if they have any relation with price\n#\u00a0First we assign the corresponding ordinal values we want to replace the categorical ones with\ncut_dict = {\n    \"Fair\" : 1,\n    \"Good\" : 2,\n    \"Very Good\" : 3,\n    \"Premium\" : 4,\n    \"Ideal\" : 5\n}\n\nclarity_dict = {\n    \"I1\" : 1,\n    \"SI2\" : 2,\n    \"SI1\" : 3,\n    \"VS2\" : 4,\n    \"VS1\" : 5,\n    \"VVS2\" : 6,\n    \"VVS1\" : 7,\n    \"IF\" : 8\n}\n\n# Create the columns in the df for them\ndf[\"cut_encoded\"] = df.cut.map(cut_dict)\ndf[\"clarity_encoded\"] = df.clarity.map(clarity_dict)\n\n#\u00a0Now, let's do the same for the nominal variable \"color\"\ndf = pd.get_dummies(df, prefix = [\"color\"], columns = [\"color\"])\n\n#\u00a0Delete the old categorical independent variables\ndf = df.drop([\"cut\", \"clarity\"], axis = 1)\n\nX = df.drop([\"price\"], axis = 1)\ny = df[\"price\"]","b81b67f6":"##########\nsplit = 0.20\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = seed)\n\nrf = RandomForestRegressor(n_estimators = 500, criterion = \"mse\", random_state = seed, n_jobs = -1)\n\nrf.fit(X_train, y_train)\n\ny_pred_train = rf.predict(X_train)\ny_pred_test = rf.predict(X_test)\n\n# Scores\nscore_train = rf.score(X_train, y_train)\nscore_test = rf.score(X_test, y_test)\n\n# RMSE\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"Train data\\nScore = {score_train}\\nRMSE = {rmse_train}\")\nprint(\"#\" * 50)\nprint(f\"Test data\\nScore = {score_test}\\nRMSE = {rmse_test}\")","ecedc8a4":"rf_final = RandomForestRegressor(n_estimators = 500, criterion = \"mse\", random_state = seed, n_jobs = -1, max_depth = 15, max_features = 9)\n\nrf_final.fit(X, y)\n\ny_pred_final = rf_final.predict(X)\n\nscore_final = rf_final.score(X, y)\nrmse_final = np.sqrt(mean_squared_error(y, y_pred_final))\n\nprint(f\"Full data\\nScore = {score_final}\\nRMSE = {rmse_final}\")","831e3744":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)\n\n#\u00a0Removing outliers\ndf = df[(df[\"depth\"] > 45) & (df[\"depth\"] < 75)]\ndf = df[(df[\"table\"] > 45) & (df[\"table\"] < 90)]\ndf = df[df[\"x\"] != 0]\n#df = df[df[\"y\"] < 10]\ndf = df[(df[\"z\"] > 2) & (df[\"z\"] < 7)]\n\n\n#\u00a0Let's start with the ordinal ones, to see if they have any relation with price\n#\u00a0First we assign the corresponding ordinal values we want to replace the categorical ones with\ncut_dict = {\n    \"Fair\" : 1,\n    \"Good\" : 2,\n    \"Very Good\" : 3,\n    \"Premium\" : 4,\n    \"Ideal\" : 5\n}\n\nclarity_dict = {\n    \"I1\" : 1,\n    \"SI2\" : 2,\n    \"SI1\" : 3,\n    \"VS2\" : 4,\n    \"VS1\" : 5,\n    \"VVS2\" : 6,\n    \"VVS1\" : 7,\n    \"IF\" : 8\n}\n\n# Create the columns in the df for them\ndf[\"cut_encoded\"] = df.cut.map(cut_dict)\ndf[\"clarity_encoded\"] = df.clarity.map(clarity_dict)\n\n#\u00a0Now, let's do the same for the nominal variable \"color\"\ndf = pd.get_dummies(df, prefix = [\"color\"], columns = [\"color\"])\n\n#\u00a0Delete the old categorical independent variables\ndf = df[['carat', 'depth', 'table', 'x', 'y', 'z', 'cut_encoded',\n       'clarity_encoded', 'color_D', 'color_G',\n       'color_H', 'color_I', 'color_J', 'price']]\n\nX = df.drop([\"price\"], axis = 1)\ny = df[\"price\"]","cbf5e382":"##########\nsplit = 0.25\nseed = 42\n# seed = 1\nestimators = 200\ncriterion = \"mae\"\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = seed)\n\nrf = RandomForestRegressor(n_estimators = estimators, criterion = criterion, random_state = seed, n_jobs = -1)\n\nrf.fit(X_train, y_train)\n\ny_pred_train = rf.predict(X_train)\ny_pred_test = rf.predict(X_test)\n\n# Scores\nscore_train = rf.score(X_train, y_train)\nscore_test = rf.score(X_test, y_test)\n\n# RMSE\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"Train data\\nScore = {score_train}\\nRMSE = {rmse_train}\")\nprint(\"#\" * 50)\nprint(f\"Test data\\nScore = {score_test}\\nRMSE = {rmse_test}\")","daf24c65":"rf_final = RandomForestRegressor(n_estimators = estimators, criterion = criterion, random_state = seed, n_jobs = -1)\n\nrf_final.fit(X, y)\n\ny_pred_final = rf_final.predict(X)\n\nscore_final = rf_final.score(X, y)\nrmse_final = np.sqrt(mean_squared_error(y, y_pred_final))\n\nprint(f\"Full data\\nScore = {score_final}\\nRMSE = {rmse_final}\")","0db271bf":"X_pred = pd.read_csv(\"diamonds_test.csv\", index_col = 0)\n\nX_pred[\"cut_encoded\"] = X_pred.cut.map(cut_dict)\nX_pred[\"clarity_encoded\"] = X_pred.clarity.map(clarity_dict)\n\n#\u00a0Now, let's do the same for the nominal variable \"color\"\nX_pred = pd.get_dummies(X_pred, prefix = [\"color\"], columns = [\"color\"])\n\n#\u00a0Delete the old categorical independent variables\nX_pred = X_pred.drop([\"cut\", \"clarity\"], axis = 1)\n\npredictions_submit = rf_final.predict(X_pred)\n\nsubmission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})\n\ntools_tb.chequeator(submission)","ec592103":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)\n\n#\u00a0Removing outliers\ndf = df[(df[\"depth\"] > 45) & (df[\"depth\"] < 75)]\ndf = df[(df[\"table\"] > 45) & (df[\"table\"] < 90)]\ndf = df[df[\"x\"] != 0]\n#df = df[df[\"y\"] < 10]\ndf = df[(df[\"z\"] > 2) & (df[\"z\"] < 7)]\n\n\n#\u00a0Let's start with the ordinal ones, to see if they have any relation with price\n#\u00a0First we assign the corresponding ordinal values we want to replace the categorical ones with\ncut_dict = {\n    \"Fair\" : 1,\n    \"Good\" : 2,\n    \"Very Good\" : 3,\n    \"Premium\" : 4,\n    \"Ideal\" : 5\n}\n\nclarity_dict = {\n    \"I1\" : 1,\n    \"SI2\" : 2,\n    \"SI1\" : 3,\n    \"VS2\" : 4,\n    \"VS1\" : 5,\n    \"VVS2\" : 6,\n    \"VVS1\" : 7,\n    \"IF\" : 8\n}\n\n# Create the columns in the df for them\ndf[\"cut_encoded\"] = df.cut.map(cut_dict)\ndf[\"clarity_encoded\"] = df.clarity.map(clarity_dict)\n\n\n#\u00a0Set for training\nX = df[['carat', 'depth', 'table', 'x', 'y', 'z', 'cut_encoded',\n       'clarity_encoded']]\ny = df[\"price\"]","6b9c8be6":"##########\nsplit = 0.25\nseed = 42\n# seed = 1\nestimators = 50\ncriterion = \"mae\"\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = seed)\n\nrf = RandomForestRegressor(n_estimators = estimators, criterion = criterion, random_state = seed, n_jobs = -1)\n\nrf.fit(X_train, y_train)\n\ny_pred_train = rf.predict(X_train)\ny_pred_test = rf.predict(X_test)\n\n# Scores\nscore_train = rf.score(X_train, y_train)\nscore_test = rf.score(X_test, y_test)\n\n# RMSE\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"Train data\\nScore = {score_train}\\nRMSE = {rmse_train}\")\nprint(\"#\" * 50)\nprint(f\"Test data\\nScore = {score_test}\\nRMSE = {rmse_test}\")","e0bdcbb2":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)\n\n#\u00a0Removing outliers\ndf = df[(df[\"depth\"] > 45) & (df[\"depth\"] < 75)]\ndf = df[(df[\"table\"] > 45) & (df[\"table\"] < 90)]\ndf = df[df[\"x\"] != 0]\ndf = df[df[\"y\"] < 10]\ndf = df[(df[\"z\"] > 2) & (df[\"z\"] < 7)]\n\n\n#\u00a0Let's start with the ordinal ones, to see if they have any relation with price\n#\u00a0First we assign the corresponding ordinal values we want to replace the categorical ones with\ncut_dict = {\n    \"Fair\" : 1,\n    \"Good\" : 2,\n    \"Very Good\" : 3,\n    \"Premium\" : 4,\n    \"Ideal\" : 5\n}\n\nclarity_dict = {\n    \"I1\" : 1,\n    \"SI2\" : 2,\n    \"SI1\" : 3,\n    \"VS2\" : 4,\n    \"VS1\" : 5,\n    \"VVS2\" : 6,\n    \"VVS1\" : 7,\n    \"IF\" : 8\n}\n\n# Create the columns in the df for them\ndf[\"cut_encoded\"] = df.cut.map(cut_dict)\ndf[\"clarity_encoded\"] = df.clarity.map(clarity_dict)\n\n#\u00a0Now, let's do the same for the nominal variable \"color\"\ndf = pd.get_dummies(df, prefix = [\"color\"], columns = [\"color\"])\n\n\n# Defining dependent and independent variables\nX = df.drop([\"cut\", \"clarity\", \"price\"], axis = 1)\ny = df[\"price\"]","f34307b1":"#\u00a0Using Grid\nsplit = .25\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = seed)\n\n\nparameters = {\n    \"criterion\" : [\"mse\"],\n    \"max_depth\" : [X.shape[1]],\n    \"max_features\" : [X.shape[1]],\n    \"n_jobs\" : [-1],\n    \"random_state\" : [seed],\n    \"warm_start\" : [True]\n}\n\nrf = RandomForestRegressor()\n\ngrid = GridSearchCV(estimator = rf, param_grid = parameters, n_jobs = -1, cv = 10)\n\ngrid.fit(X_train, y_train)\n\nprint(\"grid.best_stimator_\", grid.best_estimator_)\nprint(\"grid.best_params_\", grid.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"grid.best_score\", grid.best_score_)","96c9d7b7":"# Predictions\ny_pred_train = grid.predict(X_train)\ny_pred_test = grid.predict(X_test)\n\n# Scores\nscore_train = grid.score(X_train, y_train)\nscore_test = grid.score(X_test, y_test)\n\n# RMSE\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"Train data\\nScore = {score_train}\\nRMSE = {rmse_train}\")\nprint(\"#\" * 50)\nprint(f\"Test data\\nScore = {score_test}\\nRMSE = {rmse_test}\")","3d768681":"grid_final = GridSearchCV(estimator = rf, param_grid = parameters, n_jobs = -1, cv = 10)\n\ngrid_final.fit(X, y)\n\ny_pred_final = grid_final.predict(X)\n\nscore_final = grid_final.score(X, y)\nrmse_final = np.sqrt(mean_squared_error(y, y_pred_final))\n\nprint(f\"Full data\\nScore = {score_final}\\nRMSE = {rmse_final}\")","afa1a5b3":"X_pred = pd.read_csv(\"diamonds_test.csv\", index_col = 0)\n\nX_pred[\"cut_encoded\"] = X_pred.cut.map(cut_dict)\nX_pred[\"clarity_encoded\"] = X_pred.clarity.map(clarity_dict)\n\n#\u00a0Now, let's do the same for the nominal variable \"color\"\nX_pred = pd.get_dummies(X_pred, prefix = [\"color\"], columns = [\"color\"])\n\n#\u00a0Delete the old categorical independent variables\nX_pred = X_pred.drop([\"cut\", \"clarity\"], axis = 1)\n\npredictions_submit = grid_final.predict(X_pred)\n\nsubmission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})\n\ntools_tb.chequeator(submission)","5373cb8d":"df = pd.read_csv(\"diamonds_train.csv\", index_col = 0)\n\n#\u00a0Cleaning\ndf = tools_tb.ordinal_encoder(df)\ndf = tools_tb.nominal_encoder(df)\n#df = tools_tb.outliers_remover(df)\n\n#\u00a0Defining X and y\nto_drop = [\"cut\", \"clarity\"]\n\nX, y = tools_tb.variables_split(df, to_drop)","d0a03c72":"#\u00a0Using Grid\nsplit = .2\nseed = 7881\ncount = 1\n\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = 42)\n\n#\u00a0Cross validation\nskf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n\n#\u00a0Parameters for Random Forest\nparameters = {\n    \"criterion\" : [\"mse\"],\n    \"max_depth\" : [X.shape[1]],\n    \"max_features\" : [X.shape[1]],\n    \"n_jobs\" : [-1],\n    \"random_state\" : [seed],\n    \"warm_start\" : [True]\n}\n\n#\u00a0Model\nrf = RandomForestRegressor()\n\ngrid = GridSearchCV(estimator = rf, param_grid = parameters, n_jobs = -1, cv = skf)\n\ngrid.fit(X_train, y_train)\n\n#\u00a0Predictions\ny_pred_train = grid.predict(X_train)\ny_pred_test = grid.predict(X_test)\n\n# Scores\nscore_train = grid.score(X_train, y_train)\nscore_test = grid.score(X_test, y_test)\n\n# RMSE\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"- Model {count} | seed = {seed} -\")\nprint(f\"Train data\\nScore = {score_train}\\nRMSE = {rmse_train}\")\nprint(\"-\" * 25)\nprint(f\"Test data\\nScore = {score_test}\\nRMSE = {rmse_test}\")\nprint(\"#\" * 50)\ncount += 1","173c7f17":"tools_tb.variables_split","8f7b1d5c":"# Model 5","20c21355":"# Guardar datos de este \u00faltimo modelo","43be3156":"# Model 3","959224d9":"# Model 1","033b4664":"# Model 4","b518bb4d":"## Guardar modelo","b3faa518":"# Model 2"}}