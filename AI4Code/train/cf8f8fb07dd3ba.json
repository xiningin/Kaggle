{"cell_type":{"10657a7d":"code","e15a33b5":"code","796507bb":"code","cd83a7ba":"code","5f7459fd":"code","30170095":"code","33431a9c":"code","98326058":"code","a2051a1b":"code","a4b3212f":"code","d6f83c65":"code","f6b5d112":"code","be77c976":"code","1a14de82":"code","7a12f8b9":"code","a7e406ee":"code","17069e6f":"code","dbd3c721":"code","4eb4d1b4":"code","9143d0e3":"code","0628028f":"code","a11bcc1a":"code","468b54a9":"code","5ab13d72":"code","f0d9eeae":"code","e3f2623b":"code","68cd9295":"code","78fa7891":"code","b16e8476":"code","9582dc17":"code","adf3348e":"code","a2bdf9d6":"code","0fca3cb1":"code","6298f10c":"code","61d88a4f":"code","7e2e7288":"code","6c7e5679":"code","d67daeb4":"code","bad70b2b":"code","1615536b":"code","022e5d8d":"code","639af0d8":"code","5be9fecf":"code","8cc82902":"markdown","ae4440f0":"markdown","84a190fc":"markdown","730c7932":"markdown","6c67fa71":"markdown","3dc0c159":"markdown","df73d2b6":"markdown","a61328b3":"markdown","63780c75":"markdown","faf97294":"markdown","073e7b22":"markdown"},"source":{"10657a7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e15a33b5":"%matplotlib inline\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer # nltk is Natural Language Processing Toolkit\nimport pickle\ndef saveindisk(obj,filename):\n    pickle.dump(obj,open(filename+\".p\",\"wb\"), protocol=4)\ndef openfromdisk(filename):\n    temp = pickle.load(open(filename+\".p\",\"rb\"))\n    return temp","796507bb":"con = sqlite3.connect(\"\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite\")\n\n# For positive reviews score should be 4,5 and for negatie reviews score should b 1,2\n# So we are eliminating all those where score is 3 as it is being considered as a neutral point\nfiltered_data = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3\n\"\"\",con)\n\n# In this function from column Score we are removing the rating and putting 'negative' or 'positive' comment in score column\ndef partition(x):\n    if x<3:\n        return 'negative'\n    return 'positive'\n\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition)\nfiltered_data['Score'] = positiveNegative","cd83a7ba":"filtered_data.shape\nfiltered_data.head()","5f7459fd":"display = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews \nWHERE Score != 3 AND UserId='AR5J8UI46CURR'\nORDER BY ProductId\n\"\"\",con)\ndisplay","30170095":"# Sorting data acc to ProductId\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True)","33431a9c":"final = sorted_data.drop_duplicates(subset={\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep='first', inplace=False)\n# keep says to keep the first data and discard the rest duplicate data\nfinal.shape","98326058":"(final['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100","a2051a1b":"display = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews \nWHERE Score != 3 AND Id=44737 OR Id=64422\nORDER BY ProductId\n\"\"\",con)\ndisplay","a4b3212f":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]","d6f83c65":"print(final.shape)\nfinal['Score'].value_counts()","f6b5d112":"count_vect = CountVectorizer() # In scikit-learn\nfinal_counts = count_vect.fit_transform(final['Text'].values)","be77c976":"type(final_counts)","1a14de82":"final_counts.get_shape()","7a12f8b9":"# Find sentence containing html tags\nimport re # re is regular expression\ni=0\nfor sent in final['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break\n    i+=1","a7e406ee":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop=set(stopwords.words('english')) # Set of stopwords \nsno = nltk.stem.SnowballStemmer('english')\n\ndef cleanhtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence) # sub is substitute. html tags will be replaced by space\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]', r' ', cleaned)\n    return cleaned\n\nprint(stop)\nprint(sno.stem('tasty'))","17069e6f":"i=0\nstrl = ' '\nfinal_string =[]\nall_positive_words=[] # Store words from +ve reviews\nall_negative_words=[] # Store words from -ve reviews\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    sent=cleanhtml(sent) # Remove html tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if(cleaned_words.lower() not in stop):\n                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s)\n                    if (final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n                \n    strl = b\" \".join(filtered_sentence)\n    final_string.append(strl)\n    i+=1","dbd3c721":"final['CleanedText'] = final_string # Adding a cloumn of CleanedText","4eb4d1b4":"final.head(3)\n# Store final table into SQL table\nconn = sqlite3.connect('final.sqlite')\nc=conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace')","9143d0e3":"freq_dist_positive = nltk.FreqDist(all_positive_words)\nfreq_dist_negative = nltk.FreqDist(all_negative_words)\nprint(\"Most common +ve words : \", freq_dist_positive.most_common(20))\nprint(\"Most common -ve words : \", freq_dist_negative.most_common(20))","0628028f":"# Bi-gram\ncount_vect = CountVectorizer(ngram_range=(1,2))\nfinal_bigram_counts = count_vect.fit_transform(final['Text'].values)","a11bcc1a":"final_bigram_counts.get_shape()","468b54a9":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nfinal_tf_idf = tf_idf_vect.fit_transform(final['Text'].values) # final_tf_idf is a sparse matrix being created","5ab13d72":"final_tf_idf.get_shape()","f0d9eeae":"features = tf_idf_vect.get_feature_names()\nlen(features)","e3f2623b":"features[100000:100010]","68cd9295":"# To get vector for any review let its for review 3\nprint(final_tf_idf[3,:].toarray()[0]) # Returns a numpy array","78fa7891":"def top_tfidf_feats(row, features, top_n = 25):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ntop_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0], features, 25)","b16e8476":"top_tfidf # Top 25 words(unigram or bigram) of review 1 from vector v1","9582dc17":"# Using goggle news word2vector\n#from gensim.models import Word2Vwec\n#from gensim.models import KeyedVectors\n#import pickle\n\n#model = KeyedVectors.load_word2vec_format('GoggleNews-vectors-negative300.bin.gz') # 300 dim representation of word 2 vector","adf3348e":"#model.wv['computer'] # wv is worrdvector","a2bdf9d6":"#model.wv.similarity('woman', 'man') # Similarity between man and woman. Max similarity can be 1 and min is 0","0fca3cb1":"#model.wv.most_similar('woman') # It will return the most similar word in decreasing order","6298f10c":"#model.wv.most_similar('tasty')","61d88a4f":"# Train your own word2vec model using your own text corpus\nimport gensim\ni=0\nlist_of_sent=[]\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):\n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue\n    list_of_sent.append(filtered_sentence)","7e2e7288":"print(final['Text'].values[0])\nprint(list_of_sent[0]) # converting words to list","6c7e5679":"# To train\nw2v_model=gensim.models.Word2Vec(list_of_sent, min_count=5, size=50, workers=4)\n# min_count says that if a word doesn't occur at least 5 times then do not construct its w2v\n# size is dim of vector v","d67daeb4":"words=list(w2v_model.wv.vocab)\nprint(len(words))","bad70b2b":"w2v_model.wv.most_similar('tasty')","1615536b":"w2v_model.wv.most_similar('like')","022e5d8d":"count_vect_feat = count_vect.get_feature_names()\ncount_vect_feat.index('like')\nprint(count_vect_feat[64055])","639af0d8":"sent_vectors = []\nfor sent in list_of_sent:\n    sent_vec = np.zeros(50)\n    cnt_words = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))","5be9fecf":"# tfidf_feat = tf_idf_vect.get_feature_names()\n# tfidf_sent_vectors =[]\n# row=0\n# for sent in list_of_sent:\n    # sent_vec = np.zeros(50)\n    # weight_sum = 0\n    # for word in sent:\n        # try:\n            # vec = w2v_model.wv[word]\n            # tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n            # sent_vec += vec*tf_idf\n            # weight_sum += tf_idf\n        # except:\n            # pass\n    # sent_vec \/= weight_sum\n    # tfidf_sent_vectors.append(sent_vec)\n    # row += 1","8cc82902":"# AVG W2V, TFIDF","ae4440f0":"There are 364171 reviews and no. of unique words are 115281.","84a190fc":"# BOW","730c7932":"Out of 500000 data points only 364173 are left. Rest were duplicate datas.","6c67fa71":"# Text Pre-processing","3dc0c159":"# Load the data","df73d2b6":"# Bi-gram and n-gram","a61328b3":"# Word2Vec","63780c75":"HelpfulnessNumerator must always be < HelpfulnessDenominator. But here it is greater so this data should also be removed.","faf97294":"This tells 69% of data are still remaining.","073e7b22":"# TF-IDF"}}