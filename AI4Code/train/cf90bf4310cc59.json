{"cell_type":{"3ac41466":"code","fee113cf":"code","4a42387f":"code","558da26c":"code","ce0c34e9":"code","50b8a60a":"code","2b753a29":"code","602d1098":"code","8f3ad98b":"code","06d61366":"code","01f0efc0":"code","0c7bf553":"code","bc17dcac":"code","de6e527c":"code","b2d75c72":"code","6713523b":"markdown","936c0649":"markdown","c75abb7c":"markdown","eb7c5239":"markdown","c4232751":"markdown","cfb2623f":"markdown","5658a734":"markdown","618fa5ca":"markdown","f523f676":"markdown","f9e1df5d":"markdown","5e029ebd":"markdown"},"source":{"3ac41466":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Scaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n# Cross-Validation\nfrom sklearn.model_selection import StratifiedKFold","fee113cf":"train_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","4a42387f":"# Get train data without the target and ids\nX = train_data.iloc[:, 1:-1].copy()\n# Get the target\ny = train_data.target.copy()\n\n# Create test X, drop ids.\ntest_X = test_data.iloc[:, 1:].copy()","558da26c":"# Apply a scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_X = scaler.transform(test_X)","ce0c34e9":"# Set seeds\nmy_seed = 1\nnp.random.seed(my_seed)\ntf.random.set_seed(my_seed)","50b8a60a":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001,           # Minimium amount of change to count as an improvement\n    patience=5,                # How many epochs to wait before stopping\n    restore_best_weights=True)","2b753a29":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.2,                # Factor by which the learning rate will be reduced\n    patience=5,                # Number of epochs with no improvement\n    min_lr=0.001)              # Lower bound on the learning rate","602d1098":"CALLBACKS = [early_stopping]","8f3ad98b":"# Play with those configurations...\nEPOCHS = 100\nBATCH_SIZE = 512\nN_SPLITS = 15","06d61366":"model = keras.Sequential([\n    layers.Dense(100, activation='swish', input_shape=[X.shape[1]]),\n    layers.Dropout(0.2),\n    layers.Dense(64, activation='swish'),\n    layers.Dropout(0.2),\n    layers.Dense(32, activation='swish'),\n    layers.Dropout(0.2),\n    # For a binary classification function use sigmoid\n    layers.Dense(1, activation='sigmoid')])","01f0efc0":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['AUC'])","0c7bf553":"fold = 0\ntest_predictions = np.zeros(test_X.shape[0])\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=48, shuffle=True)\nscores = {fold:None for fold in range(skf.n_splits)}\nfor train_idx, test_idx in skf.split(X, y):\n    train_X, val_X = X[train_idx], X[test_idx]\n    train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n    history = model.fit(\n        train_X, train_y,\n        validation_data=(val_X, val_y),\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=CALLBACKS,        # Put your callbacks in a list\n        verbose=0)                  # Turn off training log\n\n    scores[fold] = (history.history)\n    print(f\"Fold {fold + 1} \\t\\t AUC: {np.max(scores[fold]['val_auc'])}\")\n\n    # Get the average values from each fold to the prediction\n    test_predictions += model.predict(test_X, batch_size=BATCH_SIZE).reshape(1,-1)[0] \/ skf.n_splits\n    fold += 1\n\noverall_auc = [np.max(scores[fold]['val_auc']) for fold in range(skf.n_splits)]\nprint('Overall Mean AUC: ', np.mean(overall_auc))","bc17dcac":"# Credits to https:\/\/www.kaggle.com\/mlanhenke\/tps-11-nn-baseline-keras?scriptVersionId=79830528\nfig, ax = plt.subplots(3, 5, tight_layout=True, figsize=(20, 15))\nax = ax.flatten()\n\nfor fold in range(skf.n_splits):\n    df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n    min_train = np.round(np.min(df_eval['train_loss']),5)\n    min_valid = np.round(np.min(df_eval['valid_loss']),5)\n    delta = np.round(min_valid - min_train,5)\n    \n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['train_loss'],\n        label='train_loss',\n        ax = ax[fold]\n    )\n\n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['valid_loss'],\n        label='valid_loss',\n        ax = ax[fold]\n    )\n    \n    ax[fold].set_ylabel('')\n    ax[fold].set_xlabel(f\"Fold {fold+1}\\nmin_train: {min_train}\\nmin_valid: {min_valid}\\ndelta: {delta}\", fontstyle='italic')\n\nsns.despine()","de6e527c":"# Run the code to save predictions in the format used for competition scoring\noutput = pd.DataFrame({'id': test_data.id, 'target': test_predictions})\noutput.to_csv('submission.csv', index=False)","b2d75c72":"output","6713523b":"## Callbacks","936c0649":"## Training","c75abb7c":"# Modelling\n\nIn this notebook, I will try different configurations and try to understand which changes affects the results most.\n\nWhat I have tried so far,\n\n```\nVersion 1\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n512 neurons, Dropout 0.3, BatchNormalization, relu\n256 neurons, Dropout 0.3, BatchNormalization, relu\n128 neurons, Dropout 0.3, BatchNormalization, relu\nOverall AUC: 0.770\n```\n---\n```\nVersion 3\ncallbacks: EarlyStopping min_delta=0.001, patience=20\ncallbacks: ReduceLROnPlateau monitor='val_loss', factor=0.2, patience=5, min_lr=0.001\n3 layers, \n512 neurons, Dropout 0.3, BatchNormalization, relu\n256 neurons, Dropout 0.3, BatchNormalization, relu\n128 neurons, Dropout 0.3, BatchNormalization, relu\nOverall AUC: 0.769\n(Notes, difference is probably caused by the ReduceLROnPlateau callback)\n```\n---\n```\nVersion 4, (Try a different activation function with Version 1's configuration)\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n512 neurons, Dropout 0.3, BatchNormalization, linear\n256 neurons, Dropout 0.3, BatchNormalization, linear\n128 neurons, Dropout 0.3, BatchNormalization, linear\nOverall AUC: 0.749\n(Notes, activation function didn't work well, let's try swish in Version 5)\n```\n---\n```\nVersion 5\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n512 neurons, Dropout 0.3, BatchNormalization, swish\n256 neurons, Dropout 0.3, BatchNormalization, swish\n128 neurons, Dropout 0.3, BatchNormalization, swish\nOverall AUC: 0.770\n(Notes, little better than Version 1, let's play with neuron numbers)\n```\n---\n```\nVersion 6\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, BatchNormalization, swish\n64 neurons, Dropout 0.3, BatchNormalization, swish\n32 neurons, Dropout 0.3, BatchNormalization, swish\nOverall AUC: 0.766\n(Notes, although AUC is less, it got a better score.\nIn the next version I will play with the n_splits.)\n```\n---\n```\nVersion 7\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 5\n3 layers, \n100 neurons, Dropout 0.3, BatchNormalization, swish\n64 neurons, Dropout 0.3, BatchNormalization, swish\n32 neurons, Dropout 0.3, BatchNormalization, swish\nOverall AUC: 0.761\n(Notes, looks like it is better having more splits.\nFor the next version I will try layers without\nbatch normalization)\n```\n---\n```\nVersion 8\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, swish\n64 neurons, Dropout 0.3, swish\n32 neurons, Dropout 0.3, swish\nOverall AUC: 0.766\n(Notes, pretty much the same AUC with Version 6 but \nit has the highest score. Let's try to change the \nbatch size in the next version)\n```\n---\n```\nVersion 9\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 1024, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, swish\n64 neurons, Dropout 0.3, swish\n32 neurons, Dropout 0.3, swish\nOverall AUC: 0.765\n(Notes, increasing the batch size reduced the score.\nFor next version, I want to try ReduceLROnPlateau again.)\n```\n---\n```\nVersion 10\ncallbacks: EarlyStopping min_delta=0.001, patience=20\ncallbacks: ReduceLROnPlateau monitor='val_loss', factor=0.2, patience=5, min_lr=0.001\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, swish\n64 neurons, Dropout 0.3, swish\n32 neurons, Dropout 0.3, swish\nOverall AUC: 0.766\n(Notes, almost the same score with Version 8. Let's\ndrop the callback again and try without dropout in layers.)\n```\n---\n```\nVersion 11\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, swish\n64 neurons, swish\n32 neurons, swish\nOverall AUC: 0.768\n(Notes, although it gets a better AUC its score is less\nprobably due to overfitting. For next version I want to try\na different scaler, MinMaxScaler probably)\n```\n---\n```\nVersion 12\ncallbacks: EarlyStopping min_delta=0.001, patience=20\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, swish\n64 neurons, Dropout 0.3, swish\n32 neurons, Dropout 0.3, swish\nOverall AUC: 0.755\n(Notes, MinMaxScaler didn't work quite well.\nNext I will try changing patience in early stopping.)\n```\n---\n```\nVersion 13\ncallbacks: EarlyStopping min_delta=0.001, patience=5\nEPOCHS: 100, BATCH_SIZE: 512, N_SPLITS: 15\n3 layers, \n100 neurons, Dropout 0.3, swish\n64 neurons, Dropout 0.3, swish\n32 neurons, Dropout 0.3, swish\nOverall AUC: 0.763\n(Notes, changing patience is worked well, it looks like\nhelped the overfitting. Let's try changing the dropout)\n```","eb7c5239":"https:\/\/www.kaggle.com\/ryanholbrook\/stochastic-gradient-descent  \nA \"loss function\" that measures how good the network's predictions are.  \nAn \"optimizer\" that can tell the network how to change its weights.\n\nSo, we can play with optimizer and loss functions but probably we should keep the metrics.","c4232751":"# What am I doing in this notebook?\n\nSo far, I have learned and tried different models to solve this problem in https:\/\/www.kaggle.com\/sfktrkl\/tps-nov-2021. That notebook includes some regression and classification models, tries those models with or without cross-validation. I have also tried doing a very simple feature selection and applied that to some of those models.\n\nAlthough couple of those models gives good results, I have seen that many people are getting better results with deep learning. Hence, I have also started learning about it.\n\nSo, in this notebook, I will try what I have learned so far. I am not expecting getting very good results at least before I understand the basics behind the neural networks but still I will try my best to get good results \ud83d\udcaa.","cfb2623f":"# Importing Librabies and Loading datasets","5658a734":"## Model","618fa5ca":"# Submission","f523f676":"# Pre-proccessing","f9e1df5d":"# Evaluation","5e029ebd":"It should be noted that I am using sigmoid activation function as output activation function to solve this binary classification problem.\n\nSo, I am planing to play with other configurations except that output activation function. I hope this is a correct approach :)  \n(It is better reading this article https:\/\/machinelearningmastery.com\/choose-an-activation-function-for-deep-learning\/)"}}