{"cell_type":{"ae486b30":"code","f92e297f":"code","20ed5ca2":"code","0daebd99":"code","93592619":"code","1bfd646b":"code","258bd967":"code","f9ca2ef6":"code","e96592a2":"code","ec67800c":"code","c35be0e5":"code","a7bd3267":"code","22c99ccf":"code","8a8ac78b":"code","42f83c83":"code","72b70537":"code","007dd755":"code","3ac57252":"code","6188e05b":"code","87343a4e":"code","930bd8c8":"code","f41136b2":"code","c13e2826":"code","8220291b":"code","5c88048f":"code","c29071b9":"code","592a43c9":"code","19271575":"code","eccd94bd":"code","f98dd216":"code","06911859":"code","4e46593b":"code","ad0e2526":"code","479b5727":"code","2ed3f6a6":"code","84701643":"code","ee28db4b":"code","58012409":"code","62b925e4":"code","f441d6d3":"code","233d8bc8":"code","ce23180f":"code","ffdcaaca":"code","2a0f178f":"code","dc96526b":"code","7673c844":"code","679a1175":"code","bca3f4cf":"code","df53e99c":"code","41abe1be":"code","7acd137e":"code","9ce4b7c5":"code","2c9cbd3c":"code","6a239096":"code","688a2928":"code","e73bd18c":"code","ae37c4a3":"code","cb7344af":"code","170da201":"code","5ac8e6a2":"code","fd4fed5f":"code","a39e0d3a":"code","067794c6":"code","b0dd1086":"code","a14595ea":"code","e2d1e922":"code","489f8532":"code","7519d93f":"code","e3bcfbbc":"code","f879b485":"code","c5c9de9d":"code","68192b35":"code","db304d33":"code","1e9915dd":"code","e1cfd9d8":"code","091e6491":"code","43c05021":"code","31e8cf53":"code","1426bbd6":"code","31d34126":"code","f937225e":"code","624fe1d6":"code","4b130869":"code","beb67334":"code","e44dc53d":"code","982c8fee":"code","5b91f04b":"code","309819f0":"code","ca4f4b00":"code","b916264c":"code","da61ceb4":"code","531ba36f":"code","dd6dc1ef":"code","4261a633":"code","d6df8ae5":"code","a80096a0":"code","7c828fb3":"code","1d899373":"code","92ccf818":"code","2906b6f3":"code","8bab14a5":"code","9300dc2e":"code","1b1d23d4":"code","2c132c7a":"code","37e80072":"code","c90d84f4":"code","bc033b63":"code","63f5f9e1":"code","ad764918":"code","c7088155":"code","daa61f3c":"code","f7b0a0b1":"code","1ee72bf1":"code","e83cf418":"code","8ab8b191":"code","0f330794":"code","0dc2d4be":"code","5db4602b":"code","e368ca2a":"code","dc814efc":"code","fa3aebff":"code","a8e83732":"code","c3790f07":"code","1941a635":"code","16563473":"code","775215c1":"code","f49776e4":"code","6ef61953":"code","35a2e21c":"code","5084496b":"code","6be5912c":"markdown","7ea2f0f9":"markdown","c264e555":"markdown","cf38ba09":"markdown","e18fa7b3":"markdown","d0c0e709":"markdown","9f04cf80":"markdown","0d598569":"markdown","2a3e3069":"markdown","c6d49e04":"markdown","8fdba060":"markdown","935174e9":"markdown","cbfdf3ba":"markdown","06912a2a":"markdown","0fc31eec":"markdown","5d279ffc":"markdown","dad37ce2":"markdown","0f313ceb":"markdown","cbf8ce56":"markdown","75a85ef0":"markdown","f41daebc":"markdown","179e0ac6":"markdown","63445995":"markdown","4d607cba":"markdown","197de2ff":"markdown","04cfad5d":"markdown","37487051":"markdown"},"source":{"ae486b30":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport time\n\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)","f92e297f":"%%time\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntrain_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntest_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")","20ed5ca2":"print (\"Training Transaction Dataset Shape: {}\".format(train_transaction.shape))\nprint (\"Testing Transaction Dataset Shape: {}\".format(test_transaction.shape))\nprint (\"Training Identity Dataset Shape: {}\".format(train_identity.shape))\nprint (\"Testing Identity Dataset Shape: {}\".format(test_identity.shape))","0daebd99":"print (\"Unique Transaction ID's in Training Set\",train_transaction['TransactionID'].nunique())\nprint (\"Unique Transaction ID's in Training Identity Set\",train_identity['TransactionID'].nunique())\nprint (\"-------------------------------------------------------------\")\nprint (\"Unique Transaction ID's in Testing Set\",test_transaction['TransactionID'].nunique())\nprint (\"Unique Transaction ID's in Testing Identity Set\",test_identity['TransactionID'].nunique())","93592619":"%%time\ntrain = pd.merge(left=train_transaction,right=train_identity,on='TransactionID',how='left')\ntest = pd.merge(left=test_transaction,right=test_identity,on='TransactionID',how='left')","1bfd646b":"del train_transaction, test_transaction, train_identity, test_identity","258bd967":"train.drop(\"TransactionID\",axis=1,inplace=True)\nSubmission = test[['TransactionID']]\ntest.drop(\"TransactionID\",axis=1,inplace=True)","f9ca2ef6":"print (train.shape)\nprint (test.shape)","e96592a2":"missing_train = []\nmissing_test = []\nfor col in train.columns:\n    if train[col].isna().sum() \/ len(train) > 0.90:\n        missing_train.append(col)\nfor col in test.columns:\n    if test[col].isna().sum() \/ len(test) > 0.90:\n        missing_test.append(col)\nprint (\"Following are the columns in Training Data with more than 90% missing values:\",missing_train)\nprint (\"Following are the columns in Test Data with more than 90% missing values:\",missing_test)","ec67800c":"common_elements = list(set(missing_train).intersection(missing_test))\ncommon_elements","c35be0e5":"train.drop(common_elements,axis=1,inplace=True)\ntest.drop(common_elements,axis=1,inplace=True)","a7bd3267":"train['ProductCD'].nunique()\n# There are 5 Levels in ProductCD variable. ","22c99ccf":"train['ProductCD'].value_counts(dropna=False)\n# Product Code of W takes the maximum values and S takes the least. There are no missing values in the variable.","8a8ac78b":"train.groupby('ProductCD')['isFraud'].value_counts(dropna=False).unstack()","42f83c83":"train.groupby('ProductCD')['isFraud'].value_counts().unstack().plot(kind='bar')\n# As Expected, the proportion of Fraudulent Transactions are higher in Product Code of W and C and are least or negligible with\n# Other Product Codes.","72b70537":"test['ProductCD'].value_counts(dropna=False)","007dd755":"card = [c for c in train.columns if c.startswith(\"card\")]\ncard_columns = train[card]\ncard_columns.head()","3ac57252":"for col in card_columns:\n    print (\"Number of Unique Values in {} column are\".format(col),card_columns[col].nunique())","6188e05b":"for col in card_columns:\n    print (\"Number of Unique Values in {} column in test dataset are\".format(col),test[col].nunique())","87343a4e":"for col in card_columns:\n    print (\"Number of Missing Values in {} column in the training set are\".format(col),card_columns[col].isna().sum())","930bd8c8":"for col in card_columns:\n    print (\"Number of Missing Values in {} column in the test set are\".format(col),test[col].isna().sum())","f41136b2":"train['card1'] = train['card1'].astype('category')\ntrain['card1'] = train['card1'].cat.codes\n\ntrain['card2'].fillna(train['card2'].value_counts(dropna=False).index[0],inplace=True)\ntrain['card2'] = train['card2'].astype('category')\ntrain['card2'] = train['card2'].cat.codes\n\ntrain['card3'].fillna(train['card3'].value_counts(dropna=False).index[0],inplace=True)\ntrain['card3'] = train['card3'].astype('category')\ntrain['card3'] = train['card3'].cat.codes\n\ntrain['card4'].fillna(\"Missing\",inplace=True)\ntrain['card4'] = train['card4'].astype('category')\ntrain['card4'] = train['card4'].cat.codes\n\ntrain['card5'].fillna(train['card5'].value_counts(dropna=False).index[0],inplace=True)\ntrain['card5'] = train['card5'].astype('category')\ntrain['card5'] = train['card5'].cat.codes\n\ntrain['card6'].fillna(\"Missing\",inplace=True)\ntrain['card6'] = train['card6'].astype('category')\ntrain['card6'] = train['card6'].cat.codes","c13e2826":"test['card1'] = test['card1'].astype('category')\ntest['card1'] = test['card1'].cat.codes\n\ntest['card2'].fillna(test['card2'].value_counts(dropna=False).index[0],inplace=True)\ntest['card2'] = test['card2'].astype('category')\ntest['card2'] = test['card2'].cat.codes\n\ntest['card3'].fillna(test['card3'].value_counts(dropna=False).index[0],inplace=True)\ntest['card3'] = test['card3'].astype('category')\ntest['card3'] = test['card3'].cat.codes\n\ntest['card4'].fillna(\"Missing\",inplace=True)\ntest['card4'] = test['card4'].astype('category')\ntest['card4'] = test['card4'].cat.codes\n\ntest['card5'].fillna(test['card5'].value_counts(dropna=False).index[0],inplace=True)\ntest['card5'] = test['card5'].astype('category')\ntest['card5'] = test['card5'].cat.codes\n\ntest['card6'].fillna(\"Missing\",inplace=True)\ntest['card6'] = test['card6'].astype('category')\ntest['card6'] = test['card6'].cat.codes","8220291b":"train.groupby('card4')['isFraud'].value_counts().unstack()","5c88048f":"train.groupby('card4')['isFraud'].value_counts().unstack().plot(kind='bar')","c29071b9":"train.groupby('card6')['isFraud'].value_counts().unstack()","592a43c9":"train.groupby('card6')['isFraud'].value_counts().unstack().plot(kind='bar')","19271575":"train[train['addr1'].isna()][['addr1','addr2']].head()\n# Rows with addr1 as missing have addr2 as missing too.","eccd94bd":"test[test['addr1'].isna()][['addr1','addr2']].head()\n# Rows with addr1 as missing have addr2 as missing too.","f98dd216":"print (\"Percentage Of Missing Values in {} Column is:\".format(\"addr1\"),train['addr1'].isna().sum()\/len(train))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"addr1\"),train['addr1'].nunique())\nprint (\"Percentage Of Missing Values in {} Column is:\".format(\"addr2\"),train['addr2'].isna().sum()\/len(train))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"addr2\"),train['addr2'].nunique())","06911859":"print (\"Percentage Of Missing Values in {} Column is:\".format(\"addr1\"),test['addr1'].isna().sum()\/len(test))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"addr1\"),test['addr1'].nunique())\nprint (\"Percentage Of Missing Values in {} Column is:\".format(\"addr2\"),test['addr2'].isna().sum()\/len(test))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"addr2\"),test['addr2'].nunique())","4e46593b":"train['addr1'].value_counts(dropna=False).head()\n# Missing values constitute the Highest number, so creating a new column addr1_Missing indicating Missing values in the column.","ad0e2526":"train['addr2'].value_counts(dropna=False).head()","479b5727":"test['addr1'].value_counts(dropna=False).head()","2ed3f6a6":"test['addr2'].value_counts(dropna=False).head()","84701643":"train['addr1_Missing'] = train['addr1'].isna()\ntrain['addr1'].fillna(\"Missing\",inplace=True)\ntrain['addr1'] = train['addr1'].astype('category').cat.codes\n\ntrain['addr2_Missing'] = train['addr2'].isna()\ntrain['addr2'].fillna(\"Missing\",inplace=True)\ntrain['addr2'] = train['addr2'].astype('category').cat.codes","ee28db4b":"test['addr1_Missing'] = test['addr1'].isna()\ntest['addr1'].fillna(\"Missing\",inplace=True)\ntest['addr1'] = test['addr1'].astype('category').cat.codes\n\ntest['addr2_Missing'] = test['addr2'].isna()\ntest['addr2'].fillna(\"Missing\",inplace=True)\ntest['addr2'] = test['addr2'].astype('category').cat.codes","58012409":"train[['P_emaildomain','R_emaildomain']].head()","62b925e4":"test[['P_emaildomain','R_emaildomain']].head()","f441d6d3":"print (\"Percentage Of Missing Values in {} Column is:\".format(\"P_emaildomain\"),train['P_emaildomain'].isna().sum()\/len(train))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"P_emaildomain\"),train['P_emaildomain'].nunique())\nprint (\"Percentage Of Missing Values in {} Column is:\".format(\"R_emaildomain\"),train['R_emaildomain'].isna().sum()\/len(train))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"R_emaildomain\"),train['R_emaildomain'].nunique())","233d8bc8":"print (\"Percentage Of Missing Values in {} Column is:\".format(\"P_emaildomain\"),test['P_emaildomain'].isna().sum()\/len(test))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"P_emaildomain\"),test['P_emaildomain'].nunique())\nprint (\"Percentage Of Missing Values in {} Column is:\".format(\"R_emaildomain\"),test['R_emaildomain'].isna().sum()\/len(test))\nprint (\"Number Of Unique Values in {} Column are:\".format(\"R_emaildomain\"),test['R_emaildomain'].nunique())","ce23180f":"# Since the R_emaildomain in both Training and Testing datasets has higher percentage of Missing values, it makes sense to delete\ntrain.drop(\"R_emaildomain\",axis=1,inplace=True)\ntest.drop(\"R_emaildomain\",axis=1,inplace=True)","ffdcaaca":"train['P_emaildomain'].value_counts(dropna=False).head()","2a0f178f":"test['P_emaildomain'].value_counts(dropna=False).head()","dc96526b":"# Since there are lot of missing values, it makes sense to create an is_missing columnn with respect to P_emaildomain variable.\ntrain['P_email_Missing'] = train['P_emaildomain'].isna()\ntrain['P_emaildomain'].fillna(\"Missing\",inplace=True)\ntrain['P_emaildomain'] = train['P_emaildomain'].astype('category').cat.codes","7673c844":"test['P_email_Missing'] = test['P_emaildomain'].isna()\ntest['P_emaildomain'].fillna(\"Missing\",inplace=True)\ntest['P_emaildomain'] = test['P_emaildomain'].astype('category').cat.codes","679a1175":"train.groupby('P_emaildomain')['isFraud'].value_counts().unstack().plot(kind='bar',figsize=(14,6))","bca3f4cf":"m_cols = [col for col in train.columns if col.startswith(\"M\")]\ntrain[m_cols].head()","df53e99c":"test[m_cols].head()","41abe1be":"for col in m_cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),train[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),train[col].isna().sum()\/len(train))\n    print (\"--------------------------------------------------------------------------\")","7acd137e":"for col in m_cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),test[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),test[col].isna().sum()\/len(test))\n    print (\"--------------------------------------------------------------------------\")","9ce4b7c5":"train[train['M1'].isna()][['M1','M2','M3']]\n# The number of Missing values across columns M1,M2 and M3 are the same.","2c9cbd3c":"train[train['M7'].isna()][['M7','M8','M9']]\n# The number of Missing values across columns M7,M8 and M9 are the same.","6a239096":"train['M4'].value_counts(dropna=False)","688a2928":"train['M1'].fillna(\"Missing\",inplace=True)\ntrain['M1'] = train['M1'].astype('category').cat.codes\n\ntrain['M2'].fillna(\"Missing\",inplace=True)\ntrain['M2'] = train['M2'].astype('category').cat.codes\n\ntrain['M3'].fillna(\"Missing\",inplace=True)\ntrain['M3'] = train['M3'].astype('category').cat.codes\n\ntrain['M4'].fillna(\"Missing\",inplace=True)\ntrain['M4'] = train['M4'].astype('category').cat.codes\n\ntrain['M5'].fillna(\"Missing\",inplace=True)\ntrain['M5'] = train['M5'].astype('category').cat.codes\n\ntrain['M6'].fillna(\"Missing\",inplace=True)\ntrain['M6'] = train['M6'].astype('category').cat.codes\n\ntrain['M7'].fillna(\"Missing\",inplace=True)\ntrain['M7'] = train['M7'].astype('category').cat.codes\n\ntrain['M8'].fillna(\"Missing\",inplace=True)\ntrain['M8'] = train['M8'].astype('category').cat.codes\n\ntrain['M9'].fillna(\"Missing\",inplace=True)\ntrain['M9'] = train['M9'].astype('category').cat.codes","e73bd18c":"test['M1'].fillna(\"Missing\",inplace=True)\ntest['M1'] = test['M1'].astype('category').cat.codes\n\ntest['M2'].fillna(\"Missing\",inplace=True)\ntest['M2'] = test['M2'].astype('category').cat.codes\n\ntest['M3'].fillna(\"Missing\",inplace=True)\ntest['M3'] = test['M3'].astype('category').cat.codes\n\ntest['M4'].fillna(\"Missing\",inplace=True)\ntest['M4'] = test['M4'].astype('category').cat.codes\n\ntest['M5'].fillna(\"Missing\",inplace=True)\ntest['M5'] = test['M5'].astype('category').cat.codes\n\ntest['M6'].fillna(\"Missing\",inplace=True)\ntest['M6'] = test['M6'].astype('category').cat.codes\n\ntest['M7'].fillna(\"Missing\",inplace=True)\ntest['M7'] = test['M7'].astype('category').cat.codes\n\ntest['M8'].fillna(\"Missing\",inplace=True)\ntest['M8'] = test['M8'].astype('category').cat.codes\n\ntest['M9'].fillna(\"Missing\",inplace=True)\ntest['M9'] = test['M9'].astype('category').cat.codes","ae37c4a3":"train['DeviceType'].value_counts(dropna=False)","cb7344af":"test['DeviceType'].value_counts(dropna=False)","170da201":"train['DeviceType_Missing'] = train['DeviceType'].isna()\ntrain['DeviceType'].fillna(\"Missing\",inplace=True)","5ac8e6a2":"test['DeviceType_Missing'] = test['DeviceType'].isna()\ntest['DeviceType'].fillna(\"Missing\",inplace=True)","fd4fed5f":"train.groupby('DeviceType')['isFraud'].mean()\n# Although the Number of Mobile devices are less in number, the percentage of Fraudulent Transactions is highest.","a39e0d3a":"train.groupby('DeviceType')['isFraud'].value_counts().unstack().plot(kind='bar',figsize=(10,6))\nplt.ylabel(\"Count\")\nplt.title(\"Fraudulent Transactions Grouped By Device Type\")","067794c6":"print (\"Number Of Unique Values in {} Variable are:\".format(\"DeviceInfo\"),train['DeviceInfo'].nunique())","b0dd1086":"train['DeviceInfo'].value_counts(dropna=False).head()","a14595ea":"print (\"Number Of Unique Values in {} Variable are:\".format(\"DeviceInfo\"),test['DeviceInfo'].nunique())","e2d1e922":"train['DeviceInfo_Missing'] = train['DeviceInfo'].isna()\ntrain['DeviceInfo'].fillna(\"Missing\",inplace=True)\ntrain['DeviceInfo'] = train['DeviceInfo'].astype('category').cat.codes","489f8532":"test['DeviceInfo_Missing'] = test['DeviceInfo'].isna()\ntest['DeviceInfo'].fillna(\"Missing\",inplace=True)\ntest['DeviceInfo'] = test['DeviceInfo'].astype('category').cat.codes","7519d93f":"cols = [col for col in train.columns if col.startswith(\"id\")]\nid_cols = train[cols]\nid_cols.head()","e3bcfbbc":"test[cols].head()","f879b485":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),train[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),train[col].isna().sum()\/len(train))\n    print (\"--------------------------------------------------------------------------\")","c5c9de9d":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),test[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),test[col].isna().sum()\/len(test))\n    print (\"--------------------------------------------------------------------------\")","68192b35":"common_id = []\nfor col in id_cols:\n    if (train[col].isna().sum()\/len(train) > 0.75) & (test[col].isna().sum()\/len(test) > 0.75):\n        common_id.append(col)\nprint (common_id)","db304d33":"train.drop(common_id,axis=1,inplace=True)\ntest.drop(common_id,axis=1,inplace=True)","1e9915dd":"cols = [col for col in train.columns if col.startswith(\"id\")]\nid_cols = train[cols]\nid_cols.head()","e1cfd9d8":"for col in cols:\n    train[col+\"_Missing\"] = train[col].isna()\n    \nfor col in cols:\n    test[col+\"_Missing\"] = test[col].isna()","091e6491":"for col in cols:\n    train[col].fillna(\"Missing\",inplace=True)\n\nfor col in cols:\n    test[col].fillna(\"Missing\",inplace=True)","43c05021":"for col in cols:\n    train[col] = train[col].astype('category').cat.codes\n\nfor col in cols:\n    test[col] = test[col].astype('category').cat.codes","31e8cf53":"train['isFraud'].value_counts(dropna=False)\/len(train)","1426bbd6":"sns.countplot(train['isFraud'])\nplt.xlabel(\"isFraud\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of isFraud(Dependent) Variable\")","31d34126":"train['dist1'].describe()","f937225e":"test['dist1'].describe()","624fe1d6":"train['dist1'].fillna(train['dist1'].median(),inplace=True)\ntest['dist1'].fillna(test['dist1'].median(),inplace=True)","4b130869":"train[train['TransactionAmt'].isna()]\n# No Missing Values Here","beb67334":"test[test['TransactionAmt'].isna()]","e44dc53d":"train['TransactionAmt'].describe()","982c8fee":"test['TransactionAmt'].describe()","5b91f04b":"train.groupby('isFraud')['TransactionAmt'].agg({'min','max','mean','median'})","309819f0":"sns.distplot(train['TransactionAmt'],kde=False)\nplt.xlabel(\"Transaction Amounts\")\nplt.ylabel(\"Distribution\")\nplt.title(\"Distribution Plot of Transaction Amount variable\")","ca4f4b00":"sns.distplot(np.log(train['TransactionAmt']),kde=False)\nplt.xlabel(\"Transaction Amounts\")\nplt.ylabel(\"Distribution\")\nplt.title(\"Distribution Plot of 'Log of Transaction Amount' variable\")\n# More or less this looks like a normal distribution.","b916264c":"sns.distplot(np.log(test['TransactionAmt']),kde=False)\nplt.xlabel(\"Transaction Amounts\")\nplt.ylabel(\"Distribution\")\nplt.title(\"Distribution Plot of 'Log of Transaction Amount' variable\")\n# More or less this looks like a normal distribution.","da61ceb4":"cols = [col for col in train.columns if col.startswith(\"C\")]\nc_cols = train[cols]\nc_cols.head()","531ba36f":"test[cols].head()","dd6dc1ef":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),train[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),train[col].isna().sum()\/len(train))\n    print (\"--------------------------------------------------------------------------\")","4261a633":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),test[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),test[col].isna().sum()\/len(test))\n    print (\"--------------------------------------------------------------------------\")","d6df8ae5":"for col in cols:\n    test[col].fillna(test[col].value_counts(dropna=False).index[0],inplace=True)","a80096a0":"cols = [\"D1\",\"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\"D7\",\"D8\",\"D9\",\"D10\",\"D11\",\"D12\",\"D13\",\"D14\",\"D15\"]\nd_cols = train[cols]\nd_cols.head()","7c828fb3":"test[cols].head()","1d899373":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),train[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),train[col].isna().sum()\/len(train))\n    print (\"--------------------------------------------------------------------------\")","92ccf818":"for col in cols:\n    print (\"Number of Unique Values in {} Column are:\".format(col),test[col].nunique())\n    print (\"Percentage of Missing Values in {} Column is:\".format(col),test[col].isna().sum()\/len(test))\n    print (\"--------------------------------------------------------------------------\")","2906b6f3":"common_d = []\nfor col in cols:\n    if (train[col].isna().sum()\/len(train) > 0.8) & (test[col].isna().sum()\/len(test) > 0.8):\n        common_d.append(col)\nprint (common_d)    ","8bab14a5":"train.drop([\"D7\",\"D8\",\"D12\"],axis=1,inplace=True)\ntest.drop([\"D7\",\"D8\",\"D12\"],axis=1,inplace=True)","9300dc2e":"cols = [\"D1\",\"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\"D9\",\"D10\",\"D11\",\"D13\",\"D14\",\"D15\"]","1b1d23d4":"for col in cols:\n    train[col].fillna(0.0,inplace=True)\n    \nfor col in cols:\n    test[col].fillna(0.0,inplace=True)","2c132c7a":"cols = [col for col in train.columns if col[0]==\"V\"]\nv_cols = train[cols]\nv_cols.head()","37e80072":"test[cols].head()","c90d84f4":"for col in cols:\n    if train[col].nunique() == 2:\n        print (\"Unique Values in {} Column in Training Set are:\".format(col),train[col].unique())\n        print (\"Percentage of Missing Values in {} Column in Training Set is:\".format(col),train[col].isna().sum()\/len(train))\n        print (\"--------------------------------------------------------------------------\")","bc033b63":"for col in cols:\n    if test[col].nunique() == 2:\n        print (\"Unique Values in {} Column in Test Set are:\".format(col),test[col].unique())\n        print (\"Percentage of Missing Values in {} Column in Test Set is:\".format(col),test[col].isna().sum()\/len(test))\n        print (\"--------------------------------------------------------------------------\")","63f5f9e1":"for col in [\"V1\",\"V14\",\"V41\",\"V65\",\"V88\",\"V107\",\"V305\"]:\n    print (\"Value Counts in {} Variable:\".format(col))\n    print (train[col].value_counts(dropna=False))","ad764918":"for col in [\"V1\",\"V14\",\"V41\",\"V65\",\"V88\",\"V107\",\"V305\"]:\n    train[col].fillna(train[col].value_counts(dropna=False).index[0],inplace=True)\nfor col in [\"V1\",\"V14\",\"V41\",\"V65\",\"V88\",\"V107\",\"V305\"]:\n    test[col].fillna(test[col].value_counts(dropna=False).index[0],inplace=True)","c7088155":"common_v = []\nfor col in cols:\n    if (train[col].isna().sum()\/len(train) > 0.8) & (test[col].isna().sum()\/len(test) > 0.8):\n        common_v.append(col)\nprint (common_v) ","daa61f3c":"train.drop(common_v,axis=1,inplace=True)\ntest.drop(common_v,axis=1,inplace=True)","f7b0a0b1":"cols = [col for col in train.columns if col[0]==\"V\"]\nfor col in cols:\n    train[col].fillna(train[col].median(),inplace=True)\nfor col in cols:\n    test[col].fillna(test[col].median(),inplace=True)","1ee72bf1":"overfit = []\nfor col in train.columns:\n    counts = train[col].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 >99:\n        overfit.append(col)\nprint (len(overfit))\nprint (overfit)","e83cf418":"train.drop(overfit,axis=1,inplace=True)\ntest.drop(overfit,axis=1,inplace=True)","8ab8b191":"train.shape, test.shape","0f330794":"%%time\n# Let's check the correlation between the variables and eliminate the one's that have high correlation\n# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()","0dc2d4be":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","5db4602b":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))","e368ca2a":"train.drop(to_drop,axis=1,inplace=True)\ntest.drop(to_drop,axis=1,inplace=True)","dc814efc":"train.shape, test.shape","fa3aebff":"X = train[[col for col in train.columns if col!='isFraud']]\ny = train['isFraud']","a8e83732":"X = pd.get_dummies(X)\ntest = pd.get_dummies(test)\nprint (train.shape)\nprint (test.shape)","c3790f07":"X_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.25)\nprint (X_Train.shape)\nprint (y_Train.shape)\nprint (X_Test.shape)\nprint (y_Test.shape)","1941a635":"\"\"\"clf = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',random_state=42,class_weight='balanced',n_jobs=-1,verbose=1)\nparams = {\"max_depth\":[3,4,5,6,-1],\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.7,0.9],\n          \"colsample_bytree\":[0.5,0.7,0.9],\n          \"reg_alpha\":[0.5,1,2],\n          \"reg_lambda\":[0.5,1,2],\n          \"num_leaves\":[7,15,31,63],\n          \"n_estimators\":list(range(50,300,50))}\nrandom_search = RandomizedSearchCV(estimator=clf,param_distributions=params,cv=5,scoring='roc_auc')\nrandom_search.fit(X_Train,y_Train)\"\"\"","16563473":"clf = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1)\nparams = {\"max_depth\":[3,5,7,9],\n          \"n_estimators\":list(range(50,301,50)),\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.7,0.9],\n          \"colsample_bytree\":[0.5,0.7,0.9],\n          \"reg_alpha\":[0.5,1,2,5],\n          \"reg_lambda\":[0.5,1,2,5]}\nrandom_search = RandomizedSearchCV(estimator=clf,param_distributions=params,cv=5,scoring='roc_auc')\nrandom_search.fit(X_Train,y_Train)","775215c1":"random_search.best_estimator_,random_search.best_params_,random_search.best_score_","f49776e4":"ser = pd.Series(random_search.best_estimator_.feature_importances_,X_Train.columns).sort_values()\nlst = list(ser[ser > 0].index)\nprint (lst)","6ef61953":"X = X[lst]\ntest = test[lst]\n\nX_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.25)\nprint (X_Train.shape)\nprint (y_Train.shape)\nprint (X_Test.shape)\nprint (y_Test.shape)\n\nclf = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1)\nparams = {\"max_depth\":[3,5,7,9],\n          \"n_estimators\":list(range(50,301,50)),\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.7,0.9],\n          \"colsample_bytree\":[0.5,0.7,0.9],\n          \"reg_alpha\":[0.5,1,2,5],\n          \"reg_lambda\":[0.5,1,2,5]}\nrandom_search = RandomizedSearchCV(estimator=clf,param_distributions=params,cv=5,scoring='roc_auc')\nrandom_search.fit(X_Train,y_Train)","35a2e21c":"random_search.best_estimator_,random_search.best_params_,random_search.best_score_","5084496b":"Submission['isFraud'] = random_search.best_estimator_.predict_proba(test)[:,1]\nSubmission.to_csv(\"XGBoost.csv\",index=None)","6be5912c":"##### TransactionAmt Variable","7ea2f0f9":"1. There is some difference between the Mean Transaction Amounts of Fraudulent and Non Fraudulent Cases.\n2. The Max value in Non Fraudulent case may be an anomaly.\n3. There is difference in the Median values as well.","c264e555":"#### Addr1 and Addr2 Variables","cf38ba09":"This is a classic case of Class Imbalance as it occurs in real life. Fraudulent transactions form just 3.5% of the total number of transactions.","e18fa7b3":"#### C1 to C14 Variables.","d0c0e709":"#### V1 - V339 Variables","9f04cf80":"1. Except Column D1 rest of the columns have high percentage of Missing values.\n2. Except D9, rest of the columns have lot of Unique Values. This can be attributed to the fact that it is pertaining to the time of the day (in terms of Hours).","0d598569":"#### ProductCD Variable","2a3e3069":"#### Correlation Between Variables.","c6d49e04":"1. There are a lot of Missing values in this variables.\n2. Maximum distance is 10286 Units, considering the distribution of this variable, this definitely has to be an outlier.\n3. The Minimum distance of 0 Units also doesn't make sense.","8fdba060":"1. Most of the M1-M9 columns have either 2 or 3 Unique Values.\n2. All of these columns have a lot of Missing Values.\n3. Except M4, rest of the columns have either True or False Values.","935174e9":"#### M1 to M9 Variables","cbfdf3ba":"#### Device Type Variable","06912a2a":"1. The Minimum percentage of values missing in id_XX columns is around 75%. This can be attributed to the fact that the train_identity data had Details with respect to only a fraction of TransactionID's in the transaction dataset. \n2. Some of these variables have high cardinality, like id_02 for example. Some of them are Binary in nature.","0fc31eec":"1. None of the Columns C1-C14 have missing values. \n2. Some of these have high cardinality.","5d279ffc":"#### Exploring the Dependent isFraud Variable","dad37ce2":"The columns in the Overfit list are dropped from the dataset because one of the values in the column represents more than 99% of the data. This will basically tilt the importance towards that value. Hence those columns are dropped.","0f313ceb":"#### dist1 and dist2 variables.","cbf8ce56":"#### Numerical Variables","75a85ef0":"#### Device Info Variable","f41daebc":"#### D1 to D15 Variables.","179e0ac6":"As we can see above, card1,card2,card3 and card5 have a very high cardinality, specially column card1.","63445995":"#### id_01 to id_38 Variables.","4d607cba":"#### Card1-Card6 Variables.","197de2ff":"#### EDA - Categorical Variables\n\nAccording to the Data Description, ProductCD, card1-card6, addr1,addr2, P_emaildomain and R_emaildomain, M1-M9, DeviceType, DeviceInfo, id_12-id_38 are all Categorical variables. The levels in these variables may vary.","04cfad5d":"#### P_emaildomain and R_emaildomain Variables","37487051":"### Model Building"}}