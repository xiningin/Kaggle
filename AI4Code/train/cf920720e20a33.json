{"cell_type":{"550674cb":"code","96315a23":"code","758a48cf":"code","30e00ce7":"code","91bdbdab":"code","3fe0ea3f":"code","6a4d026f":"code","7555b83d":"markdown","286eb0e4":"markdown","65d45534":"markdown","900df538":"markdown","0842bd81":"markdown","08cff7ec":"markdown","5c360ab9":"markdown","5edc2376":"markdown","de298aa7":"markdown","a7be39f5":"markdown","fc15f88f":"markdown","c415ac8f":"markdown","9f40e547":"markdown","3db8107d":"markdown","dbb18a26":"markdown","694f203b":"markdown"},"source":{"550674cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96315a23":"! pip install pyod","758a48cf":"import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.font_manager","30e00ce7":"from pyod.models.abod import ABOD\nfrom pyod.models.knn import KNN","91bdbdab":"from pyod.utils.data import generate_data, get_outliers_inliers\n\n#generate random data with two features\nX_train, Y_train = generate_data(n_train=200,train_only=True, n_features=2)\n\n# by default the outlier fraction is 0.1 in generate data function \noutlier_fraction = 0.1\n\n# store outliers and inliers in different numpy arrays\nx_outliers, x_inliers = get_outliers_inliers(X_train,Y_train)\n\nn_inliers = len(x_inliers)\nn_outliers = len(x_outliers)\n\n#separate the two features and use it to plot the data \nF1 = X_train[:,[0]].reshape(-1,1)\nF2 = X_train[:,[1]].reshape(-1,1)\n\n# create a meshgrid \nxx , yy = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))\n\n# scatter plot \nplt.scatter(F1,F2)\nplt.xlabel('F1')\nplt.ylabel('F2') ","3fe0ea3f":"classifiers = {\n     'Angle-based Outlier Detector (ABOD)'   : ABOD(contamination=outlier_fraction),\n     'K Nearest Neighbors (KNN)' :  KNN(contamination=outlier_fraction)\n}","6a4d026f":"#set the figure size\nplt.figure(figsize=(10, 10))\n\nfor i, (clf_name,clf) in enumerate(classifiers.items()) :\n    # fit the dataset to the model\n    clf.fit(X_train)\n\n    # predict raw anomaly score\n    scores_pred = clf.decision_function(X_train)*-1\n\n    # prediction of a datapoint category outlier or inlier\n    y_pred = clf.predict(X_train)\n\n    # no of errors in prediction\n    n_errors = (y_pred != Y_train).sum()\n    print('No of Errors : ',clf_name, n_errors)\n\n    # rest of the code is to create the visualization\n\n    # threshold value to consider a datapoint inlier or outlier\n    threshold = stats.scoreatpercentile(scores_pred,100 *outlier_fraction)\n\n    # decision function calculates the raw anomaly score for every point\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n    Z = Z.reshape(xx.shape)\n\n    subplot = plt.subplot(1, 2, i + 1)\n\n    # fill blue colormap from minimum anomaly score to threshold value\n    subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(), threshold, 10),cmap=plt.cm.Blues_r)\n\n    # draw red contour line where anomaly score is equal to threshold\n    a = subplot.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n\n    # fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n    subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n\n    # scatter plot of inliers with white dots\n    b = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1], c='white',s=20, edgecolor='k') \n    # scatter plot of outliers with black dots\n    c = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1], c='black',s=20, edgecolor='k')\n    subplot.axis('tight')\n\n    subplot.legend(\n        [a.collections[0], b, c],\n        ['learned decision function', 'true inliers', 'true outliers'],\n        prop=matplotlib.font_manager.FontProperties(size=10),\n        loc='lower right')\n\n    subplot.set_title(clf_name)\n    subplot.set_xlim((-10, 10))\n    subplot.set_ylim((-10, 10))\nplt.show() ","7555b83d":"### Table of Contents\n1. What is an Outlier?\n2. Why do we need to detect Outliers?\n3. Why should we use PyOD for Outlier Detection?\n4. Features of the PyOD library\n5. Installing PyOD in Python\n6. Some Outlier Detection Algorithms used in PyOD\n7. Extra Utilities provided by PyOD\n8. Implementation of PyoD in Python","286eb0e4":"### Installing PyOD in Python\nTime to power up our Python notebooks! Let\u2019s first install PyOD on our machines:","65d45534":"As simple as that!\n\nNote that PyOD also contains some neural network based models which are implemented in Keras. PyOD will NOT install Keras or TensorFlow automatically. You will need to install Keras and other libraries manually if you want to use neural net based models.\n\n ","900df538":"Now, we\u2019ll import the models we want to use to detect the outliers in our dataset. We will be using ABOD (Angle Based Outlier Detector) and KNN (K Nearest Neighbors):","0842bd81":"Our tendency is to use straightforward methods like box plots, histograms and scatter-plots to detect outliers. But **dedicated outlier detection algorithms are extremely valuable in fields which process large amounts of data and require a means to perform pattern recognition in larger datasets.**\n\nApplications like fraud detection in finance and intrusion detection in network security require intensive and accurate techniques to detect outliers. Can you imagine how embarrassing it would be if you detected an outlier and it turned out to be genuine?\n\nThe PyOD library can step in to bridge this gap. Let\u2019s see what it\u2019s all about.\n\n### Why should we use PyOD for Outlier Detection?\nNumerous outlier detection packages exist in various programming languages. I particularly found these languages helpful in R. But when I switched to Python, there was a glaring lack of an outlier detection library. How was this even possible?!\n\nExisting implementations like PyNomaly are not specifically designed for outlier detection (though it\u2019s still worth checking out!). To fill this gap, Yue Zhao, Zain Nasrullah, and Zheng Li designed and implemented the PyOD library.\n\n**PyOD is a scalable Python toolkit for detecting outliers in multivariate data. It provides access to around 20 outlier detection algorithms under a single well-documented API.**\n\n \n### Features of PyOD\nPyOD has several advantages and comes with quite a few useful features. Here\u2019s my pick of the bunch:\n\n* **Open Source with detailed documentation and examples** across various algorithms\n* Supports advanced models, including Neural Networks, Deep Learning, and Outlier Ensembles\n* **Optimized performance with JIT** (Just in Time) and parallelization using numba and joblib\n* **Compatible with both Python 2 & 3**","08cff7ec":"Fit the data to each model we have added in the dictionary, Then, see how each model is detecting outliers:","5c360ab9":"Now, we will create a random dataset with outliers and plot it.","5edc2376":"###  PyOD on a Simulated Dataset\nFirst, let\u2019s import the required libraries:","de298aa7":"Create a dictionary and add all the models that you want to use to detect the outliers:","a7be39f5":"### Introduction\nMy latest data science project involved predicting the sales of each product in a particular store. There were several ways I could approach the problem. But no matter which model I used, my accuracy score would not improve.\n\nI figured out the problem after spending some time inspecting the data \u2013 outliers!\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/Outliers.jpeg?raw=true'>\n\nThis is a commonly overlooked mistake we tend to make. The temptation is to start building models on the data you\u2019ve been given. But that\u2019s essentially setting yourself up for failure.\n\nThere are no shortcuts to data exploration. Building models will only get you so far if you\u2019ve skipped this stage of your data science project. After a point of time, you\u2019ll hit the accuracy ceiling \u2013 the model\u2019s performance just won\u2019t budge.\n\n[Data exploration](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/?utm_source=outlierdetectionpyod&utm_medium=blog) consists of many things, such as variable identification, treating missing values, feature engineering, etc. Detecting and treating outliers is also a major cog in the data exploration stage. The quality of your inputs decide the quality of your output!\n\nPyOD is one such library to detect outliers in your data. It provides access to more than 20 different algorithms to detect outliers and is compatible with Python 3. An absolute gem!\n\nIn this article, I will take you on a journey to understand outliers and how you can detect them using PyOD in Python.\n\n*This article assumes you have a basic knowledge of machine learning algorithms and the Python language. You can refer to this article -[\"Essentials of Machine Learning\"](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/common-machine-learning-algorithms\/?utm_source=outlierdetectionpyod&utm_medium=blog), to understand or refresh these concepts*","fc15f88f":"# An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library","c415ac8f":"### What is an Outlier?\n**An outlier is any data point which differs greatly from the rest of the observations in a dataset**. Let\u2019s see some real life examples to understand outlier detection:\n\nWhen one student averages over 90% while the rest of the class is at 70% \u2013 a clear outlier\nWhile analyzing a certain customer\u2019s purchase patterns, it turns out there\u2019s suddenly an entry for a very high value. While most of his\/her transactions fall below Rs. 10,000, this entry is for Rs. 1,00,000. It could be an electronic item purchase \u2013 whatever the reason, it\u2019s an outlier in the overall data\nHow about Usain Bolt? Those record breaking sprints are definitely outliers when you factor in the majority of athletes\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/outliervisual.jpeg?raw=true'>\n\nThere are a plethora of reasons why outliers exist. Perhaps an analyst made an error in the data entry, or the machine threw up an error in measurement, or the outlier could even be intentional! Some people do not want to disclose their information and hence input false information in forms.\n\nOutliers are of two types: **Univariate** and **Multivariate**. A univariate outlier is a data point that consists of extreme values in one variable only, whereas a multivariate outlier is a combined unusual score on at least two variables. Suppose you have three different variables \u2013 X, Y, Z. If you plot a graph of these in a 3-D space, they should form a sort of cloud. All the data points that lie outside this cloud will be the multivariate outliers.\n\n**I would highly recommend you to read this amazing [guide](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/?utm_source=outlierdetectionpyod&utm_medium=blog) on data exploration which covers outliers in detail.**","9f40e547":"### Implementation of PyOD\nEnough talk \u2013 let\u2019s see some action. In this section, we\u2019ll implement the PyOD library in Python. I\u2019m going to use two different approaches to demonstrate PyOD:\n\n* Using a simulated dataset\n* Using a real-world dataset (will cover in next notebook)","3db8107d":"### Why do we need to Detect Outliers?\nOutliers can impact the results of our analysis and statistical modeling in a drastic way. Check out the below image to visualize what happens to a model when outliers are present versus when they have been dealt with:\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/impact-of-outliers.jpeg?raw=true'>\n\n**But here\u2019s the caveat \u2013 outliers aren\u2019t always a bad thing.** It\u2019s very important to understand this. Simply removing outliers from your data without considering how they\u2019ll impact the results is a recipe for disaster.\n\n> \u201cOutliers are not necessarily a bad thing. These are just observations that are not following the same pattern as the other ones. But it can be the case that an outlier is very interesting. For example, if in a biological experiment, a rat is not dead whereas all others are, then it would be very interesting to understand why. This could lead to new scientific discoveries.  So, it is important to detect outliers.\u201d\n                                                                                                        \u2013 Pierre Lafaye de Micheaux, Author and Statistician","dbb18a26":"### Outlier Detection Algorithms used in PyOD\nLet\u2019s see the outlier detection algorithms that power PyOD. It\u2019s well and good implementing PyOD but I feel it\u2019s equally important to understand how it works underneath. This will give you more flexibility when you\u2019re using it on a dataset.\n\nNote: We will be using a term **Outlying score** in this section. It means every model, in some way, scores a data point than uses threshold value to determine whether the point is an outlier or not.\n\n \n### Angle-Based Outlier Detection (ABOD)\n* It considers the relationship between each point and its neighbor(s). It does not consider the relationships among these neighbors. The variance of its weighted cosine scores to all neighbors could be viewed as the outlying score\n* ABOD performs well on multi-dimensional data\n* PyOD provides two different versions of ABOD:\n    * **Fast ABOD**: Uses k-nearest neighbors to approximate\n    * **Original ABOD**: Considers all training points with high-time complexity\n   \n### k-Nearest Neighbors Detector\n* For any data point, the distance to its kth nearest neighbor could be viewed as the outlying score\n* PyOD supports three kNN detectors:\n    * Largest: Uses the distance of the kth neighbor as the outlier score\n    * Mean: Uses the average of all k neighbors as the outlier score\n    * Median: Uses the median of the distance to k neighbors as the outlier score\n \n\n### Isolation Forest\n* It uses the scikit-learn library internally. In this method, data partitioning is done using a set of trees. Isolation Forest provides an anomaly score looking at how isolated the point is in the structure. The anomaly score is then used to identify outliers from normal observations\n* Isolation Forest performs well on multi-dimensional data\n \n\n### Histogram-based Outlier Detection\n* It is an efficient unsupervised method which assumes the feature independence and calculates the outlier score by building histograms\n* It is much faster than multivariate approaches, but at the cost of less precision\n \n### Feature Bagging\n* A feature bagging detector fits a number of base detectors on various sub-samples of the dataset. It uses averaging or other combination methods to improve the prediction accuracy\n* By default, Local Outlier Factor (LOF) is used as the base estimator. However, any estimator could be used as the base estimator, such as kNN and ABOD\n* Feature bagging first constructs n sub-samples by randomly selecting a subset of features. This brings out the diversity of base estimators. Finally, the prediction score is generated by averaging or taking the maximum of all base detectors\n \n\n### Clustering Based Local Outlier Factor\n* It classifies the data into small clusters and large clusters. The anomaly score is then calculated based on the size of the cluster the point belongs to, as well as the distance to the nearest large cluster\n \n\n### Extra Utilities provided by PyOD\n* A function generate_data can be used to generate random data with outliers. Inliers data is generated by a multivariate Gaussian distribution and outliers are generated by a uniform distribution.\n* We can provide our own values of outliers fraction and the total number of samples that we want in our dataset. We will use this utility function to create data in the implementation part.\n ","694f203b":"Reference [here](https:\/\/www.analyticsvidhya.com\/blog\/2019\/02\/outlier-detection-python-pyod\/)"}}