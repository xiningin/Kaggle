{"cell_type":{"f5d923c4":"code","ccc78c88":"code","23be1f5d":"code","6ef84c49":"code","9f043b29":"code","0d497c45":"code","e2389517":"code","5b9985e7":"code","4868e4f8":"code","584a56ae":"code","2b6c07c7":"code","a28e93cd":"code","c49c8285":"code","52723e43":"code","827e201b":"code","7f73f10f":"code","6c90f679":"code","42478204":"code","dbab28b8":"code","670b6240":"code","47056a3b":"code","226d771f":"code","68ba3fef":"code","00a0ab7f":"code","7a8561bb":"code","17ccee15":"code","b52e64c4":"code","513be99d":"code","27f188f2":"code","6d9f1958":"code","0f1b46de":"code","d1f30c4d":"code","020f1166":"code","14858648":"code","657e1a6a":"code","677e5982":"code","45513212":"code","e814c9e3":"code","051d69f0":"markdown","f5a59b0e":"markdown","93945503":"markdown","6821184b":"markdown","2e05b9ee":"markdown","b14cf4b6":"markdown","c886763a":"markdown","d836238a":"markdown","5f3f3d61":"markdown","096cc1c4":"markdown","a4bd0694":"markdown","09df1f84":"markdown","8846deb2":"markdown","a5e4ae9f":"markdown","ecded2b4":"markdown","e4708ffa":"markdown","e43d9853":"markdown","ed78b62b":"markdown","ca7678ff":"markdown"},"source":{"f5d923c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ccc78c88":"# Let us Import the Dataset.\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","23be1f5d":"# Top 5 Entries in Training Dataset\ndf_train.head()","6ef84c49":"# This is to check the correlation between the Passenger Class and Survived ones.\ndf_train.groupby(['Pclass', 'Survived']).count()","9f043b29":"# Let Us See the Basic Information about training dataset\ndf_train.info()","0d497c45":"null = df_train.isnull().sum()\nper = null\/len(df_train)*100\nnull = pd.DataFrame(data={'Number of Null Values':null, 'Percentage Of Null Values':per})\nnull","e2389517":"df_train.drop(labels='Cabin', axis=1, inplace=True)\ndf_test.drop(labels='Cabin', axis=1, inplace=True)","5b9985e7":"# Let's replace Age column with mean of the column.\nprint(df_train['Age'].mean())\ndf_train['Age'].fillna(value=df_train['Age'].mean(), inplace=True)\ndf_test['Age'].fillna(value=df_test[\"Age\"].mean(), inplace=True)","4868e4f8":"# Let us visualise the Embarked column.\ndf_train['Embarked'].value_counts().iplot(kind='bar', color='deepskyblue')","584a56ae":"# Replace Embarked column with mode of the column which is 'S'.\n\ndf_train['Embarked'].fillna(value='S',inplace=True)\ndf_test['Embarked'].fillna(value='S', inplace=True)\ndf_train['Embarked'].unique()","2b6c07c7":"df_train.head()","a28e93cd":"combine = [df_train, df_test]\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(df_train['Title'], df_train['Sex'])","c49c8285":"\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","52723e43":"# Mapping with 1,2,3,4,5\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ndf_train.head()","827e201b":"# Let us Drop the Unwanted Columns\ndf_train.drop(labels=['PassengerId','Name','Ticket'], inplace=True, axis=1)\ndf_test.drop(labels=['PassengerId','Name','Ticket'], inplace=True, axis=1)","7f73f10f":"df_train['AgeBand'] = pd.cut(df_train['Age'], 5)\ndf_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","6c90f679":"combine = [df_train, df_test]\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n\ndf_train = df_train.drop(['AgeBand'], axis=1)\ncombine = [df_train, df_test]\ndf_train.head()    \n","42478204":"combine = [df_train, df_test]\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ndf_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","dbab28b8":"# Making New Column \"IsAlone\" which will have values 1 or 0 for whether she\/he is alone or not respectively.\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf_train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","670b6240":"# Droping Parch, SibSp and FamilySize columns.\ndf_train = df_train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndf_test = df_test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [df_train, df_test]\n\ndf_train.head()","47056a3b":"df_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","226d771f":"# Filling Null values in Fare column in test dataset.\ndf_test['Fare'].fillna(value=df_test['Fare'].mean(), inplace=True)","68ba3fef":"# Replacing values of Fare column with 0,1,2,3.\n\ncombine = [df_train, df_test]\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ndf_train = df_train.drop(['FareBand'], axis=1)\ncombine = [df_train, df_test]\n    \ndf_train.head(10)","00a0ab7f":"# Converting dataset into array.\nX_train = df_train.iloc[:, 1:].values\ny_train = df_train.iloc[:, 0].values\nX_test = df_test.iloc[:,:].values","7a8561bb":"# Dealing with categorical features. \n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelencoder_x=LabelEncoder()\nX_train[:, 1]=labelencoder_x.fit_transform(X_train[:,1])  \nX_train[:, 4]=labelencoder_x.fit_transform(X_train[:, 4])\n\nX_test[:, 1]=labelencoder_x.fit_transform(X_test[:,1])  \nX_test[:, 4]=labelencoder_x.fit_transform(X_test[:, 4])\n\n\nonehotencoder_x=OneHotEncoder(categorical_features=[0,2,3,4,5]) \nX_train=onehotencoder_x.fit_transform(X_train).toarray()\nX_test=onehotencoder_x.fit_transform(X_test).toarray()\n","17ccee15":"len(X_train[0]), len(X_test[0])","b52e64c4":"# Scaling the features on the same scale.\n# Scaling of training set and test set must be done with the same sc_x object.\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.fit_transform(X_test)","513be99d":"# Importing tensorflow library\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dropout","27f188f2":"model = tf.keras.Sequential()","6d9f1958":"# Creating Input layer and 1st Hidden layer.\n# Here Dropout is our regularization paramter(penalty parameter), which will deal with problem of overfitting .\n\nmodel.add(tf.keras.layers.Dense(units=12, input_dim=22, activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(rate=0.1))","0f1b46de":"# Creating 4 Hidden layers.\n\nmodel.add(tf.keras.layers.Dense(units=12,  activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(rate=0.1))\nmodel.add(tf.keras.layers.Dense(units=12,  activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(rate=0.1))\nmodel.add(tf.keras.layers.Dense(units=12,  activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(rate=0.1))\nmodel.add(tf.keras.layers.Dense(units=12,  activation='relu', kernel_initializer='uniform'))\nmodel.add(Dropout(rate=0.1))","d1f30c4d":"# Adding Output Layer.\n\nmodel.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))","020f1166":"# Compiling the Model.\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","14858648":"# Trian our Model.\nhistory = model.fit(X_train, y_train, batch_size=50, epochs=500, validation_split=0.2)","657e1a6a":"print(f\"Accuracy of training dataste\\t {np.mean(history.history['acc'])}\")\nprint(f\"Accuracy of Validating dataset\\t {np.mean(history.history['val_acc'])}\")\n","677e5982":"# Prediction on Test dataset.\ny_pred = model.predict(X_test)","45513212":"# Converting the values of y_pred into 0 and 1.\n# 1 means person will Survived and 0 means person will not Survived.\n\nnew_y_pred = []\nfor var in y_pred:\n    if var>=0.7:\n        new_y_pred.append(1)\n    else:\n        new_y_pred.append(0)","e814c9e3":"# Creating Submission file.\n\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_result = pd.DataFrame(data={'Passengerid':df_test['PassengerId'], 'Survived':new_y_pred})\ndf_result.to_csv(\"submission.csv\", index=False)","051d69f0":"### 4). Dealing With \"FareBand\" Column.","f5a59b0e":"### 3). Dealing With \"SibSp\" and \"Parch\" columns.","93945503":"### 2). Drop the Unwanted Columns.","6821184b":"### 2). Feature Engineering in \"Age\" Column.\n* Here first we will see the band of Age and then we will replace values of Age in column which fall in particular band with 1 or 2 or 3 or 4.","2e05b9ee":"* Here we are getting the accuracy of 85%, which is good in my point of view.","b14cf4b6":"* As 77% of values in Cabin Column are Null values then we will drop the Cabin column.\n* As 20% values in 'Age' column are Null values, here we will replace them by mean of the column.","c886763a":"# ANN FROM SCRATCH WITH 85% ACCURACY.","d836238a":"## =============================================================","5f3f3d61":"* Upper class person Survived more then the other class person.","096cc1c4":"### 1). Let us get detail about Null Values.","a4bd0694":"# FEATURE ENGINEERING","09df1f84":"* b). Deal with 'Embarked' column.","8846deb2":"### 1). Dealing With \"Name\" Column.\n* Our Target is to replace the values of Name Column with Values 0,1,2,....\n","a5e4ae9f":"## PREDICTION WITH ANN\n* 1). Here we will deal with MCSS.\n    * M = dealing with Missing data, which we have already done.\n    * C = dealing with Categorical values\n    * S = Spliting of Dataset which we do not required as data is already splited.\n    * S = Scaling of features on same scale.\n* 2). Then we will create our model and then we will Train out model and then we will make prediction on our test dataset.","ecded2b4":"<img src='https:\/\/drive.google.com\/uc?id=14nbP-PHEEtkaALbx0Yihr44qipu0Sz_l' width=1000 >","e4708ffa":"* a). Deal with 'Age' column.","e43d9853":"# BASIC EDA","ed78b62b":"### 3). Dealing With Missing Values.","ca7678ff":"# IF THIS KERNEL IS HELPFUL THEN PLEASE UPVOTE.\n<img src='https:\/\/drive.google.com\/uc?id=1qihsaxx33SiVo5dIw-djeIa5SrU_oSML' width=500 >"}}