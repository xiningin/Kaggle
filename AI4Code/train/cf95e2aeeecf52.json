{"cell_type":{"e1d02718":"code","49b612d4":"code","bb205ab5":"code","a74c9a9f":"code","28d7f52d":"code","78617858":"code","7a10b571":"code","82eab630":"code","b78ca4a6":"code","eb7f56d2":"code","5cf250ea":"code","057f50af":"code","619930e8":"code","194825c8":"code","3aa2b76d":"code","68146698":"code","1233c0ec":"code","19f23ed3":"code","fa7f2c98":"code","933340af":"code","47fed0fd":"code","359bb232":"code","0ef8f96a":"code","740ccf02":"code","6ecc1495":"code","c3945117":"code","c4f11251":"code","84378540":"code","d64646a4":"code","4d88a12a":"code","8ba23f67":"code","789612bb":"code","56938d0e":"code","8a66470a":"code","2fac2656":"code","9434c9c0":"code","6176ab85":"code","4f1baa7b":"code","08a12a44":"code","ca270000":"code","d50893a4":"code","2dd76cca":"code","9569e0d6":"code","f7e5ee1d":"code","20c4e501":"code","35dc4e6c":"code","928bb1b0":"code","7925941c":"code","249b345e":"code","9c3bf632":"code","c4d9a9e2":"code","6b4df167":"code","ccad4ddd":"code","90c28758":"code","8fa21714":"code","9fbb7daf":"code","3d669640":"code","0b0e7cfe":"code","6b5369e4":"code","ea5e964a":"code","bc904e85":"code","e6470e4a":"code","6cb06d54":"code","9e893d52":"code","2f3209b1":"code","8fda343b":"code","1fc1c96e":"markdown","d93c21f8":"markdown","5ed0fbbe":"markdown","7476db5c":"markdown","077e6f2d":"markdown","44939d7c":"markdown","0a4aa176":"markdown","b3b9f942":"markdown","4257f34c":"markdown","37132907":"markdown","9fa04083":"markdown","59dc8633":"markdown","34a751bf":"markdown","547518a0":"markdown","70fa8781":"markdown","597407eb":"markdown","18fcceab":"markdown","e65e32ff":"markdown","1c12b4f2":"markdown","5093299a":"markdown","b889fe5e":"markdown","a72d65c7":"markdown","f42921e2":"markdown","bb97c44b":"markdown","d7fef6c8":"markdown","fc9053b1":"markdown","99de83c9":"markdown","2d4148ca":"markdown","1dc8ea13":"markdown","3ecbb94d":"markdown","99b82c31":"markdown","823f735f":"markdown","8e07fc99":"markdown","4af78395":"markdown","516b09f1":"markdown","6410b2d5":"markdown","89c00b47":"markdown","755559d5":"markdown","40a41568":"markdown","e41bf173":"markdown","668bbc37":"markdown","b0bdddfe":"markdown","aae7da78":"markdown","d37c4edd":"markdown","33390849":"markdown","f79e7980":"markdown","ee7c7f08":"markdown","3f2880a8":"markdown","2396abf2":"markdown","b2c6aba1":"markdown","4ccdd846":"markdown","85942b58":"markdown","0dfeda74":"markdown","53404f9f":"markdown","88833644":"markdown","b3b8c011":"markdown"},"source":{"e1d02718":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nwarnings.filterwarnings('ignore')","49b612d4":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","bb205ab5":"train.isnull().sum().values","a74c9a9f":"train.duplicated(subset='id', keep='first').sum()","28d7f52d":"len(train)-len(train.drop_duplicates())","78617858":"#Looking unique values\nl=dict(train.nunique())\nprint(l)","7a10b571":"train.skew()","82eab630":"# Check the structure of the data\nprint(train.info())","b78ca4a6":"train.describe()","eb7f56d2":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","5cf250ea":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show","057f50af":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","619930e8":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(7, 2,figsize=(20, 24))\nfor feature in num_columns:\n    plt.subplot(7, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","194825c8":"train.corr()['target'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","3aa2b76d":"train.corr().style.background_gradient(cmap='viridis')","68146698":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);\n","1233c0ec":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","19f23ed3":"fig = plt.figure(figsize=(10,5))\nsns.barplot(y=train[cat_columns].nunique().values, x=train[cat_columns].nunique().index, color='blue', alpha=.5)\nplt.xticks(rotation=0)\nplt.title('Number of categorical unique values',fontsize=16);","fa7f2c98":"labels = train['cat7'].astype('category').cat.categories.tolist()\ncounts = train['cat7'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","933340af":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(28, 44))\nfor feature in cat_columns:\n    plt.subplot(5, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","47fed0fd":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data = train, y = 'target', x = 'cat'+str(n),ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","359bb232":"fig = plt.figure(figsize=(26,10))\ngrid =  gridspec.GridSpec(2,5,figure=fig,hspace=.2,wspace=.2)\nn =0\nfor i in range(2):\n    for j in range(5):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data = train, x = 'target', hue = 'cat'+str(n),ax=ax, alpha =.7, fill=False)\n        ax.set_title('cat'+str(n),fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('KDE plot of train target with categorical features', fontsize=16,y=.93);","0ef8f96a":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","740ccf02":"train['target'].describe()","6ecc1495":"train['target'].describe().iloc[1:].plot.barh(color=v0,alpha=.5,figsize=(12,5))\nplt.title('Target data statistics',fontsize=16)\nplt.yticks(fontsize=14)\nplt.xticks(np.arange(0,10.8,.5));","c3945117":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n\nf.suptitle('Target', fontsize=16)\ng = sns.kdeplot(train['target'], shade=True, label=\"%.2f\"%(train['target'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['target'], plot=axes[1])\nsns.boxplot(x='target', data=train, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","c4f11251":"n, bins, patches = plt.hist(x=train['target'], bins='auto', color='blue',alpha=0.7, rwidth=0.5)\nplt.xlabel(\"target\")\nplt.show()","84378540":"y=train['target']\nplt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,.5), label='.5%', c='darkblue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,1), label='1%', c='green', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99.9), label='99.9%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","d64646a4":"np.percentile(y,75)","4d88a12a":"np.percentile(y,100)","8ba23f67":"bins = [0.1403287728456096,  6.558195657350646,9.912509990109717,10.411991752210524]\n# Bin labels\nlabels1 = [ 'MinorityOne ', 'Majority', 'MinorityTwo']\ntrainessai=train.copy()\n# Bin the continuous variable ConvertedSalary using these boundaries\ntrainessai['target_binned'] = pd.cut(trainessai['target'], \n                                bins=bins,labels=labels1 )","789612bb":"labels = trainessai['target_binned'].astype('category').cat.categories.tolist()\ncounts = trainessai['target_binned'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","56938d0e":"m = TSNE(learning_rate=50)\ndf_numeric =trainessai.drop(['id','target'], axis=1).iloc[:1500,:]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(df_numeric)\n%timeit \nprint(tsne_features.shape)","8a66470a":"dataa=trainessai.drop(['id','target'], axis=1).iloc[:1500,:]\ndataa['x']=tsne_features[:, 0]\ndataa['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='target_binned', data=dataa)\n# Show the plot\nplt.show()","2fac2656":"numdata=train[num_columns].copy()\nclus = KMeans(n_clusters=5, max_iter=2000)\nkmean_no_pca= clus.fit_predict(numdata)\ntrain1=train.copy()\ntrain1['cluster'] = kmean_no_pca\ntrain1['cluster'] = train1['cluster'].astype('object')","9434c9c0":"train1.head(3)","6176ab85":"fig = plt.figure(figsize=(18,26))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(7, 2, figure= fig, hspace= .2, wspace= .05)\nn =0\nfor i in range(7):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train1, y='target', x='cont'+str(n), hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title('cont{}'.format(str(n)),fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","4f1baa7b":"# Create a series out of the Country column\ncluster = train1.cluster\n\n# Get the counts of each category\ncluster_counts = cluster.value_counts()\n\n# Print the count values for each category\nprint(cluster_counts)","08a12a44":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cluster_counts.index,cluster_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of cluster')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('cluster', fontsize=12)\nplt.show()","ca270000":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab=pd.crosstab(train1['cluster'], train1['cat1'], margins = True)\nH=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","d50893a4":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))","2dd76cca":"# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.drop(['id','target'], axis=1)","9569e0d6":"X.shape","f7e5ee1d":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","20c4e501":"# select non-numeric columns\ncat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns","35dc4e6c":"# select the float columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns","928bb1b0":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","7925941c":"if set(all_columns) == set(train.drop(['id','target'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","249b345e":"# Import PowerTransformer\nfrom sklearn.preprocessing import PowerTransformer\nnumdata=train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64'])\n# Instantiate PowerTransformer\npow_trans = PowerTransformer()\n\n# Train the transform on the data\npow_trans.fit(numdata[['cont4']])\n\n# Apply the power transform to the data\nnumdata['cont4_LG'] = pow_trans.transform(numdata[['cont4']])\n\n# Plot the data before and after the transformation\nnumdata[['cont4', 'cont4_LG']].hist()\nplt.show()","9c3bf632":"#train_std = train['cont1'].mean()\n#train_mean = train['cont1'].std()\n\n#cut_off = train_std * 3\n#train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n\n# Trim the test DataFrame\n#trimmed_df = so_test_numeric[(train['cont1'] < train_upper) \\\n                            # & (train['cont1'] > train_lower)]","c4d9a9e2":"# Sum the votes of the three models\n#votes = np.sum([mask_LR, mask_RFR, mask_GBR,mask_SVR,\n                #mask_Ridge,mask_Lasso,mask_ElasticNet], axis=0)\n\n# Create a mask for features selected by all 5 models\n#meta_mask = votes >= 4\n\n# Apply the dimensionality reduction on X\n#X_reduced = X.loc[:, meta_mask]\n#print(X.columns)\n#print(X_reduced.columns)\n\n#temp1=set(X.columns)\n#s=set(X_reduced.columns)\n#temp3 = [x for x in temp1 if x not in s]\n#print(\"list to drop {}\".format(temp3))\n","6b4df167":"for col in cat_columns :\n    cat_frenquency = (train[col].value_counts())\/train.shape[0]\n    botton_decile = cat_frenquency.quantile(q=0.1)\n    print(col , cat_frenquency,botton_decile )   ","ccad4ddd":"def condense_category(col, min_freq=0.1, new_name='other'):\n    series = pd.value_counts(col)\n    mask = (series\/series.sum()).lt(min_freq)\n    return pd.Series(np.where(col.isin(series[mask].index), new_name, col))\ntrain_condense=train.copy()\ntrain_condense[cat_columns]=train_condense[cat_columns].apply(condense_category, axis=0)\ntrain_condense[train_condense.select_dtypes(['float64']).columns] = train_condense[train_condense.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain_condense[train_condense.select_dtypes(['object']).columns] = train_condense.select_dtypes(['object']).apply(lambda x: x.astype('category'))","90c28758":"# Create arrays for the features and the response variable\ny_condense = train_condense['target'].to_numpy()\nX_condense = train_condense.drop(['id','target'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train_condense , X_test_condense , y_train_condense , y_test_condense  = train_test_split(X_condense , y_condense , test_size=0.1,random_state=0)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test_condense.shape[0], X_train_condense.shape[0], X_test_condense.shape[1]))","8fa21714":"Encoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            #OneHotEncoder(handle_unknown='ignore'),\n            OrdinalEncoder() \n           \n              )","9fbb7daf":"Scaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        \n                        RobustScaler()\n                        \n)","3d669640":"cross_validation_design = KFold(n_splits=3,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","0b0e7cfe":"from xgboost import XGBRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\n","6b5369e4":"# Cat Features  \nOHE1 = make_pipeline(OneHotEncoder()\n              )\n# Num Features \nRobustScaler1  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                        RobustScaler()\n)\nOHE1_RobustScaler = make_column_transformer(\n    ( OHE1 , cat_columns),\n    ( RobustScaler1, num_columns))\n    \nLassoRobustScaler = Pipeline([\n        ('preprocess', OHE1_RobustScaler),\n        ('classifier', Lasso())])  \n#Fit the pipeline to the training data\nLassoRobustScaler.fit(X_train, y_train)","ea5e964a":"preds_valid = LassoRobustScaler.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","bc904e85":"#rmsecatrobust =cross_val_score(XGBROHE1RobustScaler, X, y, cv=cross_validation_design,scoring='neg_root_mean_squared_error').mean()\n#print(-rmsecatrobust)","e6470e4a":"from sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nLassoRobustScalerPCA = Pipeline([\n        ('preprocess', OHE1_RobustScaler),\n        ('reducer', PCA(n_components=15)),\n        ('classifier', Lasso())])  \n#Fit the pipeline to the training data\nLassoRobustScalerPCA.fit(X_train, y_train)","6cb06d54":"from sklearn import set_config\nset_config(display='diagram')\nLassoRobustScalerPCA","9e893d52":"preds_valid = LassoRobustScalerPCA.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","2f3209b1":"\nmodel_final = LassoRobustScalerPCA\n\n\nmodel_final.fit(X, y)","8fda343b":"\npreds_valid = model_final.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))\ntest_final= test.drop(['id'], axis=1)\n# Use the model to generate predictions\npredictions = model_final.predict(test_final)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,'target': predictions})\noutput.to_csv('model_final.csv', index=False)","1fc1c96e":"###  Outlier Handling \n### Statistical outlier removal\n\nWhile removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together.\nwe can trim data like this :\n","d93c21f8":"## Convert Dtype: ","5ed0fbbe":"# Summuary : \n\n## Features Engineer:\n\nCreating Sparse interectins between categorical features and ploynominal features for num data help us get better rmse \n\nPCA don't help us to get better result \n\n## Features to reduce : \n\nGenerally, widely recognized benefits of featue selection are:\n\n    simplification of models to make them easier to interpret\n    shorter training times,\n    to avoid the curse of dimensionality,\n    more generalization by reducing overfitting (reduction of variance)\n    \n\n\n\npipe :\n\nhttps:\/\/developpaper.com\/pipeline-columntransformer-and-featureunion\n","7476db5c":"## Define the model features and target\n### Extract X and y ","077e6f2d":"### Test data ","44939d7c":"### Distribution of Target","0a4aa176":"## EDA \n\n### Explore the data\n\n    Null Data\n    Categorical data\n    Itrain.isnull().sum().valuess there Text data\n    wich columns will we use\n    IS there outliers that can destory our algo\n    IS there diffrent range of data\n    Curse of dimm...\n\n####  Null Data ","b3b9f942":"## Categorical features distribution","4257f34c":"This plot kinda agrees with previous one but it looks like the KDE of some categorical values are pretty much flat compared to other value.","37132907":"# Train Catboost \/ Xgboost \/ Lgbm\n## Define Baseline XGBR ","9fa04083":"###  Stat","59dc8633":"## check that we have all column","34a751bf":"## Compelete prerocess pipe for  Cat dara ","547518a0":"# Select Best Pipe and retrain on all data ","70fa8781":"It's clear tat there isn't any clear relation between numerical variables and target.\n\nNow Exploring correlation between all numerical variables. First we get a correlation grid of all numercial variables and target\n","597407eb":"### Select best preprocess pipe :\n\nthis job was done in this notebook : \n\nhttps:\/\/www.kaggle.com\/bannourchaker\/1-2eda-selectbestpreprocespipedataxgbr\n","18fcceab":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","e65e32ff":"### Categorical features distribution","1c12b4f2":"### Box plot of numerical columns","5093299a":"# Data Preparation\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling.\n\nWhile this assumption of similar scales is necessary, it is rarely true in real world data. For this reason you need to rescale your data to ensure that it is on the same scale. There are many different approaches to doing this but we will discuss the two most commonly used approaches here, Min-Max scaling (sometimes referred to as **normalization**), and **standardization**.\n\nNormalization,in normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest. When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.)\nNormalization scales all points linearly between the upper and lower bound.\n\n**Log Transformer**\n\nHelps with skewness No predetermined range for scaled data Useful only on non-zero, non-negative data\nThe Log Transform is one of the most popular Transformation techniques out there. It is primarily used to convert a skewed distribution to a normal distribution\/less-skewed distribution. In this transform, we take the log of the values in a column and use these values as the column instead.\n\nWhy does it work? It is because the log function is equipped to deal with large numbers. Here is an example-\n\nlog(10) = 1\n\nlog(100) = 2, and\n\nlog(10000) =4\n\nThus, the log operation had a dual role:\n\n    Reducing the impact of too-low values\n    Reducing the impact of too-high values.\n\nA small caveat though \u2013 if our data has negative values or values ranging from 0 to 1, we cannot apply log transform directly \u2013 since the log of negative numbers and numbers between 0 and 1 is undefined, we would get error or NaN values in our data. In such cases, we can add a number to these values to make them all greater than 1. Then, we can apply the log transform.\n\n**Min-Max Scaler**\n\nRescales to predetermined range [0\u20131] Doesn\u2019t change distribution\u2019s center (doesn\u2019t correct skewness) Sensitive to outliers\n\n**Max Abs Scaler**\n\nRescales to predetermined range [-1\u20131] Doesn\u2019t change distribution\u2019s center Sensitive to outliers\n\nIn simplest terms, the MaxAbs scaler takes the absolute maximum value of each column and divides each value in the column by the maximum value.\n\nThus, it first takes the absolute value of each value in the column and then takes the maximum value out of those. This operation scales the data between the range [-1, 1]\n\n**Standard Scaler**\n\nShifts distribution\u2019s mean to 0 & unit variance No predetermined range Best to use on data that is approximately normally distributed\nFor each feature, the Standard Scaler scales the values such that the mean is 0 and the standard deviation is 1(or the variance).\nx_scaled = x \u2013 mean\/std_dev\n\nHowever, Standard Scaler assumes that the distribution of the variable is normal. Thus, in case, the variables are not normally distributed, we\n\n    either choose a different scaler\n    or first, convert the variables to a normal distribution and then apply this scaler\n\n\n**Robust Scaler**\n\n0 mean & unit variance Use of quartile ranges makes this less sensitive to (a few) outliers No predetermined range\nIf you have noticed in the scalers we used so far, each of them was using values like the mean, maximum and minimum values of the columns. All these values are sensitive to outliers. If there are too many outliers in the data, they will influence the mean and the max value or the min value. Thus, even if we scale this data using the above methods, we cannot guarantee a balanced data with a normal distribution.\n\nThe Robust Scaler, as the name suggests is not sensitive to outliers. This scaler-\n\n    removes the median from the data\n    scales the data by the InterQuartile Range(IQR)\n\nAre you familiar with the Inter-Quartile Range? It is nothing but the difference between the first and third quartile of the variable. The interquartile range can be defined as-\n\n    IQR = Q3 \u2013 Q1\n\nThus, the formula would be:\n\nx_scaled = (x \u2013 Q1)\/(Q3 \u2013 Q1)\n\n**Power Transformer**\n\nHelps correct skewness 0 mean & unit variance No predetermined range Yeo-Johnson or Box-Cox Box-Cox can only be used on non-negative data\n\nI often use this feature transformation technique when I am building a linear model. To be more specific, I use it when I am dealing with heteroskedasticity. Like some other scalers we studied above, the Power Transformer also changes the distribution of the variable, as in, it makes it more Gaussian(normal). We are familiar with similar power transforms such as square root, and cube root transforms, and log transforms.\n\nHowever, to use them, we need to first study the original distribution, and then make a choice. The Power Transformer actually automates this decision making by introducing a parameter called lambda. It decides on a generalized power transform by finding the best value of lambda using either the:\n\n1. Box-Cox transform\n\n2. The Yeo-Johnson transform\n\nWhile I will not get into too much detail of how each of the above transforms works, it is helpful to know that Box-Cox works with only positive values, while Yeo-Johnson works with both positive and negative values\n\n**Quantile Transformer Scaler**\n\nOne of the most interesting feature transformation techniques that I have used, the Quantile Transformer Scaler converts the variable distribution to a normal distribution. and scales it accordingly. Since it makes the variable normally distributed, it also deals with the outliers. Here are a few important points regarding the Quantile Transformer Scaler:\n\n1. It computes the cumulative distribution function of the variable\n\n2. It uses this cdf to map the values to a normal distribution\n\n3. Maps the obtained values to the desired output distribution using the associated quantile function\n\nA caveat to keep in mind though: Since this scaler changes the very distribution of the variables, linear relationships among variables may be destroyed by using this scaler. Thus, it is best to use this for non-linear data.\n\n**Unit Vector Scaler\/Normalizer**\n\nNormalization is the process of scaling individual samples to have unit norm. The most interesting part is that unlike the other scalers which work on the individual column values, the Normalizer works on the rows! Each row of the dataframe with at least one non-zero component is rescaled independently of other samples so that its norm (l1, l2, or inf) equals one.\n\nJust like MinMax Scaler, the Normalizer also converts the values between 0 and 1, and between -1 to 1 when there are negative values in our data.\n\nHowever, there is a difference in the way it does so.\n\n    If we are using L1 norm, the values in each column are converted so that the sum of their absolute values along the row = 1\n    If we are using L2 norm, the values in each column are first squared and added so that the sum of their absolute values along the row = 1\n    \n    \n**Custom Transformer**\n\nConsider this situation \u2013 Suppose you have your own Python function to transform the data. Sklearn also provides the ability to apply this transform to our dataset using what is called a FunctionTransformer.\n\nLet us take a simple example. I have a feature transformation technique that involves taking (log to the base 2) of the values. In NumPy, there is a function called log2 which does that for us.\n\nThus, we can now apply the FunctionTransformer:\n\n\n**Cat encoding** LabelEncoder , OneHotEncoder ...ex \n\n### Preprocess: example ","b889fe5e":"Numerical Data seems to be kinda normalized with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.","a72d65c7":"### Visual Exploratory ","f42921e2":"### Crosstab","bb97c44b":"### Num\/Cat Features ","d7fef6c8":"### Duplicates ","fc9053b1":"### Preprocess Cat that occur rarely \n#### What is high cardinality?\n\nAlmost all datasets now have categorical variables. Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality.\n#### The Curse of Dimensionality\n\nHere is a simple summarization:\n\n    As the number of features grows, the amount of data we need to accurately be able to distinguish between these features (in order to give us a prediction) and generalize our model (learned function) grows EXPONENTIALLY.   \nwould like to use Yoshua Bengio\u2019s (Yes the legendary Yoshua Bengio !) quora answer to explain this in more detail. I strongly advise reading the whole answer here. According to the answer, increasing the number of different values in a feature simply increases the total number of possible combinations that can be made using the input row (containing n such features). Say we have two features with two distinct values each, this gives us a total of 4 possible ways to combine the two features. Now if one of these had three distinct values we would have 3X2 =6 possible ways to combine them.\n\nIn classical non-parametric learning algorithms (e.g. nearest-neighbor, Gaussian kernel SVM, Gaussian kernel Gaussian Process, etc.) the model needs to see at least one example for each of these combinations (or at least as many as necessary to cover all the variations of configurations of interest), in order to produce a correct answer, one that is different from the target value required for other nearby configurations.\n\nThere is a workaround to this, that is the model even in the absence of a lot of training data can discern between configurations (not in the training set) for future predictions provided there is some sort of structure (pattern) in these combinations. In most cases, high cardinality makes it difficult for the model to identify such patterns and hence the model doesn\u2019t generalise well to examples outside the training set.    \n####  Reducing Cardinality by using a simple Aggregating function\n\nBelow is a simple function I use to reduce the cardinality of a feature. The idea is very simple. Leave instances belonging to a value with high frequency as they are and replace the other instances with a new category which we will call other.\n\n    Choose a threshold\n    Sort unique values in the column by their frequency in descending order\n    Keep adding the frequency of these sorted (descending) unique values until a threshold is reached.\n    These are the unique categories we will keep and instances of all other categories shall be replaced by \u201cother\u201d.\nLet\u2019s run through a quick example before going through the code. Say our column colour has 100 values and our threshold is 90% (that is 90). We have 5 different categories of colours: Red (50), Blue(40), Yellow (5), Green (3) and Orange (2). The numbers within the bracket indicate how many instances of that category are present in the column.\n\nWe see that Red (50)+Blue (40) reaches our threshold of 90. In that case, we retain only 2 categories (Red, Blue) and mark all other instances of other colours as \u201cOther\u201d\nThus we have reduced cardinality from 5 to 3 (Red, Blue, Other)\n\nHere is the utility function I wrote to facilitate this. It\u2019s well commented and follows exactly what I described above so you won\u2019t have a problem following along. We can set a custom threshold and the return_categories option optionally lets us see the list of all unique values after reducing cardinality.\n    ","99de83c9":"\n## Feature Engineering\nFeature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables\n\n\n\n","2d4148ca":"# Evaluation \n**rmse**","1dc8ea13":"## t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. press\n","3ecbb94d":"### Correlation ","99b82c31":"### Numerical features distribution\n#### Histograms of numerical features","823f735f":"## Dimensionality reduction\n### Feature Selection\n\n\nFeature selection is a method of selecting features from your feature set to be used for modeling. It draws from a set of existing features, so it's different than feature engineering because it doesn't create new features. The overarching goal of feature selection is to improve your model's performance. Perhaps your existing feature set is much too large, or some of the features you're working with are unnecessary. There are different ways you can perform feature selection. It's possible to do it in an automated way. Scikit-learn has several methods for automated feature selection, such as choosing a variance threshold and using univariate statistical tests\n\n#### Why reduce dimensionality?\n\nYour dataset will become simpler and thus easier to work with, require less disk space to store and computations will run faster. In addition, models are less likely to overfit on a dataset with fewer dimensions.\n#### Selection vs extraction\n\nWhen we apply feature selection, we completely remove a feature and the information it holds from the dataset. We try to minimize the information loss by only removing features that are irrelevant or hold little unique information, but this is not always possible.\n\nCompared to feature selection, feature extraction is a completely different approach but with the same goal of reducing dimensionality. Instead of selecting a subset of features from our initial dataset, we'll be calculating, or extracting, new features from the original ones. These new features have as little redundant information in them as possible and are therefore fewer in number. One downside is that the newly created features are often less intuitive to understand than the original ones.\n**PCA Calculating Principal Components**\n\nThis is done in this notebook : \n\nhttps:\/\/www.kaggle.com\/bannourchaker\/1-featuresengineer-selectionpart1?scriptVersionId=72906910\n\n\n\n","8e07fc99":"# Data Modeling\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.","4af78395":"## Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.","516b09f1":"## Convert Dtypes :","6410b2d5":"**Histograms of numerical data show a desperation of values with what look like multinomial distributions, also column cont1 seems to have some areas where the distribution becomes kinda discrete and again test numerical data seems to be similar to train numerical data.**\n### Zooming on the correlation between numerical variables and target.","89c00b47":"## Num Features :","755559d5":"### Num Features ","40a41568":"Most of columns seems to have few categorical unique values except cat9 column.","e41bf173":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","668bbc37":"### Number of categorical unique values","b0bdddfe":"# CRISP-DM Methodology\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n        1.Determine business objectives\n\n        2.Assess situation\n\n        3.Determine data mining goals\n\n        4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n\n#  Buissness Understanding\/Data Understanding \n## Step 1: Import helpful libraries","aae7da78":"###  KDE plot of target with categorical features ","d37c4edd":"### Bin target ","33390849":"# Find Best Pipe\n# Steps for K-fold cross-validation\n\n    Split the dataset into K equal partitions (or \"folds\")\n        So if k = 5 and dataset has 150 observations\n        Each of the 5 folds would have 30 observations\n    Use fold 1 as the testing set and the union of the other folds as the training set\n        Testing set = 30 observations (fold 1)\n        Training set = 120 observations (folds 2-5)\n    Calculate testing accuracy\n    Repeat steps 2 and 3 K times, using a different fold as the testing set each time\n        We will repeat the process 5 times\n        2nd iteration\n            fold 2 would be the testing set\n            union of fold 1, 3, 4, and 5 would be the training set\n        3rd iteration\n            fold 3 would be the testing set\n            union of fold 1, 2, 4, and 5 would be the training set\n        And so on...\n    Use the average testing accuracy as the estimate of out-of-sample accuracy\n\nDiagram of 5-fold cross-validation\n\n![image.png](data:image\/png;base64,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)\n\n\n\n\n\n\n# Comparing cross-validation to train\/test split\n\nAdvantages of cross-validation:\n\n    More accurate estimate of out-of-sample accuracy\n    More \"efficient\" use of data\n        This is because every observation is used for both training and testing\n\nAdvantages of train\/test split:\n\n    Runs K times faster than K-fold cross-validation\n        This is because K-fold cross-validation repeats the train\/test split K-times\n    Simpler to examine the detailed results of the testing process\n\n# Cross-validation recommendations\n\n    K can be any number, but K=10 is generally recommended\n        This has been shown experimentally to produce the best out-of-sample estimate\n    For classification problems, stratified sampling is recommended for creating the folds\n        Each response class should be represented with equal proportions in each of the K folds\n            If dataset has 2 response classes\n                Spam\/Ham\n                20% observation = ham\n                Each cross-validation fold should consist of exactly 20% ham\n        scikit-learn's cross_val_score function does this by default\n\n# Feature engineering and selection within cross-validation iterations\n\n    Normally, feature engineering and selection occurs before cross-validation\n    Instead, perform all feature engineering and selection within each cross-validation iteration\n    More reliable estimate of out-of-sample performance since it better mimics the application of the model to out-of-sample data\n      \n\n\n## Step 1: Cross-Validation","f79e7980":"##  Target \n###  exploring target data main statistics","ee7c7f08":"### Box plot of target data with percentile of .1% and 99.9%","3f2880a8":"### Compose num+cat : ColumnTransformer\n![image.png](attachment:616d4220-b493-4ff4-995e-d4531000a7f7.png)\n\nYou may have noticed how easy it is to make predictions once we train a pipeline. Pipe. Predict (x) converts the original data and returns the prediction. It\u2019s also easy to see the sequence of steps. Let\u2019s summarize these two methods intuitively\nUsing pipes not only organizes and simplifies code, but also has many other benefits. Here are some of them:\n\n    Ability to fine tune the pipeline: when building a model, you may need to try different methods to preprocess the data and run the model again to see if the adjustment in the preprocessing step can improve the generalization ability of the model. When optimizing the model, fine tuning not only exists in the super parameters of the model, but also in the implementation of the preprocessing steps. With this in mind, when we have a pipeline object that unifies transformer and estimator, we can fine tune the superparameters of the whole pipeline, including estimator and two transformers using gridsearchcv or randomized searchcv.\n\n    Easier to deploy: all the conversion steps used to prepare the data when training the model can also be applied to the data in the production environment when making predictions. When we train pipes, we train an object that contains a data converter and a model. Once trained, the pipeline object can be used for smoother deployment.\n\n### ColumnTransformer\n\nIn the previous example, we interpolated and coded all columns in the same way. However, we often need to apply different transformers to different column groups. For example, we want onehotencoder to be applied only to classified columns, not to numeric columns. This is where columntransformer comes in.\n\n        #Define classification pipeline\n        cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant',       fill_value='missin,                       ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n\n        #Define value pipeline\n        num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')),\n                             ('scaler', MinMaxScaler())])\n\n        #Combined classification pipeline and numerical pipeline\n        preprocessor = ColumnTransformer(transformers=[('cat', cat_pipe, categorical),\n                                                       ('num', num_pipe, numerical)])\n                                                       \n        #Install transformer and training data estimator on the pipeline\n        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('model', LinearRegression())])\n![image.png](attachment:d52399da-57a1-4db3-811f-b363f6a07a1f.png)\n\nTo combine the preprocessing steps specified in columntransformer with the model, we use a pipe externally. The following is its visual representation:\n\n\n![image.png](attachment:2e3a4a03-b3b5-4bf0-b33a-24f78b549e3b.png)\n\n### FeatureUnion \n    #Custom pipe\n    class ColumnSelector(BaseEstimator, TransformerMixin):\n        \"\"\"Select only specified columns.\"\"\"\n        def __init__(self, columns):\n            self.columns = columns\n\n        def fit(self, X, y=None):\n            return self\n\n        def transform(self, X):\n            return X[self.columns]\n\n    #Define classification pipeline\n    cat_pipe = Pipeline([('selector', ColumnSelector(categorical)),\n                         ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n                         ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n\n    #Define value pipeline\n    num_pipe = Pipeline([('selector', ColumnSelector(numerical)),\n                         ('imputer', SimpleImputer(strategy='median')),\n                         ('scaler', MinMaxScaler())])\n\n    #Featureunion fitting training data\n    preprocessor = FeatureUnion(transformer_list=[('cat', cat_pipe),\n                                                  ('num', num_pipe)])\n    preprocessor.fit(X_train)\n\nWe can think of featureunion as creating copies of the data, converting those copies in parallel, and pasting the results together. The term copy here is more of a conceptual analogy than an actual technology.\n\nAt the beginning of each pipe, we add an extra step, where we use a custom converter to select the relevant columns: the column selectors\n\n![image.png](attachment:c1cac873-0a08-48c2-8ec9-c8093a5f1400.png)\n\n### summary\n\nAs you may have noticed, pipeline is a superstar. Columntransformer and featureunion are additional tools for pipes. Columntransformer is more suitable for parallel partitioning, while featureunion allows us to apply multiple converters in parallel on the same input data\n\n\n\n## Multiple  preprocess + Features engineer + Features Selections ","2396abf2":"# Convert Dtypes : ","b2c6aba1":"**Clustering seems like it wont add anything**\n","4ccdd846":"###  Conculsion : \nSeveral real world prediction problems involve forecasting rare values of a target variable. When this variable is nominal we have a problem of class imbalance that was already studied thoroughly within machine learning. For regression tasks, where the target variable is continuous, few works exist addressing this type of problem. Still, important application areas involve forecasting rare extreme values of a continuous target variable. Namely, we propose to address such tasks by sampling approaches. These approaches change the distribution of the given training data set to decrease the problem of imbalance between the rare target cases and the most frequent ones. We present a modification of the well-known Smote algorithm that allows its use on these regression tasks\n\n\n","85942b58":"\n#### 1-1 Pipe OneHotEncoder \tRobustScaler","0dfeda74":"### CV rmse","53404f9f":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","88833644":"**Target is really so disperssed**\n## Kmeans ","b3b8c011":"#### 1-2 Pipe addd PCA"}}