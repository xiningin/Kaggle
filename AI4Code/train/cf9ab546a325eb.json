{"cell_type":{"338a242f":"code","1f784bcb":"code","d86202e7":"code","5dd5692f":"code","4bdd34d7":"code","528be5e5":"code","0a4fec64":"code","42117b06":"code","94017cfe":"code","849626c2":"code","cb98be50":"code","73e5da75":"code","32b8f63a":"code","9d6ab23c":"code","73197169":"code","f9bf3f0f":"code","3dd48064":"code","21f1344e":"code","34a2ce0c":"code","12508702":"code","dccd39d4":"code","b7768cff":"markdown","89e58c6c":"markdown","4d4df4a6":"markdown","54c270b5":"markdown","80b49e96":"markdown"},"source":{"338a242f":"import numpy as np \nimport pandas as pd \nimport os, sys\nimport time\nimport gc\nfrom numba import jit\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\n\n# from tqdm import tqdm\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\n\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn import metrics\n\nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.options.display.precision = 15\n","1f784bcb":"# Inspired by\n# https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\n# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef convert_col_to_proper_int(df_col):\n    col_type = df_col.values.dtype\n    if ((str(col_type)[:3] == 'int') | (str(col_type)[:4] == 'uint')):\n        c_min = df_col.values.min()\n        c_max = df_col.values.max()\n        if c_min < 0:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n            # https:\/\/stackoverflow.com\/a\/42631281 through values is faster (test with timeit)\n                df_col = pd.Series(df_col.values.astype(np.int8), name=df_col.name)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df_col = pd.Series(df_col.values.astype(np.int16), name=df_col.name)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n               df_col = pd.Series(df_col.values.astype(np.int32), name=df_col.name)\n            elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n                df_col = pd.Series(df_col.values.astype(np.int64), name=df_col.name)\n        else:\n            if c_max <= np.iinfo(np.uint8).max:\n                df_col = pd.Series(df_col.values.astype(np.uint8), name=df_col.name)\n            elif c_max <= np.iinfo(np.uint16).max:\n                df_col = pd.Series(df_col.values.astype(np.uint16), name=df_col.name)\n            elif c_max <= np.iinfo(np.uint32).max:\n                df_col = pd.Series(df_col.values.astype(np.uint32), name=df_col.name)\n            elif c_max <= np.iinfo(np.uint64).max:\n                df_col = pd.Series(df_col.values.astype(np.uint64), name=df_col.name)\n            \n    return df_col\n\ndef convert_col_to_proper_float(df_col):\n    col_type = df_col.values.dtype\n    if str(col_type)[:5] == 'float':\n        unique_count = len(np.unique(df_col))\n        # https:\/\/stackoverflow.com\/a\/42631281 through values is faster (test with timeit)\n        df_col_temp = pd.Series(df_col.values.astype(np.float32), name=df_col.name)\n        if len(np.unique(df_col_temp)) == unique_count:\n            df_col = df_col_temp\n            c_min = df_col.values.min()\n            c_max = df_col.values.max()\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df_col_temp = pd.Series(df_col.values.astype(np.float16), name=df_col.name)\n                if len(np.unique(df_col_temp)) == unique_count:\n                    df_col = df_col_temp\n            \n    return df_col\n\ndef gentle_reduce_mem_usage(data, verbose = True, process_objects = False, cat_level = None):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    if cat_level is not None:\n        cat_level = np.round(abs(cat_level) % 1, 15)\n    start_mem = data.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n\n    start_time = time.time()\n    \n    if data.columns.nunique(dropna=False) < len(data.columns):\n        print('Will not process duplicated columns as it causes troubles later')\n        print(data.columns[data.columns.duplicated()].tolist())\n        proc_cols = data.columns.drop_duplicates(keep=False)\n    else:\n        proc_cols = data.columns\n\n    for col in tqdm(proc_cols, desc='columns'): #.select_dtypes(include=np.number, exclude=np.complexfloating)\n#         if verbose:\n#             print(col, type(data[col]), data[col].shape, 'started at', time.ctime())\n#         if (type(data[col]) != pd.Series):\n#             print('Column `', col, '` appears not as Series type object. Skipping.')\n#             continue #skip loop\n        col_type = data[col].values.dtype\n\n        if (process_objects & (col_type == object)):\n            if data[col].hasnans:\n                print('Column `', col, '` of object types has NaNs and has to be filled. Skipping.')\n                continue #skip loop\n            try:\n                data[col] = pd.to_numeric(data[col], downcast='float')\n            except ValueError:\n                try:\n                    data[col] = pd.to_numeric(data[col].str.replace(',', '.'), downcast='float')\n                except ValueError:\n                    data[col] = pd.to_datetime(data[col], infer_datetime_format=True, errors='ignore')\n            col_type = data[col].values.dtype\n\n        if (process_objects & (col_type == object) & (cat_level is not None)):\n            if len(np.unique(data[col].values)) <= cat_level*len(data[col].values):\n                data[col] = data[col].astype('category')\n                col_type = data[col].values.dtype\n\n        if ((col_type != object) & (col_type != '<M8[ns]') & (col_type != '<m8[ns]')\\\n                & (col_type.name != 'bool') & (col_type.name != 'category') & (col_type.name != 'complex64')\\\n                & (col_type.name != 'complex128')):#\n            c_min = data[col].values.min()\n            c_max = data[col].values.max()\n            if ((str(col_type)[:3] == 'int') | (str(col_type)[:4] == 'uint')):\n                data[col] = convert_col_to_proper_int(data[col])\n            else:\n                if np.isfinite(data[col].values).all():\n                    if c_min < 0:\n                        if abs(data[col].values - data[col].values.astype(np.int64)).sum() < 0.01:\n                            data[col] = convert_col_to_proper_int(pd.Series(data[col].values.astype(np.int64),\n                                                                            name=data[col].name))\n                        else:\n                            data[col] = convert_col_to_proper_float(data[col])\n                    else:\n                        if abs(data[col].values - data[col].values.astype(np.uint64)).sum() < 0.01:\n                            data[col] = convert_col_to_proper_int(pd.Series(data[col].values.astype(np.uint64),\n                                                                            name=data[col].name))\n                        else:\n                            data[col] = convert_col_to_proper_float(data[col])\n                else:\n                    data[col] = convert_col_to_proper_float(data[col])\n\n    end_time = time.time()\n    end_mem = data.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n        print('Done in {} seconds.'.format(end_time - start_time))\n\n    return None","d86202e7":"RANDOM_STATE = 2042\nnp.random.seed(RANDOM_STATE)","5dd5692f":"NUM_THREADS = 4","4bdd34d7":"input_path = '..\/input\/'","528be5e5":"model_name = 'model'\nmodel_num = 0\npostfix = None","0a4fec64":"%%time\ntrain = pd.read_csv(f'{input_path}train.csv', dtype = {'target': np.uint8})\ntrain_len = len(train)\ntest = pd.read_csv(f'{input_path}test.csv')\ndata = pd.concat([train.drop(columns=['target']), test], ignore_index = True)\ny = train['target']\ndel train, test","42117b06":"%%time\ngentle_reduce_mem_usage(data)\ndata.info()","94017cfe":"data.dtypes[data.dtypes == np.uint16]","849626c2":"# which columns contain any NaN value\n# https:\/\/stackoverflow.com\/a\/36226137\ndata.columns[data.isna().any()].tolist()","cb98be50":"%%time\nok_cols = [col for col in data.columns if col not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n#frequencies\ndata['wheezy-copper-turtle-magic_count'] = data.groupby(['wheezy-copper-turtle-magic'])['id'].transform('count')\n\nfor col in tqdm(ok_cols, desc='processed columns'):\n    data[f'{col}_w_mean'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform('mean').fillna(0).values.astype(np.float32)\n    data[f'{col}_w_std'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform('std').fillna(0).values.astype(np.float32)\n    data[f'{col}_w_max'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform('max').fillna(0).values.astype(np.float32)\n    data[f'{col}_w_min'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform('min').fillna(0).values.astype(np.float32)\n#     data[f'{col}_w_quantile_10'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform(lambda x: np.percentile(x.unique(), 10)).fillna(0).values.astype(np.float32)\n    data[f'{col}_w_quantile_25'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform(lambda x: np.percentile(x.unique(), 25)).fillna(0).values.astype(np.float32)\n#     data[f'{col}_w_median'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform(lambda x: np.percentile(x.unique(), 50)).fillna(0).values.astype(np.float32)\n    data[f'{col}_w_quantile_75'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform(lambda x: np.percentile(x.unique(), 75)).fillna(0).values.astype(np.float32)\n#     data[f'{col}_w_quantile_90'] = data.groupby(['wheezy-copper-turtle-magic'])[col].transform(lambda x: np.percentile(x.unique(), 90)).fillna(0).values.astype(np.float32)\n","73e5da75":"data.shape","32b8f63a":"# data['wheezy-copper-turtle-magic'] = data['wheezy-copper-turtle-magic'].astype(object) #making cat_feature for catboost\nX = data[:train_len].drop(['id'], axis=1)\nX_test = data[train_len:].drop(['id'], axis=1)\n\n","9d6ab23c":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=RANDOM_STATE)","73197169":"del data\ngc.collect()","f9bf3f0f":"@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc \/= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='cat', eval_metric='auc', columns=None,\n                               plot_feature_importance=False, model=None, cat_plot = None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC:hints=skip_train~false',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    loss_dict ={'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'Logloss',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = NUM_THREADS)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                      eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, n_jobs = NUM_THREADS,\n                              early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, thread_count=NUM_THREADS,\n                                       early_stopping_rounds = early_stopping_rounds,\n                                       eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                       loss_function=loss_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid),\n                      plot = cat_plot,\n                      use_best_model=True, verbose=verbose)\n\n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test)\n        \n#         print(oof[valid_index].shape)\n#         print(y_pred_valid.shape)\n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f} (CV score: {0:.3f}\u00b1{2:.3f}).'.format(np.mean(scores), np.std(scores),\n                                                                                     3*np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n","3dd48064":"params = {'metric_period': 50, #auc calculation is slow, so evaluate less\n#           'cat_features': [X.columns.get_loc('wheezy-copper-turtle-magic')],\n          'random_seed': RANDOM_STATE,\n          'depth': 6,\n          'bagging_temperature': 0.825,\n          'random_strength': 0.125,\n#           'l2_leaf_reg': 6.0,\n#           'learning_rate': 0.12,\n         }\nresult_dict_cat = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='cat',\n                                             cat_plot = False,\n                                             eval_metric='auc', plot_feature_importance=True, verbose=50, n_estimators=2000)","21f1344e":"result_dict_cat['prediction'].shape","34a2ce0c":"X.shape","12508702":"X_test.shape","dccd39d4":"sub = pd.read_csv(f'{input_path}sample_submission.csv')\nsub['target'] = result_dict_cat['prediction'][:, 1]\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","b7768cff":"## Data overview","89e58c6c":"## Catboost model","4d4df4a6":"Training function:","54c270b5":"# Instant Gratification\nA synchronous Kernels-only competition","80b49e96":"## Loading libraries and preparing functions"}}