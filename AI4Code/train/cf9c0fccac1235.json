{"cell_type":{"0d54b695":"code","718d0a2a":"code","fb3b11da":"code","426b8433":"code","74709523":"code","aa37e43d":"code","4344e280":"code","9373d5ab":"code","0622a208":"code","8aaa637f":"code","97330f4c":"code","1617d051":"code","1b80a1d5":"code","791e3211":"code","9721ae4a":"code","066bc9ad":"code","a8b30076":"code","6fc6894e":"code","9bb2141d":"code","b2e154c2":"code","92a8c25e":"code","061648b7":"code","081490fc":"code","c55006ee":"code","81818649":"code","93abfc15":"code","2ca0e20f":"code","6c6c7c7d":"code","69b4a94f":"code","aa32928b":"code","fa76d792":"markdown","8c9bf981":"markdown","13d94a60":"markdown","351d0cfe":"markdown","cfcbedf5":"markdown","87d39d08":"markdown","41c6b047":"markdown","3a096feb":"markdown","3e1e26f2":"markdown","ad9a5b49":"markdown","5804bffd":"markdown","e591b869":"markdown","68a71692":"markdown"},"source":{"0d54b695":"import dask.dataframe as dd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc  \n\nimport janestreet\n\n# Load data\ndata = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')","718d0a2a":"fig, ax = plt.subplots(figsize=(15, 4))\nfeature_0 = pd.Series(data['feature_0']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\nfeature_0.plot(lw=3);","fb3b11da":"feature_0_is_plus_one  = data.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = data.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']).cumsum()), lw=3, label='resp')\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']).cumsum()), lw=3, label='resp')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return')\nax1.set_title (\"feature 0 = 1\", fontsize=18)\nax2.set_title (\"feature 0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"upper left\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","426b8433":"ticks_day = data.groupby('date').count()\nticks_day.head(10)","74709523":"day_100  = train_df.loc[train_df['date'] == 100]\nday_200  = train_df.loc[train_df['date'] == 200]\nday_100_and_200 = pd.concat([day_100, day_200])\nday_100_and_200.corr(method='pearson').style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","aa37e43d":"resp_stds = (train_df[[c for c in train_df.columns if 'resp' in c]]\n    .std()\n    .sort_values()\n);\nax = resp_stds.plot(kind='bar', title='Standard Deviation of each `resp_`')\nfor bar in ax.patches:\n    bar.set_facecolor('#aa3333')\npos = resp_stds.index.get_loc('resp')\nax.patches[pos].set_facecolor('#348ABD')","4344e280":"fig, axs = plt.subplots(1, 2, figsize=(22, 6))\nsns.distplot(train_df['resp'], ax=axs[0])\nsns.distplot(train_df['weight'], ax=axs[1])\nfig.savefig('resp_weight_distplot.png')","9373d5ab":"import seaborn as sns\nn_features = 10\nnan_val = train_df.isna().sum()[train_df.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)\n\n\nfig, axs = plt.subplots(figsize=(10, 10))\n\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\n\nplt.title(f'NaN values of train dataset (Top {n_features})')\nplt.xlabel('NaN values')\nfig.savefig(f'nan_values_top_{n_features}_features.png')\nplt.show()","0622a208":"train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv',nrows=1000)\ntrain = train[train['weight'] != 0]\ntrain.head()","8aaa637f":"train['trade'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\ntrain['return'] = train['weight'].values * train['resp'].values\ntrain['return_1'] = train['weight'].values * train['resp_1'].values\ntrain['return_2'] = train['weight'].values * train['resp_2'].values\ntrain['return_3'] = train['weight'].values * train['resp_3'].values\ntrain['return_4'] = train['weight'].values * train['resp_4'].values\n\nassets = train.loc[:, train.columns.str.contains('feature')]\nreturns = train.loc[:, train.columns.str.contains('return')]\ntrades = train.loc[:, 'trade']\n\nassets.head()","97330f4c":"returns.head()","1617d051":"trades.head()","1b80a1d5":"train=train.set_index('ts_id')\ntrain['MA20'] = train['feature_2'].rolling(window=20).mean()\ntrain['20dSTD'] = train['feature_2'].rolling(window=20).std()\ntrain['Upper'] = train['MA20'] + (train['20dSTD'] * 2)\ntrain['Lower'] = train['MA20'] - (train['20dSTD'] * 2)\n\ntrain[['return','feature_2','MA20','Upper','Lower']].plot(figsize=(12,4))\nplt.grid(True)\nplt.title(' Bollinger Bands')\nplt.axis('tight')\nplt.ylabel('Price')\nplt.savefig('feature_1.png', bbox_inches='tight')","791e3211":"returns['SMA_20']=returns['return'].rolling(window=20).mean()\nreturns['SMA_10']=returns['return'].rolling(window=10).mean()\nreturns['SMA_5']=returns['return'].rolling(window=5).mean()\nreturns['SMA_200']=returns['return'].rolling(window=200).mean()\nreturns[['return', 'SMA_5']].plot(figsize=(20,10))\nplt.grid(True)\nplt.title('Simple Moving Average')\nplt.axis('tight')\nplt.ylabel('Return')\nplt.savefig('feature_2.png', bbox_inches='tight')","9721ae4a":"def wma(df, column='close', n=20, add_col=False):\n\n    weights = np.arange(1, n + 1)\n    wmas = df[column].rolling(n).apply(lambda x: np.dot(x, weights) \/\n                                       weights.sum(), raw=True).to_list()\n\n    if add_col == True:\n        df[f'{column}_WMA_{n}'] = wmas\n        return df\n    else:\n        return wmas","066bc9ad":"plt.figure(figsize = (12,4))\n# the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n# the maximum number of trades per day is 18884\n# I have used 125 bins for the 500 days\nax = sns.distplot(trades_per_day, \n             bins=125, \n             kde_kws={\"clip\":(1000,20000)}, \n             hist_kws={\"range\":(1000,20000)},\n             color='darkcyan', \n             kde=True);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Number of trades per day\", size=14)\nplt.show();","a8b30076":"#Indicator Volatility (Average True Range[ATR], Bollinger Bands[BB], Standard Deviation[STD])\n\n#Where alpha = 2 \/ (span+1)\ntrain['ATR'] = train['TR'].ewm(span = 10).mean()\n\n#Otherwise you should be able to easily do you're own smoothing like this:\n#train['ATR'] = ( train['ATR'].shift(1)*13 + train['TR'] ) \/  14","6fc6894e":"#Leptokurtic \nkurt = features.kurt(axis=0);\nkurt","9bb2141d":"skewness=features.skew(axis=0) #columns sequencial calculation\nskewness\n\nmean=features.mean()\nmean\nmedian=features.median()\nmedian\nmode=features.mode()\nmode","b2e154c2":"# 1) Pearson correlation coefficient is defined as the covariance of two variables divided by the product of their standard deviations.\ncorr_pearson = features.corr(method='pearson')\n\n\n# 2) Spearman is a nonparametric evaluation that finds the strength and direction of the monotonic relationship between two variables. \n# This method is used when the data is not normally distributed or when the sample size is small\ncorr_spearman = features.corr(method='spearman')\n# Find Spearman rank correlation between rows of different data drames\nspearmanCorrelation  = df1.corrwith(df2, axis=1, method=\"spearman\");\n\n\n# 3) Kendall or Kendall's tau It quantifies the discrepancy between the number of concordant and discordant pairs of two variables.\ncorr_kendall  = features.corr(method='kendall')","92a8c25e":"# Exemple 1 Effect of size on monte carlo sample\nfrom numpy.random import normal\nfrom matplotlib import pyplot\n# define the distribution\nmu = 50 #Gaussian distribution with the specified mean (mu)\nsigma = 5 # standard deviation (sigma)\n\n# generate monte carlo samples of differing size\nsizes = [10, 50, 100, 1000]\nfor i in range(len(sizes)):\n# generate sample\n    sample = normal(mu, sigma, sizes[i])\n    # plot histogram of sample\n    pyplot.subplot(2, 2, i+1)\n    pyplot.hist(sample, bins=20)\n    pyplot.title('%d samples' % sizes[i])\n    pyplot.xticks([])\n# show the plot\npyplot.show()","061648b7":"# Example 2 Monte Carlo between ranges (0% - 100%)\nimport random\nimport time\n\ndf_features = train.loc[:, train.columns.str.contains('feature')]\n\nprint_elapsed_time()\n\nsamples = 10000\nfeatures = df_features.columns.values\na = {}\nlistSomas={}\n# Create the pandas DataFrame \ndf = pd.DataFrame(columns=features)\ndf.head()\n\nfor k in range(samples):\n    a[0]= random.uniform(0, 1)\n    soma = a[0]\n    for i in range(1, len(features)-1):\n        a[i] = random.uniform(0, 1-soma)\n        soma+=a[i]\n    \n    a[len(features)-1] = 1 - soma\n    soma+=a[len(features)-1]\n    random.shuffle(a)\n    keys_values = a.items()\n    new_d = {'feature_'+str(key): value for key, value in keys_values}\n    #print(new_d)\n    df.loc[k] = list(new_d.values())\n\n#df.head()\n\n\nprint_elapsed_time('after heavy jobs')\n\nportfolioMA = df.mean(axis = 0) \n\nprint('Mean = '+str(portfolioMA.std()))","081490fc":"def reduce_memory_usage(df):   \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    df = df.astype({c: np.float32 for c in df.select_dtypes(include='float64').columns}) \n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df","c55006ee":"from numpy import percentile\n\ndata = features['feature_1'].to_numpy()\n\n# calculate interquartile range\nq25, q75 = percentile(data, 25), percentile(data, 75)\niqr = q75 - q25\n#the cutoff for outliers as 1.5 times the IQR and subtract this cut-off from the 25th percentile \n#and add it to the 75th percentile to give the actual limits on the data.\nprint('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n\n# calculate the outlier cutoff\ncut_off = iqr * 1.5\nlower, upper = q25 - cut_off, q75 + cut_off\n\n# identify outliers\noutliers = [x for x in data if x < lower or x > upper]\nprint('Identified outliers: %d' % len(outliers))\n# remove outliers\noutliers_removed = [x for x in data if x > lower and x < upper]\nprint('Non-outlier observations: %d' % len(outliers_removed))","81818649":"#calculated IQR score to filter out the outliers by keeping only valid values.\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\ndata_out = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]","93abfc15":"def cut_off_outliers(data, target):\n    y = data[target]\n    removed_outliers = y.between(y.quantile(.05), y.quantile(.95))\n    print(removed_outliers.value_counts())\n    index=data[~removed_outliers].index # INVERT removed_outliers!!\n    print(f'The result Outiers to drop ',len(index))\n    data.drop(index, inplace=True)","2ca0e20f":"from scipy.stats import zscore\n\n#zscore (a,axis: int=0,ddof: int=0)\n#with a as a DataFrame to get a NumPy array containing the z-score of each value in a. \nz_scores = stats.zscore(data)\nabs_z_scores = np.abs(z_scores) #Call numpy.abs(x) with x as the previous result to convert each element in x to its absolute value.\nfiltered_entries = (abs_z_scores < 3).all(axis=1)# Use the syntax (array < 3).all(axis=1) with array as the previous result to create a boolean array.\nnew_df = df[filtered_entries] #Filter the original DataFrame with this\n\n# for Pandas\ndata = data[(z < 3).all(axis=1)]","6c6c7c7d":"from sklearn.neighbors import LocalOutlierFactor\n\n# identify outliers in the training dataset\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(X_train)\n# select all rows that are not outliers\nmask = yhat != -1\nX_train, y_train = X_train[mask, :], y_train[mask]","69b4a94f":"all_columns_nan = features.isna().any()\nall_columns_nan\n\nnan_values = features[features.columns[features.isnull().any()]]\nnan_values.head()\n\nnulls = features.isnull().sum()\nnulls_list = list(nulls[nulls >(0.173 * len(features))].index)\nnulls_list\n\n#df.loc[(df['Salary_in_1000']>=100) & (df['Age']< 60) & (df['FT_Team'].str.startswith('S')),['Name','FT_Team']]\nnan_features=features[features['feature_17'].isna() & features['feature_18'].isna()]\nnan_features","aa32928b":"def concat_columns(df, cols_to_concat, new_col_name, sep=\"-\"):\n    df[new_col_name] = df[cols_to_concat[0]]\n    for col in cols_to_concat[1:]:\n        df[new_col_name] = df[new_col_name].astype(str) + sep + df[col].astype(str)","fa76d792":"<a id='reduce_memory_usage'><\/a>\n#### Reduce Memory Usage\n<a href='#index'>back to index<\/a>","8c9bf981":"<a id='queries'><\/a>\n<a href='#index'>back to index<\/a>\n\n### Queries in Pandas\n","13d94a60":"<a id='monte_carlo'><\/a>\n<a href='#index'>back to index<\/a>\n### Monte Carlo Method\nThere are three main reasons to use Monte Carlo methods to randomly sample a probability distribution; they are:\n\n#### 1) Estimate density, gather samples to approximate the distribution of a target function.\n#### 2) Approximate a quantity, such as the mean or variance of a distribution.\n#### 3) Optimize a function, locate a sample that maximizes or minimizes the target function.\n\n<a href='https:\/\/machinelearningmastery.com\/monte-carlo-sampling-for-probability\/'>Introduction to Monte Carlo Sampling for Probability<\/a>","351d0cfe":"<a id='groupby'><\/a>\n<a href='#index'>back to index<\/a>\n#### Group by Column Name ","cfcbedf5":"<a id='kurtosis'><\/a>\n<a href='#index'>back to index<\/a>\n### Kurtosis\nKurtosis is one of the two measures that quantify shape of of a distribution. The another measure is skewness.\n#### 2) Kurtosis describes the peakedness of the distribution.\n#### 3) If the distribution is tall and thin it is called a leptokurtic distribution. Values in a leptokurtic distribution are near the mean or at the extremes.\n#### 4) A normal distribution has a kurtosis of 0.","87d39d08":"<a id='concat_cols'><\/a>\n<a href='#index'>back to index<\/a>\n\n### Concat Columns into new Column","41c6b047":"<a id='lof'><\/a>\n<a href='#index'>back to index<\/a>\n\n### Automatic Outlier Detection\nLocal outlier factor - LOF","3a096feb":"<a id='skewness'><\/a>\n<a href='#index'>back to index<\/a>\n### Skewness \nskewness is a measure of asymmetry of a distribution. Another measure that describes the shape of a distribution is kurtosis.\n#### 1) When a distribution is asymmetrical the tail of the distribution is skewed to one side-to the right or to the left.\n#### 2) When the value of the skewness is negative, the tail of the distribution is longer towards the left hand side of the curve.\n#### 3) When the value of the skewness is positive, the tail of the distribution is longer towards the right hand side of the curve.","3e1e26f2":"<a id=index><\/a>\n\n<a href='#percentile'>Interquartile Range Method - IQR<\/a>\n\n<a href='#lof'>Automatic Outlier Detection<\/a>\n\n<a href='#monte_carlo'>Monte Carlo Method<\/a>\n\n<a href='#markowitz'>Markowisk - Modern Portfolio Theory<\/a>\n\n<a href='#groupby'>Group by Column Name<\/a>\n\n<a href='#kurtosis'>Kurtosis<\/a>\n\n<a href='#skewness'>Skewness<\/a>\n\n<a href='#reduce_memory_usage'>Reduce Memory Usage<\/a>\n\n<a href='#queries'>Queries in Pandas<\/a>\n\n<a href='#concat_cols'>Concat Columns into new Column<\/a>","ad9a5b49":"<a id='percentile'><\/a>\n<a href='#index'>back to index<\/a>\n### Interquartile Range Method - IQR\nA good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short.\nThe IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot.","5804bffd":"<a id='markowitz'><\/a>\n<a href='#index'>back to index<\/a>\n\n# [Markowitz - Modern Portfolio Theory](https:\/\/www.investopedia.com\/terms\/e\/efficientfrontier.asp)\n\n\n### Efficient Frontier\n\n* Efficient frontier comprises investment portfolios that offer the highest expected return for a specific level of risk.\n* Optimal portfolios that comprise the efficient frontier tend to have a higher degree of diversification.\n\n### Limitations\n\nThe efficient frontier and modern portfolio theory have many assumptions that may not properly represent reality. One of the assumptions is that asset returns follow a normal distribution.\nIn reality, securities may experience returns (also known as tail risk) that are more than three standard deviations away from the mean in more than 0.3% of the observed values. Consequently, asset returns are said to follow a leptokurtic distribution or heavy-tailed distribution.","e591b869":"<a id='correlation'><\/a>\n<a href='#index'>back to index<\/a>\n### Correlation Coefficient (Pearson, Spearman, Kendall)","68a71692":"<a id='remove_outliers_z_score'><\/a>\n<a href='#index'>back to index<\/a>\n\n### Remove Outliers Z-score\nThe Z-score is the signed number of standard deviations by which the value of an observation or data point is \nabove the mean value of what is being observed or measured.\nThe intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation \nand Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution."}}