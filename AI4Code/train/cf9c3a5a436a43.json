{"cell_type":{"3d9ccf6f":"code","20fbb896":"code","641e0c67":"code","dcb7a2a0":"code","f9082ba4":"code","ea21cea2":"code","0efe41fa":"code","4146231a":"code","381c6f4c":"code","9fe29903":"code","bf44630d":"code","c27414bd":"code","d3c56c32":"code","f04732c5":"code","d4e82f31":"code","c1a2fb79":"code","e3654368":"code","bb3ce8f2":"code","a3cce4d6":"code","f47a78c0":"code","4f30cd72":"code","1cbcc7c2":"code","fa69160e":"code","919a3b26":"code","5ae0ede8":"code","7f51b6c7":"code","217017ac":"code","c96aac33":"code","af82315b":"code","dadfd923":"code","45a6fec4":"code","88f58a2b":"code","e519e70f":"code","bc1a50ee":"code","20fc3fdc":"code","30dd9007":"code","96549061":"code","2f25fbf7":"code","c0699abe":"code","0399a8c2":"code","d142aaf9":"code","721387ca":"code","b1e26e28":"code","03001de5":"code","e2b7487f":"code","5f048acb":"code","906f2461":"code","c8278661":"code","672c3441":"code","ca49b870":"code","2f9bcc2a":"code","a46ecca3":"code","789a7b31":"code","98cb555f":"code","0a176ba2":"code","2cf1dcdb":"code","084461dc":"code","943f671c":"code","d2bc3e0f":"code","095682a9":"code","3a68a817":"code","9002c522":"code","97779b1c":"code","1373f395":"code","6842bf41":"markdown","01a2e776":"markdown","256aa3fe":"markdown","e4adc7ac":"markdown","ae051b59":"markdown","10c1a954":"markdown","a5a48d5a":"markdown","19086e90":"markdown","f405f1f8":"markdown","815fd96c":"markdown","cc79e3f0":"markdown","a97544c2":"markdown","03b16722":"markdown","841d45bb":"markdown","33641611":"markdown","fbf3be52":"markdown","866b796d":"markdown","d76bfc63":"markdown","8d51643d":"markdown","593cea37":"markdown","007c7b92":"markdown","a222e2a0":"markdown","9f25be02":"markdown"},"source":{"3d9ccf6f":"# importing the required liberaries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport random as rnd","20fbb896":"# Load The Data\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")","641e0c67":"df_train.tail()","dcb7a2a0":"df_test.head()","f9082ba4":"dfs = [df_train, df_test] # to apply any change to both data frames","ea21cea2":"df_train.info()","0efe41fa":"df_test.info()","4146231a":"# Convering PassengerId to an object\nfor df in dfs:\n    df[\"PassengerId\"] = df[\"PassengerId\"].astype(\"object\")","381c6f4c":"# How many missing points in each variable\ncount_missing_train = df_train.isnull().sum()\npercent_missing_train = round(df_train.isnull().sum()\/len(df_train) * 100, 1)\nmissing_train = pd.concat([count_missing_train, percent_missing_train], axis = 1)\nmissing_train.columns = [\"Missing (count)\", \"Missing (%)\"]\nmissing_train","9fe29903":"# How many missing points in each variable\ncount_missing_test = df_test.isnull().sum()\npercent_missing_test = round(df_test.isnull().sum()\/len(df_test) * 100, 1)\nmissing_test = pd.concat([count_missing_test, percent_missing_test], axis = 1)\nmissing_test.columns = [\"Missing (count)\", \"Missing (%)\"]\nmissing_test","bf44630d":"df_train.columns","c27414bd":"# Fill Embarked and Fare Variables \ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(df_train[\"Embarked\"].mode()[0])\ndf_test[\"Fare\"] = df_test[\"Fare\"].fillna(df_test[\"Fare\"].median())","d3c56c32":"#  Drop Cabin\ndf_train = df_train.drop(\"Cabin\", axis = 1)\ndf_test = df_test.drop(\"Cabin\", axis = 1)","f04732c5":"# fill Age. We will iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\nguess_ages = np.zeros((2,3))\ndfs = [df_train, df_test]\nfor df in dfs:\n    df[\"Sex\"] = df[\"Sex\"].map({\"male\":1, \"female\":0}) #Do not Run This Cell Twice \n    for i in range(0, 2):\n        for j in range(0,3):\n            guess_df = df[(df[\"Sex\"] == i)&(df[\"Pclass\"] == j+1)][\"Age\"].dropna()\n            \n            age_guess = guess_df.mean()\n            guess_ages[i,j] = int(age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0,3):\n            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),'Age'] = guess_ages[i,j]            \n    \n    df.Age = df.Age.astype(int)","d4e82f31":"df_train.head()","c1a2fb79":"df_train.info()","e3654368":"for df in dfs:\n    df[\"num_family\"] = 1 + df.SibSp + df.Parch\n    df[\"Title\"] = df[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\n    df[\"Title\"] = df[\"Title\"].replace([\"Mlle\", \"Major\", \"Col\", \"Jonkheer\", \"Ms\", \"Lady\", \"the Countess\", \"Mme\", \"Sir\", \"Capt\", \"Don\"], \"Other\")\n    df['numeric_ticket'] = df.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n    df[\"Ticket_Text\"] = df[\"Ticket\"].apply(lambda x: x.split(\" \")[0].replace(\"\/\", \"\").replace(\".\", \"\").lower() if len(x.split(\" \")) > 1 else 0)\n    df[\"Ticket_Text\"] = df[\"Ticket_Text\"].replace([\"swpp\", \"sotono2\", \"ppp\", \"fa\", \"casoton\", \"sop\", \"sp\", \"as\", \"sca4\", \"scow\", \"fc\", \"sc\"], \"other\")\n    df[\"Is_Alone\"] = df[\"num_family\"].apply(lambda x: 1 if x < 2 else 0)\n    ","bb3ce8f2":"df_train.head()","a3cce4d6":"for df in dfs:\n    df['Age_bins'] = pd.qcut(df['Age'], labels = [\"<19\", \"19-23\", \"24-25\", \"26-31\", \"32-40\", \"41-80\"], q = 6)\n    ","f47a78c0":"dfs = [df_train, df_test]","4f30cd72":"for df in dfs:\n    df[\"Fare_bins\"] = pd.qcut(df[\"Fare\"], labels = [\"<7\", \"7-8.5\", \"8.6-13\", \"14-25\", \"26-51\", \"52-512\"], q = 6)","1cbcc7c2":"df_train.describe()","fa69160e":"Fare_Sur = pd.pivot_table(data = df_train, index = \"Fare_bins\", values = \"Survived\").sort_values(by = \"Survived\", ascending = False) * 100\nround(Fare_Sur, 1)","919a3b26":"Age_Sur = pd.pivot_table(data = df_train, index = \"Age_bins\", values = \"Survived\").sort_values(by = \"Survived\", ascending = False) * 100\nround(Age_Sur, 1)","5ae0ede8":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"Fare_bins\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by Fare\")\nplt.show()","7f51b6c7":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"Pclass\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by Pclass\")\nplt.show()","217017ac":" df_train.groupby(\"Fare_bins\").mean()","c96aac33":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"Age_bins\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by Age\")\nplt.show()","af82315b":"df_train.groupby(\"Age_bins\").mean()","dadfd923":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"numeric_ticket\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by numeric_ticket\")\nplt.show()","45a6fec4":"df_train.groupby(\"numeric_ticket\").mean()","88f58a2b":"Tic_Sur = round(pd.pivot_table(data = df_train, index = \"Ticket_Text\", values = \"Survived\").sort_values(by = \"Survived\", ascending = False) * 100, 0)\nFare_Sur = round(pd.pivot_table(data = df_train, index = \"Ticket_Text\", values = \"Fare\"), 0)\nTic_Sur_count = df_train.Ticket_Text.value_counts()\nTic_Sur_count = pd.DataFrame(Tic_Sur_count)\nTicket_Text_Survival = pd.concat([Tic_Sur, Tic_Sur_count, Fare_Sur], axis = 1)\nTicket_Text_Survival.columns = [\"% of Survivor\", \"N\", \"Mean Fare\"]\nTicket_Text_Survival","e519e70f":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = Ticket_Text_Survival, x = Ticket_Text_Survival.index, y = Ticket_Text_Survival[\"% of Survivor\"], ci = None)\nplt.title(\"Distribution of survivors by Ticket_Text\")\nplt.xticks(rotation = 90)\nplt.show()","bc1a50ee":"df_train.Is_Alone.value_counts()","20fc3fdc":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"Is_Alone\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by Is_Alone\")\nplt.show()","30dd9007":"df_train.groupby(\"Is_Alone\").mean()","96549061":"# Rename the sex variable to prevent any misconciption\ndf_train = df_train.rename(columns = {\"Sex\": \"Is_Male\"})\ndf_test = df_test.rename(columns = {\"Sex\": \"Is_Male\"})","2f25fbf7":"plt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = \"Is_Male\", y = \"Survived\", ci = None)\nplt.title(\"Distribution of survivors by Is_Male\")\nplt.show()","c0699abe":"df_train.groupby(\"Is_Male\").mean()","0399a8c2":"pd.pivot_table(data = df_train, index = [\"Is_Male\", \"Pclass\"], values =[\"Survived\", \"Age\", \"Fare\", \"Is_Alone\"])","d142aaf9":"pd.pivot_table(data = df_train, index = [\"Is_Male\", \"Pclass\"], values = \"Survived\", columns = \"Age_bins\")","721387ca":"Tit_Sur = pd.pivot_table(data = df_train, index = \"Title\", values = \"Survived\").sort_values(by = \"Survived\", ascending = False)\nplt.figure(figsize = (8, 4), dpi = 100)\nsns.barplot(data = df_train, x = Tit_Sur.index, y = Tit_Sur.Survived, ci = None)\nplt.title(\"Distribution of survivors by Is_Alone\")\nplt.show()","b1e26e28":"pd.pivot_table(data = df_train, index = \"Title\", values = [\"Survived\", \"Age\", \"Fare\", \"Is_Alone\"]).sort_values(\"Survived\", ascending = False)","03001de5":"df_train.corr()","e2b7487f":"plt.figure(figsize = (12, 5), dpi = 100)\nsns.heatmap(round(df_train.corr(), 1), annot = True, cmap = \"viridis\", annot_kws={\"fontsize\":10})\nplt.show()","5f048acb":"pairplot_data = df_train[['Survived', 'Pclass', 'Is_Male', 'Age', 'SibSp', 'Parch', 'Fare', 'num_family', 'numeric_ticket', 'Is_Alone']]\nsns.pairplot(pairplot_data, diag_kind = \"kde\", hue = \"Survived\")\nplt.show()","906f2461":"#cols = ['Pclass', 'Is_Male', 'Age', 'SibSp', 'Parch', 'Fare', 'num_family', 'numeric_ticket', 'Is_Alone']\n#for col in cols:\n    #plt.figure(figsize = (8, 4), dpi = 100)\n    #sns.kdeplot(data = df_train, x= col, hue = \"Survived\")","c8278661":"df_train.head()","672c3441":"## dropping unnecessairy features for model training\ndf_train = df_train.drop([\"Name\", \"Ticket\", \"Fare\", \"Age\"], axis = 1)\ndf_test = df_test.drop([\"Name\", \"Ticket\", \"Fare\", \"Age\"], axis = 1)","ca49b870":"# dropping repetitive features\ndf_train = df_train.drop([\"SibSp\", \"Parch\", \"num_family\"], axis = 1)\ndf_test = df_test.drop([\"SibSp\", \"Parch\", \"num_family\"], axis = 1)","2f9bcc2a":"df_train.head()","a46ecca3":"df_train.info()","789a7b31":"#Creating dummy varaibles\nX = df_train.drop([\"Survived\", \"PassengerId\"], axis = 1)\nX_dum = pd.get_dummies(X, drop_first = True)\ndf_test_dum = df_test.drop(\"PassengerId\", axis = 1)\ndf_test_dum = pd.get_dummies(df_test, drop_first = True)\ny = df_train[\"Survived\"]","98cb555f":"#Making Sure that training and test data sets have the same columns\nTrain_cols = X_dum.columns\ntest_cols = df_test_dum.columns \nfor col in test_cols:\n    if col not in Train_cols:\n        df_test_dum = df_test_dum.drop(col, axis = 1)","0a176ba2":"#import cross validation \nfrom sklearn.model_selection import cross_val_score","2cf1dcdb":"#Logestic Regression\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(max_iter = 2000)\n\n#Model Training\nlog_reg.fit(X_dum, y)\n\n#Prediction and validation\nacc_score = round(log_reg.score(X_dum, y) * 100, 2)\nacc_score\n\n#cross validation\ncv = cross_val_score(log_reg,X_dum,y,cv=5)\ncv_mean = round(cv.mean() * 100, 2)\n\npd.DataFrame({\"acc_score\": [acc_score], \"cv_score\": [cv_mean]})","084461dc":"#Support Vector Machine\nfrom sklearn.svm import SVC\nSVC = SVC()\n\n#Model Training\nSVC.fit(X_dum, y)\n\n#Prediction and validation\nacc_score = round(SVC.score(X_dum, y) * 100, 2)\n\n\n#cross validation\ncv = cross_val_score(SVC,X_dum,y,cv=5)\ncv_mean = round(cv.mean() * 100, 2)\n\npd.DataFrame({\"acc_score\": [acc_score], \"cv_score\": [cv_mean]})","943f671c":"#Descision Tree \nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\n\n#Model Training\ndt.fit(X_dum, y)\n\n#Prediction and validation\nacc_score = round(dt.score(X_dum, y) * 100, 2)\n\n\n#cross validation\ncv = cross_val_score(dt,X_dum,y,cv=5)\ncv_mean = round(cv.mean() * 100, 2)\n\npd.DataFrame({\"acc_score\": [acc_score], \"cv_score\": [cv_mean]})","d2bc3e0f":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrs = RandomForestClassifier()\n\n#Model Training\nrs.fit(X_dum, y)\n\n#Prediction and validation\nacc_score = round(rs.score(X_dum, y) * 100, 2)\n\n#cross validation\ncv = cross_val_score(rs,X_dum,y,cv=5)\ncv_mean = round(cv.mean() * 100, 2)\n\npd.DataFrame({\"acc_score\": [acc_score], \"cv_score\": [cv_mean]})","095682a9":"#XGB\nfrom sklearn.ensemble import GradientBoostingClassifier\nxgb = GradientBoostingClassifier()\n\n#Model Training\nxgb.fit(X_dum, y)\n\n#Prediction and validation\nacc_score = round(xgb.score(X_dum, y) * 100, 2)\n\n\n#cross validation\ncv = cross_val_score(xgb,X_dum,y,cv=5)\ncv_mean = round(cv.mean() * 100, 2)\n\npd.DataFrame({\"acc_score\": [acc_score], \"cv_score\": [cv_mean]})","3a68a817":"# import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nlog_reg = LogisticRegression(max_iter = 2000)\n\n# param_grid\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 50),\n              'solver' : ['liblinear']}\n\n# grid_model: Logestic Regression\nlr_tuned = GridSearchCV(log_reg, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_lr = lr_tuned.fit(X_dum, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(lr_tuned.best_params_))\nprint(\"Best score is {}\".format(lr_tuned.best_score_ * 100))","9002c522":"rs = RandomForestClassifier()\n\n# param_grid\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\n# grid_model: Logestic Regression\nrs_tuned = GridSearchCV(rs, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_rs = rs_tuned.fit(X_dum, y)\n\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(rs_tuned.best_params_))\nprint(\"Best score is {}\".format(rs_tuned.best_score_ * 100))","97779b1c":"#Prediction and submission\ny_predict = best_lr.predict(df_test_dum)\n\n#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':df_test['PassengerId'],'Survived':y_predict})\n\n#Visualize the first 5 rows\nsubmission.head()","1373f395":"#Submission\nfilename = 'Titanic_Predictions_2.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","6842bf41":"### Done! Now It is all nice and clean\n#### But still more feature engineering is required, we will create new variables as follows: \n1. Number of family members on board = 1 + SibSp + Parch\n2. Title needs to be seprated from Name\n3. Regrouping the Titles\n4. Seprate pure numeric Tickets from Text-Numeric Tickets: It may say something about the income and thus the social class of the passenger.\n5. Regrouping Ticket labels","01a2e776":"- **537 passengers have no family relatives on board**","256aa3fe":"**Conclusion 3:** It turns out that our first impresion was somehow incorrect. Whether the ticket has text on it or not has no impact on survival. But it does have correlation with Fare and Age as we previously expected. Morover, when we look at the distribution of survivors by text wrote on each ticket we immediatly see a strong correlation.\n\n**Now lets see if having family relatives on board affects survival**","e4adc7ac":"### 4. Model Training, Validation, and Prediction","ae051b59":"**Conclusion 5:** Regardless to the Ticket Class and Age, Females have higher rates of survival than males.\n\n**Lets now explore the impact of Titles no Survival**","10c1a954":"**Conclusion 1:** The more rich the passenger was, the more likely he survived.","a5a48d5a":"#### 2. Random Forest Classifier","19086e90":"IF you tried to complete the project without running the preceding cell, you will not be able to complete the upcoming model training parts. You will face an error indicating that the test data set do not have the same collumns as the training data set. The error resulted from wrangling the two data sets in seprate, some variables in the test set had categories that do not exist in the same variable in the training set.","f405f1f8":"### Introduction:\nIn This note book we will try to explore the key determinants of survival on Titanic through the following steps:\n1. Load and cheack the data.\n2. Feature engineering.\n3. Explanatory Data analysis.\n4. Model Training, Validation, and Prediction","815fd96c":"#### 1. Logestic Regression","cc79e3f0":"As you can see the provided data is splitted into two parts: training and test data. We will combine them together so that any feature engineering applies to both of them at once. After completing our explanatory data analysis and start model training, we will split them again.","a97544c2":"### 2. Feature Engineering","03b16722":"To better understand the relationship between Survival and other Continious features such as Fare and Age, we need to convert the latter int catgorical features.","841d45bb":"**Conclusion 4:** Unexpectedly, Those who are alone are less likely to survive compared to those who are not alone. This is might be due to the fact that they are older and less well-off.\n\n**Lets now if males have higher survival rate than females?**","33641611":"**Conclusion 6:** Regardless to the Ticket Class and Age, Miss and Mrs have higher rates of survival than other titles.","fbf3be52":"### 1. Load and Check","866b796d":"### Hyper Parameter Tunning\nLets now do some hyper parameter tunning to improve model results.","d76bfc63":"The general information about the dataframe points out to several problems:\n1. PassengerId is stored as Integer, where in fact it has no numeric meaning. So, it should be converted to an Object.\n2. Categorical variables such as Sex and Embarked should be Integer not Float. But we will convert it latter before training using pd.get_dummies().\n3. Age, Fare, Cabin, and Embarked have missing values.\n4. The Name needs to be splitted into Title and Name\n5. SibSp and Parch should to be combined into 1 variable (number of family members)","8d51643d":"**Conclusion 2: from the previous tables we can cleary see why those aged 24 - 25 were the least likely to survive**\n1. They were entirely males (93% of this age band were males)\n2. They were the most disadvantaged (They paid the lowest Fare)\n3. They had many siblings and other family members on board.\n\n**Here are the possiblities:**\n1. It seems that they sacrificed themselves to rescue other.\n2. Being disadvanteged and poor, they are least probable to have had access to survival equipments such as life-jackets for example.\n3. Analyzing the second table suggests that bieng rich is the most important determinant of survival\n\n**From the previous tables, we noticed that those with numerical tickets were most likely to be youthful disadvanteged males with low odds of survival, lets invisitage it further**","593cea37":"**Age bins (execluding 24 - 25) have similar survival rate. We need to further invistigate this age band in specific.**","007c7b92":"### How to deal with Missing Data?\nThere are many strategies to fill missing data:\n1. Fill with the mean (better used in case of continous variables without outliers)\n2. Fill with the median (better used in case of continous variables with outliers)\n3. Fill with the mode (better used in case of categorical variables)\n4. Drop the entire variable if the number of missing points is too large\n\nBased on the above strategies:\n1. Embarked: has just 2 missing values, se we will fill them with the mode (the most frequent data point)\n2. Cabin: 77.1 percent of its values are missing, we will drop it entirly.\n3. Age: Just 20 percent if the values are missing, we will fill these values with the mean of age given the values of other feature such as Sex, Ticket... etc.\n4. In Test Data, Fill Fare with the mode. ","a222e2a0":"### 3. Explanatory Data analysis\nIn this section we will explore who had the highest probability of survival, during this process we will select the most relevant variables to feed the model with. ","9f25be02":"### Lets now have have a look at the statistical distribution for each feature "}}