{"cell_type":{"aae38128":"code","28b4cc43":"code","38be5b7a":"code","09be7856":"code","1b476d43":"code","568044b3":"code","a7f93803":"code","e18193f9":"code","d19f07bf":"code","d228436e":"code","c4046248":"code","b4da38b9":"code","6d17b3d4":"code","1af735c2":"code","7da2cb01":"code","dbbf7640":"code","d9b579e7":"code","9c061602":"code","474df929":"code","f6e2dc01":"code","838f7801":"code","fd5b55ce":"code","5f37e262":"code","c61cfa46":"markdown","cc66fc45":"markdown","4604314f":"markdown","b420b82e":"markdown","1d271ccb":"markdown","5a46f7d5":"markdown","bbf63729":"markdown","a863862d":"markdown","ebd040d9":"markdown","5e8a2ebb":"markdown","f9e70d69":"markdown","0bdc0bb1":"markdown","50a2632c":"markdown","4d28ac78":"markdown","6c2c5151":"markdown","c6e6bec8":"markdown","295aec75":"markdown","3ce71e68":"markdown","d4beca69":"markdown","bb15538a":"markdown","057e9bb4":"markdown","6b69e741":"markdown","6c219535":"markdown","f9f8dc23":"markdown","48816fb1":"markdown","5c1045c1":"markdown","315efb5f":"markdown","5cf0a7cd":"markdown","b211e16b":"markdown","fb5ce5e0":"markdown"},"source":{"aae38128":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","28b4cc43":"df=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","38be5b7a":"# See the number of rows and columns\nprint(\"Rows, columns: \" + str(df.shape))\n# See the first five rows of the dataset\ndf.head()","09be7856":"df.info()","1b476d43":"df.describe()","568044b3":"# The data looks very clean by looking at the first five rows, but I still wanted to make sure that there were no missing values.\n# Missing Values\nprint(df.isna().sum())","a7f93803":"# Checking For Unique Values in quality Attribute of the Dataset\n\ndf['quality'].unique()","e18193f9":"# The value_counts().sort_index() will Provide Number of data With Respect to Each value of attribue in sorted Manner\n\ndf.quality.value_counts().sort_index()","d19f07bf":"# A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. \nsns.countplot(x='quality',data=df)\n# The Below Distribution Shows Quality Range Between (3 to 8 )","d228436e":"# Create Classification version of target variable\ndf['goodquality'] = [1 if x >= 7 else 0 for x in df['quality']]\n# Separate feature variables and target variable\nX = df.drop(['quality','goodquality'], axis = 1)\ny = df['goodquality']","c4046248":"# See proportion of good vs bad wines\ndf['goodquality'].value_counts()","b4da38b9":"corr = df.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","6d17b3d4":"# Normalize feature variables\nfrom sklearn.preprocessing import StandardScaler\nX_features = X\nX = StandardScaler().fit_transform(X)","1af735c2":"# Splitting the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)","7da2cb01":"from sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nmodel1 = DecisionTreeClassifier(random_state=1)\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\nprint(classification_report(y_test, y_pred1))","dbbf7640":"from sklearn.ensemble import RandomForestClassifier\nmodel2 = RandomForestClassifier(random_state=1)\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\nprint(classification_report(y_test, y_pred2))","d9b579e7":"from sklearn.ensemble import AdaBoostClassifier\nmodel3 = AdaBoostClassifier(random_state=1)\nmodel3.fit(X_train, y_train)\ny_pred3 = model3.predict(X_test)\nprint(classification_report(y_test, y_pred3))","9c061602":"from sklearn.ensemble import GradientBoostingClassifier\nmodel4 = GradientBoostingClassifier(random_state=1)\nmodel4.fit(X_train, y_train)\ny_pred4 = model4.predict(X_test)\nprint(classification_report(y_test, y_pred4))","474df929":"import xgboost as xgb\nmodel5 = xgb.XGBClassifier(random_state=1)\nmodel5.fit(X_train, y_train)\ny_pred5 = model5.predict(X_test)\nprint(classification_report(y_test, y_pred5))","f6e2dc01":"feat_importances = pd.Series(model2.feature_importances_, index=X_features.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","838f7801":"feat_importances = pd.Series(model5.feature_importances_, index=X_features.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","fd5b55ce":"# Filtering df for only good quality\ndf_temp = df[df['goodquality']==1]\ndf_temp.describe()\n","5f37e262":"# Filtering df for only bad quality\ndf_temp2 = df[df['goodquality']==0]\ndf_temp2.describe()","c61cfa46":"### The describe() function: \nIt computes a summary of statistics pertaining to the DataFrame columns. This function gives the mean, std and IQR values. And, function excludes the character columns and given summary about numeric columns.","cc66fc45":"<ol>\n<li>Fixed acidity\n<li>Volatile acidity\n<li>Citric acid\n<li>Residual sugar\n<li>Chlorides\n<li>Free sulfur dioxide\n<li>Total sulfur dioxide\n<li>Density\n<li>pH\n<li>Sulfates\n<li>Alcohol\n    <\/ol>","4604314f":"### Convert to a Classification Problem\nGoing back to my objective, I wanted to compare the effectiveness of different classification techniques, so I needed to change the output variable to a binary output.\n\nFor this problem, I defined a bottle of wine as \u2018good quality\u2019 if it had a quality score of 7 or higher, and if it had a score of less than 7, it was deemed \u2018bad quality\u2019.\n\nOnce I converted the output variable to a binary output, I separated my feature variables (X) and the target variable (y) into separate dataframes.","b420b82e":"Below, I graphed the feature importance based on the Random Forest model and the XGBoost model. While they slightly vary, the top 3 features are the same: alcohol, volatile acidity, and sulphates. If you look below the graphs, I split the dataset into good quality and bad quality to compare these variables in more detail.","1d271ccb":"# Comparing the Top 4 Features","5a46f7d5":"# Introduction\nAs the quarantine continues, I\u2019ve picked up a number of hobbies and interests\u2026 including WINE. Recently, I\u2019ve acquired a taste for wines, although I don\u2019t really know what makes a good wine. Therefore, I decided to apply some machine learning models to figure out what makes a good quality wine!","bbf63729":"### Model 5: XGBoost\n","a863862d":"### Correlation Matrix\nNext I wanted to see the correlations between the variables that I\u2019m working with. This allows me to get a much better understanding of the relationships between my variables in a quick glimpse.\n\nImmediately, I can see that there are some variables that are strongly correlated to quality. It\u2019s likely that these variables are also the most important features in our machine learning model, but we\u2019ll take a look at that later.","ebd040d9":"### Split data\nNext I split the data into a training and test set so that I could cross-validate my models and determine their effectiveness.","5e8a2ebb":"### Proportion of Good vs Bad Wines\nI wanted to make sure that there was a reasonable number of good quality wines. Based on the results below, it seemed like a fair enough number. In some applications, resampling may be required if the data was extremely imbalanced, but I assumed that it was okay for this purpose.","f9e70d69":"### The info() function:\nIt is used to print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.","0bdc0bb1":"## Objectives\nThe objectives of this project are as follows:\n<ol>\n<li>To experiment with different classification methods to see which yields the highest accuracy.\n<li>To determine which features are the most indicative of a good quality wine.\n    <\/ol>\n    So Let's Start","50a2632c":"### Missing Values\n","4d28ac78":"### Random Forest\n","6c2c5151":"### Good Quality","c6e6bec8":"### Modelling\nFor this project, I wanted to compare five different machine learning models: decision trees, random forests, AdaBoost, Gradient Boost, and XGBoost. For the purpose of this project, I wanted to compare these models by their accuracy.","295aec75":"### Model 3: AdaBoost\nThe next three models are boosting algorithms that take weak learners and turn them into strong ones. I don\u2019t want to get sidetracked and explain the differences between the three because it\u2019s quite complicated and intricate. That being said, I\u2019ll leave some resources where you can learn about AdaBoost, Gradient Boosting, and XGBoosting.\n<ul>\n<li>StatQuest: AdaBoost\n<li>StatQuest: Gradient Boost\n<li>StatQuest: XGBoost\n    <\/ul>","3ce71e68":"### Bad Quality","d4beca69":"There are a total of 1599 rows and 12 columns.","bb15538a":"### Model 2: Random Forest\nRandom forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. What\u2019s the point of this? By relying on a \u201cmajority wins\u201d model, it reduces the risk of error from an individual tree.\n\nFor example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all 4 decision trees, the predicted value would be 1. This is the power of random forests.","057e9bb4":"## Setup\nFirst, I imported all of the relevant libraries that I\u2019ll be using as well as the data itself.\n### Importing Libraries","6b69e741":"This is a very beginner-friendly dataset. I did not have to deal with any missing values, and there isn\u2019t much flexibility to conduct some feature engineering given these variables. Next, I wanted to explore my data a little bit more.","6c219535":"### Model 4: Gradient Boosting\n","f9f8dc23":"### Understanding Data\nNext, I wanted to get a better idea of what I was working with.","48816fb1":"###  Model 1: Decision Tree\nDecision trees are a popular model, used in operations research, strategic planning, and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy.","5c1045c1":"By looking into the details, we can see that good quality wines have higher levels of alcohol on average, have a lower volatile acidity on average, higher levels of sulphates on average, and higher levels of residual sugar on average.","315efb5f":"#### Now Comes the Fun Part !","5cf0a7cd":"I used Kaggle\u2019s Red Wine Quality dataset to build various classification models to predict whether a particular red wine is \u201cgood quality\u201d or not. Each wine in this dataset is given a \u201cquality\u201d score between 0 and 10. For the purpose of this project, I converted the output to a binary output where each wine is either \u201cgood quality\u201d (a score of 7 or higher) or not (a score below 7). The quality of a wine is determined by 11 input variables:","b211e16b":"## Preparing Data for Modelling\n### Standardizing Feature Variables\nAt this point, I felt that I was ready to prepare the data for modelling. The first thing that I did was standardize the data. Standardizing the data means that it will transform the data so that its distribution will have a mean of 0 and a standard deviation of 1. It\u2019s important to standardize your data in order to equalize the range of the data.\n\nFor example, imagine a dataset with two input features: height in millimeters and weight in pounds. Because the values of \u2018height\u2019 are much higher due to its measurement, a greater emphasis will automatically be placed on height than weight, creating a bias.","fb5ce5e0":"By comparing the five models, the random forest and XGBoost seems to yield the highest level of accuracy. However, since XGBoost has a better f1-score for predicting good quality wines (1), I\u2019m concluding that the XGBoost is the winner of the five models.\n## Feature Importance"}}