{"cell_type":{"96f74547":"code","2be6962d":"code","1c8b48aa":"code","eb388553":"code","d7a05803":"code","6429c34f":"code","7a97965b":"code","eb782216":"code","50d699f6":"code","59d2f026":"code","678e3500":"code","1063aae6":"code","4c805cf8":"code","dc7509cd":"code","8440b7aa":"code","d2c9e327":"code","1fe7019c":"code","0271dd9d":"code","9980bb1f":"code","b935d8a0":"code","8d5cc496":"code","af40d697":"code","560051a5":"code","e373cdb9":"code","5e7d90e7":"code","cb17878d":"code","4732f2ea":"code","93636738":"code","d9353e49":"code","36d16a8c":"code","de1190db":"code","2157e745":"markdown","df944381":"markdown","c4611826":"markdown","481e84b0":"markdown","1c3a91d0":"markdown","39675282":"markdown","b70a7938":"markdown","f227d4d3":"markdown","88e87c6d":"markdown","bf485d00":"markdown","70122e3d":"markdown","e250038b":"markdown","ccd2011c":"markdown","aa22661e":"markdown","2a1e721c":"markdown","75a8d5c0":"markdown"},"source":{"96f74547":"import numpy as np\nimport pandas as pd\nimport os\nfrom mlens.ensemble import SuperLearner\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom mlens.ensemble import Subsemble\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier\nimport optuna\nfrom optuna.samplers import TPESampler\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# To see optuna progress you need to comment this row\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)","2be6962d":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","1c8b48aa":"train.head()","eb388553":"for col in train.columns:\n    print(col, str(round(100* train[col].isnull().sum() \/ len(train), 2)) + '%')","d7a05803":"train['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]\nds = pd.concat([train, test])\n\nsur = []\ndied = []\nfor index, row in ds.iterrows():\n    s = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==1)]\n    d = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==0)]\n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n    sur.append(s)\n    died.append(d)\nds['FamilySurvived'] = sur\nds['FamilyDied'] = died\n\nds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\nds['IsAlone'] = 0\nds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\nds['Fare'] = ds['Fare'].fillna(train['Fare'].median())\nds['Embarked'] = ds['Embarked'].fillna('Q')\n\ntrain = ds[ds['Survived'].notnull()]\ntest = ds[ds['Survived'].isnull()]\ntest = test.drop(['Survived'], axis=1)\n\ntrain['rich_woman'] = 0\ntest['rich_woman'] = 0\ntrain['men_3'] = 0\ntest['men_3'] = 0\n\ntrain.loc[(train['Pclass']<=2) & (train['Sex']=='female'), 'rich_woman'] = 1\ntest.loc[(test['Pclass']<=2) & (test['Sex']=='female'), 'rich_woman'] = 1\ntrain.loc[(train['Pclass']==3) & (train['Sex']=='male'), 'men_3'] = 1\ntest.loc[(test['Pclass']==3) & (test['Sex']=='male'), 'men_3'] = 1\n\ntrain['rich_woman'] = train['rich_woman'].astype(np.int8)\ntest['rich_woman'] = test['rich_woman'].astype(np.int8)\n\ntrain[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin']])\ntest['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin']])\n\ntrain = train.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch'], axis=1)\ntest = test.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch'], axis=1)\n\ncategorical = ['Pclass', 'Sex', 'Embarked', 'Cabin']\nfor cat in categorical:\n    train = pd.concat([train, pd.get_dummies(train[cat], prefix=cat)], axis=1)\n    train = train.drop([cat], axis=1)\n    test = pd.concat([test, pd.get_dummies(test[cat], prefix=cat)], axis=1)\n    test = test.drop([cat], axis=1)\n    \ntrain = train.drop(['Sex_male', 'Name'], axis=1)\ntest =  test.drop(['Sex_male', 'Name'], axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\ntrain.head()","6429c34f":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Age\", \n    points='all',\n    height=500,\n    width=700,\n    title='Age & Survived box plot')\nfig.show()","7a97965b":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Fare\", \n    points='all',\n    height=500,\n    width=700,\n    title='Fare & Survived box plot')\nfig.show()","eb782216":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilySize\", \n    points='all',\n    height=500,\n    width=700,\n    title='Family Size & Survived box plot')\nfig.show()","50d699f6":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilyDied\", \n    points='all',\n    height=500,\n    width=700,\n    title='Family Died & Survived box plot')\nfig.show()","59d2f026":"f = plt.figure(figsize=(19, 15))\nplt.matshow(train.corr(), fignum=f.number)\nplt.xticks(range(train.shape[1]), train.columns, fontsize=14, rotation=45)\nplt.yticks(range(train.shape[1]), train.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","678e3500":"train = train[~((train['Survived']==1) & (train['Age']>70))]\ntrain = train[~((train['Survived']==1) & (train['FamilyDied']>6))]","1063aae6":"train.head()","4c805cf8":"y = train['Survived']\nX = train.drop(['Survived', 'Cabin_T'], axis=1)\nX_test = test.copy()\n\nX, X_val, y, y_val = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=False)","dc7509cd":"class Optimizer:\n    def __init__(self, metric, trials=5):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=0)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","8440b7aa":"rf = RandomForestClassifier(random_state=0)\nrf.fit(X, y)\npreds = rf.predict(X_val)\nprint('Random Forest accuracy: ', accuracy_score(y_val, preds))\nprint('Random Forest f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 500)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(min_samples_leaf=min_samples_leaf, n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 0\nrf_f1 = RandomForestClassifier(**rf_f1_params)\nrf_f1.fit(X, y)\npreds = rf_f1.predict(X_val)\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 0\nrf_acc = RandomForestClassifier(**rf_acc_params)\nrf_acc.fit(X, y)\npreds = rf_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))","d2c9e327":"xgb = XGBClassifier(random_state=0)\nxgb.fit(X, y)\npreds = xgb.predict(X_val)\nprint('XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('XGBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    model = XGBClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, gamma=gamma, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 0\nxgb_f1 = XGBClassifier(**xgb_f1_params)\nxgb_f1.fit(X, y)\npreds = xgb_f1.predict(X_val)\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 0\nxgb_acc = XGBClassifier(**xgb_acc_params)\nxgb_acc.fit(X, y)\npreds = xgb_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))","1fe7019c":"lgb = LGBMClassifier(random_state=0)\nlgb.fit(X, y)\npreds = lgb.predict(X_val)\nprint('LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('LightGBM f1-score: ', f1_score(y_val, preds))\n\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 5000)\n    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    model = LGBMClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, num_leaves=num_leaves, min_child_samples=min_child_samples,\n                           random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\nlgb_f1_params = optimizer.optimize()\nlgb_f1_params['random_state'] = 0\nlgb_f1 = LGBMClassifier(**lgb_f1_params)\nlgb_f1.fit(X, y)\npreds = lgb_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nlgb_acc_params = optimizer.optimize()\nlgb_acc_params['random_state'] = 0\nlgb_acc = LGBMClassifier(**lgb_acc_params)\nlgb_acc.fit(X, y)\npreds = lgb_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))","0271dd9d":"lr = LogisticRegression(random_state=0)\nlr.fit(X, y)\npreds = lr.predict(X_val)\nprint('Logistic Regression: ', accuracy_score(y_val, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_val, preds))","9980bb1f":"dt = DecisionTreeClassifier(random_state=0)\ndt.fit(X, y)\npreds = dt.predict(X_val)\nprint('Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Decision Tree f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(min_samples_split=min_samples_split, min_weight_fraction_leaf=min_weight_fraction_leaf, max_depth=max_depth, \n                                   min_samples_leaf=min_samples_leaf, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 0\ndt_f1 = DecisionTreeClassifier(**dt_f1_params)\ndt_f1.fit(X, y)\npreds = dt_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 0\ndt_acc = DecisionTreeClassifier(**dt_acc_params)\ndt_acc.fit(X, y)\npreds = dt_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))","b935d8a0":"bc = BaggingClassifier(random_state=0)\nbc.fit(X, y)\npreds = bc.predict(X_val)\nprint('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 500)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\nbc_f1_params = optimizer.optimize()\nbc_f1_params['random_state'] = 0\nbc_f1 = BaggingClassifier(**bc_f1_params)\nbc_f1.fit(X, y)\npreds = bc_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nbc_acc_params = optimizer.optimize()\nbc_acc_params['random_state'] = 0\nbc_acc = BaggingClassifier(**bc_acc_params)\nbc_acc.fit(X, y)\npreds = bc_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))","8d5cc496":"knn = KNeighborsClassifier()\nknn.fit(X, y)\npreds = knn.predict(X_val)\nprint('KNN accuracy: ', accuracy_score(y_val, preds))\nprint('KNN f1-score: ', f1_score(y_val, preds))\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1', 100)\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(**knn_f1_params)\nknn_f1.fit(X, y)\npreds = knn_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(**knn_acc_params)\nknn_acc.fit(X, y)\npreds = knn_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))","af40d697":"abc = AdaBoostClassifier(random_state=0)\nabc.fit(X, y)\npreds = abc.predict(X_val)\nprint('AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('AdaBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 0\nabc_f1 = AdaBoostClassifier(**abc_f1_params)\nabc_f1.fit(X, y)\npreds = abc_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 0\nabc_acc = AdaBoostClassifier(**abc_acc_params)\nabc_acc.fit(X, y)\npreds = abc_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))","560051a5":"et = ExtraTreesClassifier(random_state=0)\net.fit(X, y)\npreds = et.predict(X_val)\nprint('ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 500)\n    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n    model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n    return model\n\noptimizer = Optimizer('f1', 100)\net_f1_params = optimizer.optimize()\net_f1_params['random_state'] = 0\net_f1 = ExtraTreesClassifier(**et_f1_params)\net_f1.fit(X, y)\npreds = et_f1.predict(X_val)\nprint('Optimized on F1-score')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc', 100)\net_acc_params = optimizer.optimize()\net_acc_params['random_state'] = 0\net_acc = ExtraTreesClassifier(**et_acc_params)\net_acc.fit(X, y)\npreds = et_acc.predict(X_val)\nprint('Optimized on accuracy')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))","e373cdb9":"model = SuperLearner(folds=5, random_state=666)\nmodel.add([bc, lgb, xgb, rf, dt, knn])\nmodel.add_meta(LogisticRegression())\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\nprint('SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('SuperLearner f1-score: ', f1_score(y_val, preds))","5e7d90e7":"mdict = {\n    'RF': RandomForestClassifier(random_state=0),\n    'XGB': XGBClassifier(random_state=0),\n    'LGBM': LGBMClassifier(random_state=0),\n    'DT': DecisionTreeClassifier(random_state=0),\n    'KNN': KNeighborsClassifier(),\n    'BC': BaggingClassifier(random_state=0),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OALGBM': LGBMClassifier(**lgb_acc_params),\n    'OFLGBM': LGBMClassifier(**lgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OABC': BaggingClassifier(**bc_acc_params),\n    'OFBC': BaggingClassifier(**bc_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OAET': ExtraTreesClassifier(**et_acc_params),\n    'OFET': ExtraTreesClassifier(**et_f1_params),\n    'LR': LogisticRegression(random_state=0),\n    'ABC': AdaBoostClassifier(random_state=0),\n    'SGD': SGDClassifier(random_state=0), \n    'ET': ExtraTreesClassifier(random_state=0),\n    'MLP': MLPClassifier(random_state=0),\n    'GB': GradientBoostingClassifier(random_state=0),\n    'RDG': RidgeClassifier(random_state=0),\n    'PCP': Perceptron(random_state=0),\n    'PAC': PassiveAggressiveClassifier(random_state=0)\n}","cb17878d":"def create_model(trial):\n    model_names = list()\n    models_list = ['RF', 'XGB', 'LGBM', 'DT', 'KNN', 'BC', 'OARF', 'OFRF', 'OAXGB', 'OFXGB', 'OALGBM', \n                   'OFLGBM', 'OADT', 'OFDT', 'OAKNN', 'OFKNN', 'OABC', 'OFBC', 'OAABC', 'OFABC', 'OAET', \n                   'OFET', 'LR', 'ABC', 'SGD', 'ET', 'MLP', 'GB', 'RDG', 'PCP', 'PAC']\n    head_list = ['RF', 'XGB', 'LGBM', 'DT', 'KNN', 'BC', 'LR', 'ABC', 'SGD', 'ET', 'MLP', 'GB', 'RDG', 'PCP', 'PAC']\n    n_models = trial.suggest_int(\"n_models\", 2, 5)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(folds=folds, random_state=666)\n    models = list()\n    for item in model_names:\n        models.append(mdict[item])\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(mdict[head])\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, y)\n    preds = model.predict(X_val)\n    score = accuracy_score(y_val, preds)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=4)","4732f2ea":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\nresult","93636738":"model = SuperLearner(folds=folds, random_state=666)\nmodels = list()\nfor item in result:\n    models.append(mdict[item])\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_val, preds))","d9353e49":"preds = model.predict(X_test)\npreds = preds.astype(np.int16)","36d16a8c":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = preds\nsubmission.to_csv('submission.csv', index=False)","de1190db":"submission.head()","2157e745":"### We are going to use our single models in the first layer and LogisticRegressor as metalearner.","df944381":"### Lets create train and test dataset and create holdout set for validation.","c4611826":"### Now we will create ensemble model named SuperLearner from mlens package. For details check https:\/\/machinelearningmastery.com\/super-learner-ensemble-in-python\/","481e84b0":"### Let's do some visualizations","1c3a91d0":"### Lets see percent of NaNs for every column in training set","39675282":"### Lets create some separate single models and check accuracy score. We also try to optimize every single model using optuna framework. As we can see we can get some better results with it.","b70a7938":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Feature engineering<center><h2>","f227d4d3":"<a id=\"4\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Final submission<center><h2>","88e87c6d":"### We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing.","bf485d00":"### In this notebook I will not focus on preprocessing and feature engineering steps, just show how to build your efficient ensemble in few lines of code. I use almost the same features as in the most of kernels in current competition.","70122e3d":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>SuperLearner training and optimization<center><h2>","e250038b":"### Let's optimize SuperLearner","ccd2011c":"### Here is some basic preprocessing to get fast training and test datasets.","aa22661e":"### As we can see we improved our best single score only in a few lines of code. Feel free to add new features and try different models inside superlearner.","2a1e721c":"### Another one thing. People with family size more than 7 didn't survive.","75a8d5c0":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>Single models training and optimization<center><h2>"}}