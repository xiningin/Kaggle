{"cell_type":{"ac7f78c0":"code","f226d755":"code","57ff6a8d":"code","8f1f73eb":"code","643b6dc6":"code","21d16b28":"code","7e6c1932":"code","6a33c39b":"code","eda8264d":"code","01228e8f":"code","3b7cab99":"code","de267da5":"code","f3f1ddc6":"code","165465b9":"code","b9d31ed5":"code","a0bf5b75":"code","a82e3e1d":"code","4592ea75":"code","b0fdce5b":"code","87025b05":"code","74ff6272":"markdown","d5b50f0d":"markdown","409853c7":"markdown","09d51aa4":"markdown","7ef197c6":"markdown","e4203b91":"markdown","ceb5527c":"markdown","2d00439c":"markdown","7e91b7a5":"markdown","1cb0cb27":"markdown","dc0fba65":"markdown","0370bc2c":"markdown","3b770b06":"markdown","e42582ed":"markdown","349dfccc":"markdown","17065b2f":"markdown"},"source":{"ac7f78c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f226d755":"# importing basic libraries\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom glob import glob ## glob is used to retrieve files \n\n# set seed\nnp.random.seed(21)","57ff6a8d":"from PIL import Image\n\ndirectory_benign_train = '..\/input\/skin-cancer-malignant-vs-benign\/train\/benign'\ndirectory_malignant_train = '..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant'\ndirectory_benign_test = '..\/input\/skin-cancer-malignant-vs-benign\/test\/benign'\ndirectory_malignant_test = '..\/input\/skin-cancer-malignant-vs-benign\/test\/malignant'\n\n## Loading images and converting them to numpy array using their RGB value\nread = lambda imname: np.asarray(Image.open(imname).convert('RGB'))\n# np.asarray converts the objects into array\/list\n\n# Loading train images\nimg_benign_train = [read(os.path.join(directory_benign_train, filename)) for filename in os.listdir(directory_benign_train)]\nimg_malignant_train = [read(os.path.join(directory_malignant_train, filename)) for filename in os.listdir(directory_malignant_train)]\n\n# Loading test images\nimg_benign_test = [read(os.path.join(directory_benign_test, filename)) for filename in os.listdir(directory_benign_test)]\nimg_malignant_test = [read(os.path.join(directory_malignant_test, filename)) for filename in os.listdir(directory_malignant_test)]\n\n#img_benign_train\ntype(img_benign_train)","8f1f73eb":"# Converting list to numpy array for faster and more convenient operations going forward\n\nX_benign_train = np.array(img_benign_train, dtype='uint8')\nX_malignant_train = np.array(img_malignant_train, dtype='uint8')\n\nX_benign_test = np.array(img_benign_test, dtype='uint8')\nX_malignant_test = np.array(img_malignant_test, dtype='uint8')\n\ntype(X_benign_train)","643b6dc6":"## Creating labels: benign is 0 and malignant is 1\n\ny_benign_train = np.zeros(X_benign_train.shape[0])\ny_malignant_train = np.ones(X_malignant_train.shape[0])\n\ny_benign_test = np.zeros(X_benign_test.shape[0])\ny_malignant_test = np.ones(X_malignant_test.shape[0])\n\ny_malignant_train","21d16b28":"## Merge data to form complete training and test sets\n# axis = 0 means rows\n\nX_train = np.concatenate((X_benign_train, X_malignant_train), axis=0) \ny_train = np.concatenate((y_benign_train, y_malignant_train), axis=0)\n\nX_test = np.concatenate((X_benign_test, X_malignant_test), axis=0)\ny_test = np.concatenate((y_benign_test, y_malignant_test), axis=0)\n\nprint(\"Shape of X_train: \", X_train.shape) # one image constitutes to (224, 224, 3) and we have 2637 total images in training set\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_test: \", y_test.shape)\n\ny_test","7e6c1932":"s1 = np.arange(X_train.shape[0])\nnp.random.shuffle(s1)\nX_train = X_train[s1]\ny_train = y_train[s1]\n\ns2 = np.arange(X_test.shape[0])\nnp.random.shuffle(s2)\nX_test = X_test[s2]\ny_test = y_test[s2]","6a33c39b":"print(\"Shuffle orders example: \", s1)","eda8264d":"fig = plt.figure(figsize=(12,8))\ncolumns = 5\nrows = 3\n\nfor i in range(1, columns*rows+1):\n    ax = fig.add_subplot(rows, columns, i)\n    if y_train[i] == 0:\n        ax.title.set_text('Benign')\n    else:\n        ax.title.set_text('Malignant')\n    plt.imshow(X_train[i], interpolation='nearest')\nplt.show()","01228e8f":"import keras\nfrom keras.utils.np_utils import to_categorical \n\ny_train = to_categorical(y_train, num_classes=2)\ny_test = to_categorical(y_test, num_classes=2)\n\ntype(y_train)","3b7cab99":"y_train","de267da5":"## Normalize by dividing by RGB value\nX_train = X_train\/255\nX_test = X_test\/255","f3f1ddc6":"#X_train","165465b9":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import Adam, RMSprop\n\ndef build_cnn_model(input_shape = (224, 224, 3), num_classes=2):\n    \n    model = Sequential()\n    \n    # adding 64 filters, each filter has a size of 3*3\n    # padding is of 2 types: SAME and VALID (SAME means doing the padding around the image, VALID means no padding)\n    # kernel initilizer is for intializing the weights of the network --> the default one is glorot_uniform so don't need to mention parameter\n    model.add(Conv2D(64, kernel_size=(3,3), padding='Same', input_shape = input_shape, activation='relu', kernel_initializer = 'glorot_uniform'))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    # 25% of the nodes will be dropped out\n    model.add(Dropout(0.25)) \n    \n    \n    model.add(Conv2D(64, kernel_size=(3,3), padding='Same', activation='relu', kernel_initializer = 'glorot_uniform'))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n    \n    \n    model.add(Flatten())\n    \n    # normal initializer draws samples from a truncated normal distribution centered at 0 and SD = sqrt(2\/number of input units)\n    model.add(Dense(128, activation='relu', kernel_initializer='normal'))\n    model.add(Dense(128, activation='relu', kernel_initializer='normal'))\n    \n    model.add(Dense(num_classes, activation = 'softmax'))\n    \n    model.summary()\n    \n    \n    ## OPTIMIZERS are the functions to adjust the weights and minimize the loss\n    # Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. \n    # Adam is relatively easy to configure where the default configuration parameters do well on most problems\n    # lr is the alpha rate i.e. learning rate\n    optimizer= Adam(lr=0.001) \n    \n    model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])\n    \n    return model   ","b9d31ed5":"## MODEL SUMMARY\n\nmodel_cnn = build_cnn_model()","a0bf5b75":"from keras.callbacks import ReduceLROnPlateau\n\n# Learning rate annealer is used to reduce the learning rate by some percentage after certain number of training iterations\/epochs\nlearning_rate_annealer = ReduceLROnPlateau(monitor='val_acc',\n                                          patience=5,\n                                          verbose=1,\n                                          factor=0.5,\n                                          min_lr = 1e-7)\n\n# epochs is the number of iterations\n# batch_size is the number of images in one epoch\n# verbose = 1 shows us the animation of the epoch using progres_bar\nhistory = model_cnn.fit(X_train, \n                    y_train, \n                    validation_split=0.2, \n                    epochs=50, \n                    batch_size = 64, \n                    verbose=1,\n                    callbacks=[learning_rate_annealer])\n\n\n# list all data in history\nprint(history.history.keys())","a82e3e1d":"## Summarize model history for accuracy and loss for training and validation\n\n# 1. Accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\n\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()\n\n# 2. Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\n\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()","4592ea75":"from sklearn.metrics import accuracy_score\n\n# Testing model on test data to evaluate\ny_pred = np.argmax(model_cnn.predict(X_test), axis=-1)\n\nprint(accuracy_score(np.argmax(y_test, axis=1),y_pred))","b0fdce5b":"y_pred = np.array(y_pred, dtype='uint8')\ny_pred","87025b05":"X_test = np.concatenate((X_benign_test, X_malignant_test), axis=0)\ny_test = np.concatenate((y_benign_test, y_malignant_test), axis=0)\n\n# shuffling data\ns2 = np.arange(X_test.shape[0])\nnp.random.shuffle(s2)\nX_test = X_test[s2]\ny_test = y_test[s2]\ny_pred = y_pred[s2]\n\n# plotting\nfig = plt.figure(figsize=(20,40))\ncolumns = 4\nrows = 10\n\nfor i in range(1, columns*rows+1):\n    ax = fig.add_subplot(rows, columns, i)\n    if y_test[i] == 0:\n        if y_pred[i] == 0:\n            ax.set_title('Actual Benign, Predicted Benign', color='green')\n        else:\n            ax.set_title('Actual Benign, Predicted Malignant', color='yellow')\n    else:\n        if y_pred[i] == 1:\n            ax.set_title('Actual Malignant, Predicted Malignant', color='green')\n        else:\n            ax.set_title('Actual Malignant, Predicted Benign', color='red')\n    \n        \n    plt.imshow(X_test[i], interpolation='nearest')\nplt.show()","74ff6272":"## References:\n\n1. https:\/\/www.kaggle.com\/fanconic\/cnn-for-skin-cancer-detection\n2. https:\/\/www.kaggle.com\/rogeriovaz\/skin-cancer-images-cnn-deep-learning\/notebook\n3. https:\/\/www.kaggle.com\/sid321axn\/step-wise-approach-cnn-model-77-0344-accuracy","d5b50f0d":"### In the above photos: <br>\n<h4>\n1. <b> Green title <\/b> means correct prediction <br><br>\n2. <b> Yellow title <\/b> means incorrect prediction of Benign Cancer as Malignant (but it is still accpetable since doctors would pay careful attention on it) <br><br>\n3. <b> Red title <\/b> means incorrect prediction of Malignant Cancer as Benign (This is the most dangerous case since we would not want a Malignant Cancer to get unnoticed or given less attention)<\/h4>","409853c7":"![CNN Structure.png](attachment:45cf54c3-114b-4e5b-8a8b-c5718d280c73.png)","09d51aa4":"## Displaying the first few images of training set","7ef197c6":"## Plotting the results","e4203b91":"## Encoding of Labels","ceb5527c":"1. <b>Convolutional Layer<\/b>: Filters\/Feature maps that are used to transform the images. This is called the Convolutional Layer.\n2. <b>Pooling Layer<\/b>: Max Pooling is useful for downsampling. It reduces computational costs and also to some extent overfitting.\n3. <b>Dropout<\/b>: Regularization method to randomly drop some nodes while training (i.e. setting their weights to 0). This forces the network to learn features in a distributed ways. Thus, prevents overfitting and improves generalization.\n4. <b>Flatten<\/b>: Flatten layer is used to convert feature maps to 1D vector so that they can be used for prediction.\n5. <b>Dense layer with Relu<\/b>: Dense Layer refers to simple ANN with non-linear Relu activation function.\n6. <b>Dense layer with Softmax<\/b>: ANN layer with binary activation function Softmax for final classification.","2d00439c":"## Loading images and making training and test set","7e91b7a5":"## Loading basic libraries","1cb0cb27":"## Input data has 2 directories: train and test each containing samples of benign and malignant skin cancer images.\n<b> I will be not be loading libaries at the top, instead will be loading them as and when required for learning purposes <\/b>","dc0fba65":"#### Shuffling Data\n\n<br> Shuffling data is important since we want independence in our samples and don't want them to be affected by samples before and after. Since we don't know who created the dataset and if the dataset is sorted in any order, it is better to shuffle the training and test set to create independent points with minimal bias.\n<br><br> Shuffling while generating epochs also ensures that the risk of creating batches that are not representative of the overall dataset is minimized ","0370bc2c":"## Building basic CNN model","3b770b06":"#### Note: Numpy array having the following advantages over a list:\n<br>\n1. Numpy array consume much less memory than lists.<br>\n2. Numpy array can support a lot of mathematical operations that list cannot. Element wise operations are possible in np array but not in case of lists. (Np array contain only homogeneuos elements while lists can have heterogenous elements). Since they are homogeneous, it is much convenient to do calculations.<br> \n3. They can perform operations much faster as compared to lists.\n\n<br><b>Detailed difference between numpy array and list found at: https:\/\/www.geeksforgeeks.org\/python-lists-vs-numpy-arrays\/ <\/b>","e42582ed":"## Testing the model","349dfccc":"## Normalization","17065b2f":"## Decent Accuracy of 82.9% with Basic CNN model"}}