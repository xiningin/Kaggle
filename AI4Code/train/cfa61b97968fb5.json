{"cell_type":{"44bb6bc6":"code","e8d719ba":"code","a8902539":"code","ea48b97f":"code","f933a395":"code","e1250f3f":"code","c67156ca":"code","75ac5791":"code","2f3edf90":"code","8ae11cdf":"code","f5d9d05f":"code","8b93e39a":"code","6a5d390f":"code","9c4ae2e3":"code","59ad4453":"code","6195ba0b":"code","5563393c":"code","533368c6":"code","e24b924a":"code","426e5093":"code","7ca82cac":"code","e72f0725":"code","c7c445f1":"code","753a2055":"code","07455dd6":"code","87b19df0":"code","fe364bed":"code","8a75c96f":"code","b01c42ba":"code","cc908e17":"code","79eff7f6":"code","d2482650":"code","3dad6b2b":"code","93939481":"code","efe56722":"code","5f8f4b16":"code","a500f2b2":"code","1743a003":"code","d96bbeae":"code","68268a2d":"code","9eb16db6":"code","8fd8712d":"code","98ddd300":"code","cff71793":"code","6e7647ee":"code","8ef96d13":"code","7859eac4":"markdown","9cd70327":"markdown","69de1106":"markdown","c40c8b87":"markdown","fd7fb2b4":"markdown","df0d99b9":"markdown","712ca66d":"markdown","61338395":"markdown","b9cd7f49":"markdown","e7feba1b":"markdown","e2ec58cc":"markdown","0921932e":"markdown","8a0f4e82":"markdown","fef1e317":"markdown","14b5fed2":"markdown","1569dc69":"markdown","43986d65":"markdown","d635c740":"markdown","efa16866":"markdown","a6bbf8b9":"markdown","6d7bd51d":"markdown","2d5290a4":"markdown","7a08b916":"markdown","be3cb349":"markdown","cb7797e3":"markdown","441c0d12":"markdown","c6aa69f1":"markdown","074ed4d5":"markdown","c2e9a468":"markdown","1af2d4ed":"markdown","b0ac09e0":"markdown","5a2d60f8":"markdown","f77a50c3":"markdown","a8be0b44":"markdown","e948f434":"markdown","0c907d9e":"markdown","5b754389":"markdown","2d2f2e9d":"markdown","8cae52af":"markdown","d33418a6":"markdown","4d1509ab":"markdown","87fd3a11":"markdown","c593861c":"markdown","9f5c7829":"markdown","21647ed2":"markdown","25d270c8":"markdown","64f7d0fe":"markdown","1aae5619":"markdown","14627ce1":"markdown","24459127":"markdown","ddf2eacb":"markdown"},"source":{"44bb6bc6":"# Fundamental libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# General ML libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport operator\nimport time\n\n# Neural networks libraries\nimport keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","e8d719ba":"sample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\nprint(\"Original data structure:\")\ndisplay(train.head())","a8902539":"fig = sns.countplot(train['label'], alpha=0.75).set_title(\"Digit counts\")\nplt.xlabel(\"Digits\")\nplt.ylabel(\"Counts\")\nplt.savefig('digit_counts.png')\nplt.show()","ea48b97f":"train.isna().sum().sort_values(ascending=False)","f933a395":"img_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef prep_data(raw, test):\n    y = raw[\"label\"]\n    x = raw.drop(labels = [\"label\"],axis = 1) \n    \n    x = x\/255.\n    x = x.values.reshape(-1, img_rows,img_cols,1)\n    \n    test = test\/255.\n    test = test.values.reshape(-1,img_rows,img_cols,1)\n    \n    return x, y, test\n\nX_train, Y_train, X_test = prep_data(train, test)\nY_train = to_categorical(Y_train, num_classes)\n\nprint(\"Data preparation correctly finished\")","e1250f3f":"batch_size = 64\n\nmodel_1 = Sequential()\nmodel_1.add(Conv2D(filters=16, kernel_size=(4,4),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_1.add(MaxPool2D())\nmodel_1.add(Flatten())\nmodel_1.add(Dense(256, activation='relu'))\nmodel_1.add(Dense(num_classes, activation='softmax'))\n\nprint(\"CNN ready to compile\")","c67156ca":"model_1.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model_1.fit(X_train, Y_train,\n          batch_size=batch_size,\n          epochs=20,\n          validation_split = 0.1)\n\nprint(\"Fitting finished\")","75ac5791":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model_1 accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.savefig('initial_cnn.png')\nplt.show()","2f3edf90":"# predict results\nresults = model_1.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step3.csv\",index=False)","8ae11cdf":"# Model_kernel_1: 3x3\nmodel_kernel_1 = Sequential()\nmodel_kernel_1.add(Conv2D(filters=16, kernel_size=(3,3), padding='same',\n                     activation='relu', \n                     input_shape=(img_rows, img_cols, 1)))\nmodel_kernel_1.add(MaxPool2D(padding='same'))\nmodel_kernel_1.add(Flatten())\nmodel_kernel_1.add(Dense(256, activation='relu'))\nmodel_kernel_1.add(Dense(num_classes, activation='softmax'))\n    \n# Model_kernel_2: 4x4\nmodel_kernel_2 = Sequential()\nmodel_kernel_2.add(Conv2D(filters=16, kernel_size=(4,4), padding='same',\n                     activation='relu', \n                     input_shape=(img_rows, img_cols, 1)))\nmodel_kernel_2.add(MaxPool2D(padding='same'))\nmodel_kernel_2.add(Flatten())\nmodel_kernel_2.add(Dense(256, activation='relu'))\nmodel_kernel_2.add(Dense(num_classes, activation='softmax'))\n    \n# Model_kernel_3: 5x5\nmodel_kernel_3 = Sequential()\nmodel_kernel_3.add(Conv2D(filters=16, kernel_size=(5,5), padding='same',\n                     activation='relu', \n                     input_shape=(img_rows, img_cols, 1)))\nmodel_kernel_3.add(MaxPool2D(padding='same'))\nmodel_kernel_3.add(Flatten())\nmodel_kernel_3.add(Dense(256, activation='relu'))\nmodel_kernel_3.add(Dense(num_classes, activation='softmax'))\n\n# Model_kernel_4: 6x6\nmodel_kernel_4 = Sequential()\nmodel_kernel_4.add(Conv2D(filters=16, kernel_size=(6,6), padding='same',\n                     activation='relu', \n                     input_shape=(img_rows, img_cols, 1)))\nmodel_kernel_4.add(MaxPool2D(padding='same'))\nmodel_kernel_4.add(Flatten())\nmodel_kernel_4.add(Dense(256, activation='relu'))\nmodel_kernel_4.add(Dense(num_classes, activation='softmax')) ","f5d9d05f":"ts = time.time()\n\nn_reps = 10\nn_epochs = 20\n\n# Keep track of the history evolution for all repetitions of the CNNs\nhistory_kernel_1, history_kernel_val_1 = [0]*n_epochs, [0]*n_epochs\nhistory_kernel_2, history_kernel_val_2 = [0]*n_epochs, [0]*n_epochs\nhistory_kernel_3, history_kernel_val_3 = [0]*n_epochs, [0]*n_epochs\nhistory_kernel_4, history_kernel_val_4 = [0]*n_epochs, [0]*n_epochs\n\n\nfor rep in range(n_reps):\n\n    # Compile model_kernel_1\n    model_kernel_1.compile(loss=keras.losses.categorical_crossentropy,\n                optimizer='adam',\n                metrics=['accuracy'])\n    model_kernel_1_history_rep = model_kernel_1.fit(X_train, Y_train,\n            batch_size=batch_size,\n            epochs=n_epochs,\n            validation_split = 0.1, \n            verbose=0)\n    history_kernel_1 = tuple(map(operator.add, history_kernel_1, model_kernel_1_history_rep.history['accuracy']))\n    history_kernel_val_1 = tuple(map(operator.add, history_kernel_val_1, model_kernel_1_history_rep.history['val_accuracy']))\n\n    # Compile model_kernel_2\n    model_kernel_2.compile(loss=keras.losses.categorical_crossentropy,\n                optimizer='adam',\n                metrics=['accuracy'])\n    model_kernel_2_history_rep = model_kernel_2.fit(X_train, Y_train,\n            batch_size=batch_size,\n            epochs=n_epochs,\n            validation_split = 0.1, \n            verbose=0)\n    history_kernel_2 = tuple(map(operator.add, history_kernel_2, model_kernel_2_history_rep.history['accuracy']))\n    history_kernel_val_2 = tuple(map(operator.add, history_kernel_val_2, model_kernel_2_history_rep.history['val_accuracy']))\n    \n    # Compile model_kernel_3\n    model_kernel_3.compile(loss=keras.losses.categorical_crossentropy,\n                optimizer='adam',\n                metrics=['accuracy'])\n    model_kernel_3_history_rep = model_kernel_3.fit(X_train, Y_train,\n            batch_size=batch_size,\n            epochs=n_epochs,\n            validation_split = 0.1, \n            verbose=0)\n    history_kernel_3 = tuple(map(operator.add, history_kernel_3, model_kernel_3_history_rep.history['accuracy']))\n    history_kernel_val_3 = tuple(map(operator.add, history_kernel_val_3, model_kernel_3_history_rep.history['val_accuracy']))\n    \n    # Compile model_kernel_4\n    model_kernel_4.compile(loss=keras.losses.categorical_crossentropy,\n                optimizer='adam',\n                metrics=['accuracy'])\n    model_kernel_4_history_rep = model_kernel_4.fit(X_train, Y_train,\n            batch_size=batch_size,\n            epochs=n_epochs,\n            validation_split = 0.1, \n            verbose=0)\n    history_kernel_4 = tuple(map(operator.add, history_kernel_4, model_kernel_4_history_rep.history['accuracy']))\n    history_kernel_val_4 = tuple(map(operator.add, history_kernel_val_4, model_kernel_4_history_rep.history['val_accuracy']))    \n    \n# Average historic data for each CNN (train and valuation)\nhistory_kernel_1 = [x\/n_reps for x in list(history_kernel_1)] \nhistory_kernel_2 = [x\/n_reps for x in list(history_kernel_2)]\nhistory_kernel_3 = [x\/n_reps for x in list(history_kernel_3)]\nhistory_kernel_4 = [x\/n_reps for x in list(history_kernel_4)]\nhistory_kernel_val_1 = [x\/n_reps for x in list(history_kernel_val_1)]\nhistory_kernel_val_2 = [x\/n_reps for x in list(history_kernel_val_2)]\nhistory_kernel_val_3 = [x\/n_reps for x in list(history_kernel_val_3)]\nhistory_kernel_val_4 = [x\/n_reps for x in list(history_kernel_val_4)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","8b93e39a":"# Plot the results\nplt.plot(history_kernel_val_1)\nplt.plot(history_kernel_val_2)\nplt.plot(history_kernel_val_3)\nplt.plot(history_kernel_val_4)\nplt.title('Model accuracy for different convolution kernel sizes')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\n#plt.ylim(0.95,1)\nplt.xlim(0,n_epochs)\nplt.legend(['3x3', '4x4', '5x5', '6x6'], loc='upper left')\nplt.savefig('convolution_kernel_size.png')\nplt.show()","6a5d390f":"# Model_layers_1: 1 Conv2d layer, same as our initial model (model_1) \n\n# Model_layers_2: 2 Conv2D layers\nmodel_layers_2 = Sequential()\nmodel_layers_2.add(Conv2D(filters=16, kernel_size=(5,5), padding='same',\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_layers_2.add(MaxPool2D(padding='same'))\nmodel_layers_2.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu'))\nmodel_layers_2.add(MaxPool2D(padding='same'))\nmodel_layers_2.add(Flatten())\nmodel_layers_2.add(Dense(256, activation='relu'))\nmodel_layers_2.add(Dense(num_classes, activation='softmax'))\n\n# Model_layers_3: 3 Conv2D layers\nmodel_layers_3 = Sequential()\nmodel_layers_3.add(Conv2D(filters=16, kernel_size=(5,5), padding='same',\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_layers_3.add(MaxPool2D())\nmodel_layers_3.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu'))\nmodel_layers_3.add(MaxPool2D(padding='same'))\nmodel_layers_3.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\nmodel_layers_3.add(MaxPool2D(padding='same'))\nmodel_layers_3.add(Flatten())\nmodel_layers_3.add(Dense(256, activation='relu'))\nmodel_layers_3.add(Dense(num_classes, activation='softmax'))","9c4ae2e3":"n_reps = 5\nn_epochs = 20\n\n# Keep track of the history evolution for all repetitions of the CNNs\nhistory_layers_1, history_layers_val_1 = [0]*n_epochs, [0]*n_epochs\nhistory_layers_2, history_layers_val_2 = [0]*n_epochs, [0]*n_epochs\nhistory_layers_3, history_layers_val_3 = [0]*n_epochs, [0]*n_epochs\n\nts = time.time()\n\nfor rep in range(n_reps):\n\n    # Compite model_1\n    model_1.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\n\n    history_layers_1_rep = model_1.fit(X_train, Y_train,\n          batch_size=batch_size,\n          epochs=n_epochs,\n          validation_split = 0.1, \n          verbose=0)\n    \n    history_layers_1 = tuple(map(operator.add, history_layers_1, history_layers_1_rep.history['accuracy']))\n    history_layers_val_1 = tuple(map(operator.add, history_layers_val_1, history_layers_1_rep.history['val_accuracy']))\n    \n\n    # Compile model_2\n    model_layers_2.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_layers_2_rep = model_layers_2.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_layers_2 = tuple(map(operator.add, history_layers_2, history_layers_2_rep.history['accuracy']))\n    history_layers_val_2 = tuple(map(operator.add, history_layers_val_2, history_layers_2_rep.history['val_accuracy']))\n\n    \n    # Compile model_3\n    model_layers_3.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_layers_3_rep = model_layers_3.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_layers_3 = tuple(map(operator.add, history_layers_3, history_layers_3_rep.history['accuracy']))\n    history_layers_val_3 = tuple(map(operator.add, history_layers_val_3, history_layers_3_rep.history['val_accuracy']))\n    \n# Average historic data for each CNN (train and valuation)\nhistory_layers_1 = [x\/n_reps for x in list(history_layers_1)] \nhistory_layers_2 = [x\/n_reps for x in list(history_layers_2)]\nhistory_layers_3 = [x\/n_reps for x in list(history_layers_3)]\nhistory_layers_val_1 = [x\/n_reps for x in list(history_layers_val_1)]\nhistory_layers_val_2 = [x\/n_reps for x in list(history_layers_val_2)]\nhistory_layers_val_3 = [x\/n_reps for x in list(history_layers_val_3)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","59ad4453":"# Plot the results\nplt.plot(history_layers_val_1)\nplt.plot(history_layers_val_2)\nplt.plot(history_layers_val_3)\nplt.title('Model accuracy for different number of Conv layers')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.98,1)\nplt.xlim(0,20)\nplt.legend(['1 layer', '2 layers', '3 layers'], loc='upper left')\nplt.savefig('number_of_layers.png')\nplt.show()","6195ba0b":"# Model_size_1: 8-16\nmodel_size_1 = Sequential()\nmodel_size_1.add(Conv2D(filters=16, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_1.add(MaxPool2D())\nmodel_size_1.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\nmodel_size_1.add(MaxPool2D(padding='same'))\nmodel_size_1.add(Flatten())\nmodel_size_1.add(Dense(256, activation='relu'))\nmodel_size_1.add(Dense(num_classes, activation='softmax'))\n\n# Model_size_2: 16-32\nmodel_size_2 = Sequential()\nmodel_size_2.add(Conv2D(filters=16, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_2.add(MaxPool2D())\nmodel_size_2.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\nmodel_size_2.add(MaxPool2D(padding='same'))\nmodel_size_2.add(Flatten())\nmodel_size_2.add(Dense(256, activation='relu'))\nmodel_size_2.add(Dense(num_classes, activation='softmax'))\n\n# Model_size_3: 32-32\nmodel_size_3 = Sequential()\nmodel_size_3.add(Conv2D(filters=16, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_3.add(MaxPool2D())\nmodel_size_3.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\nmodel_size_3.add(MaxPool2D(padding='same'))\nmodel_size_3.add(Flatten())\nmodel_size_3.add(Dense(256, activation='relu'))\nmodel_size_3.add(Dense(num_classes, activation='softmax'))\n\n# Model_size_4: 24-48\nmodel_size_4 = Sequential()\nmodel_size_4.add(Conv2D(filters=24, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_4.add(MaxPool2D())\nmodel_size_4.add(Conv2D(filters=48, kernel_size=(5,5), activation='relu'))\nmodel_size_4.add(MaxPool2D(padding='same'))\nmodel_size_4.add(Flatten())\nmodel_size_4.add(Dense(256, activation='relu'))\nmodel_size_4.add(Dense(num_classes, activation='softmax'))\n\n# Model_size_5: 32-64\nmodel_size_5 = Sequential()\nmodel_size_5.add(Conv2D(filters=32, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_5.add(MaxPool2D())\nmodel_size_5.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu'))\nmodel_size_5.add(MaxPool2D(padding='same'))\nmodel_size_5.add(Flatten())\nmodel_size_5.add(Dense(256, activation='relu'))\nmodel_size_5.add(Dense(num_classes, activation='softmax'))\n\n# Model_size_6: 48-96\nmodel_size_6 = Sequential()\nmodel_size_6.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_size_6.add(MaxPool2D())\nmodel_size_6.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_size_6.add(MaxPool2D(padding='same'))\nmodel_size_6.add(Flatten())\nmodel_size_6.add(Dense(256, activation='relu'))\nmodel_size_6.add(Dense(num_classes, activation='softmax'))","5563393c":"ts = time.time()\n\nn_reps = 3\nn_epochs = 20\n\n# Keep track of the history evolution for all repetitions of the CNNs\nhistory_size_1, history_size_val_1 = [0]*n_epochs, [0]*n_epochs\nhistory_size_2, history_size_val_2 = [0]*n_epochs, [0]*n_epochs\nhistory_size_3, history_size_val_3 = [0]*n_epochs, [0]*n_epochs\nhistory_size_4, history_size_val_4 = [0]*n_epochs, [0]*n_epochs\nhistory_size_5, history_size_val_5 = [0]*n_epochs, [0]*n_epochs\nhistory_size_6, history_size_val_6 = [0]*n_epochs, [0]*n_epochs\n\n\nfor rep in range(n_reps):\n\n    # Compite model_1\n    model_size_1.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\n\n    history_size_1_rep = model_size_1.fit(X_train, Y_train,\n          batch_size=batch_size,\n          epochs=n_epochs,\n          validation_split = 0.1, \n          verbose=0)\n    \n    history_size_1 = tuple(map(operator.add, history_size_1, history_size_1_rep.history['accuracy']))\n    history_size_val_1 = tuple(map(operator.add, history_size_val_1, history_size_1_rep.history['val_accuracy']))\n    \n\n    # Compile model_2\n    model_size_2.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_size_2_rep = model_size_2.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_size_2 = tuple(map(operator.add, history_size_2, history_size_2_rep.history['accuracy']))\n    history_size_val_2 = tuple(map(operator.add, history_size_val_2, history_size_2_rep.history['val_accuracy']))\n\n    \n    # Compile model_3\n    model_size_3.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_size_3_rep = model_size_3.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_size_3 = tuple(map(operator.add, history_size_3, history_size_3_rep.history['accuracy']))\n    history_size_val_3 = tuple(map(operator.add, history_size_val_3, history_size_3_rep.history['val_accuracy']))\n    \n    # Compile model_4\n    model_size_4.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_size_4_rep = model_size_4.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_size_4 = tuple(map(operator.add, history_size_4, history_size_4_rep.history['accuracy']))\n    history_size_val_4 = tuple(map(operator.add, history_size_val_4, history_size_4_rep.history['val_accuracy']))\n    \n    # Compile model_5\n    model_size_5.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_size_5_rep = model_size_5.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_size_5 = tuple(map(operator.add, history_size_5, history_size_5_rep.history['accuracy']))\n    history_size_val_5 = tuple(map(operator.add, history_size_val_5, history_size_5_rep.history['val_accuracy']))\n    \n    # Compile model_6\n    model_size_6.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_size_6_rep = model_size_6.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_size_6 = tuple(map(operator.add, history_size_6, history_size_6_rep.history['accuracy']))\n    history_size_val_6 = tuple(map(operator.add, history_size_val_6, history_size_6_rep.history['val_accuracy']))\n    \n    \n# Average historic data for each CNN (train and valuation)\nhistory_size_1 = [x\/n_reps for x in list(history_size_1)] \nhistory_size_2 = [x\/n_reps for x in list(history_size_2)]\nhistory_size_3 = [x\/n_reps for x in list(history_size_3)]\nhistory_size_4 = [x\/n_reps for x in list(history_size_4)] \nhistory_size_5 = [x\/n_reps for x in list(history_size_5)]\nhistory_size_6 = [x\/n_reps for x in list(history_size_6)]\nhistory_size_val_1 = [x\/n_reps for x in list(history_size_val_1)]\nhistory_size_val_2 = [x\/n_reps for x in list(history_size_val_2)]\nhistory_size_val_3 = [x\/n_reps for x in list(history_size_val_3)]\nhistory_size_val_4 = [x\/n_reps for x in list(history_size_val_4)]\nhistory_size_val_5 = [x\/n_reps for x in list(history_size_val_5)]\nhistory_size_val_6 = [x\/n_reps for x in list(history_size_val_6)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","533368c6":"# Plot the results\nplt.plot(history_size_val_1)\nplt.plot(history_size_val_2)\nplt.plot(history_size_val_3)\nplt.plot(history_size_val_4)\nplt.plot(history_size_val_5)\nplt.plot(history_size_val_6)\nplt.title('Model accuracy for different Conv sizes')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.98,1)\nplt.xlim(0,n_epochs)\nplt.legend(['8-16', '16-32', '32-32', '24-48', '32-64', '48-96', '64,128'], loc='upper left')\nplt.savefig('convolution_size.png')\nplt.show()","e24b924a":"# predict results\nresults = model_size_6.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step6.csv\",index=False)","426e5093":"# Model_dropout_1: No dropout, same as model_size_6\n\n# Model_dropout_2: 20% dropout\nmodel_dropout_2 = Sequential()\nmodel_dropout_2.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dropout_2.add(MaxPool2D())\nmodel_dropout_2.add(Dropout(0.2))\nmodel_dropout_2.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dropout_2.add(MaxPool2D(padding='same'))\nmodel_dropout_2.add(Dropout(0.2))\nmodel_dropout_2.add(Flatten())\nmodel_dropout_2.add(Dense(256, activation='relu'))\nmodel_dropout_2.add(Dense(num_classes, activation='softmax'))\n\n# Model_dropout_3: 40% dropout\nmodel_dropout_3 = Sequential()\nmodel_dropout_3.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dropout_3.add(MaxPool2D())\nmodel_dropout_3.add(Dropout(0.4))\nmodel_dropout_3.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dropout_3.add(MaxPool2D(padding='same'))\nmodel_dropout_3.add(Dropout(0.4))\nmodel_dropout_3.add(Flatten())\nmodel_dropout_3.add(Dense(256, activation='relu'))\nmodel_dropout_3.add(Dense(num_classes, activation='softmax'))\n\n# Model_dropout_4: 60% dropout\nmodel_dropout_4 = Sequential()\nmodel_dropout_4.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dropout_4.add(MaxPool2D())\nmodel_dropout_4.add(Dropout(0.6))\nmodel_dropout_4.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dropout_4.add(MaxPool2D(padding='same'))\nmodel_dropout_4.add(Dropout(0.6))\nmodel_dropout_4.add(Flatten())\nmodel_dropout_4.add(Dense(256, activation='relu'))\nmodel_dropout_4.add(Dense(num_classes, activation='softmax'))\n\n# Model_dropout_5: 80% dropout\nmodel_dropout_5 = Sequential()\nmodel_dropout_5.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dropout_5.add(MaxPool2D())\nmodel_dropout_5.add(Dropout(0.8))\nmodel_dropout_5.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dropout_5.add(MaxPool2D(padding='same'))\nmodel_dropout_5.add(Dropout(0.8))\nmodel_dropout_5.add(Flatten())\nmodel_dropout_5.add(Dense(256, activation='relu'))\nmodel_dropout_5.add(Dense(num_classes, activation='softmax'))","7ca82cac":"ts = time.time()\n\nn_reps = 3\nn_epochs = 20\n\n# Keep track of the history evolution for all repetitions of the CNNs\nhistory_dropout_1, history_dropout_val_1 = [0]*n_epochs, [0]*n_epochs\nhistory_dropout_2, history_dropout_val_2 = [0]*n_epochs, [0]*n_epochs\nhistory_dropout_3, history_dropout_val_3 = [0]*n_epochs, [0]*n_epochs\nhistory_dropout_4, history_dropout_val_4 = [0]*n_epochs, [0]*n_epochs\nhistory_dropout_5, history_dropout_val_5 = [0]*n_epochs, [0]*n_epochs\n\n\nfor rep in range(n_reps):\n\n    # Model_1 was previously computed in Step 6\n\n    # Compile model_2\n    model_dropout_2.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dropout_2_rep = model_dropout_2.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dropout_2 = tuple(map(operator.add, history_dropout_2, history_dropout_2_rep.history['accuracy']))\n    history_dropout_val_2 = tuple(map(operator.add, history_dropout_val_2, history_dropout_2_rep.history['val_accuracy']))\n\n    \n    # Compile model_3\n    model_dropout_3.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dropout_3_rep = model_dropout_3.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dropout_3 = tuple(map(operator.add, history_dropout_3, history_dropout_3_rep.history['accuracy']))\n    history_dropout_val_3 = tuple(map(operator.add, history_dropout_val_3, history_dropout_3_rep.history['val_accuracy']))\n    \n    # Compile model_4\n    model_dropout_4.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dropout_4_rep = model_dropout_4.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dropout_4 = tuple(map(operator.add, history_dropout_4, history_dropout_4_rep.history['accuracy']))\n    history_dropout_val_4 = tuple(map(operator.add, history_dropout_val_4, history_dropout_4_rep.history['val_accuracy']))\n    \n    # Compile model_5\n    model_dropout_5.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dropout_5_rep = model_dropout_5.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dropout_5 = tuple(map(operator.add, history_dropout_5, history_dropout_5_rep.history['accuracy']))\n    history_dropout_val_5 = tuple(map(operator.add, history_dropout_val_5, history_dropout_5_rep.history['val_accuracy']))\n       \n    \n# Average historic data for each CNN (train and valuation)\nhistory_dropout_2 = [x\/n_reps for x in list(history_dropout_2)]\nhistory_dropout_3 = [x\/n_reps for x in list(history_dropout_3)]\nhistory_dropout_4 = [x\/n_reps for x in list(history_dropout_4)] \nhistory_dropout_5 = [x\/n_reps for x in list(history_dropout_5)]\nhistory_dropout_val_2 = [x\/n_reps for x in list(history_dropout_val_2)]\nhistory_dropout_val_3 = [x\/n_reps for x in list(history_dropout_val_3)]\nhistory_dropout_val_4 = [x\/n_reps for x in list(history_dropout_val_4)]\nhistory_dropout_val_5 = [x\/n_reps for x in list(history_dropout_val_5)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","e72f0725":"# Plot the results\nplt.plot(history_size_val_1)\nplt.plot(history_dropout_val_2)\nplt.plot(history_dropout_val_3)\nplt.plot(history_dropout_val_4)\nplt.plot(history_dropout_val_5)\nplt.title('Model accuracy for different dropouts')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.98,1)\nplt.xlim(0,n_epochs)\nplt.legend(['0% dropout', '20% dropout', '40% dropout', '60% dropout', '80% dropout'], loc='upper left')\nplt.savefig('dropout.png')\nplt.show()","c7c445f1":"# predict results\nresults = model_dropout_3.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step7.csv\",index=False)","753a2055":"# Model_dense_64: 20% dropout\nmodel_dense_1 = Sequential()\nmodel_dense_1.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dense_1.add(MaxPool2D())\nmodel_dense_1.add(Dropout(0.4))\nmodel_dense_1.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dense_1.add(MaxPool2D(padding='same'))\nmodel_dense_1.add(Dropout(0.4))\nmodel_dense_1.add(Flatten())\nmodel_dense_1.add(Dense(64, activation='relu'))\nmodel_dense_1.add(Dense(num_classes, activation='softmax'))\n\n# Model_dense_128: 128 nodes dense layer\nmodel_dense_2 = Sequential()\nmodel_dense_2.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dense_2.add(MaxPool2D())\nmodel_dense_2.add(Dropout(0.4))\nmodel_dense_2.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dense_2.add(MaxPool2D(padding='same'))\nmodel_dense_2.add(Dropout(0.4))\nmodel_dense_2.add(Flatten())\nmodel_dense_2.add(Dense(128, activation='relu'))\nmodel_dense_2.add(Dense(num_classes, activation='softmax'))\n\n# Model_dense_3: 256 nodes dense layer. Same as model from Step 7.\n\n# Model_dense_4: 512 nodes dense layer\nmodel_dense_4 = Sequential()\nmodel_dense_4.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dense_4.add(MaxPool2D())\nmodel_dense_4.add(Dropout(0.4))\nmodel_dense_4.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dense_4.add(MaxPool2D(padding='same'))\nmodel_dense_4.add(Dropout(0.4))\nmodel_dense_4.add(Flatten())\nmodel_dense_4.add(Dense(512, activation='relu'))\nmodel_dense_4.add(Dense(num_classes, activation='softmax'))\n\n# Model_dense_5: 1024 nodes dense layer\nmodel_dense_5 = Sequential()\nmodel_dense_5.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_dense_5.add(MaxPool2D())\nmodel_dense_5.add(Dropout(0.4))\nmodel_dense_5.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_dense_5.add(MaxPool2D(padding='same'))\nmodel_dense_5.add(Dropout(0.4))\nmodel_dense_5.add(Flatten())\nmodel_dense_5.add(Dense(1024, activation='relu'))\nmodel_dense_5.add(Dense(num_classes, activation='softmax'))","07455dd6":"ts = time.time()\n\nn_reps = 3\nn_epochs = 20\n\n# Keep track of the history evolution for all repetitions of the CNNs\nhistory_dense_1, history_dense_val_1 = [0]*n_epochs, [0]*n_epochs\nhistory_dense_2, history_dense_val_2 = [0]*n_epochs, [0]*n_epochs\nhistory_dense_4, history_dense_val_4 = [0]*n_epochs, [0]*n_epochs\nhistory_dense_5, history_dense_val_5 = [0]*n_epochs, [0]*n_epochs\n\n\nfor rep in range(n_reps):\n\n    # Compile model_dense_1\n    model_dense_1.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dense_1_rep = model_dense_1.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dense_1 = tuple(map(operator.add, history_dense_1, history_dense_1_rep.history['accuracy']))\n    history_dense_val_1 = tuple(map(operator.add, history_dense_val_1, history_dense_1_rep.history['val_accuracy']))\n\n    # Compile model_dense_2\n    model_dense_2.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dense_2_rep = model_dense_2.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dense_2 = tuple(map(operator.add, history_dense_2, history_dense_2_rep.history['accuracy']))\n    history_dense_val_2 = tuple(map(operator.add, history_dense_val_2, history_dense_2_rep.history['val_accuracy']))\n    \n    # Model with 256 dense nodes was compiled in Step 7.\n    \n    # Compile model_dense_4\n    model_dense_4.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dense_4_rep = model_dense_4.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dense_4 = tuple(map(operator.add, history_dense_4, history_dense_4_rep.history['accuracy']))\n    history_dense_val_4 = tuple(map(operator.add, history_dense_val_4, history_dense_4_rep.history['val_accuracy']))\n    \n    # Compile model_dense_5\n    model_dense_5.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    history_dense_5_rep = model_dense_5.fit(X_train, Y_train,\n              batch_size=batch_size,\n              epochs=n_epochs,\n              validation_split = 0.1, \n              verbose=0)\n    \n    history_dense_5 = tuple(map(operator.add, history_dense_5, history_dense_5_rep.history['accuracy']))\n    history_dense_val_5 = tuple(map(operator.add, history_dense_val_5, history_dense_5_rep.history['val_accuracy']))\n       \n    \n# Average historic data for each CNN (train and valuation)\nhistory_dense_1 = [x\/n_reps for x in list(history_dense_2)]\nhistory_dense_2 = [x\/n_reps for x in list(history_dense_2)]\nhistory_dense_4 = [x\/n_reps for x in list(history_dense_4)] \nhistory_dense_5 = [x\/n_reps for x in list(history_dense_5)]\nhistory_dense_val_1 = [x\/n_reps for x in list(history_dense_val_2)]\nhistory_dense_val_2 = [x\/n_reps for x in list(history_dense_val_2)]\nhistory_dense_val_4 = [x\/n_reps for x in list(history_dense_val_4)]\nhistory_dense_val_5 = [x\/n_reps for x in list(history_dense_val_5)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","87b19df0":"# Plot the results\nplt.plot(history_dense_val_1)\nplt.plot(history_dense_val_2)\nplt.plot(history_dropout_val_3)\nplt.plot(history_dense_val_4)\nplt.plot(history_dense_val_5)\nplt.title('Model accuracy for different number of dense nodes')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.99,1)\nplt.xlim(0,n_epochs)\nplt.legend(['64 dense nodes', '128 dense nodes', '256 dense nodes', '512 dense nodes', '1024 dense nodes'], loc='upper left')\nplt.savefig('dense_nodes.png')\nplt.show()","fe364bed":"# predict results\nresults = model_dropout_3.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step8.csv\",index=False)","8a75c96f":"X_train_validation, X_val_validation, Y_train_validation, Y_val_validation = train_test_split(X_train, Y_train, test_size = 0.2)\n\n# Generate augmented additional data\ndata_generator_with_aug = ImageDataGenerator(width_shift_range = 0.1,\n                                   height_shift_range = 0.1,\n                                   rotation_range = 10,\n                                   zoom_range = 0.1)\ndata_generator_no_aug = ImageDataGenerator()\n\ntrain_generator = data_generator_with_aug.flow(X_train_validation, Y_train_validation, batch_size=64)\nvalidation_generator = data_generator_no_aug.flow(X_train_validation, Y_train_validation, batch_size=64)","b01c42ba":"# Model for augmented data (same as dropout_3)\nmodel_augmentation = Sequential()\nmodel_augmentation.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_augmentation.add(MaxPool2D())\nmodel_augmentation.add(Dropout(0.4))\nmodel_augmentation.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_augmentation.add(MaxPool2D(padding='same'))\nmodel_augmentation.add(Dropout(0.4))\nmodel_augmentation.add(Flatten())\nmodel_augmentation.add(Dense(256, activation='relu'))\nmodel_augmentation.add(Dense(num_classes, activation='softmax'))","cc908e17":"ts = time.time()\n\nn_reps = 10\nn_epochs = 20\n\n# Use the model with better score and include augmented data. Repeat n_reps times for averaging\nhistory_augmentation, history_augmentation_val = [0]*n_epochs, [0]*n_epochs\n\nfor rep in range(n_reps):\n    # Compile the model\n    model_augmentation.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    # Fit the model\n    history_augmentation_rep = model_augmentation.fit_generator(train_generator,\n                                                         epochs = n_epochs, \n                                                         steps_per_epoch = X_train_validation.shape[0]\/\/64,\n                                                         validation_data = validation_generator,  \n                                                         verbose=0)\n    history_augmentation = tuple(map(operator.add, history_augmentation, history_augmentation_rep.history['accuracy']))\n    history_augmentation_val = tuple(map(operator.add, history_augmentation_val, history_augmentation_rep.history['val_accuracy']))\n\nhistory_augmentation = [x\/n_reps for x in list(history_augmentation)]\nhistory_augmentation_val = [x\/n_reps for x in list(history_augmentation_val)]  \n    \nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","79eff7f6":"# Plot the results\nplt.plot(history_augmentation_val)\nplt.plot(history_dropout_val_3)\nplt.title('Model accuracy for data augmentation')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.99,1)\nplt.xlim(0,n_epochs)\nplt.legend(['with augmentation', 'without augmentation'], loc='upper left')\nplt.savefig('augmentation.png')\nplt.show()","d2482650":"# predict results\nresults = model_dropout_3.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step9.csv\",index=False)","3dad6b2b":"# Model_batch_norm: Add a batch normalization procedure after each convolution and dense layer\nmodel_batch_norm = Sequential()\nmodel_batch_norm.add(Conv2D(filters=48, kernel_size=(5,5),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_batch_norm.add(BatchNormalization())\nmodel_batch_norm.add(MaxPool2D())\nmodel_batch_norm.add(Dropout(0.4))\nmodel_batch_norm.add(Conv2D(filters=96, kernel_size=(5,5), activation='relu'))\nmodel_batch_norm.add(BatchNormalization())\nmodel_batch_norm.add(MaxPool2D(padding='same'))\nmodel_batch_norm.add(Dropout(0.4))\nmodel_batch_norm.add(Flatten())\nmodel_batch_norm.add(Dense(256, activation='relu'))\nmodel_batch_norm.add(BatchNormalization())\nmodel_batch_norm.add(Dense(num_classes, activation='softmax'))","93939481":"ts = time.time()\n\nn_reps = 10\nn_epochs = 20\n\n# Use the model with better score and include augmented data. Repeat n_reps times for averaging\nhistory_batch_norm, history_batch_norm_val = [0]*n_epochs, [0]*n_epochs\n\nfor rep in range(n_reps):\n    # Compile the model\n    model_batch_norm.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    # Fit the model\n    history_batch_norm_rep = model_batch_norm.fit_generator(train_generator,\n                                                         epochs = n_epochs, \n                                                         steps_per_epoch = X_train_validation.shape[0]\/\/64,\n                                                         validation_data = validation_generator,  \n                                                         verbose=0)\n    history_batch_norm = tuple(map(operator.add, history_batch_norm, history_batch_norm_rep.history['accuracy']))\n    history_batch_norm_val = tuple(map(operator.add, history_batch_norm_val, history_batch_norm_rep.history['val_accuracy']))\n    \nhistory_batch_norm = [x\/n_reps for x in list(history_batch_norm)]\nhistory_batch_norm_val = [x\/n_reps for x in list(history_batch_norm_val)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","efe56722":"# Plot the results\nplt.plot(history_batch_norm_val)\nplt.plot(history_augmentation_val)\nplt.title('Model accuracy for batch normalization')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.99,1)\nplt.xlim(0,n_epochs)\nplt.legend(['with batch normalization', 'without batch normalization'], loc='upper left')\nplt.savefig('batch_normalization.png')\nplt.show()","5f8f4b16":"# predict results\nresults = model_batch_norm.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step10.csv\",index=False)","a500f2b2":"# Model_batch_norm: Add a batch normalization procedure after each convolution and dense layer\nmodel_smaller_kernels = Sequential()\nmodel_smaller_kernels.add(Conv2D(filters=48, kernel_size=(3,3),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_smaller_kernels.add(BatchNormalization())\nmodel_smaller_kernels.add(Conv2D(filters=46, kernel_size=(3,3), activation='relu'))\nmodel_smaller_kernels.add(BatchNormalization())\nmodel_smaller_kernels.add(MaxPool2D())\nmodel_smaller_kernels.add(Dropout(0.4))\nmodel_smaller_kernels.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu'))\nmodel_smaller_kernels.add(BatchNormalization())\nmodel_smaller_kernels.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu'))\nmodel_smaller_kernels.add(BatchNormalization())\nmodel_smaller_kernels.add(MaxPool2D(padding='same'))\nmodel_smaller_kernels.add(Dropout(0.4))\nmodel_smaller_kernels.add(Flatten())\nmodel_smaller_kernels.add(Dense(256, activation='relu'))\nmodel_smaller_kernels.add(BatchNormalization())\nmodel_smaller_kernels.add(Dense(num_classes, activation='softmax'))","1743a003":"ts = time.time()\n\nn_reps = 10\nn_epochs = 20\n\n# Use the model with better score and include augmented data. Repeat n_reps times for averaging\nhistory_smaller_kernels, history_smaller_kernels_val = [0]*n_epochs, [0]*n_epochs\n\nfor rep in range(n_reps):\n    # Compile the model\n    model_smaller_kernels.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    # Fit the model\n    history_smaller_kernels_rep = model_smaller_kernels.fit_generator(train_generator,\n                                                         epochs = n_epochs, \n                                                         steps_per_epoch = X_train_validation.shape[0]\/\/64,\n                                                         validation_data = validation_generator,  \n                                                         verbose=0)\n    history_smaller_kernels = tuple(map(operator.add, history_smaller_kernels, history_smaller_kernels_rep.history['accuracy']))\n    history_smaller_kernels_val = tuple(map(operator.add, history_smaller_kernels_val, history_smaller_kernels_rep.history['val_accuracy']))\n    \nhistory_smaller_kernels = [x\/n_reps for x in list(history_smaller_kernels)]\nhistory_smaller_kernels_val = [x\/n_reps for x in list(history_smaller_kernels_val)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","d96bbeae":"# Plot the results\nplt.plot(history_smaller_kernels_val)\nplt.plot(history_batch_norm_val)\nplt.title('Model accuracy replacing Conv2D(5x5) by Conv2D(3x3)+Conv2D(3x3)')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.99,1)\nplt.xlim(0,n_epochs)\nplt.legend(['3x3+3x3', '5x5'], loc='upper left')\nplt.savefig('replace_big_convs.png')\nplt.show()","68268a2d":"# predict results\nresults = model_smaller_kernels.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step11.csv\",index=False)","9eb16db6":"# Model_batch_norm: Add a batch normalization procedure after each convolution and dense layer\nmodel_pool_conv = Sequential()\nmodel_pool_conv.add(Conv2D(filters=48, kernel_size=(3,3),\n                 activation='relu', \n                 input_shape=(img_rows, img_cols, 1)))\nmodel_pool_conv.add(BatchNormalization())\nmodel_pool_conv.add(Conv2D(filters=46, kernel_size=(3,3), activation='relu'))\nmodel_pool_conv.add(BatchNormalization())\nmodel_pool_conv.add(Conv2D(filters=46, kernel_size=(5,5), activation='relu', strides=2, padding='same'))\nmodel_pool_conv.add(Dropout(0.4))\nmodel_pool_conv.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu'))\nmodel_pool_conv.add(BatchNormalization())\nmodel_pool_conv.add(Conv2D(filters=96, kernel_size=(3, 3), activation='relu'))\nmodel_pool_conv.add(BatchNormalization())\nmodel_pool_conv.add(Conv2D(filters=46, kernel_size=(5,5), activation='relu', strides=2, padding='same'))\nmodel_pool_conv.add(Dropout(0.4))\nmodel_pool_conv.add(Flatten())\nmodel_pool_conv.add(Dense(256, activation='relu'))\nmodel_pool_conv.add(BatchNormalization())\nmodel_pool_conv.add(Dense(num_classes, activation='softmax'))","8fd8712d":"ts = time.time()\n\nn_reps = 10\nn_epochs = 20\n\n# Use the model with better score and include augmented data. Repeat n_reps times for averaging\nhistory_pool_conv, history_pool_conv_val = [0]*n_epochs, [0]*n_epochs\n\nfor rep in range(n_reps):\n    # Compile the model\n    model_pool_conv.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    # Fit the model\n    history_pool_conv_rep = model_pool_conv.fit_generator(train_generator,\n                                                         epochs = n_epochs, \n                                                         steps_per_epoch = X_train_validation.shape[0]\/\/64,\n                                                         validation_data = validation_generator,  \n                                                         verbose=0)\n    history_pool_conv = tuple(map(operator.add, history_pool_conv, history_pool_conv_rep.history['accuracy']))\n    history_pool_conv_val = tuple(map(operator.add, history_pool_conv_val, history_pool_conv_rep.history['val_accuracy']))\n    \nhistory_pool_conv = [x\/n_reps for x in list(history_pool_conv)]\nhistory_pool_conv_val = [x\/n_reps for x in list(history_pool_conv_val)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","98ddd300":"# Plot the results\nplt.plot(history_pool_conv_val)\nplt.plot(history_smaller_kernels_val)\nplt.title('Model accuracy replacing MaxPool by Conv2D with strides')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.ylim(0.99,1)\nplt.xlim(0,n_epochs)\nplt.legend(['Conv2D with strides', 'MaxPool'], loc='upper left')\nplt.savefig('replace_maxpool_by_conv.png')\nplt.show()","cff71793":"# predict results\nresults = model_pool_conv.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step12.csv\",index=False)","6e7647ee":"ts = time.time()\n\nn_reps = 1\nn_epochs = 40\n\n# Use the model with better score and include augmented data. Repeat n_reps times for averaging\nhistory_definitive, history_definitive_val = [0]*n_epochs, [0]*n_epochs\n\n# Callback function (early stopping)\ncallback_fcn = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x+n_epochs))\n\nfor rep in range(n_reps):\n    # Compile the model\n    model_pool_conv.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    # Fit the model\n    history_definitive_rep = model_pool_conv.fit_generator(train_generator,\n                                                         epochs = n_epochs, \n                                                         steps_per_epoch = X_train_validation.shape[0]\/\/64,\n                                                         validation_data = validation_generator,  \n                                                         callbacks=[callback_fcn],\n                                                         verbose=0)\n    history_definitive = tuple(map(operator.add, history_definitive, history_definitive_rep.history['accuracy']))\n    history_definitive_val = tuple(map(operator.add, history_definitive_val, history_definitive_rep.history['val_accuracy']))\n    \nhistory_definitive = [x\/n_reps for x in list(history_definitive)]\nhistory_definitive_val = [x\/n_reps for x in list(history_definitive_val)]\n\nprint (\"Time spent, \" + str(time.time() - ts) + \" s\")","8ef96d13":"# predict results\nresults = model_pool_conv.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"submit_step13.csv\",index=False)","7859eac4":"With this, we have covered all the basic modifications for the CNN architecture, accomplishing a reasonably high score. However, there are techniques (a bit more advanced than just tunning our CNN parameters) that may improve the prediction score of the model. The following steps are designed to be the key difference to obtain more competitive results.","9cd70327":"**Conclusion**: the obtained results are somewhat confusing. In one hand, batch normalization seems to increase the instability of the results. This should be studied more exhaustively with other datasets and I let this as a *to do* task for myself, but please feel free to add some comments about this in the kernel discussion. On the other hand, the accuracy curve seems to reach a plateaux for large epochs when batch normalization is not applied, but there's an increasing tendency in the other case. My final decision has been to keep batch normalization, since we will increase the model's complexity in the following steps and this method has demonstrated to enhance CNNs efficiency in numerous studies.","69de1106":"Compile 10 times and get statistics:","c40c8b87":"## Experiment 6. Data augmentation <a id=\"section9\"><\/a>\n\nThis technique increases the total number of images to train the model. The general idea is to generate slightly modified versions of the original images by rotating, zooming or shifting them. This modified images help the model to generalize patterns, so that it performs better in the test dataset.\n\nSee the nice Data Augmentation topic from the Deep Learning course for more details: https:\/\/www.kaggle.com\/dansbecker\/data-augmentation.\n\nFirst, we generate the augmented images:","fd7fb2b4":"**Conclusion**: performance is clearly higher when Conv2D(5x5) layers are replaced by two Conv2D(3x3) layers. The non-linearities detected in the second case seem to be key in order to generalize digit recognition in difficult cases.","df0d99b9":"## Experiment 7. Batch normalization <a id=\"section10\"><\/a>\n\nBatch normalization is a technique to improve the performance, speed and stability of neural networks. It essentially normalises the inputs of a layer by scaling the activations. See https:\/\/arxiv.org\/pdf\/1502.03167v3.pdf for in depth details.\n\nBatch normalization model:\n* **Model_batch_norm**. Conv2D (48,5x5,relu) + BatchNorm + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + BatchNorm + MaxPool + Dropout(0.4) + Dense64 + BatchNorm + output\n\nLet's define the model adding batch normalization after each convolution or dense layer (except the input\/output layers):","712ca66d":"Plot the model's performance:","61338395":"Submit best results:","b9cd7f49":"**Conclusion**: A combination of 48 and 96 nodes give the best performance for our CNN model. \n\nBest score at this stage: 0.9938.","e7feba1b":"Compile 10 times and get statistics:","e2ec58cc":"**Conclusion**: The best performance is accomplished with two convolutional layers. Hence, from now on we will will work with this architecture. \n\nBest score at this stage: 0.9917.","0921932e":"Compile 3 times and get statistics:","8a0f4e82":"Plot the model's performance:","fef1e317":"Done, we are ready to upload the \"submit_step3.csv\" and obtain our first official score. \n\nBest score at this stage: 0.9883.","14b5fed2":"## Experiment 2. Number of convolution layers <a id=\"section5\"><\/a>\n\nNow that we know the optimal kernel size, let's study how the number of convolution layers affect the model's performance, to see if a larger number increases the accuracy:\n* **Model_layers_1**. Conv2D (32,5x5,relu) + MaxPool + Dense256 + output\n* **Model_layers_2**. Conv2D (16,5x5,relu) + MaxPool + Conv2D(32,5x5,relu) + MaxPool + Dense256 + output\n* **Model_layers_3**. Conv2D (16,5x5,relu) + MaxPool + Conv2D(32,5x5,relu) + MaxPool + Conv2D(64,5x5,relu) + MaxPool + Dense256 + output\n\nDefine the models:","1569dc69":"Plot the model's performance:","43986d65":"Compile 3 times and get statistics:","d635c740":"**Conclusion**: this case in similar to batch normalization. The general behavior looks better when MaxPooling is not replaced by convolutional layers, but the long term tendency is better for convolutions with strides. Given that our objective is to increase the final CNN accuracy, I decided to keep this replacement (which I verified to be better through submissions).","efa16866":"**Conclusion**: Both a 40% and a 60% dropout present the higher accuracies. I choose to use a final 40% dropout.\n\nBest score at this stage: 0.9939","a6bbf8b9":"## Experiment 4. Dropout percentage <a id=\"section7\"><\/a>\n\nNeural networks may focus on certain paths between layers, hence being more prone to overfitting. One way to deal with this is to switch off some nodes randomly, so that particular paths are not prefered. In this section, we will include droput layers after each Conv2D and analyze the effect of the droput percentage.\n\nDroput poercentages:\n* **Model_dropout_1**. Conv2D (48,5x5,relu) + MaxPool + Conv2D (96,5x5,relu) + MaxPool + Dense256 + output\n* **Model_dropout_2**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.2) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.2) + Dense256 + output\n* **Model_dropout_3**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense256 + output\n* **Model_dropout_4**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.6) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.6) + Dense256 + output\n* **Model_dropout_5**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.8) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.8) + Dense256 + output\n\nDefine models:","6d7bd51d":"Plot the model's performance:","2d5290a4":"## Load the data <a id=\"section1\"><\/a>\n\nSince digit images are grey, we will deal with a single channel (on the contrary, a coloured image has 3 channels, one for each RGB). Let's look at the data structure:","7a08b916":"Compile 3 times and get statistics:","be3cb349":"It's always a good practice to analyze classes distribution (10 digits, from 0 to 9) in order to ensure if all of them are equally distributed:","cb7797e3":"Plot the model's performance:","441c0d12":"# Kaggle competitions: Digit recognizer\n\nThe aim of this notebook is to develop a convolutional neural network (CNN) to recognize handwritten digits. Since CNN's architecture is critical for the model's performance, we will analyze different variations in order to discover which setups better fit to our dataset.\n\nIn order to find the best architecture of the CNN, we will perform different \"experiments\" to determine which combinations of layers and parameters give better results. This procedure is extensive and requires some hours to finish, but it's completely independent on external results (i.e. other kernels or studies) and it is based on the trial and error process to be followed in any real project in which there are no previous references.\n\n\n**TABLE OF CONTENTS**:\n\n1. [Load the data](#section1)\n2. [Data preparation](#section2)\n3. [Initial CNN model](#section3)\n4. [Experiment 1. Size of the convolution kernels](#section4)\n5. [Experiment 2. Number of convolution layers](#section5)\n6. [Experiment 3. Number of convolution nodes](#section6)\n7. [Experiment 4. Dropout percentage](#section7)\n8. [Experiment 5. Dense layer size](#section8)\n9. [Experiment 6. Data augmentation](#section9)\n10. [Experiment 7. Batch normalization](#section10)\n11. [Experiment 8. Replacement of large kernel layers by two smaller ones](#section11)\n12. [Experiment 9. Replacement of max pooling by convolutions with strides](#section12)\n13. [Final model and submission](#section13)\n\n**Disclaimer**: This kernel has been strongly inspired by https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist, which is a very rich and extensive review of an ensemble of 15 CNNs for the digit recognizer competition. I highly recommend to take a look on it for a state of the art review of this competition.","c6aa69f1":"Nice, we got an impressing 98.83% accuracy on the validation set with our simple model. Let's see the its evolution with each epoch:","074ed4d5":"## Experiment 9. Replacement of max pooling by convolutions with strides <a id=\"section11\"><\/a>\n\nThere are other notable network architecture innovations which have yielded competitive results in image classification. One of these is to  replace max-pooling with a convolutional layer with increased stride, which yields competitive or state-of-the-art performance on several image recognition datasets. It has been stated that Conv2D(strides=2) layers does not just substitute MaxPooling functionality, but they also add the capability to learn from data.\n\nSee https:\/\/doi.org\/10.1016\/j.neucom.2018.07.079 for an extensive study of this method.\n\nReplace pooling by convolutions model:\n* **Model_pool_conv**. Conv2D (48,3x3,relu) + BatchNorm + Conv2D (48,3x3,relu) + BatchNorm + Conv2D (48,3x3,relu,2strides) + Dropout(0.4) + Conv2D (96,5x5,relu) + BatchNorm + Conv2D (96,5x5,relu) + BatchNorm + Conv2D (48,3x3,relu,2strides) + Dropout(0.4) + Dense64 + BatchNorm + output\n\nDefine the model:","c2e9a468":"## Experiment 5. Dense layer size <a id=\"section8\"><\/a>\n\nAt this point we have modified all but the size of dense layers. Hence, let's complete the analysis by studying different nodes on them.\n\nDense layers models:\n* **Model_dense_1**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense64 + output\n* **Model_dense_2**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense128 + output\n* **Model_dense_3**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense256 + output\n* **Model_dense_4**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense512 + output\n* **Model_dense_5**. Conv2D (48,5x5,relu) + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + MaxPool + Dropout(0.4) + Dense1024 + output\n\nDefine the models,","1af2d4ed":"Plot the model's performance:","b0ac09e0":"## Initial CNN model <a id=\"section3\"><\/a>\n\nEverythin is ready to create our CNN model, and hence obtain our first results. This step serves as a starting point to verify that the previous procedures have been succesfull, and that we can get a reasonably good accuracy. \n\nHence, this initial architecture is very simple and consists on:\n1. Input layer\n2. Convolutional layer with 32 filters, 4x4 size and relu activation function\n3. MaxPool layer\n4. Dense layer (256)\n5. Output layer\n\nNote: CNN uses max pooling to replace output with a max summary to reduce data size and processing time.","5a2d60f8":"And finally submit the results,","f77a50c3":"Compile 3 times and get statistics:","a8be0b44":"**Conclusion**: Both kernels of 5x5 and 6x6 accomplish the best accuracy. Hence, from now on, our convolution layers will be 5x5 to optimally capture image patterns while still being computatnionally efficient.\n\nBest score at this stage: 0.9902.","e948f434":"Trainig seems to have reached an accuracy plateaux. Hence, we just need to predict the test dataset, get the maximum probability results and submit the file.","0c907d9e":"## Experiment 8. Replacement of large kernel layers by two smaller ones <a id=\"section10\"><\/a>\n\nThere is evidence pointing out that convolution layers with large kernel sizes can be replaced by two (or more) convolution layers with smaller kernel size. This seems to speed up and improve the performance of CNN for computer vision, since two convolutions are able to better detect  non-linearities in data. See https:\/\/arxiv.org\/pdf\/1512.00567v1.pdf for an in depth study of this technique.\n\nReplace large kernels model:\n* **Model_batch_norm**. Conv2D (48,3x3,relu) + BatchNorm + Conv2D (48,3x3,relu) + BatchNorm + MaxPool + Dropout(0.4) + Conv2D (96,5x5,relu) + BatchNorm + Conv2D (96,5x5,relu) + BatchNorm + MaxPool + Dropout(0.4) + Dense64 + BatchNorm + output\n\nDefine the model:","5b754389":"Compile the model and fit to training data:","2d2f2e9d":"## Experiment 1. Size of the convolution kernels <a id=\"section4\"><\/a>\n\nOur first attempt to predict images with digits has proven to be reasonably successful. However, there's always room for improvement, and the following steps will focus on discovering how the network's architecture impacts the model's accuracy.\n\nWe will start by measuring the model accuracy for different kernel sizes:\n* **Model_kernel_1**. Conv2D (16,3x3,relu) + MaxPool + Dense256 + output\n* **Model_kernel_2**. Conv2D (16,4x4,relu) + MaxPool + Dense256 + output\n* **Model_kernel_3**. Conv2D (16,5x5,relu) + MaxPool + Dense256 + output\n* **Model_kernel_4**. Conv2D (16,6x6,relu) + MaxPool + Dense256 + output\n\nCreate the models:","8cae52af":"Plot the model's performance:","d33418a6":"This model reached a score of 0.99557, which is close to the top 10% models. Other kernels argued that, given the most difficult digits (some of them almost impossible to recognise by humans), the best results a single CNN could reach is around 99.8. Again, I recommend to see https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist for a reference. Hence, we can consider our results as *quite good*, and without doubt, it has been an enjoyable learning experience for me as my first image detection model.\n\nFeel free to add any comments, suggestions or questions in the kernel's discussion, all types of feedback are always very welcome. Hope you enjoyed this work and see you around!","4d1509ab":"**Conclusion**: as expected, data augmentation has helped the CNN to generalize patterns and recognise more digits. The performance has significantly improved.\n","87fd3a11":"Then compile the models several times (3 repetitions), so that we can gather some statistics and average the results:","c593861c":"Finally, plot the accuracy results:","9f5c7829":"Looks like all classes are pretty well balanced, so that stratified train\/test split won't be necessary. \n\nFinally, let's check if there are missing values (for example, due to corrupted pixels):","21647ed2":"## Data preparation <a id=\"section2\"><\/a>\n\nData is now loaded, and we verified that all classes are more ore less evenly distributed and there are no missing values. Hence, we are ready to prepare our data.\n\nWhat we will do:\n* Define the data size: 28x28 pixels\n* Extract target column\n* Normalize values. From 0 to 1, instead of the common 0-255 pixel values\n* Reshape datasets. Take  into account that there is a single channel\n* One hot encode the target column","25d270c8":"Compile 10 times and get statistics:","64f7d0fe":"Plot the model's performance:","1aae5619":"## Final model and submission <a id=\"section12\"><\/a>\n\nIt has been a long journey through the intricacies of a CNN model, from simply modifying a few layer parameters to using advanced techniques as data augmentation. With this, we have been able to analyze which is the better architecture for the model, and how this affects the performance. \n\nThe definitive CNN architecture is:\n* **Definitive model**: Conv2D (48,3x3,relu) + BatchNorm + Conv2D (48,3x3,relu) + BatchNorm + Conv2D (48,3x3,relu,2strides) + Dropout(0.4) + Conv2D (96,5x5,relu) + BatchNorm + Conv2D (96,5x5,relu) + BatchNorm + Conv2D (48,3x3,relu,2strides) + Dropout(0.4) + Dense64 + BatchNorm + output\n\nSince this model is exactly the same we used in the previous step, we don't need to define it again. Let's train it for a large enough number of epochs:","14627ce1":"Compile 10 times and get statistics:","24459127":"**Conclusion**: All models present very similar results. Hence, we will use a 128 nodes dense layer (high performance yet not the most computational consuming option).\n\nBest score at this stage: 0.9930 (which is lower than before, but this is due to instabilities)","ddf2eacb":"## Experiment 3. Number of convolution nodes <a id=\"section6\"><\/a>\n\nIt turns out that 2 convolutional layers is the magic number for our dataset, however we haven't modified the number of filters for each layer (beyond  increasing the number of nodes for each succesive layer). Let's analyze this.\n\nIn this section we will review 7 models:\n* **Model_size_1**. Conv2D (8,5x5,relu) + MaxPool + Conv2D (16,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_2**. Conv2D (16,5x5,relu) + MaxPool + Conv2D (32,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_3**. Conv2D (32,5x5,relu) + MaxPool + Conv2D (32,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_4**. Conv2D (24,5x5,relu) + MaxPool + Conv2D (48,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_5**. Conv2D (32,5x5,relu) + MaxPool + Conv2D (64,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_6**. Conv2D (48,5x5,relu) + MaxPool + Conv2D (96,5x5,relu) + MaxPool + Dense256 + output\n* **Model_size_7**. Conv2D (64,5x5,relu) + MaxPool + Conv2D (128,5x5,relu) + MaxPool + Dense256 + output\n\nDefine the models:"}}