{"cell_type":{"087b3f44":"code","a56d0515":"code","b8f57f3c":"code","8b5e66ec":"code","ed37dbaa":"code","7a36f567":"code","e6fb2699":"code","97b6a1e7":"code","5d2ba659":"code","3739e388":"code","6d6a4666":"code","90986f74":"code","61251965":"code","7fbbb7c8":"code","f27e4d51":"code","cbecc259":"code","edeb042f":"code","4ada5096":"code","ac7fcb41":"code","113790a1":"code","0be7ce27":"code","6a0b37f9":"code","fbabf999":"code","a7481c49":"code","3dd84c6d":"code","c0ddb2ba":"code","a3746b22":"code","9f526e21":"code","45983922":"code","b29d3dcc":"code","28db17b7":"code","66d0645f":"code","fa9aeebf":"code","7cebed25":"code","e43f0170":"code","0a577197":"code","96afc981":"code","105ed164":"code","54fea9d5":"code","430edd6c":"code","3ed6640b":"code","d48db58f":"code","2bcff521":"code","5863efe5":"code","bfc5d325":"code","49fffd01":"code","6500a758":"code","510b67a8":"code","10e1a346":"code","64bd51a1":"code","c84440cf":"code","fe6fc8b9":"code","ed323c35":"code","fb50779c":"code","bf35bba8":"code","d015bd37":"code","e969d34b":"code","2e2d603c":"code","affd3598":"code","a071ebca":"code","b0388911":"code","fb515f89":"code","fd6aa5a0":"code","8f5df6da":"code","afe449ad":"code","1ec764f1":"code","acc26521":"code","0d00a9cb":"code","344339c2":"code","a93f6625":"code","64452885":"code","001cf7d8":"code","e6f4ef5a":"code","81be676d":"code","7786fedc":"code","704ef8a8":"code","80308c07":"code","108ab179":"code","3fe587c0":"code","fc1cb18c":"code","3e463fde":"code","a99e911d":"code","c38301dc":"code","8341e934":"code","fd83c092":"code","7b5c6ec2":"code","add3f32c":"code","91879c18":"code","18e4088f":"code","a7a9a273":"code","d93d1957":"code","85ebae7e":"code","eae263f6":"code","83f6607e":"code","a5d467cb":"code","aca15b95":"code","6df79a0a":"code","516bb258":"code","e9b749f7":"code","f747a7ce":"code","a7c2a071":"code","ce19641a":"code","6bcc7e3c":"code","48e7be78":"code","f4fbe87d":"code","67cac19c":"code","2a319154":"code","8c7891dc":"code","f84faaae":"code","63c94eb2":"code","e27811d7":"code","e9fdfa95":"code","baef85b2":"code","e1fcd485":"code","be444f53":"code","3892a2d9":"code","66b096ab":"code","e51b1289":"code","eff67986":"code","e9d95fe4":"code","4e7e47c7":"code","7cff746b":"code","4c0e6f6c":"code","9e0c8c7f":"code","70881943":"code","f9e7c534":"code","ad682cb3":"code","a0a9e061":"code","d1ab306f":"code","d74984da":"code","5508711b":"code","571a385d":"code","1a6803a5":"code","41f985b8":"code","91506880":"code","8a510c8b":"code","c2612270":"code","3778865c":"code","aa05a563":"code","65a76f5b":"code","4450a628":"code","04e2be6c":"code","92721731":"code","38da5af3":"code","cfc8a4a8":"code","419dba0c":"code","f8a60bcc":"code","1d714b99":"code","a82026a8":"code","dc93da1f":"code","b307f0b1":"code","6eb6c571":"code","481c11b1":"code","5173ecc4":"code","b136cb86":"code","d7e11107":"code","e7275391":"code","6268e7ee":"code","2669c2d9":"code","316cdb60":"code","9a98875f":"code","05f40979":"code","0df3add5":"code","299c6b7b":"code","0c829b32":"code","2b368f28":"code","49904baf":"markdown","83edfc16":"markdown","549b1f88":"markdown","2112b79d":"markdown","d1559aba":"markdown","74ad90e0":"markdown","b4bc5d7d":"markdown","1b844012":"markdown","c85796d3":"markdown","72ccd782":"markdown","28fa4eea":"markdown","2d74b4f5":"markdown","ec5d3d92":"markdown","f3c7c30f":"markdown","72ef2c44":"markdown","57ce47c4":"markdown","b0db5bd6":"markdown","b04e35f4":"markdown","c6a88770":"markdown","8ab7437b":"markdown","01ca7b80":"markdown","2c040160":"markdown","fd9f05be":"markdown","0e7e95cd":"markdown","11a2078e":"markdown","0ce019f0":"markdown","176886a7":"markdown","6a5fea98":"markdown","ed27400e":"markdown","1f00a252":"markdown","f960b991":"markdown","a35d897e":"markdown","50799673":"markdown","e62a11cb":"markdown","4e264c9f":"markdown","736f1053":"markdown","ec46dbda":"markdown","94d24f6f":"markdown","bcad3283":"markdown","ce9d22a8":"markdown","814dbe80":"markdown","326e1d29":"markdown","9d4704e0":"markdown","901b07f6":"markdown","e3b37f2a":"markdown","43403166":"markdown","6064ddd7":"markdown","dc93ce5c":"markdown","3fa97354":"markdown","964e20c3":"markdown","01779647":"markdown","bc1f5b27":"markdown","03abe1d1":"markdown","607f74bc":"markdown","53513023":"markdown","26bbcbce":"markdown","22533074":"markdown","09c1e867":"markdown","df776f23":"markdown"},"source":{"087b3f44":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import classification_report","a56d0515":"df = pd.read_csv('\/kaggle\/input\/ramen-ratings\/ramen-ratings.csv')\ndf.head(3)","b8f57f3c":"### Identify dtypes that need to be fixed ###\ndf.dtypes","8b5e66ec":"### Cast review number as string ###\ndf['Review #'] = df['Review #'].astype('str')\n### Identify non-float numbers in Stars column ###\ndf.Stars.unique()","ed37dbaa":"### Locate the index of non-float values in Stars ###\ndf[df.Stars == 'Unrated']","7a36f567":"### Drop those indices ###\ndf = df.drop([32,122,993])","e6fb2699":"### Cast Stars as float ###\ndf['Stars'] = df['Stars'].astype('float')\ndf.dtypes","97b6a1e7":"### Identify low-count values in Style ###\ndf.Style.value_counts()","5d2ba659":"### Locate indices of low-count Style ###\ndf[df.Style == 'Box']","3739e388":"### Drop those indices ###\ndf.drop([36,80,139,161,203,1471], inplace=True)","6d6a4666":"### Locate more indices of low-count Styles ###\ndf[(df.Style == 'Can') | (df.Style == 'Bar')]","90986f74":"### Drop those indices ###\ndf.drop([67,1425], inplace=True)","61251965":"### Check work ###\ndf.Style.unique()","7fbbb7c8":"### Check null counts ###\ndf.isna().sum()","f27e4d51":"### Identify indices for null Style ###\ndf[df.Style.isna()]","cbecc259":"### Drop those indices ###\ndf.drop([2152, 2442], inplace=True)","edeb042f":"### Check work ###\ndf.isna().sum()","4ada5096":"### Drop high-nulls column Top Ten ###\ndf.drop(columns='Top Ten', inplace=True)\ndf.head(3)","ac7fcb41":"### Check value counts per Brand ###\ndf.Brand.value_counts().head(10)","113790a1":"### Check value counts per variety ###\ndf.Variety.value_counts() # 2400 unique values, too many to plot","0be7ce27":"### Plot distributions ###\n## Count by Stars ##\nplt.figure(figsize=(10,7))\ndf.Stars.value_counts().sort_index().plot(kind='barh')\nplt.show()\n## Count by Styles ##\nplt.figure(figsize=(10,7))\ndf.Style.value_counts(ascending=True).plot(kind='barh')\nplt.show()\n## Count by Country ##\nplt.figure(figsize=(10,7))\ndf.Country.value_counts(ascending=True).plot(kind='barh')\nplt.show()\n## Count by Brand ##\nplt.figure(figsize=(10,7))\nbrand_array = df.Brand.value_counts(ascending=False)[:30].sort_values(ascending=True)\nbrand_array.plot(kind='barh')\nplt.show()","6a0b37f9":"# df.Country.value_counts()\ndf = df.groupby('Country').filter(lambda x: len(x) > 10)","fbabf999":"### Split into subsets ###\ntrain_validate, test = train_test_split(df, test_size=0.2, random_state=123)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=123)\n### Isolate targets ###\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","a7481c49":"### Visualize Comparison ###\nsns.boxplot(data=train, x='Style', y='Stars')\nplt.title('Ramen Styles and their Ratings')\nplt.show()","3dd84c6d":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(train[train.Style == 'Bowl'].Stars, \n                          train[train.Style == 'Cup'].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Bowls rank statistically higher than cups. We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Bowls do not rank statistically higher than cups. Results do not breach the 95% confidence threshold.')","c0ddb2ba":"### Visualize comparison ##\nsns.boxplot(data=train[(train.Country == 'Japan') | (train.Country == 'USA')], \n           x='Country',\n           y='Stars')\nplt.title('USA and Japan for Ramen Ratings')\nplt.show()","a3746b22":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(train[train.Country == 'Japan'].Stars, \n                          train[train.Country == 'USA'].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Japan statistically produces better-rated ramen than USA.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Japan does not statistically produce better-rated ramen than USA.\\n\\\n            Results do not breach the 95% confidence threshold.')","9f526e21":"### Get Country Names (top 11 countries have over 100 ramen reviews) ###\ncountries = train.Country.value_counts()[:11].index.to_list()","45983922":"### Put top 11 countries to new dataframe ###\ntrain_top11 = train[train.Country == 'Japan']\nfor country in countries[1:]:\n    train_top11 = train_top11.append(train[train.Country == country])\ntrain_top11.Country.unique()","b29d3dcc":"### Visualize Comparisons of top 11 Countries ###\nplt.figure(figsize=(15,10))\nsns.boxplot(data=train_top11, x='Country', y='Stars')\nplt.title('Ramen Ratings by Country in Top-11 Most-Reviewed Countries')\nplt.show()","28db17b7":"### Create new column is_Japan ###\ntrain_top11['is_Japan'] = train_top11.Country == 'Japan'\ntrain_top11.sample(5)","66d0645f":"### Visualize Comparison between Japan and all of top 11 ###\nsns.boxplot(data=train_top11, x='is_Japan', y='Stars')\nplt.title('Japan versus Top 11 Most-Reviewed Countries on Ramen Ratings')\nplt.show()","fa9aeebf":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(train_top11[train_top11.is_Japan == True].Stars, \n                          train_top11[train_top11.is_Japan == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Japan statistically produces better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Japan does not statistically produce better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","7cebed25":"### Discover top 9 most-reviewed brands ###\ndf_9brands = brand_array.sort_values(ascending=False)[:9]\ndf_9brands","e43f0170":"### Push brands to list ###\nbrand_list = df_9brands.index.to_list()\n### Initialize dataframe ###\ntrain_9brands = train[train.Brand == 'Nissin']\n### Append remaining 8 brands' data to dataframe ###\nfor brand in brand_list[1:]:\n    train_9brands = train_9brands.append(train[train.Brand == brand])\n    \ntrain_9brands.Brand.unique()","0a577197":"### Plot all brands individually against their ratings ###\nplt.figure(figsize=(15,10))\nsns.boxplot(data=train_9brands, x='Brand', y='Stars')\nplt.title('Top 9 Most-Reviewed Brands by Rating')\nplt.show()","96afc981":"### New column for is_Nissin ###\ntrain_9brands['is_Nissin'] = train_9brands.Brand == 'Nissin'\ntrain_9brands.sample(3)","105ed164":"### Plot comparison ###\nsns.boxplot(data=train_9brands, x='is_Nissin', y='Stars')\nplt.title('Nissin Ratings versus Rest of Top 8 Most-Reviewed Brands')\nplt.show()","54fea9d5":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(train_9brands[train_9brands.is_Nissin == True].Stars, \n                          train_9brands[train_9brands.is_Nissin == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Nissin statistically produces better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Nissin does not statistically produce better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","430edd6c":"### Create new dataframe for countries and 5-star value counts ###\nfive_star_country_counts = pd.DataFrame(train[train.Stars == 5].Country.value_counts()).reset_index()\nfive_star_country_counts.T","3ed6640b":"### Create count of all reviews per country and merge to count of 5-stars ###\ncountry_counts = pd.DataFrame(train.Country.value_counts()).reset_index()\nfive_star_country_counts = pd.merge(five_star_country_counts, country_counts, \n                                    left_on='index',\n                                    right_on='index')\nfive_star_country_counts.head(3)","d48db58f":"### Calculate percentage of 5-star reviews to all reviews ###\nfive_star_country_counts['fiver_percent_of_total'] = five_star_country_counts.Country_x \/ five_star_country_counts.Country_y\nfive_star_country_counts","2bcff521":"### Sort by percentage ###\nfive_star_country_counts[:8].sort_values('fiver_percent_of_total', ascending=False)","5863efe5":"### Check number of 5-star reviews by brand ###\ntrain[train.Stars == 5].Brand.value_counts()","bfc5d325":"### Create dataframe of brands and their number of 5-star reviews ###\nfive_star_brand_counts = pd.DataFrame(train[train.Stars == 5].Brand.value_counts()).reset_index()\nfive_star_brand_counts.T","49fffd01":"### Affix dataframe of total review counts per brand to dataframe containing 5-star counts ###\nbrand_counts = pd.DataFrame(train.Brand.value_counts()).reset_index()\nfive_star_brand_counts = pd.merge(five_star_brand_counts, brand_counts, \n                                    left_on='index',\n                                    right_on='index')\nfive_star_brand_counts.head(3)","6500a758":"### Calculate proportion of 5-star reviews to all reviews ###\nfive_star_brand_counts['fiver_percent_of_total'] = five_star_brand_counts.Brand_x \/ five_star_brand_counts.Brand_y\nfive_star_brand_counts","510b67a8":"### Sort by percentage ###\nfive_star_brand_counts[:8].sort_values('fiver_percent_of_total', ascending=False)","10e1a346":"### Compare boxplots of MyKuali to next high-5-star-proprtion ramen brand ###\nsns.boxplot(data=train, x=train[(train.Brand == 'MyKuali') | ((train.Brand == 'Paldo'))].Brand, y=train.Stars)","64bd51a1":"### Sanity check on high-review MyKuali ###\ntrain[train.Brand == 'MyKuali']","c84440cf":"### Plot top-11 countries *without MyKuali* to see what happens to distribution ###\nplt.figure(figsize=(10,7))\nmykuali_indices = train_top11[train_top11.Brand == 'MyKuali'].index\nsns.boxplot(data=train_top11.drop(index=mykuali_indices), x='Country', y='Stars')","fe6fc8b9":"train.Stars.unique()","ed323c35":"train['Stars'] = (train.Stars * 4).round() \/ 4\nvalidate['Stars'] = (validate.Stars * 4).round() \/ 4\ntest['Stars'] = (test.Stars * 4).round() \/ 4","fb50779c":"# Checking work\ntrain.Stars.unique()","bf35bba8":"train[train.Stars.isna()]","d015bd37":"# Multiplying all values by 4 (0.25 -> 1, 0.5 -> 2, 0.75 -> 3, ...)\nprint(train.Stars[:5])\ntrain['Stars'] = train.Stars * 4\nvalidate['Stars'] = validate.Stars * 4\ntest['Stars'] = test.Stars * 4\nprint(train.Stars[:5])","e969d34b":"train.Stars.unique()","2e2d603c":"# Casting Stars column as int\ntrain['Stars'] = train['Stars'].astype('int')\ntrain['Stars'].dtype","affd3598":"# Discover most common rating (will use as baseline)\ntrain['Stars'].value_counts().head(1)","a071ebca":"# 4-star rating is most common, set to value 16\nmost_common = 16","b0388911":"# Calculate baseline accuracy\nprint('Baseline accuracy:', str(round((237 \/ len(train)) * 100, 2)) + '%')","fb515f89":"train_encoded = pd.get_dummies(train, columns=['Style','Country']).drop(columns=['Brand','Review #','Variety'])\nvalidate_encoded = pd.get_dummies(validate, columns=['Style','Country']).drop(columns=['Brand','Review #','Variety'])\ntest_encoded = pd.get_dummies(test, columns=['Style','Country']).drop(columns=['Brand','Review #','Variety'])","fd6aa5a0":"train_encoded.head(1)","8f5df6da":"X_train, y_train = train_encoded.drop(columns='Stars'), train_encoded.Stars\nX_validate, y_validate = validate_encoded.drop(columns='Stars'), validate_encoded.Stars\nX_test, y_test = test_encoded.drop(columns='Stars'), test_encoded.Stars\nX_test.head(1)","afe449ad":"rf = RandomForestClassifier(n_estimators=2, random_state=99)\nrf.fit(X_train, y_train)\nprint((rf.predict(X_train) == y_train).mean())\nprint((rf.predict(X_validate) == y_validate).mean())","1ec764f1":"tree = DecisionTreeClassifier(max_depth=16, min_samples_leaf=4, random_state=123)\ntree.fit(X_train, y_train)\nprint((tree.predict(X_train) == y_train).mean())\nprint((tree.predict(X_validate) == y_validate).mean())","acc26521":"knn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nprint((knn.predict(X_train) == y_train).mean())\nprint((knn.predict(X_validate) == y_validate).mean())","0d00a9cb":"logit = LogisticRegression(random_state=123)\nlogit.fit(X_train, y_train)\nprint((logit.predict(X_train) == y_train).mean())\nprint((logit.predict(X_validate) == y_validate).mean())","344339c2":"# Classification report\nprint(classification_report(y_train, tree.predict(X_train)))","a93f6625":"# print(classification_report(y_train, rf.predict(X_train))) # 2, 6, 8, 11, 12, 13, 14, 15, 16, 20 (10 in total)\n# print(classification_report(y_train, knn.predict(X_train))) # 7, 8, 11, 12, 13, 14, 15, 16, 17, 19, 20 (11 in total)\n# print(classification_report(y_train, logit.predict(X_train))) # 8, 12, 14, 15, 16, 20 (6 in total)","64452885":"# Brands with more than 10 reviews\ndf = df.groupby('Brand').filter(lambda x: len(x) > 10)","001cf7d8":"# Re-split data\n### Split into subsets ###\ntrain_validate, test = train_test_split(df, test_size=0.2, random_state=124, stratify=df.Brand)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=124, stratify=train_validate.Brand)\n### Isolate targets ###\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","e6f4ef5a":"train['Stars'] = (train.Stars * 4).round()\nvalidate['Stars'] = (validate.Stars * 4).round()\ntest['Stars'] = (test.Stars * 4).round()","81be676d":"# Discover most common rating (will use as baseline)\ntrain['Stars'].value_counts().head(1)","7786fedc":"# 4-star rating is most common, set to value 16\nmost_common = 16","704ef8a8":"# Calculate baseline accuracy\nprint('Baseline accuracy:', str(round((164 \/ len(train)) * 100, 2)) + '%')","80308c07":"### Encode data ###\ntrain_encoded = pd.get_dummies(train, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])\nvalidate_encoded = pd.get_dummies(validate, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])\ntest_encoded = pd.get_dummies(test, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])","108ab179":"X_train, y_train = train_encoded.drop(columns='Stars'), train_encoded.Stars\nX_validate, y_validate = validate_encoded.drop(columns='Stars'), validate_encoded.Stars\nX_test, y_test = test_encoded.drop(columns='Stars'), test_encoded.Stars\nX_test.head(1)","3fe587c0":"rf = RandomForestClassifier(n_estimators=1, random_state=99)\nrf.fit(X_train, y_train)\nprint((rf.predict(X_train) == y_train).mean())\nprint((rf.predict(X_validate) == y_validate).mean())","fc1cb18c":"tree = DecisionTreeClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\ntree.fit(X_train, y_train)\nprint((tree.predict(X_train) == y_train).mean())\nprint((tree.predict(X_validate) == y_validate).mean())","3e463fde":"knn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train, y_train)\nprint((knn.predict(X_train) == y_train).mean())\nprint((knn.predict(X_validate) == y_validate).mean())","a99e911d":"logit = LogisticRegression(penalty='l2', random_state=123)\nlogit.fit(X_train, y_train)\nprint((logit.predict(X_train) == y_train).mean())\nprint((logit.predict(X_validate) == y_validate).mean())","c38301dc":"df.head(3)","8341e934":"df[df.Variety.str[:10] == 'Artificial'].Stars.mean()","fd83c092":"df[df.Variety.str[:10] != 'Artificial'].Stars.mean()","7b5c6ec2":"eng = df.copy()\neng['Artificial'] = df.Variety.str.contains('artificial', case=False)\n#eng['Artificial'] = df.Variety.str[:10] == 'Artificial'\neng.sample(3)","add3f32c":"sns.boxplot(eng.Artificial, eng.Stars)","91879c18":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Artificial == True].Stars, \n                          eng[eng.Artificial == False].Stars,\n                          alternative='less')\n\nif t > 0 and p < alpha:\n    print('Artificial is worse-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Artificial is not worse-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","18e4088f":"eng['Instant'] = df.Variety.str.contains('instant', case=False)\neng.sample(3)","a7a9a273":"print(\"Instant:\", eng[eng.Instant].Stars.mean())\nprint(\"Not-Instant:\", eng[eng.Instant == False].Stars.mean())\nsns.boxplot(eng.Instant, eng.Stars)\nplt.show()","d93d1957":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Instant == True].Stars, \n                          eng[eng.Instant == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Instant is better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Instant is not better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","85ebae7e":"eng[eng.Variety.str.contains('flavor', case=False)].Variety.unique()","eae263f6":"eng['Chicken'] = eng.Variety.str.contains('chicken', case=False)\neng.head(2)","83f6607e":"sns.boxplot(eng.Chicken, eng.Stars)","a5d467cb":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Chicken == True].Stars, \n                          eng[eng.Chicken == False].Stars,\n                          alternative='less')\n\nif t > 0 and p < alpha:\n    print('Chicken is worse-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Chicken is not worse-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","aca15b95":"eng['Beef'] = eng.Variety.str.contains('beef', case=False)\neng.head(2)","6df79a0a":"sns.boxplot(eng.Beef, eng.Stars)","516bb258":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Beef == True].Stars, \n                          eng[eng.Beef == False].Stars,\n                          alternative='less')\n\nif t > 0 and p < alpha:\n    print('Beef is worse-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Beef is not worse-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","e9b749f7":"eng['Shrimp'] = eng.Variety.str.contains('shrimp', case=False)\neng.head(2)","f747a7ce":"sns.boxplot(eng.Shrimp, eng.Stars)","a7c2a071":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Shrimp == True].Stars, \n                          eng[eng.Shrimp == False].Stars,\n                          alternative='less')\n\nif t > 0 and p < alpha:\n    print('Shrimp is worse-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Shrimp is not worse-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","ce19641a":"eng['Seafood'] = eng.Variety.str.contains('fish', case=False) | eng.Variety.str.contains('seafood', case=False)\neng.head(2)","6bcc7e3c":"sns.boxplot(eng.Seafood, eng.Stars)","48e7be78":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Seafood == True].Stars, \n                          eng[eng.Seafood == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Seafood is better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Seafood is not better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","f4fbe87d":"eng['ChowMein'] = eng.Variety.str.contains('chow mein', case=False)\neng.head(2)","67cac19c":"sns.boxplot(eng.ChowMein, eng.Stars)","2a319154":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.ChowMein == True].Stars, \n                          eng[eng.ChowMein == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Chow Mein is better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Chow Mein is not better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","8c7891dc":"eng['Spicy'] = eng.Variety.str.contains('spicy', case=False)\neng.head(2)","f84faaae":"sns.boxplot(eng.Spicy, eng.Stars)","63c94eb2":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Spicy == True].Stars, \n                          eng[eng.Spicy == False].Stars,\n                          alternative='greater')\n\nif t > 0 and p < alpha:\n    print('Spicy is better-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Spicy is not better-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","e27811d7":"eng['Veggie'] = eng.Variety.str.contains('vegetable', case=False) | eng.Variety.str.contains('vegetarian', case=False)\neng.head(2)","e9fdfa95":"sns.boxplot(eng.Veggie, eng.Stars)","baef85b2":"### Hypothesis Testing ###\nalpha = .05\nt, p = stats.mannwhitneyu(eng[eng.Veggie == True].Stars, \n                          eng[eng.Veggie == False].Stars,\n                          alternative='less')\n\nif t > 0 and p < alpha:\n    print('Veggie is worse-rated ramen than the rest.\\n\\\n            We reject the null hypothesis with 95% confidence.')\nelse:\n    print('Veggie is not worse-rated ramen than the rest.\\n\\\n            Results do not breach the 95% confidence threshold.')","e1fcd485":"# eng.Country.value_counts() # Going to drop low value counts\neng = eng.drop([eng[eng.Country == 'Australia'].index.item()]) # Australia has one observation","be444f53":"# Encode\neng = pd.get_dummies(eng, columns=['Style'])\neng['Stars'] = (eng['Stars'] * 4).round() # Turn each 0.25-step into 1-step","3892a2d9":"# Specify columns to drop\ncolumns_to_drop = ['Review #','Brand','Variety','Country']\n# Split\ntrain_validate, test = train_test_split(eng.drop(columns=columns_to_drop), test_size=0.2, random_state=99)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=99)\ntrain.shape, validate.shape, test.shape","66b096ab":"# Check work\ntrain.head(1)","e51b1289":"# Splits\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","eff67986":"# Fix dtypes\nX_train = X_train.astype('int')\nX_validate = X_validate.astype('int')\nX_test = X_test.astype('int')","e9d95fe4":"rf2 = RandomForestClassifier(n_estimators=1, random_state=99)\nrf2.fit(X_train, y_train)\nprint((rf2.predict(X_train) == y_train).mean())\nprint((rf2.predict(X_validate) == y_validate).mean())","4e7e47c7":"tree2 = DecisionTreeClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\ntree2.fit(X_train, y_train)\nprint((tree2.predict(X_train) == y_train).mean())\nprint((tree2.predict(X_validate) == y_validate).mean())","7cff746b":"knn2 = KNeighborsClassifier(n_neighbors=20)\nknn2.fit(X_train, y_train)\nprint((knn2.predict(X_train) == y_train).mean())\nprint((knn2.predict(X_validate) == y_validate).mean())","4c0e6f6c":"logit2 = LogisticRegression(random_state=123)\nlogit2.fit(X_train, y_train)\nprint((logit2.predict(X_train) == y_train).mean())\nprint((logit2.predict(X_validate) == y_validate).mean())","9e0c8c7f":"eng = pd.get_dummies(eng, columns=['Country'])\neng.head(1)","70881943":"# Specify columns to drop\ncolumns_to_drop = ['Review #','Brand','Variety']\n# Split\ntrain_validate, test = train_test_split(eng.drop(columns=columns_to_drop), test_size=0.2, random_state=99)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=99)\ntrain.shape, validate.shape, test.shape","f9e7c534":"# Splits\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","ad682cb3":"# Encode\nX_train = X_train.astype('int')\nX_validate = X_validate.astype('int')\nX_test = X_test.astype('int')","a0a9e061":"rf3 = RandomForestClassifier(n_estimators=20, random_state=99)\nrf3.fit(X_train, y_train)\nprint((rf3.predict(X_train) == y_train).mean())\nprint((rf3.predict(X_validate) == y_validate).mean())","d1ab306f":"tree3 = DecisionTreeClassifier(max_depth=20, min_samples_leaf=1, random_state=123)\ntree3.fit(X_train, y_train)\nprint((tree3.predict(X_train) == y_train).mean())\nprint((tree3.predict(X_validate) == y_validate).mean())","d74984da":"knn3 = KNeighborsClassifier(n_neighbors=60)\nknn3.fit(X_train, y_train)\nprint((knn3.predict(X_train) == y_train).mean())\nprint((knn3.predict(X_validate) == y_validate).mean())","5508711b":"logit3 = LogisticRegression(random_state=123)\nlogit3.fit(X_train, y_train)\nprint((logit3.predict(X_train) == y_train).mean())\nprint((logit3.predict(X_validate) == y_validate).mean())","571a385d":"rfe = RFE(estimator=LinearRegression(), n_features_to_select=20)","1a6803a5":"rfe.fit(X_train, y_train)","41f985b8":"pd.Series(rfe.ranking_, index=X_train.columns)","91506880":"# Set columns\nnew_approach = df[['Country','Stars']]","8a510c8b":"# Encode\nnew_approach = pd.get_dummies(new_approach, columns=['Country'])\nnew_approach['Stars'] = (new_approach['Stars'] * 4).round()","c2612270":"# Split\ntrain_validate, test = train_test_split(new_approach, test_size=0.2, random_state=99)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=99)\ntrain.shape, validate.shape, test.shape","3778865c":"# Isolate target\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","aa05a563":"rf3 = RandomForestClassifier(n_estimators=20, random_state=99)\nrf3.fit(X_train, y_train)\nprint((rf3.predict(X_train) == y_train).mean())\nprint((rf3.predict(X_validate) == y_validate).mean())","65a76f5b":"tree3 = DecisionTreeClassifier(max_depth=20, min_samples_leaf=1, random_state=123)\ntree3.fit(X_train, y_train)\nprint((tree3.predict(X_train) == y_train).mean())\nprint((tree3.predict(X_validate) == y_validate).mean())","4450a628":"knn3 = KNeighborsClassifier(n_neighbors=20)\nknn3.fit(X_train, y_train)\nprint((knn3.predict(X_train) == y_train).mean())\nprint((knn3.predict(X_validate) == y_validate).mean())","04e2be6c":"logit3 = LogisticRegression(random_state=123)\nlogit3.fit(X_train, y_train)\nprint((logit3.predict(X_train) == y_train).mean())\nprint((logit3.predict(X_validate) == y_validate).mean())","92721731":"y_train.unique()","38da5af3":"y_train = (y_train \/ 4).round()\ny_validate = (y_validate \/ 4).round()\ny_test = (y_test \/ 4).round()\ny_train.unique()","cfc8a4a8":"rf3 = RandomForestClassifier(n_estimators=2, random_state=99)\nrf3.fit(X_train, y_train)\nprint((rf3.predict(X_train) == y_train).mean())\nprint((rf3.predict(X_validate) == y_validate).mean())","419dba0c":"tree3 = DecisionTreeClassifier(max_depth=2, min_samples_leaf=1, random_state=123)\ntree3.fit(X_train, y_train)\nprint((tree3.predict(X_train) == y_train).mean())\nprint((tree3.predict(X_validate) == y_validate).mean())","f8a60bcc":"knn3 = KNeighborsClassifier(n_neighbors=70)\nknn3.fit(X_train, y_train)\nprint((knn3.predict(X_train) == y_train).mean())\nprint((knn3.predict(X_validate) == y_validate).mean())","1d714b99":"logit3 = LogisticRegression(random_state=123)\nlogit3.fit(X_train, y_train)\nprint((logit3.predict(X_train) == y_train).mean())\nprint((logit3.predict(X_validate) == y_validate).mean())","a82026a8":"y_train.value_counts()","dc93da1f":"print(\"Baseline Accuracy:\", 536 \/ len(y_train))","b307f0b1":"### Decrease target's precision ###\ndf['Stars'] = df['Stars'].round().astype('int')\ndf.head(2)","6eb6c571":"### Split into subsets ###\ntrain_validate, test = train_test_split(df, test_size=0.2, random_state=124, stratify=df.Brand)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=124, stratify=train_validate.Brand)","481c11b1":"### Encode data ###\ntrain_encoded = pd.get_dummies(train, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])\nvalidate_encoded = pd.get_dummies(validate, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])\ntest_encoded = pd.get_dummies(test, columns=['Brand','Style','Country']).drop(columns=['Review #','Variety'])","5173ecc4":"### Isolate target ###\nX_train, y_train = train_encoded.drop(columns='Stars'), train_encoded.Stars\nX_validate, y_validate = validate_encoded.drop(columns='Stars'), validate_encoded.Stars\nX_test, y_test = test_encoded.drop(columns='Stars'), test_encoded.Stars","b136cb86":"rf_v2 = RandomForestClassifier(n_estimators=10, random_state=99)\nrf_v2.fit(X_train, y_train)\nprint((rf_v2.predict(X_train) == y_train).mean())\nprint((rf_v2.predict(X_validate) == y_validate).mean())","d7e11107":"tree_v2 = DecisionTreeClassifier(max_depth=10, min_samples_leaf=1, random_state=123)\ntree_v2.fit(X_train, y_train)\nprint((tree_v2.predict(X_train) == y_train).mean())\nprint((tree_v2.predict(X_validate) == y_validate).mean())","e7275391":"knn_v2 = KNeighborsClassifier(n_neighbors=40)\nknn_v2.fit(X_train, y_train)\nprint((knn_v2.predict(X_train) == y_train).mean())\nprint((knn_v2.predict(X_validate) == y_validate).mean())","6268e7ee":"logit_v2 = LogisticRegression(random_state=123)\nlogit_v2.fit(X_train, y_train)\nprint((logit_v2.predict(X_train) == y_train).mean())\nprint((logit_v2.predict(X_validate) == y_validate).mean())","2669c2d9":"# Initialize engineered dataframe\neng = df.copy()\n# Add columns for Variety elements\neng['Artificial'] = df.Variety.str.contains('artificial', case=False)\neng['Instant'] = df.Variety.str.contains('instant', case=False)\neng['Chicken'] = eng.Variety.str.contains('chicken', case=False)\neng['Beef'] = eng.Variety.str.contains('beef', case=False)\neng['Shrimp'] = eng.Variety.str.contains('shrimp', case=False)\neng['Seafood'] = eng.Variety.str.contains('fish', case=False) | eng.Variety.str.contains('seafood', case=False)\neng['ChowMein'] = eng.Variety.str.contains('chow mein', case=False)\neng['Spicy'] = eng.Variety.str.contains('spicy', case=False)\neng['Veggie'] = eng.Variety.str.contains('vegetable', case=False) | eng.Variety.str.contains('vegetarian', case=False)\n# Check work\neng.head(3)","316cdb60":"# Encoding\neng = pd.get_dummies(eng, columns=['Style','Country']).drop(columns=['Review #','Brand','Variety'])","9a98875f":"# Split\ntrain_validate, test = train_test_split(eng, test_size=0.2, random_state=99)\ntrain, validate = train_test_split(train_validate, test_size=0.25, random_state=99)\ntrain.shape, validate.shape, test.shape","05f40979":"# Isolate target\nX_train, y_train = train.drop(columns='Stars'), train.Stars\nX_validate, y_validate = validate.drop(columns='Stars'), validate.Stars\nX_test, y_test = test.drop(columns='Stars'), test.Stars","0df3add5":"rf_v2_1 = RandomForestClassifier(n_estimators=1, random_state=99)\nrf_v2_1.fit(X_train, y_train)\nprint((rf_v2_1.predict(X_train) == y_train).mean())\nprint((rf_v2_1.predict(X_validate) == y_validate).mean())","299c6b7b":"tree_v2_1 = DecisionTreeClassifier(max_depth=2, min_samples_leaf=1, random_state=123)\ntree_v2_1.fit(X_train, y_train)\nprint((tree_v2_1.predict(X_train) == y_train).mean())\nprint((tree_v2_1.predict(X_validate) == y_validate).mean())","0c829b32":"knn_v2_1 = KNeighborsClassifier(n_neighbors=40)\nknn_v2_1.fit(X_train, y_train)\nprint((knn_v2_1.predict(X_train) == y_train).mean())\nprint((knn_v2_1.predict(X_validate) == y_validate).mean())","2b368f28":"logit_v2_1 = LogisticRegression(random_state=123)\nlogit_v2_1.fit(X_train, y_train)\nprint((logit_v2_1.predict(X_train) == y_train).mean())\nprint((logit_v2_1.predict(X_validate) == y_validate).mean())","49904baf":"# Ramen Classification\n\n**Goal**: Accurately predict a ramen's rating (Stars) using the brand, variety, style and country of the ramen.","83edfc16":"Flavors:\n- Tomato iii \/ Tomato Chicken i \/ Spicy Flavor Tomato i \/ Hot Tomato i\n- Stir-Fried Kimchi ii\n- Kimchi iii\n- Stir Fry ii\n- Chicken iiiiiiiiiiiiiiiiiiiiii \/ Spicy Chicken iii \/ Hot and Spicy Chicken ii \/ Hot and Sour Chicken i \/ Sweet and Sour Chicken i \/ Abalone and Chicken i \/ Sriracha Chicken ii \/ Mushroom Chicken i \/ Miso Chicken i \/ Chipotle Chicken i \/ Chicken Tortilla ii \/ Roast Chicken ii \/ Spicy Szechuan Chicken i \/ Grilled Chicken i \/ Chicken Vegetable i \/ Habanero Lime Chicken i \/ Lemon Chicken i\n- Kimchi Pork ii \/ Braised Pork i \/ Minced Pork ii (mispelled Prok) i \/ Pork iiii \/ Stewed Pork i \/ BBQ Pork i \/ Pork Rib ii \/ Simmered Pork i \/ Tonkotsu\/Shoyu Rich Pork\n- Beef iiiiiiiiiiiiiiiiiii \/ Spicy Beef ii \/ Beef Pho i \/ Stewed Beef i \/ Pickled Vegetable Beef i \/ Tart Beef i \/ Braised Beef i \/ Sukiyaki Beef Flavor Savory Soy Sauce i \/ Beef Taco i \/ Beef Flavor Minestrone i\n- Hot & Sour i\n- Sour i\n- Hot & Spicy i \/ Spicy Sichuan i\n- Hot i\n- Spicy i\n- Spicy Po-Au-Feu ii\n- Sriracha i\n- Hot & Sour Fish i \/ Seafood iiii \/ Spicy Seafood iii \/ Seafood Flavor Udon i\n- Shallot i\n- Shrimp iii \/ Hot and Spicy Shrimp ii \/ Shrimp (Tom Yum) i (Shrimp Tom Yum) ii \/ Shrimp Habanero Lime i (Habanero Lime Shrimp) i \/ Picante Lime Shrimp i \/ Lime Flavor with Shrimp\n- Prawn i\n- Abalone Crab i\n- Mushroom iiii\n- Cheese ii \/ Jalapeno Cheddar i\n- Pickled Mustard i\n- Teriyaki Mayo i\n- Sweet and Sour i\n- Non-Fried i\n- Chilli Cheese (mispelled) i \/ Chili & Lime i \/ Chilli Beef (mispelled) i \/ Sichuan Chilli Eel i \/ Hot Chili \/ Tom Yum Chili i\n- Curry iiiii \/ Vegetable Curry i\n- Sweet Corn i \/ Corn, Salt & Butter i\n- Sesame ii \/ Spicy Sesame i \/ Sesame Oil i \/ Rice Vinegar Sesame i\n- Spicy Black Pepper i\n- Tom Yum ii\n- Singapura i\n- Onion iii\n- Karashi Mentaiko i\n- Tonkotsu i \/ Pork Tonkotsu \/ Umami Tonkotsu i \/ Tonkotsu Shouyu i \/ Shoyu i\n- Jjamppong i\n- Shiodare i\n- Bulgogi i\n- Shiitake Flavor Spinach i\n- La Wei (Spicy i\n- Wonton i \/ Wonton Soup i\n- Shio i\n- Hot Gulai i\n- Mexican Pizza i\n- Spicy Lime i\n- Chow Mein i \/ Chow Mein Spicy i \/ Chow Mein Premium Spicy Chicken i \/ Chow Mein Oyster Sauce BBQ i \/ Chow Mein Seafood i \/ Chow Mein Teriyaki Chicken i\n- Miso ii\n- Umami Miso i \/ Umami Soy Sauce i\n- Soy i \/ Soy Sauce i \/ Soy Sauce Flavor Udon i\n- Udon i\n- Camelia Oil Vegetable Sauce i \/ Camellia Oil Spicy Bean Sauce i\n- Soy & Vinegar i\n- Chili Picante Chicken With Lime Flavor i\n- Abalone ii\n- Bulalo i\n- Pickled Cayenne i\n- Vegetable ii \/ Vegetarian i\n- Crab i\n- Citrus i\n- Original iii\n\nExtra notes:\n- with Vegetable \/ Vegetables iiii\n- having Lime\n- Sodium (having less)\n- Simulated is used, add to Artificial\n- is Tonkotsu\n- is Shouyu \/ Shoyu\n- is Miso\n- is Abalone\n- has Soy\n- Tom Yum\n- Meatball (a few with this characteristic)\n\nIdeas:\n- Traditional ramen flavors have better ratings than non-traditional flavors\n- Meats v Vegetables\/other\n- Location-based (like Sichuan)","549b1f88":"# Recursive Feature Engineering","2112b79d":"## Does Japan produce better ramen than the countries with over 100 ramen reviews?\nConfidence interval: 95%\n\n$H_0$: Japan does not statistically produce better-rated ramen than USA.\n\n$H_a$: Japan statistically produces better-rated ramen than USA.","d1559aba":"# Imports","74ad90e0":"## Minimum Viable Product (MVP)","b4bc5d7d":"### Beef","1b844012":"### Artificial","c85796d3":"It seems Malaysia's distribution didn't change much. Maybe Malaysia is just really good at making ramen.","72ccd782":"### Model!","28fa4eea":"### Results\nDecision Tree had the best performance at 56% prediction of out-of-sample data. Let's try another feature combination.\n## Variety\n### Prep","2d74b4f5":"## Do bowls rank statistically higher than cups?\nConfidence interval: 95%\n\n$H_0$: Bowls do not rank statistically higher than cups.\n\n$H_a$: Bowls rank statistically higher than cups.","ec5d3d92":"### Chow Mein","f3c7c30f":"So this is the result of using Style and Country only, peaking at around 22% for out-of-sample data. MORE FEATURES!","72ef2c44":"# Goal Wrap-Up\nThrough this classification approach, I've been able to beat mean baseline on both the precise rating and imprecise rating target.","57ce47c4":"MyKuali has a suspiciously-high number of 5-star reviews... what's the distribution of all MyKuali reviews?","b0db5bd6":"### Modeling Results\nBy decreasing the precision of the target by four times, the models performed twice as well as before. Let's re-calculate the baseline real quick to see if this new performance still beats threshold...","b04e35f4":"## Split","c6a88770":"## What do the 5-Star Reviews look like?","8ab7437b":"### Flavor (suggesting artificial?)","01ca7b80":"## Model!","2c040160":"That's interesting... it seems the countries all are more important in determining rating than any other feature, engineered or not. Hmm.\n\nWell, let's approach this specifically from the Country column and see what happens.","fd9f05be":"### Model!","0e7e95cd":"## Does Nissin produce better ramen than the next 8 most-reviewed brands?\nConfidence interval: 95%\n\n$H_0$: Nissin does not statistically produce better-rated ramen than other brands.\n\n$H_a$: Nissin statistically produces better-rated ramen than other brands.","11a2078e":"### Modeling Results\nLogistic Regression performed best, but not better than the MVP.\n\nSince no feature combination breached 25% accuracy, I'm going to change up the target a bit. Instead of using 0.25 steps, I'm going to round the steps up to 1, so stars from 0 to 5.","0ce019f0":"### Vegetable\/Vegetarian","176886a7":"# Modeling - Post-MVP #3 - Only Country\n## Prep","6a5fea98":"### Results with new engineered features, not selecting k best or RFE\nLogistic Regression performed best on out-of-sample data than other models, but did not perform as well as the MVP.\n\nLet's see how adding Country column affects the results...","ed27400e":"### Results with unmodified Brand\nOn all 4 algorithms with varying hyperparameters, having Brand created an overfit situation causing the models to perform worse on out-of-sample data compared to models without Brand.","1f00a252":"# Modeling - Post-Precision Decrease\nSo since I've already prepared different feature combinations, I'm simply bringing the code up to this section.\n\n## Brand\n### Prep","f960b991":"### Results including Country, still not doing RFE or SelectKBest\nKNN with 60 neighbors performed best by being not-overfit-as-much compared to Logistic Regression.\n\nNow, let's run RFE to see if we can filter down features and improve model performance...","a35d897e":"# Split Data\nNote: I noticed later on that some countries and brands only have a few reviews, so I'm dropping anything less than 10 for country counts and dropping brand as a feature until I can engineer a solution. (First up is the MVP)","50799673":"### By Brand","e62a11cb":"So baseline accuracy is 55.25%. When only using Country as a one-hot-encoded feature, all models had similar performance to the baseline (within 1% of baseline).\n\nLet's try a different feature combination on this less-precise target.","4e264c9f":"### By Country","736f1053":"### Instant","ec46dbda":"## Model!","94d24f6f":"# Explore Data","bcad3283":"# Modeling\nGameplan: \n1. Filter dataframe to 0.25 increments between 0 and 5 - *Done*\n2. Multiply all values by 4 (0 -> 0, 0.25 -> 1, 0.5 -> 2, 0.75 -> 3, etc) - *Done*\n3. Choose most common rating for baseline (classification approach, will explain my reasoning below) - *Done*\n4. Calculate baseline accuracy (number of chosen-baseline values \/ all values) - *Done*\n5. Build MVP models using Style and Country all encoded\n    * Later perform Feature Engineering on Brand to simplify column for brand names only\n    * Later perform Feature Engineering on Variety to simplify column for Chicken, Beef, etc\n6. Evaluate models on validate set\n7. Re-set baseline accuracy using best-selected model for train and validate\n8. Feature engineering on Variety\n9. One-hot encoding on all engineered Variety values and all Brand, Style, and Country values (yep)\n10. SelectKBest\/RFE to determine which features to use in model\n11. Note which features SelectKBest\/RFE chose\n12. Build models using the features\n13. Evaluate on validate\n14. Compare to new baseline\n15. Choose best model\n16. Evaluate on test\n17. Formalize results in a presentation notebook\n\n## Prep","ce9d22a8":"## Isolate Target","814dbe80":"# Feature Engineering\n## Brand\nPretty easy fix, just limit column to brands with over 10 reviews\n### Prep","326e1d29":"#### Potential Features from Flavor:\n- Chicken (48); Beef (30); Pork (16); Shrimp (12); Fish\/Seafood (10); Chow Mein (6);\n- Chili\/Chilli (7); Curry (6); Kimchi (5); Stir-Fry (4);\n- Tom Yum (6); Tonkotsu (5); Miso (4); Wonton (2); Jjamppong (1); Shiodare (1); Bulgogi (1); Bulalo (1);\n- Tomato (6); Mushroom (5); Sesame (5); Onion (3); Shiitake (1);\n- Spicy (26); Hot (12); Soy (6); Sour (6); Sweet (2); Umami [savory] (2);\n- Sriracha (3); \n- Abalone (4);\n- Vegetable (11);\n- has_Lime (8);\n\n### Chicken","9d4704e0":"## Variety","901b07f6":"## Does Japan produce better ramen than USA?\nConfidence interval: 95%\n\n$H_0$: Japan does not statistically produce better-rated ramen than USA.\n\n$H_a$: Japan statistically produces better-rated ramen than USA.","e3b37f2a":"### Shrimp","43403166":"**MVP Columns**: Brand, Style, Country\n\nOne-hot encode columns","6064ddd7":"# Tidying Data (dtypes, nulls, etc)","dc93ce5c":"## Split","3fa97354":"# Modeling - Post-MVP #1\n## Prep","964e20c3":"## Model!","01779647":"### Spicy","bc1f5b27":"## Model!","03abe1d1":"# Modeling - Decrease Precision of Target\n## Prep","607f74bc":"## Isolate Target","53513023":"# Modeling - Post-MVP #2 (add Country)\n## Prep","26bbcbce":"## Results of MVP\n- All algorithms performed better than the baseline.\n- DecisionTreeClassifier with max_depth=16 and min_samples_leaf=4 performed best of all classification algorithms tried.","22533074":"Values 6, 8, 12, 14, 15, 16, and 20 had better-than-0% prediction (ratings: 1.5, 2, 3, 3.5, 3.75, 4, and 5), try a different classification report to see what it did?","09c1e867":"### Seafood","df776f23":"### Modeling Results\nDecision Tree performed best of the models, but performed almost exactly the same as baseline."}}