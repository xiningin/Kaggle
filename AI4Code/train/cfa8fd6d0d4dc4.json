{"cell_type":{"6744a11f":"code","a46a251e":"code","b52eebf0":"code","289b92af":"code","96b51420":"code","49a9805c":"code","fabe2dfc":"code","c57c03b8":"code","2818911e":"code","30d7ce5d":"code","d0296983":"code","4d2726d6":"code","d1928c1f":"code","a440b75d":"code","c996b0d3":"code","1acc672d":"code","5a3d034f":"code","f4c331cf":"code","3dec6d7f":"code","8ed29d56":"code","1d65514c":"code","9581eab6":"code","aa3e8220":"code","c929067b":"code","237b7e50":"code","b070c724":"code","3523a03b":"code","c5a87929":"code","d6ae524a":"code","243ddde5":"code","4a866a8a":"code","0e53b7a5":"code","deaed398":"code","e691694c":"code","51f7890e":"code","9907596a":"code","1fa6e1c2":"code","7249d120":"code","b1c8cfc5":"code","2cb593cf":"code","9a91fe7b":"code","3ad5fe11":"code","6195afe3":"code","9af648ce":"code","0d9a2f22":"code","d11b5b93":"code","c1532f0e":"code","31d4af38":"code","7e1db680":"code","5855dd6b":"code","2209be08":"code","d3cacdd5":"code","055707a8":"code","8e750e4f":"code","b14e15b9":"markdown","c6e4f8eb":"markdown","c7847951":"markdown","6cf9646e":"markdown","1c04ce6d":"markdown","4faa61aa":"markdown","5f034106":"markdown","72bddbf9":"markdown","9169ee3f":"markdown","eb151d75":"markdown","d7fd5cb4":"markdown","89cccd92":"markdown","d716c6a4":"markdown","bcd88c1f":"markdown","0492680f":"markdown","3436abf2":"markdown","b832753c":"markdown","db414807":"markdown","4719e86b":"markdown","d2e602a7":"markdown","fb78c6e1":"markdown","5a812d80":"markdown","9a2ebd9e":"markdown","f867b5e0":"markdown","1c558cfc":"markdown","373912c2":"markdown","a4100102":"markdown","15a76c4f":"markdown","34c8b026":"markdown","d41d743b":"markdown","16faff1a":"markdown","a017038b":"markdown","1c67af40":"markdown","5d19dcb3":"markdown","a79ec15f":"markdown","9cea40a3":"markdown","494a2e78":"markdown","30559af5":"markdown","5a017d4b":"markdown","8c95133f":"markdown","d2faa453":"markdown","d53f8247":"markdown","f15d678a":"markdown","e4b41654":"markdown","30ab15f7":"markdown","669f4ade":"markdown","0a139029":"markdown","97f83275":"markdown"},"source":{"6744a11f":"from plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly import tools\nfrom datetime import date\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport random \nimport warnings\nimport gc\n\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\npath = \"..\/input\/\"\n\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef bar_hor_noagg(x, y, title, color, w=None, h=None, lm=0, limit=100, rt=False):\n    trace = go.Bar(y=x, x=y, orientation = 'h', marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ndef bar_ver_noagg(x, y, title, color, w=None, h=None, lm=0, rt = False):\n    trace = go.Bar(y=y, x=x, marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n    \ndef gp(col, title):\n    df1 = app_train[app_train[\"TARGET\"] == 1]\n    df0 = app_train[app_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(app_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 \/ total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 \/ total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 ","a46a251e":"app_train = pd.read_csv(path + \"application_train.csv\")\napp_train.head()\n\n\n\n","b52eebf0":"# Target Variable Distribution \nbar_hor(app_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])","289b92af":"tr0 = bar_hor(app_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);","96b51420":"tr0 = bar_hor(app_train, \"NAME_FAMILY_STATUS\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('NAME_FAMILY_STATUS', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Family Status Distribution\" , \"Family Status, Target = 1\" ,\"Family Status, Target = 0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);","49a9805c":"'''\n## real estate \nt = app_train['FLAG_OWN_REALTY'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#96D38C','#FEBFB3']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Owning Real Estate', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\nt = app_train['FLAG_OWN_CAR'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#FEBFB3','#96D38C']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Owning Car', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\ntr1, tr2 = gp('FLAG_OWN_REALTY', 'Applicants Owning Real Estate wrt Target Variable')\ntr3, tr4 = gp('FLAG_OWN_CAR', 'Applicants Owning Car wrt Target Variable')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"% Applicants with RealEstate and Target = 1\", \"% Applicants with Car and Target = 1\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);\n'''\n","fabe2dfc":"tr0 = bar_hor(app_train, \"NAME_TYPE_SUITE\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1 = bar_hor(app_train, \"NAME_INCOME_TYPE\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Suite Type' , 'Applicants Income Type'])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);","c57c03b8":"tr1, tr2 = gp('NAME_TYPE_SUITE', 'Applicants Type Suites which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Type Suites distribution when Target = 1\", \"Applicants Type Suites distribution when Target = 0\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);\n\n\ntr1, tr2 = gp('NAME_INCOME_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Income Types when Target = 1\", \"Applicants Income Type When Target = 0\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);","2818911e":"'''\nt = app_train['NAME_CONTRACT_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\ncolors = ['#FEBFB3','#96D38C']\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\nlayout = go.Layout(title='Applicants Contract Type', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n'''\n","30d7ce5d":"tr1 = bar_hor(app_train, \"NAME_EDUCATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"NAME_HOUSING_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Education Type', 'Applicants Housing Type' ])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);\n\n\ntr1, tr2 = gp('NAME_EDUCATION_TYPE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('NAME_HOUSING_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Education Types, Target=1\", \"Applicants Housing Type, Target=1\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=30));\niplot(fig);","d0296983":"tr1 = bar_hor(app_train, \"ORGANIZATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"OCCUPATION_TYPE\", \"Distribution of \" ,\"#f975ae\", w=700, lm=100, return_trace = True)\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = ['Applicants Organization Type', 'Applicants Occupation Type' ])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\nfig['layout'].update(height=600, showlegend=False, margin=dict(l=150));\niplot(fig);","4d2726d6":"tr1, tr2 = gp('ORGANIZATION_TYPE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('OCCUPATION_TYPE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles = [\"Applicants Organization Types - Repayed\", \"Applicants Occupation Type - Repayed\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);","d1928c1f":"tr1 = bar_hor(app_train, \"FONDKAPREMONT_MODE\", \"Distribution of FLAG_OWN_REALTY\" ,\"#639af2\", w=700, lm=100, return_trace= True)\ntr2 = bar_hor(app_train, \"WALLSMATERIAL_MODE\", \"Distribution of FLAG_OWN_CAR\" ,\"#a4c5f9\", w=700, lm=100, return_trace = True)\ntr1 = bar_hor(app_train, \"HOUSETYPE_MODE\", \"Distribution of FLAG_OWN_CAR\" ,\"#a4c5f9\", w=700, lm=100, return_trace = True)\n\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = [ 'House Type', 'Walls Material'])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr2, 1, 2);\n# fig.append_trace(tr3, 1, 3);\n\nfig['layout'].update(height=400, showlegend=False, margin=dict(l=100));\niplot(fig);","a440b75d":"tr1, tr2 = gp('HOUSETYPE_MODE', 'Applicants Income Types which repayed the loan')\ntr3, tr4 = gp('WALLSMATERIAL_MODE', 'Applicants Income Types which repayed the loan')\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False, subplot_titles = [\"HouseTypes - Repayed\", \"WallsMaterial - Repayed\"])\nfig.append_trace(tr1, 1, 1);\nfig.append_trace(tr3, 1, 2);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=120));\niplot(fig);","c996b0d3":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_CREDIT\")\nax = sns.distplot(app_train[\"AMT_CREDIT\"])","1acc672d":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_ANNUITY\")\nax = sns.distplot(app_train[\"AMT_ANNUITY\"].dropna())","5a3d034f":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_GOODS_PRICE\")\nax = sns.distplot(app_train[\"AMT_GOODS_PRICE\"].dropna())","f4c331cf":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of REGION_POPULATION_RELATIVE\")\nax = sns.distplot(app_train[\"REGION_POPULATION_RELATIVE\"])","3dec6d7f":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_BIRTH\")\nax = sns.distplot(app_train[\"DAYS_BIRTH\"])","8ed29d56":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(app_train[\"DAYS_EMPLOYED\"])","1d65514c":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_REGISTRATION\")\nax = sns.distplot(app_train[\"DAYS_REGISTRATION\"])","9581eab6":"t = app_train[\"CNT_FAM_MEMBERS\"].value_counts()\nt1 = pd.DataFrame()\nt1['x'] = t.index \nt1['y'] = t.values \n\nplt.figure(figsize=(12,5));\nplt.title(\"Distribution of Applicant's Family Members Count\");\nax = sns.barplot(data=t1, x=\"x\", y=\"y\", color=\"#f975ae\");\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\n\nax.set_ylabel('');    \nax.set_xlabel('');","aa3e8220":"t = app_train[\"CNT_CHILDREN\"].value_counts()\nt1 = pd.DataFrame()\nt1['x'] = t.index \nt1['y'] = t.values \n\nplt.figure(figsize=(12,5));\nplt.title(\"Distribution of Applicant's Number of Children\");\nax = sns.barplot(data=t1, x=\"x\", y=\"y\", color=\"#f975ae\");\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\n\nax.set_ylabel('');    \nax.set_xlabel('');","c929067b":"bureau = pd.read_csv(path + \"bureau.csv\") #.set_index('SK_ID_CURR')\nbureau['YEAR']=((bureau['DAYS_CREDIT'] \/365)).abs().pow(0.5).round(0)\n#bureau\nbureau.head()\n","237b7e50":"\n\nbureau_balance = pd.read_csv(path + \"bureau_balance.csv\")\n#bureau_balance['EPOCH']=((bureau_balance['MONTHS_BALANCE'])).abs().pow(0.2).round(0)\nbureau_balance['YEAR']=((bureau_balance['MONTHS_BALANCE'])).abs().pow(0.2).round(0)\nbureau_balance.head(20)\n    \n\n\n\n#bureau_balance.head()","b070c724":"credit_card_balance = pd.read_csv(path + \"credit_card_balance.csv\")\ncredit_card_balance.head()","3523a03b":"pcb = pd.read_csv(path + \"POS_CASH_balance.csv\")\npcb.head()","c5a87929":"previous_application = pd.read_csv(path + \"previous_application.csv\")\nprevious_application.head()","d6ae524a":"'''\nt = previous_application['NAME_CONTRACT_STATUS'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Name Contract Status in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n'''\n","243ddde5":"'''\nt = previous_application['NAME_TYPE_SUITE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Suite Type in Previous Application Distribution', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n'''\n","4a866a8a":"'''\nt = previous_application['NAME_CLIENT_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Client Type in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n'''\n","0e53b7a5":"'''\nt = previous_application['CHANNEL_TYPE'].value_counts()\nlabels = t.index\nvalues = t.values\n\ncolors = ['#FEBFB3', '#E1396C', '#96D38C', '#D0F9B1']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='', textinfo='',\n               textfont=dict(size=12),\n               marker=dict(colors=colors,\n                           line=dict(color='#fff', width=2)))\n\nlayout = go.Layout(title='Channel Type in Previous Applications', height=400)\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n'''\n","deaed398":"installments_payments = pd.read_csv(path + \"installments_payments.csv\")\ninstallments_payments.head()","e691694c":"from sklearn.model_selection import train_test_split \nimport lightgbm as lgb\n\n# read the test files \napp_test = pd.read_csv('..\/input\/application_test.csv')\n\napp_test['is_test'] = 1 \napp_test['is_train'] = 0\napp_train['is_test'] = 0\napp_train['is_train'] = 1\n\n# target variable\nY = app_train['TARGET']\ntrain_X = app_train.drop(['TARGET'], axis = 1)\n\n# test ID\ntest_id = app_test['SK_ID_CURR']\ntest_X = app_test\n\n# merge train and test datasets for preprocessing\ndata = pd.concat([train_X, test_X], axis=0).set_index('SK_ID_CURR')","51f7890e":"# function to obtain Categorical Features\ndef _get_categorical_features(df):\n    feats = [col for col in list(df.columns) if df[col].dtype == 'object']\n    return feats\n\n# function to factorize categorical features\ndef _factorize_categoricals(df, cats):\n    for col in cats:\n        df[col], _ = pd.factorize(df[col])\n    return df \n\n# function to create dummy variables of categorical features\ndef _get_dummies(df, cats):\n    for col in cats:\n        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n    return df \n\n# get categorical features\ndata_cats = _get_categorical_features(data)\nprev_app_cats = _get_categorical_features(previous_application)\nbureau_cats = _get_categorical_features(bureau)\npcb_cats = _get_categorical_features(pcb)\nccbal_cats = _get_categorical_features(credit_card_balance)\n\n# create additional dummy features - \nprevious_application = _get_dummies(previous_application, prev_app_cats)\nbureau = _get_dummies(bureau, bureau_cats)\npcb = _get_dummies(pcb, pcb_cats)\ncredit_card_balance = _get_dummies(credit_card_balance, ccbal_cats)\n\n# factorize the categorical features from train and test data\ndata = _factorize_categoricals(data, data_cats)","9907596a":"\n## More Feature Ideas Reference : https:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm \n\n## count the number of previous applications for a given ID\nprev_apps_count = previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprevious_application['SK_ID_PREV'] = previous_application['SK_ID_CURR'].map(prev_apps_count['SK_ID_PREV'])\n\n## Average values for all other features in previous applications\nprev_apps_avg = previous_application.groupby('SK_ID_CURR').mean()\n\n'''\nfor f in ['CNT_PAYMENT','AMT_DOWN_PAYMENT','AMT_ANNUITY']:\n    lasti = 0\n    for i in [600,1200,1800,2200]:\n        prev_apps_avg['prev_wind_'+ f + str(i)] = (previous_application[(previous_application.DAYS_DECISION < i) & (previous_application.DAYS_DECISION > lasti)]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        prev_apps_avg['prev_'+ f + str(i)] = (previous_application[previous_application.DAYS_DECISION < i]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        prev_apps_avg['prev_has_'+ f + str(i)] = (np.isnan(prev_apps_avg['prev_'+ f + str(i)])).astype(int)\n        \n        if lasti > 0:\n            prev_apps_avg['prev_derivitive_' + f + str(i)] = prev_apps_avg['prev_' + f + str(lasti)] \/ (prev_apps_avg['prev_' + f + str(i)]+ 0.0001)\n\n        lasti = i\n\n\nprev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\n\n'''\n\ndata = data.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''\n\n\nfor f in ['CNT_PAYMENT','AMT_DOWN_PAYMENT','AMT_ANNUITY']:\n    for i in [600,1200,1800,2200]:\n        data['p_prev_weighted_' + f + str(i)] = (data['p_prev_' + f + str(i)] \/ data['AMT_CREDIT'])\n'''\n\n\ndata.head()","1fa6e1c2":"# BURO BALANCE\nbjoined = bureau_balance.merge(right=bureau[['AMT_CREDIT_SUM','SK_ID_BUREAU','SK_ID_CURR']].reset_index(), how='inner', on='SK_ID_BUREAU')\n#bjoined['AMT'] = bjoined['AMT_CREDIT_SUM']\nbjoined = bjoined.merge(right=app_train[['AMT_CREDIT','SK_ID_CURR']].reset_index(), how='inner', on='SK_ID_CURR')\nbjoined['AMT_WEIGHT'] = (bjoined['AMT_CREDIT_SUM'] \/ bjoined['AMT_CREDIT']).pow(.04).round(1)\n#app_train['SK_ID_CURR','AMT_CREDIT','AMT_INCOME_TOTAL']\nbpv = pd.pivot_table(bjoined[['SK_ID_CURR','AMT_WEIGHT','YEAR']][(bjoined.AMT_WEIGHT < 1.2) & (bjoined.AMT_WEIGHT > .8)],index=['SK_ID_CURR'], columns=['AMT_WEIGHT'], aggfunc=len, fill_value=0)\n#bpv.head(100)\n\nbjoined.head()\n","7249d120":"\n#bpv = pd.pivot_table(bjoined,index=['SK_ID_CURR'], columns=['YEAR','STATUS','AMT_WEIGHT'], aggfunc='count')\n\nbflattened = pd.DataFrame(bpv.to_records()).set_index(['SK_ID_CURR'])\n#bpv = None\n#pv.columns = pv.columns.to_series().str.join('_')\nbflattened.columns =  ['year_weight_table_' + col for col in bflattened.columns ]\n\nbflattened.head()\n","b1c8cfc5":"data = data.merge(right=bflattened.reset_index(), how='left', on='SK_ID_CURR')\ngc.collect()\ndata.head()\n","2cb593cf":"#bpv = pd.pivot_table(bjoined,index=['SK_ID_BUREAU','SK_ID_CURR'], columns=['YEAR','STATUS'], values=['AMT_CREDIT_SUM'], aggfunc=['sum'])\n#bjoined.head()\n\n\nbpv = pd.pivot_table(bjoined,index=['SK_ID_BUREAU','SK_ID_CURR'], columns=['YEAR','STATUS'],values='AMT_CREDIT_SUM', aggfunc='sum')\n#bjoined = None\n\n#pv.columns = (pv.columns.get_level_values(0)).join(pv.columns.get_level_values(1))\n#pv.columns = [' '.join(col).strip() for col in pv.columns.values]\n#pv.columns = [pv.columns.get_level_values(0) & '_' & pv.columns.get_level_values(1)]\n\nbflattened = pd.DataFrame(bpv.to_records()).set_index(['SK_ID_BUREAU','SK_ID_CURR'])\n#bpv = None\n#pv.columns = pv.columns.to_series().str.join('_')\nbflattened.columns =  ['year_' + col for col in bflattened.columns ]\n\n#bpv.head(10)\n\nbflattened.head(2)","9a91fe7b":"#year must more significant leaving epoch out for now\n#bpv = pd.pivot_table(bjoined,index=['SK_ID_BUREAU','SK_ID_CURR'], columns=['EPOCH','STATUS'], \\\n#                     values=['AMT_WEIGHT','AMT_CREDIT_SUM'], aggfunc=['sum'])\n\n#bflattened = bflattened.merge(right=pd.DataFrame(bpv.to_records()).reset_index(), how='left', on='SK_ID_BUREAU')\n#bflattened.columns =  ['epoch_' + col for col in bflattened.columns]\n\n#bflattened.head()\n","3ad5fe11":"bjoined = None\ngc.collect()\n\n#bpv = pd.pivot_table(bflattened,index='SK_ID_CURR', columns=bflattened.columns,values='AMT_CREDIT_SUM', aggfunc=['sum'])\nbbgrouped = bflattened.groupby('SK_ID_CURR').sum(min_count=1)\n#merge data\nbbgrouped.columns = ['bbpv_sum_' + hdr.replace(\"('\", \"_\").replace(\"',\", \"_\").replace(\")\", \"\") \\\n                     for hdr in bbgrouped.columns]\n\nbbgrouped.head()\n#bbgrouped.reset_index()\n\n#data = data.merge(right=bbgrouped.reset_index(), how='left',right_index=True,left_index=True)\ndata = data.merge(right=bbgrouped.reset_index(), how='left', on='SK_ID_CURR')\n\ndata.head()\n\n\n'''\n#repeat with max\nbbgrouped = bflattened.groupby('SK_ID_CURR').max() #min_count=1)\n#merge data\nbbgrouped.columns = ['bbpv_max_' + hdr.replace(\"('\", \"_\").replace(\"',\", \"_\").replace(\")\", \"\") \\\n                     for hdr in bbgrouped.columns]\nbbgrouped.head()\n\ndata = data.merge(right=bbgrouped.reset_index(), how='left', on='SK_ID_CURR')\n'''\n\n#bbgrouped.head()\ndata.head(2)\n\n\n","6195afe3":"bflattened = None\nbbgrouped = None\ngc.collect()\n\n\nbjoined = bureau.merge(right=app_train[['AMT_CREDIT','SK_ID_CURR']].reset_index(), how='inner', on='SK_ID_CURR')\nbjoined['AMT_WEIGHT'] = bjoined['AMT_CREDIT_SUM'] \/ bjoined['AMT_CREDIT']\n\nbjoined.head()\n\n","9af648ce":"\npv = pd.pivot_table(bjoined, index='SK_ID_CURR', columns=['CREDIT_ACTIVE','YEAR'], \\\n                    values=['AMT_WEIGHT', \\\n                            'AMT_CREDIT_MAX_OVERDUE', \\\n                            'DAYS_CREDIT_ENDDATE'],aggfunc='sum')\n\n'''\npv = pd.pivot_table(bjoined,index='SK_ID_CURR', columns=['CREDIT_ACTIVE','YEAR'], \\\n                    values=['AMT_WEIGHT','AMT_CREDIT_SUM', \\\n                            'AMT_CREDIT_MAX_OVERDUE','AMT_CREDIT_SUM_DEBT', \\\n                            'DAYS_CREDIT_ENDDATE','DAYS_CREDIT','DAYS_ENDDATE_FACT'],aggfunc=['sum'])\n'''\n\n\n\n#'AMT_CREDIT_SUM_OVERDUE','AMT_CREDIT_SUM_LIMIT','CREDIT_DAY_OVERDUE',\n\nflattened = pd.DataFrame(pv.to_records()).set_index('SK_ID_CURR')\nflattened.columns = ['bpv_' + hdr.replace(\"('\", \"_\").replace(\"',\", \"_\").replace(\")\", \"\") \\\n                     for hdr in flattened.columns]\npv = None\nflattened.head(10)\n","0d9a2f22":"bjoined = None\ngc.collect()\n\ndata.columns\n#merge data\ndata.set_index('SK_ID_CURR')\ndata = data.merge(right=flattened.reset_index(), how='left', on='SK_ID_CURR')\ndata.head()","d11b5b93":"\n\n# Average Values for all bureau features \nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n#fields = ['AMT_CREDIT_SUM','AMT_CREDIT_SUM_OVERDUE','AMT_CREDIT_MAX_OVERDUE','AMT_CREDIT_SUM_DEBT','AMT_CREDIT_SUM_LIMIT','CREDIT_DAY_OVERDUE','DAYS_CREDIT_ENDDATE','DAYS_CREDIT','DAYS_ENDDATE_FACT']\nfields = ['AMT_CREDIT_SUM','AMT_CREDIT_SUM_DEBT','CREDIT_DAY_OVERDUE','DAYS_CREDIT_ENDDATE','DAYS_CREDIT']\ndates = [300,2000]\nfor f in fields:\n    lasti = 0\n    for i in dates:\n        bureau_avg['buro_wind_'+ f + str(i)] = (bureau[(bureau.DAYS_CREDIT_ENDDATE < i) & (bureau.DAYS_CREDIT_ENDDATE > lasti )]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        bureau_avg['buro_'+ f + str(i)] = (bureau[bureau.DAYS_CREDIT_ENDDATE < i]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()        \n#        bureau_avg['buro_wind_has_'+ f + str(i)] = (np.isnan(bureau_avg['buro_wind_'+ f + str(i)])).astype(int)\n\n        \n        if lasti > 0:\n            bureau_avg['buro_derivitive_' + f + str(i)] = bureau_avg['buro_' + f + str(lasti)] \/ (bureau_avg['buro_' + f + str(i)]+ 0.0001)\n\n        lasti = i\n\n        \n        \nbureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\nbureau_avg.head()\n\n\ndata = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')\n\nfor f in fields:\n    for i in dates:\n        data['b_buro_weighted_' + f + str(i)] = (data['b_buro_' + f + str(i)] \/ data['AMT_CREDIT_x'])\n        \n#bureau2[bureau2.buro_bal_count>0].head(100)\n#bureau2.head(100)\n#bureau_avg.head()\ndata.head(10)\n        \n        \n","c1532f0e":"flattened = None\nbureau_avg = None\ngc.collect()\n\n## count the number of previous installments\ncnt_inst = installments_payments[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninstallments_payments['SK_ID_PREV'] = installments_payments['SK_ID_CURR'].map(cnt_inst['SK_ID_PREV'])\n\n\n## Average values for all other variables in installments payments\navg_inst = installments_payments.groupby('SK_ID_CURR').mean()\n\n'''\nfields = ['AMT_PAYMENT','AMT_INSTALMENT']\ndates = [-300,-900,-2700]\nfor f in fields:\n    lasti = 0\n    for i in dates:\n        avg_inst['s_'+ f + str(i)] = (installments_payments[(installments_payments.DAYS_INSTALMENT < i) ]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        \n        #if lasti > 0:\n            #pcb_avg['s_derivitive_' + f + str(i)] = pcb_avg['s_' + f + str(lasti)] \/ (pcb_avg['s_' + f + str(i)]+ 0.0001)\n        #lasti = i\n'''\n\navg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\ndata = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n","31d4af38":"cnt_inst = None\navg_inst = None\ngc.collect()\n\n### count the number of pos cash for a given ID\npcb_count = pcb[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\npcb['SK_ID_PREV'] = pcb['SK_ID_CURR'].map(pcb_count['SK_ID_PREV'])\n\n## Average Values for all other variables in pos cash\npcb_avg = pcb.groupby('SK_ID_CURR').mean()\n\nfields = ['CNT_INSTALMENT_FUTURE']\ndates = [-12,-36]\nfor f in fields:\n    lasti = 0\n    for i in dates:\n        pcb_avg['s_'+ f + str(i)] = (pcb[(pcb.MONTHS_BALANCE < i) ]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        \n        #if lasti > 0:\n            #pcb_avg['s_derivitive_' + f + str(i)] = pcb_avg['s_' + f + str(lasti)] \/ (pcb_avg['s_' + f + str(i)]+ 0.0001)\n        #lasti = i\n\n\ndata = data.merge(right=pcb_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n#add Credit weighted fields\nfor f in fields:\n    for i in dates:\n        data['pcb' + '_wg_' + f + str(i)] = (data['s_' + f + str(i)] \/ data['AMT_CREDIT_x'])\n\ndata.head()\n","7e1db680":"pcb_avg = None\ngc.collect()\n\n### count the number of previous applications for a given ID\n\nnb_prevs = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncredit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n\n### average of all other columns \navg_cc_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\n\n#fields = ['AMT_BALANCE','AMT_DRAWINGS_ATM_CURRENT','CNT_DRAWINGS_ATM_CURRENT']\nfields = ['AMT_BALANCE','AMT_DRAWINGS_ATM_CURRENT']\ndates = [-12,-36]\nfor f in fields:\n    lasti = 0\n    for i in dates:\n#        avg_cc_bal['w_'+ f + str(i)] = (credit_card_balance[(credit_card_balance.MONTHS_BALANCE < i) & (credit_card_balance.MONTHS_BALANCE > lasti)]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n#        avg_cc_bal['mx_'+ f + str(i)] = (credit_card_balance[(credit_card_balance.MONTHS_BALANCE < i) & (credit_card_balance.MONTHS_BALANCE > lasti)]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').max()\n#        avg_cc_bal['mn_'+ f + str(i)] = (credit_card_balance[(credit_card_balance.MONTHS_BALANCE < i) & (credit_card_balance.MONTHS_BALANCE > lasti)]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').min()\n        avg_cc_bal['s_'+ f + str(i)] = (credit_card_balance[(credit_card_balance.MONTHS_BALANCE < i) ]) [[f,'SK_ID_CURR']].groupby('SK_ID_CURR').sum()\n        \n        #if lasti > 0:\n            #avg_cc_bal['s_derivitive_' + f + str(i)] = avg_cc_bal['s_' + f + str(lasti)] \/ (avg_cc_bal['s_' + f + str(i)]+ 0.0001)\n        #lasti = i\n\navg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n\ndata = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n\nfor f in ['AMT_BALANCE']:  #other fields - weighting unimportant\n#    for p in ['cc_bal_w_','cc_bal_mx_','cc_bal_mn_','cc_bal_s_']:\n    for p in ['cc_bal_s_']:\n        for i in dates:\n            data[p + '_wg_' + f + str(i)] = (data[p + f + str(i)] \/ data['AMT_CREDIT_x'])\n\n\ndata.head(10)\n\navg_cc_bal.head()\n","5855dd6b":"nb_prevs = None\navg_cc_bal = None\ngc.collect()\n","2209be08":"\n\n#### prepare final Train X and Test X dataframes \nignore_features = ['SK_ID_CURR', 'is_train', 'is_test']\nrelevant_features = [col for col in data.columns if col not in ignore_features]\ntrainX = data[data['is_train'] == 1][relevant_features]\ntestX = data[data['is_test'] == 1][relevant_features]\n\n####Validation\nx_train, x_val, y_train, y_val = train_test_split(trainX, Y, test_size=0.2, random_state=18)\nlgb_train = lgb.Dataset(data=x_train, label=y_train)\nlgb_eval = lgb.Dataset(data=x_val, label=y_val)","d3cacdd5":"params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          'min_split_gain':.01, 'min_child_weight':1}\nmodel = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)","055707a8":"lgb.plot_importance(model, figsize=(12, 35), max_num_features=150);","8e750e4f":"preds = model.predict(testX)\nsub_lgb = pd.DataFrame()\nsub_lgb['SK_ID_CURR'] = test_id\nsub_lgb['TARGET'] = preds\nsub_lgb.to_csv(\"lgb_baseline.csv\", index=False)\nsub_lgb.head()","b14e15b9":"## <a id=\"7\">7. Exploration of Prev Application<\/a>\n\n### <a id=\"7.1\">7.1 Snapshot of Prev Application<\/a>","c6e4f8eb":"> Married people have applied for a larger number of loan applications about 196K, However, people having Civil Marriage has the highest percentage (about 10%) of loan problems and challenges. \n\n### <a href=\"2.5\">2.5. Does applicants own Real Estate or Car ?<\/a>","c7847951":"### <a id=\"2.17\">2.17 Distribution of Number of Days for Registration<\/a>","6cf9646e":"### <a id=\"9.6\">9.6 Feature Importance <\/a>","1c04ce6d":"> - A large number of people (about 62%) had their previous applications approved, while about 19% of them had cancelled and other 17% were resued. \n\n### <a id=\"7.3\">7.3 Suite Type Distribution of Previous Applications<\/a>","4faa61aa":"### <a id=\"2.15\">2.15 Distribution of Amount DAYS_BIRTH <\/a>","5f034106":"### <a id=\"2.10\">2.10 Walls Material, Foundation, and House Type <\/a>","72bddbf9":"### <a id=\"2.11\">2.11. Distribution of Amount Credit <\/a>","9169ee3f":"> - \"Blocks and Flats\" related house types have filed the largest number of loan applications equal to about 150K, rest of the other categories : Specific Housing and Terraced house have less than 1500 applications. Similarly houses having Panel and Stone Brick type walls material have filed the largest applciations close to 120K combined. \n\n### <a id=\"2.10.1\">2.10.1 Target Variable with respect to Walls Material, Fondkappremont, House Type <\/a>","eb151d75":"### <a id=\"9.7\">9.7 Predict<\/a>","d7fd5cb4":"> About 70% of the applicants own Real Estate, while only 34% of applicants own Car who had applied for the loan in the past years. However, a higher percentage of people having payment difficulties was observed with applicants which did not owned Car or which did not owned Real Estate. \n\n### <a href=\"2.6\">2.6 Suite Type and Income Type of Applicants <\/a>","89cccd92":"> In the applicant's data women have applied for a larger majority of loans which is almost the double as the men. In total, there are about 202,448 loan applications filed by females in contrast to about 105,059 applications filed by males. However, a larger percentage (about 10% of the total) of men had the problems in paying the loan or making installments within time as compared to women applicants (about 7%). \n\n### <a href=\"2.4\">2.4 Family Status of Applicants <\/a>","d716c6a4":"# Home Credit Default Risk - Exploration + Baseline Model\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nThis is a simple notebook on exploration and baseline model of home credit default risk data \n\n**Contents**   \n[1. Dataset Preparation](#1)    \n[2. Exploration - Applications Train](#2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Snapshot - Application Train](#2.1)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Distribution of Target Variable](#2.2)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.3 Applicant's Gender Type](#2.3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.4 Family Status of Applicants who takes the loan](#2.4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.5 Does applicants own Real Estate or Car](#2.5)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.6 Suite Type and Income Type of Applicants](#2.6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.7 Applicants Contract Type](#2.7)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.8 Education Type and Occupation Type](#2.8)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.9 Organization Type and Occupation Type](#2.9)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.10 Walls Material, Foundation and House Type](#2.10)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.11 Amount Credit Distribution](#2.11)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.12 Amount Annuity Distribution - Distribution](#2.12)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.13 Amount Goods Price - Distribution](#2.13)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.14 Amount Region Population Relative](#2.14)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.15 Days Birth - Distribution](#2.15)   \n&nbsp;&nbsp;&nbsp;&nbsp; [2.16 Days Employed - Distribution](#2.16)    \n&nbsp;&nbsp;&nbsp;&nbsp; [2.17 Distribution of Num Days Registration](#2.17)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.18 Applicants Number of Family Members](#2.18)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.19 Applicants Number of Children](#2.19)  \n[3. Exploration - Bureau Data](#3)  \n&nbsp;&nbsp;&nbsp;&nbsp; [3.1 Snapshot - Bureau Data](#3)    \n[4. Exploration - Bureau Balance Data](#4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [4.1 Snapshot - Bureau Balance Data](#3)     \n[5. Exploration - Credit Card Balance Data](#5)   \n&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Snapshot - Credit Card Balance Data](#3)   \n[6. Exploration - POS Cash Balance Data](#6)   \n&nbsp;&nbsp;&nbsp;&nbsp; [6.1 Snapshot - POS Cash Balance Data](#3)   \n[7. Exploration - Previous Application Data](#7)   \n&nbsp;&nbsp;&nbsp;&nbsp; [7.1 Snapshot - Previous Application Data](#7.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [7.2 Contract Status Distribution - Previous Applications](#7.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [7.3 Suite Type Distribution - Previous Application](#7.3)    \n&nbsp;&nbsp;&nbsp;&nbsp; [7.4 Client Type Distribution  - Previous Application](#7.4)    \n&nbsp;&nbsp;&nbsp;&nbsp; [7.5 Channel Type Distribution - Previous Applications](#7.5)  \n[8. Exploration - Installation Payments](#8)  \n&nbsp;&nbsp;&nbsp;&nbsp; [8.1 Snapshot of Installation Payments](#3)  \n[9. Baseline Model](#9)  \n&nbsp;&nbsp;&nbsp;&nbsp; [9.1 Dataset Preparation](#9.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [9.2 Handelling Categorical Features](#9.2)     \n&nbsp;&nbsp;&nbsp;&nbsp; [9.3 Create Flat Dataset](#9.3)     \n&nbsp;&nbsp;&nbsp;&nbsp; [9.4 Validation Sets Preparation](#9.4)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.5 Model Fitting](#9.5)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.6 Feature Importance](#9.6)    \n&nbsp;&nbsp;&nbsp;&nbsp; [9.7 Prediction](#9.7)   \n\n\n\n## <a id=\"1\">1. Dataset Preparation <\/a>","bcd88c1f":"> We see that Applicants having Income Types : Maternity Leaves and UnEmployed has the highest percentage (about 40% and 36% approx) of Target = 1 ie. having more payment problems, while Pensioners have the least (about 5.3%). \n\n### <a id=\"2.7\">2.7. Applicant's Contract Type<\/a>","0492680f":"### <a id=\"2.14\">2.14 Distribution of Amount REGION_POPULATION_RELATIVE <\/a>","3436abf2":"> The target variable is slightly imbalance with the majority of loans has the target equals to 0 which indicates that individuals did not had any problems in paying installments in given time. There are about 91% loans which is equal to about 282K with target = 0, While only 9% of the total loans (about 24K applicants) in this dataset involved the applicants having problems in repaying the loan \/ making installments.  \n\n### <a href=\"2.3\">2.3 Gender Type of Applicants <\/a>","b832753c":"> - About 74% of the previous applications were Repeater Clients, while only 18% are new. About 8% are refreshed. \n\n### <a id=\"7.5\">7.5 Channel Type - Previous Applications <\/a>","db414807":"> Cash loans with about 278K loans contributes to a majorty of total lonas in this dataset. Revolving loans has significantly lesser number equal to about 29K as compared to Cash loans. \n\n### <a id=\"2.8\">2.8 Education Type and Housing Type <\/a>","4719e86b":"## <a href=\"2\">2. Exploration of Applications Data <\/a>\n\n### <a href=\"2.1\">2.1 Snapshot of Application Train<\/a>\n\nApplication data consists of static data for all applications and every row represents one loan.","d2e602a7":"### <a id=\"9.5\">9.5 Fit the Model<\/a>","fb78c6e1":"> - A majority of applicants had previous applications having Unaccompanied Suite Type (about 60%) followed by Family related suite type (about 25%)\n\n### <a id=\"7.4\">7.4 Client Type of Previous Applications<\/a>","5a812d80":"> There are total 307,511 rows which contains the information of loans and there are 122 variables. \n\n> The target variable defines if the client had payment difficulties meaning he\/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.\n\n### <a href=\"2.2\">2.2 Distribution of Target Variable <\/a>\n\nThe target variable defines weather the loan was repayed or not. Let us look at what is the distribution of loan repayment in the training dataset. ","9a2ebd9e":"> Top 3 Type Suites which applies for loan are the houses which are:  \n    - Unaccompanined (about 248K applicants) \n    - Family (about 40K applicants)  \n    - Spouse, partner (about 11K applicants)    \n> The income type of people who applies for loan include about 8 categroes, top ones are : \n    - Working Class (158K)\n    - Commercial Associate (71K)\n    - Pensiner (55K)\n\n### <a id=\"2.6.1\">2.6.1 How does Target Varies with Suite and Income Type of Applicants <\/a>","f867b5e0":"### <a id=\"2.9\">2.9. Which Organization and Occupation Type applies for loan and which repays <\/a>","1c558cfc":"### <a id=\"2.13\">2.13 Distribution of Amount AMT_GOODS_PRICE <\/a>","373912c2":"### <a id=\"7.2\">7.2 Contract Status Distribution in Previously Filed Applications<\/a>","a4100102":"## <a id=\"5\">5. Exploration of Credit Card Balance<\/a>\n\nMonthly balance snapshots of previous credit cards that the applicant has with Home Credit.This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n\n### <a id=\"5.1\">5.1 Snapshot of Credit Card Balance<\/a>","15a76c4f":"> A large majority of applicants did not had children when they applied for loan\n\n## <a id=\"3\">3. Exploration of Bureau Data<\/a>\n\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\n### <a id=\"3.1\">3.1 Snapshot of Bureau Data<\/a>","34c8b026":"### <a id=\"2.18\">2.18 How many Family Members does the applicants has <\/a>","d41d743b":"> Top Applicant's who applied for loan : Laborers - Approx 55 K, Sales Staff - Approx 32 K, Core staff - Approx 28 K. Entity Type 3 type organizations have filed maximum number of loans equal to approx 67K\n\n### <a id=\"2.9.1\">2.9.1 Target Variable with respect to Organization and Occupation Type <\/a>","16faff1a":"### <a id=\"9.2\">9.2 Handelling Categorical Features<\/a>","a017038b":"### <a id=\"9.3\">9.3 Feature Engineering<\/a>","1c67af40":"## <a id=\"4\">4. Exploration of Bureau Balance Data<\/a>\n\nMonthly balances of previous credits in Credit Bureau. This table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n\n### <a id=\"4.1\">4.1 Snapshot of Bureau Balance Data<\/a>","5d19dcb3":"### <a id=\"9.3.5\">9.3.5 Feature Engineering - Credit Card Balance <\/a>","a79ec15f":"## <a id=\"6\">6. Exploration of POS CASH Balance Data<\/a>\n\nMonthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit. This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n\n### <a id=\"6.1\">6.1 Snapshot of POS CASH Balance Data<\/a>","9cea40a3":"## <a id=\"8\">8. Exploration of Installation Payments <\/a>\n### <a id=\"8.1\">8.1 Snapshot of Installation Payments <\/a>","494a2e78":"### <a id=\"9.4\">9.4 Create Validation Sets<\/a>","30559af5":"### <a id=\"2.12\">2.12 Distribution of Amount AMT_ANNUITY <\/a>","5a017d4b":"### <a id=\"2.16\">2.16 Distribution of Amount DAYS_EMPLOYED <\/a>","8c95133f":"### <a id=\"9.3.2\">9.3.2 Feature Engineering - Bureau Data<\/a>","d2faa453":"> Most of the applicants who applied for loan had 2 family members in total\n\n### <a id=\"2.19\"> 2.19 How many Children does the applicants have <\/a>","d53f8247":"### <a id=\"9.3.6\">9.3.6 Prepare Final Train and Test data<\/a>","f15d678a":"### <a id=\"9.3.1\">9.3.1 Feature Engineering - Previous Applications<\/a>\n\nCredits to excellent kernel shared by Olivier for more ideas: https:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm \n","e4b41654":"### <a id=\"9.3.4\">9.3.4 Feature Engineering - Pos Cash Balance<\/a>","30ab15f7":"## <a id=\"9\">9. Baseline Model <\/a>\n\n### <a id=\"9.1\">9.1 Dataset Preparation<\/a>","669f4ade":"> A large number of applications (218K) are filed by people having secondary education followed by people with Higher Education with 75K applications. Applicants living in House \/ apartments has the highest number of loan apllications equal to 272K. While we see that the applicants with Lower Secondary education status has the highest percentage of payment related problems. Also, Applicants living in apartments or living with parents also shows the same trend. ","0a139029":"### <a id=\"9.3.3\">9.3.3 Feature Engineering - Previous Installments<\/a>","97f83275":"Thanks for viewing. "}}