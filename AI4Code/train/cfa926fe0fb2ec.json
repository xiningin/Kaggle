{"cell_type":{"94f85de6":"code","514c4deb":"code","dced75b1":"code","c3a81d54":"code","b137b0bf":"code","f6f47568":"code","3e88ad82":"code","5c8ff511":"code","c42de13b":"code","cf91a124":"code","2ab88af9":"code","1d5cbd79":"code","02b6f59b":"code","f3d5e37e":"markdown","0e2a88cf":"markdown","46cccc2e":"markdown","94027ca3":"markdown","6f124f7b":"markdown","036a1a20":"markdown","48ea467d":"markdown","37a0eb44":"markdown","2b223f78":"markdown","66770c4b":"markdown"},"source":{"94f85de6":"from IPython.core.display import display, HTML, Javascript\ndef stylize():\n    syl = open(\"..\/input\/notebookassets\/custom.css\").read()\n    return HTML(\"<style>\"+syl+\"<\/style>\")\nstylize()","514c4deb":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\n\nfrom IPython import display","dced75b1":"data = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ndata.head()","c3a81d54":"def plot_results(train_acc, valid_acc, train_loss, valid_loss):\n    epochs = [i for i in range(len(train_acc))]\n    \n    fig, ax = plt.subplots(1, 2)\n    fig.set_size_inches(20, 10)\n    \n    ax[0].plot(epochs, train_acc, 'go-', label='Training Accuracy')\n    ax[0].plot(epochs, valid_acc, 'ro-', label='Validation Accuracy')\n    ax[0].set_title('Training & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Accuracy')\n    \n    ax[1].plot(epochs, train_loss, 'go-', label='Training Loss')\n    ax[1].plot(epochs, valid_loss, 'ro-', label='Validation Loss')\n    ax[1].set_title('Training & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Loss')\n    \n    plt.show()","b137b0bf":"train_data = data[5000:]\nvalid_data = data[:5000]\nprint(train_data.shape, valid_data.shape)","f6f47568":"def format_data(train_file):\n    \"\"\"\n    Reads the raw .csv file and returns reshaped images and categorical features\n    \"\"\"\n    images = []\n    raw_images = train_file.drop(['label'], axis=1).values\n    for current_image_idx in tqdm(range(len(raw_images))):\n        images.append(raw_images[current_image_idx].reshape(28,28))    \n    images = np.array(images, np.float32).reshape(-1, 28, 28, 1)\n    labels = tf.keras.utils.to_categorical(train_file['label'].values)\n    del raw_images, current_image_idx\n    gc.collect()\n    assert images.shape[0] == labels.shape[0]\n    return images, labels","3e88ad82":"# Get both training and validation image splits\ntrainX, trainY = format_data(train_data)\nvalidX, validY = format_data(valid_data)","5c8ff511":"# Convert to tensorflow datasets\n@tf.function\ndef to_tfds(train_files: tuple, valid_files: tuple, batch_size: int):\n    \"\"\"\n    Takes a tuple of Both Training and Validation Files { ie:(X, Y) }\n    and returns the tensorflow data equivalent of them.\n    \"\"\"\n    train_ds = tf.data.Dataset.from_tensor_slices(train_files).batch(batch_size)\n    valid_ds = tf.data.Dataset.from_tensor_slices(valid_files).batch(batch_size)\n    return train_ds, valid_ds\ntrain, valid = to_tfds((trainX, trainY), (validX, validY), batch_size=256)","c42de13b":"def get_model(input_shape, num_classes):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu', input_shape=input_shape),\n        tf.keras.layers.MaxPooling2D(pool_size=2),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(32,activation='relu'),\n        tf.keras.layers.Dense(10,activation = 'softmax')\n    ])\n    return model","cf91a124":"# Define Optimizer and Loss functions, accuracy and epochs\nEPOCHS = 50\noptimizer = tf.keras.optimizers.Adam()\ntrain_loss = tf.keras.losses.CategoricalCrossentropy()\nvalid_loss = tf.keras.losses.CategoricalCrossentropy()\n\ntrain_acc_metric = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\nvalid_acc_metric = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n\nmetrics = [train_acc_metric, valid_acc_metric]\nlosses = [train_loss, valid_loss]","2ab88af9":"# Code for One Loop of training and validation\n@tf.function\ndef train_one_epoch(model, optimizer, train_ds, loss_fn, acc_metric, device='\/cpu:0'):\n    \"\"\"\n    Trains the model for one epoch on training data\n    \"\"\"\n    step = 0\n    loss, predictions = 1.0, 1.0\n    with tf.device(device_name=device):\n        # Using the device\n        for img, label in train_ds:\n            # Iterate through the batches in training dataset\n            step += 1\n            # Increment the batch count\n            with tf.GradientTape() as tape:\n                # Use the Gradient Tape. It is used when we want tensorflow to watch the operations that we will be getting gradients of.\n                predictions = model(img)\n                # Get the predictions for the current batch of images\n                loss = loss_fn(label, predictions)\n                # Calculate the loss of the current predictions w.r.t ground truth labels.\n            grads = tape.gradient(loss, model.trainable_variables)\n            # Now use the \"tape.gradient()\" function to find out the differential (gradient) of loss with respect to weights in the model\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n            # Now apply those gradients to the respective weights in the model\n            acc_metric(label, predictions)\n            # Update the accuracy metric to keep in the check the accuracy score. \n        return loss\n@tf.function\ndef valid_one_epoch(model, valid_ds, loss_fn, acc_metric, device='\/cpu:0'):\n    \"\"\"\n    Validates the model for one epoch on validation data\n    \"\"\"\n    # We are not doing any of the gradient stuf here since we don't want our model to learn from the validation data.\n    loss, predictions = 1.0, 1.0\n    with tf.device(device_name=device):\n        for img, label in valid_ds:\n            predictions = model(img)\n            loss = loss_fn(label, predictions)\n            acc_metric(label, predictions)\n        return loss","1d5cbd79":"# Final Training Function which wraps both of them\ndef train(model,\n          optimizer=optimizer,\n          train_ds=train,\n          valid_ds=valid,\n          epochs=EPOCHS,\n          metrics=metrics,\n          losses=losses):\n    \"\"\"\n    This function puts together everything else and executes the main training and validation code.\n    \"\"\"\n    # I will be maintaining a list of all training accuracies, validation accuracies, training losses and validation losses to plot them later.\n    all_accs_tr = []\n    all_accs_va = []\n    \n    all_loss_tr = []\n    all_loss_va = []\n    # Now I will run the loop for the required number of epochs and call the training and validation functions everytime.\n    for epoch in range(epochs):\n        display.clear_output(wait=True)\n        print(f\"{'-'*10} EPOCH: {epoch+1} \/ {epochs} {'-'*10}\")\n        \n        tr_lss = train_one_epoch(\n            model=model, \n            optimizer=optimizer, \n            train_ds=train_ds, \n            loss_fn=losses[0], \n            acc_metric=metrics[0]\n        )\n        \n        va_lss = valid_one_epoch(\n            model=model,  \n            valid_ds=valid_ds, \n            loss_fn=losses[1], \n            acc_metric=metrics[1]\n        )\n        \n        # metrics[0] has the training accuracy metrics\n        # metrics[1] has the validation accuracy metrics\n        all_accs_tr.append(metrics[0].result())\n        all_accs_va.append(metrics[1].result())\n        \n        all_loss_tr.append(tr_lss.numpy())\n        all_loss_va.append(va_lss.numpy())\n        \n        # Just print these to check model performance\n        print(f\"train_loss: {tr_lss}, train_acc: {metrics[0].result()}\\n\")\n        print(f\"valid_loss: {va_lss}, valid_acc: {metrics[1].result()}\\n\")\n        \n        # Also plotting the training and validation accuracies and losses after every epoch (with all values from last all epochs)\n        plot_results(all_accs_tr, all_accs_va, all_loss_tr, all_loss_va)\n        # We will wait for 7 seconds to see the graph and after that it will be cleared.\n        # This way you can have dynamic graphs.\n        time.sleep(7)","02b6f59b":"# Initialize the model and train!\nmodel = get_model(input_shape=[28, 28, 1], num_classes=10)\ntrain(model)","f3d5e37e":"Let's also define the epochs, optimizer, training loss, validation loss, train accuracy metric and validation accuracy metric.","0e2a88cf":"Little Helper function to plot training and validation accuracy along with losses.","46cccc2e":"So now in the below training function, I will just run both functions for any number of epochs and it will train the model.\n\n**Note: I have not used the `@tf.function` decorator in this final function because I have used that in all of the smaller functions that will be called here.**","94027ca3":"Split the data into Training and Validation sets.","6f124f7b":"A little helper function that will format the images from a `784` length vectors to `28x28` images.","036a1a20":"I have included a lot of comments in the code cells itself to make this as simple to understand as possible.\nIf you still have any doubts, please ask them in the comment section down below!","48ea467d":"# Using Tensorflow Autograph to write Custom Training Code\n\n\nWe have all used Tensorflow's Functional and Sequential API to make models, but when it comes to training them, the only faithful function we ever remember to call is, `model.fit()`!\n\nAlthough it's the easiest and the most straight-forward thing to do when you are training a model, there are more sophisticated ways of training and validating your model that resemble the way you do the same thing in pytorch.\n\nThat is done by using Tensorflow's Autograph. \n\n## But what is Autograph?\n\nAutoGraph converts Python code, including control flow, print() and other Python-native features, into pure TensorFlow graph code. Writing TensorFlow code without using eager execution requires you to do a little metaprogramming \u2014 you write a program that creates a graph, and then that graph is executed later. This can be confusing, especially for new developers. \n\nSome especially tricky situations involve more complex models such as ones that use if and while, or ones that have side effects like print(), or accept structured input.","37a0eb44":"## Using @tf.function\n\nOkay, so you might be now wondering why have I used this `@tf.function`? Well this decorator is used to tell tensorflow that you want this function to get compiled by Tensorflow Autograph.\n\n**For every function that is directly related to the workings of model (like the blow dataset function), you need to have it decorated with `@tf.function`.**","2b223f78":"A simple model will work for now.","66770c4b":"## Training and Validation Functions\n\nBelow you see 2 functions decorated by `@tf.function`;\n1. `train_one_epoch()`: This function is used to iterate through all the batches in training dataset, store the predictions from the model, calculate the training loss using the appropriate loss function. The calculate the graidents of loss with respect to all the weights in the model (using `tf.GradientTape()`) and then apply those gradients to the optimizer.\n\n2. `valid_one_epoch()`: This function is fairly straight-forward, all it does is, iterate through the validation dataset, get the predictions and calculate the loss value.\n\nBoth functions return their respective loss values so we can plot and show them later."}}