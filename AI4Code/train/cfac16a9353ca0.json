{"cell_type":{"c451d6b3":"code","b9cb18a3":"code","3cbe2191":"code","ffab5bd5":"code","ae53a78a":"code","1288f541":"code","c77ee5cd":"code","5ad872f9":"code","8cc19d45":"code","9b2c8a8a":"code","adf2a9ad":"code","531ba0b0":"code","607cd199":"code","106b5f75":"code","7653e619":"code","34d413e8":"code","83dd4226":"code","0575c7d7":"code","5410a0da":"code","18d6c6d8":"code","e4320262":"code","e8d7fe4d":"code","aa729225":"code","7359b5e7":"code","c98225e8":"code","7a35ca0e":"code","c1366edd":"code","8c190ad3":"code","53538156":"code","e548d8bc":"code","7ab999f5":"code","4f90b539":"code","59d2dfef":"code","d2326d25":"code","606d1c68":"code","09a56b00":"code","35cd9f65":"markdown","216591d0":"markdown","97f741c8":"markdown","79150ae7":"markdown","b2962cb4":"markdown","ab6dddab":"markdown","b3834389":"markdown","3757e0da":"markdown","58b7ff53":"markdown","ab81014b":"markdown","c83c2a71":"markdown","723c2f1a":"markdown","1593390a":"markdown","34c9cf5a":"markdown","e941240a":"markdown","2be9cd9a":"markdown","b57a0e8f":"markdown","6422e7ce":"markdown","13b16cae":"markdown","60daa690":"markdown","ac64260e":"markdown","0461e28b":"markdown","f80615b4":"markdown"},"source":{"c451d6b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy\nimport scipy.sparse\nimport networkx as nx\nfrom sklearn import preprocessing\n\n# plot with matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from plotnine import * # used to plot data\n\n# progress bar\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9cb18a3":"marker_presence_matrix_file=\"\/kaggle\/input\/metagenomics-marker-presence-sparse-matrix\/marker_presence_matrix.npz\"\nmarkers2clades_DB_file=\"\/kaggle\/input\/human-metagenomics\/markers2clades_DB.csv\"\nabundance_file=\"\/kaggle\/input\/human-metagenomics\/abundance.csv\"\nmarker_presence_table_file=\"\/kaggle\/input\/human-metagenomics\/marker_presence.csv\"","3cbe2191":"%%time\n\nsamples_df = pd.read_csv(abundance_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","ffab5bd5":"%%time\nif 1 == 0:\n    samples_df2 = pd.read_csv(marker_presence_table_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","ae53a78a":"if 1 == 0:\n    samples_df.compare(samples_df2, align_axis=0)","1288f541":"samples_df.describe()","c77ee5cd":"samples_df.query('dataset_name in [\"t2dmeta_long\",\"t2dmeta_short\"]')['disease'].unique()","5ad872f9":"samples_df = samples_df.loc[:, samples_df.nunique() > 1].copy()","8cc19d45":"if 1 == 0:\n    for col in samples_df.loc[:, samples_df.nunique() < 20]:\n        print(\"%s:%i\" % (col,samples_df[col].nunique()))\n        print(samples_df[col].unique())\n        print(\"\")","9b2c8a8a":"samples_df = samples_df.replace(\"nd\", np.NaN)\nsamples_df = samples_df.replace(\"na\", np.NaN)\nsamples_df = samples_df.replace(\"-\", np.NaN)\nsamples_df = samples_df.replace(' -', np.NaN)\nsamples_df = samples_df.replace('unknown', np.NaN)","adf2a9ad":"# change the if statement to visualize\nif 1==0:\n    for col in samples_df.loc[:, samples_df.nunique() == 1].columns:\n        samples_df[col].fillna(\"NaN\").value_counts().sort_values().plot(\n            kind = 'bar', title=col)\n        plt.show()\n        \nsamples_df = samples_df.loc[:, samples_df.nunique() > 1].copy()","531ba0b0":"bool_vals={'True':2,\n          'False':1,\n          'Null':0}\nfor col in samples_df.loc[:, samples_df.nunique() < 4]:\n    if (\"yes\" in samples_df[col].unique() and \"no\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'yes': bool_vals['True'], 'no': bool_vals['False']}})\n    elif (\"y\" in samples_df[col].unique() and \"n\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'y': bool_vals['True'], 'n': bool_vals['False']}})\n    elif (\"positive\" in samples_df[col].unique() and \"negative\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'positive': bool_vals['True'], 'negative': bool_vals['False']}})\n    elif (\"a\" in samples_df[col].unique() and \"u\" in samples_df[col].unique()):\n            samples_df[col] = samples_df[col].fillna(bool_vals['Null'])\n            samples_df =samples_df.replace({col: {'a': bool_vals['True'], 'u': bool_vals['False']}})","607cd199":"for col in samples_df.loc[:, samples_df.nunique() == 2].columns:\n    if (not(True in samples_df[col].unique() and \n             False in samples_df[col].unique())):\n        val_i = 0\n        first_val_null=True\n        first_val = np.NaN\n        while (first_val_null):\n            first_val = samples_df[col].unique()[val_i]\n            if first_val == first_val:\n                first_val_null = False\n            else:\n                val_i += 1\n        val_i += 1\n        second_val_null=True\n        second_val= np.NaN\n        while (second_val_null):\n            second_val = samples_df[col].unique()[val_i]\n            if second_val == second_val:\n                second_val_null = False\n            else:\n                val_i += 1\n        new_col_name=(\"%s:%s|%s\" % (col,first_val, second_val))\n        # change the column name\n        samples_df = (samples_df.rename(\n            columns={col:new_col_name}))\n        # change values in the column\n        samples_df[new_col_name] = samples_df[new_col_name].fillna(bool_vals['Null'])\n        samples_df =samples_df.replace({new_col_name: {first_val: bool_vals['False'],\n                                                       second_val: bool_vals['True']}})\ncategorical_cols=samples_df.loc[:, samples_df.nunique() < 20].columns","106b5f75":"samples_df['bodysite'].value_counts().plot(kind='bar')","7653e619":"samples_df['bodysite'] == 'stool'\nprint(np.sum(samples_df.nunique() < 3))","34d413e8":"stool_samp_df = samples_df.loc[samples_df['bodysite'] == 'stool',:].copy()\n","83dd4226":"%%time\n\nabundance_df = (pd.read_csv(abundance_file,sep=\",\", dtype=object)\n               .iloc[:,211:])\nabundance_df.head()","0575c7d7":"#seen_list=[]\nredundant_dict={}\nremove_cols=[]\ni=0\nwhile i < abundance_df.shape[1]:\n    j=i+1\n    next_step=abundance_df.shape[1]\n    while j < abundance_df.shape[1]:\n        #print(\"%i,%i\" % (i,j))\n        col_i = abundance_df.columns[i]\n        col_j = abundance_df.columns[j]\n        if col_i in col_j:\n            #print(abundance_df.iloc[:,i].equals(abundance_df.iloc[:,j]))\n            if abundance_df.iloc[:,i].equals(abundance_df.iloc[:,j]):\n                # add redundant column name to data-structure\n                remove_cols.append(col_j)\n                if col_i in redundant_dict: redundant_dict[col_i].append(col_j)\n                else: redundant_dict[col_i]= [col_j]\n                # next look at i vs j+1\n                \n            else:\n                #print(\"next_step: \"+ str(next_step))\n                if next_step > j:\n                    next_step=j\n            \n            j += 1\n            if j == abundance_df.shape[1]:\n                i = j\n        else:\n            if next_step < j:\n                i=next_step\n                next_step=abundance_df.shape[1]\n            else:\n                i=j\n            j=abundance_df.shape[1]","5410a0da":"abundance_df = (\n        abundance_df.drop(\n            remove_cols,\n            axis=1))","18d6c6d8":"print(len(remove_cols))","e4320262":"# Note: to get the full abudance file back with cleaning using the following code:\nif 1==0:\n    c = samples_df.merge(abundance_df, how='left',\n                               left_index=True, right_index=True)","e8d7fe4d":"samples_df['dataset_name']","aa729225":"%%time \n\nmarkers_reader = pd.read_csv(\n        marker_presence_table_file,\n        sep=\",\", \n        dtype=object,\n        usecols=range(211,288558),\n        nrows=10)","7359b5e7":"markers_reader","c98225e8":"def graph_label(samples_df,abundance_df,dataset=None):\n    if dataset:\n        dataset = dataset if isinstance(dataset,list) else [dataset]\n        ids = samples_df['dataset_name'].isin(dataset)\n        samples_df = samples_df[ids].reset_index(drop=False)\n        abundance_df = abundance_df[ids].reset_index(drop=False)\n    le = preprocessing.LabelEncoder()\n    # target values\n    y = le.fit_transform(samples_df['disease'])\n    # get emnedding of all nodes\n    le_nodes = preprocessing.LabelEncoder()\n    # encode labels between 0 and n_classes-1 for each bacterial label\n    le_nodes.fit([gene for col in abundance_df.columns for gene in col.split('|')])\n    max_id = np.max(le_nodes.transform([gene for col in abundance_df.columns for gene in col.split('|')]))\n    data_list = []\n    for i in range(len(abundance_df)):\n        node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n        edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n        for key, val in abundance_df.iloc[i].to_dict().items():\n            if float(val) > 0:\n                bacteria_list = key.split('|')\n                node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)]\n                node_list.append(node)\n                if len(bacteria_list) >= 2:\n                    edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n        # convert `y`, `node_list`, and `edge_list` into Tensor formats\n        edge_array = np.array(edge_list)\n        \n        edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n        #print(np.array(node_list))\n        node_features = torch.LongTensor(np.array(node_list))\n        label = torch.FloatTensor([y[i]])\n        # set these Tensors into a pytorch Data() object\n        # which is used to model graphs\n        data = Data(node_features,edge_index=edge_index,y=label)\n        data_list.append(data)\n    return data_list,max_id","7a35ca0e":"import torch; print(torch.__version__)","c1366edd":"print(torch.version.cuda)","8c190ad3":"!pip install torch-scatter -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+10.2.html\n!pip install torch-sparse -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+10.2.html\n!pip install torch-cluster -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+10.2.html\n!pip install torch-spline-conv -f https:\/\/pytorch-geometric.com\/whl\/torch-1.7.0+10.2.html\n!pip install torch-geometric","53538156":"from torch_geometric.utils.convert import from_networkx\nfrom torch_geometric.data import InMemoryDataset\nfrom torch_geometric.data import Data","e548d8bc":"\"\"\"\nclass AbundanceDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None,dataset = None):\n        super(AbundanceDataset, self).__init__(root, transform, pre_transform)\n        self.dataset = dataset\n        self.data, self.slices = torch.load(self.processed_paths[0])\n    @property\n    def raw_file_names(self):\n        return []\n    @property\n    def processed_file_names(self):\n        return ['..\/input\/yoochoose_click_binary_1M_sess.dataset']\n\n    def download(self):\n        pass\n    def process(self):\n        data_list = []\n        graph_label_pair = graph_label()\n        for G,value in zip(graph_label_pair['graph'],graph_label_pair['value']):\n            data = from_networkx(G)\n            data.y = torch.float\n            data_list.append(data)\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n        \n\"\"\"","7ab999f5":"from torch_geometric.data import DataLoader\nfrom torch_geometric.nn import GINConv, global_add_pool","4f90b539":"'''\ndata_list = []\ngraph_label_pair = graph_label(samples_df,abundance_df,dataset = [\"t2dmeta_long\",\"t2dmeta_short\"])\nfor G,value in zip(graph_label_pair['graph'],graph_label_pair['value']):\n    print(G)\n    data = from_networkx(G)\n    data.y = torch.float(value)\n    data_list.append(data)\n'''\nt2dml_samples_df = samples_df.loc[samples_df['dataset_name']==\"t2dmeta_long\",:].copy()\nt2dml_abundance_values_df = abundance_df.iloc[(list(t2dml_samples_df.index))]\n# merge meta-data features with abundance features\nt2dml_abundance_df = t2dml_samples_df.merge(abundance_df, how='inner',left_index=True, right_index=True)\ndata_list,max_id = graph_label(t2dml_samples_df,t2dml_abundance_values_df)\nprint(max_id)\ntrain_datalist = data_list[len(data_list) \/\/ 10:]\ntest_datalist = data_list[:len(data_list) \/\/ 10]\n\ntrain_loader = DataLoader(train_datalist, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_datalist, batch_size=4)","59d2dfef":"embed_dim = 32\nCUDA_LAUNCH_BLOCKING=1\nfrom torch.nn import Sequential as Seq, Linear, ReLU\nfrom torch_geometric.utils import remove_self_loops, add_self_loops\nfrom torch_geometric.nn import TopKPooling,MessagePassing\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nimport torch.nn.functional as F\n\nclass SAGEConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n        self.lin = torch.nn.Linear(in_channels, out_channels)\n        self.act = torch.nn.ReLU()\n        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n        self.update_act = torch.nn.ReLU()\n        \n    def forward(self, x, edge_index):\n        # x has shape [N, in_channels]\n        # edge_index has shape [2, E]\n        \n        \n        edge_index, _ = remove_self_loops(edge_index)\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        \n        \n        return self.propagate(edge_index,x=x)\n\n    def message(self, x_j):\n        # x_j has shape [E, in_channels]\n\n        x_j = self.lin(x_j)\n        x_j = self.act(x_j)\n        \n        return x_j\n\n    def update(self, aggr_out, x):\n        # aggr_out has shape [N, out_channels]\n\n\n        new_embedding = torch.cat([aggr_out, x], dim=1)\n        \n        new_embedding = self.update_lin(new_embedding)\n        new_embedding = self.update_act(new_embedding)\n        \n        return new_embedding\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = SAGEConv(embed_dim, embed_dim)\n        self.pool1 = TopKPooling(embed_dim, ratio=0.8)\n        self.conv2 = SAGEConv(embed_dim, embed_dim)\n        self.pool2 = TopKPooling(embed_dim, ratio=0.8)\n        self.conv3 = SAGEConv(embed_dim, embed_dim)\n        self.pool3 = TopKPooling(embed_dim, ratio=0.8)\n        self.item_embedding = torch.nn.Embedding(num_embeddings=max_id +1, embedding_dim=embed_dim)\n        self.lin1 = torch.nn.Linear(embed_dim * 2, embed_dim)\n        self.lin2 = torch.nn.Linear(embed_dim, int(embed_dim \/ 2))\n        self.lin3 = torch.nn.Linear(int(embed_dim \/ 2), 1)\n        self.bn1 = torch.nn.BatchNorm1d(embed_dim)\n        self.bn2 = torch.nn.BatchNorm1d(int(embed_dim \/ 2))\n        self.act1 = torch.nn.ReLU()\n        self.act2 = torch.nn.ReLU()        \n  \n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.item_embedding(x)\n        x = x.squeeze(1)        \n\n        x = F.relu(self.conv1(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv2(x, edge_index))\n     \n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = F.relu(self.conv3(x, edge_index))\n\n        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)\n        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n\n        x = x1 + x2 + x3\n\n        x = self.lin1(x)\n        x = self.act1(x)\n        x = self.lin2(x)\n        x = self.act2(x)      \n        x = F.dropout(x, p=0.5, training=self.training)\n\n        x = F.log_softmax(self.lin3(x)).squeeze(1)\n\n        return x","d2326d25":"def train():\n    model.train()\n\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += float(loss) * data.num_graphs\n    return total_loss \/ len(train_loader.dataset)\ndevice = torch.device('cuda')\nmodel = Net().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","606d1c68":"def evaluate(loader):\n    model.eval()\n\n    predictions = []\n    labels = []\n\n    with torch.no_grad():\n        for data in loader:\n\n            data = data.to(device)\n            pred = model(data).detach().cpu().numpy()\n\n            label = data.y.detach().cpu().numpy()\n            predictions.append(pred)\n            labels.append(label)","09a56b00":"for epoch in range(30):\n    loss = train()\n    train_acc = evaluate(train_loader)\n    #val_acc = evaluate(val_loader)    \n    test_acc = evaluate(test_loader)\n    print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n          format(epoch, loss, train_acc, val_acc, test_acc))","35cd9f65":"It looks like `nd`, `na`, `unknown` and `-` all stands for no data. Therefore let's replace these values all with np.NaN","216591d0":"## Cleaning abundance file","97f741c8":"## Testing the meta data","79150ae7":"drop redundant columns","b2962cb4":"Similarly, for columns that contain 2 values (not including null) I will convert the values to numbers. For example, I will change the column named \"gender\" to \"gender:Female|Male\". The values will be 1 for Female, 2 for Male, and 0 for null.","ab6dddab":"## Construct a graph for genomic part\nIn order to capture a tree structure of the genes, we construct a directed graph where an node represents each bacteria and edge represents parent-child relationship. To quantify the presence of each bacteria, I set up a vector as a node property.","b3834389":"Unfortonately this didn't help remove any features from the meta data.","3757e0da":"## Libraries\nBelow I import some librarys that may be useful and then print the input files","58b7ff53":"# Cleaning the Data\nThe marker matrix is dependent on the abundance table in that strain-specific markers can only appear if a specific strain is abundant. Both tables can be merged together using a join-function on the 210 sample meta-data columns. However these columns are very messy. Therefore let's clean them before we move on to understanding the rest of the data.","ab81014b":"remove all column with only one value","c83c2a71":"It looks like they are basically the same so I can move forward using `samples_df`","723c2f1a":"We can remove all columns that have only 1 values and NaN. These do not seem to be too informative anyway.","1593390a":"import abundance file without the first 211 columns (since we already dealt with those above)","34c9cf5a":"It was brought to my attention that most samples come from stool. Therefore it makes sense that we remove other types of samples.","e941240a":"The meta data information is given in both the marker_presence and abundance tables. I just wanted to make sure they contain the same information.","2be9cd9a":"## Cleaning Marker Presence file\nImport this file without the first 211 columns (since we already dealt with those previously). This file could be imported as a sparse numpy matrix. it is very large.","b57a0e8f":"1,441 columns were dropped from further analysis since they were redundant with parent columns. ","6422e7ce":"## Cleaning meta features","13b16cae":"# Introduction\n\nIn this notebook we explore metagenomics data. This dataset was created by the team of Edoardo Pasolli, Duy Tin Truong, Faizan Malik, Levi Waldron, and Nicola Segata; they published [a research article in July of 2016](https:\/\/journals.plos.org\/ploscompbiol\/article?id=10.1371\/journal.pcbi.1004977). The authors used 8 publicly available metagenomic datasets, and applied [MetaPhlAn2](https:\/\/github.com\/segatalab\/metaml#metaml---metagenomic-prediction-analysis-based-on-machine-learning) to generate species abundance features.\n\n## Logistics behind the Input Data\n\nThis notebook was created to further explore the meta-genomics data on kaggle. The link to the data-set is: https:\/\/www.kaggle.com\/antaresnyc\/metagenomics. The datasets include:\n* abundance.txt: a table containing the abundances of each organism type\n  * the first 210 features include meta-data about the samples\n  * the rest of the features include the abundance data in float-type\n* marker_presence.txt: a table containing the presence of strain-specific markers. \n  * the first 210 features include meta-data about the samples (same as abundance.txt)\n  * In a previous notebook I converted the marker presence feature data into a sparse matrix for easier downloading. This sparse matrix is found on [kaggle](https:\/\/www.kaggle.com\/sklasfeld\/metagenomics-marker-presence-sparse-matrix).\n* markers2clades_DB.txt: a lookup table to associate each marker identifier to the corresponding species.","60daa690":"I want to convert some columns into booleans. For example if the values are either:\n* \"yes\",\"no\", or null\n* \"y\",\"n\", or null\n* \"positve\", \"negative\", or null\n* \"a\"(affected), \"u\" (unaffected), or null\n\nI want to convert them into `2`, `1`, and `0` respectively.","ac64260e":"In summary we have 210 samples. We know the abundance of the organisms in the sample. If an organism is in a sample we have strain-specific marker information.","0461e28b":"I am wondering if we can remove any columns that are redundant. In other words, I would like to remove columns that have identical values.\n\nIn the following for-loop I simply check columns that are consecutive of one another to see if they are identical. I then use the biggest category as the key in `redundant_dict` and all the sub-categories that are equal in the list values.","f80615b4":"Next I look at categorical columns (AKA any feature that has 20 possible values or less)"}}