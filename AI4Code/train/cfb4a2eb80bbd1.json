{"cell_type":{"9ba62a0d":"code","12e6a81b":"code","318e345d":"code","17f7b2c9":"code","4f649297":"code","de1106a6":"code","375154a8":"code","f90356d8":"code","03c00a2e":"code","0b7ea123":"code","ffe64870":"code","e8c6691e":"code","db34d845":"code","bf4effbd":"code","6b9d503b":"code","6a4f0d28":"code","bc42b829":"code","89bd8279":"code","4032f081":"code","d164fdee":"code","3bf59ec6":"markdown","a7ecfef0":"markdown","ec780eb8":"markdown","d6b47d4c":"markdown","3565cb81":"markdown","a99c7adf":"markdown","9d700ae2":"markdown","6e247b02":"markdown","fcd08bf0":"markdown","621e4c5d":"markdown","036a1f67":"markdown","4dbdf8a3":"markdown","f3dcfac7":"markdown","8bfd79f9":"markdown"},"source":{"9ba62a0d":"from time import sleep\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8]\n\ndef inc(x):\n    sleep(1)  # wait 1 sec\n    return x+1","12e6a81b":"%%time\nresults = []\n\nfor x in data:\n    y = inc(x)\n    results.append(y)\n    \ntotal = sum(results)","318e345d":"from dask import delayed\n\n# All it does is build a graph (no calculation)\n\nfor x in data:\n    y = delayed(inc)(x)\n    results.append(y)\n    \ntotal = delayed(sum)(results)\n\nprint(total)","17f7b2c9":"%%time\nz = total.compute()","4f649297":"print('result =',z)\ntotal.visualize()","de1106a6":"import dask.array as da\nimport numpy as np\n\nx = da.from_array([range(100) for i in range(100)], chunks=(10, 10))\nx","375154a8":"x.sum()","f90356d8":"x.sum().compute()","03c00a2e":"%%time\nx = np.random.normal(10, 0.1, size=(20000, 20000))\nprint(f\"This matrix has {x.nbytes\/ 1e9} GB\")\nz = x.mean(axis=0)[::100]","0b7ea123":"%%time\nx = da.random.normal(10, 0.1, size=(20000, 20000), \n                     chunks=(1000, 1000))\nprint(f\"This matrix has {x.nbytes\/ 1e9} GB\")\nz = x.mean(axis=0)[::100].compute()","ffe64870":"import pandas as pd\nimport dask.dataframe as dd\nimport warnings\nwarnings.filterwarnings('ignore')","e8c6691e":"df_dask = dd.read_csv(\"\/kaggle\/input\/flight-delays\/flights.csv\")\n\ntry :\n    df_dask.head()\nexcept :\n    print(\"Got some error\")","db34d845":"%%time\ndf_pd = pd.read_csv(\"\/kaggle\/input\/flight-delays\/flights.csv\", \n                    dtype={'SCHEDULED_TIME':float,\n                           'TAIL_NUMBER':str})\n\nx = df_pd.groupby('ORIGIN_AIRPORT').DEPARTURE_DELAY.mean()","bf4effbd":"%%time\ndf_dask = dd.read_csv(\"\/kaggle\/input\/flight-delays\/flights.csv\",\n                     dtype={'SCHEDULED_TIME':float,\n                           'TAIL_NUMBER':str})\n\ny = df_dask.groupby('ORIGIN_AIRPORT').DEPARTURE_DELAY.mean().compute()","6b9d503b":"x = df_dask.DEPARTURE_DELAY.max()\nprint(x.compute())\nx.visualize()","6a4f0d28":"# Number of rows\nlen(df_dask)","bc42b829":"# Print all columns names\ndf_dask.columns","89bd8279":"# Count cancelled flights\nlen(df_dask[df_dask.CANCELLED==1])","4032f081":"# Number of non-cancelled flights were taken from each airport\nx = df_dask[df_dask.CANCELLED==0].groupby(by='ORIGIN_AIRPORT').ORIGIN_AIRPORT.count()\nx.compute()","d164fdee":"# Average departure delay from each airport\nx = df_dask.groupby(by='ORIGIN_AIRPORT').DEPARTURE_DELAY.mean()\nx.compute()","3bf59ec6":"This is exactly what we expect. The list has 8 element and we have to wait 1 sec for each of them.\n\n### Parallelize with the `dask.delayed` decorator\n\nThose increment calls *could* be called in parallel, because they are totally **independent** of one-another.\n\n`dask.delayed` is a function which does 2 things. If you call delayed on another function, it makes that function lazy (it doesn\u2019t run immediately). If you call delayed on data, it makes everything that touch the data lazy, it will run later.","a7ecfef0":"<div class='alert alert-warning'>\n    <h3>Dask is always lazy<\/h3>\nAs we are with <code>Dask.delayed<\/code>, we always need to call <code>.compute()<\/code> because  it is a lazy object.\n    <\/div>\nWe can see how parallel Dask scheduler provides by calling <code>.visualize()<\/code> ","ec780eb8":"# Dask Basic\n---\n**High level collections:**  Dask provides high-level Array, Bag, and DataFrame\ncollections that mimic NumPy, lists, and Pandas but can operate in parallel on\ndatasets that don't fit into memory.  Dask's high-level collections are\nalternatives to NumPy and Pandas for large datasets.\n\n`dask.delayed` is the only function that you will need to convert functions for use with Dask.","d6b47d4c":"We can see the Dask computaional graph be calling `.visualize()` We will visually see how Dask do tasks in parallel.","3565cb81":"# Pandas or Dask\n---\n**If your data fits in memory then you should definitely use Pandas**, as it is more fully featured than `dask.dataframe`.  The `dask.dataframe` module gives you a solution to operate on datasets that don't fit comfortably in memory.\n\n`dask.dataframe` only really becomes meaningful when Pandas breaks with \n\n    MemoryError:  ...\n    \nFurthermore, the *distributed scheduler* allows the same dataframe expressions to be executed across a cluster. To enable massive \"big data\" processing, one could execute data ingestion functions such as `read_csv`, where the data is held on storage accessible to every worker node (e.g., amazon's S3), and because most operations begin by selecting only some columns, transforming and filtering the data, only relatively small amounts of data need to be communicated between the machines.\n\nDask.dataframe operations use `pandas` operations internally.  Generally they run at about the same speed.\n\nDask.dataframe only covers a small but well-used portion of the Pandas API.\nThis limitation is for two reasons:\n\n1.  The Pandas API is *huge*\n2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n\nAdditionally, some important operations like ``set_index`` work, but are slower\nthan in Pandas because they include substantial shuffling of data, and may write out to disk.","a99c7adf":"We can do pretty much everything like normal Numpy array object. The difference is just it will be lazy and won't be computed immediately.","9d700ae2":"And let them compute later. Doing so, behind the scene, takes advantage of Dask to schedule each task to be executed efficiently. We'll see the time is decreased into 2 sec.","6e247b02":"# Dask Array\n---\nDask array provides a parallel, larger-than-memory ability using blocked algorithms\n\n### Blocked algorithm \nA Blocked algorithm executes on a large dataset by breaking it up into many small blocks and takes advantage of `dask.delayed` function to schedule tasks in parallel.\n\nCreate a `dask.array` object with the `da.from_array` function by passing\n\n1.  `data`: Any object that supports slicing\n2.  `chunks`: A chunk size of the small block","fcd08bf0":"<div class='alert alert-info'>\n    <h3>Unlike Pandas<\/h3>\n    <code>pandas.read_csv<\/code> reads in the entire file before inferring datatypes. <code>dask.dataframe.read_csv<\/code> only reads in a sample from the beginning of the file to infer their datatypes. Thus, these inferred datatypes are then enforced when reading all partitions. The issues might happen for example when most of the beginning of the file is <code>NaN<\/code>. <br>\n    &#8594 Specifying dtypes directly using the <code>dtype<\/code> keyword is the recommended solution.\n    <\/div>","621e4c5d":"# Dask Dataframe\n---\nDask dataframes look and feel like Pandas dataframes but they run on the same infrastructure that powers `dask.delayed`. Pandas is great for tabular datasets that fit in memory. By using Blocked algorithm, Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n\n`dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a large subset of the `Pandas DataFrame` API. One Dask `DataFrame` is comprised of many in-memory `pandas DataFrames` separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.","036a1f67":"So, we'll change the code a little bit. Instead of executing the commands immediatly, we'll make them lazy.","4dbdf8a3":"Let's have a first look on how they perform on 564 MB dataset.","f3dcfac7":"## Compare with Numpy","8bfd79f9":"### Example of Pandas-like operation"}}