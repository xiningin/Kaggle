{"cell_type":{"b5a609da":"code","32c60631":"code","6124c491":"code","2b479116":"code","bcb0eac6":"code","c12fb85c":"code","0a6855f2":"code","db96880e":"code","acb6736a":"code","33075fe0":"code","6d7a5440":"code","f56ea303":"code","6d476bb2":"code","7adb5c02":"code","00dfe36c":"code","d53b69c0":"code","94f2a580":"code","7a0874e3":"code","9183ad00":"code","56687ded":"code","fea48dfc":"code","063257df":"code","b08b15e7":"code","23a5473f":"code","51c724ff":"code","ff3e23d2":"code","b7736bf3":"code","68a8eedd":"code","16b92acf":"code","b10dc453":"code","917bc645":"code","74acbdb0":"code","9307b900":"markdown","3a7c7d1e":"markdown","3695e31a":"markdown","df62b073":"markdown","6ba837d4":"markdown","481b5976":"markdown","e2f15842":"markdown","270bb77f":"markdown","ea3c38cb":"markdown","2334ae88":"markdown","16fe388e":"markdown","3c0ae0dc":"markdown","a39f7341":"markdown","e1f5c368":"markdown","7c4f3361":"markdown","76ac36d9":"markdown","511e552b":"markdown","72fb9625":"markdown","ae2368ad":"markdown"},"source":{"b5a609da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32c60631":"train = pd.read_csv('..\/input\/hr-ana\/train.csv')\ntest = pd.read_csv('..\/input\/hr-ana\/test.csv')\ntrain.head()","6124c491":"train.info()","2b479116":"test.shape","bcb0eac6":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\n\nsns.countplot(train.is_promoted)","c12fb85c":"del train['employee_id']\ndel test['employee_id']","0a6855f2":"cats = [c for c in train.columns if train[c].dtypes=='object']\nnums = [c for c in train.columns if c not in cats]\nprint(cats)\nprint(nums)","db96880e":"true_cats = ['department', 'region', 'education', 'gender', 'recruitment_channel','awards_won?', \n             'previous_year_rating','length_of_service', 'no_of_trainings']","acb6736a":"sns.catplot(x=\"education\", y=\"is_promoted\", kind=\"bar\", data=train, height=4, aspect=2)\n","33075fe0":"\nsns.catplot(x=\"gender\", y=\"is_promoted\", hue=\"department\", kind=\"bar\", data=train, height=6, aspect=2)\n","6d7a5440":"sns.catplot(x=\"gender\", y=\"is_promoted\", hue=\"recruitment_channel\", kind=\"bar\", data=train, height=6, aspect=2)","f56ea303":"sns.catplot(x=\"department\", y=\"is_promoted\", hue=\"recruitment_channel\", kind=\"bar\", data=train, height=6, aspect=4)","6d476bb2":"sns.catplot(data=train, orient=\"h\", kind=\"box\", height=6, aspect=2)","7adb5c02":"true_nums = [c for c in train.columns if c not in true_cats]\ntrue_nums.remove('is_promoted')","00dfe36c":"from scipy.stats import norm\n\nsns.set()\nsns.set_style(\"darkgrid\")\nsns.set_context(\"paper\")\nsns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\nfor c in true_nums:\n    \n    fig = plt.figure(constrained_layout=True, figsize=(14,5))\n    grid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n    ax1 = fig.add_subplot(grid[0, 0])\n    ax1.set_title('Histogram target=0')\n    sns.distplot(train[train.is_promoted==0].loc[:,c].dropna(),bins=30, fit=norm, norm_hist=True,color='teal' , ax = ax1)\n    ax2 = fig.add_subplot(grid[0, 1])\n    ax2.set_title('Histogram target=1')\n    sns.distplot(train[train.is_promoted==1].loc[:,c].dropna(), bins=30, fit=norm, norm_hist=True,color='orangered' , ax = ax2)\n    \n    ","d53b69c0":"missingtr = train.isnull().sum()\nmissingtr[missingtr>0]","94f2a580":"missingts = test.isnull().sum()\nmissingts[missingts>0]","7a0874e3":"from scipy.stats import norm\n\nsns.set()\nsns.set_style(\"darkgrid\")\nsns.set_context(\"paper\")\nsns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\nfor c in true_nums:\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,6))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram Train Vs Test')\n    ## plot the histogram. \n    sns.distplot(train.loc[:,c].dropna(),fit=norm, norm_hist=True,color='teal' , ax = ax1)\n    sns.distplot(test.loc[:,c].dropna(),  fit=norm, norm_hist=True,color='red' , ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(train.loc[:,c].dropna(), plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(train.loc[:,c].dropna(), orient='v', ax = ax3, color='white' );\n","9183ad00":"train = train[train.length_of_service<30]\n\n","56687ded":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12, 8))\nsns.heatmap(pd.DataFrame(train, columns=train.columns).corr(), annot=True, center=True)","fea48dfc":"from sklearn.impute import SimpleImputer\ntarget = train.pop('is_promoted')\n\ndata = pd.concat([train, test], axis=0)\nsi = SimpleImputer(strategy= 'most_frequent')\ndata['education'] = si.fit_transform(data.education.values.reshape(-1, 1))\nsi = SimpleImputer(strategy='mean')\ndata['previous_year_rating'] = si.fit_transform(data.previous_year_rating.values.reshape(-1, 1))\ndata.isnull().sum()","063257df":"from sklearn.preprocessing import LabelEncoder\ndatal = data.copy()\nfor c in cats:\n    le = LabelEncoder()\n    datal[c] = le.fit_transform(datal[c])\n    \ntrain_le = datal.iloc[:len(train)]\ntest_le = datal.iloc[len(train):]\ntrain_le.shape, test_le.shape","b08b15e7":"\ndata_oh = pd.get_dummies(data)\ntrain_oh = data_oh.iloc[:len(train)]\ntest_oh = data_oh.iloc[len(train):]\ntrain_oh.shape, test_oh.shape","23a5473f":"train_oh.head().T","51c724ff":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15, 10))\nsns.heatmap(train_le.corr(), annot=True, center=True)","ff3e23d2":"from sklearn.preprocessing import StandardScaler, RobustScaler\nss = StandardScaler()\ntrain_le = ss.fit_transform(train_le)\ntest_le = ss.fit_transform(test_le)\n\ntrain_oh = ss.fit_transform(train_oh)\ntest_oh = ss.fit_transform(test_oh)\n","b7736bf3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nscores = []\noof = np.zeros(len(train))\ny_le = target.values\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train_le, y_le)):\n    print('fold:', fold_)\n    X_tr, X_test = train_le[train_ind], train_le[val_ind]\n    y_tr, y_test = y_le[train_ind], y_le[val_ind]\n    clf = LogisticRegression(max_iter=200, random_state=2020)\n    clf.fit(X_tr, y_tr)\n    oof[val_ind]= clf.predict_proba(X_test)[:, 1]\n    y = clf.predict_proba(X_tr)[:,1] \n    print('train:',roc_auc_score(y_tr, y),'val :' , roc_auc_score(y_test, (oof[val_ind])))\n    print(20 * '-')\n    \n    scores.append(roc_auc_score(y_test, oof[val_ind]))\n    \n    \n    \nprint('log reg  roc_auc=  ', np.mean(scores))\nnp.save('oof_rf', oof)","68a8eedd":"from sklearn.metrics import *\noof_rnd = np.where(oof > 0.5, 1, 0)\n\nf1_score(target, oof_rnd)","16b92acf":"recall_score(target, oof_rnd)","b10dc453":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nscores = []\noof = np.zeros(len(train_oh))\ny_le = target.values\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train_oh, y_le)):\n    print('fold:', fold_)\n    X_tr, X_test = train_oh[train_ind], train_oh[val_ind]\n    y_tr, y_test = y_le[train_ind], y_le[val_ind]\n    clf = LogisticRegression(max_iter=200, random_state=2020)\n    clf.fit(X_tr, y_tr)\n    oof[val_ind]= clf.predict_proba(X_test)[:, 1]\n    y = clf.predict_proba(X_tr)[:,1] \n    print('train:',roc_auc_score(y_tr, y),'val :' , roc_auc_score(y_test, (oof[val_ind])))\n    print(20 * '-')\n    \n    scores.append(roc_auc_score(y_test, oof[val_ind]))\n    \n    \n    \nprint('log reg  roc_auc=  ', np.mean(scores))\nnp.save('oof_rf', oof)","917bc645":"oof_rnd = np.where(oof > 0.5, 1, 0)\nf1_score(target, oof_rnd)","74acbdb0":"recall_score(target, oof_rnd)","9307b900":"### EDA","3a7c7d1e":"The assumptions made by logistic regression about the distribution and relationships in the data are much the same as the assumptions made in linear regression. these assumptions have precise probabilistic and statistical language in its background and\nfollowing rules of thumb and experiment with different data preparation schemes could be considered","3695e31a":"#### Frequency Encoding","df62b073":"### Building Several data with different cat encodings","6ba837d4":"### Logistic regression","481b5976":"### Load Data","e2f15842":"### Ordinal encoding","270bb77f":"#### TBD","ea3c38cb":"#### Scale data","2334ae88":">We can see as one-hot encoded data resulted better with logistic regression in terms of roc_auc and consequently f1_score.(Althogh we should tune models before trainin ...)","16fe388e":">#### *The question remains is some features are ordinal does it make sence to label encode ordinals for linear models?*","3c0ae0dc":"#### One-Hot encoding","a39f7341":"#### Imputation","e1f5c368":"#### Logistic with one-hot encoded data","7c4f3361":"### Train Models \n","76ac36d9":"#### label encoder","511e552b":"#### Outliers","72fb9625":"#### Logistic with label encoded data","ae2368ad":"- Remove ouliers: Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n- Normalize and scale: Logistic regression is a linear algorithm (with a nonlinear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. \n- Remove Correlated Inputs: Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n- Fail to Converge: It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)"}}