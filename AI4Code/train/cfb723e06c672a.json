{"cell_type":{"c4359fed":"code","cfadb3bd":"code","cc7eb888":"code","eb05cffa":"code","9f581ff0":"code","72a8c4b9":"code","ea9e31e4":"code","5bdffd40":"code","341de1ab":"code","2e08ffa6":"code","9632401c":"code","c3305c3e":"code","e743daf0":"code","3bb5e347":"code","c65dfa88":"code","159696b9":"code","219ca406":"code","d36c4352":"code","9aebc801":"code","ed550098":"code","805782a0":"code","344ca312":"code","0f3885dc":"markdown","21feece6":"markdown","e727f34c":"markdown","15a71e2e":"markdown","8a4b2aa7":"markdown","768be1e6":"markdown","204415b4":"markdown","63519950":"markdown","21f2632d":"markdown","30b68651":"markdown","2f48cdae":"markdown","ffaa2b58":"markdown","6f5f029a":"markdown","42b98475":"markdown","cbddce60":"markdown"},"source":{"c4359fed":"#Let us first import the necessary modules.\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","cfadb3bd":"df_train = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip\")\ndf_test = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip\")\nprint(\"Train shape : \", df_train.shape)\nprint(\"Test shape : \", df_test.shape)","cc7eb888":"# Let us look at the top few rows.\ndf_train.head()","eb05cffa":"plt.figure(figsize=(8,8))\nplt.scatter(range(df_train.shape[0]),np.sort(df_train.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","9f581ff0":"ulimit = 180\ndf_train['y'].iloc[df_train['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(df_train.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","72a8c4b9":"# Now let us have a look at the data type of all the variables present in the dataset\ndtype_data=df_train.dtypes.reset_index()\ndtype_data.columns = [\"Count\", \"Column Type\"]\ndtype_data.groupby(\"Column Type\").aggregate('count').reset_index()\n","ea9e31e4":"dtype_data.loc[:10,:]","5bdffd40":"df_train.isnull().sum().sum()","341de1ab":"# Integer Columns Analysis\nunique_value_dict = {}\nfor col in df_train.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(df_train[col].unique()).tolist())\n        t_list = unique_value_dict.get(unique_value, [])\n        t_list.append(col)\n        unique_value_dict[unique_value] = t_list[:]\nfor unique_val, columns in unique_value_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"-----------------------------------------------------------\")","2e08ffa6":"# Now let us explore the categorical columns present in the dataset.\nvar=\"X0\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","9632401c":"var=\"X1\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","c3305c3e":"var=\"X2\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","e743daf0":"var=\"X3\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","3bb5e347":"var=\"X4\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","c65dfa88":"var=\"X5\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","159696b9":"var=\"X6\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","219ca406":"var=\"X8\"\ncolu_order=np.sort(df_train[var].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var,y=\"y\",data=df_train,order=colu_order)\nplt.xlabel(var,fontsize=12)\nplt.ylabel(\"y\",fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()","d36c4352":"zero_list=[]\none_list=[]\ncol_list = unique_value_dict['[0, 1]']\nfor col in col_list:\n    zero_list.append((df_train[col]==0).sum())\n    one_list.append((df_train[col]==1).sum())\nl = len(col_list)\narr = np.arange(l)\nwidth = 0.35\nplt.figure(figsize=(6,100))\nplot_1 = plt.barh(arr, zero_list, width, color='red')\nplot_2 = plt.barh(arr, one_list, width, left=zero_list, color=\"blue\")\nplt.yticks(arr, col_list)\nplt.legend((plot_1[0], plot_2[0]), ('Zero count', 'One Count'))\nplt.show()","9aebc801":"zero_mean_list = []\none_mean_list = []\ncols_list = unique_value_dict['[0, 1]']\nfor col in cols_list:\n    zero_mean_list.append(df_train.loc[df_train[col]==0].y.mean())\n    one_mean_list.append(df_train.loc[df_train[col]==1].y.mean())\nnew_df = pd.DataFrame({\"column_name\":cols_list+cols_list, \"value\":[0]*len(cols_list) + [1]*len(cols_list), \"y_mean\":zero_mean_list+one_mean_list})\nnew_df = new_df.pivot('column_name', 'value', 'y_mean')\n\nplt.figure(figsize=(8,80))\nsns.heatmap(new_df)\nplt.title(\"Mean of y value across binary variables\", fontsize=15)\nplt.show()","ed550098":"var = \"ID\"\nplt.figure(figsize=(12,6))\nsns.regplot(x=var, y='y', data=df_train, scatter_kws={'alpha':0.5, 's':30})\nplt.xlabel(var, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var, fontsize=15)\nplt.show()\n","805782a0":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values)) \n        df_train[f] = lbl.transform(list(df_train[f].values))\n        \ntrain_y = df_train['y'].values\ntrain_X =df_train.drop([\"ID\", \"y\"], axis=1)\n\n# Thanks to anokas for this #\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100, feval=xgb_r2_score, maximize=True)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","344ca312":"from sklearn import ensemble\nmodel = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\n## plot the importances ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","0f3885dc":"<center><h1 style=\"font-size:130%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">Seems like a single data point is well above the rest.Now let us plot the distribution graph.<\/h1><\/center>","21feece6":"<center><h1 style=\"font-size:130%; font-family:cursive; background:skyblue; color:black; border-radius:10px 10px; padding:10px;\">Good to see that there are no missing values in the dataset :)<\/h1><\/center>","e727f34c":"<center><h1 style=\"font-size:140%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">    \nBinary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes (can be seen from the previous graph). We will dive more into the important variables in the later part of the notebook.<\/h1><\/center>\n\n<center><h1 style=\"font-size:170%; font-family:cursive; background:yellow; color:black; border-radius:10px 10px; padding:10px;\">    \nID variable:<\/h1><\/center>\n<center><h1 style=\"font-size:140%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">    \nOne more important thing we need to look at it is ID variable. This will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)<\/h1><\/center>","15a71e2e":"<center><h1 style=\"font-size:170%; font-family:cursive; background:yellow; color:black; border-radius:10px 10px; padding:10px;\">    \nImportant Variables:\n\n<\/h1><\/center>\n<center><h1 style=\"font-size:140%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">    \nNow let us run and xgboost model to get the important variables.\n\n<\/h1><\/center>","8a4b2aa7":"\n<center><h1 style=\"font-size:160%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">Binary Variables:\n<\/h1><\/center>\n<center><h1 style=\"font-size:140%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">    \nNow we can look into the binary variables. There are quite a few of them as we have seen before. Let us start with getting the number of 0's and 1's in each of these variables.<\/h1><\/center>","768be1e6":"<center><h1 style=\"font-size:280%; font-family:cursive; background:yellow; color:black; border-radius:10px 10px; padding:10px;\">Mercedes-Benz Greener Manufacturing<\/h1><\/center>\n\n<p style=\"font-size:150%; font-family:cursive;\">Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler\u2019s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n<p style=\"font-size:150%; font-family:cursive;\">To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler\u2019s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler\u2019s production lines.","204415b4":"<center><h1 style=\"font-size:280%; font-family:cursive; background:yellow; color:black; border-radius:10px 10px; padding:10px;\">Objective:<\/h1><\/center>\n\n<p style=\"font-size:150%; font-family:cursive;\">This dataset contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing.","63519950":"<center><h1 style=\"font-size:130%; font-family:cursive; background:skyblue; color:black; border-radius:10px 10px; padding:10px;\">So all the integer columns are binary with some columns have only one unique value 0. Possibly we could exclude those columns in our modeling activity.<\/h1><\/center>","21f2632d":"<center><h1 style=\"font-size:130%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">So majority of the columns are integers with 8 categorical columns and 1 float column (target variable)<\/h1><\/center>","30b68651":"<center><img src=\"https:\/\/images.pexels.com\/photos\/2365572\/pexels-photo-2365572.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500\"><\/center>","2f48cdae":"<\/h1><\/center>\n<center><h1 style=\"font-size:140%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">    \nLet us also build a Random Forest model and check the important variables.\n\n<\/h1><\/center>","ffaa2b58":"<p style=\"font-size:150%; font-family:cursive;\">In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards.","6f5f029a":"<center><h1 style=\"font-size:130%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">Target Variable:\n\n\"y\" is the variable we need to predict. So let us do some analysis on this variable first.<\/h1><\/center>","42b98475":"# Please upvote if you like it.!","cbddce60":"\n<center><h1 style=\"font-size:160%; font-family:cursive; background:pink; color:black; border-radius:10px 10px; padding:10px;\">Missing Values<\/h1><\/center>"}}