{"cell_type":{"9cdb3ed3":"code","0d9cfd24":"code","753b5733":"code","60a88e52":"code","1b4db701":"code","25faab0d":"code","fcda633f":"code","bde5a4a7":"code","b5af137e":"code","6f3632f8":"code","c4594f4b":"code","0992bf08":"code","e4d3ab8c":"code","0900a8fe":"code","a0fe2317":"code","ac73ab30":"code","7e2d3618":"code","0f1b9805":"code","22fabbcf":"code","83a4be6e":"code","589811d3":"code","97d60d3b":"code","7daf622d":"code","5949d6f0":"code","d135aed7":"code","002a4da2":"code","875328e2":"code","1bcbdf83":"code","f2935d4b":"code","5d847f3a":"code","4efd5f09":"code","839e9ab5":"code","f0942919":"code","4ac6dc0a":"code","87e4707b":"code","422f887b":"code","929bf753":"code","58c24a46":"code","a1e0507f":"code","16f0d6b7":"code","5c61925b":"code","d1800189":"code","5b71dc4d":"code","eabd2180":"code","0b8274f8":"code","58bd901c":"code","d676694c":"code","2e382d8f":"code","19ebed6a":"code","de60e7e1":"markdown","3274e2a6":"markdown","b288aff3":"markdown","db00d941":"markdown","d0a9f5e9":"markdown","39a253e9":"markdown","fa737a32":"markdown","ab0376e7":"markdown","2645f2ce":"markdown","6d74edba":"markdown","725e7529":"markdown","0739edac":"markdown","70e48e9f":"markdown","747b3c61":"markdown","535e6899":"markdown"},"source":{"9cdb3ed3":"import os, sys\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import cohen_kappa_score\nimport category_encoders as ce\n\n# from catboost import CatBoostRegressor\nimport catboost as cat\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom functools import partial\nimport scipy as sp              # for optimize.minimize()","0d9cfd24":"# Execution environment setting\nKaggle = True\n\nif Kaggle:\n    DIR = '..\/input\/data-science-bowl-2019'\n    task_type = 'CPU'\nelse:\n    DIR = '.\/data-science-bowl-2019'\n    task_type = 'GPU'","753b5733":"train = pd.read_csv(os.path.join(DIR,'train.csv'))\ntrain_labels = pd.read_csv(os.path.join(DIR,'train_labels.csv'))\nspecs = pd.read_csv(os.path.join(DIR,'specs.csv'))\ntest = pd.read_csv(os.path.join(DIR,'test.csv'))","60a88e52":"print('train:\\t\\t',train.shape)\nprint('train_labels:\\t',train_labels.shape)\nprint('specs:\\t\\t',specs.shape)\nprint('test:\\t\\t',test.shape)","1b4db701":"train.head()","25faab0d":"train[['event_id','game_session','installation_id',\n       'title','type','world']].describe()","fcda633f":"event_code_n = train['event_code'].nunique()\nprint(\"num of unique 'event_code':\", event_code_n)\nprint(\"'event_code': \",\n      train['event_code'].min(), \"-\", train['event_code'].max())","bde5a4a7":"# 'event_data' exsample\nprint(train['event_data'][40])\nprint(train['event_data'][41])\nprint(train['event_data'][43])","b5af137e":"train_labels.head()","6f3632f8":"train_labels[['game_session','installation_id', 'title']].describe()","c4594f4b":"# unique 'title' list\ntrain_labels['title'].unique()","0992bf08":"specs.head()","e4d3ab8c":"specs.describe()","0900a8fe":"# 'info' exsample\nprint(specs['info'][0],'\\n')\nprint(specs['info'][6],'\\n')\nprint(specs['info'][7])","a0fe2317":"# 'args' exsample\nprint(specs['args'][0],'\\n')\nprint(specs['args'][1])","ac73ab30":"test.head(8)","7e2d3618":"test[['event_id','game_session','installation_id',\n       'title','type','world']].describe()","0f1b9805":"# make 'title' and 'event_code' list\ntitle_list = list(set(train['title'].value_counts().index) \\\n                   .union(set(test['title'].value_counts().index)))\nevent_code_list = list(set(train['event_code'].value_counts().index) \\\n                   .union(set(test['event_code'].value_counts().index)))","22fabbcf":"# makes dict 'title to number(integer)'\ntitle2num = dict(zip(title_list, np.arange(len(title_list))))\n# makes dict 'number to title'\nnum2title = dict(zip(np.arange(len(title_list)), title_list))\n# \nassess_titles = list(set(train[train['type'] == 'Assessment']['title'].\n                         value_counts().index).\n                         union(set(test[test['type'] == 'Assessment']['title'].\n                         value_counts().index)))\n# makes dict 'title to win event_code' \n# (4100 except 'Bird Measurer' and 4110 for 'Bird Measurer'))\ntitle2win_code = dict(zip(title2num.values() \\\n                    ,(np.ones(len(title2num))).astype('int') * 4100))\ntitle2win_code[title2num['Bird Measurer (Assessment)']] = 4110","83a4be6e":"# Convert 'title' to the number\ntrain['title'] = train['title'].map(title2num)\ntest['title'] = test['title'].map(title2num)\ntrain_labels['title'] = train_labels['title'].map(title2num)\n\n# Convert 'timestamp' to datetime\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","589811d3":"# Convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    user_sample : DataFrame from train\/test group by 'installation_id'\n    test_set    : related with the labels processing\n    '''\n    # Constants and parameters declaration\n    user_assessments = []\n    last_type = 0\n    types_count_dc = {'Clip':0, 'Activity':0, 'Assessment':0, 'Game':0}\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    time_spent_each_title_dc = {title:0 for title in title_list}\n    event_code_count_dc = {code:0 for code in event_code_list}\n    accuracy_groups_dc = {0:0, 1:0, 2:0, 3:0}\n    \n    accumu_accuracy_group = 0\n    accumu_accuracy=0\n    accumu_win_n = 0 \n    accumu_loss_n = 0 \n    accumu_actions = 0\n    counter = 0\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}   ## add\n    \n    # group by 'game_session'\n    for game_id, session in user_sample.groupby('game_session', sort=False):\n        # game_id: game_session_id\n        # session: DataFrame from user_sample group by 'game_session'\n        session_type = session['type'].iloc[0]  # Game\/Assessment\/Activity\/Clip\n        session_title = session['title'].iloc[0]        # session_title:int\n        session_title_text = num2title[session_title]   ## add\n        \n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] \/ 1000)   # [sec]\n            time_spent_each_title_dc[num2title[session_title]] += time_spent\n        \n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100(4110)\n            all_4100_df = session.query(f'event_code == \\\n                                         {title2win_code[session_title]}')\n            # numbers of wins and losses\n            win_n = all_4100_df['event_data'].str.contains('true').sum()\n            loss_n = all_4100_df['event_data'].str.contains('false').sum()\n\n            # init features_dc and then update\n            features_dc = types_count_dc.copy()\n            features_dc.update(last_accuracy_title.copy())   ## add\n            features_dc.update(time_spent_each_title_dc.copy())\n            features_dc.update(event_code_count_dc.copy())\n            features_dc['session_title'] = session_title\n            features_dc['accumu_win_n'] = accumu_win_n\n            features_dc['accumu_loss_n'] = accumu_loss_n\n            accumu_win_n += win_n\n            accumu_loss_n += loss_n\n            \n            features_dc['installation_id'] = session['installation_id'].iloc[-1] # Mod 2019-12-20\n            features_dc['day_of_the_week'] = (session['timestamp'].iloc[-1]). \\\n                                              strftime('%a')    # Mod 2019-11-17\n\n            if durations == []:\n                features_dc['duration_mean'] = 0\n            else:\n                features_dc['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n\n            # average of the all accuracy of this player\n            features_dc['accuracy_ave'] = accumu_accuracy \/ counter \\\n                                                if counter > 0 else 0\n            accuracy = win_n \/ (win_n + loss_n) \\\n                                   if (win_n + loss_n) > 0 else 0\n            accumu_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy    ## add\n            if accuracy == 0:\n                features_dc['accuracy_group'] = 0\n            elif accuracy == 1:\n                features_dc['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features_dc['accuracy_group'] = 2\n            else:\n                features_dc['accuracy_group'] = 1\n            features_dc.update(accuracy_groups_dc)\n            accuracy_groups_dc[features_dc['accuracy_group']] += 1\n            # average of accuracy_groups_dc of this player\n            features_dc['accuracy_group_ave'] = \\\n                    accumu_accuracy_group \/ counter if counter > 0 else 0\n            accumu_accuracy_group += features_dc['accuracy_group']\n            \n            # how many actions the player has done in this game_session\n            features_dc['accumu_actions'] = accumu_actions\n            \n            # if test_set, all sessions belong to the final dataset\n            # elif train, needs to be passed throught this clausule\n            if test_set or (win_n + loss_n) > 0:\n                user_assessments.append(features_dc)\n                \n            counter += 1\n        \n        # how many actions was made in each event_code\n        event_codes = Counter(session['event_code'])\n        for key in event_codes.keys():\n            event_code_count_dc[key] += event_codes[key]\n\n        # how many actions the player has done\n        accumu_actions += len(session)\n        if last_type != session_type:\n            types_count_dc[session_type] += 1\n            last_type = session_type\n            \n    # if test_set, only the last assessment must be predicted,\n    # the previous are scraped\n    if test_set:\n        return user_assessments[-1]\n    return user_assessments","97d60d3b":"# get_data function is applyed to each installation_id\ndef compile_data(df, test_set):\n    compiled_data = []\n    for ins_id, user_sample in tqdm(df.groupby('installation_id', sort=False),\n                                     total=df['installation_id'].nunique()):\n        # user_sample : DataFrame group by 'installation_id'\n        if test_set == False:\n            compiled_data += get_data(user_sample, test_set)\n        else:\n            compiled_test = get_data(user_sample, test_set)\n            compiled_data.append(compiled_test)\n            \n    compiled_df = pd.DataFrame(compiled_data)\n    del compiled_data\n    \n    # additional feature engineering\n    compiled_df['insta_session_count'] = compiled_df.groupby(['installation_id']) \\\n                                                            ['Clip'].transform('count')\n    compiled_df['insta_duration_mean'] = compiled_df.groupby(['installation_id']) \\\n                                                            ['duration_mean'].transform('mean')\n    compiled_df['insta_title_nunique'] = compiled_df.groupby(['installation_id']) \\\n                                                            ['session_title'].transform('nunique')\n    compiled_df.drop('installation_id', axis=1, inplace=True)\n    # convert day_of_the_week to int\n    compiled_df['day_of_the_week'] = compiled_df['day_of_the_week'].map(\n        {'Mon':1, 'Tue': 2, 'Wed':3, 'Thu':4, 'Fri':5,'Sat':6, 'Sun':7})\n    \n    return compiled_df","7daf622d":"# compile train data\nnew_train = compile_data(train, test_set = False).copy()\nprint(new_train.shape)\nnew_train.head(10)","5949d6f0":"# compile test data\nnew_test = compile_data(test, test_set = True).copy()\nprint(new_test.shape)\nnew_test.head(10)","d135aed7":"# rejyect almost same features\nfeatures = new_train.columns\ncounter = 0\nto_remove = []\nfor f_a in features:\n    for f_b in features:\n        if f_a != f_b and f_a not in to_remove and f_b not in to_remove:\n            c = np.corrcoef(new_train[f_a], new_train[f_b])[0][1]\n            if c > 0.99:\n                counter += 1\n                to_remove.append(f_b)\n                print('{}: {} vs {} : Correlation= {}'.format(counter, f_a, f_b, c))","002a4da2":"print(len(features))\nfeatures = [x for x in features if x not in to_remove]\nprint(len(features))\n\nnew_train = new_train[features]\nnew_test = new_test[features]\n\nnew_train.head()","875328e2":"new_test.head()","1bcbdf83":"# all_features but 'accuracy_group', that is the label y\nall_features = [x for x in new_train.columns if x not in ['accuracy_group']]\n# categorical feature\ncategorical_features = ['session_title','day_of_the_week']","f2935d4b":"# Encode categorical_features to integer(for use with LightGB,XGBoost,etc)\n\n# concatnate train and test data\ntemp_df = pd.concat([new_train[all_features], new_test[all_features]])\n# encode\nencoder = ce.ordinal.OrdinalEncoder(cols = categorical_features)\ntemp_df = encoder.fit_transform(temp_df)\n# dataset\nX, y = temp_df.iloc[:len(new_train),:].copy(), new_train['accuracy_group'].copy()\nX_test = temp_df.iloc[len(new_train):,:].copy()","5d847f3a":"X.head()","4efd5f09":"y.head()","839e9ab5":"X_test.head()","f0942919":"del train,test,new_train, new_test","4ac6dc0a":"# Create multiple datasets to create multiple models (not for CV).\nNFOLDS = 5\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)","87e4707b":"# CatBoost\nstart_time = time()\ncat_models = []\nscores = []\n\nparams = {\n    'learning_rate': 0.02,\n    'loss_function': 'RMSE',\n    'random_seed': 42,\n    'depth': 11,                            # 10\n    'border_count': 37,                     # 108\n    'bagging_temperature': 2.348502,        # \n    'task_type': task_type,\n}\n\n# Train and make models\nfor fold, (train_ids, val_ids) in enumerate(folds.split(X,y)):\n    print('\u25cf Fold :', fold+1,'\/',NFOLDS)\n    dtrain = cat.Pool(X.iloc[train_ids], y[train_ids],\n                     cat_features=categorical_features)\n    dval = cat.Pool(X.iloc[val_ids], y[val_ids],\n                   cat_features=categorical_features)\n    model = cat.train(params=params,\n                      dtrain=dtrain,\n                      eval_set=dval,        # =evals\n                      iterations=5000,      # =num_boost_round\n                      early_stopping_rounds=100,\n                      verbose=200\n                     )\n    cat_models.append(model)\n    \nprint('Time:', time() - start_time)","422f887b":"# XGBoost\nstart_time = time()\nxgb_models = []\nscores = []\n\nparams = {\n    'max_depth': 6,                     # 6,10,9\n    'learning_rate': 0.01,              # =eta 0.1: [0,1]\n    'objective': 'reg:squarederror',    # 'reg:linear'\n    'n_estimators' : 300,               # 100\n    'subsample': 0.79,                  # 1,0.8,0.6    # 1, (0,1]    \n    'colsample_bytree': 1.0,            # 1,0.8,1.0    # 1, (0, 1]   \n    'gamma': 0.14,                      # 0.0\n    'min_child_weight': 3,              # 5\n    'seed' : 42,\n}\n\n# Train and make models\nfor fold, (train_ids, val_ids) in enumerate(folds.split(X,y)):\n    print('\u25cf Fold :', fold+1,'\/',NFOLDS)\n    dtrain = xgb.DMatrix(X.iloc[train_ids], y[train_ids])\n    dval = xgb.DMatrix(X.iloc[val_ids], y[val_ids])\n    model = xgb.train(params=params,\n                      dtrain=dtrain,\n                      num_boost_round=5000,\n                      evals=[(dtrain, 'train'), (dval, 'val')],\n                      early_stopping_rounds=100,\n                      verbose_eval=200\n                     )\n    xgb_models.append(model)\n    \nprint('Time:', time() - start_time)","929bf753":"# LightGBM\nstart_time = time()\nlgb_models = []\nscores = []\n\nparams = {\n    'n_jobs': -1,\n    'seed': 42,\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'feature_fraction': 0.95,        # 0.998, 0.967\n    'bagging_fraction': 0.85,        # 0.8, 0.872    =subsample\n    'learning_rate': 0.01,\n    'max_depth': 14,                 # 10,13\n    'num_leaves': 957,               # 1024,440    # 2^max_depth < num_leaves ?\n    'min_gain_to_split': 0.096104,   # 0.086, 0.053\n    'min_child_weight': 1.189104,    # 1.087, 1.497\n    'lambda_l1': 1.8,                # 1.0\n    'lambda_l2': 1.5,                # 1.0\n}\n\n# Train and make models\nfor fold, (train_ids, val_ids) in enumerate(folds.split(X,y)):\n    print('\u25cf Fold :', fold+1,'\/',NFOLDS)\n    train_set = lgb.Dataset(X.iloc[train_ids], y[train_ids],\n                           categorical_feature=categorical_features)\n    val_set = lgb.Dataset(X.iloc[val_ids], y[val_ids],\n                         categorical_feature=categorical_features)\n    model = lgb.train(params=params,\n                      train_set=train_set,\n                      valid_sets=[train_set, val_set],\n                      num_boost_round=5000,\n                      early_stopping_rounds=100,\n                      verbose_eval=100\n                     )\n    lgb_models.append(model)\n    \nprint('\\nTime:', time() - start_time)","58c24a46":"preds = []\n\n# CatBoost models\nfor model in cat_models:\n    pred = model.predict(X)\n    preds.append(pred)\n    \n# XGBoost models\nfor model in xgb_models:\n    pred = model.predict(xgb.DMatrix(X))\n    pred = pred.flatten()\n    preds.append(pred)\n    \n# LightGBM models\nfor model in lgb_models:\n    pred = model.predict(X,num_iteration=model.best_iteration)\n    pred = pred.reshape(len(X),1).flatten()\n    preds.append(pred)\n\nreg_df = pd.DataFrame(preds).T\nreg_df.columns = ['C1','C2','C3','C4','C5',   # CatBoost\n                  'X1','X2','X3','X4','X5',   # XGBoost\n                  'L1','L2','L3','L4','L5']   # LightGBM","a1e0507f":"# Calculate the average value of each model pred\nreg_df['mean'] = reg_df.mean(axis = 'columns')\nreg_df.head(10)","16f0d6b7":"class OptRounder(object):\n    def __init__(self):\n        self.res_ = []\n        self.coef_ = []\n        \n    def get_res(self):\n        return self.res_\n    \n    # objective function\n    def func(self, coef, X, y):\n        kappa = cohen_kappa_score(self.bincut(coef, X), y,\n                                  weights='quadratic')\n        return -kappa\n\n    def bincut(self, coef, X):\n        return pd.cut(X,\n                      [-np.inf] + list(np.sort(coef)) + [np.inf],\n                      labels = [0, 1, 2, 3])\n        \n    def fit(self, X, y):\n        pfunc = partial(self.func, X=X, y=y)\n        self.res_ = sp.optimize.minimize(fun = pfunc,           # objective func\n                                         x0 = [0.7, 1.5, 2.3],  # initial coef\n                                         method='nelder-mead')  # solver\n        self.coef_ = self.res_.x\n        \n    def predict(self, X, coef):\n        return self.bincut(coef, X)","5c61925b":"optR = OptRounder()\n\n# Optimize each model's coef\ncoef = []\nfor col in tqdm(reg_df.columns[:-1]):\n    optR.fit(reg_df[col].values.reshape(-1,), y)\n    res = optR.get_res()\n    coef.append(np.append(res.x, -res.fun))  # Optimized coef & kappa\n\ncoef_df = pd.DataFrame(coef,\n                       columns = ['coef0','coef1','coef2','kappa'],\n                       index = reg_df.columns[:-1])\ncoef_df","d1800189":"# coefficients average weighted by kappa for each model\ncoefficients = []\nfor col in coef_df.columns[:-1]:\n    coefficients.append(np.average(np.array(coef_df[col]),\n                                   weights=np.array(coef_df['kappa'])))\nprint(coefficients)","5b71dc4d":"# final classification\nreg_df['predict'] = optR.predict(reg_df['mean'].values,\n                                 coefficients).astype(int)\n\nreg_df['y'] = y\nkappa = cohen_kappa_score(reg_df['predict'], y, weights='quadratic')\nprint('\u25cfCohen Kappa score (traind X):',kappa)\nreg_df[['mean','predict','y']].head(10)","eabd2180":"reg_df[['mean','predict','y']].plot(subplots=True,layout=(1, 3),\n                                    figsize=(11, 3),kind='hist')","0b8274f8":"# binning plot of 'pred' versus 'y'\nreg_df.plot.hexbin(x='y', y='predict', gridsize=(3,3),\n                   sharex=False, title = \"binning 'pred' vs 'y'\")","58bd901c":"preds = []\nfor model in cat_models:        # CatBoost\n    pred = model.predict(X_test)\n    preds.append(pred)\nfor model in xgb_models:        # XGBoost\n    pred = model.predict(xgb.DMatrix(X_test))\n    pred = pred.flatten()\n    preds.append(pred)\nfor model in lgb_models:        # LightGBM\n    pred = model.predict(X_test,num_iteration=model.best_iteration)\n    pred = pred.reshape(len(X_test),1).flatten()\n    preds.append(pred)\ndf_s = pd.DataFrame(preds).T\n\ndf_s['mean'] = df_s.mean(axis = 'columns')\n\n# Classification\ndf_s['pred'] = optR.predict(df_s['mean'].values, coefficients).astype(int)\n\nprint(df_s.shape)\ndf_s[['mean','pred']].head(10)","d676694c":"df_s[['mean','pred']].plot(subplots=True, layout=(1, 2),\n                           figsize=(7, 3), kind='hist')","2e382d8f":"submission = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\nsubmission['accuracy_group'] = df_s['pred']\nsubmission.head(10)","19ebed6a":"submission.to_csv('submission.csv', index=None)","de60e7e1":"### 2. train_labels","3274e2a6":"### -XGBoost","b288aff3":"## Regressors and Classification by Optimal Rounding\n**\u25a0Classification steps**<BR>\n**Step 1** Create Regressor Models : Create multiple train_datasets using `kFold`(**not for CV**) and create a regression model from each dataset. I used ** CatBoost **, ** XGBoost **, ** LightGBM ** and `kFold = 5`, so created 15 models. <BR>\n**Step 2** Predict each Model<BR>\n**Step 3** Optimize Rounding Coefficients : The rounding coefficients of each model is optimized using `scipy.optimize.minimize()`. And calculate the final coefficient by weighted average of the optimal coefficient of each model.<BR>\n**Step 4** Final Classification\n    \nVer.5 : Introduced weighted average to calculate final rounding coefficient.","db00d941":"## Observe the data","d0a9f5e9":"## Compile data\nBased on several kernels\n- Hosseinali: https:\/\/www.kaggle.com\/mhviraf\/a-new-baseline-for-dsb-2019-catboost-model\n- Bruno Aquino: https:\/\/www.kaggle.com\/braquino\/catboost-some-more-features\n- Heng Zheng: https:\/\/www.kaggle.com\/hengzheng\/bayesian-optimization-seed-blending","39a253e9":"### 1. train","fa737a32":"### - LightGBM","ab0376e7":"## Step 3 : Optimize Rounding Coefficients\nThe rounding coefficient is optimized using the average value of the prediction results of each model. Optimization uses `scipy.optimize.minimize()`.\n\nThe rounding coefficients of each model is optimized using `scipy.optimize.minimize()`. And calculate the final coefficient by weighted average of the optimal coefficient of each model.","2645f2ce":"## Step 4 : Final Classification","6d74edba":"## Step 2 : Predict each Model","725e7529":"## Step 1 : Create Regressor Models\nCreate multiple train_datasets using `kFold` and create a regression model from each dataset. I used ** CatBoost **, ** XGBoost **, ** LightGBM **.","0739edac":"### 3. specs","70e48e9f":"### 4. test","747b3c61":"### - CatBoost","535e6899":"## Make submission"}}