{"cell_type":{"aaa00b50":"code","e8f39892":"code","6b68229d":"code","b888c16a":"code","e67caf98":"code","3ec1dc74":"code","af8fb205":"code","525dcf80":"code","ed226f86":"code","cb8afcca":"code","9afb1ce4":"code","2b7c5484":"code","b6317e9b":"code","2de1a3ee":"code","14948e44":"code","40696d9e":"code","4971b416":"code","3c1ff12a":"code","1a93d67c":"code","0257a622":"code","b1810ed5":"markdown","5f5607be":"markdown","c4043168":"markdown","421b56d6":"markdown","d2908998":"markdown","ab635f3a":"markdown","5770b1c4":"markdown","a183b882":"markdown","264a3d32":"markdown","2fc40850":"markdown","dbad0836":"markdown","36c2f23c":"markdown","98fa92f9":"markdown","ea2c9134":"markdown"},"source":{"aaa00b50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/hw03-dataset\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8f39892":"import lightgbm as lgb","6b68229d":"df = pd.read_csv(\"\/kaggle\/input\/hw03-dataset\/hw03_train.csv\")\nprint(\"df data:\", df.shape)","b888c16a":"df.dtypes","e67caf98":"\ndef create_one_hot_encodings(df):\n    '''\n    Creates one-hot encodings for the non-numerical columns in the input dataframe\n    '''\n    return pd.get_dummies(df, columns=[c for c in df.columns if df[c].dtype == 'object'])","3ec1dc74":"df = create_one_hot_encodings(df)\nprint(\"Shape after converting categorical columns into 1-hot encodings:\", df.shape)","af8fb205":"df.isna().sum()*100\/len(df)","525dcf80":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler","ed226f86":"def scale_data(input_df):\n    scaler = StandardScaler()\n    return scaler.fit_transform(input_df)","cb8afcca":"y = df[df['target'].notna()]['target']\nX_train, X_holdout, y_train, y_holdout = train_test_split(\n    df[df['target'].notna()].values, y, test_size=0.3, random_state=20)\n\n#Scale features\nX_train = scale_data(X_train)\nX_holdout = scale_data(X_holdout)\nprint(\"X_train_scaled shape:\", X_train.shape)\nprint(\"X_train_scaled shape:\", X_holdout.shape)","9afb1ce4":"d_train = lgb.Dataset(X_train, label=y_train)\n\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 30\nparams['min_data'] = 50\nparams['max_depth'] = 10\n\nclf = lgb.train(params, d_train, 100)\nprint(\"light gbm model:\", clf)","2b7c5484":"#Prediction\ny_pred=clf.predict(X_holdout)\n\n#convert into binary values\nfor i in range(0,len(y_pred)):\n    if y_pred[i]>=.3:       # setting threshold to .3\n       y_pred[i]=1\n    else:  \n       y_pred[i]=0","b6317e9b":"y_pred","2de1a3ee":"y_holdout","14948e44":"#Accuracy calculation\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_pred,y_holdout.values))","40696d9e":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred, y_holdout.values)\ncm","4971b416":"from xgboost.sklearn import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)","3c1ff12a":"y_preds = xgb.predict(X_holdout)","1a93d67c":"#Accuracy calculation\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_preds,y_holdout.values))","0257a622":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred, y_holdout.values)\ncm","b1810ed5":"### Feature Scaling","5f5607be":"### Training the models","c4043168":"### XGBoost\n- XGBoost involves bulding trees in a sequential order with each subsequent tree built to reduce the errors of the previous tree.\n- XGBoost has regularization parameters to reduce overfitting unlike GBMs.\n- XGBoost has built-in support for handling missing values\n- XGBoost involves building trees in breadth-wise manner\n- XGBoost is widely used for its performance, speed and accuracy\n- XGBoost supports parallel jobs as well.","421b56d6":"#### 2) Training the XGBoost model","d2908998":"### LightGBM\n- LightGBM is a tree-based learning methodology involving Gradient Boosting.\n- LightGBM involves building trees in height-wise manner\n- LightGBM, going by its name is capable of handling large data sizes at blazing speed\n- The memory consumption of LightGBM is also very low\n- LightGBM has support for GPUs as well.","ab635f3a":"#### As we could see from the above that LGBM model has worked with 100% accuracy on the hold-out data.","5770b1c4":"### Model Training","a183b882":"#### As we could see from the above that even the XGBoost based model has also given us 100% accuracy on the hold-out data.","264a3d32":"\n### Handling of missing values","2fc40850":"### Dataset\nAs part of this tutorial, lets consider the dataset used for the Homework assignment HW-03 where the goal is to predict whether income exceeds 50K dollars\/year based on census data. Target variable is target which contains 1 if income exceeds 50K dollars per year and 0 otherwise.","dbad0836":"### Conclusion:\nOn this particular dataset, we could see that both the algorithms work equally good.","36c2f23c":"#### 1) Training the LightGBM model","98fa92f9":"As we can see from the above, the dataset is complete and has no missing value in any of the input columns","ea2c9134":"### Handling of Categorical data"}}