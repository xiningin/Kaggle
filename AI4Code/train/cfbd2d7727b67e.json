{"cell_type":{"19b96e96":"code","4c9c4fbf":"code","3473ec4b":"code","da8e8e49":"code","a6742adf":"code","5b29b6ce":"code","e3cfe80d":"code","35c89e42":"code","ed53753f":"code","582e8e53":"code","1e71483f":"code","a3d89a1c":"code","5bfe85df":"code","478fa88e":"code","e444676f":"code","ddf1e357":"code","0e9904b6":"code","2fb0ba49":"code","ad47c9ba":"code","c755d92d":"code","75c548af":"code","96ec6d18":"code","02c919d4":"code","a4a4920c":"markdown"},"source":{"19b96e96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c9c4fbf":"import cv2\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAvgPool2D, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nfrom tensorflow_hub import KerasLayer\n\nimport matplotlib.pyplot as plt","3473ec4b":"home_path = '\/kaggle\/input\/100-bird-species\/'\n\ngen = ImageDataGenerator(rescale=1\/255, validation_split=0.3)\ntrain_tf = gen.flow_from_directory(home_path+'train', target_size=(244,244), subset='training')\nval_tf = gen.flow_from_directory(home_path+'train', target_size=(244,244), subset='validation')\ntest_tf = gen.flow_from_directory(home_path+'valid', target_size=(244,244))","da8e8e49":"# birds = pd.read_csv(os.path.join(home_path+'birds.csv'))\n# birds.head()","a6742adf":"# train = birds[birds['data set']=='train']\n# test = birds[birds['data set']=='test']","5b29b6ce":"# from sklearn.preprocessing import OneHotEncoder\n# enc = OneHotEncoder()\n# encoded = enc.fit_transform(np.array(birds.labels).reshape(-1,1)).toarray()\n# encoded","e3cfe80d":"# dic = {}\n# for i in list(enumerate(enc.categories_[0])):\n#     dic[i[0]] = i[1]\n# mapping = lambda x: [key for key in dic if dic[key]==x][0]\n# dic","35c89e42":"# birds = birds.join(pd.DataFrame(encoded))\n# birds","ed53753f":"# num_train_images = len([os.listdir(home_path+i) for i in train.filepaths.apply(lambda x: '\/'.join(x.split('\/')[0:2]))])\n# num_test_images = len([os.listdir(home_path+i) for i in test.filepaths.apply(lambda x: '\/'.join(x.split('\/')[0:2]))])","582e8e53":"# import random\n# def dataset_gen(arr):\n#     final = []\n#     index = 0\n#     random.shuffle(arr)\n#     for i in range(len(arr)):    \n#         final.append(arr[index])\n#         index += 1\n#         if index > len(arr)-1:\n#             index = 0\n#             random.shuffle(arr)\n#     return final\n\n\n# def generator(arr, batch_size):\n#     for k in range(30):\n#         data = dataset_gen(arr)\n#         indices = list(range(0,len(data),batch_size))\n#         try:\n#             for i in range(len(indices)):\n#                 images_paths, labels = [birds.iloc[j,0] for j in data[indices[i]:indices[i+1]]], np.array([birds.iloc[j,3:].tolist() for j in data[indices[i]:indices[i+1]]])\n#                 yield np.array([cv2.resize(cv2.imread(home_path+image),(100,100)) for image in images_paths]), labels\n\n#         except:\n#             pass\n#         images2_paths, labels = [birds.iloc[j,0] for j in data[indices[-1]:]], np.array([birds.iloc[j,3:].tolist() for j in data[indices[-1]:]])\n#         yield np.array([cv2.resize(cv2.imread(home_path+image),(100,100)) for image in images2_paths]), labels","1e71483f":"# gen = generator(list(train.index), 28)\n# for i in gen:\n#     print(i[0].shape)","a3d89a1c":"# train_gen = generator(list(train.index), batch_size=batch_size)\n# test_gen = generator(list(test.index), batch_size=batch_size)","5bfe85df":"# model = Sequential()\n# model.add(Conv2D(input_shape=(100,100,3), kernel_size=(4,4), filters=500, activation='relu'))\n# model.add(MaxPooling2D())\n# model.add(Conv2D(kernel_size=(4,4),filters=100))\n# model.add(GlobalAvgPool2D())\n# model.add(Dense(69, activation='relu'))\n# model.add(Dense(325, activation='sigmoid'))","478fa88e":"# efficientnet = Sequential([KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/classification\/1\")])\n# efficientnet.build([None, 244,244, 3])","e444676f":"# efficientnet.add(Dense(100, activation='relu'))\n# efficientnet.add(Dropout(0.25))\n# efficientnet.add(Dense(325, activation='sigmoid'))\n# efficientnet.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n# efficientnet.summary()","ddf1e357":"batch_size = 32\nepochs = 10","0e9904b6":"def get_model():\n    base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(244,244,3))\n    \n    for layer in base_model.layers[:180]:\n        layer.trainable = False\n    for layer in base_model.layers[180:]:\n        layer.trainable = True    \n    \n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    # let's add a fully-connected layer\n    x = Dense(1024, activation='relu')(x)\n    # and a logistic layer -- let's say we have 200 classes\n    predictions = Dense(325, activation='softmax')(x)\n\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # first: train only the top layers (which were randomly initialized)\n    # i.e. freeze all convolutional InceptionV3 layers\n    \n    return model","2fb0ba49":"densenet = get_model()\ndensenet.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n# densenet.summary()","ad47c9ba":"early_stopping = EarlyStopping(\n    monitor=\"val_accuracy\",\n    min_delta=0.03,\n    patience=3,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=False,\n)","c755d92d":"history = densenet.fit(train_tf, epochs=epochs, batch_size=batch_size, validation_data=val_tf, callbacks=[early_stopping])","75c548af":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.legend()\nplt.title('Accuracy vs. Val_accuracy')\nplt.grid(alpha=0.4)\nplt.show()\n\nplt.plot(history.history['loss'], label='accuracy')\nplt.plot(history.history['val_loss'], label='val_accuracy')\nplt.legend()\nplt.title('Loss vs. Val_loss')\nplt.grid(alpha=0.4)\nplt.show()","96ec6d18":"scores = densenet.evaluate(test_tf)\nprint('Test Loss:',scores[0])\nprint('Test Accuracy:',scores[1])","02c919d4":"def labels(prediction):\n    for i in test_tf.class_indices:\n            if test_tf.class_indices[i] == prediction:\n                return i\n\nplt.figure(figsize=(20,25))\nfor actual_index,actual_label in enumerate(os.listdir(home_path+'valid\/')[:15]):\n    bird =  os.listdir(home_path+'valid\/'+actual_label)\n    image = cv2.resize(cv2.imread(home_path+'valid\/' + actual_label + '\/'+bird[3]), (244,244))\/255\n    prediction = np.argmax(densenet.predict(image.reshape(-1,244,244,3)))\n    plt.subplot(5,3,actual_index+1)\n    plt.imshow(image)        \n    plt.title(f'Actual:{actual_label},\/Predicted:{labels(prediction)}')\nplt.show()","a4a4920c":"# **Custom Generator Function:**"}}