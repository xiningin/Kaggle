{"cell_type":{"17bebc01":"code","262ce0bd":"code","52f65308":"code","b81fae36":"code","30f5903b":"code","506298d1":"code","1569e2cd":"code","88c33dde":"code","1e58eac1":"code","4583ab47":"code","634be5e5":"code","24026a8e":"code","b16afeb4":"code","32860ec8":"code","40d8fd77":"code","a051d511":"code","4a7778e0":"code","dd9c670b":"code","365ad4f2":"code","720dffb7":"code","4a2ec052":"code","9838e590":"code","ca923214":"code","fc946f77":"code","8dbd3b4a":"code","c6ad007e":"code","77492520":"code","a6ad0707":"code","3d0e0a55":"code","b1219e51":"code","6d3a1d36":"code","745abc11":"code","a3d03775":"code","fc0625fd":"code","7c49515b":"code","ff117b6b":"code","8a069d77":"code","77725e1b":"code","fe0384e6":"code","d43383b8":"code","cf519972":"code","76e9349c":"code","9a59b855":"markdown","03fb8036":"markdown","c4a38524":"markdown","43c6771d":"markdown","b4d37d6e":"markdown","64a5cdd4":"markdown","834b86d6":"markdown","7a32a109":"markdown","9fb0395b":"markdown","ebcd1e48":"markdown","89235b99":"markdown","580cf751":"markdown","6831aff5":"markdown","3809d448":"markdown","176f5b8d":"markdown","8891b5d6":"markdown","516c6ca1":"markdown","4620799b":"markdown","dfe25e47":"markdown","472c9908":"markdown","ef8f6501":"markdown","b78d0c22":"markdown","3620025c":"markdown","e0131ef2":"markdown","498ce447":"markdown","9cc1413e":"markdown","8f71b955":"markdown","61d3d2db":"markdown","8eeb59d6":"markdown","a9d8f277":"markdown","2b19b891":"markdown","6adee770":"markdown","8088825c":"markdown","632051ee":"markdown","575005e0":"markdown","9770d16c":"markdown","55fdc75c":"markdown","80821c19":"markdown","64ff6632":"markdown","2a0c4e81":"markdown","9fbf67f6":"markdown","fb49d285":"markdown","b555eec2":"markdown","e03c86c5":"markdown","c8dc5bea":"markdown","90cc274e":"markdown","edb1bb63":"markdown","2f8f17a9":"markdown","a427a8bf":"markdown","39af35eb":"markdown","78bb2808":"markdown","dcd86dea":"markdown","88712ae7":"markdown","cf78fa90":"markdown","d8fb7da9":"markdown"},"source":{"17bebc01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","262ce0bd":"import numpy as np\nimport pandas as pd\nimport datatable as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","52f65308":"%%time\n\ntrain_filename = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest_filename = \"..\/input\/tabular-playground-series-sep-2021\/test.csv\"\n\ntrain_orig = dt.fread(train_filename).to_pandas()\ntest_orig = dt.fread(test_filename).to_pandas()\n\ntrain_orig = train_orig.set_index('id')\ntest_orig = test_orig.set_index('id')","b81fae36":"train_orig.shape","30f5903b":"train_orig.claim = train_orig.claim.astype('int16')\ntrain_orig.info()\nprint()\ntest_orig.info()","506298d1":"pd.set_option('display.max_columns', 125)\ntrain_orig.describe()","1569e2cd":"train_orig.head(10)","88c33dde":"train_memory_orig = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original training set(in MB): {}'.format(train_memory_orig))\n\ndef reduce_memory(df):\n    for col in df.columns:\n        if str(df[col].dtypes)[:5] == 'float':\n            low = df[col].min()\n            high = df[col].max()\n            if((low > np.finfo(np.float16).min) and (high < np.finfo(np.float16).max)):\n                df[col] = df[col].astype('float16')\n            elif((low > np.finfo(np.float32).min) and (high < np.finfo(np.float).max)):\n                df[col] = df[col].astype('float32')\n    return df\n\nreduce_memory(train_orig)\ntrain_memory_reduced = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced training set(in MB): {}'.format(train_memory_reduced))","1e58eac1":"test_memory_orig = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original test set(in MB): {}'.format(test_memory_orig))\n\nreduce_memory(test_orig)\ntest_memory_reduced = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced test set(in MB): {}'.format(test_memory_reduced))","4583ab47":"#prints the number of duplicated entries(rows)\nn_duplicates_train = train_orig.duplicated().sum()\nprint(\"Number of duplicated entries in trainng set: {}\".format(n_duplicates_train))\nn_duplicates_test = test_orig.duplicated().sum()\nprint('Number of duplicated entries in test set: {}'.format(n_duplicates_test))","634be5e5":"print(train_orig.claim.dtype)\nprint(train_orig.claim[:10])","24026a8e":"claim_dist = train_orig.claim.value_counts()\ndisplay(claim_dist)","b16afeb4":"plt.figure(figsize = (10,6))\nclaim_dist.plot.pie(autopct = '%.1f', colors = ['powderblue', 'slateblue'])\nplt.title(\"Claim vlaue distribution pie chart\", pad = 20, fontdict = {'size' : 15, 'color' : 'darkblue', 'weight' : 'bold'})\nplt.show()","32860ec8":"train_orig['count_missing'] = train_orig.isna().sum(axis = 1)\ntest_orig['count_missing'] = test_orig.isna().sum(axis = 1)\n\nprint(train_orig['count_missing'].value_counts())\nprint(test_orig['count_missing'].value_counts())","40d8fd77":"train_frac = train_orig.sample(frac = 0.01).reset_index(drop = True)\n#train_frac = train_orig[0:9579]\ntarget = train_frac.claim\n#txt = \"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]).center(110)\n#print(txt)\n\nc = 4\n#r_ = int(np.ceil(len(train_frac.columns)\/4))\nr = int(np.ceil(train_frac.shape[1]\/4))\n#print(r, r_)\nfig, ax = plt.subplots(nrows = r, ncols = c, figsize = (25,80))\ni = 1\nfor col in train_frac.columns:\n    plt.subplot(r, c, i)\n    ax = sns.kdeplot(train_frac[col], hue = target, fill = True, multiple = 'stack')\n    plt.xlabel(col, fontsize = 15)\n    i = i + 1\n    \nfig.tight_layout(pad = 2.0)\nfig.subplots_adjust(top = 0.97)\nplt.suptitle(\"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]), fontsize = 20)\nplt.show()","a051d511":"corrMat = train_frac.corr()\n\nfig, ax = plt.subplots(figsize = (20,20))\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\nmask = np.triu(np.ones_like(corrMat, dtype = bool))\nsns.heatmap(corrMat, square = True, annot = False, linewidths = 1, cmap = cmap, mask = mask)","4a7778e0":"from sklearn.model_selection import train_test_split\n\nX = train_orig.copy()\nY = X.claim\nX.drop('claim', axis = 1, inplace = True)\n\nX_train_orig, X_valid_orig, Y_train_orig, Y_valid_orig = train_test_split(X, Y, test_size = 0.2,\n                                                                         random_state = 42)\nX_test_orig = test_orig.copy()","dd9c670b":"missing_val_cols = X_train_orig.isnull().sum().sort_values(ascending = False)\nmissing_val_cols = missing_val_cols[missing_val_cols > 0]\nratio_of_missing = missing_val_cols \/ X_train_orig.shape[0]\nmissing = pd.concat([missing_val_cols,ratio_of_missing], axis = 1, \n                   keys = ['Count','%'])\nmissing","365ad4f2":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = False)\nmy_imputer.fit(X_train_orig)\nX_train_imputed = my_imputer.transform(X_train_orig)\nX_valid_imputed = my_imputer.transform(X_valid_orig)\nX_test_imputed = my_imputer.transform(X_test_orig)","720dffb7":"from sklearn.preprocessing import RobustScaler, StandardScaler\n\nrobust_scaler = RobustScaler()\nrobust_scaler.fit(X_train_imputed)\nX_train_robust = robust_scaler.transform(X_train_imputed)\nX_valid_robust = robust_scaler.transform(X_valid_imputed)\nX_test_robust = robust_scaler.transform(X_test_imputed)\n\nstandard_scaler = StandardScaler()\nstandard_scaler.fit(X_train_imputed)\nX_train_scaled = standard_scaler.transform(X_train_imputed)\nX_valid_scaled = standard_scaler.transform(X_valid_imputed)\nX_test_scaled = standard_scaler.transform(X_test_imputed)","4a2ec052":"X_train_final = pd.DataFrame(X_train_scaled, index = X_train_orig.index,\n                            columns = X_train_orig.columns)\nX_valid_final = pd.DataFrame(X_valid_scaled, index = X_valid_orig.index, \n                            columns = X_valid_orig.columns)\nX_test_final = pd.DataFrame(X_test_scaled, index = X_test_orig.index, \n                           columns = X_test_orig.columns)","9838e590":"#final training set\nX_train_final.describe()","ca923214":"from xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n#library to implement Bayesian Optimization\nimport optuna","fc946f77":"#see documentation to analyse hyperparameters\n#help(XGBClassifier())","8dbd3b4a":"from sklearn.metrics import roc_auc_score\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.model_selection import StratifiedKFold\n\ndef get_score(actual, preds):\n    score = roc_auc_score(actual,preds)\n    return score\n\n# def objective_function(trial):\n#     n_estimators = trial.suggest_int('n_estimators', 5000, 30000)\n#     use_label_encoder = trial.suggest_categorical('use_label_encoder',['False'])\n#     max_depth = trial.suggest_int('max_depth', 2, 10)\n#     learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2)\n#     colsample_bytree = trial.suggest_float('colsample_bytree', 0.3, 0.9)\n#     objective = trial.suggest_categorical('objective',['binary:logistic'])\n#     eval_metric = trial.suggest_categorical('eval_metric',['auc','logloss'])\n#     booster = trial.suggest_categorical('booster',['gbtree'])\n#     tree_method = trial.suggest_categorical('tree_method',['gpu_hist'])\n#     predictor = trial.suggest_categorical('predictor',['gpu_predictor'])\n#     subsample = trial.suggest_float('subsample', 0.3, 0.9)\n#     gamma = trial.suggest_float('gamma', 0.005, 0.5, log = True)\n#     reg_alpha = trial.suggest_float('reg_alpha', 0.1, 20, log = True)\n#     reg_lambda = trial.suggest_float('reg_lambda', 0.1, 20, log = True)\n#     min_child_weight = trial.suggest_int('min_child_weight', 1, 250)\n    \n#     params = {'n_estimators' : n_estimators, \n#               'use_label_encoder' : use_label_encoder,\n#               'max_depth' : max_depth, \n#               'learning_rate' : learning_rate,\n#               'colsample_bytree' : colsample_bytree, \n#               'objective' : objective, \n#               'eval_metric' : eval_metric,\n#               'booster' : booster, \n#               'subsample' : subsample, \n#               'tree_method' : tree_method,\n#               'predictor' : predictor,\n#               'gamma' : gamma, \n#               'reg_alpha' : reg_alpha, \n#               'reg_lambda' : reg_lambda,\n#               'min_child_weight' : min_child_weight, \n#               'random_state' : 42}\n    \n#     KFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n#     fold_scores = np.zeros(5)\n    \n#     for idx, (train_idx, valid_idx) in enumerate(KFold.split(X_train_final,Y_train_orig)):\n#         print('------------------------------Fold {}------------------------------'.format(idx))\n#         Xt, Xv = X_train_final.iloc[train_idx], X_train_final.iloc[valid_idx]\n#         Yt, Yv = Y_train_orig.iloc[train_idx], Y_train_orig.iloc[valid_idx]\n    \n#         xgb = XGBClassifier(**params)\n    \n#         xgb.fit(Xt, Yt, early_stopping_rounds = 50,\n#                eval_set = [(Xv,Yv)], verbose = False)\n#                #callbacks = [XGBoostPruningCallback(trial,'validation-error')])\n    \n#         preds = xgb.predict_proba(Xv)[:,1]\n        \n#         fold_scores[idx] = get_score(Yv,preds)\n    \n#     return np.mean(fold_scores)\n\n# study = optuna.create_study(direction = 'maximize')\n# study.optimize(objective_function, show_progress_bar = True, n_trials = 100)","c6ad007e":"# trial = study.best_trial\n# print('Best Value: {}'.format(trial.value))\n# print('Best Parameters: ')\n# for param, value in trial.params.items():\n#     print('{} : {}'.format(param,value))","77492520":"import time\n\nxgb_params = {'n_estimators' : 16678,\n             'use_label_encoder' : 'False',\n             'max_depth' : 3,\n             'learning_rate' : 0.027297134107723935,\n             'colsample_bytree' : 0.7843918860573006,\n             'objective' : 'binary:logistic', \n             'subsample' : 0.7459596984766819,\n             'gamma' : 0.05008218776821978,\n             'reg_alpha' : 0.861755644724069,\n             'reg_lambda' : 0.11499104081826494,\n             'min_child_weight' : 227, \n             'eval_metric' : 'logloss',\n             'booster' : 'gbtree',\n             'tree_method' : 'gpu_hist',\n             'predictor' : 'gpu_predictor',\n             'random_state' : 42}\n\nxgb = XGBClassifier(**xgb_params)\n\n#fit model\nxgb_fit_start = time.time()\nxgb.fit(X_train_final,Y_train_orig, early_stopping_rounds = 100,\n       eval_set = [(X_valid_final,Y_valid_orig)], verbose = False)\nxgb_fit_end = time.time()\n\n#make predictions\nxgb_valid_preds = xgb.predict_proba(X_valid_final)[:,1]\nxgb_train_preds = xgb.predict_proba(X_train_final)[:,1]\n\n#get scores\nxgb_valid_score = get_score(Y_valid_orig,xgb_valid_preds)\nxgb_train_score = get_score(Y_train_orig,xgb_train_preds)\n\nprint('Validation AUC score for XGBoost: {}'.format(xgb_valid_score))\nprint('Training AUC score for XGBoost: {}'.format(xgb_train_score))\nprint('Training time: {}'.format(xgb_fit_end - xgb_fit_start))","a6ad0707":"#storing score and time for comparison\nxgb_valid_score = 0.8150344124616878\nxgb_train_score = 0.8346657409072258\nxgb_time = 73.04902482032776","3d0e0a55":"#see documentation to analyse hyperparameters\n#help(CatBoostClassifier())","b1219e51":"#Y_train_cat = Y_train_orig.astype(float)\n\n# def objective_function(trial):\n#     n_estimators = trial.suggest_int('n_estimators', 1000, 20000, step = 100)\n#     learning_rate = trial.suggest_float('learning_rate', 0.001, 2, log = True)\n#     max_depth = trial.suggest_int('max_depth', 4, 10)\n#     reg_lambda = trial.suggest_float('reg_lambda', 0.1, 50, step = 0.1)\n#     random_strength = trial.suggest_float('random_strength', 0.1, 50, log = True)\n#     loss_function = trial.suggest_categorical('loss_function', ['Logloss','CrossEntropy'])\n#     bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian','Bernoulli'])\n#     subsample = None\n#     bagging_temperature = None\n#     if(bootstrap_type == 'Bernoulli'):\n#         subsample = trial.suggest_float('subsample', 0.3, 1)\n#     elif(bootstrap_type == 'Bayesian'):\n#         bagging_temperature = trial.suggest_float('bagging_temperature', 0, 1)\n#     colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.3, 1)\n    \n#     params = {'n_estimators' : n_estimators,\n#              'learning_rate' : learning_rate,\n#              'max_depth' : max_depth,\n#              'reg_lambda' : reg_lambda,\n#              'random_strength' : random_strength,\n#              'loss_function' : loss_function,\n#              'bootstrap_type' : bootstrap_type,\n#              'subsample' : subsample,\n#              'bagging_temperature' : bagging_temperature,\n#              'colsample_bylevel' : colsample_bylevel,\n#               'thread_count' : 4,\n#              'random_state' : 42}\n    \n#     KFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n#     fold_scores = np.zeros(5)\n    \n#     for idx, (train_idx, valid_idx) in enumerate(KFold.split(X_train_final,Y_train_orig)):\n#         print('------------------------------Fold {}------------------------------'.format(idx))\n#         Xt, Xv = X_train_final.iloc[train_idx], X_train_final.iloc[valid_idx]\n#         Yt, Yv = Y_train_orig.iloc[train_idx], Y_train_orig.iloc[valid_idx]\n        \n#         cat = CatBoostClassifier(**params)\n        \n#         cat.fit(Xt, Yt, early_stopping_rounds = 50,\n#                eval_set = [(Xv,Yv)], verbose = False)\n        \n#         preds = cat.predict_proba(Xv)[:,1]\n        \n#         fold_scores[idx] = get_score(Yv,preds)\n        \n#     return np.mean(fold_scores)\n    \n\n# study = optuna.create_study(direction = 'maximize')\n# study.optimize(objective_function, show_progress_bar = True, n_trials = 20)","6d3a1d36":"# trial = study.best_trial\n# print('Best Value: {}'.format(trial.value))\n# print('Best Parameters:')\n# for param, value in trial.params.items():\n#     print('{} : {}'.format(param,value))","745abc11":"# cat_params = {'n_estimators' : 14900,\n#              'learning_rate' : 0.005321048007093712,\n#              'max_depth' : 9,\n#              'reg_lambda' : 7.3,\n#              'random_strength' : 2.948068606481162,\n#              'loss_function' : 'CrossEntropy',\n#              'bootstrap_type' : 'Bayesian',\n#              'bagging_temperature' : 0.7209605229200341,\n#              'colsample_bylevel' : 0.3672524368136758,\n#              'random_state' : 42}\n\n# cat  = CatBoostClassifier(**cat_params)\n\n# #fit model\n# cat_fit_start = time.time()\n# cat.fit(X_train_final,Y_train_orig,early_stopping_rounds = 50,\n#        eval_set = [(X_valid_final,Y_valid_orig)], verbose = False)\n# cat_fit_end = time.time()\n\n# #make predictions\n# cat_valid_preds = cat.predict_proba(X_valid_final)[:,1]\n# cat_train_preds = cat.predict_proba(X_train_final)[:,1]\n\n# #get score\n# cat_valid_score = get_score(Y_valid_orig,cat_valid_preds)\n# cat_train_score = get_score(Y_train_orig,cat_train_preds)\n\n# print('Validation AUC score for CatBoost: {}'.format(cat_valid_score))\n# print('Training AUC score for CatBoost: {}'.format(cat_train_score))\n# print('Training Time: {}'.format(cat_fit_end - cat_fit_start))","a3d03775":"cat_valid_score = 0.8141588697756169\ncat_train_score = 0.8819324067789954\ncat_time = 5925.859116315842","fc0625fd":"#see documentation to analyse hyperparameters\n#help(LGBMClassifier())","7c49515b":"# from optuna.integration import LightGBMPruningCallback\n\n# def objective_function(trial):\n#     n_estimators = trial.suggest_int('n_estimators', 5000, 20000)\n#     max_depth = trial.suggest_int('max_depth', 3, 10)\n#     learning_rate = trial.suggest_float('learning_Rate', 0.001, 10, log = True)\n#     num_leaves = trial.suggest_int('num_leaves', 1, 500, step = 10)\n#     objective = trial.suggest_categorical('objective', ['binary'])\n#     metric = trial.suggest_categorical('metric', ['auc'])\n#     min_child_weight = trial.suggest_float('min_child_weight', 0.0001, 200, log = True)\n#     subsample = trial.suggest_float('subsample', 0.3, 0.9)\n#     subsample_freq = trial.suggest_categorical('subsample_freq', [1])\n#     colsample_bytree = trial.suggest_float('colsample_bytree', 0.3, 0.9)\n#     reg_alpha = trial.suggest_float('reg_alpha', 1, 50, step = 0.1)\n#     reg_lambda = trial.suggest_float('reg_lambda', 1, 50, step = 0.1)\n#     min_gain_to_split = trial.suggest_float('min_gain_to_split', 0.1, 30, step = 0.1)\n#     #device = 'gpu'\n#     #gpu_platform_id = 0\n#     #gpu_device_id = 0\n    \n#     params = {'n_estimators' : n_estimators, \n#               'max_depth' : max_depth, \n#               'learning_rate' : learning_rate, \n#               'num_leaves' : num_leaves,\n#               'objective' : objective, \n#               'metric' : metric, \n#               'min_child_weight' : min_child_weight, \n#               'subsample' : subsample, \n#               'subsample_freq' : subsample_freq, \n#               'colsample_bytree' : colsample_bytree, \n#               'reg_alpha' : reg_alpha, \n#               'reg_lambda' : reg_lambda, \n#               'min_gain_to_split' : min_gain_to_split, \n#               #'device' : device,\n#               #'gpu_platform_id' : gpu_platform_id,\n#               #'gpu_device_id' : gpu_device_id,\n#               'random_state' : 42}\n\n#     KFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n#     fold_scores = np.zeros(5)\n    \n#     for idx, (train_idx, valid_idx) in enumerate(KFold.split(X_train_final,Y_train_orig)):\n#         print('------------------------------Fold {}------------------------------'.format(idx))\n#         Xt, Xv = X_train_final.iloc[train_idx], X_train_final.iloc[valid_idx]\n#         Yt, Yv = Y_train_orig.iloc[train_idx], Y_train_orig.iloc[valid_idx]\n        \n#         lgbm = LGBMClassifier(**params)\n        \n#         lgbm.fit(Xt,Yt,early_stopping_rounds = 50,\n#                  eval_metric = 'auc', eval_set = [(Xv,Yv)], verbose = False,\n#                  callbacks = [LightGBMPruningCallback(trial, 'auc')],verbose = False)\n        \n#         preds = lgbm.predict_proba(Xv)[:,1]\n        \n#         fold_scores[idx] = get_score(Yv,preds)\n        \n#         #results = lgbm.evals_result_\n#         #epochs = len(results['validation_0']['auc'])\n#         #fig, ax = plt.subplots()\n#         #ax.plot(range(0,epochs), results['validation_0']['auc'])\n#         #plt.xlabel('epochs') \n#         #plt.ylabel('auc')\n    \n#     return np.mean(fold_scores)\n\n# study = optuna.create_study(direction = 'maximize')\n# study.optimize(objective_function, show_progress_bar = True, n_trials = 40)","ff117b6b":"# trial = study.best_trial\n# print('Best Value: {}'.format(trial.value))\n# print('Best Parameters:')\n# for param, value in trial.params.items():\n#     print('{} : {}'.format(param,value))","8a069d77":"# lgbm_params = {'n_estimators' : 15219, \n#                'max_depth' : 8,\n#                'learning_rate' : 0.015144053813488484,\n#                'num_leaves' : 351,\n#                'objective' : 'binary',\n#                'metric' : 'auc',\n#                'min_child_weight' : 31.100300067957512,\n#                'subsample' : 0.7096068189924443,\n#                'subsample_freq' : 1,\n#                'colsample_bytree' : 0.45821320586324304,\n#                'reg_alpha' : 19.400000000000002,\n#                'reg_lambda' : 5.2,\n#                'min_gain_to_split' : 3.1,\n#                'random_state' : 42}\n\n# lgbm = LGBMClassifier(**lgbm_params)\n\n# #fit model\n# lgbm_fit_start = time.time()\n# lgbm.fit(X_train_final,Y_train_orig,early_stopping_rounds = 50,\n#         eval_metric = 'auc', verbose = False, eval_set = [(X_valid_final,Y_valid_orig)])\n# lgbm_fit_end = time.time()\n\n# #make predictions\n# lgbm_valid_preds = lgbm.predict_proba(X_valid_final)[:,1]\n# lgbm_train_preds = lgbm.predict_proba(X_train_final)[:,1]\n\n# #get score\n# lgbm_valid_score = get_score(Y_valid_orig,lgbm_valid_preds)\n# lgbm_train_score = get_score(Y_train_orig,lgbm_train_preds)\n\n# print('Validation AUC score for LGBM: {}'.format(lgbm_valid_score))\n# print('Training AUC score for LGBM: {}'.format(lgbm_train_score))\n# print('Training Time: {}'.format(lgbm_fit_end - lgbm_fit_start))","77725e1b":"lgbm_valid_score = 0.8148821960264597\nlgbm_train_score = 0.8416042244866605\nlgbm_time = 587.968177318573","fe0384e6":"#overfitting metric, training auc - validatio auc\nxgb_of = xgb_train_score - xgb_valid_score\ncat_of = cat_train_score - cat_valid_score\nlgbm_of = lgbm_train_score - lgbm_valid_score\n\nxgb_eval = {'Model' : 'XGBoost',\n           'Train Time' : xgb_time,\n           'Train AUC' : xgb_train_score,\n           'Validation AUC' : xgb_valid_score,\n           'Overfitting' : xgb_of}\n\ncat_eval = {'Model' : 'CatBoost',\n           'Train Time' : cat_time,\n           'Train AUC' : cat_train_score,\n           'Validation AUC' : cat_valid_score,\n           'Overfitting' : cat_of}\n\nlgbm_eval = {'Model' : 'LightGBM',\n            'Train Time' : lgbm_time,\n            'Train AUC' : lgbm_train_score,\n            'Validation AUC' : lgbm_valid_score,\n            'Overfitting' : lgbm_of}\n\nevaluations = pd.DataFrame({'Model' : [],\n                           'Train Time' : [],\n                           'Train AUC' : [],\n                           'Validation AUC' : [],\n                           'Overfitting' : []})\nevaluations = evaluations.append([xgb_eval,cat_eval,lgbm_eval], ignore_index = True)\nevaluations.set_index('Model', inplace = True)\n\nevaluations","d43383b8":"X_test_final.shape","cf519972":"#prediction\nxgb_test_preds = xgb.predict_proba(X_test_final)[:,1]\n\n#reshape distorted prediction array\nxgb_test_preds = xgb_test_preds.reshape(len(X_test_final),)","76e9349c":"output = pd.DataFrame({'id' : X_test_final.index, 'claim' : xgb_test_preds})\n\noutput.to_csv('submission.csv', index = False)","9a59b855":"There are a few darker cells, which represent relatively strong correlation between the concerning features\/variables. However, even these *relatively* strong correlations have very small correlation coefficient values from a general P.O.V. To elaborate, the slider on the right depicts that the upper bound on positive correlations is approx 0.04 and the lower bound on negative correlations is approx -0.06. These two bounds are too small to declare a strong correlation between the features.\n\n**Ps:** Here, I define a *strong correlation* as one having correlation coefficient value greater than 0.6 (meaning strong positive correlation) or less than -0.6 (meaning strong negative correlation). Of course, these thresholds are subject to the author.","03fb8036":"That's a lot of plots to look at. However, at a quick glance at all the plots, there doesn't seem to be a pattern in any of the distributions w.r.t. the target variable. We will now analyse these weak relations further using a correlation matrix.","c4a38524":"- **Training Final XGB Model**","43c6771d":"**2.1 MISSING VALUES**\n\nAs we saw earlier, most of the features have missing values. We will take care of that now.\n\nLuckily, for the given dataset, we have only numerical features and hence, imputation will be lot more simpler. For numerical data, two most suitable imputation techniques that could be used here are *mean imputation* and *median imputation*. I will try both these techniques and compare their performance on the validation set. In the final notebook, you will only see the technique which performed better. ","b4d37d6e":"- **Optimizing Objective Function With Optuna**","64a5cdd4":"There are a total of *957919* training examples, having *118* features ranging from 'f1' to 'f118', and *1* target column, i.e. *claim* which corresponds to - whether the claim was made (1) or not (0).","834b86d6":"- **Printing Best Parameters**","7a32a109":"![image.png](attachment:20cba787-26e2-4a48-9923-02e21a387dfc.png)","9fb0395b":"- **Printing Best Parameters**","ebcd1e48":"# **PROBLEM STATEMENT**\n\nWe are given the following:\n\n1. A train dataset (.csv) containing the index column (0 to n_train_examples-1), features ('f1' to 'f118') and the ground truth *claim* (0 or 1) respectively.\n2. A test dataset (.csv) containing the index column (0 to n_test_examples-1), features ('f1' to 'f118') respectively.\n\nWe are required to implement a binary-classification algorithm which predicts for each example of the test dataset, whether a customer made a claim upon an insurance policy. A '1' value means a claim was made, and '0' means a claim was not made.","89235b99":"**1.2 FEATURE ENGINEERING**\n\nSince we are going to impute NaN later, we would end up losing the information about missing values for the training examples. For that reason, let's add a column which stores the number of NaN entries for each training example.","580cf751":"- **Saving Scores For Comparison**","6831aff5":"**1.4 CORRELATION ANALYSIS**\n\nWe noticed earlier that the relation between features and the target variable is most likely weak. To check that further, we'll make use of a correlation matrix. Also, this matrix will help us to check which features are strongly related to one another.","3809d448":"This notebook is a continuation of my previous notebook [TPS Sept: EDA & Baseline Models](https:\/\/www.kaggle.com\/jaikr18\/tps-sept-eda-baseline-models), wherein I focused mainly on EDA and then trained baseline models for *XGBClassifier*, *CatBoostClassifier*, and *LGBMClassifier*. In this notebook, I will focus on optimization of these baseline models. ","176f5b8d":"# **5. SUBMISSION**\n\nLet's predict the claim variable for test set and submit our results!","8891b5d6":"**4.1.2 CatBoost Classifier**","516c6ca1":"# **1. EXPLORATORY DATA ANALYSIS**\n\n**1.0 DUPLICATE REMOVAL**\n\nFirst off, there is always a possibility that our dataset is having duplicate entries. This is typically a fault of the data acquisition step.","4620799b":"- **Optimizing Objective Function Wtih Optuna**","dfe25e47":"# **0. DATASET**","472c9908":"On passing through the Scalers, our Data Frame has now been converted to a numpy array. So, for convention, we will convert the array back to a Data Frame.","ef8f6501":"**4.3 COMPARE PERFORMANCES**","b78d0c22":">\"Perfectly balanced, as all things should be.\"","3620025c":"Seems pretty well balanced. Let's confirm this notion through a pie chart (because \"visual data is always more convincing\").","e0131ef2":"- **Optimizing Objective Function Using Optuna**","498ce447":"**4.1 OPTIMIZATION**\n\nWhen we talk about optimizing our models, we mean optimizing the hyperparameters of our model and getting the best set of hyperparameters that would maximize the performance of that particular model. To carry out this task, we have a number of techniques, namely:\n\n- Grid Search with cross-validation. Here, we have to feed a grid of different hyperparamer values and a model will be trained for every possible combination of hyperparameter values. In the end, the combination that gave the best result is selected.\n- Randomized Search with cross-validation. Here, we do not specify a set of hyperparameter values. Instead, we sample values from a statistical distribution for each hyperparameter. It might not select the absolute best combination of hyperparameters but it's always pretty close to the best, **and** it runs a lot faster than Grid Search. \n- Bayesian Optimization methods. Here, in contrast to random or grid search, we keep track of past evaluation results which are used to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function. Too much info right? Refer to [this](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f) blog for more details.\n\nSince we have a large dataset for which model fitting would be very costly, we will go with Bayesian Optimization.","9cc1413e":"All the features in the dataset are of type *float64*, and the ground truth column, i.e. *claim* is of type *int16*.","8f71b955":"- **Saving CV Score For Comparison**","61d3d2db":"![image.png](attachment:2985de9b-c19b-47cf-84b8-f991888b623f.png)","8eeb59d6":"![image.png](attachment:bfea779f-9394-4da7-860a-542db28fb9ce.png)","a9d8f277":"![image.png](attachment:84f46af2-599d-4c00-9252-37849be6ec3b.png)","2b19b891":"- **Training Final CatBoost Model**","6adee770":"**1.1 TARGET COLUMN**","8088825c":"**0.2 MEMORY REDUCTION**","632051ee":"![image.png](attachment:458ce40c-7305-4471-9ba8-6313f75b7727.png)","575005e0":"**0.1 DATASET OVERVIEW**","9770d16c":"- **Saving Scores For Comparison**","55fdc75c":"# **2. DATA CLEANING**\n\n**2.0 DATASET SPLIT**\n\nBefore proceeding any further, it is recommended to split the dataset into a training set and a hold-out cross-validation set. This is to ensure that the model we build won't be adversely affected by data leakage.\n\n> Any feature whose value would not actually be available in practice at the time you\u2019d want to use the model to make a prediction, is a feature that can introduce leakage to your model","80821c19":"# **IMPORT LIBRARIES**","64ff6632":"- **Training Final LGBM Model**","2a0c4e81":"**Conclusion:** We can now safely say that none of the features have a strong correlation among one another, or with the target variable. This marks the end of a fruitless correlation analysis. ","9fbf67f6":"**4.1.1 XGB CLASSIFIER**","fb49d285":"# THANK YOU FOR READING!","b555eec2":"This means that our dataset has only unique entries. Having ensured this, now we can proceed to the actual EDA for our dataset.","e03c86c5":"# **REFERENCES**\n\n- Hyperparameter optimization using Bayesian methods --> [blog](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n- How to make your model awesome with Optuna --> [blog](https:\/\/towardsdatascience.com\/how-to-make-your-model-awesome-with-optuna-b56d490368af)\n- LightGBM Classifier in Python --> [notebook](https:\/\/www.kaggle.com\/prashant111\/lightgbm-classifier-in-python)\n- Handling large datasets --> [notebook](https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro)\n- Accelerating XGBoost with GPU --> [notebook](https:\/\/www.kaggle.com\/vinhnguyen\/accelerating-xgboost-with-gpu)","c8dc5bea":"Surprisingly, every feature has missing entries. However, the number of missing entries as compared to the entire dataset is quite small.","90cc274e":"For a large dataset such as this one, one might often face situations where the system runs out of RAM. Thus, it would be wise to cut down on the memory usage.","edb1bb63":"![image.png](attachment:6fb59371-23fc-415d-9824-8c6116f5e2b9.png)","2f8f17a9":"# **4. MODEL FITTING AND EVALUATION**\n\nIn the last notebook, linked -> [TPS Sept: EDA & Baseline Models](https:\/\/www.kaggle.com\/jaikr18\/tps-sept-eda-baseline-models), we trained three baseline models, namely *XGBClassifier*, *CatBoostClassifier*, and *LGBMClassifier*. Their performance evaluations are as shown below:\n\n![image.png](attachment:e6b210b7-66b7-4c1b-889f-8931dd6610b9.png)\n\nFrom here on, the main emphasis will be on optimizing these baseline models in order to get better performance.","a427a8bf":"**4.1.3 LIGHTGBM CLASSIFIER**","39af35eb":"**CONCLUSION**\n\n- There is slight overfitting in *CatBoost* as compared to the other two.\n- If training time is considered, then *XGBoost* has clearly outperformed the other two due to the additional GPU support, whereas *CatBoost* took more than 10x the time taken by *LightGBM*.\n- If validation AUC is considered, then we can say that both *XGBoost* and *LightGBM* performed equally well on the validation set, with *XGBoost* being just a tad bit better (difference in the AUC scores is approximately **0.0001**).\n\nSo, all things considered, we have two almost equally well performing models. Out of these, *XGBoost* was just a speck ahead due to better training time and slightly higher validation score.\n\nOn a different note, the training AUC score for *XGBoost* is closer to its validation AUC as compared to the training AUC of *LightGBM* and its validation AUC.","78bb2808":"- **Printing Best Parameters**","dcd86dea":"**1.3 DISTRIBUTION ANALYSIS**\n\nLet's see how the features are distributed w.r.t. the target variable.\n\n**NOTE:** Since we have a very large dataset, we will plot these distributions taking a small sample from the dataset. For better estimations, we will take a random sample, preferably of fraction 1\/100 of the original dataset. This will help in faster generation of plots.","88712ae7":"As expected, the dataset is far from standard with some features taking exponentially large values while some other taking exponentially small values. Also, most features seem to be having missing values, so we will have to take care of these things at a later point.","cf78fa90":"# **3. FEATURE SCALING**\n\n\n\nMany machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nStandardizing is a popular scaling technique that subtracts the mean from values and divides by the standard deviation, transforming the probability distribution for an input variable to a standard Gaussian (zero mean and unit variance). Standardization can become skewed or biased if the input variable contains outlier values.\n\nTo overcome this, the median and interquartile range can be used when standardizing numerical input variables, generally referred to as robust scaling.","d8fb7da9":"**0.0 LOADING DATASET**"}}