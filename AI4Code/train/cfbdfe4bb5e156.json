{"cell_type":{"4b7967bb":"code","3a8847e8":"code","88962af9":"code","26dda4d9":"code","ea12c78a":"code","7fc3d61d":"code","f45c7021":"code","cd1275df":"code","077effe7":"code","c0e6955f":"code","0189424c":"code","295988e0":"code","76726d3e":"code","a806a794":"code","b881b25f":"code","53cbed06":"code","0e7fa7ad":"code","378c8caf":"code","7407e10e":"code","bf760deb":"code","3f71995d":"code","b048681a":"markdown"},"source":{"4b7967bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3a8847e8":"print(os.listdir(\"..\/input\/virtual-hack\"))","88962af9":"#importing all the packages\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models, datasets, transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data.sampler import SubsetRandomSampler","26dda4d9":"data_dir = \"..\/input\/virtual-hack\/car_data\/car_data\/train\"\nnames = \"..\/input\/virtual-hack\/names.csv\"\nannoTrain = \"..\/input\/virtual-hack\/anno_train.csv\"\nanno = pd.read_csv(annoTrain)\nanno.head()","ea12c78a":"# Transform the image (scaling, flipping and normalisation)\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n       # transforms.RandomRotation(30),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n    ]),\n}\n\n\nimage_datasets = {x: datasets.ImageFolder(data_dir, data_transforms[x])\n                  for x in ['train', 'valid']}\n\n#info about no. of datapoints\nimage_datasets","7fc3d61d":"valid_size = 0.2\nbatch_size = 64\n\nlength_train=len(image_datasets['train'])\nindices=list(range(length_train))\nsplit = int(np.floor(valid_size * length_train))\n\ntrain_idx, valid_idx = indices[split:], indices[:split]\ntrain_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\nvalid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n\ntrain_loader=torch.utils.data.DataLoader(image_datasets[\"train\"],batch_size=batch_size,sampler=train_sampler,shuffle=False)\nvalid_loader=torch.utils.data.DataLoader(image_datasets[\"valid\"],batch_size=batch_size,sampler=valid_sampler,shuffle=False)","f45c7021":"pd_names = pd.read_csv(names)\nprint(pd_names.shape) # number of target output classes\npd_names","cd1275df":"def show(image):\n    if isinstance(image, torch.Tensor):\n        image = image.numpy().transpose((1, 2, 0))\n    else:\n        image = np.array(image).transpose((1, 2, 0))\n    # denormalisation\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = std * image + mean\n    image = np.clip(image, 0, 1)\n    # plot\n    fig, Xaxis = plt.subplots(1, 1, figsize=(9, 9))\n    %matplotlib inline\n    plt.imshow(image)\n    Xaxis.axis('off') ","077effe7":"# Make a grid from batch (for training data)\n# This grid shows the images which are present in 1 batch\nimages, _ = next(iter(train_loader))\n#print(train_loader.dataset.targets)\ntrainGrid = torchvision.utils.make_grid(images, nrow=8)\n\nshow(trainGrid)","c0e6955f":"# Make a grid from batch (for validation\/test data)\nimages, _ = next(iter(valid_loader))\ntestGrid = torchvision.utils.make_grid(images, nrow=8)\n\nshow(testGrid)","0189424c":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","295988e0":"# Use GPU if it's available\n\nmodel = models.resnet101(pretrained=True)\n#print(model)\n\n# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n    \nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(nn.Linear(num_ftrs, 196),\n                         nn.LogSoftmax(dim=1))\n    \n# model.classifier = nn.Sequential(nn.Dropout(0.1),\n#                                  nn.Linear(9216, 2048),\n#                                  nn.ReLU(),\n#                                  nn.Dropout(0.1),\n#                                  nn.Linear(2048, 256),\n#                                  nn.ReLU(),\n#                                  nn.Linear(256, 196),\n#                                  nn.LogSoftmax(dim=1))\n\ncriterion = nn.NLLLoss()\n\n# Only train the classifier parameters, feature parameters are frozen\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\nmodel.to(device)","76726d3e":"def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n  \n    steps = 0\n    running_loss = 0\n    for e in range(epochs):\n        \n        # Model in training mode, dropout is on\n        model.train()\n        for images, labels in trainloader:\n            \n            steps += 1\n            \n            # Flatten images into a 784 long vector\n            #images.resize_(images.size()[0], -1)\n            \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            #optimizer.zero_grad()\n            \n            output = model.forward(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                # Model in inference mode, dropout is off\n                model.eval()\n                \n                # Turn off gradients for validation, will speed up inference\n                with torch.no_grad():\n                    test_loss, accuracy = validation(model, testloader, criterion)\n                \n                print(\"Epoch: {}\/{}.. \".format(e+1, epochs),\n                      \"Training Loss: {:.3f}.. \".format(running_loss\/print_every),\n                      \"Test Loss: {:.3f}.. \".format(test_loss\/len(testloader)),\n                      \"Test Accuracy: {:.3f}\".format(accuracy\/len(testloader)))\n                \n                running_loss = 0\n                \n                # Make sure dropout and grads are on for training\n                model.train()\n\ndef validation(model, testloader, criterion):\n  \n    accuracy = 0\n    test_loss = 0\n    for images, labels in testloader:\n\n        #images = images.resize_(images.size()[0], -1)\n        \n        images = images.to(device)\n        labels = labels.to(device)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ## Calculating the accuracy \n        # Model's output is log-softmax, take exponential to get the probabilities\n        ps = torch.exp(output)\n        # Class with highest probability is our predicted class, compare with true label\n        equality = (labels.data == ps.max(1)[1])\n        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n        accuracy += equality.type_as(torch.FloatTensor()).mean()\n\n    return test_loss, accuracy","a806a794":"# uncomment these two lines when you want to re-train\n\n#train(model, train_loader, valid_loader, criterion, optimizer, epochs=30)\n#validation(model, valid_loader, criterion)","b881b25f":"# train_idx, valid_idx = indices[split:], indices[split:]\n# train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n# valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n\n# train_loader=torch.utils.data.DataLoader(image_datasets[\"train\"],batch_size=batch_size,sampler=train_sampler,shuffle=False)\n# valid_loader=torch.utils.data.DataLoader(image_datasets[\"valid\"],batch_size=batch_size,sampler=valid_sampler,shuffle=False)","53cbed06":"# # Save the checkpoint\n\n# checkpoint = {'input_size': num_ftrs,\n#               'output_size': 196,\n#               'epochs': 50,\n#               'fc': model.fc,\n#               'optimizer_state': optimizer.state_dict(),\n#               'mapping': image_datasets['train'].class_to_idx,\n#               'state_dict': model.state_dict()}\n\n# # Save the checkpoint \n# torch.save(checkpoint, 'checkpoint.pth')\n# # idx_to_class = {}\n# # for x in image_datasets['train'].class_to_idx:\n# #     idx_to_class[image_datasets['train'].class_to_idx[x]] = labelToName[x]","0e7fa7ad":"def load_checkpoint(filepath):\n    print('Loading checkpoint...')\n    checkpoint = torch.load(filepath, map_location='cpu')\n    model = models.resnet101(pretrained=True)\n    model.fc = checkpoint['fc']\n    model.optimizer_state = checkpoint['optimizer_state']\n    model.mapping = checkpoint['mapping']\n    model.load_state_dict(checkpoint['state_dict'])\n    print('Done')\n    return model","378c8caf":"model = load_checkpoint('checkpoint.pth')","7407e10e":"!wget https:\/\/raw.githubusercontent.com\/Iamsdt\/60daysofudacity\/master\/day22\/Helper.py","bf760deb":"test_dir = \"..\/input\/virtual-hack\/car_data\/car_data\/test\"","3f71995d":"test_transform = transforms.Compose([transforms.Resize(224),\n                                     transforms.CenterCrop(224),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize([0.485, 0.456, 0.406], \n                                                          [0.229, 0.224, 0.225])\n                                    ]),","b048681a":"## Training the model"}}