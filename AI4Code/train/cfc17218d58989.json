{"cell_type":{"db5a829f":"code","fb062869":"code","0db87670":"code","12e7ab57":"code","708b5220":"code","c7e6591f":"code","8c0476ba":"code","00e5b500":"code","10a9eabe":"code","0997f076":"code","6bb8cc56":"code","27623aa4":"code","6f6c2f96":"code","51e3374e":"code","cc5bcff6":"code","d6be6282":"code","c8096993":"code","ec3c730d":"code","da053e92":"code","14b32ee3":"code","5fc3dac2":"code","029ae204":"code","e2e1af5f":"code","667be1a5":"code","99f4e3b0":"code","9c33416e":"code","4ef7799d":"code","44664874":"code","aa2bccf1":"code","3e1af29e":"code","8321ec91":"code","a2fd0a03":"code","12964a7b":"code","7268daf9":"code","32c324f3":"code","9ac58d4a":"code","4cde13d6":"code","dfffaab0":"code","2fc0fa50":"code","b74bad80":"code","6e7a5a2b":"code","4b634c4b":"code","01ab6d9b":"code","df37490a":"code","3375b1e4":"code","0bce9be0":"code","aee3f935":"code","fa624e27":"code","434f7525":"code","2887f127":"code","bf1aef1a":"code","10c84b7d":"code","d35d8ff8":"code","4f24f9ac":"code","7207a945":"markdown","7ff7ba49":"markdown","3fbe578a":"markdown","697e4253":"markdown","77e20395":"markdown","dcd4beb4":"markdown","276e3b1a":"markdown","221bc880":"markdown","b3de0ba6":"markdown","59f5a131":"markdown","3c6f1931":"markdown","0872a121":"markdown","60bc9689":"markdown","45946367":"markdown","18241cd2":"markdown","23a99d7d":"markdown"},"source":{"db5a829f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk \nimport string\nimport re\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n#import geopandas as gpd\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB","fb062869":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","0db87670":"train.head()","12e7ab57":"train['keyword'].unique() ","708b5220":"train['location'].unique()","c7e6591f":"train[train['keyword'] == 'accident'].head(7)","8c0476ba":"train[train['location'] == 'Canada'].head(7)","00e5b500":"tweetLengthTrain = train['text'].str.len()\ntweetLengthTest = test['text'].str.len()\n\nplt.hist(tweetLengthTrain,bins=20,label='Train_Tweet')\nplt.hist(tweetLengthTest,bins=20,label='Test_Tweet')\nplt.legend()\nplt.show()","10a9eabe":"train[\"target_mean\"] = train.groupby(\"keyword\")[\"target\"].transform(\"mean\")\n\nplt.figure(figsize=(8,72))\n\nsns.countplot(y=train.sort_values(\"target\",ascending=False)[\"keyword\"],hue=\\\n              train.sort_values(\"target\",ascending=False)[\"target\"])\nplt.tick_params(axis=\"x\",labelsize=15)\nplt.tick_params(axis=\"y\",labelsize=15)\nplt.title(\"Target Distribution in Keywords\")\nplt.show()","0997f076":"train_df=train\ntest_df=test","6bb8cc56":"combine = train.append(test,ignore_index=True)\nprint('Shape of new Dataset:',combine.shape)\ncombine.tail()","27623aa4":"text_tweet = combine[\"text\"]\ntext_tweet.tail()","6f6c2f96":"df =  pd.DataFrame(combine[['text']])\ndf.head(7)","51e3374e":"def remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation]) #all punctuations \n    text = re.sub('[0-9]+', '', text) #all numbers\n    return text\n\ndf['Tweet_punct'] = df['text'].apply(lambda x: remove_punct(x))\ndf.head(10)","cc5bcff6":"df[\"Tweet_punct\"].drop_duplicates(inplace = True)\ndf[\"Tweet_punct\"].shape","d6be6282":"combine[\"Tweet_punct\"] = df['Tweet_punct']\ncombine.head()","c8096993":"#important libraries for preprocessing using NLTK\nimport nltk\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.porter import * ","ec3c730d":"df = combine[combine['Tweet_punct'].notna()]\ndf.head()","da053e92":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","14b32ee3":"stemmer = PorterStemmer()\ndf['Tweet_tokenized'] = df['Tweet_tokenized'].apply(lambda x : [stemmer.stem(i) for i in x]  )\ndf.head()","5fc3dac2":"for i in range(len(df['Tweet_tokenized'])):\n    df['Tweet_tokenized'][i] = ' '.join(df['Tweet_tokenized'][i])    \ndf['cleanedText'] = df['Tweet_tokenized']","029ae204":"df.head()","e2e1af5f":"# Creating word Cloud for all Words in all tweets\nfrom wordcloud import WordCloud\nallWords = ' '.join([text for text in df['cleanedText']])\nwordcloud = WordCloud(background_color='white', width=800, height=500, random_state=25, max_font_size=110).generate(allWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","667be1a5":"from textblob import TextBlob\n\ndef sentiment_polarity(text):\n    tweet_text = TextBlob(str(text))\n    sentiment_value = tweet_text.sentiment.polarity\n    return sentiment_value\n\ndf['Tweet_Polarity'] = df['cleanedText'].apply(lambda x: sentiment_polarity(x.lower()))\ndf.head(10)","99f4e3b0":"def sentiment_analysis(value):\n    sentiment=\"\"\n    if(value<0.0):\n        sentiment = \"negative\"\n    elif(value>0.0):\n        sentiment = \"positive\"\n    else:\n        sentiment = \"neutral\"\n    return sentiment\n\ndf['Tweet_Sentiments'] = df['Tweet_Polarity'].apply(lambda x: sentiment_analysis(float(x)))\ndf[[\"Tweet_punct\",\"Tweet_Polarity\",\"Tweet_Sentiments\"]].head(10)","9c33416e":"import seaborn as sns\nax = sns.countplot(x=\"Tweet_Sentiments\", data=df)","4ef7799d":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['cleanedText'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","44664874":"from gensim.corpora import Dictionary\n\n#create dictionary\ntext_dict = Dictionary(df.Tweet_tokenized)\n\n#view integer mappings\ntext_dict.token2id","aa2bccf1":"tweets_bow = [text_dict.doc2bow(tweet) for tweet in df['Tweet_tokenized']]\ntweets_bow","3e1af29e":"from gensim.models.ldamodel import LdaModel\n\nk = 10\ntweets_lda = LdaModel(tweets_bow,\n                      num_topics = k,\n                      id2word = text_dict,\n                      random_state = 1,\n                      passes=10)\n\ntweets_lda.show_topics()","8321ec91":"# word_count\ntrain[\"word_count\"] = train[\"text\"].map(lambda x: len(str(x).split()))\ntest[\"word_count\"] = test[\"text\"].map(lambda x: len(str(x).split()))\n\n# unique_word_count \ntrain[\"unique_word_count\"] = train[\"text\"].map(lambda x:len(set(str(x).split())))\ntest[\"unique_word_count\"] = test[\"text\"].map(lambda x:len(set(str(x).split())))\n\n# stop_word_count\ntrain[\"stop_word_count\"] = train[\"text\"].map(lambda x: len([elt for elt in str(x).lower().split() \\\n                                                                  if elt in stopwords.words(\"english\")]))\ntest[\"stop_word_count\"] = test[\"text\"].map(lambda x:len([elt for elt in str(x).lower().split()\\\n                                                              if elt in stopwords.words(\"english\")]))\n# url_count \ntrain[\"url_count\"] = train[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                            if 'http' or 'https' in w]))\ntest[\"url_count\"] = test[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                          if \"http\" or \"https\" in w ]))\n# mean_word_length\ntrain[\"mean_word_length\"] = train[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\ntest[\"mean_word_length\"] = test[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\n\n#char_count \ntrain[\"char_count\"] = train[\"text\"].map(lambda x:len(str(x)))\ntest[\"char_count\"] = test[\"text\"].map(lambda x:len(str(x)))\n\n#punctuation_count\ntrain[\"punctuation_count\"] = train[\"text\"].map(lambda x: len([elt for elt in str(x) if elt in string.punctuation]))\ntest[\"punctuation_count\"] = test[\"text\"].map(lambda x:len([elt for elt in str(x) if elt in string.punctuation]))\n#hashtag_count\ntrain[\"hashtag_count\"] = train[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\ntest[\"hashtag_count\"] = test[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\n\n#mention_count\ntrain[\"mention_count\"] = train[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))\ntest[\"mention_count\"] = test[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))","a2fd0a03":"train.head(4)","12964a7b":"# Heureunder, we will explore the distribution of each mega-feature per target and per dataset\n# (train & test)\nMETA_FEATURES = [\"word_count\",\"unique_word_count\",\"stop_word_count\",\"url_count\",\\\n                 \"mean_word_length\",\"char_count\",\"punctuation_count\",\"hashtag_count\",\"mention_count\"]\nfig,ax = plt.subplots(nrows=len(META_FEATURES),ncols=2,figsize=(20,50),dpi=100)\nmask = train[\"target\"]==1\nfor i,feature in enumerate(META_FEATURES):\n\n    \n   sns.distplot(train[mask][feature],ax=ax[i,0],label=\"Disaster\",kde=False)\n   sns.distplot(train[~mask][feature],ax=ax[i,0],label=\"Not Disaster\",kde=False)\n   ax[i,0].set_title(\"{} target distribution in trainning dataset\".format(feature),fontsize=13)\n \n   sns.distplot(train[feature],ax=ax[i,1],label=\"Train Dataset\",kde=False)\n   sns.distplot(test[feature],ax=ax[i,1],label=\"Test Dataset\",kde=False)\n   ax[i,1].set_title(\"{} training and test dataset distributions \".format(feature),fontsize=13)\n   for j in range(2):\n        ax[i,j].set_xlabel(\" \")\n        ax[i,j].tick_params(axis=\"x\",labelsize=13)\n        ax[i,j].tick_params(axis=\"y\",labelsize=13)\n        ax[i,j].legend()\nplt.show()","7268daf9":"fig,ax = plt.subplots(1,2,figsize=(20,6))\n\ntrain.groupby(\"target\").count()[\"id\"].plot(kind=\"pie\",labels=[\"Not Disaster\",\"Disaster\"],\\\n                                              autopct=\"%1.1f pourcents\",ax=ax[0])\nsns.countplot(x=train[\"target\"],hue=train[\"target\"],ax=ax[1])\nax[1].set_xticklabels([\"Non Disaster\",\"Disaster\"])\nax[0].tick_params(axis=\"x\",labelsize=15)\nax[0].tick_params(axis=\"y\",labelsize=15)\nax[0].set_ylabel(\"\")\nax[1].tick_params(axis=\"x\",labelsize=15)\nax[1].tick_params(axis=\"y\",labelsize=15)\nax[0].set_title(\"Target distribution in training set\",fontsize=13)\nax[1].set_title(\"Target count in training set\",fontsize=13)","32c324f3":"# create the function which will be able to generate n_grams for each row of dataset.\ndef gen_n_grams(text,n_grams=1):\n    \"\"\" This function allow to extract the n_gram in the introduced text.\n    \n      @param text(str): the text that the function, will use to extract features (n_grams).\n      @param n_grams(int): the length of n_gram, that we will use.\n      @return ngrams(list): list of the ngrams in the intriduced text.\n    \"\"\"\n    tokens = [token for token in str(text).lower().split() if token not in stopwords.words(\"english\")]\n    ngrams = zip(*[tokens[i:] for i in range(n_grams)])\n    \n    return [\" \".join(gram) for gram in ngrams]\n\n# create the function which will be able to generate dataframe of n_gram features for disaster\n# and non disaster tweets.\ndef gen_df_ngrams(n_grams=1):\n    \"\"\" This function, allow to generate dataframes for n_grams in disaster tweets and non \n        disaster tweet\n    \"\"\"\n    mask = train[\"target\"]==1\n    disaster_unigrams = defaultdict(int)\n    non_disaster_unigrams = defaultdict(int)\n    \n    for tweet in train.loc[mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            disaster_unigrams[gram] +=1\n    for tweet in train.loc[~mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            non_disaster_unigrams[gram] +=1\n    df_disaster_n_grams = pd.DataFrame(sorted(disaster_unigrams.items(),reverse=True,key=\\\n                                              lambda item:item[1]))\n    df_non_disaster_n_grams = pd.DataFrame(sorted(non_disaster_unigrams.items(),reverse=True,key=\\\n                                                  lambda item:item[1]))\n    return df_disaster_n_grams,df_non_disaster_n_grams\n\n# Define function, which allow to plot the N most occured n_gram in disaster tweet \n# and non disaster tweet.\n\ndef plot_ngrams(df_disaster_n_grams,df_non_disaster_unigrams,N=100,n_grams=1):\n    \"\"\"This function,allow to plot the top most n_grams in disaster tweet and non disaser tweet.\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(18,50))\n    sns.barplot(y=df_disaster_n_grams[0].values[:N],x=df_disaster_n_grams[1].values[:N],ax=ax[0],\\\n               color=\"red\")\n    for i in range(2):\n        ax[i].tick_params(axis=\"x\",labelsize=15)\n        ax[i].tick_params(axis=\"y\",labelsize=15)\n        ax[i].set_xlabel(\"Occurences\")\n        ax[i].spines[\"right\"].set_visible(False)\n    sns.barplot(y=df_non_disaster_unigrams[0].values[:N],x=df_non_disaster_unigrams[1].values[:N],\\\n               ax=ax[1],color=\"green\")\n    ax[0].set_title(\"Top most {} {}_grams for disaster tweets\".format(N,n_grams),size=15)\n    ax[1].set_title(\"Top most {} {}_grams for non disaster tweets\".format(N,n_grams),size=15)","9ac58d4a":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams()\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams)","4cde13d6":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=2)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=2)","dfffaab0":"# extract the most 100 bigrams per target (Disaster and not Disaster) when n_grams=3\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=3)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=3)","2fc0fa50":"def process_text(text):\n    \"\"\"\n    Removes punctuations(if any), stopwords and returns a list words\n    \"\"\"\n    rm_pun = [char for char in text if char not in string.punctuation]\n    rm_pun = ''.join(rm_pun)\n    \n    return [word for word in rm_pun.split() if word.lower() not in stopwords.words('english')]","b74bad80":"cv = CountVectorizer(analyzer=process_text).fit(train['text'])","6e7a5a2b":"train_data=train\ntest_data=test","4b634c4b":"print(len(cv.vocabulary_))","01ab6d9b":"text10 = train['text'][9]\ntext10","df37490a":"cv10 = cv.transform([text10])\nprint(cv10)","3375b1e4":"print(cv.get_feature_names()[5375])\nprint(cv.get_feature_names()[11579])\nprint(cv.get_feature_names()[11856])\nprint(cv.get_feature_names()[13190])\nprint(cv.get_feature_names()[25334])","0bce9be0":"cv10.shape","aee3f935":"train_data_cv = cv.transform(train['text'])\nprint(train_data_cv.shape)","fa624e27":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer().fit(train_data_cv)","434f7525":"tfidf10 = tfidf.transform(cv10)\nprint(tfidf10)","2887f127":"train_data_tfidf = tfidf.transform(train_data_cv)\nprint(train_data_tfidf.shape)","bf1aef1a":"from sklearn.naive_bayes import MultinomialNB\ntarget_model = MultinomialNB().fit(train_data_tfidf, train_data['target'])","10c84b7d":"print('predicted:', target_model.predict(tfidf10)[0])\nprint('expected:', train_data.target[9])","d35d8ff8":"test_data_cv = cv.transform(test_data['text'])\n\ntest_data_tfidf = tfidf.transform(test_data_cv)","4f24f9ac":"target_model.predict(test_data_tfidf)","7207a945":"*Distribution of train and test tweets*","7ff7ba49":"**Topic Modelling: LDA**","3fbe578a":"*Combining train and test datasets for furthur model evaluation*","697e4253":"*Joining the tokenized tweets*","77e20395":"*Tokenizing Tweets*","dcd4beb4":"*Applying Dictionary of gensim to turn our tokenized documents into a id <-> term dictionary*","276e3b1a":"*Target Distribution of keywords*","221bc880":"*Finding tweets based on keyword 'accident'*","b3de0ba6":"*Naive Bayes Classifier Training the model*","59f5a131":"*Removing punctuation from tweets*","3c6f1931":"*Evaluating sentiment polarity of tweets using TextBlob*","0872a121":"*Predictions for test data test data vector and tf-idf*","60bc9689":"*Generate LDA model*","45946367":"*Finding tweets based on location 'Canada'*","18241cd2":"*Categorizing sentiments based on polarity value*","23a99d7d":"*convert tokenized documents into a document-term matrixa*"}}