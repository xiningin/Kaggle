{"cell_type":{"50145aef":"code","40d523e7":"code","ff1f008c":"code","147a94f5":"code","84ac4a37":"code","74e03681":"code","35e0c6ec":"code","00631efc":"code","5ef529ef":"code","a456b1f0":"code","9a619209":"code","cac79ee7":"code","9e4842fc":"code","e0702913":"code","e3d5761d":"code","7fa27ed1":"code","41d29dc4":"code","2c9090b5":"code","6a8315c2":"code","3b8e2a43":"code","eaff3f10":"code","79ba0c17":"code","b2667f84":"code","964a6dc6":"markdown","ca1e9483":"markdown","d879a37d":"markdown","b965620f":"markdown","20cbc5fa":"markdown","3f377a0d":"markdown","e5f8d297":"markdown","83516722":"markdown","12ea1fbb":"markdown","9272f7ff":"markdown"},"source":{"50145aef":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","40d523e7":"# Source Analytics Vidya","ff1f008c":"pip install findspark","147a94f5":"pip install pyspark","84ac4a37":"import pyspark.sql\nfrom pyspark.sql import SparkSession\n# create Spark Reader\nspark = SparkSession.Builder().appName('Sparky').getOrCreate()\n# read a csv file\nmy_data = spark.read.csv(r'\/kaggle\/input\/cricket-match\/ind-ban-comment.csv',header=True)\n\n# see the default schema of the dataframe\nmy_data.printSchema()","74e03681":"import pyspark.sql.types as tp\n\n# define the schema\nmy_schema = tp.StructType([\n    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),\n    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),\n    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),\n    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),\n    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),\n    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),\n    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),\n    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),\n    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True),\n    tp.StructField(name= 'Isboundary',   dataType= tp.BinaryType(),   nullable= True),\n    tp.StructField(name= 'Iswicket',     dataType= tp.BinaryType(),   nullable= True),\n    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),\n    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),\n    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    \n])\n\n# read the data again with the defined schema\n#my_data = spark.read.csv(r'C:\\Users\\muham\\Downloads\\ind-ban-comment.csv',schema= my_schema,header= True)\n\n# print the schema\n#my_data.printSchema()","35e0c6ec":"# drop the columns that are not required\nmy_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\nmy_data.columns","00631efc":"# get the dimensions of the data\n(my_data.count() , len(my_data.columns))\n# >> (605, 11)","5ef529ef":"# get the summary of the numerical columns\nmy_data.select('Isball', 'Isboundary', 'Runs').describe().show()","a456b1f0":"# import sql function pyspark\nimport pyspark.sql.functions as f\n\n# null values in each column\ndata_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])","9a619209":"data_agg","cac79ee7":"# value counts of Batsman_Name column\nmy_data.groupBy('Batsman_Name').count().show()","9e4842fc":"# String Indexing\n# String Indexing is similar to Label Encoding. \n# It assigns a unique integer value to each category.\n# 0 is assigned to the most frequent category, 1 to the next most frequent value\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\n# create object of StringIndexer class and specify input and output column\nSI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')\nSI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')\n\n# transform the data\nmy_data = SI_batsman.fit(my_data).transform(my_data)\nmy_data = SI_bowler.fit(my_data).transform(my_data)\n\n# view the transformed data\nmy_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)","e0702913":"# create object and specify input and output column\nOHE = OneHotEncoder(inputCols=['Batsman_Index', 'Bowler_Index'],outputCols=['Batsman_OHE', 'Bowler_OHE'])\n\n# transform the data\nmy_data = OHE.fit(my_data).transform(my_data)\n\n# view and transform t\nmy_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)\n","e3d5761d":"from pyspark.ml.feature import VectorAssembler\n# Since ISwicket and IsBowler are not supported here, we thus remove them\n\n# specify the input and output columns of the vector assembler\nassembler = VectorAssembler(inputCols=['Over',\n                                       'Runs',\n                                       'Batsman_Index',\n                                       'Bowler_Index',\n                                       'Batsman_OHE',\n                                       'Bowler_OHE'],\n                           outputCol='vector')\n\n# fill the null values\nmy_data = my_data.fillna(0)","7fa27ed1":"# A pipeline allows us to maintain the data flow of all the relevant transformations \n# that are required to reach the end result.","41d29dc4":"from pyspark.ml import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier","2c9090b5":"# define stage 3 : one hot encode the numeric category_2 column\n# create a sample dataframe\nsample_df = spark.createDataFrame([\n    (1, 'L101', 'R'),\n    (2, 'L201', 'C'),\n    (3, 'D111', 'R'),\n    (4, 'F210', 'R'),\n    (5, 'D110', 'C')\n], ['id', 'category_1', 'category_2'])\n\nsample_df.show()","6a8315c2":"# define stage 1 : transform the column category_1 to numeric\nstage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')\n# define stage 2 : transform the column category_2 to numeric\nstage_2 = StringIndexer(inputCol= 'category_2', outputCol= 'category_2_index')\n# define stage 3 : one hot encode the numeric category_2 column\nstage_3 = OneHotEncoder(inputCols=['category_2_index'], outputCols=['category_2_OHE'])\n\n# setup the pipeline\npipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n\n# fit the pipeline model and transform the data as defined\npipeline_model = pipeline.fit(sample_df)\nsample_df_updated = pipeline_model.transform(sample_df)\n\n# view the transformed data\nsample_df_updated.show()","3b8e2a43":"from pyspark.ml.classification import LogisticRegression\n\n# create a sample dataframe with 4 features and 1 label column\nsample_data_train = spark.createDataFrame([\n    (2.0, 'A', 'S10', 40, 1.0),\n    (1.0, 'X', 'E10', 25, 1.0),\n    (4.0, 'X', 'S20', 10, 0.0),\n    (3.0, 'Z', 'S10', 20, 0.0),\n    (4.0, 'A', 'E10', 30, 1.0),\n    (2.0, 'Z', 'S10', 40, 0.0),\n    (5.0, 'X', 'D10', 10, 1.0),\n], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])\n\n# view the data\nsample_data_train.show()","eaff3f10":"# define stage 1: transform the column feature_2 to numeric\nstage_1 = StringIndexer(inputCol= 'feature_2', outputCol= 'feature_2_index')\n# define stage 2: transform the column feature_3 to numeric\nstage_2 = StringIndexer(inputCol= 'feature_3', outputCol= 'feature_3_index')\n# define stage 3: one hot encode the numeric versions of feature 2 and 3 generated from stage 1 and stage 2\nstage_3 = OneHotEncoder(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], \n                                 outputCols= ['feature_2_encoded', 'feature_3_encoded'])\n# define stage 4: create a vector of all the features required to train the logistic regression model \nstage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'],\n                          outputCol='features')\n# define stage 5: logistic regression model                          \nstage_5 = LogisticRegression(featuresCol='features',labelCol='label')\n\n# setup the pipeline\nregression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5])\n\n# fit the pipeline for the trainind data\nmodel = regression_pipeline.fit(sample_data_train)\n# transform the data\nsample_data_train = model.transform(sample_data_train)\n\n# view some of the columns generated\nsample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()","79ba0c17":"# create a sample data without the labels\nsample_data_test = spark.createDataFrame([\n    (3.0, 'Z', 'S10', 40),\n    (1.0, 'X', 'E10', 20),\n    (4.0, 'A', 'S20', 10),\n    (3.0, 'A', 'S10', 20),\n    (4.0, 'X', 'D10', 30),\n    (1.0, 'Z', 'E10', 20),\n    (4.0, 'A', 'S10', 30),\n], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n\n# transform the data using the pipeline\nsample_data_test = model.transform(sample_data_test)\n\n# see the prediction on the test data\nsample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()","b2667f84":"# define stage 1: transform the column feature_2 to numeric\nstage_1 = StringIndexer(inputCol= 'Batsman_Name', outputCol='Batsman_Index' )\n# define stage 2: transform the column feature_3 to numeric\nstage_2 = StringIndexer(inputCol= 'Bowler_Name', outputCol= 'Bowler_Index')\n                               \n# define stage 4: create a vector of all the features required to train the logistic regression model \nstage_4 = VectorAssembler(inputCols=[ 'Batsman_Name', 'Bowler_Index'],\n                          outputCol='features')\n# define stage 5: logistic regression model                          \nstage_5 = LogisticRegression(featuresCol='features',labelCol='Bowler_Index')\n\n# setup the pipeline\nregression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_4, stage_5])\n\n# fit the pipeline for the trainind data\nmodel = regression_pipeline.fit(data_agg)\n# transform the data\nsample_data_train = model.transform(data_agg)\n\n# view some of the columns generated\nsample_data_train.select('rawPrediction', 'probability', 'prediction').show()","964a6dc6":"# Using pyspark for Data Exploration","ca1e9483":"# Encode Categorical Variables using PySpark","d879a37d":"# Machine Learning Modelling with Pyspark","b965620f":"# India vs Bangladesh Cricket Match and Predictive outcomes","20cbc5fa":"# Let\u2019s create a sample dataframe with three columns as shown below. Here, we will define some of the stages in which we want to transform the data and see how to set up the pipeline:","3f377a0d":"# Building Machine Learning Pipelines using PySpark","e5f8d297":"# First, we need to use the String Indexer to convert the variable into numerical form and then use OneHotEncoderEstimator to encode multiple columns of the dataset.","83516722":"# To Test the model","12ea1fbb":"# Thus the Model works with Pyspark so lets test our data in the model now","9272f7ff":"# The Vector Assembler now converts them into a single feature column in order to train the machine learning model"}}