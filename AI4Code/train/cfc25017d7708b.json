{"cell_type":{"d989ad4e":"code","79332e1e":"code","7145da4e":"code","8aa0d938":"code","04c780a2":"code","2ed6f911":"code","d6807cc3":"code","14b599f4":"code","4a08cbe0":"code","10d20a5b":"code","c38b9346":"code","252020dc":"code","455ea8d8":"code","91d4d2c4":"code","fd768993":"code","2d95bf1d":"code","af937088":"code","0e7390cc":"code","f3cb1e95":"code","1ba07db8":"code","28b26ca0":"code","ed0fcb80":"code","15eea31f":"code","b9f23693":"code","d6e025d8":"code","60f03ea1":"code","6276304c":"code","48e92711":"code","0b1eba45":"code","b9b97946":"code","c76dae52":"code","6f60489c":"code","07af9fbf":"code","2ace7ad7":"code","6ac1146e":"code","6f1a3ad4":"markdown","48055b73":"markdown","f6afe5c7":"markdown","f826e42a":"markdown","4d4ed27b":"markdown","f2b7e960":"markdown","176ba884":"markdown","58bc1a24":"markdown","bbd1e9f0":"markdown","6b712aad":"markdown","8ac62dcc":"markdown","cd2b4afb":"markdown","8f4d86cc":"markdown","9990f373":"markdown","882712f0":"markdown","9810082b":"markdown","1ba3c7e3":"markdown","24f35e8c":"markdown","25e6a530":"markdown","22a7dfeb":"markdown"},"source":{"d989ad4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79332e1e":"n = 5000000  # Decide with how many rows you want to start working\ndf = pd.read_csv(\"\/kaggle\/input\/chess-evaluations\/chessData.csv\", delimiter=',', nrows =n)\nfens = df[\"FEN\"]\nevals = df[\"Evaluation\"]","7145da4e":"def simple_stats(inp_df, to_exclude):\n    for col in inp_df.columns:\n        if col not in to_exclude:\n            print(f\"Unique elements for column - {col} -:\")\n            print(set(inp_df[col]))\n\nfens_stratified = [fen.split(\" \") for fen in fens]\ndf_rep = pd.DataFrame.from_records(fens_stratified, columns=[\"fen\", \"turn\", \"castle\", \"enpassant\", \"halfmove\", \"fullmove\"])\nsimple_stats(df_rep, [\"fen\"])","8aa0d938":"df_rep.head()","04c780a2":"import torch\nPIECE_TO_INT_2 = {key: i for i, key in enumerate([\"r\", \"n\", \"b\", \"q\", \"k\", \"p\", \"R\", \"N\", \"B\", \"Q\", \"K\", \"P\"])}\ndef transform_fen(inp_fen:str) -> torch.Tensor:\n    board_tensor = []\n    for i, piece in enumerate(PIECE_TO_INT_2.keys()):\n        piece_matrix = []\n        for row in inp_fen.split(\"\/\"):\n            row_vec = []\n            assert isinstance(row, str)\n            for element in row:\n                if element.isalpha():\n                    if element == piece:\n                        row_vec += [1]  \n                    else:\n                        # it's a piece but not the one on the loop\n                        row_vec += [0]\n                else:\n                    if element.isdigit():\n                        row_vec += int(element) * [0]  # add as many zeros as the number\n                    else:\n                        raise ValueError  # raise error if that element is not an integer or a string\n            assert len(row_vec) == 8\n            piece_matrix.append(row_vec)\n        assert len(piece_matrix) == 8\n        board_tensor.append(piece_matrix)\n    assert len(board_tensor) == 12\n    return torch.ByteTensor(board_tensor)","2ed6f911":"!pip install chess\nimport chess\nexample_fen = df_rep[\"fen\"].values[300]\nchess.Board(example_fen)","d6807cc3":"board_tensor = transform_fen(example_fen)\nprint(board_tensor.size())\nboard_tensor","14b599f4":"CASTLE_TO_INT = {key: i for i, key in enumerate([\"K\", \"Q\", \"k\", \"q\"])}\ndef castle_to_vec(inp_castle_str):\n    out_vec = 4 * [0]\n    if inp_castle_str != \"-\":\n        for side in inp_castle_str:\n            out_vec[CASTLE_TO_INT[side]] = 1\n    return torch.ByteTensor(out_vec)","4a08cbe0":"print(castle_to_vec('KQkq'))\nprint(castle_to_vec('-'))\nprint(castle_to_vec('Qq'))\nprint(castle_to_vec('Kkq').size())","10d20a5b":"TURN_TO_INT = {\"w\": 0, \"b\": 1}\nENPASSANT_TO_INT = {key: str(i + 1) for i, key in enumerate([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"])}\n\ndef en_passant_to_vec(inp_en_passant):\n    output_tensor = torch.zeros([8, 8], dtype=torch.uint8)\n    if inp_en_passant != \"-\":\n        assert len(inp_en_passant) == 2\n        assert inp_en_passant[0].isalpha()\n        assert inp_en_passant[1].isdigit()\n        square = (int(ENPASSANT_TO_INT[inp_en_passant[0]])-1, int(inp_en_passant[1])-1)\n        assert isinstance(square, tuple)\n        output_tensor[7-square[1],square[0]] = 1\n    return output_tensor","c38b9346":"print(en_passant_to_vec(\"e5\"))\nprint(en_passant_to_vec(\"-\"))\nprint(en_passant_to_vec(\"a1\"))\nprint(en_passant_to_vec(\"h8\"))\nprint(en_passant_to_vec(\"f5\"))","252020dc":"TURN_TO_INT = {\"w\": 0, \"b\": 1}\ndef encode_fen_flat(inp_fen_string):\n    board_str, turn_str, castling_str, enpassant_str, _, _ = inp_fen_string.split(\" \")\n    board_tensor = transform_fen(board_str).type(torch.FloatTensor)\n    assert board_tensor.size() == torch.Size([12, 8, 8])\n    enpassant_tensor = en_passant_to_vec(enpassant_str).type(torch.FloatTensor)\n    assert enpassant_tensor.size() == torch.Size([8,8])\n    castling_tensor = castle_to_vec(castling_str).type(torch.FloatTensor)\n    assert castling_tensor.size() == torch.Size([4])\n    turn_tensor = torch.ByteTensor([TURN_TO_INT[turn_str]]).type(torch.FloatTensor)\n    assert turn_tensor.size() == torch.Size([1])\n    \n    # Concatenate vectors for Feed-forward network stacking all values in a single dimension\n    output_tensor = torch.cat((board_tensor.flatten(), enpassant_tensor.flatten(),castling_tensor.flatten(),turn_tensor.flatten()), 0)\n    \n    return output_tensor                                                                              \n                                                                                  ","455ea8d8":"example = df[\"FEN\"].values[300]\nprint(example)","91d4d2c4":"encode_fen_flat(\"8\/p7\/1prn2k1\/3r1pp1\/PR1Pp3\/2P3P1\/2KN1P2\/4R3 b - - 5 42\").size()","fd768993":"encode_fen_flat(\"8\/p7\/1prn2k1\/3r1pp1\/PR1Pp3\/2P3P1\/2KN1P2\/4R3 b - - 5 42\")","2d95bf1d":"len(encode_fen_flat(\"8\/p7\/1prn2k1\/3r1pp1\/PR1Pp3\/2P3P1\/2KN1P2\/4R3 b - - 5 42\"))","af937088":"evals.values[0:10]","0e7390cc":"mates = [x for x in evals.values if \"#\" in x]\nfor x in mates[0:10]:\n    print(x) ","f3cb1e95":"sorted([int(mate.replace(\"#\",\"\").replace(\"+\",\"\").replace(\"-\", \"\")) for mate in mates], reverse=True)[0:5]","1ba07db8":"import matplotlib.pyplot as plt\nnonmates = [int(x.replace(\"+\",\"\").replace(\"\\ufeff\",\"\"))\/100 for x in evals.values if \"#\" not in x]\nplt.hist(nonmates, 50)\nplt.show()","28b26ca0":"nonmates = [x for x in nonmates if -10<x<10]\nplt.hist(nonmates, 50)\nplt.show()","ed0fcb80":"def convert_mates(inp_mate, base = 20000, spacing=1):\n    value = int(inp_mate.replace(\"#\",\"\").replace(\"+\",\"\").replace(\"\\ufeff\",\"\")) * spacing\n    if value > 0:\n        return base - value\n    else:\n        return -base - value","15eea31f":"all_labels_corrected = [int(x.replace(\"+\",\"\").replace(\"\\ufeff\",\"\"))\/100 if \"#\" not in x else convert_mates(x)\/100 for x in evals.values]\nplt.hist(all_labels_corrected, 50)\nplt.show()","b9f23693":"USE_ARCTAN = True","d6e025d8":"if USE_ARCTAN:\n    import numpy as np\n    all_labels_corrected =list(np.arctan(all_labels_corrected))\n    plt.hist(all_labels_corrected, 50)\n    plt.show()","60f03ea1":"from tqdm import tqdm\nimport numpy as np\nfeatures = []\nlabels = []\nfor fen_str, eval_str in tqdm(zip(df[\"FEN\"].values,df[\"Evaluation\"].values)):\n    fen_tensor = encode_fen_flat(fen_str)\n    if \"#\" not in eval_str:\n        eval_value = int(eval_str.replace(\"+\",\"\").replace(\"\\ufeff\",\"\"))\/100\n    else:\n        eval_value = convert_mates(eval_str)\/100\n    if USE_ARCTAN:\n        eval_value = np.arctan(eval_value)\n    \n    eval_tensor = torch.Tensor([eval_value])\n    features.append(fen_tensor)\n    labels.append(eval_tensor)\n    ","6276304c":"print(len(features))\nprint(len(labels))","48e92711":"!pip install scikit-learn==0.24.2","0b1eba45":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=42, shuffle=True)","b9b97946":"from torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nclass ChessDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = dict(features=self.encodings[idx], labels=self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","c76dae52":"from torch.nn.init import xavier_uniform_\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef init_weights(m):\n    try:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n    except Exception:\n        return\n    \nclass MLP(torch.nn.Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()    \n        self.base_model = torch.nn.Sequential(\n        torch.nn.Linear(n_inputs, 600),\n        torch.nn.ReLU(),\n        torch.nn.Dropout(.2),\n        torch.nn.Linear(600, 300),\n        torch.nn.Dropout(.2),\n        torch.nn.ReLU(),\n        torch.nn.Linear(300, 100),\n        torch.nn.Dropout(.2),\n        torch.nn.ReLU(),\n        torch.nn.Linear(100, 1),\n    ).to(device)\n        self.base_model.apply(init_weights)\n\n    # forward propagate input\n    def forward(self, X):\n        X = X.to(device)\n        # input to first hidden layer\n        X = self.base_model(X)\n        return X","6f60489c":"from tqdm import tqdm\nfrom numpy import vstack\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import sqrt\n\n# evaluate the model\ndef evaluate_model(test_dl, model,criterion):\n    predictions, actuals = list(), list()\n    stop_training=False\n    model.eval()\n    with torch.no_grad():\n        print(\"Running evaluation\")\n        total_loss = []\n        for i, inputs in enumerate(test_dl):\n            # evaluate the model on the test set\n            yhat = model(inputs[\"features\"])\n            # retrieve numpy array\n            targets = inputs[\"labels\"].to(device)\n            \n            if USE_ARCTAN:\n                yhat = torch.tan(yhat).to(device)\n                tagets = torch.tan(yhat).to(device)\n            loss = criterion(yhat, targets)\n            total_loss.append(loss.item())\n    mloss = sum(total_loss) \/ len(total_loss)\n    return mloss\n\ndef train_model(train_dl, test_dl, model, n_epochs, lr):\n    # define the optimization\n    criterion = torch.nn.MSELoss()\n    #optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n    \n    # enumerate epochs\n    for e_n, epoch in tqdm(enumerate(range(n_epochs))):\n        # enumerate mini batches\n        model.train()\n        training_loss = []\n        for i, inputs in enumerate(train_dl):\n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs[\"features\"])\n            # calculate loss\n            targets = inputs[\"labels\"].to(device)\n            loss = criterion(yhat, targets)\n            \n            loss.backward() # calculate gradient\n    \n            optimizer.step() # update model weights\n            \n            optimizer.zero_grad()\n            \n            training_loss.append(loss.item())\n            #print(loss.item())\n        mse = evaluate_model(test_dl, model,criterion)\n        msesqrt = sqrt(mse)\n        \n        print('Epoch: %.0f MSE: %.3f, RMSE: %.3f, Training loss: %.3f' % (e_n, mse, msesqrt,sum(training_loss) \/ len(training_loss)))\n        if msesqrt < 2:\n            break","07af9fbf":"torch.cuda.empty_cache()\n# prepare the datasets\n\ntrain_batch_size = 512*3\ntest_batch_size = 512*1000\n\ntrain_dataset = ChessDataset(encodings = X_train, labels=y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n\ntest_dataset = ChessDataset(encodings = X_test, labels=y_test)\ntest_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)","2ace7ad7":"n_epochs = 1000\nlearning_rate = 1e-3\ninput_size = X_train[0].size()[0]\n\nmodel = MLP(n_inputs=input_size)\n# train the model\ntrain_model(train_dl=train_loader, test_dl=test_loader, model=model, n_epochs=n_epochs, lr=learning_rate)\n","6ac1146e":"import chess\n\nexamples = [\"r1bqk1nr\/pp1pppbp\/2n3p1\/1Bp5\/4P3\/5N2\/PPPP1PPP\/RNBQ1RK1 w Qkq - 0 1\",\n            \"rnbqkbnr\/pppppppp\/8\/8\/8\/8\/PPPPPPPP\/R1BQKBNR w KQkq - 0 1\",\n           \"r1bqkbnr\/pppp1ppp\/2n5\/4p3\/4P3\/5N2\/PPPP1PPP\/RNBQKB1R w KQkq - 0 1\",\n            \"8\/8\/1q1k4\/8\/8\/3K4\/8\/8 w - - 0 1\",\n            \"8\/5r2\/3k4\/8\/8\/3K4\/8\/8 w - - 0 1\",\n           \"r1bqkbnr\/1ppp1ppp\/p1n5\/4p2Q\/2B1P3\/8\/PPPP1PPP\/RNB1K1NR w KQkq - 0 1\",\n           \"4k3\/8\/8\/8\/8\/8\/PPPPPPPP\/RNBQKBNR w KQ - 0 1\",\n           \"8\/8\/8\/4k3\/8\/8\/4K3\/2Q5 w - - 0 1\"]\n\n\n\ndef predict(fen_reps, model):\n    \n    features = []\n    for fen_rep in fen_reps:\n        features.append(encode_fen_flat(fen_rep))\n    \n    features = torch.stack(\n    features\n    ,dim=0\n    )\n    # make predictions\n    model.eval()\n    with torch.no_grad():\n        yhats = model(features)\n    # retrieve numpy array\n    yhats = yhats.to('cpu').detach().numpy()\n    if USE_ARCTAN:\n        yhats = np.tan(yhats)\n    for yh, fen in zip(yhats, fen_reps):\n        display(chess.svg.board(chess.Board(fen), size=350))\n        print(f\"Model evaluation {yh}\")\n\npredict(fen_reps=examples,model=model)","6f1a3ad4":"# Model development","48055b73":"# Create features and labels","f6afe5c7":"Note that this might not be an optimal approach depending on the application and it makes a big difference in the magnitude of the loss. Further thinking about this might be needed.\n\nAny views about this?","f826e42a":"## Train model","4d4ed27b":"Seems that we can now deal with these labels as regression values. Also, we can see we have maany positions with evaluation close to 0.0 (probably early stages in the game). This might be a limitation in terms of the diversity of the dataset","f2b7e960":"Arctan transformation:","176ba884":"# Preprocess LABELS","58bc1a24":"### 1. Convert board-section of a FEN string into 12x8x8 tensor","bbd1e9f0":"### 3. Convert all others","6b712aad":"The majority of values can be converted to an integer\/float directly. It's centipawn loss, which means that 1 pawn = 100 points, so these values could be divided by 100 to get a more compact distribution. However, we also realised that there were some cases in which there was the hash element in the string, indicating mate in x moves:","8ac62dcc":"# Qualitative evaluation","cd2b4afb":"## Model definition","8f4d86cc":"We can see that this tensor could be directly used as a representation for the model","9990f373":"If we start thinking of this problem, we'll probably be using a regressor for predicting the evaluation of a particular chess position. Then, we would like to convert #1 (mate in one move) to a higher value than #10 (mate in 10), since we want to penalize not seeing mate in 1 more than not seeing mate in 10. Also, we probably want #1 or #x to have pretty high values (higher than non-mate positions). Below you can see a simple approach in which I set two extreme values 200 and -200 and compute 200-#mate\/-200+#-mate. ","882712f0":"### Let's stack board (12x64), castling (4) and en passant (8x8) and turn representations (1)\n\nFor now we'll only be using these features and we'll include half&full moves later","9810082b":"### 2. Convert castling to 4-element vector","1ba3c7e3":"# Preprocess FEN\n\nWe want to convert everything to numerical values. First let's have a look at the elements of the FEN notation. We can see what each column means in the following link: https:\/\/www.wikiwand.com\/en\/Forsyth%E2%80%93Edwards_Notation","24f35e8c":"We can see that in the first 10K games, the maximum depth is mate in 23","25e6a530":"So we need some logic to decide how to deal with these cases since they can't be directly converted. First, let's look at the range of mates that we see","22a7dfeb":"## Dataloader"}}