{"cell_type":{"efb73ad6":"code","5b4e20ab":"code","bb800aec":"code","29a64e00":"code","01abeb8b":"code","030125af":"code","d882eade":"code","88caae19":"code","b0afeeae":"code","6808554f":"code","de4242e9":"code","e9b61e74":"code","078a315b":"code","23fdb1b5":"code","2445a732":"code","bdcaf253":"code","812f836f":"code","3ba536fc":"code","918dcba6":"code","b0713c4a":"code","08d694ee":"code","e944163f":"code","9496584c":"code","1da5cdab":"code","2eb7410b":"code","32cc70d7":"code","dc5e001a":"code","078b5f9b":"code","89dba010":"code","90566481":"code","31de8dc8":"code","4819868b":"code","de5b06ba":"code","0740a94f":"code","6e9b9165":"code","b03360c1":"code","a06a2aa6":"code","d8d8e3be":"code","0228a6eb":"markdown","8ddbff8b":"markdown","0d852507":"markdown","a052e430":"markdown","a64cf28c":"markdown","8196d3b6":"markdown","2b92b239":"markdown","b5b4cc83":"markdown","e5c6e218":"markdown","eaccb1c5":"markdown","67eb2115":"markdown","7f5bb653":"markdown","c33e389d":"markdown","0f4fade0":"markdown","6e23defc":"markdown","4c2601d9":"markdown","bb9c0e3d":"markdown"},"source":{"efb73ad6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5b4e20ab":"## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,roc_auc_score\nfrom google.cloud import bigquery\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.linear_model import Lasso,Ridge\nfrom datetime import date\nfrom datetime import timedelta\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","bb800aec":"def MyLabelEncodeSingle(col):\n    levels=col.unique().tolist()\n    for l in levels:\n        if l is np.nan:\n            levels.remove(np.nan)\n    levelmap={e:i for i,e in enumerate(levels)}\n    return col.map(levelmap)","29a64e00":"## defining constants\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 1990\n## reading data\ntrain = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')\n#https:\/\/www.kaggle.com\/rohanrao\/covid19-forecasting-metadata\nregion_metadata = pd.read_csv('\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv')\nregion_date_metadata = pd.read_csv('\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv')\n","01abeb8b":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\n#df_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\n#df_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\ndf_panel[\"Country_Region\"]=df_panel[\"Country_Region\"].fillna('Unknown')\ndf_panel[\"Province_State\"]=df_panel[\"Province_State\"].fillna('Unknown')\n# fixing data issues with cummax\n#df_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\n#df_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\ndf_panel.ConfirmedCases = df_panel.groupby([\"Country_Region\",\"Province_State\"])[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby([\"Country_Region\",\"Province_State\"])[\"Fatalities\"].cummax()\n# merging external metadata\nregion_metadata[\"Country_Region\"]=region_metadata[\"Country_Region\"].fillna('Unknown')\nregion_metadata[\"Province_State\"]=region_metadata[\"Province_State\"].fillna('Unknown')\nregion_date_metadata[\"Country_Region\"]=region_date_metadata[\"Country_Region\"].fillna('Unknown')\nregion_date_metadata[\"Province_State\"]=region_date_metadata[\"Province_State\"].fillna('Unknown')\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"], how = \"left\")\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = MyLabelEncodeSingle(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\n#df_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n\n## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby([\"Country_Region\",\"Province_State\"])[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby([\"Country_Region\",\"Province_State\"])[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby([\"Country_Region\",\"Province_State\"])[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby([\"Country_Region\",\"Province_State\"])[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = [\"Country_Region\",\"Province_State\"], how = \"left\")\n","030125af":"#https:\/\/www.kaggle.com\/jasonbenner\/world-bank-datasets#World_Happiness_Index.csv\nworld_happiness = pd.read_csv(\"..\/input\/world-bank-datasets\/World_Happiness_Index.csv\")\nworld_happiness=world_happiness.iloc[:,:19]\nworld_happiness.columns=[c.replace('(','').replace(')','').replace('(','').replace(',','').replace('-','_').replace('\/','_').replace(' ','_') \n                               for c in world_happiness.columns]","d882eade":"average_year={}\ntemp_matrix=world_happiness.iloc[:,2:]\nfor y in world_happiness.Year.unique():\n    average_year[y]=temp_matrix.loc[world_happiness.Year==y,:].mean()\ndel temp_matrix\ngc.collect()","88caae19":"distance=0\nwhile world_happiness.isna().sum().sum()!=0:\n    for y in world_happiness.Year.unique():\n        yhat=y-distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness.columns[2:]:\n            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n        yhat=y+distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness.columns[2:]:\n            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n        distance += 1","b0afeeae":"world_happiness_latest = world_happiness.groupby('Country_name').nth(-1)\nworld_happiness_first = world_happiness.groupby('Country_name').agg('first')\nworld_happiness_last = world_happiness.groupby('Country_name').agg('last')\nworld_happiness_count = world_happiness.groupby('Country_name').count()\nworld_happiness_range=(world_happiness_last-world_happiness_first)\/world_happiness_count\nworld_happiness_range.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_latest.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_range.columns=[c+'_range' for c in world_happiness_range.columns]\nworld_happiness_latest.columns=[c+'_latest' for c in world_happiness_latest.columns]\nworld_happiness_grouped=pd.concat((world_happiness_latest,world_happiness_range),axis=1).reset_index()","6808554f":"malaria_world_health = pd.read_csv(\"..\/input\/world-bank-datasets\/Malaria_World_Health_Organization.csv\")\nmalaria_world_health.columns=[c.replace(' ','_') for c in malaria_world_health.columns]","de4242e9":"human_development = pd.read_csv(\"..\/input\/world-bank-datasets\/Human_Development_Index.csv\")\nhuman_development.columns=[c.replace(')','').replace('(','').replace(' ','_') for c in human_development.columns]\nhuman_development['Gross_national_income_GNI_per_capita_2018']= human_development['Gross_national_income_GNI_per_capita_2018'].apply(lambda x: x if x!=x else x.replace(',','')).astype(float)","e9b61e74":"#https:\/\/www.kaggle.com\/nightranger77\/covid19-demographic-predictors\nnight_ranger = pd.read_csv(\"..\/input\/covid19-demographic-predictors\/covid19_by_country.csv\")\nnight_ranger.columns=[c.replace(' ','_') for c in night_ranger.columns]\nnight_ranger = night_ranger[night_ranger.Country != \"Georgia\"]\nnight_ranger=night_ranger[['Country','Median_Age','GDP_2018','Crime_Index','Population_2020','Smoking_2016','Females_2018']]","078a315b":"#https:\/\/www.kaggle.com\/londeen\/world-happiness-report-2020\nhappiness_df = pd.read_csv(\"..\/input\/world-happiness-report-2020\/WHR20_DataForFigure2.1.csv\")\nhappiness_df.columns=[c.replace(':','').replace('+','').replace(' ','_') for c in happiness_df.columns]\nhappiness_df['Regional_indicator']=MyLabelEncodeSingle(happiness_df['Regional_indicator'])","23fdb1b5":"#https:\/\/www.kaggle.com\/alizahidraja\/world-population-by-age-group-2020\nage_df = pd.read_csv(\"..\/input\/world-population-by-age-group-2020\/WorldPopulationByAge2020.csv\")\nage_df['AgeGrp']=MyLabelEncodeSingle(age_df['AgeGrp'])\ndef processAge(df):\n    ageindex=df['AgeGrp']\n    sexsum=df[['PopMale', 'PopFemale', 'PopTotal']].sum()\n    mp=sexsum['PopMale']\/sexsum['PopTotal']\n    fp=sexsum['PopFemale']\/sexsum['PopTotal']\n    p0=df.loc[ageindex==0,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p1=df.loc[ageindex==1,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p2=df.loc[ageindex==2,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p3=df.loc[ageindex==3,'PopTotal'].values[0]\/sexsum['PopTotal']\n    m0=df.loc[ageindex==0,'PopMale'].values[0]\/sexsum['PopMale']\n    m1=df.loc[ageindex==1,'PopMale'].values[0]\/sexsum['PopMale']\n    m2=df.loc[ageindex==2,'PopMale'].values[0]\/sexsum['PopMale']\n    m3=df.loc[ageindex==3,'PopMale'].values[0]\/sexsum['PopMale']\n    f0=df.loc[ageindex==0,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f1=df.loc[ageindex==1,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f2=df.loc[ageindex==2,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f3=df.loc[ageindex==3,'PopFemale'].values[0]\/sexsum['PopFemale']\n    return pd.DataFrame({'MaleP':mp,'MaleP_0':m0,'MaleP_1':m1,'MaleP_2':m2,'MaleP_3':m3,'FemaleP':fp,\n                         'FemaleP_0':f0,'FemaleP_1':f1,'FemaleP_2':f2,'FemaleP_3':f3,'PopTotal':sexsum['PopTotal'],\n                         'Pop_0':p0,'Pop_1':p1,'Pop_2':p2,'Pop_3':p3},index=[0])\nage_df=age_df.groupby('Location').apply(processAge).reset_index().drop('level_1',axis=1)","2445a732":"#https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020\npop_df = pd.read_csv(\"..\/input\/population-by-country-2020\/population_by_country_2020.csv\")\npop_df.columns=[c.replace('.',' ').split(' ')[0]+'_pop2020' for c in pop_df.columns]\npercent_col=['Yearly_pop2020','Urban_pop2020', 'World_pop2020']\ndef depercent(x):\n    if x=='N.A.':\n        return np.nan \n    else:\n        return float(x.replace('%',''))\nfor c in percent_col:\n    pop_df[c]=pop_df[c].apply(lambda x: depercent(x))\npop_df=pop_df.replace('N.A.',np.nan)\npop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']]=pop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']].astype(float)","bdcaf253":"#https:\/\/www.kaggle.com\/hbfree\/covid19formattedweatherjan22march24\nweather_df = pd.read_csv(\"..\/input\/covid19formattedweatherjan22march24\/covid_dataset.csv\")\nweather_df=weather_df[['Province\/State',\n'Country\/Region',\n'lat',\n'long',\n'day',\n'pop',\n'urbanpop',\n'density',\n'medianage',\n'smokers',\n'health_exp_pc',\n'hospibed',\n'temperature',\n'humidity']]","812f836f":"weather_df=weather_df.replace(-999,np.nan)\nweather_df['Province\/State']=weather_df['Province\/State'].fillna('Unknown')\nweather_df['day']=pd.to_datetime('2020-01-22')+weather_df['day'].apply(lambda x: timedelta(days=x))\nweather_df['month']=weather_df['day'].dt.month\nweather_df.drop('day',axis=1,inplace=True)\nweather_df=weather_df.groupby(['Country\/Region','Province\/State','month']).mean().reset_index()\nweather_df_latest = weather_df.groupby(['Country\/Region','Province\/State']).nth(-1).reset_index()\nweather_df_latest['month']=4\nweather_df=pd.concat((weather_df,weather_df_latest),sort=True,axis=0,ignore_index=True)","3ba536fc":"#https:\/\/www.kaggle.com\/danevans\/world-bank-wdi-212-health-systems\nhealthsys_df = pd.read_csv(\"..\/input\/world-bank-wdi-212-health-systems\/2.12_Health_systems.csv\")\nhealthsys_df.columns=[c.replace('-','_') for c in healthsys_df.columns]\nhealthsys_df.drop('World_Bank_Name',axis=1,inplace=True)\nnan_country=healthsys_df[['Country_Region', 'Province_State']].isna().all(axis=1)\nhealthsys_df=healthsys_df.loc[nan_country==False,:].reset_index(drop=True)\nhealthsys_df['Province_State']=healthsys_df['Province_State'].fillna('Unknown')","918dcba6":"#https:\/\/www.kaggle.com\/koryto\/countryinfo\ncompre_df = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\ncompre_df['region']=compre_df['region'].fillna('Unknown')\nkeepcol=['region', 'country', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', 'quarantine', 'schools',\n       'publicplace', 'gatheringlimit', 'gathering', 'nonessential',\n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung', 'gdp2019',\n       'healthexp', 'healthperpop', 'fertility', 'firstcase']","b0713c4a":"def tempfun(x):\n    if x is np.nan:\n        return x\n    else:\n        return float(x.replace(',',''))\nfor c in ['gdp2019','healthexp']:\n    compre_df[c]=compre_df[c].apply(lambda x: tempfun(x) )\n    todate_col=['quarantine', 'schools','publicplace', 'gathering', 'nonessential','firstcase']\nfor c in todate_col:\n    compre_df[c]= (pd.to_datetime(date.today())-pd.to_datetime(compre_df[c])).dt.days.astype(float)\ncompre_df=compre_df[keepcol]","08d694ee":"#https:\/\/www.kaggle.com\/imdevskp\/sars-outbreak-2003-complete-dataset\nsars_df = pd.read_csv(\"..\/input\/sars-outbreak-2003-complete-dataset\/sars_2003_complete_dataset_clean.csv\")","e944163f":"def getProvince(x):\n    x_seg=x.split(',')\n    if len(x_seg)==2:\n        if 'SAR' in x_seg[0]:\n            return x_seg[0][:-4]\n        else:\n            return x_seg[0]\n    else:\n        return np.nan\nsars_df['Province']=sars_df['Country'].apply(lambda x: getProvince(x))\nsars_df['Country']=sars_df['Country'].apply(lambda x: x.split(',')[-1])\nsars_df['Country']=sars_df['Country'].replace('Viet Nam','Vietnam')\ndef getSlope(ses,segs):\n    segsize=np.floor(len(ses)\/segs)\n    slope=[]\n    for i in range(segs):\n        if i==segs-1:\n            slope.append((ses[-1]-ses[int(i*segsize)])\/(len(ses)-1-i*segsize))\n        else:\n            slope.append((ses[int((i+1)*segsize-1)]-ses[int(i*segsize)])\/(segsize-1))\n    return slope   \ndef aggSARS(df):\n    df=df.sort_values('Date')\n    case=df['Cumulative number of case(s)']\n    death=df['Number of deaths'].cumsum()\n    recover=df['Number recovered'].cumsum()\n    Sars_dict={}\n    Sars_dict['SARS_CaseMax']=case.max()\n    Sars_dict['SARS_DeathMax']=death.max()\n    Sars_dict['SARS_RecoverMax']=recover.max()\n    segs=df['Date'].apply(lambda x: x.split('-')[1]).nunique()\n    for i,s in enumerate(getSlope(case.values,segs)):\n        Sars_dict['SARS_Case_'+str(i)]=s\n    for i,s in enumerate(getSlope(death.values,segs)):\n        Sars_dict['SARS_Death_'+str(i)]=s\n    for i,s in enumerate(getSlope(recover.values,segs)):\n        Sars_dict['SARS_Recover_'+str(i)]=s\n    return pd.DataFrame(Sars_dict,index=[0])\nsars_df['Province']=sars_df['Province'].fillna('Unknown')\nsars_df_grouped=sars_df.groupby(['Country','Province']).apply(aggSARS).reset_index().drop('level_2',axis=1)\nsars_df_grouped=sars_df_grouped.rename(columns={'Country':'Country_Region','Province':'Province_State'})","9496584c":"#https:\/\/www.kaggle.com\/worldkeeping\/2009-h1n1-flu-with-readable-filename\nh1n1_df = pd.read_csv(\"..\/input\/2009-h1n1-flu-with-readable-filename\/Pandemic_H1N1_2009.csv\")","1da5cdab":"spelreplace={'\\xa0\\xa0\\xa0\\xa0\\xa0French Polynesia, FOC':'French Polynesia',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Guadaloupe, FOC':'Guadaloupe',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Martinique, FOC':'Martinique',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0New Caledonia, FOC':'New Caledonia',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Saint Martin, FOC':'Saint Martin', \n    '\\xa0\\xa0\\xa0\\xa0\\xa0Netherlands, Aruba':'Netherlands, Aruba',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Netherlands Antilles, Cura\u00e7ao':'Netherlands Antilles, Cura\u00e7ao',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Netherlands Antilles, Sint Maarten':'Netherlands Antilles, Sint Maarten',\n     '\\xa0\\xa0\\xa0\\xa0French Polynesia, FOC':'French Polynesia',\n       '\\xa0\\xa0\\xa0\\xa0Martinique, FOC':'Martinique',\n       '\\xa0\\xa0\\xa0\\xa0France, New Caledonia, FOC':'France, New Caledonia', \n       '\\xa0\\xa0\\xa0\\xa0\\xa0Netherlands Antilles, Cura\u00e7ao *':'Netherlands Antilles, Cura\u00e7ao', \n       '\\xa0\\xa0\\xa0\\xa0\\xa0Netherlands Antilles, Cura\u00e7ao **':'Netherlands Antilles, Cura\u00e7ao',\n            '\\xa0\\xa0\\xa0\\xa0\\xa0Guernsey, Crown Dependency':'Guernsey, Crown Dependency',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Isle of Man, Crown Dependency':'Isle of Man, Crown Dependency',\n       '\\xa0\\xa0\\xa0\\xa0\\xa0Jersey, Crown Dependency':'Jersey, Crown Dependency',\n        '\\xa0\\xa0\\xa0\\xa0\\xa0Puerto Rico':'Puerto Rico','*':'','Guatemala\\xa0':'Guatemala',\n            'Korea, Republic of':'Korea, South','\\xa0\\xa0\\xa0\\xa0\\xa0Virgin Islands':'Virgin Islands',\n            'Costa Rica*':'Costa Rica','Morocco *':'Morocco','Viet Nam':'Vietnam','United States of America*':'United States of America'}","2eb7410b":"h1n1_df['Country']=h1n1_df['Country'].replace(spelreplace)","32cc70d7":"def getP(x):\n    x_seg=x.split(',')\n    if len(x_seg)>1:\n        if x_seg[0].strip() != 'Korea':\n            if x_seg[1].strip() == 'UKOT':\n                return x_seg[0]\n            elif x_seg[-1].strip() == 'Crown Dependency':\n                return x_seg[-2]\n            else:   \n                return x_seg[1]\n    else:\n        return np.nan","dc5e001a":"def getC(x):\n    x_seg=x.split(',')\n    if len(x_seg)>1:\n        if x_seg[0].strip() != 'Korea':\n            if x_seg[1].strip() == 'UKOT':\n                return 'United Kingdom'\n            elif x_seg[-1].strip() == 'Crown Dependency':\n                return 'United Kingdom'\n            else:   \n                return x_seg[0]\n    elif x == 'United States of America':\n        return 'US'\n    else:\n        return x","078b5f9b":"h1n1_df['Province']=h1n1_df['Country'].apply(getP)\nh1n1_df['Country']=h1n1_df['Country'].apply(getC)","89dba010":"def getSlope(ses,segs):\n    segsize=np.floor(len(ses)\/segs)\n    slope=[]\n    for i in range(segs):\n        if i==segs-1:\n            slope.append((ses[-1]-ses[int(i*segsize)])\/(len(ses)-1-i*segsize))\n        else:\n            slope.append((ses[int((i+1)*segsize-1)]-ses[int(i*segsize)])\/(segsize-1))\n    return slope   \ndef aggh1n1(df):\n    df=df.sort_values('Update Time')\n    case=df['Cases']\n    death=df['Deaths']\n    Sars_dict={}\n    Sars_dict['H1N1_CaseMax']=case.max()\n    Sars_dict['H1N1_DeathMax']=death.max()\n    segs=df['Update Time'].apply(lambda x: x.split('\/')[0]).nunique()\n    for i,s in enumerate(getSlope(case.values,segs)):\n        Sars_dict['H1N1_Case_'+str(i)]=s\n    for i,s in enumerate(getSlope(death.values,segs)):\n        Sars_dict['H1N1_Death_'+str(i)]=s\n    return pd.DataFrame(Sars_dict,index=[0])\nh1n1_df['Province']=h1n1_df['Province'].fillna('Unknown')\nh1n1_df_grouped=h1n1_df.groupby(['Country','Province']).apply(aggh1n1).reset_index().drop('level_2',axis=1)\nh1n1_df_grouped=h1n1_df_grouped.rename(columns={'Country':'Country_Region','Province':'Province_State'})","90566481":"def merge2layer(left,right):\n    rightdf=right[['Country_Region','Province_State']]\n    rightdf['mark']=1\n    leftdf=left[['Country_Region','Province_State']]\n    countryNoP=pd.merge(left=leftdf,right=rightdf,on=['Country_Region','Province_State'],how='left')\n    firstdf=pd.merge(left=left,right=right,on=['Country_Region','Province_State'],how='left')\n    nextidx=countryNoP['mark'].isna()\n    firstdf=firstdf.loc[~nextidx,:]\n    countryNoP_right=right.loc[right['Province_State'].isna(),:].drop('Province_State',axis=1)\n    countryNoP_left=left.loc[nextidx,:]\n    seconddf=pd.merge(left=countryNoP_left,right=countryNoP_right,on='Country_Region',how='left')\n    finaldf=pd.concat((firstdf,seconddf),sort=True,axis=0,ignore_index=True)\n    return finaldf","31de8dc8":"#all extra features calculated above\ndef extrafeatures(df):\n    #print('before: {}'.format(len(df)))\n    has_col=df.columns.tolist()\n    df['UpToNow']=(pd.to_datetime(date.today())-pd.to_datetime(df['Date'])).dt.days.astype(float)\n   # print('after UpToNow: {}'.format(len(df)))\n    df = pd.merge(left=df, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\n    df.drop(\"Country_name\", axis=1, inplace=True)\n    #print('after world_happiness: {}'.format(len(df)))\n    df = pd.merge(left=df, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\n    df.drop(\"Country\", axis=1, inplace=True)\n   # print('after malaria: {}'.format(len(df)))\n    df = pd.merge(left=df, right=human_development, how='left', left_on='Country_Region', right_on='Country')\n    df.drop(\"Country\", axis=1, inplace=True)\n   # print('after human: {}'.format(len(df)))\n    df = pd.merge(left=df, right=night_ranger, how='left', left_on='Country_Region', right_on='Country')\n    df.drop(\"Country\", axis=1, inplace=True)\n    #print('after night: {}'.format(len(df)))\n    df = pd.merge(left=df, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\n    df.drop('Country_name', axis=1, inplace=True)\n    #print('after happiness: {}'.format(len(df)))\n    df = pd.merge(left=df, right=age_df, how='left', left_on='Country_Region', right_on='Location')\n    df.drop('Location', axis=1, inplace=True)\n    #print('after age: {}'.format(len(df)))\n    df = pd.merge(left=df, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\n    df.drop('Country_pop2020', axis=1, inplace=True)\n    #print('after pop: {}'.format(len(df)))\n    df['month']=df['Date'].dt.month\n    df = pd.merge(left=df, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country\/Region','Province\/State','month'])\n    df.drop(['Country\/Region','Province\/State','month'], axis=1, inplace=True)\n    #print('after weather: {}'.format(len(df)))\n    df = merge2layer(df,healthsys_df)\n    #print('after healthsys: {}'.format(len(df)))\n    df = pd.merge(left=df, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\n    df.drop(['country','region'], axis=1, inplace=True)\n    #print('after compre: {}'.format(len(df)))\n    df = merge2layer(df,sars_df_grouped)\n    #print('after sars: {}'.format(len(df)))\n    df = merge2layer(df,h1n1_df_grouped)\n    #print('after h1n1: {}'.format(len(df)))\n    #df['Country_Region']=MyLabelEncodeSingle(df['Country_Region'])\n    #df['Province_State']=MyLabelEncodeSingle(df['Province_State'])\n    df['thishour'] = df['Date'].dt.hour\n    df['thisdayofweek'] = df['Date'].dt.dayofweek\n    df['thisquarter'] = df['Date'].dt.quarter\n    df['thismonth'] = df['Date'].dt.month\n    df['thisdayofyear'] = df['Date'].dt.dayofyear\n    df['thisdayofmonth'] = df['Date'].dt.day\n    df['thisweekofyear'] = df['Date'].dt.weekofyear\n    extra_col=[c for c in df.columns.tolist() if c not in has_col]\n    return df,extra_col","4819868b":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n    df[\"density\"] = df.population \/ df.area\n    \n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n    \n    df,extra_col=extrafeatures(df.copy())\n    \n    features = [\n        #'Country_Region','Province_State',\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_123_cc\",\n        \"change_123_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        #\"lat\",\n        #\"lon\",\n        \"continent\",\n        #\"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]+extra_col\n    df=df.sort_values(['Country_Region','Province_State','Date'])\n    return df[features]\n","de5b06ba":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    LGB_PARAMS_C = {\"objective\": \"regression\",\n              \"num_leaves\": 30,\n              \"learning_rate\": 0.1,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.71,\n              #\"min_data_in_leaf\" : 50,\n              #\"max_bin\":200,\n              #\"reg_alpha\": 0.01,\n              #\"reg_lambda\": 1,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n    \n    LGB_PARAMS_F = {\"objective\": \"regression\",\n              \"num_leaves\": 28,\n              \"learning_rate\": 0.1,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.4,\n              #\"min_data_in_leaf\" : 50,\n              #\"max_bin\":400,\n              #\"reg_alpha\": 0.01,\n              #\"reg_lambda\": 1,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = ['continent','Regional_indicator']#,'Country_Region','Province_State']\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS_C, train_set = dtrain_cc, num_boost_round = 1000)\n    model_ft = lgb.train(LGB_PARAMS_F, train_set = dtrain_ft, num_boost_round = 1500)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 1000) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 1500) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n","0740a94f":"## function for building and predicting using logistic\ndef build_predict_reg(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    categorical_features = ['continent','Regional_indicator']#,'Country_Region','Province_State']\n    \n    df_train.drop([\"target_cc\", \"target_ft\"]+categorical_features, axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"]+categorical_features, axis = 1, inplace = True)\n    \n    df_train=df_train.replace(np.inf,np.nan)\n    df_test=df_test.replace(np.inf,np.nan)\n    for c in df_train.columns:\n        df_train[c]=df_train[c].fillna(df_train[c].mean())\n        df_test[c]=df_test[c].fillna(df_train[c].mean())\n    model_cc=Ridge(alpha=300, fit_intercept=True, normalize=True, max_iter=1000, tol=0.0001, random_state=SEED)\n    model_ft=Ridge(alpha=10, fit_intercept=True, normalize=True, max_iter=1000, tol=0.0001, random_state=SEED)\n    \n    model_cc.fit(df_train, target_cc)\n    model_ft.fit(df_train, target_ft)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n","6e9b9165":"## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor pdate in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", pdate)\n    \n    # ignore date already present in train data\n    if pdate in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == pdate, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == pdate]\n        \n        gap = (pd.Timestamp(pdate) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            #print('len of df_val{}, len of X_val{}'.format(len(df_val),len(X_val)) )\n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build.copy(), X_val.copy(), gap)\n            y_val_cc_logy, y_val_ft_logy, _, _ = build_predict_reg(X_build.copy(), X_val.copy(), gap)         \n            #y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            #print('{}_{}_{}'.format(len(df_val.Id.values),len(y_val_cc_lgb),len(y_val_ft_lgb)))\n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_logy\": y_val_cc_logy,\n                                        \"Fatalities_val_logy\": y_val_ft_logy,\n                                       # \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                      #  \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train.copy(), X_test.copy(), gap)\n        y_test_cc_logy, y_test_ft_logy, _, _ = build_predict_reg(X_train.copy(), X_test.copy(), gap)\n       # y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n            features_1=features_1[~np.isin(features_1,[\"target_cc\", \"target_ft\"])]\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n            features_14=features_14[~np.isin(features_14,[\"target_cc\", \"target_ft\"])]\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n            features_28=features_28[~np.isin(features_28,[\"target_cc\", \"target_ft\"])]\n        \n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_logy\": y_test_cc_logy,\n                                     \"Fatalities_test_logy\": y_test_ft_logy,\n                                   #  \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                  #   \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)\nprint(len(X_val.columns))","b03360c1":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_logy = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_logy.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_logy.isna()].ConfirmedCases_val_logy)))\nrmsle_ft_logy = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_logy.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_logy.isna()].Fatalities_val_logy)))\n\n#rmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\n#rmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\nprint(\"\\n\")\nprint(\"Logistic CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_logy, 2))\nprint(\"Logistic FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_logy, 2))\nprint(\"Logistic Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_logy + rmsle_ft_logy) \/ 2, 2))\n#print(\"\\n\")\n#print(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\n#print(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\n#print(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))\n","a06a2aa6":"## feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature[:25], title = \"Feature Importance of LGB Model 1\")\nv1.vbar(x = df_fimp_1_cc.feature[:25], top = df_fimp_1_cc.importance[:25], width = 1)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature[:25], title = \"Feature Importance of LGB Model 14\")\nv14.vbar(x = df_fimp_14_cc.feature[:25], top = df_fimp_14_cc.importance[:25], width = 1)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature[:25], title = \"Feature Importance of LGB Model 28\")\nv28.vbar(x = df_fimp_28_cc.feature[:25], top = df_fimp_28_cc.importance[:25], width = 1)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)\n","d8d8e3be":"df_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\",\n                                                     \"ConfirmedCases_test_logy\",\"Fatalities_test_logy\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\"]].reset_index()\n#df_test[\"ConfirmedCases\"] = df_test.ConfirmedCases_test_lgb\n#df_test[\"Fatalities\"] = df_test.Fatalities_test_lgb\n\ndf_test[\"ConfirmedCases\"] = 0.8 * df_test.ConfirmedCases_test_lgb + 0.2 * df_test.ConfirmedCases_test_logy\ndf_test[\"Fatalities\"] = 0.8 * df_test.Fatalities_test_lgb + 0.2 * df_test.Fatalities_test_logy\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\ndf_submission.to_csv('submission.csv', index = False)","0228a6eb":"## LGB Model","8ddbff8b":"## External dataset of my choise, data cleaning and feature engineering","0d852507":"### Customized label encoding that leaves NaN value as NaN","a052e430":"## Validation","a64cf28c":"### Function to merge my added data to main dataframe","8196d3b6":"## Regression","2b92b239":"## writing final submission and complete output\ndf_submission.to_csv(PATH_SUBMISSION, index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)\n","b5b4cc83":"I borrowed the frame in [Vopani's work in week 2](https:\/\/www.kaggle.com\/rohanrao\/covid-19-w2-lgb-mad) to save some time. He did a excellent work there. I didn't use his model on moving average though.","e5c6e218":"## Modelling","eaccb1c5":"## Submission","67eb2115":"## Visualizing Predictions\n* Viewing the actual, validation and test values together for each geography for ConfirmedCases as well as Fatalities.","7f5bb653":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft\n","c33e389d":"## MAD Model","0f4fade0":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.41 * df_test.ConfirmedCases_test_lgb + 0.59 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.05 * df_test.Fatalities_test_lgb + 0.95 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these countries well\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission\n","6e23defc":"## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 ConfirmedCases over time\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases, color = \"green\", legend_label = \"CC (Train)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_lgb, color = \"blue\", legend_label = \"CC LGB (Val)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_mad, color = \"purple\", legend_label = \"CC MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"CC LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"CC MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","4c2601d9":"## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Fatalities over time\")\n    v.line(df_geography.Date, df_geography.Fatalities, color = \"green\", legend_label = \"FT (Train)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_lgb, color = \"blue\", legend_label = \"FT LGB (Val)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_mad, color = \"purple\", legend_label = \"FT MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"FT LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"FT MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","bb9c0e3d":"## Covid-19 Week 4 light gbm model\n\nweek 4 of COVID-19 reseasrch using light gbm as base learner.\n\nPlease upvote:) and enjoy\n\n"}}