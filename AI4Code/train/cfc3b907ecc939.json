{"cell_type":{"c2de25a7":"code","72dbf28d":"code","b850d59e":"code","c8b6b105":"code","8126c5cd":"code","ed034a98":"code","cb517cc9":"code","c6b37cf5":"code","684ffa29":"code","f101cfea":"code","5955c540":"code","4113db2c":"code","ace27ba2":"code","80464d21":"code","b757cf2a":"code","c9b76fb0":"markdown","e8b72f8c":"markdown","79e5494d":"markdown","2a8825e1":"markdown","35526d2c":"markdown","ca552a92":"markdown","c51abd3c":"markdown","8bb44fac":"markdown","25db676d":"markdown","38bd19c6":"markdown","ab86415a":"markdown","baeb18c5":"markdown"},"source":{"c2de25a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72dbf28d":"%matplotlib inline\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F","b850d59e":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","c8b6b105":"class Config():\n    training_dir = \"..\/input\/oneshotlearning\/data\/faces\/training\"\n    testing_dir = \"..\/input\/oneshotlearning\/data\/faces\/testing\"\n    train_batch_size = 64\n    train_number_epochs = 100\n","8126c5cd":"class SiameseNetworkDataset(Dataset):\n    \n    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n        self.imageFolderDataset = imageFolderDataset    \n        self.transform = transform\n        self.should_invert = should_invert\n        \n    def __getitem__(self,index):\n        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n        #we need to make sure approx 50% of images are in the same class\n        should_get_same_class = random.randint(0,1) \n        if should_get_same_class:\n            while True:\n                #keep looping till the same class image is found\n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1]==img1_tuple[1]:\n                    break\n        else:\n            while True:\n                #keep looping till a different class image is found\n                \n                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n                if img0_tuple[1] !=img1_tuple[1]:\n                    break\n                    \n        img0 = Image.open(img0_tuple[0])\n        img1 = Image.open(img1_tuple[0])\n        img0 = img0.convert(\"L\")\n        img1 = img1.convert(\"L\")\n        \n        if self.should_invert:\n            img0 = PIL.ImageOps.invert(img0)\n            img1 = PIL.ImageOps.invert(img1)\n\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n    \n    def __len__(self):\n        return len(self.imageFolderDataset.imgs)","ed034a98":"folder_dataset = dset.ImageFolder(root=Config.training_dir)","cb517cc9":"siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n                                        transform=transforms.Compose([transforms.Resize((100,100)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)","c6b37cf5":"vis_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=4,\n                        batch_size=4)\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","684ffa29":"class SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(1, 4, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(4),\n            \n            nn.ReflectionPad2d(1),\n            nn.Conv2d(4, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(8, 8, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(8),\n\n\n        )\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(8*100*100, 500),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(500, 500),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(500, 5))\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2","f101cfea":"class ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","5955c540":"train_dataloader = DataLoader(siamese_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=Config.train_batch_size)","4113db2c":"net = SiameseNetwork().cuda()\ncriterion = ContrastiveLoss()\noptimizer = optim.Adam(net.parameters(),lr = 0.0005 )","ace27ba2":"counter = []\nloss_history = [] \niteration_number= 0","80464d21":"for epoch in range(0,Config.train_number_epochs):\n    for i, data in enumerate(train_dataloader,0):\n        img0, img1 , label = data\n        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n        optimizer.zero_grad()\n        output1,output2 = net(img0,img1)\n        loss_contrastive = criterion(output1,output2,label)\n        loss_contrastive.backward()\n        optimizer.step()\n        if i %10 == 0 :\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(loss_contrastive.item())\nshow_plot(counter,loss_history)","b757cf2a":"folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\nsiamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n                                        transform=transforms.Compose([transforms.Resize((100,100)),\n                                                                      transforms.ToTensor()\n                                                                      ])\n                                       ,should_invert=False)\n\ntest_dataloader = DataLoader(siamese_dataset,num_workers=6,batch_size=1,shuffle=True)\ndataiter = iter(test_dataloader)\nx0,_,_ = next(dataiter)\n\nfor i in range(2):\n    _,x1,label2 = next(dataiter)\n    concatenated = torch.cat((x0,x1),0)\n    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n    euclidean_distance = F.pairwise_distance(output1, output2)\n    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))","c9b76fb0":"Code and description from: \n\n1. https:\/\/github.com\/harveyslash\/Facial-Similarity-with-Siamese-Networks-in-Pytorch\n\n2. https:\/\/hackernoon.com\/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7\n\nThank you!","e8b72f8c":"# Importing Libraries","79e5494d":"# Custom Siamese Dataset\nThe architecture requires an input pair, along with the label (similar\/dissimilar). Therefore, we created our own custom data loader to do the job. It uses the image folder to read images from folders. This means that you can use this on any dataset that you wish. The Siamese Network dataset generates a pair of images, along with their similarity label (0 if genuine, 1 if imposter). To prevent imbalances, I ensure that nearly half of the images are from same class, while the other half is not.","2a8825e1":"# Standard Convolutional Neural Network\nWe will use a standard convolutional neural network architecture. We use batch normalization after each convolution layer, followed by dropout. It accepts an input of 100px*100px and has 3 full connected layers after the convolution layers. We use one model and feed it two images in succession. After that we calculate the loss value using both the images, and then back propagate. This saves a lot of memory at absolute no hit on other metrics (like accuracy).","35526d2c":"# Setting up the helper function\nIt performs part of the computation of another function.","ca552a92":"# Contrastive Loss\nIt is distance-based loss as opposed to more convention error-prediction loss. Loss is used to learn embeddings in which two similar points have a low Euclidian distance. \n\nWe defined contrastive loss to be:\n\n![](https:\/\/hackernoon.com\/hn-images\/1*tDo6545MUvW9t-A8Sd-hHw.png)\n\nAnd we defined Dw(which is just the euclidean distance)as :\n\n![](https:\/\/hackernoon.com\/hn-images\/1*Tj-3urWYfhUD7pQ8N_SDCw.png)\n\nGw is the output of our network for one image.\n\nThe contrastive loss in PyTorch looks like this:","c51abd3c":"# Testing\n\nWe had held out 2 subjects for the test set, which will be used to evaluate the performance our model.","8bb44fac":"# Using the dataset\nThe dataset contains a single image of 6 subjects. We put aside 2 subjects from training to test our model.","25db676d":"# Visualising the data","38bd19c6":"# Importing the dataset and configuring it","ab86415a":"The results are quite good. The network is able to distinguish between the same person. It also does a good job at discriminating dissimilar images.","baeb18c5":"# Training\nThe training process of a Siamese network is as follows:\n1.\tPass the first image of the image pair through the network.\n2.\tPass the 2nd image of the image pair through the network.\n3.\tCalculate the loss using the ouputs from 1 and 2.\n4.\tBack propagate the loss to calculate the gradients.\n5.\tUpdate the weights using an optimiser. We will use Adam for this example.\n"}}