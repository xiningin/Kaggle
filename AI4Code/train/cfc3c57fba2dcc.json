{"cell_type":{"00f87d22":"code","dbf6a9d4":"code","737c7541":"code","f00620af":"code","9dd40abc":"code","64879a1c":"code","d01892a3":"code","36746996":"code","c2bc7471":"markdown","98b82624":"markdown","c960abb2":"markdown","cbca272b":"markdown","02d6c624":"markdown","2e257e28":"markdown","7205d702":"markdown"},"source":{"00f87d22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndatadir = '\/kaggle\/input\/multimodal-classification-2021-mi203\/data'\n#datadir = 'data'\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ndata_df = pd.read_csv(os.path.join(datadir,'data_train.csv'), delimiter=',', nrows = None)\ndata = np.array(data_df)\n\nlabels = data[:,-1].astype('int32')\n\naudio = data[:, 1:-1].astype('float32')\n\nimg_list = data_df['IMAGE']","dbf6a9d4":"import torch\nimport random\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n","737c7541":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# visu image\nidx = 5\nclass_list = ['FOREST', 'CITY', 'BEACH', 'CLASSROOM', 'RIVER', 'JUNGLE', 'RESTAURANT', 'GROCERY-STORE', 'FOOTBALL-MATCH']\nimg = Image.open(os.path.join(datadir, img_list.iloc[idx]))\nplt.imshow(np.asarray(img))\nprint(class_list[labels[idx]])","f00620af":"from PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\nclass ImageAudioDataset(Dataset):\n    def __init__(self, root_dir, files, audio, labels=None, img_transform=None, audio_transform=None):\n        self.root_dir = root_dir\n        self.files = files\n        self.audio = audio\n        self.labels = labels\n        self.img_transform = img_transform\n        self.audio_transform = audio_transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.root_dir, self.files.iloc[idx]))\n        audio = self.audio[idx,:]\n        if self.img_transform is not None:\n            img = self.img_transform(img)\n        if self.audio_transform is not None:\n            audio = self.audio_transform(audio)\n        if self.labels is not None:\n            return img, audio, int(self.labels[idx])\n        else:\n            return img, audio\n\nimport torchvision\n\nimg_list_transform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224,224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\naudio_transform = None\n\nimg_dataset = ImageAudioDataset(root_dir=datadir,\n                               files=img_list,\n                                 audio=audio,\n                                 labels=labels,\n                              img_transform=img_list_transform,\n                                 audio_transform=audio_transform)\n## Taille du batch\nnsample = 100\n\n# img_dataset = ImageAudioDataset(root_dir=datadir,\n#                                files=img_list[:10*nsample],\n#                                  audio=audio[:10*nsample,:],\n#                                  labels=labels[:10*nsample],\n#                               img_transform=img_list_transform,\n#                                  audio_transform=audio_transform)\n\n# Shuffle = false pour echantillonner les donn\u00e9es dans l'ordre\nimg_loader = DataLoader(img_dataset, batch_size=nsample, shuffle=False, num_workers=4, pin_memory=True)","9dd40abc":"import torchvision.models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nresnext = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n\n# Si besoin de changer la derniere couche\n#num_ftrs = resnext.fc.in_features\n#resnext.fc = nn.Linear(num_ftrs, 9)\n\n\nif device.type == 'cuda':\n    resnext = resnext.cuda()\n\nmodel = resnext\n\n# Enleve la derni\u00e8re couche\nmodules=list(model.children())[:-1]\nmodel=nn.Sequential(*modules)\nfor p in model.parameters():\n    p.requires_grad = False\n\n#print(model)","64879a1c":"from tqdm import tqdm\n\nfeat = []\nfor i, data in enumerate(tqdm(img_loader)):   ## on itere sur les donn\u00e9es \n    img, audio, targets = data\n    #print(targets)\n    if device.type == 'cuda':\n        img, audio, targets = img.cuda(), audio.cuda(), targets.cuda()\n    with torch.no_grad():\n        outputs = model(img)\n    if device.type == 'cuda':\n        feat.append(outputs.cpu().numpy().squeeze())\n    else:\n        feat.append(outputs.numpy().squeeze())\n        ","d01892a3":"imgfeat = np.concatenate(feat)\n\nnp.save(os.path.join('\/kaggle\/working\/', 'img_feat'), imgfeat)\nprint(imgfeat.shape)","36746996":"imgfeatcopy = np.load(os.path.join('\/kaggle\/working\/', 'img_feat.npy'))\n\nprint(imgfeatcopy.shape)","c2bc7471":"### Sauvegarde des caract\u00e9ristiques pr\u00e9-calcul\u00e9es","98b82624":"### Charge un r\u00e9seau pr\u00e9-appris","c960abb2":"### Calcul des caracteristiques (par batch + concatenation)","cbca272b":"### Visualisation d'une image","02d6c624":"### Lecture des caract\u00e9ristiques pr\u00e9-calcul\u00e9es","2e257e28":"## Generation de caracteristiques profondes d'images","7205d702":"### Data loader (pour Pytorch)"}}