{"cell_type":{"09bdb5b3":"code","c112c9e7":"code","f4e243f2":"code","e679b035":"code","8caefe0f":"code","ab1e9717":"code","c955800e":"code","5b54a5ad":"code","af2f139a":"code","294fb58e":"code","2a97bffe":"code","384c661a":"code","7144897b":"code","5e81570c":"markdown"},"source":{"09bdb5b3":"from tensorflow.keras import layers, optimizers, callbacks, utils, losses, metrics, backend as K\nfrom sklearn import metrics as skmetrics, preprocessing\nfrom tensorflow.keras.models import Model, load_model\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport os, gc, joblib, warnings\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\nwarnings.filterwarnings('ignore')","c112c9e7":"def create_model(data, catcols):    \n    inputs = []\n    outputs = []\n    for c in catcols:\n        num_unique_values = int(data[c].nunique())\n        embed_dim = int(min(np.ceil((num_unique_values)\/2), 20))\n        inp = layers.Input(shape=(1,))\n        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n        out = layers.SpatialDropout1D(0.25)(out)\n        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    x = layers.Concatenate()(outputs)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dense(300, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dense(300, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.BatchNormalization()(x)\n    \n    y = layers.Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=inputs, outputs=y)\n    return model","f4e243f2":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\n","e679b035":"test_id = test.id.values\ntrain = train.drop(['id'], axis=1)\ntest = test.drop(['id'], axis=1)","8caefe0f":"sparse_features = [f for f in train.columns if 'cat' in f]\ndense_features = [feat for feat in train.columns if  feat not in sparse_features+['target']]\n\nfor col in sparse_features:\n    train_only = list(set(train[col].unique()) - set(test[col].unique()))\n    test_only = list(set(test[col].unique()) - set(train[col].unique()))\n    both = list(set(test[col].unique()).union(set(train[col].unique())))\n    train.loc[train[col].isin(train_only), col] = np.nan\n    test.loc[test[col].isin(test_only), col] = np.nan\n    mode = train[col].mode().values[0]\n    train[col] = train[col].fillna(mode)\n    test[col] = test[col].fillna(mode)\n    ","ab1e9717":"for feat in dense_features:\n    test[feat] = np.clip(test[feat], train[feat].min(), train[feat].max())","c955800e":"test[\"target\"] = -1\ndata = pd.concat([train, test]).reset_index(drop=True)\n\nfor c in dense_features:\n    data[f'q_{c}'], bins_ = pd.qcut(data[c], 25, retbins=True, labels=[i for i in range(25)])\n    data[f'q_{c}'] = data[f'q_{c}'].astype('str')\n    sparse_features.append(f'q_{c}')","5b54a5ad":"features = sparse_features\nfor feat in features:\n    lbl_enc = preprocessing.OrdinalEncoder()\n    data[feat] = lbl_enc.fit_transform(data[feat].fillna('-1').values.reshape(-1,1).astype(str))\n    \ntrain = data[data.target != -1].reset_index(drop=True)\ntest = data[data.target == -1].reset_index(drop=True)\ntest_data = [test.loc[:, features].values[:, k] for k in range(test.loc[:, features].values.shape[1])]","af2f139a":"oof_preds = np.zeros((len(train)))\nbagged_oof_preds = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\nbagged_test_preds = np.zeros((len(test)))\nlearning_rate = 1e-3\nlabel_smoothing = 0.0\nVerbose = 0\nn_splits = [10, 15]\nn_bags = 2\nseeds = [2021, 2021]","294fb58e":"for bag in range(n_bags):\n    print(f'Iteration {bag+1} splits {n_splits[bag]} seed {seeds[bag]}')\n    for fold, (train_index, test_index) in enumerate(StratifiedKFold(n_splits=n_splits[bag], shuffle=True, random_state=seeds[bag]).split(train, train.target.values)):\n        X_train, X_test = train.iloc[train_index, :], train.iloc[test_index, :]\n        X_train = X_train.reset_index(drop=True)\n        X_test = X_test.reset_index(drop=True)\n        y_train, y_test = X_train.target.values, X_test.target.values\n        model = create_model(data, features)\n        model.compile(\n            optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(learning_rate=learning_rate)),\n            loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n            metrics=metrics.AUC(name=\"AUC\"),\n        )\n\n        X_train = [X_train.loc[:, features].values[:, k] for k in range(X_train.loc[:, features].values.shape[1])]\n        X_test = [X_test.loc[:, features].values[:, k] for k in range(X_test.loc[:, features].values.shape[1])]\n    \n        es = callbacks.EarlyStopping(monitor='val_AUC', min_delta=0.000001, patience=10, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n        sb = callbacks.ModelCheckpoint('.\/nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose, monitor='val_AUC',mode='max')\n        plateau  = callbacks.ReduceLROnPlateau(monitor='val_AUC', factor=0.5, patience=2, verbose=Verbose,\n                                        mode='max', min_delta=0.0001, cooldown=0, min_lr=1e-7)\n        model.fit(X_train,\n                  y_train,\n                  validation_data=(X_test, y_test),\n                  verbose=Verbose,\n                  batch_size=1024,\n                  callbacks=[es, sb, plateau],\n                  epochs=100\n             )\n        valid_fold_preds = model.predict(X_test)\n        test_fold_preds = model.predict(test_data)\n        oof_preds[test_index] = rankdata(valid_fold_preds.ravel())\/len(X_test)\n        test_preds += rankdata(test_fold_preds.ravel() \/ n_splits[bag])\/len(test)\n        print(f'fold {fold+1} AUC : {skmetrics.roc_auc_score(y_test, valid_fold_preds)}')\n        K.clear_session()\n    print(f'Overall AUC of Iteration {bag+1} = {skmetrics.roc_auc_score(train.target.values, oof_preds)}')\n    np.save(f'oof_preds_{bag}',oof_preds)\n    np.save(f'test_preds_{bag}',test_preds)\n    bagged_test_preds += test_preds \/ n_bags\n    bagged_oof_preds += oof_preds \/ n_bags","2a97bffe":"print(\"Overall AUC={}\".format(skmetrics.roc_auc_score(train.target.values, bagged_oof_preds)))","384c661a":"print('Saving submission file')\nsubmission = pd.DataFrame.from_dict({\n    'id': test_id,\n    'target': bagged_test_preds,\n})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(3)","7144897b":"top_public = pd.read_csv('\/kaggle\/input\/tps-mar-2021-stacked-starter\/submission.csv')\nsubmission['target'] = (rankdata(submission.target) * 0.275 + rankdata(top_public.target) * 0.725)\/len(submission)\nsubmission.to_csv('blend.csv', index=False)\nsubmission.head(3)","5e81570c":"This **Code** is mainly based on [https:\/\/www.kaggle.com\/abhishek\/same-old-entity-embeddings](https:\/\/www.kaggle.com\/abhishek\/same-old-entity-embeddings) and the submission from [https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter](https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter) was used for blend,"}}