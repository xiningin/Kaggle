{"cell_type":{"dcf443e6":"code","6acedde6":"code","0aa29b03":"code","c82df58d":"code","68760ba6":"code","5c2d939b":"code","dfb229d2":"code","1523898e":"code","4f05dee7":"code","623ab995":"code","f40f69b2":"code","a26f6896":"code","384e86d5":"code","7e851561":"code","a4a784e8":"code","058866bd":"code","ce6d8fc2":"code","65bfa356":"code","b70f0864":"code","4226e36c":"code","4c258843":"markdown","89f3c885":"markdown","27e91d26":"markdown","4d3c9866":"markdown","c08fb09f":"markdown","9b947f3b":"markdown","2cf0ce69":"markdown","6394fa1b":"markdown","e5fea488":"markdown","7ee95518":"markdown","137bb566":"markdown","6d66baa8":"markdown","a0838d61":"markdown","57859428":"markdown","2da4f274":"markdown","e0e8db6d":"markdown","6305e5fe":"markdown","cf84c357":"markdown","4ab96ac4":"markdown","3d968f1a":"markdown"},"source":{"dcf443e6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split","6acedde6":"file_path=\"..\/input\/iowa-house-prices\/train.csv\"\nmy_data=pd.read_csv(file_path,index_col='Id')\n\n# Separate target from predictors\ny = my_data.SalePrice\nX = my_data.drop(['SalePrice'], axis=1)\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)","0aa29b03":"# MoSold, YrSOld, SaleType, SaleCondition are variables that can be provided only after the sale action\n#so we will drop these columns \nleakage_columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\nX_train.drop(labels = leakage_columns, axis = 1, inplace = True , errors='ignore')\nX_valid.drop(labels = leakage_columns, axis = 1, inplace = True , errors= ' ignore')\n\n\n","c82df58d":"categorical_columns=X_train.select_dtypes(include=[\"object\",\"category\"])\nnumerical_columns=X_train.select_dtypes(exclude=[\"object\",\"category\"])","68760ba6":"print(f' The dataset contains {my_data.isna().sum().sum() } missing values in total')","5c2d939b":"cols_with_missing_values=[]\nfor i in X_train.columns:\n    if(X_train[i].isna().sum()!=0):\n        cols_with_missing_values.append(i)\n        print(f'{i}---->{X_train[i].isna().sum()} missing values')\n        ","dfb229d2":"# Loop over columns containing > 90% nan\n# Drop these columns\nfor i in cols_with_missing_values:\n    if (X_train[i].isna().sum()) \/ (X_train.shape[0]) >= 0.9:\n        print(f'{i} ----->{(X_train[i].isna().sum()) \/ (X_train.shape[0])}')\n        X_train.drop(i,axis=1,errors='ignore',inplace=True)\n# Remove the deleted features from the col_with_missing_values to prevent errors  when using these columns\n        cols_with_missing_values.remove(i)\n      ","1523898e":"healthy_nan_cols=['BsmtExposure','BsmtCond','BsmtQual','Fence','GarageCond','GarageQual','GarageFinish','GarageType',\n                 'FireplaceQu','BsmtFinType2','BsmtFinType1']","4f05dee7":"X_train[healthy_nan_cols]=X_train[healthy_nan_cols].fillna('No')\nX_valid[healthy_nan_cols]=X_valid[healthy_nan_cols].fillna('No')","623ab995":"# checking the remaining missing values to this step\ncols_with_missing_values=[]\nfor i in X_train.columns:\n    if(X_train[i].isna().sum()!=0):\n        cols_with_missing_values.append(i)\n        print(f'{i}---->{X_train[i].isna().sum()} missing values')\n        ","f40f69b2":"plt.figure(figsize=(14,4))\nsns.kdeplot(X_train['LotFrontage'],shade=True,color='orange')","a26f6896":"sns.kdeplot(X_train['GarageYrBlt'],shade=True,color='green')","384e86d5":"sns.kdeplot(X_train['MasVnrArea'],shade=True,color='violet')","7e851561":"# Categorical columns with more than 10 modalities shouldn't be encoded by the oneHotEncoder \n# So we need to divide Categorical columns to >10 modality and <= 10 modality\ncolumns_to_be_OH_encoded = [x for x in X_train.columns if X_train[x].nunique() <= 10 and \n                        X_train[x].dtype == \"object\"]\n\ncolumns_not_to_be_OH_encoded=[x for x in X_train.columns if X_train[x].nunique() > 10 and \n                        X_train[x].dtype == \"object\"]\n\nnumerical_cols = [x for x in X_train.columns if X_train[x].dtype in ['int64', 'float64']]\n","a4a784e8":"my_train_valid_cols = columns_to_be_OH_encoded + columns_not_to_be_OH_encoded + numerical_cols","058866bd":"X_train=X_train[my_train_valid_cols].copy()\nX_valid=X_valid[my_train_valid_cols].copy()","ce6d8fc2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute  import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline","65bfa356":"label_X_train=X_train.copy()\nlabel_X_valid=X_valid.copy()\n\n# Apply ordinal encoder to each column with categ data >10\nordinal_encoder=OrdinalEncoder()\n\nlabel_X_train[columns_not_to_be_OH_encoded]=ordinal_encoder.fit_transform(X_train[columns_not_to_be_OH_encoded])\nlabel_X_valid[columns_not_to_be_OH_encoded]=ordinal_encoder.transform(X_valid[columns_not_to_be_OH_encoded])\n\n","b70f0864":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data with oneHotEncoder\ncategorical_transformer1 = Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Preprocessing for categorical data with OrdinalEncoder\ncategorical_transformer2 = SimpleImputer(strategy='most_frequent')\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat1', categorical_transformer1, columns_to_be_OH_encoded),\n        ('cat', categorical_transformer2, columns_not_to_be_OH_encoded)\n    ])","4226e36c":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nmodel_XGB1 = XGBRegressor(learning_rate = 0.05, n_estimators=1000, random_state=0)\npipeline_XGB1 = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', model_XGB1)])\npipeline_XGB1.fit(label_X_train, y_train)\npreds = pipeline_XGB1.predict(label_X_valid)\nmae = mean_absolute_error(y_valid, preds)\nprint('MAE for XGBRegressor with pipeline_XGB1 :', mae)","4c258843":"## Data cleaning \ud83e\uddf9","89f3c885":"\ud83d\ude4c We ended with only 5 columns needing to be imputed :) ","27e91d26":"\ud83d\udccc Note that even if the Nan value is logical , when it comes to modeling , these missing values will cause errors So ,Now that we have identified the healthy_nan_cols , we will replace their nan by 'NO'\nExample : The GarageCond column describes the condition of the garage , but since the house doesn't have a garage , the garageCond is NAN .So It will be replaced by 'NO' ( As to say No garage ) ","4d3c9866":"## why is the data missing ? \ud83e\udd14","c08fb09f":"\u2714\ufe0f The code above has removed these three columns \n\n* Alley ----->0.9376712328767123\n* PoolQC ----->0.9952054794520548\n* MiscFeature ----->0.963013698630137","9b947f3b":"![image.png](attachment:5f4c1088-3c15-4575-8910-e36bfa859755.png)","2cf0ce69":"This is my first notebook to share on kaggle . \nI am looking forward to adding new stuff to this notebook and make it better .For now , I tried to apply  what I have learned. \nHope you like it \ud83d\ude03\ud83d\ude03\n\n","6394fa1b":"## Data leakage","e5fea488":"\ud83d\udccc Before pluging categorical variables into machine learning models we need to preprocess them by \n1. Deleting them if they are not useful for the modeling\n2. Encode them with OrdinalEncoder \n3. Encode them with OneHotEncoder \ud83d\udd25 ( Do not use OneHotENcoder to encode variables of more than 10 categories )","7ee95518":"## Imputation AND Encoding\n","137bb566":"\ud83d\udccc At this  point , we need to use our intuition by looking at our data , and trying to understand why values are missing.\nTo figure this out we should ask ourselves is this value missing because it wasn't recorded or because it doesn't exist?\nIf a value is missing becuase it doesn't exist , we won't bother replacing it by imputation \nFor example : If the house is a two bedroom house , it's normal to have a nan value for the column giving the surface of the third room ( No third room ==> no third room surface )","6d66baa8":"\ud83d\udcdd Since the cleaning process depends on whether the variable is numerical or categorical , we will divide the columns into categorical and numerical .","a0838d61":"We will be referring to columns which don't need imputation by healthy_nan_cols , Next we will treat them seperatly .","57859428":"So this was my first try with house prices predictions . Next i will work on feature engineering and tune my model in oredr to make better predictions :) ","2da4f274":"\ud83d\udcdd Let's have a look at how many missing values each column contains ","e0e8db6d":"# IMPORTS \ud83d\udcc1","6305e5fe":"\ud83d\udcdd Columns containing most of their values missing won't help in the prediction\n   So we will  start by dropping columns containing 90% or more of their values as NAN","cf84c357":"### Missing values ","4ab96ac4":"### \ud83d\udcca Let's have a look at the distributions of the numerical columns before imputing them","3d968f1a":"## Data setup \ud83d\udcbe"}}