{"cell_type":{"238abc7b":"code","a6e2a885":"code","71e66c6f":"code","9fd895cb":"code","b67c057d":"code","09c99be5":"code","616a18fa":"code","f76ef5e8":"code","db2cd21c":"code","0b102584":"code","f678e82a":"code","bdff020c":"code","ec136ab3":"code","1105774a":"code","e0510653":"code","82f074e8":"code","ed0e00fa":"code","959e669d":"code","89c9143b":"code","5864e206":"code","23fff015":"code","8d08efff":"code","fb7ce783":"code","59c9d717":"code","2b8c6a43":"code","4b6d8e80":"code","fbf4b7fa":"code","e881819b":"code","49e20449":"code","79b19743":"code","6a5f7f2d":"code","3b258c24":"code","4114be62":"code","32263f7d":"code","f8720fbc":"code","07e997d6":"code","58b625bb":"code","fd9d9448":"markdown","141d1ed8":"markdown","af671f13":"markdown","2a7bcb0e":"markdown","0ef06e97":"markdown","af99dc74":"markdown","1c8907ff":"markdown","c6792a38":"markdown","08a83138":"markdown","7161ce60":"markdown","e39ec027":"markdown","046d707a":"markdown","5a080bd4":"markdown","daa8cfce":"markdown","c3862efe":"markdown","4e92306a":"markdown","5f81b6e8":"markdown","1d226b62":"markdown","ce824d30":"markdown","865da83a":"markdown","50801e87":"markdown","5c996654":"markdown","7ae14ee1":"markdown","a52151a8":"markdown","ec36cf91":"markdown","f15b5e98":"markdown","e105525e":"markdown","4f83d22e":"markdown","8bb18a6e":"markdown","c80929f2":"markdown"},"source":{"238abc7b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import mean_squared_error,r2_score \nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor","a6e2a885":"price=pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')","71e66c6f":"price.head()","9fd895cb":"price.info()","b67c057d":"price.describe()","09c99be5":"price.corr()['price'].sort_values(ascending=False)","616a18fa":"price['CarName'].unique()","f76ef5e8":"price['company'] = [x.split()[0] for x in price['CarName']]\nprice['company'] = price['company'].replace({'maxda': 'Mazda','mazda': 'Mazda', \n                                     'nissan': 'Nissan', \n                                     'porcshce': 'Porsche','porsche':'Porsche', \n                                     'toyouta': 'Toyota', 'toyota':'Toyota',\n                            'vokswagen': 'Volkswagen', 'vw': 'Volkswagen', 'volkswagen':'Volkswagen'})","db2cd21c":"price.drop(['CarName','car_ID'],axis=1,inplace=True)","0b102584":"num_atr=['wheelbase','carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','compressionratio', 'horsepower','peakrpm','citympg','highwaympg','price']\n\ncat_atr=['symboling','fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem','company']","f678e82a":"price[num_atr].hist(bins=40, figsize=(20,15)) \nplt.show()","bdff020c":"plt.figure(figsize=(20,12))\nx=1\nfor i in num_atr:\n    plt.subplot(4,4,x)\n    plt.scatter(data=price,y='price',x=i,alpha=0.5)\n    plt.ylabel('Price',fontsize=14)\n    plt.xlabel(i,fontsize=14)\n    plt.tight_layout()\n    x=x+1","ec136ab3":"bins = (0, 15, np.inf)\ngroup_names = ['low', 'high']\nprice['compressionratio'] = pd.cut(price['compressionratio'], bins = bins, labels = group_names)\n\nnum_atr.remove('compressionratio')\nnum_atr.remove('price')\ncat_atr.append('compressionratio')","1105774a":"plt.figure(figsize=(20,12))\nx=1\nfor i in cat_atr:\n    plt.subplot(3,4,x)\n    sns.boxplot(data=price,y='price',x=i)\n    plt.ylabel('Price',fontsize=14)\n    plt.xlabel(i,fontsize=14)\n    plt.tight_layout()\n    x=x+1","e0510653":"price = pd.get_dummies(price, columns=cat_atr, drop_first=True)\nprice.head()","82f074e8":"X= price.drop(['price'],axis=1)\ny= price['price']\n\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","ed0e00fa":"atr=X_train.select_dtypes('number').columns","959e669d":"pipe = make_column_transformer((StandardScaler(),atr),remainder='passthrough')","89c9143b":"pipe.fit(X_train)\npipe.fit(X_test)","5864e206":"#Learning Curve\n#give training set and training labels as parameter, they will be then split in training and validation sets\ndef plot_learning_curve(model,X,y):   \n    X_train_lc,X_val_lc,y_train_lc,y_val_lc=train_test_split(X,y,test_size=0.2,random_state=42)\n    train_errors,val_errors=[],[]\n    for m in range(1,len(X_train)):\n        model.fit(X_train_lc[:m],y_train_lc[:m])\n        y_train_predict=model.predict(X_train_lc[:m])\n        y_val_predict=model.predict(X_val_lc)\n        train_errors.append(mean_squared_error(y_train_lc[:m],y_train_predict))\n        val_errors.append(mean_squared_error(y_val_lc,y_val_predict))\n    plt.plot(np.sqrt(train_errors),'r-+',linewidth=2,label='train')       \n    plt.plot(np.sqrt(val_errors),'b-',linewidth=3,label='val')\n    plt.ylim(0,10000)\n    plt.legend()\n    plt.grid()\n    plt.xlabel('No. of instances in training set')\n    plt.ylabel('RMSE')\n    plt.title(\"Learning Curve\")","23fff015":"def initial_check(model,X_train,y_train):\n    rmse_score=cross_val_score(model,X_train,y_train,scoring='neg_mean_squared_error',cv=10)\n    r2_score=cross_val_score(model,X_train,y_train,scoring='r2',cv=10)\n    print(\"RMSE (cross_val_score): \",np.sqrt(-rmse_score).mean())\n    print(\"R2 Score (cross_val_score): \",r2_score.mean())\n    plot_learning_curve(model,X_train,y_train)\n    plt.show()","8d08efff":"lin_reg=LinearRegression()\ninitial_check(lin_reg,X_train,y_train)\n","fb7ce783":"ridge_reg=Ridge(alpha=1.4)\ninitial_check(ridge_reg,X_train,y_train)","59c9d717":"poly_features=PolynomialFeatures(degree=2,include_bias=False)\nX_train_poly=poly_features.fit_transform(X_train)\nX_train_poly=StandardScaler().fit_transform(X_train_poly)\n\nX_test_poly=poly_features.fit_transform(X_test)\nX_test_poly=StandardScaler().fit_transform(X_test_poly)","2b8c6a43":"initial_check(lin_reg,X_train_poly,y_train)","4b6d8e80":"poly_ridge_reg=Ridge(alpha=120)\ninitial_check(poly_ridge_reg,X_train_poly,y_train)","fbf4b7fa":"forest_reg=RandomForestRegressor(random_state=42)\ninitial_check(forest_reg,X_train,y_train)","e881819b":"param_grid=([\n    {'n_estimators':[180,190,200]}\n])","49e20449":"grid_search=GridSearchCV(forest_reg,param_grid,cv=10,scoring='r2',return_train_score=True)","79b19743":"grid_search.fit(X_train,y_train)","6a5f7f2d":"grid_search.best_estimator_","3b258c24":"final_reg=grid_search.best_estimator_","4114be62":"final_reg.fit(X_train,y_train)","32263f7d":"predictions=final_reg.predict(X_test)","f8720fbc":"print(\"RMSE: \",np.sqrt(mean_squared_error(y_test,predictions)))\nprint(\"R2 Score: \",r2_score(y_test,predictions))","07e997d6":"importances = final_reg.feature_importances_\nfeature_names = X.columns\n\nl=len(final_reg.feature_importances_)\ndf=pd.DataFrame(importances,columns=['importance'],index=feature_names)\ndf.sort_values('importance',ascending=False,inplace=True)","58b625bb":"df[df['importance']>0.001]","fd9d9448":"The above DataFrame gives the most important features for determining the cost of a car.","141d1ed8":"#### Numerical Data Visualizations","af671f13":"## Fine tuning the best model (RandomForestRegressor)","2a7bcb0e":"We can observe that some of the numerical features are skewed towards left, specially the features which have positive strong correlation with price, so we will scale the data before training our model","0ef06e97":"The RMSE decreased and the gap between the curves is also less\n\nLets explore some more options then we'll go to polynomial regression","af99dc74":"### i. Linear Regression","1c8907ff":"RandomForestRegressor gives the best results without adjusting the hyperparameter and the gap in Learning Curve is also less, lets try adjusting its hyperparameters to make it better.","c6792a38":"### Plots\n1. num_atr --> Numerical attributes\n2. cat_atr --> Categorical attributes","08a83138":"##### Ridge","7161ce60":"Okay, this is a lot of data, lets break each feature one by one:\n1. <u>symboling<\/u>: more -ve value means car is more safe and more +ve means it is risky, the plot shows more safe as well as more risky cars are expensive and most of the affordable cars fall between 0 to 2, this type of variation might be because of some other features in car.\n2. <u>fueltype<\/u>: doesn't matter for most cars, but the highly expensive cars mostly runs on gas.\n3. <u>aspiration<\/u>: doesn't matter for most cars, but highly expensive cars mostly have std.\n4. <u>doornumber<\/u>: doesn't matter except for some outlier for 2 door cars.\n5. <u>carbody<\/u>: if the body of the car is hardtop, it is pretty likely that it will be expensive.\n6. <u>drivewheel<\/u>: car with rwd class drivewheel are generally expensive.\n7. <u>enginelocation<\/u>: cars which have engine at their back are mostly expensive but majority of the cars have engine in the front.\n8. <u>enginetype<\/u>: cars with 'ochv' engine type are expensive.\n9. <u>cylindernumber<\/u>: cars with eight cylinders are generally expensive followed by six and five.\n10. <u>fuelsystem<\/u>: 'mpfi' fuel system cars are slightly expensive than others.\n11. <u>company<\/u>: prices does depend on the company, some have high and some have low prices.\n12. <u>compressionratio<\/u>: doesn't matter for most cars, but there highly is a slight difference in mean price for both the classes.","e39ec027":"The maximum **enginesize** seems to be a outlier, since 75% of the values of this feature are less than 141 which is less than half of the maximum value, the same thing can also be observed in **compressionratio, horsepower** we'll have look at them later to be sure about it.","046d707a":"On testing, we can observe that the model hai performed well on the test set, the RMSE value is the least and R2 score is the highest.","5a080bd4":"So, earlier we saw that there might be some outliers in some features but now we can see from the scatter plot that some values in horsepower and engine size are high but they seems to follow the trends with price so we don't need to remove them. \n\nFor compressionratio, we can observe that it can be converted into classes (high and low) and the prices does not seem to depend on it, we may either remove it or convert it into categorical attribute as low and high.\n\nWe can also observe that some features seems to show a quadratic relationship with price, we should remember this point and maybe use it for selecting our ML model","daa8cfce":"We have come from a score of 2945 to 2268 by using polynomial regression instead of linear regression but the R2 score has been decreased, also there is a large gap in Learning Curve which represens overfitting hence the model might not be generalized well. So its performance might vary with test\/new data.","c3862efe":"What do we need to do for preparing the data?\n1. use pd.get_dummies to encode the categorical attributes.\n\n2. scale the data.","4e92306a":"### ii. Polynomial Regression","5f81b6e8":"## ML model\n1. X_train --> Training data\n2. y_train --> Training labels\n\n3. X_test --> Test Data\n4. y_test --> Test Labels","1d226b62":"The RMSE is not so good considering cost of 50% of the cars is less than almost $10,000.\n\nThe leaning curve shows us the model's performance on the training set and the validation set as a function of training set size. We can observe that the error on the training data is much lower than validation data and there is large gap between the curves. This means the model performs significantly better on the training data than on the validation data, which is hall-mark of an overfitting model.\n\nNOTE that I've set the range for y-axis from 0 to 10000, the plot goes beyond that upto 200000 at its peak, this is done for better visualization.\n\nLets try regularizing the model.","ce824d30":"## User-Defined Functions","865da83a":"So, upto now we can conclude that the following variables are really helpful in determining the price of the car:\n\nNumerical Attributes:\n1. features enginesize \n2. curbweight\n3. horsepower\n4. carwidth\n5. carlength\n6. wheelbase\n7. boreratio\n8. citympg\n9. higwaympg\n\nCategorical Attributes:\n1. symboling\n2. carbody\n3. drivewheel\n4. enginelocation\n5. enginetype\n6. cylindernumber\n7. fuelsystem\n8. company\n\nNote: fueltype aspiration and compressionratio are important to some extent because expensive cars tends to have only one class from these.","50801e87":"This model is severly overfitted as expected, the Learning Curve stays above 10000 for most of the time, showing that the model is too overfitted. Now, I am going to regularization it.","5c996654":"## Preparation for ML","7ae14ee1":"## Testing ","a52151a8":"We need to understand the factors on which the pricing of cars depends.\n\n#### The company wants to know:\n1. Which variables are significant in predicting the price of a car.\n2. How well those variables describe the price of a car\n\n#### Goal\n1. We are required to model the price of cars with the available independent variables. \n2. It will be used by the management to understand how exactly the prices vary with the independent variables.","ec36cf91":"We can observe that among the numerical features enginesize, curbweight, horsepower, carwidth, carlength,wheelbase, boreratio, citympg and higwaympg have considerable correlationw with the price of the car","f15b5e98":"### Looking at the data","e105525e":"#### Categorical Data Visualizations","4f83d22e":"### iii. RandomForestRegressor","8bb18a6e":"Now we have the best model from GridSearchCV.","c80929f2":"Replacing the name of car by name of company"}}