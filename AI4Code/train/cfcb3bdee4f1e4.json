{"cell_type":{"27f71b48":"code","698e1ae1":"code","e02d2f9c":"code","d80e32b7":"code","816c729b":"code","850782e5":"code","fe4fdabd":"code","8d821763":"code","0de12cc6":"code","c204ffe8":"code","cf4d0646":"code","112e94a1":"code","0b3f1fea":"code","be699df1":"code","02d71870":"code","14457a9d":"code","f7b3e078":"code","8e727275":"code","3de30ab1":"code","4877d63f":"code","ee061c80":"code","dd47aa47":"code","7ca2a95a":"code","650bc72e":"code","158bb9ba":"code","49eefca7":"code","d1b853c5":"code","d9f830f3":"code","8a958bd3":"code","263b83be":"code","20fcd91d":"code","72a29072":"code","9cd39098":"code","3c4ce499":"code","39836534":"code","ef92320b":"code","82c77ace":"code","9723eebc":"code","3f662a32":"code","cbc8b51a":"code","91808157":"code","5bddecbc":"code","11e9be71":"markdown","524dcd98":"markdown","c55d270e":"markdown","13b4f1ca":"markdown"},"source":{"27f71b48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom collections import Counter\nimport re\nimport string\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\nfrom wordcloud import WordCloud\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, Input, LSTM,Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n#settings\n#start_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.","698e1ae1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.shape, test.shape","e02d2f9c":"train.head()","d80e32b7":"# train.columns.values\nlist(train)","816c729b":"test.head()\n# list(test)","850782e5":"'check for nulls'\ntrain.isnull().any(),test.isnull().any()","fe4fdabd":"x=train.iloc[:,2:].sum()\nx_values = np.sort(x.values)\n#plot  \nplt.figure(figsize=(10,5))\nax= sns.barplot( x_values, x.index, orient='h')\nplt.xlabel('# of Occurrences', fontsize=12)\nplt.ylabel('Type ', fontsize=12)\n\n# rects = ax.patches\n# labels = x.values\n   \nplt.show()\nnp.sort(x.values)","8d821763":"columns = list(x.index)\ntrain.groupby(columns).size().sort_values(ascending=False).reset_index().rename(columns={0: 'count'}).head(15)","0de12cc6":"print(\"Out of {} rows: \\n {} are toxic \\n {}  are severe_toxic \\n {}  are obscene \\n {}   are threat \\n {}  are insult and \\n {}  are identity_hate\". \\\n      format(len(train),len(train[train.toxic==1]),len(train[train.severe_toxic==1]),len(train[train.obscene==1]), \\\n             len(train[train.threat==1]),len(train[train.insult==1]),len(train[train.identity_hate==1])))","c204ffe8":"print(\"toxic examples:\")\ntrain[train['toxic']==1]['comment_text'][:5]","cf4d0646":"print(\"severe_toxic examples:\")\ntrain[train['severe_toxic']==1]['comment_text'][:5]","112e94a1":"print(\"obscene examples:\")\ntrain[train['obscene']==1]['comment_text'][:5]","0b3f1fea":"print(\"threatthreat examples:\")\ntrain[train['threat']==1]['comment_text'][:5]","be699df1":"print(\"insult examples:\")\ntrain[train['insult']==1]['comment_text'][:5]","02d71870":"fig, ax = plt.subplots(figsize=(10, 6))\nfig.suptitle('Correlation Matrix')\nsns.heatmap(train[columns].corr(), annot=True, cmap=\"YlGnBu\", linewidths=.5, ax=ax);","14457a9d":"pd.set_option('display.max_colwidth', -1)\n\nFifth_line = train.comment_text.iloc[4]\nFifth_line,len(Fifth_line)","f7b3e078":"train_length = train.comment_text.apply(len)\ntrain_length.head(6)","8e727275":"comments_max_ln  =np.max(train_length)\ncomments_min_ln  =np.min(train_length)\ncomments_mean_ln  =np.mean(train_length)\nprint (' comments_max_ln: {}, \\n comments_min_ln: {} \\n comments_mean_ln: {}'.format( comments_max_ln, \n                                                                                 comments_min_ln, comments_mean_ln ))","3de30ab1":"plt.figure(figsize = (12, 6))\nplt.hist(train_length, bins = 60, alpha = 0.5, color = 'r')\nplt.show()","4877d63f":"print(\"max length : \", np.max(train_length))\nprint(\"min length : \", np.min(train_length))\nprint(\"mean length : \", np.mean(train_length))","ee061c80":"'Check Missing Data'\n\ndef check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)\n    \ncheck_missing_data(train)","dd47aa47":"print(train.comment_text.isna().sum())\nprint(test.comment_text.isna().sum())","7ca2a95a":"test_comments = test.comment_text\ntest_comments[0:3]","650bc72e":"# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n# y = train[list_classes].values\ny = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\ny[:5]","158bb9ba":"comments_train = train.comment_text\ncomments_test = test.comment_text\n# create the tokenizer\ntok = Tokenizer()\n# fit the tokenizer on the documents\ntok.fit_on_texts(list(comments_train))  # train.comment_text\n\nnum_words_count = len(tok.word_index) + 1  # 210338 +1\ntokenizer_all_comments = Tokenizer(num_words=num_words_count)\ntokenizer_all_comments.fit_on_texts(list(comments_train))\n\nlist_tokenized_train = tokenizer_all_comments.texts_to_sequences(comments_train)\nlist_tokenized_test = tokenizer_all_comments.texts_to_sequences(comments_test)","49eefca7":"list_tokenized_test[:1]","d1b853c5":"'to find max length of words in comments'\n# distribution of number of words in sentence''\ntotalNumWords = [len(word_in_comment) for word_in_comment in list_tokenized_train]\ntotalNumWords.sort(reverse=True) # to find max word in list\ntotalNumWords[:5]","d9f830f3":"plt.hist(totalNumWords,bins = np.arange(0,400,10));","8a958bd3":"#  Most of the sentence length is about 30+. We could set the \"maxlen\" to about 50,\n#  but I'm being paranoid so I have set to 200. \nmax_len = 200\nX_train = pad_sequences(list_tokenized_train, maxlen=max_len)\nX_test = pad_sequences(list_tokenized_test, maxlen=max_len)","263b83be":"X_train[0]","20fcd91d":"# By indicating an empty space after comma, we are telling Keras to infer the number automatically.\ninp = Input(shape=(max_len, )) #maxlen=200 as defined earlier","72a29072":"embed_size = 128\nlayer = Embedding(num_words_count, embed_size)(inp);   # num_words_count = 210338 +1","9cd39098":"# layer = LSTM(60, return_sequences=True,name='lstm_layer')(layer)\n# layer = GlobalMaxPool1D()(layer)\n# layer = Dropout(0.1)(layer)\n# layer = Dense(50, activation=\"relu\")(layer)\n# layer = Dropout(0.1)(layer)\n# layer = Dense(6, activation=\"sigmoid\")(layer)\n# model = Model(inputs = inp, outputs = layer)\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# batch_size = 32\n# epochs = 2\n# model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","3c4ce499":"def model():\n    embed_size = 128\n    inp = Input(shape=(max_len, ))\n    layer = Embedding(num_words_count, embed_size)(inp)\n    layer = LSTM(60, return_sequences=True,name='lstm_layer')(layer)\n#     layer = Bidirectional(LSTM(50, return_sequences = True, recurrent_dropout = 0.15))(layer)\n    layer = GlobalMaxPool1D()(layer)\n    layer = Dropout(0.1)(layer)\n    layer = Dense(50, activation=\"relu\")(layer)\n    layer = Dropout(0.1)(layer)\n    layer = Dense(6, activation=\"sigmoid\")(layer)\n    model = Model(inputs = inp, outputs = layer)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","39836534":"model = model()\nmodel.summary()","ef92320b":"file_path = 'save_analysis'\ncheckpoint = ModelCheckpoint(file_path, monitor = 'val_loss', verbose = 1, save_best_only=True)\n\nearly_stop = EarlyStopping(monitor = 'val_loss', patience = 1)","82c77ace":"hist = model.fit(X_train, y, batch_size = 32, epochs = 2, verbose=1, \n                 validation_split = 0.2, callbacks = [checkpoint, early_stop]);","9723eebc":"vloss = hist.history['val_loss']\nloss = hist.history['loss']\n\n#x_len = np.arange(len(loss))\n#plt.plot(x_len, vloss, marker='.', lw=2.0, c='red', label='val')\n#plt.plot(x_len, loss, marker='.', lw=2.0, c='blue', label='train')\nplt.figure()\nplt.plot(loss, marker='.', lw=2.0, c='blue', label='train')\nplt.plot(vloss, marker='.', lw=2.0, c='red', label='val')\n\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.grid()\nplt.show()","3f662a32":"plt.figure()\nplt.plot(hist.history['val_acc'], marker='.', c='blue', label='train')\nplt.plot(hist.history['acc'], marker='.', c='red', label='val')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.grid()\nplt.show()","cbc8b51a":"y_pred = model.predict(X_test)","91808157":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nlist_classes=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] \nsubmission[list_classes] = y_pred\n\nsubmission.to_csv(\"submission.csv\", index=False)","5bddecbc":"submission.head()","11e9be71":"The approach that we are taking is to feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.\n\nSo this is what we are going to do:\n\n1.     Tokenization - break down the sentence into words.\n1.     Indexing - add token into dictionary and index them For eg, {1:\"Natural\",2:\"language\",3:\"understading\",4:\"with\",5:\"deep_learning\"}\n1.     Index Representation- represent the sequence of tokens\/words in the comments in the form of index, and feed this chain of index into our Long short-term memory (LSTM) . For eg, [1,2,3,4,2,5]","524dcd98":"Embedding layer, project the words to a defined vector space depending on the distance of the surrounding words in a sentence. <br> \nEmbedding allows us to reduce model size and most importantly the huge dimensions we have to deal with.","c55d270e":"**Exploratory data analyis**","13b4f1ca":"**Comments length**"}}