{"cell_type":{"156a5232":"code","6a836a3e":"markdown","c5e7665c":"markdown","640005d2":"markdown","3342dc6b":"markdown","dfcb9365":"markdown"},"source":{"156a5232":"print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","6a836a3e":"**Why evaluate topic models?**","c5e7665c":"But, **WHAT IS PERPLEXITY?**","640005d2":"It isimportant to identify if a trained model is objectively good or bad, as well have an ability to compare different models\/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if \u201cthe correct thing\u201d has been learned about the corpus, an implicit knowledge and \u201ceyeballing\u201d approaches are used. Ideally, we\u2019d like to capture this information in a single metric that can be maximized, and compared.","3342dc6b":"Perplexity is a measurement of how well a probability model predicts a test data. In the context of Natural Language Processing, perplexity is one way to evaluate language models. Perplexity is just an exponentiation of the entropy!","dfcb9365":"THIS IS TO EVALUATE THE LDA ACCURACY- it was etablished when performing an LDA analysis based on this notebook https:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers\/comments. In the end i am not using it,that is why this notebook has no imports neither database but I leave it here if someone would like to use it for their own work. It's really an extra-miny code. \n"}}