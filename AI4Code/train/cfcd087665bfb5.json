{"cell_type":{"27c72f34":"code","a273fb21":"code","9a69bc55":"code","5b620ef9":"code","578c8dac":"code","2746755b":"code","51c47efc":"code","2bb208cb":"code","e4328593":"code","dff2154f":"code","b28d206d":"code","35e1c060":"code","193cc9dc":"code","fb28b03a":"code","9ceadd5d":"code","e572a340":"code","ba82eba2":"code","0f9291df":"code","d066645e":"code","567d5a98":"code","0f365761":"code","d7f4b815":"code","8bd0513d":"code","9ac450e4":"code","71bde116":"code","a16705f5":"code","f718c19b":"code","75d19b1d":"code","78954814":"code","c2843520":"code","f55d8976":"code","b5e4990c":"code","8fa4c94c":"code","3cc85555":"code","dfc36af0":"code","e804d076":"code","de8eff19":"code","7fed77da":"code","02e22b58":"code","087d42cd":"code","5a58b7bb":"code","3bd7cd59":"code","62254eea":"code","ce37c257":"code","1d1a0af0":"code","c2a67c77":"code","02c906d1":"code","a08d14ee":"code","eea84772":"code","cbb49a9b":"markdown","03f952fd":"markdown","fa094d2a":"markdown","1234fce2":"markdown","bc4be8be":"markdown"},"source":{"27c72f34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a273fb21":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVR\nfrom sklearn import *\nimport datetime as dt","9a69bc55":"#### Import Dependencies\n%matplotlib inline\n#### Start Python Imports\nimport math, time, random, datetime\n#### Data Manipulation\nimport numpy as np\nimport pandas as pd\n#### Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n#### Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n#### Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n##### Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","5b620ef9":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\nprint (\"Data is loaded!\")","578c8dac":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred) ** 0.5","2746755b":"data = train.copy()\nvalid = test.copy()","51c47efc":"#data.nunique()\n#valid.nunique()\n\n# in case needs","2bb208cb":"# get a list of object cat columns \n# Get list of categorical variables\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","e4328593":"len (object_cols)\n","dff2154f":"OH_col = data.loc[:, data.nunique() < 15].columns\n\nnew_OH = []\nfor x in OH_col:\n    if x in object_cols:\n        new_OH.append(x)\n        \n#new_OH","b28d206d":"LE_col = data.loc[:, data.nunique() >= 15].columns\nnew_LE = []\nfor x in LE_col:\n    if x in object_cols:\n        new_LE.append(x)\n\n#new_LE","35e1c060":"# Make copy to avoid changing original data \nlabel_X_train = data.copy()\nlabel_X_valid = valid.copy()","193cc9dc":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in new_LE:\n    label_X_train[col] = label_encoder.fit_transform(data[col])\n    label_X_valid[col] = label_encoder.fit_transform(valid[col])\n","fb28b03a":"print(label_X_train.shape)\nprint(label_X_valid.shape)","9ceadd5d":"label_X_train.head(2)","e572a340":"label_X_valid.head(2)","ba82eba2":"# use label_X_train and label_X_valid for next calculations ( One hot encoding )","0f9291df":"#label_X_train[new_OH].nunique()","d066645e":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(label_X_train[new_OH]))\nOH_cols_valid = pd.DataFrame(OH_encoder.fit_transform(label_X_valid[new_OH]))\n##  check if fit_transform or just transform should be used.... for valid data set....\n","567d5a98":"print(OH_cols_train.shape)\nprint(OH_cols_valid.shape)","0f365761":"label_X_train[new_OH].nunique().sum()\n# means OH_cols_train has no data of rest of columns....\n# so now add the data back","d7f4b815":"# One-hot encoding removed index; put it back\nOH_cols_train.index = label_X_train.index\nOH_cols_valid.index = label_X_valid.index","8bd0513d":"# Remove categorical columns (will replace with one-hot encoding)\n# these are columns which has numerical data and lebel encoding columns that's been processed already.\nnum_X_train = label_X_train.drop(new_OH, axis=1)\nnum_X_valid = label_X_valid.drop(new_OH, axis=1)","9ac450e4":"#num_X_train.head(2)","71bde116":"#num_X_valid.head(2)","a16705f5":"# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","f718c19b":"#OH_X_train.head(2)","75d19b1d":"#OH_X_valid.head(2)","78954814":"print(OH_X_train.shape)\nprint(OH_X_valid.shape)","c2843520":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.testing import ignore_warnings","f55d8976":"rf = RandomForestClassifier( \n                             n_estimators=200,\n                             n_jobs=-1,\n                             verbose = 2)\n#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n\nlr1 = LogisticRegression(solver='lbfgs', C=0.1)","b5e4990c":"X_train = OH_X_train.drop(\"target\", axis = 1)\ny_train = OH_X_train[\"target\"]\nX_train = X_train.drop(\"id\", axis=1)\nX_test = OH_X_valid.drop(\"id\",axis = 1)","8fa4c94c":"#scaler = MinMaxScaler(feature_range=(0, 1))\n#X_train = scaler.fit_transform(X_train)\n#X_test = scaler.fit_transform(X_test)","3cc85555":"print(X_train.shape)\nprint(y_train.shape)","dfc36af0":"#rf.fit(X_train, y_train)\n#lr1.fit(X_train, y_train)","e804d076":"# alternate cv method\nX, X_hideout, y, y_hideout = model_selection.train_test_split(X_train, y_train, test_size=0.13, random_state=42)","de8eff19":"# Set up folds\nK = 4\nkf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\nnp.random.seed(1)","7fed77da":"#model = SVR(kernel='rbf')\nparams = {'n_estimators': 10, # change to 9000 to obtain 0.505 on LB (longer run time expected)\n        'max_depth': 5,\n        'min_samples_split': 200,\n        'min_samples_leaf': 50,\n        'learning_rate': 0.005,\n        'max_features':  'sqrt',\n        'subsample': 0.8,\n        'loss': 'ls'}\n#model = ensemble.GradientBoostingRegressor(**params)\nmodel = ensemble.RandomForestClassifier(n_jobs = -1, verbose = 2)","02e22b58":"print(\"Started CV at \", dt.datetime.now())\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    # Create data for this fold\n    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n    X_train, X_valid = X.iloc[train_index, :].copy(), X.iloc[test_index, :].copy()\n    #X_test = test[col]\n    print(\"\\nFold \", i)\n    \n    fit_model = model.fit(X_train, y_train)\n    pred = model.predict(X_valid)\n    print('RMSLE GBM Regressor, validation set, fold ', i, ': ', RMSLE(y_valid, pred))\n    \n    pred_hideout = model.predict(X_hideout)\n    print('RMSLE GBM Regressor, hideout set, fold ', i, ': ', RMSLE(y_hideout, pred_hideout))\n    print('Prediction length on validation set, GBM Regressor, fold ', i, ': ', len(pred))\n    # Accumulate test set predictions\n    \n    del  X_train, X_valid, y_train\n    \nprint(\"Finished CV at \", dt.datetime.now())","087d42cd":"# scores = []\n# best_svr = SVR(kernel='rbf')\n# #random_state=42, shuffle=False\n# cv = KFold(n_splits=10)\n# for train_index, test_index in cv.split(X_train):\n#     print(\"Train Index: \", train_index, \"\\n\")\n#     print(\"Test Index: \", test_index)\n\n#     X_tr = X_train.iloc[train_index,:]\n#     X_tes = X_train.iloc[test_index,:]\n#     y_tr = y_train.iloc[train_index]\n#     y_tes = y_train.iloc[test_index]\n#     print(X_tr.shape)\n#     print(X_tes.shape)\n#     print(y_tr.shape)\n#     print(y_tes.shape)\n    \n    \n#     #best_svr.fit(X_tr, y_tr)\n#     #scores.append(best_svr.score(X_tes, y_tes))","5a58b7bb":"#X_train.iloc[[1,3],:]\n#y_train.iloc[30000]","3bd7cd59":"X_test.head(2)","62254eea":"# predictions = rf.predict(X_test)\n# predict_lr = lr1.predict_proba(X_test)\n# prediction_svr = best_svr.predict(X_test)\n\n# submission = pd.DataFrame()\n# submission_LR = pd.DataFrame()\n# submission_svr = pd.DataFrame()\n\n# submission[\"id\"] = OH_X_valid[\"id\"]\n# submission_LR[\"id\"] = OH_X_valid[\"id\"]\n# submission_svr['id'] = OH_X_valid[\"id\"]\n\n# submission[\"target\"] = predictions\n# submission_LR[\"target\"] = predict_lr[:, 1]\n# submission_svr[\"target\"] = prediction_svr","ce37c257":"prediction = model.predict(X_test)\nsubmission = pd.DataFrame()\nsubmission['id'] = OH_X_valid[\"id\"]\nsubmission[\"target\"] = prediction","1d1a0af0":"submission.to_csv(\"cat_submission1.csv\", index = False)","c2a67c77":"predict_lr[:, 1]","02c906d1":"submission.target.value_counts().sum()","a08d14ee":"submission.to_csv(\"cat_submission1.csv\", index = False)\nsubmission_LR.to_csv(\"cat_submission_lr.csv\", index = False)","eea84772":"from sklearn.model_selection import cross_validate\n\nscore=cross_validate(lr1, X_train, y_train, cv=3, scoring=\"roc_auc\")[\"test_score\"].mean()\nprint(f\"{score:.6f}\")","cbb49a9b":"### * One Hot encoding","03f952fd":"### Lebel encoding : inplace","fa094d2a":"We will seperate the object columns that should be one hot encoded (< 12 unique values) and columns that should be label encoded (rest of the object categorical columns)","1234fce2":"### 1] Try with all one hot encoding using get dummies once and see the improvements............                                                             2] Try with k - fold CV for training and cs score test ","bc4be8be":"> ### * ML Algo"}}