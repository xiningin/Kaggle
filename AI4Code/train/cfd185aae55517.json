{"cell_type":{"0d14de09":"code","2b0aab36":"code","de2a6d08":"code","1180607e":"code","22434098":"code","783be39e":"code","c56d4bf7":"code","1321b3cb":"code","c0892126":"code","4e80f959":"code","530f7afe":"code","387c6bf1":"markdown"},"source":{"0d14de09":"\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2b0aab36":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\n\nfrom dateutil.relativedelta import relativedelta","de2a6d08":"\n## 2. Adding some datetime related features\n\ndef add_datetime_features(df):\n    features = [\"Year\", \"Week\", \"Day\", \"Dayofyear\", \"Month\", \"Dayofweek\",\n                \"Is_year_start\",\"Is_month_start\",\n                \"Hour\", \"Minute\"]\n    #features = [\"Year\", \"Week\", \"Dayofyear\",\"Dayofweek\",\n                #\"Is_year_end\", \"Is_year_start\",\n                #\"Hour\"]\n    one_hot_features = [\"Month\", \"Dayofweek\"]\n    one_hot_features=[]\n    \n    datetime = pd.to_datetime(df.Date * (10 ** 9))\n\n    df['Datetime'] = datetime  # We won't use this for training, but we'll remove it later\n\n    for feature in features:\n        new_column = getattr(datetime.dt, feature.lower())\n        if feature in one_hot_features:\n            df = pd.concat([df, pd.get_dummies(new_column, prefix=feature)], axis=1)\n        else:\n            df[feature] = new_column\n    \n  \n    return df","1180607e":"\n\ndf = pd.read_csv(\"..\/input\/train_electricity.csv\")\n\nprint(\"Dataset has\", len(df), \"entries.\")\n\nprint(f\"\\n\\t{'Column':20s} | {'Type':8s} | {'Min':12s} | {'Max':12s}\\n\")\nfor col_name in df.columns:\n    col = df[col_name]\n    print(f\"\\t{col_name:20s} | {str(col.dtype):8s} | {col.min():12.1f} | {col.max():12.1f}\")\n    \n    ","22434098":"test_df = pd.read_csv(\"..\/input\/test_electricity.csv\")\ntest_df = add_datetime_features(test_df)\nprint(test_df.columns)\n\n\ndf = add_datetime_features(df)\n\n\n\n#remove outlier\ndf = df[ (df['Consumption_MW']>1000)  & (df['Consumption_MW']<15000)]","783be39e":"\n\n## 3. Split data into train \/ validation (leaving the last six months for validation)\n\n\nmonth_t1=0\nmonth_t2=0\nmonth_start=0\nthreshold_1 = df['Datetime'].max() + relativedelta(months=-month_t1)  # Here we set the 6 months threshold\nthreshold_2 = df['Datetime'].max() + relativedelta(months=-month_t2)  # Here we set the 6 months threshold\nthreshold_0 = df['Datetime'].min() + relativedelta(months=month_start)  # Here we set the 6 months threshold\n\n#threshold_3 = df['Datetime'].min() + relativedelta(months=month_start)  # Here we set the 6 months threshold\n\n#train_df = df[ ( df['Datetime'] < threshold_1 ) |  (( df['Datetime'] < threshold_3 ) & (df['Datetime'] > threshold_2) )]\n\ntrain_df = df[ ( ( df['Datetime'] > threshold_0) & ( df['Datetime'] < threshold_1)) | (df['Datetime'] > threshold_2 ) ]\n\nvalid_df = df[(df['Datetime'] >= threshold_1) & (df['Datetime'] <= threshold_2)]\n\n\n#train_df, valid_df = train_test_split(df, test_size=0.1)\n\n#print(f\"Train data: {train_df['Datetime'].min()} -> {train_df['Datetime'].max()} | {len(train_df)} samples.\")\n#print(f\"Valid data: {valid_df['Datetime'].min()} -> {valid_df['Datetime'].max()} | {len(valid_df)} samples.\")\n\nlabel_col = \"Consumption_MW\"  # The target values are in this column\n#to_drop = [label_col, \"Date\", \"Datetime\"]  # Columns we do not need for training\nto_drop = [\"Date\", \"Datetime\",\"Dayofyear\"]  # Columns we do not need for training\n\n\nto_drop_train=[label_col]+to_drop\n\n\n\nprint(f\"Train data: {train_df['Datetime'].min()} -> {train_df['Datetime'].max()} | {len(train_df)} samples.\")\nprint(f\"Valid data: {valid_df['Datetime'].min()} -> {valid_df['Datetime'].max()} | {len(valid_df)} samples.\")\n","c56d4bf7":"\nmodel = xgb.XGBRegressor(n_estimators=500,min_child_weight=1.77, max_depth=5,gamma=9.107,learning_rate=0.2509,\n subsample=0.6383, colsample_bylevel=0.7685, reg_lambda=10,n_jobs=-1, random_state=14)# rand 435\n","1321b3cb":"\nbst = model.fit(\n    train_df.drop(to_drop_train, axis=1),train_df[label_col],\n    eval_set=[(valid_df.drop(to_drop_train, axis=1), valid_df[label_col])],eval_metric='rmse'\n    #,early_stopping_rounds=30\n)\n\npred_score = bst.predict(valid_df.drop(to_drop_train, axis=1))\nrms=np.sqrt(np.mean((pred_score-valid_df[label_col].values)**2))","c0892126":"\nfig,ax=plt.subplots(figsize=(5,15))\nxgb.plot_importance(bst,ax=ax)","4e80f959":"\nypredicted = bst.predict(test_df.drop(to_drop, axis=1))\n\nplt.plot(ypredicted)","530f7afe":"ratio=1.01*1.005\nyfinal=ypredicted*ratio\npred = pd.Series(yfinal, test_df.Date).rename('Consumption_MW').to_frame()\npred.to_csv('final_prediction.csv')","387c6bf1":"This kernel is for prediction task."}}