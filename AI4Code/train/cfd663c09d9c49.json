{"cell_type":{"6ddbca3f":"code","6047a00c":"code","aa61f935":"code","763f797b":"code","89e00450":"code","7257f811":"code","aab444a2":"code","2cf4d4f4":"code","cc83c523":"code","a46c9914":"code","e771db84":"code","56eb4605":"code","a720a0fd":"code","ce9bbfda":"code","32d9d800":"code","bbc89d86":"code","06d0ff2f":"code","c17f99bd":"code","d270e81d":"code","fa397320":"code","d8d39fcf":"code","96715fb0":"code","5c46518d":"code","463c620f":"code","55840a46":"code","d7d1bcdf":"code","1d698c10":"code","546f3b61":"code","5d23662c":"code","d155d047":"code","96e17bbc":"code","b4628904":"code","136b20d5":"code","94397224":"code","fe07a7e5":"code","e806f4c4":"code","167a340c":"markdown","fec8ed67":"markdown","d15cb4c7":"markdown","624180ef":"markdown","a57a4618":"markdown","acd0a522":"markdown","d13e7985":"markdown","b013aec8":"markdown","f840643f":"markdown","961d3010":"markdown","58807a7f":"markdown","955b9678":"markdown","20b9a31a":"markdown","8a0eb9e6":"markdown","0afdb62f":"markdown","3c0aebb2":"markdown","c3862418":"markdown","dbfdc0b7":"markdown","69e3adc2":"markdown","7f75c040":"markdown","0bcc5008":"markdown","bda641de":"markdown","6a7693e5":"markdown","6f174ad6":"markdown","dede4808":"markdown","d4eacddd":"markdown","48e6421d":"markdown","66e46330":"markdown","0490b368":"markdown","3cd87516":"markdown","fb5b3c29":"markdown","b8cd0103":"markdown"},"source":{"6ddbca3f":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.optimizers import *\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import *\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport time\nimport gc\nimport re\nimport glob","6047a00c":"train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')","aa61f935":"train.info()","763f797b":"test.info()","89e00450":"ax, fig = plt.subplots(figsize=(10, 7))\nquestion_class = train[\"target\"].value_counts()\nquestion_class.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Bar chart')\nplt.show()","7257f811":"print(\"T\u1ec9 l\u1ec7 ph\u1ea7n tr\u0103m s\u1ed1 c\u00e2u h\u1ecfi Insincere l\u00e0:\", (len(train.loc[train.target==1])) \/ (len(train.loc[train.target == 0])) * 100)","aab444a2":"words = train['question_text'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain['words'] = words\nwords = train.loc[train['words']<200]['words']\nsns.distplot(words, color='g')\nplt.show()","2cf4d4f4":"print('S\u1ed1 l\u01b0\u1ee3ng t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n l\u00e0 {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 l\u01b0\u1ee3ng t\u1eeb trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u ki\u1ec3m th\u1eed l\u00e0 {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x.split())))))","cc83c523":"print('S\u1ed1 l\u01b0\u1ee3ng t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n l\u00e0 {0:.0f}.'.format(np.max(train['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 l\u01b0\u1ee3ng t\u1eeb l\u1edbn nh\u1ea5t c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u ki\u1ec3m th\u1eed l\u00e0 {0:.0f}.'.format(np.max(test['question_text'].apply(lambda x: len(x.split())))))","a46c9914":"print('S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n l\u00e0 {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x)))))\nprint('S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong d\u1eef li\u1ec7u ki\u1ec3m th\u1eed l\u00e0 {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x)))))","e771db84":"embed_size = 300 #\u0111\u1ed9 d\u00e0i c\u1ee7a vector\nmax_features = 120000 #s\u1ed1 l\u01b0\u1ee3ng t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t \u0111\u1ec3 hu\u1ea5n luy\u1ec7n\nmaxlen = 80 #s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u ","56eb4605":"train[\"question_text\"] = train[\"question_text\"].apply(lambda x: x.lower())\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: x.lower())","a720a0fd":"def clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x","ce9bbfda":"train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))","32d9d800":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x","bbc89d86":"train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))","06d0ff2f":"mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","c17f99bd":"train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))","d270e81d":"# split to train and val\ntrain, val = train_test_split(train, test_size=0.1, random_state=2018) ","fa397320":"# fill up the missing values\ntrain_X = train[\"question_text\"].fillna(\"_na_\").values\nval_X = val[\"question_text\"].fillna(\"_na_\").values\ntest_X = test[\"question_text\"].fillna(\"_##_\").values","d8d39fcf":"# Get the target values\ntrain_y = train['target'].values\nval_y = val['target'].values ","96715fb0":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X)) # g\u00e1n ch\u1ec9 s\u1ed1 cho t\u1eebng t\u1eeb\nword_index = tokenizer.word_index # l\u1ea5y ch\u1ec9 s\u1ed1 c\u1ee7a c\u00e1c t\u1eeb\ntrain_X = tokenizer.texts_to_sequences(train_X) # thay th\u1ebf c\u00e1c t\u1eeb b\u1edfi ch\u1ec9 s\u1ed1 t\u01b0\u01a1ng \u1ee9ng c\u1ee7a t\u1eeb \u0111\u00f3\nval_X = tokenizer.texts_to_sequences(val_X)  # thay th\u1ebf c\u00e1c t\u1eeb b\u1edfi ch\u1ec9 s\u1ed1 t\u01b0\u01a1ng \u1ee9ng c\u1ee7a t\u1eeb \u0111\u00f3\ntest_X = tokenizer.texts_to_sequences(test_X) # thay th\u1ebf c\u00e1c t\u1eeb b\u1edfi ch\u1ec9 s\u1ed1 t\u01b0\u01a1ng \u1ee9ng c\u1ee7a t\u1eeb \u0111\u00f3","5c46518d":"train_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)","463c620f":"#shuffling the data\nnp.random.seed(1203)\ntrn_idx = np.random.permutation(len(train_X))\nval_idx = np.random.permutation(len(val_X))\n\ntrain_X = train_X[trn_idx]\nval_X = val_X[val_idx]\ntrain_y = train_y[trn_idx]\nval_y = val_y[val_idx]   ","55840a46":"! unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip -d input\/","d7d1bcdf":"EMBEDDING_FILE = '.\/input\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE)) # l\u1ea5y h\u1ec7 s\u1ed1 c\u1ee7a t\u1eebng t\u1eeb trong t\u1eadp embedding\n\nall_embs = np.stack(embeddings_index.values()) \nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\n# word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) # t\u1ea1o h\u1ec7 s\u1ed1 ma tr\u1eadn ng\u1eabu nhi\u00ean \nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word) \n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector # thay c\u00e1c h\u1ec7 s\u1ed1 c\u1ee7a nh\u1eefng t\u1eeb c\u00f3 trong t\u1eadp embedding","1d698c10":"!pip install tensorflow-addons","546f3b61":"f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\naccuracy = tf.keras.metrics.BinaryAccuracy(\n    name='binary_accuracy', dtype=None, threshold=0.5\n)","5d23662c":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001, verbose=0)\nearlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto', restore_best_weights=True)\ncallbacks = [reduce_lr, earlystopping]","d155d047":"def model_lstm_atten(embedding_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    return model","96e17bbc":"model = model_lstm_atten(embedding_matrix)\nmodel.summary()","b4628904":"model.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y), callbacks=callbacks)","136b20d5":"pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\npred_test_y = model.predict([test_X], batch_size=1024, verbose=0)","94397224":"def f1_smart(y_true, y_pred):\n    thresholds = []\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        res = f1_score(y_true, (y_pred > thresh).astype(int))\n        thresholds.append([thresh, res])\n        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n\n    thresholds.sort(key=lambda x: x[1], reverse=True)\n    best_thresh = thresholds[0][0]\n    best_f1 = thresholds[0][1]\n    print(\"Best threshold: \", best_thresh)\n    return  best_f1, best_thresh","fe07a7e5":"f1, threshold = f1_smart(val_y, pred_val_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, threshold))","e806f4c4":"pred_test_y = (pred_test_y >threshold).astype(int)\nout_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","167a340c":"Nh\u01b0 \u0111\u00e3 ph\u00e2n t\u00edch v\u1ec1 d\u1eef li\u1ec7u \u1edf tr\u00ean, ta s\u1ebd quy chu\u1ea9n s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong m\u1ed9t c\u00e2u h\u1ecfi. Do ta s\u1eed d\u1ee5ng t\u1eadp t\u1eeb \u0111i\u1ec3n \u0111\u00e3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n s\u1eb5n l\u00e0 t\u1eadp embeddings n\u00ean m\u1ed7i vector c\u00f3 \u0111\u1ed9 d\u00e0i l\u00e0 300, v\u00e0 m\u1ed7i c\u00e2u h\u1ecfi s\u1ebd c\u00f3 t\u1ed1i \u0111a 80 t\u1eeb","fec8ed67":"**Lowering**","d15cb4c7":"**Xo\u00e1 c\u00e1c s\u1ed1**","624180ef":"**Xo\u00e1 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t**","a57a4618":"#### Thi\u1ebft l\u1eadp model","acd0a522":"***Nh\u1eadn x\u00e9t***: D\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 ki\u1ec3m th\u1eed kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb null","d13e7985":"**Tr\u00e1o d\u1eef li\u1ec7u**","b013aec8":"\u1ede B\u00e0i to\u00e1n n\u00e0y, \u0111\u1ec3 t\u0103ng hi\u1ec7u qu\u1ea3 ph\u00e2n l\u1edbp, ta s\u1ebd s\u1eed d\u1ee5ng t\u1eeb \u0111i\u1ec3n ch\u1ee9a tr\u1ecdng s\u1ed1 c\u1ee7a c\u00e1c t\u1eeb \u0111\u00e3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n s\u1eb5n (Glove)","f840643f":"## HU\u1ea4N LUY\u1ec6N M\u00d4 H\u00ccNH","961d3010":"#### Ph\u00e2n t\u00edch t\u1eebng c\u00e2u h\u1ecfi","58807a7f":"#### Predict","955b9678":"#### Train model","20b9a31a":"## PH\u00c2N T\u00cdCH D\u1eee LI\u1ec6U\n","8a0eb9e6":"## SUBMISSION","0afdb62f":"***Nh\u1eadn x\u00e9t:*** C\u00f3 th\u1ec3 th\u1ea5y \u0111\u1ed9 d\u00e0i trung b\u00ecnh c\u1ee7a c\u00e1c c\u00e2u h\u1ecfi trong t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 ki\u1ec3m th\u1eed t\u01b0\u01a1ng t\u1ef1 nhau, tuy nhi\u00ean c\u00f3 nh\u1eefng c\u00e2u h\u1ecfi kh\u00e1 d\u00e0i trong t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n","3c0aebb2":"**Tokenize**\n\nTokenize c\u00e1c c\u00e2u h\u1ecfi th\u00e0nh c\u00e1c token(c\u00e1c t\u1eeb) sau \u0111\u00f3 g\u00e1n ch\u1ec9 s\u1ed1 cho t\u1eebng t\u1eeb v\u00e0 thay t\u1eebng t\u1eeb th\u00e0nh ch\u1ec9 s\u1ed1 t\u01b0\u01a1ng \u1ee9ng c\u1ee7a t\u1eeb \u0111\u00f3.","c3862418":"X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n b\u1eb1ng LSTM (Long Short-Term Memory Networks)\n\nTr\u01b0\u1edbc khi hi\u1ec3u m\u00f4 h\u00ecnh LSTM l\u00e0 g\u00ec th\u00ec ta c\u1ea7n hi\u1ec3u qua v\u1ec1 m\u1ea1ng RNN - recurrent neural network. RNN c\u00f3 th\u1ec3 mang th\u00f4ng tin t\u1eeb c\u00e1c layer tr\u01b0\u1edbc \u0111\u1ebfn layer sau, n\u00ean n\u00f3 c\u00f3 th\u1ec3 d\u00f9ng \u0111\u1ec3 x\u1eed l\u00fd th\u00f4ng tin d\u1ea1ng chu\u1ed7i. M\u1ed9t v\u00ed d\u1ee5 c\u1ee7a RNN trong b\u00e0i to\u00e1n d\u1ef1 \u0111o\u00e1n video, RNN c\u00f3 th\u1ec3 mang th\u00f4ng tin c\u1ee7a frame \u1ea3nh t\u1eeb state tr\u01b0\u1edbc t\u1edbi state sau, tuy nhi\u00ean state \u1edf tr\u01b0\u1edbc \u0111\u00f3 c\u00e0ng xa th\u00ec c\u00e0ng b\u1ecb vanishing gradient, ngh\u0129a l\u00e0 th\u00f4ng tin ch\u1ec9 mang \u0111\u01b0\u1ee3c qua m\u1ed9t l\u01b0\u1ee3ng state nh\u1ea5t \u0111\u1ecbnh hay n\u00f3i c\u00e1ch kh\u00e1c l\u00e0 model ch\u1ec9 h\u1ecdc \u0111\u01b0\u1ee3c t\u1eeb c\u00e1c state g\u1ea7n n\u00f3 - short term memory\n\nV\u00ec v\u1eady, m\u00f4 h\u00ecnh Long short term memory ra \u0111\u1eddi \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 khi ta c\u1ea7n c\u00e1c th\u00f4ng tin t\u1eeb state \u1edf tr\u01b0\u1edbc \u0111\u00f3 r\u1ea5t xa v\u00e0 tr\u00e1nh \u0111\u01b0\u1ee3c vanishing gradient. N\u00f3 v\u1eabn gi\u1eef t\u01b0 t\u01b0\u1edfng ch\u00ednh c\u1ee7a RNN l\u00e0 s\u1ef1 sao ch\u00e9p ki\u1ebfn tr\u00fac theo d\u1ea1ng chu\u1ed7i nh\u01b0ng c\u00f3 ph\u1ea7n ph\u1ee9c t\u1ea1p h\u01a1n.","dbfdc0b7":"**Pad chu\u1ed7i** - n\u1ebfu s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u h\u1ecfi l\u1edbn h\u01a1n 'max_len' th\u00ec chuy\u1ec3n th\u00e0nh 'max_len' ho\u1eb7c n\u1ebfu s\u1ed1 t\u1eeb trong v\u0103n b\u1ea3n \u00edt h\u01a1n 'max_len' th\u00ec b\u1ed5 sung th\u00eam s\u1ed1 0 v\u00e0o c\u00e1c gi\u00e1 tr\u1ecb c\u00f2n l\u1ea1i","69e3adc2":"Nh\u1eadn x\u00e9t v\u1ec1 d\u1eef li\u1ec7u\nD\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 tr\u00ean ta c\u00f3 th\u1ec3 th\u1ea5y, s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi g\u00e1n nh\u00e3n ch\u00e2n th\u00e0nh chi\u1ebfm ph\u1ea7n l\u1edbn v\u1edbi t\u1ef7 l\u1ec7 93.4% c\u00f2n nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh ch\u1ec9 chi\u1ebfm 6.6%\n\n=> T\u1eadp d\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng, v\u00ec th\u1ebf n\u00ean khi \u0111\u00e1nh gi\u00e1 ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh ta kh\u00f4ng n\u00ean l\u1ef1a ch\u1ecdn \u0111\u1ed9 ch\u00ednh x\u00e1c(accuracy) l\u00e0m ch\u1ec9 s\u1ed1 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh. Thay v\u00e0o \u0111\u00f3 ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c metric thay th\u1ebf nh\u01b0: F1_score, Recall,..","7f75c040":"## M\u00d4 T\u1ea2 B\u00c0I TO\u00c1N\nQuora l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean Quora, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c, nh\u1eefng ng\u01b0\u1eddi \u0111\u00f3ng g\u00f3p th\u00f4ng tin chi ti\u1ebft \u0111\u1ed9c \u0111\u00e1o v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng. \n\nM\u1ed9t th\u00e1ch th\u1ee9c quan tr\u1ecdng l\u00e0 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch.\n\nTrong cu\u1ed9c thi n\u00e0y, ch\u00fang ta s\u1ebd ph\u00e1t tri\u1ec3n c\u00e1c m\u00f4 h\u00ecnh x\u00e1c \u0111\u1ecbnh v\u00e0 g\u1eafn c\u1edd cho c\u00e1c c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh.\n\n* Input: C\u00e1c c\u00e2u h\u1ecfi tr\u00ean Quora d\u01b0\u1edbi d\u1ea1ng text\n* Output: gi\u00e1 tr\u1ecb 0 ho\u1eb7c 1 (0: c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh; 1: c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh)","0bcc5008":"**Mispelling**","bda641de":"## B\u00c1O C\u00c1O B\u00c0I T\u1eacP L\u1edaN H\u1eccC M\u00c1Y CU\u1ed0I K\u1ef2\n\nM\u00e3 L\u1edbp: INT3405_1\n\nH\u1ecd v\u00e0 t\u00ean: Nguy\u1ec5n Tr\u1ecdng H\u1ea3i\n\nM\u00e3 s\u1ed1 sinh vi\u00ean: 18020447","6a7693e5":"#### \u0110\u1ecdc d\u1eef li\u1ec7u","6f174ad6":"#### \u0110\u1ecbnh ngh\u0129a h\u00e0m Learning rate v\u00e0 Early stopping \u0111\u1ec3 gi\u00fap h\u00e0m loss h\u1ed9i t\u1ee5 hi\u1ec7u qu\u1ea3 h\u01a1n","dede4808":"### X\u1eed L\u00dd D\u1eee LI\u1ec6U","d4eacddd":"#### \u0110\u1ecbnh ngh\u0129a F1-score\n\nDo d\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng n\u00ean metric ph\u00f9 h\u1ee3p nh\u1ea5t l\u00e0 F1 score","48e6421d":"**Chia d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n v\u1edbi t\u1ec9 l\u1ec7 90:10**","66e46330":"#### C\u00c1C B\u01af\u1edaC TI\u1ec0N X\u1eec L\u00dd D\u1eee LI\u1ec6U:\n\n* Lowering\n* Xo\u00e1 c\u00e1c s\u1ed1 (do c\u00e1c s\u1ed1 kh\u00f4ng c\u00f3 \u00fd ngh\u0129a trong vi\u1ec7c ph\u00e2n l\u1edbp)\n* Xo\u00e1 c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n* Mispelling (thay th\u1ebf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft)","0490b368":"Ta s\u1ebd t\u1ea1o ma tr\u1eadn embedding ch\u1ec9 ch\u1ee9a nh\u1eefng t\u1eeb trong ma tr\u1eadn word2vec m\u00e0 c\u00f3 trong word_index","3cd87516":"**S\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u**","fb5b3c29":"#### Import th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft","b8cd0103":"#### X\u00e1c \u0111\u1ecbnh threshold c\u00f3 F1 cao nh\u1ea5t v\u1edbi threshold trong kho\u1ea3ng t\u1eeb 0.10 \u0111\u1ebfn 0.50"}}