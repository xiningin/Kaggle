{"cell_type":{"7ca2ab8e":"code","1dabf27d":"code","70243ba0":"code","90ec80fe":"code","e313245a":"code","173bbe24":"code","3a5c9047":"code","5f6aee7a":"code","b80fc4f3":"code","f6f08d3c":"code","92470420":"code","2fcd376c":"code","08d1a72a":"code","9ce686df":"code","5830d248":"code","b18c0e37":"code","cb2dc071":"code","3ab8f0d3":"code","7d574b01":"code","4c4dd491":"code","2c568f28":"code","80bc4577":"code","dd4c7390":"code","fed81d96":"code","ccc3ef0e":"code","430a36a9":"code","c4a927c1":"code","3eac1f27":"code","169eabc4":"code","ad18c8b6":"code","f136096f":"code","f90cdf66":"code","f5a06e7b":"code","a3c406d7":"code","b74cb82a":"code","17b31565":"code","a9c6c3d7":"code","647c5c21":"code","9c076a76":"code","9ed5707f":"code","3a7f88f5":"markdown","7f2122ad":"markdown","e9ebf9cc":"markdown","900d5aae":"markdown","fdcfa667":"markdown","4b83d780":"markdown","c0314701":"markdown","cf36c87a":"markdown","f9d078e1":"markdown","6a9d2c1e":"markdown","c933b8b4":"markdown","7e9fb293":"markdown","a688bd6b":"markdown","553af2ba":"markdown","ed8fc1e5":"markdown","5d41de17":"markdown","91e5832b":"markdown","673aa34f":"markdown","510853d0":"markdown","7d063e30":"markdown","62775821":"markdown","bf7dd93f":"markdown","f09838d0":"markdown","4cfe0c06":"markdown","47ed84a6":"markdown","5904042c":"markdown","20c5f603":"markdown","0b3ca148":"markdown","4cb43bfb":"markdown","d801b190":"markdown","236a965b":"markdown","8431a63e":"markdown","537224a6":"markdown","7aa859b7":"markdown","022ef658":"markdown","a1e16b15":"markdown","dd2b487b":"markdown"},"source":{"7ca2ab8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1dabf27d":"import scipy.stats as scipyStats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as mpl\nfrom sklearn.preprocessing import StandardScaler, RobustScaler   #MODULE FOR SCALING DATA\nimport seaborn as sb\nimport imblearn\n##IMPORTING ALL THE LIBRARIES WE WILL NEED FOR THIS PROJECT","70243ba0":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\") ## TO READ THE CSV FILE creditcard.csv WHICH CONTAINS OUR DATA","90ec80fe":"df.head()","e313245a":"df.info()","173bbe24":"df.isnull().sum() ## TO CHECK THE TOTAL NUMBER OF NULL VALUES IN EVERY COLUMN. OUTPUT SAYS NO NULL VALUES","3a5c9047":"class_values = df[\"Class\"].value_counts()\ncount_0 = class_values[0] ## STORE THE COUNT OF 0 IN THE VARIABLE\ncount_1 = class_values[1] ## STORE THE COUNT OF 1 IN THE VARIABLE\ncount_0_percent = ((count_0)\/(count_0+count_1))*100 ##CALCULATE THE PERCENTAGES OF THE 0's AND 1's IN THE TOTAL DATA SET\ncount_1_percent = ((count_1)\/(count_0+count_1))*100\nclass_values","5f6aee7a":"fig = mpl.figure()\nax = fig.add_axes([0,0,1,1])\nClass_labels = [\"Fraud\",\"Not Fraud\"]\npercentages = [count_1_percent,count_0_percent]\nax.bar(Class_labels,percentages)\nmpl.title(\"Percentage of Fraudulent vs Non Fraudulent Transactions\")\nmpl.show()","b80fc4f3":"amount_value = df[\"Amount\"].value_counts()\nmpl.style.use('ggplot')\nmpl.hist(df[\"Amount\"],bins=100)\nmpl.title(\"Distribution of Transactions Amounts\")\nsb.distplot(df[\"Amount\"])\nmpl.show()","f6f08d3c":"mpl.scatter(df[\"Class\"].values, df[\"Amount\"].values)\nmpl.title(\"Transaction Amounts vs Class\")\nmpl.xlabel(\"Class-Fruadulent\/Not Fraudulent\")\nmpl.ylabel(\"Transaction Amounts\")\nmpl.show","92470420":"mpl.style.use('ggplot')\nmpl.hist(df[\"Time\"],bins=100)\nmpl.title(\"Distribution of time of Transactions\")\nmpl.show()","2fcd376c":"df_without_time = df.iloc[:,1:31]  ##REMOVING THE TIME COLUMN BECAUSE DOESNOT GIVE ANY SIGNIFICANT INSIGHTS\ndf_without_time.head() ","08d1a72a":"df_corr = df.corr()\ndf_corr","9ce686df":"map = sb.heatmap(df_corr, linewidth = 1.0)\nmpl.title(\"Heat Map\")\nmpl.show()","5830d248":"scaler = RobustScaler().fit(df_without_time.iloc[:,:-1])\nscaler.transform(df_without_time.iloc[:,:-1])\ndf_without_time.head()","b18c0e37":"df_without_time[\"Class\"] = df_without_time[\"Class\"].astype(\"category\")\ndf_without_time[\"Class\"] = df_without_time[\"Class\"].cat.rename_categories({0:\"Not Fraud\",1:\"Fraud\"})\ndf_without_time[\"Class\"]","cb2dc071":"from sklearn.model_selection import train_test_split","3ab8f0d3":"Output_para = df_without_time.iloc[:,:30] ## SEPARATING OUTPUT AND INPUT PARAMETER COLUMNS\nInput_parameters = df_without_time.iloc[:,0:29]\nOutput_parameter = Output_para.iloc[:,-1]\nOutput_parameter.value_counts()","7d574b01":"Data_train, Data_test, Class_train, Class_test = train_test_split(Input_parameters,Output_parameter, test_size=0.3, random_state=100)","4c4dd491":"print(Class_train.value_counts())","2c568f28":"print(\"Test Data Size: \",Data_test.shape)\nprint(\"Training Data Size: \", Data_train.shape)\nprint(\"Training Data Output Data Size: \", Class_train.shape)\nprint(\"Testing Data Output Size: \", Class_train.shape)","80bc4577":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler","dd4c7390":"random_under = RandomUnderSampler(sampling_strategy = 'auto')\nData_under_sampled, Class_under_sampled = random_under.fit_resample(Data_train, Class_train)\nData_under_sampled.shape","fed81d96":"random_over = RandomOverSampler(sampling_strategy = 'auto')\nData_over_sampled, Class_over_sampled = random_over.fit_resample(Data_train, Class_train)\nClass_over_sampled.shape","ccc3ef0e":"from imblearn.over_sampling import SMOTE","430a36a9":"oversample = SMOTE()\nSmote_Data_train,Smote_class_train = oversample.fit_resample(Data_train,Class_train) \nSmote_Data_train.shape","c4a927c1":"from imblearn.under_sampling import NearMiss","3eac1f27":"sample = NearMiss()\nData_NM_train,Class_NM_train = sample.fit_resample(Data_train,Class_train)\nClass_NM_train.shape","169eabc4":"from sklearn.metrics import roc_curve,roc_auc_score\nfrom sklearn import tree ","ad18c8b6":"auc_values = []\ndef calc_auc(clf):\n    probs = clf.predict_proba(Data_test)\n    probs = probs[:,1]\n    auc_calc = round(roc_auc_score(Class_test, probs),2)\n    auc_values.append(auc_calc)\n    fpr, tpr, thresholds = roc_curve(Class_test,probs, pos_label='Not Fraud')\n    mpl.plot(fpr, tpr, color='red', label='ROC')\n    mpl.plot([0, 1], [0, 1], color='DarkBlue', linestyle='--')\n    mpl.title(\"Receiver Operating Characteristic (ROC) Curve\")\n    mpl.xlabel(\"False Positive Rate\")\n    mpl.ylabel(\"True Positive Rate\")\n    mpl.legend()\n    mpl.show()\n    return auc_calc","f136096f":"clf = tree.DecisionTreeClassifier()\n    #for original training dataset\nclf = clf.fit(Data_train, Class_train)\nprint(\"AUC is: \",calc_auc(clf))","f90cdf66":"clf = tree.DecisionTreeClassifier()\n\n#for original training dataset\nclf = clf.fit(Data_over_sampled, Class_over_sampled)\nauc = calc_auc(clf)\nprint(\"AUC is: \",auc)","f5a06e7b":"clf = tree.DecisionTreeClassifier()\n#for original training dataset\nclf = clf.fit(Data_under_sampled, Class_under_sampled)\nauc = calc_auc(clf)\nprint(\"AUC is: \",auc)","a3c406d7":"clf = tree.DecisionTreeClassifier()\n#for original training dataset\nclf = clf.fit(Smote_Data_train,Smote_class_train)\nauc = calc_auc(clf)\nprint(\"AUC is: \",auc)","b74cb82a":"clf = tree.DecisionTreeClassifier()\n#for original training dataset\nclf = clf.fit(Data_NM_train,Class_NM_train)\nauc = calc_auc(clf)\nprint(\"AUC is: \",auc)","17b31565":"from sklearn.linear_model import LogisticRegression #linear regression\nfrom sklearn.svm import SVC #svc\nfrom sklearn.neighbors import KNeighborsClassifier #knn\nfrom sklearn.ensemble import RandomForestClassifier #random forest\nfrom sklearn.metrics import classification_report \nimport xgboost as xgb #XGBoost\n\n##IMPORTING ALL THE ALGORITHMS","a9c6c3d7":"log_reg = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000, multi_class='ovr').fit(Data_under_sampled, Class_under_sampled)\nauc = calc_auc(log_reg)\nprint(\"AUC is: \",auc)","647c5c21":"random_forest = RandomForestClassifier().fit(Data_under_sampled,Class_under_sampled)\nauc = calc_auc(random_forest)\nprint(\"AUC is: \",auc)","9c076a76":"KNN_value = KNeighborsClassifier().fit(Data_under_sampled,Class_under_sampled)\nauc = calc_auc(KNN_value)\nprint(\"AUC is: \",auc)","9ed5707f":"xgb_Boost = xgb.XGBClassifier().fit(Data_under_sampled,Class_under_sampled)\nauc = calc_auc(xgb_Boost)\nprint(\"AUC is: \",auc)","3a7f88f5":"**SCALING OF DATA IN THE DATA FRAME**","7f2122ad":"**PLOTTING THE FRAUD AND NOT FRAUD BAR CHART TO EASE THE PROCESS OF DATA EXPLORATION**","e9ebf9cc":"**SCATTERPLOT OF TRANSACTIONS VS. CLASSES**\n\n\n","900d5aae":"Data sampling is the process of making the dataset unbiased. As we have seen in Data Exploration that the number of fraudulent transactions as very few as compared to the non-fraudulent transactions. If we use the same data set for the predictions, the model will predict all the transactions as non-fraudulent in the future, which can have adverse effect on the further processes. \n\nWe will sample the data in different ways and make it unbiased. To do so, we will be using the following algorithms:\n1. Random Sampler(UP-Sampling, Down-Sampling \n2. SMOTE\n3. NEAR-MISS","fdcfa667":"**Since the time column in the dataset plays no significant role in predicting whether the transaction is fraudulent or not, we remove it from the data set.**","4b83d780":"4. XG BOOST ","c0314701":"**SMOTE SAMPLING**","cf36c87a":"3. **FOR DOWN SAMPLED USING RANDOM SAMPLER**","f9d078e1":"# Which Algorithm performs the best!!\n\nUsing the right algorithm for predictions is very important. The accurate prediction of False Negatives and False Positives is a highly important factor in all fields, for example in the case of detection of a particular disease or in the detection of spam emails or in the detection of some documents as confidential or not confidential. Hence Machine Learning algorithms play a very important role.\n\nWe used total of four algorithms, the best performance is given by the algorithm getting the highest AUC. Therefore the accuracy of this algorithm in predicting whether a transaction is Fraudulent(True Positives) or Non Fraudulent(True Negatives) is the highest.\n","6a9d2c1e":"2. RANDOM FORESTS","c933b8b4":"# MACHINE LEARNING ALGORITHMS TO PREDICT","7e9fb293":"3. K NEAREST NIGHBOURS","a688bd6b":"RANDOM SAMPLER","553af2ba":"# IMPORTING THE DATA","ed8fc1e5":"# **DATA EXPLORATION**","5d41de17":"# DATA PREPARATION","91e5832b":"**INSIGHTS INTO CORRELATIONS BETWEEN THE SCALED DATA**","673aa34f":"4. **FOR SMOTE SAMPLING**","510853d0":"**CONVERTING INTO CATEGORICAL VARIABLES**","7d063e30":"2. **FOR UP SAMPLED DATA SET(RANDOM SAMPLER)**","62775821":"Looking at the preformances by the data sets sampled by various sampling techniques, we found out that the best performance is given by the data set which is down sampled by the Random Sampler. Therefore we will use the down sampled to predict whether a transaction is fraudulent or non fraudulent using various machine learning algorithms. The algorithms we will use areas follows:\n1. Logistic Regression\n2. Random Forests\n3. K- Nearest Neighbours\n4. XG Boost","bf7dd93f":"**HISTOGRAM OF DISTRIBUTION OF THE TIME OF TRANSACTIONS**","f09838d0":"**RANDOM SAMPLER - DOWN SAMPLING**","4cfe0c06":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n1. True Positive Rate\n2. False Positive Rate\nAUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0 and one whose predictions are 100% correct has an AUC of 1.0.","47ed84a6":"**Scaling of data is a very important process in the preparation of the data set that has to be used in the prediction algorithms. In our case,we are using a robust sampler. Robust sampler is very effective in terms of removing the outliers. Presence of outliers can lead to change in the decision boundary further affecting the prediction of the data.**","5904042c":"5. **FOR NEAR MISS DATA SAMPLING**","20c5f603":"**TESTING VARIOUS ALGORITHMS**","0b3ca148":"SPLITTING DATA INTO 70%-30% AS TRAINING AND TESTING DATA SETS","4cb43bfb":"**PLOT TO SEE THE DISTRIBUTION OF TRANSACTIONS**","d801b190":"**NEAR-MISS SAMPLING**","236a965b":"**RANDOM SAMPLER - UP SAMPLING**","8431a63e":"# DATA VISUALIZATION","537224a6":"# ROC AUC PLOT AND CALCULATION","7aa859b7":"**The count shows that the dataset is highly biased towards not fraud transactions which is obvious in the real world as the total number of fraudulent transactions are less in comparison with the non fraudulent transactions.**","022ef658":"1. LOGISTIC REGRESSION","a1e16b15":"# DATA SAMPLING","dd2b487b":"**1. FOR ORIGINAL DATA SET**"}}