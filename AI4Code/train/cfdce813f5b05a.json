{"cell_type":{"0abc2a5e":"code","e155509a":"code","efc9fd74":"code","3ea96873":"code","aadfebeb":"code","f46697ca":"code","29d1ef14":"code","83546b04":"code","2b9c0d2a":"code","1fcc5951":"code","0617a830":"code","227a437e":"code","f653d966":"code","c3edd84a":"code","976a1f5e":"code","72c15922":"code","b5af76c5":"code","33833471":"code","156c16ba":"code","5223201c":"code","c99a134d":"code","e2e0df68":"code","4530bf07":"code","7a46353f":"code","4e3047f0":"code","bf27b70d":"code","22ee8f07":"code","ab3b88a9":"code","4f364a92":"code","6cb6597e":"code","43136806":"code","db70feb0":"code","2020413d":"markdown"},"source":{"0abc2a5e":"!pip install --upgrade xgboost","e155509a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport cupy as cp # linear algebra\nimport cudf # data processing, CSV file I\/O (e.g. cudf.read_csv)\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom cuml.metrics import roc_auc_score\nimport shap\nimport gc\nfrom random import shuffle\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efc9fd74":"import xgboost\nxgboost.__version__","3ea96873":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')","aadfebeb":"columns = test.columns[1:]\ncolumns","f46697ca":"train.shape","29d1ef14":"test.shape","83546b04":"train.head()","2b9c0d2a":"target = np.hstack([np.ones(train.shape[0]), np.zeros(test.shape[0])])","1fcc5951":"train_test = np.vstack([train[columns].values, test[columns].values])","0617a830":"train_test.shape","227a437e":"index = list(range(train_test.shape[0]))\nshuffle(index)","f653d966":"train_test = train_test[index, :]\ntarget = target[index]","c3edd84a":"train_test[:,0]","976a1f5e":"np.unique(train_test[:,0])","72c15922":"for jj in range(10):\n    le = LabelEncoder()\n    train_test[:,jj ] = le.fit_transform(train_test[:,jj ])","b5af76c5":"train_test = train_test.astype(np.float)","33833471":"train_test[:,0]","156c16ba":"np.unique(train_test[:,0])","5223201c":"train, test, y_train, y_test = train_test_split(train_test, target, test_size=0.33, random_state=42)","c99a134d":"del train_test\ngc.collect()\ngc.collect()","e2e0df68":"train = xgboost.DMatrix(train, label=y_train)\ntest = xgboost.DMatrix(test, label=y_test)","4530bf07":"%%time\nparam = {\n    'eta': 0.05,\n    'max_depth': 10,\n    'subsample': 0.8,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist', \n    'predictor': 'gpu_predictor'\n}\nclf = xgboost.train(param, train, 600)","7a46353f":"preds = clf.predict(test)","4e3047f0":"roc_auc_score(y_test, preds)","bf27b70d":"AUC of 0.5004 is very low, and statistically indistinguishable from a perfectly shuffled train\/test split. Nonetheless, for the sake of an exercise, let's try to see if we can find which features are the most responsible for the discrepancy. In order to do this, we'll resort to calculating SHAP values, which can be done directly on GPUs with the version 1.3 of XGBoost. this is not very useful with this problem, but it's a good to show how it wouold be done with a much more imbalanced train and test datasets.","22ee8f07":"%%time\nshap_preds = clf.predict(test, pred_contribs=True)","ab3b88a9":"shap_preds.shape","4f364a92":"shap_preds[:,:-1].shape","6cb6597e":"shap.initjs()","43136806":"shap.summary_plot(shap_preds[:,:-1])","db70feb0":"shap.summary_plot(shap_preds[:,:-1], plot_type=\"bar\")","2020413d":"One of the main issues that make Kaggle (and for tahat matter any other) predictive modeling tricky are the discrepancies between the training and the test datasets. In order to get an idea of the magnitude of these differences, one of the more valuable tools to use is adversarial validation. With aversariel validation we try to build an auxiliary model that predicts whether given data points belong to the train and the test set. If we can make predictions with such a model with a high degree of confidence, then that usually means that the train and test sets are significantly different, and we need to be careful to make a model that will take that into the account.\n\nWe will make this adversarial validation notebook with the Rapids library. [Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html).\n\nFor the modeling part we'll use the latest version of XGBoost, which allows for GPU accelerated calculation of Shapely Values. We'll use these \"SHAP\" values to calculate correct feature importances. Starting with the version 1.3, XGBoost supports fast calculation of the SHAP values on GPU. However, as of the time of creating this notebook, that version of XGBoost is still not available in Kaggle Docker, so we'll have to install it."}}