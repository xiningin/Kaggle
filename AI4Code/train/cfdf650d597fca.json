{"cell_type":{"98620523":"code","31f1bc7d":"code","8bbae45a":"code","2833e1b1":"code","f3e5df9e":"code","1c115988":"code","dc176a4e":"code","0163e48f":"code","45e8a4ba":"code","6064ce9d":"code","080bfa0b":"code","dbdf9d40":"code","c805cec2":"code","b0a550b7":"code","7a3b0dc2":"code","6fa1514c":"code","dd66d370":"code","7a9dcc5f":"code","25148975":"code","62ec675e":"code","c83fc098":"code","342b8bb5":"code","56a89f18":"code","cea7cbd9":"code","85fcd382":"code","27fd5506":"code","fa4a9cbd":"code","06a8eebe":"code","2963c0b7":"code","246ec56e":"code","66cac43c":"code","89551775":"code","199084ca":"code","47934fb0":"code","59a57f4c":"markdown","634282f5":"markdown"},"source":{"98620523":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31f1bc7d":"import matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","8bbae45a":"print(\"TF version: \", tf.__version__)\nif tf.__version__ < \"2.0.0\":\n    tf.enable_eager_execution()\n    print(\"Eager execution enabled.\")\nelse:\n    print(\"Eager execution enabled by default.\")\n\nif tf.test.gpu_device_name(): \n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n   print(\"Please install GPU version of TF\")","2833e1b1":"train_data = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', sep = '\\t')\ntrain_data.head()","f3e5df9e":"train_data.info()","1c115988":"train_data.shape","dc176a4e":"test_data = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip',sep = '\\t')\ntest_data.head()","0163e48f":"print(test_data.shape)\ntest_data.info()","45e8a4ba":"train_data.columns","6064ce9d":"print(train_data['Sentiment'].unique())\ntrain_data['Sentiment'].nunique()","080bfa0b":"train_data['Sentiment'].value_counts()","dbdf9d40":"val_array = train_data['Sentiment'].value_counts().to_xarray()\nplt.bar(val_array.index, val_array)\nplt.xlabel('review rating\/Sentiments')\nplt.ylabel('No of reviews')\nplt.show()","c805cec2":"%%time\ntrain_data['Length'] = train_data['Phrase'].apply(lambda x: len(str(x).split(' ')))\nimport seaborn as sns\nsns.pairplot(train_data,hue='Sentiment',vars=['PhraseId','SentenceId','Length'])","b0a550b7":"from tqdm import tqdm\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport re","7a3b0dc2":"def clean_sentences(df):\n    \n    reviews = []\n    for sent in tqdm(df['Phrase']):\n        # removing non-alphabetical characters \n        text = re.sub(\"[^a-zA-Z]\",\" \",sent)\n        \n        # Now tokenizing the sentence : \n        words = word_tokenize(text.lower())\n        \n        #removing stop words :\n        new_words = [ ele for ele in words if ele.lower() not in stopwords.words('english') ]\n        \n        # Lemmatizing each word to its lemma\n        lem = WordNetLemmatizer()\n        lem_words = [lem.lemmatize(i) for i in new_words]\n        \n        #finally\n        reviews.append(lem_words)\n        \n    return(reviews)\n","6fa1514c":"%%time\ntrain_sentences = clean_sentences(train_data)\ntest_sentences = clean_sentences(test_data)\n\nprint(len(train_sentences))\nprint(len(test_sentences))","dd66d370":"print(train_data['Phrase'][0])\nprint((\" \").join(train_sentences[0]))","7a9dcc5f":"from keras.utils import to_categorical\n\ny_target = to_categorical(train_data['Sentiment'].values)\n","25148975":"y_target.shape","62ec675e":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,y_train,y_val = train_test_split(train_sentences,y_target,test_size = 0.2,stratify = y_target)","c83fc098":"X_train[0]","342b8bb5":"unique_words = set()\nlen_max = -1\n\nfor sent in tqdm(X_train):\n    unique_words.update(sent)\n    if(len_max < len(sent)):\n        len_max = len(sent)\n\nprint('Words in vocab : ' , len(list(unique_words)))\nprint('Max_length : ' , len_max)","56a89f18":"vocab_size = len(list(unique_words))\nembedding_dim = 300\nmax_length = len_max\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'","cea7cbd9":"%%time\ntokenizer = Tokenizer(num_words = vocab_size,\n                      # filters = '#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                      oov_token = oov_tok,\n                      # lower = True,\n                      char_level = False)\n\ntokenizer.fit_on_texts(list(X_train))\n\n# Training\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train,\n                        maxlen = max_length,\n                        padding = padding_type,\n                        truncating = trunc_type)\n\n# Validation\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = pad_sequences(X_val,\n                      maxlen = max_length,\n                      padding = padding_type,\n                      truncating = trunc_type)\n\n# Testing\nX_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = pad_sequences(X_test,\n                       maxlen = max_length,\n                       padding = padding_type,\n                       truncating = trunc_type)","85fcd382":"print(\"X_training shape   : \",X_train.shape)\nprint(\"X_validation shape : \",X_val.shape)\nprint(\"X_testing shape    : \",X_test.shape)","27fd5506":"print(X_train[2])","fa4a9cbd":"from keras.models import Sequential\nfrom keras.layers import Dense,Bidirectional,LSTM,Activation,Conv1D,MaxPool1D,Dropout\nfrom keras.layers.embeddings import Embedding","06a8eebe":"model = Sequential()\nmodel.add(Embedding(vocab_size,embedding_dim,input_length = max_length))\nmodel.add(Bidirectional(LSTM(128,dropout = 0.2, recurrent_dropout = 0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences=False)))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dense(y_target.shape[1],activation = 'softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer = 'adam',\n             metrics = ['accuracy'])\n\nmodel.summary()","2963c0b7":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(min_delta = 0.001,\n                               mode = 'max',\n                               monitor = 'val_acc',\n                               patience = 2)\ncallback = [early_stopping]","246ec56e":"%%time\n\nnum_epochs = 4\n\nhistory = model.fit(X_train,y_train,\n                    validation_data = (X_val, y_val),\n                    epochs = num_epochs,\n                    batch_size = 256,\n                    verbose = 1,\n                    callbacks = callback)","66cac43c":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n  \nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","89551775":"test_id = test_data['PhraseId']","199084ca":"%%time\n\ny_pred = np.argmax(model.predict(X_test), axis = -1)","47934fb0":"submission_df = pd.DataFrame({'PhraseId': test_id, 'Sentiment': y_pred})\nsubmission_df.to_csv('submission_.csv', index=False)\nsubmission_df.head()","59a57f4c":"The sentiment labels are:\n\n0 - negative\n\n1 - somewhat negative\n\n2 - neutral\n\n3 - somewhat positive\n\n4 - positive\n\nSet Training & Validation set to 80\/20","634282f5":"Now tokenizing the data set :"}}