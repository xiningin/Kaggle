{"cell_type":{"092679c6":"code","3e3b4b6e":"code","0d60c483":"code","2a412bf4":"code","d6ef8705":"code","fb3a1487":"code","919c494c":"code","6f20d334":"code","6a5c5f6e":"code","9ba1007f":"code","ce22ae7e":"code","96bbd10d":"code","a9c08159":"code","3e568ff5":"code","8491726b":"code","75ecbc4d":"code","1f394b68":"code","5e3258a5":"markdown","240eef3d":"markdown","9ceb6369":"markdown","f99d53c5":"markdown","a1cabed9":"markdown","222cb5aa":"markdown","c527fce3":"markdown","4d6f9de9":"markdown","5d02200f":"markdown","02122893":"markdown","c2d34a50":"markdown","d9c22592":"markdown"},"source":{"092679c6":"# import libraries that will be used in the analysis\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3e3b4b6e":"# loading all data into a list structure\nfiles=os.listdir('..\/input\/acea-water-prediction')\nprint(files)\n# read cvs data files\nAquifer_Doganella=pd.read_csv('..\/input\/acea-water-prediction\/'+'Aquifer_Doganella.csv')\nAquifer_Auser=pd.read_csv('..\/input\/acea-water-prediction\/'+'Aquifer_Auser.csv')\nWater_Spring_Amiata=pd.read_csv('..\/input\/acea-water-prediction\/'+'Water_Spring_Amiata.csv')\nLake_Bilancino=pd.read_csv('..\/input\/acea-water-prediction\/'+'Lake_Bilancino.csv')\nWater_Spring_Madonna_di_Canneto=pd.read_csv('..\/input\/acea-water-prediction\/'+'Water_Spring_Madonna_di_Canneto.csv')\nAquifer_Luco=pd.read_csv('..\/input\/acea-water-prediction\/'+'Aquifer_Luco.csv')\nAquifer_Petrignano=pd.read_csv('..\/input\/acea-water-prediction\/'+'Aquifer_Petrignano.csv')\nWater_Spring_Lupa=pd.read_csv('..\/input\/acea-water-prediction\/'+'Water_Spring_Lupa.csv')\nRiver_Arno=pd.read_csv('..\/input\/acea-water-prediction\/'+'River_Arno.csv')\n# combine data into datasets\ndatasets = [Aquifer_Doganella,Aquifer_Auser,Water_Spring_Amiata,Lake_Bilancino,Water_Spring_Madonna_di_Canneto,\n           Aquifer_Luco,Aquifer_Petrignano,Water_Spring_Lupa,River_Arno]\ndatasets_names=['Aquifer_Doganella.csv', 'Aquifer_Auser.csv', 'Water_Spring_Amiata.csv', 'Lake_Bilancino.csv', 'Water_Spring_Madonna_di_Canneto.csv', 'Aquifer_Luco.csv', 'Aquifer_Petrignano.csv', 'Water_Spring_Lupa.csv', 'River_Arno.csv']","0d60c483":"#boxplot of all variables in each file to examine data range and distribution\nfig,ax1 = plt.subplots(3,3,figsize=(15,15))\nfor i in range(len(datasets)):\n    ax1.flatten()[i].set_title(datasets_names[i][:-4])\n    datasets[i][datasets[i].columns[1:]].boxplot(ax=ax1.flatten()[i],rot=90)\nplt.tight_layout()\n    ","2a412bf4":"# define a function to plot features vs. target variables\ndef plot1(inputdata,features,target_var,ylabel1,ylabel2):\n    fig, ax1= plt.subplots(figsize=(15,5))\n    #inputdata['Year-mon']=pd.to_datetime(inputdata['Year-mon'])\n    ax1.bar(inputdata['Year-mon'],inputdata[features])\n    ax1.spines['left'].set_color('blue')\n    ax1.spines['left'].set_linewidth(3)\n    ax1.legend([features],loc=2)\n    ax2 =ax1.twinx()\n    ax2.plot(inputdata['Year-mon'],inputdata[target_var],color='red')\n    ax2.spines['right'].set_color('red')\n    ax2.spines['right'].set_linewidth(3)\n    ax1.set_ylabel(ylabel1)\n    ax1.set_xticks(range(0,len(inputdata),10))\n    ax1.set_xticklabels(inputdata['Year-mon'][range(0,len(inputdata),10)],rotation=90)\n    ax2.set_ylabel(ylabel2)\n    ax2.set_xticks(range(0,len(inputdata),10))\n    ax2.set_xticklabels(inputdata['Year-mon'][range(0,len(inputdata),10)],rotation=90)\n    ax2.legend([target_var[0]],loc=1)\n","d6ef8705":"# transform daily data into montly data using groupby\nfor i in range(len(datasets)): \n  datasets[i].drop(datasets[i][datasets[i].Date.isnull()].index,inplace = True,axis=0)\n  datasets[i]['Year-mon']=pd.to_datetime(datasets[i].Date).apply(lambda x: x.strftime('%Y-%m'))\ndatasets_monthly = [datasets[i].groupby('Year-mon').mean().reset_index() for i in range(len(datasets))]\n","fb3a1487":"target_var=['Depth_to_Groundwater_Pozzo_1', 'Depth_to_Groundwater_Pozzo_2',\n       'Depth_to_Groundwater_Pozzo_3', 'Depth_to_Groundwater_Pozzo_4',\n       'Depth_to_Groundwater_Pozzo_5', 'Depth_to_Groundwater_Pozzo_6',\n       'Depth_to_Groundwater_Pozzo_7', 'Depth_to_Groundwater_Pozzo_8',\n       'Depth_to_Groundwater_Pozzo_9', ]\nplot1(datasets_monthly[0],'Rainfall_Monteporzio',target_var,'Rainfall, mm','Groundwater Level, m')","919c494c":"plot1(datasets_monthly[0],'Volume_Pozzo_9',['Depth_to_Groundwater_Pozzo_9'],'Volume cm','Groundwater Level, m')","6f20d334":"# setup target variabe list\ntarget_var = [[col for col in datasets_monthly[0].columns if 'Depth' in col]]\ntarget_var.append([col for col in datasets_monthly[1].columns if 'Depth' in col])\ntarget_var.append([col for col in datasets_monthly[2].columns if 'Flow_Rate' in col])\ntarget_var.append([col for col in datasets_monthly[3].columns if 'Lake' in col])\ntarget_var.append([col for col in datasets_monthly[4].columns if 'Flow_Rate' in col])\ntarget_var.append([col for col in datasets_monthly[5].columns if 'Depth' in col])\ntarget_var.append([col for col in datasets_monthly[6].columns if 'Depth' in col])\ntarget_var.append([col for col in datasets_monthly[7].columns if 'Flow_Rate' in col])\ntarget_var.append([col for col in datasets_monthly[8].columns if 'Hydrometry' in col])","6a5c5f6e":"# create a season variable to see if it can help improve prediction\nfor i in range(len(datasets_monthly)):\n    datasets_monthly[i].loc[datasets_monthly[i]['Year-mon'].str.split('-').str[1].str.contains('12|01|02'),'season']=1\n    datasets_monthly[i].loc[datasets_monthly[i]['Year-mon'].str.split('-').str[1].str.contains('03|04|05'),'season']=2\n    datasets_monthly[i].loc[datasets_monthly[i]['Year-mon'].str.split('-').str[1].str.contains('06|07|08'),'season']=3\n    datasets_monthly[i].loc[datasets_monthly[i]['Year-mon'].str.split('-').str[1].str.contains('09|10|11'),'season']=4\n# create a dryness variable based on rainfall amount\nfor i in range(len(datasets_monthly)):\n    datasets_monthly[i].loc[datasets_monthly[i].iloc[:,1]>(datasets_monthly[i].iloc[:,1].mean()+datasets_monthly[i].iloc[:,1].std()),'dryness']=4\n    datasets_monthly[i].loc[(datasets_monthly[i].iloc[:,1]<=(datasets_monthly[i].iloc[:,1].mean()+datasets_monthly[i].iloc[:,1].std())) & (datasets_monthly[i].iloc[:,1]>datasets_monthly[i].iloc[:,1].mean()),'dryness']=3\n    datasets_monthly[i].loc[(datasets_monthly[i].iloc[:,1]>=(datasets_monthly[i].iloc[:,1].mean()-datasets_monthly[i].iloc[:,1].std())) & (datasets_monthly[i].iloc[:,1]<=datasets_monthly[i].iloc[:,1].mean()),'dryness']=2\n    datasets_monthly[i].loc[datasets_monthly[i].iloc[:,1]<(datasets_monthly[i].iloc[:,1].mean()-datasets_monthly[i].iloc[:,1].std()),'dryness']=1\n# create features of previous month conditions which may contribute to the current month if water flow has some lags\nfor i in range(len(datasets_monthly)):\n    feature =[feature for feature in datasets_monthly[i].columns if feature not in target_var[i]][1:]\n    for j in range(len(feature)):\n        datasets_monthly[i][f'{feature[j]}_previous_month1']=np.append([np.nan],datasets_monthly[i].iloc[:-1][f'{feature[j]}'].values)\n        #datasets_monthly[i]['rainfall_previous_month2']=np.append([np.nan,np.nan],datasets_monthly[i].iloc[:-2,1].values)","9ba1007f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nf,ax3 = plt.subplots(9,2,figsize=(10,50))\n# create a dataframe to store prediction accuracy results\nstats_index=[datasets_names[k][:-4]+': '+item for k, sublist in enumerate(target_var) for item in sublist]\nstats = pd.DataFrame(columns=['NRMSE','MAE','R-Squared','Top 3 important features'],index=stats_index)\nfor j in range(len(target_var)):\n    for i,target in enumerate(target_var[j]):\n        #select non-null values of responsive variable and imputing nan values of predictive variables by mean\n        idx1 = datasets_monthly[j][target].notnull()\n        data = datasets_monthly[j][idx1].fillna(datasets_monthly[j][idx1].mean())\n        x = data.drop(target_var[j],axis=1)\n        x = x.drop('Year-mon',axis=1)\n        y = data[target]\n        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\n        np.random.seed(1234)\n        RFG = RandomForestRegressor(n_estimators=40,random_state=0)\n        RFG.fit(x_train,y_train)\n        importance = RFG.feature_importances_\n        # output statistics of prediction\n        stats.loc[f'{datasets_names[j][:-4]}: {target}','NRMSE']=f'{mean_squared_error(y_test,RFG.predict(x_test),squared=False)\/np.std(y_test):.3f}'\n        stats.loc[f'{datasets_names[j][:-4]}: {target}','MAE']=f'{mean_absolute_error(y_test,RFG.predict(x_test)):.3f}' \n        stats.loc[f'{datasets_names[j][:-4]}: {target}','R-Squared']=f'{r2_score(y_test,RFG.predict(x_test)):.3f}'\n        stats.loc[f'{datasets_names[j][:-4]}: {target}','Top 3 important features']= [list(x.columns[np.argsort(importance)[::-1][:3]])]\n        # give an example of prediction in visulization\n        if j == 0:\n            ax3[i,0].plot(y_test,RFG.predict(x_test),'.')\n            ax3[i,0].plot(np.linspace(np.amin(y_test),np.amax(y_test),100),np.linspace(np.amin(y_test),np.amax(y_test),100),'k')\n            ax3[i,0].set_ylabel('Predicted')\n            ax3[i,0].set_xlabel('Observed')\n            ax3[i,0].text(np.min(y_test),np.max(y_test)-1,f'NRMSE: {mean_squared_error(y_test,RFG.predict(x_test)):.3f}')\n            ax3[i,0].text(np.min(y_test),np.max(y_test)-2,f'MAE: {mean_absolute_error(y_test,RFG.predict(x_test)):.3f}')\n            ax3[i,0].text(np.min(y_test),np.max(y_test)-3,f'R-Squared: {r2_score(y_test,RFG.predict(x_test)):.3f}' )\n            ax3[i,0].set_title(f'{target}, meter')   \n            ax3[i,1].bar(range(len(importance)),importance)\n            ax3[i,1].set_xticks(range(len(importance)))\n            ax3[i,1].set_xticklabels(x.columns,rotation=90)\n            ax3[i,1].set_ylabel('Importance')\n    plt.tight_layout()","ce22ae7e":"stats","96bbd10d":"print('Average of all predictions')\nstats.iloc[:,0:3].astype(float).mean()","a9c08159":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 80, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","3e568ff5":"import datetime\n# create a dataframe to store prediction accuracy results\nstats_index=[datasets_names[k][:-4]+': '+item for k, sublist in enumerate(target_var) for item in sublist]\nstats = pd.DataFrame(columns=['NRMSE','MAE','R-Squared'],index=stats_index)\nprint(datetime.datetime.now())\nfor j in range(len(target_var)):\n    for i,target in enumerate(target_var[j]):\n        #select non-null values of responsive variable and imputing nan values of predictive variables by mean\n        idx1 = datasets_monthly[j][target].notnull()\n        data = datasets_monthly[j][idx1].fillna(datasets_monthly[j][idx1].mean())\n        x = data.drop(target_var[j],axis=1)\n        x = x.drop('Year-mon',axis=1)\n        y = data[target]\n        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\n        RFG = RandomForestRegressor()\n        # search across 100 different combinations, and use all available cores\n        rf_random = RandomizedSearchCV(estimator = RFG, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=0, random_state=42, n_jobs = -1)\n        # Fit the random search model\n        rf_random.fit(x_train, y_train)\n        best_model=rf_random.best_estimator_\n        # output statistics of prediction\n        stats.loc[f'{datasets_names[j][:-4]}: {target}','NRMSE']=f'{mean_squared_error(y_test,best_model.predict(x_test),squared=False)\/np.std(y_test):.3f}'\n        stats.loc[f'{datasets_names[j][:-4]}: {target}','MAE']=f'{mean_absolute_error(y_test,best_model.predict(x_test)):.3f}' \n        stats.loc[f'{datasets_names[j][:-4]}: {target}','R-Squared']=f'{r2_score(y_test,best_model.predict(x_test)):.3f}'\nprint(datetime.datetime.now())","8491726b":"# cross-validation and hyperparameter tuning results:\nstats","75ecbc4d":"stats.iloc[:,0:3].astype(float).mean()","1f394b68":"stats.to_csv('final_stats3.csv')","5e3258a5":"The Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania.\n\nIn this competition Kagglers were asked to focus on the water sector to help Acea Group preserve precious waterbodies such as water springs, lakes, rivers, and aquifers. To help preserve the health of these waterbodies it is important to predict the water availability in terms of level and water flow for each day\/month of the year.\n\nIn the following sections, I would like to show my work on the prediction of water availabilities and its accuracies in terms of mean absolute error, mean squared error, and R^2 beween obervation and prediction values.","240eef3d":"# 7.Use Cross-validation to improve accuray","9ceb6369":"**Observation 2:** Relative dry years (less rainfall) appeared in 2015-2017, which seems to cause well water depth droped, especially at wells Pozzo_1 and Pozzo_9. While water usage seems not to be very important in water depth drops as there is some cooccurence between less water usage and greater water depth drop at the well Pozzo_9. Of course, these are just observation for occasion wells at their occasion times. More important info about important features can be seen later in prediction and feature importance analysis.","f99d53c5":"**Observation 1\uff1a**Data visulization using boxplot tells what variables are included in each data file and how responsive varibles and predictive variables are distributed. However, variable values are not at same scale (e.g. rainfall in mm, flow rate in cubic meter per sencond, and volume in cubic meter) which make it hard to fully view the distributions. Further data analysis including correlation, exploratory factor, and feature engineering are necessary. ","a1cabed9":"# Content:\n\n1. Loading Data\n2. Visulization of Data\n3. Data imputation\n4. Exploratory Analysis and Feature Engineering\n5. Prediction","222cb5aa":"# 6. Statistics of prediction","c527fce3":"# 1. Loading Data","4d6f9de9":"# 2. Visulization of Data \n","5d02200f":"# Conclusions:\n1. As shown by the stats table, the models have acceptable accuracy in test data with features engineered as input to a random forest algorithm. Of course, if more meaningful features can be developed and the model accuracies can be further improved.\n2. The volumes of water used are the most important features in most of the simulations especially for the water table depth predictions. \n3. In my previous try, if target variables (when not be predicted) were used as features, the models peformed much better. However, I dont think it is a justified way to do and give it up because there are high correlations between the target variables (such as the water table depths) and in reality some are available while others may be not.\n4. Models are further improved by tuning hyperparameters and cross validation and R-squred improved from 0.61 to 0.64.","02122893":"# 5. Predictive Accuracy and Feature Importance**","c2d34a50":"# 4. Exploratory Analysis and Feature Engineering","d9c22592":"# 3.Data imputation and missing value treatment \nThere are a lot of missing values in the daily time series. for example the water use vollumes are mostly missing and they are hard to be replaced with meaningful values. In this analysis, monthly data were firstly grouped by from daily values and then monthly means were used to replace mising values"}}