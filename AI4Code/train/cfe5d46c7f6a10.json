{"cell_type":{"b892602d":"code","e689c9b0":"code","957d5b0b":"code","fb989c28":"code","e28fa871":"code","8dccbd7d":"code","755c9074":"code","9e7276d8":"code","da5c897c":"code","d4609f47":"code","745319b3":"code","a041c5f0":"code","9f3950ce":"code","a8198729":"code","f79b35b1":"code","8d0c270d":"code","86b42dcf":"code","9c1d3284":"code","e7b2e677":"code","8d66c49b":"code","75a871d6":"code","90581c3f":"code","1dd6aa90":"code","325d061f":"code","d4a1ec20":"code","666741c5":"code","49ca27ad":"code","18f787d4":"code","609500ea":"code","abddfb26":"code","f1a9fa59":"code","8b613ad7":"code","1403bf89":"code","5dc0f542":"code","fb2979cb":"code","e5a86f85":"code","a56ef6ad":"code","646b75e2":"code","5b7d8647":"code","d1069857":"code","4fba29d7":"code","cb4ea03a":"code","36af488d":"code","d8f40176":"code","10563ff6":"code","72676f79":"code","69d68770":"code","efa433ca":"code","9d62d467":"code","c52ba945":"code","f303e749":"code","74aa20d0":"code","a7bc1c0e":"code","209bdae8":"code","22c37dfa":"code","bea08208":"markdown","b6d62f98":"markdown","a32d15f2":"markdown","36a312bf":"markdown"},"source":{"b892602d":"# Installing the package for lemmatization (Russian language)\n!pip install pymorphy2\n","e689c9b0":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport json\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib import rcParams\n%matplotlib inline\nimport nltk\nimport pymorphy2\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwordsrus = set(stopwords.words('russian'))\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","957d5b0b":"# Checking up Kaggle directories, loading the dataset\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nrus_data = pd.read_csv(\"\/kaggle\/input\/corpus-of-russian-news-articles-from-lenta\/lenta-ru-news.csv\")\n\n\n\n","fb989c28":"# Let's have a look at the data:\nrus_data","e28fa871":"\nrus_data['topic'].unique()\n","8dccbd7d":"# Let's shorten the dataset and exclude some of the topics: \n\nrusdata = rus_data['text'][(rus_data['topic']!='\u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430')&(rus_data['topic']!='\u0411\u044b\u0432\u0448\u0438\u0439 \u0421\u0421\u0421\u0420')&(rus_data['topic']!='69-\u044f \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c')].reset_index(drop=True)","755c9074":"# These Russian news articles we will use for LDA analysis:\nrusdata","9e7276d8":"# Loading stopwords and combining them:\nstops1 = []\nwith open('\/kaggle\/input\/russianstopwords\/RussianStopWords.txt', \"r\", encoding=\"utf-8\", newline=None) as readfile:\n     [stops1.append(line.rstrip()) for line in readfile]\n\nstops2 = []\nwith open('\/kaggle\/input\/stopwords\/stopwords.txt', \"r\", encoding=\"utf-8\", newline=None) as readfile:\n     [stops2.append(line.rstrip()) for line in readfile]\n\nstopw_all = stops1 + stops2 + list(stopwordsrus)\nstopwordsru = list(dict.fromkeys(stopw_all))","da5c897c":"# Tokenizing and removing stopwords:\ndef process(text):\n    return list(t.lower() for t in word_tokenize(text) if t.isalpha() and t.lower() not in stopwordsru)","d4609f47":"# Let's take only first 10000 articles for our analysis:\ndata = [process(t) for t in rusdata[:10000]]","745319b3":"# Lemmatizer for russian language:\nmorph = pymorphy2.MorphAnalyzer()\ndef lemmatizer(texts):\n    return [[morph.parse(word)[0] for word in text] for text in texts]","a041c5f0":"morph_data = lemmatizer(data)","9f3950ce":"# We need only lemma of the word, without additional information, so let's extract it:\ndef extract_lemma(texts):\n    norm = []\n    for t in texts:\n        res = []\n        for word in t:\n            n = word.normal_form\n            res.append(n)\n        norm.append(res)\n    return norm","a8198729":"# This is our lemmatized data ready to be used further:\ndata_norm = extract_lemma(morph_data)","f79b35b1":"# Let's build the bigram and trigram models using gensim\nbigram = gensim.models.Phrases(data_norm, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_norm], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","8d0c270d":"def make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndata_words_trigrams = make_trigrams(data_norm)","86b42dcf":"# We can see that bigrams and trigrams are build successfully \nprint(data_words_trigrams[0])","9c1d3284":"# Let's create dictionary of all our unique words from the dataset using corpora from gensim\ndictionary = corpora.Dictionary(data_words_trigrams)\n# Now let's create corpus where we count occurances for each word from dictionary in texts\ncorpus = [dictionary.doc2bow(doc) for doc in data_words_trigrams]\n# We will also try to filter unimportant words by their tf-idf score, so let's create the tf-idf scores here too\ntfidf = gensim.models.TfidfModel(corpus, id2word = dictionary)","e7b2e677":"# A word from our dictionary:\ndictionary[8]","8d66c49b":"# Let's see what are the max and min values of tf-idf score:\ntf_max = round(max([max([value for id, value in tfidf[corpus[x]]]) for x in range(len(corpus))]), 4)\ntf_min = round(min([min([value for id, value in tfidf[corpus[x]]]) for x in range(len(corpus))]), 4)\nprint(tf_max, tf_min)\ntfidf_range = [round(num, 3) for num in np.arange(tf_min, tf_max, 0.005).tolist()]\n# We will be cutting the highest and the lowest tf-idf, e.g. <10 and >95% of all tf-idf values, so let's obtain those values:\nprint(np.percentile(tfidf_range, 95), np.percentile(tfidf_range, 10))\n","75a871d6":"# Let's select high and low tfidf threshold values, and filter them out. \n# Thus, we will filter out very common (low tf-idf) and very rare (big tf-idf) words\n\nlow_value = np.percentile(tfidf_range, 5) \nhigh_value = np.percentile(tfidf_range, 95) \n\nfiltered_corpus = []\nfor i in range(0, len(corpus)):\n        \n    filter_ids = [id for id, value in tfidf[corpus[i]] if value < low_value or value > high_value ]\n   \n    new_bow = [(index, value) for (index, value) in corpus[i] if index not in filter_ids] \n\n      \n    filtered_corpus.append(new_bow)","90581c3f":"# Now we have prepered the data. We have two types of corpus:\n# original without tf-idf filtering called \"corpus\", and the one with tf-idf filtering called \"filtered_corpus\"\n\n# Firtst, let's build LDA model from Gensim without tf-idf filtering.\n# Since the number of texts is quite large (10000), let's chose number of topics equal to 80.\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=80, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","1dd6aa90":"# Show Topics\npprint(lda_model.show_topics(formatted=False))","325d061f":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. The lower the better.\n","d4a1ec20":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","666741c5":"# Now let's build basic Gensim LDA model with tf-idf filtering:\nlda_model = gensim.models.ldamodel.LdaModel(corpus=filtered_corpus,\n                                           id2word=dictionary,\n                                           num_topics=80, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","49ca27ad":"# Show Topics\npprint(lda_model.show_topics(formatted=False))","18f787d4":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(filtered_corpus))  # a measure of how good the model is. lower the better.\n","609500ea":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","abddfb26":"!wget http:\/\/mallet.cs.umass.edu\/dist\/mallet-2.0.8.zip","f1a9fa59":"!unzip mallet-2.0.8.zip","8b613ad7":"\nmallet_path = '\/kaggle\/working\/mallet-2.0.8\/bin\/mallet'","1403bf89":"# Let's run Mallet LDA model with Nr. of topics 145\nldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=145, id2word=dictionary, random_seed=0)","5dc0f542":"# Show Topics\npprint(ldamallet.show_topics(formatted=False))","fb2979cb":"# Compute Coherence Score\ncoherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_ldamallet = coherence_model_ldamallet.get_coherence()\nprint('\\nCoherence Score: ', coherence_ldamallet)","e5a86f85":"def format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame (index=range(10000), columns = ['Dominant_Topic1', 'Dominant_Topic2', '%Topic_Contribution1', '%Topic_Contribution2', 'Topic_Keywords1', 'Topic_Keywords2'])\n    #sent_topics_df = pd.DataFrame()\n    \n\n    # Get main topic in each document\n    for i, text in enumerate(ldamodel[corpus]):\n        text = sorted(text, key=lambda x: (x[1]), reverse=True) #sort % contributions of topic  \n        # Get the Dominant topic, % of topic contribution and Keywords for each document\n        for j, (topic_num, topic_contrib) in enumerate(text):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df.Dominant_Topic1[i] = int(topic_num)\n                sent_topics_df['%Topic_Contribution1'][i] = round(topic_contrib,4)\n                sent_topics_df['Topic_Keywords1'][i] = topic_keywords\n                \n                #sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n                \n            elif j == 1:  # => second dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df.Dominant_Topic2[i] = int(topic_num)\n                sent_topics_df['%Topic_Contribution2'][i] = round(topic_contrib,4)\n                sent_topics_df['Topic_Keywords2'][i] = topic_keywords\n                \n            else:\n                break\n    \n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=rusdata[:10000])\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index(drop = True)\n\n\n# Show\ndf_dominant_topic.head(10)","a56ef6ad":"# Here we can select a text and see its first and second most dominant topics (keywords of those topics)\nindex = 0\ndf_dominant_topic['text'][index]","646b75e2":"df_dominant_topic['Topic_Keywords1'][index]+'\/\/ '+df_dominant_topic['Topic_Keywords2'][index]","5b7d8647":"# Now let's build Mallet LDA with tf-idf -filtered corpus:\n\nldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=filtered_corpus, num_topics=145, id2word=dictionary, random_seed=0)#150","d1069857":"# Compute Coherence Score\ncoherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_trigrams, dictionary=dictionary, coherence='c_v')\ncoherence_ldamallet = coherence_model_ldamallet.get_coherence()\nprint('\\nCoherence Score: ', coherence_ldamallet)","4fba29d7":"# Show Topics\npprint(ldamallet.show_topics(formatted=False))\n","cb4ea03a":"df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=filtered_corpus, texts=rusdata[:10000])\n\ndf_dominant_topic_filtered_idfs = df_topic_sents_keywords.reset_index(drop = True)\n\n# Show\ndf_dominant_topic_filtered_idfs.head(10)","36af488d":"# To see an example of how the model worked: we can pick a text by index, and see the keywords of most dominant topic\n# (with higest contribution, as well as the second one)\nindex = 2\ndf_dominant_topic_filtered_idfs['text'][index]","d8f40176":"# Two most dominint topics (their keywords)\ndf_dominant_topic_filtered_idfs['Topic_Keywords1'][index]+'\/\/ '+df_dominant_topic_filtered_idfs['Topic_Keywords2'][index]","10563ff6":"# Now let's plot 20 most frequent topics from our 10000 texts, as well as the second most frequent topic\n# Let's organize data for the plot","72676f79":"# Counting texts for each topic number\ncounts = []\nn_topics = 145\nfor x in range(0, n_topics):\n    z = df_dominant_topic['Dominant_Topic1'][df_dominant_topic['Dominant_Topic1'] == x].count()\n    counts.append([x,z])","69d68770":"# Sorting by number of texts\nsorted_counts = sorted(counts, key=lambda x: int(x[1]), reverse=True) \n","efa433ca":"# Selecting most popular n-themes\nn_themes = 25\nmost_popular = [sorted_counts[x][0] for x in range(n_themes)] \n","9d62d467":"# Selecting keywords for most popular n-themes\ntheme_keywords = [df_dominant_topic['Topic_Keywords1'][df_dominant_topic['Dominant_Topic1']==x].unique().tolist() for x in most_popular]","c52ba945":"# Count number of texts for second topics\ntopic_counts = []\nn_topics = 145\nfor x in range(0, n_topics):\n    for y in range(0, n_topics):\n        z = df_dominant_topic['Dominant_Topic1'][(df_dominant_topic['Dominant_Topic1'] == x) & (df_dominant_topic['Dominant_Topic2'] == y)].count()\n        topic_counts.append([x,y,z])","f303e749":"# Sorting \ntwo_themes_count = sorted(topic_counts, key=lambda x: int(x[2]), reverse=True) \n# Selecting second topics for first popular n-topics\nmost_frequent = [two_themes_count[ind] for ind in range(len(two_themes_count)) if two_themes_count[ind][0] in most_popular]\nmost_frequent_sorted = sorted(most_frequent, key=lambda x: int(x[0]), reverse=True) ","74aa20d0":"# Selecting most frequent second topics that follow those most frequent first topics\nsecond_topic = []\nfor x in most_popular:\n    for y in range(len(most_frequent_sorted)):\n        if most_frequent_sorted[y][0] == x:\n            second_topic.append(most_frequent_sorted[y:y+1][0])\n            break","a7bc1c0e":"# Getting array of second topic numbers\nsecond_topic_num = [second_topic[x][1] for x in range(n_themes)]","209bdae8":"# Corresponding keywords for second topics\ntheme_keywords_second_topic = [df_dominant_topic['Topic_Keywords2'][df_dominant_topic['Dominant_Topic2']==x].unique().tolist() for x in second_topic_num]","22c37dfa":"rcParams['font.size'] = 9\nrcParams['axes.titlesize'] = 14\nplt.rc('xtick', labelsize=14)\nplt.rc('ytick', labelsize=14)\nplt.rc('axes', labelsize=14)\nrcParams['figure.dpi']= 600\n\ntheme = [sorted_counts[x][0] for x in range(n_themes)]\ntext_count = [sorted_counts[x][1] for x in range(n_themes)]\ny_pos = np.arange(len(theme))\n\n\nfig, ax = plt.subplots(figsize=(16, 20)) # 16 14\n\nplot = ax.barh(y_pos, text_count, align='center')\n\nplt.xlim(0, 500)\n\nax.set_yticks(y_pos)\nax.set_yticklabels(theme)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('N of texts')\nax.set_ylabel('Topic number')\nax.set_title('25 most frequent topics')\n\nx_offset = -100\ny_offset = 0.3\n\nfor ind, bar in enumerate(plot):\n    \n    yloc = bar.get_y() + bar.get_height() \/ 2\n    width = int(bar.get_width())\n    ax.annotate('T1: '+', '.join(theme_keywords[ind][0].split(', ')[:3])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[4:7])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[8:10]), xy=(width+x_offset, yloc+y_offset), fontsize=10)\n\n\n\n#for ind, p in enumerate(ax.patches):\n   # b = p.get_bbox()\n    #val = \"{:.0f}\".format(b.x1)        \n    #ax.annotate('T1: '+', '.join(theme_keywords[ind][0].split(', ')[:4])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[5:10]), (5, b.y1 + y_offset), fontsize=4)\n    \n\nx_offset = 30\ny_offset = 0.3\nfor ind, bar in enumerate(plot):\n    #b = p.get_bbox()\n    \n    #val = \"{:.0f}\".format(b.x1) \n    yloc = bar.get_y() + bar.get_height() \/ 2\n    width = int(bar.get_width())\n    #ax.annotate('T2: '+', '.join(theme_keywords_second_topic[ind][0].split(', ')[:4])+'\\n'+', '.join(theme_keywords_second_topic[ind][0].split(', ')[5:10]), (500, b.y1 + y_offset), fontsize=4)\n    ax.annotate('T2: '+', '.join(theme_keywords_second_topic[ind][0].split(', ')[:3])+'\\n'+', '.join(theme_keywords_second_topic[ind][0].split(', ')[4:7])+'\\n'+', '.join(theme_keywords[ind][0].split(', ')[8:10]), xy=(width+x_offset, yloc+y_offset), fontsize=10)\n\n\n\nplt.show()","bea08208":"# Intro\nIn natural language processing, the **Latent Dirichlet Allocation** (LDA) is a generative statistical model that allows to divide a collection of texts into N-number of subgroups, where each subgroup is characterized by a set of X-number of keywords, and this set of keywords is associated with a topic. Both the topic (as a set of words) and the text (as a set of topics describing the text) are described by Dirichlet-distribution. \n\nLet's say we have three texts:<br>\n\"Dogs like playing.\"<br>\n\"Cats like milk.\"<br>\n\"Cats and dogs like eating and playing. I love dogs. They are adorable.\" <br>\n\nThe results from LDA model could be the following:<br>\n\nText1: 100% Topic1 + 0% Topic2<br>\nText2: 100% Topic2 + 0% Topic1<br>\nText3: 70% Topic2 + 30% Topic1<br>\n\nWhere each topic represented by a set of words (from most to least relevant), which forms the topic:<br>\n\nTopic1: 30% dog, 30% playing, 20% like 10% adorable 10% love<br>\nTopic2: 50% cat, 30% milk, 20% like<br>\n\n# When to apply LDA topic modeling\n\nWhen we have a collection of documents and wish to understand what the collection\/archive contains without necessarily reading every document.\n   - If we are working with a small number of documents (or even a single document), word frequency counts (or TF-IDF) might be sufficient in order to get an idea what the text is about. <br>\n   - However, if we have a large number of documents, then topic modeling might be a good approach.\n   \n# Theory: How LDA model works and what's behind it\nIn latent diriclet allocaton (LDA) model, each document is considered to be characterized by a set of topics that is following the Dirichlet distribution. \n\n\nIn LDA probabilistic topic modeling:\n- a collection of documents (texts) $D$ is given\n- each document $d$ from the collection is a sequence of words $W_{d} = (w_{1}, ..., w_{n_{d}})$ from dictionary $W$, where $n_{d}$ - length of document d. \n- each document may be related to one or several themes\n- order of documents in collection is not important\n- order of words in documents is neglected, each document is considered as \"bag of words\"\n- document collection is considered as set of pairs \"document-word\" $(d,w), d \\in D, w \\in W_{d}$\n- each topic $t\\in T$, where $T$ - set of topics, is described by Dirichlet-distribution $p(w|t)$ on the range of $w\\in W$, in other words there are topic vectors: $\\phi_{t} = (p(w|t):w \\in W)$\n- each document $d\\in D$ is described by Dirichlet-distribution $p(t|d)$ on the range of $t\\in T$, in other words there are document vectors: $\\theta_{d} = (p(t|d):t \\in T)$\n<br>\n\nProbability of a pair \"document-word\" to occure can be written as: \n$$\np(d,w)=\\sum\\limits_{t\\in T}p(t)p(w|t)p(d|t)\n$$\n\nTo build a topic model means to find matrices $\\Phi = ||p(w|t)||$ and $\\Theta = ||p(t|d)||$ given collection $D$.\n\nIn order to find a solution, we need to solve the optimization problem, i.e. to maximaze the function: \n$$\n\\sum\\limits_{d\\in D}\\sum\\limits_{w\\in d}n_{dw}logp(w|d)\\to\\max\\limits_{\\Phi,\\Theta},\n$$\nwhere $n_{dw}$ is frequency of word $w$ in document $d$.\n\n# The goals:\n\nThis notebook aimes to investigate the capabilities of LDA topic modelling techniques on Russian texts. The main goals will be:\n- to test LDA and Mallet LDA on Russian texts\n- to find whether filtering of most common and most rare words will increase model performance\n- to find most frequent topics(e.g. first 20) and their sets of keywords for the given collection of documents","b6d62f98":"- Each document can be described by a distribution of topics [T1 + T2 + T3 + ... + T150] and each topic can be described by a distribution of words, where T1 - topic with highest contribution, thus T1 considered as the prevalent topic of text.\n<br>\n- Hovewer it makes sence to look at the second or even third topic in order to get broader overview and better idea of what the text is about.\n<br>\n- In the graph, the most frequent topics (T1) are represented, as well as most frequent T2, following that T1. \n- In this test, words filtering by their tf-idf score (most rare and most common words) didn't lead to accuracy improvement. Hovewer more investigation is needed.\n- More investigation on optimizing the number of topics is required\n- Further word filtering might be required to improve accuracy of the model","a32d15f2":"# Mallet LDA Model","36a312bf":"# LDA Mallet with high and low tf-idf filtered out"}}