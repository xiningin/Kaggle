{"cell_type":{"fb1af5e6":"code","dc9e59a5":"code","11ab5f0d":"code","be9a22a4":"code","dde05409":"code","115aa372":"code","cbd51e21":"code","4e7ca402":"code","1743f76b":"code","9a517260":"code","9ff981ee":"code","d4b550db":"code","972cd6f5":"code","43d4a02b":"code","ce306197":"markdown","fc854216":"markdown","52166eba":"markdown","8c3259d8":"markdown","f24396f9":"markdown","ebdc2ea5":"markdown","fd116e72":"markdown","f0ea005e":"markdown","7ed6ea58":"markdown","32b1d5bb":"markdown","cd5ed865":"markdown","b3db37f7":"markdown","190d32a2":"markdown","df9106f2":"markdown","940dc3b6":"markdown","b36cc97f":"markdown","c73b0e06":"markdown","b2399736":"markdown","96454179":"markdown","07ba3533":"markdown","5df61ffe":"markdown","a0cdb0f1":"markdown"},"source":{"fb1af5e6":"import numpy as np\nimport pandas as pd\n\n# For visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.interpolate as interp\n\n# For BigQuery client\nfrom google.cloud import bigquery\n\n# For Geospatial\nimport geopandas as gpd     \n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster, FloatImage\n\n# For data parsing\nimport calendar\nimport math\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='1000px')\n        \nclient = bigquery.Client()","dc9e59a5":"# Dataset addess\nproject = 'bigquery-public-data'\ndataset_ref = 'san_francisco_bikeshare'\ntable_trips_ref = 'bikeshare_trips'\ntable_station_ref = 'bikeshare_station_info'\ntrips_table = f'{project}.{dataset_ref}.{table_trips_ref}'\nstation_table = f'{project}.{dataset_ref}.{table_station_ref}'\n\n# Used columns\ntrips_cols = ['trip_id', 'duration_sec', 'start_date', 'start_station_name', 'end_date', 'end_station_name', 'subscriber_type', \n        'start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'member_birth_year']\ntrips_cols_wo_lat_long = ['trip_id', 'duration_sec', 'start_date', 'start_station_name', \n                    'end_date', 'end_station_name', 'subscriber_type', 'member_birth_year']\n\n# Base Query\ndef query_base(query):\n    return f\"\"\"\n    WITH r_sf AS (\n    SELECT name, region_id, lat, lon\n    FROM {station_table}\n    ),\n    t_s AS (\n        SELECT \n        {', '.join(trips_cols_wo_lat_long)}, \n        r_sf.lat AS start_station_latitude, \n        r_sf.lon AS start_station_longitude,\n        FROM {trips_table} AS b_t\n        INNER JOIN r_sf\n            ON r_sf.name = b_t.start_station_name\n        WHERE r_sf.region_id = 3\n    ), \n    t_e AS (\n        SELECT \n        t_s.*,\n        r_sf.lat AS end_station_latitude, \n        r_sf.lon AS end_station_longitude,\n        FROM t_s\n        INNER JOIN r_sf\n            ON r_sf.name = t_s.end_station_name\n        WHERE r_sf.region_id = 3\n            AND (EXTRACT(YEAR FROM start_date) = 2017 OR EXTRACT(YEAR FROM start_date) = 2018)\n    )\n    {query}\n    \"\"\"","11ab5f0d":"query_trips = f'''\nSELECT *\nFROM t_e\n'''\n\ntrips_df = client.query(query_base(query_trips)).to_dataframe()\nprint(f'Shape of the data \\n\\\nrows: {trips_df.shape[0]}, cols: {trips_df.shape[1]}')","be9a22a4":"trips_df.select_dtypes(include=['datetime64[ns, UTC]']).describe(datetime_is_numeric=True)","dde05409":"trips_df.select_dtypes(include=['object']).describe()","115aa372":"trips_df.select_dtypes(include=['int64', 'float64']).describe()","cbd51e21":"sf_buildings_df = pd.read_csv('..\/input\/san-francisco-buildings-energy-performance\/sf_buildings.csv').loc[:, ['Building Name', 'Building Address', 'Postal Code', 'Property Type', 'Latitude', 'Longitude']]\nprint(f'Shape of the data \\n\\\nrows: {sf_buildings_df.shape[0]}, cols: {sf_buildings_df.shape[1]}')","4e7ca402":"sf_buildings_df.select_dtypes(include=['object']).describe()","1743f76b":"sf_buildings_df.select_dtypes(include=['int64', 'float64']).describe()","9a517260":"# colors_format = ['#', '#', '#', '#', '#']\ncolors_blue = ['#133c55', '#386fa4', '#59a5d8', '#84d2f6', '#91e5f6']\ncolors_dark = [\"#1f1f1f\", \"#313131\", '#636363', '#aeaeae', '#dadada']\ncolors_red = ['#ff0a54', '#ff5c8a', '#ff85a1', '#fbb1bd', '#f7cad0']\ncolors_bright_pastel = ['#ee6055', '#60d394', '#aaf683', '#ffd97d', '#ff9b85']\n\nfont_title = {'fontname': 'monospace',\n        'color':  colors_dark[1],\n        'weight': 'bold',\n        'size': 18,\n        }\n\nfont_title_sub = {'fontname': 'monospace',\n        'color':  colors_dark[1],\n        'weight': 'semibold',\n        'size': 16,\n        }\n\nfont_axis_label = {\n        'fontstyle': 'italic',\n        'color':  'black',\n        'weight': 'bold',\n        'size': 12,\n        }\n\nsns.palplot(colors_blue)\nsns.palplot(colors_red)\nsns.palplot(colors_dark)\nsns.palplot(colors_bright_pastel)","9ff981ee":"# Data Query: Total Bikeshare 2017 - 2018\nquery_monthly_17_18 = f\"\"\"\nSELECT \n    EXTRACT(YEAR FROM t_e.start_date) AS year,\n    EXTRACT(MONTH FROM t_e.start_date) AS month,\n    COUNT(*) AS total_trips\nFROM t_e\nGROUP BY year, month\nORDER BY year, month\n\"\"\"\n\nbikeshare_monthly_17_18 = client.query(query_base(query_monthly_17_18)).to_dataframe().set_index([pd.Index(range(1,12))])\n\n# Add new columns: date (year-month)\ndef concat_year_month(row):\n    return f'{int(row.year)} - {calendar.month_abbr[int(row.month)]}'\n\nbikeshare_monthly_17_18['date'] = bikeshare_monthly_17_18.apply(concat_year_month, axis='columns')\n\n# Data Interpolation: Total Bikeshare 2017 - 2018\nmonth_new = np.linspace(1, 11, 11*6)\na_BSpline = interp.make_interp_spline(bikeshare_monthly_17_18.index.values, bikeshare_monthly_17_18.total_trips)\nbikeshare_monthly_17_18_interp = a_BSpline(month_new)\n\n# Data Plot: Total Bikeshare 2017 - 2018\nplt.figure(figsize=(15, 5))\nsns.lineplot(x=month_new, y=bikeshare_monthly_17_18_interp, ls='-.',alpha=0.7,color=colors_blue[1])\nsns.scatterplot(x=bikeshare_monthly_17_18.index.values, y=bikeshare_monthly_17_18.total_trips, legend=False, palette=[colors_blue[0]])\\\n                .set(xlabel= '', xlim=(0.9, 11.1))\nplt.title('Total Bikeshare Trips 2017 - 2018', fontdict=font_title, pad=20)\nplt.minorticks_on()\nplt.xticks(range(1, 12), list(bikeshare_monthly_17_18.date.array), fontsize=10)\nplt.show()","d4b550db":"query_daily = f\"\"\"\n,\ntrips_by_day AS (\n    SELECT \n        EXTRACT(DAYOFYEAR FROM start_date) AS day_of_year,\n        EXTRACT(DAYOFWEEK FROM start_date) AS day_of_week,\n        COUNT(trip_id) AS total_trips \n    FROM t_e\n    GROUP BY day_of_year, day_of_week\n    ORDER BY day_of_year, day_of_week\n)\nSELECT day_of_week, \n    AVG(total_trips) AS average_total_trips\nFROM trips_by_day\nGROUP BY day_of_week\nORDER BY day_of_week\n\"\"\"\n\nquery_hourly_weekdays = f\"\"\"\n,\ntrips_by_hour AS (\n    SELECT \n        EXTRACT(DAYOFWEEK FROM start_date) AS day_of_week,\n        EXTRACT(HOUR FROM start_date) AS start_hour,\n        COUNT(trip_id) AS total_trips \n    FROM t_e\n    GROUP BY day_of_week, start_hour\n    ORDER BY day_of_week, start_hour\n)\nSELECT start_hour, \n    AVG(total_trips) AS average_total_trips\nFROM trips_by_hour\nWHERE day_of_week != 1\n    OR day_of_week != 7\nGROUP BY start_hour\nORDER BY start_hour\n\"\"\"\n\nquery_hourly_weekends = f\"\"\"\n,\ntrips_by_hour AS (\n    SELECT \n        EXTRACT(DAYOFWEEK FROM start_date) AS day_of_week,\n        EXTRACT(HOUR FROM start_date) AS start_hour,\n        COUNT(trip_id) AS total_trips \n    FROM t_e\n    GROUP BY day_of_week, start_hour\n    ORDER BY day_of_week, start_hour\n)\nSELECT start_hour, \n    AVG(total_trips) AS average_total_trips\nFROM trips_by_hour\nWHERE day_of_week = 1\n    OR day_of_week = 7\nGROUP BY start_hour\nORDER BY start_hour\n\"\"\"\n\nquery_age_groups = f\"\"\"\nSELECT \n    CASE\n        WHEN member_birth_year IS NULL THEN 'Unknown'\n        WHEN (2021 - member_birth_year) <= 22 THEN '\u2264 23'\n        WHEN (2021 - member_birth_year) <= 64 THEN '24 - 64'\n        ELSE '\u2265 65'\n    END age_group,\n    COUNT(trip_id) AS total\nFROM t_e\nWHERE EXTRACT(YEAR FROM start_date) = 2017\n    OR EXTRACT(YEAR FROM start_date) = 2018\nGROUP BY age_group\n\"\"\"\n\ndef f_to_c(row):\n    return (row.Value - 32) * 5\/9\n\nbikeshare_daily = client.query(query_base(query_daily)).to_dataframe().set_index('day_of_week')\n\nage_groups = client.query(query_base(query_age_groups)).to_dataframe().set_index('age_group').reindex(['\u2264 23', '24 - 64', '\u2265 65', 'Unknown'])\n\nbikeshare_hourly_weekdays = client.query(query_base(query_hourly_weekdays)).to_dataframe().set_index('start_hour')\nbikeshare_hourly_weekends = client.query(query_base(query_hourly_weekends)).to_dataframe().set_index('start_hour')\n\ndays = [day for day in calendar.day_abbr]\ndays.insert(0, days.pop())\n\n# Bikeshare 2015 - Daily\nfig, ax = plt.subplots(1, 2,figsize=(12, 5))\nfig.suptitle('Trips Summary', fontsize=18, weight='bold', fontdict=font_title, y=1.2, x=0.56)\n\nsns.barplot(ax=ax[0], x=bikeshare_daily.index, y=bikeshare_daily.average_total_trips, alpha=0.8,color=colors_blue[3])\nsns.despine(ax=ax[0], top=True)\nplt.sca(ax[0])\nplt.title('Average Daily Trips', fontdict=font_title_sub, pad=20, alpha=0.8)\nplt.ylabel('')\nplt.xlabel('')\nplt.xticks(range(0, 7), list(days), fontsize=10)\n\n# user's Age \nplt.sca(ax[1])\nplt.pie(age_groups.total,autopct='{:.1f}%'.format,colors=colors_bright_pastel,wedgeprops={'width':0.5,'alpha':0.7},\n          pctdistance=1.18,radius=1.1,textprops={'fontsize': 9, 'color': colors_dark[2],'fontweight': 'bold'},\n         counterclock=False)\nplt.legend(age_groups.index,frameon=False,bbox_to_anchor=(1.5,0.9),labelspacing=3,title='Age Interval')\nplt.title('User\\'s Age', fontdict=font_title_sub, pad=20, alpha=0.85)\nplt.show()\n\nhours_new = np.linspace(0, 23, 24*10)\n# Data Interpolation: Bikeshare - Hourly (Weekdays)\na_BSpline = interp.make_interp_spline(bikeshare_hourly_weekdays.index.values, bikeshare_hourly_weekdays.average_total_trips)\naverage_trips_new_weekdays = a_BSpline(hours_new)\n\n# Data Interpolation: Bikeshare - Hourly (Weekends)\na_BSpline = interp.make_interp_spline(bikeshare_hourly_weekends.index.values, bikeshare_hourly_weekends.average_total_trips)\naverage_trips_new_weekends = a_BSpline(hours_new)\n\n# Bikeshare - Hourly\nplt.figure(figsize=(15, 5))\nsns.lineplot(x=hours_new, y=average_trips_new_weekdays, ls='-.',alpha=0.7,color=colors_blue[1], label='Weekdays')\nsns.lineplot(x=hours_new, y=average_trips_new_weekends, ls='-.',alpha=0.7,color=colors_red[2], label='Weekends')\nsns.scatterplot(data=bikeshare_hourly_weekdays, legend=False, palette=[colors_blue[0]])\\\n                .set(xlabel= '', xlim=(-0.1, 23.1))\nsns.scatterplot(data=bikeshare_hourly_weekends, legend=False, palette=[colors_red[0]])\\\n                .set(xlabel= '', xlim=(-0.1, 23.1))\nsns.despine(top=True)\nplt.title('Average Hourly Trips', fontdict=font_title_sub, pad=20, alpha=0.85)\nplt.xticks([0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 23], fontsize=10)\nplt.legend()\nplt.show()","972cd6f5":"def int_to_str(row, col):\n    row[col] = str(row[col])\n    return row\n\n\ndef round_float(row, col):\n    row[col] = round(row[col])\n    return row\n\n\nsf_buildings_df = pd.read_csv('..\/input\/san-francisco-buildings-energy-performance\/sf_buildings.csv').apply(int_to_str, args=['Postal Code'], axis='columns')\nca_areas = gpd.read_file('..\/input\/bay-area-geospatial-data\/ark28722-s7888q-shapefile\/s7888q.shp')\nsf_postal_codes = sf_buildings_df['Postal Code'].unique()\n\n\n\n# PART - 1: Data Query\ndef query_weekdays_prime(clock_i, clock_f):\n    return f'''\n,\nweekdays_trips AS (\n    SELECT\n        EXTRACT(DAYOFYEAR FROM start_date) AS day,\n        start_station_name, end_station_name,\n        start_station_latitude, start_station_longitude, \n        end_station_latitude, end_station_longitude\n    FROM t_e\n    WHERE (EXTRACT(DAYOFWEEK FROM start_date) BETWEEN 2 AND 6)\n        AND (EXTRACT(HOUR FROM start_date) BETWEEN {clock_i} AND {clock_f})\n)\n,\ns_t AS (\n    SELECT\n        start_station_name as station_name,\n        (COUNT(*) \/ COUNT(DISTINCT day))  as avg_total_start,\n        start_station_latitude AS latitude, start_station_longitude AS longitude\n    FROM weekdays_trips\n    GROUP BY station_name, latitude, longitude\n)\n,\ne_t AS (\n    SELECT \n        end_station_name AS station_name,\n        (COUNT(*) \/ COUNT(DISTINCT day)) as avg_total_end\n    FROM weekdays_trips\n    GROUP BY end_station_name\n)\nSELECT \n    s_t.station_name, \n    s_t.avg_total_start, \n    e_t.avg_total_end, \n    s_t.latitude, \n    s_t.longitude\nFROM s_t\nINNER JOIN e_t\n    ON s_t.station_name = e_t.station_name\nORDER BY s_t.avg_total_start, e_t.avg_total_end\n'''\n\n\n# PART - 1.1. WeekDays (Morning)\nweekdays_m_df = client.query(query_base(query_weekdays_prime(6, 10))).to_dataframe()\\\n    # .apply(round_float, args=['avg_total_start'], axis='columns')\\\n    # .apply(round_float, args=['avg_total_end'], axis='columns')\n# PART - 1.2. WeekDays (Evening)\nweekdays_e_df = client.query(query_base(query_weekdays_prime(15, 19))).to_dataframe()\\\n    # .apply(round_float, args=['avg_total_start'], axis='columns')\\\n    # .apply(round_float, args=['avg_total_end'], axis='columns')\n\n# PART - 2: Choropleth maps\nca_areas = ca_areas[['ZIP', 'geometry']]\nsf_areas = ca_areas[ca_areas['ZIP'].isin(sf_postal_codes)].set_index('ZIP')\nplot_dict = sf_buildings_df['Postal Code'].loc[sf_buildings_df['Property Type'] == 'Commercial'].value_counts()\n\n\ndef apply_choropleth(geo_data_df, data_df, map_name):\n    Choropleth(\n        control=False,\n        geo_data=geo_data_df.__geo_interface__,\n        data=data_df,\n        key_on=\"feature.id\",\n        fill_color='Purples',\n        line_color='#393e46',\n        line_opacity=0.5,\n        legend_name='Commercial Buildings'\n    ).add_to(map_name)\n\n\nm_m_e = folium.plugins.DualMap(location=[37.7782797, -122.41098671765084], tiles=None, zoom_start=13, layout='vertical')\n\napply_choropleth(sf_areas, plot_dict, m_m_e)\n\n\n# PART - 3: Circle Marker\ndef origin_destination_circle(df, map_name):\n    depart_trips = folium.FeatureGroup(name='Origin')\n    arrive_trips = folium.FeatureGroup(name='Destination', show=False)\n    map_name.add_child(depart_trips)\n    map_name.add_child(arrive_trips)\n\n#     o_color = '#da723c'\n#     d_color = '#45526c'\n    o_color = colors_red[0]\n    d_color = colors_blue\n    p_color = '#000000'\n    f_size = '10'\n\n    for i in range(len(df)):\n\n        o_num_text = f\"<span style=\\\"font-size:{f_size}px; color:{p_color}\\\">{round(df['avg_total_start'][i])}<\/span>\"\n        d_num_text = f\"<span style=\\\"font-size:{f_size}px; color:{p_color}\\\">{round(df['avg_total_end'][i])}<\/span>\"\n        if round(df['avg_total_start'][i]) > round(df['avg_total_end'][i]):\n            o_num_text = f\"<span style=\\\"font-size:{f_size}px; color:{o_color}\\\"><b>{round(df['avg_total_start'][i])}<\/b><\/span>\"\n        elif round(df['avg_total_start'][i]) < round(df['avg_total_end'][i]):\n            d_num_text = f\"<span style=\\\"font-size:{f_size}px; color:{d_color[3]}\\\"><b>{round(df['avg_total_end'][i])}<\/b><\/span>\"\n\n        # Departure\n        map_name.keep_in_front(Circle(\n            location=[df['latitude'].iloc[i], df['longitude'].iloc[i]],\n            radius=(math.log(df['avg_total_start'].iloc[i]) * 75),\n            color='b',\n            popup=(\n                f\"<b style=\\\"font-size:{f_size}px;\\\">Station Name<\/b><br>\"\n                f\"<span style=\\\"font-size:{f_size}px; white-space:nowrap;\\\">{df['station_name'][i]}<\/span><br><br>\"\n                f\"<b style=\\\"font-size:{f_size}px; white-space:nowrap;\\\">Avg. Departure : Arrival<\/b><br>\"\n                f\"{o_num_text}\"\n                f\" : \"\n                f\"{d_num_text}\"\n            ),\n            fill_color=o_color,\n            fill=True,\n            fill_opacity=0.7\n        ).add_to(depart_trips))\n\n        # Arrival\n        map_name.keep_in_front(Circle(\n            location=[df['latitude'].iloc[i], df['longitude'].iloc[i]],\n            radius=(math.log(df['avg_total_end'].iloc[i]) * 75),\n            color='b',\n            popup=(\n                f\"<b style=\\\"font-size:{f_size}px;\\\">Station Name<\/b><br>\"\n                f\"<span style=\\\"font-size:{f_size}px; white-space:nowrap;\\\">{df['station_name'][i]}<\/span><br><br>\"\n                f\"<b style=\\\"font-size:{f_size}px; white-space:nowrap;\\\">Avg. Departure : Arrival<\/b><br>\"\n                f\"{o_num_text}\"\n                f\" : \"\n                f\"{d_num_text}\"\n            ),\n            fill_color=d_color[2],\n            fill=True,\n            fill_opacity=0.8\n        ).add_to(arrive_trips))\n\n    folium.LayerControl(collapsed=False).add_to(map_name)\n\nfolium.TileLayer('cartodbpositron', name='06:00 - 10:00').add_to(m_m_e.m1)\nfolium.TileLayer('cartodbpositron', name='15:00 - 19:00').add_to(m_m_e.m2)\n    \norigin_destination_circle(weekdays_m_df, m_m_e.m1)\norigin_destination_circle(weekdays_e_df, m_m_e.m2)\n\nurl = ('https:\/\/i.ibb.co\/tYdwWxj\/legend-geospatial-bikeshare.png')\nFloatImage(url, bottom=77, left=0).add_to(m_m_e)\n\nembed_map(m_m_e, 'm_m_e.html')","43d4a02b":"query_subscriber_type = \"\"\"\nSELECT \n    EXTRACT(YEAR FROM start_date) AS year,\n    EXTRACT(MONTH FROM start_date) AS month,\n    SUM(\n        CASE\n            WHEN subscriber_type = 'Subscriber' THEN 1\n            ELSE 0\n        END\n    ) AS total_subscriber_user,\n    SUM(\n        CASE\n            WHEN subscriber_type = 'Subscriber' THEN 0\n            ELSE 1\n        END\n    ) AS total_non_subscriber_user\nFROM t_e\nGROUP BY year, month\nORDER BY year, month\n\"\"\"\nsubscriber_type_df = client.query(query_base(query_subscriber_type)).to_dataframe().set_index([pd.Index(range(1,12))])\n\ndef subs_percentage(row):\n    return (row.total_subscriber_user \/ (row.total_subscriber_user + row.total_non_subscriber_user) * 100)\n\ndef non_subs_percentage(row):\n    return (100 - row.subscriber_pecentage)\n\ndef concat_year_month_n(row):\n    return f'{int(row.year)} \\n {calendar.month_abbr[int(row.month)]}'\n\nsubscriber_type_df['subscriber_pecentage'] = subscriber_type_df.apply(subs_percentage, axis='columns')\nsubscriber_type_df['non_subscriber_pecentage'] = subscriber_type_df.apply(non_subs_percentage, axis='columns')\nsubscriber_type_df['date'] = subscriber_type_df.apply(concat_year_month, axis='columns')\nsubscriber_type_df['date_n'] = subscriber_type_df.apply(concat_year_month_n, axis='columns')\n\nmonth_new = np.linspace(1, 11, 11*6)\na_BSpline = interp.make_interp_spline(subscriber_type_df.index.values, subscriber_type_df.subscriber_pecentage)\nsubscriber_pecentage_interp = a_BSpline(month_new)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nplt.text(x=0.5, y=0.94, s=\"Total User by Subscription Types\", fontdict=font_title_sub, ha=\"center\", transform=fig.transFigure)\n# plt.suptitle('Total User by Subscription Types', fontdict=font_title_sub, alpha=0.8)\n\nplt.sca(ax[0])\nsns.scatterplot(x=subscriber_type_df.date_n, y=subscriber_type_df.subscriber_pecentage, alpha=1, color=colors_blue[0])\nsns.lineplot(x=subscriber_type_df.date_n, y=subscriber_type_df.subscriber_pecentage, color=colors_blue[3], legend='brief',label=\"Subscriber\")\nplt.ylabel('%')\nplt.yticks(range(60,101, 20))\nplt.xlabel('date')\n\nplt.sca(ax[1])\nsns.scatterplot(x=subscriber_type_df.date_n, y=subscriber_type_df.non_subscriber_pecentage, alpha=1, color=colors_red[0])\nsns.lineplot(x=subscriber_type_df.date_n, y=subscriber_type_df.non_subscriber_pecentage, color=colors_red[3], legend='brief',label=\"Non-Subscriber\")\nplt.ylabel('%')\nplt.yticks(range(0,41, 20))\nplt.xlabel('')\nplt.show()\n\nplt.figure(figsize=(15, 5))\nsns.barplot(x=subscriber_type_df.date, y=(subscriber_type_df.total_subscriber_user + subscriber_type_df.total_non_subscriber_user), alpha=0.9, color=colors_blue[3], label='Subscriber')\nsns.barplot(x=subscriber_type_df.date, y=subscriber_type_df.total_non_subscriber_user, alpha=1, color=colors_red[0], label='Non-Subscriber')\nplt.legend(loc='upper right', bbox_to_anchor=(1, 1.15))\nplt.ylabel('')\nplt.xticks(range(0, 11), list(subscriber_type_df.date.array), fontsize=10)\nplt.xlabel('')\n\nsns.despine(bottom=True)\nplt.show()","ce306197":"The things that we want to check in the data are the presence of null value and the data itself in general. For this, we can use `.describe()` method, and the filtered data has 589704 rows. \n\nIt can be seen that there is no null value in all of the columns except for the `member_birth_year` column that has `523176` data, it means that there are `66528` null values. The null values in `member_birth_year` are acceptable because not all of the people want to disclose this data. Later, to tackle this problem, the null values in `member_birth_year` can be set as 'unknown'.\n\n","fc854216":"# Conclusion\n\nFrom the analysis above. It can be concluded that in between 2017-2018:\n\n- Generally, the San Francisco bike-share trips **number was increasing**.\n- **Majority of the bike-share user** in San Francisco were **in productive age**.\n- The bike-share trips number in **weekdays** was **higher** compared to the number of trips in **weekends**.\n- In weekdays, The **peak hour** happened **in the morning (06:00-10:00)** and **in the afternoon (15:00-19:00)**\n- Most of the user were using bike-share trips for working related\n- Although **by percentage** portion, the **subscriber** user was **declining**. The **total number** of subscriber user was **increasing**.","52166eba":"---","8c3259d8":"## When was the high usage of bikeshare? - Part 2\n\nSo, let's have a look on the average trips per day. For the Sunday and Saturday, the average trips number were around 1000. And, for Monday to Friday, the trips number were around 2000. So, The average trips number on weekdays was 2 times higher compared to the weekend.\n\n### Why the weekdays trips were higher compared to the weekend?\n\nTo answer this question, let's see the user's age distribution for a while. More than 80% of the user were around 24-64, 11.3% was unknown, and  below 5% user were \u2264 23 and \u2265 65. It can be seen that the majority of the bike-share user were in productive age. Commonly, the productive age is working in weekdays, and resting on weekends. So, it might be true if many of bike-share user's were using it for working-related.\n\n\n### Is the statment of user was using bike-share for work correct?\n\nTo make sure this, we will investigate the average hourly trips data and the start and end location of the trips later on.\n\n#### 1. Prime Hour\nIf we see the average hourly trips graph, the trips number in weekdays was rising for some periods of hour; in the morning at around 06:00 to 10:00 and in the afternoon at 15:00 to 19:00. And, for the rest of the time periods, the trips number were quite the same as the trips number on weekend.\n","f24396f9":"# Data Overview\nThere are 3 datasets that used in this EDA. `san_francisco_bikeshare`, San Francisco Buildings Energy Performance, and Bay Area Geospatial data.\n\n### 1. San Francisco Bikeshare Dataset\n`san_francisco_bikeshare` is a BigQuery dataset that contains the information of bike trips in San Francisco, San Jose, Oakland, Emeryville, and Berkeley in between 2013 and 2018. And, it should be noted that the bike-share in San Francisco (Bay Area) was started on 2017.\n\nThe dataset itself consists of 4 tables:\n- `bikeshare_regions`\n- `bikeshare_station_info`\n- `bikeshare_station_status`\n- `bikeshare_trips`\n\nBecause we want to observe the bikeshare trips that happened in San Francisco area, we have to use `bikeshare_station_info` for filtering the `bikeshare_trips` data table based on the trips station.\n\n\n#### 1.1. bikeshare_station_info\n`bikeshare_station_info` has 12 columns, and we are going to use 2 of it's columns:\n- `name`: Station Name\n- `region_id` : Numeric region id (San Francisco id is `3`)\n- `lat` : Latitude\n- `lon` : Longitude\n\n#### 1.2. bikeshare_trips\nIt records information that described into columns. However, this analysis only use several columns, namely:\n- `trip_id` : Numeric ID of bike trip\n- `duration_sec` : Time of trip in seconds\n- `start_date` : Start date of trip with date and time, in PST\n- `start_station_name` : Station name of start station\n- `end_date` : End date of trip with date and time, in PST\n- `end_station_name` : Station name for end station\n- `start_station_latitude`\n- `start_station_longitude`\n- `end_station_latitude`\n- `end_station_longitude`\n- `subscriber_type` : Subscriber = annual or 30-day member; Customer = 24-hour or 3-day member\n- `member_birth_year`\n\n### 2. San Francisco Buildings Energy Performance\nSan Francisco Buildings Energy Performance is acquired from the [DataSF](https:\/\/datasf.org\/). Originally, this data contains the energy usage for every building in San Francisco. But, We will use this data to know the location of the office building in San Francisco. So, we only take several columns from it:\n\n- `Building Name`\n- `Building Address`\n- `Postal Code`\n- `Property Type`\n- `Latitude`\n- `Longitude`\n\n### 3. Bay Area Geospatial Data\nThis is the Bay Area geospatial area data based on the postal code that is acquired from the [UC Berkeley GeoData Repository](https:\/\/geodata.lib.berkeley.edu\/). A unique value for each areas (multi-polygon \/ polygon) are based on the ZIP codes.","ebdc2ea5":"# Closing\n\nThank you for visiting\/catch a glimpse on this notebook. If there is any suggestion or inquiry related to this notebook or anything else, please don't hestitate to reach me.\n\nLastly, I would like to thank to the authors of these inspirational notebooks:\n- https:\/\/www.kaggle.com\/geometrein\/helsinki-city-bikes-eda\n- https:\/\/www.kaggle.com\/jaykumar1607\/heart-attack-advanced-visualizations-modelling","fd116e72":"---","f0ea005e":"---","7ed6ea58":"## When was the high usage of bikeshare? - Part 1\n\nLet's have a look on the total bikeshare trips for each month. At the first of the bike-share operation, the trips number was around 2000. But, in the following month, it was soaring to 15 fold. And, the trips number are stagnantly increasing for 3 next consecutive months.\n\nIn between November 2017 to December 2017, the trips number were having decrement 5000 ~ 10,000 compared to the trips on the previous month that achieved 67075 trips.\n\nIn 2018, although the trips was thriving again. the growth was not fast as the previous increment in 2017. However, in April 2018, the number was skyrocketing at 78956 trips.\n\n\n### Why the trips were dropping in the November and December 2017?\n\nThere is a hypothetical factor that can made the trips number for November and December 2017 decreased.\n\nWe know that many holidays happens at the end of the year. Naturally on this period, many workers is taking on-leave. But, was the majority of the bikeshare users in San Francisco were workers? we will discuss about it on the next part.","32b1d5bb":"---","cd5ed865":"---","b3db37f7":"# Importing Libraries\n\nIn general, we are going to use 4 groups of libraries that have specific purpose: visualization, query, geospatial, and date parsing.\n","190d32a2":"#### Is the statment of user was using bike-share for work correct? - conclusion\n\nSo, based on the 2 observation, the statement might be true that many of the bike-share users were using it for work-related, especially for working.","df9106f2":"___","940dc3b6":"## Was the number of subscriber user thriving?\n\nBy plotting the total number of bike-share user monthly that grouped into the subscriber types, we can see the increment and\/or decrement for each groups.\n\nIt can be seen that, at the first month of the San Francisco bike-share operations, more than 80% of the users were subscriber. And, the following month, the percentage of the subscriber user was dropped by around 10%.\n\nAnd for the following month until January 2018, the percentage of the subscription user was stepping up. Lastly, From January to April 2018, the percentage of the subscription user were declining in small margin, and for non-subscriber user were inclining also in small margin.\n\nAlthough, in some month\/periods the percentage of the subscriber user were declining. It is a common thing for a new thing that many people want to give it a shot at first, and if they like it, they want to subscribe it. Moreover the number of the user was increasing, which means that many of them can be converted into a subscriber user.","b36cc97f":"---","c73b0e06":"# Styling","b2399736":"# Data Analysis","96454179":"# Introduction\n![160512_bike_work_100_1.jpg](attachment:33f49b6e-15be-4af2-ace4-ab401ed80302.jpg)\n[Cycling in San Francisco](https:\/\/www.sfmta.com\/bicycling-san-francisco)\n\n> Cycling has thrived in recent years in many countries. One of the best cities for cycling is San Francisco. In 2016, it was nominated as the second friendliest city for cycling in the US.\n\nIn this notebook, the author tries to do exploratory data analysis (EDA) for the **San Francisco Bikeshare In Mid 2017 to Mid 2018**.","07ba3533":"# Checking the Data\n\n### 1. bikeshare_trips\nAs already mentioned before, since we only focus on the trips that start and end in San Francisco. Before we check the data, we have to filter the data trips that happened in between 2017 and 2018, and also it starts and ends in San Francisco (So, we are neglecting the trips that starts from San Francisco but end outside it and vice-versa). This function below will be used as the base query, so we don't have to write it down again if we need to filter the data again.","5df61ffe":"### 2. San Francisco Buildings Energy Performance\n\nThe data checking in this dataset is the same with the previous data; checking the null value presence and using `.describe()` method to scan it. Before doing it, we can filter out the unused columns.\n\nIt can be seen that, there is no null value. Although in `Building Name` and also `Building Address` columns, there are the data that has the same value, we will not drop it because it might be an office that has the same latitude and longitude but in different position, because there are some latitude and longitude data that gathered from google maps and it can't get the specific location of the building by using the address. (for instance there are 2 office that has the same address but the first office located in front of the second office)","a0cdb0f1":"#### 2. Trips Origin-Destination During Prime Hour (Weekday)\n\nIt can be seen that in the morning, the trips that started from the area that had less commercial building was quite high. And, their destination were dominated by the station that located in high-density commercial building areas.\n\nHowever, this case was altered for the afternoon time period. Many of the trips were started from the station that located on the high-density commercial building areas, and the trips that had destination to station that located on the low-density commercial building areas were quietly high.\n\nFor instance, The Powell St BART Station (Market St at 4th St) that located on the area that had many commercial buildings. In the morning period, the average departure and arival ratio was 12:24, and in the afternoon period, the ratio was 28:20. And for comparison, The Jersey St at Castro St station that located in the area that had less number of commercial buildings, in the morning, the ratio was 7:1, and in the afternoon was 2:3\n\nIn a nutshell, in the morning, many people were going to the station that located, and some of them were originated from the station that located on the area that had less commercial buildings (it is a common thing that people tend to choose housing that located outside the commercial buildings since the price is not higher as the housing that located near the commercial buildings, although this statement has to be verified any further)."}}