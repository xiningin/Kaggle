{"cell_type":{"5feafd71":"code","ed9d0755":"code","fcf49383":"code","359f1947":"code","3b7a886c":"code","7ba3f075":"code","28b6237e":"code","23c70817":"code","9974fa64":"code","1bfe95d9":"code","ecbdc7ea":"code","00c2ee84":"code","ce222c34":"code","653166cf":"code","3367e90d":"code","3fb0d0a9":"code","3acaebf5":"code","e48b96bc":"code","f1d679bf":"code","44137b0a":"code","b8073f5a":"code","483cf10c":"code","3f1a0ccd":"markdown","5a318092":"markdown","43bc7cd2":"markdown","ccb1fced":"markdown","d7dce5f9":"markdown","50b3a0ea":"markdown","9a322303":"markdown","9c18bfab":"markdown"},"source":{"5feafd71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed9d0755":"!pip install pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom tqdm import tqdm\nimport pandas as pd\n\ndef load_raw():\n    #Create Spark Session\n    spark = SparkSession.builder \\\n        .master('local') \\\n        .appName('myAppName') \\\n        .config('spark.executor.memory', '12gb') \\\n        .config(\"spark.cores.max\", \"10\") \\\n        .getOrCreate()\n\n    #Get spark context\n    sc = spark.sparkContext\n\n\n    sqlContext = SQLContext(sc)\n    \n    df=pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\n    \n    \n    df['Respiratory Syncytial Virus']=df['Respiratory Syncytial Virus'].astype(str)\n    df['Influenza A']=df['Influenza A'].astype(str)\n    df['Influenza B']=df['Influenza B'].astype(str)\n    df['Parainfluenza 1']=df['Parainfluenza 1'].astype(str)\n    df['CoronavirusNL63']=df['CoronavirusNL63'].astype(str)\n    df['Rhinovirus\/Enterovirus']=df['Rhinovirus\/Enterovirus'].astype(str)\n    df['Coronavirus HKU1']=df['Coronavirus HKU1'].astype(str)\n    \n    for column in df.columns:\n        df[column]=df[column].astype(str)\n    \n    df=sqlContext.createDataFrame(df)\n    \n    \n    \n   \n    \n    \n    return df,sqlContext","fcf49383":"df,sqlContext=load_raw()","359f1947":"print('Number of lines ',df.count())","3b7a886c":"df.printSchema()","7ba3f075":"df=df.fillna(0)\nfrom pyspark.sql.functions import *\ndf=df.replace(\"nan\", \"0\")\npd.DataFrame(df.head(5),columns=df.schema.names)","28b6237e":"df_hemoglobin=df.select(\"Hemoglobin\").toPandas()\ndf_hemoglobin['Hemoglobin']=pd.to_numeric(df_hemoglobin['Hemoglobin'])\ndf_hemoglobin['Hemoglobin'].hist()","23c70817":"df.select(\"SARS-Cov-2 exam result\").show()","9974fa64":"df_=df.select(col(\"SARS-Cov-2 exam result\").alias(\"result\"),col('Patient age quantile').alias('age'))\ndf_.show()","1bfe95d9":"df_.select(\"result\", \"age\").write.mode('overwrite').option(\"header\", \"true\").save(\"result_age.parquet\", format=\"parquet\")","ecbdc7ea":"df_ = sqlContext.sql(\"SELECT * FROM parquet.`\/kaggle\/working\/result_age.parquet`\")","00c2ee84":"df_.printSchema()","ce222c34":"import pyspark.sql.functions as func","653166cf":"df_.groupBy(\"result\").agg(func.max(\"age\"), func.avg(\"age\")).show()","3367e90d":"df_pandas_age=df_.groupBy(\"result\").agg(func.max(\"age\"), func.avg(\"age\")).toPandas()\ndf_pandas_age.plot()","3fb0d0a9":"from pyspark.sql.types import IntegerType\ncolumns=df.schema.names\nfor column in columns:\n    df= df.withColumn(column, df[column].cast(IntegerType()))","3acaebf5":"df.printSchema()","e48b96bc":"import re\n\ndf_numeric_pandas=df_numeric.toPandas()\ndf_class_1=df_numeric_pandas[df_numeric_pandas['SARS-Cov-2 exam result']!='negative']\ndf_class_0=df_numeric_pandas[df_numeric_pandas['SARS-Cov-2 exam result']=='negative']\ndf_class_0=df_class_0[:len(df_class_1)]\n\ndf_numeric_concat=pd.concat([df_class_0,df_class_1],axis=0)\n\ny=df_numeric_concat['SARS-Cov-2 exam result']\n\ny_l=[0 if r=='negative' else 1 for r in y]\n\n\ncolumns_to_drop = ['SARS-Cov-2 exam result','Patient ID']\nX = df_numeric_concat.drop('SARS-Cov-2 exam result',axis=1)\n\ncolumns=X.columns\nX.columns=[str(re.sub(r\"[^a-zA-Z0-9]+\", ' ', column)) for column in columns]\ncolumns=X.columns\nX.columns=[column.replace(\"!@#$%^&*()[]{};:,.\/<>?\\|`~-=_+\", \" \") for column in columns]\n\ncolumns=X.columns\n\n\nfor column in X.columns:\n    X[column]=pd.to_numeric(X[column],errors='ignore')\n\nX=pd.get_dummies(X)\n\n","f1d679bf":"for column in X.columns:\n    if '<' in column:\n        X=X.drop([column],axis=1)","44137b0a":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nX_train,X_test,y_train,y_test=train_test_split(X,y_l,random_state=10,shuffle=True)\n\n#print(y_train)\nimport shap\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\n\ndef xgboost_classifier(X_train,y_train,X_test,y_test):\n\n    model = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X_train, label=y_train), 100)\n    dtest = xgboost.DMatrix(X_test,label=y_test)\n    preds = model.predict(dtest)\n    \n    y_pred=[int(pred>0.5) for pred in preds]\n    \n    score=classification_report(y_test,y_pred)\n    \n    return model,score\n\ndef xai_plot_values(model,X_):\n    shap.initjs()\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_)\n    return explainer,shap_values\n    \n\n\nmodel,report=xgboost_classifier(X_train,y_train,X_test,y_test)","b8073f5a":"print(report)","483cf10c":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\n# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nmodel = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\n\n\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(model, \n                                   \n                                   param_distributions=params, \n                                   n_iter=10, \n                                   scoring='roc_auc', n_jobs=-1, \n                                   cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\n\n\nrandom_search.fit(X_train, y_train)\n","3f1a0ccd":"### 1.4 Aggregation Functions","5a318092":"### 1.1 Print schema","43bc7cd2":"### Print first ten lines","ccb1fced":"### 1.2 to_Pandas() ","d7dce5f9":"### Hemoglobin values","50b3a0ea":"### 1.3 Save a parquet file","9a322303":"### Treinando Classificador com XGBoost","9c18bfab":"***\n## 1. Reading the Data\n***"}}