{"cell_type":{"ffa7e84b":"code","82e91223":"code","2540edc1":"code","9c1fc1cf":"code","a0982b83":"code","d49327cc":"code","035d6cc5":"code","f7c9dc32":"code","af7bcb72":"code","f6411a07":"code","961e6c01":"code","da83d031":"code","ab2896ca":"code","7350f0e6":"code","6c1ed1ea":"code","f8cd0b3a":"code","1f202b56":"code","0d510cad":"code","920609ff":"code","7e35d86f":"markdown","672fde37":"markdown","7962bf6d":"markdown","b5f4e41b":"markdown","c4f2dc5e":"markdown","6a663a9d":"markdown","8830b611":"markdown","ce96ac8b":"markdown","9b0b9304":"markdown","5e7a75c9":"markdown","e18300fa":"markdown","2d2e6759":"markdown","581e8a50":"markdown","e513ab68":"markdown","5dca2710":"markdown","5d9a2af5":"markdown","4edeae3d":"markdown","f5451633":"markdown","a002b301":"markdown"},"source":{"ffa7e84b":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom keras import optimizers\nfrom keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.models import *\nfrom keras.callbacks import ModelCheckpoint\nfrom keras. optimizers import *\nfrom keras.utils.vis_utils import plot_model\nimport numpy as np\nimport pickle, os\nimport time\nimport warnings\nfrom sklearn import linear_model\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom urllib.request import urlopen ","82e91223":"np.random.seed(42)\n\n\nL=40 \nJ=-1.0 \nT=np.linspace(0.25,4.0,16)\nT_c=2.26","2540edc1":"url_main = 'https:\/\/physics.bu.edu\/~pankajm\/ML-Review-Datasets\/isingMC\/';\n\ndata_file_name = \"Ising2DFM_reSample_L40_T=All.pkl\" \nlabel_file_name = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n\ndata = pickle.load(urlopen(url_main + data_file_name)) # 1d bit array\ndata = np.unpackbits(data).reshape(-1, 1600) # dekompression der Bits\ndata=data.astype('int')\ndata[np.where(data==0)]=-1\n\nlabels = pickle.load(urlopen(url_main + label_file_name))","9c1fc1cf":"num_classes=2\ntrain_to_test_ratio=0.5 # Verh\u00e4ltniss Trainingdaten\/Gesamtdaten. Bei 0.5 gibt es also genauso viele Testdaten wie Trainingsdaten\n\nX_ordered=data[:70000,:] # Die ersten 70000 Eintr\u00e4ge geh\u00f6hren zu den Temp. 0.25 - 1.75 und sind somit geordnet\nY_ordered=labels[:70000]\n\nX_critical=data[70000:100000,:]\nY_critical=labels[70000:100000]\n\nX_disordered=data[100000:,:] # Ab 10000 geh\u00f6hren die Eintr\u00e4ge geh\u00f6hren zu den Temp. > 2.5 und sind somit ungeordnet\nY_disordered=labels[100000:]\n\ndel data,labels\n\nX=np.concatenate((X_ordered,X_disordered)) # Dies entspricht dem Datensatz ohne kritische Konfigurationen\nY=np.concatenate((Y_ordered,Y_disordered))\n\n# Teilt in Trainings- und Testdaten auf\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=train_to_test_ratio,test_size=1.0-train_to_test_ratio)\n\nX=np.concatenate((X_critical,X))\nY=np.concatenate((Y_critical,Y))\n\nprint('X_train shape:', X_train.shape)\nprint('Y_train shape:', Y_train.shape)\n\nprint()\nprint(X_train.shape[0], 'train samples')\nprint(X_critical.shape[0], 'critical samples')\nprint(X_test.shape[0], 'test samples')","a0982b83":"cmap_args=dict(cmap='plasma_r')\n\nfig, axarr = plt.subplots(nrows=1, ncols=3)\n\naxarr[0].imshow(X_ordered[20001].reshape(L,L),**cmap_args)\naxarr[0].set_title('$\\\\mathrm{ordered\\\\ phase}$',fontsize=16)\naxarr[0].tick_params(labelsize=16)\n\naxarr[1].imshow(X_critical[10001].reshape(L,L),**cmap_args)\naxarr[1].set_title('$\\\\mathrm{critical\\\\ region}$',fontsize=16)\naxarr[1].tick_params(labelsize=16)\n\nim=axarr[2].imshow(X_disordered[50001].reshape(L,L),**cmap_args)\naxarr[2].set_title('$\\\\mathrm{disordered\\\\ phase}$',fontsize=16)\naxarr[2].tick_params(labelsize=16)\n\nfig.subplots_adjust(right=2.0)\n\nplt.show()","d49327cc":"\nlmbdas=np.logspace(-5,5,11)\n\n\ntrain_accuracy=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy=np.zeros(lmbdas.shape,np.float64)\n\ntrain_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy_SGD=np.zeros(lmbdas.shape,np.float64)\n\n\nfor i,lmbda in enumerate(lmbdas):\n\n    # Logistische Regression mit L2\n    logreg=linear_model.LogisticRegression(C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                           solver='liblinear')\n\n    logreg.fit(X_train, Y_train)\n\n    train_accuracy[i]=logreg.score(X_train,Y_train)\n    test_accuracy[i]=logreg.score(X_test,Y_test)\n    critical_accuracy[i]=logreg.score(X_critical,Y_critical)\n    \n    \n    # Sochastic Gradient Descent\n    logreg_SGD = linear_model.SGDClassifier(loss='log', penalty='l2', alpha=lmbda, max_iter=100, \n                                           shuffle=True, random_state=1, learning_rate='optimal')\n\n    logreg_SGD.fit(X_train,Y_train)\n\n    train_accuracy_SGD[i]=logreg_SGD.score(X_train,Y_train)\n    test_accuracy_SGD[i]=logreg_SGD.score(X_test,Y_test)\n    critical_accuracy_SGD[i]=logreg_SGD.score(X_critical,Y_critical)\n    ","035d6cc5":"plt.semilogx(lmbdas,train_accuracy,'*-b',label='liblinear train')\nplt.semilogx(lmbdas,test_accuracy,'*-r',label='liblinear test')\nplt.semilogx(lmbdas,critical_accuracy,'*-g',label='liblinear critical')\n\nplt.semilogx(lmbdas,train_accuracy_SGD,'*--b',label='SGD train')\nplt.semilogx(lmbdas,test_accuracy_SGD,'*--r',label='SGD test')\nplt.semilogx(lmbdas,critical_accuracy_SGD,'*--g',label='SGD critical')\n\nplt.xlabel('$\\\\lambda$')\nplt.ylabel('$\\\\mathrm{accuracy}$')\n\nplt.grid()\nplt.legend()\n\n\nplt.show()","f7c9dc32":"lmbdas=np.logspace(-5,5,11)\n\ntrain_accuracy_l1=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy_l1=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy_l1=np.zeros(lmbdas.shape,np.float64)\n\ntrain_accuracy_el=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy_el=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy_el=np.zeros(lmbdas.shape,np.float64)\n\nfor i,lmbda in enumerate(lmbdas):\n    \n    \n    # Logistische Regression mit L1\n    logreg_l1=linear_model.LogisticRegression(penalty='l1',C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,\n                                              tol=1E-5,solver='liblinear')\n\n    logreg_l1.fit(X_train, Y_train)\n\n    train_accuracy_l1[i]=logreg_l1.score(X_train,Y_train)\n    test_accuracy_l1[i]=logreg_l1.score(X_test,Y_test)\n    critical_accuracy_l1[i]=logreg_l1.score(X_critical,Y_critical)\n    \n    \n    # Logistische Regression mit L1 + L2 (elasticnet), dabei muss Solver=saga\n    logreg_el=linear_model.LogisticRegression(penalty='elasticnet',C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                           solver='saga',l1_ratio=0.5)\n\n    logreg_el.fit(X_train, Y_train)\n\n    train_accuracy_el[i]=logreg_el.score(X_train,Y_train)\n    test_accuracy_el[i]=logreg_el.score(X_test,Y_test)\n    critical_accuracy_el[i]=logreg_el.score(X_critical,Y_critical)","af7bcb72":"plt.semilogx(lmbdas,train_accuracy_l1,'*-b',label='liblinear(l1) train')\nplt.semilogx(lmbdas,test_accuracy_l1,'*-r',label='liblinear(l1) test')\nplt.semilogx(lmbdas,critical_accuracy_l1,'*-g',label='liblinear(l1) critical')\n\nplt.semilogx(lmbdas,train_accuracy_el,'*--b',label='Saga(l1+l2) train')\nplt.semilogx(lmbdas,test_accuracy_el,'*--r',label='Saga(l1+l2) test')\nplt.semilogx(lmbdas,critical_accuracy_el,'*--g',label='Saga(l1+l2) critical')\n\nplt.xlabel('$\\\\lambda$')\nplt.ylabel('$\\\\mathrm{accuracy}$')\n\nplt.grid()\nplt.legend()\n\n\nplt.show()","f6411a07":"lmbdas=np.logspace(-5,5,11)\n\ntrain_accuracy_cg=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy_cg=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy_cg=np.zeros(lmbdas.shape,np.float64)\n\ntrain_accuracy_fgs=np.zeros(lmbdas.shape,np.float64)\ntest_accuracy_fgs=np.zeros(lmbdas.shape,np.float64)\ncritical_accuracy_fgs=np.zeros(lmbdas.shape,np.float64)\n\n\nfor i,lmbda in enumerate(lmbdas):\n    \n    \n    # Logistische Regression mit newton-cg\n    logreg_cg=linear_model.LogisticRegression(C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,\n                                              tol=1E-5,solver='newton-cg')\n\n    logreg_cg.fit(X_train, Y_train)\n\n    train_accuracy_cg[i]=logreg_l1.score(X_train,Y_train)\n    test_accuracy_cg[i]=logreg_l1.score(X_test,Y_test)\n    critical_accuracy_cg[i]=logreg_l1.score(X_critical,Y_critical)\n    \n    \n    # Logistische Regression mit lbgfs\n    logreg_fgs=linear_model.LogisticRegression(C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                           solver='lbfgs')\n\n    logreg_fgs.fit(X_train, Y_train)\n\n    train_accuracy_fgs[i]=logreg_fgs.score(X_train,Y_train)\n    test_accuracy_fgs[i]=logreg_fgs.score(X_test,Y_test)\n    critical_accuracy_fgs[i]=logreg_fgs.score(X_critical,Y_critical)","961e6c01":"plt.semilogx(lmbdas,train_accuracy_cg,'*-b',label='Newton-cg train')\nplt.semilogx(lmbdas,test_accuracy_cg,'*-r',label='Newton-cg test')\nplt.semilogx(lmbdas,critical_accuracy_cg,'*-g',label='Newton-cg critical')\n\nplt.semilogx(lmbdas,train_accuracy_fgs,'*--b',label='lbgfs train')\nplt.semilogx(lmbdas,test_accuracy_fgs,'*--r',label='lbgfs test')\nplt.semilogx(lmbdas,critical_accuracy_fgs,'*--g',label='lbgfs critical')\n\n\nplt.xlabel('$\\\\lambda$')\nplt.ylabel('$\\\\mathrm{accuracy}$')\n\nplt.grid()\nplt.legend(loc='lower left')\n\n\nplt.show()","da83d031":"X_supercritical=X_critical[20000:300000,:] # Es werden die Konfigurationen mit T = 2.5 ausgew\u00e4hlt\nY_supercritical=Y_critical[20000:300000]\n\nlmbda_fgs = 1000\nlmbda = 10000\n\nlogreg_fgs=linear_model.LogisticRegression(C=1.0\/lmbda_fgs,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                               solver='lbfgs')\nlogreg_fgs.fit(X_train, Y_train)\ncrit_accuracy_fgs=logreg_fgs.score(X_supercritical,Y_supercritical)\nprint('Critical Accuracy with lbfgs:' + str(crit_accuracy_fgs))\n    \n    \nlogreg=linear_model.LogisticRegression(C=1.0\/lmbda,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                               solver='liblinear')\nlogreg.fit(X_train, Y_train)\ncrit_accuracy=logreg.score(X_supercritical,Y_supercritical)\nprint('Critical Accuracy with liblinear:' + str(crit_accuracy))\n    ","ab2896ca":"X_own_2d = np.load(\"\/kaggle\/input\/phasen-bergang-am-2-d-ising-modell\/ml_eigene_daten_bilder.npy\")\nJ = np.load('\/kaggle\/input\/phasen-bergang-am-2-d-ising-modell\/ml_eigene_daten_labels.npy')\n\nX_own = X_own_2d.reshape(16000, 1600)\nY_own = np.where(J > 0.441, 1, 0)\n\nnot_crit = np.concatenate([np.where(J > 0.5)[0], np.where(J <= 0.4)[0]])\ncrit = np.setdiff1d(np.arange(16000), not_crit)\n\nX_own_notcrit = X_own[not_crit]\nY_own_notcrit = Y_own[not_crit]\nJ_own_notcrit = J[not_crit]\n\nX_own_crit = X_own[crit]\nY_own_crit = Y_own[crit]\nJ_own_crit = J[crit]\n\nprint(\"Noncritical Samples: \" + str(len(Y_own_notcrit)))\nprint(\"Critical Samples: \" + str(len(Y_own_crit)))","7350f0e6":"lmbda_fgs = 1000\nlmbda_lib = 10000\n\nlogreg_fgs=linear_model.LogisticRegression(C=1.0\/lmbda_fgs,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                               solver='lbfgs')\nlogreg_fgs.fit(X_train, Y_train)\ncrit_accuracy_fgs=logreg_fgs.score(X_own_crit,Y_own_crit)\nnotcrit_accuracy_fgs=logreg_fgs.score(X_own_notcrit,Y_own_notcrit)\n\nlogreg=linear_model.LogisticRegression(C=1.0\/lmbda_lib,random_state=1,verbose=0,max_iter=1E3,tol=1E-5,\n                                               solver='liblinear')\nlogreg.fit(X_train, Y_train)\ncrit_accuracy=logreg.score(X_supercritical,Y_supercritical)\nnotcrit_accuracy=logreg.score(X_own_notcrit,Y_own_notcrit)\n\nprint('Critical Accuracy with liblinear:' + str(crit_accuracy))\nprint('Non-Critical Accuracy with liblinear:' + str(notcrit_accuracy))\nprint('Critical Accuracy with lbfgs:' + str(crit_accuracy_fgs))\nprint('Non-Critical Accuracy with lbfgs:' + str(notcrit_accuracy_fgs))","6c1ed1ea":"# Silly Reshape\nX_train, X_test, X_critical = X_train.reshape((-1, 40, 40, 1)), X_test.reshape((-1, 40, 40, 1)), X_critical.reshape((-1, 40, 40, 1))\nX_own_notcrit, X_own_crit = X_own_notcrit.reshape((-1, 40, 40, 1)), X_own_crit.reshape((-1, 40, 40, 1))","f8cd0b3a":"in_layer = Input(shape=(40, 40, 1 ))\n\nconv = Conv2D(10, (3,3), activation='relu', padding='same')(in_layer)\nconv = BatchNormalization()(conv)\nconv = Conv2D(20, (5,5), activation='relu', padding='same')(conv)\nconv = BatchNormalization()(conv)\nconv = MaxPooling2D((2,2))(conv)\n\nconv = Conv2D(16, (3,3), activation='relu', padding='same')(conv)\nconv = BatchNormalization()(conv)\nconv = Conv2D(32, (4,4), activation='relu', padding='same')(conv)\nconv = BatchNormalization()(conv)\nconv = MaxPooling2D((2,2))(conv)\n\nconv = Conv2D(16, (3,3), activation='relu', padding='same')(conv)\nconv = BatchNormalization()(conv)\nconv = Conv2D(32, (2,2), activation='relu', padding='same')(conv)\nconv = BatchNormalization()(conv)\n\nflat = Flatten()(conv)\n\ndense = Dense(350, activation='relu')(flat)\ndense = Dropout(0.9)(dense)\ndense = Dense(50, activation='relu')(dense)\ndense = Dropout(0.85)(dense)\nout = Dense(1, activation='sigmoid')(dense)\n\nmodel = Model(inputs=in_layer, outputs=out)\n\nmodel.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n\n# plot_model(model, to_file='model_plot.png', show_shapes=False, show_layer_names=False, dpi=100)","1f202b56":"filepath=f\"{time.time()}-best_weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n\nhistory = model.fit(X_train, Y_train, epochs=30, batch_size=128, validation_data=[X_critical, Y_critical], callbacks=[checkpoint], verbose=0)","0d510cad":"# Plot training & validation accuracy values\nplt.figure(dpi=200, figsize=(20,5))\nplt.plot(history.history['accuracy'], label='Trainings Daten')\nplt.plot(history.history['val_accuracy'], label='Kritische Daten')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot training & validation loss values\nplt.figure(dpi=200, figsize=(20,5))\nplt.plot(history.history['loss'], label='Trainings Daten')\nplt.plot(history.history['val_loss'], label='Kritische Daten')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc=('upper right'))\nplt.show()","920609ff":"model.load_weights(filepath)\nscores_train = model.evaluate(X_train, Y_train, verbose=0)\nprint(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\nscores_test = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Test Accuracy: %.2f%%\" % (scores_test[1]*100))\nscores_crit = model.evaluate(X_critical, Y_critical, verbose=0)\nprint(\"Critical Accuracy: %.2f%%\" % (scores_crit[1]*100))\nscores_crit = model.evaluate(X_own_notcrit, Y_own_notcrit, verbose=0)\nprint(\"Test Accuracy (own): %.2f%%\" % (scores_crit[1]*100))\nscores_crit = model.evaluate(X_own_crit, Y_own_crit, verbose=0)\nprint(\"Critical Accuracy (own): %.2f%%\" % (scores_crit[1]*100))","7e35d86f":"Nun sollen auch noch einiger solver ausprobiert und verglichen werden. Bis jetzt haben wir liblinear verwendet, dabei handelt es sich um den alten Standard in sklearns linear Regression. Nun probieren wir noch newton-cg und lbgfs aus, bei letzterem handelt es sich um den seit Version 0.22 Standardsolver.\n\nEs werden wiederum jeweils Traings-, Test- und Kritische Genauigkeit \u00fcber $\\lambda$ aufgetragen.","672fde37":"Nun werten wir unsere selbst erstellten Daten durch \u00fcber importierte Traingsdaten trainierte Algorithmen aus.\nEs werden wiederum die Solver liblinear und lbfgs mit $\\lambda_{lib} = 10^4$ und $\\lambda_{fgs} = 10^3$ verwendet.","7962bf6d":"# Vergleich verschiedener Machine-Learning Algorithmen\nZun\u00e4chst betrachten wir die beiden optimization routines die auch schon im Notebook der BU betrachtet werden: Logistische Regression mit 'liblinear' als solver und SGD (stochastic gradient descent). Diese Vergleichen wir bei verschiedenen Lern-regularisierungen $\\lambda$ um den Einfluss von overfitting auf unsere Ergebnisse absch\u00e4tzen zu k\u00f6nnen.\n\nDazu werden die Anteile der korrekt zugeordneten Konfigurationen f\u00fcr Trainings- und Testdaten f\u00fcr beide Routinen \u00fcber $\\lambda$ (halblogarithmisch) aufgetragen. Bei einem guten Modell ist dabei kaum ein unterschied zwischen Trainings- und Testdaten zu erwarten.\nZus\u00e4tzlich wurde auch die Genauigkeit f\u00fcr die kritischen Konfigurationen aufgetragen, um eine Information bez\u00fcglich der \u00dcbertragbarkeit der Modelle auf den kritischen Bereich zu erhalten.","b5f4e41b":"# Anwendung auf selbst erzeugte Daten\nNun wollen wir \u00fcber die Trainingsdaten trainierte Routinen auf unsere eigenen (ebenfalls \u00fcber MCS erzeugte) Konfigurationen anwenden und die Genauigkeiten vergleichen. Dazu importieren wir zun\u00e4chst unsere Daten und Teilen sie in kritisch und nichtkritisch auf.","c4f2dc5e":"# Convolutional Neural Network\nIm folgenden soll nun noch ein eigenes Machine Learning Netzwerk entwickelt werden, in dessen Zentrum ein CNN steht. Als Entwicklungs Umgebung soll Keras mit Tensorflow Backend dienen.","6a663a9d":"Ziel ist es, die Phase von gegebenen 2d Ising-Gittern \u00fcber Machine Learning zu bestimmen.\nDas bedeuted, es soll bestimmt werden, ob eine gegebene 40x40 Spinkonfiguration geordnet oder ungeordnet ist.\nDaf\u00fcr bietet sich eine logistische Regression an.\nDie Genauigkeit dieser Bestimmung soll unter Variation verschiedener Parameter anhand von Trainingsdaten untersucht und schlie\u00dflich auf unsere eigenen Spinkonfigurationen angewandt werden.\n\nDie Trainingsdaten werden nicht selbst erstellt, sonder importiert. Dabei handelt es sich um 40x40 mit 1 bzw -1 besezte Spingitter, welche \u00fcber MCS erzeugt wurden. Dabei werden zu jeder Temperatur $T\/J\\in[0.25,0.5,\\cdots,4.0]$ $10^4$ Spingitter erzeugt. Da bei einer Temperatur nahe der kritischen Temperatur u.a. die MCS langsamer zum gew\u00fcnschten Ziel f\u00fchren. Daher werden die Trainingsdaten in geordnet ($T\/J<2.0$), kritisch ($2.0\\leq T\/J\\leq 2.5)$ and ungeordnet ($T\/J>2.5$) eingeteilt. Die kritischen Daten werden nicht zum Training verwendet, sondern sind lediglich ein Teil der Testgruppe.\n\nHier ist anzumerken, dass in Anlehnung an das Notebook \"Section VII: Logistic Regression (Ising)\" der Boston University die Temperatur in T\/J und nicht in J\/T angegeben wird. Somit verschiebt sich der kritische Bereich auf die oben genannten Werte.","8830b611":"Diese niedrigen Werte zeigen, dass sich die G\u00fcltigkeit der Netzwerke nicht in den (sehr) kritischen Bereich erstreckt.","ce96ac8b":"Nun muss der Datensatz noch in Trainigs und Testdaten aufgeteilt werden, dabei ist darauf zu achten dass die kritischen Konfigurationen lediglich im Testdatensatz vorhanden sein sollen.","9b0b9304":"# G\u00fcltigkeit im Kritischen Bereich\nDie Motivation des Trainierens solcher Routinen liegt im Endeffekt nicht im Reproduzieren bekannter Daten, sondern in der Anwendbarkeit auf Bereiche die sich unserer Kenntniss entziehen. Obwohl das 2d Ising-Modell bereits gel\u00f6st ist, sind die Konfigurationen nahe $T_c$ somit von besonderem Interesse.\nDaher betrachten wir nun die Genauigkeit bei kritischen Konfigurationen m\u00f6glichst nahe der kritischen Temperatur.\n\nWir verwenden daf\u00fcr die Solver liblinear und lbfgs mit $\\lambda_{lib} = 10^4$ und $\\lambda_{fgs} = 10^3$.","5e7a75c9":"Nachdem das Netzwerk erfolgreich konvergiert ist, sollen nun die Genauigkeiten der Trainings-, Test-, kritischen und eignen Daten berechnet werden.","e18300fa":"W\u00e4hrend die nicht kritischen Genauigkeiten der beiden Netzwerke sehr \u00e4hnlich sind, unterscheiden sich die Genauigkeiten im Kritischen Bereich ungef\u00e4hr um einen Faktor 2 zugunsten von lbfgs. Dieses Ergebnis spiegelt die Werte des importierten Datensatzes (in angemessenem Fehlerramen) wieder. Somit k\u00f6nnen die Netzwerke auf unsere selbst erzeugten Daten \u00fcbertragen werden.","2d2e6759":"Nun wollen wir auch noch einige andere Parameter an der Mathode der logistischen Regression \u00e4ndern und die Ergebnisse vergleichen.\nZun\u00e4chst ver\u00e4ndern wir daf\u00fcr die Kostenfunktion von L2, bei welchem der Loss-Funktion Terme entsprechend dem Quadrat der einzelnen Koeffizienten hinzugef\u00fcgt wird, auch Ridge-Regression, zu L1 (Lasso-Regression), bei welcher der Koeffizient nur im Betrag eingeht.\nDesweiteren soll ein elasticnet betrachtet werden, bei welchem es sich um eine lineare Kombination von L1 und L2 handelt. Da liblinear dies nicht unterst\u00fctzt, wird hier in \"Saga\" auf einen anderen Solver zur\u00fcckgegriffen.\n\nAuch hier werden jeweils Traings-, Test- und Kritische Genauigkeit \u00fcber $\\lambda$ aufgetragen.","581e8a50":"# Visualisierung der Spin-Konfigurationen\nZun\u00e4chst betrachten wir eine beispielhafte Visualisierung f\u00fcr geordnete, ungeordnete und kritische Zust\u00e4nde. An dieser soll deutlich werden, dass selbst f\u00fcr ein Menschliches Auge der Unterschied zwischen einem geordneten und ungeordneten Zustand in der kritischen Phase keineswegs eindeutig ist.","e513ab68":"Es ist leicht zu erkennen, dass sich Newton-cg mit den betrachteten $\\lambda$ nicht f\u00fcr unsere Zwecke eignet. lbfgs hingegen zeigt ein \u00e4hnliches Bild wie schon liblinear(mit L2 Penalty) allerdings ist hier bei hohen $\\lambda$ die kritische Genauigkeit h\u00f6her.\n\nF\u00fcr die weitere Betrachtung und Auswertung unserer eigenen Daten bieten sich anhand oben genannter Argumente liblinear(L2 Penalty) und lbfgs an. Diese werden daher (mit geeigneten $\\lambda$) in den nachfolgenden Abschnitten verwendet, was auch einen Vergleich des neuen und alten Standards von sklearns LogisticRegression erm\u00f6glicht.","5dca2710":"Auff\u00e4llig ist hier die gr\u00f6\u00dftenteils konstante Genauigkeit der liblinear-Methode. Test- und Trainingsdaten werden zwar nicht gleichgut bestimmt, allerdings liegt die Abweichung mit ca. 5% in einem zu erwartenden Ramen. Die Genauigkeit bez\u00fcglich der kritischen Konfigurationen ist ebenfalls erwartungsgem\u00e4\u00df niedriger als die Test-Genauigkeit, da ja auch nicht anhand von kritischen Daten Trainiert wurde.\n\nF\u00fcr die SGD stellen wir fest, dass die Genauigkeit bei $\\lambda \\approx 10^{-2} - 10^{-1}$ am besten wird.","5d9a2af5":"# Laden der Daten\nZun\u00e4chst m\u00fcssen die ben\u00f6tigten Daten geladen werden. Daf\u00fcr werden url und Dateinamen angegeben. Die Spinkonfigurationen werden in ein $16 \\cdot 10^4 \\times 1600$ Array gespeichert und die Nullen in Einsen umgewandelt. Die Label werden als 1D Array gespeichert (0: ungeordnet, 1: geordnet).\nDie Daten stammen von hier: https:\/\/physics.bu.edu\/~pankajm\/ML-Review-Datasets\/isingMC\/","4edeae3d":"\n # Bestimmung der Phase \u00fcber logistische Regression\n","f5451633":"Es zeigt sich, wie zu erwarten war (da es ein zeimlich einfaches Problem ist), dass f\u00fcr die Trainings- und Testdaten das Netzwerk quasi fehlerfrei die richtige Zuordnung findet. Aber auch f\u00fcr die Daten aus dem kritischen bereich ist das Netzwerk sehr gut geeignet. Dabei ist fast nicht davon auszugehen, dass 100% aus dem kritischen bereich richig zugeordnet werden k\u00f6nnen, da sich die Bilder beider Phasen teilweise kaum bis gar nicht unterscheiden, vorallem wenn man bedengt, dass die Bilder nur  $40\\times40$  gro\u00df sind.\n\nEin Test mit den eigenen Bildern aus den vorherigen Aufgaben zeigt deutlich, dass sich die Daten etwas unterscheiden, vorallem die aus dem kritischen Bereich h\u00e4ngen deutlich hinterher. Denoch kann erkannt werden, dass das Netzwerk sehr gut verallgemeinert hat und um L\u00e4ngen besser als die Regressionen aus dem obigen Abschnitt ist.\n\nF\u00fcr bessere Ergebnisse m\u00fcssten die Spingitter gr\u00f6\u00dfer sein und mehr Trainingsdaten vorliegen. Auch m\u00fcssten die Daten aus unserem Monte-Carlo Teil vorsichtiger erstellt werden.\n\nNichts desto trotz konnte gezeigt werden, dass maschinelles Lernen ein sehr gutes Werkzeug zur Klassifikation der Phase von 2d-Ising Spingittern ist.","a002b301":"In den Graphen ist zu erkennen, dass sich die Unterschiedliche Penalty erst bei $\\lambda > 10$ bemerkbar macht, f\u00fcr kleinere Werte erzielen beide Routinen gleiche Ergebnisse, die auch denen der L2 Penalty gleichen.\nInteressanterweise steigt die kritische Genauigkeit bei verwendung des elasticnet f\u00fcr $\\lambda > 10$, allerdings sinkt die Genauigkeit f\u00fcr Nichtkritische Daten.\nF\u00fcr die L1 Penalty ist ab $\\lambda > 100$ ein rapider Abfall in allen Bereichen zu sehen."}}