{"cell_type":{"3e531df3":"code","6a8dd089":"code","d4dccc95":"code","7a9a2fcd":"code","920a9a15":"code","8531c56a":"markdown","a3ef49ff":"markdown","929698d9":"markdown","c5dbfa85":"markdown","8f4cc2ac":"markdown","bf4df7e2":"markdown","7085e0b9":"markdown"},"source":{"3e531df3":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\noof_svnu = np.zeros(len(train))\noof_svc = np.zeros(len(train))\noof_knn = np.zeros(len(train))\noof_quad = np.zeros(len(train))\n\npred_svnu = np.zeros(len(test))\npred_svc = np.zeros(len(test))\npred_knn = np.zeros(len(test))\npred_quad = np.zeros(len(test))\n\ncols = [c for c in train.columns if c not in ['id', 'target',\n                                              'wheezy-copper-turtle-magic']]\n\nfor i in range(512):\n    train2 = train[train['wheezy-copper-turtle-magic'] == i]\n    test2 = test[test['wheezy-copper-turtle-magic'] == i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    \n    data = pd.concat([pd.DataFrame(train2[cols]),\n                      pd.DataFrame(test2[cols])])\n    data2 = StandardScaler().fit_transform(PCA(n_components=40, random_state=4).fit_transform(data[cols]))\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=5, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n        \n        clf = NuSVC(probability=True, kernel='poly', degree=4,\n                    gamma='auto', random_state=4, nu=0.6, coef0=0.08)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_svnu[idx1[test_index]] = clf.predict_proba(train3[test_index,])[:,1]\n        pred_svnu[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n\n        clf = SVC(probability=True, kernel='poly', degree=4, gamma='auto',\n                  random_state=42)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_svc[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        pred_svc[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n        clf = KNeighborsClassifier(n_neighbors=17, p=2.9)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_knn[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        pred_knn[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n        \n        clf = QuadraticDiscriminantAnalysis(reg_param=0.111)\n        clf.fit(train3[train_index, :], train2.loc[train_index]['target'])\n        oof_quad[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        pred_quad[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","6a8dd089":"oof_svnu.shape, oof_svc.shape, oof_knn.shape, oof_quad.shape","d4dccc95":"oof_svnu.reshape(-1, 1)\noof_svc.reshape(-1, 1)\noof_knn.reshape(-1, 1)\noof_quad.reshape(-1, 1)\n\n# Make dataframe of probabilities and target\noof_df = pd.concat([pd.DataFrame(oof_svnu), pd.DataFrame(oof_svc),\n                    pd.DataFrame(oof_quad), pd.DataFrame(oof_knn),\n                    train.target], axis=1)\n\n# Rename columns\noof_df.columns = ['NuSVC', 'SVC', 'QDA', 'KNN', 'target']\n\n# Use first 2000 data (to easily visualize and save time)\noof_df = oof_df[:2000]\n\n# Transform 'target' to categorical feature\nfor i in range(len(oof_df)):\n    if oof_df['target'][i] == 1:\n        oof_df['target'][i] = '= 1'\n    elif oof_df['target'][i] == 0:\n        oof_df['target'][i] = '= 0'\n\noof_df.head()","7a9a2fcd":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# Use first 2000 data (to visualize easily)\noof_df = oof_df[:2000]\n\nplt.figure(figsize=(10,10))\nsns.pairplot(oof_df, hue='target',\n             plot_kws=dict(s=10, edgecolor='None'))\nplt.show();","920a9a15":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = pred_quad\nsub.to_csv('submission.csv', index=False)","8531c56a":"Using [this kernel](https:\/\/www.kaggle.com\/speedwagon\/quadratic-discriminant-analysis) and [this kernel](https:\/\/www.kaggle.com\/tunguz\/pca-nusvc-knn), we got probabilities of 4 algorithm. Then I plotted them and got clear visualization of each algorithm's classification accuracy. I posted it [here](https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/94053) and was asked to share how to make graph. So I am sharing.\n  \nI hope it will help for someone.","a3ef49ff":"## Make pairplot\nWe can get a beautiful graph with seaborn.","929698d9":"Year! we got the beautiful picture. We can know QDA has quite accurately (It is little different from my discussion [post](https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/94053#latest-541549) because some parameter has been changed).  \nSecond NuSVC is good, and KNN is not very good..","c5dbfa85":"## Prepare for visualization","8f4cc2ac":"# Compare with NuSVC, SVC, QDA and KNN","bf4df7e2":"# Thanks","7085e0b9":"## Get each algorithm's Probability of Prediction"}}