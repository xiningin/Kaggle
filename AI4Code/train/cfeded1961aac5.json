{"cell_type":{"b21dc7e5":"code","1b593234":"code","3814ac7e":"code","053253d3":"code","ebb671ca":"code","8160f4c7":"code","507be6ca":"code","86925d5d":"code","1bb68aba":"code","5bc71a35":"code","3594ac38":"code","4824418f":"code","305c0e7a":"code","600a3df3":"code","b542c506":"code","bee17d8c":"code","19395708":"code","77c33d5c":"code","cbb7b14a":"code","ceb9e33e":"code","3038a4c6":"code","92d03434":"code","07ee8295":"code","32672218":"code","278f08c2":"code","058c73f0":"code","a8cdf0bc":"code","2ffdf88f":"code","95012cc4":"code","56a245f6":"code","0f0f4c0e":"code","1862ad1c":"code","87c6af74":"code","2e60639b":"code","e656b8e1":"code","31fdf3c6":"code","43dbb089":"code","63f5f46a":"code","304b2a92":"code","827c615d":"code","2b99f9c9":"code","66fdd6e1":"code","7bf7bc9f":"code","b961a6fc":"code","81d65b40":"code","4fecf9ef":"code","1418658f":"code","076d4ab2":"code","ab2b5cac":"code","389315aa":"code","6798d704":"code","b136c359":"code","a4be84e4":"code","182de584":"code","8e431a47":"code","971342e2":"code","447f6cae":"code","cfb19e21":"code","bbfb5abd":"markdown","c55002d0":"markdown","125c5351":"markdown","fa4a9658":"markdown","52cf9d94":"markdown","94625ce6":"markdown","d5e7f8be":"markdown","25c3dfa1":"markdown","899ae76f":"markdown","137a1d1c":"markdown","bd72e28c":"markdown","4a981e76":"markdown","5aa2970e":"markdown","39e01b1c":"markdown","e4206c18":"markdown","ac514680":"markdown","af5d5bb3":"markdown","aa137ad3":"markdown","c371446a":"markdown","0cd66417":"markdown","e505437e":"markdown","1fcf8291":"markdown","69390c0f":"markdown","605319e3":"markdown","c847da78":"markdown","557f644c":"markdown","a802967d":"markdown","75dc8801":"markdown","65e332af":"markdown","a2a926a3":"markdown","5ec0b6b3":"markdown","b4488101":"markdown","d11d5560":"markdown","869e0dcf":"markdown","b5118dcc":"markdown","247b056c":"markdown","81320599":"markdown","57b87f4c":"markdown","e695a984":"markdown","84deb44e":"markdown","92a3a679":"markdown","dd45069c":"markdown","b2b921d7":"markdown","e453f371":"markdown","3bc35ea8":"markdown","0a202f58":"markdown","8c468e7f":"markdown","5eda4a19":"markdown","5dbd8873":"markdown","ff6d182a":"markdown","0a9fa9c0":"markdown","634a37ea":"markdown","1341f48b":"markdown","9b5ae31b":"markdown","e1d8b277":"markdown","d292cc4f":"markdown","22762820":"markdown"},"source":{"b21dc7e5":"from fastai.text.all import *","1b593234":"path = Path('\/kaggle\/input\/')\npath.ls()","3814ac7e":"vax_tweets = pd.read_csv(path\/'all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\nvax_tweets.head()","053253d3":"tweets = pd.read_csv(path\/'complete-tweet-sentiment-extraction-data\/tweet_dataset.csv')\ntweets.head()","ebb671ca":"# Code via https:\/\/www.kaggle.com\/garyongguanjie\/comments-analysis\ndef de_emojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n# Code via https:\/\/www.kaggle.com\/pawanbhandarkar\/generate-smarter-word-clouds-with-log-likelihood\ndef tweet_proc(df, text_col='text'):\n    df['orig_text'] = df[text_col]\n    # Remove twitter handles\n    df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\\s]+','',x))\n    # Remove URLs\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Remove emojis\n    df[text_col] = df[text_col].apply(de_emojify)\n    # Remove hashtags\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    return df[df[text_col]!='']\n\n# Clean the text data and combine the dfs\ntweets = tweets[['old_text', 'new_sentiment']].rename(columns={'old_text':'text', 'new_sentiment':'sentiment'})\nvax_tweets['sentiment'] = np.nan\ntweets = tweet_proc(tweets)\nvax_tweets = tweet_proc(vax_tweets)\ndf_lm = tweets[['text', 'sentiment']].append(vax_tweets[['text', 'sentiment']])\ndf_clas = df_lm.dropna(subset=['sentiment'])\nprint(len(df_lm), len(df_clas))","8160f4c7":"df_clas.head()","507be6ca":"dls_lm = TextDataLoaders.from_df(df_lm, text_col='text', is_lm=True, valid_pct=0.1)","86925d5d":"dls_lm.show_batch(max_n=2)","1bb68aba":"learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()","5bc71a35":"learn.lr_find()","3594ac38":"learn.fit_one_cycle(1, 3e-2)","4824418f":"learn.unfreeze()\nlearn.lr_find()","305c0e7a":"learn.fit_one_cycle(4, 1e-3)","600a3df3":"# Text generation using the language model\nTEXT = \"I love\"\nN_WORDS = 30\nN_SENTENCES = 2\nprint(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","b542c506":"learn.save_encoder('finetuned_lm')","bee17d8c":"dls_clas = DataBlock(\n    blocks = (TextBlock.from_df('text', seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock),\n    get_x=ColReader('text'),\n    get_y=ColReader('sentiment'),\n    splitter=RandomSplitter()\n).dataloaders(df_clas, bs=64)","19395708":"dls_clas.show_batch(max_n=2)","77c33d5c":"learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()","cbb7b14a":"learn = learn.load_encoder('finetuned_lm')","ceb9e33e":"learn.fit_one_cycle(1, 3e-2)","3038a4c6":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))","92d03434":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))","07ee8295":"learn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3))","32672218":"learn.save('classifier')","278f08c2":"learn.predict(\"I love\")","058c73f0":"learn.predict(\"I hate\")","a8cdf0bc":"pred_dl = dls_clas.test_dl(vax_tweets['text'])","2ffdf88f":"preds = learn.get_preds(dl=pred_dl)","95012cc4":"# Get predicted sentiment\nvax_tweets['sentiment'] = preds[0].argmax(dim=-1)\nvax_tweets['sentiment'] = vax_tweets['sentiment'].map({0:'negative', 1:'neutral', 2:'positive'})\n\n# Convert dates\nvax_tweets['date'] = pd.to_datetime(vax_tweets['date'], errors='coerce').dt.date\n\n# Save to csv\nvax_tweets.to_csv('vax_tweets_sentiment.csv')\n\n# Plot sentiment value counts\nvax_tweets['sentiment'].value_counts(normalize=True).plot.bar();","56a245f6":"# Remove today's date since data is incomplete\ntoday = pd.Timestamp.today().date()\nvax_tweets = vax_tweets[vax_tweets['date']!=today]\n\n# Get counts of number of tweets by sentiment for each date\ntimeline = vax_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\n\n# Plot results\nimport plotly.express as px\nfig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},\n             title='Timeline showing sentiment of tweets about COVID-19 vaccines')\nfig.show()","0f0f4c0e":"spike = vax_tweets[vax_tweets['date'].astype(str)=='2021-03-01']\nspike['user_location'].value_counts(ascending=False).head(10)","1862ad1c":"spike = spike.sort_values('user_location', ascending=False)\nspike['orig_text'].head()","87c6af74":"all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n\n# Function to filter the data to a single vaccine and plot the timeline\n# Note: a lot of the tweets seem to contain hashtags for multiple vaccines even though they are specifically referring to one vaccine - not very helpful!\ndef filtered_timeline(df, vax, title):\n    df = df.dropna()\n    title_str = 'Timeline showing sentiment of tweets about the '+title+' vaccine'\n    df_filt = pd.DataFrame()\n    for o in vax:\n        df_filt = df_filt.append(df[df['orig_text'].str.lower().str.contains(o)])\n    other_vax = list(set(all_vax)-set(vax))\n    for o in other_vax:\n        df_filt = df_filt[~df_filt['orig_text'].str.lower().str.contains(o)]\n    df_filt = df_filt.drop_duplicates()\n    timeline = df_filt.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n    fig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},title=title_str)\n    fig.show()\n    return df_filt\n\ncovaxin = filtered_timeline(vax_tweets, ['covaxin'], title='Covaxin')","2e60639b":"# Function to filter the data to a single date and print tweets from users with the most followers\ndef date_filter(df, date):\n    return df[df['date'].astype(str)==date].sort_values('user_followers', ascending=False)[['date' ,'orig_text']]\n\ndef date_printer(df, dates, num=10): \n    for date in dates:\n        display(date_filter(df, date).head(num))\n\ndate_printer(covaxin, ['2021-03-01', '2021-03-03'])","e656b8e1":"sinovac = filtered_timeline(vax_tweets, ['sinovac'], title='Sinovac')","31fdf3c6":"date_printer(sinovac, ['2021-02-22', '2021-02-28', '2021-03-01', '2021-03-03', '2021-03-08'], 3)","43dbb089":"vax_progress = pd.read_csv(path\/'covid-world-vaccination-progress\/country_vaccinations.csv', parse_dates=['date'])\nvax_progress = vax_progress.replace([np.inf, -np.inf], np.nan)\nvax_progress = vax_progress[vax_progress['date']!=today]\ncountries = ['Brazil', 'Thailand', 'Hong Kong', 'Colombia', 'Mexico', 'Philippines', 'Indonesia']\nfig = px.line(vax_progress[vax_progress['country'].isin(countries)], x='date', y='daily_vaccinations_per_million', color='country',\n             title='Daily vaccinations per million (all vaccines) in selected countries')\nfig.show()","63f5f46a":"sinopharm = filtered_timeline(vax_tweets, ['sinopharm'], title='Sinopharm')","304b2a92":"date_printer(sinopharm, ['2021-02-18', '2021-02-24', '2021-03-02'], 3)","827c615d":"countries = ['Senegal', 'Nepal', 'Hungary', 'Bolivia', 'Lebanon']\nfig = px.line(vax_progress[vax_progress['country'].isin(countries)], x='date', y='daily_vaccinations_per_million', color='country',\n             title='Daily vaccinations per million (all vaccines) in selected countries')\nfig.show()","2b99f9c9":"moderna = filtered_timeline(vax_tweets, ['moderna'], title='Moderna')","66fdd6e1":"date_printer(moderna, ['2021-02-17', '2021-03-05', '2021-03-11'], 3)","7bf7bc9f":"countries = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czechia', 'Denmark', \n             'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', \n             'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', \n             'Romania', 'Slovakia', 'Slovenia', 'Spain','Sweden']\neu = vax_progress[vax_progress['country'].isin(countries)].groupby('date')['daily_vaccinations_per_million'].median().reset_index()\neu['region'] = 'EU'\nrow = vax_progress[~vax_progress['country'].isin(countries)].groupby('date')['daily_vaccinations_per_million'].median().reset_index()\nrow['region'] = 'Rest of world'\nfig = px.line(eu.append(row), x='date', y='daily_vaccinations_per_million', color='region',\n             title='Median daily vaccinations per million (all vaccines) in EU countries vs the rest of the world')\n\nfig.add_annotation(x='2021-02-17', y=2120,\n            text=\"EU makes a deal to purchase up to 300m extra Moderna vaccines\",\n            showarrow=True,\n            arrowhead=5, ax=-220, ay=-30)\n\nfig.add_vline(x='2021-02-17', line_width=3, line_dash=\"dash\", line_color=\"limegreen\")\n\nfig.show()","b961a6fc":"sputnikv = filtered_timeline(vax_tweets, ['sputnik'], title='Sputnik V')","81d65b40":"date_printer(sputnikv, ['2021-03-04', '2021-03-05', '2021-03-10', '2021-03-11', '2021-03-15'], 3)","4fecf9ef":"pfizer = filtered_timeline(vax_tweets, ['pfizer', 'biontech'], title='Pfizer\/BioNTech')","1418658f":"timeline = pfizer.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n\nfig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},\n              title='Timeline showing sentiment of tweets about the PfizerBioNTech vaccine')\n\nfig.add_annotation(x='2020-12-14', y=timeline[(timeline['date']==pd.Timestamp('2020-12-14'))&(timeline['sentiment']=='positive')]['tweets'].values[0],\n            text=\"USA and UK start vaccinating\",\n            showarrow=True,\n            arrowhead=3, ax=11, ay=-220)\n\nfig.add_annotation(x='2020-12-22', y=timeline[(timeline['date']==pd.Timestamp('2020-12-22'))&(timeline['sentiment']=='positive')]['tweets'].values[0],\n            text=\"Joe Biden receives first dose\",\n            arrowhead=3, ax=80, ay=-60)\n\nfig.add_annotation(x='2021-01-08', y=timeline[(timeline['date']==pd.Timestamp('2021-01-08'))&(timeline['sentiment']=='positive')]['tweets'].values[0],\n            text=\"Vaccine shown to resist new variant\",\n            showarrow=True, align='left',\n            arrowhead=3, ax=-70, ay=-25)\n\nfig.add_annotation(x='2021-01-16', y=timeline[(timeline['date']==pd.Timestamp('2021-01-16'))&(timeline['sentiment']=='negative')]['tweets'].values[0],\n            text=\"23 elderly Norwegians die after vaccine dose\",\n            showarrow=True, align='left',\n            arrowhead=3, ax=15, ay=-175)\n\nfig.add_annotation(x='2021-02-19', y=timeline[(timeline['date']==pd.Timestamp('2021-02-19'))&(timeline['sentiment']=='positive')]['tweets'].values[0],\n            text=\"Israeli study shows 85% efficacy after one dose\",\n            showarrow=True, align='left',\n            arrowhead=3, ax=-170, ay=-50)\n\nfig.add_annotation(x='2021-02-25', y=timeline[(timeline['date']==pd.Timestamp('2021-02-25'))&(timeline['sentiment']=='positive')]['tweets'].values[0],\n            text=\"Peer review of Israeli study shows 94% efficacy after two doses\",\n            showarrow=True, align='left',\n            arrowhead=3, ax=0, ay=-90)\n\nfig.show()","076d4ab2":"oxford = filtered_timeline(vax_tweets, ['oxford', 'astrazeneca'], title='Oxford\/AstraZeneca')","ab2b5cac":"date_printer(oxford, ['2021-02-19', '2021-03-06'], 5)","389315aa":"# At the time of writing, these countries have completely suspended the use of the vaccine\n# Note that several other countries continued mostly as normal but suspended the use of one batch of Oxford\/AstraZeneca vaccines\ncountries = ['Germany', 'France', 'Spain', 'Italy', 'Netherlands', 'Ireland', 'Denmark', 'Norway', 'Bulgaria', 'Iceland', 'Thailand']\nox_prog = vax_progress[vax_progress['country'].isin(countries)].groupby('date')['daily_vaccinations_per_million'].median().reset_index()\nox_prog['Use of Oxford\/AstraZeneca'] = 'Suspended'\nother_prog = vax_progress[vax_progress['vaccines'].str.contains('Oxford\/AstraZeneca')]\nother_prog = vax_progress[~vax_progress['country'].isin(countries)].groupby('date')['daily_vaccinations_per_million'].median().reset_index()\nother_prog['Use of Oxford\/AstraZeneca'] = 'Ongoing'\nfig = px.line(ox_prog.append(other_prog), x='date', y='daily_vaccinations_per_million', color='Use of Oxford\/AstraZeneca',\n             title=\"Median daily vaccinations per million (all vaccines) in countries that have completely suspended the use of the\\\n              <br>Oxford\/AstraZeneca vaccine vs countries that continue to use it\")\nfig.add_vrect(x0=\"2021-03-11\", x1=\"2021-03-14\", \n              annotation_text=\"vaccine<br>suspended\", annotation_position=\"bottom right\",\n              fillcolor=\"limegreen\", opacity=0.25, line_width=0)\nfig.show()","6798d704":"# Get z scores of sentiment for each vaccine\nvax_names = {'Covaxin': covaxin, 'Sinovac': sinovac, 'Sinopharm': sinopharm,\n            'Moderna': moderna, 'Oxford\/AstraZeneca': oxford, 'PfizerBioNTech': pfizer}\nsentiment_zscores = pd.DataFrame()\nfor k, v in vax_names.items():\n    senti = v['sentiment'].value_counts(normalize=True)\n    senti['vaccine'] = k\n    sentiment_zscores = sentiment_zscores.append(senti)\nfor col in ['negative', 'neutral', 'positive']:\n    sentiment_zscores[col+'_zscore'] = (sentiment_zscores[col] - sentiment_zscores[col].mean())\/sentiment_zscores[col].std(ddof=0)\nsentiment_zscores.set_index('vaccine', inplace=True)\n\n# Plot the results\nax = sentiment_zscores.sort_values('negative_zscore')['negative_zscore'].plot.barh(title='Z scores of negative sentiment')\nax.set_ylabel('Vaccine')\nax.set_xlabel('Z score');","b136c359":"!pip install wordninja\n!pip install pyspellchecker","a4be84e4":"from wordcloud import WordCloud, ImageColorGenerator\nimport wordninja\nfrom spellchecker import SpellChecker\nfrom collections import Counter\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))  \nstop_words.add(\"amp\")","182de584":"# FUNCTIONS REQUIRED\n\ndef flatten_list(l):\n    return [x for y in l for x in y]\n\ndef is_acceptable(word: str):\n    return word not in stop_words and len(word) > 2\n\n# Color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \n\n# Reusable function to generate word clouds \ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Words\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();\n\ndef get_top_percent_words(doc, percent):\n    # Returns a list of \"top-n\" most frequent words in a list \n    top_n = int(percent * len(set(doc)))\n    counter = Counter(doc).most_common(top_n)\n    top_n_words = [x[0] for x in counter]\n    \n    return top_n_words\n    \ndef clean_document(doc):\n    spell = SpellChecker()\n    lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize words (needed for calculating frequencies correctly )\n    doc = [lemmatizer.lemmatize(x) for x in doc]\n    \n    # Get the top 10% of all words. This may include \"misspelled\" words \n    top_n_words = get_top_percent_words(doc, 0.1)\n\n    # Get a list of misspelled words \n    misspelled = spell.unknown(doc)\n    \n    # Accept the correctly spelled words and top_n words \n    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n    \n    # Try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n    \n    # Some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n    clean_words.extend(spell.known(split_words))\n    \n    return clean_words\n\ndef get_log_likelihood(doc1, doc2):    \n    doc1_counts = Counter(doc1)\n    doc1_freq = {\n        x: doc1_counts[x]\/len(doc1)\n        for x in doc1_counts\n    }\n    \n    doc2_counts = Counter(doc2)\n    doc2_freq = {\n        x: doc2_counts[x]\/len(doc2)\n        for x in doc2_counts\n    }\n    \n    doc_ratios = {\n        # 1 is added to prevent division by 0\n        x: math.log((doc1_freq[x] +1 )\/(doc2_freq[x]+1))\n        for x in doc1_freq if x in doc2_freq\n    }\n    \n    top_ratios = Counter(doc_ratios).most_common()\n    top_percent = int(0.1 * len(top_ratios))\n    return top_ratios[:top_percent]\n\n# Function to generate a document based on likelihood values for words \ndef get_scaled_list(log_list):\n    counts = [int(x[1]*100000) for x in log_list]\n    words = [x[0] for x in log_list]\n    cloud = []\n    for i, word in enumerate(words):\n        cloud.extend([word]*counts[i])\n    # Shuffle to make it more \"real\"\n    random.shuffle(cloud)\n    return cloud","8e431a47":"# Convert string to a list of words\nvax_tweets['words'] = vax_tweets.text.apply(lambda x:re.findall(r'\\w+', x ))\n\ndef get_smart_clouds(df):\n\n    neg_doc = flatten_list(df[df['sentiment']=='negative']['words'])\n    neg_doc = [x for x in neg_doc if is_acceptable(x)]\n\n    pos_doc = flatten_list(df[df['sentiment']=='positive']['words'])\n    pos_doc = [x for x in pos_doc if is_acceptable(x)]\n\n    neu_doc = flatten_list(df[df['sentiment']=='neutral']['words'])\n    neu_doc = [x for x in neu_doc if is_acceptable(x)]\n\n    # Clean all the documents\n    neg_doc_clean = clean_document(neg_doc)\n    neu_doc_clean = clean_document(neu_doc)\n    pos_doc_clean = clean_document(pos_doc)\n\n    # Combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\n    top_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\n    top_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\n    top_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))\n\n    # Generate syntetic a corpus using our loglikelihood values \n    neg_doc_final = get_scaled_list(top_neg_words)\n    neu_doc_final = get_scaled_list(top_neu_words)\n    pos_doc_final = get_scaled_list(top_pos_words)\n\n    # Visualise our synthetic corpus\n    generate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)\n    \nget_smart_clouds(vax_tweets)","971342e2":"vax_tweets['has_url'] = np.where(vax_tweets['orig_text'].str.contains('http'), 'yes', 'no')\nvax_tweets['has_url'].value_counts(normalize=True).plot.bar();","447f6cae":"def get_cloud(df, string, c_func):\n    string_l = string.lower()\n    df[string_l] = np.where(df['text'].str.lower().str.contains(string_l), 1, 0)\n    cloud_df = df.copy()[df[string_l]==1]\n    doc = flatten_list(cloud_df['words'])\n    doc = [x for x in doc if is_acceptable(x)]\n    doc = clean_document(doc)\n    fig, axes = plt.subplots(figsize=(9,5))\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(doc))\n    axes.imshow(wordcloud.recolor(color_func=c_func, random_state=3), interpolation='bilinear')\n    axes.set_title(\"Naive word cloud for tweets Containg '%s'\" % (string))\n    axes.axis(\"off\")\n    plt.show();\n    \nget_cloud(vax_tweets, 'Canada', red_color_func)","cfb19e21":"countries = ['Canada', 'United Kingdom', 'United States', 'Chile', 'Singapore', 'Israel', 'Australia']\nselected = vax_progress[vax_progress['country'].isin(countries)]\neu['country'] = 'EU median'\nfig = px.line(vax_progress[vax_progress['country'].isin(countries)].append(eu), x='date', y='daily_vaccinations_per_million', color='country',\n             title='Daily vaccinations per million (all vaccines) in Canada vs selected other developed nations')\nfig.show()","bbfb5abd":"The overall sentiment of the Oxford\/AstraZeneca vaccine is therefore significantly more negative than average:","c55002d0":"### Pfizer\/BioNTech","125c5351":"## Transfer learning in NLP - the ULMFiT approach\n\nWe will be making use of *transfer learning* to help us create a model to analyse tweet sentiment. The idea behind transfer learning is that neural networks learn information that generalises to new problems, [particularly the early layers of the network](https:\/\/arxiv.org\/pdf\/1311.2901.pdf). In computer vision, for example, we can take a model that was trained on the ImageNet dataset to recognise different features of images such as circles, then apply that to a smaller dataset and *fine-tune* the model to be more suited to a specific task (e.g. classifying images as cats or dogs). This technique allows us to train neural networks much faster and with far less data than we would otherwise need.\n\nIn 2018 [a paper](https:\/\/arxiv.org\/abs\/1801.06146) introduced a transfer learning technique for NLP called 'Universal Language Model Fine-Tuning' (ULMFiT). The approach is as follows:\n1. Train a *language model* to predict the next word in a sentence. This step is already done for us; with [`fastai`](https:\/\/docs.fast.ai\/) we can download a model that has been pre-trained for this task on millions of Wikipedia articles. A good language model already knows a lot about how language works in general - for  instance, given the sentence 'Tokyo is the capital of', the model might predict 'Japan' as the next word. In this case the model understands that Tokyo is closely related to Japan and that 'capital' refers to 'city' here instead of 'upper-case' or 'money'.\n2. Fine-tune the language model to a more specific task. The pre-trained language model is good at understanding Wikipedia English, but Twitter English is a bit different. We can take the information the Wikipedia model has learned and apply that to a Twitter dataset to get a Twitter language model that is good at predicting the next word in a tweet.\n3. Fine-tune a *classification model* to identify sentiment using the pre-trained language model. The idea here is that since our language model already knows a lot about Twitter English, it's not a huge leap from there to train a classifier that understands that 'love' refers to positive sentiment and 'hate' refers to negative sentiment. If we tried to train a classifier without using a pre-trained model it would have to learn the whole language from scratch first, which would be very difficult and time consuming.\n\n<img alt=\"Diagram of the ULMFiT process (source: course.fast.ai)\" width=\"700\" align=\"left\" caption=\"The ULMFiT process\" id=\"ulmfit_process\" src=https:\/\/i.imgur.com\/8XLluAn.png>","fa4a9658":"Finally, we want to load the encoder from the language model we trained earlier, so our classifier uses pre-trained weights.","52cf9d94":"### Fine-tuning the classifier\nNow we can train the classifier using *discriminative learning rates* and *gradual unfreezing*, which has been found to give better results for this type of model. First let's freeze all but the last layer:","94625ce6":"Here we told [`fastai`](https:\/\/docs.fast.ai\/) that we are working with text data, which is contained in the `text` column of a [`pandas`](https:\/\/pandas.pydata.org\/docs\/) [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html) called `df_lm`. We set [`is_lm=True`](https:\/\/docs.fast.ai\/text.data.html#TextDataLoaders) since we want to train a language model, so [`fastai`](https:\/\/docs.fast.ai\/) needs to label the input data for us. Finally, we told [`fastai`](https:\/\/docs.fast.ai\/) to hold out a random 10% of our data for a validation set using [`valid_pct=0.1`](https:\/\/docs.fast.ai\/text.data.html#TextDataLoaders).\n\nLet's take a look at the first two rows of the [`DataLoader`](https:\/\/docs.fast.ai\/data.load.html#DataLoader) using [`show_batch`](https:\/\/docs.fast.ai\/data.core.html#TfmdDL.show_batch).","d5e7f8be":"Let's save the model *encoder* so we can use it to fine-tune our classifier. The encoder is all of the model except for the final layer, which converts activations to probabilities of picking each token in the vocabulary. We want to keep the knowledge the model has learned about tweet language but we won't be using our classifier to predict the next word in a sentence, so we won't need the final layer any more.","25c3dfa1":"It looks like Indian Prime Minister Narendra Modi received the first dose of Indian developed Covaxin on 1st March. No wonder there were lots of tweets! To dig deeper, let's plot timelines for each vaccine indvidually.","899ae76f":"We can then make predictions using [`get_preds`](https:\/\/docs.fast.ai\/learner.html#Learner.get_preds):","137a1d1c":"## Timeline analysis\nA lot of the tweets appear to be from users in India:","bd72e28c":"These tweets are about countries starting their vaccination programme or receiving a new shipment of vaccines. Let's use the ['COVID-19 World Vaccination Progress'](https:\/\/www.kaggle.com\/gpreda\/covid-world-vaccination-progress) dataset to plot daily vaccinations for the mentioned countries:","4a981e76":"## Training a language model\nTo train our language model we can use self-supervised learning; we just need to give the model some text as an independent variable and [`fastai`](https:\/\/docs.fast.ai\/) will automatically preprocess it and create a dependent variable for us. We can do this in one line of code using the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) class, which converts our input data into a [`DataLoader`](https:\/\/docs.fast.ai\/data.load.html#DataLoader) object that can be used as an input to a [`fastai`](https:\/\/docs.fast.ai\/) [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner).","5aa2970e":"### Sinopharm","39e01b1c":"We can see spikes in positive sentiment after various countries agreed to produce the Sputkik V vaccine, and on March 11th after ABC news reported that it was the safest vaccine.","e4206c18":"### Oxford\/AstraZeneca","ac514680":"Here we passed [`language_model_learner`](https:\/\/docs.fast.ai\/text.learner.html#language_model_learner) our [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders), `dls_lm`, and the pre-trained [RNN](https:\/\/www.simplilearn.com\/tutorials\/deep-learning-tutorial\/rnn) model, [*AWD_LSTM*](https:\/\/docs.fast.ai\/text.models.awdlstm.html), which is built into [`fastai`](https:\/\/docs.fast.ai\/). [`drop_mult`](https:\/\/docs.fast.ai\/text.learner.html#text_classifier_learner) is a multiplier applied to all [dropouts](https:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/) in the AWD_LSTM model to reduce overfitting. For example, by default [`fastai`](https:\/\/docs.fast.ai\/)'s AWD_LSTM applies [`EmbeddingDropout`](https:\/\/docs.fast.ai\/text.models.awdlstm.html#EmbeddingDropout) with 10% probability (at the time of writing), but we told [`fastai`](https:\/\/docs.fast.ai\/) that we want to reduce that to 3%. The [`metrics`](https:\/\/docs.fast.ai\/metrics.html) we want to track are *perplexity*, which is the exponential of the loss (in this case cross entropy loss), and *accuracy*, which tells us how often our model predicts the next word correctly. We can also train with fp16 to use less memory and speed up the training process.\n\nWe can find a good learning rate for training using [`lr_find`](https:\/\/docs.fast.ai\/callback.schedule.html#Learner.lr_find) and use that to fit our model.","af5d5bb3":"On March 2nd Dolly Parton received her dose of the vaccine she helped fund, which explains the initial increase in positive tweets prior to the news about Moderna's collaboration with IBM. By looking at the vaccine progress, we can see that the median daily vaccinations per million in EU countries started to pull further ahead of the rest of the world after news that they would purchase up to 300m extra Moderna vaccines:","aa137ad3":"However, negative sentiment is increasing after [numerous countries have suspended the use of the vaccine over safety concerns](https:\/\/www.cornwalllive.com\/news\/cornwall-news\/every-country-stopped-using-astrazeneca-5189849). We can see that vaccination progress has slowed significantly over the past few days as a result:","c371446a":"This notebook will walk through steps 2 and 3 with [`fastai`](https:\/\/docs.fast.ai\/). Afterwards we can use our new classifier to analyse sentiment in the COVID-19 vaccine tweets and compare changes in sentiment over time to the progress of vaccination in different countries.","0cd66417":"Initialising the [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) is similar to before, but in this case we want a [`text_classifier_learner`](https:\/\/docs.fast.ai\/text.learner.html#text_classifier_learner).","e505437e":"As with Sinovac, most of the Sinopharm tweets appear to be positive news regarding countries receiving a shipment of the vaccine:","1fcf8291":"After a bit more training we can predict the next word in a tweet around 29% of the time. Let's test the model out by using it to write some random tweets (in this case it will generate some text following 'I love').","69390c0f":"Some notable dates:","605319e3":"After one epoch our language model is predicting the next word in a tweet around 25% of the time - not too bad! We can [`unfreeze`](https:\/\/docs.fast.ai\/learner.html#Learner.unfreeze) the entire model, find a more suitable learning rate and train for a few more epochs to improve the accuracy further.","c847da78":"Now all but the last three:","557f644c":"### Sinovac","a802967d":"We can see that the predominant sentiment is neutral, with more positive tweets than negative. It's encouraging that negative sentiment isn't higher! We can also visualise how sentiment changes over time:","75dc8801":"Some notable dates:","65e332af":"Interestingly, Canada shows up in the negative word cloud, as well as a couple of Canadian cities. Looking at a 'naive' word cloud for tweets containing 'Canada' shows us that this appears to be a political\/economic issue:","a2a926a3":"When we created our [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) the embeddings from the pre-trained AWD_LSTM model were merged with random embeddings added for words that weren't in the vocabulary. The pre-trained layers were also automatically frozen for us. Using [`fit_one_cycle`](https:\/\/docs.fast.ai\/callback.schedule.html#Learner.fit_one_cycle) with our [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) will train only the *new random embeddings* (i.e. words that are in our Twitter vocab but not the Wikipedia vocab) in the last layer of the neural network.","5ec0b6b3":"As we can see, Modi wasn't the only person to make news on March 1st; India's External Affairs Minister and a 100-year-old Hyderabad resident also received their first dose of Covaxin. On March 3rd, phase 3 trial results for Covaxin were published, showing 81% efficacy. It makes sense for there to be a spike in the number of neutral and positive tweets about Covaxin on those dates!","b4488101":"Let's go ahead and check out the results.","d11d5560":"### Sputnik V","869e0dcf":"We can see that daily vaccinations per million increased significantly in Colombia and Mexico after they received new shipments of vaccines. Daily vaccinations are also increasing rapidly in Hong Kong after Carrie Lam received the vaccine on February 22nd; however, progress has been slower in Thailand and the Philippines so far.","b5118dcc":"### Fine-tuning the language model\nThe next step is to create a language model using [`language_model_learner`](https:\/\/docs.fast.ai\/text.learner.html#language_model_learner).","247b056c":"Our model correctly predicts sentiment around 75% of the time. We could perhaps do better with a larger dataset as mentioned earlier, or different model hyperparameters. It might be worth experimenting with this yourself to see if you can improve the accuracy.\n\nWe can quickly sense check the model by calling [`predict`](https:\/\/docs.fast.ai\/learner.html#Learner.predict), which returns the predicted sentiment, the index of the prediction and predicted probabilities for negative, neutral and positive sentiment.","81320599":"Some notable dates:","57b87f4c":"### Moderna","e695a984":"At the time of writing Canada's vaccination progress has been slower than other developed nations, and people are predicting that it [might have an impact on Canada's economic recovery](https:\/\/www.theguardian.pe.ca\/news\/canada\/slow-vaccine-rollout-in-canada-expected-to-be-a-drag-on-economic-recovery-558857\/):","84deb44e":"## Loading the data\nFirst, let's import [`fastai`](https:\/\/docs.fast.ai\/)'s [`text`](https:\/\/docs.fast.ai\/tutorial.text.html) module and take a look at our data.","92a3a679":"## Training a sentiment classifier\nTo get the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) for our classifier let's use the [`DataBlock`](https:\/\/docs.fast.ai\/tutorial.datablock.html#Text) API this time, which is more customisable.","dd45069c":"Now freeze all but the last two layers:","b2b921d7":"Interestingly, there are small positive spikes on February 19th and March 6th, with people tweeting after receiving the vaccine:","e453f371":"# COVID-19 Vaccine Sentiment Analysis with fastai\nIn this notebook we will perform sentiment analysis on tweets about COVID-19 vaccines using the [`fastai`](https:\/\/docs.fast.ai\/) library. I will provide a brief overview of the process here, but a much more in-depth explanation of NLP with [`fastai`](https:\/\/docs.fast.ai\/) can be found in [lesson 8](https:\/\/course.fast.ai\/videos\/?lesson=8) of the [`fastai`](https:\/\/docs.fast.ai\/) course. For convenience clicking on inline code written like [`this`](https:\/\/docs.fast.ai\/tutorial.text.html) will take you to the relevant part of the [`fastai`](https:\/\/docs.fast.ai\/) documentation where appropriate.","3bc35ea8":"## Analysing the tweets\nTo carry out sentiment analysis on the vaccine tweets, we can add them to the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) as a test set:","0a202f58":"There was a big spike in the number of tweets on March 1st 2021, so let's investigate further.","8c468e7f":"We have a new column, `text_`, which is `text` offset by one. This is the dependent variable [`fastai`](https:\/\/docs.fast.ai\/) created for us. By default [`fastai`](https:\/\/docs.fast.ai\/) uses *word tokenization*, which splits the text on spaces and punctuation marks and breaks up words like *can't* into two separate tokens. [`fastai`](https:\/\/docs.fast.ai\/) also has some special tokens starting with 'xx' that are designed to make things easier for the model; for example [`xxmaj`](https:\/\/docs.fast.ai\/text.data.html) indicates that the next word begins with a capital letter and [`xxunk`](https:\/\/docs.fast.ai\/text.data.html) represents an unknown word that doesn't appear in the vocabulary very often. You could experiment with *subword tokenization* instead, which will split the text on commonly occuring groups of letters instead of spaces. This might help if you wanted to leave hashtags in since they often contain multiple words joined together with no spaces, e.g. #CovidVaccine. The [`fastai`](https:\/\/docs.fast.ai\/) tokenization process is explained in much more detail [here](https:\/\/youtu.be\/WjnwWeGjZcM?t=626) for those interested.","5eda4a19":"This looks pretty good! The positive tweets appear to be from people who have just received their first vaccine or are grateful for the job scientists and healthcare workers are doing, whereas the negative tweets seem to be from people who have suffered adverse reactions to the vaccine. The neutral tweets seem to be more like news, which could explain why it is the most prevelant sentiment; in fact, the vast majority of tweets contain urls:","5dbd8873":"There is a lot to unpack here, so to make things easier let's annotate some of the key dates:","ff6d182a":"To use the API, [`fastai`](https:\/\/docs.fast.ai\/) needs the following:\n* [`blocks`](https:\/\/docs.fast.ai\/data.block.html#TransformBlock):\n    * [`TextBlock`](https:\/\/docs.fast.ai\/text.data.html#TextBlock): Our x variable will be text contained in a [`pandas`](https:\/\/pandas.pydata.org\/docs\/) [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html). We want to use the same sequence length and vocab as the language model [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) so we can make use of our pre-trained model.\n    * [`CategoryBlock`](https:\/\/docs.fast.ai\/data.block.html#CategoryBlock): Our y variable will be a single-label category (negative, neutral or positive sentiment).\n* [`get_x`](https:\/\/docs.fast.ai\/data.transforms.html#ColReader), [`get_y`](https:\/\/docs.fast.ai\/data.transforms.html#ColReader): Get data for the model by reading the `text` and `sentiment` columns from the [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n* [`splitter`](https:\/\/docs.fast.ai\/data.transforms.html#RandomSplitter): We will use [`RandomSplitter()`](https:\/\/docs.fast.ai\/data.transforms.html#RandomSplitter) to randomly split the data into a training set (80% by default) and a validation set (20%).\n* [`dataloaders`](https:\/\/docs.fast.ai\/data.block#DataBlock.dataloaders): Builds the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) using the [`DataBlock`](https:\/\/docs.fast.ai\/tutorial.datablock.html#Text) template we just defined, the *df_clas* [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html) and a batch size of 64.\n\nWe can call show batch as before; this time the dependent variable is sentiment.","0a9fa9c0":"We could use the `text` column of this dataset to train a Twitter language model, but since our end goal is sentiment analysis we will need to find another dataset that also contains sentiment labels to train our classifier. Let's use ['Complete Tweet Sentiment Extraction Data'](https:\/\/www.kaggle.com\/maxjon\/complete-tweet-sentiment-extraction-data), which contains 40,000 tweets labelled as either negative, neutral or positive sentiment. For more accurate results you could use the ['sentiment140'](https:\/\/www.kaggle.com\/kazanova\/sentiment140) dataset instead, which contains 1.6m tweets labelled as either positive or negative.","634a37ea":"For our language model, the only input we need is the tweet text. As we will see in a moment [`fastai`](https:\/\/docs.fast.ai\/) can handle text preprocessing and tokenization for us, but it might be a good idea to remove things like twitter handles, urls, hashtags and emojis first. You could experiment with leaving these in for your own models and see how it affects the results. There are also some rows with blank tweets which need to be removed.\n\nWe ideally want the language model to learn not just about tweet language, but more specifically about vaccine tweet language. We can therefore use text from both datasets as input for the language model. For the classification model we need to remove all rows with missing sentiment, however.","1341f48b":"## Conclusion\n`fastai` make NLP really easy and we were able to get quite good results with a limited dataset and not a lot of training time by using the ULMFiT approach. To summarise, the steps are:\n1. Fine-tune a language model to predict the next word in a tweet, using a model pre-trained on Wikipedia.\n2. Fine-tune a classification model to predict tweet sentiment using the pre-trained language model.\n3. Apply the classifier to unlabelled tweets to analyse sentiment.\n\nWe were also able to use the model to gain a lot of insights about how sentiment has changed over time for each vaccine and how that relates to the overall progress of vaccination programmes in different countries. I hope you found this useful, and thanks very much to [Gabriel Preda](https:\/\/www.kaggle.com\/gpreda) for providing the data!","9b5ae31b":"Finally, let's unfreeze the entire model and train a bit more:","e1d8b277":"We can see that Hungary ramped up their vaccination programme after the news on February 18th that they would [become the first EU country to start administering Sinopharm](https:\/\/www.gbcghanaonline.com\/world\/hungary-becomes-first-eu-country-to-administer-sinopharm-vaccine\/2021\/). In addition, Senegal started vaccinating shortly after positive tweets confirmed that they had received a shipment of Sinopharm vaccines. Unfortunately there is no data for Iraq, but they also [started their programme just hours after receiving a donation of vaccines](https:\/\/medicalxpress.com\/news\/2021-03-iraq-vaccine-china-doses.html) from China.","d292cc4f":"### Further analysis using 'smarter' word clouds\nThe final thing we will do is to generate word clouds to see which words are indicative of each sentiment. The code below is from [this notebook](https:\/\/www.kaggle.com\/pawanbhandarkar\/generate-smarter-word-clouds-with-log-likelihood), which contains a more detailed explanation of the methodology used to generate 'smarter' word clouds. Please go and upvote the original notebook if you find this part useful!","22762820":"### Covaxin"}}