{"cell_type":{"ba61bfc3":"code","a08364fe":"code","ada69101":"code","fa16f8f1":"code","12fd5f86":"code","a9fe3b15":"code","5fddaed0":"code","27d83802":"code","86577847":"code","9f5b497d":"code","596baaa2":"code","1dd20ad9":"code","cfc0e0e9":"code","2720ce4c":"code","ff13ad5f":"code","fe21d24b":"code","eab53905":"code","16cff98a":"code","6c2e9070":"code","5648956d":"code","4f623116":"code","097368c0":"code","06bd9e66":"code","a1b1cd35":"code","19a66e8c":"code","9510fbdf":"code","7f375784":"code","1a12846e":"code","c25d4557":"code","d9f0b788":"code","96d5a63a":"code","0a61ecb2":"code","ef5e4f01":"code","80df5bf0":"code","0b85cced":"code","a1c870bd":"code","3cd357bb":"code","0255ebad":"code","33921252":"code","e33c191d":"code","3ee9dc22":"code","8b7c04e1":"code","221cf0fc":"code","d133e3dc":"code","3c72f6fb":"code","a85bf748":"code","aaf98e6e":"code","005b6fc3":"code","50aff346":"code","8d33124a":"code","2f1fe037":"code","60edf616":"code","f067a5c8":"code","6dc3cb0e":"code","5c78ca09":"code","19e5dd9c":"code","bc4d402e":"code","e7ca944f":"code","8d900d2e":"markdown","a4cd510c":"markdown","3b6a6d2d":"markdown","50547ae5":"markdown","239d5857":"markdown","6686f143":"markdown","a9e6a22c":"markdown","ed5ea12d":"markdown","acf31da0":"markdown","b2940b11":"markdown","1ca15688":"markdown","48c01e19":"markdown","c084b62a":"markdown","c9088752":"markdown"},"source":{"ba61bfc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a08364fe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report,recall_score,precision_score, f1_score, accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,  RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier","ada69101":"df = pd.read_csv(\"..\/input\/wine-quality-prediction\/QualityPrediction.csv\")\ndf.head()","fa16f8f1":"print(\"Dimension of the data :\", df.shape)","12fd5f86":"df.info()","a9fe3b15":"df.describe()","5fddaed0":"df.isnull().sum()","27d83802":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = df)","86577847":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"citric acid\", data = df)","9f5b497d":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"volatile acidity\", data = df)","596baaa2":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"residual sugar\", data = df)","1dd20ad9":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"chlorides\", data = df)","cfc0e0e9":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"free sulfur dioxide\", data = df)","2720ce4c":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"total sulfur dioxide\", data = df)","ff13ad5f":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"density\", data = df)","fe21d24b":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"pH\", data = df)","eab53905":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"sulphates\", data = df)","16cff98a":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = \"quality\", y = \"alcohol\", data = df)","6c2e9070":"df[\"quality\"].unique()","5648956d":"df[\"good_quality\"] = [1 if i >=7 else 0 for i in df[\"quality\"]]\ndf[\"good_quality\"].value_counts()","4f623116":"le = LabelEncoder()\nle.fit(df[\"good_quality\"])\ny_transform = le.fit_transform(df[\"good_quality\"])\nprint(y_transform)\nprint(y_transform.shape)\nnp.unique(y_transform)","097368c0":"sns.countplot(df[\"good_quality\"])","06bd9e66":"features = df.iloc[:,:11]\nfeatures[:5]","a1b1cd35":"x =  df.iloc[:,:11].values\n#x = df.drop([\"quality\",\"good_quality\"], axis = 1)\nprint(x.shape)\n\ny = df[\"good_quality\"].values\nprint(y)","19a66e8c":"# Normalize feature variables\nfrom sklearn.preprocessing import StandardScaler\nX_features = x\nx = StandardScaler().fit_transform(x)","9510fbdf":"x_train, x_test , y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 42)","7f375784":"print(x_train.shape)\nprint(y_train.shape)","1a12846e":"dt = DecisionTreeClassifier(max_depth = 4, min_samples_split = 2, random_state = 42)\ndt.fit(x_train, y_train)","c25d4557":"dt.score(x_train, y_train)","d9f0b788":"dt.score(x_test, y_test)","96d5a63a":"y_pred_dt = dt.predict(x_test)\ny_pred_dt[:5]","0a61ecb2":"c_m = confusion_matrix(y_test,y_pred_dt)\nprint(\"Confusion Matrix :\\n\",c_m, \"\\n\")\n\nc_r = classification_report(y_test,y_pred_dt)\nprint(\"Classfication Report : \\n\", c_r)\n\ndt_score = accuracy_score(y_test, y_pred_dt)\n\nprint(\"Accuracy:\",dt_score)","ef5e4f01":"rf = RandomForestClassifier()\nrf.fit(x_train, y_train)","80df5bf0":"rf.score(x_train, y_train)","0b85cced":"rf.score(x_test, y_test)","a1c870bd":"y_pred_rf = rf.predict(x_test)\ny_pred_rf[:5]","3cd357bb":"c_m = confusion_matrix(y_test,y_pred_rf)\nprint(\"Confusion Matrix :\\n\",c_m, \"\\n\")\n\nc_r = classification_report(y_test,y_pred_rf)\nprint(\"Classfication Report : \\n\", c_r)\n\nrf_score = accuracy_score(y_test, y_pred_rf)\n\nprint(\"Accuracy:\",rf_score)","0255ebad":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)","33921252":"nb.score(x_train,y_train)","e33c191d":"nb.score(x_test,y_test)","3ee9dc22":"y_pred_nb = nb.predict(x_test)","8b7c04e1":"c_m = confusion_matrix(y_test,y_pred_nb)\nprint(\"Confusion Matrix :\\n\",c_m, \"\\n\")\n\nc_r = classification_report(y_test,y_pred_nb)\nprint(\"Classfication Report : \\n\", c_r)\n\nnb_score = accuracy_score(y_test, y_pred_nb)\n\nprint(\"Accuracy:\",nb_score)","221cf0fc":"from sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier()\nkn.fit(x_train,y_train)","d133e3dc":"y_pred_kn = kn.predict(x_test)","3c72f6fb":"kn.score(x_train, y_train)","a85bf748":"kn.score(x_test, y_test)","aaf98e6e":"c_m = confusion_matrix(y_test,y_pred_kn)\nprint(\"Confusion Matrix :\\n\",c_m, \"\\n\")\n\nc_r = classification_report(y_test,y_pred_kn)\nprint(\"Classfication Report : \\n\", c_r)\n\nkn_score = accuracy_score(y_test, y_pred_kn)\n\nprint(\"Accuracy:\",kn_score)","005b6fc3":"parameters = {'max_depth': [2,4,6,8,10],\n              'criterion': ['gini', 'entropy'],\n              'bootstrap': [True, False],\n              'max_features': ['auto', 'sqrt', 'log2', None],\n              #'min_samples_leaf': [3, 4, 5],\n              #'min_samples_split': [4, 6, 8],\n              'n_estimators': [100, 200, 300, 1000]\n              #\"oob_score\": [True,False]\n             }","50aff346":"grid_cv = GridSearchCV(rf, param_grid = parameters, cv = 5, n_jobs = -1)\n\ngrid_cv.fit(x_train, y_train)\n\nprint(\"Best parameters :\",grid_cv.best_params_)","8d33124a":"# set the parameters as given\nrf.set_params(bootstrap = True, \n              criterion = 'gini',\n              max_depth = 10, \n              max_features = \"log2\",\n              n_estimators = 100)","2f1fe037":"rf.score(x_train, y_train)  ","60edf616":"rf.score(x_test, y_test)  ","f067a5c8":"y_pred_rf  = rf.predict(x_test)","6dc3cb0e":"c_m = confusion_matrix(y_test,y_pred_rf)\nprint(\"Confusion Matrix :\\n\",c_m, \"\\n\")\n\nc_r = classification_report(y_test,y_pred_rf)\nprint(\"Classfication Report : \\n\", c_r)\n\nrf_score = accuracy_score(y_test, y_pred_rf)\n\nprint(\"Accuracy:\",rf_score)\nprint(\"Error:\",1 - rf_score)","5c78ca09":"rf_cv = cross_val_score(estimator = rf, X = x_train, y = y_train, cv = 10)\nprint(rf_cv)\nprint(rf_cv.mean())","19e5dd9c":"summary = pd.DataFrame({\"Model Used\" : \n                        [\"Decision Tree\", \"Random Forest\", \"Gaussian NB\", \"KNearest Neighbor\"],\n                       \"Scores\" : [dt_score, rf_score, nb_score, kn_score]})\nsummary = summary.set_index(\"Scores\")\nsummary = summary.sort_values(by = \"Scores\", ascending = False)\nsummary","bc4d402e":"## feature importance\n\ns = rf.feature_importances_\ne = df.columns[0:11]\nfor s, e in zip(s, e):\n    s = round(s*100,4)\n    print(e ,\":\",s)\n\n\n#for i in range(len(e)):\n#    print(e[i])\n#    print(s[i],\"\\n\")","e7ca944f":"l = []\nfor i in df.columns[0:11]:\n    print(i)\n    n = input(\"\")\n    l.append(n)\nprint(l)    \n\nrf.predict([l])\nif l == 0 :\n    print(\"The quality of wine is good.\")\nelse:\n    print(\"The quality of wine is bad.\")","8d900d2e":"## Check Data Types & Data Cleaning","a4cd510c":"### Using RandomForest","3b6a6d2d":"## Get Data","50547ae5":"### Using DecisionTree ","239d5857":"### Using GaussianNB","6686f143":"## Hyperparameter tuning for RandomForest","a9e6a22c":"## Summary ","ed5ea12d":"## Visualization","acf31da0":"## EDA","b2940b11":"## Splitting the dataset into Train-Test ","1ca15688":"### Using KNN","48c01e19":"## Importing Libraries","c084b62a":"## Predict the values","c9088752":"## Cross_Validation"}}