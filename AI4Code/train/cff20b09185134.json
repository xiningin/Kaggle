{"cell_type":{"75aa3cda":"code","c6936c9f":"code","802b261c":"code","d8798d17":"code","bd1f533a":"code","330e3189":"code","8691341a":"code","15f4ecc9":"code","6654985a":"code","a2912eb0":"code","ec9c0628":"code","df0a86fb":"code","ea54e343":"code","e29298f7":"code","92d5395b":"code","ddc61d28":"code","8bca5dbb":"markdown","d477cc52":"markdown","0b1c9199":"markdown","5428e280":"markdown","f96e1d70":"markdown","90e7e50f":"markdown","9407fb49":"markdown","0e34d65d":"markdown","41470cd2":"markdown","78923e55":"markdown","3d02a10f":"markdown","5feb2658":"markdown"},"source":{"75aa3cda":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n#print(os.listdir(\"..\/input\"))","c6936c9f":"train=pd.read_csv(\"..\/input\/train.csv\", nrows=6000000,dtype={'acoustic_data':np.int16,'time_to_failure':np.float64})\ntrain.head(10)","802b261c":"#Lets plot the data to see and understand the data columns and our problem .\n#We will use a small subset of dataset for understanding the pattern ,since the data is large\n\ntrain_acoustic_df = train['acoustic_data'].values[::100]\ntrain_time_to_failure_df = train['time_to_failure'].values[::100]\n\nfig, ax1 = plt.subplots(figsize=(10,10))\nplt.title('Acoustic data and Time to Failure')\nplt.plot(train_acoustic_df, color='r')\nax1.set_ylabel('acoustic data', color='r')\nplt.legend(['acoustic data'], loc=(0.01, 0.9))\n\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_df, color='b')\nax2.set_ylabel('time to failure', color='b')\nplt.legend(['time to failure'], loc=(0.01, 0.8))\n\nplt.grid(True)\n\n    \n\n","d8798d17":"def gen_features(X):\n    fe = []\n    fe.append(X.mean())\n    fe.append(X.std())\n    fe.append(X.min())\n    fe.append(X.max())\n    fe.append(X.kurtosis())\n    fe.append(X.skew())\n    fe.append(np.quantile(X,0.01))\n    fe.append(np.quantile(X,0.05))\n    fe.append(np.quantile(X,0.95))\n    fe.append(np.quantile(X,0.99))\n    fe.append(np.abs(X).max())\n    fe.append(np.abs(X).mean())\n    fe.append(np.abs(X).std())\n    return pd.Series(fe)","bd1f533a":"#Lets read the training set again now in chunks and append features \ntrain = pd.read_csv('..\/input\/train.csv', iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\nX_train = pd.DataFrame()\ny_train = pd.Series()\nfor df in train:\n    ch = gen_features(df['acoustic_data'])\n    X_train = X_train.append(ch, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))","330e3189":"X_train.head(10) #Let's check the training dataframe","8691341a":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id') #Taking the segment id from sample_submission file","15f4ecc9":"#Applying Feature Engineering on test data files\nX_test = pd.DataFrame()\nfor seg_id in submission.index:\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    ch = gen_features(seg['acoustic_data'])\n    X_test = X_test.append(ch, ignore_index=True)","6654985a":"X_test.head(10) #Lets check the testing dataframe","a2912eb0":"#Catboost regressor model \n\"\"\"       \n#Catboost Regressor model\n\ntrain_pool = Pool(X_train, y_train)\n\nm = CatBoostRegressor(iterations=10000, loss_function='MAE', boosting_type='Ordered')\nm.fit(X_train, y_train, silent=True)\nm.best_score_\n\"\"\"","ec9c0628":"#Scale Train Data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = pd.DataFrame(scaler.transform(X_train))\nX_train_scaled.head(10)","df0a86fb":"#We will also scale the train data\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))\nX_test_scaled.head(10)","ea54e343":"parameters = [{'gamma': [0.001, 0.005, 0.01, 0.02, 0.05, 0.1],\n               'C': [0.1, 0.2, 0.25, 0.5, 1, 1.5, 2]}]\n\nreg1 = GridSearchCV(SVR(kernel='rbf', tol=0.01), parameters, cv=5, scoring='neg_mean_absolute_error')\n","e29298f7":"reg1.fit(X_train_scaled, y_train.values.flatten())\n","92d5395b":"submission.time_to_failure = reg1.predict(X_test_scaled) \nsubmission","ddc61d28":"submission.to_csv('submission.csv',index=True)","8bca5dbb":"**Submission**","d477cc52":"**3) Feature Engineering**","0b1c9199":"*We will be using the SVR model for prediction and GridSearchCV for hyperparameter tuning of the model.*","5428e280":"**4) Catboost Regressor ** *(optional)*","f96e1d70":"**6) Support Vector Regression**","90e7e50f":"**5) Scale the Data**","9407fb49":"**2) Exploratory Data Analysis**","0e34d65d":"The size of Train data is large . The two columns in the train dataset have the following meaning:\n\n1. *accoustic_data*: is the accoustic signal measured in the laboratory experiment;\n2. *time to failure*: this gives the time until a failure will occurs.\n\n**The above plot shows that the failure occur after a large oscilation and also that the large oscilation is followed by a series of small minor oscilations before the final time of failure .**","41470cd2":"**Earthquake Prediction Model using Laboratory Data**\n\nForecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be.\n\nIn this notebook we will work on addressing when the earthquake will take place. \n*We will try to predict the time remaining before laboratory earthquakes occur from real-time seismic data.*\n\nPredicting earthquake can save billions of dollars in infrastructure **but above all a large number of human lives can be saved.**","78923e55":"**Let's Pray that this Year no major earthquake occurs and people remain safe.  **","3d02a10f":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated .\n\nPlease drop down suggestions and comments if any, so that i can learn to build better solutions.\n\n**Thank You :-)**","5feb2658":"**1)  Load the Data**"}}