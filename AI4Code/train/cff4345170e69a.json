{"cell_type":{"397effc0":"code","f8e48ea7":"code","fcd8feec":"code","15db8a05":"code","d0f77400":"code","f1ddfb2b":"code","838c1409":"code","20d09186":"code","c48d25dc":"code","01825dcd":"code","3c5297d1":"code","c3d69452":"code","7d7a202f":"code","5dc24217":"code","71e001ad":"code","690cddd8":"code","525f2e26":"code","ddc2fb66":"code","602507df":"code","509fcaa4":"code","cbdac59c":"code","d822e942":"code","bee8d263":"markdown","67699fb2":"markdown","1d4ad866":"markdown","27969c7f":"markdown","8cbbc809":"markdown","a1908a96":"markdown","89fe73c4":"markdown","183add67":"markdown","cdcab631":"markdown","ff656cee":"markdown","49dcb308":"markdown","b9307cde":"markdown","109dfa89":"markdown","624b2171":"markdown","a58497c8":"markdown","5a6e6fd6":"markdown","465596e6":"markdown","31978b9d":"markdown"},"source":{"397effc0":"import nltk\n\nnltk.download(\"gutenberg\")","f8e48ea7":"hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\nprint(hamlet_raw[:1000])","fcd8feec":"from nltk.tokenize import sent_tokenize\n\nsentences = sent_tokenize(hamlet_raw)\n\nprint(sentences[:10])\n","15db8a05":"from nltk.tokenize import word_tokenize\n\nwords = word_tokenize(sentences[0])\n\nprint(words)","d0f77400":"from nltk.corpus import stopwords\n\nstopwords_list = stopwords.words('english')\n\nprint(stopwords_list)","f1ddfb2b":"non_stopwords = [w for w in words if not w.lower() in stopwords_list]\nprint(non_stopwords)","838c1409":"import string\npunctuation = string.punctuation\nprint(punctuation)","20d09186":"non_punctuation = [w for w in non_stopwords if not w in punctuation]\n\nprint(non_punctuation)","c48d25dc":"from nltk import pos_tag\n\npos_tags = pos_tag(words)\n\nprint(pos_tags)","01825dcd":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\nsample_sentence = \"He has already gone\"\nsample_words = word_tokenize(sample_sentence)\n\nstems = [stemmer.stem(w) for w in sample_words]\n\nprint(stems)","3c5297d1":"nltk.download('wordnet')","c3d69452":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\npos_tags = nltk.pos_tag(sample_words)\n\nlemmas = []\nfor w in pos_tags:\n    if w[1].startswith('J'):\n        pos_tag = wordnet.ADJ\n    elif w[1].startswith('V'):\n        pos_tag = wordnet.VERB\n    elif w[1].startswith('N'):\n        pos_tag = wordnet.NOUN\n    elif w[1].startswith('R'):\n        pos_tag = wordnet.ADV\n    else:\n        pos_tag = wordnet.NOUN\n        \n    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n    \nprint(lemmas)","7d7a202f":"from nltk import word_tokenize\n\nfrase = 'o cachorro correu atr\u00e1s do gato'\n\n\nngrams = [\"%s %s %s\" % (nltk.word_tokenize(frase)[i], \\\n                      nltk.word_tokenize(frase)[i+1], \\\n                      nltk.word_tokenize(frase)[i+2]) \\\n          for i in range(len(nltk.word_tokenize(frase))-2)]\n\nprint(ngrams)\n","5dc24217":"non_punctuation = [w for w in words if not w.lower() in punctuation]\n\nn_grams_3 = [\"%s %s %s\"%(non_punctuation[i], non_punctuation[i+1], non_punctuation[i+2]) for i in range(0, len(non_punctuation)-2)]\n\nprint(n_grams_3)","71e001ad":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(ngram_range=(3,3))\n\nimport numpy as np\n\narr = np.array(sentences[0:10])\n\nprint(arr)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray())\n\nprint(count_vect.vocabulary_)","690cddd8":"arr = np.array(sentences)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray()[:20])\n\nprint([k for k in count_vect.vocabulary_.keys()][:20])","525f2e26":"from nltk.tokenize import sent_tokenize\nnltk.corpus.reuters.fileids()\nreuters_raw = nltk.corpus.reuters.raw()\n\nprint(reuters_raw[:100])","ddc2fb66":"from nltk.tokenize import word_tokenize\nwords = word_tokenize(reuters_raw)\nprint(words[:100])","602507df":"from nltk import pos_tag\npos_tags = pos_tag(words)\nprint(pos_tags[:100])","509fcaa4":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\nlemmas = []\nfor w in pos_tags:\n    if w[1].startswith('J'):\n        pos_tag = wordnet.ADJ\n    elif w[1].startswith('V'):\n        pos_tag = wordnet.VERB\n    elif w[1].startswith('N'):\n        pos_tag = wordnet.NOUN\n    elif w[1].startswith('R'):\n        pos_tag = wordnet.ADV\n    else:\n        pos_tag = wordnet.NOUN\n        \n    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n    \nprint(lemmas[:100])","cbdac59c":"from nltk.corpus import stopwords\nstopwords_list = stopwords.words('english')\nnon_stopwords = [w for w in lemmas if not w.lower() in stopwords_list]\n\nimport string\npunctuation = string.punctuation\nnon_punctuation = [w for w in non_stopwords if not w in punctuation]\nprint(non_punctuation[:100])","d822e942":"from collections import Counter\nx = Counter(non_punctuation)\nx.most_common(10)","bee8d263":"<b>1. Baixando o corpus Gutenberg<\/b>","67699fb2":"<p><b>Exerc\u00edcio 2:<\/b>Exiba 10 lemmas mais frequentes do corpus Reuters, ignorando pontua\u00e7\u00f5es e stopwords.<\/p>\n\n","1d4ad866":"Tamb\u00e9m podemos usar a classe <b>CountVectorizer<\/b>, do scikit-learn:","27969c7f":"Al\u00e9m da t\u00e9cnica de <i>Bag-of-Words<\/i>, outra op\u00e7\u00e3o \u00e9 utilizar n-gramas (onde \"n\" pode variar)","8cbbc809":"<b>4. Removendo stopwords e pontua\u00e7\u00e3o<\/b>","a1908a96":"<b>5. Part of Speech (POS) Tags <\/b>","89fe73c4":"As tags indicam a classifica\u00e7\u00e3o sint\u00e1tica de cada palavra no texto. Ver <a href=\"https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" target=\"blank\">https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html<\/a> para uma lista completa","183add67":"Consiste em dividir o texto em senten\u00e7as. Por\u00e9m, n\u00e3o \u00e9 uma tarefa t\u00e3o trivial quanto usar uma fun\u00e7\u00e3o split(\".\") no Python, por exemplo. Isso porque o uso do ponto (.) nem sempre \u00e9 empregado como ponto final em uma senten\u00e7a. Por exemplo:\n\n\"Dr. Rodolfo S. Silva, um renomado cardiologista, adquiriu um novo consult\u00f3rio na Av. Edson A. Nascimento pela quantia de R$ 4.5 milh\u00f5es, que atender\u00e1 pacientes do S.U.S. e do plano Sa\u00fade Mais Inc.\"\n\nUma das t\u00e9cnicas mais conhecidas para resolver esse problema \u00e9 o algoritmo Punkt. Trata-se de um modelo que consiste em identificar abrevia\u00e7\u00f5es, retic\u00eancias, iniciais e n\u00fameros ordinais para, a\u00ed ent\u00e3o, identificar os pontos finais em senten\u00e7as.\n\n![Arquitetura Punkt](https:\/\/www.researchgate.net\/profile\/Jan_Strunk\/publication\/220355311\/figure\/fig1\/AS:277323623485449@1443130512700\/Architecture-of-the-Punkt-System.png)","cdcab631":"<p>Vamos avaliar as t\u00e9cnicas mais comuns para prepararmos o texto para usar com algoritmos de aprendizado de m\u00e1quina logo mais.<\/p>\n<p>Como estudo de caso, vamos usar o texto de <i>Hamlet<\/i>, encontrado no corpus <i>Gutenberg<\/i> do pacote <b>NLTK<\/b><\/p>","ff656cee":"Agora, vamos contar os n-grams (no nosso caso, trigramas) de todas as senten\u00e7as do texto:","49dcb308":"<h2> T\u00e9cnicas para Pr\u00e9-Processamento <\/h2>","b9307cde":"<b>7. N-gramas<\/b>","109dfa89":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 02 - T\u00e9cnicas de Pr\u00e9-Processamento de Texto<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>","624b2171":"<b>2. Exibindo o texto \"Hamlet\"<\/b>","a58497c8":"<b>6. Stemming e Lemmatization<\/b>","5a6e6fd6":"Stemming permite obter a \"raiz\" da palavra, removendo sufixos, por exemplo.","465596e6":"<b>3. Segmenta\u00e7\u00e3o de senten\u00e7as e tokeniza\u00e7\u00e3o de palavras<\/b>","31978b9d":"J\u00e1 lemmatization vai al\u00e9m de somente remover sufixos, obtendo a raiz lingu\u00edstica da palavra. Vamos usar as tags POS obtidas anteriormente para otimizar o lemmatizer."}}