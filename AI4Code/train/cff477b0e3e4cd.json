{"cell_type":{"5029dbf8":"code","42edc673":"code","1aa0d3fa":"code","0661c940":"code","4b7fd599":"code","b681401a":"code","7164aabf":"code","d7b0acd6":"code","e6c9fcac":"code","e3d13c7b":"code","13451dd5":"code","92f400a8":"code","3d15f68e":"code","d47d0cca":"code","e194545e":"code","da788886":"code","afacd842":"code","079bfe75":"code","65ce4e7f":"code","489d6a1d":"code","3b9de7ad":"code","57ef8c76":"code","8df9980d":"code","3336082f":"code","d7d21869":"code","867ff167":"code","06c5b35b":"code","660190ca":"code","feef85eb":"code","4f436744":"code","372505f4":"code","bd39faa6":"code","e2b12220":"code","59da12bd":"code","7474c67e":"code","2c0d0bc7":"code","d13e1794":"code","22481aa4":"code","ef8c7095":"code","f8a3bba1":"code","134ea074":"code","d08d332e":"code","44967b51":"code","74cca646":"code","ac8a4144":"code","89af23a2":"code","4328f72f":"code","43f5485e":"code","0fbc6c81":"code","3ad5943e":"code","b5d45b05":"code","f56be089":"code","f475be6f":"code","2cef8a9f":"code","86dfc36a":"code","1ba1f245":"code","b3c1e30b":"code","f1da7dee":"code","ca98bb8b":"code","62f86e4e":"code","5d9b5e86":"code","39ac1bb0":"code","acf123bc":"code","24b0384c":"code","88748f02":"code","bcbfc117":"code","fe595c69":"code","d64a2500":"code","d0b70118":"code","65b30502":"code","e0ba8010":"code","d56840aa":"code","56949225":"code","080997c3":"code","9f5c0ec9":"code","fed6fa13":"code","3fea439e":"code","39869593":"code","a3681621":"code","6d67c9a6":"code","6bbf90d9":"code","65fddaeb":"code","f6c8f8df":"code","5815d2fc":"code","e3792298":"code","48b26e10":"code","a5475dd1":"code","5bbba516":"code","af580deb":"code","656c62cc":"code","480f83c7":"code","087deb0b":"code","424b56fe":"code","c02ef07d":"code","00dc0096":"code","3c450e53":"code","ae89b630":"code","23728da5":"code","4b99fcd1":"code","6f1fa96a":"code","e5a5f85b":"code","26c80c2a":"code","c4b80ef5":"code","fddbd6e8":"code","a4849bd3":"code","04e59faa":"code","69fb0f3c":"code","0820dab0":"code","9607f7c2":"code","19e1bea6":"code","3c0da34f":"code","2322c2f5":"code","414b65e0":"code","10bc780b":"code","dbb35fc1":"code","9900122d":"markdown","aa6a91a0":"markdown","429bb1c8":"markdown","57d695dc":"markdown","b7bde683":"markdown","6654a600":"markdown","903e1f7b":"markdown","45c68a74":"markdown","722a915c":"markdown"},"source":{"5029dbf8":"import numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt","42edc673":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aa0d3fa":"wrng=pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nwrng","0661c940":"df=pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nd=df\ndf","4b7fd599":"df['Outcome'].value_counts()","b681401a":"plt.figure(figsize=(14,10))\nsn.heatmap(df.corr(),annot=True)\nplt.show()","7164aabf":"r=df[\"Outcome\"]\ndf=df.drop([\"Outcome\"],axis=1)\ndf.corr()","d7b0acd6":"sn.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsn.heatmap(df.corr(), vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"black\")\nplt.title('Correlation between features');","e6c9fcac":"d.columns","e3d13c7b":"d.describe(include='all')","13451dd5":"d.info()","92f400a8":"d.isnull().sum()","3d15f68e":"fig,ax=plt.subplots(4,2,figsize=(16,16))\nsn.distplot(d.Age, bins = 20, ax=ax[0,0]) \nsn.distplot(d.Pregnancies, bins = 20, ax=ax[0,1]) \nsn.distplot(d.Glucose, bins = 20, ax=ax[1,0]) \nsn.distplot(d.BloodPressure, bins = 20, ax=ax[1,1]) \nsn.distplot(d.SkinThickness, bins = 20, ax=ax[2,0])\nsn.distplot(d.Insulin, bins = 20, ax=ax[2,1])\nsn.distplot(d.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsn.distplot(d.BMI, bins = 20, ax=ax[3,1]) ","d47d0cca":"sn.pairplot(d, x_vars=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'], y_vars='Outcome', height=7, aspect=0.7, kind='reg');","e194545e":"for x1 in df.columns:\n    for y1 in df.columns:\n        sn.lmplot(x=x1,y=y1,data=d,hue='Outcome')","da788886":"for x2 in d.columns:\n    print (x2)","afacd842":"for x2 in d.columns:\n    sn.FacetGrid(d, hue = 'Outcome' , size = 5)\\\n      .map(sn.distplot , x2)\\\n      .add_legend()\n    plt.show() ","079bfe75":"sn.set_style(\"whitegrid\")\nsn.pairplot(d,hue=\"Outcome\",size=3);\nplt.show()","65ce4e7f":"tmp=pd.cut(d['Age'],[18,30,42,54,66,78,80])","489d6a1d":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['Glucose'],hue=d['Outcome'])","3b9de7ad":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['Pregnancies'],hue=d['Outcome'])","57ef8c76":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['BMI'],hue=d['Outcome'])","8df9980d":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['BloodPressure'],hue=d['Outcome'])","3336082f":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['Insulin'],hue=d['Outcome'])","d7d21869":"plt.figure(figsize=(10,6))\nsn.boxplot(x=tmp,y=d['DiabetesPedigreeFunction'],hue=d['Outcome'])","867ff167":"d['SkinThickness'].max()","06c5b35b":"tmp=pd.cut(d['SkinThickness'],[0,15,30,45,60,75,90,105])","660190ca":"for y1 in d.columns:\n    plt.figure(figsize=(10,6))\n    sn.boxplot(x=tmp,y=d[y1],hue=d['Outcome'])","feef85eb":"df[df['SkinThickness']>60]","4f436744":"d.loc[(d['SkinThickness'] == 0) , 'SkinThickness' ] = np.nan\nd['SkinThickness'] = d['SkinThickness'].fillna(d['SkinThickness'].median())","372505f4":"d['BloodPressure'].max()","bd39faa6":"d['BloodPressure'].min()","e2b12220":"tmp=pd.cut(d['BloodPressure'],[0,30,60,90,120,150])\nfor y1 in d.columns:\n    plt.figure(figsize=(10,6))\n    sn.boxplot(x=tmp,y=d[y1],hue=d['Outcome'])","59da12bd":"d.loc[(d['BloodPressure'] < 30) | (d['BloodPressure']>120) , 'BloodPressure' ]","7474c67e":"d.loc[ d['BloodPressure'] == 0 , 'BloodPressure' ] = np.nan\nd['BloodPressure'] = d['BloodPressure'].fillna(d['BloodPressure'].median())","2c0d0bc7":"d.loc[(d['BloodPressure'] < 30) | (d['BloodPressure']>120) , 'BloodPressure' ]","d13e1794":"d[d['BloodPressure']<35]","22481aa4":"d.loc[d['BMI']==0,'BMI']= np.nan\nd['BMI'] = d['BMI'].fillna(d['BMI'].median())","ef8c7095":"d['BMI'].min()","f8a3bba1":"d['BMI'].max()","134ea074":"d.Age.max()","d08d332e":"d.Age.min()","44967b51":"d.Glucose.min()","74cca646":"d.Glucose.max()","ac8a4144":"tmp=pd.cut(d['Glucose'],[0,30,60,90,120,150,180,210])\nfor y1 in d.columns:\n    plt.figure(figsize=(10,6))\n    sn.boxplot(x=tmp,y=d[y1],hue=d['Outcome'])","89af23a2":"d.loc[d['Glucose']==0,'Glucose']=np.nan\nd['Glucose'] = d['Glucose'].fillna(d['Glucose'].median())","4328f72f":"d.loc[ d['Glucose'] <60 , 'Glucose' ]","43f5485e":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_validate\n\n# split the data set into train and test\nx_1, x_test, y_1, y_test = train_test_split(df, r, test_size=0.3, random_state=0)\n\n# split the train data set into cross validation train and cross validation test\nx_tr, x_cv, y_tr, y_cv = train_test_split(x_1, y_1, test_size=0.3)","0fbc6c81":"for i in range(1,30,2):\n    # instantiate learning model (k = 30)\n    knn = KNeighborsClassifier(n_neighbors=i)\n\n    # fitting the model on crossvalidation train\n    knn.fit(x_tr, y_tr)\n\n    # predict the response on the crossvalidation train\n    pred = knn.predict(x_cv)\n\n    # evaluate CV accuracy\n    acc = accuracy_score(y_cv, pred, normalize=True) * float(100)\n    print('\\nCV accuracy for k = %d is %d%%' % (i, acc))","3ad5943e":"y_pred=knn.predict(x_test)\nacc=accuracy_score(y_pred,y_test)*float(100)\nacc","b5d45b05":"myList = list(range(0,50))\nneighbors = list(filter(lambda x: x % 2 != 0, myList))\n\ncv_scores=[]\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, x_tr, y_tr, cv=3, scoring='accuracy')\n    cv_scores.append(scores.mean())\n\n# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint('\\nThe optimal number of neighbors is %d.' % optimal_k)\n\n# plot misclassification error vs k \nplt.plot(neighbors, MSE)\n\nfor xy in zip(neighbors, np.round(MSE,3)):\n    plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()\n\nprint(\"the misclassification error for each k value is : \", np.round(MSE,3))","f56be089":"knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)\n \n# fitting the model\nknn_optimal.fit(x_tr, y_tr)\n\n# predict the response\npred = knn_optimal.predict(x_test)\n\n# evaluate accuracy\nacc = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))","f475be6f":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test , pred)\ncm","2cef8a9f":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","86dfc36a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test , pred))","1ba1f245":"def k_classifier_brute(X_train , Y_train):\n    neighbors = list(range(5 , 51 , 2))\n    cv_scores = []\n    for i in neighbors:\n        neigh = KNeighborsClassifier(n_neighbors = i,metric='correlation' )\n        scores = cross_val_score(neigh , x_tr , y_tr , cv = 10 , scoring = 'accuracy')\n        cv_scores.append(scores.mean())\n    MSE = [1-x for x in cv_scores]\n    optimal_k = neighbors[MSE.index(min(MSE))]\n    print('Optimal k is {}'.format(optimal_k))\n    print('Misclassification error for each k is {}'.format(np.round(MSE , 3)))\n    plt.plot(neighbors , MSE)\n    plt.xlabel('Number of Neighbors')\n    plt.ylabel('Misclassification Error')\n    plt.title('Neighbors v\/s Misclassification Error')\n    plt.show()\n    \n    return optimal_k","b3c1e30b":"optimal_k_pidd = k_classifier_brute(x_tr , y_tr)","f1da7dee":"knn_optimal_for_pidd = KNeighborsClassifier(n_neighbors = optimal_k_pidd , metric = 'correlation')\nknn_optimal_for_pidd.fit(x_tr , y_tr)\npred = knn_optimal_for_pidd.predict(x_test)\naccuracy_score(pred,y_test)","ca98bb8b":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test , pred)\ncm","62f86e4e":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","5d9b5e86":"from sklearn.metrics import classification_report\nprint(classification_report(y_test , pred))","39ac1bb0":"X_train, X_test, Y_train, Y_test = train_test_split(df, r, random_state=1,test_size=0.2)\nfrom sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(X_train,Y_train)\npredic=nb.predict(X_test)","acf123bc":"accuracy_score(predic,Y_test)","24b0384c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , predic)\ncm","88748f02":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","bcbfc117":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","fe595c69":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,Y_train)\npredic=lr.predict(X_test)\naccuracy_score(predic,Y_test)","d64a2500":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , predic)\ncm","d0b70118":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","65b30502":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","e0ba8010":"from sklearn.naive_bayes import BernoulliNB\nnb=BernoulliNB()\nnb.fit(X_train,Y_train)\npredic=nb.predict(X_test)\naccuracy_score(predic,Y_test)","d56840aa":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB()\nmodel.fit(X_train,Y_train)\npredic=model.predict(X_test)\naccuracy_score(predic,Y_test)","56949225":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_train , Y_train = smt.fit_sample(X_train , Y_train)\nprint(Y_train.value_counts())","080997c3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix","9f5c0ec9":"def k_classifier_brute(X_train , Y_train):\n    neighbors = list(range(5 , 51 , 2))\n    cv_scores = []\n    for i in neighbors:\n        neigh = KNeighborsClassifier(n_neighbors = i,metric='correlation')\n        scores = cross_val_score(neigh , X_train , Y_train , cv = 10 , scoring = 'accuracy')\n        cv_scores.append(scores.mean())\n    MSE = [1-x for x in cv_scores]\n    optimal_k = neighbors[MSE.index(min(MSE))]\n    print('Optimal k is {}'.format(optimal_k))\n    print('Misclassification error for each k is {}'.format(np.round(MSE , 3)))\n    plt.plot(neighbors , MSE)\n    plt.xlabel('Number of Neighbors')\n    plt.ylabel('Misclassification Error')\n    plt.title('Neighbors v\/s Misclassification Error')\n    plt.show()\n    \n    return optimal_k","fed6fa13":"optimal_k_pidd = k_classifier_brute(X_train , Y_train)","3fea439e":"knn_optimal_for_pidd = KNeighborsClassifier(n_neighbors = optimal_k_pidd , metric = 'correlation')\nknn_optimal_for_pidd.fit(X_train , Y_train)\npred = knn_optimal_for_pidd.predict(X_test)","39869593":"train_acc = knn_optimal_for_pidd.score(X_train , Y_train)\nprint('Training Accurcy = {}'.format(train_acc))\n\ntrain_error = 1 - train_acc\nprint('Training Error = {}'.format(train_error))","a3681621":"accuracy = accuracy_score(Y_test , pred)\nprint('The accuracy of the model for k = {} is {}'.format(optimal_k_pidd , accuracy))","6d67c9a6":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , predic)\ncm","6bbf90d9":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","65fddaeb":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","f6c8f8df":"nb1=GaussianNB()\nnb1.fit(X_train,Y_train)\npredic=nb1.predict(X_test)\naccuracy_score(predic,Y_test)","5815d2fc":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test , predic)\ncm","e3792298":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","48b26e10":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","a5475dd1":"nb1=BernoulliNB()\nnb1.fit(X_train,Y_train)\npredic=nb1.predict(X_test)\naccuracy_score(predic,Y_test)","5bbba516":"cm = confusion_matrix(Y_test , predic)\ncm","af580deb":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","656c62cc":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","480f83c7":"lr1=LogisticRegression()\nlr1.fit(X_train,Y_train)\npredic=lr1.predict(X_test)\naccuracy_score(predic,Y_test)","087deb0b":"cm = confusion_matrix(Y_test , predic)\ncm","424b56fe":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","c02ef07d":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic))","00dc0096":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB()\nmodel.fit(X_train,Y_train)\npredic=model.predict(X_test)\naccuracy_score(predic,Y_test)","3c450e53":"from sklearn import svm\nclfr1=svm.SVC(kernel='linear')\nclfr2=svm.SVC(kernel='rbf')\nclfr1.fit(X_train,Y_train)\nclfr2.fit(X_train,Y_train)\npredic1=clfr1.predict(X_test)\npredic2=clfr2.predict(X_test)\nprint(\"The accuracy for SVM model With linear Kernel is {} \", accuracy_score(predic1,Y_test))\nprint(\"The accuracy for SVM model With RBF Kernel is {} \", accuracy_score(predic2,Y_test))","ae89b630":"cm1 = confusion_matrix(Y_test , predic1)\ncm1","23728da5":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm1 , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","4b99fcd1":"cm2= confusion_matrix(Y_test , predic2)\ncm2","6f1fa96a":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm2 , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","e5a5f85b":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic1))\nprint(classification_report(Y_test , predic2))","26c80c2a":"import xgboost as xgb\nxgb_model=xgb.XGBClassifier().fit(X_train,Y_train)\npredictions=xgb_model.predict(X_test)\nactuals=Y_test\nprint(accuracy_score(actuals,predictions))","c4b80ef5":"cm4= confusion_matrix(actuals,predictions)\ncm4","fddbd6e8":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm4 , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","a4849bd3":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic1))","04e59faa":"from sklearn import tree\nclfr3=tree.DecisionTreeClassifier()\ndt_model=clfr3.fit(X_train,Y_train)\npredic3=clfr3.predict(X_test)\naccuracy_score(predic3,Y_test)","69fb0f3c":"cm3= confusion_matrix(Y_test , predic3)\ncm3","0820dab0":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm3 , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()\n","9607f7c2":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic3))","19e1bea6":"from sklearn.ensemble import RandomForestClassifier\nrf_mdl=RandomForestClassifier().fit(X_train,Y_train)\npredic5=rf_mdl.predict(X_test)\naccuracy_score(predic5,Y_test)","3c0da34f":"cm5= confusion_matrix(Y_test , predic5)\ncm5","2322c2f5":"labels = ['negative' , 'positive']\ndf_cm = pd.DataFrame(cm5 , index = labels , columns = labels)\n\nsn.heatmap(df_cm , annot = True , fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()","414b65e0":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test , predic5))","10bc780b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ntuned_param=[{'c':[10**-4,10**-2,10**0,10**2]}]\nlr1=GridSearchCV(LogisticRegression(),tuned_param)\nlr1.fit(X_train,Y_train)\npredic=lr1.predict(X_test)\naccuracy_score(predic,Y_test)","dbb35fc1":"print(lr.best_estimator_)\nprint(model.score(X_test,Y_test))","9900122d":"# Decision Trees and Random Forest","aa6a91a0":"# Importing Libraries and Datasets","429bb1c8":"# Imputation","57d695dc":"# Oversampling","b7bde683":"# Building Models and Evaluating","6654a600":"# EDA","903e1f7b":"# Fine Tuning Hyperparameters using GridSearchCV","45c68a74":"# SVM and XGBoost Classifiers","722a915c":"# EDA using Plotting"}}