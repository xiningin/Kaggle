{"cell_type":{"d52870df":"code","8eb16979":"code","7fdc16ec":"code","8b4d82f8":"code","d70e4998":"code","6a3774a6":"code","9c2ff288":"code","689c0e06":"code","88b04b78":"code","c8bb9403":"code","e0af1056":"code","6c2d9776":"code","4676d271":"code","87ec160d":"code","3ac2895f":"code","367e1e4c":"code","72d03f26":"code","90cc3780":"code","04c76b21":"code","a5216af7":"code","6ef23c67":"code","95578c33":"code","a030947d":"code","a4f6ea32":"code","7a29988c":"code","82d7e87b":"code","9e17dd68":"code","020d3fda":"code","359d207b":"code","2e91e8d4":"code","fcefd0ef":"code","e403ab08":"code","5b660db1":"code","52d59b17":"code","7775645b":"code","e3b78f5e":"code","be29916b":"code","97cc9804":"code","74538e28":"markdown","c70f0389":"markdown","2f5fd035":"markdown","72589607":"markdown","5b9b461f":"markdown","9d78a576":"markdown","153bce55":"markdown","f91cb2b5":"markdown","5781b9e5":"markdown","634f3db5":"markdown","29edd9ad":"markdown","69b9fa78":"markdown","d7711555":"markdown","19ceaf60":"markdown","b33524d7":"markdown","35046e00":"markdown","ce7eeb8d":"markdown","0526bc2e":"markdown","577efe63":"markdown","1e7aa98d":"markdown","baffaedd":"markdown"},"source":{"d52870df":"import os\nimport nltk\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom datasets import Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer","8eb16979":"# Constants\nTRAIN_CSV = \"..\/input\/feedback-prize-2021\/train.csv\"\nSUB_CSV = \"..\/input\/feedback-prize-2021\/sample_submission.csv\"\nTRAIN_PATH = \"..\/input\/feedback-prize-2021\/train\"\nTEST_PATH = \"..\/input\/feedback-prize-2021\/test\"\n\n# Load DF\ndf = pd.read_csv(TRAIN_CSV, dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\ndf.head()","7fdc16ec":"# No nulls\ndf.isnull().sum()","8b4d82f8":"a_id = \"423A1CA112E2\"","d70e4998":"def get_text(a_id):\n    a_file = f\"{TRAIN_PATH}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ntxt = get_text(a_id)\nprint(txt)","6a3774a6":"df_example = df[df['id'] == a_id]\ndf_example","9c2ff288":"# Files in train path: 15595\n!ls -l {TRAIN_PATH} | wc -l","689c0e06":"# Files in test path: 6\n!ls -l {TEST_PATH} | wc -l","88b04b78":"# There are 7 classes:\ndf['discourse_type'].value_counts(normalize=True)","c8bb9403":"ID2CLASS = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\nCLASS2ID = {v: k for k, v in ID2CLASS.items()}\nprint(ID2CLASS)\nCLASS2ID","e0af1056":"text_ids = df['id'].unique().tolist()","6c2d9776":"text_id = text_ids[5]\ntext = get_text(text_id)\nprint(text)","4676d271":"# Extract element boundaries and classes  with to_records\n\ndf_text = df[df['id'] == text_id]\nelements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\nelements","87ec160d":"# Fill \"No class\" chunks: beginning and end\n\ninitial_idx = 0\nfinal_idx = len(text)\n\n# Add element at the beginning if it doesn't in index 0\nnew_elements = []\nif elements[0][0] != initial_idx:\n    starting_element = (0, elements[0][0]-1, 'No Class')\n    new_elements.append(starting_element)\n\n    \n# Add element at the end if it doesn't in index \"-1\"\nif elements[-1][1] != final_idx:\n    closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n    new_elements.append(closing_element)\n    \nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\n# See first element (new)\nelements","3ac2895f":"# Add \"No class\" elements inbetween separated elements \nnew_elements = []\nfor i in range(1, len(elements)-1):\n    if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n        new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n        new_elements.append(new_element)\n\nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\nelements","367e1e4c":"# Finall \"fill_gaps\" functions, wrapping up the above cells\n\ndef fill_gaps(elements, text):\n    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n    initial_idx = 0\n    final_idx = len(text)\n\n    # Add element at the beginning if it doesn't in index 0\n    new_elements = []\n    if elements[0][0] != initial_idx:\n        starting_element = (0, elements[0][0]-1, 'No Class')\n        new_elements.append(starting_element)\n\n\n    # Add element at the end if it doesn't in index \"-1\"\n    if elements[-1][1] != final_idx:\n        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n        new_elements.append(closing_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n\n    # Add \"No class\" elements inbetween separated elements \n    new_elements = []\n    for i in range(1, len(elements)-1):\n        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n            new_elements.append(new_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n    return elements\n\n\ndef get_elements(df, text_id, do_fill_gaps=True, text=None):\n    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n    text = get_text(text_id) if text is None else text\n    df_text = df[df['id'] == text_id]\n    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n    if do_fill_gaps:\n        elements = fill_gaps(elements, text)\n    return elements","72d03f26":"def get_x_samples(df, text_id, do_fill_gaps=True):\n    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n    text = get_text(text_id)\n    elements = get_elements(df, text_id, do_fill_gaps, text)\n    sentences = []\n    for start, end, class_ in elements:\n        elem_sentences = nltk.sent_tokenize(text[start:end])\n        sentences += [(sentence, class_) for sentence in elem_sentences]\n    df = pd.DataFrame(sentences, columns=['text', 'label'])\n    df['label'] = df['label'].map(CLASS2ID)\n    return df\n\nget_x_samples(df, text_ids[1])","90cc3780":"# This takes a while. I created a dataset with the output here: https:\/\/www.kaggle.com\/julian3833\/feedback-df-sentences\n#x = []\n#for text_id in tqdm(text_ids):\n#    x.append(get_x_samples(df, text_id))\n\n#df_sentences = pd.concat(x)","04c76b21":"df_sentences = pd.read_csv(\"..\/input\/feedback-df-sentences\/df_sentences.csv\")","a5216af7":"df_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\ndf_sentences.head()","6ef23c67":"df_sentences.to_csv(\"df_sentences.csv\", index=False)","95578c33":"len(df_sentences)","a030947d":"MODEL_CHK = \"..\/input\/huggingface-bert\/bert-large-cased\"\n\nNUM_LABELS = 8\n\nNUM_EPOCHS = 2","a4f6ea32":"ds_train = Dataset.from_pandas(df_sentences.iloc[:340000])\nds_val = Dataset.from_pandas(df_sentences.iloc[340000:])","7a29988c":"transformers.logging.set_verbosity_warning() # Silence some annoying logging of HF\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHK)\n\ndef preprocess_function(examples):    \n    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n\n\n# Tokenizer dataset\nds_train_tokenized = ds_train.map(preprocess_function, batched=True)\nds_val_tokenized = ds_val.map(preprocess_function, batched=True)","82d7e87b":"# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_CHK, num_labels=NUM_LABELS)","9e17dd68":"os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['WANDB_DISABLED'] = 'true'\n\ntraining_args = TrainingArguments(\n    output_dir='feeeback-classifier',\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=NUM_EPOCHS,\n    weight_decay=0.01,\n    report_to=\"none\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train_tokenized,\n    eval_dataset=ds_val_tokenized,\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n)","020d3fda":"trainer.train()","359d207b":"trainer.save_model(\"feedback-bert-trained\")","2e91e8d4":"df_sub = pd.read_csv(SUB_CSV)\ndf_sub","fcefd0ef":"def get_test_text(a_id):\n    a_file = f\"{TEST_PATH}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)]\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in senteces to \"word indexes\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n    return df_test","e403ab08":"df_test = create_df_test()\ndf_test.head()","5b660db1":"ds_test = Dataset.from_pandas(df_test)\nds_test_tokenized = ds_test.map(preprocess_function, batched=True)","52d59b17":"# Get the predictions!!\ntest_predictions = trainer.predict(ds_test_tokenized)","7775645b":"# Turn logits into classes\ndf_test['predictions'] = test_predictions.predictions.argmax(axis=1)\n\n# Turn class ids into class labels\ndf_test['class'] = df_test['predictions'].map(ID2CLASS)\ndf_test.head()","e3b78f5e":"# Turn the word ids into this weird predictionstring required\ndf_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","be29916b":"# Drop \"No class\" sentences\ndf_test = df_test[df_test['class'] != 'No Class']\ndf_test.head()","97cc9804":"# And submit!! \ud83e\udd1e\ud83e\udd1e \ndf_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)","74538e28":"# Create a sentence classification datasety\n\nAs far as I know, this problem is not trivially mapped to one of the \"typical\" NLP tasks. \nIt might be close to NER \/ POS, but the fact that the entities are large makes me doubt about it.\n\nI'm looking forward for the community discussion about the different possible approaches to this problem. \n\n\nAlthough I might be missing something very obvious, this notebook proposes the following approach, that is a multiclass classifier:\n\n1. Split the texts into sentences (x)\n2. Assign each sentence a class (y).\n3. Train a normal sequence classifier on those sentences\n\nThere are 7 classes and the labeled sections (sometimes) exceed sentences. We will preprocess them to have only sentences. That way, we avoid the problem of detecting when a element starts and when it ends for now.\n","c70f0389":"## Prepare test dataset","2f5fd035":"From the [Data tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/data):\n\n* id - ID code for essay response\n* discourse_id - ID code for discourse element\n* discourse_start - character position where discourse element begins in the essay response\n* discourse_end - character position where discourse element ends in the essay response\n* discourse_text - text of discourse element\n* discourse_type - classification of discourse element\n* discourse_type_num - enumerated class label of discourse element\n* predictionstring - the word indices of the training sample, as required for predictions\n","72589607":"## Train","5b9b461f":"# Modeling!!!\n\nWe will use a `BERT` and the `Trainer` API from Hugging Face. \n\nWe are using a dataset to avoid using internet (a restriction of the competition for submission notebooks)\n\nReferences:\n* https:\/\/huggingface.co\/docs\/transformers\/training\n* https:\/\/huggingface.co\/docs\/transformers\/custom_datasets","9d78a576":"# 10,000 foot view of the data\n\nLet's take a quick glance at the `train.csv` file:","153bce55":"## Please, _DO_ upvote if you find it useful or interesting!! \n#### I'm very close to becoming a grandmaster and very excited about it \ud83d\ude07\ud83d\ude4f","f91cb2b5":"## Encode classes as ints\nSome sections don't belong to any class. We will label them as `No Class` so we can discard those sections and avoid false positives.","5781b9e5":"For now, we are submitting one row per sentence and not \"elements\". \n\nHow to convert sentences into \"elements\" (blocks of setences) is not clear since there are times when various sentences with the same class are flagged in independent \"elements\".","634f3db5":"# Submit\n\nWe will apply a process similar to the one we applied to the original train data, splitting each text into its sentences.\n\nSee the [Evaluation tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation) for details about the `predictionstring` column","29edd9ad":"# Sentence Classifier with HuggingFace \ud83e\udd17","69b9fa78":"\n\n## Dataset functions: `fill_gaps()`, `get_elements()`, and `get_x_samples()`\n\nHere we write the functions `fill_gaps` which will to just that. I leave the code I use for developing and below there is the condensed function.","d7711555":"## Tokenize","19ceaf60":"### Prepare trainer","b33524d7":"I will stop with the EDA here because I have already seen various very good notebooks around. I suggest you the following ones:\n\n* [Feedback Prize EDA with displacy](https:\/\/www.kaggle.com\/thedrcat\/feedback-prize-eda-with-displacy) by [thedrcat](https:\/\/www.kaggle.com\/thedrcat\/)\n* [[Feedback prize] Simple EDA](https:\/\/www.kaggle.com\/ilialar\/feedback-prize-simple-eda) by [ilialar](https:\/\/www.kaggle.com\/ilialar)\n* [\ud83d\udd25\ud83d\udcca Feedback Prize - EDA \ud83d\udcca\ud83d\udd25](https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda) by [odins0n](https:\/\/www.kaggle.com\/odins0n\/)\n* [Feedback Prize - EDA](https:\/\/www.kaggle.com\/yamqwe\/feedback-prize-eda) by [yamqwe](https:\/\/www.kaggle.com\/yamqwe\/)\n\n\nLet's get into the sentence classification idea!\n","35046e00":"# Imports","ce7eeb8d":"## Let's see the first example in some more detail","0526bc2e":"## HuggingFace Dataset","577efe63":"## Build the full dataframe for sentence classification","1e7aa98d":"# cpoied from https:\/\/www.kaggle.com\/julian3833\/feedback-baseline-sentence-classifier-0-226\n# Thanks for his work!\n\n# If this notebook is useful to you, can you give me an upvote\n\n###  Starter noteboook for the competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/), framing the task as a sentence classificaiton problem, using HuggingFace, BERT and the Trainer API.\n\n\n#### A lot of competitions going on, right my friends? ;)\n\n### The nature of the task of this competition is not trivially easy to map to an \"orthodox\" NLP problem, at least as far as I can tell. Is it a Token classification as NER\/POS? Is it a... hindi bilingual question answering lol?\n\n<h2> In this notebook I will try to present one of the possible approaches, this is: <span style=\"color:blue\"> Sentence Classification<\/span>.<\/h2>\n\n---\n\nThe agenda is as follows:\n1. A very quick EDA (there are various EDA notebooks already against which I cannot offer any value)\n2. Preprocess to obtain a sentence classification dataset\n3. Fine-tune a BERT over that sentence classficiation datasdet\n4. Submit\n\n---\n\n## Please, _DO_ upvote if you find it useful or interesting!! \n","baffaedd":"## Predict"}}