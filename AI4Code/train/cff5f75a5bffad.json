{"cell_type":{"e44b0152":"code","687dcb30":"code","d36971ef":"code","0500dec4":"code","96dca730":"code","74a0b3e7":"code","02ffc146":"code","f97a3972":"code","f562192b":"code","ccdd2b59":"code","66e38b35":"code","020a0869":"code","b78862c0":"code","ff48339a":"code","8e152db9":"code","b150c868":"code","20b5399f":"code","1403e4a7":"code","d4105db0":"code","83334449":"code","dca30139":"code","78913fda":"code","dcf86722":"code","c3471551":"code","ad032b37":"code","ffbba392":"code","16496b47":"code","5d71fe14":"code","c105b165":"code","1deea145":"code","576f004c":"code","fa2ed90d":"code","5ef714f8":"code","94ebdc0f":"code","0f34f7d3":"code","08a45488":"code","98ca2e39":"code","900049e9":"code","1e355427":"code","9f1eca42":"code","18380b6c":"code","3ea12aff":"code","d8c127eb":"code","241add93":"code","5760c871":"code","1fd8890b":"code","b0fbc6aa":"code","6f7bc3d1":"code","215c9d73":"code","1cd25c9d":"code","a3cad00a":"code","40f085e0":"code","03b62b0a":"code","8bef5de5":"code","2df335be":"code","e22bcd20":"code","fde93e8b":"code","e21ee92c":"code","ab312c7f":"code","c63e67fd":"code","a9e299a3":"code","18a51857":"code","52857390":"code","3e16c8cd":"markdown","15603ee1":"markdown","43723d0a":"markdown","715d04f4":"markdown","90719df2":"markdown","403db35b":"markdown","f550faf0":"markdown","27c61aae":"markdown","259fb9fd":"markdown","cdce3b97":"markdown","577564cd":"markdown","62b3f3bd":"markdown","f41dba70":"markdown","57cc603b":"markdown","d51ff85f":"markdown","477008cc":"markdown","aa8f10ad":"markdown","64b96784":"markdown","88e2c192":"markdown","0c365aa5":"markdown","980e49e4":"markdown","0738d859":"markdown","0e104679":"markdown","0b402a92":"markdown","bd16528b":"markdown","2465fd53":"markdown","3e916c0f":"markdown","06a82e8a":"markdown","39d16e50":"markdown","ddd2fc86":"markdown","987edb52":"markdown","be664670":"markdown","32ed706e":"markdown","2f5dcd56":"markdown","9f6f1ce8":"markdown","a2285565":"markdown","61a154a2":"markdown","05d07acc":"markdown","06934c38":"markdown","faeec744":"markdown","a41362c1":"markdown","47a03375":"markdown","382ecff6":"markdown","48177dfc":"markdown","05e3493d":"markdown","a08b29e4":"markdown","459b3519":"markdown","6c1ce0a3":"markdown","e15732ff":"markdown","5b71b3f2":"markdown","43fbebac":"markdown","095128a9":"markdown","0bbf0d65":"markdown"},"source":{"e44b0152":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","687dcb30":"# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d36971ef":"churn=pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","0500dec4":"churn.head()","96dca730":"churn.info()","74a0b3e7":"churn.describe()","02ffc146":"sns.set_style('whitegrid')","f97a3972":"sns.countplot(churn['Geography'])","f562192b":"sns.countplot(churn['Gender'])","ccdd2b59":"sns.distplot(churn['Age'],bins=50)\nplt.title('Distribution of Age')","66e38b35":"sns.countplot(churn['NumOfProducts'])","020a0869":"sns.countplot(churn['HasCrCard'])","b78862c0":"sns.countplot(churn['IsActiveMember'])","ff48339a":"churn.isnull().sum()","8e152db9":"churn.duplicated().sum()","b150c868":"geography=pd.get_dummies(churn['Geography'],drop_first=True)\ngender=pd.get_dummies(churn['Gender'],drop_first=True)","20b5399f":"churn.drop(['Geography','Gender'],axis=1,inplace=True)","1403e4a7":"churn=pd.concat([churn,geography,gender],axis=1)","d4105db0":"churn.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)","83334449":"churn.head()","dca30139":"X=churn.drop(['Exited'],axis=1).values","78913fda":"X","dcf86722":"y=churn['Exited'].values","c3471551":"y","ad032b37":"from sklearn.model_selection import train_test_split","ffbba392":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","16496b47":"from sklearn.preprocessing import StandardScaler","5d71fe14":"sc=StandardScaler()","c105b165":"sc.fit(X_train)","1deea145":"X_train=sc.transform(X_train)","576f004c":"X_test=sc.transform(X_test)","fa2ed90d":"# Import required models and layers and otheer important libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense","5ef714f8":"# Initializing the ANN\nClassifier=Sequential()","94ebdc0f":"# Adding layers\n#input layer\nClassifier.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n#hidden layers\nClassifier.add(Dense(6, kernel_initializer = 'uniform',activation='relu'))\n#Output layer\nClassifier.add(Dense(1, kernel_initializer = 'uniform',activation='sigmoid'))","0f34f7d3":"# compiling the model\nClassifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","08a45488":"hist=Classifier.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=10,epochs=100)","98ca2e39":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['loss','val_loss'], loc='upper right')\nplt.show()","900049e9":"plt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model loss')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['accuracy','val_accuracy'], loc='upper right')\nplt.show()","1e355427":"predictions=Classifier.predict(X_test)","9f1eca42":"predictions","18380b6c":"predictions=(predictions>0.5)","3ea12aff":"predictions","d8c127eb":"from sklearn.metrics import accuracy_score","241add93":"accuracy_score(y_test,predictions)","5760c871":"from keras.wrappers.scikit_learn import KerasClassifier","1fd8890b":"from sklearn.model_selection import cross_val_score","b0fbc6aa":"def build_classifier():\n    Classifier=Sequential()\n    Classifier.add(Dense(6,activation='relu'))\n    Classifier.add(Dense(6,activation='relu'))\n    Classifier.add(Dense(1,activation='sigmoid'))\n    Classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return Classifier","6f7bc3d1":"classifier=KerasClassifier(build_fn=build_classifier,batch_size=10,epochs=100)","215c9d73":"accuracies=cross_val_score(estimator=classifier,X=X_train,y=y_train,cv=10,n_jobs=-1)","1cd25c9d":"mean=accuracies.mean()\nvariance=accuracies.std()","a3cad00a":"mean","40f085e0":"variance","03b62b0a":"from keras.layers import Dropout","8bef5de5":"Classifier=Sequential()\nClassifier.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\nClassifier.add(Dropout(p=0.1))\nClassifier.add(Dense(6, kernel_initializer = 'uniform',activation='relu'))\nClassifier.add(Dropout(p=0.1))\nClassifier.add(Dense(1, kernel_initializer = 'uniform',activation='sigmoid'))\nClassifier.add(Dropout(p=0.1))\nClassifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","2df335be":"from sklearn.model_selection import GridSearchCV","e22bcd20":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n    classifier.add(Dense(6, kernel_initializer = 'uniform',activation='relu'))\n    classifier.add(Dense(1, kernel_initializer = 'uniform',activation='sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier)","fde93e8b":"parameters={'batch_size':[25,32],'epochs':[100,500],'optimizer':['adam','rmsprop']}","e21ee92c":"grid_search = GridSearchCV(estimator =classifier,param_grid = parameters,scoring = 'accuracy',cv= 10)","ab312c7f":"grid_search =grid_search.fit(X_train,y_train)","c63e67fd":"best_parameters =grid_search.best_params_","a9e299a3":"best_parameters","18a51857":"best_accuracy =grid_search.best_score_","52857390":"best_accuracy","3e16c8cd":"![image.png](attachment:image.png)","15603ee1":"1. Data Cleaning\n2. Create Dummy Variables\n3. Feature Scaling\n4. Feature Transformation","43723d0a":"Most of the customers have either one or two product of the bank. This means the intention of the majority of the customers is only to maintaining a simple account or having a debit or credit card rather than the other products.","715d04f4":"we can see that most of the customers are around age 38 with 10000 estimated salary. According to the US Census Bureau, this salary range considered as the low-income category. ","90719df2":"# 4. Exploratory Data Analysis","403db35b":"# 8.Evaluating","f550faf0":"\nThis is the graph of the gender of customers. As we can see most of the customers are males.","27c61aae":"X and y are numpy arrays","259fb9fd":"Here we start with very low accuracy during first couple of epoch runs and then when the epoch runs increasing, the accuracy also tend increase and finally it will continue to be consistent.","cdce3b97":"\nWe're going to compute the mean of these accuracies.\nSo let's create a new variable mean, to take the mean of this vector of accuracies, And let's also compute the variance.\nSo we're going to call this variance then again we take our accuracies vector to compute it.","577564cd":"## 5.3 Feature Scaling","62b3f3bd":"# 4. Methodology","f41dba70":"This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he\/she continues to be a customer. This dataset consists of 10,000 customers (rows) and 14 columns. Here we're trying to separate customers for two particular groups based on customers personal-details and their bank details.","57cc603b":"\nIf you're interserted to read more about Data Cleaning, read my article on   [Data Cleaning in Nutshell](https:\/\/medium.com\/@duminyk95\/data-cleaning-in-nutshell-4e017dd86fb6)","d51ff85f":"Parameter tuning consists of finding the best values of these hyperparameters and we're going to do this with a technique called **Grid Search** , that basically will test several combinations of these values and will eventually return the best selection that leads to the best accuracy","477008cc":"\nGeography, Gender is the categorical variables in this dataset. To do a proper analysis it is best practice to create dummy variables for these variables. So let\u2019s create dummy variables.","aa8f10ad":"If we train  our ANN model twice or more than that we can notice that for each time it shows different accuracies. By using K -Fold cross_validation we're trying to train a model that will **not only be accurate** but also that should **not have too much variance of accuracy.** \n\njudging our model performance only on one accuracy\non one test set,\nis actually not super relevant.\nThat's not the most relevant way,\nto evaluate the model performance.\nAnd so there is this technique called\nK-Fold Cross Validation that improves this a lot.\n\nIt will fix it by splitting the training set into 10 folds\nwhen K equals 10, and most of the time, K equals 10.\nAnd we train our model on 9-folds\nand we test it on the last remaining fold.\nAnd since with 10-folds\nwe can make ten different combinations of 9-folds\nto train a model and 1-fold to test it.\nThat means that we can train the model\nand test the model on ten combinations\nof training and test sets.\nAnd that will already give us a much better idea","64b96784":"Now our dataset is well preprocessed and ready for the advanced analysis.","88e2c192":"As we can see there is no significance difference among the active and non-active customers.It seems like most of the customers not happy about the bank and its services, their low-income level are maybe the reason for that. ","0c365aa5":"This dataset has no missing values and duplicated items.","980e49e4":"If you\u2019re from a **customer-based** company, business or an organization, **customer success** is the foundation of your company. When your customer achieving their desired outcome through their interactions with your company (focus on customer success) churn will not an issue at all. But in general, churn can happen in everywhere. So what if we told you there was a way to predict, at least to some degree, how and when your customers will cancel? Or how many numbers of customers will leave you or stay at you? That\u2019s exactly what a churn model can do. So basically customer churn is a method used to identify the numbers of customers who have either unsubscribed or cancelled their service contract.","0738d859":"Since this, a type of classification problem I used supervised deep learning model called ***Artificial Neural Network (ANN)*** to classify the customers of the bank.\nFirst of all, I carry out a data preprocessing step to get a better dataset. Then I start building the artificial neural network by splitting the dataset into train and test sets. The models' architecture will contain three layers. The first layer will have 6 neurons and use the **Relu** activation function, the second layer will also have 6 neurons and use the **Relu** activation function, and the third and the final layer will have 1 neuron with a sigmoid activation function. Then compile the model and give it the **binary_crossentopy** loss function (Used for binary classification) to measure how well the model performs on training, and then give it the Stochastic Gradient Descent **adam** optimizer to improve upon the loss. Also, I want to measure the accuracy of the model so add \u2018accuracy\u2019 to the metrics.\n\n##### Analysis Procedure.\n1.\tExploratory Data Analysis\n2.\tData preprocessing\n3.\tSplitting the dataset\n4.\tBuild the ANN model\n5.\tMake Predictions\n6.\tEvaluation\n7.\tImprove the Model\n8.\tParameter Tuning\n\n","0e104679":"\nWhen your model was trained too much on the training set, that becomes much less performance on the test set. That is called overfitting. Dropout Regularization is the technique used to remove the overfitting. To avoid overfitting at each iteration of the training we add dropout layer after each existing layer in our neural network. Let's see what happen to our neural network with dropout layers.","0b402a92":"# 5. Data Preprocessing","bd16528b":"This graph illustrates the distribution of age of the customers. Majority of the customers in the bank are in age around 38. That means this bank has middled age customer base. It seems like there is a low percentage of an older crowd. ","2465fd53":"# 2. DataSet","3e916c0f":"# 6 Build the ANN model","06a82e8a":"We made our predictions using test data where our model hasn't seen before. Now it's time to evaluate how accurate our model is.. ","39d16e50":"Read my 'Data Pre-processing article -[A simple introduction to Data Pre-processing](http:\/\/medium.com\/analytics-vidhya\/a-simple-introduction-to-data-pre-processing-4cac052df4a4)","ddd2fc86":"## Leave any comment or feedback\n\n# Thank You !!!","987edb52":"# 11. Conclusions","be664670":"As we can see most of the customers have a credit card","32ed706e":"we have two type of parameters.\nWe have the parameters that are learnt from the model\nduring the training and these are the weights\nand we have some other parameters that stay fixed\nand these parameters are called the hyper parameters.\nSo for example these hyper parameters\nare the number of a epoch, the batch size, the optimizer\nor the number of neurons in the layers.\n\nAnd when we trained our artificial neural network\nwell, we trained it with some fixed values\nof these hyper parameters. But maybe that\nby taking some other values, we would get to\na better accuracy overall with careful cross validation.\nAnd so that's what parameter tuning is all about.\n\nIt consists of finding the best values\nof these hyper parameters and we are gonna do this\nwith a technique called grid search\nthat basically will test several combinations\nof these values and will eventually return\nthe best selection, the best choice that leads\nto the best accuracy with careful cross validation.","2f5dcd56":"## 5.1 Data Cleaning","9f6f1ce8":"# 1. Introduction","a2285565":"# 10. Parameter Tuning","61a154a2":"Our ANN model predicts almost 84% results correctly. ","05d07acc":"## Evaluating Model Using K-Fold Cross Validation","06934c38":"## 5.2 Create Dummy Variables","faeec744":"# 9. Improving the model using Dropout regularization","a41362c1":"## 5.4 Feature Transormation","47a03375":"* People from France are the main customers of this bank. This is a place where middle-aged and low-income people do their transactions and get other services. Having a credit card is common to the majority of customers and most of the customers have two bank products including their credit card. Most probably the second product might be the accounts of the customers.\n\n\n* ANN classify the customers with 86% model accuracy.","382ecff6":"Here we start with a very high loss during our first couple of epoch runs and then as the weight and biases start to get adjusted, we'll hopefully see the gradual drop in our loss or error. Validation loss is also going down. There are some spikes in validation loss here.Training loss and validation loss go down and then continue down together.","48177dfc":"# 3. Objective\n\n* Build the classification algorithm to classify customers who are left and stay.","05e3493d":"## Split the dataset into training and test sets","a08b29e4":"Now we are going to wrap the whole thing.\nSo now what we're going to do is create a new classifier\nthat will be the global classifier variable\nbecause the classifier that we define here\nis a local variable because it is a variable\ninside the function.\nSo this variable will only be created inside the function\nwhen we execute this code here that defines the function.\nThen, as soon as the function is defined\nand we execute the next line of code,\nwell, the local classifier variable will no longer exist.\nAnd so now what we can do is create a new classifier\nthat we're going to also call classifier.\nAnd this classifier will be the same classifier\nas we built here but this will not be trained on\nthe whole training set composed of x train and y train.\nIt will be built through careful cross validation\non 10 different training folds\nand by each time measuring the model performance\non one test fold.\nSo the only thing that is different between this classifier\nand the one above that we built in the previous section,\nis only in the training part.","459b3519":"These numbers make no sense. Our objective is to classify our customers as leave or stay. Therefore we rename our predictions,\n* Predictions>0.5 = True (Customer Will leave the Bank)\n* Predictions<0.5 = False (Customer Will Stay the Bank)","6c1ce0a3":"# 7.Make Predictions","e15732ff":"## Divide the variables into set of independent variables and single dependent variable","5b71b3f2":"It seems like these customers are from a bank which is located in the European region. As we can see most of the customers are from France while the representatives of the equal number of customers from both Spain and Germany.","43fbebac":"Data pre-processing is an important step in the data mining process. The real-world datasets are incomplete, inconsistency and lacking certain behaviours. As a result of that, this kind of data leads to misleading results. Therefore let's carry out some data preprocessing steps to get a better dataset.","095128a9":"So now we have all the tools we need\nand so basically we're ready to start implementing\nK-fold cross validation inside Keras.\nAnd to do this we're going to use a function\nbecause the Keras classifier that we imported here\nexpects for one of its arguments a function.\nIt's actually its first argument we will see\ncalled Build FN -Build Function.\nAnd this function is simply a function\nthat returns the classifier that we made here\nwith all this architecture.\nSo basically this function just builds the architecture\nof our artificial neural network.\nSo here we go.\nLet's start making this function.","0bbf0d65":"Compiling means we appying Stochastic gradient descent on the whole ANN."}}