{"cell_type":{"0a149b18":"code","74b8fc44":"code","a891a3ba":"code","858fc832":"code","47836463":"code","11d34fe3":"code","a9beafe4":"code","fbf38b8e":"code","99dd6f7d":"code","b1e4f344":"code","cb542157":"code","a1b44918":"code","79a523cd":"code","4cc63cb5":"code","11f47bae":"code","895638b6":"code","e12be827":"code","b126be46":"code","e25f3346":"code","bb7a6d70":"code","489f6304":"code","3198e344":"code","dcb38779":"code","b960bab8":"code","6946a929":"code","e28b9e37":"code","126340dd":"code","7893afb0":"code","7b9d9ac1":"code","b06ad835":"code","24fd77b2":"code","30df9b85":"code","fa9438c6":"code","a9564749":"markdown","192f857c":"markdown","553756e5":"markdown","f21474ce":"markdown","68dd3733":"markdown","ad65f89e":"markdown","3a0fa05b":"markdown","eaef3f7b":"markdown","5715def5":"markdown"},"source":{"0a149b18":"# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","74b8fc44":"# configure matplotlib\nplt.rcParams[\"figure.figsize\"] = 12,8","a891a3ba":"# import data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","858fc832":"# info\ntrain.info()","47836463":"# describe\ntrain.describe()","11d34fe3":"# head\ntrain.head()","a9beafe4":"# print out the unique values of certain variables\nfor col in [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]:\n    print(f\"Variable: {col}\")\n    print(train[col].unique())\n    print(\"\\n\")","fbf38b8e":"# are there as many tickets as there are passengers?\nprint(len(train[\"PassengerId\"].unique()) == len(train[\"Ticket\"].unique()))","99dd6f7d":"# so it seems that multiple people boarded with one ticket then\n# but are there enough shared tickets to constitute using the column as a variable?\nlen(train[\"PassengerId\"].unique()) - len(train[\"Ticket\"].unique())","b1e4f344":"# are there cases where one record is assigned to multiple cabins?\ntrain[train[\"Cabin\"] == \"C23 C25 C27\"]","cb542157":"# who are the people with no embarked value?\ntrain[pd.isnull(train[\"Embarked\"])]","a1b44918":"# distribution of fares and ages\nf,ax = plt.subplots(1,2)\nax = ax.ravel()\n\nax[0].hist(train.Fare, bins=100)\nax[1].hist(train[pd.notnull(train[\"Age\"])][\"Age\"], bins=100) # excluding null ages\nplt.show()","79a523cd":"# counts of pclass, sex, sibsp, parch, embarked, and survived\nf, ax = plt.subplots(3,2)\nf.tight_layout(h_pad=4, w_pad=2)\nax = ax.ravel()\nfor pos, col in enumerate([\"Pclass\", \"Sex\", \"SibSp\",\"Parch\", \"Embarked\", \"Survived\"]):\n    vis1 = sns.countplot(data=train, x=col, ax=ax[pos])\n    vis1.set_title(col)\n    vis1.set_xlabel(\"\")","4cc63cb5":"# get the median value for age from the training data, this will be used on the test data as well\nmedian_age = train[\"Age\"].median()\n# get the median fare from the training data since the test dataset has one fare value missing\nmedian_fare = train[\"Fare\"].median()","11f47bae":"# cleaning the training dataset\n\n# take a copy of the data so we won't be changing the original dataframe\ntraincp = train.copy()\n# impute nan values for age using the median we calculated before\ntraincp[\"Age\"] = traincp[\"Age\"].fillna(value=median_age)\n# replace nan values for cabin with \"Shared\"\ntraincp[\"Cabin\"] = traincp[\"Cabin\"].fillna(value=\"Shared\")\n# remove name and passenger ID column from the training data\ntraincp = traincp.drop(columns=[\"Name\", \"PassengerId\"])\n# turn PClass, Ticket, Cabin, Embarked and Sex into dummies\ntraincp = pd.get_dummies(data=traincp, columns=[\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\", \"Ticket\"])\n# divide train_clean into the dependent and independent variables\ny_interim = traincp[\"Survived\"]\nX_interim = traincp.iloc[:, 1:]","895638b6":"# clean the test dataset\n\n# take a copy of the data so we won't be changing the original dataframe\ntestcp = test.copy()\n# impute nan values for age using the median we calculated before\ntestcp[\"Age\"] = testcp[\"Age\"].fillna(value=median_age)\n# impute nan values for fare using the median we calculated before\ntestcp[\"Fare\"] = testcp[\"Fare\"].fillna(value=median_fare)\n# replace nan values for cabin with \"Shared\"\ntestcp[\"Cabin\"] = testcp[\"Cabin\"].fillna(value=\"Shared\")\n#remove name and passenger ID column from the training data\ntestcp = testcp.drop(columns=[\"Name\", \"PassengerId\"])\n# turn PClass, Ticket, Cabin, Embarked and Sex into dummies\ntestcp = pd.get_dummies(data=testcp, columns=[\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\", \"Ticket\"])\n# assign testcp to X_test\nX_test = testcp.copy()","e12be827":"# get a list of the columns in the training dataset that aren't in the test dataset since we need to add them to the test dataset\n# likewise get a list of the columns in the test dataset that aren't in the training dataet\ncols_not_in_test = []\ncols_not_in_train = []\n\nfor col in X_interim.columns:\n    if col not in X_test.columns:\n        cols_not_in_test.append(col)\n        \nfor col in X_test.columns:\n    if col not in X_interim.columns:\n        cols_not_in_train.append(col)","b126be46":"# add columns not in the test\/train dataset to it with a value of 0 for every record\nfor col in cols_not_in_test:\n    X_test[col] = np.zeros((len(X_test)))\n\nfor col in cols_not_in_train:\n    X_interim[col] = np.zeros((len(X_interim)))","e25f3346":"# check if both the training data and test data have the same number of columns\nlen(X_test.columns) == len(X_interim.columns)","bb7a6d70":"# to avoid the dummy variable trap we need to drop one column from the training and test datasets\n# since if Pclass_1 and Pclass_2 are equal to 0, Pclass_3 will automatically be 1, we can drop Pclass_3\nX_test = X_test.drop(columns=[\"Pclass_3\"])\nX_interim = X_interim.drop(columns=[\"Pclass_3\"])","489f6304":"# divide the training data into training data and cross validation data\nfrom sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(X_interim, y_interim, test_size=0.25, random_state=0)","3198e344":"# import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","dcb38779":"# train a logistic regression\ndef train_log_reg(X_train, y_train, X_cv):\n    # train\n    log_reg = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n    log_reg.fit(X_train, y_train)\n    # predict using X_cv\n    y_pred = log_reg.predict(X_cv)\n    return y_pred\n\n# train a decision tree model\ndef train_dec_tree(X_train, y_train, X_cv):\n    # train\n    dec_tree = DecisionTreeClassifier()\n    dec_tree.fit(X_train, y_train)\n    # predict\n    y_pred = dec_tree.predict(X_cv)\n    return y_pred\n\n# train a random forest model\ndef train_rand_for(X_train, y_train, X_cv, max_depth=None, random_state=None):\n    # train\n    rand_for = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=random_state)\n    rand_for.fit(X_train, y_train)\n    # predict\n    y_pred = rand_for.predict(X_cv)\n    return y_pred","b960bab8":"# import metric calculators\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# function to calculate a classifier model's metrics\ndef evaluate_model(y_true, y_pred):\n    # calculate the scores\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = ((precision * recall) \/ (precision + recall)) * 2\n    return accuracy, f1","6946a929":"# function to train a model a given number of times and plot the accuracy and f1 metrics\ndef train_and_plot(model, X_train, y_train, X_cv, y_cv, training_times=10):\n    # lists to store y axis data\n    accuracy_y_axis, f1_y_axis = [], []\n    for i in range(0, training_times):\n        # train the model\n        y_pred = model(X_train, y_train, X_cv)\n        # get the evaluation metrics\n        accuracy, f1 = evaluate_model(y_cv, y_pred)\n        # append to y_axis lists\n        accuracy_y_axis.append(accuracy)\n        f1_y_axis.append(f1)\n    # plot the graphs\n    plt.plot(range(0, training_times), accuracy_y_axis, label=\"Accuracy\")\n    plt.plot(range(0, training_times), f1_y_axis, label=\"F1\")\n    plt.legend(loc=\"upper right\")\n    plt.show()","e28b9e37":"# check the logistic regression model\ntrain_and_plot(train_log_reg, X_train, y_train, X_cv, y_cv, training_times=10)","126340dd":"# check the decision tree model\ntrain_and_plot(train_dec_tree, X_train, y_train, X_cv, y_cv, training_times=10)","7893afb0":"# check the random forest model\ntrain_and_plot(train_rand_for, X_train, y_train, X_cv, y_cv, training_times=10)","7b9d9ac1":"# determine the best max_depth for the random forest model\naccuracy_y_axis, f1_y_axis = [], [] # to store the y_axis_data\nnum_of_depths = 500\n\n# test out various depths keeping the random state static\nfor depth in range(1, num_of_depths):\n    y_pred = train_rand_for(X_train, y_train, X_cv, max_depth=depth, random_state=9631)\n    accuracy, f1 = evaluate_model(y_cv, y_pred)\n    accuracy_y_axis.append(accuracy)\n    f1_y_axis.append(f1)","b06ad835":"# plot results\nplt.plot(range(0, num_of_depths-1), accuracy_y_axis, label=\"Accuracy\")\nplt.plot(range(0, num_of_depths-1), f1_y_axis, label=\"F1\")\nplt.legend(loc=\"upper left\")\nplt.show()","24fd77b2":"# train the random forest model\nrand_for = RandomForestClassifier(max_depth=120)\nrand_for.fit(X_train, y_train)\n# predict on X_test\ny_pred_final = rand_for.predict(X_test)","30df9b85":"# create the submission document\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": y_pred_final})\nsubmission.to_csv(\"submission.csv\", index=False)","fa9438c6":"# # REMOVED CODE\n\n# # get only the letter of the cabin number and assign it to a new column\n# traincp = traincp.assign(cabin_new = traincp[\"Cabin\"].apply(lambda x: x[0] if pd.isnull(x) == False else np.nan))\n# traincp[\"cabin_new\"] = traincp[\"cabin_new\"].fillna(value=\"Shared\")\n# # assign a value of group to non-unique tickets and individual to unique tickets\n# traincp = traincp.assign(ticket_new = traincp[\"Ticket\"].apply(lambda x: x if train_tick_freq[x] > 1 else \"Individual\"))\n# traincp = traincp.drop(columns=[\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"])\n# traincp = pd.get_dummies(data=traincp, columns=[\"Pclass\", \"Sex\", \"cabin_new\", \"Embarked\", \"ticket_new\"])\n# # get only the letter of the cabin number and assign it to a new column\n# testcp = testcp.assign(cabin_new = testcp[\"Cabin\"].apply(lambda x: x[0] if pd.isnull(x) == False else np.nan))\n# testcp[\"cabin_new\"] = testcp[\"cabin_new\"].fillna(value=\"Shared\")\n# # assign a value of group to non-unique tickets and individual to unique tickets\n# testcp = testcp.assign(ticket_new = testcp[\"Ticket\"].apply(lambda x: x if test_tick_freq[x] > 1 else \"Individual\"))\n# testcp = testcp.drop(columns=[\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"])\n# testcp = pd.get_dummies(data=testcp, columns=[\"Pclass\", \"Sex\", \"cabin_new\", \"Embarked\", \"ticket_new\"])","a9564749":"# Submission","192f857c":"### It seems like the random forest model peforms better than the other two","553756e5":"## Column Meanings\n\n**Variable Definitions**\n\n* survival -  Survival 0 = No, 1 = Yes \n* pclass -  Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n* sex -  Sex \n* Age -  Age in years \n* sibsp -  # of siblings \/ spouses aboard the Titanic \n* parch -  # of parents \/ children aboard the Titanic \n* ticket -  Ticket number \n* fare -  Passenger fare \n* cabin -  Cabin number \n* embarked -  Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton \n---\n* pclass: A proxy for socio-economic status (SES)\n    * 1st = Upper\n    * 2nd = Middle\n    * 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n    * Sibling = brother, sister, stepbrother, stepsister\n    * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n    * Parent = mother, father\n    * Child = daughter, son, stepdaughter, stepson\n    * Some children travelled only with a nanny, therefore parch=0 for them","f21474ce":"# Examination","68dd3733":"### It flatlines around 120, so we'll go with a max_depth of 120","ad65f89e":"# Titanic: Machine Learning from Disaster\n## First Attempt","3a0fa05b":"# Garage","eaef3f7b":"# Training and Evaluating Models","5715def5":"# Cleaning\n\n**Notes**\n* PClass, Embarked, Ticket, Cabins, and Sex take on a limited set of values and so should be turned into dummies.\n* SibSp and Parch take on a limited set of values but their values (and ordering) need to be accounted for by the ML algorithm. So we won't be turning them into categorical variables.\n* The nan values for age should be imputed. The nan values for cabin should be changed to \"Shared\".\n* We don't need the name and PassengerId column."}}