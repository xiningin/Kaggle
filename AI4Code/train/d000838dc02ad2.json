{"cell_type":{"05c575d9":"code","147ee527":"code","50d95c72":"code","82aa7372":"code","4bbdc3f9":"code","ae102d79":"code","f575c5e1":"code","10a8def7":"code","4d038400":"code","6414e3c9":"code","b403bb63":"code","9fd3e261":"code","0608c480":"code","4732721e":"markdown","a2c37d2f":"markdown","fefe88d9":"markdown","f72d9c86":"markdown","23451460":"markdown","24d25f64":"markdown","0cc5b51d":"markdown","2e839a20":"markdown"},"source":{"05c575d9":"!pip install kaggle-environments -U > \/dev\/null 2>&1\n!cp -r ..\/input\/lux-ai-2021\/* .","147ee527":"# !cp  ..\/input\/lux-jit-models\/sub_models\/*.pth .\n!cp ..\/input\/lux-ai-off-policy-models\/lux_ai_off_policy_models\/*.onnx .\n!cp ..\/input\/onnxruntime-wheel-for-lux\/onnxruntime_wheel\/onnxruntime-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl .","50d95c72":"!cp ..\/input\/lux-ai-with-il-decreasing-learning-rate\/model.pth .\n!cp ..\/input\/lux-ai-with-il-decreasing-learning-rate\/agent.py  .\/agent_imitation_baseline.py","82aa7372":"%%writefile agent.py\nimport copy\nimport json\nimport math\nimport os\nimport random\nimport subprocess\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, Union\n\nimport numpy as np\nimport skimage.measure\nimport skimage.transform\nimport torch\nfrom lux.game import Game\n\npath = \"\/kaggle_simulations\/agent\" if os.path.exists(\"\/kaggle_simulations\") else \".\"\n# from https:\/\/stackoverflow.com\/questions\/4256107\/running-bash-commands-in-python\nonnxruntime_wheel_path = os.path.join(\n    path, \"onnxruntime-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\"\n)\nbashCommand = \"pip install --disable-pip-version-check\" + \" \" + onnxruntime_wheel_path\nprocess = subprocess.Popen(\n    bashCommand.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE\n)\noutput, error = process.communicate()\nimport onnxruntime as ort\n\nmodel_path = os.path.join(path, \"model_rl.onnx\")  # \nmodel = ort.InferenceSession(model_path)\n\nACTION_DIRECTIONS = [(0, -1), (0, +1), (-1, 0), (1, 0)]\n\n\nclass UnitOrder(NamedTuple):\n    step: int\n    order_strings: List[str]\n\n\ndef get_cargo_fuel_value(unit):\n    \"\"\"\n    Returns the fuel-value of all the cargo this unit has.\n    \"\"\"\n    wood_rate = 1\n    coal_rate = 10\n    uranium_rate = 40\n    if hasattr(unit, \"get_cargo_fuel_value\"):\n        return unit.get_cargo_fuel_value()\n    else:\n        return (\n            unit.cargo.wood * wood_rate\n            + unit.cargo.coal * coal_rate\n            + unit.cargo.uranium * uranium_rate\n        )\n\n\ndef to_numpy(tensor):\n    if isinstance(tensor, torch.Tensor):\n        return (\n            tensor.detach().cpu().numpy()\n            if tensor.requires_grad\n            else tensor.cpu().numpy()\n        )\n    if isinstance(tensor, np.ndarray):\n        return tensor\n\n\n\ndef get_unit_sequence_obs(\n    game,\n    player: int,\n    b_active: np.ndarray,\n    can_act_units: Dict[str, Any],\n    turn: int = 0,\n    unit_length: int = 64,\n    action_dim: int = 5,\n    input_dim=4,\n):\n    \"\"\"\n    Implements getting a observation from the current game for this unit or city\n    \"\"\"\n\n    if len(can_act_units) == 0:\n        obs = {\n            \"image\": b_active.astype(np.float32),\n            \"input_sequence\": np.zeros((unit_length, input_dim), dtype=np.float32),\n            # \"orig_length\": 0,\n            \"sequence_mask\": np.zeros((unit_length, 1), dtype=np.int64),\n            \"rule_mask\": np.zeros((unit_length, action_dim), dtype=np.int64),\n        }\n        unit_order = UnitOrder(step=turn, order_strings=[])\n        return obs, unit_order\n\n    ordered_unit_points, ordered_units = order_unit(\n        units=list(can_act_units.values()),\n        is_debug=False,\n        random_start=False,\n    )\n\n    (\n        orig_length,\n        sequence_mask,\n        input_sequence,\n        _,\n        unit_order,\n        action_masks,\n    ) = generate_sequence(\n        game=game,\n        state=b_active,\n        can_act_units=can_act_units,\n        ordered_units=ordered_units,\n        max_sequence=unit_length,  # or len(can_act_units)\n        input_size=b_active.shape[1:],\n        # input_size=input_size,\n        # no_action=no_action,\n        # in_features=in_features,\n        # ignore_class_index=ignore_class_index,\n        actions=None,\n        action_length=action_dim,\n    )\n\n    obs = {\n        \"image\": b_active.astype(np.float32),\n        \"input_sequence\": input_sequence.astype(np.float32),\n        # \"orig_length\": orig_length,\n        \"sequence_mask\": sequence_mask.astype(np.int64),\n        \"rule_mask\": action_masks.astype(np.int64),\n    }\n    unit_order = UnitOrder(step=turn, order_strings=unit_order)\n    return obs, unit_order\n\n\ndef order_points(points, units, ind):\n    points_new = [\n        points.pop(ind)\n    ]  # initialize a new list of points with the known first point\n    units_new = [\n        units.pop(ind)\n    ]  # initialize a new list of points with the known first point\n    pcurr = points_new[-1]  # initialize the current point (as the known point)\n    while len(points) > 0:\n        d = np.linalg.norm(\n            np.array(points) - np.array(pcurr), axis=1\n        )  # distances between pcurr and all other remaining points\n        ind = d.argmin()  # index of the closest point\n        points_new.append(points.pop(ind))  # append the closest point to points_new\n        units_new.append(units.pop(ind))  # append the closest point to points_new\n        pcurr = points_new[-1]  # update the current point\n    return points_new, units_new\n\n\ndef order_unit(\n    units: List[Any],\n    is_debug: bool = False,\n    random_start: bool = False,\n    split_proba: float = 0.1,\n) -> List[Tuple[int, int]]:\n    \"\"\"\n    from\n    https:\/\/stackoverflow.com\/questions\/37742358\/sorting-points-to-form-a-continuous-line\n    \"\"\"\n    # assemble the x and y coordinates into a list of (x,y) tuples:\n    ordered_units = sorted(units, key=lambda x: (x.pos.y, x.pos.x))\n    points = [(unit.pos.x, unit.pos.y) for unit in ordered_units]\n\n    # order the points based on the known first point:\n    start_ind = 0\n    if random_start:\n        start_ind = random.choice(range(len(points)))\n    points_new, units_new = order_points(\n        points.copy(), units=ordered_units.copy(), ind=start_ind\n    )\n\n    if random_start:\n        points_new = points_new[::-1]\n        if random.random() < split_proba:\n            split_ind = random.choice(range(len(points)))\n            points_new = points_new[split_ind:] + points_new[:split_ind]\n\n    if is_debug:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n        points = np.stack(points, axis=0)\n        xn, yn = np.array(points_new).T\n        ax[0].plot(points[:, 0], points[:, 1], \"-o\")  # original (shuffled) points\n        ax[1].plot(xn, yn, \"-o\")  # new (ordered) points\n        ax[0].set_title(\"Original\")\n        ax[1].set_title(\"Ordered\")\n        ax[0].grid()\n        ax[1].grid()\n        plt.tight_layout()\n        plt.show()\n    return points_new, units_new\n\n\ndef generate_sequence(\n    game: Any,\n    state: np.ndarray,\n    can_act_units: Dict[str, Any],\n    # ordered_unit_points: List[Tuple[int, int]],\n    ordered_units: List[Any],\n    input_size: Tuple[int, int] = (32, 32),\n    max_sequence: int = 128,\n    no_action: int = 6,\n    ignore_class_index: int = 20,\n    in_features: int = 4,\n    actions: Optional[Dict[str, int]] = None,\n    action_length: int = 5,\n):\n\n    # unit feature\n    # (N_u, 4), x, y, act_tpye, dirction\/None\n    if actions is None:\n        actions = {}\n\n    unit_feature = []\n    unit_order = []\n    action_masks = []\n\n    ban_map = generate_pos_ban_map(b_active=state)\n    for unit in ordered_units:\n        action_mask = np.ones((action_length,), dtype=np.int64)\n        unit_id = unit.id\n        y_shift, x_shift = calc_game_map_shift(input_size=input_size, game=game)\n        unit_feature.append(\n            [\n                unit.pos.x + x_shift,\n                unit.pos.y + y_shift,\n                can_act_units[unit_id].get_cargo_space_left(),\n                get_cargo_fuel_value(can_act_units[unit_id]),\n                actions.pop(unit_id, no_action),\n            ]\n        )\n        assert (\n            # state[CH_MAP[\"UnitPos\"], unit_feature[-1][1], unit_feature[-1][0]]\n            state[2, unit_feature[-1][1], unit_feature[-1][0]]\n            == 1.0\n        )\n        unit_order.append(unit_id)\n        action_mask[4] = int(can_act_units[unit_id].can_build(game.map))\n        action_mask[:4] = not_go_pos(\n            pos_y=unit_feature[-1][1], pos_x=unit_feature[-1][0], ban_map=ban_map\n        )\n        action_masks.append(action_mask)\n\n    assert len(actions) == 0\n    unit_feature = np.stack(unit_feature, axis=0)\n    action_masks = np.stack(action_masks, axis=0)\n    unit_feature = unit_feature[:max_sequence]\n    action_masks = action_masks[:max_sequence]\n    # debug\n    # fig, axes = plt.subplots(1, 3)\n    # state_map = state[CH_MAP[\"UnitPos\"]] -  (state[CH_MAP[\"UnitCooldown\"]] > 0).astype(int)\n    # cell_map = np.zeros_like(state_map)\n    # for x, y in unit_feature[:, :2]:\n    #     cell_map[y, x] = 1.\n    # axes[0].imshow(state_map)\n    # axes[1].imshow(cell_map)\n    # axes[2].imshow((state_map - cell_map) * 0.5)\n\n    orig_length = unit_feature.shape[0]\n    pad_len = max_sequence - orig_length\n    if pad_len > 0:\n        unit_feature = np.pad(unit_feature, [[0, pad_len], [0, 0]], constant_values=-1)\n        action_masks = np.pad(action_masks, [[0, pad_len], [0, 0]], constant_values=-1)\n    sequence_mask = np.zeros_like(unit_feature[:, 0:1])\n    sequence_mask[:orig_length, :] = 1.0\n\n    input_sequence = unit_feature[:, :in_features] \/ 100.0\n    output_sequence = unit_feature[:, in_features:]\n    output_sequence[output_sequence == -1] = ignore_class_index\n\n    return (\n        orig_length,\n        sequence_mask,\n        input_sequence,\n        output_sequence,\n        unit_order,\n        action_masks,\n    )\n\n\ndef calc_game_map_shift(input_size: List[int], game: Any) -> Tuple[int, int]:\n    x_shift = (input_size[1] - game.map.width) \/\/ 2\n    y_shift = (input_size[0] - game.map.height) \/\/ 2\n    return (y_shift, x_shift)\n\n\ndef crop_state(state: np.ndarray, game: Any, input_size=[32, 32]):\n    y_shift, x_shift = calc_game_map_shift(input_size=input_size, game=game)\n\n    if state.ndim == 2:\n        state = state[\n            y_shift : input_size[1] - y_shift, x_shift : input_size[0] - x_shift\n        ]\n    elif state.ndim == 3:\n        state = state[\n            :, y_shift : input_size[1] - y_shift, x_shift : input_size[0] - x_shift\n        ]\n    return state\n\n\ndef pred_with_onnx(model: ort.InferenceSession, obs: Dict[str, np.ndarray]):\n    ort_inputs = {}\n    for ort_in in model.get_inputs():\n        ort_inputs[ort_in.name] = to_numpy(obs[ort_in.name])[\n            np.newaxis,\n        ]\n    out = model.run(\n        None,\n        ort_inputs,\n    )\n    action_logit = out[0]\n    action_logit = (action_logit - (obs[\"rule_mask\"] == 0) * 1e32).squeeze(0)\n    return action_logit\n\n\ndef get_resource_distribution(b_active: np.ndarray, game: Any) -> np.ndarray:\n    # resoure_map = b_active[[12, 13, 14]].transpose(1, 2, 0)\n    # resoure_map = b_active[[12, 13, 14]]\n    input_size = (32, 32)\n    y_shift, x_shift = calc_game_map_shift(input_size=input_size, game=game)\n\n    research_point = int(b_active[[15]].max() * 200)\n    research_mask = [True, research_point >= 50, research_point >= 200]\n\n    fuel_rate = [1, 10, 40]\n    resoure_ch = [12, 13, 14]\n    resoure_map = np.zeros_like(b_active[[12]])\n\n    for resouce_index, mask in enumerate(research_mask):\n        if mask:\n            resoure_map += (\n                b_active[[resoure_ch[resouce_index]]] * fuel_rate[resouce_index]\n            )\n\n    resoure_map = resoure_map[\n        :, y_shift : input_size[1] - y_shift, x_shift : input_size[0] - x_shift\n    ]\n\n    res_avg_map = torch.nn.functional.avg_pool2d(\n        input=torch.from_numpy(resoure_map), kernel_size=5, stride=1, padding=2\n    ).numpy()\n\n    return res_avg_map, resoure_map\n\n\ndef get_act_cities_map(player_cities: list, game_map: Any):\n    act_cities_map = np.zeros((1, game_map.height, game_map.width), dtype=np.float32)\n    posyx2tile = {}\n\n    for city in player_cities:\n        if hasattr(city, \"city_cells\"):\n            for cell in city.city_cells:\n                city_tile = cell.city_tile\n                if city_tile.can_act():\n                    act_cities_map[:, city_tile.pos.y, city_tile.pos.x] = 1.0\n                    if city_tile.pos.y in posyx2tile.keys():\n                        posyx2tile[city_tile.pos.y].update({city_tile.pos.x: city_tile})\n                    else:\n                        posyx2tile[city_tile.pos.y] = {city_tile.pos.x: city_tile}\n\n        else:\n            for city_tile in city.citytiles:\n                if city_tile.can_act():\n                    act_cities_map[:, city_tile.pos.y, city_tile.pos.x] = 1.0\n                    if city_tile.pos.y in posyx2tile.keys():\n                        posyx2tile[city_tile.pos.y].update({city_tile.pos.x: city_tile})\n                    else:\n                        posyx2tile[city_tile.pos.y] = {city_tile.pos.x: city_tile}\n\n    return act_cities_map, posyx2tile\n\n\ndef decide_worker_gen_place(\n    res_avg_map: np.ndarray,\n    act_cities_map_max: np.ndarray,\n    resource_map: np.ndarray,\n    num_units: int,\n    num_city_tiles: int,\n):\n\n    places = serarch_with_pooled_feats(res_avg_map, act_cities_map_max)\n    if len(places) == 0:\n        squeeze_factor = 3\n        orig_shape = resource_map.shape[1:]\n        kernel_size = (\n            resource_map.shape[1] \/\/ squeeze_factor,\n            resource_map.shape[2] \/\/ squeeze_factor,\n        )\n        pooled_res = skimage.measure.block_reduce(\n            resource_map.squeeze(), kernel_size, np.mean\n        )\n        pooled_res = skimage.transform.resize(\n            pooled_res,\n            tuple(orig_shape),\n            order=None,\n            mode=\"constant\",\n            clip=True,\n            preserve_range=False,\n        )\n\n        places = serarch_with_pooled_feats(pooled_res, act_cities_map_max)\n    return places[: num_city_tiles - num_units]\n\n\ndef serarch_with_pooled_feats(res_avg_map, act_cities_map_max):\n    intersection = (res_avg_map * act_cities_map_max).squeeze()\n    places = np.where(intersection > 0)\n    if places[0].shape[0] > 0:\n        value = intersection[places]\n        places = np.stack(places, axis=-1)\n        return places[np.argsort(value)[::-1]]\n    else:\n        return []\n\n\ndef check_action_plan(\n    action_code: int,\n    our_city: np.ndarray,\n    pos_x: int,\n    pos_y: int,\n    current_plan: np.ndarray,\n    is_center: bool = False,\n):\n    assert type(action_code) == int or np.int64\n    is_center = is_center or (action_code == 4)\n\n    use_cooldown_as_center = False\n    if is_center:\n        pos_y_next = pos_y\n        pos_x_next = pos_x\n    else:\n        direc = ACTION_DIRECTIONS[action_code]\n        pos_y_next = direc[1] + pos_y\n        pos_x_next = direc[0] + pos_x\n        # for random agent\n        if (\n            (pos_x_next >= our_city.shape[1])\n            or (pos_y_next >= our_city.shape[0])\n            or (pos_x_next < 0)\n            or (pos_y_next < 0)\n        ):\n            pos_y_next = pos_y\n            pos_x_next = pos_x\n\n    is_no_unit = current_plan[pos_y_next, pos_x_next]\n    is_citytile = our_city[pos_y_next, pos_x_next]\n    is_ok = is_no_unit or is_citytile\n    if is_ok:\n        if not is_citytile:\n            current_plan[pos_y_next, pos_x_next] = False\n    else:\n        current_plan[pos_y, pos_x] = False\n        if not is_center:\n            use_cooldown_as_center = True\n\n    return current_plan, use_cooldown_as_center\n\n\ndef check_is_center_action(action_code: int):\n    return action_code == 5\n\n\ndef generate_pos_ban_map(b_active: np.ndarray):\n    our_units = b_active[3] > 0\n    our_city = b_active[8] > 0\n\n    opp_units = b_active[6] > 0\n    unit_stack_map = np.logical_or(our_units, opp_units)\n    unit_stack_map[our_city] = False\n\n    map_range = b_active[19]\n    opp_city = b_active[10]\n    ban_map = np.logical_or((map_range == 0), (opp_city > 0))\n    ban_map = np.logical_or(ban_map, unit_stack_map)\n    return ban_map\n\n\nclass StateHist:\n    def __init__(self, input_size: List[int] = [32, 32], unit_hist_length: int = 4):\n        self.input_size = input_size\n        self.unit_hist_length = unit_hist_length\n        self.reset()\n\n    def reset(self):\n        self.unit_hists = np.zeros(\n            [self.unit_hist_length] + self.input_size, dtype=np.float32\n        )\n\n        self.initial_resource_map = np.zeros(self.input_size, dtype=np.float32)\n        self.resource_reduction_map = np.zeros(self.input_size, dtype=np.float32)\n        self.resource_delta = np.zeros(self.input_size, dtype=np.float32)\n\n        self.opponent_unit_track = np.zeros(self.input_size, dtype=np.float32)\n        self.player_unit_track = np.zeros(self.input_size, dtype=np.float32)\n        self.step = 0\n\n    def initialize(self, initial_state: np.ndarray):\n        self.initial_resource_map = self._extract_resource(b_active=initial_state)\n        self.resource_reduction_map = self.initial_resource_map \/ (\n            self.initial_resource_map + 1e-6\n        )\n\n    def _extract_resource(self, b_active: np.ndarray):\n        return b_active[[12, 13, 14]].sum(axis=0)\n\n    def _get_unit_hist(self, b_active: np.ndarray):\n        our_units = b_active[2]\n        opp_units = b_active[5]\n        unit_hist = our_units + opp_units * 0.5\n        return our_units, opp_units, unit_hist\n\n    def get_hist(self):\n        targets = [\n            self.resource_reduction_map,\n            self.resource_delta,\n            self.player_unit_track,\n            self.opponent_unit_track,\n        ]\n        targets = np.stack(targets, axis=0)\n        targets = np.concatenate([self.unit_hists, targets], axis=0)\n        return targets[:-2], targets[-2:]\n\n    def update(self, current_state: np.ndarray, env_step: int):\n        if self.step == 0:\n            self.initialize(initial_state=current_state)\n\n        assert self.step == env_step, (self.step, env_step)\n\n        player_units, opp_units, unit_hist = self._get_unit_hist(b_active=current_state)\n\n        self.unit_hists[0:-1] = self.unit_hists[1:]\n        self.unit_hists[-1] = unit_hist\n\n        self.opponent_unit_track += opp_units \/ 360\n        self.player_unit_track += player_units \/ 360\n        self.opponent_unit_track = self.opponent_unit_track.clip(0, 1)\n        self.player_unit_track = self.player_unit_track.clip(0, 1)\n\n        current_res_stack = (self._extract_resource(b_active=current_state)) \/ (\n            self.initial_resource_map + 1e-6\n        )\n        self.resource_delta = self.resource_reduction_map - current_res_stack\n        self.resource_reduction_map = current_res_stack\n        self.step += 1\n\n\ndef not_go_pos(pos_y: int, pos_x: int, ban_map: np.ndarray):\n    # n , s, w, e\n    # (x, y)\n    # action_directions = [(0, -1), (0, +1), (-1, 0), (1, 0)]\n    # possible_yx_places = [(direc[1] + pos_y, direc[0] + pos_x) for direc in action_directions]\n    action_mask = []\n    for direc in ACTION_DIRECTIONS:\n        pos_y_next = direc[1] + pos_y\n        pos_x_next = direc[0] + pos_x\n        if (pos_x_next > 31) or (pos_y_next > 31):\n            action_mask.append(False)\n        elif (pos_x_next < 0) or (pos_y_next < 0):\n            action_mask.append(False)\n        else:\n            action_mask.append(~ban_map[pos_y_next, pos_x_next])\n    return np.array(action_mask).astype(int)\n\n\n# snippet to find the closest city tile to a position\ndef find_closest_city_tile(pos, player):\n    closest_city_tile = None\n    if len(player.cities) > 0:\n        closest_dist = math.inf\n        # the cities are stored as a dictionary mapping city id to the city object, which has a citytiles field that\n        # contains the information of all citytiles in that city\n        for k, city in player.cities.items():\n            for city_tile in city.citytiles:\n                dist = city_tile.pos.distance_to(pos)\n                if dist < closest_dist:\n                    closest_dist = dist\n                    closest_city_tile = city_tile\n    return closest_city_tile\n\n\ndef make_input(obs, unit_id, is_xy_order: bool = False):\n    width, height = obs[\"width\"], obs[\"height\"]\n    x_shift = (32 - width) \/\/ 2\n    y_shift = (32 - height) \/\/ 2\n    cities = {}\n\n    b = np.zeros((20, 32, 32), dtype=np.float32)\n\n    for update in obs[\"updates\"]:\n        strs = update.split(\" \")\n        input_identifier = strs[0]\n\n        if input_identifier == \"u\":\n            x = int(strs[4]) + x_shift\n            y = int(strs[5]) + y_shift\n            wood = int(strs[7])\n            coal = int(strs[8])\n            uranium = int(strs[9])\n            if unit_id == strs[3]:\n                # Position and Cargo\n                b[:2, x, y] = (1, (wood + coal + uranium) \/ 100)\n            else:\n                # Units\n                team = int(strs[2])\n                cooldown = float(strs[6])\n                idx = 2 + (team - obs[\"player\"]) % 2 * 3\n                b[idx : idx + 3, x, y] = (\n                    1,\n                    cooldown \/ 6,\n                    (wood + coal + uranium) \/ 100,\n                )\n        elif input_identifier == \"ct\":\n            # CityTiles\n            team = int(strs[1])\n            city_id = strs[2]\n            x = int(strs[3]) + x_shift\n            y = int(strs[4]) + y_shift\n            idx = 8 + (team - obs[\"player\"]) % 2 * 2\n            b[idx : idx + 2, x, y] = (1, cities[city_id])\n        elif input_identifier == \"r\":\n            # Resources\n            r_type = strs[1]\n            x = int(strs[2]) + x_shift\n            y = int(strs[3]) + y_shift\n            amt = int(float(strs[4]))\n            b[{\"wood\": 12, \"coal\": 13, \"uranium\": 14}[r_type], x, y] = amt \/ 800\n        elif input_identifier == \"rp\":\n            # Research Points\n            team = int(strs[1])\n            rp = int(strs[2])\n            b[15 + (team - obs[\"player\"]) % 2, :] = min(rp, 200) \/ 200\n        elif input_identifier == \"c\":\n            # Cities\n            city_id = strs[2]\n            fuel = float(strs[3])\n            lightupkeep = float(strs[4])\n            cities[city_id] = min(fuel \/ lightupkeep, 10) \/ 10\n\n    # Day\/Night Cycle\n    b[17, :] = obs[\"step\"] % 40 \/ 40\n    # Turns\n    b[18, :] = obs[\"step\"] \/ 360\n    # Map Size\n    b[19, x_shift : 32 - x_shift, y_shift : 32 - y_shift] = 1\n\n    if not is_xy_order:\n        b = b.transpose(0, 2, 1)\n\n    return (\n        b,\n        b[[8, 10], x_shift : 32 - x_shift, y_shift : 32 - y_shift] == 1,\n        b[[12, 13, 14], x_shift : 32 - x_shift, y_shift : 32 - y_shift].sum(axis=0) > 0,\n    )\n\n\ngame_state = None\n\n\ndef get_game_state(observation):\n    global game_state\n\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation[\"player\"]\n    else:\n        game_state._update(observation[\"updates\"])\n    return game_state\n\n\ndef in_city(pos):\n    try:\n        city = game_state.map.get_cell_by_pos(pos).citytile\n        return city is not None and city.team == game_state.id\n    except:\n        return False\n\n\ndef call_func(obj, method, args=[]):\n    return getattr(obj, method)(*args)\n\n\nunit_actions = [\n    (\"move\", \"n\"),\n    (\"move\", \"s\"),\n    (\"move\", \"w\"),\n    (\"move\", \"e\"),\n    (\"build_city\",),\n    #     (\"move\", \"c\"),\n]\n\n\nACTION_DIM = len(unit_actions)\n\n\nhist_folder = {0: StateHist(), 1: StateHist()}\nhist_folder[0].reset()\nhist_folder[1].reset()\nUSE_HIST = True if ACTION_DIM == 6 else False\nunit_length = 128\n\n\ndef agent(observation, configuration):\n    global game_state\n    global hist_folder\n\n    game_state = get_game_state(observation)\n    player = game_state.players[observation.player]\n    actions = []\n\n    state, citymap, resource_map = make_input(observation, \"u_-00\", is_xy_order=False)\n    state[:2] = 0.0\n\n    # City Actions\n\n    city_tile_count = (\n        player.city_tile_count\n    )  # :sum([len(city.city_cells) for city in player_cities])\n    player_cities = list(player.cities.values())\n    # units = player.units  # game.state[\"teamStates\"][team][\"units\"].values()\n    unit_count = len(player.units)  # unit_count = len(units)\n    game_map = game_state.map\n\n    player_tiles = None\n    act_cities_map_max, posyx2tile = get_act_cities_map(\n        player_cities=player_cities, game_map=game_map\n    )\n\n    if (len(posyx2tile) > 0) and (unit_count < city_tile_count):\n        res_avg_map, resource_dist = get_resource_distribution(\n            b_active=state, game=game_state\n        )\n        places = decide_worker_gen_place(\n            res_avg_map=res_avg_map,\n            resource_map=resource_dist,\n            act_cities_map_max=act_cities_map_max,\n            num_units=unit_count,\n            num_city_tiles=city_tile_count,\n        )\n        if len(places) > 0:\n            player_tiles = []\n            for y, x in places:\n                player_tiles.append(posyx2tile[y][x])\n\n    acted_cities = []\n    if player_tiles is not None:\n        for city_tile in player_tiles:\n            assert city_tile.can_act()\n            actions.append(city_tile.build_worker())\n            unit_count += 1\n            acted_cities.append(city_tile)\n        assert unit_count <= player.city_tile_count\n\n    for city_id, city in player.cities.items():\n        for city_tile in city.citytiles:\n            if city_tile.can_act() and (city_tile not in acted_cities):\n                if unit_count < player.city_tile_count:\n                    actions.append(city_tile.build_worker())\n                    unit_count += 1\n                elif not player.researched_uranium():\n                    actions.append(city_tile.research())\n                    player.research_points += 1\n\n    # Worker Actions\n    dest = []\n    friendly_cites = None\n    is_near_end = game_state.turn > 400\n    can_act_units = {unit.id: unit for unit in player.units if unit.can_act()}\n    team = observation.player\n\n    current_action_plan = np.ones(\n        (game_state.map.height, game_state.map.width), dtype=bool\n    )\n    if len(can_act_units) > 0:\n        unit = next(iter(can_act_units.values()))\n        obs, unit_order = get_unit_sequence_obs(\n            game=game_state,\n            player=observation.player,\n            b_active=state,\n            can_act_units=can_act_units,\n            turn=game_state.turn,\n            unit_length=unit_length,\n            action_dim=ACTION_DIM,\n            input_dim=4,\n        )\n        hist_right, hist_left = hist_folder[team].get_hist()\n        if USE_HIST:\n            obs[\"image\"][:2] = hist_left\n            obs[\"image\"] = np.concatenate([obs[\"image\"], hist_right], axis=0)\n\n        action_logit = pred_with_onnx(model=model, obs=obs)\n        our_city = crop_state(state[8] > 0, game=game_state)\n\n        for seq_ind, unit_id in enumerate(unit_order.order_strings):\n            if seq_ind >= unit_length:\n                break\n            unit = can_act_units[unit_id]\n            action_code = np.argmax(action_logit[seq_ind])\n            is_center = check_is_center_action(action_code=action_code)\n            current_action_plan, use_cooldown_as_center = check_action_plan(\n                action_code=action_code,\n                our_city=our_city,\n                pos_x=unit.pos.x,\n                pos_y=unit.pos.y,\n                current_plan=current_action_plan,\n                is_center=is_center,\n            )\n            if use_cooldown_as_center:\n                actions.append(unit.move(\"c\"))\n            elif not is_center:\n                act = unit_actions[action_code]\n                actions.append(call_func(unit, *act))\n\n    env_step = game_state.turn\n    if USE_HIST:\n        hist_folder[team].update(current_state=state, env_step=env_step)\n\n    return actions\n","4bbdc3f9":"!cp .\/model_vtrace500_upgo500.onnx .\/model_rl.onnx","ae102d79":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 32, \"height\": 32, \"loglevel\": 2, \"annotations\": True, \"seed\":1}, debug=False)\nsteps = env.run(['agent.py', 'agent_imitation_baseline.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","f575c5e1":"!cp .\/model_100.onnx .\/model_rl.onnx","10a8def7":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 32, \"height\": 32, \"loglevel\": 2, \"annotations\": True, \"seed\":1}, debug=False)\nsteps = env.run(['agent.py', 'agent_imitation_baseline.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","4d038400":"!cp .\/model_300.onnx .\/model_rl.onnx","6414e3c9":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 32, \"height\": 32, \"loglevel\": 2, \"annotations\": True, \"seed\":1}, debug=False)\nsteps = env.run(['agent.py', 'agent_imitation_baseline.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","b403bb63":"!cp .\/model_800.onnx .\/model_rl.onnx","9fd3e261":"from kaggle_environments import make\n\nenv = make(\"lux_ai_2021\", configuration={\"width\": 32, \"height\": 32, \"loglevel\": 2, \"annotations\": True, \"seed\":1}, debug=False)\nsteps = env.run(['agent.py', 'agent_imitation_baseline.py'])\nenv.render(mode=\"ipython\", width=1200, height=800)","0608c480":"!tar -cvzf submission.tar.gz *","4732721e":"# Off policy reinforcement learning approach \n\n\n* please check [my kaggle post](https:\/\/www.kaggle.com\/c\/lux-ai-2021\/discussion\/294601) or my [github](https:\/\/github.com\/Fkaneko\/kaggle_lux_ai) for more detail.\n![model](https:\/\/user-images.githubusercontent.com\/61892693\/145679319-db3b0f4e-e1eb-449d-94f8-c048d1bf2f79.png)\n* Learning result\n  - evaluate every 100 epochs\n  - match 50 episodes with [imitation learning baseline model](https:\/\/www.kaggle.com\/realneuralnetwork\/lux-ai-with-il-decreasing-learning-rate) (score ~1350) without internal unit move resolution\n  \n  \n| epoch | win rate | tile diff |\n| --- | --- | --- |\n|100 |  0.058 |   -28.4|\n|200 |  0.212 |   -13.0|\n|300 |  0.442 |    -0.0|\n|400 |  0.615 |    10.4|\n|500 |  0.731 |    15.4|\n|600 |  0.673 |     9.7|\n|700 |  0.712 |    14.1|\n|800 |  0.750 |    15.1|","a2c37d2f":"# ","fefe88d9":"# Load Opponent: Imitation Learning model(score ~ 1350)\n* use imitation baseline -> https:\/\/www.kaggle.com\/huikang\/lux-ai-agent-evaluation","f72d9c86":"## Epoch 100 result (500 x 100 episodes)","23451460":"# Score ~1200 rl model\n* this model training with ~500 epochs with V-trace and then ~500 epochs with UPGO\n","24d25f64":"# UPGO models vs imitation model\n* UPGO: The convergence of UPGO is faster than V-trace\n* visualize 100, 300 and 800 epoch match results here.","0cc5b51d":"## Epoch 300 result (500 x 300 episodes)","2e839a20":"## Epoch 800 result (500 x 800 episodes)"}}