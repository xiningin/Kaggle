{"cell_type":{"172544c7":"code","33424acd":"code","9b00af0f":"code","1ee59808":"code","01d8d2c1":"code","46b4e5b6":"code","0583acf6":"code","add89235":"code","23f1b9bd":"code","f79c5a41":"code","55c515d5":"code","c4a6e0e0":"code","926bb080":"code","7d29fe5e":"markdown","e8f95b57":"markdown","36154426":"markdown","9be6e25c":"markdown","d9e90323":"markdown","75ed48b4":"markdown","29621c86":"markdown","865b8a18":"markdown","c3bac618":"markdown","735afdd9":"markdown","0c398742":"markdown","b44bae50":"markdown"},"source":{"172544c7":"import gc\nimport os\nimport random\nimport sys\nimport time\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n#import pdb\n#import zipfile\n#import pydicom\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensor\nimport cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nimport tifffile as tiff\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom tqdm import tqdm_notebook as tqdm","33424acd":"#https:\/\/www.kaggle.com\/hfutybx\/unet-densenet121-lung-of-segmentation\/data\n\nsys.path.append('..\/input\/efficientnetpytorchaug252020\/EfficientNet-PyTorch-master')\nsys.path.append('..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/')\nsys.path.append('..\/input\/pytorchimagemodelsoct302020\/pytorch-image-models-master')\nsys.path.append('..\/input\/segmentation-models-pytorch0-1-2\/segmentation_models.pytorch-master')\nimport segmentation_models_pytorch as smp","9b00af0f":"def set_seed(seed=2**3):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\nset_seed(121)","1ee59808":"fold = 0\nnfolds = 3\nreduce = 4\nsz = 256\n\nBATCH_SIZE = 16\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nNUM_WORKERS = 4\nSEED = 2020\nTH = 0.50  #threshold for positive predictions\n\nDATA = '..\/input\/hubmap-kidney-segmentation\/test\/'\nLABELS = '..\/input\/hubmap-kidney-segmentation\/train.csv'\nMASKS = '..\/input\/hubmap-256x256\/masks\/'\nTRAIN = '..\/input\/hubmap-256x256\/train\/'\ndf_sample = pd.read_csv('..\/input\/hubmap-kidney-segmentation\/sample_submission.csv')","01d8d2c1":"#https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding\n#with bug fix\ndef rle_encode_less_memory(img):\n    #watch out for the bug\n    pixels = img.T.flatten()\n    \n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)","46b4e5b6":"def get_aug(p=1.0):\n    return Compose([\n        HorizontalFlip(),\n        VerticalFlip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        OneOf([\n            OpticalDistortion(p=0.3),\n            GridDistortion(p=.1),\n            IAAPiecewiseAffine(p=0.3),\n        ], p=0.3),\n        OneOf([\n            HueSaturationValue(10,15,10),\n            CLAHE(clip_limit=2),\n            RandomBrightnessContrast(),            \n        ], p=0.3),\n    ], p=p)","0583acf6":"def get_UnetPlusPlus():\n    model =  smp.UnetPlusPlus(\n                 encoder_name='efficientnet-b3',\n                 encoder_weights=None,\n                 in_channels=3,\n                 classes=1)\n    return model","add89235":"# https:\/\/www.kaggle.com\/iafoss\/256x256-images\nmean = np.array([0.65459856,0.48386562,0.69428385])\nstd = np.array([0.15167958,0.23584107,0.13146145])\n\ndef img2tensor(img,dtype:np.dtype=np.float32):\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass HuBMAPTestDataset(Dataset):\n    def __init__(self, imgs, idxs):\n        self.imgs = imgs\n        self.fnames = idxs\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        return img2tensor((self.imgs[idx]\/255.0 - mean)\/std)","23f1b9bd":"models = []\n\n\"\"\"for fold in range(nfolds):\n\n    model = get_UnetPlusPlus().to(DEVICE)\n    model.load_state_dict(torch.load(f\"..\/input\/hubmap-pytorch-smp-unet\/FOLD{fold}_.pth\"))\n    models.append(model)\"\"\"\nmodel = smp.Unet('se_resnext50_32x4d', encoder_weights=None, classes=1).cuda()\nmodel.load_state_dict(torch.load(\"..\/input\/testngerror\/FOLD-2-model.pth\"))\nmodels.append(model)\n","f79c5a41":"#iterator like wrapper that returns predicted masks\nclass Model_pred:\n    def __init__(self, models, dl, tta:bool=True, half:bool=False):\n        self.models = models\n        self.dl = dl\n        self.tta = tta\n        self.half = half\n        \n    def __iter__(self):\n        count=0\n        with torch.no_grad():\n            for x in self.dl: #iter(self.dl):\n                x = x.to(DEVICE)\n                if self.half: x = x.half()\n                py = None\n                for model in self.models:\n                    p = model(x)\n                    p = torch.sigmoid(p).detach()\n                    if py is None: py = p\n                    else: py += p\n                if self.tta:\n                    #x,y,xy flips as TTA\n                    flips = [[-1],[-2],[-2,-1]]\n                    for f in flips:\n                        xf = torch.flip(x,f)\n                        for model in self.models:\n                            p = model(xf)\n                            p = torch.flip(p,f)\n                            py += torch.sigmoid(p).detach()\n                    py \/= (1+len(flips))        \n                py \/= len(self.models)\n                    \n                py = F.upsample(py, scale_factor=reduce, mode=\"bilinear\")\n                py = py.permute(0,2,3,1).float().cpu()\n                batch_size = len(py)\n                for i in range(batch_size):\n                    yield py[i]\n                    count += 1\n                    \n    def __len__(self):\n        return len(self.dl.dataset)","55c515d5":"#Somehow I cannot resolve the submission error with consideration of the\n#private LB data, and the submission error doesn't give an informative\n#output. So, for now I share the notbook that makes a submission only\n#to the public LB, and later I'll try to resolve the issue.\n#IMPORTANT: This notebook doesn't perform predictions for the private LB.\nnames,preds = [],[]\nsamples = ['b9a3865fc','b2dc8411c','26dc41664','c68fe75ea','afa5e8098']\nsamples_n = [id for id in df_sample.id if id not in samples]\n\nnames += samples_n\npreds += [np.NaN]*len(samples_n)\ndf_sample = df_sample.loc[df_sample.id.isin(samples)]","c4a6e0e0":"#https:\/\/www.kaggle.com\/iafoss\/256x256-images\ns_th = 40  #saturation blancking threshold\np_th = 200*sz\/\/256 #threshold for the minimum number of pixels\n#names,preds = [],[]\nfor idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n    idx = row['id']\n    #read image\n    img = tiff.imread(os.path.join(DATA,idx+'.tiff'))\n    if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1,2,0))\n    \n    #add padding to make the image dividable into tiles\n    img_shape = img.shape\n    pad0 = (reduce*sz - img_shape[0]%(reduce*sz))%(reduce*sz)\n    pad1 = (reduce*sz - img_shape[1]%(reduce*sz))%(reduce*sz)\n    img = np.pad(img,[[pad0\/\/2,pad0-pad0\/\/2],[pad1\/\/2,pad1-pad1\/\/2],[0,0]],\n                 constant_values=0)\n\n    #split image into tiles using the reshape+transpose trick\n    if reduce != 1:\n        img = cv2.resize(img,(img.shape[1]\/\/reduce,img.shape[0]\/\/reduce),\n                     interpolation = cv2.INTER_AREA)\n    img_shape_p = img.shape\n    img = img.reshape(img.shape[0]\/\/sz,sz,img.shape[1]\/\/sz,sz,3)\n    img = img.transpose(0,2,1,3,4).reshape(-1,sz,sz,3)\n\n    #select tiles for running the model\n    imgs,idxs = [],[]\n    for i,im in enumerate(img):\n        #remove black or gray images based on saturation check\n        hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n        h, s, v = cv2.split(hsv)\n        if (s>s_th).sum() <= p_th or im.sum() <= p_th: continue\n        imgs.append(im)\n        idxs.append(i)\n    #tile dataset\n    ds = HuBMAPTestDataset(imgs,idxs)\n    dl = DataLoader(ds,batch_size=BATCH_SIZE, shuffle=False)\n    #dl = DataLoader(ds,BATCH_SIZE,num_workers=NUM_WORKERS,shuffle=False,pin_memory=True)\n    mp = Model_pred(models,dl)\n    \n    #generate masks\n    mask = torch.zeros(img.shape[0],sz*reduce,sz*reduce,dtype=torch.int8)\n    for i,p in zip(idxs,iter(mp)): mask[i] = p.squeeze(-1) > TH\n    \n    #reshape tiled masks into a single mask and crop padding\n    mask = mask.view(img_shape_p[0]\/\/sz,img_shape_p[1]\/\/sz,sz*reduce,sz*reduce).\\\n        permute(0,2,1,3).reshape(img_shape_p[0]*reduce,img_shape_p[1]*reduce)\n    mask = mask[pad0\/\/2:-(pad0-pad0\/\/2) if pad0 > 0 else img_shape_p[0]*reduce,\n        pad1\/\/2:-(pad1-pad1\/\/2) if pad1 > 0 else img_shape_p[1]*reduce]\n    \n    #convert to rle\n    #https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding\n    rle = rle_encode_less_memory(mask.numpy())\n    names.append(idx)\n    preds.append(rle)\n    gc.collect()","926bb080":"df = pd.DataFrame({'id':names,'predicted':preds})\ndf.to_csv('submission.csv',index=False)","7d29fe5e":"## Util functions","e8f95b57":"## Load libraries","36154426":"--------------------------","9be6e25c":"To use segmentation_models_pytorch in offline environment, I loaded modules I cloned and uploaded to dataset.","d9e90323":"##  Set parameters","75ed48b4":"## Dataset","29621c86":"## Model","865b8a18":"To save time, the number of \"folds\" is smaller. \nThese days, depending on the model, I think it's more common to use 5 ~ 10.","c3bac618":"# Submission notebook of HuBMAP - Pytorch smp Unet++ Inference\n\n\nTrain part is here.\n\nhttps:\/\/www.kaggle.com\/nayuts\/hubmap-pytorch-smp-unet","735afdd9":"I create train and inference notebook for Unet++ of [segmentation_models.pytorch](https:\/\/github.com\/qubvel\/segmentation_models.pytorch).\n\nI use only pytorch for framework.\n\nThe accuracy of the model is going to be tuned and improved in the future.\n\nI published this notebook for our reference as an example implementation using only pytorch.","0c398742":"I refered following two great notebooks for training and inference.\n\n- https:\/\/www.kaggle.com\/iafoss\/hubmap-pytorch-fast-ai-starter\n\n- https:\/\/www.kaggle.com\/curiosity806\/hubmap-use-catalyst-smp-and-albumentations\n\nAnd refered following great notebook for dice loss.\n\n- https:\/\/www.kaggle.com\/bigironsphere\/loss-function-library-keras-pytorch","b44bae50":"# Inference"}}