{"cell_type":{"28ab35ab":"code","19994479":"code","1dc93254":"code","5b703941":"code","84378c7b":"code","4636c5a6":"code","0f47678d":"code","148911b3":"code","a0361eba":"code","e0ea84b3":"code","e63612e9":"code","103a83e3":"code","a3f640e5":"code","ada502f8":"code","d6a61b63":"code","fc4e7ea8":"code","2089086a":"code","96992511":"code","073f1838":"code","752e3aa7":"code","79bb236a":"code","e59301fb":"code","21c05e5a":"code","238897ea":"code","1419fc93":"code","900dcd55":"code","4413f998":"code","b40d5918":"code","6f5110e7":"code","5eca426f":"code","87cd097f":"code","dcf29ab3":"code","367ceabf":"code","01cc2893":"code","8c26fd5c":"code","b5f9a542":"code","84610e49":"code","b5237c63":"code","2364593e":"code","a12d977c":"code","9b8e17a8":"code","bc0bf071":"code","3f9b7570":"code","c6a0c26b":"code","8da391c9":"code","e8d5f3ae":"code","f1785cbf":"code","12b07165":"code","087d0b16":"code","5fe08b79":"code","1f776207":"code","6a6ddb93":"code","777163eb":"code","7ffc719f":"code","83da59f6":"markdown","8a7b6013":"markdown","18645a37":"markdown","bb979723":"markdown","293c1cb9":"markdown","f0e7f2b4":"markdown","195a916e":"markdown","5661a12f":"markdown","7fbc189a":"markdown","ac516db5":"markdown","589f185e":"markdown","44aa1366":"markdown","33de643b":"markdown","fce042ce":"markdown","85c9337e":"markdown","bef4820f":"markdown","a68424af":"markdown"},"source":{"28ab35ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix,f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt \n  \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nimport missingno as ms\n\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport nltk\nnltk.download('stopwords')\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","19994479":"training_data = pd.read_csv('..\/input\/twitter-hate-speech\/train_E6oV3lV.csv') #to read and store in panda dataframe\ntesting_data = pd.read_csv('..\/input\/twitter-hate-speech\/test_tweets_anuFYb8.csv') #to read and store in panda dataframe","1dc93254":"len(testing_data)","5b703941":"len(training_data)","84378c7b":"nltk.download('stopwords')\neng_stops = set(stopwords.words(\"english\"))","4636c5a6":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() ","0f47678d":"def process_message(review_text):\n    # remove all the special characters\n    new_review_text = re.sub(\"[^a-zA-Z]\", \" \",review_text) \n    # convert all letters to lower case\n    words = new_review_text.lower().split()\n    # remove stop words\n    words = [w for w in words if not w in eng_stops]    \n    # lemmatizer\n    words = [lemmatizer.lemmatize(word) for word in words]\n    # join all words back to text\n    return (\" \".join(words))","148911b3":"training_data['clean_tweet']=training_data['tweet'].apply(lambda x: process_message(x))","a0361eba":"training_data.head()","e0ea84b3":"training_data.shape","e63612e9":"testing_data.shape","103a83e3":"training_data.info()","a3f640e5":"training_data.label.value_counts()","ada502f8":"# Data balance\ndef createPieChartFor(t_df):\n    Lst = 100*t_df.value_counts()\/len(t_df)\n    \n    # set data for pie chart\n    labels = t_df.value_counts().index.values\n    sizes =  Lst \n    \n    # set labels\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90)\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()","d6a61b63":"createPieChartFor(training_data.label)","fc4e7ea8":"training_data['length'] =  training_data['tweet'].apply(len)\nfig1 = sns.barplot('label', 'length', data= training_data)\nplt.title('Average word length vs label')\n","2089086a":"from wordcloud import WordCloud,STOPWORDS\ndef createWrdCloudForSentiment(sentiment):\n    sentiment_num = 1 if sentiment== 'Hate' else 0\n    temp_df = training_data[training_data.label==sentiment_num]\n    words = \" \".join(temp_df.clean_tweet)\n    cleaned_words = \" \".join([w for w in words.split()\n                                  if 'http' not in w\n                                    and not w.startswith('@')\n                                    and w!='RT'])\n\n    wrdcld = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=1500,\n                      height=1000).generate(cleaned_words)\n    plt.figure(figsize=(10,10))\n    plt.imshow(wrdcld)\n    plt.axis('off')\n    plt.show","96992511":"createWrdCloudForSentiment('Hate')","073f1838":"createWrdCloudForSentiment('Positive')","752e3aa7":"#from spellchecker import SpellChecker\n\n#spell = SpellChecker()","79bb236a":"from sklearn.model_selection import train_test_split\ntrain_df, test_df =  train_test_split(training_data, test_size = 0.2, random_state = 42)","e59301fb":"vectorizer = TfidfVectorizer()\ntrain_tfidf_model = vectorizer.fit_transform(train_df.clean_tweet)\ntest_tfidf_model = vectorizer.transform(test_df.clean_tweet)","21c05e5a":"train_tfidf_model","238897ea":"train_tfidf = pd.DataFrame(train_tfidf_model)\ntrain_tfidf","1419fc93":"cls = [LogisticRegression(),\n       MultinomialNB(), \n       DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=200),\n       KNeighborsClassifier(n_neighbors = 5)]\n\ncls_name = []","900dcd55":"test_tfidf_model","4413f998":"test_df.label.count()","b40d5918":"lbl_actual = test_df.label\ni = 0\naccuracy = []\nfor cl in cls:\n    model = cl.fit(train_tfidf_model,train_df.label)\n    lbl_pred = model.predict(test_tfidf_model)\n    a = (100*accuracy_score(lbl_pred, lbl_actual))\n    a = round(a,2)\n    accuracy.append(a)\n    cls_name.append(cl.__class__.__name__)\n    print (\"{}  Accuracy Score : {}%\".format(cls_name[i],a))\n    print ( classification_report(lbl_pred, lbl_actual))\n    i +=1","6f5110e7":"plt.bar(cls_name, accuracy)\nplt.xticks(rotation=70)","5eca426f":"# Predict accuracy\ndef getModelAccuracy_LogicalReg(model_name, sampled_train_df) :\n    # bag of words model\n    vectorizer = TfidfVectorizer()\n    sampled_train_tfidf_model = vectorizer.fit_transform(sampled_train_df.clean_tweet)\n    sampled_test_tfidf_model = vectorizer.transform(test_df.clean_tweet)\n\n\n    # let's look at the dataframe\n    #sampled_train_tfidf = pd.DataFrame(sampled_train_tfidf_model.toarray(), columns=vectorizer.get_feature_names())\n    #sampled_train_tfidf\n\n    # Predict\n    sample_model = LogisticRegression().fit(sampled_train_tfidf_model,sampled_train_df.label)\n    lg_lbl_pred = sample_model.predict(sampled_test_tfidf_model)\n    a = (100*accuracy_score(lg_lbl_pred, lbl_actual))\n    a = round(a,2)\n    print (\"{}  Accuracy Score : {}%\".format(model_name,a))\n    #print(type(a))\n    return float(a)","87cd097f":"# Predict accuracy using rfc\ndef getModelAccuracy_RFC(model_name, sampled_train_df) :\n\n    # bag of words model\n    vectorizer = TfidfVectorizer()\n    sampled_train_tfidf_model = vectorizer.fit_transform(sampled_train_df.clean_tweet)\n    sampled_test_tfidf_model = vectorizer.transform(test_df.clean_tweet)\n\n\n    # let's look at the dataframe\n    #sampled_train_tfidf = pd.DataFrame(sampled_train_tfidf_model.toarray(), columns=vectorizer.get_feature_names())\n    #sampled_train_tfidf\n\n    # Predict\n    sample_model = RandomForestClassifier(n_estimators=200).fit(sampled_train_tfidf_model,sampled_train_df.label)\n    lg_lbl_pred = sample_model.predict(sampled_test_tfidf_model)\n    a = (100*accuracy_score(lg_lbl_pred, lbl_actual))\n    a = round(a,2)\n    print (\"{}  Accuracy Score : {}%\".format(model_name,a))\n    return float(a)","dcf29ab3":"log_accuracy = []\nrfc_accuracy = []","367ceabf":"a = getModelAccuracy_LogicalReg(\"Train dataset\", train_df)\n#print(a)\nlog_accuracy.append(a)","01cc2893":"log_accuracy","8c26fd5c":"a = getModelAccuracy_RFC(\"Train dataset\", train_df)\nrfc_accuracy.append(a)","b5f9a542":"rfc_accuracy","84610e49":"createPieChartFor(train_df.label)","b5237c63":"print(train_df.label.value_counts())","2364593e":"#As this dataset is highly imbalance we have to balance this by under sampling\ncount_hate = train_df[train_df['label'] == 1]['clean_tweet'].count()\ndf_non_hate_speech = train_df[train_df['label'] == 0]\ndf_hate_speech = train_df[train_df['label'] == 1]\ndf_hate_speech_undersample = df_non_hate_speech.sample(count_hate, replace=True)\ntrain_df_undersampled = pd.concat([df_hate_speech, df_hate_speech_undersample], axis=0)\n\nprint('Random under-sampling:')\nprint(train_df_undersampled['label'].value_counts())","a12d977c":"a = getModelAccuracy_LogicalReg(\"Under Sampling\", train_df_undersampled)\nlog_accuracy.append(a)","9b8e17a8":"#As this dataset is highly imbalance we have to balance this by over sampling\ncount_non_hate = train_df[train_df['label'] == 0]['clean_tweet'].count()\ndf_hate_speech = train_df[train_df['label'] == 1]\ndf_non_hate_speech = train_df[train_df['label'] == 0]\ndf_hate_speech_oversample = df_hate_speech.sample(count_non_hate, replace=True)\ntrain_df_oversampled = pd.concat([df_non_hate_speech, df_hate_speech_oversample], axis=0)\n\nprint('Random over-sampling:')\nprint(train_df_oversampled['label'].value_counts())","bc0bf071":"a = getModelAccuracy_LogicalReg(\"Over Sampling\", train_df_oversampled)\nlog_accuracy.append(a)","3f9b7570":"a= getModelAccuracy_RFC(\"Under Sampling\", train_df_undersampled)\nrfc_accuracy.append(a)","c6a0c26b":"a = getModelAccuracy_RFC(\"Over Sampling\", train_df_oversampled)\nrfc_accuracy.append(a)","8da391c9":"rfc_accuracy","e8d5f3ae":"\nX = ['Train Dataset','Under Sampled','Over Sampled']\n\n  \nX_axis = np.arange(len(X))\n  \nplt.bar(X_axis - 0.2, log_accuracy, 0.4, label = 'Log')\nplt.bar(X_axis + 0.2, rfc_accuracy, 0.4, label = 'RFC')\n  \nplt.xticks(X_axis, X)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs model and sampling\")\nplt.legend()\nplt.show()","f1785cbf":"training_data","12b07165":"train_df","087d0b16":"testing_data","5fe08b79":"testing_data['clean_tweet']=testing_data['tweet'].apply(lambda x: process_message(x))","1f776207":"# bag of words model\nvectorizer = TfidfVectorizer()\nsampled_train_tfidf_model = vectorizer.fit_transform(train_df.clean_tweet)\nsampled_test_tfidf_model = vectorizer.transform(testing_data.clean_tweet)\n\n\n# Predict\nsample_model = LogisticRegression().fit(sampled_train_tfidf_model,train_df.label)\nlg_lbl_pred = sample_model.predict(sampled_test_tfidf_model)","6a6ddb93":"lg_lbl_pred_df = pd.DataFrame({'id': testing_data.id,\n                            'tweet' : testing_data.tweet,\n                            'label' : lg_lbl_pred})\nlg_lbl_pred_df.head()","777163eb":"lg_lbl_pred_df.label.value_counts()","7ffc719f":"lg_lbl_pred_df.to_csv('hate_speech_output.csv', index=False)","83da59f6":"# Output","8a7b6013":"### Undersampling","18645a37":"# Conclusion\n\nLogistic regression gives good accuracy on the current data set. \n","bb979723":"Let's see the distribution of data","293c1cb9":"TF-IDF","f0e7f2b4":"### Model selection","195a916e":"# Data Processing","5661a12f":"we can see that word's common in positive comments are: love, life, makepeople, today, happy","7fbc189a":"# Data Imbalance Handling\n\n### Check the data imbalance","ac516db5":"### Oversampling","589f185e":"# Model Building: Process\n![Capture.JPG](attachment:f7dcd3ac-e2c9-499b-8345-dd193adaca8d.JPG)","44aa1366":"# EDA","33de643b":"## Model Building","fce042ce":"We can see that we have only 7% data availble classified as hate comment.\nsince data is imbalanced, we should explore data balancing techinques. First let's continue with current data ","85c9337e":"**We can see that there hasn't been much improvement with over sampling and under sampling, so we can go with trained data**\n\n","bef4820f":"## Make test-train split","a68424af":"we can see that word's common in hate comments are: trump, libtard, hate, white, black, racist"}}