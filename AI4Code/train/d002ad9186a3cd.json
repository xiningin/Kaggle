{"cell_type":{"c74907a5":"code","b870e7f0":"code","d8e3300c":"code","a3a2e8f7":"code","f81f91a9":"code","91b11f88":"code","57cb77ba":"code","25048371":"code","533d7ce1":"code","e9576399":"code","4f654a27":"code","0c59faf5":"code","145a6c28":"code","5043e288":"code","ba076f61":"code","bfdf4d4a":"code","2713927f":"code","8681981d":"code","3c713e1f":"code","5b781e22":"code","e3c536d1":"code","2c3e3ac4":"code","538dcd61":"code","cce1dc36":"code","07cd28ce":"code","ee5afd14":"code","c233d900":"code","af7b81df":"code","dd7ac1b8":"code","4e85390e":"code","70e83dbb":"code","472c0a7b":"code","92248e54":"code","e716ffdc":"code","af5da3f9":"code","4d6ead53":"code","e0494c08":"code","56313ad5":"code","da819cdf":"code","b9436827":"code","1e22efb2":"code","7e6e4d25":"code","a4cca089":"code","a9eb1b66":"code","4dfb0e4a":"code","39f6d9bf":"code","5334d047":"code","7c6d9581":"code","da5ac9e4":"code","4111ab15":"code","e92df2af":"code","901cf4a2":"code","8de19448":"code","5783e056":"code","1cde5a44":"code","c36efb2a":"code","7f00df66":"code","9686e4b8":"code","8537b0ba":"code","10bcb8e9":"code","ee1f249d":"code","6d851512":"markdown","3da8c8f0":"markdown","7d46bb01":"markdown","7716be6c":"markdown","3c91ea30":"markdown","172dae41":"markdown","d0ac96da":"markdown","2675ff07":"markdown","d0cc71aa":"markdown","72729333":"markdown","8ff04ae5":"markdown","45d4ea44":"markdown","f64e4bf5":"markdown","7b43dfcf":"markdown","91f6d762":"markdown","420f85e8":"markdown"},"source":{"c74907a5":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context='notebook',\n        style='whitegrid',\n        palette='deep',\n        font='sans-serif',\n        font_scale=1,\n        color_codes=True,\n        rc=None)\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import TimeseriesGenerator","b870e7f0":"from statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.graphics.tsaplots as sgt\nimport statsmodels.tsa.stattools as sts","d8e3300c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nimport datetime, os\nfrom keras.preprocessing.sequence import TimeseriesGenerator","a3a2e8f7":"tf.__version__","f81f91a9":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","91b11f88":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","57cb77ba":"train_data = pd.read_csv('\/kaggle\/input\/web-traffic-data-set\/train_1.csv')\ntrain = train_data\ntrain.head(3)","25048371":"train.info()","533d7ce1":"%%time\ntrain = train.fillna(method='ffill', downcast='infer')\ntrain.tail(3)","e9576399":"%%time\nfor cols in train.columns[1:]:\n    train[cols] = pd.to_numeric(train[cols], downcast='integer')","4f654a27":"train.info()","0c59faf5":"df = pd.DataFrame(train.iloc[:,1:].values.T,\n            columns=train.Page.values, index=train.columns[1:])\ndf.index = pd.to_datetime(df.index, errors='ignore',\n                                            dayfirst=False,\n                                            yearfirst=False,\n                                            utc=None,\n                                            format=\"%Y\/%m\/%d\",\n                                            exact=False,\n                                            unit=None,\n                                            infer_datetime_format=True,\n                                            origin='unix',\n                                            cache=True)\ndf.head(3)","145a6c28":"list(df.columns)[:10]  # First 10 pages","5043e288":"wikipedia = (df.filter(like='wikipedia'))\nwikipedia","ba076f61":"wikipedia.iloc[:,0:10].plot(figsize=(20,10))\nplt.show()","bfdf4d4a":"def get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page)\n    if res:\n        return res[0][0:2]\n    return 'other'\n\n(wikipedia.columns.map(get_language)).unique()","2713927f":"len((wikipedia.columns.map(get_language)).unique())","8681981d":"languages = list((wikipedia.columns.map(get_language)).unique())\nlanguages.remove('other')\nlanguages","3c713e1f":"for lang in (languages):\n    locals()['lang_'+str(lang)] = wikipedia.loc[:, wikipedia.columns.str.contains('_'+str(lang)+'.wiki')]","5b781e22":"lang_en.head(3)","e3c536d1":"for lang in (languages):\n    locals()['hits_'+str(lang)] = np.array(locals()['lang_'+str(lang)].iloc[:,:].sum(axis=1))","2c3e3ac4":"for lang in (languages):\n    print((locals()['hits_'+str(lang)]).shape)","538dcd61":"keys = languages\nvalues = ['Chinese', 'French', 'English', 'Russian', 'German', 'Japanese', 'Spanish']","cce1dc36":"d = dict(zip(keys,values))","07cd28ce":"index = wikipedia.index\n\nhits = pd.DataFrame(index=index, columns=list(d.values()))\nhits = hits.fillna(0)","ee5afd14":"for key, value in d.items():\n    hits[value] = locals()['hits_'+str(key)]","c233d900":"hits","af7b81df":"hits.plot(figsize=(25,8), title ='Hits on Wikipedia pages per Language', fontsize=15)\nplt.legend(loc='upper left')\nplt.show()","dd7ac1b8":"plt.rcParams[\"figure.dpi\"] = 100\nhits.iloc[:,0:1].plot(figsize=(20,4))\nsgt.plot_acf(np.array(hits.iloc[:,0:1]),\n            ax=None,\n            lags=None,\n            alpha=0.05,\n            use_vlines=True,\n            unbiased=False,\n            fft=False,\n            missing='none',\n            title='Autocorrelation',\n            zero=False,  # Not including the 1st term as its acf w.r.t. itself will always be 1.\n            vlines_kwargs=None)\nplt.show()","4e85390e":"plt.rcParams[\"figure.dpi\"] = 100\nhits.iloc[:,0:1].plot(figsize=(20,4))\nsgt.plot_pacf(np.array(hits.iloc[:,0:1]),\n            ax=None,\n            lags=None,\n            alpha=0.05,\n            method='ols',\n            use_vlines=True,\n            title='Partial Autocorrelation',\n            zero=False,    # Not including the 1st term as its pacf w.r.t. itself will always be 1.\n            vlines_kwargs=None)\nplt.show()","70e83dbb":"brk = 0.8\ndata_split = int(len(hits)*brk)\ndata_split","472c0a7b":"X, y = hits.iloc[:data_split,:], hits.iloc[data_split:,:]","92248e54":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X)\n\nscaled_X = scaler.transform(X)\nscaled_y = scaler.transform(y)","e716ffdc":"print(scaled_X.max(), scaled_X.min())\nprint(scaled_y.max(), scaled_y.min())","af5da3f9":"X_df = (pd.DataFrame(scaled_X))\ny_df = (pd.DataFrame(scaled_y))","4d6ead53":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8), dpi=100)\nplt.suptitle('Train-Test Split', fontsize=20)\nX_df.plot(ax=axes[0], title='Train Data')\ny_df.plot(ax=axes[1], title='Test Data')\n\nplt.show()","e0494c08":"pd.DataFrame(scaled_y[3:13,:]).plot(figsize=(15,5), title='Periodicity')\nplt.show()","56313ad5":"print(scaled_X.shape)\nprint(scaled_y.shape)\nprint('No. of features = '+str(scaled_X.shape[1]))\nprint('No. of train instances = '+str(scaled_X.shape[0]))\nprint('No. of test instances = '+str(scaled_y.shape[0]))","da819cdf":"length = 7\nbatch = 1\n\nn_features = scaled_X.shape[1]\nn_features","b9436827":"generator = TimeseriesGenerator(data = scaled_X,\n                                targets = scaled_X,\n                                length = length,\n                                sampling_rate=1,\n                                stride=1,\n                                start_index=0,\n                                end_index=None,\n                                shuffle=False,\n                                reverse=False,\n                                batch_size=batch)","1e22efb2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nimport datetime, os\n\nmodel = Sequential(layers=None, name=\"LSTM_Model\")\n\nmodel.add(LSTM( units = 400,               \n                activation='tanh',\n                input_shape=( length, n_features),                \n                recurrent_activation='sigmoid',\n                use_bias=True,\n                kernel_initializer='glorot_uniform',\n                recurrent_initializer='orthogonal',\n                bias_initializer='zeros',\n                unit_forget_bias=True,\n                kernel_regularizer=None,\n                recurrent_regularizer=None,\n                bias_regularizer=None,\n                activity_regularizer=None,\n                kernel_constraint=None,\n                recurrent_constraint=None,\n                bias_constraint=None,\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                implementation=2,\n                return_sequences=True,\n                return_state=False,\n                go_backwards=False,\n                stateful=False,\n                time_major=False,\n                unroll=False\n            ) )\nmodel.add(LSTM(units = 500, return_sequences=True))\n\nmodel.add(LSTM(units = 500, return_sequences=False))\n\nmodel.add(Dense(700, activation=\"relu\", name=\"layer1\"))\n\nmodel.add(Dense(100, activation=\"relu\", name=\"layer2\"))\n\n\nmodel.add(Dense( units = n_features,               \n                activation='relu',\n                use_bias=True,                        \n                kernel_initializer='glorot_uniform',  \n                bias_initializer='zeros',             \n                kernel_regularizer=None,              \n                bias_regularizer=None,                \n                activity_regularizer=None,            \n                kernel_constraint=None,               \n                bias_constraint=None))                \n\n\n\nmodel.compile(optimizer='adam', loss='mse')","7e6e4d25":"model.summary()","a4cca089":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss',\n                        min_delta=0,\n                        patience=20,\n                        verbose=1,  \n                        mode='auto',\n                        baseline=None,  \n                                               \n                        restore_best_weights=False)","a9eb1b66":"validation_generator = TimeseriesGenerator(scaled_y,scaled_y, length=length, batch_size=batch)","4dfb0e4a":"%%time\nhistory = model.fit(generator,\n                    steps_per_epoch=None,\n                    epochs=500,\n                    verbose=1,\n                    callbacks=[early_stop],\n                    validation_data = validation_generator,\n                    validation_steps=None,\n                    validation_freq=1,\n                    class_weight=None,\n                    max_queue_size=10,\n                    workers=1,\n                    use_multiprocessing=False,\n                    shuffle=True,\n                    initial_epoch=0)","39f6d9bf":"print(history.history.keys())","5334d047":"losses = pd.DataFrame(model.history.history)","7c6d9581":"plt.rcParams[\"figure.dpi\"] = 100\nlosses.plot(figsize=(10,5))\nplt.title('Losses')\nplt.show()","da5ac9e4":"%%time\ntest_predictions = []\n\nfirst_eval_batch = scaled_X[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(scaled_y)):\n    \n    current_pred = model.predict(current_batch,verbose=0)[0]\n    \n    test_predictions.append(current_pred) \n    \n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","4111ab15":"print(np.array(test_predictions).shape)\nprint(scaled_y.shape)","e92df2af":"print(np.array(test_predictions).max(), np.array(test_predictions).min())\nprint(scaled_y.max(), scaled_y.min())","901cf4a2":"true_predictions = scaler.inverse_transform(test_predictions)\nprint(true_predictions.shape)","8de19448":"print(np.array(true_predictions).max(), np.array(true_predictions).min())\nprint(np.array(y).max(), np.array(y).min())","5783e056":"t_l = len(scaled_y)\nt_l","1cde5a44":"plt.figure(dpi=100)\nplt.plot(np.linspace(0,t_l,t_l), scaled_y[:,0:1] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,0:1], label='Predicted Values',c='r')\nplt.title(hits.columns[0], fontsize=20)\nplt.legend()\nplt.show()","c36efb2a":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,1:2] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,1:2], label='Predicted Values',c='r')\nplt.title(hits.columns[1], fontsize=20)\nplt.legend()\nplt.show()","7f00df66":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,2:3] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,2:3], label='Predicted Values',c='r')\nplt.title(hits.columns[2], fontsize=20)\nplt.legend()\nplt.show()","9686e4b8":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,3:4] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,3:4], label='Predicted Values',c='r')\nplt.title(hits.columns[3], fontsize=20)\nplt.legend()\nplt.show()","8537b0ba":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,4:5] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,4:5], label='Predicted Values',c='r')\nplt.title(hits.columns[4], fontsize=20)\nplt.legend()\nplt.show()","10bcb8e9":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,5:6] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,5:6], label='Predicted Values',c='r')\nplt.title(hits.columns[5], fontsize=20)\nplt.legend()\nplt.show()","ee1f249d":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,6:7] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,6:7], label='Predicted Values',c='r')\nplt.title(hits.columns[6], fontsize=20)\nplt.legend()\nplt.show()","6d851512":"## Partial Auto-correlation\nShows that upto 11 lags, the data points are relvant.","3da8c8f0":"## Inference:\nIt can be infered that prediction on the spanish wikipedia pages has been the best. Now it can be improved by tweaking the LSTM architecture and the length of time sequence to be fed. The model has been tested with 440 training instances, which is too low, and tested on 110 instances. No. of features is 7.","7d46bb01":"## Train-Test Split","7716be6c":"## Auto Correlation\nIt is showing that data points even after lag of 25 is also relevant.","3c91ea30":"# LSTM \n## (Predicting on sum of web hits of wikipedia pages per language)","172dae41":"# Transposing the dataframe with time stamps in index, and page names in columns","d0ac96da":"#### Predictions","2675ff07":"### Creating Separate Dataframe for Wikipedia hits","d0cc71aa":"### Replacing nan values with forward fill","72729333":"# Loading Data","8ff04ae5":"### Converting data to integer values to reduce memory consumption","45d4ea44":"### List of different types of webpages","f64e4bf5":"# Plotting 1st 10 page hits","7b43dfcf":"### Time Series Generator\n#### From ACF and PACF plots, it is optimal to select a time sequence of 7-10 time stamps. The reason of choosing 7 is for weekly cycle.","91f6d762":"### Languages of wikipedia pages","420f85e8":"### Creating Separate Dataframes for every language of wikipedia pages"}}