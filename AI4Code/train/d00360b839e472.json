{"cell_type":{"8487f222":"code","c652ab43":"code","61dbc6f6":"code","76069e4c":"code","ccdb45f5":"code","b8caf2d6":"code","36fd961b":"code","bdb03700":"code","a4625369":"code","1f9ab2a8":"code","d67754b6":"code","758044de":"code","d04d6f6b":"code","b7911440":"code","9b4d147a":"code","7c75625b":"code","d2e5f5d7":"code","a6d3d479":"code","ed9df2db":"code","78a6aa89":"code","22872fea":"code","32960a2e":"code","98b86f72":"markdown","d293c0e3":"markdown","16acaf1c":"markdown"},"source":{"8487f222":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c652ab43":"import pandas as pd\nimport numpy as np\nimport bz2\nimport matplotlib.pyplot as plt\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n%matplotlib inline","61dbc6f6":"def labels_texts(file):\n    labels = []\n    texts = []\n    for line in bz2.BZ2File(file):\n        x = line.decode(\"utf-8\")\n        labels.append(int(x[9]) - 1)\n        texts.append(x[10:].strip())\n    return np.array(labels), texts","76069e4c":"train_label, train_text = labels_texts('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_label, test_text = labels_texts('..\/input\/amazonreviews\/test.ft.txt.bz2')","ccdb45f5":"# labels_mod = []\n# texts_mod = []\n","b8caf2d6":"print(train_label[0])\nprint(train_text[0])","36fd961b":"import re\nnot_numChar = re.compile(r'[\\W]')\nno_encode = re.compile(r'[^a-z0-1\\s]')\ndef normalisation(texts):\n    norm_text = []\n    for word in texts:\n        lower = word.lower()\n        not_punct = not_numChar.sub(r' ', lower)\n        exclude_no_encode = no_encode.sub(r'', not_punct)\n        norm_text.append(exclude_no_encode)\n    return norm_text","bdb03700":"train_text = normalisation(train_text)\ntest_text = normalisation(test_text)","a4625369":"print(train_text[0])","1f9ab2a8":"y_train = np.array(train_label)\ny_test = np.array(test_label)","d67754b6":"y_test.shape","758044de":"max_features = 8192\nmaxlen = 128\nembed_size = 64\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_text)","d04d6f6b":"training_token = tokenizer.texts_to_sequences(train_text)\ntesting_token = tokenizer.texts_to_sequences(test_text)","b7911440":"x_train = pad_sequences(training_token, maxlen = maxlen, padding = 'post')\nx_test = pad_sequences(testing_token, maxlen = maxlen, padding = 'post')","9b4d147a":"\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strata = tf.distribute.experimental.TPUStrategy(tpu)","7c75625b":"BATCH_SIZE = 16* tpu_strata.num_replicas_in_sync\nwith tpu_strata.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(max_features, embed_size, input_length = maxlen),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Conv1D(64, 5, activation = 'relu'),\n        tf.keras.layers.MaxPooling1D(pool_size = 4),\n        tf.keras.layers.GlobalAveragePooling1D(),\n        tf.keras.layers.Dense(128, activation = 'relu'),\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n\n    model.compile('adam', 'binary_crossentropy', metrics = ['accuracy'], steps_per_execution=16)\n    model.summary()\n\nhistory = model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 5, validation_data = (x_test, y_test), verbose = 2)","d2e5f5d7":"model.evaluate (x_test, y_test)","a6d3d479":"import matplotlib.image  as mpimg\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()","ed9df2db":"BATCH_SIZE = 16* tpu_strata.num_replicas_in_sync\nwith tpu_strata.scope():\n    model_lstm = tf.keras.Sequential([\n        tf.keras.layers.Embedding(max_features, embed_size, input_length = maxlen),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    \n        tf.keras.layers.Dense(128, activation = 'relu'),\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n\n    model_lstm.compile('adam', 'binary_crossentropy', metrics = ['accuracy'], steps_per_execution=16)\n    model_lstm.summary()\n\nhistory_lstm = model_lstm.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 5, validation_data = (x_test, y_test), verbose = 2)","78a6aa89":"model_lstm.evaluate(x_test, y_test)","22872fea":"import matplotlib.image  as mpimg\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history_lstm.history['accuracy']\nval_acc=history_lstm.history['val_accuracy']\nloss=history_lstm.history['loss']\nval_loss=history_lstm.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()","32960a2e":"# from sklearn.feattokenizerxtraction.text import CountVectorizer\n# countVector = CountVectorizer(True)\n# countVector.fit(train_text)\n# x = countVector.transform(train_text)\n# X_test = countVector.transform(test_text)","98b86f72":"### fully connected convolutional ","d293c0e3":"-------------------------------------------------------------------------------\n## NLP using simple linear regression ","16acaf1c":"------------------------------------------------\n### Using a bidirectional LSTM model"}}