{"cell_type":{"51da79b4":"code","a9d93295":"code","13148e0f":"code","6d1bbb02":"code","5bfe50ac":"code","838914d7":"code","290056da":"code","2c7236a2":"code","d689ab05":"code","f33382c0":"code","9f7e7324":"code","fa93e3e3":"code","7e284f12":"code","7308b8b3":"code","cdfa8b15":"code","e04e2d95":"markdown","57cf37f9":"markdown","5172c58e":"markdown","5f04ebde":"markdown","5f3c48a1":"markdown","de74091b":"markdown","0e23aba3":"markdown","e586a599":"markdown","5d5f1d93":"markdown","1f0f2bce":"markdown","e12bea85":"markdown"},"source":{"51da79b4":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","a9d93295":"full_data = pd.read_csv('..\/input\/used-bikes-prices-in-india\/Used_Bikes.csv')\nX, y = full_data.drop('price', axis = 1), full_data['price']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 2207)","13148e0f":"X_train.head()","6d1bbb02":"X_train.info()\n\n## No missing value\n## 3 continuous and 4 categorical predictors","5bfe50ac":"y_train","838914d7":"fig, ax = plt.subplots(1, 2, figsize = (10, 5))\nsns.histplot(y_train, kde = True, ax = ax[0]).set_title('Price')\nsns.histplot(y_train, kde = True, log_scale = True, ax = ax[1]).set_title('Price, in log-scale')\nplt.show()\n\n## The distribution of response variable is skewed,\n# so maybe it's a good idea? to try transforming the response using log later.","290056da":"sns.jointplot(\n    x = np.log1p(X_train['power']),\n    y = np.log(y_train),\n    kind = 'reg'\n)\n\nsns.jointplot(\n    x = np.log1p(X_train['age']),\n    y = np.log(y_train),\n    kind = 'reg'\n)\n\nsns.jointplot(\n    x = np.log1p(X_train['kms_driven']),\n    y = np.log(y_train),\n    kind = 'reg'\n)\n\nplt.show()","2c7236a2":"from sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n\n# mapper that transforms each specified column\n# with the specified preprocessor\/transformer\n# note that I don't include bike names on this preprocessor\n# because I want to drop it entirely\npreprocessor = DataFrameMapper([\n    (['kms_driven'], FunctionTransformer(np.log1p)),\n    (['age'], FunctionTransformer(np.log1p)),\n    (['power'], FunctionTransformer(np.log1p)),\n    (['city'], OneHotEncoder(handle_unknown = 'ignore')),\n    (['brand'], OneHotEncoder(handle_unknown = 'ignore'))\n], df_out = True)","d689ab05":"## Regularized regression using cross-validation and grid search to choose\n## the best value of regularization term.\n\nfrom sklearn.linear_model import Lasso, Ridge\n\nalphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nlasso = Lasso()\nridge = Ridge()","f33382c0":"from sklearn.pipeline import Pipeline\n\nlasso_pipeline = Pipeline([\n    ('prep', preprocessor),\n    ('clf', lasso)\n])\n\nridge_pipeline = Pipeline([\n    ('prep', preprocessor),\n    ('clf', ridge)\n])","9f7e7324":"from sklearn.model_selection import GridSearchCV\n\nlasso_gs = GridSearchCV(\n    lasso_pipeline,\n    {\n        'clf__alpha': alphas\n    },\n    scoring = 'neg_mean_squared_error'\n)\n\nridge_gs = GridSearchCV(\n    ridge_pipeline,\n    {\n        'clf__alpha': alphas\n    },\n    scoring = 'neg_mean_squared_error'\n)","fa93e3e3":"%%capture --no-display --no-stdout\n# This just jupyter magic\n# to suppress so many ConvergenceWarnings that becomes\n# annoying to read.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# I have to define RMSE manually because regression models in\n# sklearn have R^2 as default scoring method.\n# Also, sklearn only provides -RMSE instead of RMSE.\nRMSE_score = make_scorer(mean_squared_error, squared = False)\n\nlasso_score = cross_val_score(\n    lasso_gs,\n    X_train, y_train,\n    scoring = RMSE_score\n)\n\nridge_score = cross_val_score(\n    ridge_gs,\n    X_train, y_train,\n    scoring = RMSE_score\n)","7e284f12":"_score = pd.DataFrame({\n    'lasso': lasso_score,\n    'ridge': ridge_score\n})\n\n_score.loc['mean'] = _score.mean()\n_score","7308b8b3":"%%capture\n\n# Fitting the model using entire data\nlasso_pipeline.fit(X_train, y_train)\nridge_pipeline.fit(X_train, y_train)","cdfa8b15":"from sklearn.metrics import mean_squared_error\n\nlasso_pred = lasso_pipeline.predict(X_test)\nridge_pred = ridge_pipeline.predict(X_test)\n\nprint(\"Lasso test RMSE:\", mean_squared_error(y_test, lasso_pred, squared = False))\nprint(\"Ridge test RMSE:\", mean_squared_error(y_test, ridge_pred, squared = False))","e04e2d95":"# Model","57cf37f9":"# Data\n\nI will split the data into training and testing, and in the model building phase I will only use the training set. Imagine the testing set as \"new\" data that I can't see at all before I finish building my models.","5172c58e":"Thanks for visiting my notebook!","5f04ebde":"# Continuous predictor","5f3c48a1":"# Preprocessors","de74091b":"# Joining into pipeline\n\nPreprocessor and model is joined into a pipeline. Raw data then can be fed into the pipeline. The preprocessors, transformers, models inside the pipeline will work automatically (we need not to transform manually etc.)","0e23aba3":"In this notebook I will demonstrate the process of model building and selection to predict used bike prices.","e586a599":"# Cross-validation score on training set\n\nThis is just to demonstrate how I can gauge\/evaluate the performance of my model only using the training set. Remember, up until now, I haven't seen the test set yet! ;)","5d5f1d93":"# Wrapping the grid search","1f0f2bce":"# The final test","e12bea85":"# Response"}}