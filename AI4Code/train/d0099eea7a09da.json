{"cell_type":{"7ba64acc":"code","0809b85d":"code","faeb1829":"code","690b76dd":"code","8f9f685d":"code","3c966e1c":"code","361e881c":"code","a083212d":"code","7a9f5c14":"code","76c8f680":"code","50adb8ab":"code","f3f696aa":"code","8c15fde9":"code","14b5d08e":"code","9fe26655":"code","c1ec6743":"code","96b4ba69":"code","88729b2c":"code","aa3d2720":"code","1d74883e":"code","4fb39451":"code","d875421a":"code","feb44092":"code","bfe4b1dd":"code","b360ca83":"code","9b5fafd6":"markdown","c4e59365":"markdown","c581259c":"markdown","9572d48a":"markdown","e18e189f":"markdown","99fe8945":"markdown","f34a9714":"markdown","b752131f":"markdown","e1f2f84f":"markdown","4eed0ecf":"markdown","df64d2d8":"markdown","d460c07b":"markdown","89a9592e":"markdown","55eea0a4":"markdown","78e43646":"markdown","b9be8e61":"markdown","8a36d272":"markdown","3b8b8444":"markdown","12cdd443":"markdown","3de7dc2e":"markdown","ada45928":"markdown","d6e07d9e":"markdown","c1cb48e7":"markdown","3c6f58a9":"markdown","ae67347d":"markdown","7d189dff":"markdown","c8346339":"markdown"},"source":{"7ba64acc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0809b85d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics","faeb1829":"df_wine = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","690b76dd":"df_wine.info()","8f9f685d":"df_wine.describe()","3c966e1c":"df_wine[\"quality\"].value_counts()","361e881c":"wine_cat=[]\nfor i in df_wine[\"quality\"]:\n    if i >=7:\n        wine_cat.append(1)\n    else:\n        wine_cat.append(0)\ndf_wine[\"wine_cat\"]=wine_cat","a083212d":"df_wine.wine_cat.value_counts()","7a9f5c14":"df_Predictors=df_wine.drop([\"quality\",\"wine_cat\"],axis=1)\ndf_target=df_wine.wine_cat","76c8f680":"Xtrain,Xtest,ytrain,ytest=train_test_split(df_Predictors,df_target,random_state=10,test_size=0.2)\nprint(\"Shape of Xtrain:{} and Shape of ytrain:{} \".format(Xtrain.shape,ytrain.shape))\nprint(\"Shape of Xtest:{} and Shape of ytest:{} \".format(Xtest.shape,ytest.shape))","50adb8ab":"logit_regression=sm.Logit(ytrain,Xtrain).fit()\nlogit_regression.summary()","f3f696aa":"# we are creating a different Performance metrics for the Logistic Regression giving different threshold","8c15fde9":"score_card = pd.DataFrame(columns=['Probability Cutoff', 'AUC Score', 'Precision Score', 'Recall Score',\n                                       'Accuracy Score', 'Kappa Score', 'f1-score'])\ndef update_score_card(model, cutoff):\n    from sklearn import metrics\n    # let 'y_pred_prob' be the predicted values of y\n    y_pred_prob = logit_regression.predict(Xtest)\n    \n    # convert probabilities to 0 and 1 using 'if_else'\n    y_pred = [ 0 if x < cutoff else 1 for x in y_pred_prob]\n\n# assign 'score_card' as global variable\n    global score_card\n\n# append the results to the dataframe 'score_card'\n# 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Probability Cutoff': cutoff,\n                                    'AUC Score' : metrics.roc_auc_score(ytest, y_pred),\n                                    'Precision Score': metrics.precision_score(ytest, y_pred),\n                                    'Recall Score': metrics.recall_score(ytest, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(ytest, y_pred),\n                                    'Kappa Score':metrics.cohen_kappa_score(ytest, y_pred),\n                                    'f1-score': metrics.f1_score(ytest, y_pred)}, \n                                    ignore_index = True)","14b5d08e":"\nupdate_score_card(logit_regression,0.2)\nupdate_score_card(logit_regression,0.4)\nupdate_score_card(logit_regression,0.6)\n\nupdate_score_card(logit_regression,0.8)","9fe26655":"score_card","c1ec6743":"y_pred_prob=logit_regression.predict(Xtest)\n","96b4ba69":"ypred=[1 if x>0.5 else 0 for x in y_pred_prob]\n","88729b2c":"from sklearn import metrics\nsns.heatmap(metrics.confusion_matrix(ytest,ypred),annot=True,annot_kws={\"size\":25},fmt=\"d\")\nplt.show()","aa3d2720":"Naive_bayes=GaussianNB()\nNaive_bayes_model=Naive_bayes.fit(X=Xtrain,y=ytrain)\nNBpred=Naive_bayes.predict(Xtest)","1d74883e":"from sklearn import metrics\nsns.heatmap(metrics.confusion_matrix(ytest,NBpred),annot=True,annot_kws={\"size\":25},fmt=\"d\")\nplt.show()","4fb39451":"from sklearn.ensemble import RandomForestClassifier","d875421a":"\nrf=RandomForestClassifier(n_estimators=1000)\nmodelrf=rf.fit(Xtrain,ytrain)\nrf_predict=rf.predict(Xtest)\nmetrics.accuracy_score(ytest,rf_predict)\nsns.heatmap(metrics.confusion_matrix(ytest,rf_predict),annot=True,annot_kws={\"size\":25},linewidths=0.2,fmt='d',cmap=\"viridis\")\nplt.show()","feb44092":"from sklearn.neighbors import KNeighborsClassifier\nKNN=KNeighborsClassifier(n_neighbors=5)\nKNN=KNN.fit(Xtrain,ytrain)\nKnn_predict=KNN.predict(Xtest)\n","bfe4b1dd":"metrics.accuracy_score(ytest,Knn_predict)","b360ca83":"def accuracy(ytest,pred):\n    return metrics.accuracy_score(ytest,pred)*100\n\n    \nprint(\"The Accuracy for the Logistic regression is {},\\nThe Accuracy for the 1000 Random forest is {} and \\nThe Accuracy for the Bayesian classifier is {}\\n The Accuracy for the K-Nearest Neighbors is {}\".format(accuracy(ytest,ypred),accuracy(ytest,rf_predict),accuracy(ytest,NBpred),accuracy(ytest,Knn_predict)))","9b5fafd6":"* Lets Create a feature \n\n    `0 for the Wine quality<7` \n\n    `1 for the Wine quality>=7`\n","c4e59365":"#### The Following is the confusion Matrix for Logistic Regression","c581259c":"* What might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good\/1' and the remainder as 'not good\/0'.\nThis allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value.\nWithout doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)`","9572d48a":"* We are taking the Random Forest Classification with Hyperparameter n_estimator=1000 that is we want 1000 trees ","e18e189f":"`Lets Check the min max and median of the data `","99fe8945":"* **The Data shows that the data has is having appropriate Datatypes and has no null values**","f34a9714":"* **The Features with pvalues less than 0.05 are to considered significant hence the Features below are significant**\n\n       ","b752131f":"* **Assinging the Df_predictor and Df_Target**   ","e1f2f84f":"* Check the Value counts for the Quality in the dataframe","4eed0ecf":"* We Have Created the following Models without dropping the Features we will generate the Model using K fold Cross Validation after the Model Is built","df64d2d8":"* Getting the confusion Matrix for the Bayesian Classification with the Help of Heatmap","d460c07b":"* **As we know that the Logistic Regression gives us the Probability we need to convert the Probabilities to `0 and 1`**","89a9592e":"#### Lets create a full model on the Data using Classification Algorithm and checking the Accuracy metrics","55eea0a4":"* After  looking at the Data we can see that the mean and median(50%) of the density is normally distributed","78e43646":"### Random Forest Classification","b9be8e61":"### Naive Bayes Classification\n* Lets Use the Naive Bayes Classification and  check the  Accuracy","8a36d272":"* **We are converting the Problem in the classification problem**\n\n    `Creating the Feature Wine_Cat by distinguishing into the quality`","3b8b8444":"### K-Nearest Neighbors\n*  We are using the n_neighbors=5 which tells us the value of top 5 nearest Neighbors to be considered","12cdd443":"**We got Pseudo Rsquared as 0.2871**\n\n**Which seems to be good Fit**","3de7dc2e":"### Logistic Regression ","ada45928":"* **` Check the value_counts for 0 and 1`** ","d6e07d9e":"* We are converting the following problem into the Classification Problem \n    \n    `Wine Quality>7 is Good `\n    \n    `Wine Quality<7 is not Good`","c1cb48e7":"* **Creating the train-test Split on the data and check for the shape of the Train test split**","3c6f58a9":"* **`Importing the WINE DATA and Eyeballing the Data`**","ae67347d":"**`volatile acidity`\n`chlorides`\n`total sulfur dioxide`\n`density`\n`sulphates`\n`alcohol`** ","7d189dff":"**This is the  List Comprehension for Converting the ypredicted_probalities to 0 and 1**","c8346339":"* We Import Naives Bayes from Gaussian NB"}}