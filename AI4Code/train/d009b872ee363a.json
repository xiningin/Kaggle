{"cell_type":{"9b6d695e":"code","66880d44":"code","1d0b7663":"code","abc322ac":"code","1e1cf8da":"code","3b01a6bc":"code","43aabc50":"code","5108cde4":"code","1f1386eb":"code","b31b2154":"code","40164a9e":"code","9e190029":"code","37cae657":"code","21a5c9f9":"code","fdece8ad":"code","e981d432":"code","06e24473":"code","58e9dee5":"code","64125438":"code","8dcbafd0":"code","40b0d208":"code","fcf4fa25":"code","c6caf254":"code","4125b38d":"code","27139262":"code","59a5c87d":"code","9aa7dfb9":"code","e112c6ba":"code","3e7bc76d":"code","415c08e1":"code","7137bcde":"code","49dfbac5":"code","45fa07f8":"code","84c89a08":"code","98426707":"code","2cc19097":"code","e8c24a5a":"code","d682f2b9":"code","a2aa4904":"code","aeb219ee":"code","427d2393":"markdown","125c1cba":"markdown","261c6ab9":"markdown","75bee37a":"markdown","bf831b0d":"markdown","895d40e0":"markdown","e2637723":"markdown"},"source":{"9b6d695e":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nimport tensorflow as tf\nimport pylab as pl\nfrom PIL import Image\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","66880d44":"data_dir = '..\/input\/the-simpsons-characters-dataset\/simpsons_dataset'\ntest_dir = '..\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/kaggle_simpson_testset'","1d0b7663":"Name=[]\nfor file in os.listdir(data_dir):\n    Name+=[file]\n    \nprint(Name)\nprint(len(Name))","abc322ac":"N=[]\nfor i in range(len(Name)):\n    N+=[i]\n    \nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) \n\ndef mapper(value):\n    return reverse_mapping[value]","1e1cf8da":"anno = pd.read_csv('..\/input\/the-simpsons-characters-dataset\/annotation.txt',header=None) \nanno.columns=['filepath','x1','y1','x2','y2','character']\nanno[0:5]","3b01a6bc":"dataset=[]\ndatapath=[]\ncount=0\nfor name in Name:\n    path=os.path.join(data_dir,name)\n    for im in os.listdir(path):\n        if im[-4:]=='.jpg':\n            image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(60,60))\n            image=img_to_array(image)\n            image=image\/255.0\n            dataset.append([image,count])\n            datapath.append(os.path.join(path,im))     \n    count=count+1","43aabc50":"testset=[]\ntestpath=[]\nfor im in os.listdir(test_dir):\n    if im[-4:]=='.jpg':\n        image=load_img(os.path.join(test_dir,im), grayscale=False, color_mode='rgb', target_size=(60,60))\n        image=img_to_array(image)\n        image=image\/255.0\n        testset.append([image,im[0:-4]])\n        testpath.append(os.path.join(test_dir,im))","5108cde4":"data,labels0=zip(*dataset)\ntest,tlabels0=zip(*testset)","1f1386eb":"filepath2=[]\nfor item in anno['filepath']:\n    filepath2+=['..\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/'+item[13:]]\nanno['filepath2']=filepath2","b31b2154":"print(anno['character'].unique())\nprint(anno['character'].nunique())\nprint(anno['character'].value_counts())","40164a9e":"num2=14\nannopath=anno.iloc[num2,6]\n_ = plt.figure(figsize = (8,8))\n_ = plt.axis('off')\n_ = plt.imshow(mpimg.imread(annopath))","9e190029":"### num0 as index number of anno DataFrame\n\ndef draw_data_bbox2(num0):\n    annopath=anno.iloc[num0,6]\n    df=anno[anno['filepath2']==annopath]\n    x1=df.iloc[0,1]\n    y1=df.iloc[0,2]\n    x2=df.iloc[0,3]\n    y2=df.iloc[0,4]\n    name=df.iloc[0,5]\n    im = Image.open(annopath)\n    W,H = im.size\n    _ = plt.figure(figsize = (8,8))\n    _ = plt.axis('on')\n    _ = plt.imshow(mpimg.imread(annopath))   \n    ax = plt.gca()\n    ax.text(W*0.008,H*0.04,f'{name}',fontsize=16,color='yellow')\n    x=x1\n    y=y1\n    w=(x2-x1)\n    h=(y2-y1)\n    rect = patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='yellow',fill = False)\n    ax.add_patch(rect)\n    plt.show","37cae657":"draw_data_bbox2(14)","21a5c9f9":"objects=[]\nobjlabel=[]\n\ndef extract_data_bbox2(num0, show=False):\n    annopath=anno.iloc[num0,6]\n    df=anno[anno['filepath2']==annopath]\n    x1=df.iloc[0,1]\n    y1=df.iloc[0,2]\n    x2=df.iloc[0,3]\n    y2=df.iloc[0,4]\n    name=df.iloc[0,5]\n    im = Image.open(annopath)\n    W,H = im.size\n    \n    if x2>x1 and y2>y1:\n        pass\n    else:\n        return None\n\n    x=x1\n    y=y1\n    w=(x2-x1)\n    h=(y2-y1)\n    img = np.array(mpimg.imread(annopath))\n    obj = img[int(y):int(y+h),int(x):int(x+w)]\n    obj1 = Image.fromarray(np.uint8(obj))\n    obj2 = np.asarray(obj1.resize((60,60))) \n    objects.append(obj2\/255.0)\n    objlabel.append(normal_mapping[name])\n    ty=h\/H\n    tx=w\/W\n\n    if show:\n        _ = plt.figure(figsize = (8*tx,8*ty))\n        _ = plt.xticks([])\n        _ = plt.yticks([])\n        _ = plt.imshow(obj)\n    ","fdece8ad":"objects=[]\nextract_data_bbox2(14, show=True)","e981d432":"objects=[]\nobjlabel=[]\nfor i in range(len(anno)):\n    extract_data_bbox2(i)","06e24473":"objects_ay=np.array(objects)\nobjlabel_ay=np.array(objlabel)\nprint(objects_ay.shape)\nprint(objlabel_ay.shape)","58e9dee5":"data_ay=np.array(data)\nlabels0_ay=np.array(labels0)\nprint(data_ay.shape)\nprint(labels0_ay.shape)","64125438":"labels1=np.concatenate([labels0_ay,objlabel_ay])\nprint(labels1.shape)","8dcbafd0":"data=np.concatenate([data_ay,objects_ay])\nprint(data.shape)","40b0d208":"labels1=to_categorical(labels1)\nlabels=np.array(labels1)","fcf4fa25":"test=np.array(test)\ntlabels=np.array(tlabels0)","c6caf254":"print(len(data))\nprint(len(test))","4125b38d":"trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=44)","27139262":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","59a5c87d":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                    width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","9aa7dfb9":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(60,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","e112c6ba":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(43, activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","3e7bc76d":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","415c08e1":"his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=20)","7137bcde":"y_pred=model.predict(testx)\npred=np.argmax(y_pred,axis=1)\nground = np.argmax(testy,axis=1)\nprint(classification_report(ground,pred))","49dfbac5":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","45fa07f8":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","84c89a08":"load_img(\"..\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/kaggle_simpson_testset\/abraham_grampa_simpson_34.jpg\",target_size=(60,60))","98426707":"image=load_img(\"..\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/kaggle_simpson_testset\/abraham_grampa_simpson_34.jpg\",target_size=(60,60))\n\nimage=img_to_array(image) \nimage=image\/255.0\nprediction_image=np.array(image)\nprediction_image= np.expand_dims(image, axis=0)","2cc19097":"prediction=model.predict(prediction_image)\nvalue=np.argmax(prediction)\nmove_name=mapper(value)\nprint(\"Prediction is {}.\".format(move_name))","e8c24a5a":"print(test.shape)\nprediction2=model.predict(test)\nprint(prediction2.shape)\n\nPRED0=[]\nfor item in prediction2:\n    value2=np.argmax(item)\n    name=mapper(value2)\n    PRED0+=[name]","d682f2b9":"ANS=tlabels","a2aa4904":"PRED=[]\nfor pred0,ans in zip(PRED0,ANS):\n    if pred0 in ans:\n        PRED+=[ans]\n    else:\n        PRED+=[pred0]\n        \nprint(ANS[0:5])\nprint(PRED0[0:5])\nprint(PRED[0:5])","aeb219ee":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","427d2393":"# Original image data setting","125c1cba":"# Concatenate original data and extracted data","261c6ab9":"# Function to show image and YOLO rectangle","75bee37a":"# Function to extract YOLO rectangle","bf831b0d":"# Model for learning","895d40e0":"# Predict","e2637723":"# Use annotation data to make extracted images"}}