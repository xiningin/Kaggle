{"cell_type":{"bb6447f0":"code","a1217e95":"code","19ff2a2b":"code","d25d33d7":"code","1490978d":"code","47fe5c62":"code","4784aab7":"code","bf25501d":"code","10b7987a":"code","751589fd":"code","04306d04":"code","b842df70":"code","f86b7427":"code","a16ad9d0":"code","05a1aa71":"markdown","6d3b2d49":"markdown","0a12c26f":"markdown","1591bbdc":"markdown","fa1b1edf":"markdown","3eb235d9":"markdown","94390962":"markdown","61c40c49":"markdown","10dd987e":"markdown"},"source":{"bb6447f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a1217e95":"import struct\nimport numpy as np\nfrom numpy.random import randn, seed\nimport matplotlib.pyplot as plt","19ff2a2b":"def read_idx(filename):\n    with open(filename, \"rb\") as f:\n        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n    \n#dont worry i wont swap the test and train    \nx_train_ = read_idx(\"\/kaggle\/input\/fashionmnist\/train-images-idx3-ubyte\")\ny_train_ = read_idx(\"\/kaggle\/input\/fashionmnist\/train-labels-idx1-ubyte\")\nx_test_  = read_idx(\"\/kaggle\/input\/fashionmnist\/t10k-images-idx3-ubyte\")\ny_test_  = read_idx(\"\/kaggle\/input\/fashionmnist\/t10k-labels-idx1-ubyte\")\n","d25d33d7":"print(x_train_.shape)  #data gambar untuk training\nprint(y_train_.shape)  #data label untuk training sebelum one-hot encoding","1490978d":"x_train = []\nfor x in x_train_:\n    x_train.append(x.flatten())\n\n\nx_test = []\nfor x in x_test_:\n    x_test.append(x.flatten())\n\nprint(\"x_train shape:\",np.array(x_train).shape)\nprint(\"x_test shape:\",np.array(x_test).shape)","47fe5c62":"y_train = np.eye(y_train_.max()+1)[y_train_]\ny_test = np.eye(y_test_.max()+1)[y_test_]","4784aab7":"print(\"y_train shape:\",np.array(y_train).shape)\nprint(\"y_test shape:\",np.array(y_test).shape)","bf25501d":"# Dictionary of labels\nlabel = {0: 'T-shirt\/top',\n         1: 'Trouser',\n         2: 'Pullover',\n         3: 'Dress',\n         4: 'Coat',\n         5: 'Sandal',\n         6: 'Shirt',\n         7: 'Sneaker',\n         8: 'Bag',\n         9: 'Ankle boot'}","10b7987a":"def softmax(s):\n    exps = np.exp(s - np.max(s, axis = 1, keepdims = True))\n    return exps\/np.sum(exps, axis = 1, keepdims = True)\n\ndef sigmoid(s):\n    return 1\/(1+np.exp(-s))\n\ndef tanh(s):\n    return (np.exp(s)-np.exp(-s))\/(np.exp(s)+np.exp(-s))\n\ndef linear(s):\n    return s\n\ndef quadratic(s):\n    return s**2\n\ndef cubic(s):\n    return s**3\n\ndef newton_2nd(a):\n    m = 1000\n    return m*a\n\ndef kepler_3rd_law(s):\n    g_constant = 6.67e-11\n    solar_mass = 2e30\n    earth_solar_distance = 150e8\n    p = np.sqrt(((4*(np.pi**2))\/(g_constant*(solar_mass+s)))*earth_solar_distance**3)\n    return p\/(365*24*3600)\n\ndef sinh(s):\n    return (np.exp(s)-np.exp(-s))\/2\n\ndef cross_entropy(pred, real):\n    n_samples = real.shape[0]\n    res = pred - real\n    return res\/n_samples\n\n\n\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.maxx  = np.max(x)\n        self.meanx = np.mean(x)\n        self.stdx  = np.std(x)\n        \n        self.X  = x \n        self.y = y\n        \n        seed(1)\n        H  = 784\n        Ni = self.X.shape[1]\n        No = self.y.shape[1]\n    \n        self.w0 = randn(Ni,H)\n        self.b0 = randn(1,H)\n        self.w1 = randn(H,H)\n        self.b1 = randn(1,H)\n        self.w2 = randn(H,No)\n        self.b2 = randn(1,No)\n        \n    def forward(self):\n        self.A0 = self.X\n        Z1      = self.A0@self.w0 + self.b0   \n        self.A1 = kepler_3rd_law(Z1)\n        Z2      = self.A1@self.w1 + self.b1 \n        self.A2 = kepler_3rd_law(Z2)\n        Z3      = self.A2@self.w2 + self.b2   \n        self.A3 = sinh(Z3)  \n        \n    def backward(self):\n        alpha    =  1\n        e        = self.y - self.A3\n        delta3   = -e\/e.shape[0]  # delta_w2\n        delta2   = delta3@self.w2.T*self.A2*(1-self.A2) # delta_w1\n        delta1   = delta2@self.w1.T*self.A1*(1-self.A1) # delta_w0\n        self.w2 -= alpha*self.A2.T@delta3\n        self.b2 -= alpha*sum(delta3)\n        self.w1 -= alpha*self.A1.T@delta2\n        self.b1 -= alpha*sum(delta2)\n        self.w0 -= alpha*self.A0.T@delta1\n        self.b0 -= alpha*sum(delta1)\n        \n\n    def predict(self,xs):\n        self.X   = xs\n        \n        self.forward()\n        predict = np.argmax(self.A3, axis = 1)\n        return predict","751589fd":"sigmoid(3)\n#wth why this one was here","04306d04":"from datetime import datetime\n\nthis = datetime.now()\n\nX   = np.array(x_train)\ny   = np.array(y_train)\nann = NeuralNetwork(X, y)\n\nepochs = 5\n\nfor x in range(epochs):\n    print(\"Epoches: \", x, '\/',epochs)\n    ann.forward()\n    ann.backward()\n    \nthen = datetime.now()\nprint('Time elapsed: ', then - this)","b842df70":"pred_train = ann.predict(X)\nmatch_train    = sum((pred_train == np.argmax(y, axis = 1))*1)\ntrain_acc  = match_train\/len(y)*100\n\nprint('Train Accuracy:', train_acc, '%')","f86b7427":"pred_test = ann.predict(x_test)\nmatch_test    = sum((pred_test == np.argmax(y_test, axis = 1))*1)\ntest_acc  = match_test\/len(y_test)*100\n\nprint('Test Accuracy:', test_acc, '%')","a16ad9d0":"plt.matshow(x_train_[100])\nplt.show()\n\nprint('What your NN think: ',label[pred_train[100]])\nprint('The reality: ', label[np.argmax(y[100])])\n","05a1aa71":"### Flatten, turn 2D into 1D array","6d3b2d49":"#### We will start modeling the deep learning using simple ANN. \n\n#### Showed skipped memory if we use the wrong approach, simulating a problem in the robot's brain. Then, \"stupid\" model will be the outcome.\n\n#### Here is my modified Deep Learning course (Math 4072) exercise notebook, but instead of competing for best score, I am heading for worst score.","0a12c26f":"## Loading the necessary library\n\nYea, I think the worst is for not doing anything, but I wont replicate my real world stupidity. ","1591bbdc":"### One hot encoding","fa1b1edf":"# Here is your meme material!","3eb235d9":"## The most interesting Part\n\nThere is some function, you know $softmax$?\n\n![Source: wikipedia](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/bdc1f8eaa8064d15893f1ba6426f20ff8e7149c5)\nSource: wikipedia\n\n\n\nWe will use $sinh$ for hidden layer, and $kepler's \\  third \\ law$ function for output layer. Lets see\n","94390962":"# Here is a kind of AI that facing some brain problem!","61c40c49":"Its okay, low enough for now. But how could you achieve it with kepler's third law?\n\nOkay. the result is so big and it aims for the 0th label.","10dd987e":"# Why we are racing for smartest AI? \n\n#### As we know, we live in community, with every individuals facing a weakness. To specify, there is some kind of disorder named brain problem, or maybe in another term, some diseases affect the brain. \n\n#### In the middle school, I am the one of the most stupid students in class, ranked 34th out of 36. Got exam score of 10\/100, which means I am far too inferior from the simple machine learning, even my in-class exercise who had 50% accuracy is far smarter than me. In the world full of intelligent system, it would not be colorful if all of them are smart.[](http:\/\/)"}}