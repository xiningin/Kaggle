{"cell_type":{"9b9a9f44":"code","1b2286c3":"code","ffdba466":"code","54853c66":"code","77f4c021":"code","320d6484":"code","df2c638f":"code","10c0b043":"code","15c94ddf":"code","2c307846":"code","c65af683":"code","f61b5dd2":"code","c4d2fb11":"code","9f3b8ddf":"code","b8074c8b":"code","6062fcb3":"code","8b68c943":"code","ef909eee":"code","da7ea7b8":"code","db7086eb":"code","995427ad":"code","e7c06175":"code","d40030fd":"code","1defa011":"code","aaff99d5":"code","769be229":"code","dd4bca9b":"code","82541fa6":"code","6002b826":"code","632b201b":"code","206429c2":"code","313773be":"code","089b65cc":"code","88d90c4f":"code","15c864f7":"code","e5c3bec1":"code","128eba73":"code","5d2414ef":"code","075b51e3":"code","e25c7a48":"code","f481c9a0":"code","1d7502e5":"code","980a3621":"code","f2e9faf1":"code","5627bae2":"code","d98dbfaa":"code","e900549d":"code","497c6991":"code","28a18941":"code","a79a9bd2":"code","f78fab2b":"code","efc1be21":"code","1000299e":"code","9902eba9":"code","a0882a3c":"code","4f0d92a4":"code","40badfd4":"code","a9913ad0":"code","421ecd0d":"code","05720208":"code","f59c8076":"markdown","f5baa809":"markdown","760b63d1":"markdown","118a5d20":"markdown","70169c65":"markdown","b07fab34":"markdown","953f1392":"markdown","17262aa2":"markdown","8ff91b61":"markdown","e1125ab4":"markdown","95644fb2":"markdown","54f0e418":"markdown"},"source":{"9b9a9f44":"!pip install lofo-importance","1b2286c3":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\nimport plotly.express as px\n\nimport os\nimport random\nimport re\nimport math\nimport time\nimport gc\ngc.collect()\n\nimport warnings\n\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb\n\nwarnings.filterwarnings('ignore') # Disabling warnings for clearer outputs","ffdba466":"# Custom aggregation functions\n\ndef average_difference(series):\n    values = sorted(series.values)\n    if len(values) < 2:\n        return np.nan\n    return np.mean(np.diff(values))\n\ndef difference_to_latest(series):\n    values = sorted(series.values)\n    if len(values) < 2:\n        return np.nan\n    return np.diff(values)[0]\n\ndef nth_max(series, n):\n    values = sorted(series.values)\n    if len(values) < n:\n        return np.nan\n    return values[-n]\n\ndef nth_min(series, n):\n    values = sorted(series.values)\n    if len(values) < n:\n        return np.nan\n    return values[n-1]","54853c66":"# https:\/\/maxhalford.github.io\/blog\/lightgbm-focal-loss\/\n\nfrom scipy import optimize\nfrom scipy import special\n\nclass FocalLoss:\n\n    def __init__(self, gamma, alpha=None):\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def at(self, y):\n        if self.alpha is None:\n            return np.ones_like(y)\n        return np.where(y, self.alpha, 1 - self.alpha)\n\n    def pt(self, y, p):\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.where(y, p, 1 - p)\n\n    def __call__(self, y_true, y_pred):\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        return -at * (1 - pt) ** self.gamma * np.log(pt)\n\n    def grad(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n\n    def hess(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n\n        u = at * y * (1 - pt) ** g\n        du = -at * y * g * (1 - pt) ** (g - 1)\n        v = g * pt * np.log(pt) + pt - 1\n        dv = g * np.log(pt) + g + 1\n\n        return (du * v + u * dv) * y * (pt * (1 - pt))\n\n    def init_score(self, y_true):\n        res = optimize.minimize_scalar(\n            lambda p: self(y_true, p).sum(),\n            bounds=(0, 1),\n            method='bounded'\n        )\n        p = res.x\n        log_odds = np.log(p \/ (1 - p))\n        return log_odds\n\n    def lgb_obj(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        return self.grad(y, p), self.hess(y, p)\n\n    def lgb_eval(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        is_higher_better = False\n        return 'focal_loss', self(y, p).mean(), is_higher_better","77f4c021":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (\n        df.isnull().sum().sort_values(ascending=False) \/ len(df) * 100)[\n        (df.isnull().sum().sort_values(ascending=False) \/ len(df) *100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","320d6484":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df2c638f":"# test df\ntest_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/sample_submission.csv\", low_memory=False)\nprint(test_df.head())\nprint(len(test_df))\n\ntest_ids = set(test_df[\"Id\"].values)","10c0b043":"# servis bakim\nservis_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/MASK_SERVIS_BAKIM_DATATHON_FINAL.csv\", low_memory=False)\nservis_df = servis_df.drop(servis_df.columns[0], axis=1)\nservis_df[\"CREATE_DATE\"] = pd.to_datetime(servis_df[\"CREATE_DATE\"])\nprint(servis_df.head())\nprint(f\"Servis_df: {len(servis_df)}\")\nprint(servis_df.dtypes)","15c94ddf":"missing_percentage(servis_df)","2c307846":"# sifir arac alanlar\nsifir_arac_alanlar_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/FINAL_SIFIR_ARAC_ALANLAR_DATATHON.csv\", low_memory=False)\nsifir_arac_alanlar_df = sifir_arac_alanlar_df.drop(sifir_arac_alanlar_df.columns[0], axis=1)\nsifir_arac_alanlar_df[\"CREATE_DATE\"] = pd.to_datetime(sifir_arac_alanlar_df[\"CREATE_DATE\"])\nprint(sifir_arac_alanlar_df.head())\nprint(f\"Sifir arac alanlar veri miktari: {len(sifir_arac_alanlar_df)}\")\nprint(sifir_arac_alanlar_df.dtypes)","c65af683":"sifir_arac_alanlar_df","f61b5dd2":"# satis \nsatis_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/FINAL_SALES_FILE_DATATHON.csv\", low_memory=False)\nsatis_df = satis_df.drop(satis_df.columns[0], axis=1)\nsatis_df[\"SF_CREATE_DATE\"] = pd.to_datetime(satis_df[\"SF_CREATE_DATE\"])\nsatis_df[\"SALES_RECENCY\"] = (pd.to_datetime(\"2021\/06\/01\") - satis_df['SF_CREATE_DATE']).dt.total_seconds() \/ (60*60*24*12)\nsatis_df = satis_df.sort_values(\"SF_CREATE_DATE\").drop(\"SALESFILE_ID\", axis=1).reset_index(drop=True)\nprint(satis_df.head())\nprint(f\"Sifir arac alanlar veri miktari: {len(satis_df)}\")\nprint(satis_df.dtypes)","c4d2fb11":"missing_percentage(satis_df)","9f3b8ddf":"# satis \nvehicle_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/FINAL_VEHICLE_TABLE_DATATHON.csv\", low_memory=False)\nvehicle_df = vehicle_df.drop(vehicle_df.columns[0], axis=1)\nvehicle_df[\"TRAFFIC_DATE\"] = pd.to_datetime(vehicle_df[\"TRAFFIC_DATE\"])\n#vehicle_df = vehicle_df.sort_values(\"TRAFFIC_DATE\")\nprint(vehicle_df.head())\nprint(f\"Arac veri miktari: {len(vehicle_df)}\")\nprint(vehicle_df.dtypes)","b8074c8b":"missing_percentage(vehicle_df)","6062fcb3":"vehicle_df[\"MOTOR_GAS_TYPE\"].unique()","8b68c943":"def check_is_hybrid(row):\n    if not row or pd.isna(row):\n        return False\n    return \"Hibrit\" in row or \"Hybrid\" in row\nvehicle_df[\"IS_HYBRID\"] = vehicle_df[\"MOTOR_GAS_TYPE\"].apply(check_is_hybrid)","ef909eee":"def correcting_gas_type(row):\n    if not row or pd.isna(row):\n        return None\n    row = row.lower()\n    if \"benzin\" in row:\n        return \"benzin\"\n    elif \"dizel\" in row:\n        return \"dizel\"\n    elif row == \"diesel\":\n        return \"dizel\"\n    elif row == \"se\u00e7iniz\":\n        return None\n    else:\n        assert False\nvehicle_df[\"MOTOR_GAS_TYPE\"] =vehicle_df[\"MOTOR_GAS_TYPE\"].apply(correcting_gas_type)","da7ea7b8":"def correcting_gear_box_type(row):\n    if not row or pd.isna(row):\n        return None\n    if row in [\n        \"Otomatik (DSG)\", \"Otomatik\", \"Mekanik\", \"S\u00fcrekli De\u011fi\u015fken\", \"Otomaik\", \"Otomatik De\u011fi\u015fken\", \"?automatisch\", \n        \"S\u00fcrekli de\u011fi\u015fken\", \"Otomatik (DSG \u015eanzuman)\", \"Otomatik(DSG)\"]:\n        return \"otomatik\"\n    elif row in [\"?Handschaltung\", \"Manuel\", \"manuel\", \"D\u00fcz\"]:\n        return \"manuel\"\n    else:\n        print(row)\n        assert False\nvehicle_df[\"GEAR_BOX_TYPE\"] =vehicle_df[\"GEAR_BOX_TYPE\"].apply(correcting_gear_box_type)","db7086eb":"vehicle_df","995427ad":"# satis \ncustomer_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/FINAL_CUSTOMER_DATATHON.csv\", low_memory=False)\ncustomer_df = customer_df.drop(customer_df.columns[0], axis=1)\ncustomer_df = customer_df.drop([\"GENDER_ID\", \"MARITAL_STATUS_ID\"], axis=1)\ncustomer_df = customer_df.sort_values(\"BASE_CUSTOMER_ID\")\ncustomer_df = customer_df.reset_index(drop=True)\ncustomer_df[\"BIRTH_DATE\"] = 2021 - customer_df[\"BIRTH_DATE\"]\n#customer_df[\"TRAFFIC_DATE\"] = pd.to_datetime(customer_df[\"TRAFFIC_DATE\"])\nprint(customer_df.head())\nprint(f\"Customer veri miktari: {len(customer_df)}\")\nprint(customer_df.dtypes)","e7c06175":"print(customer_df[\"OCCUPATION\"].value_counts())","d40030fd":"print(missing_percentage(customer_df))","1defa011":"# customer vehicle\ncustomer_vehicle_df = pd.read_csv(\"\/kaggle\/input\/dogus-datathon-otomotiv\/FINAL_CUSTOMER_RELATED_TABLE_FOR_DATATHON.csv\", low_memory=False)\ncustomer_vehicle_df = customer_vehicle_df.drop(customer_vehicle_df.columns[0], axis=1)\ncustomer_vehicle_df = customer_vehicle_df.drop([\"FK_RELATION_STATUS_ID\"], axis=1)\ncustomer_vehicle_df[\"START_DATE\"] = pd.to_datetime(customer_vehicle_df[\"START_DATE\"])\ncustomer_vehicle_df[\"END_DATE\"] = pd.to_datetime(customer_vehicle_df[\"END_DATE\"])\n#customer_df = vehicle_df.sort_values(\"customer_df\")\nprint(customer_vehicle_df.head())\nprint(f\"Customer vehicle veri: {len(customer_vehicle_df)}\")\nprint(customer_vehicle_df.dtypes)","aaff99d5":"missing_percentage(customer_vehicle_df)","769be229":"# base customer id mappingleri\ncustomer_id_2_base_customer_id = dict(zip(customer_df[\"CUSTOMER_ID\"], customer_df[\"BASE_CUSTOMER_ID\"]))\nvehicle_id_2_base_customer_id = dict(zip(customer_vehicle_df[\"VEHICLE_ID\"], customer_vehicle_df[\"BASE_CUSTOMER_ID\"]))","dd4bca9b":"def process_sifir_arac_alanlar(sifir_arac_alanlar_df, pivot_date):\n    sifir_arac_alanlar_df[\"BASE_CUSTOMER_ID\"] = sifir_arac_alanlar_df[\"CUSTOMER_ID\"].map(customer_id_2_base_customer_id)\n    sifir_arac_alanlar_df[\"NEW_CAR_RECENCY\"] = (pd.to_datetime(pivot_date) - sifir_arac_alanlar_df['CREATE_DATE']).dt.total_seconds() \/ (60*60*24*12)\n    sifir_arac_df = sifir_arac_alanlar_df.drop([\"VEHICLE_ID\", \"CUSTOMER_ID\", \"CREATE_DATE\"], axis=1)\n    mask = sifir_arac_df[\"NEW_CAR_RECENCY\"] > 0\n    sifir_arac_df = sifir_arac_df[mask]\n    sifir_arac_df = sifir_arac_df.reset_index(drop=True)\n    sifir_arac_df = sifir_arac_df.groupby(\"BASE_CUSTOMER_ID\").agg(\n        total_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=\"count\"),\n        average_new_car_diff = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=average_difference),\n        average_new_car_recency = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=\"mean\"),\n        difference_to_latest_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=difference_to_latest),\n        second_latest_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 2)),\n        third_latest_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 3)),\n        fourth_latest_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 4)),\n        latest_new_car = pd.NamedAgg(column=\"NEW_CAR_RECENCY\", aggfunc=\"min\"))\n    return sifir_arac_df.reset_index()","82541fa6":"def process_service_df(servis_df, pivot_date):\n    servis_df[\"MAINTENANCE_SERVICE_RECENCY\"] = (pd.to_datetime(pivot_date) - servis_df['CREATE_DATE']).dt.total_seconds() \/ (60*60*24*12)\n    servis_df[\"MAINTENANCE\"] = servis_df[\"IS_MAINTENANCE\"] == 1\n    servis_df[\"SERVICE\"] = servis_df[\"IS_MAINTENANCE\"] == 0\n    mask = servis_df[\"MAINTENANCE_SERVICE_RECENCY\"] > 0\n    servis_df = servis_df[mask]\n    car_service_df = servis_df.groupby(\"VEHICLE_ID\").agg(\n        total_maintenance_and_service = pd.NamedAgg(column=\"IS_MAINTENANCE\", aggfunc=\"count\"),\n        total_spent = pd.NamedAgg(column=\"TOTAL_AMOUNT_TL\", aggfunc=\"sum\"),\n        latest_main_ser = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=\"min\"),\n        total_service = pd.NamedAgg(column=\"SERVICE\", aggfunc=\"sum\"),\n        total_maintenance = pd.NamedAgg(column=\"MAINTENANCE\", aggfunc=\"sum\"),\n        average_main_ser_diff = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=average_difference),\n        average_main_ser_recency = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=\"mean\"),\n        difference_to_latest_main_ser = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=difference_to_latest),\n        second_latest_main_ser = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=lambda x: nth_min(x, 2)),\n        third_latest_main_ser = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=lambda x: nth_min(x, 3)),\n        fourth_latest_main_ser = pd.NamedAgg(column=\"MAINTENANCE_SERVICE_RECENCY\", aggfunc=lambda x: nth_min(x, 4)))\n    return car_service_df.reset_index()","6002b826":"def process_customer_vehicle_df(customer_vehicle_df, pivot_date):\n    mask = customer_vehicle_df[\"START_DATE\"] > pd.to_datetime(PIVOT_DATE)\n    masked_customer_vehic_df = customer_vehicle_df[~mask]\n    masked_customer_vehic_df[\"HAS_CAR\"] = masked_customer_vehic_df[\"END_DATE\"].isnull()\n    masked_customer_vehic_df[\"SOLD_CAR\"] = masked_customer_vehic_df[\"END_DATE\"].notnull()\n    masked_customer_vehic_df[\"HAS_CAR_RECENCY\"] = (\n        pd.to_datetime(pivot_date) - masked_customer_vehic_df['START_DATE']).dt.total_seconds() \/ (60*60*24*12)\n    masked_customer_vehic_df[\"SOLD_CAR_RECENCY\"] = (\n        pd.to_datetime(pivot_date) - masked_customer_vehic_df['END_DATE']).dt.total_seconds() \/ (60*60*24*12)\n    masked_customer_vehic_df[\"HAD_CAR_IN_MONTHS\"] = \\\n        (masked_customer_vehic_df['END_DATE'].fillna(\n            pd.to_datetime(PIVOT_DATE)) - masked_customer_vehic_df['START_DATE']).dt.total_seconds() \/ (60*60*24*12)\n    car_df = masked_customer_vehic_df.groupby(\"BASE_CUSTOMER_ID\").agg(\n        total_cars_owned = pd.NamedAgg(column=\"HAS_CAR\", aggfunc=\"sum\"),\n        total_cars_sold = pd.NamedAgg(column=\"SOLD_CAR\", aggfunc=\"sum\"),\n        has_car_recency = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=\"min\"),\n        sold_car_recency = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=\"min\"),\n        average_has_car_recency_difference = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=average_difference),\n        average_has_car_recency = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=\"mean\"),\n        difference_to_latest_has_car = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=difference_to_latest),\n        second_has_car_recency = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 2)),\n        third_has_car_recency = pd.NamedAgg(column=\"HAS_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 3)),\n        average_sold_car_recency_difference = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=average_difference),\n        average_sold_car_recency = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=\"mean\"),\n        difference_to_latest_sold_car = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=difference_to_latest),\n        second_sold_car_recency = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 2)),\n        third_sold_car_recency = pd.NamedAgg(column=\"SOLD_CAR_RECENCY\", aggfunc=lambda x: nth_min(x, 3)),\n        average_ownership_duration = pd.NamedAgg(column=\"HAD_CAR_IN_MONTHS\", aggfunc=\"mean\"))\n    return car_df\n","632b201b":"def process_vehicle_df(vehicle_df, car_service_df, pivot_date):\n    vehicle_df[\"CAR_AGE\"] = (pd.to_datetime(\"2021\/06\/01\") - vehicle_df['TRAFFIC_DATE']).dt.total_seconds() \/ (60*60*24*365.25)\n    mask = vehicle_df[\"CAR_AGE\"] > 0\n    vehicle_df = vehicle_df[mask]\n    vehicle_df = vehicle_df.drop(\"TRAFFIC_DATE\",axis=1)\n    vehicle_df[\"BASE_CUSTOMER_ID\"] = vehicle_df[\"VEHICLE_ID\"].map(vehicle_id_2_base_customer_id)\n    vehicle_df = pd.merge(vehicle_df, car_service_df, on=\"VEHICLE_ID\")\n    grouped_vehicle_df = vehicle_df.groupby(\"BASE_CUSTOMER_ID\").agg(\n        average_car_age = pd.NamedAgg(column=\"CAR_AGE\", aggfunc=\"mean\"),\n        total_spent = pd.NamedAgg(column=\"total_spent\", aggfunc=\"sum\"),\n        total_cars = pd.NamedAgg(column=\"VEHICLE_ID\", aggfunc=\"count\"),\n        total_unique_brands = pd.NamedAgg(column=\"BRAND_CODE\", aggfunc=\"nunique\"),\n        total_unique_model_codes = pd.NamedAgg(column=\"BASEMODEL_CODE\", aggfunc=\"nunique\"),\n        total_maintenance_and_service = pd.NamedAgg(column=\"total_maintenance_and_service\", aggfunc=\"sum\"),\n        total_service = pd.NamedAgg(column=\"total_service\", aggfunc=\"sum\"),\n        total_maintenance = pd.NamedAgg(column=\"total_service\", aggfunc=\"sum\"),\n        difference_to_latest_main_ser = pd.NamedAgg(column=\"difference_to_latest_main_ser\", aggfunc=\"min\"),\n        average_main_ser_recency = pd.NamedAgg(column=\"average_main_ser_recency\", aggfunc=\"mean\"),\n        latest_main_ser = pd.NamedAgg(column=\"latest_main_ser\", aggfunc=\"min\"))\n    return grouped_vehicle_df.reset_index()","206429c2":"PIVOT_DATE = \"2021\/03\/01\"\nmerged_df = pd.merge(satis_df, customer_df, on=\"CUSTOMER_ID\", how=\"outer\")\nmerged_df[\"MARITAL_STATUS\"] = merged_df[\"MARITAL_STATUS\"].fillna(\"\")\nmerged_df[\"BIRTH_DATE\"] = merged_df[\"BIRTH_DATE\"].fillna(-1)\nmerged_df[\"GENDER\"] = merged_df[\"GENDER\"].fillna(\"\")\nmerged_df[\"REQ_TOPMODEL_CODE\"] = merged_df[\"REQ_TOPMODEL_CODE\"].fillna(-1)\nmerged_df[\"IS_101\"] = merged_df[\"STATUS\"] == 101\nmerged_df[\"IS_102\"] = merged_df[\"STATUS\"] == 102\nmerged_df[\"IS_105\"] = merged_df[\"STATUS\"] == 105\nmerged_df[\"IS_109\"] = merged_df[\"STATUS\"] == 109\nmerged_df[\"IS_100\"] = merged_df[\"STATUS\"] == 100\nmerged_df[\"IS_106\"] = merged_df[\"STATUS\"] == 106","313773be":"customers_with_sales = merged_df[\"SF_CREATE_DATE\"].notnull()\nmerged_df_with_sales = merged_df[customers_with_sales]\nmerged_df_without_sales = merged_df[~customers_with_sales]\nmask_date = (pd.to_datetime(PIVOT_DATE) > merged_df_with_sales[\"SF_CREATE_DATE\"])\nmask_3 = np.random.rand(len(merged_df_without_sales)) < 0.7\ntrain_merged_df = pd.concat([merged_df_with_sales[mask_date], merged_df_without_sales[mask_3]], axis=0)\ntest_merged_df = pd.concat([merged_df_with_sales[~mask_date], merged_df_without_sales[~mask_3]], axis=0)","089b65cc":"# Sehir ve meslek bilgilerinden base customer id bazinda en cok tekrar eden goz onunde bulundurulacaktir.\n# Bir kisi eger 2 defa muhendis, 1 defa emekli yazilmissa: muhendis\n\ndef occupation_and_city(series):\n    count_indexes = list(series.value_counts().index)\n    if not count_indexes:\n        return np.nan\n    return count_indexes[0]\n\ndef group_merged_df(df):\n    grouped_df = df.groupby(\"BASE_CUSTOMER_ID\").agg(\n        total_sales_requests = pd.NamedAgg(column=\"SF_CREATE_DATE\", aggfunc=\"count\"),\n        average_sales_request_diff = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=average_difference),\n        average_sales_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=\"mean\"),\n        difference_to_latest = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=difference_to_latest),\n        sale_request_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=\"min\"),\n        second_sale_request_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=lambda x: nth_min(x, 2)),\n        third_sale_request_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=lambda x: nth_min(x, 3)),\n        fourth_sale_request_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=lambda x: nth_min(x, 4)),\n        fifth_sale_request_recency = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=lambda x: nth_min(x, 5)),\n        gender = pd.NamedAgg(column=\"GENDER\", aggfunc=\"max\"),\n        marital_status = pd.NamedAgg(column=\"MARITAL_STATUS\", aggfunc=\"max\"),\n        n_101 = pd.NamedAgg(column=\"IS_101\", aggfunc=\"sum\"),\n        n_102 = pd.NamedAgg(column=\"IS_102\", aggfunc=\"sum\"),\n        n_105 = pd.NamedAgg(column=\"IS_105\", aggfunc=\"sum\"),\n        n_109 = pd.NamedAgg(column=\"IS_109\", aggfunc=\"sum\"),\n        city = pd.NamedAgg(column=\"FK_ADDRESS_COMMUNICATION_CITY\", aggfunc=occupation_and_city),\n        occupation = pd.NamedAgg(column=\"OCCUPATION\", aggfunc=occupation_and_city),\n        total_requested_brands = pd.NamedAgg(column=\"REQ_BRAND_CODE\", aggfunc=\"nunique\"),\n        total_requested_models = pd.NamedAgg(column=\"REQ_TOPMODEL_CODE\", aggfunc=\"nunique\"),\n        unique_sales_status = pd.NamedAgg(column=\"STATUS\", aggfunc=\"nunique\"),\n        birth_date = pd.NamedAgg(column=\"BIRTH_DATE\", aggfunc=\"max\"))\n    \n    temp_df = df.groupby([\"BASE_CUSTOMER_ID\", \"IS_101\"]).agg(\n    sale_request_recency101 = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=\"min\")).reset_index()\n    temp_df = temp_df[temp_df[\"IS_101\"]==True]\n    temp_df = temp_df.drop(\"IS_101\", axis=1)\n    grouped_df = pd.merge(grouped_df, temp_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\n\n    temp_df = df.groupby([\"BASE_CUSTOMER_ID\", \"IS_105\"]).agg(\n        sale_request_recency105 = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=\"min\")).reset_index()\n    temp_df = temp_df[temp_df[\"IS_105\"]==True]\n    temp_df = temp_df.drop(\"IS_105\", axis=1)\n    grouped_df = pd.merge(grouped_df, temp_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\n\n    temp_df = df.groupby([\"BASE_CUSTOMER_ID\", \"IS_102\"]).agg(\n        sale_request_recency102 = pd.NamedAgg(column=\"SALES_RECENCY\", aggfunc=\"min\")).reset_index()\n    temp_df = temp_df[temp_df[\"IS_102\"]==True]\n    temp_df = temp_df.drop(\"IS_102\", axis=1)\n    grouped_df = pd.merge(grouped_df, temp_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\n    return grouped_df\n\ngrouped_train_df = group_merged_df(train_merged_df)","88d90c4f":"# def latest_sales_status(row):\n#     min_recency_idx = row[\"SALES_RECENCY\"].idxmin()\n#     if pd.isna(min_recency_idx):\n#         return np.nan\n#     status = row.loc[min_recency_idx, \"STATUS\"]\n#     return status\n# temp_series = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(latest_sales_status)\n# grouped_train_df[\"latest_sales_status\"] = temp_series","15c864f7":"# def sales_in_last_period(row, months):\n#     pivot = pd.to_datetime(PIVOT_DATE) - relativedelta(months=months)\n#     recent_sales_count = sum(row[\"SF_CREATE_DATE\"] > pivot)\n#     return recent_sales_count\n# grouped_df[\"sale_reqs_in_last_3_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 3))\n# grouped_df[\"sale_reqs_in_last_6_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 6))\n# grouped_df[\"sale_reqs_in_last_9_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 9))\n# grouped_df[\"sale_reqs_in_last_12_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 12))\n# grouped_df[\"sale_reqs_in_last_15_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 15))\n# grouped_df[\"sale_reqs_in_last_18_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 18))\n# grouped_df[\"sale_reqs_in_last_21_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 21))\n# grouped_df[\"sale_reqs_in_last_24_months\"] = train_merged_df.groupby(\"BASE_CUSTOMER_ID\").apply(lambda x: sales_in_last_period(x, 24))","e5c3bec1":"car_df = process_customer_vehicle_df(customer_vehicle_df, PIVOT_DATE)\nsifir_arac_df = process_sifir_arac_alanlar(sifir_arac_alanlar_df, PIVOT_DATE)\ncar_service_df = process_service_df(servis_df, PIVOT_DATE)\nprocessed_vehicle_df = process_vehicle_df(vehicle_df, car_service_df, PIVOT_DATE)\ngrouped_train_df = pd.merge(grouped_train_df, sifir_arac_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\ngrouped_train_df = pd.merge(grouped_train_df, processed_vehicle_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\ngrouped_train_df = pd.merge(grouped_train_df, car_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")","128eba73":"positive_test_cases = set(test_merged_df[test_merged_df[\"SF_CREATE_DATE\"].notnull()][\"BASE_CUSTOMER_ID\"].unique())","5d2414ef":"# Toplam pozitif de\u011fer\nprint(len(positive_test_cases))\nprint(sum(test_merged_df[\"SF_CREATE_DATE\"].isna()))\nprint(len(test_merged_df))","075b51e3":"grouped_train_df[\"label\"] = grouped_train_df[\"BASE_CUSTOMER_ID\"].apply(lambda x: x in positive_test_cases)\ngrouped_train_df = grouped_train_df.sort_values(\"sale_request_recency\", ascending=False)\ngrouped_train_df = grouped_train_df.reset_index(drop=True)\ngrouped_train_df[\"gender\"] = grouped_train_df[\"gender\"].replace(\"\", np.nan)\ngrouped_train_df[\"marital_status\"] = grouped_train_df[\"marital_status\"].replace(\"\", np.nan)\ngrouped_train_df[\"birth_date\"] = grouped_train_df[\"birth_date\"].replace(-1, np.nan)\ngrouped_train_df[\"total_requested_models\"] = grouped_train_df[\"total_requested_models\"].replace(-1, np.nan)\nprint(len(grouped_train_df))","e25c7a48":"# Ayr\u0131lan verideki sat\u0131\u015f dosyas\u0131 a\u00e7t\u0131ranlar ve a\u00e7t\u0131rmayanlar\u0131n say\u0131s\u0131\ngrouped_train_df[\"label\"].value_counts()","f481c9a0":"# tum featurelar\ngrouped_train_df.columns","1d7502e5":"from lofo import LOFOImportance, Dataset, plot_importance\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# define the validation scheme\ncv = StratifiedKFold(n_splits=3, shuffle=True)\n\n\nchosen_columns = [\n    'total_sales_requests', 'sale_request_recency', 'occupation', 'birth_date', 'latest_new_car', 'total_cars', \n    'latest_main_ser', 'has_car_recency', 'sold_car_recency', 'gender', 'label']\n\ndropped_columns = [\n    \"BASE_CUSTOMER_ID\", 'average_has_car_recency', 'average_has_car_recency_difference', 'average_main_ser_recency', 'average_sold_car_recency',\n     'average_sold_car_recency_difference', 'city', 'fifth_sale_request_recency', 'fourth_latest_new_car', 'fourth_sale_request_recency', 'n_109',\n    'third_has_car_recency', 'third_latest_new_car', 'third_sold_car_recency', 'total_maintenance', 'total_maintenance_and_service', 'total_service']\ndata = grouped_train_df.copy(deep=True)\n\ncategorical_feats = [\"gender\", \"marital_status\", \"occupation\", \"city\"]\nfor c in categorical_feats:\n    data[c] = LabelEncoder().fit_transform(data[c].astype(str))\n    data[c] = data[c].astype('category')\n\ndata[\"label\"] = LabelEncoder().fit_transform(data[\"label\"])\n\n# train_data = data.drop(dropped_columns, axis=1)\ntrain_data = data[chosen_columns]\ndataset = Dataset(df=train_data, target=\"label\", features=[col for col in train_data.columns if col != \"label\"])\n\nlofo_imp = LOFOImportance(dataset, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n\nimportance_df = lofo_imp.get_importance()\n\nplot_importance(importance_df, figsize=(12, 20))","980a3621":"print(importance_df.to_string())","f2e9faf1":"data = grouped_train_df.copy(deep=True)\n\ncategorical_feats = [\"gender\", \"marital_status\", \"occupation\", \"city\"]\nfor c in categorical_feats:\n    data[c] = data[c].astype('category')\n\ndropped_columns = [\n    \"BASE_CUSTOMER_ID\", 'label', 'n_105', 'total_new_car', 'total_cars_owned', 'second_has_car_recency', 'average_sales_request_diff',\n    'total_spent', 'total_cars', 'third_sold_car_recency', 'total_maintenance', 'difference_to_latest_main_ser', \n    'fourth_latest_new_car', 'average_sold_car_recency', 'second_sold_car_recency', 'average_new_car_recency',\n    'difference_to_latest_new_car', 'marital_status', 'total_maintenance_and_service', 'average_sold_car_recency_difference']\n\nchosen_columns = [\n    'total_sales_requests','sale_request_recency', 'occupation', 'birth_date', 'latest_new_car', 'total_cars', \n    'latest_main_ser', 'has_car_recency', 'sold_car_recency', 'gender']\n\nfl = FocalLoss(alpha=None, gamma=0)\n\n# X_train = data.drop(dropped_columns, axis=1)\nX_train = data[chosen_columns]\n\ny_train = data[\"label\"]\n\nnum_round = 1000\nparam = {\n     \"metric\": \"auc\",\n     \"early_stopping_round\": 100, \n     \"learning_rate\": 0.05,\n     \"verbosity\": -1}\ntrain_data_ = lgb.Dataset(X_train, label=y_train)\nbst = lgb.cv(param, train_data_, num_round, stratified=True, fobj=fl.lgb_obj, feval=fl.lgb_eval)","5627bae2":"# AUC\nprint(max(bst[\"auc-mean\"]))","d98dbfaa":"temp_df = grouped_train_df[grouped_train_df[\"BASE_CUSTOMER_ID\"].isin(test_ids)]\n\nmsk = np.random.rand(len(temp_df)) < 0.7\ntrain_data = temp_df[msk]\ntest_data = temp_df[~msk]\n\nprint(train_data[\"label\"].value_counts())\nprint(test_data[\"label\"].value_counts())\n\ncategorical_feats = [\"gender\", \"marital_status\", \"occupation\"]\nfor c in categorical_feats:\n    train_data[c] = train_data[c].astype('category')\n    test_data[c] = test_data[c].astype('category')","e900549d":"dropped_columns = [\n    \"BASE_CUSTOMER_ID\", 'label', 'average_has_car_recency', 'average_main_ser_recency', 'city', 'fifth_sale_request_recency','n_109',\n    'total_maintenance', 'total_unique_brands', 'n_105', 'total_new_car', 'total_cars_owned', 'second_has_car_recency', 'average_sales_request_diff',\n    'total_spent', 'total_cars', 'third_sold_car_recency', 'total_maintenance', 'fourth_latest_new_car', 'average_sold_car_recency', \n    'second_sold_car_recency', 'average_new_car_recency', 'marital_status', 'total_service', 'unique_sales_status',\n    'total_unique_model_codes', 'total_requested_models', 'total_requested_brands', 'birth_date']\n\nchosen_columns = [\n    'total_sales_requests','sale_request_recency', 'second_sale_request_recency', 'occupation', 'birth_date', 'latest_new_car', 'latest_main_ser', \n    'has_car_recency', 'sold_car_recency']\n    \n# X_train = train_data.drop(dropped_columns, axis=1)\nX_train = train_data[chosen_columns]\ny_train = train_data[\"label\"]\n\n# X_test = test_data.drop(dropped_columns, axis=1)\nX_test = test_data[chosen_columns]\ny_test = test_data[\"label\"]","497c6991":"fl = FocalLoss(alpha=None, gamma=0)\n\nparam = {\n     \"metric\": \"auc\",\n     \"early_stopping_round\": 100, \n     \"learning_rate\": 0.05,\n     \"lambda_l1\": 10,\n     \"lambda_l2\": 10,\n     \"min_data_per_group\": 2500,\n     \"cat_l2\": 10,\n     \"cat_smooth\": 10,\n     \"verbosity\": -1}\ntrain_data_ = lgb.Dataset(X_train, label=y_train)\nvalidation_data_ = lgb.Dataset(X_test, label=y_test)\n\n# train_data_ = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_feats)\n# validation_data_ = lgb.Dataset(X_test, label=y_test, categorical_feature=categorical_feats)\n\n#all_data_ = lgb.Dataset()\nnum_round = 1000\nbst = lgb.train(param, train_data_, num_round, valid_sets=[train_data_, validation_data_], fobj=fl.lgb_obj, feval=fl.lgb_eval)\n# bst = lgb.train(param, train_data_, num_round, valid_sets=[train_data_, validation_data_])","28a18941":"feat_importances = pd.Series(bst.feature_importance(), index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","a79a9bd2":"categorical_feats = [\"gender\", \"occupation\"]\nfor c in categorical_feats:\n    grouped_train_df[c] = grouped_train_df[c].astype('category')\n\nchosen_columns = [\n    'total_sales_requests','sale_request_recency', 'second_sale_request_recency', 'occupation', 'birth_date', 'latest_new_car', 'latest_main_ser', \n    'has_car_recency', 'sold_car_recency']\n \nX_train = grouped_train_df[chosen_columns]\ny_train = grouped_train_df[\"label\"]","f78fab2b":"grouped_train_df[\"label\"].value_counts()","efc1be21":"fl = FocalLoss(alpha=None, gamma=0)\n\nparam = {\n     \"metric\": \"auc\",\n     \"early_stopping_round\": 100, \n     \"learning_rate\": 0.05,\n     \"lambda_l1\": 10,\n     \"lambda_l2\": 10,\n     \"min_data_per_group\": 2500,\n     \"cat_l2\": 10,\n     \"cat_smooth\": 10,\n     \"verbosity\": -1}\ntrain_data_ = lgb.Dataset(X_train, label=y_train)\n\nnum_round = 300\nbst = lgb.train(param, train_data_, num_round, valid_sets=[train_data_], fobj=fl.lgb_obj, feval=fl.lgb_eval)\n#bst = lgb.train(param, train_data_, num_round, valid_sets=[train_data_])","1000299e":"all_grouped_df = group_merged_df(merged_df)\nall_grouped_df.head()","9902eba9":"all_grouped_df[\"gender\"] = all_grouped_df[\"gender\"].replace(\"\", np.nan)\nall_grouped_df[\"marital_status\"] = all_grouped_df[\"marital_status\"].replace(\"\", np.nan)\nall_grouped_df[\"occupation\"] = all_grouped_df[\"occupation\"].replace(\"\", np.nan)\nall_grouped_df[\"birth_date\"] = all_grouped_df[\"birth_date\"].replace(-1, np.nan)","a0882a3c":"PIVOT_DATE = \"2021\/06\/01\"\ncar_df = process_customer_vehicle_df(customer_vehicle_df, PIVOT_DATE)\nsifir_arac_df = process_sifir_arac_alanlar(sifir_arac_alanlar_df, PIVOT_DATE)\ncar_service_df = process_service_df(servis_df, PIVOT_DATE)\nprocessed_vehicle_df = process_vehicle_df(vehicle_df, car_service_df, PIVOT_DATE)\nall_grouped_df = pd.merge(all_grouped_df, sifir_arac_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\nall_grouped_df = pd.merge(all_grouped_df, processed_vehicle_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")\nall_grouped_df = pd.merge(all_grouped_df, car_df, on=\"BASE_CUSTOMER_ID\", how=\"left\")","4f0d92a4":"categorical_feats = [\"gender\", \"occupation\"]\nfor c in categorical_feats:\n    all_grouped_df[c] = all_grouped_df[c].astype('category')","40badfd4":"data = all_grouped_df[bst.feature_name()]\npred = bst.predict(data, num_iteration=bst.best_iteration)","a9913ad0":"all_grouped_df[\"PRED\"] = special.expit(pred)","421ecd0d":"id_2_pred_sales = dict(zip(all_grouped_df[\"BASE_CUSTOMER_ID\"], all_grouped_df[\"PRED\"]))","05720208":"test_df[\"Expected\"] = test_df[\"Id\"].map(id_2_pred_sales)\ntest_df.to_csv(\"submission.csv\", index=False)","f59c8076":"## Final modelinin olu\u015fturulmas\u0131\n\nT\u00fcm veri kullan\u0131larak final modeli olu\u015fturulmu\u015ftur. (T\u00fcm veriden kas\u0131t 2021\/03\/01'e kadar filtrelenmi\u015f verideki t\u00fcm kullan\u0131c\u0131lar)","f5baa809":"Model \u00fcretiminde kullan\u0131lmayan feature'lar comment i\u00e7inde yer almaktad\u0131r.\n\n\u00d6rn: son sat\u0131\u015f durumu kodu\nson 3-6-9... ayda yap\u0131lan sat\u0131\u015f miktar\u0131","760b63d1":"\u00dcretilen featurelar\u0131n isimlerinden ne olduklar\u0131 anla\u015f\u0131labilmektedir. \u00d6rnek olarak alttaki tablo (s\u0131f\u0131r ara\u00e7 alanlar) \u00fczerinden gidersek:\n\n**total_new_car**: Toplam al\u0131nan araba say\u0131s\u0131\n\n**latest_new_car**: Ay baz\u0131nda en yak\u0131n al\u0131m (Pivot date 2021\/03\/01 verildi ise ve ki\u015finin son al\u0131m\u0131 2021\/02\/01'de ise 1 olarak kaydedilir)\n\n**second_latest_new_car**: Ay baz\u0131nda en yak\u0131n 2. al\u0131m \n\n**third_latest_new_car**: Ay baz\u0131nda en yak\u0131n 3. al\u0131m \n\n**fourth_latest_new_car**: Ay baz\u0131nda en yak\u0131n 4. al\u0131m \n\n**average_new_car_diff**: Ortalama ara\u00e7 al\u0131mlar\u0131 aras\u0131nda fark\n\n**average_new_car_recency**: Ortalama ara\u00e7 al\u0131m yak\u0131nl\u0131\u011f\u0131\n\n**difference_to_latest_new_car**: En son ara\u00e7 al\u0131m\u0131 ile bir \u00f6nceki aras\u0131ndaki fark\n\n\u0130simlendirmeler di\u011fer tablolarda benzer oldu\u011fu i\u00e7in feature'lar\u0131n ne olduklar\u0131 buradaki a\u00e7\u0131klamalardan anla\u015f\u0131labilmektedir.\n","118a5d20":"## Verileri gruplamak i\u00e7in kullan\u0131lan metotlar\n\nBase customer id baz\u0131nda verileri gruplamak ve yeni featurelar eklemek i\u00e7in bu metotlar kullan\u0131labilir.\n\nPivot date adl\u0131 parametere ile, sadece o tarihe kadar olan veriler kullan\u0131lacakt\u0131r. \n\n\u00d6rn: \"2021\/03\/01\" yap\u0131l\u0131rsa son \u00fc\u00e7 ay\u0131n verisi filtrelenicektir.  ","70169c65":"## Tahminler","b07fab34":"## Verinin train ve test olarak ayr\u0131lmas\u0131\n\ntrain: 2021\/03\/01'e kadar olan veri\ntest: 2021\/03\/01 sonras\u0131\n\nTrain datas\u0131nda label (sat\u0131\u015f talebi var ya da yok) olarak test k\u0131sm\u0131ndaki \u00f6rnekler kullan\u0131lm\u0131st\u0131r.\n\nE\u011fer bir BASE CUSTOMER ID test verisinde bulunuyor ise, train verisinde label'\u0131 True olarak kaydedilmi\u015ftir.","953f1392":"2021\/06\/01'e kadar olan veri ile kullan\u0131c\u0131lar\u0131n feature'lar\u0131n\u0131n tekrar hesaplanmas\u0131.","17262aa2":"## Cross validation","8ff91b61":"Focal loss algoritmas\u0131 kullanarak verideki \"imbalance\"\u0131n \u00f6n\u00fcne ge\u00e7meye \u00e7al\u0131\u015ft\u0131m.","e1125ab4":"## Tablolar\u0131n birle\u015ftirilmesi","95644fb2":"# Gerekli k\u00fct\u00fcphaneler","54f0e418":"## Feature selection\nFeature say\u0131s\u0131 \u00e7ok oldu\u011fu i\u00e7in birden fazla model denenmi\u015ftir.\nEn \u00f6nemli featurelar\u0131 bulmak i\u00e7in LOFO kullan\u0131lm\u0131\u015ft\u0131r."}}