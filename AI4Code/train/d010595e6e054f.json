{"cell_type":{"72c77250":"code","41909586":"code","364e130d":"code","f385b9c9":"code","f1b30d8f":"code","3959d0a1":"code","751d9be1":"code","4a3d634f":"code","c62b53a6":"code","9b423ade":"code","0de5d1d2":"code","dd85b23d":"code","2e2fc918":"code","cae74ca1":"code","48e52c77":"code","d453e278":"code","1b33f245":"code","3e63692e":"code","2770b5ef":"code","607d0d18":"code","6f17f564":"code","9dd1cf16":"code","0b109b0c":"code","17e1e9b4":"code","701cd88f":"code","3090ab64":"code","6167cbfe":"code","bf4287f5":"code","82b5a68e":"code","ac7ffcaa":"code","b627cb1c":"code","fe952e12":"code","0272a32d":"code","8653416d":"code","d5955921":"code","03c9fee8":"code","9299d11b":"code","32e02088":"code","88f47ef6":"code","764a8223":"code","c87cb63c":"code","3a15170f":"code","a98cc0e1":"code","c43091ea":"code","42ebf14b":"code","7db323b1":"code","99324e99":"code","343848d4":"code","c0e018df":"code","3bec6a2a":"code","cad7654d":"code","5cf57081":"code","dd72dc04":"code","24431984":"code","8f830c89":"code","15b4ee10":"code","069ea5f7":"code","968f0351":"code","6dcbb0f9":"code","1abbd41c":"code","5a4a96db":"code","4d87dca3":"markdown","98900070":"markdown","2e9315a9":"markdown","a12849dc":"markdown","04f0577f":"markdown","c4907bfc":"markdown","fe54a704":"markdown"},"source":{"72c77250":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41909586":"pip install talos","364e130d":"#import libraries\nimport talos","f385b9c9":"#Importing dataset\ndata = pd.read_csv(\"\/kaggle\/input\/delinquency-telecom-dataset\/sample_data_intw.csv\")","f1b30d8f":"#Taking a look at the data set\ndata.head(2)","3959d0a1":"#Create a list of columns to be used later\ncolumns = [x for x in data.columns]\ncolumns","751d9be1":"#Check null and dtypes\ndata.info()","4a3d634f":"#import matplotlib for visualisation, seaborn can also be used.\nimport matplotlib.pyplot as plt\nplt.hist(data.label)\nplt.grid(True)\nplt.show()","c62b53a6":"#seperate categorical and numerical columns\nnon_num=[]\nfor i in data.columns:\n    if data[i].dtype==\"object\":\n        non_num.append(i)\n        \nprint(non_num)\n","9b423ade":"#drop these columns\ndata.drop(labels=non_num,axis=1,inplace=True)","0de5d1d2":"#Prepare feature and target sets\nx = data.drop(labels=[\"label\"],axis=1)\ny = data[\"label\"]\nx = talos.utils.rescale_meanzero(x)","dd85b23d":"#since our data is completely numeric, we can proceed for upsampling\nfrom imblearn.over_sampling import SMOTE","2e2fc918":"#Initialise smote\nsmote = SMOTE()\n# fit predictor and target variable on smote\nx_smote, y_smote = smote.fit_resample(x,y)\n\nprint('old shape', x.shape)\nprint('new shape', x_smote.shape)","cae74ca1":"y_smote = pd.DataFrame(y_smote, columns =['label'])\ny_smote.head()","48e52c77":"x_smote = pd.DataFrame(x_smote, columns = columns.remove(\"label\"))\n\nx_smote.head()","d453e278":"#Visualise data imbalance\nimport seaborn as sns\nplt.figure(figsize=(5,5))\nsns.countplot('label',data=y_smote,palette='hls')\nplt.show()","1b33f245":"#for building our model lets split the data into three parts i.e. train,test and validate.\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=0.25,random_state=123)\nprint(\"length of x_train {} and y_train {}\".format(len(x_train),len(y_train)))\nprint(\"length of x_test {} and y_test {}\".format(len(x_test),len(y_test)))","3e63692e":"#split for validation set\nx_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size=0.20,random_state=123)\nprint(\"length of x_train {} and y_train {}\".format(len(x_train),len(y_train)))\nprint(\"length of x_valid {} and y_valid {}\".format(len(x_valid),len(y_valid)))","2770b5ef":"#Model Building\nimport tensorflow as tf\nimport keras as ks","607d0d18":"#Prepare a function for fitting model on dataset.\n#This will be usefull in checking what learning rate is suitable for our purpose.\ndef modelfitter(x_train,y_train,epchs,x_valid,y_valid,batch_size,lr):\n    model = ks.models.Sequential()\n    opt = ks.optimizers.Adam(learning_rate=lr)\n    #building architecture\n    #Adding layers\n    model.add(ks.layers.Dense(32,activation=\"relu\",name=\"layer1\",input_shape=(32,))) \n    model.add(ks.layers.Dense(32,activation=\"relu\",name=\"layer2\"))\n    model.add(ks.layers.Dense(32,activation=\"relu\",name=\"layer3\"))\n    model.add(ks.layers.Dense(1,activation=\"sigmoid\",name=\"output\"))#2 classes \n    summmry = model.summary()\n    #Compile the model.\n    #setting optimisation, cost funciton and metric to be used\n    model.compile(loss=\"binary_crossentropy\",\n                 optimizer=opt,\n                 metrics=[\"accuracy\"])\n    history = model.fit(x_train,y_train,epochs=epchs,validation_data=(x_valid,y_valid),batch_size=batch_size)\n    #Visualise curves\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.title('lrate='+str(lr), pad=-50)\n    plt.grid(True)\n    #plt.savefig(\"Performance on Dilenquency Prediction.jpeg\")\n    ","6f17f564":"#Check output after fitting model on different lr.\nlearning_rates = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\nfor i in range(len(learning_rates)):\n    plt.figure(figsize=(10,10))\n    # determine the plot number\n    plot_no = 420 + (i+1)\n    plt.subplot(plot_no)\n    # fit model and plot learning curves for a learning rate\n    modelfitter(x_train,y_train,5,x_valid,y_valid,batch_size=30,lr=learning_rates[i])\nplt.show()","9dd1cf16":"#Using sequential API for final model building with higher number of epch\nmodel = ks.models.Sequential()","0b109b0c":"#set the optimiser on best lr found\nopt = ks.optimizers.Adam(learning_rate=1E-3)","17e1e9b4":"#building architecture\n#Adding layers\nmodel.add(ks.layers.Dense(32,activation=\"relu\",name=\"layer1\",input_shape=(32,))) #do this or just add a simple layer but flattened inputs.(2d to 1d)\nmodel.add(ks.layers.Dense(32,name=\"layer2\"))\nmodel.add(ks.layers.Dropout(0.2))\nmodel.add(ks.layers.Dense(32,activation=\"relu\",name=\"layer3\"))\nmodel.add(ks.layers.Dropout(0.2))\nmodel.add(ks.layers.Dense(1,activation=\"sigmoid\",name=\"output\"))#2 classes ","701cd88f":"#Getting summary of architecture\nmodel.summary()","3090ab64":"#Compile the model.\n#setting optimisation, cost funciton and metric to be used\nmodel.compile(loss=\"binary_crossentropy\",\n             optimizer=opt,\n             metrics=[\"accuracy\"])","6167cbfe":"#Training and evaluation\nhistory = model.fit(x_train,y_train,epochs=100,validation_data=(x_valid,y_valid),batch_size=32)","bf4287f5":"#Visualise curves\npd.DataFrame(history.history).plot(figsize=(10,8))\nplt.grid(True)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.gca().set_ylim(0,1)\nplt.savefig(\"Dilenquency.jpeg\")\nplt.show()\n","82b5a68e":"#Final Evaluation\nresult = model.evaluate(x=x_test,y=y_test)\nprint(\"Accuracy Score on Test Data: {}%\".format(round(result[1]*100,3)))","ac7ffcaa":"#Importing all the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom sklearn import metrics\nimport keras\nfrom keras import Sequential, regularizers\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder \nsns.set_style(\"whitegrid\")  # Set style of seaborn","b627cb1c":"#Reading the uploaded CSV file and storing in a dataframe\ndf = pd.read_csv('..\/input\/cell2cell\/cell2celltrain.csv')","fe952e12":"#storing all the categorical values in a list\nvar_mod = [\"Churn\",\"ChildrenInHH\",\"HandsetRefurbished\",\"HandsetWebCapable\",\"TruckOwner\",\"RVOwner\",\n           \"Homeownership\",'BuysViaMailOrder',\"RespondsToMailOffers\",\"OptOutMailings\",\n           \"NonUSTravel\",\"OwnsComputer\",\"HasCreditCard\",\"NewCellphoneUser\",\"NotNewCellphoneUser\",\n           \"OwnsMotorcycle\",\"PrizmCode\",\"MadeCallToRetentionTeam\",\"Occupation\",\"MaritalStatus\"]\n#using for loop to select column name from the list and using label encoder to convert \n#the categorical data to numeric(pre processing)\nle = LabelEncoder() \nfor i in var_mod: \n    df[i] = le.fit_transform(df[i])\n#print(df.dtypes) ","0272a32d":"#Identifying columns with missing values\nnan_values = df.isna()\nnan_columns = nan_values.any()\n#print(nan_columns)\n# count the number of missing values for each column\nnum_missing = (df[df.columns].isnull()).sum()\n#print(num_missing)","8653416d":"#storing all the numerical values with missing value in a list\nVariables=['MonthlyRevenue','MonthlyMinutes','TotalRecurringCharge','DirectorAssistedCalls','OverageMinutes','RoamingCalls','PercChangeMinutes','PercChangeRevenues','Handsets','AgeHH2','AgeHH1','CurrentEquipmentDays','HandsetModels']\n#inserting the mean value of the column in the null value place\nfor var in Variables:\n    df[var]=df[var].fillna((df[var].mean()))","d5955921":"#Removing all the columns in the Service Area with null values\nindex_serv=[]\nfor index, row in df.iterrows():\n    #print(index)\n    if pd.isnull(df['ServiceArea'][index]):\n        index_serv.append(index)\ndf = df.drop(index = index_serv)    ","03c9fee8":"#Customer ID column is not needed for our classification thus removing the column\ndf.drop('CustomerID', axis=1, inplace= True)\n#Converting the VarChar column to variable column by storing the substring of the service area\ndf['ServiceArea'] = df['ServiceArea'].str[-3:]","9299d11b":"#To find out whether our data is imbalance we find the count of outcomes(either 0 or 1) \n#in the target variable(churn)\ndf['Churn'].value_counts()","32e02088":"#Diving data into independent (X) and dependant\/target (y) variables\nX = df.iloc[:, 1:] #all columns except the last column\ny = df.iloc[:, 0] #only the last column","88f47ef6":"from imblearn.over_sampling import SMOTE \n#As we found out the data is imbalanced, we use the SMOTE function to balance the data\nsm = SMOTE(random_state=42)\nX_sm, y_sm = sm.fit_resample(X, y)\nprint(f'''Shape of X before SMOTE: {X.shape}\nShape of X after SMOTE: {X_sm.shape}''')","764a8223":"#converting the array obtained from Smote into dataframe for further processing\nx_sm = pd.DataFrame(X_sm)\ny_sm = pd.DataFrame(y_sm, columns =['Churn'])","c87cb63c":"#for building our model lets split the data into three parts i.e. train,test and validate.\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_sm,y_sm,test_size=0.20,random_state=123)\nprint(\"length of x_train {} and y_train {}\".format(len(x_train),len(y_train)))\nprint(\"length of x_test {} and y_test {}\".format(len(x_test),len(y_test)))\n","3a15170f":"#split for validation set\nx_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size=0.20,random_state=123)\nprint(\"length of x_train {} and y_train {}\".format(len(x_train),len(y_train)))\nprint(\"length of x_valid {} and y_valid {}\".format(len(x_valid),len(y_valid)))","a98cc0e1":"#Building a Sequential Neural Network with 1 dense layer as input, 2 Dense layers as hidden layers and one output Dense layer.\na=0.0005\nimport tensorflow as tf\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(56, kernel_regularizer=regularizers.l1_l2(l1=a, l2=a), activation=\"relu\",name='Dense_Layer_1',input_dim=56),\n    tf.keras.layers.Dropout(0.2),\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(56, kernel_regularizer=regularizers.l1_l2(l1=a, l2=a), activation=\"relu\", name='Dense_Layer_2'),\n    tf.keras.layers.Dropout(0.2),\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(56, kernel_regularizer=regularizers.l1_l2(l1=a, l2=a), activation=\"relu\", name='Dense_Layer_3'),\n    tf.keras.layers.Dropout(0.2),\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"Output_layer\")\n])","c43091ea":"model.summary() ","42ebf14b":"# Visualize the model\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model1.png',show_shapes=True)","7db323b1":"# Compile the model\n# The optimizer and the loss function and the metrics the model are mentioned\n# We also tried 'sgd' and 'adam' optimizer to get good the results. We obtained the best accuracy with rmsprop\nmodel.compile(optimizer='rmsprop', \n             loss='binary_crossentropy',\n             metrics=['accuracy'])","99324e99":"# Converting into numpy arrays and float values like the data\nx_train = np.asarray(x_train).astype('float32')\ny_train = np.asarray(y_train).astype('float32')\nx_valid = np.asarray(x_valid).astype('float32')\ny_valid = np.asarray(y_valid).astype('float32')\nx_test = np.asarray(x_test).astype('float32')\ny_test = np.asarray(y_test).astype('float32')","343848d4":"# Finally train the model on the train_data and validation data\nhistory = model.fit(x_train,y_train,epochs=100,validation_data=(x_valid,y_valid),batch_size=30)","c0e018df":"#Visualise curves\npd.DataFrame(history.history).plot(figsize=(10,8))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\n\nplt.gca().set_ylim(0,1)\n#plt.savefig(\"Performance of the model.jpeg\")\nplt.show()","3bec6a2a":"#Final Evaluation\nresult = model.evaluate(x=x_test,y=y_test)\nprint(\"Accuracy Score on Test Data: {}%\".format(round(result[1]*100,3)))","cad7654d":"# Importing the data for which we need to predict the Churn\nX_test = pd.read_csv(\"..\/input\/cell2cell\/cell2cellholdout.csv\")","5cf57081":"#performing the same preprocessing steps that we performed on the training dataset\/\nvar_mod = [\"ChildrenInHH\",\"HandsetRefurbished\",\"HandsetWebCapable\",\"TruckOwner\",\n           \"RVOwner\",\"Homeownership\",'BuysViaMailOrder',\"RespondsToMailOffers\",\n           \"OptOutMailings\",\"NonUSTravel\",\"OwnsComputer\",\"HasCreditCard\",\"NewCellphoneUser\",\n           \"NotNewCellphoneUser\",\"OwnsMotorcycle\",\"PrizmCode\",\"MadeCallToRetentionTeam\",\n           \"Occupation\",\"MaritalStatus\",\"HandsetPrice\"]\nle = LabelEncoder() \nfor i in var_mod: \n    X_test[i] = le.fit_transform(X_test[i])\n#print(df.dtypes) ","dd72dc04":"#Identifying columns with missing values\nnan_values = X_test.isna()\nnan_columns = nan_values.any()\n#print(nan_columns)","24431984":"cat=['MonthlyRevenue','MonthlyMinutes','TotalRecurringCharge','DirectorAssistedCalls','OverageMinutes','RoamingCalls','PercChangeMinutes','PercChangeRevenues','Handsets','AgeHH2','AgeHH1','CurrentEquipmentDays','HandsetModels']\nfor var in cat:\n    X_test[cat]=X_test[cat].fillna((X_test[cat].mean()))","8f830c89":"index_serv=[]\nfor index, row in X_test.iterrows():\n    #print(index)\n    if pd.isnull(X_test['ServiceArea'][index]):\n        index_serv.append(index)\ndf = X_test.drop(index = index_serv)   ","15b4ee10":"X_test.drop('CustomerID', axis=1, inplace= True)\nX_test['ServiceArea'] = X_test['ServiceArea'].str[-3:]","069ea5f7":"# Let us predict and see\nX_pred = X_test.iloc[:, 1:].values #all columns except the target(Churn) column\nX_pred = np.asarray(X_pred).astype('float32')","968f0351":"#Predicting the Churn value of the processed test data on the model we built\nresults = model.predict_classes(X_pred)\nresults = results.flatten()\nser = pd.Series(results)\nprint(ser)","6dcbb0f9":"#appending the predicted value to the test data\nX_test['Predicted_Churn']=ser\nX_test","1abbd41c":"#Sequential model with one hidden layer and a test accuracy of 56%\n#a=0.0005\n#import tensorflow as tf\n#model = tf.keras.models.Sequential([\n#    tf.keras.layers.Dense(56, activation=\"relu\",name='Dense_Layer_1',input_dim=56),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(56, activation=\"relu\", name='Dense_Layer_2'),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"Output_layer\")\n#])","5a4a96db":"#Sequential model with three hidden layer and a test accuracy of 57.977%\n#a=0.0005\n#import tensorflow as tf\n#model = tf.keras.models.Sequential([\n#    tf.keras.layers.Dense(56, activation=\"relu\",name='Dense_Layer_1',input_dim=56),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(56, activation=\"relu\", name='Dense_Layer_2'),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(56, activation=\"relu\", name='Dense_Layer_3'),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(56, activation=\"relu\", name='Dense_Layer_4'),\n#    tf.keras.layers.Dropout(0.2),\n#    tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"Output_layer\")\n#])","4d87dca3":"# Model Selection and Building\n#### Before buidling the final model. We checked the different architecture that can give the best result, These changes were:\n   * Variant:1 - Architecture with one hidden layer and optimiser as SGD which was peaked at 75% and started slowing down.\n   * Variant:2 - Architecture with one layer and optimiser as ADAM which was peaked at 79% \n   * Variant:3 - In order to further increase the score, we tried increasing the number of layers to 2 hidden layer and varying batch size and epochs.\n    - Further changes were done to the Variant 3rd Below since it was giving the highest score.","98900070":"    - Here, we have holded out 20% of train data for validaiton purpose","2e9315a9":"# SCIT Deep Learning Mini Project:\nMBA Data Science and Data Analytics (20-22)\n   ##### Group No. 12\n    Name:                    PRN:\n    Ankita Nayak            20030242013   \n    Ishuita Saxena          20030242028\n    Leni Fernandes          20030242039\n    Rishi Rajak             20030242057","a12849dc":"### Other Models Tried","04f0577f":"# Model-01: Delinquency Classification Based on DNN (Telecomm).\n- Delinquency is a condition that happens when an occurrence or event does not occur at its planned (or anticipated) date i.e., it occurs longer than expected.\n- A Telecom collaborates with an MFI to provide micro-credit on mobile balances to be paid back in 5 days. The Consumer is believed to be delinquent if he deviates from the path of paying back the loaned amount within 5 days.\n- In this case, we are building a model that can predict the delinquency of a consumer beforehand by using deep learning techniques. A Binary Classifier will be built by using Sequential API available in Keras library.\n## About Data:\n- [Columns and Description](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F4378858%2F8d6c62159a033854dc4ca79d2cfbf094%2FCapture.PNG?generation=1589482946434860&alt=media)\n- Total Number of Records: 186243\n- Number of olumns: 36","c4907bfc":"***In the above graph, we can see that at learning rate 0.001, model is converging fastly and achieved 79% accuracy higher than any other.***","fe54a704":"# Model-02: Customer Churn Prediction Based on DNN (Telecomm).\n- Nowadays, telecom industry faces fierce com-petition in satisfying its customers. The role of churn prediction system is not only restricted to accurately predict churners but also to interpret customer churn behavior.\n\n## About Data:\n- Total Number of Records: 71047\n- Number of olumns: 58"}}