{"cell_type":{"3a9c2fb4":"code","40e29d48":"code","79d5c511":"code","5b647316":"code","f1f4a8b3":"code","8f12011e":"code","a22b1143":"code","a4d2596f":"code","5c6c7c02":"code","04923bfd":"code","c157ff7b":"code","998c338f":"code","38b5d635":"code","ed8071b4":"code","588c986d":"code","5cd38bbb":"code","3c5730b5":"code","bf11eb29":"code","7aef5954":"code","3f523db5":"code","398fca1b":"code","2153969c":"code","4d344e3f":"markdown","f1d439b9":"markdown","b4261b57":"markdown","468d5abd":"markdown","dc8f1624":"markdown","9079dd16":"markdown","fa6d80db":"markdown","57171651":"markdown"},"source":{"3a9c2fb4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","40e29d48":"import pandas as pd\nimport numpy as np\nimport re\nimport plotly.express as px\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\nimport collections\nimport nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nnltk.download('punkt')","79d5c511":"train = pd.read_csv('\/kaggle\/input\/training-meta-info-nlp-tweets\/train_v3.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain['text'] = train['text'].astype(str)\ntest['text'] = test['text'].astype(str)\n\ndef cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\ndef removeurl(raw_text):\n    clean_text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', raw_text, flags=re.MULTILINE)\n    return clean_text\n\n\ntrain['text'] = train['text'].apply(lambda x: cleanhtml(x))\ntest['text'] = test['text'].apply(lambda x: cleanhtml(x))\n\ntrain['text'] = train['text'].apply(lambda x:removeurl(x))\ntest['text'] = test['text'].apply(lambda x: removeurl(x))\n","5b647316":"train","f1f4a8b3":"fig = px.histogram(train, x=\"Number_of_words\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","8f12011e":"sns.set()\nsns.set(rc={'figure.figsize':(10,8.27)})\n\nsns.kdeplot(train[train['target']==0].Number_of_words, color=\"red\",shade=True) \nsns.kdeplot(train[train['target']==1].Number_of_words, color=\"blue\",shade=True ) \n\nplt.show()\n","a22b1143":"fig = px.histogram(train, x=\"Number_of_Sentences\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","a4d2596f":"sns.set()\nsns.set(rc={'figure.figsize':(10,8.27)})\n\nsns.kdeplot(train[train['target']==0].Number_of_Sentences, color=\"red\",shade=True) \nsns.kdeplot(train[train['target']==1].Number_of_Sentences, color=\"blue\",shade=True ) \n\nplt.show()\n","5c6c7c02":"fig = px.histogram(train, x=\"Number_of_Unique_Words\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","04923bfd":"sns.set()\nsns.set(rc={'figure.figsize':(10,8.27)})\n\nsns.kdeplot(train[train['target']==0].Number_of_Unique_Words, color=\"red\",shade=True) \nsns.kdeplot(train[train['target']==1].Number_of_Unique_Words, color=\"blue\",shade=True ) \n\nplt.show()\n","c157ff7b":"fig = px.histogram(train, x=\"Number_of_Stop_Words\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","998c338f":"sns.set()\nsns.set(rc={'figure.figsize':(10,8.27)})\n\nsns.kdeplot(train[train['target']==0].Number_of_Stop_Words, color=\"red\",shade=True) \nsns.kdeplot(train[train['target']==1].Number_of_Stop_Words, color=\"blue\",shade=True ) \n\nplt.show()\n","38b5d635":"def Number_of_Hashtage(tweet):\n  return tweet.count('#')","ed8071b4":"train['Number_of_Hashtage'] = train['text'].apply(lambda x: Number_of_Hashtage(x))\ntest['Number_of_Hashtage'] = test['text'].apply(lambda x: Number_of_Hashtage(x))\n","588c986d":"def Number_of_Mentions(tweet):\n  return tweet.count('@')","5cd38bbb":"train['Number_of_Mentions'] = train['text'].apply(lambda x: Number_of_Mentions(x))\ntest['Number_of_Mentions'] = test['text'].apply(lambda x: Number_of_Mentions(x))\n","3c5730b5":"fig = px.histogram(train, x=\"Number_of_Hashtage\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","bf11eb29":"fig = px.histogram(train, x=\"Number_of_Mentions\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","7aef5954":"def Average_Word_Length(tweet):\n  \n  words = tweet.split()\n  try:\n      average = sum(len(word) for word in words) \/ len(words)\n  except:\n        average = 0\n  return average","3f523db5":"train['Average_Word_Length'] = train['text'].apply(lambda x: Average_Word_Length(x))\ntest['Average_Word_Length'] = test['text'].apply(lambda x: Average_Word_Length(x))\n","398fca1b":"fig = px.histogram(train, x=\"Average_Word_Length\", color=\"target\", marginal=\"rug\", # can be `box`, `violin`\n                         hover_data=train.columns)\nfig.show()\n","2153969c":"sns.set()\nsns.set(rc={'figure.figsize':(10,8.27)})\n\nsns.distplot(train[train['target']==0].Average_Word_Length, color=\"red\") \nsns.distplot(train[train['target']==1].Average_Word_Length, color=\"blue\") \n\nplt.show()\n","4d344e3f":"Importing Libraries","f1d439b9":"Average Word Length","b4261b57":"**Readind Data and Cleaning**","468d5abd":"**Author** - Mihir Ahuja\n\n**Data Citation** - https:\/\/www.kaggle.com\/c\/nlp-getting-started\n\n**Other Citation** - Followed Steps by ratan123 ( Ratan's notebook is a great starting guide for this dataset)\n Citaion For  https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model (Snippets used)","dc8f1624":"**Meta Features covered Include**\n1. Number_of_words\t\n1. Number_of_Sentences\t\n1. Number_of_Unique_Words\t\n1. Number_of_Stop_Words\t\n1. Number_of_Hashtage\t\n1. Number_of_Mentions\t\n1. Average_Word_Length","9079dd16":"Feature for number of Hashtags and Mentions","fa6d80db":"<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcS-q0mYQisDuvtExBrKpnN2m23erAVGrwF0FQ&usqp=CAU\"><\/center>","57171651":"**We look at the distribution of the metrics**:\nIs there any characterics shown in the distribution?"}}