{"cell_type":{"676da371":"code","01e52c2c":"code","0d7b354e":"code","ceb76682":"code","574a9a51":"code","d0f08808":"code","ff153a5a":"code","143d5787":"code","ae19cedd":"code","5c48fa93":"code","235b29ce":"code","3e6928f2":"code","c3431afd":"code","f38e4766":"code","cd46c468":"code","8640d6fe":"code","7ba7f2b7":"code","ed5a216c":"code","75648c1f":"code","7c8fbbbc":"code","d63fea93":"markdown","389d0d82":"markdown","c203dc1a":"markdown","9e4badc4":"markdown","e824b61e":"markdown","7a2888bd":"markdown","3e49f6ce":"markdown","fbc62115":"markdown","c6037246":"markdown","9a22352d":"markdown","e36fe957":"markdown","bba4d726":"markdown","b52d8468":"markdown","bfe9f33a":"markdown","d706104c":"markdown","f842ad7f":"markdown","8a9aae0a":"markdown","e09d3974":"markdown","ee712a9b":"markdown"},"source":{"676da371":"%matplotlib notebook\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport json\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport math\nfrom PIL import Image","01e52c2c":"og = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\ndf = og.copy()\nwith open(\"..\/input\/youtube-new\/US_category_id.json\") as category:\n    category = json.load(category)\n    \n#Extract the category information from the JSON File\nvid_cat = []\ncat_id = []\n\nfor i in category['items']:\n    vid_cat.append(i['snippet']['title'])\n    cat_id.append(int(i['id']))\n    \n#Mapping the category_id\ndf.category_id = og.category_id.map(dict(zip(cat_id,vid_cat)))\ndf.category_id.isnull().sum()#we have no nan values","0d7b354e":"\n##Prepare data type columns\ndf['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')\ndf['publish_time'] = pd.to_datetime(df['publish_time'], infer_datetime_format=True)\n\n##Add column for publish time\ndf['publish_date'] = df['publish_time'].dt.date\ndf['publish_wd'] = df['publish_time'].dt.weekday\ndf['publish_hr'] = df['publish_time'].dt.hour\ndf['publish_time'] = df['publish_time'].dt.time\n\ndf.head()\n","ceb76682":"#Dropping some columns and removing duplicates\ndf = df.drop(['tags', 'video_error_or_removed', 'description'],axis = 1)\ndf = df.drop_duplicates(keep = 'first')","574a9a51":"df.info()","d0f08808":"dff = df[['category_id', 'views']].groupby('category_id').aggregate(np.sum).reset_index()\\\n.sort_values(by='views', ascending=False)\ndff.views = df.views\/10**6\nplt.figure(figsize=(10,8))\nview_box = sns.barplot(x='views', y='category_id',data=dff, orient='h')\nplt.title('Barplot of number of views in each category (Unit:millions)')\nplt.ylabel('Category')\nplt.xlabel('views')\n#view_box.set_xticklabels(view_box.get_xticklabels(), rotation=45, horizontalalignment='right')","ff153a5a":"print(df[['views', 'likes']].corr())\nprint(df[['views', 'dislikes']].corr())","143d5787":"data = df['publish_wd'].map(dict(zip(range(7),\n    ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']))).value_counts()\n\n#Using text position = 'auto' for direct tax\nfig = go.Figure(data=[go.Bar(x=data.index.values, y=data, textposition='auto')])\n\nfig.update_layout(title=\"Number of Videos Published in Weekday\", yaxis=dict(title=\"Videos\"))\nfig.show()","ae19cedd":"#Load data, define hover text and bubble size, only look at videos with 10M views or above\ndata = df[['title', 'channel_title', 'category_id', 'views', 'publish_wd',\n          'publish_hr', 'likes', 'dislikes']].loc[df.views > 10**7].reset_index()\ndata.publish_wd = data.publish_wd.map(dict(zip(range(7),\n            ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])))\n\ndef bubble_plt(target, plot_title, target_title, data):\n    hover_text = []\n    bubble_size = []\n    for index, row in data.iterrows():\n        hover_text.append(('Title: {title}<br>'+\n                          'Category: {category_id}<br>'+\n                          'Channel: {channel_title}<br>'+\n                          'Views: {views}<br>'+\n                          'Likes: {likes}<br>'+\n                          'Dislikes: {dislikes}<br>'\n                          ).format(title=row['title'],\n                                  channel_title=row['channel_title'],\n                                  category_id=row['category_id'],\n                                  views = row['views'],\n                                  likes = row['likes'],\n                                  dislikes = row['dislikes']))\n        bubble_size.append(row[target]\/row['views'])\n    data['text'] = hover_text\n    data['size'] = bubble_size\n    fig = go.Figure()\n    \n    #Dictionary with dataframes for each weekday\n    weekday = ['Monday', 'Tuesday', 'Wednesday', 'Thurday', 'Friday', 'Saturday', 'Sunday']\n    wd_data = {wd:data.query(\"publish_wd == '%s'\"%wd)\n              for wd in weekday}\n    \n    #Create Figure\n    for key, values in wd_data.items():\n        fig.add_trace(go.Scatter(\n            x=values['views'], y=values[target]\/values['views'],\n            name=key, text=values['text'],\n            marker_size=values['size'],\n            ))\n        \n    # The following formula is recommended by https:\/\/plotly.com\/python\/bubble-charts\/\n    sizeref = 2.*max(data['size'])\/(1000)\n    \n    #Tune marker appearance and layout\n    fig.update_traces(mode='markers', marker=dict(sizemode='area',sizeref=sizeref, line_width=2))\n    \n    fig.update_layout(\n        title=plot_title,\n        xaxis=dict(\n            title='Number of views in millions',\n            gridcolor='white',\n            type='log',\n            gridwidth=2,\n        ),\n        yaxis=dict(\n            title=target_title,\n            gridcolor='white',\n            gridwidth=2,\n        ),\n        paper_bgcolor='rgb(243, 243, 243)',\n        plot_bgcolor='rgb(243, 243, 243)',\n        legend = {'itemsizing': 'constant'}\n    )\n    \n    fig.show()\nbubble_plt('likes',\"like\/view Ratio vs. Number of views\", \"Like\/view Ratio\", data)\n","5c48fa93":"bubble_plt('dislikes', \"Dislikes\/view ratio vs. Number of views\", \"Dislikes\/view Ratio\",data)","235b29ce":"#Create a dataframe for modeling\nnew_data = df.loc[(df.comments_disabled) &\n                 (~df.ratings_disabled)].copy()\n\n#Create a column for number of days a video takes to get on the trending list\nnew_data['day_to_trend'] = abs(np.subtract(new_data.trending_date.dt.date,new_data.publish_date,dtype=np.float32)\n                               .apply(lambda x: x.days))\nleft_vars = ['views','likes','dislikes','comment_count','publish_wd','publish_hr','day_to_trend','title']\n\nnew_data = new_data[left_vars]\nnew_data.reset_index(inplace=True)\nnew_data.head()","3e6928f2":"from pandas.plotting import scatter_matrix\nscatter_matrix(new_data[['publish_wd', 'publish_hr', 'day_to_trend']])\nplt.show()\nplt.hist(new_data['day_to_trend'])\nplt.title(\"Histogram of Original Days to Trend\")\nplt.show()\n\nnew_data = new_data.loc[new_data.day_to_trend <= 14]\nplt.hist(new_data['day_to_trend'])\nplt.title(\"Histogram of Days to Trend After Removing values > 7\")\nplt.show()","c3431afd":"import sklearn\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nnew_data.day_to_trend = new_data.day_to_trend <= 7","f38e4766":"def rf_model(X, y, my_pg = None):\n    #perform Grid-search\n    if my_pg is None:\n        #the followings are hyperparameters to optimize:max depth of a tree and number of trees in the forest\n        my_pg={\n            'max_depth': range(6,10),\n            'n_estimators': range(155,170),\n            }\n        \n    gsc = GridSearchCV(\n        estimator=RandomForestClassifier(),\n        param_grid = my_pg,cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n    \n    grid_result = gsc.fit(X,y)\n    \n    return grid_result.best_params_,grid_result.best_score_","cd46c468":"X = new_data[['views', 'likes', 'dislikes', 'publish_wd', 'publish_hr']]\ny = new_data['day_to_trend']\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=4,test_size=.3)","8640d6fe":"print(rf_model(X_train,y_train)) #({'max_depth':9, 'n_estimators': 165}, 0.895050301818652)","7ba7f2b7":"from sklearn.metrics import classification_report\nrfc = RandomForestClassifier(max_depth = 9, n_estimators = 165, oob_score = True, warm_start = True)\nrfc.fit(X_train, y_train)\n#OOB Score(Out of Bag)\nprint(rfc.oob_score_) #0.8640226628895185\nprint(rfc.score(X_test,y_test)) # 0.9210526315789473\nprint(rfc.feature_importances_)\n#print(pd.crosstab(pd.Series(y_train, name='Actual'), pd.Series(rfc.predict(X_train),name='predicted')))\nprint(pd.crosstab(pd.Series(y_test,name='Actual'), pd.Series(rfc.predict(X_test), name='predicted')))\npred = rfc.predict(X_test)\nprint(classification_report(y_test, pred))","ed5a216c":"import scikitplot as skplt\nfrom sklearn.metrics import average_precision_score, plot_precision_recall_curve\nprob = rfc.predict_proba(X_test)\nmyplot = skplt.metrics.plot_roc(y_test, prob)\naverage_precision = average_precision_score(y_test, prob[:,1]) #prob[:,1] is the estimated probability of positive outcome\ndisp = plot_precision_recall_curve(rfc, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve:'\n                  'AP={0:0.2f}'.format(average_precision))\nscore = metrics.f1_score(np.array(y_test),pred)\nprint('The f1 score for this model is {}'.format(score))","75648c1f":"from xgboost import XGBClassifier\nparameters = [{'n_estimators':range(100,150,1)},\n             {'learning_rate':np.arange(0.01,1.0,0.01)}]\ngbm = XGBClassifier(max_features='sqrt', subsample=0.8, random_state=10)\ngrid_search = GridSearchCV(estimator = gbm, param_grid = parameters, scoring='accuracy', cv=4, n_jobs=-1)\ngrid_search = grid_search.fit(X_train, y_train)\n\n#grid_search.cv_results_\n#grid_search.best_params_, grid_search.best_score_\ngrid_search.best_estimator_","7c8fbbbc":"gbm = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n                   colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n                   importance_type='gain', interaction_contraints=None,\n                   Learning_rate=0.24, max_delta_step=0, max_depth=6,\n                   max_features='sqrt', min_child_weight=1, missing=None,\n                   monotone_constraints=None, n_estimators=100, n_jobs=0,\n                   num_parallel_tree=1, objective='binary:logistic',random_state=10,\n                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n                   tree_method=None, validate_parameters=False, verbosity=None)\ngbm.fit(X_train, y_train)\ny_pred = gbm.predict(X_test)\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\nprint(classification_report(y_test, y_pred))","d63fea93":"# Predict the number of days to make your video trending\n\n* **Idea** - Videos appearing on the trending page is very important. Some videos takes too long to become trending, we will look into those factors.\n* **Target** - Number of days to make a video trending\n* **Predictors** - Publish Day(Weekday), Publish hour(0-24)\n* **Note** - Number of views, likes, dislikes and comments are not important values for making video trending. Therefore, it should not be used. Still we will proceed with our dataset to experiment.\n* **Model** - Random Forest and XGBoost","389d0d82":"Youtube videos are generated by various companies like Apple, Bed, Bath & Beyond, Best Buy as well media corporations like CNN, Disney, BBC and Hulu also offers their materials via Youtube as part of the youtube partnership program.\n\nThe data used in this report can be found - https:\/\/www.kaggle.com\/datasnaek\/youtube-new\/\nWebsite says that this was last updated on May 2019; however the latest publish date in the data in 2018\/06\/14\nReview Collection Year of Data column:https:\/\/www.kaggle.com\/mohitmanjaria\/youtube-video-trending-eda-and-nlp\/edit","c203dc1a":"# Description\nThe dataset includes data gathered from 40949 videos on YouTube that are contained within the trending category each day.\n\nThere are two kinds of data files, one includes comments (JSON) and one includes video statistics (CSV). They are linked by the unique video_id field.\n\nThe columns in the video file are:\n1. title\n2. channel_title\n3. video_id(Unique id of each video)\n4. trending_date\n5. title\n6. channel_title\n7. category_id (Can be looked up using the included JSON file)\n8. publish_time\n9. tags (Separated by | character, [none] is displayed if there are no tags)\n10. views\n11. likes\n12. dislikes\n13. comment_count\n14. thumbnail_link\n15. comments_disabled\n16. ratings_disabled\n17. video_error_or_removed\n18. description","9e4badc4":"# Comments:\n* with the Random forest algorithm, we obtained the parameter estimates that can predict whether or not a video can be trend within a week with 90.05% accuracy for the training dataset and 88.5% for the testing dataset. \n* Since Random Forest Algorithm uses a stochastic process to yield a model, what we obtain each time from fitting it to the data will be different. \n* The feature importances indicate that whether or not the comment\/rating section is available does not seem to affect the chance of getting on the trend within one week.\n* It also reveals that the most three important factors are number of views, likes, dislikes.we will try fitting the model again without these two variables.\n\nAlso, since we are interested in both true positive and true negative guesses and since we have a some imbalances between two classes(whether a video gets on the trend within one week),we first use ROC Curve to check the performance on both of the two classes. The ROC-AUC of both classes is about 92%\n\nLet's say, we only want to focus on how good we predict the positive class(or when the positive case is rare in the data). The Precision-Recall curve should be used instead. The PR-AUC is 96%, meaning that the model seems to predict very well for the positive class. Another way to look at this is using F1 score whose formula is ![math.svg](attachment:math.svg)\n\nThis score gives a balance between the precision and recall values. Using this score avoids misleading information from either precision or recall values in certain cases(e.g. data imbalance). Our Model F1 is about .95","e824b61e":"# Check at Distribution of the data\nScatter matrix is used to make estimates of the covariance matrix, for instance multivariate normal distribution","7a2888bd":"# What we discovered:\n1. Despite that we can assume both number of views and likes can tell us about how good a video is, the ration between them may not. we showed earlier the correlation between like count is highly positive - means they grow together, but this graph reveals that the view count grows much faster than the like count.Hence, we should not use this ratio to evaluate the content quality of a video.\n\n2. Double-click on each weekday to observe the impact of publish day on the number of views. Most if not all videos that have more than 100million views were published on **Wednesday, Friday, Sunday** and those that were published on **Monday, Tuesday, Thursday, Saturday** Could not reach 100 million views in the latest version of this dataset. Therefore, half of the Youtubers did the right thing to publish on the three \"hot days\", while other half did not have a very great choice. However, this impact is only obvious when the number of views passes 100 million.","3e49f6ce":"# Relationship between number of likes and Views\nI believe view count is very important, how about number of likes? Being able to make someone willing to double-tap on that thumbs-up button may be more crucial than just getting as many views as possible. However, logically these numbers should vary together. Although, number of dislikes is in question. Would the dislike count vary together with number of views too? Or if a video is popular, it gets less dislikes?","fbc62115":"# Comments:\nIt seems that the XGboost using random forest performs very similarly to the random forest model. However it took a longer time to train the original random forest model. Again, our models are not capable of giving legitimate predictions, because we asssumed that the number of views, dislikes, and likes are the values at the time these videos become trending, which is not true. When the appropriate data become available, we can be more confident about our results. Also, we should consider using thumbnail pictures as a feaure in predicting days to trend target. However, this would be a big project by itself and need contributions from more people","c6037246":"# Random Forest Algorithm\n\n**we will use Random forest classifier to predict if the trending days are less than a week**","9a22352d":"# XGboost Algorithm\n\n## we will use Xgboost classifier to predict if the trending day is less than a week","e36fe957":"From the histograms of the numerical variables in our dataset, we can see that none of them follow Gaussian distribution. The Number of views seem to follow exponential\/gamma distribution, and the target variable seems to only cluster at two locations. This suggest that we may need to discard the few observations, and lets narrow down to videos that become trending within two weeks.\n\nSecondly, scatter plots indicate that an **OLS Linear regression will not be a sufficient model**. This is the reason why we should try using more complex learning algorithms like random forest and Xgboost","bba4d726":"What we discovered:\n1. The correlation between the view and like count is 0.85, very high, which confirms our thoughts. If your video can attract a lot of viewers(high view count).\n2. The correlation between the view count and dislike count is 0.47, implying that the dislike count would vary together with the view count too. In other words, popularity does not equate high content quality\/positive viewer reaction. we will look more into this in the following part.","b52d8468":"# Introduction","bfe9f33a":"# Publish Date\n* What day of the week should i publish my video, you might ask? Does it really matter? \n* Isn't if my video has really good content, the number of views will eventually increase? \n* It is not quite that simple. How important is it to choose a right day, at the right time to post your video?\nI strongly believe that Time is the key point. If your video can get a strong burst of views in the first three days after it comes out, and if its content is excellent, it will get onto the trending list faster. Once your video appears on the trending list, more people will see and click on it, meaning more views, longer time remaining on the trending list and harder for other videos to beat yours","d706104c":"# What we discovered:\nUnlike the like\/view ratio, which decreases as the number of view increases, the Dislike\/view remains almost the same regardless of the change in views. If your video does not receive favourable reviews in the first couple of days, it may very likely remain so, even though your views increases eventually as time goes.\n\n**If someone works in a marketing team and chooses a channel to carry out plan with, closely observing a youtuber's newly published videos reviews after their week is already enough to make your decision. If you are a youtuber, do not experiment new\/unsure content, as \"bad videos will likely just stay \"bad\": note it down if you receive too many dislikes in the first three days**","f842ad7f":"**Comments:**\n\n* It is reasonable to assume that videos posted on the weekend, when people are off from work will be spending time on youtube or social media, will get more views within 24hrs as compare to weekdays. \n* However, from the histogram we can understand that most of the videos are published on weekdays instead of saturday and sunday which can increase the chances of more views. \n\nWe will evaluate this with following bubble charts.","8a9aae0a":"# What we discovered:\nEducation, Film&Animation, and comedy are what Americans watch the most, which may not be remarkably surprising, but having a look at real data is apparently much better than a mere guess.","e09d3974":"# Data Preparation","ee712a9b":"# **Data Exploration**\n\n**First Let's look at how many views are associated with each category**\n\nThis number is important, since it tells us about popularity of video. How can we utilize this piece of information? If you are a Youtuber, it is quite straight forward: the more views, which usually implies the more popular your channel is, the more money you can make from ads. If you are in marketing team, knowing what type of video people watch the most would help your advertising plan be more effective. Politicians, producers, media companies are other examples who can deploy this information. It is about how to spread out what you want to convey as quick as possible."}}