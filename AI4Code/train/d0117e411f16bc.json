{"cell_type":{"1d947fd7":"code","6b87f0af":"code","c5a211ac":"code","187a3cf5":"code","f0188f68":"code","f771f9f9":"code","6f2b8b0c":"code","cfe2f026":"code","0f3a5a7a":"code","157c7f3c":"code","fba29405":"code","ce3fff72":"code","c1f7b6c8":"code","477faf17":"code","8c541de3":"code","c19157dc":"code","87c57d50":"code","45f94288":"code","cba37f67":"code","0a10231f":"code","bb381f2e":"code","82a894f9":"code","ea29ae92":"code","6d8a7695":"code","7db5c7dd":"code","d021f852":"code","cdfab3fa":"code","9d2f7e6a":"code","7fbd1d41":"code","518a4e86":"markdown","bc5396fa":"markdown","94b681c1":"markdown","5272c40f":"markdown","042c9b35":"markdown","419c70d8":"markdown","b19d6e68":"markdown","97593bb6":"markdown","5e60dc77":"markdown","0dbcd5f0":"markdown","bd6a97f5":"markdown","0338103d":"markdown","7e1026b1":"markdown","04e498f9":"markdown","cc3f6421":"markdown","0bb350b1":"markdown"},"source":{"1d947fd7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler","6b87f0af":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","c5a211ac":"# Visualize the training set\ntrain.head()","187a3cf5":"# Visualize the test set\ntest.head()","f0188f68":"print(\"Training data shape:\",train.shape)\nprint(\"Test data shape:\",test.shape)","f771f9f9":"#checking for null values\nprint(train.isna().sum().sum())\nprint(test.isna().sum().sum())","6f2b8b0c":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","cfe2f026":"cols = train.columns.tolist()","0f3a5a7a":"plt.figure(figsize=(24, 156))\nfor i in range(len(train.columns.tolist())):\n    plt.subplot(26, 4, i+1)\n    if i <= 99:\n        plt.hist(train[f'f{i}'])\n        plt.xlabel(f'f{i}')\n    else:\n        plt.hist(train['loss'])\n        plt.xlabel('Loss')\nplt.show()","157c7f3c":"corr = train.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot = False, cmap= 'coolwarm')\nplt.show()","fba29405":"print(\"Training data shape after droping ID colunmn:\",train.shape)\nprint(\"Test data shape after droppig ID column:\",test.shape)","ce3fff72":"cols = test.columns","c1f7b6c8":"X = train[cols]\ny = train['loss']\ntest = test","477faf17":"X.head()","8c541de3":"test.head()","c19157dc":"y.head()","87c57d50":"#scaling the data \nss = StandardScaler()\nX_scaled = ss.fit_transform(X)\ntest_scaled = ss.fit_transform(test)","45f94288":"import tensorflow as tf\nfrom tensorflow import keras","cba37f67":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(1024,input_dim=X_scaled.shape[1],kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(512,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(256,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='relu')\n])\nmodel.summary()","0a10231f":"adam = tf.keras.optimizers.Adam()\nmodel.compile(loss='mean_squared_error',optimizer=adam)","bb381f2e":"history = model.fit(X_scaled,y,validation_split=0.25,epochs=5,verbose=1,shuffle=True)","82a894f9":"#Deffining figure:\nf = plt.figure(figsize = (20, 20))\n#Loss curve for the training set\nplt.plot(history.epoch,history.history['loss'],label=\"loss\")\n#Loss curve for the test set\nplt.plot(history.epoch,history.history['val_loss'],label=\"val_loss\")\n\nplt.title(\"Loss Curve\", fontsize = 18)\nplt.xlabel(\"Epochs\", fontsize = 15)\nplt.ylabel(\"Loss\", fontsize = 15)\nplt.grid(alpha = 0.3)\nplt.legend()\n\nplt.show()","ea29ae92":"train_pred = model.predict(X_scaled)","6d8a7695":"from sklearn.metrics import mean_squared_error\nprint(\"RMSE for Neural Network Model\",np.sqrt(mean_squared_error(y,train_pred)))","7db5c7dd":"y_pred = model.predict(test_scaled)","d021f852":"sub","cdfab3fa":"sub['loss'] = y_pred","9d2f7e6a":"sub","7fbd1d41":"sub.to_csv('NN_submission.csv',index=False)","518a4e86":"# **1.Basic EDA**","bc5396fa":"Import libraries","94b681c1":"Set the optimizer and loss","5272c40f":"We create the model","042c9b35":"We can see that relationship between the features and loss is very low.","419c70d8":"## Neural Network Model","b19d6e68":"## Predictions","97593bb6":"We can see that all the features are continuous and there is no discrete values.","5e60dc77":"Training","0dbcd5f0":"## Plotting the distribution of each feature","bd6a97f5":"The ID column in both training and test set is innecesary.","0338103d":"You can use hyperparameter tuning with this network too","7e1026b1":"# Results","04e498f9":"## Data preparation","cc3f6421":"# Model training","0bb350b1":"## Plotting the correlation matrix"}}