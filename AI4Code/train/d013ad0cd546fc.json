{"cell_type":{"565fae36":"code","fd403ad3":"code","b6313c4e":"code","52d48d12":"code","6500d23b":"code","298f5431":"code","3f1af4e1":"code","0243ea7a":"code","db04c3b1":"code","b319b2c8":"code","c33a5781":"code","d9ee54dc":"code","3b41bb29":"code","ad015fe0":"code","59cb47b3":"code","e432ca58":"code","50a812f4":"code","cb7c5994":"code","5db45546":"code","5e9eb581":"markdown","b8909fc2":"markdown","21f9cabd":"markdown","7ed1bb11":"markdown","9e2081f3":"markdown","b7998549":"markdown","28dc8708":"markdown","ceefe3de":"markdown","904c2452":"markdown","43216908":"markdown","f2927316":"markdown","72bc05d1":"markdown"},"source":{"565fae36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd403ad3":"# Load the training data\nX_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\nX_full.describe(include=\"all\")","b6313c4e":"X_full.head()","52d48d12":"X_full.isna().sum()","6500d23b":"X=X_full.copy()\nX_test=X_test_full.copy()\ny=X.target \nX.drop([\"target\"],axis=1,inplace=True)","298f5431":"### Common libraries \nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n### sklearn libraries \nimport sklearn as sk\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,OrdinalEncoder, FunctionTransformer,OneHotEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","3f1af4e1":"X_train,X_valid,y_train,y_valid=train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=69)","0243ea7a":"categorial_cols=[c for c in X_train.columns if X_train[c].dtype==\"object\"]\ngood_cat_cols=[c for c in categorial_cols if set(X_valid[c]).issubset(set(X_train[c]))]\nbad_cat_cols=list(set(categorial_cols)-set(good_cat_cols))\n               \n","db04c3b1":"low_cord_cols=[c for c in good_cat_cols if X_train[c].nunique()<10]\nhigh_cord_cols=list(set(good_cat_cols)-set(low_cord_cols))","b319b2c8":"ohe=OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\nohe_cols_train=pd.DataFrame(ohe.fit_transform(X_train[low_cord_cols]))\nohe_cols_valid=pd.DataFrame(ohe.transform(X_valid[low_cord_cols]))\n                           ","c33a5781":"ohe_cols_train.index=X_train.index\nohe_cols_valid.index=X_valid.index\n","d9ee54dc":"num_X_train=X_train.drop(categorial_cols,axis=1)\nnum_X_valid=X_valid.drop(categorial_cols,axis=1)\n\n### Adding the chosen cols \n\nX_trained=pd.concat([num_X_train,ohe_cols_train],axis=1)\nX_valided=pd.concat([num_X_valid,ohe_cols_valid],axis=1)","3b41bb29":"def get_score(models,x_t=X_trained,x_v=X_valided,y_t=y_train,y_v=y_valid):\n    model.fit(x_t,y_t)\n    X_predicted=model.predict(x_v)\n    mae=mean_absolute_error(X_predicted,y_v)\n    print(mae)","ad015fe0":"model=RandomForestRegressor(n_estimators=100,random_state=0,min_samples_leaf=2)\nget_score(model)","59cb47b3":"num_X_test=X_test.drop(categorial_cols,axis=1)\nohe_cols_test=pd.DataFrame(ohe.transform(X_test[low_cord_cols]))\nohe_cols_test.index=X_test.index\nX_tested=pd.concat([num_X_test,ohe_cols_test],axis=1)\n\n","e432ca58":"# imp=SimpleImputer()\n# imp_X_train=pd.DataFrame(imp.fit_transform(X_trained))\n# imp_X_test=pd.DataFrame(imp.transform(X_tested))\n# imp_X_train.columns=X_trained.columns\n# imp_X_test.columns=X_tested.columns\n# imp_X_test=imp_X_test[0:200000]","50a812f4":"X_tested.fillna(0,inplace=True)","cb7c5994":"X_predicted=model.predict(X_tested)","5db45546":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_tested.index,\n                       'target': X_predicted})\noutput.to_csv('submission1.csv', index=False)","5e9eb581":"# we see from the above that the data set doesnt have any null values so we proceed to the next level ","b8909fc2":"## Defining a mean absolute error function","21f9cabd":"## now we import libraries that we will need in our model ","7ed1bb11":"## turning the test data into the format so that we could predict the target from the encoded version of the test data ","9e2081f3":"### OneHotEncoder removes the index of the train set so now we will fix this","b7998549":"### Now that we found the columns that allows us to use RandomForestRegressor we encode these categorial columns","28dc8708":"## so now we determine the y value, remove na y values and copy features ","ceefe3de":"## Defininf the test and valid data from the train data \n","904c2452":"### In this step we remove all the categorial columns in the training set and add the processed ones (cleaned,matched,low_cord,encoded)","43216908":"## now that we have previewd data we need to check for the missing values that the data may have ","f2927316":"### now we want to extract the low cordinality columns of the good categorial columns that we just found because if the data is a lot it takes a lot of computational power to","72bc05d1":"## Since we have categorial data we need to encode those to be able to use them in our prediction "}}