{"cell_type":{"3a2c2e59":"code","19778d82":"code","14f89502":"code","e43d9d98":"code","e8b7f362":"code","0d5f054a":"code","a99e21bf":"code","2aadf4cc":"code","b8d31c8a":"code","cb866b28":"code","992828b9":"code","e5e29f26":"code","d1883838":"code","f8dd3db6":"code","ed577ad5":"code","8b1015c5":"code","1d5de9ad":"code","6d31d159":"code","2a91cee7":"markdown","f9b16b76":"markdown","bf85c0a1":"markdown","046cc008":"markdown","016e2478":"markdown","8b0d3335":"markdown","e9ef46ef":"markdown","f4c0be86":"markdown","3717a2b6":"markdown","a876c014":"markdown","70175a4a":"markdown","accaccc4":"markdown","6da9d1b6":"markdown","08774bbf":"markdown","0e73a6e6":"markdown","379fbf39":"markdown","dbad8223":"markdown","aae89f03":"markdown","4d1354a2":"markdown"},"source":{"3a2c2e59":"!pip install wikipedia","19778d82":"# import and check\nimport wikipedia as wiki\n\n\ntry:\n    not_going_to_work = wiki.summary('Dog')\n\nexcept:\n    print(\"Be specific. \\n\")\n\ntext = wiki.summary('German Shepherd') # German_Shepherd will work to, the underscore is present in the wikipedia link\ntext","14f89502":"from nltk.tokenize import sent_tokenize\n\nsent_tokenize(text)","e43d9d98":"sentence = \"I am bored, I need . popcorn : and { netflix * `. I am a big @ fan of Big Bang ~ theory.\"\nsent_tokenize(sentence)","e8b7f362":"from nltk.tokenize import RegexpTokenizer\n\n# regex for matching capital letters\ntokenizer = RegexpTokenizer('[A-Z]\\w+')\ntokenizer.tokenize(sentence)","0d5f054a":"# getting rid of special characters used in text\ntokenizer = RegexpTokenizer(r'\\w+')\ntext_list = tokenizer.tokenize(text)\ntext_list","a99e21bf":"from nltk.corpus import stopwords\n\nstopwords_eng = stopwords.words('english')\nstopwords_eng","2aadf4cc":"add_stop_words = ['The','is','an','a','as','\u02c8d\u0254\u028ft\u0283\u0250','\u02c8\u0283\u025b\u02d0f\u0250\u02cch\u028ant']\nstopwords_eng.extend(add_stop_words)","b8d31c8a":"text_list = [word for word in text_list if word not in stopwords_eng]\ntext_list","cb866b28":"from wordcloud import WordCloud,ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\nstr_text_list = \"\"\nfor word in text_list:\n    str_text_list += word + \" \"\n\nwordcloud = WordCloud(background_color=\"white\",mode=\"RGBA\",).generate(str_text_list)\n\nplt.figure(figsize = (10,10))\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","992828b9":"mask = np.array(Image.open(\"..\/input\/visual-yolo-opencv\/german_shephard.jpg\"))\nwordcloud = WordCloud(background_color=\"white\", mask=mask, mode=\"RGBA\",max_words=700).generate(wiki.page('German Shephard').content)\n\nplt.figure(figsize = (10,10))\n\nimage_colors = ImageColorGenerator(mask)\nplt.imshow(wordcloud.recolor(color_func=image_colors), interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e5e29f26":"from nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nbiagram =  BigramCollocationFinder.from_words(text_list)\nbiagram.nbest(BigramAssocMeasures.likelihood_ratio, 10)","d1883838":"from nltk.collocations import TrigramCollocationFinder\nfrom nltk.metrics import TrigramAssocMeasures\n\ntrigram =  TrigramCollocationFinder.from_words(text_list)\ntrigram.nbest(TrigramAssocMeasures.likelihood_ratio, 10)","f8dd3db6":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import RegexpStemmer\n\nporter = PorterStemmer()\nlancaster = LancasterStemmer()\nregexp = RegexpStemmer('ing')\n\nprint(\"Porter Stemmer: \")\nprint(porter.stem(\"dance\"))\nprint(porter.stem(\"dancing\"))\nprint(porter.stem(\"danced\"))\nprint(porter.stem(\"dances\"))\n\nprint(\"\\nLancaster Stemmer: \")\nprint(lancaster.stem(\"dance\"))\nprint(lancaster.stem(\"dancing\"))\nprint(lancaster.stem(\"danced\"))\nprint(lancaster.stem(\"dances\"))\n\nprint(\"\\nRegexp Stemmer: \")\nprint(regexp.stem(\"dance\"))\nprint(regexp.stem(\"dancing\"))\nprint(regexp.stem(\"danced\"))\nprint(regexp.stem(\"dances\"))","ed577ad5":"from nltk.stem import WordNetLemmatizer\n\n\nlemmatizer = WordNetLemmatizer()\n\nprint(lemmatizer.lemmatize(\"dance\", pos=\"v\"))\nprint(lemmatizer.lemmatize(\"dancing\", pos=\"v\"))\nprint(lemmatizer.lemmatize(\"danced\", pos=\"v\"))\nprint(lemmatizer.lemmatize(\"dances\", pos=\"v\"))","8b1015c5":"# import from the library\nfrom nltk.corpus import wordnet","1d5de9ad":"# we will explore the word gain\nfor synset in wordnet.synsets(\"gain\")[:7]:\n    \n    print(\"\\n Synset name :  \", synset.name())   \n    print(\"\\n meaning : \", synset.definition())       \n    print(\"\\n example : \", synset.examples()) \n    print(\"\\n part of speech : \", synset.pos())\n    \n    # printing lemmas for a synset\n    for i,lemma in enumerate(synset.lemmas()):\n        print(\"\\n lemma \" , (i+1) , \" :\", lemma.name())\n        \n        # printing antonyms for the above lemma\n        if(lemma.antonyms()):\n            print(\" antonyms: \")\n            \n            for i,antonym in enumerate(lemma.antonyms()):\n                print(i+1, \": \" , antonym.name())\n        \n    print(\"\\n hypernyms : \", synset.hypernyms())\n    print(\"\\n hyponyms : \", synset.hyponyms())\n    #print(\"\\n root_hypernyms :\", synset.root_hypernyms())\n        \n    # divider        \n    print(\"\\n\"+\"##\"*20)","6d31d159":"profit = wordnet.synset('net_income.n.01') #[Synset('profit.n.01')]\ngain = wordnet.synset('gain.n.01') # [Synset('gain.n.01')]\nloss = wordnet.synset('loss.n.01') # [Synset('loss.n.01')]\n\nprint(\"profit and gain:\")\nprint(\"\\tPath similarity :\", profit.path_similarity(gain))\nprint(\"\\tLeacock-Chodorow similarity:\", profit.lch_similarity(gain))\nprint(\"\\tWu-Palmer similarity: \", profit.wup_similarity(gain))\n\nprint(\"\\nprofit and loss:\")\nprint(\"\\tPath similarity :\", profit.path_similarity(loss))\nprint(\"\\tLeacock-Chodorow similarity:\", profit.lch_similarity(loss))\nprint(\"\\tWu-Palmer similarity: \", profit.wup_similarity(loss))\n","2a91cee7":"**Finding Similarity**: One important application of wordnet can be finding the percentage of similarity between two words\n\n1. **Path similarity**: it shows the similarity in the senses by returning the score based on the shortest path between the hyponyms and hypernyms \n\n2. **Leacock-Chodorow similarity**: it shows the similarity in the senses by returning the score based on shortest path between the hyponyms and hypernyms and the maximum depth of the taxonomy. Taxonomy: distance to their top most hypernym.\n\n3. **Wu-Palmer similarity**: it shows the similarity in the senses by returning the score based on shortest path between the hyponyms and hypernyms , the maximum depth of the taxonomy and that of their most specific ancestor node.\n","f9b16b76":"As you might have noticed, it's creating a list by breaking the sentences and storing them as an element inside the list. Let's play around a bit more with this to know more about it. As you can see, it is tokenizing based on full stop (.)","bf85c0a1":"Problems in the wordnet:\n\n1. It is not updated with the latest meaning of the words:like badass for tough\n2. can't calculate the accurate word similarity","046cc008":"Let's try RegexpTokenizer to dela with the extra characters used above. Regex is mainly used for matching alphabetical patterns in words , or dealing with some charectes. Regex in itself a interesting approach for solving complex string based problems and I have seen its application in some coding challenges to reduce the compilation time and code complexity. But let's leave it for another repo :)  ","016e2478":"**Lemmatization**: It reduces the given word to a meaningful\/english word. The root word obtained is called lemma. We will discuss more about it in the next section. \n\n**Note: You must pass the part of speech as the parameter along with the word so that the algorithm can analyse the context and find the root word.**","8b0d3335":"Content:\n\n1. Generating text from Wikipedia\n2. Cleaning text\n3. WordCloud\n4. Bigrams and Trigrams\n5. Stemming and Lemmatization\n6. Wordnet: Synset and Similarity\n\n\n## Generating text from Wikipedia##\n\n\nLet's get started. First thing we need is text data to perform actions on. So let's use wikipedia for this. The packages is not preinstalled, so you have to download it first.","e9ef46ef":"![Original image](https:\/\/images.unsplash.com\/photo-1589070680566-0ccca496dbf2?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=334&q=80)","f4c0be86":"Using RegexTokenizer, let's get rid of any special characters used inside the text","3717a2b6":"## WordCloud ##\n\nLet's make it a bit dramatic by using word cloud","a876c014":"* According to [Princeton](https:\/\/wordnet.princeton.edu\/) , wordnet is a database of english words. For each word, we have set of synonyms and for each `synonym` , we have a `meaning`, `example(s)`, `name`, `part of speech`:\n\n1. noun : **n**\n2. verb : **v** \n3. adjective : **a**\n4. adjectives : **s**\n5. and adverb : **r** \n\n`Lemma` : Lemmas are words or synonyms having the same sense as the given word. \n\nLet's see an example to understand.  The word `gain` has many synonyms, we will take first 7 synsets. Each synset has it own meaning, you can check it from the examples. As you can see, the word `gain` can have meaning like *a quantity that is added (in the sense of addition)* or *the advantageous quality of being beneficial (in the sense of advantage)* etc. The lemmas for the former can be the synonym in this same sense ie `addition`, `increase`, `gain` (the word itself is included).\n\nAlso, for each lemma , we can search for antonyms for that particular sense. \n\nLast but not the least:\n\n1. hyponyms : these are specific words\n2. hypernyms : these are generalized words\n\nhyponyms and hypernyms shows a `is-a` relationship. For example: Apple is-a phone, Nokia is-a phone.\n\n'apple\u2019 and 'nokia' are a hyponym of \u2018phones\u2019 and \u2018phone\u2019 is a hypernym of \u2018apple\u2019 and 'nokia'.\n\nLet's see the code now.","70175a4a":"One think to remember here, it migh not load data for general topics like dogs. So be specific. I am using German Shephard as the word. \n\nYou can match the loaded data with the Wikipedia page for [German Shephard](https:\/\/en.wikipedia.org\/wiki\/German_Shepherd).","accaccc4":"Remove the stop words from the text_list. I am adding few more words to the stopwords.","6da9d1b6":"## Cleaning text ##\n\nOur next goal is to tokenize the textual data. Importnat thing to remember here is that we have many tokenizers, but I am only including the basic one along with few interesting one. The full documentation is [available here](https:\/\/www.nltk.org\/api\/nltk.tokenize.html). ","08774bbf":"Let's try somethnig new. We will genrate wordcloud for german shephard but we will add an image of german shephard as mask over it.\n\nAlso, limit the word count for this, othersize with too many words, size of words will be drastically reduced and image will look blurry. Ideally 700-1000 words are generating good images. And as you can see we dont have that many words, I am loading the whole Wikipedia article to get better image. To see some exciting examples check [DataCamp article] (https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python). \n\nThe photo is taken from [Unplash](https:\/\/unsplash.com\/)","0e73a6e6":"# References:\n\n1. [Documentation: WordNet Interface](https:\/\/www.nltk.org\/howto\/wordnet.html)\n2. [Stanford Course on NLP: Youtube](https:\/\/www.youtube.com\/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=1)\n3. [Sentdex tutorial](https:\/\/pythonprogramming.net\/wordnet-nltk-tutorial\/)\n4. [Wikipedia Wordnet](https:\/\/en.wikipedia.org\/wiki\/WordNet)\n5. [Accessing Text Corpora and Lexical Resources](https:\/\/www.nltk.org\/book\/ch02.html)\n6. [Maximum depth of taxonomy: StackOverflow](https:\/\/stackoverflow.com\/questions\/36206023\/wordnet-3-0-maximum-depth-of-the-taxonomy)\n7. [NLTK Text processing series Rocky DeRaze: Youtube](https:\/\/stackoverflow.com\/questions\/36206023\/wordnet-3-0-maximum-depth-of-the-taxonomy)","379fbf39":"\n## Wordnet: Synset and Similarity ##\n\n\nUsing an example to learn about nltk and wordnet","dbad8223":"# Natural Language Processing : Starter#\n","aae89f03":"## Stemming and Lemmatization ##\n\nStemming is basicly the process fo reducing the word to its root level. The root level might not always be a meaningful word. Let's see some popular stemming algorithm: \n\n1. **Porter**: It uses set of rules to find the suffixs of any word and is suitable, it removes them even if the reults doesn't make sense. [Algorithm Link](http:\/\/snowball.tartarus.org\/algorithms\/porter\/stemmer.html)\n\n2. **Lancaster**: It uses 120 rules to remove\/modify the suffix. It is an iterative process and it stops when there is no other rule left to apply on the word. This may end in over stemming. [Algorithm Link](https:\/\/www.nltk.org\/_modules\/nltk\/stem\/lancaster.html)\n\n3. **RegexpStemmer**: We can define the suffix which will be removed from the word\n\n\nRead the first two algorithms, its easy to understand whats going on even if you don't understand the whole algorithm.\n\nSee more examples on [DataCamp Article](https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python)","4d1354a2":"The result is relatable. Images with less distinct colors can generate better wordcloud. \n\n\n\n## Bigrams and Trigrams ##\n\n\nNext, let's learn about Bigrams and Trigrams. Sometimes pair of words can help in learning more the context than just a single word. The occurance of second (or third) word seems like dependent on the occurance of first. As the name suggest:\n\n`Bigrams`: 2 words occuring simultaneously \n\n`Trigrams`: 3 words occuring simultaneously\n\nWe can have n-grams starting from 1. "}}