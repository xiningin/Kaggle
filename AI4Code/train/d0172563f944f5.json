{"cell_type":{"45903842":"code","a001b862":"code","d5871a83":"code","2e656ca7":"code","5837d730":"code","8d8c2fa9":"code","6396b700":"code","2799c1fd":"code","ac80050d":"code","663c3948":"code","d5a8f0b1":"code","eec9656a":"code","2dda9189":"code","8b4b6e97":"code","d3234831":"code","38ba0383":"markdown","db4a712e":"markdown","e00f692e":"markdown","595ff57c":"markdown","c5e7a85e":"markdown","71e9a831":"markdown","ed884f99":"markdown","b5e03806":"markdown","f7fec6d3":"markdown"},"source":{"45903842":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scipy import interp\nfrom itertools import cycle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","a001b862":"df = pd.read_csv(\"\/kaggle\/input\/fetal-health-classification\/fetal_health.csv\")\nprint(df.shape)\ndf.head()","d5871a83":"df.info()","2e656ca7":"df.describe()","5837d730":"sns.countplot(df.fetal_health)","8d8c2fa9":"y_orig = df.fetal_health\nprint(y_orig.unique())\ny = label_binarize(y_orig, classes=[1,2,3])\nn_classes = 3\n# X = df.drop([\"fetal_health\"], axis=1)\nX = df[[\"baseline value\", \"accelerations\", \"fetal_movement\", \"uterine_contractions\", \"light_decelerations\",\n        \"severe_decelerations\", \"prolongued_decelerations\", \"abnormal_short_term_variability\", \"percentage_of_time_with_abnormal_long_term_variability\"]]","6396b700":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","2799c1fd":"clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=500, random_state=42))\ny_score = clf.fit(x_train,y_train)\ny_pred = clf.score(x_test,y_test)\nprint(\"Validation Accuracy\",clf.score(x_test,y_test)*100,\"%\")","ac80050d":"print(f1_score(y_test, clf.predict(x_test), average='macro'))\nprint(f1_score(y_test, clf.predict(x_test), average='micro'))\nprint(f1_score(y_test, clf.predict(x_test), average='weighted'))\n\nprecision, recall, fscore, support = score(y_test, clf.predict(x_test))\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","663c3948":"y_prob = clf.predict_proba(x_test)\nmacro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n                                  average=\"macro\")\nweighted_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n                                     average=\"weighted\")\nmacro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n                                  average=\"macro\")\nweighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n                                     average=\"weighted\")\nprint(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\nprint(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))","d5a8f0b1":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","eec9656a":"all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\nlw = 2\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Fetal Health Classification')\nplt.legend(loc=\"lower right\")\nplt.show()","2dda9189":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        y_prob[:, i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_prob[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n    y_prob.ravel())\naverage_precision[\"micro\"] = average_precision_score(y_test, y_prob,\n                                                     average=\"micro\")\nprint('Average precision score, micro-averaged over all classes: {0:0.2f}'\n      .format(average_precision[\"micro\"]))  ","8b4b6e97":"plt.figure()\nplt.step(recall['micro'], precision['micro'], where='post')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n    .format(average_precision[\"micro\"]))","d3234831":"colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\nplt.figure(figsize=(7, 8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlw = 2\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x \/ (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\nl, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\nlines.append(l)\nlabels.append('micro-average Precision-recall (area = {0:0.2f})'\n              ''.format(average_precision[\"micro\"]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                  ''.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.25)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Fetal Health Classification')\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n\n\nplt.show()","38ba0383":"## Precision recall curve","db4a712e":"# Fetal Health Classification\n* Data Cleaning and Preprocessing\n* Feature Selection\n* Model Selection - Random Forest Classifier - 92%\n* F1_Score, Recall and Precision for each class\n* ROC Curve\n* precision_recall_curve","e00f692e":"## F1_Score, Recall and Precision","595ff57c":"## Data Cleaning and Preprocessing ","c5e7a85e":"# Please Upvote if you like the content","71e9a831":"## Feature Selection","ed884f99":"## ROC Curve","b5e03806":"## Splitting Data - **not stratified**","f7fec6d3":"## Model Selection"}}