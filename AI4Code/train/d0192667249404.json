{"cell_type":{"f47a515c":"code","8372694b":"code","12d750bf":"code","f6681095":"code","b3c6a251":"code","19bbc193":"code","ace5b1b8":"code","1c0b12ef":"code","ecab95d5":"code","217e9fe0":"code","32295ea6":"code","a1452b2f":"code","7f5957e6":"code","67e1b672":"code","77cac72b":"code","6e458c78":"code","e17a8912":"code","83ec5e33":"code","17874edf":"code","35423bdd":"code","c973512e":"code","5758d014":"code","0d184509":"code","c8ccd75a":"code","88a4641d":"code","46705d9e":"code","81aabf5d":"code","87742a71":"code","a1b2decb":"code","d8b283d2":"code","2ec86d1c":"code","5b7962fe":"code","78b129cf":"code","2eab4710":"code","38941936":"code","e168ef33":"code","add38598":"code","390a9733":"code","8b3f1145":"code","430be3ee":"code","5c64903e":"code","be758535":"code","96196798":"code","578c508d":"code","657baa5c":"code","22cc5867":"code","a4537106":"code","1f1f1d06":"code","0d68d79d":"code","a738301a":"code","6726dc2d":"code","f09f5fd8":"code","2af2b897":"code","653ee6c3":"code","80e53c89":"code","2623bc55":"code","678df5ef":"code","1964de8b":"code","232a47db":"code","a109d64a":"code","ce074ad0":"code","cb9b2d66":"code","6f74c252":"code","d20d0a0c":"code","187b4354":"code","5892f3f7":"code","e7b22cd1":"code","8f76b0bc":"code","0856e4f1":"code","c70d67e8":"code","3b9d9970":"code","3b24b0d7":"code","f07a2f4e":"markdown","b0e8c362":"markdown","83b4a3c5":"markdown","3228ba93":"markdown","059248bc":"markdown","eef139d6":"markdown","6cbb00bc":"markdown","2086aab1":"markdown","3a0322e7":"markdown","c0f4dbe9":"markdown","8971bb9d":"markdown","72f3298e":"markdown","ece72e79":"markdown","36b42c99":"markdown","418c8362":"markdown","f262cd70":"markdown","350aa9c1":"markdown","1954e1d6":"markdown","e8bd5c71":"markdown","9dc19204":"markdown","6818d826":"markdown","bf8827ce":"markdown","a4b70e77":"markdown","05126dcf":"markdown","bb9b1fd0":"markdown","362458e2":"markdown","0b99543d":"markdown","9882efa0":"markdown","b092a90f":"markdown","66b4b09d":"markdown","ee2ec023":"markdown","6a832585":"markdown","e6184c53":"markdown","fb1913f9":"markdown","451f2ff8":"markdown","db5d95f1":"markdown","cf479f59":"markdown","d3ac0035":"markdown","2724f6e8":"markdown","5c6b5a59":"markdown","b42b1adb":"markdown","43e9d62a":"markdown","2392b546":"markdown","ee0905e7":"markdown","e5fec362":"markdown","100a0823":"markdown","41444cce":"markdown","8864b9c4":"markdown","6cfb3d40":"markdown"},"source":{"f47a515c":"# data analysis libraries\nimport pandas as pd\nimport numpy as np\n# visualization libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# ignore warnings library\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8372694b":"Train= pd.read_csv(\"..\/input\/titanic\/train.csv\")\nTest= pd.read_csv(\"..\/input\/titanic\/test.csv\")","12d750bf":"print(Train.isnull().sum())\n\n","f6681095":"print(Test.isnull().sum())","b3c6a251":"print(\"Train shape:\", Train.shape)\nprint(\"Test shape:\", Test.shape)","19bbc193":"Train.info()","ace5b1b8":"Test.info()","1c0b12ef":"# printing first five lines of our training dataset\nTrain.head()","ecab95d5":"# printing first five lines of our testing dataset\nTest.head()","217e9fe0":"# here we are calculating summary of our train dataset\nTrain.describe()","32295ea6":"# Here we are calculating summary of our test dataset\nTest.describe()","a1452b2f":"# ploting bar plot for sex vs survived\nsns.barplot(x=\"Sex\",y=\"Survived\",data=Train)\n# printing survival percentage of female\nprint(\"Percentage of females who survived:\", \n      Train[\"Survived\"][Train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n# printing survival percentage of male\nprint(\"Percentage of males who survived:\", \n      Train[\"Survived\"][Train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","7f5957e6":"# ploting bar plot for SibSp vs Survived\nsns.barplot(x=\"SibSp\",y=\"Survived\",data= Train)\n# printing survival percentage of sibsp\nprint(\"Percentage of Sibsp- 0 who survived:\", \n      Train[\"Survived\"][Train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Sibsp- 1 who survived:\", \n      Train[\"Survived\"][Train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Sibsp- 2 who survived:\", \n      Train[\"Survived\"][Train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Sibsp- 3 who survived:\", \n      Train[\"Survived\"][Train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Sibsp- 4 who survived:\", \n      Train[\"Survived\"][Train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)","67e1b672":"# ploting bar plot for Pclass vs Survived\nsns.barplot(x=\"Pclass\",y=\"Survived\",data= Train)\n# printing survival percentage of Pclass\nprint(\"Percentage of pclass- 1 who survived:\", \n      Train[\"Survived\"][Train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass- 2 who survived:\", \n      Train[\"Survived\"][Train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass- 3 who survived:\", \n      Train[\"Survived\"][Train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","77cac72b":"# ploting bar plot for Parch vs Survived\nsns.barplot(x=\"Parch\",y=\"Survived\",data= Train)\n# printing survival percentage of Parch\nprint(\"Percentage of parch- 0 who survived:\", \n      Train[\"Survived\"][Train[\"Parch\"] == 0].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Parch- 1 who survived:\", \n      Train[\"Survived\"][Train[\"Parch\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Parch- 2 who survived:\", \n      Train[\"Survived\"][Train[\"Parch\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Parch- 3 who survived:\", \n      Train[\"Survived\"][Train[\"Parch\"] == 3].value_counts(normalize = True)[1]*100)\n","6e458c78":"#sort the ages into logical categories\nTrain[\"Age\"] = Train[\"Age\"].fillna(-0.5)\nTest[\"Age\"] = Test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\nTrain['AgeGroup'] = pd.cut(Train[\"Age\"], bins, labels = labels)\nTest['AgeGroup'] = pd.cut(Test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=Train)\nplt.show()\n","e17a8912":"# ploting bar plot for Embarked vs Survived\nsns.barplot(x=\"Embarked\",y=\"Survived\",data= Train)\n# printing survival percentage of Embarked\nprint(\"Survived :\\n\",Train[Train['Survived']==1]['Embarked'].value_counts())\nprint(\"Dead:\\n\",Train[Train['Survived']==0]['Embarked'].value_counts())\n\n","83ec5e33":"# We will first drop tha cabin column because there is not need of this in our prediction.\nTrain = Train.drop([\"Cabin\"],axis=1)\nTest = Test.drop([\"Cabin\"],axis=1)","17874edf":"# We will also drop the Ticket column because there is not need of this in our prediction\nTrain = Train.drop([\"Ticket\"],axis=1)\nTest = Test.drop([\"Ticket\"],axis=1)","35423bdd":"#now we will fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = Train[Train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = Train[Train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = Train[Train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","c973512e":"# now we will replace missing value with Southampton\nTrain = Train.fillna({\"Embarked\": \"S\"})","5758d014":"# here we are combining our dataset\ntrain_test_data = [Train,Test]\nfor dataset in train_test_data:\n    dataset[\"Title\"] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","0d184509":"Train[\"Title\"].value_counts()","c8ccd75a":"Test[\"Title\"].value_counts()","88a4641d":"# Map each of title groups to numerical values.\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2,\n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset[\"Title\"].map(title_mapping)","46705d9e":"Test.head()","81aabf5d":"Train.head()","87742a71":"sns.barplot(x=\"Title\",y=\"Survived\",data= Train)","a1b2decb":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\nfor x in range(len(Train[\"Age\"])):\n    if Train[\"Age\"][x] == \"Unknown\":\n        Train[\"Age\"][x] = title_mapping[Train[\"Title\"][x]]\n        \nfor x in range(len(Test[\"Age\"])):\n    if Test[\"Age\"][x] == \"Unknown\":\n        Test[\"Age\"][x] = title_mapping[Test[\"Title\"][x]]","d8b283d2":"age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\nTrain['AgeGroup'] = Train['AgeGroup'].map(age_mapping)\nTest['AgeGroup'] = Test['AgeGroup'].map(age_mapping)\n\nTrain.head()\n\n","2ec86d1c":"Train[\"AgeGroup\"].fillna(Train.groupby(\"Title\")[\"AgeGroup\"].transform(\"median\"), inplace= True)\nTest[\"AgeGroup\"].fillna(Test.groupby('Title')['AgeGroup'].transform(\"median\"), inplace= True)","5b7962fe":"# here we are mapping sex feature into numerical values\nsex_mapping = {\"male\": 0, \"female\": 1}\nTrain['Sex'] = Train['Sex'].map(sex_mapping)\nTest['Sex'] = Test['Sex'].map(sex_mapping)\n\nTrain.head()","78b129cf":"# here we are removing name column because it is not playing important role in doing prediction\nTrain = Train.drop(['Name'], axis = 1)\nTest = Test.drop(['Name'], axis = 1)","2eab4710":"# here we are mapping embarked feature into numerical values\nembarked_mapping = {\"S\":1,\"C\":2,\"Q\":3}\nTrain[\"Embarked\"] = Train[\"Embarked\"].map(embarked_mapping)\nTest[\"Embarked\"] = Test[\"Embarked\"].map(embarked_mapping)","38941936":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(Test[\"Fare\"])):\n    if pd.isnull(Test[\"Fare\"][x]):\n        pclass = Test[\"Pclass\"][x] #Pclass = 3\n        Test[\"Fare\"][x] = round(Train[Train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\nTrain['FareBand'] = pd.qcut(Train['Fare'], 4, labels = [1, 2, 3, 4])\nTest['FareBand'] = pd.qcut(Test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\nTrain = Train.drop(['Fare'], axis = 1)\nTest = Test.drop(['Fare'], axis = 1)","e168ef33":"Train.head()","add38598":"Test.head()","390a9733":"Train.head()","8b3f1145":"Train.head()","430be3ee":"Train = Train.drop([\"Age\"],axis=1)\nTest = Test.drop([\"Age\"],axis=1)","5c64903e":"Train.head()","be758535":"Test.head()","96196798":"x = Train.iloc[:, 2:10].values\ny = Train.iloc[:, 1].values","578c508d":"x","657baa5c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.25,random_state=0)","22cc5867":"print(x_train.shape)\nprint(x_test.shape)","a4537106":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression (solver='liblinear', random_state=0)\nclassifier.fit(x_train,y_train)","1f1f1d06":"y_pred = classifier.predict(x_test)","0d68d79d":"# here we are calculating accuracy rate of our model\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc_lg= accuracy_score(y_test, y_pred)\nacc_lg","a738301a":"from sklearn.neighbors import KNeighborsClassifier\nclassifierr = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                     weights='uniform')\nclassifierr.fit(x_train,y_train)","6726dc2d":"y_pred = classifierr.predict(x_test)","f09f5fd8":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc_knn = accuracy_score(y_test, y_pred)\nacc_knn","2af2b897":"from sklearn.svm import SVC\nclassifier1 = SVC(kernel=\"linear\",random_state=0,C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale',\n    max_iter=-1, probability=True,shrinking=True, tol=0.001,\n    verbose=False)\nclassifier1.fit(x_train,y_train)\n","653ee6c3":"y_pred = classifier1.predict(x_test)","80e53c89":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc_svm = accuracy_score(y_test, y_pred)\nacc_svm","2623bc55":"from sklearn.svm import SVC\nclassifier2 = SVC(kernel=\"rbf\",random_state=0,probability=True)\nclassifier2.fit(x_train,y_train)","678df5ef":"y_pred = classifier2.predict(x_test)","1964de8b":"cm = confusion_matrix(y_test,y_pred)\nprint(cm)\nacc_kernel = accuracy_score(y_test,y_pred)\nacc_kernel","232a47db":"from sklearn.naive_bayes import GaussianNB\nclassifier3 = GaussianNB()\nclassifier3.fit(x_train, y_train)","a109d64a":"y_pred = classifier3.predict(x_test)","ce074ad0":"cm = confusion_matrix(y_test,y_pred)\nprint(cm)\nacc_naive = accuracy_score(y_test,y_pred)\nacc_naive","cb9b2d66":"from sklearn.tree import DecisionTreeClassifier\nclassifier4 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier4.fit(x_train, y_train)","6f74c252":"y_pred = classifier4.predict(x_test)","d20d0a0c":"cm = confusion_matrix(y_test,y_pred)\nprint(cm)\nacc_dt = accuracy_score(y_test,y_pred)\nacc_dt","187b4354":"from sklearn.ensemble import RandomForestClassifier\nclassifier5 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier5.fit(x_train, y_train)","5892f3f7":"y_pred = classifier5.predict(x_test)","e7b22cd1":"cm = confusion_matrix(y_test,y_pred)\nprint(cm)\nacc_rf = accuracy_score(y_test,y_pred)\nacc_rf","8f76b0bc":"models = pd.DataFrame({\n    'Model': [\"LOGISTIC REGRESSION\",\"K NEAREST NEIGHBORS\",\"SUPPORT VECTOR MACHINE\",\"KERNEL SVM\",\"NAIVE BAYES\",\"DECISION TREE\",\"RANDOM FOREST\"],\n    'Score': [acc_lg,acc_knn,acc_svm,acc_kernel,acc_naive,acc_dt,acc_rf\n              ]})\nmodels.sort_values(by='Score', ascending=False)","0856e4f1":"# importing libraries for plotting roc_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","c70d67e8":"classifiers = [LogisticRegression(random_state=0),\n               KNeighborsClassifier(),\n               SVC(random_state=0,probability=True), \n               GaussianNB(), \n               DecisionTreeClassifier(random_state=0),\n               RandomForestClassifier(random_state=0)]\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\nfor cls in classifiers:\n    model = cls.fit(x_train, y_train)\n    yproba = model.predict_proba(x_test)[:,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\nresult_table.set_index('classifiers', inplace=True)\n","3b9d9970":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()\n","3b24b0d7":"#set ids as PassengerId and predict survival \nids = Test['PassengerId']\npredictions = classifier5.predict(Test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\nprint(output)","f07a2f4e":"Here we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be not be correct to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages.","b0e8c362":"1. Importing important libraries\n2. Reading the dataset\n3. Exploratory data analysis\n4. Data Visualization\n5. Data pre-processing\n6. Building Model\n7. Classifications model\n8. Plotting ROC( Receiver Operating Characteristic)curve\n9. Creating submission file\n10. Conclusion","83b4a3c5":"Babies are more likely to survive than any other age group.","3228ba93":"We can see that except for the above mentioned missing values, no NaN values exist.","059248bc":"Tere we have completed our analysis and our prediction . If there is need of any improvement that please feel free to share it.","eef139d6":"Firstly we are importing the libraries which are important to do analysis.","6cbb00bc":"# Data pre-processing...","2086aab1":"Embarked feature","3a0322e7":"We can see that except for the above mentioned missing values, no NaN values exist.","c0f4dbe9":"The plot confirms a person aboarded with more than 2 parents or children are more likely survived whereas a person aboarded alone more likely dead.","8971bb9d":"Sex feature","72f3298e":"RANDOM FOREST","ece72e79":"# CONTENTS","36b42c99":"# Plotting ROC( Receiver Operating Characteristic)curve...","418c8362":"Spliting the dataset","f262cd70":"# Reading the dataset...","350aa9c1":"# Data Visualization...","1954e1d6":"**Age feature**","e8bd5c71":"# Exploratory data analysis...","9dc19204":"SUPPORT VECTOR MACHINE","6818d826":"**Ticket feature**","bf8827ce":"**Age feature**","a4b70e77":"The plot confirms 1st class more likely survivied than other classes whereas\n 3rd class more likely dead than other classes.","05126dcf":"**cabin feature**","bb9b1fd0":"**Embarked feature**","362458e2":"from all the classification model Random Forest classifier is best model with accuracy 85 percent.","0b99543d":"# Importing important libraries...","9882efa0":"K NEAREST NEIGHBORS","b092a90f":"\nThe plot confirms a person aboarded from C slightly more likely survived,a person aboarded from Q more likely dead and a person aboarded from S more likely dead.","66b4b09d":"**Sex mapping**","ee2ec023":"ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u201cideal\u201d point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.","6a832585":"Now we can see that there are 891 rows and 12 columns in training dataset,\nand there are 481 rows and 12 columns in testing dataset.\n","e6184c53":"KERNEL SVM","fb1913f9":"# Creating submission file","451f2ff8":"LOGISTIC REGRESSION","db5d95f1":"DECISION TREE","cf479f59":"**Fare Feature**","d3ac0035":"# INRODUCTION","2724f6e8":"This plot shows that title with mr and master are less survived where as miss and mrs are survived most.","5c6b5a59":"# CONCLUSION","b42b1adb":"here in a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity\/specificity pair corresponding to a particular decision threshold.","43e9d62a":"This plot shows that women are more likley survived then men.","2392b546":"# Building Model...","ee0905e7":"The objective of this analysis is to determine that on which variable of our dataset the survival of titanician are more dependent, how they are been survived,on which basis they have been survived or died, here we are also analysing by use of data Visualization for geeting more clear idea, we are applying different types of classifiaction model for predicting best outcome and at last we are making ROC for analysing that which classification model is best.","e5fec362":"# Classification Models...","100a0823":"\nThe plot confirms a person aboarded with more than 2 siblings or spouse more likely survived\nwhereas a person aboarded without siblings or spouse more likely dead","41444cce":"**Name Feature**","8864b9c4":"NAIVE BAYES","6cfb3d40":"From above it is clear that more no of people are embarking in Southamoton, so we will fill NaN value with Southamonton."}}