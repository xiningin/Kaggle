{"cell_type":{"309d0584":"code","cbcb1105":"code","65bdbc17":"code","4e102216":"code","0a562798":"code","2e39d348":"code","c5e5e101":"code","4b28d383":"code","3f90fdde":"code","ec35b5c0":"markdown","213a5e8a":"markdown","86be4a54":"markdown","59b37e2b":"markdown","bc4eb6d8":"markdown","cbe2d71d":"markdown"},"source":{"309d0584":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntf.enable_eager_execution()\n\nprint(\"TensorFlow version: {}\".format(tf.VERSION))\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))","cbcb1105":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/Iris.csv\")\ndf.Species = df.Species.astype('category').cat.codes\ndf.set_index('Id').to_csv(\"foo.csv\")","65bdbc17":"def parse_csv(line):\n    \"\"\"Parse a single line of a CSV file into constituent features and labels.\"\"\"\n    # Tensors are typed, and TF expects those types at declare time.\n    # You can feed them in using typed examples.\n    # The value you select may also be used as a default in case of NaN values!\n    example_defaults = [[0.], [0.], [0.], [0.], [0.], [0]]\n    \n    # Decoding is linewise.\n    parsed_line = tf.decode_csv(line, example_defaults)\n    \n    # Subselect the features as one tensor.\n    features = tf.reshape(parsed_line[1:-1], shape=(4,))\n    \n    # Subselect the label as a unary tensor.\n    label = tf.reshape(parsed_line[-1], shape=())\n    return features, label\n\n\n# Configure the dataset iterator. Results in a randomized 32-item-per-batch draw of a 1000-item subset of the original dataset.\ntrain_dataset = tf.data.TextLineDataset(\"foo.csv\")\ntrain_dataset = train_dataset.skip(1) \ntrain_dataset = train_dataset.map(parse_csv)\ntrain_dataset = train_dataset.shuffle(buffer_size=1000)\ntrain_dataset = train_dataset.batch(32)\n\n\n# View a single example entry from a batch.\nfeatures, label = iter(train_dataset).next()\nprint(\"example features:\", features[0])","4e102216":"# Create a two-layer ten node dense Keras layer.\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(4,)),  # input shape required\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(3)\n])","0a562798":"def loss(model, x, y):\n    \"\"\"Defines a loss function on a batch of predictions. Uses cross-entropy loss.\"\"\"\n    y_ = model(x)\n    return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)","2e39d348":"optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)","c5e5e101":"x = tf.constant(3.)\nwith tfe.GradientTape() as g:\n    g.watch(x)\n    y = x * x\ngrad = g.gradient(y, [x])\ngrad","4b28d383":"def grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets)\n    return tape.gradient(loss_value, model.variables)","3f90fdde":"train_loss_results = []\ntrain_accuracy_results = []\n\nnum_epochs = 200\n\nfor epoch in range(num_epochs + 1):\n    epoch_loss_avg = tfe.metrics.Mean()\n    epoch_accuracy = tfe.metrics.Accuracy()\n\n    # Training loop. Recall that we are using a batch size of 32.\n    for x, y in train_dataset:\n        # Optimize the model\n        grads = grad(model, x, y)\n        optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n\n        # Track progress\n        epoch_loss_avg(loss(model, x, y))\n        epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n\n    # end epoch\n    train_loss_results.append(epoch_loss_avg.result())\n    train_accuracy_results.append(epoch_accuracy.result())\n\n    if epoch % 50 == 0:\n        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n                                                                    epoch_loss_avg.result(),\n                                                                    epoch_accuracy.result()))","ec35b5c0":"Note: this notebook is a translated version of https:\/\/www.tensorflow.org\/get_started\/eager.\n\nImport Tensorflow and turn on eager execution. Eager execution performs NN tasks right away, instead of waiting for two-step graph initialization and runtime, as in the (notably more awkward) low-level API utilization.","213a5e8a":"Eager execution introduces a gradient tape, or `GradientTape`. This object is a context manager that \"records operations for automatic differentiation\". Variables (persistant tensors) are automatically recorded, whilst regular tensors can be recorded using an explicit `GradientTape.watch` function call.\n\nThe tape is a space for performing automatic differentiation. Graphs defined within the tape gain a `gradient` object method that can be used to take the gradient. This gradient can then be given to an optimizer, which will determine, based on the gradient and other factors, how best to tune the model parameters.\n\nFor example, here is a gradient computation for the function `x * x` at the value 3:","86be4a54":"Training a neural network model means optimizing the kernel (the weights) and the bias. To do so, we must define a loss to minimize, and a gradient function to use to \"seek out\" that loss minimum. In this next code block we define a loss functor.","59b37e2b":"We will define a `grad` method which, given the model inputs and a `loss` factory function, returns the gradient applied to that loss function within that input space, using a fresh gradient tape to do it.","bc4eb6d8":"Here's a configured optimizer.","cbe2d71d":"Next we create a three-layer Keras model. The last layer, the output layer, has as many nodes as classes in this classification problem. The hidden layers can have as many nodes as you would like, and you can have as many hidden layers as you would like. The input layer is not declared explicitly, but instead configured via the `input_shape` parameter on the first hidden layer's declaration, which is required.\n\nThe `Dense` layer is fully interconnected. In other words, every node in a successor layer relies on every node in the previous layer. \n\nTensors passed between nodes in the hidden layers have a functor applied to them. The `Dense` layer uses the following functor by default:\n\n    output = activation(dot(input, kernel) + bias)\n    \nThe `input` is the input tensor. The `kernel` is a group of weights shared over the input space (basically feature importance). The dot product of the kernel and the input is a transformed feature vector. The bias is a linear transform applied to the entire input tensor. The activation function is a function that is applied to the result of the rest of the operations right before the whole thing is handed on down to the next neuron in the set."}}