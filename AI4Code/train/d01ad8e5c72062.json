{"cell_type":{"408b1d11":"code","a5049c59":"code","0f15e225":"code","7a6d9f05":"code","a21ae6dc":"code","15042e7c":"code","c41c7687":"code","a10f13df":"code","03b257d1":"code","46fe9c61":"code","80b66ca8":"code","03181bfc":"code","41e9c4d3":"markdown","b2fcab47":"markdown","545f4edc":"markdown","0c54fad4":"markdown","5e87e905":"markdown","31c35f6f":"markdown","667d6a03":"markdown","44e5ba1c":"markdown","35d96aa6":"markdown","44816bd5":"markdown","5b49cbbd":"markdown","6d123740":"markdown"},"source":{"408b1d11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5049c59":"import pandas as pd\n\nwlcota_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/wcota\/covid19br\/master\/cases-brazil-states.csv\")\n\ndrop_columns = [\"deaths_per_100k_inhabitants\", \"totalCases_per_100k_inhabitants\", \"deathsMS\", \n                \"deaths_by_totalCases\", \"tests_per_100k_inhabitants\", \"totalCasesMS\", \"tests\"]\nwlcota_df = wlcota_df.drop(columns=drop_columns)\nwlcota_df.dropna().head()","0f15e225":"import yaml\n\n# Reading states static information, such as \n# estimated population, size of the state...\nwith open('\/kaggle\/input\/state_pop.yaml') as file:\n    state_pop = yaml.load(file, Loader=yaml.FullLoader)","7a6d9f05":"\n# Creating the correct component values\ncountry_df = wlcota_df.where(wlcota_df[\"state\"] == \"TOTAL\").dropna(subset=[\"state\"]).reset_index().dropna()\nactive_infected = [ country_df[\"totalCases\"].iloc[0] ]\ncountry_df[\"newRecovered\"] = country_df[\"recovered\"].diff()\nfor nc, nd, nr in zip(country_df[\"newCases\"].iloc[1:], \n                      country_df[\"newDeaths\"].iloc[1:], \n                      country_df[\"newRecovered\"].iloc[1:]):\n    active_infected.append(active_infected[-1] + nc - nd - nr)\ncountry_df[\"activeCases\"] = active_infected\n\n# Plotting the correct results components\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    name=\"Active Cases\",\n    x=country_df[\"date\"],\n    y=country_df[\"activeCases\"],\n    mode='lines',\n    line_shape='spline',\n    line=dict(width=3)))\nfig.add_trace(go.Scatter(\n    name=\"Deaths\",\n    x=country_df[\"date\"],\n    y=country_df[\"deaths\"],\n    mode='lines',\n    line_shape='spline'))\nfig.add_trace(go.Scatter(\n    name=\"Recovered\",\n    x=country_df[\"date\"],\n    y=country_df[\"recovered\"],\n    mode='lines',\n    line_shape='spline'))\n\nfig.update_layout(\n    template='xgridoff',\n    xaxis=dict(showgrid=False),\n    xaxis_title='Date',\n    legend_orientation=\"h\", legend=dict(x=0.35, y=1.0),\n    title_text=\"Main SIRD data components for Brazil\")\niplot(fig)","a21ae6dc":"\n# Get the state labels\nstates_list = wlcota_df[\"state\"].unique().tolist()\nstates_list.pop(states_list.index(\"TOTAL\"))\n\n# Plot the recovered data at each state\nfig = go.Figure()\nfor state in states_list:\n    state_df = wlcota_df.where(wlcota_df[\"state\"] == state).dropna()\n    fig.add_trace(go.Scatter(\n        name=state,\n        x=state_df[\"date\"],\n        y=state_df[\"recovered\"],\n        mode='lines',\n        line_shape='spline'))\n    \nfig.update_layout(\n    template='xgridoff',\n    xaxis=dict(showgrid=False),\n    xaxis_title='Date',\n    title_text=\"Recovered data collected for each state\")\niplot(fig)\n","15042e7c":"\nimport numpy as np\n\nfrom datetime import timedelta, datetime\n\n# Create the interpolated dataframe for each state\nstate_df = pd.DataFrame()\nfor state in states_list:\n    # Getting the state data\n    new_df = wlcota_df.where(wlcota_df[\"state\"] == state).dropna()\n    # Tranform the datetime from datetime64 to datetime\n    new_df[\"date\"] = [datetime.fromisoformat(d) for d in new_df[\"date\"].values]\n    # Check if the state has enought data collected\n    if len(new_df) > 5:\n        # Creating the time periods for the interpolation\n        first_date, last_date = new_df[\"date\"].iloc[0], new_df[\"date\"].iloc[-1]\n        day_diff = (last_date - first_date).days\n        date_vector = [first_date + timedelta(days=k) for k in range(day_diff)]\n        date_df = pd.DataFrame(date_vector, columns = ['date'])\n        # Merging with the datetime to interpolate\n        new_df = pd.merge(date_df, new_df, how=\"left\", on=\"date\")\n        # Interpolate (fill) the non existing data\n        new_df = new_df.fillna(method=\"ffill\", axis=0)\n        # Remove the first NaN values\n        new_df = new_df.dropna()\n    \n        # Create the correct components... Mostly here we \n        # create the active cases component...\n        active_infected = [ new_df[\"totalCases\"].iloc[0] ]\n        for nc, nd, nr in zip(new_df[\"totalCases\"].diff().iloc[1:],\n                              new_df[\"deaths\"].diff().iloc[1:],\n                              new_df[\"recovered\"].diff().iloc[1:]):\n            active_infected.append(active_infected[-1] + nc - nd - nr)\n        new_df[\"activeCases\"] = active_infected \n        \n        # Computing the susceptible component\n        pop = state_pop[state][\"population\"]\n        new_df[\"susceptible\"] = pop - new_df[\"recovered\"] - new_df[\"activeCases\"] - new_df[\"deaths\"]\n        \n        # Include this state in the general dataframe\n        state_df = pd.concat((state_df, new_df.dropna()))\n\n\nat_state = \"RJ\"\ncomponent_name = \"recovered\"\ncomp = state_df.where(state_df[\"state\"] == at_state).dropna()\ndate = comp[\"date\"].tolist()\ncomp = comp[component_name].tolist()\n\nreliable_date = np.array(date)[[True] + (np.diff(comp) != 0).tolist()]\nreliable_points = np.array(comp)[[True] + (np.diff(comp) != 0).tolist()]\n\n\n# Show the reliable data points\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=state_df[\"date\"].where(state_df[\"state\"]==at_state).dropna(), \n    y=state_df[component_name].where(state_df[\"state\"]==at_state).dropna(), \n    name=\"Component :{}: data\".format(component_name)))\nfig.add_trace(go.Scatter(\n    mode='markers',\n    x=reliable_date, \n    y=reliable_points, \n    name=\"Reliable points\"))\n\nfig.update_layout(template='xgridoff',\n                  xaxis_title='Date',\n                  legend_orientation=\"h\", legend=dict(x=0.35, y=1.0),\n                  title_text=\"Reliable points of the {} component\".format(component_name))\niplot(fig)\n","c41c7687":"# Getting the prediction data and historical\ndf = pd.read_csv(\"\/kaggle\/input\/predictions.csv\")\n\nfig = go.Figure()\n\nvisibles = [\"SP\", \"MG\", \"RS\", \"BA\", \"MA\"]\n\nfor state in df[\"state\"].unique():\n    if state != \"SC\" and state != \"SE\" and state != \"TO\":\n        \n        s_df = df.where(df[\"state\"]==state).dropna()\n        fig.add_trace(go.Scatter3d(\n            legendgroup=state,\n            name=state,\n            visible = True if state in visibles else \"legendonly\", \n            x=s_df[\"date\"], \n            y=s_df[\"state\"], \n            z=s_df[\"active_cases\"]\/s_df[\"active_cases\"].max(),\n            mode=\"lines\",\n            line=dict(width=6)))\n        \n        s_ddf = state_df.where(state_df[\"state\"]==state).dropna()\n        fig.add_trace(go.Scatter3d(\n            legendgroup=state,\n            showlegend=False,\n            visible = True if state in visibles else \"legendonly\", \n            name=state,\n            x=s_ddf[\"date\"], \n            y=s_ddf[\"state\"], \n            z=s_ddf[\"activeCases\"]\/s_df[\"active_cases\"].max(),\n            mode=\"markers\",\n            marker=dict(size=3)))\n    \nfig.update_layout(\n    template='xgridoff',\n    title_text=\"Active cases predictions for each state\",\n    xaxis_title='Date', \n    yaxis_title='States',\n    scene_camera=dict(\n        eye=dict(x=-1.1, y=1.0, z=0.3)\n    ),\n    scene=dict(\n        aspectratio = dict(x=1.3,y=1.0,z=0.4),\n        aspectmode = 'manual',\n        xaxis=dict(showgrid=False),\n        yaxis=dict(showgrid=False),\n        zaxis=dict(showgrid=False)\n    ))\n\niplot(fig)","a10f13df":"\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    name=\"Data - Recovered\",\n    x=state_df[\"date\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    y=state_df[\"recovered\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    mode=\"markers\",\n    marker=dict(size=6, color=\"#005005\")))\nfig.add_trace(go.Scatter(\n    name=\"Data - Active cases\",\n    x=state_df[\"date\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    y=state_df[\"activeCases\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    mode=\"markers\",\n    marker=dict(size=6, color=\"#38006b\")))\nfig.add_trace(go.Scatter(\n    name=\"Data - Deaths\",\n    x=state_df[\"date\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    y=state_df[\"deaths\"].where(state_df[\"state\"] == \"MA\").dropna(),\n    mode=\"markers\",\n    marker=dict(size=6, color=\"#ab000d\")))\n\n# Reading the model parameters\npars_df = pd.read_csv(\"\/kaggle\/input\/state_parameters.csv\")\n\n# Computing the initial population proportion\npop_prop = pars_df[\"pop\"].where(pars_df[\"state\"]==\"MA\").dropna().values\npop = state_pop[\"MA\"][\"population\"] * pop_prop\n\n# Getting the correct model components\nactive_cases = df[\"active_cases\"].where(df[\"state\"] == \"MA\").dropna()\nrecovered = df[\"recovered\"].where(df[\"state\"] == \"MA\").dropna()\ndeaths = df[\"deaths\"].where(df[\"state\"] == \"MA\").dropna()\nsusceptible = pop - active_cases - recovered - deaths\n\n# Getting the data vector\ndates = df[\"date\"].where(df[\"state\"] == \"MA\").dropna()\n\nfig.add_trace(go.Scatter(\n    name=\"Model - Suceptible\",\n    x=dates,\n    y=susceptible,\n    mode=\"lines\",\n    line=dict(width=3, dash=\"dash\", color=\"#0288d1\")))\nfig.add_trace(go.Scatter(\n    name=\"Model - Recovered\",\n    x=dates,\n    y=recovered,\n    mode=\"lines\",\n    line=dict(width=3, dash=\"dash\", color=\"#2e7d32\")))\nfig.add_trace(go.Scatter(\n    name=\"Model - Active cases\",\n    x=dates,\n    y=active_cases,\n    mode=\"lines\",\n    line=dict(width=3, dash=\"dash\", color=\"#6a1b9a\")))\nfig.add_trace(go.Scatter(\n    name=\"Model - Deaths\",\n    x=dates,\n    y=deaths,\n    mode=\"lines\",\n    line=dict(width=3, dash=\"dash\", color=\"#e53935\")))\n\n\nfig.update_layout(\n    template='xgridoff',\n    title_text=\"Model prediction on Maranh\u00e3o Brazil\",\n    xaxis_title='Date')\n\niplot(fig)","03b257d1":"\nimport json\n\nwith open('\/kaggle\/input\/brasil_estados.geojson') as handle:\n    states_geo = json.load(handle)\n    \npars_df = pd.read_csv(\"\/kaggle\/input\/state_parameters.csv\")\n\nfig = go.Figure(go.Choroplethmapbox(geojson=states_geo, \n                                    locations=pars_df[\"state\"], \n                                    z=pars_df[\"pop\"],\n                                    colorscale=\"Viridis\", \n                                    marker_opacity=0.5, \n                                    marker_line_width=0, \n                                    featureidkey=\"properties.uf_05\"))\nfig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=2.5, mapbox_center = {\"lat\": -15.7801, \"lon\": -47.9292})\nfig.update_layout(margin={\"r\":0,\"l\":0,\"b\":0}, title_text=\"Proportion of population to attend health care systems\")\niplot(fig)","46fe9c61":"from plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=3,\n                   subplot_titles=(\"Average days to recover\", \"Average Ro\", \"Average dailly mortality rate\"))\n\nfig.add_trace(go.Scatter(\n    name=\"Days to recover\",\n    x=pars_df.sort_values(by=\"D\")[\"state\"],\n    y=pars_df.sort_values(by=\"D\")[\"D\"],\n    line=dict(width=3)\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    name=\"One person infects...\",\n    x=pars_df.sort_values(by=\"ro\")[\"state\"],\n    y=pars_df.sort_values(by=\"ro\")[\"ro\"],\n    line=dict(width=3)\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    name=\"Mortality rate\",\n    x=pars_df.sort_values(by=\"mu\")[\"state\"],\n    y=pars_df.sort_values(by=\"mu\")[\"mu\"],\n    line=dict(width=3)\n), row=1, col=3)\n\nfig.update_xaxes(title_text=\"States\", row=1, col=1)\nfig.update_xaxes(title_text=\"States\", row=1, col=2)\nfig.update_xaxes(title_text=\"States\", row=1, col=3)\n\n# fig.update_yaxes(title_text=\"Days to recover\", row=1, col=1)\n# fig.update_yaxes(title_text=\"Number of infections for each infected\", row=2, col=1)\n# fig.update_yaxes(title_text=\"Rate\", row=3, col=1)\n\n# Update title and height\nfig.update_layout(title_text=\"Analysing the parameters information\", template='xgridoff', showlegend=False)\n\n\niplot(fig)","80b66ca8":"cities_loc_df = pd.read_csv(\"\/kaggle\/input\/cities_locations.csv\")\ncities_df = pd.read_csv(\"\/kaggle\/input\/cities_start_day.csv\")\ncities_df = pd.merge(cities_df, cities_loc_df[[\"city\",\"state\",\"country\",\"lat\",\"long\"]], how=\"left\", on=[\"city\",\"state\"]).dropna()\n\nfstate_df = pd.DataFrame()\n\nfor start_at in cities_df[\"time_from_start\"].unique():\n    new_df = cities_df.where(cities_df[\"time_from_start\"] <= start_at).dropna()\n    new_df[\"time_from_start\"] = start_at\n    fstate_df = pd.concat((fstate_df, new_df))\n\nfor state in cities_df[\"state\"].unique():\n    new_df = pd.DataFrame({\"state\": [state], \"time_from_start\": [0.0]})\n    fstate_df = pd.concat((fstate_df, new_df))\n    \nfstate_df = fstate_df.sort_values(by=\"time_from_start\")\nfstate_df.head()\n\n\nimport plotly.express as px\n\nfig = px.scatter_geo(fstate_df, \n                     lat=\"lat\", \n                     lon=\"long\",\n                     color=fstate_df[\"state\"],\n                     size_max=2,\n                     animation_frame=\"time_from_start\",\n                     animation_group=\"city\",\n                     scope='south america',\n                     projection='natural earth')\n\nfig.update_layout(template='xgridoff', margin={\"r\":0,\"l\":0,\"b\":0}, title_text=\"Cities contamination on time\")\niplot(fig)","03181bfc":"from geopy import distance\nfrom geopy.point import Point\n\naero_df = pd.read_csv(\"\/kaggle\/input\/aerodromos-publicos.csv\")\naero_df[\"COMPRIMENTO\"] = aero_df[\"COMPRIMENTO\"].apply(lambda c: int(c[:-2]))\naero_df[\"ALTITUDE\"] = aero_df[\"ALTITUDE\"].apply(lambda c: float(c[:-2].replace(\",\",\".\")))\naero_df[\"LARGURA\"] = aero_df[\"LARGURA\"].apply(lambda c: float(c[:-2].replace(\",\",\".\")))\naero_df[\"NOME\"] = aero_df[\"NOME\"].apply(lambda c: c.lower())\naero_df[\"POSITION\"] = aero_df[\"LATITUDE\"] + \" \" + aero_df[\"LONGITUDE\"]\naero_df[\"POSITION\"] = aero_df[\"POSITION\"].apply(lambda p: Point.from_string(p))\naero_df[\"LATITUDE\"] = aero_df[\"POSITION\"].apply(lambda p: p.latitude)\naero_df[\"LONGITUDE\"] = aero_df[\"POSITION\"].apply(lambda p: p.longitude)\n\n# Selecting the aiport type\naero_df = aero_df.where( (aero_df[\"SUPERF\u00cdCIE\"] == 'Asfalto') | (aero_df[\"SUPERF\u00cdCIE\"] == 'Concreto') ).dropna()\naero_df = aero_df.where( aero_df[\"LARGURA\"] >= 30.0 ).dropna()\n\naero_df = aero_df[[\"NOME\", \"MUNIC\u00cdPIO ATENDIDO\", \"UF\", \"LATITUDE\", \"LONGITUDE\"]]\n\n# Computin the ditance of nearest aiportorts\n\nmin_distance = []\ncities_list, states_list = cities_df[\"city\"].tolist(), cities_df[\"state\"].tolist()\ncity_lat, city_lon = cities_df[\"lat\"].tolist(), cities_df[\"long\"].tolist()\naero_lat, aero_lon = aero_df[\"LATITUDE\"].tolist(), aero_df[\"LONGITUDE\"].tolist()\nfor city, state, lat, lon in zip(cities_list, states_list, city_lat, city_lon):\n    \n    state_aero_df = aero_df.where(aero_df[\"UF\"] == state).dropna()\n    aero_lat, aero_lon = state_aero_df[\"LATITUDE\"].tolist(), state_aero_df[\"LONGITUDE\"].tolist()\n    aero_pos = zip(aero_lat, aero_lon)\n    \n    dists = [distance.distance((lat,lon),(a_lat, a_lon)).km for a_lat, a_lon in aero_pos]\n    \n    min_distance.append(min(dists))\n    \ncities_dist_df = pd.DataFrame({\n    \"city\": cities_list, \n    \"state\": states_list,\n    \"min_dist\": min_distance,\n    \"city_lat\": city_lat,\n    \"city_lon\": city_lon\n})    \n\ncities_dist_df[\"Accessibility\"] = cities_dist_df[\"min_dist\"].apply(lambda d: \"by road\" if d >= 40.0 else \"by airport\")\ncities_df = pd.merge(cities_df, cities_dist_df[[\"city\", \"Accessibility\", \"min_dist\"]], how='left', on=\"city\")\n\nfstate_df = pd.DataFrame()\n\nfor start_at in cities_df[\"time_from_start\"].unique():\n    new_df = cities_df.where(cities_df[\"time_from_start\"] <= start_at).dropna()\n    new_df[\"time_from_start\"] = start_at\n    fstate_df = pd.concat((fstate_df, new_df))\n\nfor state in cities_df[\"Accessibility\"].unique():\n    new_df = pd.DataFrame({\"Accessibility\": [state], \"time_from_start\": [0.0]})\n    fstate_df = pd.concat((fstate_df, new_df))\n    \nfstate_df = fstate_df.sort_values(by=\"time_from_start\")\n\nfig = px.scatter_geo(fstate_df, \n                     lat=\"lat\", \n                     lon=\"long\",\n                     color=fstate_df[\"Accessibility\"],\n                     size_max=2,\n                     animation_frame=\"time_from_start\",\n                     animation_group=\"city\",\n                     scope='south america',\n                     projection='natural earth')\n\nfig.update_layout(template='xgridoff', margin={\"r\":0,\"l\":0,\"b\":0}, title_text=\"Cities contamination on time - Airport and road cities\")\niplot(fig)","41e9c4d3":"# Heuristic learning - Learning each Brazil state\n\nThe main idea of this section is to present a powerfull algorithm capable of using the covid data of each state from Brazil to predict the future values of the main epidemic components: the amount of infected, recovered and dead people on time. \n\n> Here we present out particular developement based on heuristic search algorithms to find the biological parameters that guide the covid epidemic, to then, not only predict the behavior of the epidemic for each state, but also analyse the parameter to extract meaningfull information from the model.\n\n## Collecting the best Brazil dataset\n\nThe most reliable dataset in Brazil was built by:\n\n- [Wesly Cota](https:\/\/github.com\/wcota\/covid19br)\n\nAt this database we can found the most complete informations going from cities to the country point of view. Mostly we have the information of the so called  SIR components variations, _i.e._ the dailly new deaths, new recovered and new cases. For eacmple, using the city dataset, we have the following information:","b2fcab47":"From the figure above, we can see that we have a point considered as the initial of the epidemic and the last weeks collected data as the reliable points for learning. So, using only those points we could adjust the parameters for the dynamical model responsible for predicting the behavior of the epidemy for this state, using our heuristic learning approach.\n\n\n## Training the SIR model using genetic optimization\n\nHere we are using our algorithm developed in the EpidemicModels project to search for the parameters of the SIRD model using the povided components. If the reader want to check out the developed content, please checkout the projects [github](https:\/\/github.com\/lafetamarcelo\/epidemicModels) or the [documentation](https:\/\/epidemicmodels.readthedocs.io\/en\/latest\/). Soo we can find the parameters for each state and check how the model can predict the next 120 days of the epidemy:","545f4edc":"This is actually the city level database from [Wesly Cota](https:\/\/github.com\/wcota\/covid19br). We can then create the default components of an epidemic __SIRD__ model _i.e._ the **S**uceptible (people that can be reached by the desease), the **I**nfected (or the dailly active cases of the desease), the **R**ecovered (accumulated values of people that recovered from the desease), and the **D**eaths (also accumulated value of the deaths). \n\n> For those not familliar with the SIR* models structures, those are dynamical models (differential equations) that represent the time behavior of the epidemy. Notice that we said, SIR famillies, because there are several structures that models different components of an epidemy. For example, here we will use the SIRD models, because it is the best familly for the covid epidemy, since it models suceptible, infected, recovered and dead people. Other such as the SEIR models, represent the suceptible, exposed, infected and recovered, which is not the major state sequency of covid (the exposed component if not actually the exposed people, but actually the people that has the infection but are not in a distramition phase... would be better represented by the word \"incubated\").\n\nBut nevertheless... Country wise we have the following pattern for each component:","0c54fad4":"We can also break down each component into their state portions, to build a more focus model, since Brazil is a pretty big country. It is probable that severeal different dynamic behaviors will present themselves for each state in Brazil. As an example, lets checkout the recovered data for each state, as an example.","5e87e905":"### Understanding the model parameters\n\nAs saw above, we can extract several informations of the phenomenon using the estimated model. In the figure above we provided the prediction of the health care usage by state. We could also determine the efficency of the health care system for each state... this can be achieved by analysing the parameter of the SIRD model that defines the time (days) that a person takes to go from the infected state to the recovered. Also we could map the so called $R_0$ parameter, that shows the average number of people that an infected individual infects during its infectious period. ","31c35f6f":"![5658_6CB5A4A43AB318D2.jpg](attachment:5658_6CB5A4A43AB318D2.jpg)","667d6a03":"This prediction provide a lot of meaninful informations. For example, in Maranh\u00e3o, there was one of the most extrict isolation regimes in Brazil, and as a consequence of that, we can see easily see the SIRD dinamical pattern on the data. Also the model does not have any problem in finding the parameters that fit this data... so if all other states had the same discipline as Maranh\u00e3o, probably we could trust even more the prediction models to define governamental decision, such as when the lock down should finish. And using the parameters of the model we could go even further, by also predicting how many people would attend to the health care system.\n\nTo see this second information more clearlly, we need to discuss two major topics. The first is the reason why our algorithm is better than most on predicting and learning the epidemic pattern on the data. And the second is related to the phenomenological definition of the model parameters, provided that each parameter has a specific role in the dinamical model. \n\n### Why our model is better...?\n\nA person familiarized with the SIR model families, when analysing the above figure, would be very uncomfortable. The reason is that the suceptible component from the model, which should describe the number of people suceptible to the deasease, does not start with the value of the total population of the state. **Usually people assume that the amount of initial suceptible people is the number of people of the region... that is nothing wrong about that. But in the case of the COVID data, this is a pretty big problem.** The COVID is very particular in the fact that it only affects heavly a small proportion of the people, but it has a very big spreadiness capability. Now, notice that the data that we have, is actually the registries of people that attend to hospitals or clinics, but not the total amount of infected people. We know that a big part of the population will not attend the health care system, and will never be cataloged.\n\n> **We included in our model a feature, which is actually a pondering parameter, that will learn the proportion of the total population who actually should be considered as the suceptible group. Interesting enough, without this feature it is not possible to find the SIR model parameters, without including bias in at least one of the dynamical parameters of the model. If you want to test your luck, and think otherwise, [you can check out out little game](), where we developed a platform where you can handly define the model parameters and try to fit the model to the data, without and with our feature... and see for yourself if it is possible to fit the model without it.**\n\nThis feature actually provides more information that initially we imagined. Looking at the figure above, we can see that $S(t) \\rightarrow 0$ when $t \\rightarrow \\infty$. So, if the value of suceptible tends to zero at the final of the epidemy, this means that all suceptible people, became infected, and then or recovered, or died. In other words, the initial value of suceptible, will be the total number of people that will be infected during the epidemic process. But, since the data collected is actually only the information of the people who attend the health care system, we actually have the number of people that will attend to the health care system.\n\n> **Concluding this feature measures the amount of people who will use the health care system of the state.**\n\nIn the next figure we present the information of the proportion of the population, of each state that our model says that will use the health care system.","44e5ba1c":"# Spreadiness analysis\n---\n\nOne most concearning characteristic of the COVID is its spreadiness velocity. This brings the curiosity of undestanding how the desease spread accross a country, and check if governmental measures really were effective, or the neglicence ruled during the epidemy period. For that, we can track each first case in Brazil along time:","35d96aa6":"## Who we are...\n\nHey there, we are reasearchers of the Intituto Mau\u00e1 de Tecnologia, from Brazil. In this project we present our heuristic learning based project focused on learning the main parameters of epidemic models that fits the covid data for each state of the country. Showing from the preprocessing of the data until the prediction of each state infected, **with our particular approach to make the algorithm to have real prediction capabilities**. If you want a more detailed content please check out our project documentation and GitHub repository:\n\n- [Documentation](https:\/\/epidemicmodels.readthedocs.io\/)\n- [GitHub](https:\/\/github.com\/lafetamarcelo\/epidemicModels)\n\nby [_Marcelo Lima_](https:\/\/github.com\/lafetamarcelo\/), [_Felipe Ippolito_](https:\/\/github.com\/feippolito) and [_Vanderlei Cunha Parro_](https:\/\/github.com\/vparro).","44816bd5":"Notice, that most states do not have a continuous collection during the epidemic (a common phenomenon in Brazil... usually most datasets are filled with gaps, or biased as you can see in the SP state, for example). Therefore **we cannot fully trust the dataset... but we can create some algorithms to map only the informative collection of points of each state, and then we will only use those points to train our model!** \n\n## Preprocessing the components time series\n\nThis can be done using a derivative based technique, that will track the derivative of a sequence of points, and it will only consider informative the points whose derivative has some absolute value bigger than a particular thershold. And also, since we have some periods with no data collected (the series gaps), we can fill the void space with contant values... This is a pretty interesting approach because due to the derivative technique to find informative points, the ones filled with contants will present themselves as non informative to the algorithm, since they are all contants and therefore, does not have any variation.\n\nFor example, lets check the RJ (Rio de Janeiro) state. Notice that for Rio, we have a pretty gap of data on the recovered component. It is something like, from Apr 4, 2020, until Jun 5, 2020, we do not have data on how the epidemy progresses. There fore, we could use the above approach to see what points we could get as informative ones:","5b49cbbd":"Notice that the algorithm is quite able to predict the amount of infected people through time. Also an interesting aspect of these results is that all states have a pretty different dynamical behavior. This is expected in a country with the dimensions such as the Brazil's. Each state has its own culture and reacted different regarding the epidemic outbreak. As an example, one of the most strict lockdowns in Brazil happened in the MA (Maranh\u00e3o) state, which is the one that we can see the most characteristical behavior of the SIRD model.\n\nFor example, lets check out the dynamical behavior of each component at Maranh\u00e3o:","6d123740":"## Spreadiness on airport and road accessible cities\n\nFor example, using this information we could track the first case of each city that is accessible through airport and that are only accessible by road. From that we could verify if the airport safety measures were effective during the epidemy period. In Brazil we suspect that only after airport cities saturation (all cities accessible by airport beeing infected) that the cities accessible by road started to be the new infected points. Therefore, or the airport measures were not effective or they were not respect, we can see that in the next plot."}}