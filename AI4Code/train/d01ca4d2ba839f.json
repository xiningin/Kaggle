{"cell_type":{"f59cd308":"code","1ba71e80":"code","5ae0c52d":"code","3cb44550":"code","ffec34a1":"code","658f7071":"code","bedf60fd":"code","3b1f968d":"code","5ec1f579":"code","c808af20":"code","3e1ad971":"code","0a439383":"code","82a336f0":"code","58a4e199":"code","e5292cbf":"code","bea922f0":"code","7ee8c7cb":"code","fc991cec":"code","f24d6173":"code","d63f6073":"code","a0e021ef":"code","61c83ca5":"code","0f10987b":"code","d41de420":"code","92710389":"code","12264d05":"code","6be4225c":"code","c6673e6b":"code","e9cc1305":"code","380d774c":"code","dbd0e0b8":"code","5a7eab0a":"code","1791a00e":"code","5cabf773":"code","e6ede39a":"code","ef0e614f":"code","5a6ab9d5":"code","6ee89fef":"code","51e71aeb":"markdown","985e445a":"markdown","06bb3a3a":"markdown","d80a2396":"markdown","9ce974f5":"markdown","42c290e1":"markdown","5e4de03e":"markdown","cba5a929":"markdown","0dda1401":"markdown","6ed1a79b":"markdown","5ac6a300":"markdown","ac2c67e9":"markdown","f0053824":"markdown","d34fae18":"markdown","d2a4e193":"markdown","b2f443cd":"markdown","7c42a5db":"markdown","8973239b":"markdown"},"source":{"f59cd308":"#The baseline modules\nimport numpy as np\nimport pandas as pd\n\n#For text cleaning\nimport spacy\n\n#For plotting\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#Model packages\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#Pipeline, Vectorizers and accuracy metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","1ba71e80":"df = pd.read_csv('..\/input\/reddit-india-flair-detection\/datafinal.csv', index_col='Unnamed: 0')\ndf.head()","5ae0c52d":"df.isnull().sum().any()","3cb44550":"df.isnull().sum()","ffec34a1":"msno.matrix(df)\nplt.show()","658f7071":"df.columns","bedf60fd":"df.drop(['score','url','comms_num','author','timestamp'], axis=1, inplace=True)\ndf.head()","3b1f968d":"df['title'][0]","5ec1f579":"df['body'][0]","c808af20":"df['comments'][0]","3e1ad971":"df['combined_features'][0]","0a439383":"df.drop(['combined_features'], axis=1, inplace=True)\ndf.head()","82a336f0":"df.info()","58a4e199":"df.describe()","e5292cbf":"df['flair'].unique()","bea922f0":"df.groupby('flair')['title'].describe()","7ee8c7cb":"fla_df = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['title'].describe()['freq']})\n\nfig = px.bar(fla_df, x='Flair', y='Number', title='Flair Counts by Title in r\/india')\nfig.show()","fc991cec":"fla_df_1 = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['body'].describe()['freq']})\n\nfig = px.bar(fla_df_1, x='Flair', y='Number', title='Flair Counts by body in r\/india')\nfig.show()","f24d6173":"fla_df_2 = pd.DataFrame({\"Flair\":df['flair'].unique(), \"Number\":df.groupby('flair')['comments'].describe()['freq']})\n\nfig = px.bar(fla_df_2, x='Flair', y='Number', title='Flair Counts by comments in r\/india')\nfig.show()","d63f6073":"df[df['flair'] == np.nan].describe()","a0e021ef":"df.dropna(subset=['flair'], inplace=True)","61c83ca5":"df.dtypes","0f10987b":"df['text'] = df['title'].astype(str) + df['body'].astype(str) + df['comments'].astype(str)\ndf.drop(['title', 'body', 'comments'], axis=1, inplace=True)\ndf.head()","d41de420":"nlp = spacy.load('en')\n\ndef normalize(msg):\n    \n    doc = nlp(msg)\n    res=[]\n    \n    for token in doc:\n        if(token.is_stop or token.is_punct or not(token.is_oov)): #Removing stopwords punctuations and words out of vocab\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    \n    return \" \".join(res)","92710389":"df['text'] = df['text'].apply(normalize)\ndf.head()","12264d05":"c = TfidfVectorizer() # Convert our strings to numerical values\nmat=pd.DataFrame(c.fit_transform(df[\"text\"]).toarray(),columns=c.get_feature_names(),index=None)\nmat","6be4225c":"X = mat\ny = df[\"flair\"]","c6673e6b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","e9cc1305":"pipeline = Pipeline([\n    ('classifier',DecisionTreeClassifier()),\n    ])\n\npipeline.fit(X_train, y_train)","380d774c":"y_pred = pipeline.predict(X_test)","dbd0e0b8":"print(confusion_matrix(y_test, y_pred))","5a7eab0a":"print(classification_report(y_test, y_pred))","1791a00e":"print(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","5cabf773":"X_train, X_test, y_train, y_test = train_test_split(df['text'], df['flair'], test_size = 0.2, random_state = 0)","e6ede39a":"ids = [df.iloc[int(i)]['id'] for i in X_test.index]\nfinal_df = pd.DataFrame({\"ID\":ids, \"Text\":X_test, \"Flair\":y_pred}).reset_index()\n\nfinal_df.head()","ef0e614f":"final_df.to_csv('.\/test.csv')","5a6ab9d5":"'''classifiers = {\n    'mnb': MultinomialNB(),\n    'gnb': GaussianNB(),\n    'svm1': SVC(kernel='linear'),\n    'svm2': SVC(kernel='rbf'),\n    'svm3': SVC(kernel='sigmoid'),\n    'mlp1': MLPClassifier(),\n    'mlp2': MLPClassifier(hidden_layer_sizes=[100,100]),\n    'ada': AdaBoostClassifier(),\n    'dtc': DecisionTreeClassifier(),\n    'rfc': RandomForestClassifier(),\n    'gbc': GradientBoostingClassifier(),\n    'lr': LogisticRegression()\n}'''","6ee89fef":"'''acc_scores = dict()\nfor classifier in classifiers:\n    pipeline = Pipeline([\n    ('classifier',classifiers[classifier]),\n    ])\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    acc_scores[classifier] = accuracy_score(y_test, y_pred)\n    print(classifier, acc_scores[classifier])'''","51e71aeb":"# Finding Best Classifier\n\nThese cells are used to find which classifier is the best. Takes a VERY long time","985e445a":"We can combine title, body and comments into a single column called text ","06bb3a3a":"Gradient Boosting Classifier takes the crown with a 81.14% accuracy but since it takes too long we'll go with Decision Tree Classifier","d80a2396":"# Normalisation","9ce974f5":"## Removing uneccessary columns","42c290e1":"# Model Training and Prediction","5e4de03e":"Well will you look at that. The combined features column is just the combo of title, body and comments. So, that's safe to drop as well","cba5a929":"**Time to explore the flairs cause that's our target to predict**","0dda1401":"# The toolkit","6ed1a79b":"Things that can be removed:\n\n* score - Karma(basically the number of upvotes a post gets) doesn't contribute in figuring out a flair\n* url - We can't do much off the url of the post\n* comms_num - Number of comments isn't that important\n* timestamp - Timestamp cannot factor into predicting the flair \n* author - We can't base the flair based on who writes it","5ac6a300":"How many flairs are present?","ac2c67e9":"# Null values","f0053824":"Now to clean the text","d34fae18":"# Final Model","d2a4e193":"## Ah, Reddit\n\nThe front page of the internet as exclaimed by itself and rightly so. You can find posts about anything and everything over there. Every community in reddit is known as a \"subreddit\" and users there are called \"redditors\"\n\n## About r\/india\n\nThis is the official subreddit of everything about India. \n\n## Flairs\n\nThese are something which can be best defined as subtopics in a subreddit that are set by the moderators of the subreddit. I will slowly show what they are in this notebook.\n","b2f443cd":"# Final Output","7c42a5db":"# Cleaning Data","8973239b":"Same thing can be followed for body and comments"}}