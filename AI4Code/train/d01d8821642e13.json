{"cell_type":{"d072c697":"code","652578a5":"code","ffcaf455":"code","77209c02":"code","b78e1684":"code","7aff94c3":"code","af550412":"code","73e8cd8f":"code","fa54e265":"code","ede02072":"code","20525128":"code","db6cc052":"code","08abc5f7":"code","8c50a04f":"code","8f8ad6ad":"code","cbf92354":"code","14f25ed9":"code","42650b7a":"code","eff9f769":"code","8b03d3f9":"code","140264bc":"code","c6c6ac64":"code","42165bfb":"code","8493ac1e":"code","8f68f233":"code","dcf3bf2c":"code","b72cfc09":"code","9520767d":"code","d3810dd7":"code","fb70df0c":"code","ca0b75fa":"code","2b335f39":"code","4b56f0e0":"code","2c44119c":"code","c99f0c91":"code","1c001b29":"code","79c3f87b":"code","a7d665dd":"code","7e4ed6f1":"code","5fec44af":"code","b99267ce":"code","1edfa61d":"code","3fd54beb":"code","f4c380db":"code","c8caca2e":"code","4a4fc946":"code","a67f1e85":"code","ad710bf9":"code","beaa0443":"code","31f4925d":"code","821b3177":"code","5500a910":"code","4c757b21":"code","4a562e5a":"code","b37d4a9c":"code","2201c21b":"code","31116c0d":"markdown","34260956":"markdown","794b64b8":"markdown","83fec2de":"markdown","6778ce13":"markdown","a62bae27":"markdown","fc6a2e89":"markdown","c69c9dc7":"markdown","81275b24":"markdown","96f3155b":"markdown","9d57bc12":"markdown","8b99cb69":"markdown","3ce44bf8":"markdown","e5be0e3b":"markdown","40279b52":"markdown","f9e3fb28":"markdown","72e94cee":"markdown","0d2b3fcc":"markdown","d6c48247":"markdown","508afdc6":"markdown","f0cef8a6":"markdown","5ae5ba61":"markdown","fcfa9480":"markdown","9d7a3af2":"markdown","67cc50f7":"markdown","a544d446":"markdown","90952759":"markdown","f5b0225a":"markdown","05e63b75":"markdown","a9451bc4":"markdown","089ae232":"markdown"},"source":{"d072c697":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","652578a5":"# load train and test data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ffcaf455":"# view the head\ntrain.head()","77209c02":"train.tail()","b78e1684":"train.columns","7aff94c3":"print(\"There are {} rows in the training data\".format(len(train)))\nprint(\"There are {} rows in the test data\".format(len(test)))","af550412":"train['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([train,test])","73e8cd8f":"train.info()","fa54e265":"train.describe()","ede02072":"train.sample(10)","20525128":"train.isna().sum()","db6cc052":"# separate numerical and categorical data because the way we visualize them will be different. \ntrain_num = train[['Age','SibSp','Parch','Fare']]\ntrain_cat = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","08abc5f7":"train_num.hist(figsize=[14,14], grid=False)","8c50a04f":"# let's count the percentage of passengers with the age between 20-30\ncount_2030 = (train_num['Age'][(train_num['Age'] <= 30) & (train_num['Age'] >= 20)].count())\/(train_num['Age'].count())\nprint(count_2030*100)\nprint(\"There are {:.2f} percent of people who were aged 20-30\".format(count_2030*100))\n# do the same for other 3 numerical features\ncount_zerosib = (train_num['SibSp'][(train_num['SibSp'] == 0)].count())\/(train_num['Age'].count())\nprint(\"There are {:.2f} percent of people who had no siblings and spouse accompanying them\".format(count_zerosib*100))\n\ncount_mostfare = (train_num['Fare'][(train_num['Fare'] <= 100) & (train_num['Fare'] >= 0)].count())\/(train_num['Fare'].count())\nprint(\"There are {:.2f} percent of people who paid between 0 and 100 pounds\".format(count_mostfare*100))\n\ncount_mostparch = (train_num['Parch'][(train_num['Parch'] == 0)].count())\/(train_num['Parch'].count())\nprint(\"There are {:.2f} percent of people who had no parents and children accompanying them\".format(count_mostparch*100))","8f8ad6ad":"print(\"There were {} males and {} females in the cabin.\".format(train_cat['Sex'].value_counts().male, train_cat['Sex'].value_counts().female))","cbf92354":"for j in train_cat.columns:\n    plt.figure()\n    ax = train_cat[j].value_counts().plot(kind='bar', title=j)\n    ax.set_ylabel('Count')\n    ax.set_xlabel('Labels')","14f25ed9":"# We don't need the ticket and cabin features yet.\ntrain_cat_plot = train_cat.drop(['Ticket', 'Cabin'], axis=1)\nfig = plt.figure(figsize=[10,10])\n# make plots using all other features\nfor i, j in enumerate(train_cat_plot.columns):\n    fig.add_subplot(2,2, i+1)\n    sns.barplot(x=j, y='Survived', data=train_cat_plot)\n    plt.title('{} vs Survived Plot'.format(j))","42650b7a":"# also plot the numerical data\ntrain_num_plot = train_num = train[['SibSp','Parch', 'Survived']]\nfig = plt.figure(figsize=[10,10])\n# make plots using all other features\nfor i, j in enumerate(train_num_plot.columns):\n    fig.add_subplot(2,2, i+1)\n    sns.barplot(x=j, y='Survived', data=train_num_plot)\n    plt.title('{} vs Survived Plot'.format(j))","eff9f769":"#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","8b03d3f9":"all_data.isna().sum()","140264bc":"all_data.Age = all_data.Age.fillna(train.Age.median()) # we use median because of some outliers.\nall_data.Fare = all_data.Fare.fillna(train.Fare.median())\nall_data = all_data.dropna(subset=['Embarked'])","c6c6ac64":"all_data.isna().sum()","42165bfb":"# Now handling duplicates\nall_data.duplicated().sum()","8493ac1e":"# drop passenger id\nall_data = all_data.drop(['PassengerId'], axis=1)","8f68f233":"all_data_corrplot = all_data.drop(['train_test'], axis=1)\nfig, axs = plt.subplots(nrows=1, figsize=(13, 13))\nsns.heatmap(all_data_corrplot.corr(), annot=True, square=True, cmap='YlGnBu', linewidths=2, linecolor='black', annot_kws={'size':12})","dcf3bf2c":"all_data['name_title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","b72cfc09":"all_data['name_title'].unique()","9520767d":"all_data['name_title'].value_counts()","d3810dd7":"# Replacing less familiar names with more familiar names\nall_data['name_title'] = all_data['name_title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer')\nall_data['name_title'] = all_data['name_title'].replace(['Jonkheer', 'Master'], 'Master')\nall_data['name_title'] = all_data['name_title'].replace(['Don', 'Sir', 'the Countess', 'Lady', 'Dona'], 'Royalty')\nall_data['name_title'] = all_data['name_title'].replace(['Mme', 'Ms', 'Mrs'], 'Mrs')\nall_data['name_title'] = all_data['name_title'].replace(['Mlle', 'Miss'], 'Miss')\n  \n\n# Imputing missing values with 0\nall_data['name_title'] = all_data['name_title'].fillna(0)\n\nall_data['name_title'].value_counts()","fb70df0c":"plt.figure(figsize=[10,10])\nall_data['name_title'].value_counts().plot(kind='bar')\nplt.title('Count of Titles')\nplt.xlabel('Title')\nplt.ylabel('Count')","ca0b75fa":"all_data['Cabin'] = all_data['Cabin'].fillna('Unknown')\nall_data['Deck'] = all_data['Cabin'].str.get(0)","2b335f39":"plt.figure(figsize=[10, 10])\nsns.barplot(x=all_data['Deck'], y='Survived', data=all_data)\nplt.title('Deck Category vs Survived Plot')","4b56f0e0":"all_data['ticket_numeric'] = all_data['Ticket'].apply(lambda x: 1 if x.isnumeric() else 0)","2c44119c":"plt.figure(figsize=[10, 10])\nsns.barplot(x=all_data['ticket_numeric'], y='Survived', data=all_data)\nplt.title('Is the ticket numeric only? vs Survived Plot')","c99f0c91":"all_data['num_family_member'] = all_data['SibSp'] + all_data['Parch']","1c001b29":"all_data.head()","79c3f87b":"all_data = all_data.drop(['Name','SibSp', 'Parch', 'Ticket', 'Cabin', 'train_test'], axis=1)","a7d665dd":"all_data.head()","7e4ed6f1":"scale = StandardScaler()\nall_data_scaled = all_data.copy()\nall_data_scaled[['Age','num_family_member','ticket_numeric','Fare']]= scale.fit_transform(all_data_scaled[['Age','num_family_member','ticket_numeric','Fare']])","5fec44af":"all_data.dtypes","b99267ce":"all_data_encoded = all_data_scaled\nfor j in ['Sex', 'Embarked', 'name_title', 'Deck']:\n    all_data_encoded[j] = all_data_encoded[j].astype('category')\n    all_data_encoded[j] = all_data_encoded[j].cat.codes","1edfa61d":"all_data_encoded.head()","3fd54beb":"train_data = all_data[:len(train)-2] # because we dropped 2 column back in embarked column missing value\ntest_data = all_data[len(train)-2:]\nprint(\"Dimension of train data is: \", train_data.shape)\nprint(\"Dimension of test data is: \", test_data.shape)","f4c380db":"X_train_scaled = train_data.drop(['Survived'], axis=1)\ny_train_scaled = train_data['Survived']\nX_test_scaled = test_data.drop(['Survived'], axis=1)\ny_test_scaled = test_data['Survived']","c8caca2e":"X_train_scaled.head()","4a4fc946":"# NB model\nprint(\"The CV mean score for Naive Bayes model is {:.3f}\".format(cross_val_score(GaussianNB(), X_train_scaled, y_train_scaled).mean()))","a67f1e85":"# KNN model\nprint(\"The CV mean score for K Nearest Neighbor model is {:.3f}\".format(cross_val_score(KNeighborsClassifier(), X_train_scaled, y_train_scaled).mean()))","ad710bf9":"# SVM model\nprint(\"The CV mean score for Support Vector Machine model is {:.3f}\".format(cross_val_score(SVC(), X_train_scaled, y_train_scaled).mean()))","beaa0443":"# XGBoost model\nprint(\"The CV mean score for XGBoost model is {:.3f}\".format(cross_val_score(XGBClassifier(), X_train_scaled, y_train_scaled).mean()))","31f4925d":"# RF model\nprint(\"The CV mean score for Random Forest model is {:.3f}\".format(cross_val_score(RandomForestClassifier(), X_train_scaled, y_train_scaled).mean()))","821b3177":"xgb = XGBClassifier()\nxgb.fit(X_train_scaled, y_train_scaled)","5500a910":"y_pred = xgb.predict(X_test_scaled).astype('int32')","4c757b21":"print(y_pred.shape)","4a562e5a":"final_result = {'PassengerId': test['PassengerId'], 'yhat': y_pred}","b37d4a9c":"final_dataframe = pd.DataFrame(final_result)","2201c21b":"final_dataframe.to_csv('submission.csv')","31116c0d":"# We try to use the XGBoost method.","34260956":"# Correlation plot\nNow let's combine the data to gain insights on the predictor and the target relationship. This also prevents data leakage in the model (source: https:\/\/towardsdatascience.com\/preventing-data-leakage-in-your-machine-learning-model-9ae54b3cd1fb). We won't peek the test data, though.","794b64b8":"# Encode the categorical features","83fec2de":"# Planning the Project\n\n## Data Cleaning\n1. .info() and .describe() to see the quick overview of the data (data types, how many missing values, etc)\n2. Dropping the irrelevant column and fixing the missing data\n3. Checking duplicates\n## EDA\n4. Understanding the data visually (histogram, box plot, outlier, etc)\n5. Checking correlation\n6. Feature engineering (Does title also matter?)\n7. Gaining insight and what to put as target (y) and features. (does ticket class affect survival rate? Cabin number? Port embarkation?)\n## Model Training and Evaluation\n8. Scaling and normalization\n9. Model training (SVM, RF, DT, KNN)\n10. Model tuning\n11. Model testing\n12. Saving the .csv","6778ce13":"# Dropping unnecessary columns\n> Now we have done our feature engineering by adding 3 new columns, now we can delete the unnecessary features.","a62bae27":"Quick insights:\n1. Ignore the top left plot, because it's just a survived vs survived plot.\n2. First class passengers are the most likely to survive (because the first class cabin was on the upper part of the ship so they were able to escape quicker?). There is positive correlation between the survivability and the higher passenger class.\n3. Females are more likely to survive than males, because the emergency ship was prioritized for women and children (no surprising facts here).\n4. People embarking from Cherbourg survives the most, but the correlation is not very clear.","fc6a2e89":"# Engineering the Ticket Feature\nThe ticket formatting is weird, because some have numbers only while some others also have letters before the numbers.","c69c9dc7":"We can't do anything yet to the ticket and cabin features before we do some feature engineering. However, we can just impute the missing value in Fare, Age, and Embarked columns","81275b24":"# Number of family members\nSibSp and Parch seems redundant, so we can just combine them.","96f3155b":"Let's do another quick plot session to visualize between \"Survived\" and any other columns. This can also be done using pivotal table, but I chose to do it with the bar plot instead.","9d57bc12":"# Defining the problem\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nInteresting things:\n1. Only 722 out of 2224 survives (32% survival rate)\n2. What affects the survival rate?\n3. How can we mitigate this to ensure the increase of survival rate in the future?","8b99cb69":"The missing values should be dealt with now.","3ce44bf8":"The ticket and cabin features are quite messy with too many labels. Feature engineering will help in this case. For the time being, we will not use those two features to determine the correlation between the Survived column and those columns.","e5be0e3b":"# Feature Engineering","40279b52":"The recap:\n* Naive Bayes (79.2%)\n* KNN (69.7%)\n* SVM (66.9%)\n* <b> XGBoost (82.9%) <\/b>\n* Random Forest (81.2%)","f9e3fb28":"There are 177 null data in Age and 687 null data in Cabin. Also 2 null data in Embarked data that can be dropped easily later.\nLooks like we have to plot Cabin later to see what NaN here means. Can it be dropped altogether if turned out it does not correlate to the Survived column?","72e94cee":"# Normalize some values\nBefore splitting the train-test set, we need to normalize the numerical values altogether so the normalization process can be uniform between train and test data.","0d2b3fcc":"# Categorical data overview","d6c48247":"# Tweaking the name feature\nThe name was organized in the following pattern:\n\nBraund, Mr. Owen Harris\n\nName, \"title\". Name\n\nWhat we want to do is to extract the \"title\".\n\nThe first thing to do is to split by ,\n\n**The second thing is to split by . so we get only the title.\n","508afdc6":"# Data Dictionary\n\n|Variable|Definition|Key    |\n|--------|----------|-------|\n|survival|Survival\t|0 = No, 1 = Yes|\n|pclass\t|Ticket class|\t1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex\t|Sex\t|---|\n|Age|\tAge in years|---|\t\n|sibsp\t|# of siblings \/ spouses aboard the Titanic|---|\t\n|parch\t|# of parents \/ children aboard the Titanic|---|\t\n|ticket\t|Ticket number| ---|\n|fare| Passenger fare | ---|\t\n|cabin|\tCabin number| --- |\t\n|embarked |\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton| --- |","f0cef8a6":"# Finding the correlation between each features and whether the person will survive","5ae5ba61":"# Handling Missing and Duplicate Data\nThe EDA helps us to choose the treatment for the missing data. ","fcfa9480":"In general:\n1. Most people are aged 20-30 (34.41%)\n2. Most have 0 siblings and spouses (85.11%)\n3. Most have 0 parents\/children with them. (76.04%)\n4. The fare is highly skewed towards 0-100.(94.04%)\n\nThe data is heavily skewed, so we can consider the normalization in the preprocessing.","9d7a3af2":"First impression towards the data:\n1. Some null entries exist in Embarked column\n2. We need to predict the data in \"survived\" column\n3. There are 891 and 418 rows in the training and testing data.\n4. Names are not clean.\n5. What does NaN mean in the Cabin column?","67cc50f7":"# Engineering the cabin feature\nThere are a lot of missing data in the cabin feature, most likely because most people don't stay in a particular cabin, or an economic cabin is not classified in the data.","a544d446":"It can be seen that the passenger class has some negative correlation with the survival (-0.34). Lower Pclass means higher passenger class. Passenger class also has some correlation with the fare, which makes sense since the higher the passenger class, the more expensive people have to pay.","90952759":"No duplicate rows, so we should be fine moving forward.","f5b0225a":"# Train - Test split","05e63b75":"# Titanic Project to Predict the Survival of Passengers\nWritten by: Christopher Salim, 2020\n\nIn this project, I would like to explore the Titanic passenger data and and try to answer what factors influence the survivability of the passengers in Titanic ship. After that, the machine learning predictions will be conducted on the data.","a9451bc4":"# Numerical data quick overview","089ae232":"# Model Building\nThere are 5 models that I use in this project:\n1. Naive Bayes\n2. K-Nearest Neighbor\n3. Support Vector Machine\n4. XGBoost\n5. Random Forest"}}