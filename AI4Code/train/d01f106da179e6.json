{"cell_type":{"00738187":"code","8bc34a06":"code","da06f947":"code","a5199e65":"code","936e96c2":"code","a56077d6":"code","a31a5a0c":"code","e90e1569":"code","87cc297d":"code","8eac747b":"code","d045d721":"code","771fbc4f":"code","a917eb04":"code","d1fefb21":"code","134ac5da":"code","044e2ba5":"code","851d5ef1":"code","0d3aae73":"code","123a1910":"code","e1b8e921":"code","7b3846f1":"code","10f5e3bb":"code","e9a0596e":"markdown","40ee6b10":"markdown","053b44c1":"markdown","30a52d9e":"markdown","c5bd6227":"markdown","e8d83b1b":"markdown","fadd5127":"markdown","7594857f":"markdown","dd9ccfab":"markdown","196c8af7":"markdown","44ec9055":"markdown","7f803af5":"markdown","1b3d5bf4":"markdown","6841fd23":"markdown","9d8cf5f1":"markdown","111e1509":"markdown","f61a1251":"markdown","ac750ce0":"markdown","95066c4c":"markdown","c4747d58":"markdown","a2ac93b9":"markdown","356688a0":"markdown","09912add":"markdown","475f2e76":"markdown","e897c1f3":"markdown","fcf17865":"markdown","232b969b":"markdown","0599221b":"markdown","bb3dd68c":"markdown"},"source":{"00738187":"import pandas as pd\nimport numpy as np\nimport gc, warnings\nwarnings.filterwarnings('ignore')","8bc34a06":"sale_train = pd.read_csv('..\/input\/sales_train.csv')","da06f947":"print(\"----------Top-5- Record----------\")\nprint(sale_train.head(5))\nprint(\"-----------Information-----------\")\nprint(sale_train.info())\nprint(\"-----------Data Types-----------\")\nprint(sale_train.dtypes)\nprint(\"----------Missing value-----------\")\nprint(sale_train.isnull().sum())\nprint(\"----------Null value-----------\")\nprint(sale_train.isna().sum())\nprint(\"----------Shape of Data----------\")\nprint(sale_train.shape)","a5199e65":"print('Number of duplicates:', len(sale_train[sale_train.duplicated()]))","936e96c2":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\nsale_train = downcast_dtypes(sale_train)\nprint(sale_train.info())","a56077d6":"sales_by_item_id = sale_train.pivot_table(index=['item_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)\nsales_by_item_id = sales_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_item_id.columns.values[0] = 'item_id'","a31a5a0c":"sales_by_item_id.sum()[1:].plot(legend=True, label=\"Monthly sum\")","e90e1569":"sales_by_item_id.mean()[1:].plot(legend=True, label=\"Monthly mean\")","87cc297d":"outdated_items = sales_by_item_id[sales_by_item_id.loc[:,'27':].sum(axis=1)==0]\nprint('Outdated items:', len(outdated_items))","8eac747b":"test = pd.read_csv('..\/input\/test.csv')\nprint('Outdated items in test set:', len(test[test['item_id'].isin(outdated_items['item_id'])]))","d045d721":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sale_train['item_cnt_day'])\nprint('Sale volume outliers:',sale_train['item_id'][sale_train['item_cnt_day']>500].unique())\n\nplt.figure(figsize=(10,4))\nplt.xlim(sale_train['item_price'].min(), sale_train['item_price'].max())\nsns.boxplot(x=sale_train['item_price'])\nprint('Item price outliers:',sale_train['item_id'][sale_train['item_price']>50000].unique())","771fbc4f":"sales_by_shop_id = sale_train.pivot_table(index=['shop_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)\nsales_by_shop_id = sales_by_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_shop_id.columns.values[0] = 'shop_id'\n\nfor i in range(6,34):\n    print('Not exists in month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,'0':str(i)].sum(axis=1)==0].unique())\n\nfor i in range(6,28):\n    print('Shop is outdated for month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,str(i):].sum(axis=1)==0].unique())\n","a917eb04":"print('Recently opened shop items:', len(test[test['shop_id']==36]))","d1fefb21":"shops = pd.read_csv('..\/input\/shops.csv')\nshops.head()","134ac5da":"shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'] = shops['shop_name'].str.partition(' ')[0]\nshops['shop_type'] = shops['shop_name'].apply(lambda x: '\u043c\u0442\u0440\u0446' if '\u043c\u0442\u0440\u0446' in x else '\u0442\u0440\u0446' if '\u0442\u0440\u0446' in x else '\u0442\u0440\u043a' if '\u0442\u0440\u043a' in x else '\u0442\u0446' if '\u0442\u0446' in x else '\u0442\u043a' if '\u0442\u043a' in x else 'NO_DATA')\nshops.head()","044e2ba5":"items = pd.read_csv('..\/input\/items.csv')\nitems.head()","851d5ef1":"# Ugly code to show the idea\nfrom collections import Counter\nfrom operator import itemgetter\nitems['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems = items.fillna('0')\n\nresult_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\nresult_1 = sorted(result_1, key=itemgetter(1))\nresult_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\nresult_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n\nresult_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\nresult_2 = sorted(result_2, key=itemgetter(1))\nresult_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\nresult_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n\nresult = pd.concat([result_1, result_2])\nresult = result.drop_duplicates(subset=['feature'])\n\nprint('Most common aditional features:', result)","0d3aae73":"print('Unique item names:', len(items['item_name'].unique()))","123a1910":"import re\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.head()","e1b8e921":"print('Unique item names after correction:', len(items['item_name'].unique()))","7b3846f1":"categories = pd.read_csv('..\/input\/item_categories.csv')\ncategories.head()","10f5e3bb":"test = pd.read_csv('..\/input\/test.csv')\ngood_sales = test.merge(sale_train, on=['item_id','shop_id'], how='left').dropna()\ngood_pairs = test[test['ID'].isin(good_sales['ID'])]\nno_data_items = test[~(test['item_id'].isin(sale_train['item_id']))]\n\nprint('1. Number of good pairs:', len(good_pairs))\nprint('2. No Data Items:', len(no_data_items))\nprint('3. Only Item_id Info:', len(test)-len(no_data_items)-len(good_pairs))\n  ","e9a0596e":"## 1.5 Shop info\n* * *\nThe structure of the shop information is evident.\n### Shop City | Shop type | Shop name","40ee6b10":"### Possible Category features\n1. Section\n2. Main Category name\n3. Main SubCategory name \n4. Secondary SubCategory name\n","053b44c1":"## 1.2 shop_id\n* * *\n### Lets now group train data by shop_id.\nWe can see new shops - probably there will be a sales spike (opening event for example).\nApparently closed shops (ill call it \"outdated shops\")  - no sales for last 6 months.","30a52d9e":"In our test set we have 5100 sales in really new shop and no \"outdated shops\" but anyway it is good feature for future.","c5bd6227":"We have duplicated rows, but I don't think that it is a mistake.\n\nIt could be different sales methods or client type, etc.\n\nYou can remove it, but I really don't believe that 6 rows of 3m can make the difference.","e8d83b1b":"### Outliers by price and sales volume\nWe will get rid of them later\n\n#### please see lovely kernel made by Denis Larionov (I stole few graphs from there)\n* https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost","fadd5127":"## 1.3 Price\n* * *\n### Possible Price features:\n1. Price category (1$\/10$\/20$\/ etc.) - obviously (or not obviously),  items with smaller price have greater volumes\n2. Discount and Discount duration\n3. Price lag (shows discount)\n4. Price correction (rubl\/usd pair)\n5. Shop Revenue","7594857f":"#### Is it feature? Yes. We need to apply different prediction approach for each type of items in the test set.\n####  For example - \"No Data Items\" - it is more likely classification task.","dd9ccfab":"### Item name correction\nFor our basic \"name feature\" it is enough to find identical items (not similar but identical),","196c8af7":"### Let's see how many products are outdated (no sales for the last 6 months)\n12391 of 21807 is a huge number. Probably we can set 0 for all that items and do not make any model prediction.","44ec9055":"### Possible shop_id features\n1. Lags (shop_id\/shp_cnt_mth)\n2. Opening month (possible  opening sales)\n3. Closed Month (possible stock elimination)","7f803af5":"# Overview\nWhat is this kernel about?\n* No predictions to make \n* No features to create\n\nWe will load competition data and look closer on it. We will try to understand what we have in our hands and how we can work with it.\n* * *","1b3d5bf4":"## 1.4 Dates\n* * *\n### Possible Date features:\n1. Weekends and holidays sales (to correct monthly sales)\n2. Number of days in the month (to correct monthly sales)\n3. Month number (for seasonal items)","6841fd23":"## 1.8 Test Set\n* * *\nThe key to my success was the analysis of Test test data.\n\nWe have three groups of items:\n1. Item\/shop pairs that are in train\n2. Items without any data\n3. Items that are in train","9d8cf5f1":"## 1.6 Item info\n* * *\nLet's see what we can get from this file.","111e1509":"## 1.1 Item_id\n* * *\n### Lets group data by item_id and date_block_num and look closer on it.\n","f61a1251":"We can enconde \"features\" that many items have.\n\nThe structure is always the same\n### Item name [category feature] (additional feature)\nwe can split it, and \"one hot encode it.\"","ac750ce0":"## 1.7 Category info\n* * *\nThe structure here is\n### Section name - subsection\nwe can split it and have two features from one","95066c4c":"### Possible Shop features:\n1. Shop City\n2. Shop Type","c4747d58":"### Possible item_id features:\n1. Lags\n2. Release date\n3. Last month sale\n4. Days on sale\n5. Neighbors (items with id 1000 and 1001 could be somehow similar - genre, type, release date)","a2ac93b9":"### But I did manual feature extraction here to have four features.\nSection \/ Main Category name \/ Main SubCategory name \/ Secondary SubCategory name\n#### \u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b \/ PS2\t\/ PS \/ 2","356688a0":" ## Load train data\n * * *","09912add":"We can view basic DafaFrame information. \n\nAs you can see, we do not have broken and nan data that is good.","475f2e76":"### Next part will be about data aggregation and feature preparation.\n## To be continued...","e897c1f3":"With a close look we can find out that some shops have duplicated id\/name - probably it changed location (within commercial center), or it has a different type (isle sale point), but I decided to merge it.\n* 11 => 10\n* 1  => 58\n* 0  => 57\n* 40 => 39\n\nI converted train shop_id to shop_id that is in the test set","fcf17865":"### Possible Item features:\n1. Item name\n2. Encoded aditional feature ","232b969b":"### How many outdated items in test set?\n6888 - not much but we have such items","0599221b":"I can advise downcasting your DataFrame. It will save you some memory, and believe me you will need all memory possible.\n\nIn our case from 134.4+ MB, we went to 61.6+ MB\n\nNot a great deal right now but such approach works with bigger DF also.\n\n#### please see this two links for more tips (I stole that downcast basic snippet from anqitu)))\n* https:\/\/www.kaggle.com\/anqitu\/feature-engineer-and-model-ensemble-top-10\n* https:\/\/www.kaggle.com\/yuliagm\/how-to-work-with-big-datasets-on-16g-ram-dask","bb3dd68c":"### Simple graph\nWhat this graph is telling us. Basically nothing.)) I only see that train data has many old products (degradation line) and many 1c products are seasonal and probably release date depended.\n\n#### I'm not very good with graphs and presentations - there are better data representation examples:\n* https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda\n* https:\/\/www.kaggle.com\/jagangupta\/time-series-basics-exploring-traditional-ts"}}