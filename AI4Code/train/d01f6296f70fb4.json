{"cell_type":{"74444222":"code","3d680ec4":"code","1f104503":"code","fa94d94d":"code","db917066":"code","a2a8cc19":"code","24d96fe6":"code","5f420c09":"code","0c7d9c7d":"code","fdec16a2":"code","b12b428e":"code","c294cfd0":"code","14e3e2fe":"code","f2916769":"markdown","c2a07420":"markdown","c3d7a14d":"markdown","eb8d2ccb":"markdown"},"source":{"74444222":"import random\nimport gc\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AdamW\n\ngc.collect()","3d680ec4":"random.seed(42)\ntorch.manual_seed(42)\nnp.random.seed(42)","1f104503":"!apt-get install unzip\n!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip test.tsv\n!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip train.tsv","fa94d94d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nsample_submission = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')\n\ntrain_df = pd.read_csv('train.tsv', sep='\\t')\nprint(train_df.shape)\nprint(train_df.info())\ntrain_df.head()","db917066":"test_df = pd.read_csv('test.tsv', sep='\\t')\nprint(test_df.shape)\nprint(test_df.info())\ntest_df.head()","a2a8cc19":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', lower=True)","24d96fe6":"class MovieReviewsDataset(Dataset):\n    def __init__(self, df, max_len, test_only=False):\n        self.max_len = max_len\n        self.test_only = test_only\n        self.text = df['Phrase'].tolist()\n        if not self.test_only:\n            self.sentiments = df['Sentiment'].values\n            \n        self.encode = tokenizer.batch_encode_plus(\n            self.text,\n            padding='max_length',\n            max_length=self.max_len,\n            truncation=True,\n            return_attention_mask=True\n        )\n        \n    def __getitem__(self, i):\n        input_ids = torch.tensor(self.encode['input_ids'][i])\n        attention_mask = torch.tensor(self.encode['attention_mask'][i])\n        \n        if self.test_only:\n            return (input_ids, attention_mask)\n        else:\n            sentiments = self.sentiments[i]\n            return (input_ids, attention_mask, sentiments)\n    \n    def __len__(self):\n        return len(self.text)\n","5f420c09":"max_len = 64\ntrain_dataset = MovieReviewsDataset(train_df, max_len)\ntest_dataset = MovieReviewsDataset(test_df, max_len, test_only=True)\n\nlengths = [int(len(train_dataset) * 0.8), int(len(train_dataset) * 0.2)]\ntrain_dataset, valid_dataset = random_split(train_dataset, lengths=lengths, generator=torch.Generator().manual_seed(42))\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_dataloader = DataLoader(valid_dataset, batch_size=128)\ntest_dataloader = DataLoader(test_dataset, batch_size=128)","0c7d9c7d":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        bert_base_config = AutoConfig.from_pretrained('bert-base-uncased')\n        self.bert_base = AutoModel.from_pretrained('bert-base-uncased', config=bert_base_config)\n        self.classifier = nn.Linear(bert_base_config.hidden_size, 5)\n\n    def forward(self, input_ids, attention_mask):\n        bert_base_output = self.bert_base(input_ids=input_ids, attention_mask=attention_mask)\n        # get last hidden state\n        # bert_base_last_hidden_state = bert_base_output[0]\n        # or\n        # roberta_base_last_hidden_state = roberta_base_output.hidden_states[-1]\n\n        # pooler_output \u2013 Last layer hidden-state of the first token of the sequence \n        # (classification token) further processed by a Linear layer and a Tanh activation function\n        pooler_output = bert_base_output[1] # [batch_size, hidden] \n        out = self.classifier(pooler_output)\n        return out\n\n\ngc.collect()","fdec16a2":"model = Model()\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriteron = nn.CrossEntropyLoss()\ngc.collect()","b12b428e":"total_loss = []\ntotal_val_acc = []\nfor epoch in range(3):\n    model.train()\n    epoch_loss = []\n    for input_ids, attention_mask, target in tqdm(train_dataloader):\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)            \n        target = target.to(device)\n        \n        optimizer.zero_grad()\n        \n        y_pred = model(input_ids, attention_mask)\n        \n        loss = criteron(y_pred, target)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n\n    input_ids = input_ids.to(torch.device('cpu'))\n    attention_mask = attention_mask.to(torch.device('cpu'))            \n    target = target.to(torch.device('cpu'))\n    gc.collect()\n\n    val_accs = []\n    model.eval()\n    for input_ids, attention_mask, target in tqdm(val_dataloader):\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)        \n        y_pred = model(input_ids, attention_mask)\n        _, y_pred = torch.max(y_pred, -1)\n        acc = torch.mean((torch.tensor(y_pred.cpu() == target.cpu(), dtype=torch.float)))\n        val_accs.append(acc.cpu())\n\n    el = sum(epoch_loss)\/len(epoch_loss)\n    total_loss.append(el)\n    acc = np.array(val_accs).mean()\n    total_val_acc.append(acc)\n    print(\"Epoch:\", epoch+1, \"-- loss:\", el, \"-- acc:\", acc)\n    gc.collect()","c294cfd0":"model.eval()\npredictions = []\nfor text, attention_mask in tqdm(test_dataloader):\n    text = text.to(device)\n    attention_mask = attention_mask.to(device)\n    preds = model(text, attention_mask)\n    _, preds = torch.max(preds, -1)\n    for pred in preds: predictions.append(pred.item())\nprint(len(predictions))","14e3e2fe":"submission = pd.DataFrame()\nsubmission['PhraseId'] = test_df['PhraseId']\nsubmission['Sentiment'] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Sumbssion is ready!\")","f2916769":"# Prepare data","c2a07420":"# Prepare Submission","c3d7a14d":"# Text Processing","eb8d2ccb":"# Modeling"}}