{"cell_type":{"f1613d12":"code","31e60f24":"code","997e9011":"code","f61fdbdb":"code","47e40262":"code","fec4b16b":"code","419289ab":"code","e91c8696":"code","30bf7113":"code","74123b6f":"code","17e29e96":"code","c04d4a61":"code","007a53da":"code","4cf68317":"code","5a337f96":"code","f9bc4ef6":"code","2a966a68":"markdown","494a9e62":"markdown","0922b88a":"markdown","9e05aeb2":"markdown","0a17e6f6":"markdown","2b674d1b":"markdown","c37faf54":"markdown","0a9e3b45":"markdown"},"source":{"f1613d12":"import warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_classification\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.svm import SVC\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly import tools\nimport plotly.graph_objs as go\n\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\ncolor = sns.color_palette()\nsns.set(style='ticks', palette=\"muted\", color_codes=True)","31e60f24":"X_, y_ = make_classification(n_features=4, n_samples=150, n_classes=3, class_sep=2, n_redundant=0, n_informative=4,\n                             n_clusters_per_class=1, random_state=0)","997e9011":"X = pd.DataFrame(X_)\nX.columns = ['A', 'B', 'C', 'D']\n\ny = pd.DataFrame(y_)\ny.columns = ['Target']\n\ndf = pd.concat([X, y], axis=1)","f61fdbdb":"df.head(5)","47e40262":"df.Target.value_counts()","fec4b16b":"sns.FacetGrid(df, hue=\"Target\", size=5).map(plt.scatter, \"A\", \"B\").add_legend();","419289ab":"# http:\/\/scikit-learn.org\/stable\/auto_examples\/datasets\/plot_iris_dataset.html\n\nX_3D = X_[:, :2]\n\nx_min, x_max = X_3D[:, 0].min() - .5, X_3D[:, 0].max() + .5\ny_min, y_max = X_3D[:, 1].min() - .5, X_3D[:, 1].max() + .5\n\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(X)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y_,\n           cmap=plt.cm.RdYlBu, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","e91c8696":"ax = sns.boxplot(x=\"Target\", y=\"A\", data=df)\nax = sns.stripplot(x=\"Target\", y=\"A\", data=df, jitter=True, edgecolor=\"gray\")","30bf7113":"sns.FacetGrid(df, hue=\"Target\", size=6).map(sns.kdeplot, \"C\").add_legend();","74123b6f":"sns.pairplot(df, hue=\"Target\", vars=[\"A\", \"B\", \"C\", \"D\"], diag_kind=\"kde\", size=3);","17e29e96":"corrmat = X.corr()\ncols = X.columns\nsns.set(font_scale=1.25)\nhm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, cmap='YlGnBu', fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c04d4a61":"from sklearn.cross_validation import train_test_split\n\ntrain, test = train_test_split(df, test_size = 0.3)\n\nprint('Training set shape: ', train.shape)\nprint('Test set shape: ', test.shape)","007a53da":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","4cf68317":"# https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nMLA = [    \n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = df['Target']\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, df.iloc[:,:-1], df.iloc[:,-1:], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(df.iloc[:,:-1], df.iloc[:,-1:])\n    MLA_predict[MLA_name] = alg.predict(df.iloc[:,:-1])\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","5a337f96":"sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","f9bc4ef6":"colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\ncmap = ListedColormap(colors[:len(np.unique(y))])\n    \ndef make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\nX = df.values[:, :2]\ny = df.values[:, -1:].ravel()\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (discriminant_analysis.QuadraticDiscriminantAnalysis(),\n          svm.LinearSVC(),\n          gaussian_process.GaussianProcessClassifier(),\n          neighbors.KNeighborsClassifier())\nmodels = (clf.fit(X, y) for clf in models)\nscore = (clf.score(X, y) for clf in models)\n\n# title for the plots\ntitles = ('Discriminant Analysis',\n          'SVM',\n          'Gaussian Process',\n          'Nearest Neighbor')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=cmap, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=cmap, s=20, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('A')\n    ax.set_ylabel('B')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\nplt.show()","2a966a68":"A few more visualizations to understand the relationships in our data:","494a9e62":"Hope you enjoyed this quick (and easy) example! Any feedback is greatly appreciated! ","0922b88a":"The clusters are well seperated and easily distinguishable. This should make classification problem easier.\n\nAnd here are the same two features on a 3D axes:","9e05aeb2":"# Practice using machine learning classifiers on a synthetic dataset\n\n\n*This notebook borrows heavily from these excellent Kaggle kernels:*\n\n1. https:\/\/www.kaggle.com\/benhamner\/python-data-visualizations\n\n2. https:\/\/www.kaggle.com\/arthurtok\/decision-boundaries-visualised-via-python-plotly\n\n3. https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy","0a17e6f6":"We have an equal number of observations (50) in each class. That should make things much easier :)\n\nLet's plot our first two features - \"A\" and \"B\" - and see how the target classes are clustered:","2b674d1b":"Next, we construct a pandas DataFrame from our generated numpy array with labeled columns, then combine our feature and target columns:","c37faf54":"Now that we have a good sense of our data, lets get ready for the classification problem:","0a9e3b45":"Let's start by generating a random classification problem with four features and three classes:"}}