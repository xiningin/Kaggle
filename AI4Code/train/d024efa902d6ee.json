{"cell_type":{"2c50803a":"code","9b46c759":"code","d430a4f9":"code","a5b00676":"code","a31eb150":"code","0f11e8eb":"code","aef69093":"code","c1924c49":"code","bb14e953":"code","90c80853":"code","423a1b1b":"code","d953163d":"code","8f34dfe0":"code","8f368570":"code","8b2d8560":"code","ad781ce4":"markdown"},"source":{"2c50803a":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport subprocess\nimport array\nimport pickle\nimport numpy as np\nimport pandas as pd","9b46c759":"DATASET_VERSION = 'en-100'\nDATASET_ROOT = f'..\/input\/wikipedia-en\/{DATASET_VERSION}'\nWORKING_ROOT = f'data\/{DATASET_VERSION}'\nDATASET_PREFIX = 'en.wiki'","d430a4f9":"params = SimpleNamespace(\n    window_size = 5,\n    cutoff = 3,\n    maxtokens = 100000,\n    maxlines = 500000,\n    dataset = f'{DATASET_ROOT}\/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}\/{DATASET_PREFIX}',\n)","a5b00676":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","a31eb150":"class Punctuation:\n    html = re.compile(r'&apos;|&quot;')\n    punctuation = re.compile(r'[^\\w\\s\u00b7]|_')\n    spaces = re.compile(r'\\s+')\n    ela_geminada = re.compile(r'l \u00b7 l')\n\n    def strip(self, s):\n        '''\n        Remove all punctuation characters.\n        '''\n        s = self.html.sub(' ', s)\n        s = self.punctuation.sub(' ', s)\n        s = self.spaces.sub(' ', s).strip()\n        s = self.ela_geminada.sub('l\u00b7l', s)\n        return s","0f11e8eb":"def remove_punctuation(input_path, output_path):\n    punc = Punctuation()\n    with open(input_path, 'r', encoding='utf-8') as inpf, open(output_path, 'w', encoding='utf-8') as outf:\n        for line in inpf:\n            line = punc.strip(line)\n            print(line, file=outf)","aef69093":"def get_token_counter(file_path):\n    counter = Counter()\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                tokens = line.split()\n                counter.update(tokens)\n    return counter","c1924c49":"def get_token_vocabulary(token_counter, cutoff=3, maxtokens=None, verbose=1, eos_token=None):\n    vocab = Vocabulary(eos_token=eos_token)\n    total_count = sum(token_counter.values())\n    in_vocab_count = 0\n\n    for token, count in token_counter.most_common(maxtokens):\n        if count >= cutoff:\n            vocab.add_token(token)\n            in_vocab_count += count\n\n    if verbose:\n        OOV_count = total_count - in_vocab_count\n        print('OOV ratio: %.2f%%.' % (100*OOV_count \/ total_count))\n    return vocab","bb14e953":"def get_token_index(file_path, vocab, eos_token=None):\n    index_list = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                if eos_token is not None:\n                    line += ' ' + eos_token\n                tokens = line.strip().split()\n                index_list.append([vocab.get_index(token) for token in tokens])\n    return index_list","90c80853":"def get_number_of_samples(idx_list, window_size):\n    nsamples = 0\n    for line in idx_list:\n        if len(line) <= window_size \/\/ 2:\n            continue\n        nsamples += len(line)\n    return nsamples","423a1b1b":"def get_data(idx_list, window_size, pad_index=0):\n    nsamples = get_number_of_samples(idx_list, window_size)\n    winput = np.empty((nsamples, window_size - 1), dtype=np.int32)\n    target = np.empty(nsamples, dtype=np.int32)\n    left_window = window_size \/\/ 2\n    right_window = window_size - left_window - 1\n    sample = 0\n    for line in idx_list:\n        if len(line) <= window_size \/\/ 2:\n            continue\n        ext_line = [pad_index] * left_window + line + [pad_index] * right_window\n        for i, token_id in enumerate(line):\n            winput[sample] = ext_line[i:i + left_window] + ext_line[i + left_window + 1:i + window_size]\n            target[sample] = token_id\n            sample += 1\n    assert nsamples == sample\n    return winput, target","d953163d":"def prepare_dataset(params):\n    dataset_prefix = params.dataset\n    working_prefix = params.working\n    cutoff = params.cutoff\n    maxtokens = params.maxtokens\n    maxlines = params.maxlines\n    window_size = params.window_size\n\n    for part in ['train', 'valid', 'test']:\n        data_filename = f'{dataset_prefix}.{part}.tokens'\n        data_filename_nopunct = f'{working_prefix}.{part}.tokens.nopunct'\n        remove_punctuation(data_filename, data_filename_nopunct)\n\n        if part == 'train':\n            # Basic token statistics\n            token_counter = get_token_counter(data_filename_nopunct)\n            print(f'Number of Tokens: {sum(token_counter.values())}')\n            print(f'Number of different Tokens: {len(token_counter)}')\n            pickle.dump(token_counter, open(f'{data_filename_nopunct}.dic', 'wb'))\n\n            # Token vocabulary\n            token_vocab = get_token_vocabulary(token_counter, cutoff=cutoff, maxtokens=maxtokens)\n            token_vocab.save(f'{working_prefix}.vocab')\n            print(f'Vocabulary size: {len(token_vocab)}')\n\n        # Token indexes\n        train_idx = get_token_index(data_filename_nopunct, token_vocab)\n        print(f'Number of lines ({part}): {len(train_idx)}')\n\n        # Get input and target arrays\n        idata, target = get_data(train_idx, window_size)\n        print(f'Number of samples ({part}): {len(target)}')\n\n        # Save numpy arrays\n        np.savez(f'{working_prefix}.{part}.npz', idata=idata, target=target)\n    return token_vocab","8f34dfe0":"# Create working dir\npathlib.Path(WORKING_ROOT).mkdir(parents=True, exist_ok=True)","8f368570":"vocab = prepare_dataset(params)","8b2d8560":"for word in ['Jose', 'Jos\u00e9', 'Josep', 'Joan']:\n    print(f'{word} -> {vocab.get_index(word)}')","ad781ce4":"Check the vocabulary with some words:"}}