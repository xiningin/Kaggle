{"cell_type":{"01ae325e":"code","240dc3ba":"code","22e21cd3":"code","e5da2bd5":"code","7fa2fbe9":"code","2cf27335":"code","f1ea31b9":"code","83061d07":"code","5ad35881":"code","5fd1aa68":"code","bef84f17":"code","6b9c5624":"code","f30f3edb":"code","9c26ac55":"code","29bbc6df":"code","d539b44f":"code","468a821d":"code","3d9cad7a":"code","fee82922":"code","9e363330":"code","a02121d0":"code","89b70a71":"code","dcdef397":"code","b5f95816":"code","6feac8eb":"code","16793e0a":"code","a126abcd":"markdown","3acd07f7":"markdown","552710b0":"markdown"},"source":{"01ae325e":"# libraries\nimport os, math\nimport numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.impute import SimpleImputer\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","240dc3ba":"!pip install pycaret==2.3.6","22e21cd3":"from pycaret.classification import *","e5da2bd5":"# loading training and testing data\ndf_train = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/test.csv\")","7fa2fbe9":"df_train.head()","2cf27335":"df_train.describe()","f1ea31b9":"# Imputing few cat columns with mode and numeric with median, in both train and test\nfor df in [df_train, df_test]:\n    for col in df_test.columns:\n        if col in [\"key\", \"audio_mode\", \"time_signature\"]:\n            mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n            df[col] = mode_imputer.fit_transform(df[col].values.reshape(df.shape[0], 1)).flatten()\n            df[col] = df[col].astype(int)\n        else:\n            median_imputer = SimpleImputer(strategy=\"median\")\n            df[col] = median_imputer.fit_transform(df[col].values.reshape(df.shape[0], 1)).flatten() \n            df[col] = df[col].astype(\"float32\")\n# target label in train data\ndf_train.song_popularity = df_train.song_popularity.astype(int)","83061d07":"# Setting up the pipeline\n\nexpt = setup(data=df_train, target='song_popularity', \n                  session_id=22,\n                  preprocess=False,\n                  ignore_low_variance = True,\n                  remove_multicollinearity = False, \n                  ignore_features=[\"id\"],\n                  fold=5,\n                  fold_strategy='stratifiedkfold',\n                  fold_shuffle=True,\n                  log_experiment = True, \n                  experiment_name = 'song_popularity'\n                )","5ad35881":"# compare models performance and select top 5 for blending results\nbest_models = compare_models(n_select=5, \n                             sort=\"AUC\",\n                             exclude=[\"ridge\", \"lda\", \"qda\", \"svm\"],  # got some weird errors with lgb and xgboost\n                            )","5fd1aa68":"model1 = create_model('gbc', fold=5)","bef84f17":"model2 = create_model('rf', fold=5)","6b9c5624":"model3 = create_model('catboost', fold=5)","f30f3edb":"model4 = create_model('ada', fold=5)","9c26ac55":"model5 = create_model('et', fold=5)","29bbc6df":"tuned_gbc = tune_model(model1, optimize='AUC', n_iter=5)","d539b44f":"tuned_rf = tune_model(model2, optimize='AUC', n_iter=5)","468a821d":"tuned_catb = tune_model(model3, optimize='AUC', n_iter=5)","3d9cad7a":"tuned_ada = tune_model(model4, optimize='AUC', n_iter=5)","fee82922":"tuned_et = tune_model(model5, optimize='AUC', n_iter=5)","9e363330":"# blend individual models\nsoft_blend = blend_models(estimator_list=[tuned_gbc, tuned_rf, tuned_catb, tuned_ada, tuned_et], method='soft')","a02121d0":"soft_blend.estimators_","89b70a71":"# can do Stacking as well","dcdef397":"# test data\nX_test = df_test.drop(columns=[\"id\"])","b5f95816":"# prediction on test set with blended model\ny_proba = soft_blend.predict_proba(X_test)[:, 1]\ny_proba","6feac8eb":"ans = pd.DataFrame(data={\"id\": df_test.id.values, \"song_popularity\": y_proba})\nans.head()","16793e0a":"ans.to_csv(\"submission.csv\", index=False)\nprint(\"Done!\")","a126abcd":"![PyCaret](https:\/\/miro.medium.com\/max\/1024\/1*Cku5-rqmqSIuhUyFkIAdIA.png)","3acd07f7":"#### A few observations:\n\nI have another Notebook: https:\/\/www.kaggle.com\/rnepal2\/lightgbm-optuna-song-popularity-prediction where I have acheived a decent LB score with LightGBM model with parameter tuning. Based on some other experiments I have done, a few observations:\n\n - Imputation with IterativeImputer works pretty well\n - Feature transormations are not helping to increase the performance, rather oppositive most of the times.\n - Scaling of SOME parameters can be helpful.\n - Oversmapling with SMOTENC is also not helping much.\n - Models ensembling helps well - the extent will vary based on the models inlcuded and the averaging weights. \n \n \n##### *Upvote will be much appreciated!*\n\nHappy *Kaggling*!","552710b0":"# PyCaret: Automated ML\n\n- Models comparision and top 3 models selection\n- Models tuning\n- Best models blending:\n    ##### Features transformation, nan value imputation, and models blending\/stacking\/ensembling will likely become key to this competition! \n- Prediction on test data with blended model\n- Results submission"}}