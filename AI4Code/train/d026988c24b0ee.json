{"cell_type":{"f4c767ae":"code","064509cc":"code","361963bb":"code","2f583798":"code","db42d174":"code","1c651171":"code","e135e913":"code","3c141c40":"code","9d78960c":"code","19a16656":"code","56274a9b":"code","c3b02d14":"code","88ab1e8a":"code","8ee54777":"code","d67ad224":"markdown","eeef05a2":"markdown","bb9a48dc":"markdown","786c70d9":"markdown","f39055a4":"markdown","51b6aa7d":"markdown","e23f2b95":"markdown","c0d1797f":"markdown","d3b1a194":"markdown","a8aac7c2":"markdown","2774deec":"markdown","c91994ff":"markdown","95b5e235":"markdown","e0bd34b6":"markdown","3c3623c5":"markdown","061bfb80":"markdown","26cb396e":"markdown","c5ed2579":"markdown"},"source":{"f4c767ae":"# Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('..\/input\/voice.csv')\ndata.info()","064509cc":"# Top 10 records in the dataset\ndata.head(10)","361963bb":"data.label = [1 if each=='female' else 0 for each in data.label]\n# Our y-axis(Outcome)\ny = data.label.values\n# Our features for prediction&training, x will include all of data except the outcome(label)\nx_data = data.drop([\"label\"],axis=1)","2f583798":"# Find the max&min value of the each column then apply the formula. This is a way to re-scaling\nx = (x_data -np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\nprint(x.head().kurt)","db42d174":"# Select Train-Test split randomly every-time\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\n# Features - Records\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","1c651171":"# Initialize Parameters(dimensin = count of the features)\ndef initialize_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","e135e913":"# Sigmoid function\n# Calculation of z\n# z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))\n# test\nprint(sigmoid(0))","3c141c40":"# Forward and Backward Propogarion Combined\ndef ForwardBackwardP(w,b,x_train,y_train):\n    #Forward Propogation\n    z = np.dot(w.T,x_train)+b # Multiply with weight then sum each data\n    y_head = sigmoid(z) # Scale the result into a probablistic value\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    # Cost is summation of all losses\n    cost = (np.sum(loss))\/x_train.shape[1] # x_train.shape[1] is count of the samples\n    # Divided to sample size because of scaling\n    \n    # Backward Propogation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    # Save into Dictionary\n    gradients = {\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    return cost,gradients","9d78960c":"# Updating Learning Parameters(wi,b)\ndef update(w,b,x_train,y_train,learning_rate,iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Updating learning parameters by number of iterations\n    for i in range(iterations):\n        # Make forward and backward propogation and find cost and gradients\n        cost,gradients = ForwardBackwardP(w,b,x_train,y_train)\n        cost_list.append(cost)\n        #UPDATE\n        w = w- learning_rate*gradients[\"derivative_weight\"]\n        b = b- learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n            \n        # Update learn parameters and bias\n        parameters = {\"weight\":w,\"bias\":b}\n        \n    # Plot\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","19a16656":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b) # Forward propogation for x_test\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction\n    ","56274a9b":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,iterations):\n    # Initialize \n    dimension = x_train.shape[0] # Feature size\n    w,b = initialize_weight_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return y_prediction_test\n    \ny_pred_test = logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, iterations = 100)\n","c3b02d14":"# Top 20 prediction and real values\nhead_y_pred= [\"Female\" if each == 1 else \"Male\" for each in y_pred_test[0][:20]]\nhead_y_real = [\"Female\" if each == 1 else \"Male\" for each in y_test[:20]]\nprint(head_y_pred)\nprint(head_y_real)","88ab1e8a":"# Count of male-female\nmale_count = 0 \nfemale_count = 0\nfor i in range(y_pred_test.shape[1]):\n    if y_pred_test[0][i] == 0:\n        male_count+=1\n    else:\n        female_count+=1\n\n# Visualization\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nlabels = ['Male','Female']\nvalues = [male_count,female_count]\n\ntrace = go.Pie(labels=labels, values=values)\n\npy.iplot([trace], filename='basic_pie_chart')","8ee54777":"# Calculation of rate of raw test data\nmale_test=0\nfemale_test=0\nfor i in y_test:\n    if i == 0:\n        male_test+=1\n    else:\n        female_test+=1\n\nvalues = [male_test,female_test]\ntrace = go.Pie(labels=labels, values=values)\npy.iplot([trace], filename='basic_pie_chart')","d67ad224":"## INTRODUCTION\n\n### In this notebook, gender will be predicted with Logistic Regression algorithm. I will apply this algorithm by hand, I won't use scikit-learn library directly.\nThe steps of the work is below\n\n**1. EDA(Expolatory Data Analysis) for the Gender Recognition by Voice dataset**\n\n   * [Getting familiar with the dataset](#get-familiar)\n    \n**2. Normalization (If needed)**\n\n* [Normalization](#normalization)\n\n**3. Logistic Regression implementation with python (Scikit-learn library will be used different parts of the algorithm)**\n\n* [Choosing train-test split randomly](#traintest)\n\n* [Initialize Parameters (w,b)](#initparam)\n\n* [Implementing Sigmoid function](#sigmoid)\n\n* [Calculation of cost-loss function](#cost)\n\n* [Forward-Backward function](#fb)\n\n* [Purpose of the Derivation *-(Optimization)*](#derivation)\n\n* [Updating Weight and Bias](#update)\n\n* [Prediction](#predict)\n\n* [Logistic Regression](#lr)\n\n* [Interpreting the result via visualization](#visualization)\n\n**[References](#references)**\n","eeef05a2":" <a id=\"normalization\"><\/a> <br>\n ## NORMALIZATION\n\nNormalization have to applied to dataset, because some features can brake the balance with their proportion to other features like kurtosis and skew. These attributes should be re-scaled.","bb9a48dc":"<a id=\"get-familiar\"><\/a> <br>\n## Getting familiar with the dataset","786c70d9":"***IS THERE ANY MISSING FEATURE VALUE?***\n\nWe can see that, there is no missing data.","f39055a4":"## Rate of male-female for real test data\nWe can see accuracy of the our prediction","51b6aa7d":"<a id=\"cost\"><\/a> <br>\n## Cost\/Loss Function\nWe want to minimize the cost, hence we will take derivative of the funvtion then, when the derivative equal to zero the root of the function gives to minimum value. \n\n![](http:\/\/image.ibb.co\/dFCR3H\/6.jpg)\nFigure 3 : Loss function **(Cross Entropy Formula)**","e23f2b95":"## Logistic Regression Steps\n\nLogistic Regression Algorithm's steps are represented below in computation graph way:\n\n![](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression_files\/logistic_regression_schematic.png)\n*Source: [https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression_files\/logistic_regression_schematic.png](http:\/\/)*","c0d1797f":"<a id=\"derivation\"><\/a> <br>\n## PURPOSE OF THE DERIVATION\n\n![](http:\/\/image.ibb.co\/dAaYJH\/7.jpg)\nFigure 4 : Purpose of the Derivation","d3b1a194":"<a id=\"lr\"><\/a> <br>\n## Logistic Regression","a8aac7c2":"<a id=\"visualization\"><\/a> <br>\n# Visualization\n## Rate of the Male-Female Predictions","2774deec":" <a id=\"traintest\"><\/a> <br>\n ## TRAIN TEST SPLIT\n\n20% of the data will be used for the test, rest of the data will be used for the training.","c91994ff":"<a id=\"sigmoid\"><\/a> <br>\n## IMPLEMENTING SIGMOID FUNCTION\n\nSigmoid function is the one of the magical function from the mathland. It scales our data to probabilistic values. All of data will be represented in between 0 and 1 correctly. \nAfter the implementation we can check the correctness the function f(0)=0.5. \nHere is the graph and formula of the sigmoid function.\n\n![](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression_files\/logistic_function.png)\n\n*Figure1 : https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression_files\/logistic_function.png*","95b5e235":"*  References : This work is includes some parts of 'kanncaa1's deep-learning kernel. Thanks for sharing.*\n*  Source: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners*","e0bd34b6":"<a id=\"initparam\"><\/a> <br>\n## Initializing Parameters ","3c3623c5":"<a id=\"update\"><\/a> <br> \n# Update Learning Parameters (Weights and Bias)\n\nAlgorithm should update the weights and bias by number of iterations. While we doing this, we will use **learning_rate parameter** It is coefficient of derivatives and it is a *hyperparameter*. So it means we should tune value of the paramter by manually.\n\n![](http:\/\/image.ibb.co\/hYTTJH\/8.jpg)\n> Alpha value = learning_rate","061bfb80":"<a id=\"predict\"><\/a> <br>\n# Prediction","26cb396e":"***TRANSLATE MALE-FEMALE OPTIONS TO COMPUTER'S LANGUAGE***\n\nIn order to make train,test, and prediction, we have to convert our binary male-female option into 0-1's. I choose male as '0' and female as '1'. ","c5ed2579":"<a id=\"references\"><\/a> <br>\n# References"}}