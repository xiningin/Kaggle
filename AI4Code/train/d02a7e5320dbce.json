{"cell_type":{"04dd2274":"code","14d84893":"code","8333ef42":"code","8e573bae":"code","73e34c72":"code","e4501c2d":"code","d73d41ee":"code","c10c8528":"code","8368c91d":"code","6ac426f2":"code","ab729a7c":"code","c3eba11d":"code","b945032b":"code","c3dce0ce":"code","a8495f62":"code","83b1cfb2":"code","b7b8a7d3":"code","90abde08":"code","f4907328":"code","75fb5091":"code","1521eebd":"code","d6595751":"code","8dc72652":"code","a5d66d84":"code","84e86928":"code","9452062f":"code","9b1c2b9f":"code","f983b394":"code","0ed19bff":"code","4cf13b2b":"code","3ec12419":"code","53326817":"code","0fc2ed0e":"code","ba366457":"code","5fc2d7f6":"code","c45cdbad":"code","a1d1af83":"code","27da02c3":"code","51b7ff3d":"code","fb19da74":"code","8fa12d22":"code","24deba29":"code","60e71fe8":"code","50f77cdd":"code","61b21c9d":"code","da7a6ceb":"code","720af728":"code","80b0a4a2":"code","30bbfba2":"code","97220f3b":"code","16bbf087":"code","e37c5581":"code","7187348d":"code","5fcd19e3":"code","73c9aa4d":"code","c7f8b673":"code","15786a44":"code","2d4f98f9":"markdown","6eb93318":"markdown","c823216d":"markdown","62b66892":"markdown","058be33f":"markdown","91053dbf":"markdown","3134275f":"markdown","7044dcc5":"markdown","c373635b":"markdown","cee630de":"markdown","a9adec5d":"markdown","98d4548f":"markdown","b5c62ae3":"markdown","c35c0a3a":"markdown","6ee7835e":"markdown","50598dc0":"markdown","4daaaf91":"markdown","cfbe6bf3":"markdown","5c151c64":"markdown","9c143fb9":"markdown","afbef524":"markdown","96faf7d7":"markdown","02ddf82a":"markdown","a38cd8cf":"markdown","5bfff97b":"markdown","43780c85":"markdown","60fd0aa8":"markdown","3e877508":"markdown","e0c9b522":"markdown","932b8548":"markdown","e7d89b8f":"markdown","23b8d35b":"markdown","e8eaab2c":"markdown"},"source":{"04dd2274":"#!pip list # list packages ","14d84893":"# import packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nsns.set()\n%matplotlib inline","8333ef42":"rd = load_wine()\nX, y = load_wine(return_X_y=True)\n\ndf = pd.DataFrame(X, columns=rd.feature_names)\ndf['target'] = y\ndf.head()","8e573bae":"df.target.value_counts()","73e34c72":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:,-1], random_state=34)","e4501c2d":"import matplotlib.pyplot as plt\n\nmask = np.zeros_like(X_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nplt.figure(figsize=(10,10))\nplt.title(\"Wine Feature Correlation Matrix\", fontsize=40)\nx = sns.heatmap(\n    X_train.corr(), \n    cmap='coolwarm',\n    annot=True,\n    mask=mask,\n    linewidths = .5,\n    vmin = -1, \n    vmax = 1,\n)","d73d41ee":"# x.get_figure().savefig('images\/wine_corr.png')","c10c8528":"X_train.corr()","8368c91d":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=34)","6ac426f2":"logistic_regression_model = LogisticRegression(random_state=34, solver='lbfgs', multi_class=\"auto\", n_jobs=-1, C=1)\nlogistic_regression_model.fit(X_train, y_train)","ab729a7c":"accuracy_score = logistic_regression_model.score(X_val, y_val)\naccuracy_score ","c3eba11d":"predictions = logistic_regression_model.predict(X_val)\npredictions[:10]","b945032b":"confusion_matrix(y_val, predictions)","c3dce0ce":"logistic_regression_model = LogisticRegression(random_state=34, solver='saga', multi_class=\"auto\", n_jobs=-1, C=1)\nlogistic_regression_model.fit(X_train, y_train)","a8495f62":"accuracy_score = logistic_regression_model.score(X_val, y_val)\naccuracy_score ","83b1cfb2":"predictions = logistic_regression_model.predict(X_val)\npredictions[:10]","b7b8a7d3":"confusion_matrix(y_val, predictions)","90abde08":"from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\nX_train = mm_scaler.fit_transform(X_train)\nX_val = mm_scaler.transform(X_val)\nX_test = mm_scaler.transform(X_test)","f4907328":"logistic_regression_model_scaled = LogisticRegression(random_state=34, solver='saga', multi_class=\"auto\", n_jobs=-1, C=1)\nlogistic_regression_model_scaled.fit(X_train, y_train)\naccuracy_score = logistic_regression_model_scaled.score(X_val, y_val)\naccuracy_score ","75fb5091":"predictions = logistic_regression_model_scaled.predict(X_val)\npredictions[:10]","1521eebd":"confusion_matrix(y_val, predictions)","d6595751":"from sklearn.model_selection import GridSearchCV\n\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:,-1], random_state=34)","8dc72652":"solver_list = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\nparameters = dict(solver=solver_list)\nlr = LogisticRegression(random_state=34, multi_class=\"auto\", n_jobs=-1, C=1)\nclf = GridSearchCV(lr, parameters, cv=5)","a5d66d84":"clf.fit(X_train, y_train)","84e86928":"clf.cv_results_['mean_test_score']","9452062f":"scores = clf.cv_results_['mean_test_score']\nfor score, solver, in zip(scores, solver_list):\n    print(f\"{solver}: {score:.3f}\")","9b1c2b9f":"sns.barplot(x=solver_list, y=scores). set_title(\"Wine Accuracy with Unscaled Features\")","f983b394":"ax = sns.barplot(x=solver_list, y=scores)\nax.set_title(\"Wine Accuracy with Unscaled Features\", fontsize = 20)","0ed19bff":"fig = ax.get_figure()\n# fig.savefig('images\/wine_unscaled.png')","4cf13b2b":"params = dict(solver=solver_list)\nlog_reg = LogisticRegression(C=1, n_jobs=-1, random_state=34)\nclf = GridSearchCV(log_reg, params, cv=5)\nclf.fit(X_train, y_train)","3ec12419":"scores = clf.cv_results_['mean_test_score']\n\nfor score, solver in zip(scores, solver_list):\n    print(f\"  {solver} {score:.3f}\" )","53326817":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","0fc2ed0e":"solver_list = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\nparameters = dict(solver=solver_list)\nlr = LogisticRegression(random_state=34, multi_class=\"auto\", n_jobs=-1, C=1)\nclf = GridSearchCV(lr, parameters, cv=5)\nclf.fit(X_train, y_train)","ba366457":"clf.cv_results_['mean_test_score']\nscores = clf.cv_results_['mean_test_score']\nfor score, solver, in zip(scores, solver_list):\n    print(f\"{solver}: {score:.3f}\")","5fc2d7f6":"ax =sns.barplot(x=solver_list, y=scores).set_title(\"Wine Accuracy with Scaled Features\", fontsize=\"20\")","c45cdbad":"fig = ax.get_figure()\n# fig.savefig('images\/wine_scaled.png')","a1d1af83":"from sklearn.datasets import load_breast_cancer\n\nraw = load_breast_cancer()\nX, y = load_breast_cancer(return_X_y=True)\n\ndf = pd.DataFrame(X, columns=raw.feature_names)\ndf['target'] = y\ndf.head()","27da02c3":"df.target.value_counts()","51b7ff3d":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.loc[:,'target'], random_state=34)","fb19da74":"params = dict(solver=solver_list)\nlog_reg = LogisticRegression(C=1, n_jobs=-1, random_state=34)\nclf = GridSearchCV(log_reg, params, cv=5)\nclf.fit(X_train, y_train)","8fa12d22":"scores = clf.cv_results_['mean_test_score']\n\nfor score, solver in zip(scores, solver_list):\n    print(f\"  {solver} {score:.3f}\" )","24deba29":"sns.barplot(x=solver_list, y=scores). set_title(\"Cancer Accuracy with Unscaled Features\")\nsns.set()","60e71fe8":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","50f77cdd":"params = dict(solver=solver_list)\nlog_reg = LogisticRegression(C=1, n_jobs=-1, random_state=34)\nclf = GridSearchCV(log_reg, params, cv=5)\nclf.fit(X_train, y_train)","61b21c9d":"scores = clf.cv_results_['mean_test_score']\n\nfor score, solver in zip(scores, solver_list):\n    print(f\"  {solver} {score:.3f}\" )","da7a6ceb":"wine_results_scaled = zip(scores, solver_list)","720af728":"sns.barplot(x=solver_list, y=scores). set_title(\"Cancer Accuracy with Scaled Features\")","80b0a4a2":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","30bbfba2":"log_reg.intercept_","97220f3b":"log_reg.coef_","16bbf087":"odds = [(prob\/10)\/(1-(prob\/10)) for prob in range(1,10)]\nodds","e37c5581":"for prob, odd in zip(range(1, 10), odds):\n    print(prob\/10, odd)","7187348d":"from math import log","5fcd19e3":"log_odds = [ log(odd) for odd in odds]\nlog_odds","73c9aa4d":"a = [1, 1, 2, 3, 4]\nb = [2, 2, 3, 2, 1]\nc = [4, 6, 7, 8, 9]\nd = [4, 3, 4, 5, 4]\n\ndf = pd.DataFrame({'a':a,'b':b,'c':c,'d':d})","c7f8b673":"df","15786a44":"diagonal_value_list = []\n\nmat = np.array(df.values)\n\ndf_cor = df.corr()\ndf_cor_inv = pd.DataFrame(np.linalg.inv(df.corr().values), index = df_cor.index, columns=df_cor.columns)\n\nfor a in range(len(df.columns)):\n    val = df_cor_inv.iloc[a,a]\n    diagonal_value_list.append(val)\n    \nprint(dict(zip(df_cor.columns, [x.round() for x in diagonal_value_list])))","2d4f98f9":"## Let's make a bar chart.","6eb93318":"Let's make some pretty charts.","c823216d":"Let's look at what happens if we scale the data before fitting the model with the *saga* solver.","62b66892":"Let's print the accuracy for each of the solvers.","058be33f":"# Don't Sweat the Solver Stuff: Tips for Better Logistic Regression Models in Scikit-Learn\n## Jeff Hale\n\n### This is a notebook is for analysis used in this article: [Don't Sweat the Solver Stuff: Tips for Better Logistic Regression Models in Scikit-Learn](https:\/\/towardsdatascience.com\/dont-sweat-the-solver-stuff-aea7cddc3451?source=friends_link&sk=6cab311ddbae8ab168c9e347ae1bc707). See the article for discussion.","91053dbf":"## Logistic Regression with *saga* solver.","3134275f":"We have more positive cases than negative.","7044dcc5":"## Logistic Regression with *lbfgs* solver","c373635b":"Let's split the data into training, validation, and test sets. Let's make the test set first.","cee630de":"### log odds","a9adec5d":"## Scale features","98d4548f":"That's it. \n\n# Thanks for reading! \ud83d\ude00 Please upvote if you found this helpful. And check out the accompanying article [Don't Sweat the Solver Stuff: Tips for Better Logistic Regression Models in Scikit-Learn](https:\/\/towardsdatascience.com\/dont-sweat-the-solver-stuff-aea7cddc3451?source=friends_link&sk=6cab311ddbae8ab168c9e347ae1bc707). \ud83d\udc4d","b5c62ae3":"Now let's split a validation set off of the training set.","c35c0a3a":"The model with *saga* as the solver always predicted class 0 or 1 and didn't converge. ","6ee7835e":"*sag* and *saga* do better relative to their earlier performance with wine. Let's see how they do when scaled.","50598dc0":"Let's do some quick EDA by looking at a correlation matrix of the features.","4daaaf91":"Let's make the title bigger.","cfbe6bf3":"## Scaled data with *saga* solver logistic regression.","5c151c64":"## VIF \nShowing how to compute the variance inflation factor.","9c143fb9":"94% is a pretty decent accuracy. The dataset is small. We're not going to hyperparameter search over values of *C*, but we would vary the amount of regularization in a larger project. We could also use cross validation.","afbef524":"## GridSearch over parameters","96faf7d7":"Just to get an error for a screenshot for the article.","02ddf82a":"On this small data set we learned that Logistic Regression with the *saga* solver benefits from scaled features.","a38cd8cf":"## Let's make a bar chart with scores now that the features are scaled.","5bfff97b":"That looks reasonable. Let's look at the count of each outcome class.","43780c85":"# Let's look at the breast_cancer dataset from Scikit-learn for an example of binary logisitc regression.","60fd0aa8":"Much better for *sag* and *saga*. Now everyone is on par. :)","3e877508":" *liblinear*, *newton-cg*, *lbfgs* are pretty equivalent, but *sag* and *saga* aren't perfomring nearly as well.","e0c9b522":"## Let's make a bar chart with scores now that the features are scaled.","932b8548":"Once again, *saga* and *solver* perform on par with the other solvers when the scores are scaled.","e7d89b8f":"Load the data into a Pandas DataFrame","23b8d35b":"Let's try scaling the features.","e8eaab2c":"## Exploring log odds and probabiilty\n### odds to probability"}}