{"cell_type":{"d67335bf":"code","01f1bd7b":"code","d4ed66d8":"code","af2d0a70":"code","4acf457c":"code","733c2fbe":"code","0f84acf3":"code","00083694":"code","b4a485b8":"code","cf6ece82":"code","128b35da":"code","d9b3d7b3":"code","21913b3c":"code","160e8f7a":"code","70adab3a":"code","a5bc7c10":"code","89ec6b1e":"code","9c3877ea":"code","4b4fa8d5":"code","ff573b98":"code","f991dff2":"code","e67ca83c":"code","107f1146":"code","41e423d8":"code","962f31c0":"code","795ff2de":"code","acde75f5":"markdown","fb1f61a4":"markdown","0ad214d5":"markdown","3e655d78":"markdown","e4e3fb17":"markdown","0f326671":"markdown","d2ebd454":"markdown","0c59dc04":"markdown","55f3669e":"markdown","a36d585b":"markdown","5162945e":"markdown","f441a841":"markdown"},"source":{"d67335bf":"from collections import Counter\nimport itertools\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom scipy.sparse import linalg \nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity","01f1bd7b":"df = pd.read_csv('..\/input\/abcnews-date-text.csv')\ndf.head()","d4ed66d8":"headlines = df['headline_text'].tolist()\n# remove stopwords\nstopwords_set = set(stopwords.words('english'))\nheadlines = [\n    [tok for tok in headline.split() if tok not in stopwords_set] for headline in headlines\n]\n# remove single word headlines\nheadlines = [hl for hl in headlines if len(hl) > 1]\n# show results\nheadlines[0:20]","af2d0a70":"tok2indx = dict()\nunigram_counts = Counter()\nfor ii, headline in enumerate(headlines):\n    if ii % 200000 == 0:\n        print(f'finished {ii\/len(headlines):.2%} of headlines')\n    for token in headline:\n        unigram_counts[token] += 1\n        if token not in tok2indx:\n            tok2indx[token] = len(tok2indx)\nindx2tok = {indx:tok for tok,indx in tok2indx.items()}\nprint('done')\nprint('vocabulary size: {}'.format(len(unigram_counts)))\nprint('most common: {}'.format(unigram_counts.most_common(10)))","4acf457c":"# note add dynammic window hyperparameter\nback_window = 2\nfront_window = 2\nskipgram_counts = Counter()\nfor iheadline, headline in enumerate(headlines):\n    for ifw, fw in enumerate(headline):\n        icw_min = max(0, ifw - back_window)\n        icw_max = min(len(headline) - 1, ifw + front_window)\n        icws = [ii for ii in range(icw_min, icw_max + 1) if ii != ifw]\n        for icw in icws:\n            skipgram = (headline[ifw], headline[icw])\n            skipgram_counts[skipgram] += 1    \n    if iheadline % 200000 == 0:\n        print(f'finished {iheadline\/len(headlines):.2%} of headlines')\n        \nprint('done')\nprint('number of skipgrams: {}'.format(len(skipgram_counts)))\nprint('most common: {}'.format(skipgram_counts.most_common(10)))","733c2fbe":"row_indxs = []\ncol_indxs = []\ndat_values = []\nii = 0\nfor (tok1, tok2), sg_count in skipgram_counts.items():\n    ii += 1\n    if ii % 1000000 == 0:\n        print(f'finished {ii\/len(skipgram_counts):.2%} of skipgrams')\n    tok1_indx = tok2indx[tok1]\n    tok2_indx = tok2indx[tok2]\n        \n    row_indxs.append(tok1_indx)\n    col_indxs.append(tok2_indx)\n    dat_values.append(sg_count)\n    \nwwcnt_mat = sparse.csr_matrix((dat_values, (row_indxs, col_indxs)))\nprint('done')","0f84acf3":"def ww_sim(word, mat, topn=10):\n    \"\"\"Calculate topn most similar words to word\"\"\"\n    indx = tok2indx[word]\n    if isinstance(mat, sparse.csr_matrix):\n        v1 = mat.getrow(indx)\n    else:\n        v1 = mat[indx:indx+1, :]\n    sims = cosine_similarity(mat, v1).flatten()\n    sindxs = np.argsort(-sims)\n    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n    return sim_word_scores\n\nprint('done')","00083694":"ww_sim('strike', wwcnt_mat)","b4a485b8":"wwcnt_norm_mat = normalize(wwcnt_mat, norm='l2', axis=1)\nprint('done')","cf6ece82":"ww_sim('strike', wwcnt_norm_mat)","128b35da":"num_skipgrams = wwcnt_mat.sum()\nassert(sum(skipgram_counts.values())==num_skipgrams)\n\n# for creating sparse matrices\nrow_indxs = []\ncol_indxs = []\n\n# pmi: pointwise mutual information\npmi_dat_values = []\n# ppmi: positive pointwise mutual information\nppmi_dat_values = []\n# spmi: smoothed pointwise mutual information\nspmi_dat_values = []\n# sppmi: smoothed positive pointwise mutual information\nsppmi_dat_values = []\n\n# Sum over words and contexts\nsum_over_words = np.array(wwcnt_mat.sum(axis=0)).flatten()\nsum_over_contexts = np.array(wwcnt_mat.sum(axis=1)).flatten()\n\n# Smoothing\n# According to [Levy, Goldberg & Dagan, 2015], the smoothing operation \n# should be done on the context \nalpha = 0.75\nnca_denom = np.sum(sum_over_contexts**alpha)\n# sum_over_words_alpha = sum_over_words**alpha\nsum_over_contexts_alpha = sum_over_contexts**alpha\n\nii = 0\nfor (tok1, tok2), sg_count in skipgram_counts.items():\n    ii += 1\n    if ii % 1000000 == 0:\n        print(f'finished {ii\/len(skipgram_counts):.2%} of skipgrams')\n    tok1_indx = tok2indx[tok1]\n    tok2_indx = tok2indx[tok2]\n    \n    nwc = sg_count\n    Pwc = nwc \/ num_skipgrams\n\n    nw = sum_over_contexts[tok1_indx]\n    Pw = nw \/ num_skipgrams\n    \n    nc = sum_over_words[tok2_indx]\n    Pc = nc \/ num_skipgrams\n    \n    pmi = np.log2(Pwc\/(Pw*Pc))\n    ppmi = max(pmi, 0)\n    \n#   nca = sum_over_words_alpha[tok2_indx]\n    nca = sum_over_contexts_alpha[tok2_indx]\n    Pca = nca \/ nca_denom\n\n    spmi = np.log2(Pwc\/(Pw*Pca))\n    sppmi = max(spmi, 0)\n    \n    row_indxs.append(tok1_indx)\n    col_indxs.append(tok2_indx)\n    pmi_dat_values.append(pmi)\n    ppmi_dat_values.append(ppmi)\n    spmi_dat_values.append(spmi)\n    sppmi_dat_values.append(sppmi)\n        \npmi_mat = sparse.csr_matrix((pmi_dat_values, (row_indxs, col_indxs)))\nppmi_mat = sparse.csr_matrix((ppmi_dat_values, (row_indxs, col_indxs)))\nspmi_mat = sparse.csr_matrix((spmi_dat_values, (row_indxs, col_indxs)))\nsppmi_mat = sparse.csr_matrix((sppmi_dat_values, (row_indxs, col_indxs)))\n\nprint('done')","d9b3d7b3":"ww_sim('strike', pmi_mat)","21913b3c":"ww_sim('strike', ppmi_mat)","160e8f7a":"ww_sim('strike', spmi_mat)","70adab3a":"ww_sim('strike', sppmi_mat)","a5bc7c10":"pmi_use = ppmi_mat\nembedding_size = 50\nuu, ss, vv = linalg.svds(pmi_use, embedding_size) \n\nprint('done')","89ec6b1e":"print('vocab size: {}'.format(len(unigram_counts)))\nprint('embedding size: {}'.format(embedding_size))\nprint('uu.shape: {}'.format(uu.shape))\nprint('ss.shape: {}'.format(ss.shape))\nprint('vv.shape: {}'.format(vv.shape))","9c3877ea":"unorm = uu \/ np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\nvnorm = vv \/ np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n#word_vecs = unorm\n#word_vecs = vnorm.T\nword_vecs = uu + vv.T\nword_vecs_norm = word_vecs \/ np.sqrt(np.sum(word_vecs*word_vecs, axis=1, keepdims=True))\n\nprint('done')","4b4fa8d5":"def word_sim_report(word, sim_mat):\n    sim_word_scores = ww_sim(word, word_vecs)\n    for sim_word, sim_score in sim_word_scores:\n        print(sim_word, sim_score)\n        word_headlines = [hl for hl in headlines if sim_word in hl and word in hl][0:5]\n        for headline in word_headlines:\n            print(f'    {headline}')\n            \nprint('done')","ff573b98":"word = 'strike'\nword_sim_report(word, word_vecs)","f991dff2":"word = 'war'\nword_sim_report(word, word_vecs)","e67ca83c":"word = 'bank'\nword_sim_report(word, word_vecs)","107f1146":"word = 'car'\nword_sim_report(word, word_vecs)\n","41e423d8":"word = 'football'\nword_sim_report(word, word_vecs)","962f31c0":"word = 'tech'\nword_sim_report(word, word_vecs)","795ff2de":"# check a few things\nalpha = 0.75\ndsum = wwcnt_mat.sum()\nnwc = skipgram_counts[(tok1, tok2)]\nPwc = nwc \/ dsum\n\nindx1 = tok2indx[tok1]\nindx2 = tok2indx[tok2]\n\nnw = wwcnt_mat[indx1, :].sum()\nPw = nw \/ dsum\nnc = wwcnt_mat[:, indx2].sum()\nPc = nc \/ dsum\n\nnca = nc**alpha\nnca_denom = np.sum(np.array(wwcnt_mat.sum(axis=0)).flatten()**alpha)\nPca = nca \/ nca_denom\n\nprint('dsum=', dsum)\nprint('Pwc=', Pwc)\nprint('Pw=', Pw)\nprint('Pc=', Pc)\nprint('Pca=', Pca)\npmi1 = Pwc \/ (Pw * Pc)\npmi1a = Pwc \/ (Pw * Pca)\npmi2 = (nwc * dsum) \/ (nw * nc)\n\nprint('pmi1=', pmi1)\nprint('pmi1a=', pmi1a)\nprint('pmi2=', pmi2)","acde75f5":"# Unigrams\nNow lets calculate a unigram vocabulary.  The following code assigns a unique ID to each token, stores that mapping in two dictionaries (`tok2indx` and `indx2tok`), and counts how often each token appears in the corpus. ","fb1f61a4":"# Read Data and Preview","0ad214d5":"# Word Similarity with Sparse PMI Matrices","3e655d78":"# Word-Word Count Matrix\nOur very first word vectors will come from a word-word count matrix.  We can (equivalently) take the word vectors to be the rows or columns.  ","e4e3fb17":"# Sparse Matrices\n\nWe will calculate several matrices that store word-word information.  These matrices will be $N \\times N$ where $N \\approx 100,000$ is the size of our vocabulary.  We will need to use a sparse format so that it will fit into memory.  A nice implementation is available in [scipy.sparse.csr_matrix](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.csr_matrix.html).  To create these sparse matrices we create three iterables that store row indices, column indices, and data values. ","0f326671":"# Minimal Preprocessing\nWe're going to create a word-word co-occurrence matrix from the text in the headlines.  We will define two words as \"co-occurring\" if they appear in the same headline.  Using this definition, single word headlines are not interestintg for us.  Lets remove them as well as a common set of english stopwords.  ","d2ebd454":"# Word Vectors from Decomposing a Word-Word Pointwise Mutual Information Matrix\nLets create some simple [word vectors](https:\/\/en.wikipedia.org\/wiki\/Word_embedding) by applying a [singular value decomposition](https:\/\/en.wikipedia.org\/wiki\/Singular-value_decomposition) to a [pointwise mutual information](https:\/\/en.wikipedia.org\/wiki\/Pointwise_mutual_information) word-word matrix.  There are many other ways to create word vectors, but matrix decomposition is one of the most straightforward.  A well cited description of the technique used in this notebook can be found in Chris Moody's blog post [Stop Using word2vec](https:\/\/multithreaded.stitchfix.com\/blog\/2017\/10\/18\/stop-using-word2vec\/).    If you are interested in reading further about the history of word embeddings and a discussion of modern approaches check out the following blog post by Sebastian Ruder, [An overview of word embeddings and their connection to distributional semantic models](http:\/\/blog.aylien.com\/overview-word-embeddings-history-word2vec-cbow-glove\/).  Especially interesting to me is the work by Omar Levy, Yoav Goldberg, and Ido Dagan which shows that tuning hyperparameters is as (if not more) important as the algorithm chosen to build word vectors. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/570).\n\nWe will be using the [\"A Million News Headlines\"](https:\/\/www.kaggle.com\/therohk\/million-headlines) dataset which contains headlines published over a period of 15 years from the Australian Broadcasting Corporation (ABC).\nIt is a great clean corpus that is large enough to be interesting and small enough to allow for quick calculations.  In this notebook tutorial we will implement as much as we can without using libraries that obfuscate the algorithm.  We're not going to write our own linear algebra or sparse matrix routines, but we will calculate unigram frequency, skipgram frequency, and the pointwise mutual information matrix \"by hand\".  Hopefully this will make the method easier to understand!   ","0c59dc04":"# Word Similarity with Sparse Count Matrices","55f3669e":"# Pointwise Mutual Information Matrices\nThe pointwise mutual information (PMI) for a (word, context) pair in our corpus is defined as the probability of their co-occurrence divided by the probabilities of them appearing individually, \n<br\/><br\/>\n$$\n\\large {\\rm pmi}(w, c) = \\log \\frac{p(w, c)}{p(w) p(c)}\n$$\n<br\/>\n$$  \\large p(w_i,c_j) = \\frac{\\#(w_i,c_j)}{ \\sum\\limits_{k}^{N}\\sum\\limits_{l}^{N} \\#(w_k,c_l) }$$\n<br\/>\n$$ \\large p(w_i) = \\frac{\\sum\\limits_{i}^{N}\\#(w_i)}{ \\sum\\limits_{k}^{N}\\sum\\limits_{l}^{N} \\#(w_k,c_l) }$$\n<br\/>\n$$ \\large p(c_j) = \\frac{\\sum\\limits_{j}^{N}\\#(c_j)}{ \\sum\\limits_{k}^{N}\\sum\\limits_{l}^{N} \\#(w_k,c_l) }$$\n<br\/>\nwhere $\\large \\#(w_k,c_l)$ is the word-word count matrix we defined above.\nIn addition we can define the positive pointwise mutual information as, \n<br\/><br\/>\n$$\n{\\large \\rm ppmi}(w, c) = {\\rm max}\\left[{\\rm pmi(w,c)}, 0 \\right]\n$$\n\nNote that the definition of PMI above implies that $\\large {\\rm pmi}(w, c) = {\\rm pmi}(w, c)$ and so this matrix will be symmetric.  ","a36d585b":"# Scratch Pad","5162945e":"# Skipgrams\nNow lets calculate a skipgram vocabulary.  We will loop through each word in a headline (the focus word) and then form skipgrams by examing `back_window` words behind and `front_window` words in front of the focus word (the context words).  As an example, the first sentence (after preprocessing removes the stopword `against`) ,\n```\naba decides community broadcasting licence\n```\nwould produce the following skipgrams with `back_window`=`front_window`=`2`, \n```\n('aba', 'decides')\n('aba', 'community')\n('decides', 'aba')\n('decides', 'community')\n('decides', 'broadcasting')\n('community', 'aba')\n('community', 'decides')\n('community', 'broadcasting')\n('community', 'licence')\n('broadcasting', 'decides')\n('broadcasting', 'community')\n('broadcasting', 'licence')\n('licence', 'community')\n('licence', 'broadcasting')\n```","f441a841":"# Singular Value Decomposition\nWith the PMI and PPMI matrices in hand, we can apply a singular value decomposition to create dense word vectors from the sparse ones we've been using. "}}