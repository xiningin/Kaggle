{"cell_type":{"727d738d":"code","58219b66":"code","ca0c56d8":"code","738ff4b2":"code","638421aa":"code","bc171ee8":"code","37e601ce":"code","df0b23a6":"code","5962603e":"code","ac13a6d1":"code","dbd1206c":"code","64c0ce2b":"code","cf93da8b":"code","65d1752e":"code","e06a2aa0":"code","87c38f45":"code","f2974d4d":"code","679abbf5":"code","5ed594ec":"code","8b66297d":"code","595b2c63":"code","d8bdc71f":"code","35f5b09b":"code","c5a4b1bf":"code","085af603":"code","c715fe4d":"code","c8711745":"code","fa726827":"code","82c63621":"code","3e55535f":"code","60fe43d3":"code","7e8b55a9":"code","e26464ac":"code","0a72f6d3":"code","2a18cfe1":"code","4439f32d":"code","e0b0d3bb":"code","634cd3a4":"code","cdc74360":"code","16b28991":"code","4a60850d":"code","6dc73925":"code","42b090b9":"code","b5f4404e":"code","1bf3ca5a":"code","e4b81ab8":"code","bb49423e":"code","5207ef4d":"code","2e45d7f5":"code","14ee79f8":"code","de6c7d3e":"code","3f1751ea":"code","2d2aa4e2":"code","8caeea79":"code","25e2aa37":"code","820ee31d":"code","33bd0d36":"code","9f9c4295":"code","e8c0496d":"code","8414808f":"code","57536d35":"code","f300ae92":"code","be7b9197":"code","9012417c":"code","a81b5894":"code","a8ae6458":"code","57473379":"code","243fe74a":"code","0deaf61f":"code","bf8e775e":"code","3754857f":"code","dcf2bfc3":"code","8d954833":"code","311c2af7":"code","dccbc99a":"code","ddd74534":"code","bd343abd":"code","107abe38":"code","10b7e02d":"code","e10d0c7b":"code","eee053d9":"code","1d26a222":"code","1d744621":"code","d91cc5d2":"code","5e1c685b":"code","9b650bb6":"code","512c706f":"code","75281b02":"code","c4ca944b":"code","91b5dd34":"code","b89ba1be":"code","a46570a9":"code","fc84c136":"code","ae32e1da":"code","b927c927":"code","5b03e42a":"code","d6e81ccc":"code","fc34bee1":"code","6f171574":"code","6f7f3760":"code","7caeb4ed":"code","ae7e1b74":"code","fde44c87":"code","27a30f8a":"code","5524b79a":"code","b3bc74b0":"code","beab6950":"code","9148a267":"code","63eef3d7":"code","a935dcbe":"code","9c717280":"code","ec8ebb24":"markdown","d2e16bad":"markdown","229c816e":"markdown","2f10634f":"markdown","17c89242":"markdown","294b74c1":"markdown","d677cb9c":"markdown","e8afc3f3":"markdown","0cf8ca94":"markdown","9e3db9d5":"markdown","3a8b0da7":"markdown","e60da16f":"markdown","ee32e72b":"markdown","4a405a80":"markdown","20846d1f":"markdown","f226412c":"markdown","8b0e1b20":"markdown","e00fa5b6":"markdown","4eb0b0bd":"markdown","d1347065":"markdown","ab6584a0":"markdown","18680a2e":"markdown","982f74a1":"markdown","94412a94":"markdown","eba12e8e":"markdown","b367003b":"markdown","a1447f0a":"markdown","7a5ae243":"markdown","83a5bd52":"markdown","7634fb4d":"markdown","a4f17c47":"markdown","d2640533":"markdown","fbdea1bc":"markdown","cf0c3e76":"markdown","744646e4":"markdown","4c21ab55":"markdown","7b9cdd92":"markdown","58c55baf":"markdown","6fc288c6":"markdown","1aa0699f":"markdown","df9a3560":"markdown","90758a3b":"markdown"},"source":{"727d738d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58219b66":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","ca0c56d8":"train.head()","738ff4b2":"train.shape","638421aa":"train.info()","bc171ee8":"import seaborn as sns\nimport matplotlib.pyplot as plt","37e601ce":"fig,ax = plt.subplots(1,2,figsize=(15,5))\nax[0].hist(x = train.SalePrice)\nax2 = sns.distplot(x = train.SalePrice,ax=ax[1])","df0b23a6":"sns.boxplot(x= train.OverallQual , y = train.SalePrice)","5962603e":"sns.scatterplot(x= train.GrLivArea , y = train.SalePrice)","ac13a6d1":"sns.boxplot(x= train.TotRmsAbvGrd , y = train.SalePrice)","dbd1206c":"sns.scatterplot(x= train.TotalBsmtSF , y = train.SalePrice)","64c0ce2b":"cat = ['OverallQual','TotRmsAbvGrd','GarageCars','OverallCond','MSSubClass', 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr', 'Fireplaces']","cf93da8b":"y = train['SalePrice']","65d1752e":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","e06a2aa0":"print('Total no. of train samples: ',len(train))\nprint('Total no. of test samples: ',len(test))","87c38f45":"total_data = pd.concat([train.drop('SalePrice',axis=1),test],axis=0,ignore_index = True)\nprint(total_data.shape)","f2974d4d":"total_data.drop('Id',axis=1,inplace=True)","679abbf5":"object_type = total_data.dtypes[total_data.dtypes == 'object'].index\nobject_type","5ed594ec":"correlation_matrix = train.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(correlation_matrix,vmax=0.8,square=True)","8b66297d":"k = 10\ncols = correlation_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(10,10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","595b2c63":"null = total_data.isnull().sum()\nnull_values = pd.DataFrame({'No. of null': null[null != 0].sort_values(ascending=False)})\nnull_values","d8bdc71f":"total_data.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1,inplace=True)\nobject_type = object_type.drop(['PoolQC','MiscFeature','Alley','Fence'])","35f5b09b":"total_data['FireplaceQu'].describe()","c5a4b1bf":"total_data[\"FireplaceQu\"] = total_data[\"FireplaceQu\"].fillna(\"None\")","085af603":"total_data['LotFrontage'].median()","c715fe4d":"x = total_data['LotFrontage'].median()\ntotal_data['LotFrontage'] = total_data['LotFrontage'].fillna(x)","c8711745":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    total_data[col] = total_data[col].fillna('None')","fa726827":"total_data.drop('GarageYrBlt',axis=1,inplace=True)","82c63621":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    total_data[col] = total_data[col].fillna('None')","3e55535f":"total_data['MasVnrArea'] = total_data['MasVnrArea'].fillna(0)","60fe43d3":"total_data['MasVnrType'].value_counts()","7e8b55a9":"total_data['MasVnrType'] = total_data['MasVnrType'].fillna('None')","e26464ac":"total_data['Electrical'].value_counts()","0a72f6d3":"total_data['Electrical'] = total_data['Electrical'].fillna('SBrkr')","2a18cfe1":"total_data['Utilities'].value_counts()","4439f32d":"total_data.drop('Utilities',axis=1,inplace=True)","e0b0d3bb":"object_type = object_type.drop('Utilities')","634cd3a4":"for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF','GarageArea']:\n    total_data[col] = total_data[col].fillna(0)","cdc74360":"total_data['MSZoning'] = total_data['MSZoning'].fillna(total_data['MSZoning'].mode()[0])","16b28991":"total_data['MSZoning'].value_counts()","4a60850d":"total_data['KitchenQual'] = total_data['KitchenQual'].fillna(total_data['KitchenQual'].mode()[0])\ntotal_data['Exterior1st'] = total_data['Exterior1st'].fillna(total_data['Exterior1st'].mode()[0])\ntotal_data['Exterior2nd'] = total_data['Exterior2nd'].fillna(total_data['Exterior2nd'].mode()[0])","6dc73925":"for col in ['GarageCars','BsmtFullBath','BsmtHalfBath'] :\n    total_data[col].fillna(0,inplace=True)","42b090b9":"total_data['SaleType'].value_counts()","b5f4404e":"total_data['SaleType'] = total_data['SaleType'].fillna(total_data['SaleType'].mode()[0])\ntotal_data['Functional'] = total_data['Functional'].fillna('Typ')","1bf3ca5a":"total_data.isnull().sum().sum()","e4b81ab8":"total_data['MSSubClass'] = total_data['MSSubClass'].apply(str)","bb49423e":"object_type = list(object_type) + ['MSSubClass']","5207ef4d":"import sklearn\nfrom sklearn.preprocessing import LabelEncoder","2e45d7f5":"for i in object_type:\n    le = LabelEncoder()\n    le.fit(total_data[i].unique())\n    total_data[i] = le.transform(total_data[i])","14ee79f8":"categorical_features = object_type + cat","de6c7d3e":"total_data['TotalSF'] = total_data['TotalBsmtSF']+ total_data['1stFlrSF'] + total_data['2ndFlrSF']\n\ntotal_data['Exterior'] = total_data['Exterior1st'] + total_data['Exterior2nd']","3f1751ea":"total_data.drop(['GarageArea','1stFlrSF'],axis=1,inplace=True)\ntotal_data.drop(['Exterior1st','Exterior2nd'],axis=1,inplace=True)","2d2aa4e2":"categorical_features.append('Exterior')\ncategorical_features.remove('Exterior1st')\ncategorical_features.remove('Exterior2nd')","8caeea79":"from scipy import stats\nfrom scipy.stats import norm,skew","25e2aa37":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(x = total_data.GrLivArea,y=train.SalePrice,ax=ax[0])\nsns.scatterplot(x = total_data.OverallQual,y=train.SalePrice,ax=ax[1])","820ee31d":"total_data.drop(train[(train['GrLivArea']>4000)&(train['SalePrice']<300000)].index,inplace=True)\ny.drop(train[(train['GrLivArea']>4000)&(train['SalePrice']<300000)].index,inplace=True)","33bd0d36":"sns.scatterplot(x = total_data['GrLivArea'],y=train.SalePrice)","9f9c4295":"sns.scatterplot(x = total_data.TotalSF,y=train.SalePrice)","e8c0496d":"sns.distplot(y,fit=norm)\nfig =plt.figure()\nr = stats.probplot(y,plot = plt)","8414808f":"y = np.log(y)","57536d35":"sns.distplot(y,fit=norm)\nfig =plt.figure()\nr = stats.probplot(y,plot = plt)","f300ae92":"sns.distplot(total_data.GrLivArea,fit=norm)\nfig =plt.figure()\nr = stats.probplot(total_data.GrLivArea,plot = plt)","be7b9197":"total_data['GrLivArea'] = np.log(total_data['GrLivArea'])","9012417c":"sns.distplot(total_data.GrLivArea,fit=norm)\nfig =plt.figure()\nr = stats.probplot(total_data.GrLivArea,plot = plt)","a81b5894":"sns.distplot(total_data.TotalBsmtSF,fit=norm)\nfig =plt.figure()\nr = stats.probplot(total_data.TotalBsmtSF,plot = plt)","a8ae6458":"total_data.loc[total_data['TotalBsmtSF']>0,'TotalBsmtSF'] = np.log(total_data.loc[total_data['TotalBsmtSF']>0,'TotalBsmtSF'])","57473379":"sns.distplot(total_data[total_data['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nr = stats.probplot(total_data[total_data['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","243fe74a":"numerical_features = total_data.drop(categorical_features,axis=1).columns","0deaf61f":"skew = total_data[numerical_features].apply(lambda x:skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'skewness': skew})\nskewness","bf8e775e":"cols = skewness[skewness['skewness'] > 0.5].index","3754857f":"from scipy.special import boxcox1p","dcf2bfc3":"for col in cols :\n    total_data[col] = boxcox1p(total_data[col],0.15)","8d954833":"x_train = total_data[:-len(test)]\nx_test = total_data[-len(test):]","311c2af7":"print(x_train.shape)\nprint(x_test.shape)","dccbc99a":"from sklearn.linear_model import LinearRegression, Lasso , ElasticNet ,Ridge\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import cross_val_score,cross_validate,GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error,make_scorer\nfrom sklearn.pipeline import make_pipeline","ddd74534":"lasso = make_pipeline(RobustScaler(),Lasso())\nscores = cross_validate(lasso,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","bd343abd":"np.sqrt(-scores['train_score'].mean())","107abe38":"np.sqrt(-scores['test_score'].mean())","10b7e02d":"param_grid = {'alpha' : [0.00005,0.0005,0.007,0.1,0.00009], 'max_iter':[1000,2000,1500,2500]}\ngrid = GridSearchCV(Lasso(),param_grid=param_grid,scoring='neg_mean_squared_error')\ngrid.fit(x_train,y)","e10d0c7b":"grid.best_params_","eee053d9":"np.sqrt(-grid.best_score_)","1d26a222":"ridge = make_pipeline(RobustScaler(),Ridge(alpha=10))\nscores2 = cross_validate(ridge,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","1d744621":"np.sqrt(-scores2['train_score'].mean())","d91cc5d2":"np.sqrt(-scores2['test_score'].mean())","5e1c685b":"elastic = make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005))\nscores3 = cross_validate(elastic,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","9b650bb6":"np.sqrt(-scores3['train_score'].mean())","512c706f":"np.sqrt(-scores3['test_score'].mean())","75281b02":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.svm import SVR","c4ca944b":"svm = SVR()\nscores4 = cross_validate(svm,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","91b5dd34":"print(np.sqrt(-scores4['train_score'].mean()))\nprint(np.sqrt(-scores4['test_score'].mean()))      ","b89ba1be":"grid = {'degree':[3,5,7,9,10,15,20],\n    'gamma' : ['scale','auto'],\n       'C':[0.0001,0.001,0.01,0.1,1,10,100,1000],\n       'epsilon':[0.001,0.01,0.1,1,5,10,100]} \nrandom = RandomizedSearchCV(SVR(),grid,scoring='neg_mean_squared_error',cv=5,n_iter=20)\nrandom.fit(x_train,y)","a46570a9":"print(random.best_params_)","fc84c136":"np.sqrt(-random.best_score_)","ae32e1da":"svm = SVR(C=80000,epsilon=0.01)\nscores5 = cross_validate(svm,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","b927c927":"np.sqrt(-scores5['train_score'].mean())","5b03e42a":"np.sqrt(-scores5['test_score'].mean())","d6e81ccc":"random_forest = RandomForestRegressor()\nscores6 = cross_validate(random_forest,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","fc34bee1":"print(np.sqrt(-scores6['train_score'].mean()))\nprint(np.sqrt(-scores6['test_score'].mean()))","6f171574":"import xgboost as xgb","6f7f3760":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, \n                             random_state =7, nthread = -1)","7caeb4ed":"scores7 = cross_validate(model_xgb,x_train,y,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","ae7e1b74":"print(np.sqrt(-scores7['train_score'].mean()))\nprint(np.sqrt(-scores7['test_score'].mean()))","fde44c87":"ridge.fit(x_train,y)\nridge_pred = ridge.predict(x_test)","27a30f8a":"elastic.fit(x_train,y)\nelastic_pred = elastic.predict(x_test)","5524b79a":"model_xgb.fit(x_train,y)\nxgb_pred = model_xgb.predict(x_test)","b3bc74b0":"pd.Series(np.expm1(elastic_pred)).describe()","beab6950":"pd.Series(np.expm1(xgb_pred)).describe()","9148a267":"pd.Series(np.expm1(ridge_pred)).describe()","63eef3d7":"np.expm1(y).describe()","a935dcbe":"y_pred = 0.6*np.expm1(elastic_pred) + 0.4*np.expm1(xgb_pred)","9c717280":"submission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['SalePrice'] = y_pred\nsubmission.to_csv('submission.csv',index=False)","ec8ebb24":"### Filling Missing Values","d2e16bad":"Look at feature pairs (GarageCars,GarageArea), (TotalBsmtSF,1stFlrSF),(GarageYrBlt,YearBuilt),(TotRmsAbvGrd,GrLivArea) . They are highly correlated. It's better to remove one of the features or create a new feature by using the two features.","229c816e":"But when it comes to in range value prediction, xgb outperforms every other model. So, for better score we can average the prediction of xgb and elasticnet.","2f10634f":"After experiments with parameter C using GridSearchCV i found 80000 is the best value.","17c89242":"We can see the score is not much improvement over our linear models. It's beacause even though the dataset may have non-linear relationships our penalty C is high enough(80000) to make it linear. without regularization parameter the train_score is 0.09(which is better than our linear models).","294b74c1":"Saleprice has positive skewness, we can use log transformation to reduce it.","d677cb9c":"Let's look at the features which are highly correlated with SalePrice","e8afc3f3":"we will drop 'PoolQC','MiscFeature','Alley','Fence' as there are lot of null values.","0cf8ca94":"No outliers in TotalSF","9e3db9d5":"Before preprocessing and feature engineering, it's better to have basic intuition about our features(like the dependencies with saleprice).","3a8b0da7":"Of course test data don't contain Saleprice column :)","e60da16f":"Now its time to view the correlation between numerical features and Saleprice. ","ee32e72b":"As there are lots of features in our data it is good to store and track categorical and numerical features seperatly while preprocessing (believe me, it takes a lot of time if confused in between). ","4a405a80":"let's do boxcox transformation for features with skewness greater than 0.5","20846d1f":"look at max value saleprice!!!!. our linear models predict upto 900000 but xgb is only predicting upto 600000 ,this is because our train set have max value of 700000. We are safe to say our test set demands out of range values. It's also proved with my submissions too( my xgb got less score than elasticnet).","f226412c":"## Outliers and skewness","8b0e1b20":"Let's concatenate and preprocess our train and test data simultaneously to avoid doing it twice and save lot of time ( 80 features !!!!!!) ","e00fa5b6":"## Pre Processing","4eb0b0bd":"The best way to know your data when there are lots of features is using seaborn's heatmap .    The notebook by PEDRO MARCELINO helped me a lot here.","d1347065":"# **House Price Prediction**\n   \nBefore we discuss the code let me tell you this is one of my first kaggle projects and the below notebook is inspired by several other notebooks.\n\nI would like to thank owners of below mentioned notebooks which helped me in understanding basic concepts and are very useful in my works. \n\n1. https:\/\/www.kaggle.com\/s\/10533521 by Naresh bhatt\n2. https:\/\/www.kaggle.com\/s\/314923 by Serigne\n3. https:\/\/www.kaggle.com\/s\/96093 by Alexandru Papiu\n4. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python by pedro marcelino.\n","ab6584a0":"No missing values.","18680a2e":"we will remove outliers of highly related features with saleprice because as they have larger effect. We will also remove skewness by using log and boxcox transformation of numerical features.","982f74a1":"### **Visualization**","94412a94":"Categorical Features are : \n1) Every object datatype features                    \n2) Some integer\/float type which are categorical (eg OverallQual)","eba12e8e":"Let's load our dataset","b367003b":"Upvote my notebook if you like it. Please send me your feedback ,I'm just a beginner and could have made mistakes :) ...","a1447f0a":"We can see GarageCars and GarageArea are highly realted with saleprice and also with each other ,so we will drop GarageArea. Similarly we will drop 1stFlrSF and GarageYrBlt. ","7a5ae243":"OverallQual effect Saleprice quite considerably.","83a5bd52":"we will also test with svr because if our model have more non-linear relationships it can fit better.","7634fb4d":"Let's try our data on basic regression models (lasso,ridge,elasticnet).\nWe will also use cross_validate to avoid train_test_split.","a4f17c47":"The score improved. similarly we will do it to ridge and elasticnet to see the best score.","d2640533":"We used the default lasso. We can improve the model by using gridsearchcv.","fbdea1bc":"Let's try some other features. It's always good to know your features relation with targets which gives you the idea to treat some features seperatly than the others for better results.","cf0c3e76":"The following features are actually categorical.      \n\n","744646e4":"we will scale our data using RobustScaler() before feeding it to Lasso() as it is sensitive to outliers.","4c21ab55":"We will create some new features.","7b9cdd92":"As there are 80 features its hard to visualize every feature. So we guess some features randomly.\n\nHouse price most likely depends on size\/Area and quality of house.GrLivArea(ground living area) and OverallQual( overall quality of the house) seems to be our best options to try.","58c55baf":"we will convert MSSubclass to object type and then use label encoder because the values in MSSubclass are high(though they are categorical).","6fc288c6":"I tried optimizing the parameters for random forest but it didn't improve much. It might be because random forest cannot extrapolate( it cannot predict out of range values than in train data ) .I will explain this using our test dataset with more strong xgb model.","1aa0699f":"So as GrLivArea !!.","df9a3560":"surely xgb is better than random forest regressor but let's see it's performance with test data.","90758a3b":"As you can see, the train data has 80 features and target is SalePrice.     \nWe have both continuous and categorical data."}}