{"cell_type":{"f1fe3ad0":"code","2987f2c0":"code","3f967a20":"code","26fc3c53":"code","f403fa15":"code","f3896647":"code","5ae282a1":"code","00b6ecde":"code","0efaee65":"code","819463dd":"code","5ceb6397":"code","f60ca503":"code","b7b10df5":"code","a7223099":"code","8a50c91a":"code","8eacd228":"code","11ba52df":"code","de8e1039":"code","262cf5ce":"code","9a510c90":"code","075a8d04":"code","fa5e7e12":"code","4c934273":"code","d06d3689":"code","5fb80fae":"code","d88c9d8e":"code","415e007e":"code","4a2bbfd0":"code","e79dafc4":"code","ff91a09c":"code","038d7f49":"code","3dbef62c":"code","79efad6b":"code","ed84172c":"code","8e1271e2":"code","6f2ea154":"code","8ccb1825":"code","4dc9a3a9":"code","db656512":"code","100b363e":"code","d6021b54":"code","43baf741":"code","035e941e":"code","c50a6201":"code","61bf7450":"markdown","d48fbd43":"markdown","b761d6bd":"markdown","7b7f8656":"markdown","ff62ed07":"markdown","f82a2676":"markdown","150f06a5":"markdown","2ea25078":"markdown","09a7aa56":"markdown","dd0cf3d2":"markdown","6e88c821":"markdown","2207cfdd":"markdown","4ba073c7":"markdown","b9dadf16":"markdown","cb0c167f":"markdown","af99f28d":"markdown","ef2a2863":"markdown","a61c7f8f":"markdown","bb39ef85":"markdown","1c61cfe2":"markdown","d74d33ff":"markdown","686e5b4e":"markdown"},"source":{"f1fe3ad0":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport lightgbm as lgb\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.feature_selection import RFE\nfrom plotly.offline import iplot, init_notebook_mode\nfrom sklearn.metrics import mean_squared_log_error\nfrom plotly.subplots import make_subplots\ninit_notebook_mode()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2987f2c0":"rootPath = \"\/kaggle\/input\/store-sales-time-series-forecasting\"\ntrain = pd.read_csv(rootPath + \"\/train.csv\")\ntest = pd.read_csv(rootPath + \"\/test.csv\")\nholidays = pd.read_csv(rootPath + \"\/holidays_events.csv\")\nstores = pd.read_csv(rootPath + \"\/stores.csv\")\ntransactions = pd.read_csv(rootPath  +\"\/transactions.csv\")\noil = pd.read_csv(rootPath +\"\/oil.csv\")","3f967a20":"def describe(df):\n    '''\n    make dataframe which describe the details about null count, etc\n    '''\n    print(f'Shape : {df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['DataType']).reset_index()\n    summary = summary.rename(columns={'index': 'Feature'})\n    summary['null count'] = df.isnull().sum().values\n    summary['unique count'] = df.nunique().values\n    summary['First value'] = df.loc[0].values\n    summary['Second value'] = df.loc[1].values\n    summary['Third value'] = df.loc[2].values\n    \n    return summary\ndisplay(describe(train))","26fc3c53":"_, axes = plt.subplots(2, 2, figsize=(20, 10), facecolor='yellow')\nplt.suptitle('Check the numeric distribution', color='blue', fontsize=25)\n\nsns.distplot(train['sales'], ax=axes[0, 0])\naxes[0, 0].set_title('sales displot', fontsize=25)\n\nsns.boxplot(x='sales', data=train, ax=axes[0, 1])\naxes[0, 1].set_title('sales boxplot', fontsize=25)\n\nsns.histplot(x='onpromotion', data=train, bins=20, ax=axes[1, 0])\naxes[1, 0].set_title('onpromotion hist', fontsize=25)\n\nsns.boxplot(x='onpromotion', data=train, ax=axes[1, 1])\naxes[1, 1].set_title('onpromotion boxplot', fontsize=22)\n\nplt.tight_layout()\nplt.show()","f403fa15":"fig = make_subplots(rows=1, cols=1, specs=[[ {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=train['family'].value_counts().index, values=train['family'].value_counts()),\n              1, 1)","f3896647":"display(describe(transactions))\nsns.displot(x='transactions', data=transactions)\n","5ae282a1":"display(describe(holidays))\nholidays['locale_name'].unique()\n","00b6ecde":"locale_name = holidays['locale_name'].value_counts()\nlocale_name","0efaee65":"fig = px.pie(stores, values=locale_name, names=locale_name.index)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\n\nfig.show()","819463dd":"holidays['transferred'] = holidays['transferred'].apply(lambda x: 1 if x else 0) # encoding transfer\nspecs = [[{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=1, cols=2, specs=specs, subplot_titles=['type_holiday', 'transferred'])\ntype_holiday = holidays['type'].value_counts()\ntransferred = holidays['transferred'].value_counts()\n\nfig.add_trace(go.Pie(labels=type_holiday.index, values=type_holiday),\n              row=1, col=1)\nfig.add_trace(go.Pie(labels=transferred.index, values=transferred),\n              row=1, col=2)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\nfig = go.Figure(fig)\nfig.show()","5ceb6397":"display(describe(stores))","f60ca503":"stores = stores.drop('state', axis=1)\ncity = stores['city'].value_counts()\ncluster = stores['cluster'].value_counts()\n\nspecs = [[{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=1, cols=2, specs=specs, subplot_titles=['city', 'cluster'])\n\nfig.add_trace(go.Pie(labels=city.index, values=city), row=1, col=1)\nfig.add_trace(go.Pie(labels=cluster.index, values=cluster), row=1, col=2)\n\nfig.update_layout(\ntitle_font_color=\"#fff\",paper_bgcolor=\"#283747\",title_font_size=20,title_x=.5,font_color=\"#bbb\",\n    plot_bgcolor=\"#D6EAF8\")\nfig = go.Figure(fig)\nfig.show()","b7b10df5":"sns.countplot(x='type', data=stores)","a7223099":"display(describe(oil))\nsns.displot(x='dcoilwtico', data=oil)","8a50c91a":"oil['dcoilwtico'] = oil['dcoilwtico'].fillna(0)","8eacd228":"merge_data = train.merge(oil, on='date', how='left')\nmerge_data = merge_data.merge(holidays, on='date', how='left')\nmerge_data = merge_data.merge(stores, on='store_nbr', how='left')\nmerge_data = merge_data.merge(transactions, on=['date', 'store_nbr'], how='left')","11ba52df":"merge_copy = merge_data.copy()\n\n# change dtype and get the date col\nmerge_copy['date'] = pd.to_datetime(merge_copy['date']).dt.date\nmerge_copy['year'] = pd.to_datetime(merge_copy['date']).dt.year\nmerge_copy['month'] = pd.to_datetime(merge_copy['date']).dt.month\nmerge_copy['day'] = pd.to_datetime(merge_copy['date']).dt.day","de8e1039":"describe(merge_copy)","262cf5ce":"merge_copy['transactions'] = merge_copy['transactions'].fillna(0)\nmerge_copy['dcoilwtico'] = merge_copy['dcoilwtico'].fillna(0)\ndisplay(describe(merge_copy))","9a510c90":"merge_copy['holiday_flag'] = [1 if not val else 0 for val in merge_copy['type_x'].isnull()]\nmerge_copy = merge_copy.drop(['type_x', 'locale_name', 'transferred'], axis=1)\nmerge_copy = merge_copy.rename(columns={'type_y': 'stores_type'})\ndisplay(describe(merge_copy))","075a8d04":"df = merge_copy.copy()\ndf = df.sort_values('date')\ndf_g = df[['date', 'sales']].groupby('date').agg(date_sum=('sales', np.mean))\n# month avg \ndf_g['moving_avg'] = df_g.date_sum.rolling(30, min_periods=3).mean()\n\nplt.figure(figsize=(20, 5))\nplt.plot(df_g['moving_avg'])\nplt.title(\"Average sales per month\")\nplt.show()\n\ndel df","fa5e7e12":"df = merge_copy.groupby(['year', 'month'], as_index=False).agg(sales_mean=('sales', np.mean))\nplt.figure(figsize=(20, 5))\nplt.title('Mean sales each year-month', fontsize=20)\nsns.barplot(x='month', y='sales_mean', data=df, hue='year')\nplt.show()\ndel df","4c934273":"plt.figure(figsize=(10, 10))\ncorr = merge_copy.corr()\nsns.heatmap(corr, annot=True, cmap=\"YlGnBu\")","d06d3689":"del merge_data","5fb80fae":"data = merge_copy.copy().drop(['id', 'date','locale','description','day','dcoilwtico'], axis=1)\ndata = pd.get_dummies(data, drop_first=True)\nX = data.drop('sales', axis=1)\ny = data['sales']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1234)","d88c9d8e":"display(describe(data))","415e007e":"model = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_pred = np.where(y_pred<0, 0, y_pred)\nprint(np.sqrt(mean_squared_log_error(y_true=y_test, y_pred=y_pred)))","4a2bbfd0":"\nn = [2,3,10,15,20,25,30,35,40]\nfor i in n:\n    pipe = Pipeline([('pca', PCA(n_components=i)), ('lr', LinearRegression())])\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    y_pred = np.where(y_pred<0, 0, y_pred)\n    print(\"PCA: {} ------- RMSLE: {}\".format(i, np.sqrt(mean_squared_log_error(y_true=y_test, y_pred=y_pred))))\n\n","e79dafc4":"RF = RandomForestRegressor(random_state=12, n_jobs=-1,max_depth=30, max_features='log2',max_leaf_nodes=20, verbose  =2)\nRF.fit(X_train,y_train)\ny_pred = RF.predict(X_test)\ny_pred = np.where(y_pred<0, 0, y_pred)\nnp.sqrt(mean_squared_log_error(y_true=y_test, y_pred=y_pred))","ff91a09c":"importances = np.array(RF.feature_importances_)\nforest_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)[:16]\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\nforest_importances.plot.bar(ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","038d7f49":"important_cols = [ 'onpromotion', 'cluster', 'transactions', 'year', 'store_nbr','family_BEVERAGES', 'family_CLEANING',\n                  'family_DAIRY', 'family_GROCERY I', 'family_PRODUCE']\nX_importance = X[important_cols]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_importance, y, test_size=0.2, random_state=123)\n","3dbef62c":"lgb_train = lgb.Dataset(X_train2, y_train2)\nlgb_test = lgb.Dataset(X_test2, y_test2)\n\nparams = {'metric' : 'rmsle', 'seed': 123, 'verbosity':-1}\n\n# train data\ngbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=[lgb_test])","79efad6b":"y_pred = gbm.predict(X_test2, num_iteration=gbm.best_iteration)\ny_pred = np.where(y_pred<0, 0, y_pred)\nprint(np.sqrt(mean_squared_log_error(y_true=y_test2, y_pred=y_pred)))","ed84172c":"importance = pd.DataFrame(gbm.feature_importance(), index=X_train2.columns, columns=['importance'])\n\nimportance.sort_values(by='importance', ascending=False).plot.bar(figsize=(20, 8))","8e1271e2":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras import optimizers\n\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\nfrom sklearn.metrics import mean_squared_error\nfrom keras import backend as K\nfrom keras import optimizers\nimport tensorflow as tf \nfrom tensorflow import keras","6f2ea154":"epochs = 40\nbatch = 2046\nlr = 0.0003\nadam = tf.keras.optimizers.Adam(lr)\ndef root_mean_squared_log_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(K.log(y_pred+1) - K.log(1+y_true)))) ","8ccb1825":"model_mlp = Sequential()\nmodel_mlp.add(Dense(100, activation='relu', input_dim=X_train2.shape[1]))\nmodel_mlp.add(Dense(1))\nmodel_mlp.compile(loss='mse', optimizer=adam)\nmodel_mlp.summary()","4dc9a3a9":"mlp_history = model_mlp.fit(X_train2.values, y_train2, validation_data=(X_test2.values, y_test2), epochs=epochs, verbose=5)","db656512":"plt.plot(mlp_history.history['loss'], label='Train loss')\nplt.plot(mlp_history.history['val_loss'], label='Validation loss')\nplt.legend(loc='best')\nplt.title('MLP')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MSE\")\nplt.show()","100b363e":"pred = model_mlp.predict(X_test2.values)\npred = np.where(pred<0, 0, pred)\nprint(np.sqrt(mean_squared_log_error(y_true=y_test2, y_pred=pred)))","d6021b54":"col = importance.query('importance>100').index\n\ntest_copy = test.copy()\n# merge data\ntest_copy = test_copy.merge(oil, on='date', how='left')\ntest_copy = test_copy.merge(holidays, on='date', how='left')\ntest_copy = test_copy.merge(stores, on='store_nbr', how='left')\ntest_copy = test_copy.merge(transactions, on=['date', 'store_nbr'], how='left')\n# change dtype and get the date col\ntest_copy['date'] = pd.to_datetime(test_copy['date']).dt.date\ntest_copy['year'] = pd.to_datetime(test_copy['date']).dt.year\ntest_copy['month'] = pd.to_datetime(test_copy['date']).dt.month\ntest_copy['day'] = pd.to_datetime(test_copy['date']).dt.day\n# fillna with 0\ntest_copy['transactions'] = test_copy['transactions'].fillna(0)\ntest_copy['dcoilwtico'] = test_copy['dcoilwtico'].fillna(0)\n# create new col as I did above\ntest_copy['holiday_flag'] = [1 if not val else 0 for val in test_copy['type_x'].isnull()]\n# test_copy = merge_copy.drop(['type_x', 'locale_name', 'transferred'], axis=1)\ntest_copy = test_copy.rename(columns={'type_y': 'stores_type'})\n\ntest_copy = test_copy.drop(['id', 'date'], axis=1)\ntest_copy = pd.get_dummies(test_copy, drop_first=True)","43baf741":"describe(test_copy)","035e941e":"col = importance.query('importance>0').index\nX2 = data[col]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X2)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=12)\n\n# best param!\nparams = {\n    'metric' : 'rmsle', \n    'verbosity': -1, \n    'seed': 123,\n    'boosting_type': 'gbdt'\n}\n\n# Preparing dataset for LightGBM\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test)\n# train data\ngbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=[lgb_test], verbose_eval=False)\n\n\n# choose importance cols\ntest_copy = test_copy[col]\nscaler = StandardScaler()\ntest_scaled = scaler.fit_transform(test_copy)\nprediction= gbm.predict(test_scaled, num_iteration=gbm.best_iteration)\nprediction = np.where(prediction<0, 0, prediction)\ndel X2\ndel lgb_train\ndel lgb_test","c50a6201":"submission = pd.read_csv(rootPath+'\/sample_submission.csv')\n\nsubmission['sales'] = prediction\nsubmission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission","61bf7450":"## 2.4 RandomForest","d48fbd43":"Key: id-year, sales-onpromotion","b761d6bd":"\"dcoilwtico\" has null values, I will replace them by 0","7b7f8656":"# 3. Submit","ff62ed07":"## 2.3 PCA + Linear Regression","f82a2676":"It's is imbalance in transactions when Ecuador took up nearly a half.","150f06a5":"## 2.1 Prepare data","2ea25078":"## 2.4 LightGBM","09a7aa56":"## 2.5 MLP","dd0cf3d2":"The percentage of components in family are same.","6e88c821":"## Merge file","2207cfdd":"## Train","4ba073c7":"**Correlation**","b9dadf16":"## Holidays","cb0c167f":"There are several holiday events per year, so encoding 1 if occurs holiday else 0","af99f28d":"## store","ef2a2863":"## Transactions","a61c7f8f":"# 2.Model","bb39ef85":"## 2.2 Linear Regression","1c61cfe2":"# 1.FILE","d74d33ff":"We can see there is outlier in sales column","686e5b4e":"## Oil"}}