{"cell_type":{"39f896e2":"code","14e4dc37":"code","92514fea":"code","4cb444fa":"code","d4ccce9b":"code","b7d9ddeb":"code","78543397":"code","b4973bed":"code","c1a69505":"code","837d7ede":"code","c1a2ba9c":"code","7a3f88b4":"code","046f97d3":"code","2a97dae1":"code","791b773f":"code","57458794":"code","bdcd9aec":"code","fabc3c2a":"code","0a6e8ee5":"code","710905d5":"code","0cafb827":"code","b68b65c1":"code","9fd7b89c":"code","83cf5bc4":"code","c3d07572":"code","7e12ebb5":"code","47d40fdf":"code","36cdf93d":"code","e17d90b1":"code","c25476c6":"code","075abeb8":"code","a9d89204":"code","43f85e6d":"code","6418c783":"code","ab7a84bb":"code","42269208":"code","fb4a7f4d":"code","f052335d":"code","3035d7b1":"code","7d0dae4b":"code","8199d855":"code","89795e3e":"code","a2535660":"code","e1006fc6":"code","a572991e":"code","202dd6b5":"code","20b5a0db":"code","38921e0f":"code","1521c068":"code","f0ffc214":"code","6368e41e":"code","b18a82a0":"code","0e662c66":"code","dd820ff9":"code","efe30aca":"code","c78abc47":"code","13c86c59":"code","13d4afb4":"code","819b91ce":"markdown","8322abb8":"markdown","29a4ef68":"markdown","e1754f72":"markdown","eaabdc8c":"markdown","691e209a":"markdown","31ca3b96":"markdown","5af7c821":"markdown","99371914":"markdown","65d8f5c3":"markdown","afa6de82":"markdown","039f234d":"markdown","0a077da8":"markdown","6b7ef798":"markdown","ff3178f8":"markdown","5ebfad31":"markdown","b2373f40":"markdown","f1f123f0":"markdown","0e3091f6":"markdown","5adee7d6":"markdown","131f7155":"markdown"},"source":{"39f896e2":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","14e4dc37":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf=pd.read_csv('\/kaggle\/input\/cardiovascular-disease-dataset\/cardio_train.csv',sep=';')","92514fea":"df.info()","4cb444fa":"df.describe().transpose()","d4ccce9b":"df[(df.ap_hi>=400)]\n\n# We will drop the BP above 400 as highest BP recorded is 370\/360. Clearly the values are not recorded properly","b7d9ddeb":"df=df[~(df.ap_hi>=400)] # dropping a few rows","78543397":"df[(df.ap_lo>df.ap_hi) & (df.ap_lo>300)]\n\n","b4973bed":"df[ (df.ap_lo>300)]","c1a69505":"df=df[ ~(df.ap_lo>300)]","837d7ede":"df[(df.ap_hi<0)| (df.ap_lo<0)]","c1a2ba9c":"# Take absolute value for the record with ap_hi <0 as clearly the data looks to be wrong and we can change it to positive\n\ndf[(df.ap_hi<0)]=df[(df.ap_hi<0)].abs()","7a3f88b4":"# Drop the record with ap_lo <0 as clearly the data looks to be wrong\n\ndf=df[~(df.ap_lo<0)]","046f97d3":"df.shape","2a97dae1":"df.describe().transpose()","791b773f":"df[(df.ap_hi<df.ap_lo)]","57458794":"# Need to remove the records where in the ap_hi is less than ap_lo\n\ndf=df[~(df.ap_hi<df.ap_lo)]","bdcd9aec":"df.shape","fabc3c2a":"# Merge Weight and Height into 1 feature: BMI\n\ndf['BMI']=round(df['weight']\/((df['height']\/100)**2),2)","0a6e8ee5":"# Convert Gender to binary where 0: Female and 1: Male\n\ndf['gender']=df['gender']-1","710905d5":"# Convert numeric values in cholestrol to english text\n\ndf['cholesterol']=df['cholesterol'].map({1:\"Normal\", 2:\"Ab_Normal\", 3:'High'})\n\n\n# Convert numeric values for glucose to english text\n\ndf['gluc']=df['gluc'].map({1:\"Normal\", 2:\"Ab_Normal\", 3:'High'})","0cafb827":"# Creating Dummy Variable for Cholestrol\nd1=pd.get_dummies(df['cholesterol'],prefix='chol',drop_first=True)\n\n# Creating Dummy Variable for Glucose\nd2=pd.get_dummies(df['gluc'],prefix='gluc',drop_first=True)\n\ndf=pd.concat([df,d1,d2],axis=1)","b68b65c1":"df.head()","9fd7b89c":"df.drop(['id','cholesterol','gluc'],axis=1,inplace=True)","83cf5bc4":"# Drop Height and Weight\n\ndf.drop(['height','weight'],axis=1,inplace=True)","c3d07572":"df.head()","7e12ebb5":"df.info()","47d40fdf":"#Putting feature variables into X\n\nX=df.drop('cardio',axis=1)\n\n# Fetch Target varaible\n\ny=df['cardio']","36cdf93d":"# Split the data\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,train_size=0.7,test_size=0.3,random_state=100)","e17d90b1":"scaler=StandardScaler()\nX_train[['age','BMI','ap_hi','ap_lo']]=scaler.fit_transform(X_train[['age','BMI','ap_hi','ap_lo']])","c25476c6":"X_train.head()","075abeb8":"# Checking for Data Imbalance\n\ndf['cardio'].sum()\/len(df) *100","a9d89204":"# Importing other ML packages\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nfrom sklearn import metrics","43f85e6d":"#before starting with  the Model building, checking the correlation\nplt.figure(figsize=(15,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu')\nplt.show()","6418c783":"X_train.shape","ab7a84bb":"col=X_train.columns","42269208":"X_train1=sm.add_constant(X_train[col])\nml1=sm.GLM(y_train, X_train1, family=sm.families.Binomial()).fit()\nml1.summary()","fb4a7f4d":"from statsmodels.stats.outliers_influence import variance_inflation_factor","f052335d":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3035d7b1":"col=col.drop('gluc_Normal',1)","7d0dae4b":"X_train2=sm.add_constant(X_train[col])\nml2=sm.GLM(y_train, X_train2, family=sm.families.Binomial()).fit()\nml2.summary()","8199d855":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","89795e3e":"col=col.drop('gender',1)\nX_train3=sm.add_constant(X_train[col])\nml3=sm.GLM(y_train, X_train3, family=sm.families.Binomial()).fit()\nml3.summary()","a2535660":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e1006fc6":"y_predict=ml3.predict(X_train3)","a572991e":"y_predict.head()","202dd6b5":"fpr, tpr, threshold= metrics.roc_curve(y_train, y_predict, drop_intermediate=False)\nauc=metrics.roc_auc_score(y_train,y_predict)\nplt.plot(fpr,tpr, label='ROC Curve (Area=%0.2f)' %auc)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]')\nplt.ylabel('True Positive Rate')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","20b5a0db":"y_train_predict=pd.DataFrame({'Actual':y_train.values, 'Pred_Prob': y_predict.values})\ny_train_predict.head()","38921e0f":"num=[float(i)\/10 for i in range(10)]\n\nfor i in num:\n    y_train_predict[i]=y_train_predict['Pred_Prob'].apply(lambda x: 1 if x>i else 0)\ny_train_predict.head()","1521c068":"cut_off_mat=pd.DataFrame(columns=('Prob','Accuracy','Sensitivity','Specificity'))\nfor i in num:\n    confusion=metrics.confusion_matrix(y_train_predict.Actual,y_train_predict[i])\n    acc=(confusion[0,0]+confusion[1,1]) \/ sum(sum(confusion))\n    sens= confusion[1,1]\/(confusion[1,1]+confusion[1,0])\n    spec= confusion[0,0]\/(confusion[0,0]+confusion[0,1])\n    \n    cut_off_mat.loc[i]=[i,acc,sens,spec]\n    \ncut_off_mat","f0ffc214":"cut_off_mat.plot(x='Prob',y=['Accuracy','Sensitivity','Specificity'])\nplt.show()","6368e41e":"y_train_predict['Final_Pred']=y_train_predict['Pred_Prob'].apply(lambda x: 1 if x>0.4 else 0)\n\nconfusion=metrics.confusion_matrix(y_train_predict['Actual'],y_train_predict['Final_Pred'])\nTN=confusion[0,0]\nFP=confusion[0,1]\nFN=confusion[1,0]\nTP=confusion[1,1]","b18a82a0":"#Sesitivity\nprint('Sensitivity: ',confusion[1,1]\/(confusion[1,1]+confusion[1,0]))\n\n#Specificity\nprint('Specificity: ',confusion[0,0]\/(confusion[0,0]+confusion[0,1]))\n\n#Accuracy\nprint('Accuracy: ', (confusion[0,0]+confusion[1,1]) \/ sum(sum(confusion)))\n\n#Recall\nr=confusion[1,1]\/(confusion[1,1]+confusion[1,0])\nprint('Recall: ',r)\n\n#Precision\np=confusion[1,1]\/(confusion[1,1]+confusion[0,1])\nprint('Precision: ',p)\n\n#F1-Score\nprint('F1-Score: ',(2*p*r\/(p+r)))\n","0e662c66":"X_test[['age','BMI','ap_hi','ap_lo']]=scaler.transform(X_test[['age','BMI','ap_hi','ap_lo']])","dd820ff9":"X_test=X_test[col]\nX_test_sm=sm.add_constant(X_test)\n\ny_test_prob = ml3.predict(X_test_sm)","efe30aca":"y_test_pred = pd.DataFrame({\"Test_Acutal\":y_test.values, \"Test_Prob\":y_test_prob.values})\ny_test_pred['Pred_Val']=y_test_pred.Test_Prob.apply(lambda x: 1 if x>0.4 else 0)","c78abc47":"confusion2=metrics.confusion_matrix(y_test_pred['Test_Acutal'],y_test_pred['Pred_Val'])\nTN=confusion2[0,0]\nFP=confusion2[0,1]\nFN=confusion2[1,0]\nTP=confusion2[1,1]","13c86c59":"#Sesitivity\nprint('Sensitivity: ',confusion2[1,1]\/(confusion2[1,1]+confusion2[1,0]))\n\n#Specificity\nprint('Specificity: ',confusion2[0,0]\/(confusion2[0,0]+confusion2[0,1]))\n\n#Accuracy\nprint('Accuracy: ', (confusion2[0,0]+confusion2[1,1]) \/ sum(sum(confusion2)))\n\n#Recall\nr=confusion2[1,1]\/(confusion2[1,1]+confusion2[1,0])\nprint('Recall: ',r)\n\n#Precision\np=confusion2[1,1]\/(confusion2[1,1]+confusion2[0,1])\nprint('Precision: ',p)\n\n#F1-Score\nprint('F1-Score: ',(2*p*r\/(p+r)))\n","13d4afb4":"ml3.summary()","819b91ce":"## Step 2: Data Preparation","8322abb8":"Looking at the above information:\n- ID need to be dropped\n- Age provided is in days. We will leave it as it is\n- Gender can be done gender-1 to convert it to binary\n- ap_hi and ap_lo has negative numbers. This need to be fixed\n- Gluc and Cholestrol need to be convereted to dummies","29a4ef68":"We have dropped a total of 1267 records. That is 1.8% records which is not too high","e1754f72":"p-value and VIF seems to be much better now.\n\nUsing the model to predict Y","eaabdc8c":"#### LOG(ODDS) = 0.4684 + 0.3410* AGE + 0.9334 * AP_HI + 0.1220 * AP_LO - 0.1261 * SMOKE - 0.2539 * ALCO - 0.2131 * ACTIVE -     0.7023 * CHOL_HIGH - 0.3891 * CHOL_NORMAL - 0.3425 * gluc_High + 0.1270 * BMI","691e209a":"As seen from above somewhere around 0.4 we can see the three line merging. So we will take 0.4 as or cut-off probability","31ca3b96":"Data is ready for further processing\n\n**Train-Test Split**","5af7c821":"From above we can see that the number of records with ap_lo>300 or ap_hi>ap_lo also seem to be errorneous. We would be dropping these records too","99371914":"**Optimal Cutoff**","65d8f5c3":"## Step 5: Model Prediction","afa6de82":"#### Model 1:","039f234d":"## Step 4: Model Evaluation","0a077da8":"## Final Summary","6b7ef798":"**Dropping repeated features**","ff3178f8":"**Data Scaling**","5ebfad31":"**Outlier Treatment**","b2373f40":"p-value of gender is very high. Dropping the variable\n\n#### Model 3","f1f123f0":"# Cardiovascular Disease\n\n\n## Step1: Load Dataset and inspecting Dataframe","0e3091f6":"Creating Dummy Variables","5adee7d6":"## Step 3: Model Building","131f7155":"gluc_Normal has very high p-Value as well as VIF. Droping the variable \n\n#### Model 2"}}