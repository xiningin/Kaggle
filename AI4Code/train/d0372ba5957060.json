{"cell_type":{"4bea9411":"code","ec4abed4":"code","eb20680c":"code","7f8095da":"code","32ede30f":"code","65b7d434":"code","be6ad9ad":"code","67069f72":"code","ea1934d2":"code","1b345aaa":"code","228ac312":"code","133d9dc9":"code","0796354f":"code","ca1901f1":"code","867eb50d":"code","10bb8654":"code","cae32e5a":"code","11090434":"markdown","bc8a7783":"markdown","0f7fb49f":"markdown","7f5b7f27":"markdown","fa1aa332":"markdown","dc3c018d":"markdown","f47f5cbf":"markdown","eefdff8e":"markdown","4797f2bf":"markdown","17bb34c8":"markdown","4a62e9be":"markdown","d8661f90":"markdown","04f355ab":"markdown","f138563c":"markdown","f5f4a8e6":"markdown","95295217":"markdown","85c5b84a":"markdown","03181852":"markdown","72945bad":"markdown","33d5f23e":"markdown","f82e6d36":"markdown","22a793b9":"markdown","9c88eedc":"markdown","a104c152":"markdown","adcd7f6b":"markdown","3166e6fe":"markdown","8da19913":"markdown"},"source":{"4bea9411":"# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Dict\n\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet50\nfrom tqdm import tqdm\n\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\nfrom prettytable import PrettyTable\nfrom pathlib import Path\n\nimport os","ec4abed4":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"","eb20680c":"# UTILITY FUNCTIONS\n\ndef visualize_image(ds, idx, axis=None):\n    data = ds[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = ds.rasterizer.to_rgb(im)\n    if axis:\n        axis.imshow(im[::-1])\n    else:\n        plt.imshow(im[::-1])\n\ndef build_model(cfg: Dict) -> torch.nn.Module:\n    # Backbone model\n    model = resnet50(pretrained=True)\n    \n    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n    \n    # Getting input channels for first layer of the model.\n    num_in_channels = 3 + num_history_channels\n    \n    # Getting the number of targets.\n    num_targets = 2*cfg[\"model_params\"][\"future_num_frames\"]\n    \n    # Adjusting the first layer for the number of input channels.\n    model.conv1 = nn.Conv2d(\n        num_in_channels,\n        model.conv1.out_channels,\n        kernel_size=model.conv1.kernel_size,\n        stride=model.conv1.stride,\n        padding=model.conv1.padding,\n        bias=False\n    )\n    \n    # Adjusting the final layer for the number of targets.\n    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n    return model\n\ndef forward(data, model, device, criterion):\n    inputs = data[\"image\"].to(device)\n    targets = data[\"target_positions\"].to(device)\n    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n    \n    outputs = model(inputs).reshape(targets.shape).to(device)\n    \n    loss = torch.sqrt(criterion(outputs, targets))\n    loss = loss * target_availabilities\n    loss = loss.mean()\n    return loss, outputs","7f8095da":"dm = LocalDataManager()\ncfg = load_config_data(\"..\/input\/lyft-config-files\/agent_motion_config.yaml\")\nsample_zarr = dm.require(\"scenes\/sample.zarr\")\nsample_chunk = ChunkedDataset(sample_zarr).open()","32ede30f":"# Let us have a look at the above discussed attributes of ChunkedDataset instance.\nprint(sample_chunk.scenes)\nprint()\nprint(sample_chunk.agents)\nprint()\nprint(sample_chunk.tl_faces)\nprint()\nprint(sample_chunk.frames)","65b7d434":"# Look at the cfg\ncfg","be6ad9ad":"# Semantic Rasterizer\ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nraster_sem = build_rasterizer(cfg, dm)\n\n# Satellite Rasterizer\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nraster_sat = build_rasterizer(cfg, dm)","67069f72":"cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nego_dataset_sem = EgoDataset(cfg, sample_chunk, raster_sem)\n\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nego_dataset_sat = EgoDataset(cfg, sample_chunk, raster_sat)","ea1934d2":"fig, ax = plt.subplots(2, 3, figsize=(15, 10))\nax = ax.flatten()\nfor i in range(3):\n    visualize_image(ego_dataset_sem, i+10, axis=ax[i])\n    ax[i].set_title(\"Semantic Image\")\n    \n    visualize_image(ego_dataset_sat, i+10, axis=ax[i+3])\n    ax[i+3].set_title(\"Satellite Image\")\n    ","1b345aaa":"cfg = {\n    'format_version': 4,\n \n    'model_params': {'model_architecture': 'resnet50',\n    'history_num_frames': 0,\n    'history_step_size': 1,\n    'history_delta_time': 0.1,\n    'future_num_frames': 50,\n    'future_step_size': 1,\n    'future_delta_time': 0.1},\n \n    'raster_params': {'raster_size': [224, 224],\n    'pixel_size': [0.5, 0.5],\n    'ego_center': [0.25, 0.5],\n    'map_type': 'py_semantic',\n    'satellite_map_key': 'aerial_map\/aerial_map.png',\n    'semantic_map_key': 'semantic_map\/semantic_map.pb',\n    'dataset_meta_key': 'meta.json',\n    'filter_agents_threshold': 0.5},\n \n    'train_data_loader': {'key': 'scenes\/sample.zarr',\n    'batch_size': 12,\n    'shuffle': True,\n    'num_workers': 16},\n \n    'test_data_loader': {'key': 'scenes\/test.zarr',\n    'batch_size': 8,\n    'shuffle': False,\n    'num_workers': 8},\n\n    'train_params': {'checkpoint_every_n_steps': 10000,\n    'max_num_steps': 5,\n    'eval_every_n_steps': 10000}}","228ac312":"sample_agent = AgentDataset(cfg, sample_chunk, raster_sem)","133d9dc9":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = build_model(cfg)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","0796354f":"# Generating Dataloader object for out sample_agent dataset\nsample_config = cfg[\"train_data_loader\"]\nsample_dataloader = DataLoader(sample_agent, batch_size=sample_config[\"batch_size\"],\n                               shuffle=sample_config[\"shuffle\"], num_workers=sample_config[\"num_workers\"])","ca1901f1":"## TRAINING LOOP\niter_train = iter(sample_dataloader)\nprogress = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n\nfor _ in progress:\n    try:\n        train_data = next(iter_train)\n    except:\n        iter_train = iter(sample_dataloader)\n        train_data = next(iter_train)\n    \n    model.train()\n    torch.set_grad_enabled(True)\n    loss, _ = forward(train_data, model, device, criterion)\n    \n    # Zeroing out gradients\n    optimizer.zero_grad()\n    \n    # Backprop\n    loss.backward()\n    \n    # Update the weights\n    optimizer.step()\n    \n    progress.set_description(f\"Loss: {loss.item()}\")","867eb50d":"# Preparing Test Data\ntest_config = cfg[\"test_data_loader\"]\ntest_zarr = dm.require(\"scenes\/test.zarr\")\ntest_chunk  = ChunkedDataset(test_zarr).open()\ntest_mask  = np.load(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/mask.npz\")[\"arr_0\"]\ntest_agent = AgentDataset(cfg, test_chunk, raster_sem, agents_mask = test_mask)\ntest_dataloader = DataLoader(test_agent, batch_size=test_config[\"batch_size\"], shuffle=test_config[\"shuffle\"],\n                            num_workers=test_config[\"num_workers\"])","10bb8654":"model.eval()\n\nfuture_coords = []\ntimestamps = []\ntrack_ids = []\n\nwith torch.no_grad():\n    iter_test = tqdm(test_dataloader)\n    for data in iter_test:\n        _, outputs = forward(data, model, device, criterion)\n        timestamps.append(data[\"timestamp\"])\n        track_ids.append(data[\"track_id\"])","cae32e5a":"write_pred_csv(\"submission.csv\",\n              timestamps=np.concatenate(timestamps),\n              track_ids=np.concatenate(track_ids),\n              coords=np.concatenate(future_coords))","11090434":"As we know that the following competition is based on predicting the future motions of the traffic agents based on the input received from the **Perception Task**. Basically this is the task of **Motion Forecasting**. Don't worry if these terms are not clear to you right now. I hope after reading this notebook, they won't be so much abstract.\n\nThis Notebook will follow the below mentioned topics sequentially and by the end, you would surely be having enough knowledge to proceed further with your approach for this scenario.","bc8a7783":"**Let us just compare the images of satellite as well as semantic map for different frames.**","0f7fb49f":"**Now that we have looked over some of the basic definitions under self driving domain, lets understand what does Level-5 mean in this competition. Basically Level-5 corresponds to the fifth level of automation. So a genuine question would be: What are the levels of automation?**\n\nThe answer is fairly simple, the increasing capability of automation offered by vehicles is being classified in different levels of automation. Let's just look at these different levels- \n\n- **Level-0 automation:** At this level, no automation is expected from the vehicle.\n\n\n- **Level-1 automation:** This level of automation basically comes with a driver assistance in which the machine assists the drive to take actions such as taking the best route, steering or accelerating etc. As discussed above, ACC is commonly used at this level.\n\n\n- **Level-2 automation:** This level of automation is slightly improved from the previous one with an addition of lateral and longitudnal controls by the vehicle. It is also known as [advance drive assistance system](https:\/\/www.synopsys.com\/automotive\/what-is-adas.html)  \n\n\n- **Level-3 automation:** This level includes all the functionalities of level 2 with an addition that these vehicles gain the ability to sense the surrounding environment but upto a limited extent. In most cases the vehicle is automatically able to tackle the situation but in some of the cases it just alerts the driver and the driver has to take appropriate actions.\n\n\n- **Level-4 automation:** This level comes gets improvement from level 3 from the perspective of sensing its surroundings better but still is limited to some edge cases. It has a limited ODD meaning that it can work in most of the cases but a handful of cases are out of reach for it. The system can handle itself if things go wrong or any type of failure occurs.\n\n\n- **Level-5 automation:** This level enjoys superiority over all the levels with an unlimited ODD meaning that vehicle would be able to handle each and every possible condition. It would not need any human intervention to while driving.","7f5b7f27":"Moreover I would also introduce a notebook for detailed model analysis using some of the dominant model architectures of the field. Till then stay tuned! Happy Coding.","fa1aa332":"**Let us now try to reach the AgentDataset or EgoDataset instance from a sample zarr file.**","dc3c018d":"### Step 4. Inference\n\nThis step deals with computing predictions for the test dataset and preparing a submission file for it.","f47f5cbf":"![Dataset_diag_new.jpg](attachment:Dataset_diag_new.jpg)","eefdff8e":"Finally we write our predictions to a CSV file and for this also we have multi modal and single mode submission files which allow us to either ensemble the predictions or use a prediction by single model. \n\nWe are also provided with the function `write_pred_csv` which automatically writes the predictions into the submission file.","4797f2bf":"Above diagram represents that how the data exists in different forms. Let us understand the process.\n\nWe have been provided with a python package named `l5kit` which enables us to structure raw data according to the task and train a model over it.\n> We can also structure the data and train the model without using `l5kit` module but that would require redundant and very hard work.\n\nNow the `l5kit` module has functions namely \"load_config_data\" and \"LocalDataManager\" (not following the complete heirarchy). `load_config_data` basically loads the configrations needed for the dataset and to train the model and `LocalDataManager` is basically instantiated to manage different operations related to data such as creating Dataset instance or building rasterizer. At first we take the raw `zarr files` and dm (LocalDataManager instance) and build a `ChunkedDataset` which contains 4 attributes namely `scenes`, `agents`, `tl_faces`, `frames`.\n> The details of these attributes are very well described in other notebooks. I will add a link to that in the reference section.\n\nWe use cfg (configration) and dm to build rasterizer using the `build_rasterizer` method also provided under `l5kit` module. The rasterizer is basically used to generate a map containing information of the ego vehicle and other traffic agents. This map could be one of the `semantic` or `aerial(satellite)`.\n\nAs such there is no image information present in the dataset until we rasterize it. The `ChunkedDataset` instance that we made previously, has information regarding both the traffic agents and ego vehicle. But the task for this competition is to identify the motion of traffic agents (for 50 frames in the future) and we need something to sample the dataset regarding traffic agents and thus we convert the `ChunkedDataset` instance into `AgentDataset` or `EgoDataset` instance which samples the data with a focus to traffic agents and ego vehicle respectively. As seen from the figure that we need both the `ChunkedDataset` instance and the `rasterizer` to generate the `AgentDataset` and `EgoDataset`.\n> Note that the AgentDataset and EgoDataset when instantiated, contains the information of image (map representation of the particular frame) as now the data is being rasterized. But it contains the data in the world coordinates which we need to project onto the image subspace in order to visualize it.\n\nWe would finally use the instance of `AgentDataset` to train our model. More specifically our main feature would be the data[\"image\"] and our target variable would be the data[\"target_positions\"] where data is a particular element of the instance of `AgentDataset`.\n\n\nNote that we can also adjust the size of image built with the rasterizer using the cfg[\"raster_params\"][\"raster_size\"] which defaults to **224 x 224** in size.","17bb34c8":"<a id=\"2\"><\/a>\n# 2. Understanding the Data","4a62e9be":"### Step.1 Declare the configrations\n\nThe configrations that we would use are already contained in the cfg variable but it is quite easier to work when you have all the parameters required in front of you. Hence we would declare a `cfg` dictionary containing the necessary configrations for model training.\n\nPS: Declaring LocalDataManager instance also comes under this step.","d8661f90":"Now to visualize the images we need either EgoDataset or AgentDataset instance as discussed above. We would go with EgoDataset instance. We would also prepare this dataset for the two versions of rasterizer.","04f355ab":"### Step 3. Instantiating and training the model","f138563c":"Okk, so now we are having a good amount of knowledge about the terms and definitions in the self driving domain. Let us now look over the **Self Driving Stack** which is usually implemented to accomplish a complete self driving task.\n\nA typical software stack would look something like-\n\n\n![Software_Stack.png](attachment:Software_Stack.png)\n> Image from Coursera's Self Driving course [here](https:\/\/www.coursera.org\/learn\/intro-self-driving-cars\/lecture\/RQCqM\/lesson-3-software-architecture).\n\nLet's just understand these sections in short.\n\n- **Sensors Output:** The data collected by the different sensors attached to the AV(autonomous vehicle).\n\n\n- **Environmental Perception:** In this module the AV uses the sensors' output to localize itself and understand the area around it (not only the road).\n\n\n- **Environmental Mapping:** This module creates a set of maps to localize the objects in the environment around ego vehicle. This module and the previous one interacts heavily to achieve a great efficiency in the perception task.\n\n\n- **Motion Planning:** On the basis of outputs recieved by `Environmental Perception` and `Environmental Mapping` modules, this module plans the motion of the AV. This motion planning is categorized in three layers of abstraction-\n    \n    - a. **Mission Planner:** It plans the Long term goal of the AV i.e the route to the destination where it has to go.\n    \n    - b. **Behavioural Planner:** It plans the Short term goal of the AV i.e given certain conditions and parameters, it decides what to do for a short span of time. For example if the ego velocity is some **x** per square meters and there is a truck ahead of it then plans how to action with this given data (either change a lane or slow speed).\n    \n    - c. **Local Planner:** It plans the immediate actions to take by the AV and draws a planned trajectory and velocity profile that the AV should follow for a short period of time.\n    \n    Consider these layers as zooming into the scenerio from outside. Driving from point A to point B is handled by the `Mission Planner`, actions taken on a particular path such as whether to change the lane or whether to speed up\/slow down etc are handled by the Behavioral Planner and finally the immediate actions such as planning the specific trajectory followed by AV for short period of time is handled by Local Planner.\n    \n\n- **Controller:** This module controls the AV according to the data recieved from the `Motion Planner`. It basically consists of **velocity controller** and **steering controller** which controls longitudnal and lateral motion of the AV respectively.\n\n\n- **System Supervisor:** This module acts as a supervisor which manages and alerts about the failures in the system. It supervises the hardware and software components seperately.  ","f5f4a8e6":"- **Please have a look [this](https:\/\/www.kaggle.com\/c\/lyft-motion-prediction-autonomous-vehicles\/discussion\/178781) discussion thread in which I've posted my analysis about the white paper explaning the structure and components of the dataset.**","95295217":"In the similar way you could create the AgentDataset instance.","85c5b84a":"<a id=\"4\"><\/a>\n\n# 4. References:\n\n- https:\/\/www.kaggle.com\/c\/lyft-motion-prediction-autonomous-vehicles\/discussion\/178323\n- https:\/\/www.kaggle.com\/nxrprime\/lyft-understanding-the-data-baseline-model\n- https:\/\/www.kaggle.com\/corochann\/lyft-deep-into-the-l5kit-library\n- https:\/\/www.kaggle.com\/corochann\/lyft-comprehensive-guide-to-start-competition\n","03181852":"<a id=\"1\"><\/a>\n# 1. Understanding the concepts ","72945bad":"# Preface","33d5f23e":"<a id=\"3\"><\/a>\n# 3. Sample Model Structure\n\n**Congratulations if you're following the tutorial from starting as now you have the knowledge of**\n\n- self driving vehicles\n- concepts related to self driving vehicles\n- Different Levels of automation\n- Self driving Stack\n- Dataset for this competition<br>\n\nand many more...\n\nWe can now finally talk about the model that would be used to train the data that we structured in previous sections. In the following, I would create a dummy model using the `resnet50` architecture.","f82e6d36":">In the above code, I am using the `sample.zarr` file for training for illustration purpose as `train.zarr` is computationally expensive to process through training but we would use `train.zarr` file for training the model.","22a793b9":"# Contents\n\n1. [Understanding the concepts](#1)\n2. [Understanding Data](#2)\n3. [Sample Model structure](#3)\n4. [References](#4)","9c88eedc":"## Steps of Model training","a104c152":"### Step 2. Prepairing the Dataset\n\nNote that we need to predict traffic agents' future motion and thus creating AgentDataset. Moreover we have the `sample_chunk` dataset and hence we can move directly for creating the AgentDataset. \n\nWe would also need rasterizer to create the AgentDataset instance and we can choose anyone b\/w raster_sem and raster_sat.**","adcd7f6b":"We would not dive into the very depth of the concepts but a high level knowledge of the things would be enough to proceed in the competition. After this notebook you would have a clear view of whether you should focus in the domain of self driving or not.\n\nLet's just get started-\n\n- **When to say vehicle is self driving:** We can say that a vehicle is self driving when it requires minimum human intervention while driving. If a vehicle is able to take up the essential decisions of driving task such as driving along a route, stopping on encountering an obstacle (OEDR), estimating the type of movement and type objects and responding accordingly etc.\n\n                                        \n- **Ego Vehicle:** The vehicle on which self automation is to be achieved. The word **Ego** denotes a notion of **self**.\n\n\n- **Traffic Agents:** Surrounding objects on the road other than the ego vehicle. These include pedestrians, other vehicles, cyclists etc.\n\n\n- **Ego Localization:** Apart from estimating the position of the surroundings, it is also required that we calculate the position of ego vehicle for calculating its velocity, angular motion, acceleration etc.\n\n\n- **Sensors:** Sensors are the devices that measures or detects a property of the environment or the changes in it. Basically there are two types of sensors used for self driving tasks viz **Exteroceptive** and **Proprioceptive** sensors. Let's just discuss these two sensors-\n         \n    - A. **Exteroceptive Sensors:** The sensors which are responsible for capturing the information of the environment around the ego vehicle. Information about the traffic agents and other static and dynamic objects are captured through these sensors.\n    \n    - B. **Proprioceptive Sensors:** The sensors which are responsible for ego localization or which records the properties of ego vehicle. These sensors help in determining the position, velocity, acceleration, angular motion etc of the ego vehicle.\n    \n    Let's just take some examples of Exteroceptive and Proprioceptive sensors. (Aa, Ba- First index of Exteroceptive and Proprioceptive sensors respectively)\n    \n    - Aa. **Lidars:** Lidar stands for Light Detection and Ranging. As the name suggests these sensors emit light and the time of reflection of the light from surrounding objects is used to calculate the distance of ego vehicle from those objects. \n    > Output from the Lidar(s) is used to create a \"Localization map\".\n    \n    - Ab. **Radars:** Radar stands for Radio Detection and Ranging. These sensors use radio signals to calculate the distance of ego vehicle from the surrounding objects.\n    \n    - Ba. **GNSS:** GNSS stands for Global Navigation Satellite System. These are used for calculating the position of any vehicle. GPS is one of the class of GNSS.\n    \n    - Bb. **IMU:** IMU stands for Intertial Measurment Unit. These sensors are responsible for calculating the  acceleration and velocity of the vehicle or in other words, the \"longitudnal motion\" of the vehicle.\n  \n    Other class of sensors are SONARS(A), WHEEL ODOMETRY(B) etc.\n\n\n- **Lateral Control:** Lateral control corresponds to the control of vehicle over its angular motion like changing lane, taking turn etc. This motion deals with controlling the steering of the vehicle.\n\n\n- **Longitudnal Control:** Longitudnal control corresponds to the control of vehicle over its velocity, acceleration, braking etc.\n\n\n- **OEDR:** OEDR stands for Object and Event Detection and Response. It corresponds to the immediate actions taken by the vehicles such as applying brake on encountering a sudden obstacle or handling of overtaking by other vehicles.\n\n\n- **ACC:** ACC stands for Adaptive Cruise Control. It is a system for controlling longitudnal speeds and widely used in Level-2 vehicles(see below).\n\n\n- **FMEA:** FMEA stands for Failure mode and effects analysis. It is a bottom up approach to detect failures in subsystems and their impact on higher levels.\n\n\n- **ODD:** ODD stands for Operational Design Domain. These are the set of conditions under which a particular system is designed to funtion.\n\n\n- **Perception task:** Understanding and estimating the future of objects around the ego vehicle is referred to as perception task.\n\n\n- **Motion Forecasting:** From the view of perception task, motion forecasting is the predicting of future motion of the traffic agents.","3166e6fe":"Now we need basically two different type of maps viz semantic and satellite to visualize the image generated by rasterizer in different ways. For this we create two different instances of rasterizer viz `raster_sem` and `raster_sat`.  ","8da19913":"**THAT'S IT FOR THIS NOTEBOOK GUYS!!!**\n\n**HOPE YOU LIKED MY WORK AND IF SO, PLEASE UPVOTE.**"}}