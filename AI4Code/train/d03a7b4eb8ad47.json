{"cell_type":{"54973644":"code","4f9036b3":"code","8373415c":"code","c9e55583":"code","83104b43":"code","20e658bc":"code","c1229a82":"code","78f4a927":"code","02a10f98":"code","d6afe816":"code","398e70b1":"code","8e0e5e04":"code","7148e7be":"code","ab9f9790":"code","4fc54db7":"code","38a1d9d1":"code","d259a872":"code","3c2b2d95":"code","c6bd1fdc":"code","4eba7eed":"code","657356f0":"code","92e22800":"code","eb77bca7":"code","14ff34b2":"code","d58feedc":"code","4d94e195":"code","3f4bad15":"code","6108c153":"code","df7992e4":"code","b8f02f78":"code","0c4b907b":"code","fe66909f":"code","ffe5fa03":"code","80a63bd8":"code","8f253256":"code","2a900233":"code","79cea102":"code","aa021d2b":"code","484d9482":"markdown","1e726548":"markdown","8ee8c080":"markdown","eb0d38fd":"markdown","53937542":"markdown","1854fc06":"markdown","23d5f5cd":"markdown","e56dec43":"markdown"},"source":{"54973644":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense ,LSTM ,Embedding , Dropout\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk import word_tokenize , sent_tokenize\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score , confusion_matrix","4f9036b3":"df = pd.read_csv('..\/input\/traindata\/train.csv')\ndf","8373415c":"df.dropna(inplace = True)\ndf.reset_index(inplace =True)","c9e55583":"df = df.sample(frac = 1).reset_index(drop = True)","83104b43":"df = df.head(8000)","20e658bc":"x = df[['title' , 'author' , 'text']]\ny = df['label']","c1229a82":"x","78f4a927":"x.shape , y.shape","02a10f98":"tensorflow.__version__","d6afe816":"#preprocessing\nimport time\ns = time.time()\ncorpus = []\nfor i in range(len(x)):\n    if i+1 % 100 == 0:\n        print(i)\n    \n    text = re.sub('[^a-zA-Z]' , \" \", x['text'][i])\n    text = text.lower()\n    text = text.split()\n    \n    word = [words for words in text if words not in stopwords.words('english') ]\n    word = \" \".join(word) \n    corpus.append(word)\nprint('done')\nprint((time.time() - s)*1000)","398e70b1":"corpus","8e0e5e04":"#one hot\nvoc_size =6000","7148e7be":"one_hot_sentence = [one_hot(words , voc_size) for words in corpus]","ab9f9790":"one_hot_sentence[:20]","4fc54db7":"# to make fixed length\nmax_length_of_sent = 50\nembedding_sent = pad_sequences(one_hot_sentence,padding='pre' , maxlen=max_length_of_sent)","38a1d9d1":"embedding_sent[0]","d259a872":"len(embedding_sent[0])","3c2b2d95":"len(embedding_sent)","c6bd1fdc":"len(x) , len(y)","4eba7eed":"embedding_feature_size = 256\n#after taking input , how much should be length of feature vector after passing into model","657356f0":"#make model\nmodel = Sequential()\nmodel.add(Embedding(voc_size , embedding_feature_size , input_length = max_length_of_sent ))\nmodel.add(LSTM(256 , return_sequences = True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1 , activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy',metrics = ['accuracy'],optimizer = 'adam')\nmodel.summary()","92e22800":"X = np.array(embedding_sent)\nY = np.array(y)","eb77bca7":"X.shape , y.shape","14ff34b2":"x_train , x_test , y_train , y_test = train_test_split(X ,Y,test_size = 0.25 , random_state =100)","d58feedc":"#train\nhistory = model.fit(x_train , y_train , validation_data = (x_test , y_test) , epochs =10 , batch_size= 128)","4d94e195":"plt.plot(history.history['accuracy'] , label = 'train_acc')\nplt.plot(history.history['val_accuracy'] , label = 'val_acc')\nplt.legend()\n\n","3f4bad15":"plt.plot(history.history['loss'] , label = 'train_loss')\nplt.plot(history.history['val_loss'] , label ='val_loss')\nplt.legend()\n","6108c153":"y_pred = model.predict_classes(x_test)","df7992e4":"y_pred","b8f02f78":"confusion_matrix(y_pred , y_test)","0c4b907b":"accuracy_score(y_pred, y_test)","fe66909f":"user_inp = 'There were many forest fire and Trump was seen dancing there :)'","ffe5fa03":"#preprocessing\nimport time\ns = time.time()\ncorpus = []\n\ntext = re.sub('[^a-zA-Z]' , \" \", user_inp)\ntext = text.lower()\ntext = text.split()\n\nword = [words for words in text if words not in stopwords.words('english') ]\nword = \" \".join(word) \ncorpus.append(word)\nprint('done')\nprint((time.time() - s)*1000)","80a63bd8":"user_one_hot = [one_hot(words , voc_size) for words in corpus]","8f253256":"user_one_hot","2a900233":"# to make fixed length\nmax_length_of_sent = 50\nembedding_sent_user = pad_sequences(user_one_hot,padding='pre' , maxlen=max_length_of_sent)\nembedding_sent_user","79cea102":"check = np.array(embedding_sent_user)","aa021d2b":"model.predict_classes(check)","484d9482":"# Import Libs","1e726548":"## 1 - Represent Fake , Model working fine :)","8ee8c080":"## Since , there are 20k records , Training a LSTM model took me around 2hr with CPU , So here am going to cut short it to 8k","eb0d38fd":"# User's input","53937542":"## If you find this notebook useful , please upvote and leave a comment ! Thank you :)","1854fc06":"# Read Data","23d5f5cd":"# Drop NaN","e56dec43":"## We get 85 % accuracy just for 10 epochs !"}}