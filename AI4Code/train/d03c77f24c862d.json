{"cell_type":{"a38d9cc3":"code","2dd1e7d3":"code","f1baf8f4":"code","4df12a31":"code","1073b248":"code","72edcf3f":"code","ebc5982e":"code","e650782b":"code","2895cad2":"code","91b2c58d":"code","ab5bd92b":"code","8bcaf992":"code","24ba24c9":"code","7eed67ce":"code","86897c2d":"code","4a9a217b":"markdown","f73266b1":"markdown","8667a8d4":"markdown","2b93aa28":"markdown","7ff1b6f2":"markdown","5c3533ac":"markdown","1e172178":"markdown","4de7e2c0":"markdown","a7049222":"markdown","a30adb28":"markdown","3ef5766a":"markdown","6199316c":"markdown","97800d84":"markdown","eb31759a":"markdown","c5abc441":"markdown","0f87d92f":"markdown"},"source":{"a38d9cc3":"# importing libraries\n\nimport os\nimport sys \nimport cv2 as cv\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom sklearn.model_selection import train_test_split","2dd1e7d3":"# Loading data and reducing size to 64 x 64 pixels\n\nIMG_DIR = '..\/input\/animefacedataset\/images'\nprint(\"Loading...\")\nX = []\nX_flat = []\ncount = 1\ntotal = 200\nfor img in os.listdir(IMG_DIR):\n    if count == total + 1:\n        break\n    sys.stdout.write(\"\\r\" + str(count) + \" \/ \" + str(total))\n    sys.stdout.flush()\n    img_array = cv.imread(os.path.join(IMG_DIR,img), cv.IMREAD_GRAYSCALE)\n    img_pil = Image.fromarray(img_array)\n    img_28x28 = np.array(img_pil.resize((64, 64), Image.ANTIALIAS))\n    X.append(img_28x28)\n    img_array = img_28x28.flatten()\n    X_flat.append(img_array)\n    count += 1\nprint()\nprint(\"Done!\")","f1baf8f4":"# visualizing some images\n\nfig, ax = plt.subplots(figsize = (20, 20))\nfor i in range(1, 26):\n    plt.subplot(5, 5, i)\n    plt.title('Anime image {}'.format(i-1))\n    plt.imshow(X[i-1])","4df12a31":"# converting X_flat to numpy array\n\nX_flat = np.asarray(X_flat)\nX_flat.shape","1073b248":"# Test-Train split\n\nX_train, X_test = train_test_split(X_flat, test_size = 0.5, random_state = 42)\nX_train = X_train.T\nX_test = X_test.T\nprint(X_train.shape)\nprint(X_test.shape)","72edcf3f":"# function for standardizing image\n\ndef Standardize(X):\n    \n    # calcualte the mean of each column mu   \n    mu = np.mean(X, axis = 0) \n    \n    # Substract the mean from X\n    X = X - mu  \n    \n    # calcualte the standard deviation of each column std\n    std = np.std(X, axis = 0)  \n    \n    # handleing zero standard deviation\n    std_filled = std.copy()\n    std_filled[std == 0] = 1.0\n    \n    # calculate standardized X called Xbar\n    Xbar = (X-mu) \/ std_filled  \n    \n    return Xbar, mu, std","ebc5982e":"# function for calcualting eigen values and eigen vectors\n\ndef eig(S):\n    \n    # calculate the eigen values and eigen vectors\n    eig_val, eig_vec = np.linalg.eigh(S)  \n    \n    # sorting them in decrasing order\n    sorted_eig  = np.argsort(-eig_val)\n    eig_val = eig_val[sorted_eig]\n    eig_vec = eig_vec[:, sorted_eig]\n    \n    return (eig_val, eig_vec)","e650782b":"# function for projection matrix\n\ndef projection_matrix(B):\n    \n    # calculate the projection matrix P\n    P = B @ B.T \n    \n    return P","2895cad2":"# implementing PCA\n\ndef PCA(X, num_components):\n    \n    # calculate the data covariance matrix S\n    S = np.cov(X)  \n    \n    # now find eigenvalues and corresponding eigenvectors for S by implementing eig().\n    eig_vals, eig_vecs = eig(S)\n    \n    # select eigen vectors\n    U = eig_vecs[:, range(num_components)]\n    \n    # reconstruct the images from the lowerdimensional representation\n    # to do this, we first need to find the projection_matrix (which you implemented earlier)\n    # which projects our input data onto the vector space spanned by the eigenvectors\n    P = projection_matrix(U) # projection matrix\n    \n    return P","91b2c58d":"# standardizing\n\nXbar_train, mu_train, std_train = Standardize(X_train)\nXbar_test, mu_test, std_test = Standardize(X_test)","ab5bd92b":"# function for mean square error\n\ndef mse(predict, actual):\n    return np.square(predict - actual).sum(axis = 1).mean()","8bcaf992":"# calculating loss and reconstructing image\n\nloss = []\nreconstructions = []\nmax_components = len(X_train.T)\nprint(\"Processing...\")\nanimation = np.arange(1, max_components + 1, 1)\nfor num_component in range(1, max_components + 1):\n    sys.stdout.write(\"\\r\" + str(animation[num_component - 1]) + \" \/ \" + str(max_components))\n    sys.stdout.flush()\n    projection = PCA(Xbar_train.T, num_component)\n    reconst = Xbar_test @ projection\n    error = mse(reconst, Xbar_test)\n    reconstructions.append(reconst)\n    loss.append((num_component, error))\nprint()\nprint(\"Done!\")","24ba24c9":"# \"unstandardize\" the reconstructed images\n\nreconstructions = np.asarray(reconstructions)\nreconstructions = reconstructions * std_test + mu_test \nloss = np.asarray(loss)","7eed67ce":"# visualizing mse vs number of principal components\n\nfig, ax = plt.subplots(figsize = (20, 8))\nax.plot(loss[:,0], loss[:,1])\nax.xaxis.set_ticks(np.arange(0, max_components, 15))\nax.set(xlabel = 'num_components', ylabel = 'MSE', title = 'MSE vs number of principal components')\nplt.grid()","86897c2d":"# plotting the reconstructed images\n\nn = 25 # number of double-rows\nm = 5 # number of columns\nt = 0.9 # 1-t == top space \nb = 0.1 # bottom space      (both in figure coordinates)\nmsp = 0.3 # minor spacing\nsp = 0.8  # major spacing\noffs = (1+msp) * (t-b) \/ (2*n + n*msp + (n-1)*sp) # grid offset\nhspace = sp + msp + 1 #height space per grid\ngso = GridSpec(n, m, bottom = b+offs, top = t, hspace = hspace)\ngse = GridSpec(n, m, bottom = b, top = t-offs, hspace = hspace)\nfig = plt.figure(figsize=(20, 200))\naxes = []\nfor i in range(n*m):\n    axes.append(fig.add_subplot(gso[i]))\n    axes.append(fig.add_subplot(gse[i]))\nfor i in range(25):\n    k = 1\n    for j in range(10):\n        axes[j + i*10].set_title('{} principal components'.format(k))\n        axes[j + i*10].imshow(reconstructions[k, :, i].reshape(64, 64))\n        k += 10\nplt.show()","4a9a217b":"### PCA","f73266b1":"### Loss and Reconstruction","8667a8d4":"### Test and Train split","2b93aa28":"PCA is used when we need to tackle the curse of dimensionality among data with linear relationships , i.e. where having too many dimensions (features) in your data causes noise and difficulties.","7ff1b6f2":"### Conclusion:\n\nThe more the principle components used, the better is the accuracy of reconstruction.","5c3533ac":"Please do not import any other libraries. These should be sufficient to implement PCA so that we can learn PCA from scratch.","1e172178":"Now we will split the data into test and train. We will reconstruct test faces using information about faces learned from the training data.","4de7e2c0":"For the sake of visually being able to see that our algorthm works or not, we will work with image data.","a7049222":"### Loading dataset","a30adb28":"PCA is an unsupervised linear dimensionality reduction algorithm to find a more meaningful basis or coordinate system for our data and works based on covariance matrix to find the strongest features in our samples.","3ef5766a":"Now we will implement PCA. Before we do that, let's pause for a moment and think about the steps for performing PCA. Assume that we are performing PCA on some dataset $\\boldsymbol X$ for $M$ principal components. We then need to perform the following steps, which we break into parts:\n\nData standardization.\n\nFind eigenvalues and corresponding eigenvectors for the covariance matrix $\\boldsymbol S$. Sort by the largest eigenvalues and the corresponding eigenvectors (eig).\n\nAfter these steps, we can then compute the projection and reconstruction of the data onto the spaced spanned by the top $M$ eigenvectors.","6199316c":"In this notebook, we will implement PCA. We will break down the task of implementing PCA into small components and combine them in the end to produce the final algorithm.","97800d84":"### Standardization","eb31759a":"### Importing libraries","c5abc441":"# Principal Component Analysis (PCA)","0f87d92f":"Now, with the help of the functions you have implemented above, let's implement PCA!"}}