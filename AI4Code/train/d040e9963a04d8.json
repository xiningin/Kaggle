{"cell_type":{"89a2bba6":"code","5fa64438":"code","cff63504":"code","4b30bf7f":"code","8ad844e3":"code","c98f8401":"code","8eb96250":"code","ca327d43":"code","0e45e4ce":"code","280ad51a":"markdown","3d2d1b8a":"markdown","e5180f49":"markdown","2c79fdd0":"markdown","a87d7c45":"markdown","41dcd537":"markdown","3240dc30":"markdown"},"source":{"89a2bba6":"%%time\n\nimport sys\n!cp ..\/input\/rapids\/rapids.0.12.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","5fa64438":"from cuml.neighbors import KNeighborsClassifier, NearestNeighbors\nimport cuml; cuml.__version__","cff63504":"from sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=100, \n                  centers=5,\n                  cluster_std=5.0,\n                  n_features=4)\n\nknn = KNeighborsClassifier(n_neighbors=10)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80)\n\nknn.fit(X_train, y_train)","4b30bf7f":"X_test.shape, y_test.shape","8ad844e3":"y_test","c98f8401":"knn.predict(X_test)","8eb96250":"knn.predict_proba(X_test)","ca327d43":"import numpy as np\n\nKNN=10\nbatch=5\n\nclf = NearestNeighbors(n_neighbors=KNN)\n\nclf.fit(X_train)\n\ndistances, indices = clf.kneighbors(X_test)\n\nct = indices.shape[0]\n\npred = np.zeros((ct, KNN),dtype=np.int8)\n\nprobabilities = np.zeros((ct, len(np.unique(y_train))),dtype=np.float32)\n\nit = ct\/\/batch + int(ct%batch!=0)\n\nfor k in range(it):\n    \n    a = batch*k; b = batch*(k+1); b = min(ct,b)\n    pred[a:b,:] = y_train[ indices[a:b].astype(int) ]\n    \n    for j in np.unique(y_train):\n        probabilities[a:b,j] = np.sum(pred[a:b,]==j,axis=1)\/KNN","0e45e4ce":"probabilities","280ad51a":"## The purpose of this notebook\n\nI think most of us are familiar with [this great kernel](https:\/\/www.kaggle.com\/cdeotte\/rapids-knn-30-seconds-0-938) by Chris Deotte showing how to build a very promissing KNN model on the competition data using RAPIDS. A few days ago, I decided to play with this model and discovered a very annoying bug in the RAPIDS implementation of the KNN algorithm: its `predict_proba()` method returns a binary outcome of zeros and ones instead of actual probabilities. The good new is that Chris has already submitted a bug report, so hopefully it will be fixed soon. The bad news is that fixing it might take a while and the competition might be over by then. Fortunately, Chris suggested a clever workaround which I am going to share with you in this notebook. \n\nI will start by generating a simple data set for classification. Then I will train a RAPIDS KNN model on these data and try to make predictions using the `predict()` method first and the `predict_proba()` method second. The former will successfully return the class labels but the latter will only return binary outcomes. Then I will show you how to get actual KNN probabilities using the `kneighbors()` method of the RAPIDS `NearestNeighbors()` class. \n\n## Loading RAPIDS","3d2d1b8a":"## Predictions\n\nLet's try to predict classes (labels):","e5180f49":"## Generate data\n\nGenerate data for KNN classifier; then train the classifier on these data.","2c79fdd0":"We have successfully computed the probabilities! This method works pretty fast on the competition data -- it is possible to build it in a 5-fold cross-validation algorithm.","a87d7c45":"Predicting the labels seems to be working just fine. Let's try predicting probabilities using the `predict_proba` method.","41dcd537":"Predicting probabilities fails: we only get binary zeros and ones, not the actual probabilities (in the case at hands we got only zeros -- it is possible to change the parameters of the `make_blobs()` function to get both zeros and ones).\n\n## Workaround\n\nThe idea of the workaround is very simple: let's find the train set indecies of the nearest neighbours for all points in the test set. Here is how it can be done: ","3240dc30":"Here is a gilimpse of our test data:"}}