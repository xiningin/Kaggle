{"cell_type":{"8f689881":"code","fced57fa":"code","571f30eb":"code","8c4fcd9a":"code","95a98212":"code","ec3d35c1":"code","4c973a6b":"code","6324146e":"code","e58e3f7f":"code","0d8022a6":"code","50f913d8":"code","74b9a82e":"code","6b67417a":"code","e34e5cbf":"code","dd72ec72":"code","499241e5":"code","16a627a6":"code","da913b20":"code","147325cf":"code","42e92aab":"code","a06c1363":"markdown","c549de3d":"markdown","cc895f28":"markdown","a2943802":"markdown","5ee42e9e":"markdown","3a183714":"markdown","b552bdf9":"markdown","669d77f0":"markdown","32018822":"markdown","5d9cb45a":"markdown","c2cdae61":"markdown","8793f7d6":"markdown"},"source":{"8f689881":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport tensorflow as tf\nimport pandas as pd","fced57fa":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","571f30eb":"train_df.head()","8c4fcd9a":"print(f'Shape of data: {train_df.shape}')","95a98212":"# Find the number of missing values\nprint(train_df.info())","ec3d35c1":"# Fitting to the input data\ntokenizer = Tokenizer(num_words=700, oov_token='OOV')\ntokenizer.fit_on_texts(train_df.head()['text'])","4c973a6b":"# Tokens \nword_index = tokenizer.word_index\nprint(word_index)","6324146e":"# Generate text sequences\nsequences = tokenizer.texts_to_sequences(train_df.head()['text'])\nprint(sequences)","e58e3f7f":"padded = pad_sequences(sequences, padding='post')\nprint(padded)","0d8022a6":"# Splitting the data into 2\/3 as train and 1\/3 as test\nX_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], test_size=0.33, random_state=42)","50f913d8":"vocab_size = 500\nembedding_dim = 16\nmax_length = 50\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\n# Tokenization\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X_train)\ntesting_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Padding\npadded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)  # \ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length)  # , maxlen=max_length","74b9a82e":"padded.shape","6b67417a":"testing_padded.shape","e34e5cbf":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","dd72ec72":"num_epochs = 10\nhistory = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","499241e5":"# Getting weights from the embedding\ne = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)\n\n# Reverse mapping function from token to word\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","16a627a6":"# Combining embedding and words into a DataFrame\nembedding_df = pd.DataFrame()\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    embedding_df = embedding_df.append(pd.Series({'word':word, 'em':embeddings}), ignore_index=True)\n    \nembedding_df = pd.concat([embedding_df['em'].apply(pd.Series), embedding_df['word']], axis=1)","da913b20":"# Using PCA to map 16 embedding values to 3 to plot\np = PCA(n_components=3)\nprincipal_components = p.fit_transform(embedding_df.filter(regex=r'\\d'))\nembedding_df[['x', 'y', 'z']] = pd.DataFrame(principal_components)\n\nembedding_df.shape","147325cf":"fig = px.scatter_3d(embedding_df, x='x', y='y', z='z', hover_name='word', color='x')\nfig.show()","42e92aab":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend(loc=0)\nplt.show()","a06c1363":"# Tokenization\n\nLets suppose you have two statements you need to classify as positive or negative.\n\n| **text**         | **sentiment** |\n|------------------|---------------|\n| I am happy today | positive      |\n| He hit me today! | negative      |\n\nThe neural network does't understands words like `I, am, today`. To feed these into the neural network, each word must be converted into a unique number or **token**.\n\n> {'i': 1, 'am': 2, 'happy': 3, 'today': 5, 'he': 4, 'hit': 6, 'me': 7}\n\nNow the sentences becomes-<br>\n`I am happy today` -> `(1, 2, 3, 5)`<br>\n`He hit me today!` -> `(4, 6, 7, 5)`<br>\n\nNote that `today` has the same token in both the sentences. By default symbols are stripped and every character is converted to lower case. This array of tokens is **sequence**.\n\n### Tokenizer\n> * **num_words** Number of unique tokens to be used while creating embeddings from text. If more words are present in the data and number of tokens are less, it takes the most common words to generate embeddings<br>\n* **oov_token** Token to be used if some new value is encountered while converting text to sequence or embedding\n\n### Tokenizer.fit_on_text()\n> fits the tokenizer of the given input text\n\n### Tokenizer.text_to_sequences(texts)\n> converts `texts` to sequences\n\nLets try the tokenization code on just 5 rows of data.","c549de3d":"# Importing Libraries and reading data","cc895f28":"# Modelling\n\n### tf.keras.layers.Embedding\n`tf.keras.layers.Embedding` Turns positive integers (indexes) into dense vectors of fixed size.\n\n> * **input_dim** size of the vocabulary\n* **output_dim** Dimension of the dense embedding\n* **input_length** Length of input sequences, when it is constant.This argument is required if you are going to connect\n    `Flatten` then `Dense` layers upstream\n    (without it, the shape of the dense outputs cannot be computed).","a2943802":"Think of loss as the **error** in prediction. **With increasing epochs the training accuracy increases constantly while the validation accuracy increases then slowly decreases as overfitting occurs.\nWith increasing epoch the training loss decreases constantly while validation loss decreases first than slowly increases.**","5ee42e9e":"As you can see, after 3-4 epochs the model is overfitting. Try tweaking the model or early stopping (discussed is Part 2) to avoid that.","3a183714":"# Preprocessing","b552bdf9":"# Overview of Dataset\n\n### Data Format\n\nEach sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)\n\n### Target\n\n**You are predicting whether a given tweet is about a real disaster or not**. If so, predict a 1. If not, predict a 0.\n\n### Columns\n\nid - a unique identifier for each tweet\ntext - the text of the tweet\nlocation - the location the tweet was sent from (may be blank)\nkeyword - a particular keyword from the tweet (may be blank)\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n> NOTE: **We will be using just the text and target features of the data**","669d77f0":"# Visualization\nNow lets plot the embedding into a 3D space to see if the embedding is able to separate features in words.\n\nWe will be using **PCA-Principal Component Analysis** to reduce the dimensionality of the data with minimal loss in features. The theory of PCA is beyond the scope of this tutorial. We will be using PCA to reduce 16 points to 3 to facilitate plot.","32018822":"### pad_sequence\nPads zeros in sequence to make the length of sequences equal\n> * **sequences** list of sequences to be padded\n* **maxlen** Maximum length of all the sequence (if not passed, sequence will be padded to the length of the longest sequence\n* **padding** `pre` or `post`, pad before or after the sequence\n* **truncating** `pre` or `post`, remove values from sequences larger than `maxlen`, either at the beginning or at the end of the sequences.","5d9cb45a":"# Tensorflow Keras Tutorial - Basics of NLP (Part 5)\n\n**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Network without getting into intrinsic details of the Network. It can use Tensorflow or Theano as backend. This tutorial series will cover Keras from beginner to intermediate level.\n\nYOU CAN CHECK OUT REST OF THE TUTORIALS OF THIS SERIES.\n\n[PART 1](https:\/\/www.kaggle.com\/akashkr\/tf-keras-tutorial-neural-network-part-1)<br>\n[PART 2](https:\/\/www.kaggle.com\/akashkr\/tf-keras-tutorial-cnn-part-2)<br>\n[PART 3](https:\/\/www.kaggle.com\/akashkr\/tf-keras-tutorial-binary-classification-part-3)<br>\n[PART 4](https:\/\/www.kaggle.com\/akashkr\/tf-keras-tutorial-pretrained-models-part-4)\n\nIn the previous notebooks we worked on image data. Now we are going to see text data. The common places where NLP is applied is Document Classification, Sentiment Analysis, Chat-bots etc.\n\nIn this tutorial we are going to see,\n* Tokenization\n* padding\n* Embedding\n* Modelling\n* Visualizing Embedding weights","c2cdae61":"You can hover to the points on the +X side and see words like **hiroshima, derailment, earthquake, drought, suicide** etc and **bags, better, full** etc on the -X side. This shows how well the embedding layer is trained to separate words which might possibiliy be tweet of disaster. Try increasing the `vocab_size` to visualize more words.","8793f7d6":"# Embedding\nA word embedding is a learned representation for text where words that have the same meaning have a similar representation. As in tokenization, a word is represented as a number, in Embedding a word is represented as a vector\/set of numbers which represent meaning of the word i.e. similar words will have similar embedding. This preserves the sentiment of the words."}}