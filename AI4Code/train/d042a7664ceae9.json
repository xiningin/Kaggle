{"cell_type":{"5f6ca146":"code","96e0f2f9":"code","5a258160":"code","a6a917c2":"code","98edc417":"code","7a2d3516":"code","f873adfb":"code","fe7a1f49":"code","fff16d4f":"code","37c0fa15":"code","ff3e59fc":"code","94b2555d":"code","adbbef28":"code","4d156357":"code","1fa18cb4":"code","714ab573":"code","322afa03":"code","85e25624":"code","8c799f4f":"code","3cdb707d":"code","6f1bbd19":"code","af524c02":"code","360ad4cb":"code","c431cbef":"code","cf231b19":"code","cf39a80e":"code","867dbf2d":"markdown","394a39be":"markdown","e5bb708b":"markdown","4b29d15b":"markdown","cd02b152":"markdown","22783c1f":"markdown","80eb3a68":"markdown","f962123a":"markdown","8a8b9965":"markdown","010638fa":"markdown","cb6a7e44":"markdown"},"source":{"5f6ca146":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/face-expression-recognition-dataset\/images\/images\/'):\n    print(dirname)\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96e0f2f9":"cd '\/kaggle\/input\/face-expression-recognition-dataset\/images\/images\/'","5a258160":"pip install livelossplot==0.5.2","a6a917c2":"import tensorflow.keras as keras\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#import utils\nimport os\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nfrom IPython.display import SVG, Image\nfrom livelossplot import PlotLossesKerasTF\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","98edc417":"for expression in os.listdir(\"train\/\"):\n    print(str(len(os.listdir(\"train\/\" + expression))) + \" \" + expression + \" images\")","7a2d3516":"from matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nnrows = 4\nncols = 4\n\npic_index = 0\n# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols*4, nrows*4)\n\npic_index+=4\n\nnext_pix = [os.path.join(\"train\/\" + expression+'\/'+ os.listdir(\"train\/\" + expression)[0]) for expression in os.listdir(\"train\/\")]\n\nfor i, img_path in enumerate(next_pix):\n  # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(nrows, ncols, i + 1)\n    sp.axis('Off') # Don't show axes (or gridlines)\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\n    plt.title(img_path)\n\nplt.show()","f873adfb":"img_size = 48\nbatch_size = 64\n\ndatagen_train = ImageDataGenerator(horizontal_flip=True)\n#ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data\n#Translations, Rotations, Changes in scale, Shearing, Horizontal (and in some cases, vertical) flips\n\ntrain_generator = datagen_train.flow_from_directory(\"train\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\ndatagen_validation = ImageDataGenerator(horizontal_flip=True)\nvalidation_generator = datagen_validation.flow_from_directory(\"validation\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","fe7a1f49":"# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(7, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","fff16d4f":"%%time\n\nepochs = 15\nsteps_per_epoch = train_generator.n\/\/train_generator.batch_size\nvalidation_steps = validation_generator.n\/\/validation_generator.batch_size\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=2, min_lr=0.00001, mode='auto')\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n                             save_weights_only=True, mode='max', verbose=1)\ncallbacks = [PlotLossesKerasTF(), checkpoint, reduce_lr]\n\nhistory = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data = validation_generator,\n    validation_steps = validation_steps,\n    callbacks=callbacks\n)","37c0fa15":"cd \/kaggle\/working\/","ff3e59fc":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","94b2555d":"img_size = 224\nbatch_size = 64\n\ndatagen_train = ImageDataGenerator(horizontal_flip=True)\n#ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data\n#Translations, Rotations, Changes in scale, Shearing, Horizontal (and in some cases, vertical) flips\n\ntrain_generator = datagen_train.flow_from_directory(\"train\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"rgb\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\ndatagen_validation = ImageDataGenerator(horizontal_flip=True)\nvalidation_generator = datagen_validation.flow_from_directory(\"validation\/\",\n                                                    target_size=(img_size,img_size),\n                                                    color_mode=\"rgb\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","adbbef28":"from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\n\nIMAGE_SIZE = [224, 224]\n# add preprocessing layer to the fromt of VGG\nvgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) #pooling='max')","4d156357":"# don't train existing weights\nfor layer in vgg.layers:\n    layer.trainable = False","1fa18cb4":"# added layers \nx = Flatten()(vgg.output)\nx = Dense(1024, activation = 'relu')(x)\nx = Dropout(0.15)(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.15)(x)\nprediction = Dense(7, activation='softmax')(x)","714ab573":"# create a model object\nmodel = Model(inputs=vgg.input, outputs=prediction)\n# view the structure of the model\nmodel.summary()","322afa03":"#Compiling the model adding loss function and optimizer and evaluation metrics\nmodel.compile( loss='categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = ['accuracy']\n)","85e25624":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n# Set a Learning Rate Annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                           patience=3,\n                                           verbose=1,\n                                           factor=0.5,\n                                           min_lr=0.00001)\n","8c799f4f":"history = model.fit_generator(train_generator,\n        validation_data = validation_generator,\n        epochs = 15,\n        steps_per_epoch = len(train_generator),\n        validation_steps = len(validation_generator)#,\n        #callbacks = [learning_rate_reduction]\n)","3cdb707d":"# loss\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\n\n# accuracies\nplt.plot(history.history['accuracy'], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()","6f1bbd19":"from keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\n\nfrom keras.models import load_model","af524c02":"cd '\/kaggle\/input\/densenet-keras'\n","360ad4cb":"ls","c431cbef":"# create the base pre-trained model\nbase_model = DenseNet121(weights='DenseNet-BC-121-32-no-top.h5', include_top=False)\n\nx = base_model.output\n\n# add a global spatial average pooling layer\nx = GlobalAveragePooling2D()(x)\n\n# and a logistic layer\npredictions = Dense(7, activation=\"softmax\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])\n\n#model.load_weights(\".\/nih\/pretrained_model.h5\")","cf231b19":"cd '\/kaggle\/input\/face-expression-recognition-dataset\/images\/'","cf39a80e":"history = model.fit_generator(train_generator, \n                               validation_data=validation_generator,\n                               steps_per_epoch = len(train_generator),\n                               validation_steps = len(validation_generator), \n                               epochs = 15)\n \nplt.plot(history.history['loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()","867dbf2d":"make your model work with grayscale images. You just need to make the image to appear to be RGB. ","394a39be":"VGG-19 is a convolutional neural network which is 19 layers deep. It's pretrained version of the network has trained on more than a million images from the ImageNet database. This network is trained on images with an input size of 224-by-224. By adding few more layers to this pretrained model, we can use it for classification of the various dance forms. As, pretrained model has already extracted out the edges and important features from millions of images, so it can classify images with much higher accuracy.","e5bb708b":"Changing the path to the dataset location","4b29d15b":"Defining the model architecture. With Convolution layers, setting activation function for the layer, batch normalization and drop out regularization against overfitting. Max Pooling Layers too.","cd02b152":"Checking the number of images (Imbalanced Classes)","22783c1f":"**Import Libraries**","80eb3a68":"Plotting a sample of the images from each category","f962123a":"Creating train and validation image generators for generating images with data augmentation while training the model, in memory.","8a8b9965":"The accuracy didn't improve. The gap between training and validation accuracy seems to be increasing in every epoch, probably overfitting...","010638fa":"**Trying Transfer Learning**  \n* VGG","cb6a7e44":"Training the model"}}