{"cell_type":{"c52597a3":"code","9227eff1":"code","cd6faf4d":"code","20daf001":"code","ef8358c8":"code","06c2be7b":"code","1f90f296":"code","8a47a1be":"code","2b5d6d53":"code","999dfb20":"code","58dcc8f5":"code","263d79c0":"code","f235e2d6":"code","8337bdb0":"code","ec10cb27":"code","87db61b2":"code","f6d67450":"code","6f27a9b7":"code","7ce814ab":"code","b5406145":"code","27b9c0a2":"code","b144b9d6":"code","02b10cd1":"code","1304312d":"code","f34ae450":"code","4092172c":"code","4901d6db":"code","5244dacb":"code","35490492":"code","3175e2cd":"code","2ae80a14":"code","8ce7fb97":"code","00e1f6b3":"code","8dabdd17":"code","5bf02da5":"code","5bca7e5e":"code","d33b63b9":"code","70a15848":"code","8ae341c7":"code","1514ebde":"code","e658ff78":"code","d91a13c2":"code","3ffe7a0c":"code","cd628da4":"code","1156bdb4":"code","8a6eda25":"code","a76f8262":"code","3d5437f8":"code","e3f4559f":"code","0cada9f5":"code","9bee9551":"code","a9673ff4":"code","add61772":"code","aa1f7b9c":"code","ee39acde":"code","93bc568f":"code","cda1e944":"code","ea9bec67":"code","ebd5101d":"code","0ce826f7":"code","5385e5d4":"code","a8f4ec79":"code","02a410be":"code","6f1c7f6d":"code","129fcdc3":"code","e140c3cf":"code","f70b5bcf":"code","faf5e76d":"code","5352453d":"code","bffa8ef9":"code","36523ff8":"code","12988c61":"code","d456c805":"code","8bdb978f":"code","714e926a":"code","c023c66d":"code","15040618":"code","0e6b5cc5":"code","f7c2ec7d":"code","f8e07d0a":"code","75580e66":"code","f2cf728e":"code","25b563dc":"code","1d033add":"code","c2540573":"code","752a11a5":"code","ec24c87f":"code","d4373fe0":"code","74ab514c":"code","4e394ae9":"code","29ca83fa":"code","24475357":"code","c4b4b81a":"code","4d823394":"code","a7096dee":"code","1eca328f":"code","74bf3c17":"code","35592e86":"code","816ba463":"code","42961b0f":"code","3129044c":"code","198713ea":"code","405d477f":"code","79f4a4ad":"code","13f68243":"code","6197e5d5":"code","961b134b":"code","28cfd018":"code","4c22ff30":"code","22954cb7":"code","c447bbc9":"code","167a8683":"code","4dfa258f":"code","2b3b8b18":"code","1798e751":"code","fe1bf2e6":"code","dd3ea551":"code","7fab010d":"code","3f72f3fc":"code","16a6c023":"code","bb0160d7":"code","1d115c3b":"code","fc053f4b":"code","f08b75d6":"code","765afbae":"code","8b27ad8a":"code","ac1488dd":"code","14f20ac3":"code","6f4abbe9":"code","b6b16496":"code","c1f4ebc9":"markdown","355eb2f5":"markdown","7abcf7e0":"markdown","74aa4517":"markdown","bc9bdc28":"markdown","796cc1bb":"markdown","3467b2b4":"markdown","af4cad67":"markdown","1c3a4de4":"markdown","6ef4cfc1":"markdown","b3697327":"markdown","11db9780":"markdown","bea60120":"markdown","f3f41051":"markdown","2dbde47d":"markdown","b7ebcd0e":"markdown","70004805":"markdown","083250da":"markdown","093af062":"markdown","2691aa63":"markdown","3540cb2c":"markdown","4710c208":"markdown","3c6b177c":"markdown","f4e2b2bc":"markdown","eeb5ab8f":"markdown","eb938c1f":"markdown","ddb49cea":"markdown","2ba17319":"markdown","1de527ab":"markdown","8bb2ca7f":"markdown","5ee47315":"markdown","fda3d5f4":"markdown","1a504907":"markdown","278c5709":"markdown","20809458":"markdown","c5cca36e":"markdown","38611360":"markdown","cbc027ba":"markdown","266708b4":"markdown","9c4d15ca":"markdown","12a6d66a":"markdown","a328ff29":"markdown","9d73bafb":"markdown","3fdc878f":"markdown","4015602a":"markdown","5fe4ab9d":"markdown","22c24a78":"markdown","d60aae89":"markdown","f98a3e9a":"markdown","13520e54":"markdown","7611f02c":"markdown","3547bf42":"markdown","b9c42ca7":"markdown","f50c35fc":"markdown","10d400c1":"markdown","d2ddd58b":"markdown","fc6673c5":"markdown","61d407dc":"markdown","5e557ad0":"markdown","4bec8a26":"markdown","490195a3":"markdown","1b104630":"markdown","ae5f328c":"markdown","39ec6887":"markdown","fe3b16b9":"markdown","ac2e0f82":"markdown","3a1e802b":"markdown","a93c805b":"markdown","43abb9ab":"markdown","06dd2cc2":"markdown","fb666ebf":"markdown","9409a4b6":"markdown","4eda60ae":"markdown","28f00925":"markdown","72dc05a1":"markdown","47cfe56e":"markdown","95275830":"markdown","60100c57":"markdown","184b5999":"markdown","ff30c2a9":"markdown","9eba08e5":"markdown","5ce2d500":"markdown","2f5b4c8b":"markdown","7db080b6":"markdown","ed81305e":"markdown","8a635fe7":"markdown","0df56180":"markdown","25a98866":"markdown","d00f26bc":"markdown","306dacd5":"markdown","5068847e":"markdown","4bea173c":"markdown"},"source":{"c52597a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9227eff1":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport gc\nimport lightgbm as lgb\nimport time\n# import datetime\n# import xgboost as xgb\n# import time\n# import itertools\n# from sklearn.linear_model import LinearRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline\nsns.set()","cd6faf4d":"INPUT_DIR = '\/kaggle\/input\/m5-forecasting-accuracy'\n\ncalendar_df = pd.read_csv(f\"{INPUT_DIR}\/calendar.csv\")\nsell_prices_df = pd.read_csv(f\"{INPUT_DIR}\/sell_prices.csv\")\nsales_train_validation_df = pd.read_csv(f\"{INPUT_DIR}\/sales_train_validation.csv\")\nsample_submission_df = pd.read_csv(f\"{INPUT_DIR}\/sample_submission.csv\")","20daf001":"# Calendar data type cast -> Memory Usage Reduction\ncalendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]] = calendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]].astype(\"int8\")\ncalendar_df[[\"wm_yr_wk\", \"year\"]] = calendar_df[[\"wm_yr_wk\", \"year\"]].astype(\"int16\") \ncalendar_df[\"date\"] = calendar_df[\"date\"].astype(\"datetime64\")\n\nnan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    calendar_df[feature].fillna('unknown', inplace = True)\n\ncalendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] = calendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] .astype(\"category\")","ef8358c8":"# Sales Training dataset cast -> Memory Usage Reduction\nsales_train_validation_df.loc[:, \"d_1\":] = sales_train_validation_df.loc[:, \"d_1\":].astype(\"int16\")","06c2be7b":"sell_prices_df.head()","1f90f296":"# Make ID column to sell_price dataframe\nsell_prices_df.loc[:, \"id\"] = sell_prices_df.loc[:, \"item_id\"] + \"_\" + sell_prices_df.loc[:, \"store_id\"] + \"_validation\"","8a47a1be":"sell_prices_df.head()","2b5d6d53":"sell_prices_df = pd.concat([sell_prices_df, sell_prices_df[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\nsell_prices_df = sell_prices_df.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\nsell_prices_df[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\"]] = sell_prices_df[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\"]].astype(\"category\")\nsell_prices_df = sell_prices_df.drop(columns=2)","999dfb20":"sell_prices_df.head()","58dcc8f5":"def make_dataframe():\n    # Wide format dataset \n    df_wide_train = sales_train_validation_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"state_id\",\"store_id\", \"id\"]).T\n    df_wide_train.index = calendar_df[\"date\"][:1913]\n    df_wide_train.columns = sales_train_validation_df[\"id\"]\n    \n    # Making test label dataset\n    df_wide_test = pd.DataFrame(np.zeros(shape=(56, len(df_wide_train.columns))), index=calendar_df.date[1913:], columns=df_wide_train.columns)\n    df_wide = pd.concat([df_wide_train, df_wide_test])\n\n    # Convert wide format to long format\n    df_long = df_wide.stack().reset_index(1)\n    df_long.columns = [\"id\", \"value\"]\n\n    del df_wide_train, df_wide_test, df_wide\n    gc.collect()\n    \n    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on=\"date\"), sell_prices_df, on=[\"id\", \"wm_yr_wk\"])\n    df = df.drop(columns=[\"d\"])\n#     df[[\"cat_id\", \"store_id\", \"item_id\", \"id\", \"dept_id\"]] = df[[\"cat_id\"\", store_id\", \"item_id\", \"id\", \"dept_id\"]].astype(\"category\")\n    df[\"sell_price\"] = df[\"sell_price\"].astype(\"float16\")   \n    df[\"value\"] = df[\"value\"].astype(\"int32\")\n    df[\"state_id\"] = df[\"store_id\"].str[:2].astype(\"category\")\n\n\n    del df_long\n    gc.collect()\n\n    return df\n\ndf = make_dataframe()","263d79c0":"df.dtypes","f235e2d6":"def add_date_feature(df):\n    df[\"year\"] = df[\"date\"].dt.year.astype(\"int16\")\n    df[\"month\"] = df[\"date\"].dt.month.astype(\"int8\")\n    df[\"week\"] = df[\"date\"].dt.week.astype(\"int8\")\n    df[\"day\"] = df[\"date\"].dt.day.astype(\"int8\")\n    df[\"quarter\"]  = df[\"date\"].dt.quarter.astype(\"int8\")\n    return df","8337bdb0":"df = add_date_feature(df)\ndf","ec10cb27":"temp_series = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()\ntemp_series","87db61b2":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category\")\nplt.legend()\n","f6d67450":"temp_series = temp_series.loc[temp_series.index.get_level_values(\"date\") >= \"2015-01-01\"]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year-Month\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category from 2015\")\nplt.legend()","6f27a9b7":"# Plot only December, 2015\ntemp_series = temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-01\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-31\")]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item per day in December, 2015\")\nplt.legend()","7ce814ab":"temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-24\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-26\")]","b5406145":"temp_series = df.groupby([\"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series","27b9c0a2":"plt.figure(figsize=(6, 4))\nleft = np.arange(1,8) \nwidth = 0.3\nweeklabel = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]    # Please Confirm df\n\n\nplt.bar(left, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, width=width, label=\"FOODS\")\nplt.bar(left + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, width=width, label=\"HOUSEHOLD\")\nplt.bar(left + width + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, width=width, label=\"HOBBIES\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.xticks(left, weeklabel, rotation=60)\nplt.xlabel(\"day of week\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item in each daytype\")","b144b9d6":"temp_series = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_series","02b10cd1":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values, label=\"CA\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, label=\"TX\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, label=\"WI\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each State\")\nplt.legend()","1304312d":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Store in CA\")\nplt.legend()","f34ae450":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each CA stores\")\nplt.legend()","4092172c":"temp_series = df.groupby([\"state_id\", \"store_id\", \"year\", \"month\"])[\"value\"].std()\ntemp_series","4901d6db":"fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True)\n\n# We can use for loop, of course! And that'll be better, sorry, this is for my easy trial. \nsns.lineplot(x=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"month\"), \n             y=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].values, \n             hue=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[0])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[1])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"store_id\"),\n             ax=axs[2])\n\n\n\nplt.legend(bbox_to_anchor=(1.01, 1.01))\naxs[0].set_title(\"CA\")\naxs[0].set_xticks(range(1, 13))\naxs[0].set_ylabel(\"Standard deviation of sold items in one month\")\naxs[1].set_title(\"TX\")\naxs[1].set_xticks(range(1, 13))\naxs[2].set_title(\"WI\")\naxs[2].set_xticks(range(1, 13))\n\nfig.suptitle(\"Standard deviation of sold items in one month in each store\")","5244dacb":"# \uc6d0\ubb38 \ucee4\ub110\uc5d0\ub294 \uc5c6\uc5b4\uc9d0\ntemp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].std()\ntemp_series","35490492":"# \uc6d0\ubb38 \ucee4\ub110\uc5d0\ub294 \uc5c6\uc5b4\uc9d0\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"Standard deviation of sold items\")\nplt.title(\"Standard deviation of sold items in CA stores\")\nplt.legend()","3175e2cd":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total item sold in each WI stores\")\nplt.legend()","2ae80a14":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each WI stores\")\nplt.legend()","8ce7fb97":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total sold item per day\")\nplt.title(\"Total item sold in each TX stores\")\nplt.legend()","00e1f6b3":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total item entries\")\nplt.title(\"Total item entries in each TX stores\")\nplt.legend()","8dabdd17":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\ntemp_series","5bf02da5":"# Find the day when items are sold less than 1000 of each store\n# Let's take a look at TX_2 for example\ntemp_series.loc[(temp_series.values < 1000) & (temp_series.index.get_level_values(\"date\") <= \"2016-04-22\")].loc[\"TX_2\"]","5bca7e5e":"# Find the day when items are sold most of each store\ntemp_series.groupby([\"store_id\"]).idxmax()","d33b63b9":"temp_series = temp_series.reset_index()\ntemp_series","70a15848":"plt.plot(temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"date\"],\n         temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"value\"])\nplt.xticks(rotation=60)\nplt.ylabel(\"# of sold items\")\nplt.xlabel(\"date\")\nplt.title(\"Item sold transition around its most sold day in CA_1 store\")","8ae341c7":"import statsmodels.api as sm\nimport scipy","1514ebde":"# Analysis target item is the most sold one.\n# In this Dynamic Factor Analysis Session, we'll try to find the hidden factor of this item sales transition among states.\nitem_id = \"FOODS_3_090\"\nstate_list = [\"CA\", \"TX\", \"WI\"]","e658ff78":"# First, we extract the target item sold sum in each state\ntemp_series = df[(df.date >= \"2015-01-01\") & (df.date <= \"2016-01-01\") & (df.item_id == item_id)].groupby([\"date\", \"state_id\"])[\"value\"].sum()\n\n# Then convert it to dataframe type.\ntemp_df = pd.concat([pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values),\n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values), \n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values)], axis=1) \ntemp_df.columns = state_list\ntemp_df.index = temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\")\ntemp_df.head()","d91a13c2":"# Next Step, we create diff column, which shows the difference of today and tommorow's sold count.  \n# For flattening, we apply log function and standardization\n\ndiff_cols = [\"diff_\" + state for state in state_list]      # diff columns (row data)\nstd_cols = [\"std_diff_\" + state for state in state_list]   # diff columns (after standardization)\n\nfor state in state_list:\n    col = \"diff_\" + state\n    temp_df[col] = np.log(temp_df[state] + 0.1).diff() * 100\n    temp_df[col] = temp_df[col].fillna(method=\"bfill\")\n    \n    # Standardization\n    std_col = \"std_\" + col\n    temp_df[std_col] = (temp_df[col] - temp_df[col].mean()) \/ temp_df[col].std()","3ffe7a0c":"# Conveert it to Z-value\ntemp_df[std_cols] = temp_df[std_cols].apply(scipy.stats.zscore, axis=0)","cd628da4":"temp_df","1156bdb4":"# Create the model\n# This time, for simplicity, unobserved factor (k_factors) is 1, and factor_order is 1 (i.e. it follows an AR(1) process) , \n# error_order is 1 (i.e. error has order 1 autocorelated), \n\nendog = temp_df.loc[:, std_cols]\nmod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=1, error_order=1)\ninitial_res = mod.fit(method='powell', disp=False)\nres = mod.fit(initial_res.params, disp=False)","8a6eda25":"print(res.summary(separate_params=False))","a76f8262":"from pandas_datareader.data import DataReader\n\nfig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=temp_df.index.min(), end=temp_df.index.max())\nylim = ax.get_ylim()\nplt.title(\"Fluctuations extracted by Factor 1\")\nplt.ylabel(\"Fluctuations\")\nplt.xlabel(\"Year-Month\")","3d5437f8":"res.coefficients_of_determination","e3f4559f":"res.plot_coefficients_of_determination();","0cada9f5":"temp_series = df.groupby([\"store_id\", \"cat_id\"])[\"value\"].sum()","9bee9551":"store_id_list_by_state = [[\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"], [\"TX_1\", \"TX_2\", \"TX_3\"], [\"WI_1\", \"WI_2\", \"WI_3\"]] ","a9673ff4":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values,\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"# of items\")\n\nfig.suptitle(\"Each category item sold in each store\")","add61772":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values \/ temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].sum(),\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"% of each category\")\n\nfig.suptitle(\"Each category item sold percentage in each store\")","aa1f7b9c":"cat_id = \"FOODS\"\n\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series = temp_series[temp_series.index.get_level_values(\"cat_id\") == cat_id]\ntemp_series","ee39acde":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]","93bc568f":"# Combine all these three figures.\ncat_list = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\ncolor_list = [\"orange\", \"green\", \"blue\"]\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\nwidth = 0.25\n\nfig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        for i, cat in enumerate(cat_list):\n            height_numerator = temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].values\n            height_denominater = height_numerator.sum()\n\n            axs[row, col].bar(x=temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].index.get_level_values(\"wday\") + width * (i-1),\n                              height=height_numerator \/ height_denominater,\n                             tick_label=weekday, color=color_list[i], width=width, label=cat)\n            axs[row, col].set_title(store_id_list_by_state[row][col])\n            axs[row, col].legend()\n            \nfig.suptitle(\"HOBBIES item sold in each store in each day\")","cda1e944":"fig, axs = plt.subplots(1, 3, sharey=True)\nfig.suptitle(\"Snap Purchase Enable Day Count of each store\")\n\nsns.countplot(x=\"snap_CA\", data =calendar_df, ax=axs[0])\nsns.countplot(x=\"snap_TX\", data =calendar_df, ax=axs[1])\nsns.countplot(x=\"snap_WI\", data =calendar_df, ax=axs[2])","ea9bec67":"temp_df = calendar_df.groupby([\"year\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","ebd5101d":"# This cell is just visuallizing the above dataframe.\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Year\")\nplt.title(\"Snap Purchase allowed day yearly transition\")","0ce826f7":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"month\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","5385e5d4":"# Just visualizing the above dataframe\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Month\")\nplt.title(\"Snap Purchase allowed day monthly trend\")","a8f4ec79":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"weekday\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","02a410be":"plt.bar(temp_df.index, temp_df.snap_CA)\nplt.xticks(rotation=60)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Day type\")\nplt.title(\"Snap Purchase allowed day weekly trend\")","6f1c7f6d":"# Make temp dataframe with necessary information\ntemp_df = df.groupby([\"date\", \"state_id\"])[[\"value\"]].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.merge(calendar_df[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]], on=\"date\")\ntemp_df","129fcdc3":"np.argmax(temp_df.groupby([\"date\", \"state_id\"])[\"value\"].sum())","e140c3cf":"temp_df = temp_df[(temp_df.date >= \"2016-02-15\") & (temp_df.date <= \"2016-03-25\") & (temp_df.state_id == \"CA\")]\ntemp_df","f70b5bcf":"fig, ax1 = plt.subplots()\nplt.xticks(rotation=60)\nax1.plot(\"date\", \"value\", data=temp_df[temp_df.state_id == \"CA\"])\nax2 = ax1.twinx()  \nax2.scatter(\"date\", \"snap_CA\", data=temp_df[temp_df.state_id == \"CA\"])","faf5e76d":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"])\nplt.xticks(rotation=90)\nplt.title(\"Event Type Count in event name 1 column\")","5352453d":"# Let's check the distribution of snap purchase day and event day\n# Accirding to the graph, Snap CA is allowed especially when sport event occurs.\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"], hue=\"snap_CA\")\nplt.xticks(rotation=90)\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Snap Purchse allowed day Count in each event category\")","bffa8ef9":"temp_series = df.groupby([\"cat_id\", \"event_type_1\"])[\"value\"].mean()\ntemp_series","36523ff8":"plt.bar(x=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"event_type_1\"), \n        height=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values)\nplt.title(\"HOBBIES Item Sold mean in each event type\")\nplt.ylabel(\"Item sold mean\")\nplt.xlabel(\"Event Type\")","12988c61":"# find out most sold item for example\ndf[df[\"value\"] == df[\"value\"].max()]","d456c805":"target_id = \"FOODS_3_090_CA_3_validation\"\ntemp_df = df[df[\"id\"] == target_id]\ntemp_df","8bdb978f":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\n\n# Create one hot weekday column from wday column to calculate correlation later. \nfor idx, val in enumerate(weekday):\n    temp_df.loc[:, val] = (temp_df[\"wday\"] == idx + 1).astype(\"int8\")\n\ntemp_df\n# sns.heatmap(temp_df[[\"value\", \"snap_CA\", ]].corr(), annot=True)","714e926a":"# Create Event Flag (Any events occur: 1, otherwise: 0)\n# Create Each Event Type Flag\ntemp_df.loc[:, \"is_event_day\"] = (temp_df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ntemp_df.loc[:, \"is_sport_event\"] = (temp_df[\"event_type_1\"] == \"Sporting\").astype(\"int8\")\ntemp_df.loc[:, \"is_cultural_event\"] = (temp_df[\"event_type_1\"] == \"Cultural\").astype(\"int8\")\ntemp_df.loc[:, \"is_national_event\"] = (temp_df[\"event_type_1\"] == \"National\").astype(\"int8\")\ntemp_df.loc[:, \"is_religious_event\"] = (temp_df[\"event_type_1\"] == \"Religious\").astype(\"int8\")\n\ntemp_df.head()","c023c66d":"# Plot Heatmap with these columns made in previous cells\nplt.figure(figsize=(14, 10))\nsns.heatmap(temp_df[[\"value\", \"sell_price\", \"snap_CA\", \"is_event_day\", \"is_sport_event\", \"is_cultural_event\", \"is_national_event\", \"is_religious_event\"] + weekday].corr(), annot=True)\nplt.title(\"Heatmap with values, snap_CA,  event_flag and weekday columns\")","15040618":"df.groupby(\"cat_id\")[\"sell_price\"].mean()","0e6b5cc5":"df.groupby(\"cat_id\")[\"sell_price\"].describe()","f7c2ec7d":"sns.boxplot(data=df, x=\"cat_id\", y='sell_price')\nplt.title(\"Boxplot of sell prices in each category\")","f8e07d0a":"plt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x=\"cat_id\", y='sell_price', hue=\"store_id\")\nplt.title(\"Boxplot of sell prices in each store\")","75580e66":"# One Item Sell Price Transition\nsns.lineplot(data=df[df[\"item_id\"] == \"FOODS_3_090\"], x='date', y='sell_price', hue=\"store_id\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Sell price change of 'FOODS_3_090' in each store\")","f2cf728e":"df[\"is_event_day\"] = (df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ndf.head()","25b563dc":"sns.heatmap(df[df[\"item_id\"] == \"FOODS_3_090\"][[\"value\", \"sell_price\", \"is_event_day\"]].corr(), annot=True)\nplt.title(\"Heatmap of value, sell_price and event flag\")","1d033add":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"sell_price\"].mean()\ntemp_df","c2540573":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Mean price\")\nplt.title(\"Mean price transition of each category\")","752a11a5":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"item_id\"].count()","ec24c87f":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Registered Item Counts\")\nplt.title(\"Registered Item Counts Transition in each category\")","d4373fe0":"sns.jointplot(df[\"value\"], df[\"sell_price\"])","74ab514c":"df[\"sell_price_diff\"] = df.groupby(\"id\")[\"sell_price\"].transform(lambda x: x - x.mean()).astype(\"float32\")","4e394ae9":"sns.lineplot(df[df[\"item_id\"] == \"FOODS_3_090\"][\"date\"],df[df[\"item_id\"] == \"FOODS_3_090\"][\"sell_price_diff\"], hue=df[\"store_id\"]) \nplt.legend(bbox_to_anchor=(1.01, 1.01))","29ca83fa":"temp_df = df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"value\"].sum()","24475357":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"FOODS_3_090\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"FOODS_3_090 sold number and price difference from mean\")","c4b4b81a":"# Find the most sold items in  HOBBIES section\nnp.argmax(df[df[\"cat_id\"] == \"HOBBIES\"].groupby(\"item_id\")[\"value\"].sum())","4d823394":"temp_df = df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"value\"].sum()","a7096dee":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"HOBBIES_1_371\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"HOBBIES_1_371 sold number and price difference from mean\")","1eca328f":"temp_series = df.groupby([\"date\", \"item_id\"])[\"value\"].sum()\ntemp_series","74bf3c17":"# Find Top 12 items that the mean of sold counts in all stores is high\nhigh_sold_item_top12 = temp_series.groupby(\"item_id\").mean().sort_values(ascending=False)[:12].index\nhigh_sold_item_top12","35592e86":"fig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True, sharex=True) \n\nfor row in range(3):\n    for col in range(4):\n        target_item = high_sold_item_top12[row*4+col]\n        \n        axs[row, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n        axs[row, col].set_title(target_item)\n        axs[row, col].set_ylabel(\"# of sold items in all stores\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n        axs[row, col].legend()\n            \nfig.suptitle(\"Top 12 item sold in all stores in each day\")","816ba463":"df.head()","42961b0f":"# Take a closer look,\nsns.violinplot(x = df.loc[df[\"year\"] == 2015, \"quarter\"], y = df.loc[df[\"year\"] == 2015, \"value\"])\nplt.ylim(0, 10)","3129044c":"temp_df = df.groupby([\"date\", \"cat_id\", \"dept_id\"])[\"value\", \"sell_price\"].mean()","198713ea":"fig, axs = plt.subplots(2, 3, figsize=(14, 10)) \n\n\nfor col in range(3):\n    target_cat = cat_list[col]\n    sns.scatterplot(\"value\", \"sell_price\", hue=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat].index.get_level_values(\"dept_id\") ,\n                data=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat], ax=axs[0, col])\n#     axs[0, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n    axs[0, col].set_title(f\"{target_cat} \")\n    axs[0, col].set_ylabel(\"sell_price\")\n    axs[0, col].set_xlabel(\"value\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n    axs[0, col].legend()\n    \n    temp_series = df[df[\"cat_id\"] == target_cat].groupby([\"date\", \"dept_id\"])[\"item_id\"].count()\n    sns.lineplot(x=temp_series.index.get_level_values(\"date\"), y=temp_series.values, hue=temp_series.index.get_level_values(\"dept_id\"), ax=axs[1, col])\n    axs[1, col].set_title(f\"{target_cat}\")\n    axs[1, col].set_ylabel(\"count\")\n    axs[1, col].legend()\n            \nfig.suptitle(\"Daily Average value - price plot and item count transition in each dept.\")","405d477f":"df[\"num_of_next_week_event\"] = df.groupby(\"id\")[\"is_event_day\"].transform(lambda x: x.shift(-7).rolling(7).sum().fillna(0)).astype(\"int8\")","79f4a4ad":"df[df.id == \"HOBBIES_1_008_CA_1_validation\"][25:50]","13f68243":"fig, axs = plt.subplots(1, 3, figsize=(8, 6), sharey=True)\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOBBIES\"], color=\"green\",ax=axs[0])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOUSEHOLD\"], color=\"orange\", ax=axs[1])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"FOODS\"], ax=axs[2])\naxs[0].set_title(\"HOBBIES\")\naxs[1].set_title(\"HOUSEHOLD\")\naxs[2].set_title(\"FOODS\")\n\nfig.suptitle(\"Relationship between next week events count and sold item counts\")","6197e5d5":"df[\"lag_1\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(1)).astype(\"float16\")\ndf[\"lag_7\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(7)).astype(\"float16\")","961b134b":"# plt.figure(figsize=(8, 8))\n# sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\"]], hue=\"cat_id\")","28cfd018":"sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\", \"lag_7\"]], hue=\"cat_id\")","4c22ff30":"!pip install calmap","22954cb7":"import calmap\n\ntemp_df = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.set_index(\"date\")\ntemp_df","c447bbc9":"fig, axs = plt.subplots(3, 1, figsize=(10, 10))\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"CA\", \"value\"], year=2015, ax=axs[0])\naxs[0].set_title(\"CA\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"TX\", \"value\"], year=2015, ax=axs[1])\naxs[1].set_title(\"TX\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"WI\", \"value\"], year=2015, ax=axs[2])\naxs[2].set_title(\"WI\")","167a8683":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder","4dfa258f":"# First, we try store clustering by using weekly total sales \ntemp_df = df.groupby([\"store_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","2b3b8b18":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"store_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","1798e751":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","fe1bf2e6":"pca.fit(temp_df_wide)","dd3ea551":" pca.explained_variance_ratio_","7fab010d":"result = pca.transform(temp_df_wide)\nresult","3f72f3fc":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","16a6c023":"ax = result_df.plot(kind='scatter', x='PC2', y='PC1', figsize=(12, 6))\n\nfor idx, store_id in enumerate(result_df.index):\n    ax.annotate(  \n        store_id,\n       (result_df.iloc[idx].PC2, result_df.iloc[idx].PC1)\n    )\n\nax.set_title(\"PCA result of all shops\")","bb0160d7":"temp_df = df.groupby([\"item_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","1d115c3b":"temp_df = temp_df.fillna(method=\"bfill\")","fc053f4b":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"item_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","f08b75d6":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","765afbae":"pca.fit(temp_df_wide)","8b27ad8a":"pca.explained_variance_ratio_","ac1488dd":"result = pca.transform(temp_df_wide)\nresult","14f20ac3":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","6f4abbe9":"result_df.index.str[:5]","b6b16496":"plt.figure(figsize=(8, 6))\nsns.scatterplot(x=\"PC2\", y=\"PC1\", data=result_df, hue=result_df.index.str[:5])\nplt.title(\"PCA result for item_id column\")","c1f4ebc9":"(20200528 \uc2dc\uc791)","355eb2f5":"\ub098\ub294 \uba87\uba87\uc758 \ubb38\ud654\uc801\uc774\uac70\ub098 \uc2a4\ud3ec\uce20 \ud589\uc0ac\uac00 \uc77c\uc5b4\ub0ac\uc744 \ub54c, HOBBIES \uc0c1\ud488\uc774 \ub354 \uc798 \ud314\ub9ac\ub294 \uacbd\ud5a5\uc774 \uc788\uc9c0 \uc54a\uc744\uae4c \uc0dd\uac01\ud588\ub2e4.  \n\uadf8\ub7ec\ub098, \uc774 plot\uc740 \uc774\ub7f0 \uac00\uc124\uc774 \ubd84\uba85\ud558\ub2e4\uace0 \uc758\ubbf8\ud558\uc9c0 \uc54a\ub294\ub2e4.\n\n<!--\nI thought when some cultual or sporting event occurs, HOBBIES item are more likely to be sold.  \nHowever, this plot doesn't mean this hypothesis clearly.\n-->","7abcf7e0":"# Store Analysis","74aa4517":"\uc774\uc81c \uc6b0\ub9ac\ub294 \uc0c1\ud488 \uce74\ud14c\uace0\ub9ac\uc5d0 \uac19\uc740 \uac83\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0 \uc0c1\ud488\uc744 clustering \ud574\ubd05\uc2dc\ub2e4.\n\n<!--\nNow we can do the same thing in item category.  \nAnd let's try item clustering.\n-->","bc9bdc28":"## Sell Price Analysis","796cc1bb":"Dark Red days \ub294 \uadf8 \ub0a0\uc5d0 \ud310\ub9e4\uac00 \ud070 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n\ubc18\ub300\ub85c, white days\ub294 \uadf8 \ub0a0\uc5d0 \ud310\ub9e4\uac00 \uc801\uc740 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n(\uc5ec\ub7ec\ubd84\uc740 \ud06c\ub9ac\uc2a4\ub9c8\uc2a4 \ub54c plot\uc774 \ud558\uc580 cell\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.)\n\n<!--\nDark Red days mean the sales on that day is large.  \nOn the contrary, white days mean the sales on that day is low.  \n(You can see on Christmas day the plot has white cells.)\n-->","3467b2b4":"\uc774\ub7f0 \ubd84\uc11d\uc744 \ud1b5\ud574, \uc6b0\ub9ac\ub294 \ubd80\uc11c\uc5d0 \ub300\ud55c \ub2e4\uc74c\uc758 \uac83\ub4e4\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n**Common**\n- \uac01 \uce74\ud14c\uace0\ub9ac\uc758 \ubd80\uc11c\ub294 \uac01 \ubd80\uc11c\uc758 \ud3c9\uade0 \ud310\ub9e4 \uac00\uaca9\uc5d0 \uc758\ud574 \ubd84\ub958\ub418\ub294 \uac83 \uac19\ub2e4.\n\n**FOODS**\n- dept_3 \uc740 \ub2e4\ub978 \uac83\uacfc \ube44\uad50\ud574\uc11c \ub354 \ub0ae\uc740 \uac00\uaca9\uacfc \ub192\uc740 \uc6a9\ub7c9\uc758 plot \uc744 \uac00\uc9c0\uace0 \uc788\ub2e4.\n- \ucd94\uac00\ub85c, dept_3\uc740 \ub2e4\ub978 \ubd80\uc11c\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uc0c1\ud488\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n**HOBIES**\n- dept_1\uc740 dept2 \ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uc0c1\ud488\uc744 \uac16\uace0 \uc788\ub2e4.\n\n**HOUSEHOLD**\n- \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\uc640 \ub2ec\ub9ac, \uc774 \uce74\ud14c\uace0\ub9ac\ub294 \ubd80\uc11c \ubcc4\ub85c \ubd84\uba85\ud55c \ucc28\uc774\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\n\n<!--\nFrom these analysis, we can find the following things regarding department.\n\n**Common**\n- Departments in each category are seemed to be classified by average sell price of its department.  \n\n**FOODS**\n- dept_3 has lower price and high volume plot compared to others.  \n- In addition, dept_3 has more items than other departments.\n\n**HOBBIES**\n- dept_1 has more items than dept_2\n\n**HOUSEHOLD**\n- Unlike other categories, this category seemed that it doesn't have clear difference in each department.\n-->","af4cad67":"\uc774\uac83\uc740 \ubaa8\ub378\uc744 \uc801\uc6a9\ud55c \uacb0\uacfc\uc758 \uc694\uc57d\uc785\ub2c8\ub2e4.\n\uc774\uac83\uc740 \ubaa8\ub378 \uc815\ubcf4\uc640 \uba87\uba87\uc758 \ud1b5\uacc4\uc801\uc778 \uac12\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n<!--\nThis is the summary result of model fitting. \nIt describes model information and some statistical values.  \n-->","1c3a4de4":"\uc704 \uadf8\ub9bc\uc5d0\uc11c, \uac01 plot\uc740 CA\uc5d0\uc11c snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc778 \uc9c0 \uc544\ub2cc \uc9c0\ub97c \uc758\ubbf8\ud55c\ub2e4.  \n\uc5ec\ub7ec\ubd84\ub4e4\uc774 \ubcf4\ub294 \uac83\ucc98\ub7fc, snap \uad6c\ub9e4\ub294 3\uc77c \ud6c4\uc5d0 \ud558\ub8e8\ucc98\ub7fc \uaddc\uce59\uc801\uc73c\ub85c \ubd84\ud3ec\ub418\uc5b4 \uc788\uc9c0 \uc54a\ub2e4.  \n(ex. 2016-03-31 > 2016-03-04 > 2016-03-07 > ...)  \n\uc774\uac83\uc740 \uc704 \uadf8\ub7fc\ucc98\ub7fc \uc0ac\uc2e4 \ud3b8\ud5a5\ub418\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub2e4.  \n(\uc608, Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c \ub0a0\uc740 2016-03-01 \ubd80\ud130 2016-03-19\uae4c\uc9c0 \uacc4\uc18d\ub41c\ub2e4.)  \n\uadf8\ub9ac\uace0 \uc774\ub7f0 \ub0a0\uc5d0, \ud310\ub9e4\ub294 \ub610\ud55c \uc99d\uac00\ud588\ub2e4.\n\n<!--\nIn above figure, each plot means whether the day allows snap purchase or not in CA.  \nAs you can see, snap purchase enable day is not regularly distributed like one day in three consective days.  \n(ex. 2016-03-01 > 2016-03-04 > 2016-03-07 > ...)  \nIt is actually biasedly distributed like the figure above.  \n(i.e. Snap purchase Enable Day continues from 2016-03-01 to 2016-03-10)  \nAnd on these days, sales are also increased.  \n-->","6ef4cfc1":"# Snap Purchase Analysis\nsnap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc740 \uc5b4\ub5bb\uac8c \ubd84\ud3ec\ub418\uc5c8\ub294 \uc9c0 \ud655\uc778\ud558\uc790.\n\n<!--\nLet's see how snap purchase allowed day is distributed.\n-->","b3697327":"1. \uc774\ub7f0, 2015\ub144\uc5d0 \uba87\uba87\uc758 \uadf9\uc2ec\ud55c \uc9c0\uc810\uc774 \uc788\ub294 \uac83 \uac19\ub2e4.  \n   \uc608\ub97c \ub4e4\uc5b4, 2\uc6d4 \ucbe4, TX2\ub294 \ud310\ub9e4\ub41c \uc0c1\ud488\uc774 \uac70\uc758 0\uac1c\uc774\ub2e4. (\ub098\ub294 \uc774 \uc0c1\uc810\uc774 \uc608\uc678\uc801\uc73c\ub85c \ub2eb\uc558\uc744 \uac83\uc774\ub77c \uac00\uc815\ud55c\ub2e4.)  \n   \ubc18\ub300\ub85c, \uadf8 \ud574 \ud55c \uc5ec\ub984\ub0a0\uc5d0, TX_3\ub294 \uadf8\uac83\uc758 \ucd1d \ud310\ub9e4\ub7c9\uc774 \ub9e4\uc6b0 \uc99d\uac00\ud588\ub2e4.  \n\n2. TX_2\ub294 2014\ub144 \uc804\uc5d0 \ud2b9\ud788, \uac00\uc7a5 \ub9ce\uc740 \uc0c1\ud488\uc744 \ud310\ub9e4\ud588\ub2e4.\n\n<!--\n1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   In contrast, in one summer day of that year, TX_3 increased its total item sold exprosively.\n   \n2. TX_2 has most item sold especially before 2014.\n-->","11db9780":"\uac70\ub798\uac00 \uc62c\ubc14\ub978 \uc9c0 \uc544\ub2cc \uc9c0 \ud655\uc778\ud574\ubcf4\uc790.\n\n\"num_of_next_week_event\" \uceec\ub7fc\uc744 \ubcf4\uba74, \uc704 dataframe\uc5d0 \uc788\ub294 0\uc774 \uc544\ub2cc \uccab \ubc88\uc9f8 \uc9c0\uc810\uc774 \"2011-03-02\" (\uc218\uc694\uc77c) \uc774\ub2e4.  \n\uc6b0\ub9ac\uac00 \ub2e4\uc74c \uc8fc \ud589\uc0ac\ub97c \uc54c\uace0 \uc2f6\uae30 \ub54c\ubb38\uc5d0, \uc774\uac83\uc740 \"2011-03-09\" (\ub2e4\uc74c \uc8fc \uc218\uc694\uc77c)\uc5d0 \uba87\uba87\uc758 \ud589\uc0ac\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0 \uc774\uac83\uc740 \uc0ac\uc2e4\uc774\ub2e4.  \n\"2011-03-10\" \uacfc \"2011-03-15\" \uc0ac\uc774\uc5d0, \uc6b0\ub9ac\ub294 \uc5b4\ub5a4 \ub354 \ub9ce\uc740 \ud589\uc0ac\uac00 \uc788\uc9c0 \uc54a\ub2e4. \uadf8\ub798\uc11c num_of_next_week_event \uceec\ub7fc\uc740 \"2011-03-03\" \uacfc \"2011-03-08\" \uc740 1\uc774 \ub3fc\uc57c \ud55c\ub2e4.\n(\uc774\uac83\uc740 \uc6b0\ub9ac\uac00 \uc624\uc9c1 \"2011-03-09\" \ub9cc \uc138\uae30 \ub54c\ubb38\uc774\ub2e4.)\n\uadf8\ub9ac\uace0 \"2011-03-16\" \ud589\uc0ac\uc5d0 \uc0c1\uc751\ud558\ub294 next_week_event_sum \uceec\ub7fc\uc740 \"2011-03-09\" \uc5d0 1\uc744 \uac16\ub294\ub2e4.  \n\ub2e4\uc74c \ub0a0\uc5d0, \uc6b0\ub9ac\ub294 \ub610\ud55c \ud589\uc0ac\ub97c \uac00\uc9c0\uae30 \ub54c\ubb38\uc5d0, \uc774\uc5d0 \uc0c1\uc751\ud558\ub294, next_week_event_sum \uceec\ub7fc\uc740 \"2011-03-10\"\uc5d0 2\ub97c \uac16\ub294\ub2e4.  \n\n\uc774 \uc124\uba85\uc774 \uc5ec\ub7ec\ubd84\uc740 \uc774\ud574\uac00 \ub418\ub098\uc694?\n\n\uc774\ub7f0 \uc2dd\uc73c\ub85c, \uc6b0\ub9ac\ub294 next_week \ud589\uc0ac feature\ub97c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74, \uc6b0\ub9ac\ub294 \uc885\uc885 \uadfc \ubbf8\ub798\uc758 \ud070 \ud589\uc0ac\uac00 \uc788\uc744 \ub54c \uc1fc\ud551\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.  \n\uc774\uc81c \uc774 \uceec\ub7fc\uc774 \uc815\ub9d0\ub85c \uae0d\uc815\uc801\uc778 \uad00\uacc4\uac00 \uc788\ub294 \uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4.\n\n<!--\nLet's check whether the trainsaction is right.  \n\nLooking at \"num_of_next_week_event\" column, the first point of non-zero in the above dataframe is on \"2011-03-02\" (Wednesday) .  \nSince we want to know the next week event sum, this means on \"2011-03-09\" (next Wednesday) we have some events and this is true.  \nBetween \"2011-03-10\" and \"2011-03-15\", we dont' have any more events, so the num_of_next_week_event column between \"2011-03-03\" and \"2011-03-08\" should be 1 \n(This is because we count only \"2011-03-09\" day)  \nAnd corresponding to the event on \"2011-03-16\", next_week_event_sum column gets 1 on \"2011-03-09'.  \nNext day, we also have events, so corresponding to that, next_week_event_sum column gets 2 from \"2011-03-10\".  \n\nDoes this explanation make sense to you?\n\nLike this way, we can add feature of next_week event, since we ofen go shopping when we have big events in the near furture.  \nNow let's see whether this column has really positive relation to the value.\n-->","bea60120":"## Relationship of Lag Variables","f3f41051":"# Import Libraries and Data Input","2dbde47d":"# Data Cleaning\n\uba3c\uc800, \ubaa8\ub4e0 3\uac1c\uc758 \ub370\uc774\ud130 \ud504\ub808\uc784\uc744 \ud569\uccd0\ubd05\uc2dc\ub2e4.  \n\uc911\uc694\ud55c \uac83\uc740 \ubaa8\ub378\uc744 \ub354 \uc27d\uac8c \uc608\uce21\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130 \ud3ec\ub9f7\uc744 \ub113\uc740 \uac83\uc5d0\uc11c \uae34 \uac83\uc73c\ub85c \ubc14\uafb8\ub294 \uac83\uc785\ub2c8\ub2e4.  \n(\uc774 \ub178\ud2b8\ubd81\uc740 \uc608\uce21 \ubaa8\ub378 \uc790\uccb4\ub97c \ub2e4\ub8e8\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4.)\n\n<!--\nFirst, let's combine all three dataframe.  \nThe important thing is changing data format from wide to long to make prediction model easier  \n(Though this notebook doesn't dive into predicition model itself.)\n-->\n","b7ebcd0e":"FOODS_3_090\uc758 \uacbd\uc6b0, \uac00\uaca9\uc774 \ud3c9\uade0\ubcf4\ub2e4 \ub0ae\uc544\uc84c\uc744 \ub54c, \ud310\ub9e4 \uc218\ub294 \ub354 \ub192\uc558\ub2e4.  \n\ud2b9\ud788, 2013\uc758 \ud558\ubc18\uae30\uc758 \uc0c1\ud488\uc758 \ud310\ub9e4 \uc218\ub294 \ud3c9\uc18c\ubcf4\ub2e4 \uc57d 2.5\ubc30 \ub354 \ub192\uc558\ub2e4. \uadf8\ub9ac\uace0 \uc774\uac83\uc740 \uc774 \uc0c1\ud488\uc774 \ud3c9\uc18c\ubcf4\ub2e4 \ub354 \ub0ae\uc740 \uac00\uaca9\uc77c \ub54c\uc758 \uae30\uac04\uc774\ub2e4.  \n\uc774\uac83\uc740 \uc774 \uc0c1\ud488\uc774 \uac19\uc740 \uc9c0\uc810\uc5d0 \uc5f0\uc18d\uc801\uc778 \ub0a0\ub4e4\uc5d0 \ud310\ub9e4\uac00 \uc5c6\ub358 \uac83 \uac19\ub2e4.  \n\uc774\uac83\uc740 \ubaa8\ub378\uc744 \ub9cc\ub4e4 \ub54c \ub178\uc774\uc988\uac00 \ub420 \uc218 \uc788\ub2e4.  \n\n\ub2e4\ub978 \uc0c1\ud488\uc744 \ubcf4\uc790.\n\n<!--\nAs for FOODS_3_090, when the price gets lower than its mean, the number of sold gets higher. \nEspecially, the latter half of 2013, # of sold items gets higher about 2.5 times than usual, and this is the period when this item had lower price than usual. \nIt seems that this item has no sell for some consectuive days in some points.  \nIt can be noise when making models.\n\n\nLet's see some other items.\n-->","70004805":"# Acknowledgment\n\nI apologize that my english are somewhat wrong and my codes are not so beautiful one like others' codes.  \nHowever, I tried hard to make simle codes as much as I can especially for beginners like me to learn how to use matplotlib and seaborn to do data visualization.  \nAny comments or upvotes can be my very strong motivation towards much harder work! Thank you!","083250da":"### Standard deviation analysis in each store\n\uc774\uc81c \uac01 \uc0c1\uc810\uc758 \ud45c\uc900 \ud3b8\ucc28\ub97c \ud655\uc778\ud574\ubcf4\uc790.  \n\uc774\uc804 \uc139\uc158\uc5d0\uc11c, \uc6b0\ub9ac\ub294 CA_3\uc774 CA\uc758 \ub2e4\ub978 \uc0c1\uc810\ubcf4\ub2e4 \uc57d\uac04 \ub354 \ub9ce\uc740 \ud310\ub9e4\ub97c \uac00\uc9c4\ub2e4\ub294 \uac83\uc744 \uc54c\uc544\ub0c8\ub2e4.  \n\uc774 \uc0c1\uc810\ub4e4\uc758 \ud3b8\ucc28\ub294 \uc5b4\ub5a8\uae4c?\n\n\n<!--\n### Standard deviation analysis in each store\nNow let's check each store's standard deviation.  \nIn the previous section, we found that CA_3 has a little bit more sales than other stores in CA.  \nHow about deviation of these stores?\n-->","093af062":"\uadf8 \ud574 \ub3d9\uc548, \ud55c \ub2ec\ub3d9\uc548, 10\uc77c\uc758 snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\ub4e4\uc774 \uc788\uc5c8\ub2e4.  \n\uc774 \uacbd\ud5a5\uc740 2012\ub144\ubd80\ud130 2015\ub144\uae4c\uc9c0 \uac19\ub2e4.  \n(2011\ub144\uc5d0 1\uc6d4\uc5d0\ub294 snap \ub0a0\uc774 \uc5c6\ub2e4.)\n\n<!--\nThrough the year, we have 10 snap purchase allowed days in one month.  \nThis tendency is the same from 2012 to 2015.  \n(In 2011, no snap days in January)\n-->","2691aa63":"(20200528 \uac31\uc2e0)","3540cb2c":"## One Item Features Analysis","4710c208":"# Event Pattern Analysis\nevent_name_1 \uceec\ub7fc\uc5d0\uc11c \ud589\uc0ac \ud328\ud134\uc744 \ud655\uc778\ud558\uc790.  \n(event_name_2 \uceec\ub7fc\uc758 \uacbd\uc6b0, event_name_1 \uceec\ub7fc\uacfc \ube44\uad50\ud558\uba74 null\uc774 \uc544\ub2cc \uac12\ub4e4\uc774 \ud6e8\uc52c \uc801\ub2e4.)\n\n<!--\nLet's check event pattern in event_name_1 column.  \n(As for event_name_2 column, there are much less non-null values compeared to event_name_1 column.\n-->","3c6b177c":"1. CA \uc5d0\uc11c\uc758 3\uac1c\uc758 \uc0c1\uc810\uc740 \uc0c1\ud488\uc758 \ud310\ub9e4 \uae30\ub85d\uc774 \ube44\uc2b7\ud558\ub2e4.\n   CA_3\uc740 \ub2e4\ub978 \uc0c1\uc810\uacfc \ube44\uad50\ud574\uc11c \uc57d\uac04 \ub354 \ub9ce\uc740 \uc591\uc744 \ud310\ub9e4\ud588\ub2e4.\n\n2. \uac01\uac01\uc758 \uc0c1\uc810\uc758 \ud45c\uc900 \ud3b8\ucc28\ub294 \ub2e4\ub978 \uac83 \uac19\ub2e4. \ub098\uc911\uc5d0 \ud655\uc778\ud574\ubcf4\uc790.\n\n3. 2015\ub144 \ubd04\/\uc5ec\ub984\ucbe4\ubd80\ud130, CA_2\uc758 \ud310\ub9e4 \uae30\ub85d\uc774 \ube60\ub974\uac8c \uc99d\uac00\ud588\ub2e4. \uc6b0\ub9ac\ub294 \uc774\uc720\ub97c \uc870\uc0ac\ud574\uc57c\ud55c\ub2e4.\n\n<!--\n1. Three stores in CA have similar amount of item sold record.  \n   CA_3 has more item sold a little bit compared to others.  \n\n2. The standard deviation of each store seems different, confirm it later.\n\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n-->","f4e2b2bc":"\ud06c\ub9ac\uc2a4\ub9c8\uc2a4\uc5d0 \ud310\ub9e4\ub41c \ud488\ubaa9\uc774 0\ucc98\ub7fc \ubcf4\uc778\ub2e4. *.loc* \ud568\uc218\ub85c \ud655\uc778\ud574\ubcf4\uc790.\n\n<!--\nOn Christmas Day, the items sold are seemed to be 0, let's check it with *.loc* method\n-->","eeb5ab8f":"*.decribe* \ud568\uc218\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c, mean = NaN\uc778 \uc774\uc720\ub97c \uc774\ud574\ud558\uc9c0 \ubabb \ud588\ub2e4. \uadf8\ub7ec\ub098 *.mean()* \ud568\uc218\ub294 \uce74\ud14c\uace0\ub9ac\uc758 \ud310\ub9e4 \uac00\uaca9 \ud3c9\uade0\uc744 \uc815\ud655\ud788 \uacc4\uc0b0\ud55c\ub2e4.  \n\uc5ec\ub7ec \ubc29\uc2dd\uc73c\ub85c \uadf8\ub824\ubcf4\uc790.\n\n<!--\nI don't understand why mean = NaN when using *.describe* method, however, *.mean()* method accurately calculate category's sell price mean.  \nLet's plot it with some ways!\n-->","eb938c1f":"\uc9c0\uae08\ubd80\ud130, PCA\ub97c \uc0ac\uc6a9\ud574\uc11c \uc0c1\uc810 \uc0ac\uc774\uc5d0 \uc720\uc0ac\ud568\uc744 \uc54c\uc544\ubd05\uc2dc\ub2e4!\n\n<!--\nFrom now on, let's find out the similarities among stores by using PCA!\n-->","ddb49cea":"\uc6b0\ub9ac\ub294 \ub2e4\uc74c\uc758 \uac83\ub4e4\uc744 \uc54c \uc218 \uc788\ub2e4.\n1. value\uc640 \ub2e4\ub978 \uc5f4\ub4e4\uc758 \uc0c1\uad00\uad00\uacc4\uc5d0 \uad00\ud574:\n   - snap \uad6c\ub9e4\uc640 \ub2e4\ub978 \ud589\uc0ac flag\ub294 \uac70\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uc5c6\ub2e4.\n   - \ud1a0\uc694\uc77c\uc740 value\uc5d0 \uac00\uc7a5 \ub9ce\uc740 \uae0d\uc815\uc801 \uc601\ud5a5\uc744 \uac16\uace0 \uc788\uace0, \ud654\uc694\uc77c\uc740 \uac00\uc7a5 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\uc744 \uac16\uace0 \uc788\ub2e4.\n     (\uc6b0\ub9ac\ub294 \uc774\uc804\uc5d0 \ud1a0\uc694\uc77c\uc774 \ud55c \uc8fc\uc5d0 \uac00\uc7a5 \ub9ce\uc740 \uc0c1\ud488\uc744 \ud310\ub9e4\ud55c \ub0a0\uc774\ub77c\uace0 \ubcf4\uc558\ub2e4. [here](#Item-Sold-in-each-day-type).)  \n     \n     \n2. snap_CA\uc640 \ud3c9\uc77c \ucee4\ub7fc\uc758 \uc0c1\uad00\uad00\uacc4\uc5d0 \uad00\ud574:\n   - \uc6b0\ub9ac\uac00 \uc774\uc804\uc5d0 \ubcf8 \uac83\ucc98\ub7fc, snap_CA\ub294 \uac01 \ub0a0\uc758 \ud0c0\uc785\uc5d0 \ub3d9\uc77c\ud558\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub2e4.\n     \ub530\ub77c\uc11c, snap_CA\uc640 \ud3c9\uc77c(\uc6d4,\ud654,\uc218 ..) \uceec\ub7fc \uc0ac\uc774\uc5d0 \uc0c1\uad00\uad00\uacc4\ub294 \uac70\uc758 0\uc774\ub2e4.  \n\n\n3. \ub098\uba38\uc9c0:\n   - \ud589\uc0ac\uc640 \uc77c\uc694\uc77c\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uba74, \uac70\uc758 0.089\uc774\ub2e4.\n     \uc800\ub294 \ub300\ubd80\ubd84\uc758 \ud589\uc0ac \uc77c\uc694\uc77c\uc5d0 \ud588\ub2e4\uace0 \uc0dd\uac01\ud588\ub2e4. \uadf8\ub7ec\ub098 \uc774\uac83\uc740 \uc81c\uac00 \uc608\uc0c1\ud55c \uac83\ucc98\ub7fc \ub9ce\uc9c0 \uc54a\uc558\ub2e4.\n   - \ud310\ub9e4 \uac00\uaca9\uc5d0 \uad00\ud574\uc11c\ub294 \uc544\ub798\ub97c \ubcf4\uc790.\n\n<!--\nWe can find the following things.\n1. Regarding value and other columns correlation:\n   - snap purchase and other events flag has little correltion.\n   - Saturday has the most positive effect on values, and Tuesday has the most negative effect.  \n     (We've previously seen Saturday is the most item sold day in one week [here](#Item-Sold-in-each-day-type).)\n     \n2. Regarding snap_CA and weekdays columns correlation:\n   - As we've previously seen, snap_CA is uniformly distributed in each day type.  \n     Thus, the correlation between snap_CA and weekdays columns (ex. Monday, Tuesday, ...) are almost 0.\n\n3. Others:\n   - Looking at event and sunday correlation,it is just 0.089.  \n     I thought most part of events oocur on Sunday, but it wasn't so much as I had exoected.\n   - Regarding sell price, we look below.\n-->","2ba17319":"## Discount Season Presumption\n\ubaa8\ub4e0 \uc0c1\ud488\uc774 Walmart\uc758 \ud3c9\uc18c\ubcf4\ub2e4 \ub354 \ub0ae\uc740 \uac00\uaca9\uc778 \uae30\uac04\uc774 \uc788\uc744\uae4c?\n\n<!--\nIs there a period when all items have lower price than usual in Walmart?\n-->","1de527ab":"\uc704 plot\uc5d0\uc11c, \uc6b0\ub9ac\ub294 FOODS \uce74\ud14c\uace0\ub9ac\ub3c4 \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac \ubcf4\ub2e4 \ub354 \ub9ce\uc740 \ubd84\uc0b0\uc744 \uac16\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\nFOODS \uce74\ud14c\uace0\ub9ac\ub294 \ub2e4\ub978 \uac83\uacfc \ube44\uad50\ud574\uc11c PC1 \uc5d0\uc11c \uba87\uba87\uc758 \ub9e4\uc6b0 \ub192\uc740 \uc810\ub4e4\uc744 \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\nFrom the above plot, we could also find FOODS category has more variance than other categories.  \nFOODS category has some extra high point in PC1 compared to others.\n-->","8bb2ca7f":"(20200528) \ucd94\uac00 \ub05d (1\uae4c\uc9c0)","5ee47315":"## Quarter Analysis","fda3d5f4":"1. \ubaa8\ub4e0 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c, \uc8fc\uae30\uc801 \ucd94\uc138\ub294 \uc8fc\ubcc4\ub85c \ubcf4\uc5ec\uc9c4\ub2e4.  \n   \uc774\uc804 \uadf8\ub798\ud504\uc5d0\uc120, \uc6b0\ub9ac\ub294 \uc27d\uac8c HOUSEHOLD\uc640 HOBBIES\uac00 \uc8fc\uac04 \ud2b9\uc131\uc744 \uc778\uc2dd\ud560 \uc218 \uc5c6\uc5c8\uc73c\ub098, \uc774 \uadf8\ub798\ud504\uc5d0\uc120\ub290 \ud560 \uc218 \uc788\ub2e4.\n   \n2. \ubaa8\ub4e0 \ud488\ubaa9\uc774 \ud310\ub9e4\ub41c \uc591\uc774 0\uc778 \ub0a0\uc740 \ud06c\ub9ac\uc2a4\ub9c8\uc2a4 \ucc98\ub7fc \ubcf4\uc778\ub2e4. \uc2e0\ub144\uc740 \uc544\ub2c8\ub77c\ub294 \uac83\uc740 \ubc11\uc5d0\uc11c \ud655\uc778\ud558\uc790.\n\n<!--\n1. In all categories, the periodical trends is seemed weekly.  \n   In previous graph, we can't easily recognize that HOUSEHOLD and HOBBIES have weekly features, but in this graph we can.\n   \n2. The day when all item sold is 0 is seemed to be Christmas Day, not new year's day, confirm it below.\n-->","1a504907":"(20200528 \ub05d)","278c5709":"1. seaborn \uc5d0\uc11c lineplot \ud568\uc218\ub85c \uc0ac\uc6a9\ud558\uba74, \uc6b0\ub9ac\ub294 \uc2e0\ub8b0\uad6c\uac04\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc5d0\ub7ec \ubc94\uc704\uc5d0 \ub300\ud55c \ud558\ub098\uc758 \uc120\uc744 \uadf8\ub9b4 \uc218 \uc788\ub2e4.  \n   \uc5ec\ub7ec\ubd84\uc740 seaborn lineplot\uc744 \ucc3e\uace0, \uc774 \ubb38\uc7a5\uc744 \ubcfc \uc218 \uc788\ub2e4. (https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html)\n   > \uae30\ubcf8\uc801\uc73c\ub85c, \uc774 plot\uc740 \uac01 x \uac12\uc5d0 \ub300\ud55c \ub9ce\uc740 y\uac12\uc744 \uc885\ud569\ud558\uace0, \uc911\uc2ec\uc758 \uacbd\ud5a5\uc758 \ud3c9\uac00\uc640 \uadf8 \ud3c9\uac00\uc5d0 \ub300\ud55c \uc2e0\ub8b0\uad6c\uac04\uc744 \ubcf4\uc5ec\uc900\ub2e4.\n   \n\uc774 \uacbd\uc6b0\uc5d0 \uc774\uac83\uc740 4\ub144 \ub3d9\uc548\uc758 \uac01 \uc6d4\ubcc4 \ud310\ub9e4 \uc0c1\ud488\uc758 \uc2e0\ub8b0\uad6c\uac04\uc5d0 \ub300\ud55c \ud45c\uc900 \ud3b8\ucc28\ub97c \uadf8\ub9b0\ub2e4. (2011\ub144\ubd80\ud130 2015\ub144\uae4c\uc9c0)\n\n\n2. CA_3\uac00 CA\uc5d0\uc11c \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 \uc0c1\uc810\uc774 \ub41c \uc774\ud6c4\ub85c, \ud45c\uc900 \ud3b8\ucc28 \ub610\ud55c \ub2e4\ub978 \uacf3\ubcf4\ub2e4 \ub192\uc544\uc84c\ub2e4.\n   seaborn\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc6b0\ub9ac\ub294 \uc774\ub7f0 \uc88b\uc740 plot\uc744 \ub9cc\ub4e4 \uc218 \uc788\ub2e4.\n\n\n\ub2e4\ub978 \uc8fc\ub97c \ud655\uc778\ud574\ubcf4\uc790. \ub2e4\uc74c\uc740 WI \uc774\ub2e4.!\n\n<!--\n1. With lineplot method in seaborn, we can plot single line with error bands showing a confidence interval.  \n   You can serach seaborn lineplot and find this sentence. (https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html)\n>  By default, the plot aggregates over multiple y values at each value of x and shows an estimate of the central tendency and a confidence interval for that estimate.  \n\n   In this case, this command plots each month item sold standard deviation with confidence interval in 4 years. (From 2011 to 2015)\n   \n\n2. Since CA_3 is the most sold store in CA, standard deviation of this store is also higher than others.  \n   By using seaborn, we can create these good plots.\n   \n \nLet's check other state, WI next!\n-->","20809458":"\uc8fc\uac04 \uacbd\ud5a5\uc5d0 \uc598\uae30\ud55c \uac83\ucc98\ub7fc, \uc6b0\ub9ac\ub294 \ud3b8\ud5a5\ub41c \ubd84\ud3ec\ub97c \ucc3e\uc744 \uc218 \uc5c6\ub2e4.  \n\uc5f0\uac04 \ucd1d\ud569\uacfc \uc6d4\uac04 \ucd1d\ud569\ucc98\ub7fc \uac70\uc758 \ub3d9\uc77c\ud558\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub2e4.\n\n\n<!--\nRegarding weekly trend, we can find no biased distribution.  \nThis is also almost uniformly distributed like year total and month total.\n-->","c5cca36e":"\uc6b0\ub9ac\uac00 \uc774\ubbf8 \uc704\uc5d0\uc11c \ubcf8 \uac83\ucc98\ub7fc, \ub4f1\ub85d\ub41c \uc0c1\ud488\uc758 \uc218\uc758 \ucd94\uc138\ub294 \uac01 \uc0c1\uc810\uc5d0\uc11c\uc640\ub294 \ub2e4\ub974\ub2e4.  \nWI_2\ub294 2012\ub144 \uc5ec\ub984\ucbe4\uc5d0 \uc0c1\ud488 \ub4f1\ub85d\uc774 \uc99d\uac00\ud588\uace0, WI_3\ub294 \uadf8 \ud574 11\uc6d4 \ucbe4\uc5d0 \uc99d\uac00\ud588\ub2e4.\n\n2013\ub144 \ubd80\ud130, \ubaa8\ub4e0 \uc0c1\uc810\uc740 \ube44\uc2b7\ud55c \ucd94\uc138\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4. \ub2e4\uc74c\uc740, TX \uc8fc\uc5d0\uc11c\uc758 \uc0c1\uc810\uc774\ub2e4!\n\n<!--\nAs we've already seen above, the registered item count trends are different in each store.  \nWI_2 increased its item register around summer in 2012, then WI_3 increased around November of that year.  \n\nFrom 2013, all stores have similar trend.  Next, stores in TX states!\n-->","38611360":"# Item Sold relation Analysis\n\uc9c4\ud589 \uc911...  \n(\uc800\ub294 Dynamic Factor Analysis \ub97c \uc801\uc6a9\ud558\uace0 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\ub824\uace0 \ub178\ub825\ud588\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc720\uc775\ud55c \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc5c6\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4:\nhttps:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html\n\n\ub354 \uc0c1\uc138\ud558\uac8c \uc774 \ubc29\ubc95\uc744 \uc774\ud574\ud560 \uc218 \uc788\ub294 \ub204\uad6c\ub4e0\uc9c0, \ub313\uae00\uc744 \ub0a8\uaca8\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n<!--\nUnder Construction...  \n(I tried to apply Dynamic Factor Analysis and execute the codes of this tutorial, but it seems I couldn't get informative outcome:\nhttps:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html\n\nTo Whoever can understand this method more specifically, I appreciate your comments.\n-->","cbc027ba":"# References\nFollowing notebooks are the great notebooks in this competition. \nFor whom hasn't check these notebooks, I strongly recommend you to take a look at these notebooks.\n(I'm sorry if I missed some other great kernels, I'll take a lookt at other notebooks if I have enough time.)\n\nData Visualization:\n\n- **M5 Forecasting - Starter Data Exploration**  \n  https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration  \n\n-  **Back to (predict) the future - Interactive M5 EDA**  \n   https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda\n\nMaking Prediction:\n\n- **M5 - Three shades of Dark: Darker magic**  \n  https:\/\/www.kaggle.com\/kyakovlev\/m5-three-shades-of-dark-darker-magic\n\n-  **Very fst Model**  \n   https:\/\/www.kaggle.com\/ragnar123\/very-fst-model","266708b4":"\uc0ac\uc2e4, \uc6b0\ub9ac\ub294 \ub2e4\uc74c \uc8fc \ud589\uc0ac\uc640 \uc774\ubc88 \uc8fc \ud310\ub9e4 \uc218 \uc0ac\uc774\uc5d0 \ubd84\uba85\ud55c \uad00\uacc4\ub97c \uc54c \uc218 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4.  \n\uc800\ub294 \ub9cc\uc57d \ub2e4\uc74c \uc8fc\uc5d0 \ub9ce\uc740 \ud589\uc0ac\uac00 \uc788\ub2e4\uba74 \ud310\ub9e4\uac00 \uc99d\uac00\ud558\uae30\ub97c \uae30\ub300\ud588\uc2b5\ub2c8\ub2e4.  \n\uc6b0\ub9ac\ub294 \uc774 \ubd80\ubd84\uc5d0 \ub300\ud574 \ub354 \ub9ce\uc740 \ubd84\uc11d\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.  \n\n\uc774\uc81c\uae4c\uc9c0, \uc6b0\ub9ac\ub294 \uc885\uc885 subplot\uc744 \uc0ac\uc6a9\ud558\uace0 \ub2e4\ub978 plot\uc778 \uac83 \ucc98\ub7fc \uac01 \uce74\ud14c\uace0\ub9ac\ub97c \ube90\uc2b5\ub2c8\ub2e4.  \nseaborn\uc758 \ub9ce\uc740 \ud568\uc218\ub4e4\uc740 \uac70\ub300\ud55c \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uace0, \uc774\uac83\uc740 \ud558\ub098\uc758 plot\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \uce74\ud14c\uace0\ub9ac\ub97c \ub2e4\ub978 \uc0c9\uc73c\ub85c \ube84 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n\uc774\uac83\uc740 \ub9e4\uc6b0 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub2c8 \uc774\ubc88\uc5d0\ub294, \uc774\uac83\uc740 \uc885\uc885 \uba54\ubaa8\ub9ac \uc5d0\ub7ec\ub97c \uc77c\uc73c\ud0a4\uace0, \ucee4\ub110\uc744 \uc8fd\uac8c \ub9cc\ub4ed\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc774\ubc88\uc5d0\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n<!--\nActually, we couldn't see the clear relationship between next week event and this week's sold count.  \nI had expected that the sales would increase if there are many events in the next week. \nWe need more analysis regarding this point.\n\nSo far, we often use subplots and drop each category as a different plot.  \nMany methods of Seaborn have hue attribute and this can drop all categories in one plot with different color.  \nIt is very useful, but this time, it often causes Memory Error and makes kernel die, so this time I didn't use it.\n-->","9c4d15ca":"> \uba54\ubaa8\ub9ac \uc0ac\uc6a9 \uac10\uc18c\ub97c \uc704\ud574\n>\n> calendar \uad00\ub828 \uc22b\uc790 \ub370\uc774\ud130\ub4e4\uc744 \uc22b\uc790 \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd \n>\n> calendar \uad00\ub828 \ubb38\uc790 \ub370\uc774\ud130\ub4e4\uc744 category \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd ","12a6d66a":"# Data Visualization\n## Total Item Sold Transition","a328ff29":"(20200528 \ucd94\uac00 \uc2dc\uc791)","9d73bafb":"## Department Analysis","3fdc878f":"\ub2e4\ub978 \uc8fc\uc640 \ube44\uad50\ud574\uc11c, TX \uc0c1\uc810\uc740 \ub4f1\ub85d\ub41c \ud56d\ubaa9\uacfc \ube44\uc2b7\ud55c \uacbd\ud5a5\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4.\n\n<!--\nCompared to other states, TX stores have similar tendency regarding registered entries.\n-->","4015602a":"\uac01 \uce74\ud14c\uace0\ub9ac\uc758 \uc0c1\ud488 \uac00\uaca9\uc758 \ud3c9\uade0\uc740 \uc774 \ub370\uc774\ud130\uc14b\uc758 \uc8fc\uae30\uc5d0\uc11c \uba87\uba87\uc758 \ubcc0\ud654\uac00 \uc788\ub294 \uac83 \uac19\ub2e4.\n\uc774\uac83\uc740 \ub2e8\uc21c\ud788 \uc774\ud6c4 \uc8fc\uae30\uc5d0 \ube44\uc2fc \uc0c1\ud488\uc774 \uc99d\uac00\ud588\uae30 \ub54c\ubb38\uc77c\uae4c?\n\n<!--\nThe mean of item price in each category seems to have some change in these dataset periods.  \nIs this simply because the expensive item increased in later periods?\n-->","5fe4ab9d":"1. WI\uc5d0\uc11c\uc758 \uc0c1\uc810\uc740 \uc0c1\ud488 \ud310\ub9e4\ud558\ub294 \uc0c1\ud488\uc758 \uc218\uac00 \ube44\uc2b7\ud558\ub2e4.  \n   2013 \uc804\uc5d0\ub294, WI_3\ub294 WI\uc5d0\uc11c \uac00\uc7a5 \ub9ce\uc774 \ud310\ub9e4\ub41c \uc0c1\uc810\uc774\ub2e4. \uadf8\ub7ec\ub098 WI_2\uac00 \uc810\uc810 \uadf8 \ube44\uc728\uc774 \ucee4\uc84c\ub2e4. (\ud2b9\ud788 2012\ub144 \uc5ec\ub984 \ucbe4)\n   \n2. \uba87\uba87\uc758 \uc9c0\uc810\uc5d0\uc11c, WI_1\uc5d0\uc11c \ud310\ub9e4\ud558\ub294 \uc0c1\ud488\uc758 \uc218\uac00 \ube60\ub974\uac8c \uc99d\uac00\ud588\ub2e4. (\uc57d 2012\ub144 11\uc6d4)\n\n-> \uc804\uccb4 \ud488\uc808 \uc218\ub294 \uadf8 \ub0a0\uc758 \ud56d\ubaa9\uc758 \uc218\uc5d0 \uc758\uc874\ud55c\ub2e4. \uadf8\ub798\uc11c \uc9c0\uae08 \uc6b0\ub9ac\ub294 CA \uc0c1\uc810\uc5d0\uc11c \uc6b0\ub9ac\uac00 \ud588\ub358 \uac83\ucc98\ub7fc \uac01 \uc0c1\uc810\uc5d0\uc11c\uc758 \uc804\uccb4 \uc0c1\ud488 \ud56d\ubaa9\uc744 \ud655\uc778\ud558\ub2e4.\n   \n<!--\n1. Stores in WI have similar item sold count.  \n   Before 2013, WI_3 is the most sold store in WI, but WI_2 gradually increases its proportion. (Especially around summer in 2012)\n   \n2. In some point, WI_1 rapidly increase its sold item count. (Around on November 2012)\n\n-> Total Sold out count depends on the number of entries at that day.  So Now we check the total item entries in each store as we did in CA stores.\n-->","22c24a78":"\ubaa8\ub4e0 \uc0c1\uc810\uc774 2013\ub144 \ud558\ubc18\uae30\uc5d0 \ud3c9\uade0 \uac00\uaca9\ubcf4\ub2e4 \ub354 \ub0ae\uc740 \uac00\uaca9\uc774\uc5c8\ub2e4.\n\n<!--\nAll store has lower price than mean value in the latter half of 2013.  \n-->","d60aae89":"## Item Sold in each day type","f98a3e9a":"> \uba54\ubaa8\ub9ac \uc0ac\uc6a9 \uac10\uc18c\ub97c \uc704\ud574\n>\n> \uc22b\uc790 \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd","13520e54":"\uc774\ub7f0 \uac12\uc758 \uc591\uc774 \ub2e4\ub978\ub2e4\ub294 \uac83\uc744 \ud1b5\ud574\uc11c \ubaa8\ub4e0 \uce74\ud14c\uace0\ub9ac\ub294 \ube44\uc2b7\ud55c \ucd94\uc138\ub97c \uac16\uace0 \uc788\ub2e4.\n2015\ubd80\ud130, \ub4f1\ub85d\ub41c \uc0c1\ud488\uc758 \uc218\ub294 \uc0c1\ub300\uc801\uc73c\ub85c \uc0c1\uc218\ucc98\ub7fc \ubcf4\uc778\ub2e4.\n\n<!--\nAll categories have similar trends though the volume of these values are different.  \nFrom 2015, the number of registered items is seemed to be relatively constant.\n-->","7611f02c":"\uc0c1\ud488\uc774 \uac19\ub2e4\uace0 \ud558\uba74, \ud310\ub9e4 \uac00\uaca9\uc740 \uac01 \uc0c1\uc810\uacfc \uac01 \uacc4\uc808\uc5d0 \ub530\ub77c \uc57d\uac04 \ub2e4\ub974\ub2e4. -> \ud504\ub85c\ubaa8\uc158 \uc2dc\uc98c?\n\n<!--\nThough the item is same, sell price is slightly different in each store and each season. -> Some promotion season?\n-->","3547bf42":"## Next Week Events and This Week Sales Relationship Analysis","b9c42ca7":"## PCA Trial","f50c35fc":"\uc704\uc5d0 \uac83\ub4e4\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \"snap\uc774 \uac00\ub2a5\ud55c \ub0a0\uc740 3\uc77c\uc5d0 \ud55c\ubc88 \ucc98\ub7fc \ub3d9\uc77c\ud558\uac8c \ubd84\ud3ec\ub418\uc5c8\uad6c\ub098.\" \ub77c\uace0 \uc0dd\uac01\ud560 \uc9c0\ub3c4 \ubaa8\ub978\ub2e4. \uc65c\ub0d0\ud558\uba74 \ubaa8\ub4e0 barplot\uc774 \uc5b4\ub5a4 \ub2ec\uc774\ub098 \uc5b4\ub5a4 \ub0a0\uc774\ub098 \uadf8\ub807\uac8c \ub2e4\ub974\uc9c0 \uc54a\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uae30 \ub54c\ubb38\uc774\ub2e4.  \n\uadf8\ub7ec\ub098, \uc774\uac83\uc740 **\uc644\uc804\ud788 \ub2e4\ub974\ub2e4** \ub77c\ub294 \uac83\uc744 \ubc11\uc5d0\uc11c \ubcf4\uc5ec\uc904 \uac83\uc774\ub2e4.\n(\uc608, snap \uad6c\ub9e4 \uac00\ub2a5\ud55c \ub0a0\uc740 \ud3b8\ud5a5\ub418\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub2e4.)\n\n<!--\nFrom above things, we may think \"oh, snap_enable day is distributed uniformly, like 1 day in 3 consecutive days.\" because all of these barplot show it's not so different in any month, any day.  \nHowever, it is **completely different** as I'll show you below.  \n(i.e. snap purchase enable day is distributed biasedly.)\n-->","10d400c1":"\uc774\ubca4\ud2b8 \ub0a0\uc758 \ud310\ub9e4\ub97c \ud655\uc778\ud558\uc790!\n\n<!--\nLet's check the sales of event day!\n-->","d2ddd58b":"\uc88b\ub2e4, \ud55c \ud574\uc5d0 \ucd1d \uc218\uac00 \ubaa8\ub4e0 \ud574\uc640 \ubaa8\ub4e0 \uc8fc\uc5d0\uc11c \uac70\uc758 \uac19\ub2e4.  \n\uc6d4\uac04 \ubd84\ud3ec\ub294 \uc5b4\ub5a8\uae4c?\n\n<!--\nOK, total count in one year is almost the same in all years and all states.\nHow about monthly distribution?\n-->","fc6673c5":"(20200528 \ub05d)","61d407dc":"\uc608\ub97c \ub4e4\uc5b4 \uac00\uc7a5 \ub9ce\uc774 \ud310\ub9e4\ub41c \uc0c1\ud488\uc744 \ucc3e\uace0, snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c flag\uc640 values\ub97c \uc0ac\uc774\uc758 \uad00\uacc4\ub97c \ubcf4\uc790.\n\n<!--\nFind the most item sold day for example and take a look at the relationship between snap purchase allowed flag and values.\n-->","5e557ad0":"# Summary\n   \uc774 \ub178\ud2b8\ubd81\uc5d0\uc11c, \uba87\uba87\uc758 \uc26c\uc6b4 \ub370\uc774\ud130 \uc2dc\uac01\ud654\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \ub370\uc774\ud130\uc14b\uc5d0 \uad00\ud55c \uba87\uba87 \uc694\uc18c\ub97c \uc54c \uc218 \uc788\uc5c8\ub2e4.\n   1. \uac01 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c \ubaa8\ub4e0 \uc0c1\ud488\uc758 \ud310\ub9e4 \ubcc0\ud654\n      - \uba87\uba87\uc758 \uc8fc\uae30\uc801\uc778 \uc601\ud5a5 (\uc8fc\uac04\uacfc \uc6d4\uac04)\n      - \ud06c\ub9ac\uc2a4\ub9c8\uc2a4\uc5d0 \uac70\uc758 \ud310\ub9e4\uac00 \uc5c6\ub2e4.\n   2. \uc5b4\ub5a4 \uce74\ud14c\uace0\ub9ac\uac00 \uac00\uc7a5 \ub9ce\uc774 \ud314\ub838\ub098?\n      - FOODS\uac00 \uc774 3\uac1c\uc758 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c \uac00\uc7a5 \ub9ce\uc774 \uc0c1\ud488\uc774 \ud314\ub838\ub2e4.\n      - HOUSEHOLD \uce74\ud14c\uace0\ub9ac\uac00 \ub450\ubc88\uc9f8\uc774\uace0, HOBBIES\uac00 \uac00\uc7a5 \uc801\uac8c \ud314\ub9b0 \uac83\uc774\ub2e4.\n   3. \uc5b4\ub5a4 \ub0a0\uc774 \uac00\uc7a5 \ub9ce\uc774 \ud314\ub838\ub098?\n      - \ud1a0\uc694\uc77c\uacfc \uc77c\uc694\uc77c\uc774 \uac00\uc7a5 \ub9ce\uc740 \uc0c1\ud488\uc774 \ud310\ub9e4\ub41c \ub0a0\uc774\ub2e4.\n      - \ubc18\ub300\ub85c, \ud654\uc694\uc77c \uac19\uc740 \ud3c9\uc77c\uc774 \uac00\uc7a5 \uc801\uac8c \ud314\ub9b0 \ub0a0\uc774\ub2e4.\n   4. \uac01 \uc8fc\ub9c8\ub2e4 \ubaa8\ub4e0 \uc0c1\uc810\uc5d0\uc11c \uc774\ub3d9\n      - CA\uc5d0\uc11c CA_3 \uc0c1\uc810\uc774 \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 \uc0c1\uc810\uc774\ub2e4.\n      - \ub2e4\ub978 \uc8fc\uc5d0\uc11c \uadf8\ub807\uac8c \ub9ce\uc740 \ucc28\uc774\uac00 \ub098\ud0c0\ub098\uc9c0 \uc54a\uc558\ub2e4.\n      - \uc774\ub7f0 \ud310\ub9e4 \uc774\ub3d9\uc740 \uc885\uc885 \ub4f1\ub85d\ub41c \uc0c1\ud488 \ud56d\ubaa9\uacfc \uc5f0\uad00\uc774 \uc788\ub2e4.\n   5. Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c \ub0a0\uc758 \uc2dc\uac01\ud654\n      - \uc804\uccb4 \ub144\uc5d0 Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c \ub0a0\uc758 \ucd1d \uc218\ub294 2011\ub144 \ubd80\ud130 \uac70\uc758 \uac19\ub2e4.\n      - \ud55c\ub2ec \ub3d9\uc548 Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c \ub0a0\uc758 \ucd1d \uc218\ub294 \ub9e4\ub2ec 10\uc77c\uc774\ub2e4.\n      - \uac01 \uc77c\ubcc4\ub85c Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c \ub0a0\uc758 \ucd1d \uc218\ub294 \ub3d9\ub4f1\ud558\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub2e4.\n      - \uadf8\ub7ec\ub098 Snap \uad6c\ub9e4\uac00 \uac00\ub2a5\ud55c flag\ub97c \ub530\ub974\uba74, \ud3b8\ud5a5\ub41c \ud328\ud134\uc774 \uc788\ub2e4.\n        (\uc608, \uaddc\uce59\uc801\uc73c\ub85c 3\uc77c \ud6c4\uc5d0 \ud558\ub8e8\uac19\uc9c0\ub294 \uc54a\uc73c\ub098, \ud55c\uc8fc\uc758 \ubaa8\ub4e0 \ub0a0\uc774\uac70\ub098 \ub2e4\uc74c \uc8fc\uc5d0 \uc5c6\uac70\ub098 \ucc98\ub7fc\uc740 \uc788\ub2e4.)\n        \n   (20200528 6\ubc88 \ucd94\uac00)\n   6. Dynamic \uc694\uc18c \ubd84\uc11d \uc2dc\ud5d8\n      - \uc6b0\ub9ac\ub294 \ud558\ub098\uc758 \uc0c1\ud488 (FOODS_3_090) \uc5d0 \uc788\ub294 dynamic \uc694\uc18c \ubd84\uc11d\uc744 \uc0ac\uc6a9\ud574\uc11c \uad00\uce21\ub418\uc9c0 \uc54a\uc740 \uc694\uc18c\ub97c \uc54c\uc544\ub0c8\uc2b5\ub2c8\ub2e4.\n      - \uc774 \uc694\uc18c\ub294 \uac01 \uc0c1\uc815\uc5d0\uc11c \ud310\ub9e4 \ubcc0\ud654\ub97c \uc798 \uc124\uba85\ud574\uc90d\ub2c8\ub2e4.\n        \n   \uadf8\ub9ac\uace0 \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ud788\ud2b8\ub9f5\uc744 \uc0ac\uc6a9\ud574\uc11c \uba87 \uac1c\uc758 \uc694\uc18c\ub97c \uc2dc\uac01\ud654\ud55c\ub2e4.\n\n<!--\n   In this notebook, through some easy data visualization, we found some points regarding this dataset like below.\n   1. The transition of all items sold in each category \n      - Some periodical effect. (Weekly and monthly)\n      - On christmas day, there are almost no sales\n   2. Which category is the most sold one?  \n      - FOODS is the most sold item category of these three categories.\n      - HOUSEHOLD category is the 2nd one, and the HOBBIES are the least sold one.\n   3. Which day type is the most sold day? \n      - Saturday and Sunday is the most item sold day types.\n      - In contrast, on weekdays like Tuesday, there are less item sold.\n   4. The transition in all stores by each state\n      - In CA, CA_3 store is the most sold store.  \n      - In other states, not so much difference appeared.  \n      - These sales trasition often corresponds to the registered item entries.\n   5. Snap purchase allowed day visuaizaiton\n      - The total count of Snap purchase allowed day in whole year is almost the same from 2011.\n      - The total count of Snap purchase allowed day in one month is 10 in every month.\n      - The total count of Snap purchase allowed day in each day type is almost uniformly distributed.\n      - However, there are some biased patterns regarding snap purchase allowed flag.\n        (i.e. it is not like one day in three consective days regularly, but all days in one week and none in next week)\n\n   6. Dynamic Factor Analysis Trial\n      - We found the unobserveed factor by using dynamic factor analysis in one item (FOODS_3_090) \n      - This factor explains the sales transition in each state well.\n    \n   And finally we visualize some points by using heatmap.\n-->","4bec8a26":"## Dynamic Factor Analysis Trial\nDynamic Factor \ubd84\uc11d\uc740 PCA (Principle Component Analysis) \uac19\uc740 \ucc28\uc6d0 \ucd95\uc18c \uae30\ubc95 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.  \n\uc6b0\ub9ac\uac00 \uba87\uba87\uc758 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uba74, \uc6b0\ub9ac\ub294 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc758 \uc228\uaca8\uc9c4 \uc694\uc18c\ub97c \ubc1c\uacac\ud558\uae30 \uc704\ud574 \uc774 \uae30\ubc95\uc744 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n(\uc5ec\ub7ec\ubd84\uc774 \uc5ec\uae30\uc5d0\uc11c \ub354 \uc54c\uace0 \uc2f6\uc73c\uba74: https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html)  \n\uadf8\ub9ac\uace0 \uc81c\uac00 \ubc11\uc5d0\uc11c \uc2dc\ub3c4\ud558\ub294 \uac83\uc740 \uc774 statsmodel's \uc6f9 \uc0ac\uc774\ud2b8\uc758 \ud29c\ud1a0\ub9ac\uc5bc\uacfc \uc720\uc0ac\ud569\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 \uc774\uac83\uc744 \uac01 \uc0c1\uc810\uc758 \ud310\ub9e4 \uc0c1\ud488\uc744 \uc815\ud558\uac70\ub098 \uac01 \uc0c1\uc810 \uc0ac\uc774\uc758 \uba87\uba87\uc758 \uc228\uaca8\uc9c4 \uc694\uc18c\ub97c \ubc1c\uacac\ud558\ub294 \ub370 \uc801\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n<!--\nDynamic Factor analysis is one of the dimention reduction technoloies like PCA (Principle Component Analysis)  \nSuppose we have some time-series data, we can apply this technique to find the hidden factor of these time-series data.  \n(You can learn more here: https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html)  \nAnd what I tried below is similar to the tutorial of this statsmodel's website.\n\nWe apply this to the target item sold of each store and find some hidden factors among stores.\n-->","490195a3":"\uc774 \uac12\uc740 \uc694\uc18c\ub4e4\uc774 \ucd1d \ub370\uc774\ud130\uc14b\uc744 \uc5bc\ub9c8\ub098 \ub9ce\uc740 \ud37c\uc13c\ud2f0\uc9c0\ub85c \uc124\uba85\ud558\ub294 \uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\uc774 \uacbd\uc6b0\uc5d0, \uc8fc\uc694 \uc694\uc18c 1\uc740 \ub370\uc774\ud130\uc758 85%\ub97c \uc124\uba85\ud558\uace0 \uc694\uc18c 2\uac00 8%\ub97c \uc124\uba85\ud55c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\uadf8\ub798\uc11c \ub370\uc774\ud130\uc758 \ucd1d 93%\uac00 \uc774 \uc694\uc18c\ub4e4\ub85c \uc124\uba85\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n<!--\nThis value shows how much percentage do these components explain the total dataset.  \nIn this case, principal component 1 explains 85 % of the data and component 2 does 8 %.   \nSo the total 93 % of the data could be explained with these components\n-->","1b104630":"(20200528 \uc2dc\uc791)","ae5f328c":"\uc88b\ub2e4, \ud55c\ub2ec\uc5d0 \ucd1d \uc218\ub294 \uc804\uccb4 \ud574\ub85c \ubcf4\uba74 \uac19\ub2e4.  \n\uc8fc\uac04 \ubd84\ud3ec\ub294 \uc5b4\ub5a8\uae4c?\n\n<!--\nOK, total count in one month is the same through the whole year.\nHow about weekly distribution?\n-->","39ec6887":"1. \uc88b\ub2e4, snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc758 \ucd1d \uc218\ub294 3\uac1c\uc758 \uc0c1\uc810\uc5d0\uc11c \uc720\uc0ac\ud55c \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.\n\n2. snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc758 \ucd1d \uc218\ub294 \uac00\ub2a5\ud558\uc9c0 \uc54a\uc740 \ub0a0\uc758 \uc57d \uc808\ubc18\uc815\ub3c4\uc774\ub2e4.\n\n\ub2e4\uc74c\uc73c\ub85c snap \uad6c\ub9e4\uac00 \uc77c\ub144 \ub3d9\uc548 \uc5bc\ub9c8\ub098 \ubd84\ud3ec\ub418\uc5c8\ub294 \uc9c0 \ud655\uc778\ud574\ubcf4\uc790.\n\n<!--\n1. OK, the total count of snap purchase enable day looks similar in these three stores.\n\n2. The total count of snap purchase enable day is about one half of that of non-enable day.\n\nNext Let's see whether snap purchase is how-distributed in one year.\n-->","fe3b16b9":"\uc6b0\ub9ac\ub294 \uc774 \uc694\uc18c\uac00 \uac01 \uc0c1\uc810 \uceec\ub7fc\uc744 \uc798 \uc124\uba85\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c\uc544\ubd24\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 TX \uc8fc\uc5d0\uc11c.\n\ub530\ub77c\uc11c, \uc6b0\ub9ac\ub294 \uc774 \uc138 \uac1c\uc758 \uc8fc\ub4e4\uc774 \ud558\ub098\uc758 \uad00\uce21\ub418\uc9c0 \uc54a\uc740 \uc694\uc18c\uc5d0 \uc758\ud574 \uc124\uba85\ub41c\ub2e4\uace0 \uacb0\ub860\uc9c0\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\uc8fc \ubcc4\ub85c \uadf8\ub807\uac8c \ub2e4\ub974\uc9c0 \uc54a\ub2e4.)\n\n<!--\nWe can find this factor explains each store column well, especially for TX state,  \nThus, we can conclude these three states are explained by one unobserved factor.  \n(Not so different from state to state)\n-->","ac2e0f82":"\ud589\uc0ac \ub0a0 flag\uc640 \ud310\ub9e4 \uac00\uaca9\uc740 \uadf8\ub807\uac8c \uac15\ud55c \uad00\uacc4\uac00 \uc5c6\ub2e4.\n\uadf8\ub7ec\ub098, \uc6b0\ub9ac\uac00 \uba87\uba87 \ud589\uc0ac\ub97c \uc704\ud574 \uc0c1\ud488\uc744 \uc0c0\uc744 \ub54c, \uc6b0\ub9ac\ub294 \uc544\ub9c8\ub3c4 \ud589\uc0ac 1\uc8fc\uc5d0\uc11c 1\uc77c \uc804\uc5d0 \uc0b0\ub2e4. \ub2f9\uc77c\uc774 \uc544\ub2c8\ub77c.\n\uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \uc774\uac83\uc744 \uace0\ub824\ud574\uc57c\ud55c\ub2e4. (\ub098\ub294 \uc774\uac83\uc744 \ub098\uc911\uc5d0 \ubd84\uc11d\ud560 \uac83\uc774\ub2e4.)\n\n<!--\nEvent Day flag and Sell Price don't have so strong relationship.  \nHowever, when we buy items for some events, we perhaps buy items 1 week ~ 1 day before the event, not on the same day.  \nSo we have to take this into consideration.  (I'll tackle with this analysis later.)\n-->","3a1e802b":"1. \uc88b\ub2e4. \uac01 \uc8fc\ubcc4\ub85c \ucd1d snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc758 \ucd1d \uc218\ub294 \ubaa8\ub4e0 \ud574\uc5d0\uc11c \ub3d9\uc77c\ud558\ub2e4.\n\n2. 2011\ub144\ubd80\ud130 2015\ub144\uae4c\uc9c0, snap \uad6c\ub9e4\uac00 \ud5c8\uc6a9\ub41c \ub0a0\uc740 \uc57d 120\uc77c\uc774\ub2e4.  \n   (2016\ub144\uc5d0 \uc774\ub974\ub7ec\uc11c\uc57c, 1\ub144\uc758 \uc808\ubc18\uc774\uc5c8\ub2e4.)\n\n<!--\n1. OK. Total snap purchase allowed day of each state is the same in all years.\n\n2. From 2011 to 2015, there are about 120 days when snap purchase is allowed.  \n   (As for 2016, we only have the first half of whole year.)\n-->","a93c805b":"\uc774 \uacb0\uacfc \ub610\ud55c \ud765\ubbf8\ub86d\ub2e4.\n\ubb3c\ub860, \uc0c1\ud488\uc758 \uac00\uaca9\uc774 \ube44\uc300 \ub54c, \uc0c1\ud488\uc774 \ub35c \ud310\ub9e4\ub41c\ub2e4. (\uc67c\ucabd\uc704 \uc601\uc5ed)\n\uadf8\ub9ac\uace0 \uac00\uaca9\uc774 \ub0ae\uc544\uc9c8 \ub54c, \ub354 \ub9ce\uc740 \uc0c1\ud488\uc774 \ud314\ub9b0\ub2e4. (\uc624\ub978\ucabd\uc544\ub798 \uc601\uc5ed)\n\uadf8\ub7ec\ub098, \uad00\uacc4\ub294 \uc120\ud615\uc801\uc774\uc9c0 \uc54a\uc740 \uac83 \uac19\uc73c\ub098 \uc5ed \ube44\ub840\ucc98\ub7fc \ubcf4\uc778\ub2e4.\n\n<!--\nThis result is also interesting.  \nOf course, when the price of item is expensive, less items are sold. (left top field)  \nAnd when the price gets lower, more items are sold.  (Right bottom field)  \nHowever, the relationship is not likely linear but likely to be inverse propotion.\n-->","43abb9ab":"\uc774\ubbf8 HOUSEHOLD \uc911 \uc77c\ubd80\uac00 \uc608\uc678\uc801\uc73c\ub85c \ud310\ub9e4 \uac00\uaca9\uc774 \ube44\uc2f8\ub2e4\ub294 \uac83\uc744 \ubd24\ub2e4.\n\uc774\uc81c \uc774 \uc0c1\ud488\ub4e4\uc744 \uba87\uba87\uc758 \uc0c1\uc810\uc744 \uc81c\uc678\ud558\uba74 \ub354 \uc774\uc0c1 \uc5b4\ub514\uc11c\ub3c4 \ud310\ub9e4\ud558\uc9c0 \uc54a\ub294 \ub2e4\ub294 \uac83\uc744 \uc54c\uc558\ub2e4.\n\n<!--\nWe've already seen HOUSEHOLD has some exceptionally expensive sell price.  \nNow we found these items aren't sold anywhere, but only in some stores.\n-->","06dd2cc2":"\uc77c\ubd80 HOUSEHOLD \uce74\ud14c\uace0\ub9ac\uc758 \uac00\uaca9\uc740 100 \\$ \ubcf4\ub2e4 \ud6e8\uc52c \ube44\uc2f8\ub2e4.  \n\ubc18\uba74\uc5d0, FOODS\uc740 \uac70\uc758 \uc57d5\uc5d0\uc11c 10 \\$ \uc774\uace0, \ud070 \ud3b8\ucc28\uac00 \uc5c6\ub2e4.\n\n<!--\nThe price of some Household category is super expensive like over 100 \\$.  \nOn the other hand, foods are mostly around 5 to 10 \\$ and don't have a large deviation.\n-->","fb666ebf":"## Calendar Visualization","9409a4b6":"\uc774 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub300\ubd80\ubd84\uc758 \ub9e4\uc9c4 \uc0c1\ud488\uc740 FOODS_3_090_CA_3_validation \uc774\ub2e4.\n\n<!--\nThe most sold out item in this dataseet is FOODS_3_090_CA_3_validation\n-->","4eda60ae":"\uc774\ub7f0 \uacbd\uc6b0\uc5d0\ub294 \uc544\ub9c8\ub3c4 \ud558\ub8e8\ub098 \ud55c \uc8fc\uc758 \uc9c0\uc5f0 \ubcc0\uc218\uac00 \uc911\uc694\ud558\ub2e4.\n\ub098\uc758 \ub2e4\uc74c \ubc84\uc804\uc5d0\uc11c, \ud310\ub9e4 \ub0a0\uacfc \uacfc\uac70 \uba70\uce60\uc0ac\uc774\uc758 \uad00\uacc4\uc758 \uac15\uc810\uc744 \uc54c\uc544\ubcfc \uac83\uc774\ub2e4.\n\n<!--\nMaybe 1 day or 1 week lag variables are important in this case.  \nI'll find the strength of correlation between the sold of the day and past few days in my next version.\n-->","28f00925":"3. 2015\ub144 \ubd04\/\uc5ec\ub984\ucbe4\ubd80\ud130, CA_2\uc758 \ud310\ub9e4 \uae30\ub85d\uc774 \ube60\ub974\uac8c \uc99d\uac00\ud588\ub2e4. \uc6b0\ub9ac\ub294 \uc774\uc720\ub97c \uc870\uc0ac\ud574\uc57c\ud55c\ub2e4.\n\n-> \uc774\uac83\uc740 CA_2\uc5d0 \ub4f1\ub85d\ub41c \uc0c1\ud488\uc774 \ube60\ub974\uac8c \uc99d\uac00\ud588\uae30 \ub54c\ubb38\uc774\ub2e4.  \n2015\ub144 \uc5ec\ub984 \uc774\ud6c4\uc5d0, CA\uc758 \ubaa8\ub4e0 \uc0c1\uc810\ub4e4\uc774 \uc0c1\ud488\uc758 \uc591\uc744 \ube44\uc2b7\ud558\uac8c \ub4f1\ub85d\ud588\ub2e4.\n\n<!--\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n-> It is because item registered in CA_2 increased rapidly.  \nAfter summer in 2015, all stores in CA have similar registered item count\n-->","72dc05a1":"**Point of the graph**\n\n1. FOODS\ub294 \uc774 3\uac1c\uc758 \uce74\ud14c\uace0\ub9ac \uc911\uc5d0\uc11c \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 \ud488\ubaa9 \uce74\ud14c\uace0\ub9ac\uc774\ub2e4.  \n   HOUSEHOLD\ub294 2\ubc88\uc9f8\uc774\uace0, HOBBIES\ub294 \uac00\uc7a5 \uc801\uac8c \ud314\ub9b0 \ud488\ubaa9\uc774\ub2e4.\n   \n2. FOODS \uce74\ud14c\uace0\ub9ac\ub294 \ubd84\uba85\ud788 \uba87\uba87\uc758 \uc8fc\uae30\uc801\uc778 \ud2b9\uc131\uc744 \uac16\uace0 \uc788\ub2e4.  \n   \uc77c\ub144 \ub3d9\uc548, \uc774\uac83\uc740 \uaca8\uc6b8\ubcf4\ub2e4 \uc5ec\ub984\uc5d0 \ub354 \ub9ce\uc774 \ud314\ub9b0 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. \uadf8\ub7ec\ub098, \uc774\uac83\uc744 \uac80\uc99d\ud574\ubd10\uc57c\ud55c\ub2e4.\n\n3. HOUSEHOLD \uce74\ud14c\uace0\ub9ac \ud488\ubaa9\uc758 \ud310\ub9e4\ub7c9\uc740 2011\ub144\ubd80\ud130 \uc810\uc810 \uc99d\uac00\ud588\ub2e4.  \n   \uadf8\ub7ec\ub098, 2011\ub144\uc5d0 \uc0c1\uc810\uc5d0 \uba87\uba87 \ud488\ubaa9\uc774 \uc5c6\uc5c8\uae30 \ub54c\ubb38\uc77c \uc9c0\ub3c4 \ubaa8\ub978\ub2e4.  \n   \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \uc0c1\uc810\uc5d0\uc11c\uc758 \ucd1d \ud488\ubaa9\uc744 \uace0\ub824\ud574\ubd10\uc57c \ud55c\ub2e4.  \n   \uc8fc\uae30\uc801\uc778 \ud2b9\uc131\uc740 FOODS\uc640 \ube44\uad50\ud574\uc11c \uc774 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c\ub294 \uadf8\ub807\uac8c \ubd84\uba85\ud558\uc9c0\ub294 \uc54a\ub2e4.\n\n4. HOBBIES \uce74\ud14c\uace0\ub9ac\uc5d0\uc120, \uc8fc\uae30\uc801 \ud2b9\uc131\uc774 HOUSEHOLD \uce74\ud14c\uace0\ub9ac\ucc98\ub7fc \ubd84\uba85\ud558\uc9c0 \uc54a\ub2e4.  \n\n5. \uba87\uba87 \ubd80\ubd84\uc5d0\uc11c (\uc5f0\ub9d0 \ucbe4), \ubaa8\ub4e0 \uce74\ud14c\uace0\ub9ac\ub4e4\uc740 \uc5b4\ub5a4 \ud310\ub9e4\ub3c4 \uc5c6\ub2e4. \uadf8\ub798\uc11c \ub098\ub294 \ud6c8\ub828 \ubaa8\ub378\uc5d0\uc11c \uc774\ub7f0 \ub0a0\ub4e4\uc744 \uac00\uc838\uc654\ub294 \uc9c0 \uace0\ub824\ud574\ubd10\uc57c \ud55c\ub2e4\uace0 \uc0dd\uac01\ud55c\ub2e4.\n\n\uc790, \uac00\uc7a5 \ucd5c\uadfc\uc778 2015\ub144\uc744 \ubcf4\uc790!\n\n<!--\n1. FOODS is the most sold item category of these three categories.  \n   HOUSEHOLD is the 2nd one, and HOBBIES are the least sold one.  \n\n\n2. FOODS category appearently has some periodical feature.   \n   During one year, it seems more items are sold in summer than in winter, however, we have to verify this.  \n   As for more short time interval, it seems the trend has monthly or weekly features. (Let's take a look below)\n\n3. HOUSEHOLD category items sold is gradually increasing from 2011.  \n   However, it may be because some items are not in the store in 2011.  \n   So we have to take the total item in the store into account.\n   Periodical Features are not so clear in this category compared to FOODS.\n  \n4. In HOBBIES category, periodical features are less appearent like HOUSEHOLD category.\n\n5. In some point (around the end of year), all categories don't have any sold.  So I think we have to consider whether we take these days into account when training models.\n\nSo let's take a look at the latest year, 2015!\n-->","47cfe56e":"## Item sold in each State and Store","95275830":"\uc800\ub294 \ubaa8\ub4e0 \uc0c1\uc810\uc5d0\uc11c \ud558\ub8e8 \ub3d9\uc548 \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 12\uac1c\uc758 \uc0c1\ud488\uc744 \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.\n\uadf8 \uacb0\uacfc\ub294 \"FOODS_3_090\"\uc774 \ub2e4\ub978 \uac83\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ub192\uc740 \ud310\ub9e4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0 \uc5ec\uae30\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \uc0c1\ud488\uc740 FOODS \uce74\ud14c\uace0\ub9ac\uc774\uace0 \ubd80\uc11c \ubc88\ud638\ub294 3\uc785\ub2c8\ub2e4!\n\n\uadf8\ub798\uc11c, \ub2e4\uc74c \uc9c8\ubb38\uc740, \uac01 \ubd80\uc11c\uc5d0 \uc5b4\ub5a4 feature\uac00 \uc788\uc744\uae4c\uc694?\n\uc608\ub97c \ub4e4\uc5b4, FOODS_3 \ubd80\uc11c\uc5d0\uc11c, \uc2f8\uace0 \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \uc0c1\ud488\uc744 \ub2e4\ub8e8\uace0 \uc788\uc744\uae4c\uc694?\n\n<!--\nI got top 12 sold items in all stores per one day.  \nThe result shows \"FOODS_3_090\" has very higher sold than others.  \nAnd all items here are FOODS category and department no is 3!\n\nSo, next question, are there any features in each dept?  \nFor example, in FOODS_3 department, do they treat cheap and frequently-used item?\n-->","60100c57":"\uac04\ub2e8\ud788 \ub9d0\ud574\uc11c, \ubaa9\ud45c \ud310\ub9e4 \uc0c1\ud488\uc758 \ub370\uc774\ud130\uc14b\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \uc8fc \uc0ac\uc774\uc5d0 \uad00\uce21\ub418\uc9c0 \uc54a\uc740 \ud558\ub098\uc758 \uc694\uc18c\ub97c \ucd94\ucd9c\ud569\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0 \uc704 plot\uc740 \uc774 \uc694\uc18c\uac00 \uc5b4\ub5bb\uac8c \uac01 \uc0c1\uc810\uc5d0 \uc788\ub294 \ud310\ub9e4 \uc0c1\ud488\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n\uc544\ub798 \ucf54\ub4dc\ub294 \uc774 \uc694\uc18c\uac00 \uc804\uccb4 \ud310\ub9e4 \ubcc0\ud654\ub97c \uc5b4\ub5bb\uac8c \uc124\uba85\ud558\ub294 \uc9c0\uc5d0 \ub300\ud55c $R^2$ \uac12\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\n<!--\nIn short, from the dataset of the target item sold, we extract one unobserved factor among states.  \nAnd the above plot shows how this factor effects to the item sold in each state.  \n\nThe code below calculates $R^2$ values of how this factor explains the total sold transition.\n-->","184b5999":"\uba87\uba87\uc758 \ud488\ubaa9\uc740 \uc2ec\uc9c0\uc5b4 \ud06c\ub9ac\uc2a4\ub9c8\uc2a4\uc5d0\ub3c4 \ud314\ub838\ub2e4. \uadf8\ub7ec\ub098 \ub098\ub294 noisy\ub41c \uac12\uc774 \uc788\ub2e4\uace0 \uc644\uc804\ud788 \uc0dd\uac01\ud55c\ub2e4.\n\n\uc9c0\uae08\uae4c\uc9c0, \uc6b0\ub9ac\ub294 \ud310\ub9e4\ub41c \ud488\ubaa9\uc774 \uc8fc\uac04 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\ub2e4. \uc774\uac83\uc744 \uc0dd\uac01\ud574\ubcf4\uc790:\n**\ub2e4\uc74c \uc9c8\ubb38: \uadf8 \uc8fc\uc758 \uc5b4\ub290 \ub0a0\uc5d0 \uac00\uc7a5 \ub9ce\uc774 \ud310\ub9e4\ub420\uae4c?**\n\n<!--\nSome items are sold even on Christmas Day, but I think these are completely noisy values.   \n\nUntil now, we can find the items sold have something weekly fetures. So let's think this:   \n**Next Question: Which day of the week is the items sold most?**\n-->","ff30c2a9":"# Data Visualization Notebook\n\n1st public version: Created on May 6th, 2020.  \n2nd public version: Created on May 7th, 2020.  \n3rd public version: Created on May 10th, 2020.  \n4th public version: Created on May 15th, 2020.  (Add Department Analysis)  \n5th public version: Created on May 16th, 2020.  (Add Next week event and value analysis)  \n6th public version: Created on May 16th, 2020.  (Add [Dynamic Factor Analysis](#Dynamic-Factor-Analysis-Trial))  \n7th public version: Created on May 23th, 2020.  (Fix some codes in [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store), Thank you for your comment, Kevin.)  \n8th public version: Created on May 26th, 2020. (Fix Bugs and Add Calendar Plot)\n9th public version: Created on May 26th, 2020. (Add more precise PCA Plot, See [PCA Section](#PCA-Trial))\n\n\uc548\ub155\ud558\uc138\uc694, \uc5ec\ub7ec\ubd84.  \n\uc800\ub294 \uc774 competition ** M5 -accuracy- ** \ub3c4\uc804\uc790 \uc911 \ud55c\uba85\uc785\ub2c8\ub2e4.  \n\uc800\ub294 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \uc608\uce21\ud558\uae30 \uc704\ud574 \uba87\uba87\uc758 \uc720\uc6a9\ud55c insight\ub97c \uc54c\uace0 \uc2f6\uc2b5\ub2c8\ub2e4.\n\n\uc5ec\uae30\uc11c \uc800\ub294 \ud2b9\ud788 \uc800 \uac19\uc740 \ucd08\ubcf4\uc790\ub4e4\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc2dc\uac01\ud654\ub97c \uc704\ud55c \ub178\ud2b8\ubd81\uc744 \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.  \n\uc800\ub294 \uc5ec\ub7ec\ubd84\ub4e4\uc758 \uc870\ud68c\uc640 \ub313\uae00 \ubaa8\ub450 \uac10\uc0ac\ud569\ub2c8\ub2e4.\n\nhttps:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration   \n\uc774 \ub178\ud2b8\ubd81\uc740 \uc815\ub9d0 \uc88b\uc2b5\ub2c8\ub2e4.  \n\ub9cc\uc57d \uc5ec\ub7ec\ubd84\ub4e4\uc774 \uc2dc\uac04\uc774 \uc788\ub2e4\uba74, \uc800\ub294 \ud55c \ubc88 \ubcf4\ub294 \uac83\uc744 \uac15\ub825 \ucd94\ucc9c\ud569\ub2c8\ub2e4.  \n\uadf8\ub9ac\uace0 \uc800\ub294 Reference \uc139\uc158\uc5d0\uc11c \ub2e4\ub978 \uc88b\uc740 \ucee4\ub110\ub4e4\uc744 \uc18c\uac1c\ud560 \uac83\uc785\ub2c8\ub2e4.  \n\n\n<!--\n\nHello, everyone.  \nI'm one of the challenger of this competition **M5 -accuracy- **.  \nI'd like to find some useful insights for predictiong the test dataset.  \n\nHere I created a notebook for data visualization especially for beginners like me.  \nI appreciate all of your views and comments.\n\nhttps:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration  \nHis notebook is super great.  \nIf you have time, I strongly recommend you to take a look.\nAnd I'll introduce other good kernels in Reference Section\n-->\n\nHere is table of contents in this notebook:\n- [Import Libraries and Data Input](#Import-Libraries-and-Data-Input)\n- [Data Cleaning](#Data-Cleaning)\n- [Data Visualization](#Data-Visualization)\n   -  [Total Item Sold Transition](#Total-Item-Sold-Transition)\n   -  [Item Sold in each day type](#Item-Sold-in-each-day-type)\n   -  [Item sold in each State and Store](#Item-sold-in-each-State-and-Store)\n      - [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store)\n   -  [Item Sold relation Analysis](#Item-Sold-relation-Analysis)\n   -  [Dynamic Factor Analysis Trial](#Dynamic-Factor-Analysis-Trial)\n   -  [Store Analysis](#Store-Analysis)\n   -  [Snap Purchase Analysis](#Snap-Purchase-Analysis)\n   -  [Event Pattern Analysis](#Event-Pattern-Analysis)\n   -  [One Item Features Analysis](#One-Item-Features-Analysis)\n   -  [Sell Price Analysis](#Sell-Price-Analysis)\n   -  [Sell price and value relationship](#Sell-price-and-value-relationship)\n   -  [Department Analysis](#Department-Analysis)\n   -  [Next Week Events and This Week Sales Relationship Analysis](#Next-Week-Events-and-This-Week-Sales-Relationship-Analysis)\n   -  [Relationship of Lag Variables](#Relationship-of-Lag-Variables)\n   -  [Calendar Visualization](#Calendar-Visualization)\n   -  [PCA Trial](#PCA-Trial)\n  \n- [Summary](#Summary)\n- [Future Work](#Future-Work)\n- [References](#References)","9eba08e5":"# Future Work\nIf I have time, I'd like to tackle with the following things.\n\n1. Apply Dynamic Analysis and find out the relationship among state and stores.\n2. Make One item or one store prediction model for beginners like me to learn how to use lightgbm as a regressor.\n3. Check out the pre-processing effect. Is that effective considering the noise samples like Christmas or other irregularly days.\n4. More detailed analysis and find out some useful information for making prediction.","5ce2d500":"\uc6b0\ub9ac\ub294 3\ubd84\uae30\uc5d0 (7\uc6d4\ubd80\ud130 9\uc6d4) \ud310\ub9e4\ub41c \uc0c1\ud488\uc774 \ub2e4\ub978 \ubd84\uae30 \ubcf4\ub2e4 \uc801\ub2e4\ub294 \uac83\uc744 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\nviolinplot \ud568\uc218\ub294 \uc774 \ubd84\ud3ec\uc5d0 \ub300\ud55c \uac12\uc744 \uadf8\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \n\uc5ec\ub7ec\ubd84\uc740 seaborn violinplot \uc744 \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html)  \n> violin plot\uc740 box\uc640 whisker plot\uacfc \ube44\uc2b7\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uc815\ub7c9\uc801\uc778 \ub370\uc774\ud130\uc758 \ubd84\ud3ec\ub97c \ud558\ub098 (\ub610\ub294 \uadf8 \uc774\uc0c1)\uc758 categorical \ubcc0\uc218\uc758 \uba87 \ub2e8\uacc4\ub85c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uadf8\ub798\uc11c \uadf8 \ubd84\ud3ec\ub4e4\uc774 \ube44\uad50\ub420 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4. \uc2e4\uc81c datapoint\uc5d0 \uc0c1\uc751\ud558\ub294 plot \uc694\uc18c\uc758 \uc804\ubd80\uac00 \uc788\ub294 box plot\uacfc \ub2ec\ub9ac, violin plot\uc740 \ubd84\ud3ec\ub97c \ud558\ub098\uc758 \uadfc\ubcf8\uc801\uc778 \ubd84\ud3ec\uc5d0 \ub300\ud55c kernel density estimation(\ucee4\ub110\ubc00\ub3c4\ucd94\uc815)\ub97c \uc911\uc694\ud558\uac8c \uc5ec\uae41\ub2c8\ub2e4.\n\n\ub530\ub77c\uc11c, boxplot\uacfc violinplot \uc0ac\uc774\uc758 \ucc28\uc774\ub294, \uacf5\uc2dd \ubb38\uc11c\uc5d0 \uc368\uc788\ub294 \uac83\ucc98\ub7fc, violinplot\uc740 kernel density estimation(\ucee4\ub110\ubc00\ub3c4\ucd94\uc815) \uc744 \uadf8\ub9bd\ub2c8\ub2e4.\n\n<!--\nWe can see in quarter 3 (from July to September), the item sold seems less than other quarters.\n\nviolinplot method can plot values with its distribution.  \nYou can search seaborn violinplot (https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html)\n> A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the underlying distribution.\n\nThus, the difference between boxplot and violinplot is, as the official document says, violinplot plots with its kernel density estimation.\n-->","2f5b4c8b":"(20200528 \ub05d)","7db080b6":"1. \uc6b0\ub9ac\uac00 \ucd94\uce21\ud560 \uc218 \uc788\ub294 \uac83\ucc98\ub7fc, \ud1a0\uc694\uc77c\uc774\ub098 \uc77c\uc694\uc77c\uc774 \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 \ub0a0\uc774\ub2e4.  \n   \ud654\uc694\uc77c\uacfc \uc218\uc694\uc77c\uc740 \uac00\uc7a5 \uc801\uac8c \ud314\ub9b0 \ub0a0\uc774\ub2e4.  \n   -> \ub098\uc911\uc5d0, \uc6b0\ub9ac\ub294 \uc774\ub7f0 \uad00\uacc4 \uc694\uc18c\ub97c \ud788\ud2b8\ub9f5\uc73c\ub85c \uc2dc\uac01\ud654\ud560 \uac83\uc774\ub2e4. \ub2e4\uc74c\uc744 \ubcf4\uc790!\n\n2. HOBBIES\uac00 FOODS\ub098 HOUSEHOLD\uc640 \ube44\uad50\ud574\uc11c \ub0a0\uc9dc\uc5d0 \uadf8\ub807\uac8c \uc758\uc874\uc801\uc774\uc9c0\ub294 \uc54a\ub2e4.\n\n<!--\n1. As we can probraly guess, Saturday or Sunday is the day which the items are most sold.  \n   Tuesday or Wednesday is the least sold days.  \n   -> Later, we visualize these correlation factors with heatmap. Looking forward to it!\n\n2. HOBBIES are not so day dependent compared to FOODS or HOUSEHOLD.  \n-->","ed81305e":"\uc88b\ub2e4, \ud589\uc0ac \ud0c0\uc785 \ubd84\ud3ec\ub294 \uc704 \uadf8\ub798\ud504\uc640 \uac19\ub2e4.  \n(\ub300\ubd80\ubd84\uc758 \uac12\ub4e4\uc774 \uc0ac\uc2e4 \"unknown\" \uc774\ub098 \uc2dc\uac01\ud654\ub97c \uc704\ud574 \ubd88\uba85\uac12\uc740 \uc81c\uc678\ud588\ub2e4.)\n\n<!--\nOK, event tyoe distributes like the graph above.   \n(Most of the values are actually \"unknown\", but for visualization, I omitted unknown value)\n-->","8a635fe7":"1. \uc774\ub7f0, 2015\ub144\uc5d0, \uc774\uac83\uc740 \uadf9\ubcc0\ud55c \uc9c0\uc810\uc774 \uc788\ub294 \uac83 \uac19\ub2e4.  \n   \uc608\ub97c \ub4e4\uc5b4, 2\uc6d4 \ucbe4\uc5d0, TX_2 \ub294 \uac70\uc758 0\uac1c\uc758 \uc0c1\ud488\uc744 \ud310\ub9e4\ud588\ub2e4. (\ub098\ub294 \uc774 \uc0c1\uc810\uc774 \uc608\uc678\uc801\uc73c\ub85c \ub2eb\uc558\uc744 \uac83\uc774\ub77c \uac00\uc815\ud55c\ub2e4.)\n   \n   -> 2015-03-24 \uc5d0, TX_2\ub294 \uac70\uc758 \uc0c1\ud488\uc744 \ud310\ub9e4\ud558\uc9c0 \uc54a\uc558\ub2e4.\n\n<!--\n1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   \n   -> On 2015-03-24, TX_2 has very little item sold. \n-->","0df56180":"## Sell price and value relationship","25a98866":"\uc774\uc804\uc758 FOOD \uc0c1\ud488 \uacb0\uacfc\uc5d0 \ube44\uad50\ud558\uba74, \uc774 \uc0c1\ud488\uc740 \ud310\ub9e4 \uac00\uaca9\uc758 \ub354 \ub0ae\uc740 \ubcc0\ub3d9\uc131\uc744 \uac16\uace0 \uc788\ub2e4.  \n\uadf8\ub9ac\uace0 \ud310\ub9e4 \uc0c1\ud488\uc758 \uc218 \ub610\ud55c \ub354 \ub0ae\uc740 \ubcc0\ub3d9\uc131\uc774\ub2e4.\n(FOOD \uc0c1\ud488\uc758 \uacbd\uc6b0, \ucd5c\uace0 \uc2dc\uc98c\uc5d0 \ud3c9\uc18c\ubcf4\ub2e4 \uc57d 2.5\ubc30 \ub354 \ub9ce\uc558\ub2e4.)\n\n\ub2e4\uc2dc, 2012\ub144 \ud558\ubc18\uae30\uc5d0 \uc77c\ubd80 \ud310\ub9e4\ub418\uc9c0 \uc54a\uc740 \uae30\uac04\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c\uc558\ub2e4.\n\ubaa8\ub4e0 \uc0c1\ud488\uc774 \ud310\ub9e4\ub418\uc9c0 \uc54a\uc740 \uc774\ub7f0 \uae30\uac04\uc774 \uc788\uc744\uae4c?\n\n<!--\nCompared to the previous FOOD item result, this item has lower volatility of sell price.   \nAnd the number of sold items has also lower volatility.  \n(In the FOOD item, we have around 2.5 times as much as usual sell on the top season)  \n\nAgain, we see some non sold period in the latter half of 2012.  \nDoes all item have these kinds of non-sold period?\n-->","d00f26bc":"> sell_price \uc5d0 ID \uceec\ub7fc \uc0dd\uc131","306dacd5":"1. CA\ub294 3\uac1c\uc758 \uc8fc \uc911\uc5d0\uc11c \uac00\uc7a5 \ub9ce\uc774 \ud314\ub9b0 \uc8fc\uc774\ub2e4.\n   TX\uc640 WI\ub294 2011\ub144\uacfc 2012\ub144\uc744 \uc81c\uc678\ud558\uace4 \uc5c4\uccad \ub2e4\ub974\uc9c0\ub294 \uc54a\ub2e4.\n\n2. \ubaa8\ub4e0 3\uac1c\uc758 \uc8fc\ub294 \uce74\ud14c\uace0\ub9ac \uae30\ubc18\uc758 \ud488\ubaa9 \ud310\ub9e4 \uadf8\ub798\ud504\uc5d0\uc11c \uc774\ubbf8 \ubcf8 \uac83\ucc98\ub7fc \uc77c\ubd80 \uc8fc\uae30\uc801\uc778 \ud2b9\uc9d5\uc774 \uc788\ub2e4.\n\n\uba3c\uc800, CA\uc5d0\uc11c\uc758 \uc0c1\uc810\uc744 \ubcf4\uc790.\n\n<!--\n1. CA is the most sold state of these three states.  \n   TX and WI are not so different except for the year 2011 and 2012.\n  \n2. All three states have some periodical features as we've already seen in category-based item sold graph. \n\nFirst, let's focus on stores in CA.\n-->","5068847e":"(20200528 \ub05d) ","4bea173c":"(20200628) \uc2dc\uc791 (\uac1c\uc120\ub428)"}}