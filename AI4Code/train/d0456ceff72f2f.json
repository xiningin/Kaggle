{"cell_type":{"ccae51f4":"code","aa4b06cb":"code","9397083d":"code","99fdce20":"code","1f656bb3":"code","ccec212a":"code","a52d2dcf":"code","fa34069e":"code","8182ff27":"code","ea9c9c70":"code","009c9b04":"code","49f5e4f1":"code","2f79c207":"code","023a21ae":"code","91fef48c":"code","a624b0ba":"code","0fe3163a":"code","b1c89b86":"code","5ddd0f88":"code","8d3481cd":"code","61031231":"code","cfe83940":"code","cf2a2010":"code","445147d8":"code","b53fd287":"code","5e7606ce":"code","7a2c11b2":"code","065b5f8e":"code","9a6bbfdc":"code","9b8a4c08":"code","a64fa024":"code","3b201571":"code","a2f91e8d":"code","1fb0ef3d":"code","3a842d5c":"code","cfa67847":"code","7c4a8e48":"code","a31e5542":"code","9db7aaf0":"code","1ac4342c":"markdown","aa2d9d5e":"markdown","69f6505a":"markdown","09f3a27f":"markdown","3fe76e50":"markdown","5e05f81b":"markdown","875c1e16":"markdown","0f75a11b":"markdown","fc496c77":"markdown","548e23c7":"markdown","e4182970":"markdown","dc858cec":"markdown","574317b3":"markdown","2b1bac06":"markdown","02b332df":"markdown","a8f08ddb":"markdown","2c023f87":"markdown"},"source":{"ccae51f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa4b06cb":"import pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn import metrics","9397083d":"train_dataset = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntest_dataset = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv\")","99fdce20":"print('Number of data points : ', train_dataset.shape[0])\nprint('Number of features : ', train_dataset.shape[1])\nprint('Features : ', train_dataset.columns.values)\ntrain_dataset.head()","1f656bb3":"print('Number of data points : ', test_dataset.shape[0])\nprint('Number of features : ', test_dataset.shape[1])\nprint('Features : ', test_dataset.columns.values)\ntest_dataset.head()","ccec212a":"train_dataset.isnull().sum()","a52d2dcf":"train_dataset[\"Response\"].value_counts(normalize= True)","fa34069e":"import seaborn as sns\nsns.countplot(x ='Response',data = train_dataset)","8182ff27":"sns.countplot(x ='Gender', hue='Response',data = train_dataset)","ea9c9c70":"sns.FacetGrid(train_dataset,hue=\"Response\",size=8)\\\n   .map(sns.distplot,\"Age\")\\\n   .add_legend()\nplt.show()","009c9b04":"sns.countplot(x ='Response', hue='Previously_Insured',data = train_dataset)","49f5e4f1":"fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\nsns.countplot(ax=axes[0],x ='Vehicle_Damage', hue='Previously_Insured',data = train_dataset)\nsns.countplot(ax=axes[1],x ='Vehicle_Damage', hue='Response',data = train_dataset)","2f79c207":"fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\nsns.countplot(ax=axes[0],x ='Vehicle_Age', hue='Previously_Insured',data = train_dataset)\nsns.countplot(ax=axes[1],x ='Vehicle_Age', hue='Response',data = train_dataset)","023a21ae":"X = train_dataset.groupby([\"Age\"]).mean().reset_index()\nsns.lineplot(x=X['Age'],y=X['Annual_Premium'])\nsns.lineplot(x=X['Age'],y=train_dataset['Annual_Premium'].mean())\n","91fef48c":"\nlabelencoder = LabelEncoder()\ntrain_dataset[\"Gender\"] = labelencoder.fit_transform(train_dataset[\"Gender\"].values)\ntest_dataset[\"Gender\"] = labelencoder.transform(test_dataset[\"Gender\"].values)\ntrain_dataset[\"Vehicle_Damage\"] = labelencoder.fit_transform(train_dataset[\"Vehicle_Damage\"].values)\ntest_dataset[\"Vehicle_Damage\"] = labelencoder.transform(test_dataset[\"Vehicle_Damage\"].values)\ntrain_dataset[\"Vehicle_Age\"] = labelencoder.fit_transform(train_dataset[\"Vehicle_Age\"].values)\ntest_dataset[\"Vehicle_Age\"] = labelencoder.transform(test_dataset[\"Vehicle_Age\"].values)","a624b0ba":"One_Hot_Categorical_features = [\"Gender\",\"Driving_License\",\"Previously_Insured\",\"Vehicle_Age\",\"Vehicle_Damage\"]\ntrain_one_hot_encoding_features = train_dataset[One_Hot_Categorical_features].values\ntest_one_hot_encoding_features= test_dataset[One_Hot_Categorical_features].values \n\nonehotencoder = OneHotEncoder(sparse=False)\ntrain_one_hot_encoded_features = onehotencoder.fit_transform (train_one_hot_encoding_features)\ntest_one_hot_encoded_features = onehotencoder.fit_transform (test_one_hot_encoding_features)","0fe3163a":"print('Region_Code:',train_dataset[\"Region_Code\"].nunique())\nprint('Policy_Sales_Channel:',train_dataset[\"Policy_Sales_Channel\"].nunique())","b1c89b86":"# code for response coding with Laplace smoothing.\n# alpha : used for laplace smoothing\n# feature: ['Region_Code', 'Policy_Sales_Channel']\n\ndef get_response_coded_feature_dict(alpha, feature, df):\n    value_count = df[feature].value_counts()\n    n = df[feature].nunique()\n    feature_dict = dict()\n    for i, denominator in value_count.items():\n        vec = []        \n        for k in range(0,2):\n            cls_cnt = df.loc[(df['Response']==k) & (df[feature]==i)]\n            vec.append((cls_cnt.shape[0] + alpha)\/ (denominator + n*alpha))\n        feature_dict[i]=vec\n    return feature_dict\n\ndef get_response_coded_feature(alpha, feature, train_df,test_df):\n    response_coded_feature_dict = get_response_coded_feature_dict(alpha, feature, train_df)\n    train_value_count = train_df[feature].value_counts()\n    n = train_df[feature].nunique()\n    train_response_coded_feature = []\n    test_response_coded_feature = []\n    for index, row in train_df.iterrows():\n        if row[feature] in dict(train_value_count).keys():\n            train_response_coded_feature.append(response_coded_feature_dict[row[feature]])\n        else:\n            train_response_coded_feature.append([1\/n,1\/n])\n    for index, row in test_df.iterrows():\n        if row[feature] in dict(train_value_count).keys():\n            test_response_coded_feature.append(response_coded_feature_dict[row[feature]])\n        else:\n            test_response_coded_feature.append([1\/n,1\/n])        \n    return train_response_coded_feature,test_response_coded_feature\n\n\n\ntrain_region_code_feature_responseCoding,test_region_code_feature_responseCoding = np.array(get_response_coded_feature(1, \"Region_Code\", train_dataset, test_dataset))\n\ntrain_Policy_Sales_Channel_feature_responseCoding,test_Policy_Sales_Channel_feature_responseCoding = np.array(get_response_coded_feature(1, \"Policy_Sales_Channel\", train_dataset, test_dataset))\n","5ddd0f88":"X = np.hstack((train_one_hot_encoded_features,train_region_code_feature_responseCoding,\n               train_Policy_Sales_Channel_feature_responseCoding,\n               train_dataset[[\"Age\",\"Annual_Premium\",\"Vintage\"]]))\ny = train_dataset[\"Response\"].values","8d3481cd":"minmaxscaler = MinMaxScaler()\nX =minmaxscaler.fit_transform(X)","61031231":"X_train,X_test,y_train,y_test = train_test_split(X ,y ,test_size=0.3, random_state=42)","cfe83940":"def batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","cf2a2010":"def find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"The maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n \ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","445147d8":"from sklearn.naive_bayes import MultinomialNB\ngrid_params ={'alpha':[10**x for x in range(-4,4)]}\nalpha_log = [math.log(x,10) for x in grid_params[\"alpha\"]]\n\nMultinomialNB_model = GridSearchCV(MultinomialNB(),grid_params,\n                     scoring = 'roc_auc', cv=10,n_jobs=-1, return_train_score=True)\nMultinomialNB_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(MultinomialNB_model.cv_results_)\nresults = results.sort_values(['param_alpha'])\n\nplt.plot(alpha_log, results[\"mean_train_score\"], label='Train AUC')\nplt.plot(alpha_log, results[\"mean_test_score\"].values, label='CV AUC')\n\nplt.scatter(alpha_log, results[\"mean_train_score\"].values, label='Train AUC points')\nplt.scatter(alpha_log, results[\"mean_test_score\"].values, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"AUC PLOT\")\nplt.grid()\nplt.show()\nprint(MultinomialNB_model.best_estimator_)","b53fd287":"MultinomialNB_model =MultinomialNB(alpha=0.0001)\nMultinomialNB_model.fit(X_train,y_train)\n\ny_train_pred = batch_predict(MultinomialNB_model,X_train)    \ny_test_pred = batch_predict(MultinomialNB_model,X_test)\ny_pred = MultinomialNB_model.predict(X_test)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds= roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"C:hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"AUC PLOT\")\nplt.grid()\nplt.show()","5e7606ce":"cm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1],columns=[0,1])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","7a2c11b2":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['Response'].astype(str).unique()))","065b5f8e":"grid_params ={'C':[10**x for x in range(-4,4)]}\nc_log = [math.log(x,10) for x in [10**x for x in range(-4,4)]]\n\nLogisticRegression_model = GridSearchCV(LogisticRegression(class_weight = 'balanced'), grid_params,\n                     scoring = 'roc_auc', cv=5,n_jobs=-1,return_train_score=True )\nLogisticRegression_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(LogisticRegression_model.cv_results_)\nresults = results.sort_values(['param_C'])\n\nplt.plot(c_log, results[\"mean_train_score\"], label='Train AUC')\nplt.plot(c_log, results[\"mean_test_score\"].values, label='CV AUC')\n\nplt.scatter(c_log, results[\"mean_train_score\"].values, label='Train AUC points')\nplt.scatter(c_log, results[\"mean_test_score\"].values, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"C: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"AUC PLOT\")\nplt.grid()\nplt.show()\nprint(LogisticRegression_model.best_estimator_)","9a6bbfdc":"LogisticRegression_model = LogisticRegression(C=100, class_weight='balanced')\nLogisticRegression_model.fit(X_train,y_train)\n\ny_train_pred = batch_predict(LogisticRegression_model,X_train)    \ny_test_pred = batch_predict(LogisticRegression_model,X_test)\ny_pred = LogisticRegression_model.predict(X_test)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds= roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"C:hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"AUC PLOT\")\nplt.grid()\nplt.show()\n","9b8a4c08":"cm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1],columns=[0,1])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","a64fa024":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['Response'].astype(str).unique()))","3b201571":"max_depth = [1,5,10,50]\nn_estimators = [5,10,100,500]\ngrid_params ={'max_depth':max_depth,'n_estimators':n_estimators}\n\nRandomFoest_model = GridSearchCV(RandomForestClassifier(class_weight = 'balanced'), grid_params,\n                  scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nRandomFoest_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(RandomFoest_model.cv_results_)\nprint(RandomFoest_model.best_estimator_)","a2f91e8d":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib\n\nmax_depth = [1,1,1,1,5,5,5,5,10,10,10,10,50,50,50,50]\nn_estimators = [5,10,100,500,5,10,100,500,5,10,100,500,5,10,100,500]\nmean_train_score = list(results[\"mean_train_score\"].values)\nmean_test_score = list(results[\"mean_test_score\"].values)\n\nfig = matplotlib.pyplot.figure(figsize=(12,6))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(max_depth, n_estimators, mean_train_score, c='r', marker='o')\nax.scatter(max_depth, n_estimators, mean_test_score, c='b', marker='o')\n\nax.set_xlabel('max_depth ')\nax.set_ylabel('n_estimators')\nax.set_zlabel('roc_auc')\nprint(RandomFoest_model.best_estimator_)","1fb0ef3d":"RandomFoest_model = RandomForestClassifier(class_weight='balanced', max_depth=10, n_estimators=500)\nRandomFoest_model.fit(X_train,y_train)\n\ny_train_pred = batch_predict(RandomFoest_model,X_train)    \ny_test_pred = batch_predict(RandomFoest_model,X_test)\ny_pred = RandomFoest_model.predict(X_test)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds= roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"AUC PLOT\")\nplt.grid()\nplt.show()","3a842d5c":"cm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1],columns=[0,1])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","cfa67847":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['Response'].astype(str).unique()))","7c4a8e48":"test_data = np.hstack((test_one_hot_encoded_features,test_region_code_feature_responseCoding,\n               test_Policy_Sales_Channel_feature_responseCoding,\n               test_dataset[[\"Age\",\"Annual_Premium\",\"Vintage\"]]))","a31e5542":"test_data = minmaxscaler.transform(test_data)","9db7aaf0":"prediction = RandomFoest_model.predict(test_data)\ntest_dataset[\"Response\"] = list(prediction)\nsample_submission = test_dataset[[\"id\",\"Response\"]]","1ac4342c":"The Notebook is organised as follows.\n\n1.Exploratory Data Analysis(EDA)\n\n\n2.Data Preprocessing\n* Label Encoding \n* One Hot Encoding \n* Response Coding\n* Normalization of Data\n\n\n3.Modeling\n* MultinomialNB\n* Logistic Regression\n* Random Forest\n\n\n4.Conclusion","aa2d9d5e":"# Preprocessing","69f6505a":"# Random Forest Classifier","09f3a27f":"From the chart you can see **175000** people was **previously insured** but their current response is **NO**.","3fe76e50":"# Response Encoding","5e05f81b":"# Logistic Regression","875c1e16":"# Label Encoding","0f75a11b":"# Conclusion","fc496c77":"# **Exploratory Data Analysis**","548e23c7":"# Health Insurance Cross Sell Prediction -  Classifiation\nThis notebook aims at building at classification engine to predict whether customers are interested in vehicle insurance or not from the information of customers health insurance  - Binary Classifiation dataset that contains around 391109 data points. Basically, the engine works as follows: after user has provided with customers health insurance information, the engine cleans the data and tries to predict whether customers is interested in vehicle insurance or not.","e4182970":"# Modeling","dc858cec":"# One Hot Encoding","574317b3":"There are 53 unique values in region code and 155 unique values in policy sales channel. If we apply one hot encoding on these features which creates 208 dimensions. Means two dimensions are transformed into 208 dimension. If we apply any ensemble technique like random forest or XGBoost, the base models are decision trees. If 1st split is on eithere region code or policy sales channel, 53 or 155 branches will be created which will increases computational latency. To minimize computation latency using response ecoding technique. ","2b1bac06":"# Naive Bays","02b332df":"The AUC score for Random Forest is better than Naive Bays and Logistics Regression.","a8f08ddb":"People,Age between 30-60 are having vehicle insurance and more in number than age between 18-30. ","2c023f87":"# Applying Random Forest to test data"}}