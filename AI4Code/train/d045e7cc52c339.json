{"cell_type":{"616b9bd5":"code","89684cad":"code","e3cec841":"code","7bc7c0f0":"code","fb4c11e1":"code","d9a5e413":"code","f5498c73":"code","8f1a4bae":"code","21ef64c9":"code","fa620a77":"code","29029f29":"code","2fe7cb53":"code","d29f1476":"code","1e89c03b":"code","6d598fb8":"code","3f61be07":"code","31af2db6":"code","54bdcdab":"code","f2512556":"code","06bbff75":"code","1a1ea086":"code","67579a8e":"code","6d30d394":"code","f4ec14e8":"code","f574467a":"code","a334eedb":"code","98bcf1fc":"code","5fcdef20":"code","1db8a05b":"code","cd8b9c0b":"code","a0f3ea58":"code","191c488e":"code","9f5390a5":"code","306f6b1e":"code","e55f90d1":"code","14415e86":"code","30453e08":"code","4dc72256":"code","c2f0ac92":"code","83dadad0":"code","8898562e":"code","f8b9d792":"code","396587b2":"code","431de5c2":"code","53839855":"code","ccd34baf":"code","3a3437a3":"code","2d8a6739":"markdown","59f4b7a9":"markdown","87df775b":"markdown","418690a0":"markdown","5f87ac26":"markdown","be65e6ca":"markdown","b73479d9":"markdown"},"source":{"616b9bd5":"pip install mne","89684cad":"import mne\r\n# Importing numpy \r\nimport numpy as np\r\n# Importing Scipy \r\nimport scipy as sp\r\n# Importing Pandas Library \r\nimport pandas as pd\r\n# import glob function to scrap files path\r\nfrom glob import glob\r\n# import display() for better visualitions of DataFrames and arrays\r\nfrom IPython.display import display\r\n# import pyplot for plotting\r\nimport matplotlib.pyplot as plt\r\nimport math\r\nfrom skimage.restoration import denoise_wavelet\r\nfrom scipy.signal import savgol_filter\r\nfrom scipy.signal import medfilt\r\nimport seaborn as sns\r\nimport pywt","e3cec841":"d_frame=pd.read_csv('..\/input\/eeg-dataset-collected-from-students-using-vr\/EEG_Dataset\/Subject00_0.csv')        \r\nd_frame.drop(columns='TimeDate',inplace=True)          # converting the raw file to data frame\r\nplt.plot(d_frame['RAW'][0:])           # visualizing the first 10000 values of channel ECG ECG \r\nplt.xlabel('Time')\r\nplt.title('The first three seconds of subject_01 FP1 Channel')\r\nd_frame.head()\r\n","7bc7c0f0":"d_frame['RAW'].hist()\r\nplt.xlabel('Time')\r\nplt.title('The Distribution of the EEG FP1 ')","fb4c11e1":"Raw_data_paths = sorted(glob(\"..\/input\/eeg-dataset-collected-from-students-using-vr\/EEG_Dataset\/*\"))","d9a5e413":"len(Raw_data_paths)","f5498c73":"Raw_data_paths","8f1a4bae":"def create_student_data_dict(Raw_data_paths):\n    \"\"\"\n    This function creates a dictionary that contains students as keys \n    and the eeg data as values.\n    INPUT -----> the sorted list of student' EEG files.\n    OUTPUT ----> A python Dict that contains \n    keys: Students \n    Values: Data\n    \"\"\"\n    raw_dic={}                                   \n    for path_index in range(1,6):\n        key= Raw_data_paths[path_index][-15:-4]     # to extract the subjectxx-x from the files\n        data_frame=pd.read_csv(Raw_data_paths[path_index])\n        data_frame.drop(columns='TimeDate',inplace=True) \n        raw_dic[key]=data_frame\n    return raw_dic\nraw_dic=create_student_data_dict(Raw_data_paths)","21ef64c9":"# raw_dic is a dictionary contains 61 DF\r\nprint('raw_dic contains %d DataFrame' % len(raw_dic))\r\n\r\n# print the first 3 rows of dataframe exp01_user01\r\nraw_dic['Subject00_0']","fa620a77":"number_of_channels= raw_dic['Subject00_0'].shape[1]\r\nnumber_of_channels","29029f29":"names_of_channels=raw_dic['Subject01_1'].columns\r\nnames_of_channels","2fe7cb53":"#band pass filter between 0.5 and 40 hz\r\nfrom scipy.signal import butter, lfilter\r\ndef butter_bandpass(lowcut, highcut, fs, order=5):\r\n    nyq = 0.5 * fs\r\n    low = lowcut \/ nyq\r\n    high = highcut \/ nyq\r\n    b, a = butter(order, [low, high], btype='band')\r\n    return b, a\r\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\r\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\r\n    y = lfilter(b, a, data)\r\n    return y","d29f1476":"from scipy.signal import medfilt # import the median filter function\r\ndef median(signal):# input: numpy array 1D (one column)\r\n    array=np.array(signal)   \r\n    #applying the median filter\r\n    med_filtered=sp.signal.medfilt(array, kernel_size=3) # applying the median filter order3(kernel_size=3)\r\n    return  med_filtered # return the med-filtered signal: numpy array 1D","1e89c03b":"#notch filter apllied at 50hz\r\ndef Implement_Notch_Filter(time, band, freq, ripple, order, filter_type, data):\r\n    from scipy.signal import iirfilter\r\n    fs   = 1\/time\r\n    nyq  = fs\/2.0\r\n    low  = freq - band\/2.0\r\n    high = freq + band\/2.0\r\n    low  = low\/nyq\r\n    high = high\/nyq\r\n    b, a = iirfilter(order, [low, high], rp=ripple, btype='bandstop',\r\n                     analog=False, ftype=filter_type)\r\n    filtered_data = lfilter(b, a, data)\r\n    return filtered_data","6d598fb8":"def SignalProcessing(raw_dic):\n    \n    \"\"\"\n    This function impelments the signal processing pipeline through\n    1- Median Filter \n    2-band pass filter \n    3-wavelet_denoise\n    4-savgol_filter\n    INPUT -------> the raw signals\n    OUTPUT ------> A dictionary that contains the denoised signals \n    \"\"\"\n    time_sig_dic={} # An empty dictionary will contains dataframes of all time domain signals\n    raw_dic_keys=sorted(raw_dic.keys()) # sorting dataframes' keys\n    for key in raw_dic_keys:\n        \n        raw_df=raw_dic[key]\n        time_sig_df=pd.DataFrame()\n        for column in raw_df.columns:\n            \n            t_signal=np.array(raw_df[column]) # copie the signal values in 1D numpy array\n            med_filtred=median(t_signal) # apply 3rd order median filter and store the filtred signal in med_filtred\n            fs = 50\n            lowcut = 0.05\n            highcut = 5\n            band_pass=butter_bandpass_filter(med_filtred, lowcut, highcut, fs, order=5)\n            #notch=Implement_Notch_Filter(0.02, 1, 50, 1, 2, 'butter',band_pass)\n            wavelet_denoise=denoise_wavelet(band_pass,method='BayesShrink',mode='hard',wavelet='sym9',wavelet_levels=5,rescale_sigma=True)\n            clean_signals=savgol_filter(wavelet_denoise, 1111, 3,mode='wrap')\n            time_sig_df[column]=clean_signals\n            time_sig_dic[key]=time_sig_df\n    return time_sig_dic","3f61be07":"time_sig_dic=SignalProcessing(raw_dic)","31af2db6":"########################################################################################\n# Here I wanted to generate a df for the time length                                        \n########################################################################################\ntime_list=[]\nfor i in range (1,6):\n    time=(time_sig_dic[Raw_data_paths[i][-15:-4]].index.values[-1])\n    time_list.append(time)\ndata=np.array(time_list)\ntime_length=pd.DataFrame(data=data,columns=['Signl length'])\ntime_length","54bdcdab":"# example: 679 ==> '00679'; 50 ==> '00050'\r\n\r\n# it add '0's to the left of the input until the new lenght is equal to 5\r\ndef normalize5(number): \r\n    stre=str(number)\r\n    if len(stre)<5:\r\n        l=len(stre)\r\n        for i in range(0,5-l):\r\n            stre=\"0\"+stre\r\n    return stre \r\n\r\n# it add '0's to the left of the input until the new lenght is equal to 2\r\ndef normalize2(number):\r\n    stre=str(number)\r\n    if len(stre)<2:\r\n        stre=\"0\"+stre\r\n    return stre","f2512556":"def Windowing(time_sig_dic):\n    \n    \n    \"\"\"\n    This Function is used to segment the data to small windows through \n    looping over each dataframe's index and store each 4 secs of the data \n    in a single window with a key that has this pattern\n    the window size is calculated as follows:\n    wind_size=(1\/Fs)*required_window_time_length\n    'Subject' + normalize2(int(subject_id))  +  '_' + str(state)\n    INPUT ----> The denoised signal dictionary \n    OUTPUT ---> A dictionary that contains the windows \n    \"\"\"\n    window_dict={} \n    columns=time_sig_dic['Subject02_0'].columns\n    \n    for subject_id , state in zip([0,1,2,3,4],[0,1,0,1,0]):\n        \n        \n    \n        file_key= 'Subject' + normalize2(int(subject_id))  +  '_' + str(state)\n        dic_update=raw_dic[file_key]\n        for sig_time in range(0,len(time_length)):\n            \n            \n            \n            sig_time_length=(time_length['Signl length'][sig_time])\n            window_ID=0\n            for cursor in range(0,(sig_time_length-199),50):\n                end_point=cursor+200\n                data=np.array(dic_update.iloc[cursor:end_point])\n                window=pd.DataFrame(data=data,columns=columns)\n                key='t_W'+normalize5(window_ID)+'_'+file_key\n                window_dict[key]=window\n                wind_dic=window_dict[key]\n                window_ID=window_ID+1\n    return window_dict","06bbff75":"window_dict=Windowing(time_sig_dic)","1a1ea086":"new_frames = {k:v for (k,v) in window_dict.items() if not v.empty}","67579a8e":"len(new_frames)","6d30d394":"sorted(new_frames.keys())[0]","f4ec14e8":"#Fourier transform function \r\nfrom scipy import fftpack # import fftpack to use all fft functions\r\nfrom numpy.fft import *\r\n##################### fast_fourier_transform_one_signal #################\r\n# Inputs: time signal 1D array\r\n# Output: amplitude of fft components 1D array having the same lenght as the Input\r\ndef fast_fourier_transform_one_signal(t_signal):\r\n    # apply fast fourrier transform to the t_signal\r\n    complex_f_signal= fftpack.fft(t_signal)\r\n    #compute the amplitude each complex number\r\n    amplitude_f_signal=np.abs(complex_f_signal)\r\n    # return the amplitude\r\n    return amplitude_f_signal\r\n##################### fast fourier transform for data frames #################\r\ndef fast_fourier_transform(t_window):\r\n  f_window=pd.DataFrame() # create an empty dataframe will include frequency domain signals of window\r\n  for column in t_window.columns: \r\n    t_signal=np.array(t_window[column]) # convert the column to a 1D numpy array\r\n    f_signal= np.apply_along_axis(fast_fourier_transform_one_signal,0,t_signal) # apply the function defined above to the column\r\n    f_window[\"freq_\"+column[0:]]=f_signal # storing the frequency signal in f_window with an appropriate column name\r\n  return f_window # return the frequency domain window","f574467a":"f_window_dict = {'f'+key[1:] : t_w1_df.pipe(fast_fourier_transform) for key, t_w1_df in new_frames.items()}","a334eedb":"fnew_frames = {k:v for (k,v) in f_window_dict.items() if not v.empty}","98bcf1fc":"len(fnew_frames)","5fcdef20":"def tf(t_freq_signal):\n    \n    \"\"\"\n    This Function is to obtian the signals in the time-frequency domian.\n    \n    \"\"\"\n    (cA, cD) = pywt.dwt(t_freq_signal, 'db1')\n    x=np.concatenate((cD,cA),axis=0)\n    return x","1db8a05b":"time_freq_dic={}\ntime_dic_keys=sorted(new_frames.keys())\nfor k in time_dic_keys:\n  time_df=new_frames[k]\n  time_freq_df=pd.DataFrame()\n  for c in time_df.columns:\n    t_freq_signal=np.array(time_df[c])\n    sum_of_coff=np.apply_along_axis(tf,0,t_freq_signal)\n    time_freq_df['time_freq'+c]=sum_of_coff\n    time_freq_dic[k]=time_freq_df","cd8b9c0b":"time_freq_dic['t_W00000_Subject00_0']\ntime_freq_dic['t_W00000_Subject00_0'].columns[0]","a0f3ea58":"t_f_newframes = {k:v for (k,v) in time_freq_dic.items() if not v.empty}","191c488e":"t_f_newframes['t_W00000_Subject00_0']","9f5390a5":"# df is dataframe contains 3 columns (3 axial signals X,Y,Z)\n\n# mean\ndef mean_axial(df):\n    array=np.array(df) # convert dataframe into 2D numpy array for efficiency\n    mean_vector = array.mean(axis=0) # calculate the mean value of each column\n    return mean_vector # return mean vetor\n# std\ndef std_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    std_vector =array.std(axis=0)# calculate the standard deviation value of each column\n                     \n    return std_vector\n\n# mad\nfrom statsmodels.robust import mad as median_deviation # import the median deviation function\ndef mad_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    mad_vector = median_deviation(array,axis=0) # calculate the median deviation value of each column\n    return mad_vector\n\n# max\n\ndef max_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    max_vector=array.max(axis=0)# calculate the max value of each column\n    return max_vector\n# min\ndef min_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    min_vector=array.min(axis=0)# calculate the min value of each column\n    return min_vector\n# IQR\nfrom scipy.stats import iqr as IQR # import interquartile range function (Q3(column)-Q1(column))\ndef IQR_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    IQR_vector=np.apply_along_axis(IQR,0,array)# calculate the inter quartile range value of each column\n    return IQR_vector\n\n\n# Entropy\nfrom scipy.stats import entropy # import the entropy function\ndef entropy_axial(df):\n    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n    entropy_vector=np.apply_along_axis(entropy,0,abs(array))# calculate the entropy value of each column\n    return entropy_vector","306f6b1e":"pip install spectrum","e55f90d1":"# energy\ndef t_energy_axial(df):\n    array=np.array(df)\n    energy_vector=(array**2).sum(axis=0) # energy value of each df column\n    return energy_vector # return energy vector energy_X,energy_Y,energy_Z","14415e86":"#Time Features PipeLine\ndef t_axial_features_generation(t_window):\n  axial_columns=t_window.columns[0]\n  axial_df=t_window[axial_columns]\n  t_axial_features=[]\n  df=axial_df  \n  mean_vector   = list(mean_axial(t_window)) \n  std_vector    = list(std_axial(t_window)) \n  mad_vector    = list(mad_axial(t_window))\n  max_vector    = list(max_axial(t_window))\n  min_vector    = list(min_axial(t_window))\n  energy_vector = list(t_energy_axial(t_window))\n  IQR_vector    = list(IQR_axial(t_window))\n  entropy_vector= list(entropy_axial(t_window))\n  # 40 value per each 3-axial signals\n  t_3axial_vector= mean_vector + std_vector + mad_vector + max_vector + min_vector + energy_vector + IQR_vector + entropy_vector \n  t_axial_features= t_axial_features+ t_3axial_vector\n  return t_axial_features","30453e08":"def time_features_names():\n    # Generating time feature names\n    t_axis_signals=[['EEG '],]\n    # functions' names:\n    t_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n    t_one_input_features_name2=['_energy()','_iqr()','_entropy()']\n    features=[]# Empty list : it will contain all time domain features' names\n    for columns in t_axis_signals: # iterate throw  each group of 3-channels'        \n        for feature in t_one_input_features_name1: # iterate throw the first list of functions names            \n            for column in columns: # iterate throw each axial signal in that group              \n                newcolumn=column+feature # build the feature name\n                features.append(newcolumn) # add it to the global list\n        for feature in t_one_input_features_name2: # same process for the second list of features functions\n            for column in columns:\n                newcolumn=column+feature\n                features.append(newcolumn)         \n    ###########################################################################################################\n    time_list_features=features\n    \n    return time_list_features # return all time domain features' names","4dc72256":"#Frequency Features \ndef f_energy_axial(df):\n    \n    array=np.array(df)\n    \n    # spectral energy vector\n    energy_vector=(array**2).sum(axis=0)\/float(len(array))\n    \n    return energy_vector \n########## Skewness & Kurtosis Functions #######################################\nfrom scipy.stats import kurtosis       # kurtosis function\nfrom scipy.stats import skew           # skewness function\n    \ndef f_skewness_and_kurtosis_axial(df):\n    array=np.array(df)\n    \n    skew_X= skew(array)  \n    kur_X= kurtosis(array)  \n    \n    #skew_Y= skew(array[:,1]) \n    #kur_Y= kurtosis(array[:,1])\n    \n    #skew_Z= skew(array[:,2])\n    #kur_Z= kurtosis(array[:,2])\n    \n    skew_kur_3axial_vector=[skew_X,kur_X]  #,skew_Y,kur_Y,skew_Z,kur_Z] \n    \n    return skew_kur_3axial_vector","c2f0ac92":"#Time_freq features\ndef t_f_skewness_and_kurtosis_axial(df):\n    array=np.array(df)\n    \n    skew_X= skew(array)  \n    kur_X= kurtosis(array)  \n    \n    #skew_Y= skew(array[:,1]) \n    #kur_Y= kurtosis(array[:,1])\n    \n    #skew_Z= skew(array[:,2])\n    #kur_Z= kurtosis(array[:,2])\n    \n    skew_kur_3axial_vector=[skew_X,kur_X]#,skew_Y,kur_Y,skew_Z,kur_Z] \n    \n    return skew_kur_3axial_vector\n","83dadad0":"def f_axial_features_generation(f_window):\n\n\n  axial_columns=f_window.columns[0]\n  axial_df=f_window[axial_columns] \n  f_all_axial_features=[] \n  df=axial_df     \n  mean_vector   = list(mean_axial(f_window)) # 3values\n  std_vector    = list(std_axial(f_window)) # 3 values\n  mad_vector    = list(mad_axial(f_window))# 3 values\n  max_vector    = list(max_axial(f_window))# 3 values\n  min_vector    = list(min_axial(f_window))# 3 values\n  IQR_vector    = list(IQR_axial(f_window))# 3 values\n  entropy_vector= list(entropy_axial(f_window))# 3 values\n  energy_vector                = list(f_energy_axial(f_window))\n  skewness_and_kurtosis_vector = f_skewness_and_kurtosis_axial(f_window)# 6 values\n  f_3axial_features =  mean_vector +std_vector + mad_vector+max_vector+min_vector+  energy_vector + IQR_vector + entropy_vector +  skewness_and_kurtosis_vector  \n  f_all_axial_features = f_all_axial_features+ f_3axial_features # add features to the global list\n  return f_all_axial_features","8898562e":"def t_f_axial_features_generation(t_f_window):\n  axial_columns=t_f_window.columns[0]\n  axial_df=t_f_window[axial_columns] \n  t_f_all_axial_features=[] \n  df=axial_df    \n  mean_vector   = list(mean_axial(t_f_window)) # 3values\n  std_vector    = list(std_axial(t_f_window)) # 3 values\n  mad_vector    = list(mad_axial(t_f_window))# 3 values\n  max_vector    = list(max_axial(t_f_window))# 3 values\n  min_vector    = list(min_axial(t_f_window))# 3 values\n  IQR_vector    = list(IQR_axial(t_f_window))# 3 values\n  entropy_vector= list(entropy_axial(t_f_window))# 3 values\n  #energy_vector                = f_energy_axial(df)\n  skewness_and_kurtosis_vector = t_f_skewness_and_kurtosis_axial(t_f_window)# 6 values\n  f_3axial_features =  mean_vector +std_vector + mad_vector+max_vector+min_vector+  IQR_vector + entropy_vector +  skewness_and_kurtosis_vector  \n  t_f_all_axial_features = t_f_all_axial_features+ f_3axial_features # add features to the global list\n  return t_f_all_axial_features","f8b9d792":"def frequency_features_names():\n    axial_signals=[['EEG ']]\n    # features functions names will be applied to f_signals\n    f_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n    f_one_input_features_name2=['_energy()','_iqr()','_entropy()']\n    f_one_input_features_name3= ['_skewness()','_kurtosis()']    \n    frequency_features_names=[] # global list of frequency features\n    for columns in axial_signals: # iterate throw each group of 3-axial signals\n        # iterate throw the first list of features\n        for feature in f_one_input_features_name1: \n            for column in columns:# iterate throw each signal name of that group\n                newcolumn=column+feature # build the full feature name\n                frequency_features_names.append(newcolumn) # add the feature name to the global list\n        # iterate throw the first list of features\n        for feature in f_one_input_features_name2:\n            for column in columns:\n                newcolumn=column+feature\n                frequency_features_names.append(newcolumn)\n        # iterate throw each signal name of that group\n        for column in columns:\n            for feature in f_one_input_features_name3: # iterate throw [skewness ,kurtosis]\n                newcolumn=column+feature # build full feature name\n                frequency_features_names.append(newcolumn) # append full feature names#\n    return frequency_features_names","396587b2":"def t_frequency_features_names():\n    axial_signals=[['EEG ']]\n    # features functions names will be applied to f_signals\n    f_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n    f_one_input_features_name2=['_iqr()','_entropy()']\n    f_one_input_features_name3= ['_skewness()','_kurtosis()']    \n    time_frequency_features_names=[] # global list of frequency features\n    for columns in axial_signals: # iterate throw each group of 3-axial signals\n        # iterate throw the first list of features\n        for feature in f_one_input_features_name1: \n            for column in columns:# iterate throw each signal name of that group\n                newcolumn=column+feature # build the full feature name\n                time_frequency_features_names.append(newcolumn) # add the feature name to the global list\n        # iterate throw the first list of features\n        for feature in f_one_input_features_name2:\n            for column in columns:\n                newcolumn=column+feature\n                time_frequency_features_names.append(newcolumn)\n        # iterate throw each signal name of that group\n        for column in columns:\n            for feature in f_one_input_features_name3: # iterate throw [skewness ,kurtosis]\n                newcolumn=column+feature # build full feature name\n                time_frequency_features_names.append(newcolumn) # append full feature names#\n    return time_frequency_features_names","431de5c2":"# conctenate all features names lists and we add two other columns state and subject will be related to each row\nall_columns=time_features_names()+frequency_features_names()+t_frequency_features_names()+['state','subject']\ndef Dataset_Generation_PipeLine(t_dic,f_dic,t_f_dic):\n    # t_dic is a dic contains time domain windows\n    final_Dataset=pd.DataFrame(data=[],columns= all_columns) # build an empty dataframe to append rows\n    for i in range(len(t_dic)): # iterate throw each window\n        # t_window and f_window should have the same window id included in their keys\n        t_key=sorted(t_dic.keys() )[i] # extract the key of t_window \n        f_key=sorted(f_dic.keys() )[i] # extract the key of f_window\n        t_f_key=sorted(t_f_dic.keys() )[i]\n        t_window=t_dic[t_key] # extract the t_window\n        f_window=f_dic[f_key] # extract the f_window\n        t_f_window=t_f_dic[t_f_key]\n\n        window_user_id= int(t_key[-4:-2]) # extract the user id from window's key\n        window_activity_id=int(t_key[-1]) # extract the activity id from the windows key\n        # generate all time features from t_window \n        time_features = t_axial_features_generation(t_window) \n        frequency_features = f_axial_features_generation(f_window)\n        time_freq_features=t_f_axial_features_generation(t_f_window)\n        # concatenate all features and append the activity id and the user id\n        row= time_features +frequency_features+time_freq_features+ [int(window_activity_id),int(window_user_id)]\n        # go to the first free index in the dataframe\n        free_index=len(final_Dataset)\n        # append the row\n        final_Dataset.loc[free_index]= row  \n    return final_Dataset # return the final dataset","53839855":"Dataset= Dataset_Generation_PipeLine(new_frames,fnew_frames,t_f_newframes)","ccd34baf":"print('The shape of Dataset is :',Dataset.shape) # shape of the dataset \ndisplay(Dataset.describe()) # statistical description\ndisplay(Dataset.head(10)) # the first three rows","3a3437a3":"\npath=\".\/EEG_Clean_Data.csv\" \nDataset.to_csv(path_or_buf=path, na_rep='NaN',  \n             columns=None, header=True, \n             index=False, mode='w', \n             encoding='utf-8',  \n             line_terminator='\\n', \n             )","2d8a6739":"### Fourier Transform  ","59f4b7a9":"\n## Wavelet Transform ","87df775b":"## Signal Processing \nThe signal processing methods applied in the proposed system are the same as the authors in this [paper](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7278014\/) used. The filters are defined using the [Scipy package](https:\/\/www.scipy.org\/) to remove artifacts.\n\nThe filters used are third-order:\n* `median filter`\n* `low pass filter`\n* `high pass filter`\n* `notch filter`\n\nFor denoising and smoothing:\n\n* `wavelet denoising`\n* `Savitzky\u2013Golay filter`\n\nFirst, the raw signals were filtered using the third-order median filter to remove the background noise. Then, the filtered signals were further filtered using low-pass and high-pass filters. A Butterworth filter with order 5 was used to design these filters at cutoff frequencies of 0.5 HZ and 50 HZ for the low and high filters respectively. The Butterworth filter is a signal processing filter with a frequency response in the passband that is as flat as possible.\n\nSubsequently, a notch filter was applied to reject power interference at 60 HZ. Furthermore, the signals were denoised using a de-noising method based on multilevel wavelet decomposition. The number of wavelet levels was 5, the mother wavelet was Symlets. Lastly, the signals were smoothed using the Savitzky\u2013Golay filter.","418690a0":"### Importing Packages ","5f87ac26":"## Windowing \n\nAfter filtering the signals, the clean signals were segmented using a sliding window. The process involved some steps. First, a 4-second width window was used to loop through the time-domain signal dividing it into small segments for 4 seconds each. The window width was decided based on the literature done. To decide whether the sliding window has to be overlapping or non-overlapping, both methods were tried. The result was that the overlapping method was more efficient and resulted in higher accuracy. We started with a 50% overlap.\n\nThen, we tried different values until the highest accuracy was achieved at a 4-second sliding window and 3-second overlap. The segments after that were converted from the time-domain to the frequency-domain using the Fast Fourier transform. In addition to that, the signals were obtained in the time-frequency domain as stated in the features extracted in the time-frequency domain result in getting a higher classification accuracy. The Wavelet transformation was used in converting the signals to the time-frequency domain.","be65e6ca":" ## Feature Extraction\nThe features extracted from each column in time, frequency, and time-frequency domains were: The mean x\u0304 of a data set is the sum of all the data divided by the count n.\n\n                   (Sum of terms)\/(number of terms )\nThe standard deviation is a statistic that measures the amount of variation or dispersion in a set of numbers. A low standard deviation implies that the values are close to the set's mean, whereas a high standard deviation shows that the values are dispersed across a larger range.\n\n                          \u03c3= \u221a\u2211(X_(i )-\u03bc)\/N\n\u03c3 = population standard deviation\nN = the size of the population\nX_(i ) = each value from the population\n\u03bc = the population mean\nIn statistics, the median absolute deviation is a robust measure of the variability of a univariate sample of quantitative data. It can also refer to the population parameter that is estimated by the MAD calculated from a sample.\n\n                  median absolute deviation=|X_(i )-mean|\nX_i = each value\n\nIn descriptive statistics, the interquartile range, also called the midspread, middle 50%, or H\u2011spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles. In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. In addition to the minimum and maximum value in each column. The entropy, energy, skewness, and kurtosis features that work well in classifying EEG data were also extracted from the data in the three domains.","b73479d9":"## Project Aim\nTo sum up, our project is aiming to provide a practical and low cost solution for the complications that might be found in the real world\u2019s engineering diesel engine laboratories through providing a virtual model that is characterized by the reality of the diesel engine lab, ease of use using only VR headset and input gloves and also provides additional features that would help students achieve maximum output through immersion with the machine and visualize everything that\u2019s happening inside it.\n## Project objectives\nIn order to achieve the project aim, we need to divide our project into three main objectives. First, we have to study the diesel engine lab in details and learn how to design a 3D simulation for the engine. After building the 3D model, the next step is to bring this model into a virtual world in which the student will be able to interact with our model. Now we have the VR world with our model inside. The next step is providing the hardware to be used to ensure the full immersion of the student with the virtual lab. We have decided to use human activity recognition input as a way of convincing the student\u2019s mind? that everything around him is a reality rather than a virtual one to ensure full engagement. Finally, we need to work on the integration of all these parts to provide the best immersive learning experience.\n## The Brain Computer Interface\nModel evaluation is an essential phase in building engineering systems to ensure the quality and efficiency of the system. The proposed model is meant to replace the traditional mechanical labs that contain heave machines with a virtual-based technology that is assumed to be helpful in student\u2019s understanding and interaction with machines. The challenge was how to evaluate the system so that student\u2019s understanding can be indicated. After a lot of research, it\u2019s found that a brain-computer interface (BCI) is the most suitable way to achieve that since it is widely used in applications similar to the one our team was working on.\n\nBCI is a system that uses electrodes to establish direct communication between the brain and an external device such computer or robotic arm. There are two ways to establish a BCI system the invasive and non-invasive. The invasive requires a surgical operation to insert the electrodes under the patient\u2019s skin. On the other hand, the non-invasive does not require that it is implemented by placing the electrodes on one\u2019s scalp. Electroencephalography (EEG) is the most well-known technique in building a non-invasive BCI system that captures the brain's electrical activity. The simplicity of use, mobility, lower cost, and high temporal resolution are the main reasons for EEG popularity and what motivated the team to use it. There are other alternatives, but they are not as efficient as the EEG.\n\nBCI is used in many useful applications such as stress detection, motor imagery tasks, and spinal cord rehabilitation. A typical BCI is used to retrieve data from the brain and send them to an external device to perform a certain task. In the proposed system BCI using EEG is only used to extract the signals and classify them to get specific information.\nFor More information and details about the proposed system, you can read the project thesis from [here](https:\/\/drive.google.com\/file\/d\/1TIZG_nHjvSIpkaF6ljJbaHdDGIdiOCbI\/view?usp=sharing)"}}