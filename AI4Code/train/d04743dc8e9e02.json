{"cell_type":{"8b54899f":"code","d86430c2":"code","7cf50574":"code","58822f6c":"code","9f4a5243":"code","b137f915":"code","aeefcbb5":"code","1165ba05":"code","28e140cf":"code","2b9fd41e":"code","31eb2049":"code","67a4ffe4":"code","7db5ca7c":"code","240fa90a":"code","38226bdd":"code","4f7be0d8":"code","f6783d94":"code","06b72b93":"code","9d81faf5":"markdown"},"source":{"8b54899f":"#Importing \n#Importing a few extra classifiers and stats which I may not use. Those can be removed.\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n#setting sns\nsns.set(style='white', context='notebook', palette='deep')\n\nfrom scipy.stats import mode, norm, skew, kurtosis\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#handle warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#ensuring inline display of plots\nget_ipython().run_line_magic('matplotlib', 'inline')","d86430c2":"#Checking for GPU\n!nvidia-smi","7cf50574":"#read data files\ntrdf=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", header='infer')\ntsdf=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", header='infer')\nsubmission=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv',header='infer')\n\n#Take a glimpse at the data\nprint(trdf.head())\n#tsdf.head()\n\n#trdf.describe()\n#tsdf.describe()\n\n#observe data types and missing values\ntrdf.info()\ntsdf.info()","58822f6c":"#Examine Null Percentages in train attributewise\nper_null_series=(trdf.isnull().sum()\/len(trdf))*100\nprint(type(per_null_series))\nprint(per_null_series)\nsorted_per_null_series=per_null_series.sort_values(ascending=False)\nprint(sorted_per_null_series)\ntemp=pd.DataFrame({\"Missing Ratio in Train\":sorted_per_null_series})\nprint(temp.head())\n\n#Examine null percentages in test attributewise\nper_null_series_test=(tsdf.isnull().sum()\/len(tsdf))*100\nsorted_per_null_series_test=per_null_series_test.sort_values(ascending=False)\ntemp_ts=pd.DataFrame({\"Percentage Missing Value in Test\":sorted_per_null_series_test})\nprint(temp_ts)\n\n#In output, observe that missing percentages are almost same in train and test, so that is good","9f4a5243":"sns.histplot(x=\"Survived\", stat=\"percent\", data=trdf) #This works\n\nper_class0=round(((trdf.loc[:,\"Survived\"]==0).sum()\/len(trdf))*100,2)\nper_class1=round(((trdf.loc[:,\"Survived\"]==1).sum()\/len(trdf))*100,2)\n\nfor i, fr in [(0,per_class0), (0.92,per_class1)]:\n    plt.text(i, fr+0.1, str(fr))","b137f915":"#We have seen the movie, we suspect more females should have been survived. \n#Let us examine percentage of male and female survived\nper_males= ( (trdf.loc[:,\"Sex\"]==\"male\") & (trdf.loc[:,\"Survived\"]==1) ).sum()\/(trdf.loc[:,\"Sex\"]==\"male\").sum()\nper_females=( (trdf.loc[:,\"Sex\"]==\"female\") & (trdf.loc[:,\"Survived\"]==1) ).sum()\/(trdf.loc[:,\"Sex\"]==\"female\").sum()\n\nplt.bar(x=[\"male\", \"female\"], height=[per_males, per_females])\nplt.ylabel(\"Survived(%)\")\n\n\nfor x,per in [(0,per_males), (0.92, per_females)]:\n    plt.text(x,per+0.005, str(round(per,2)))\n\n#Output shows that only 19% males survived while this is almost 74% in females","aeefcbb5":"#Pclass might have played some role in not survival as priority among same gender type may be based on that\n#So let us examine Pclass among the females who did not survive.\n\n#First overall Pclass distribution among females\nax=sns.countplot(x=\"Pclass\", data=trdf.loc[ (trdf.loc[:,\"Sex\"]=='female'), :])\nax.set_ylabel('Pclass counts among females')\n\n#Now Pclass among females who died\nplt.figure()\n\nax=sns.countplot(x=\"Pclass\", data=trdf.loc[ (trdf.loc[:,\"Sex\"]=='female') & (trdf.loc[:,\"Survived\"]==0), :])\nax.set_ylabel('Pclass counts among females who died')\n\ncounts=trdf.loc[ trdf.loc[:,\"Sex\"]=='female' ,[\"Sex\", \"Pclass\"]].groupby(\"Pclass\").count()\n\n#Following is the other way \ncounts=trdf.loc[ trdf.loc[:,\"Sex\"]=='female' ,[\"Sex\", \"Pclass\"]].groupby(\"Pclass\")[\"Sex\"].count()\n#other way ends\n\ntotal_n_females_Pclasswise=counts.tolist()\n\n\nnot_survived_n_females_Pclasswise = trdf.loc[ (trdf.loc[:,\"Sex\"]=='female') & (trdf.loc[:, \"Survived\"]==0) ,[\"Sex\", \"Pclass\"]].groupby(\"Pclass\")[\"Sex\"].count().tolist()\n\nplt.figure()\nheight1=[round(i\/j,2) for i, j in zip(not_survived_n_females_Pclasswise, total_n_females_Pclasswise)]\n\nplt.bar(x=[1,2,3], height=height1)\nplt.xlabel(\"Pclass\")\nplt.ylabel(\"Pclasswise Not Survived Female Percentage\")\n\nfor i, h in zip([1,2,3],height1):\n    plt.text(i,h+0.005,str(h))\n\n#The output clearly shows that the Pclass did matter","1165ba05":"#Let us try some visualization\n\n#count plot Pclasswise survival\nax=sns.countplot(x=\"Pclass\", hue=\"Survived\", data=trdf) #cannot take both x and y\nax.set_ylabel(\"Survival Count\")\n\n#count plots for Pclasswise and Genderwise Survival\nplt.figure()\nsns.catplot(x=\"Pclass\", hue=\"Survived\", col=\"Sex\", data=trdf, kind=\"count\")\n\n#Let us see barplot of Pclasswise and Genderwise Survival\nplt.figure()\nsns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=trdf, kind=\"bar\")\n#Another way for the same thing\nplt.figure()\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=trdf)\n\n#Let us try to see relation between Survival and Age\nplt.figure()\nsns.regplot(x=\"Age\", y=\"Survived\", data=trdf)","28e140cf":"#Impute missing age Salutationwise\n\n#first let us create an attribute Salutation\n\n#For training data\ntrdf[\"Salutation\"]=trdf.loc[:, \"Name\"].str.extract(pat='([a-zA-Z]+)\\.', expand=False)\nprint(trdf[\"Salutation\"].unique())\n\n#For testing data\ntsdf[\"Salutation\"]=tsdf.loc[:, \"Name\"].str.extract(pat='([a-zA-Z]+)\\.', expand=False)\n\n#Let us use below mapping to combine some of the common salutations\nmapping={'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\ntrdf.replace({\"Salutation\":mapping}, inplace=True)\ntsdf.replace({\"Salutation\":mapping}, inplace=True)\nprint(trdf[\"Salutation\"].unique())\n\n#get the meadian age Salutationwise\ntemp=trdf.loc[:,[\"Age\", \"Salutation\"]].groupby(\"Salutation\")[\"Age\"].median()\n\n#do the imputation now\nfor i in range(len(temp.index)):\n    trdf.loc[ trdf.loc[:,\"Salutation\"]==temp.index[i] , \"Age\"]=temp[i]\n    tsdf.loc[ tsdf.loc[:,\"Salutation\"]==temp.index[i] , \"Age\"]=temp[i]","2b9fd41e":"#Construct a new attribute let us name 'GenderPlus' with possible values female, boy and male\n#We suspect that boys will have better chance of survival than adult males\ntrdf[\"GenderPlus\"]=trdf[\"Sex\"]\ntsdf[\"GenderPlus\"]=tsdf[\"Sex\"]\n\ntrdf.loc[ trdf.loc[:,\"Salutation\"]==\"Master\", \"GenderPlus\"]=\"boy\"\ntsdf.loc[ tsdf.loc[:,\"Salutation\"]==\"Master\", \"GenderPlus\"]=\"boy\"","31eb2049":"#Visualize survuval percentage of boys vs adult males\nn_males_survived=trdf.loc[ (trdf.loc[:,\"GenderPlus\"]=='male') & (trdf.loc[:,\"Survived\"]==1) ,\"GenderPlus\"].count()\nn_males=trdf.loc[ trdf.loc[:,\"GenderPlus\"]=='male' ,\"GenderPlus\"].count()\nsurv_perc_of_males= round(n_males_survived\/n_males,2)\nprint(surv_perc_of_males)\n\nn_boys_survived=trdf.loc[ (trdf.loc[:,\"GenderPlus\"]=='boy') & (trdf.loc[:,\"Survived\"]==1) ,\"GenderPlus\"].count()\nn_boys=trdf.loc[ trdf.loc[:,\"GenderPlus\"]=='boy' ,\"GenderPlus\"].count()\nsurv_perc_of_boys= round(n_boys_survived\/n_boys,2)\nprint(surv_perc_of_boys)\n\n#From output it is clear that boys have better chance of being survived than adult males and therefore GenderPlus can be useful","67a4ffe4":"#Construct Family Size, we may use it.\ntrdf[\"Family_Size\"]=trdf[\"SibSp\"]+trdf[\"Parch\"]+1\ntsdf[\"Family_Size\"]=tsdf[\"SibSp\"]+tsdf[\"Parch\"]+1","7db5ca7c":"#Construct FamilySurvivalRate\n#The hypothesis is that if somebody from a family of a passenger under focus is survived then the chances of that passenger\n#survived is also more\n\n#First combine train and test\ndata=pd.concat([trdf,tsdf], axis=0, ignore_index=True)\n\n#Add surname as it can help identify family\n\ndata[\"Surname\"]=data.loc[:,\"Name\"].str.split(pat=\",\").str[0]\n#print(type(data.loc[:,\"Name\"].str.split(pat=\",\")))\n#print(data.loc[:,\"Name\"].str.split(pat=\",\").str[0])\n\n#Add Family_Survival_Rate\n\ndata[\"FamilySurvivalRate\"]=0.5\n\nfor grpid, grpdf in data.groupby(\"Surname\"):\n    if len(grpdf) > 1:\n        for ind, row in grpdf.iterrows():\n            smax=grpdf.drop(ind).loc[:, \"Survived\"].max()\n            smin=grpdf.drop(ind).loc[:, \"Survived\"].min()\n            pid=row[\"PassengerId\"]\n            \n            if smax == 1:\n                data.loc[ data.loc[:,\"PassengerId\"]==pid, \"FamilySurvivalRate\"]=1\n            elif smin==0:\n                data.loc[ data.loc[:,\"PassengerId\"]==pid, \"FamilySurvivalRate\"]=0\n            \n#Same ticket is also an indication of the same family\nfor grpid1, grpdf1 in data.groupby(\"Ticket\"):\n    if len(grpdf1) > 1:\n        for ind, row in grpdf1.iterrows():\n            smax=grpdf1.drop(ind).loc[:, \"Survived\"].max()\n            smin=grpdf1.drop(ind).loc[:, \"Survived\"].min()\n            pid=row[\"PassengerId\"]\n            \n            if smax == 1:\n                data.loc[ data.loc[:,\"PassengerId\"]==pid, \"FamilySurvivalRate\"]=1\n            elif smin==0:\n                data.loc[ data.loc[:,\"PassengerId\"]==pid, \"FamilySurvivalRate\"]=0         \n\n#drop Salutation and Surname as they are no longer required\n#data.drop(columns=[\"Salutation\", \"Surname\"], inplace=True)          \n\n#split back to trdf and tsdf\n\ntrdf=data.loc[0:890,:].copy()\ntsdf=data.loc[891:,:].copy()\n\n#label encoding of GenderPlus and Sex\nlec=LabelEncoder()\nlec.fit(trdf.loc[:,\"GenderPlus\"])\ntrdf.loc[:,\"GenderPlus\"]=lec.transform(trdf.loc[:,\"GenderPlus\"])\ntsdf.loc[:,\"GenderPlus\"]=lec.transform(tsdf.loc[:,\"GenderPlus\"])\n\nlec=LabelEncoder()\nlec.fit(trdf.loc[:,\"Sex\"])\ntrdf.loc[:,\"Sex\"]=lec.transform(trdf.loc[:,\"Sex\"])\ntsdf.loc[:,\"Sex\"]=lec.transform(tsdf.loc[:,\"Sex\"])","240fa90a":"#There is missing value in Fare in test data. Let us impute it first.\ntsdf[\"Fare\"].fillna(trdf[\"Fare\"].mean(), inplace=True)\n\n#Create Fare_Code column but before that create Fare_Bins\ntrdf[\"Fare_Bins\"], bins=pd.qcut(trdf[\"Fare\"], q=5, retbins=True)\n\nlec=LabelEncoder()\nlec.fit(trdf[\"Fare_Bins\"])\ntrdf[\"Fare_Code\"]=lec.transform(trdf[\"Fare_Bins\"])\n\ntsdf[\"Fare_Bins\"]=pd.cut(tsdf[\"Fare\"], bins=bins, include_lowest=True)\n\ntsdf[\"Fare_Code\"]=lec.transform(tsdf[\"Fare_Bins\"])\n","38226bdd":"!pip install pycaret","4f7be0d8":"trdf.loc[:, [\"GenderPlus\", \"Pclass\", \"FamilySurvivalRate\", \"Family_Size\", \"Sex\"]].info()","f6783d94":"#Try pycaret.anomaly for anomaly detection\n\n#import first\nfrom pycaret.anomaly import *\n\n#setup the experiment\nexp_name=setup(data=trdf.loc[:, [\"GenderPlus\", \"Pclass\", \"FamilySurvivalRate\", \"Family_Size\", \"Sex\"]], silent=True, verbose=False)\n\n#Choose the mode for anomlay detection\npca=create_model('pca')\n\n#apply the model to generate Anomaly_Score and Anomaly\npca_df=assign_model(pca, transformation=True)\n\n#check the info of the new dataframe\npca_df.info()\n\n#display anomaly index and convert it to list\nanomaly_index=pca_df.loc[ pca_df[\"Anomaly\"]==1].index.tolist()\n\n#drop examples with anomaly and then check\npca_df.drop(anomaly_index, inplace=True)\npca_df.info()","06b72b93":"#Train model with hyperparameter tuning\n\n#Use only attributes which matter\n#We tried to include Fare_Code but it reduced the accuracy marginally, so we won't use it\nX_train=trdf.loc[:, [\"GenderPlus\", \"Pclass\", \"FamilySurvivalRate\", \"Family_Size\", \"Sex\"]]\ny_train=trdf.loc[:, \"Survived\"]\n\n#removing examples which have anomlay, however, even without removing them we got the same performance\nX_train.drop(anomaly_index, inplace=True)\ny_train.drop(anomaly_index, inplace=True)\n\nX_test=tsdf.loc[:, [\"GenderPlus\", \"Pclass\", \"FamilySurvivalRate\", \"Family_Size\", \"Sex\"]]\n\n#Hyper-parameter Grid for XGBoost\nparam_grid={\n    'max_depth': range (2, 8, 1),\n    'n_estimators': range(10, 100, 10),\n    'learning_rate': [0.1, 0.01, 0.05, 0.001]\n}\n\n#Learn the classifier\nclf=XGBClassifier(eval_metric='logloss')\nclf_gs=GridSearchCV(clf, param_grid,cv=5)\nclf_gs.fit(X_train,y_train)\n\nprint(clf_gs.best_params_)\n\n#make prediction\npredictions=clf_gs.predict(X_test)\n\n'''\nclf=RandomForestClassifier(n_estimators=110, max_depth= 8, max_features='auto',\n                                   random_state=0, oob_score=False, min_samples_split = 2,\n                                   criterion= 'gini', min_samples_leaf=2, bootstrap=False)\nclf.fit(X_train,y_train)\npredictions=clf.predict(X_test)'''\n\n#write prediction in submission.csv\noutput=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv',header='infer')\noutput['Survived']=predictions.astype('int')\noutput.to_csv('submission.csv', index=False)","9d81faf5":"This notebook is to help students beginning with EDA and ML. \nThe basic steps are:\n1. Import the necessary stuff.\n2. Load the data.\n3. Examine null percentages in train and test data attributewise\n4. Examine class distribution, in this case distribution of Survived\n5. From the movie, we can guess that more females should have been survived than males. Examine that.\n6. Pclass (proxy for social status) might have played some role in not survival as priority among same gender type may be based on that. So, Examine Pclass among the females who did not survive.\n7. Let us try to visualize a few relations using seaborn\n8. Create a new feature - Salutation, merge similar Salutations, i.e. replace similar salutations with one and find meadian age Salutationwise. Impute Age Salutationwise (e.g. handle missing value in age Salutationwise).\n9. Construct a new attribute, let us name it 'GenderPlus' with possible values female, boy and male. This is because we suspect that boys might had better chance of survival than adult males.\n10. Visualize the survuval percentage of boys vs adult males to test the above point.\n11. Construct a new feature Family_Size as the sum of SibSp (Sibling and Spouce), Parch (Parents and Children) + 1.\n12. Construct FamilySurvivalRate. The hypothesis is that if somebody from a family of a passenger under focus is survived then the chances of that passenger survived is also more.\n13. LabelEncode GenderPlus and Sex\n14. There is a missing value in Fare, impute it and create Fare_Bins and Fare_Code\n15. Do anomaly detection using pycaret.anomaly\n16. Use only Gender, GenderPlus, FamilySize, FamilySurvivalRate and Pclass attribute in the final training and testing set (We tried others but this set of attributes worked well).\n17. Use XGBClassifier with hyperparameter tuning.\n\nThat is it. \n\nThe notebook is not original and it is inspired from many excellent notebooks available in the public domain on Kaggle. Some of the excellent notebooks from which this notebook has taken clues include:\n\nhttps:\/\/www.kaggle.com\/pliptor\/how-am-i-doing-with-my-score\n\nhttps:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic\n\nhttps:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210\n\nhttps:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83\n\nhttps:\/\/www.kaggle.com\/tomigelo\/titanic-with-family-survival-tpot-0-81818\n\nhttps:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688\n\nI hope the notebook will be helpful. Do Upvote if you find it useful."}}