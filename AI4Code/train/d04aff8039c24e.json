{"cell_type":{"a4d6440b":"code","6d60fce9":"code","ab81c526":"code","d8819ab3":"code","01f74afc":"code","bc8bec01":"code","408a8b0a":"code","d823c25e":"code","b4fa23e7":"code","650e09ec":"code","fcc3fd59":"code","8fa1a339":"code","2eb9fcf5":"code","c230ecc4":"code","9706865d":"code","6c60af12":"code","751ca7bf":"code","f6df40b9":"markdown","d6b642ea":"markdown","cc221edb":"markdown","585e7755":"markdown","6193db1f":"markdown"},"source":{"a4d6440b":"import numpy as np\nimport pandas as pd","6d60fce9":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntrain.head()","ab81c526":"test = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\ntest.head()","d8819ab3":"# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\n\n# axis=1\u65b9\u5411(\u5217\u65b9\u5411)\u306b\u898b\u3066\u3001\"id\"\u3068\"target\"\u306e\u5217\u3092\u9664\u5916\nX = train.drop([\"id\", \"target\"], axis=1)\n\nX.head()","01f74afc":"# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\n\ny = train[\"target\"]\ny.head()","bc8bec01":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n\nX_test = test.drop(\"id\", axis=1)\nX_test.head()","408a8b0a":"# pd.dataframe\u578b\u3060\u3068xgboost lightgmb \u306b\u30c7\u30fc\u30bf\u3092\u7a81\u3063\u8fbc\u3093\u3060\u6642\u3001\u307e\u308c\u306b\u30a8\u30e9\u30fc\u304c\u51fa\u308b\n# np.array\u578b\u306b\u3057\u3066\u304a\u3051\u3070\u7d76\u5bfe\u30a8\u30e9\u30fc\u306f\u51fa\u306a\u3044\u306e\u3067\u3001np.array\u578b\u306b\u5909\u63db\u3057\u3066\u304a\u304f\n\nX = np.array(X)\ny = np.array(y)\n\nX_test = np.array(X_test)","d823c25e":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb","b4fa23e7":"# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306eX, y\u3092\u3055\u3089\u306b\u5b66\u7fd2\u7528\u3068\u7cbe\u5ea6\u691c\u8a3c\u7528\u306b\u5206\u3051\u308b\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=912)\n\nprint(X_train.shape)\nprint(X_val.shape)","650e09ec":"# \u307e\u305a\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\n# First, we set the parameters\n\nparams_xgb = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 8,\n    \"eta\": 0.07,\n    \"random_state\": 234\n}","fcc3fd59":"# \u6b21\u306b\u3001\u30c7\u30fc\u30bf\u3092xgboost\u5c02\u7528\u306e\u30c7\u30fc\u30bf\u578b\u306b\u5909\u63db\u3059\u308b\n# Next, we convert the data to the xgboost-specific data type\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_val = xgb.DMatrix(X_val, label=y_val)\n\nd_test = xgb.DMatrix(X_test)","8fa1a339":"# \u305d\u3057\u3066\u3001\u5b66\u7fd2\u3055\u305b\u308b\u3002\n# and start learning\n\nmodel = xgb.train(params=params_xgb,    # set the parameters, \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30bb\u30c3\u30c8\n                  dtrain=d_train,    # set the train data, \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092\u30bb\u30c3\u30c8\n                  num_boost_round=10000,    # number of DecisionTrees, \u6c7a\u5b9a\u6728\u306e\u672c\u6570\n                  early_stopping_rounds=20,    # early_stopping, \u30a2\u30fc\u30ea\u30fc\u30b9\u30c8\u30c3\u30d4\u30f3\u30b0\u306e\u56de\u6570\n                  verbose_eval=10,    # Display the learning process every 10 times. \u5b66\u7fd2\u904e\u7a0b\u309210\u56de\u3054\u3068\u306b\u8868\u793a\n                  evals=[(d_train, \"train\"), (d_val, \"val\")])    # Set data for evaluation, \u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u3092\u30bb\u30c3\u30c8","2eb9fcf5":"# \u6700\u5f8c\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\n# Finally, make predictions on the test data\n\n# ntree_limit=model.best_ntree_limit <- Use the decision tree with the best accuracy for prediction.\n# ntree_limit=model.best_ntree_limit < -\u6700\u3082\u7cbe\u5ea6\u306e\u826f\u3044\u6c7a\u5b9a\u6728\u3092\u4f7f\u3046\u3002\u3068\u3044\u3046\u610f\u5473\nprediction = model.predict(d_test, ntree_limit=model.best_ntree_limit)","c230ecc4":"prediction","9706865d":"sample_sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\nmy_submission = sample_sub.copy()\nmy_submission","6c60af12":"my_submission[\"target\"] = prediction\nmy_submission","751ca7bf":"my_submission.to_csv(\"submission_xgb.csv\", index=False)","f6df40b9":"\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf  \nX, y  \n  \n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf  \nX_test","d6b642ea":"# XGBoost (Regressor, \u56de\u5e30) ","cc221edb":"# Submission (\u63d0\u51fa)","585e7755":"# Modeling","6193db1f":"objective (\u76ee\u7684\u95a2\u6570)  \n\u30fbreg:squarederror <- for regression, \u56de\u5e30\u7528, \u6700\u5c0f\u4e8c\u4e57\u8aa4\u5dee  \n\u30fbreg:logistic <- for logistic regression, for binary classification, \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u7528, \u4e8c\u5024\u5206\u985e\u7528    \n\u30fbmulti:softmax <- for multi class classification, \u591a\u5024\u5206\u985e\u7528  \n(num_class) <- you have to set \"num_class\" if objective = multi:softmax. \u3082\u3057objective\u306bmulti:softmax\u3092\u9078\u3093\u3060\u6642\u306f\u3001\"num_class\"\u3082\u8a2d\u5b9a\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044  \n  \neval_metric (\u8a55\u4fa1\u6307\u6a19)  \n\u30fbrmse <- for reg, \u56de\u5e30\u7528, \u5e73\u65b9\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee  \n\u30fbmae <- for reg, \u56de\u5e30\u7528, \u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee  \n\u30fblogloss <- for binary classification, \u4e8c\u5024\u5206\u985e\u7528  \n\u30fbmlogloss <- for multi classification, \u591a\u5024\u5206\u985e\u7528  \n  \nmax_depth (\u6728\u306e\u6df1\u3055)  \nusually, 4 ~ 10. \u666e\u901a4~10\u304f\u3089\u3044\u3002  \n  \neta (\u5b66\u7fd2\u7387)  \nusually, 0.05 ~ 0.12. \u666e\u901a0.05 ~ 0.12\u304f\u3089\u3044\u3002"}}