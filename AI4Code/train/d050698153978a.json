{"cell_type":{"76449c45":"code","f068d97c":"code","a975e950":"code","19be8971":"code","6c247469":"code","a78bcdfb":"code","95ffd35e":"code","3066e74e":"code","f86bb76b":"code","47507a07":"code","bfe28213":"markdown","05e2dcbb":"markdown","10a1ff5e":"markdown","f9b04c1a":"markdown","95e98c17":"markdown","5ef4dd1f":"markdown"},"source":{"76449c45":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as ms\nimport warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\nfrom sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom collections import namedtuple\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)","f068d97c":"\ndef reduce_df(df):\n\n    print(f\"orginal dataset :{df.memory_usage().sum() \/ 1024 ** 2} mb\")\n    for i in df.columns:\n        col_type = df[i].dtypes\n\n        if str(col_type)[0:1] in [\"i\", \"f\"]:\n            col_min, col_max = np.min(df[i]), np.max(df[i])\n            if str(col_type)[0:1] == \"i\":\n                for j in [np.int8,np.int16,np.int32, np.int64]:\n                    if col_min > np.iinfo(j).min and col_max < np.iinfo(j).max:\n                        df[i] = df[i].astype(j)\n                        break\n            else:\n                for j in [np.float16,np.float32,np.float64]:\n                    if col_min > np.finfo(j).min and col_max < np.finfo(j).max:\n                        df[i] = df[i].astype(j)\n                        break\n\n    print(f\"dataset reduced to :{df.memory_usage().sum() \/ 1024 ** 2} mb\")\n    print()\n    return df\n","a975e950":"train = reduce_df(pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\").set_index(\"id\"))\ntest_o = reduce_df(pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\"))","19be8971":"y = train[\"target\"]\nx = train.drop(columns = [\"target\"])\ntest = test_o.drop(columns=[\"id\"])\n\n\nfolds = 10\n","6c247469":"\n\ngbdt_para = {'n_jobs':-1, 'n_estimators': 847, 'max_depth': 7, 'learning_rate': 0.04617423130344099, 'lambda_l1': 1.6029632425074436, 'lambda_l2': 0.0010928490115681689, 'num_leaves': 124, 'min_child_samples': 93, 'feature_fraction': 0.7917548593828119, 'bagging_fraction': 0.96375720421119, 'bagging_freq': 3}\ngoss_para = {\"boosting_type\": 'goss','n_jobs':-1,'n_estimators': 888, 'max_depth': 3, 'lambda_l1': 0.045547053858182196, 'lambda_l2': 1.290891976923166, 'num_leaves': 614, 'min_child_samples': 261, 'min_child_weight': 15.811750102552908}\ncat_para = {'colsample_bylevel': 0.05606508594613661, 'depth': 4, 'learning_rate': 0.3840012528742531, 'bootstrap_type': 'Bernoulli', 'subsample': 0.645075461245303}\n\ngbdt_ = LGBMClassifier(**gbdt_para)\ngoss_ = LGBMClassifier(**goss_para)\ncat_ = CatBoostClassifier(**cat_para)\n\n","a78bcdfb":"models_lst = []\nmodels = namedtuple(\"models\", \"ind type fit_\")\nmodels_lst.append(models(ind = 0, type= \"gbdt\", fit_ = gbdt_))\nmodels_lst.append(models(ind = 1, type=\"goss\", fit_=goss_))\nmodels_lst.append(models(ind = 2, type=\"cat\", fit_=cat_))\n\n\ndf_split = ms.StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\ntrain_meta_x = np.zeros((len(train.index), 3))\ntrain_meta_y = np.zeros((len(train.index), 3))\nweights = np.zeros((folds, 3))\nfold_score = np.zeros((folds, 3))\n\n\nfold_pred_cv = np.zeros((len(test.index) , folds))\nfold_pred = np.zeros((len(test.index), 3))\n\nfor m in models_lst:\n\n    start = 0\n    end = 0\n\n    for counter, (trn, val) in enumerate(df_split.split(x, y)):\n\n        end += len(val)\n        mod_ = m.fit_.fit(x.iloc[trn, :].values, y.iloc[trn])\n        meta_pred = mod_.predict_proba(x.iloc[val, :])[:, 1]\n        fold_pred_cv[:, counter] = mod_.predict_proba(test.values)[:, 1]\n        train_meta_x[start:end, m.ind] = meta_pred\n        train_meta_y[start:end, m.ind] = y.iloc[val]\n\n        weights[counter, m.ind] = roc_auc_score(y.iloc[val], meta_pred)\n        fold_score[counter, m.ind] = weights[counter, m.ind]\n        print(counter)\n\n        if counter == folds -1:\n\n            weights[:, m.ind] = weights[:, m.ind]\/np.sum(weights[:, m.ind], axis=0)\n            fold_pred[:,m.ind] = np.dot(fold_pred_cv,weights[:, m.ind])\n        start += len(val)","95ffd35e":"print(fold_score)","3066e74e":"score_weight = np.zeros((folds, 1))\nmeta_pred = np.zeros((len(fold_pred), folds))\nfinal_data = np.zeros((len(fold_pred), 1))\n","f86bb76b":"\n\ndf_split = ms.StratifiedKFold(n_splits=folds, shuffle=True, random_state=45)\nfor counter, (trn, val) in enumerate(df_split.split(train_meta_x, train_meta_y[:,0])):\n    model = SGDClassifier(max_iter=10000, loss='log')\n    model.fit(train_meta_x[trn, :], train_meta_y[trn, 0])\n    meta_pred[:,counter] = model.predict_proba(fold_pred)[:, 1]\n    pred = model.predict_proba(train_meta_x[val, :])[:, 1]\n    score_weight[counter] = roc_auc_score(train_meta_y[val, 0], pred)\n    if counter == folds - 1:\n        score_weight = score_weight \/ np.sum(score_weight, axis=0)\n        final_data = np.dot(meta_pred, score_weight)\n\n","47507a07":"\nfinal = pd.DataFrame(test_o[\"id\"])\nfinal = final.merge(pd.DataFrame(final_data), right_index=True, left_index=True)\n\nfinal.columns = [\"id\", \"target\"]\nfinal.to_csv(\"sub.csv\", index=False)\n","bfe28213":"## Step 2\n\nBelow is the final step. Below the meta data is used with 10 folds to create a weighted array so the folds with a low AUC with have a lower effect than the folds with a higher AUC score.\n","05e2dcbb":"## Step 1\n\nIn this step meta model was generated. Using the 3 algorithms outlined in the introduction. Ten folds were used, and for each fold there was a AUC score recorded. These scores were used after the last fold as weighted averages for each fold. This process was repeated for all 3 models. The meta data was stored into a NumPy array. Also, the prediction for each model was also stored into a NumPy array. \n","10a1ff5e":"## Data Preparation\nThe method below is the primary data preparation step used to compress the original dataset read by pandas. ","f9b04c1a":"## Introduction\nMy approach to this month\u2019s challenge was to use 3 algorithms. I used the GBDT, GOSS, and CatBoost and stack them together. There were 10 folds used where each fold was weighted by the validation AUC score. This output was collected along with the prediction on the test data. This information was then passed into a secondary model ","95e98c17":"## Hyperparameters\nAll hyperparameters were derived using Optuna. I removed the execution because this hyperparameter search took some time to run. ","5ef4dd1f":"## Output"}}