{"cell_type":{"bc16dde4":"code","5b03209d":"code","d3851a52":"code","329dd561":"code","aa5c4356":"code","d2981195":"code","0b2274de":"code","705e8d5c":"code","a023f6af":"code","1f6ec4e0":"code","882b2f80":"code","5e352805":"code","2fd67756":"code","71c897bd":"code","946699ef":"code","e876e057":"code","5f020bca":"code","d5626013":"code","43c35d01":"code","a63d6b6b":"code","c91774b1":"code","b327ef00":"code","399b4dde":"code","4685d875":"code","5d416837":"code","c932bbcd":"code","f9437bc4":"code","c09bd169":"code","622e20b6":"markdown","f9160fbc":"markdown","4081bd7f":"markdown","609285a2":"markdown","6f4f0ebc":"markdown","a522afe6":"markdown","dd69e931":"markdown","ba8ea259":"markdown","c20b7ebd":"markdown","2718c229":"markdown","a9da73c2":"markdown","b93284ec":"markdown","8b020991":"markdown","af6ab894":"markdown","c08ba673":"markdown","90395db0":"markdown","cb94acb6":"markdown","22976a18":"markdown","4efcc2f5":"markdown"},"source":{"bc16dde4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport shutil   \nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import History \nfrom sklearn.metrics import classification_report, confusion_matrix\nimport keras\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","5b03209d":"train = '\/kaggle\/input\/10-monkey-species\/training\/training\/'\nval = '\/kaggle\/input\/10-monkey-species\/validation\/validation\/'\nlabels = pd.read_csv(\"\/kaggle\/input\/10-monkey-species\/monkey_labels.txt\")\nlabels","d3851a52":"# Total number of training images\nnum_of_train_samples = 0\nfor train_dataset in os.listdir(train):\n    in_folder = train + \"\/\" + train_dataset \n    in_folder_list = os.listdir(in_folder)\n    num_of_train_samples = num_of_train_samples + len(in_folder_list)\nprint(\"Number of Training samples   : \",num_of_train_samples)\n\n# Total number of validation images\nnum_of_validation_samples = 0\nfor validation_dataset in os.listdir(val):\n    in_folder_val = val + \"\/\" + validation_dataset\n    in_folder_val_list = os.listdir(in_folder_val)\n    num_of_validation_samples = num_of_validation_samples + len(in_folder_val_list)\nprint(\"Number of Validation samples : \", num_of_validation_samples)","329dd561":"train_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\nval_datagen = ImageDataGenerator(rescale=1. \/ 255)","aa5c4356":"batch_size = 16\nlearning_rate = 0.0001\nepoch = 35\n\n# Defining image width and height respectively\nimg_rows = 256\nimg_cols = 256","d2981195":"train_generator = train_datagen.flow_from_directory(train,\n                                                    target_size = (img_rows, img_cols),\n                                                    batch_size = batch_size,\n                                                    class_mode = 'categorical')\n\nvalidation_generator = val_datagen.flow_from_directory(val,\n                                                        target_size = (img_rows, img_cols),\n                                                        batch_size = batch_size,\n                                                        shuffle = False, class_mode='categorical')","0b2274de":"steps_per_epoch = num_of_train_samples \/\/ batch_size\nprint(\"Steps per epoch: \",steps_per_epoch)","705e8d5c":"from keras.applications import ResNet50\n# The sequential API allows you to create models layer-by-layer\nresnet_model = Sequential()\nresnet_model.add(ResNet50(include_top=False, \n                   pooling='max', \n                   weights='imagenet'))\nresnet_model.add(Dense(10, activation=\"softmax\"))\n\n# Summary: to find the number of parameters\nresnet_model.layers[0].trainable=False\nresnet_model.summary()\n\nsgd = optimizers.SGD(lr=learning_rate, decay=0.00001,momentum = 0.0,nesterov=False)\nresnet_model.compile(loss=\"categorical_crossentropy\",\n              optimizer=sgd,\n              metrics=[\"accuracy\"])","a023f6af":"# Trains the model for a given number of epochs (iterations on a dataset).\nresnet_training = resnet_model.fit_generator(train_generator,\n                               steps_per_epoch = steps_per_epoch,\n                               epochs = epoch,\n                               validation_data = validation_generator,\n                               validation_steps = num_of_validation_samples \/\/ batch_size)","1f6ec4e0":"training_accuracy_resnet      = resnet_training.history['accuracy'][-1]\ntraining_loss_resnet          = resnet_training.history['loss'][-1]\nvalidation_accuracy_resnet    = resnet_training.history['val_accuracy'][-1]\nvalidation_loss_resnet        = resnet_training.history['val_loss'][-1]\nprint(\"Training Accuracy ResNet   :\", training_accuracy_resnet )\nprint(\"Training Loss ResNet       :\", training_loss_resnet)\nprint(\"Validation Accuracy ResNet :\", validation_accuracy_resnet)\nprint(\"Validation Loss ResNet     :\", validation_loss_resnet)","882b2f80":"# Generating Confusion Matrix and Classification Report\nY_pred_res = resnet_model.predict_generator(validation_generator, num_of_validation_samples \/\/ batch_size+1)\ny_pred_res = np.argmax(Y_pred_res, axis=1)\nprint('Confusion Matrix')\nconf_matrix_res = confusion_matrix(validation_generator.classes, y_pred_res)\ncm_res = np.array2string(conf_matrix_res)\nprint(conf_matrix_res)\nprint(\"=============================================================================================\")\nprint('Classification Report')\ntarget_names = ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9']\nclass_rep_res = classification_report(validation_generator.classes, y_pred_res, target_names=target_names)\nprint(class_rep_res)\n","5e352805":"epoch_inc = 30\nlearning_rate_inc = 0.001\nbatch_size_inc = 32","2fd67756":"steps_per_epoch_inc = num_of_train_samples \/\/ batch_size_inc\nprint(\"Steps per epoch: \",steps_per_epoch_inc)","71c897bd":"from keras.applications import InceptionV3\n# The sequential API allows us to create model layer by layer\ninc_model = Sequential()\ninc_model.add(InceptionV3(include_top=False, \n                      pooling='max',\n                      weights='imagenet'))\ninc_model.add(Dense(10, activation=\"softmax\"))\n\n# Summary: to find the number of parameters\ninc_model.layers[0].trainable=False\ninc_model.summary()\n\nadam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.00001)\ninc_model.compile(loss=\"categorical_crossentropy\",\n              optimizer=adam,\n              metrics=[\"accuracy\"])\n","946699ef":"# Trains the model for a given number of epochs (iterations on a dataset).\ninc_training = inc_model.fit_generator(train_generator,\n                                       steps_per_epoch = steps_per_epoch_inc,\n                                       epochs = epoch_inc,\n                                       validation_data = validation_generator,\n                                       validation_steps = num_of_validation_samples \/\/ batch_size_inc)","e876e057":"training_accuracy_inc      = inc_training.history['accuracy'][-1]\ntraining_loss_inc          = inc_training.history['loss'][-1]\nvalidation_accuracy_inc    = inc_training.history['val_accuracy'][-1]\nvalidation_loss_inc        = inc_training.history['val_loss'][-1]\nprint(\"Training Accuracy Inception   :\", training_accuracy_inc )\nprint(\"Training Loss Inception       :\", training_loss_inc)\nprint(\"Validation Accuracy Inception :\", validation_accuracy_inc)\nprint(\"Validation Loss Inception     :\", validation_loss_inc)","5f020bca":"# Generating Confusion Matrix and Classification Report\nY_pred_inc = inc_model.predict_generator(validation_generator, num_of_validation_samples \/\/ batch_size+1)\ny_pred_inc = np.argmax(Y_pred_inc, axis=1)\nprint('Confusion Matrix')\nconf_matrix_inc = confusion_matrix(validation_generator.classes, y_pred_inc)\ncm_inc = np.array2string(conf_matrix_inc)\nprint(conf_matrix_inc)\nprint(\"=============================================================================================\")\nprint('Classification Report')\ntarget_names = ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9']\nclass_rep_inc = classification_report(validation_generator.classes, y_pred_inc, target_names=target_names)\nprint(class_rep_inc)","d5626013":"epoch_vgg = 30\nlearning_rate_vgg = 0.001\nbatch_size_vgg = 64\nsteps_per_epoch_vgg = num_of_train_samples \/\/ batch_size_vgg\nprint(\"Steps per epoch: \",steps_per_epoch_vgg)","43c35d01":"from keras.applications import vgg16\n    # The sequential API allows you to create models layer-by-layer\nvgg_model=Sequential()\nvgg_model.add(vgg16.VGG16(include_top = False, pooling = 'max', weights = 'imagenet'))\nvgg_model.add(Dense(10, activation=\"softmax\"))\n\n    # Summary: to find the number of parameters\nvgg_model.layers[0].trainable=False\nvgg_model.summary() \n\nadam = optimizers.Adam(lr=learning_rate_vgg, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.00001)\nvgg_model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=adam,\n                  metrics=[\"accuracy\"])","a63d6b6b":"# Trains the model for a given number of epochs (iterations on a dataset).\nvgg_training = vgg_model.fit_generator(train_generator,\n                                       steps_per_epoch = steps_per_epoch_vgg,\n                                       epochs = epoch_vgg,\n                                       validation_data = validation_generator,\n                                       validation_steps = num_of_validation_samples \/\/ batch_size_vgg)","c91774b1":"training_accuracy_vgg      = vgg_training.history['accuracy'][-1]\ntraining_loss_vgg          = vgg_training.history['loss'][-1]\nvalidation_accuracy_vgg    = vgg_training.history['val_accuracy'][-1]\nvalidation_loss_vgg        = vgg_training.history['val_loss'][-1]\nprint(\"Training Accuracy VGG    :\", training_accuracy_vgg )\nprint(\"Training Loss VGG        :\", training_loss_vgg)\nprint(\"Validation Accuracy VGG  :\", validation_accuracy_vgg)\nprint(\"Validation Loss VGG      :\", validation_loss_vgg)","b327ef00":"# Generating Confusion Matrix and Classification Report\nY_pred_vgg = vgg_model.predict_generator(validation_generator, num_of_validation_samples \/\/ batch_size+1)\ny_pred_vgg = np.argmax(Y_pred_vgg, axis=1)\nprint('Confusion Matrix')\nconf_matrix_vgg = confusion_matrix(validation_generator.classes, y_pred_vgg)\ncm_vgg = np.array2string(conf_matrix_vgg)\nprint(conf_matrix_vgg)\nprint(\"=============================================================================================\")\nprint('Classification Report')\ntarget_names = ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9']\nclass_rep_vgg = classification_report(validation_generator.classes, y_pred_vgg, target_names=target_names)\nprint(class_rep_vgg)","399b4dde":"epoch_cn = 30\nlearning_rate_cn = 0.001\nbatch_size_cn = 64\nsteps_per_epoch_cn = num_of_train_samples \/\/ batch_size_cn\nprint(\"Steps per epoch: \",steps_per_epoch_cn)","4685d875":"# Custom network\nmodel_cn = Sequential()\nmodel_cn.add(Conv2D(16,(3,3),input_shape=(256,256,3),padding='same'))\nmodel_cn.add(Activation('relu'))\nmodel_cn.add(BatchNormalization())\nmodel_cn.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel_cn.add(Conv2D(32,(3,3),padding='same'))\nmodel_cn.add(Activation('relu'))\nmodel_cn.add(BatchNormalization())\nmodel_cn.add(MaxPooling2D(pool_size=(2,2)))\nmodel_cn.add(Dropout(0.25))\n\n\nmodel_cn.add(Conv2D(32,(3,3),padding='same'))\nmodel_cn.add(Activation('relu'))\nmodel_cn.add(BatchNormalization())\nmodel_cn.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel_cn.add(Conv2D(64,(3,3),padding='same'))\nmodel_cn.add(Activation('relu'))\nmodel_cn.add(BatchNormalization())\nmodel_cn.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel_cn.add(Flatten())\nmodel_cn.add(Dense(256,activation='relu'))\n#model.add(LeakyReLU(0.1))\nmodel_cn.add(Dropout(0.5))\nmodel_cn.add(Dense(10))\nmodel_cn.add(Activation(\"softmax\"))\n\nmodel_cn.summary()\n\n\nmodel_cn.compile(loss=\"categorical_crossentropy\",\n                  optimizer= 'adam',\n                  metrics=[\"accuracy\"])","5d416837":"# Trains the model for a given number of epochs (iterations on a dataset).\ncn_training = model_cn.fit_generator(train_generator,\n                                       steps_per_epoch = steps_per_epoch_cn,\n                                       epochs = epoch_cn,\n                                       validation_data = validation_generator,\n                                       validation_steps = num_of_validation_samples \/\/ batch_size_cn)","c932bbcd":"training_accuracy_cn      = cn_training.history['accuracy'][-1]\ntraining_loss_cn          = cn_training.history['loss'][-1]\nvalidation_accuracy_cn    = cn_training.history['val_accuracy'][-1]\nvalidation_loss_cn        = cn_training.history['val_loss'][-1]\nprint(\"Training Accuracy CN    :\", training_accuracy_cn )\nprint(\"Training Loss CN        :\", training_loss_cn)\nprint(\"Validation Accuracy CN  :\", validation_accuracy_cn)\nprint(\"Validation Loss CN      :\", validation_loss_cn)","f9437bc4":"# Generating Confusion Matrix and Classification Report\nY_pred_cn = model_cn.predict_generator(validation_generator, num_of_validation_samples \/\/ batch_size+1)\ny_pred_cn = np.argmax(Y_pred_cn, axis=1)\nprint('Confusion Matrix')\nconf_matrix_cn = confusion_matrix(validation_generator.classes, y_pred_cn)\ncm_cn = np.array2string(conf_matrix_cn)\nprint(conf_matrix_cn)\nprint(\"=============================================================================================\")\nprint('Classification Report')\n# target_names = ['n0','n1','n2','n3','n4','n5','n6','n7','n8','n9']\nclass_rep_cn = classification_report(validation_generator.classes, y_pred_cn, target_names=target_names)\nprint(class_rep_cn)","c09bd169":"\nmodel_comp = pd.DataFrame({\"Models\": ['ResNet50', 'Inception', 'VGG16', 'Custom Network'],\n                           \"Batch Size\":[16,32,64,64],\n                           \"Epochs\":[45,30,30,30],\n                           \"Learning Rate\": [0.0001,0.001,0.001,0.001],\n                           \"Steps per epoch\":[68,34,17,17],\n                           \"Image Resolution\":['256*256','256*256', '256*256', '256*256'],\n                          \"Training Accuracy\": [training_accuracy_resnet,training_accuracy_inc,training_accuracy_vgg,training_accuracy_cn],\n                          \"Training Loss\": [training_loss_resnet,training_loss_inc,training_loss_vgg,training_loss_cn],\n                          \"Validation Accuracy\": [validation_accuracy_resnet,validation_accuracy_inc,validation_accuracy_vgg,validation_accuracy_cn],\n                          \"Validation Loss\": [validation_loss_resnet,validation_loss_inc,validation_loss_vgg,validation_loss_cn],\n                          })\nmodel_comp","622e20b6":"# Image Generator: \nReal time data augmentation, the data will be  looped over (in batches)","f9160fbc":"Importing required libraries","4081bd7f":"# Saving the model history for ResNet - 50","609285a2":"# Saving the model history for Inception","6f4f0ebc":"# ResNet50","a522afe6":"# Saving the model history for Custom network","dd69e931":"# Generating Confusion Matrix and Classification Report for ResNet 50","ba8ea259":"# Transfer Learning - Comparing losses and accuracies for different standard algorithms, Confusion Matrix and Classification Report. ","c20b7ebd":"# Saving the model history for VGG","2718c229":"# Comparing Accuracies and losses for all the models","a9da73c2":"# Generating Confusion Matrix and Classification Report for Inception","b93284ec":"# Inception","8b020991":"class_mode = 'binary' (2 output classes) ---- if more class prefer class_mode = 'categorical'.\n\nTotal number of steps (batches of samples) before declaring one epoch finished and starting the next epoch","af6ab894":"# Generating Confusion Matrix and Classification Report for Custom Network","c08ba673":"Output:  Post running the below script, output resembles as shown in the below image.  \n![Screenshot%20from%202020-07-30%2019-17-20.png](attachment:Screenshot%20from%202020-07-30%2019-17-20.png)","90395db0":"defining hyperparameters, image resolution and other stuffs...","cb94acb6":"# VGG16","22976a18":"# Generating Confusion Matrix and Classification Report for VGG\n","4efcc2f5":"# Custom Network"}}