{"cell_type":{"e3707920":"code","00b93048":"code","e6b708fc":"code","6e6bf343":"code","4bda8647":"code","5db8b0db":"code","6a390486":"code","9dc5efc6":"code","591970a0":"code","5698fd0d":"code","c713ebea":"code","396c9b6b":"code","4d2c88cc":"code","d0f3d503":"code","0ca3f145":"code","5efe82a4":"code","9962e43f":"code","c936719a":"code","eadbba34":"code","0bcfc7d4":"code","765661c0":"code","25ab4ff5":"code","e10b397f":"code","3e52b86b":"code","e9062287":"code","e3c94616":"code","f8f567a4":"code","68495c9e":"code","af518810":"code","93f0956d":"code","4c4d9ebe":"code","135ae490":"code","5fb63124":"code","cac83d47":"code","e1660bec":"code","749f8767":"code","37a2f0b8":"code","bc73fdb2":"code","da5def2c":"code","ccc522c2":"code","c25aaf35":"code","894b2962":"code","6622f69d":"code","dfd1ea80":"code","daf34902":"code","edf8d802":"code","c321ae46":"code","a7f2cc1a":"code","a1c9ade1":"code","d6c99a83":"code","5a6a3e97":"code","50bda40f":"code","887fd23b":"code","3daf8fe2":"code","e1db68bc":"code","4af4ab4e":"code","42c6cfdd":"code","9c787cec":"code","2c7528ed":"code","3f1abd08":"code","cced7bd9":"code","3a0967d5":"code","2cf99408":"code","f0275477":"code","50652280":"code","3fcb83a6":"code","e0a2ca12":"code","b0fdd434":"code","4a6f06b0":"code","89e638fd":"code","89a4c175":"code","b09c8a65":"code","a51d3216":"code","8fcab448":"code","4fd5747f":"code","e9225ea8":"code","ca9e5a83":"code","d9591d11":"code","1302ff09":"code","27b61dd5":"code","22defa43":"code","d26dcfa3":"code","58c53cde":"code","24ba6bf7":"code","31b6dd7f":"code","6c8ba50a":"code","70ef8936":"code","8e5cc1a3":"code","5f541e6e":"code","c111f476":"code","6788ccb1":"code","888ddda9":"markdown","fc5c6987":"markdown","50009944":"markdown","9a265610":"markdown","8d938315":"markdown","cfc82324":"markdown","6e6b75ce":"markdown","78d21a1b":"markdown","552028d5":"markdown","17f5fa1c":"markdown","9ae23b60":"markdown","89e7838e":"markdown","5aa02353":"markdown","0492611c":"markdown","d6495ca0":"markdown","1d452e89":"markdown","7aee2444":"markdown","b0417999":"markdown","725fe696":"markdown","73d0fa27":"markdown","625e4c9e":"markdown","bfd896a2":"markdown","8840c53f":"markdown","814252bf":"markdown","4e4b9f8e":"markdown","99cce153":"markdown","a72a1222":"markdown","b88e0cad":"markdown","a0c3403f":"markdown","8158cf37":"markdown","f5af1777":"markdown","fa0f7f20":"markdown","f6989dff":"markdown","5d5bf14a":"markdown","1d389367":"markdown","b471d8cc":"markdown","30cf9074":"markdown"},"source":{"e3707920":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\nfrom pandas import ExcelWriter\nfrom math import sqrt\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nplt.style.use('bmh')\n\n\n## Setting plotting configurations\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\npd.options.display.float_format = '{:.2f}'.format\nimport numpy as np\nimport matplotlib.pyplot as plt\n#plt.style.context('seaborn-talk')\nplt.style.use(['tableau-colorblind10'])\n#plt.style.use('fivethirtyeight')\nparams = {'legend.fontsize': '16',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': '20',\n         'axes.titlesize':'30',\n         'xtick.labelsize':'18',\n         'ytick.labelsize':'18'}\nplt.rcParams.update(params)\n\nplt.rcParams['text.color'] = '#A04000'\nplt.rcParams['xtick.color'] = '#283747'\nplt.rcParams['ytick.color'] = '#808000'\nplt.rcParams['axes.labelcolor'] = '#283747'","00b93048":"raw_reviews_df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\") #read from csv file","e6b708fc":"raw_reviews_df.shape #view dimensions of the pandas daframe","6e6bf343":"raw_reviews_df.iloc[0:1] #view one movie review","4bda8647":"raw_reviews_df.info() #vew information abot Daframe in general","5db8b0db":"plt.figure(figsize = (17,9))\nraw_reviews_df[\"sentiment\"].value_counts().plot(kind = 'barh')\nplt.title(\"Distribution of Classes in the Reviews\")\nplt.ylabel(\"Sentiment\")\nplt.xlabel(\"Number of samples\")\nplt.show();","6a390486":"print(\"Total Samples for Positive Sentiments: \",len(raw_reviews_df[raw_reviews_df[\"sentiment\"]==\"positive\"]))\nprint(\"Total Samples for Negative Sentiments: \",len(raw_reviews_df[raw_reviews_df[\"sentiment\"]==\"negative\"]))","9dc5efc6":"print(\"Total Samples which do not have reviews: \",len(raw_reviews_df[raw_reviews_df[\"review\"]==None]))","591970a0":"from PIL import Image\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud #library that allows word cloud to be generated\n\ndef generate_word_cloud(dataframe_df,column_name,img_file=None,background_color=\"black\",max_words=2000):\n    \"\"\" This function generates word cloud\n    \n    Args:\n        dataframe: the datafram object which contains column whose word cloud is to be generated\n        column_name: the name of the column in dataframw whose cloud is to be generated\n        \n    Returns:\n        None\n    \"\"\"\n    #convert the column specified by column_name in dataset_df(dataframe) to list\n    sentences_list = dataframe_df[column_name].tolist()\n    \n    #convert a list to sentence\n    #sentences_single_string = \" \".join(sentences_list)\n    sentences_single_string = \" \".join(map(str,sentences_list))\n    \n    #plot word cloud\n    plt.figure(figsize=(15,15))\n    \n    img_mask = None\n    \n    if img_file != None:\n        img_mask = np.array(Image.open(img_file))\n\n    wc = WordCloud(mask=img_mask,\n                   max_words=max_words,\n                   background_color=background_color)        \n        \n    plt.imshow(wc.generate(sentences_single_string),interpolation=\"bilinear\")\n    plt.title(\"Word Cloud For \" +  column_name)\n    plt.show()","5698fd0d":"import time\ntic= time.time()\ngenerate_word_cloud(raw_reviews_df,\"review\",background_color=\"white\")\ntoc = time.time()\ndiff = 1000*(toc - tic)\nprint(\"Total Time Taken: \" + str(diff) + \" ms\")","c713ebea":"#import required libraries\nfrom nltk.corpus import stopwords #for stopwords\nfrom nltk.stem import PorterStemmer #for word stemming\nfrom nltk.tokenize import TweetTokenizer #for toekinizing string to list of words\nimport string #for punctuation\nimport re #for regular expression\nimport numpy as np","396c9b6b":"def process_string(text):\n    \"\"\"This function returns a processed list of words from the given text\n    \n    This function removes html elements and urls using regular expression, then\n    converts string to list of workds, them find the stem of words in the list of words and\n    finally removes stopwords and punctuation marks from list of words.\n    \n    Args:\n        text(string): The text from which hrml elements, urls, stopwords, punctuation are removed and stemmed\n        \n    Returns:\n        clean_text(string): A text formed after text preprocessing.\n    \"\"\"\n    \n    #remove any urls from the text\n    text = re.sub(r\"https:\\\/\\\/.*[\\r\\n]*\",\"\",text)\n    \n    #remove any urls starting from www. in the text\n    text = re.sub(r\"www\\.\\w*\\.\\w\\w\\w\",\"\",text)\n    \n    #remove any html elements from the text\n    text = re.sub(r\"<[\\w]*[\\s]*\/>\",\"\",text)\n    \n    #remove prediods  marks\n    text = re.sub(r\"[\\.]*\",\"\",text)\n    \n    #initilze tweet tokenizer \n    tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n    \n    #tokenize text\n    text_tokens = tokenizer.tokenize(text)\n    \n    #intizlize porter stemmer\n    porter_stemmer = PorterStemmer()\n    \n    #get english stopwords\n    english_stopwords = stopwords.words(\"english\")\n    \n    cleaned_text_tokens = [] # a list to hold cleaned text tokens\n    \n    for word in text_tokens:\n        if((word not in english_stopwords) and #remove stopwords\n            (word not in string.punctuation)): #remove punctuation marks\n                \n                stemmed_word = porter_stemmer.stem(word) #get stem of the current word\n                cleaned_text_tokens.append(stemmed_word) #appened stemmed word to list of cleaned list\n    \n    #combine list into single string\n    clean_text = \" \".join(cleaned_text_tokens)\n    \n    return clean_text","4d2c88cc":"raw_reviews_df[\"review\"].iloc[0]","d0f3d503":"process_string(raw_reviews_df[\"review\"].iloc[0])","0ca3f145":"raw_reviews_df[\"review\"] = raw_reviews_df[\"review\"].apply(process_string)","5efe82a4":"#save processed reviews for future use\n#save file to csv\nraw_reviews_df.to_csv(\"imdb_reviews_cleaned.csv\")","9962e43f":"raw_reviews_df = pd.read_csv(\"imdb_reviews_cleaned.csv\")","c936719a":"raw_reviews_df.info()","eadbba34":"raw_reviews_df[\"review\"].iloc[0]","0bcfc7d4":"tic = time.time()\ngenerate_word_cloud(raw_reviews_df,\"review\",background_color=\"white\")\ntoc = time.time()\ndiff = toc - tic\nprint(\"Total Time Taken: \" + str(diff) + \" ms\")","765661c0":"#scikit learn library that splits data\nfrom sklearn.model_selection import train_test_split","25ab4ff5":"#first spli data into train and test set\nreview_train, review_test, labels_train, labels_test = train_test_split(raw_reviews_df[\"review\"], raw_reviews_df[\"sentiment\"], test_size=0.1, random_state=0)","e10b397f":"#second split train set into train and valid set\nreview_train, review_valid, labels_train, labels_valid = train_test_split(review_train, labels_train, test_size=0.1111, random_state=0)","3e52b86b":"#see the sample count in all sets\nprint(\"Train Set: \")\nprint(\"Positive Samples count: \" + str(len(review_train[labels_train==\"positive\"] )  ) )\nprint(\"Negative Samples count: \" + str(len(review_train[labels_train==\"negative\"] )  ) )\nprint(\"Total Samples count: \" + str(len(review_train)  ) )","e9062287":"#see the sample count in all sets\nprint(\"Valid Set: \")\nprint(\"Positive Samples count: \" + str(len(review_valid[labels_valid==\"positive\"] )  ) )\nprint(\"Negative Samples count: \" + str(len(review_valid[labels_valid==\"negative\"] )  ) )\nprint(\"Total Samples count: \" + str(len(review_valid)  ) )","e3c94616":"#see the sample count in all sets\nprint(\"Test Set: \")\nprint(\"Positive Samples count: \" + str(len(review_test[labels_test ==\"positive\"] )  ) )\nprint(\"Negative Samples count: \" + str(len(review_test[labels_test ==\"negative\"] )  ) )\nprint(\"Total Samples count: \" + str(len(review_test) ))","f8f567a4":"#convert train matrix from numpy matrix to pandas\nreview_train_df = pd.DataFrame()\nreview_train_df[\"review\"] = review_train\nreview_train_df[\"sentiment\"] = labels_train\nreview_train_df.info()","68495c9e":"#convert valid matrix from numpy matrix to pandas\nreview_valid_df = pd.DataFrame()\nreview_valid_df[\"review\"] = review_valid\nreview_valid_df[\"sentiment\"] = labels_valid\nreview_valid_df.info()","af518810":"#convert test matrix from numpy matrix to pandas\nreview_test_df = pd.DataFrame()\nreview_test_df[\"review\"] = review_test\nreview_test_df[\"sentiment\"] = labels_test\nreview_valid_df.info()","93f0956d":"#make sure the reviews are in string format\nreview_train_df[\"review\"] = review_train_df[\"review\"].apply(lambda row_text: str(row_text))\nreview_valid_df[\"review\"] = review_valid_df[\"review\"].apply(lambda row_text: str(row_text))\nreview_test_df[\"review\"] = review_test_df[\"review\"].apply(lambda row_text: str(row_text))","4c4d9ebe":"from sklearn.feature_extraction.text import CountVectorizer","135ae490":"vectorizer = CountVectorizer() #instantiate vectorizer","5fb63124":"feature_matrix_train = vectorizer.fit_transform(review_train_df[\"review\"].tolist())","cac83d47":"feature_matrix_train.shape","e1660bec":"feature_matrix_valid = vectorizer.transform(review_valid_df[\"review\"].tolist())","749f8767":"feature_matrix_valid.shape","37a2f0b8":"feature_matrix_test = vectorizer.transform(review_test_df[\"review\"].tolist())","bc73fdb2":"feature_matrix_test.shape","da5def2c":"#import label encoder\nfrom sklearn import preprocessing #library that alllows label encoding\nlabel_encoder = preprocessing.LabelEncoder() #instantiate label encoder","ccc522c2":"label_matrix_train = label_encoder.fit_transform(review_train_df[\"sentiment\"]) #encoded","c25aaf35":"label_matrix_train.shape","894b2962":"label_matrix_valid = label_encoder.fit_transform(review_valid_df[\"sentiment\"]) #encoded","6622f69d":"label_matrix_valid.shape","dfd1ea80":"label_matrix_test = label_encoder.fit_transform(review_test_df[\"sentiment\"]) #encoded","daf34902":"label_matrix_test.shape","edf8d802":"dt_train_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_train, columns = vectorizer.get_feature_names())\ndt_valid_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_valid, columns = vectorizer.get_feature_names())\ndt_test_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_test, columns = vectorizer.get_feature_names())","c321ae46":"from matplotlib import pyplot as plt\nimport time\n\ndef plot_top_words(document_term_df,upper_threshold=10000,lower_threshold=10,count=20):\n    \n    #for time purposes\n    tic = time.time()\n    \n    top_word_freq = {}\n    #build frequency dictionary\n    for col in document_term_df.columns:\n        \n        col_count = document_term_df[col].to_numpy().max()\n        \n        if lower_threshold < col_count < upper_threshold:\n            top_word_freq[col] = col_count\n            \n        if len(top_word_freq) > count:\n            break\n\n    #print thresholds\n    print(\"Upper Limit: \" + str(upper_threshold))\n    print(\"Lower Limit: \" + str(lower_threshold))\n    print(\"Requested Count: \" + str(count))\n    \n    plt.figure(figsize=(20,10))\n    \n    y_vals = [top_word_freq[word] for word in top_word_freq.keys()]\n    \n    anotates = list(top_word_freq.keys())\n    \n    x_vals = [i for i in range(0,len(y_vals))]\n    \n    plt.scatter(x_vals,y_vals)\n\n    plt.title(\"Top Word Counts\")\n    plt.xlabel(\"Top Words\")\n    plt.ylabel(\"Word Frequency\")\n    \n    for y,a,x in zip(y_vals,anotates,x_vals):\n        plt.annotate(a, #anotate\n                     (x,y), #for this data point \n                    textcoords=\"offset points\", #how to position text\n                    xytext=(0,10), #distance from text to points (x,y)\n                    ha=\"center\") #horizontal alignment can beleft, right or center \n    \n    #for time purposes\n    toc = time.time()\n    tictoc = (toc-tic)*1000\n    print(\"Time Taken: \" + str(tictoc) + \" ms\")","a7f2cc1a":"plot_top_words(dt_train_df,upper_threshold=10000,lower_threshold=20,count=20)","a1c9ade1":"plot_top_words(dt_train_df[label_matrix_train==0],upper_threshold=10000,lower_threshold=10,count=30)","d6c99a83":"plot_top_words(dt_train_df[label_matrix_train==1],upper_threshold=10000,lower_threshold=10,count=30)","5a6a3e97":"from sklearn.feature_extraction.text import TfidfVectorizer","50bda40f":"tfidf_vectorizer = TfidfVectorizer()","887fd23b":"feature_matrix_train = tfidf_vectorizer.fit_transform(review_train_df[\"review\"].tolist())","3daf8fe2":"feature_matrix_train.shape","e1db68bc":"feature_matrix_valid = tfidf_vectorizer.transform(review_valid_df[\"review\"].tolist())","4af4ab4e":"feature_matrix_valid.shape","42c6cfdd":"feature_matrix_test = tfidf_vectorizer.transform(review_test_df[\"review\"].tolist())","9c787cec":"feature_matrix_test.shape","2c7528ed":"dt_train_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_train, columns = vectorizer.get_feature_names())\ndt_valid_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_valid, columns = vectorizer.get_feature_names())\ndt_test_df = pd.DataFrame.sparse.from_spmatrix(data = feature_matrix_test, columns = vectorizer.get_feature_names())","3f1abd08":"tic = time.time()\n#Train Naive Bayes Model\n#More Details: https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\nfrom sklearn.naive_bayes import BernoulliNB #Gaussain assume fatures from normal distribution, Bernouli for boolean, multionomial for discreeet\nmodel_nb = BernoulliNB()\nmodel_nb.fit(feature_matrix_train, label_matrix_train)\ntoc = time.time()\ny_pred_nb = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_nb.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_nb.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Naive Bayes Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Naive Bayes Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","cced7bd9":"tic = time.time()\n#Train Descision Trees Model\n#More Details: https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\nmaximum_tree_depth= 15\nmodel_dt = DecisionTreeClassifier(max_depth=maximum_tree_depth)\nmodel_dt.fit(feature_matrix_train,label_matrix_train)\ntoc = time.time()\ny_pred_dt = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_dt.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_dt.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Descision Tree Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Descision Tree Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","3a0967d5":"tic = time.time()\n#Train K Nearest Negihbor Model\n#More Details: https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm\n#MOre Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nnumber_of_neigbors = 3\nminkowski_power = 2 # Manhattan Distance = 1, Euclidean Distance = 2\nmodel_knn = KNeighborsClassifier(n_neighbors=number_of_neigbors, p =minkowski_power)\nmodel_knn.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_knn = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_knn.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_knn.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"K Nearest Neighbors Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"K Nearest Neighbors Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","2cf99408":"tic = time.time()\n#Train Logistic Regression Model\n#More Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression #import model for logistic regression\nl2_norm = 0.4 # regularization parameter\nl2_norm_inverse = 1\/l2_norm \nmaximum_iterations=4000 #maximum number of iterations\nmodel_lr = LogisticRegression(C=l2_norm_inverse,max_iter=maximum_iterations) #create logistic regression model\nmodel_lr.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_lr = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_lr.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_lr.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Logistic Regression Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Logistic Regression Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","f0275477":"tic = time.time()\n#Train Support Vector Machine Model\n#More Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html\nfrom sklearn.svm import LinearSVC\nl2_norm = 2.1\nl2_norm_inverse = 1\/l2_norm\nmaximum_iterations=4000 #maximum number of iterations\nmodel_svm = LinearSVC(C=l2_norm_inverse,max_iter=maximum_iterations) #create support vector machine model\nmodel_svm.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_svm = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_svm.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_svm.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Support Vector Machine Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Support Vector Machine Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","50652280":"tic = time.time()\n#Train a MLP classifier\n# Details:https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifierfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import LinearSVC\nl2_norm = 2\nl2_norm_inverse = 1\/l2_norm\nmaximum_iterations=4000 #maximum number of iterations\n \nmodel_bc = BaggingClassifier(base_estimator=LinearSVC(C=l2_norm_inverse,max_iter=maximum_iterations),\n                             n_estimators=30, \n                             random_state=0)\n\nmodel_bc.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_stacked = model_bc.predict(feature_matrix_valid)\naccuracy_train_set = model_bc.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_bc.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Bagging Classifier ML Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Bagging Classifier ML Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","3fcb83a6":"tic = time.time()\n#Create a stack of these estimators\nestimators = [\n    (\"nb\",model_nb), #stack naive bayes\n    (\"knn\",model_knn), #stack K nearest neighor\n    ('svm', model_svm) #stack support vector machine\n]\n\n#Train a stacked model\n# Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html\nfrom sklearn.ensemble import StackingClassifier\nmodel_stacked = StackingClassifier(estimators=estimators, final_estimator=model_lr) #use logisitc regression as the final estimator\nmodel_stacked.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_stacked = model_nb.predict(feature_matrix_valid)\naccuracy_train_set = model_stacked.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_stacked.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Stacked ML Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Stacked ML Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","e0a2ca12":"tic = time.time()\n#Train a Random Forest model\n# Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\nestimators = 100\nforest_depth = 25\nmodel_rf = RandomForestClassifier(n_estimators=estimators, #The number of trees in the forest\n                                  max_depth = forest_depth) #The maximum depth of the tree.\nmodel_rf.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_stacked = model_rf.predict(feature_matrix_valid)\naccuracy_train_set = model_rf.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_rf.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Random Forests ML Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Random Forests ML Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","b0fdd434":"tic = time.time()\n#Train a Random Forest model\n# Details: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import AdaBoostClassifier\nestimators = 100\nmodel_abc = AdaBoostClassifier(n_estimators=estimators) #The number of trees in the forest\nmodel_abc.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_stacked = model_abc.predict(feature_matrix_valid)\naccuracy_train_set = model_abc.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_abc.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Ada Boost CLassifier ML Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Ada Boost CLassifier ML Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","4a6f06b0":"tic = time.time()\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nestimators = 200\ndepth = 25\nmodel_etc = ExtraTreesClassifier(n_estimators = estimators, max_depth = depth)\nmodel_etc.fit(feature_matrix_train,  label_matrix_train)\ntoc = time.time()\ny_pred_stacked = model_etc.predict(feature_matrix_valid)\naccuracy_train_set = model_etc.score(feature_matrix_train, label_matrix_train)  #get accuracy on train set\naccuracy_valid_set = model_etc.score(feature_matrix_valid, label_matrix_valid)  #get accuracy on valid set\nprint(\"Extra Trees Classifer ML Model, Accuracy (Train Set) : \", accuracy_train_set)\nprint(\"Extra Trees Classifer ML Model, Accuracy (Valid Set) : \", accuracy_valid_set)\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","89e638fd":"from sklearn.model_selection import train_test_split #split data into train,test sets\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve,  plot_precision_recall_curve\nfrom sklearn.metrics import precision_score, recall_score\nfrom matplotlib import pyplot as plt","89a4c175":"#make a list of classifers\nml_models_list = [model_nb, #Naive Bayes\n                  model_dt, #Descision Trees\n                 model_knn, #K nearest neighbors\n                 model_lr, #Logistic Regression\n                 model_svm, #Support Vector Machine\n                  model_bc, #bagging classifer (base: descision tree)\n                 model_stacked, #Stacked Model: NB, SVM, KNN, LR\n                  model_rf, #random forestws\n                  model_abc, #adaboost classifier\n                  model_etc #extra trees classifier\n                 ]","b09c8a65":"models = []\naccuracies = []\nprecisions = []\nrecalls = []\nroc_scores = []\nfrom sklearn.metrics import roc_auc_score\ntic = time.time()\nfor model in ml_models_list:\n    accuracy = model.score(feature_matrix_test, label_matrix_test)  #get accuracy\n    y_pred = model.predict(feature_matrix_test) #get predictions\n    model_name = type(model).__name__\n    print(\"Accuracy(\"+ model_name + \"): \", accuracy.round(4)) #display accuracy\n    print(\"Precision(\"+ model_name + \"): \", precision_score(label_matrix_test,y_pred).round(4)) #display precision\n    print(\"Recall(\"+ model_name + \"): \",recall_score(label_matrix_test,y_pred).round(4)) #display recall\n    print(\"ROC Score(\"+ model_name + \"): \",roc_auc_score(label_matrix_test,y_pred).round(4)) #display recall\n    \n    print(\"\\n\")\n    models.append(model_name)\n    accuracies.append(accuracy.round(4))\n    precisions.append(precision_score(label_matrix_test,y_pred).round(4))\n    recalls.append(recall_score(label_matrix_test,y_pred).round(4))\n    roc_scores.append(roc_auc_score(label_matrix_test,y_pred).round(4))\ntoc = time.time()\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","a51d3216":"tic = time.time()\n#Plot Confusion Matrix For all Models\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(17,20))\n\nfor model,ax in zip(ml_models_list, axes.flatten()):\n    \n    plot_confusion_matrix(model, \n                          feature_matrix_test, \n                          label_matrix_test, \n                          ax=ax, \n                          cmap='Blues',\n                         display_labels=[\"negative\",\"positive\"])\n    \n    ax.title.set_text(type(model).__name__)\nplt.tight_layout()  \nplt.show()\ntoc = time.time()\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","8fcab448":"tic = time.time()\n#Plot ROC Curve For all Models\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(17,30))\n\nfor model, ax in zip(ml_models_list,axes.flatten()):\n    \n    plot_roc_curve(model,feature_matrix_test,label_matrix_test,ax=ax)\n    \n    ax.title.set_text(type(model).__name__)\n\nplt.tight_layout()  \nplt.show()\ntoc = time.time()\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","4fd5747f":"tic = time.time()\n#Plot Precision Recall CUrve FOr ALl Models\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(17,20))\n\nfor model, ax in zip(ml_models_list,axes.flatten()):\n    \n    plot_precision_recall_curve(model,feature_matrix_test,label_matrix_test,ax=ax)\n    \n    ax.title.set_text(type(model).__name__)\n\nplt.tight_layout()  \nplt.show()\ntoc = time.time()\nprint(\"Time Taken: \" + str(((toc-tic)*1000)) + \" ms\")","e9225ea8":"results = pd.DataFrame({\"Model\" : models, \"Accuracy\" : accuracies, \"Precision\" : precisions, \"Recall\" : recalls, 'ROC Score' :roc_scores})\nresults","ca9e5a83":"results.set_index('Model', inplace=True)\nresults","d9591d11":"plt.figure(figsize = (17,9))\nresults[\"Accuracy\"].plot(kind = 'barh')\nplt.title(\"Comparison of Accuracies of the Models\")\nplt.ylabel(\"Models\")\nplt.xlim(0.70, 1.0)\nplt.xlabel(\"Accuracy Score\")\nplt.show();","1302ff09":"plt.figure(figsize = (17,9))\nresults[\"Precision\"].plot(kind = 'barh', color = 'red')\nplt.title(\"Comparison of Precision of the Models\")\nplt.ylabel(\"Models\")\nplt.xlim(0.50, 1.0)\nplt.xlabel(\"Precision Score\")\nplt.show();","27b61dd5":"plt.figure(figsize = (17,9))\nresults[\"Recall\"].plot(kind = 'barh', color = 'blue')\nplt.title(\"Comparison of Recall of the Models\")\nplt.ylabel(\"Models\")\nplt.xlim(0.70, 1.0)\nplt.xlabel(\"Recall Score\")\nplt.show();","22defa43":"plt.figure(figsize = (17,9))\nresults[\"ROC Score\"].plot(kind = 'barh', color = 'cyan')\nplt.title(\"Comparison of ROC Score of the Models\")\nplt.ylabel(\"Models\")\nplt.xlim(0.70, 1.0)\nplt.xlabel(\"ROC score\")\nplt.show();","d26dcfa3":"plt.figure(figsize = (17,9))\nresults.plot(kind = 'bar')\nplt.title(\"Comparison of ROC Score of the Models\")\nplt.ylabel(\"Models\")\nplt.ylim(0.70, 1.0)\nplt.xlabel(\"ROC score\")\nplt.show();","58c53cde":"from textblob import TextBlob\n\ndef generate_rating(ml_models,movie_review,preprocessed=False):\n    \n    processed_text = \"\"\n    \n    if not preprocessed:\n        #pre process review\n        processed_text = [process_string(movie_review)]\n\n        print(\"PreProcessed Review: \")\n        print(processed_text)\n        print(\"\")\n        #make sure the text in string\n        #processed_text = [str(processed_text)]\n    else:\n        processed_text = [movie_review]\n    \n    #generate feature matrix for the review using TF-IDF Scheme\n    feature_matrix = tfidf_vectorizer.transform(processed_text)\n    \n    #container to store predictions\n    predictions = []\n    model_names = []\n    \n    #get predictions from all 10 classifiers\n    for model in ml_models:\n        \n        #get model name\n        model_names.append(model.__class__.__name__)\n        \n        #get prediction from\n        predictions.append(model.predict(feature_matrix))\n\n    \n    print(\"Predictions By Classifiers: \\n\")\n    for model, prediction in zip(model_names,predictions):\n        print(model,\":\", prediction)\n    \n    #convert predictions from llist to numpy array\n    predictions = np.array(predictions)\n    \n    #sum predictions from all models to get rating\n    rating = np.sum(predictions)\n    \n    print(\"\")\n    print(\"Movie Rating(0-10): \" + str(rating))\n    \n    print(\"\")\n    \n    try:\n        #The polarity score is a float within the range [-1.0, 1.0].\n        pol =  TextBlob(movie_review).sentiment.polarity\n    except:\n        pol = -999999\n        \n    print(\"TextBlob Sentiment Polarity(UnPreprocessed): \" + str(pol))","24ba6bf7":"review_test_df.info()","31b6dd7f":"r_num = 50\nreview_test_df.iloc[r_num]\n#web: https:\/\/www.imdb.com\/title\/tt1635327\/reviews?ref_=tt_urv","6c8ba50a":"custom_review = \"I dont know how i feel about this movie. Guess i dont care https:\/\/www.imdb.com\/title\/tt1635327\/reviews?ref_=tt_urv\"\ncustom_review","70ef8936":"generate_rating(ml_models_list,custom_review)","8e5cc1a3":"custom_review = \"I dont know how i feel about this movie. www.google.com www.imdb.com Guess i dont care \"\ncustom_review","5f541e6e":"generate_rating(ml_models_list,custom_review)","c111f476":"custom_review = \"\"\"I have honestly never felt so confused after watching a film, a lot of people saying watch it twice but I couldn't put myself through that again. The idea is good.. but they have made it far too complicated, a bit pretentious and unfortunately no real heart to any of the characters. Disappointed.\"\"\"\ncustom_review","6788ccb1":"generate_rating(ml_models_list,custom_review)","888ddda9":"<p> <h5> \nClearly the new text preprocessing method resulted in much more cleaner reviews, the prominent \"br\" tag that has massive occurence is not visible anymore. Thanks to regular epression. Lets further examine these word could by looking into positive only and negative only reviews <\/h5> <\/p>","fc5c6987":"<p><h5> \nTerm-Doument Matrix is a poular matrix design in NLP. Lets create a term document matrix\n<\/h5><\/p>","50009944":"<p> <h5> Lets Look at the top words used in trainning set <\/h5> <\/p>","9a265610":"<p> \n<h4>\n    Classifer 10:-ExtraTreesClassifier (Base Estimator = Descision Tree)\n<\/h4>\n<\/p>","8d938315":"<p> <h4> Classifer 4:- Logistic Regression <\/h4> <\/p>","cfc82324":"### Task 4: Split Data into Train and Test Set","6e6b75ce":"### Task 5: Generate Features from reviews And Numerical labels for Sentiment","78d21a1b":"### ANALYSIS OF MOVIE REVIEW From IMDB\n##### USING  Classical Machine Learning Models and Ensemble of Classical Machine Learning Models ","552028d5":"## Task2: Preprocess Data","17f5fa1c":"<p> <h4> Classifer 5:- Support Vector Machine <\/h4> <\/p>","9ae23b60":"<p> <h5> Lets Look at the top words used in trainning set whose sentiment is labelled as negative <\/h5> <\/p>","89e7838e":"Looks like the classes(sentiments) are balanced perfectly. SO need for sampling. Now check if there are any missing values in review column.","5aa02353":"### Task 8: Train and Test Machine Learning Models","0492611c":"## Task1: Load Data","d6495ca0":"<p> <h5> Lets Look at the top words used in training set whose sentiment is labelled as negative <\/h5> <\/p>","1d452e89":"### Task 9: Plot Evaluation Metrics for Machine Learning Models","7aee2444":"In this task, data preprocessing tasks such as adjust class imbalances , filling out missing values are done. This is done to prevent bias later for Classification Task","b0417999":"<p> <h5>\nLets look at reviews after they have been cleaned\n<\/h5> <\/p>","725fe696":"### Task 10: Summarize Performace of Machine Learning Models","73d0fa27":"Lets Plot word cloud for all the reviews","625e4c9e":"<p> <h5> The review have been saved and loaded.<\/h5> <\/p>","bfd896a2":"<p> \n<h4>\n    Classifer 9:- AdaBoostClassifier (Base Estimator = Descision Tree)\n<\/h4>\n\n<\/p>","8840c53f":"### Task 3: Text Preprocessing","814252bf":"<h3> \n     Author: Sushma G.\n<\/h3>","4e4b9f8e":"Awesome, no missing reviews so not need for data imputation. Now lets view frequenly used words in reviews in a word cloud","99cce153":"<p> <h5>\nThe data set has to split into train(80%)\/valid(10%)\/test(10%) set. Train set will be used to train machine learning models and validation set will be used to tune the machine learning models and the test will be used to evaluate the performance of ml models. <br> <br>\nThere are two classes (\"positive\" and \"negative\") in the dataset. A blanced dataset containing equal number of samples from both classes.\n<\/h5> <\/p>","a72a1222":"<p> <h5> clearly this cleaned review is free of html tags as well now. Lets apply this new text cleaning method to all 50k reviews <\/h5> <\/p>","b88e0cad":"<p> <h4> Classifer 1:- Naive Bayes <\/h4> <\/p>","a0c3403f":"<p> <h4> Classifer 2:- Descision Tree <\/h4> <\/p>","8158cf37":"<p> \n<h4>\n    Classifer 7:- Stacked ML CLassifer (Naive Bayes, K Nearest Neighbor, Support Vector Machine)\n<\/h4>\n<\/p>","f5af1777":"<p> <h5> The plot above shows frequnecy some top words used in the reviews. Now lets crea pipeline to do this task <\/h5> <\/p>","fa0f7f20":"### Task 11: Movie Rating Generator","f6989dff":"<p> \n<h4>\n    Classifer 8:- Random Forests (Base Estimator = Descision Tree)\n<\/h4>\n<\/p>","5d5bf14a":"### Task 7: Re-weighing the document term matrix using TF-IDF Scheme","1d389367":"<p> <h4> Classifer 6:- Bagging Classifier (Base Estimator = LinearSVC) <\/h4> <\/p>","b471d8cc":"<p> <h5>\nThe sentiment for each review is labelled as positive or negative. Machine Learning Models require Numberical values so lets convert these labels to numerical labels\n<\/h5> <\/p>","30cf9074":"<p> <h4> Classifer 3:- K Nearest Neighbor <\/h4> <\/p>"}}