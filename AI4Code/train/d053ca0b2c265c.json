{"cell_type":{"16076fba":"code","24ceaf93":"code","daf9c532":"code","3cf1e394":"code","b80482f8":"code","0705ff7e":"code","b91b7d86":"code","df2175a3":"code","0d13e144":"code","7ea58ac3":"code","9fe69392":"code","3a607d58":"code","5f2be101":"code","d2927c65":"code","87a20398":"code","7f9ffb91":"code","733e73dc":"code","d59f2b6b":"code","83d80d53":"markdown","bed05bbd":"markdown","624834b3":"markdown"},"source":{"16076fba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk \nfrom nltk import word_tokenize\n\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom sklearn import svm\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nfrom sklearn.linear_model import LogisticRegression\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","24ceaf93":"df = pd.read_csv('..\/input\/all.csv') #load dataset","daf9c532":"df.head() #getting top 5 head","3cf1e394":"df.shape #getting shape","b80482f8":"df.info() #info of dataset","0705ff7e":"df.isnull().sum() #checking again null vaules","b91b7d86":"df.groupby('type').count()","df2175a3":"#looking in content\ndf['content']","0d13e144":"\n\n\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='orange',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=15\n                         ).generate(str(df[df['type']=='Mythology & Folklore']['content']))\n\nfig = plt.figure(1,figsize=(12,18))\nplt.imshow(wordcloud)\nplt.title('Mythology & Folklore')\nplt.axis('off')\nplt.show()","7ea58ac3":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='orange',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=15\n                         ).generate(str(df[df['type']=='Love']['content']))\n\nfig = plt.figure(1,figsize=(12,18))\nplt.title('Love')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","9fe69392":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='orange',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=15\n                         ).generate(str(df[df['type']=='Nature']['content']))\n\nfig = plt.figure(1,figsize=(12,18))\nplt.title('Nature')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","3a607d58":"labels = 'Love', 'Mythology & Folklore', 'Nature'\nsizes = [326, 59, 188]\n\n\nfig1, ax1 = plt.subplots(figsize=(5,5))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","5f2be101":"#function to remove ounctuation\ndef removePunctuation(x):\n    x = x.lower()\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    x = x.replace('\\r','')\n    x = x.replace('\\n','')\n    x = x.replace('  ','')\n    x = x.replace('\\'','')\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n\n\n#getting stop words\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\")) \n\n\n#function to remove stopwords\ndef removeStopwords(x):\n    filtered_words = [word for word in x.split() if word not in stops]\n    return \" \".join(filtered_words)\n\n\ndef processText(x):\n    x= removePunctuation(x)\n    x= removeStopwords(x)\n    return x\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nX= pd.Series([word_tokenize(processText(x)) for x in df['content']])\nX.head()","d2927c65":"#vectorizing X and y to process\nvectorize=CountVectorizer(max_df=0.95, min_df=0.005)\nX=vectorize.fit_transform(df['content'], df['author'])\nvect = CountVectorizer(tokenizer = lambda x: x.split(), binary = 'true')\ny = vect.fit_transform(df.type)\n","87a20398":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)\n\n\n","7f9ffb91":"#sgd classifier\n\nclassifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\n\nprint(\"accuracy :\",classifier.score(X_train, y_train))\nprint(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\nprint(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\nprint(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\nprint(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))","733e73dc":"#logistic regression\n\nclassifier = OneVsRestClassifier(LogisticRegression(penalty='l1'), n_jobs=-1)\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\n\nprint(\"accuracy :\",classifier.score(X_train, y_train))\nprint(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\nprint(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\nprint(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\nprint(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))","d59f2b6b":"#linear svc classifier\nfrom sklearn.svm import LinearSVC\n\nclassifier = OneVsRestClassifier(LinearSVC(random_state=0, tol=1e-5), n_jobs=-1)\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\n\nprint(\"accuracy :\",classifier.score(X_train, y_train))\nprint(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\nprint(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\nprint(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\nprint(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))","83d80d53":"1. In our dataset we have html tag. we have to remove it first","bed05bbd":"**Micro-averaging & Macro-averaging** \nTo measure a multi-class classifier we have to average out the classes somehow. There are two different methods of doing this called micro-averaging and macro-averaging.   \n\n  \nIn **micro-averaging** all TPs, TNs, FPs and FNs for each class are summed up and then the average is taken.      \n**Macro-averaging** is straight forward. We just take the average of the precision and recall of the system on different sets.   \n\n **Hamming-Loss** is the fraction of labels that are incorrectly predicted","624834b3":"2 values are missing in poem name"}}