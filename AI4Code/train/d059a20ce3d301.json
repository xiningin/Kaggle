{"cell_type":{"32ca7b6a":"code","ad835f1f":"code","0cd9e78f":"code","ed53de68":"code","d2f40aa0":"code","23b9d734":"code","6298fea6":"code","705fd84c":"code","63a5bf1e":"code","43996535":"code","3f5066a4":"code","893a1926":"code","d1a2af53":"code","b32f3395":"code","8657f912":"code","81fd1d98":"code","765d3bfe":"code","20dbe36c":"code","801b88c1":"code","d0cd483d":"code","2f68cdc1":"code","748212ce":"code","a74e7875":"code","1f712268":"code","6e4e56d9":"code","5608be13":"code","53c30e13":"code","aa868238":"code","bd12b7d0":"code","9049f94f":"code","7ab89c32":"code","7d38f538":"code","98dfc4d4":"markdown"},"source":{"32ca7b6a":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence,text\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation,Flatten\nfrom keras.layers import  Embedding, SimpleRNN, LSTM,Masking,Bidirectional\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras import metrics, regularizers\nfrom keras.optimizers import RMSprop\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ad835f1f":"df = pd.read_excel(\"\/kaggle\/input\/shared-tweets-for-turkish-gsm-operators\/gsm-tweets.xlsx\",dtype=str)","0cd9e78f":"df.describe()","ed53de68":"print(df.head())\nlen(df)","d2f40aa0":"df = df.apply(lambda x: x.astype(str).str.lower())\ndf.head()","23b9d734":"df.isnull().any()","6298fea6":"df['tweet'][1294]","705fd84c":"df['tweet']=df.tweet.str.replace(r'http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.& +]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',regex=True)\ndf['tweet']=df.tweet.str.replace(r'\\@\\w*\\b','',regex=True)\ndf['tweet']=df.tweet.str.replace(r'\\b\\d+','',regex=True)\ndf['tweet']=df.tweet.str.replace(r'\\W*\\b\\w{1,2}\\b','',regex=True)\ndf['tweet']=df['tweet'].str.findall('\\w{2,}').str.join(' ')","63a5bf1e":"df['tweet'][1294]","43996535":"stop_words = stopwords.words('turkish')    \nstop_words","3f5066a4":"df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","893a1926":"df = shuffle(df)\nlen(df)","d1a2af53":"df[df.tweet.duplicated()].count()","b32f3395":"df = df.drop_duplicates()\nprint(len(df))\ndf.reset_index(drop=True, inplace=True)","8657f912":"df.tags.value_counts()","81fd1d98":"le = LabelEncoder()\ntags = le.fit_transform(df.tags)","765d3bfe":"# Kelime numalarand\u0131rmada en b\u00fcy\u00fck say\u0131\nnum_max = 10000\nmax_len = 15\n## The process of enumerating words\ntok = Tokenizer(num_words=num_max)\ntok.fit_on_texts(df.tweet)","20dbe36c":"print(df.tweet[10])\n\nfor item in df.tweet[10].split():    \n    print(tok.word_index[item])\n","801b88c1":"cnn_texts_seq = tok.texts_to_sequences(df.tweet)\n","d0cd483d":"cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len,padding='post')\n\n# \u00d6rnek\nprint('***************************************************')\nprint(df.tweet[50])\nprint(cnn_texts_mat[50])\nprint('***************************************************')\n","2f68cdc1":"X_train,X_test,y_train,y_test=train_test_split(cnn_texts_mat,tags,test_size=0.15, random_state=42)","748212ce":"labels = 'X_train', 'X_test'\nsizes = [len(X_train), len(X_test)]\nexplode = (0, 0.1,)\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","a74e7875":"#epochs_sayisi=5\nbatch_size=16\n# \u00c7\u0131kt\u0131 g\u00f6z\u00fckmemesi i\u00e7in\nverbose=1\nvalidation_split=0.1\nmax_len=15\nvocab_size=10000","1f712268":"def rnn(epoch_sayisi):\n    model=Sequential()\n    model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n    model.add(SimpleRNN(128,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(SimpleRNN(64,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(SimpleRNN(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(SimpleRNN(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(SimpleRNN(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001), metrics=['acc'])\n    history=model.fit(X_train, y_train, epochs=epoch_sayisi, batch_size=batch_size,\n                      verbose=verbose,validation_split=validation_split)\n    return history,model","6e4e56d9":"def lstm(epoch_sayisi):\n    model=Sequential()\n    model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n    model.add(LSTM(128,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(64,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['acc'])\n    history=model.fit(X_train, y_train, epochs=epoch_sayisi, batch_size=batch_size,\n                      verbose=verbose,validation_split=validation_split)\n    return history,model","5608be13":"def bilstm(epoch_sayisi):\n    model=Sequential()\n    model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n    model.add(Bidirectional(LSTM(128,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(64,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(Bidirectional(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01))))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['acc'])\n    history=model.fit(X_train, y_train, epochs=epoch_sayisi, batch_size=batch_size,\n                      verbose=verbose,validation_split=validation_split)\n    return history,model","53c30e13":"def cnn(epoch_sayisi):\n    model=Sequential()\n    model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n    model.add(Conv1D(128,1,kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64,1,kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(32,1,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(16,1,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(4,1,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(512,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['acc'])\n    history=model.fit(X_train, y_train, epochs=epoch_sayisi, batch_size=batch_size,\n                      verbose=verbose,validation_split=validation_split)\n    return history,model","aa868238":"def test(algoritma,model):\n    #Hi\u00e7 e\u011fitime sokulmam\u0131\u015f veri ile test\n    test=model.evaluate(X_test, y_test, verbose=0)\n    print(\"\\n\\nTest %s modelinde test %s: %.2f%% --- %s: %.2f%%\" % (algoritma,model.metrics_names[1], test[1]*100,model.metrics_names[0], test[0]*100))\n","bd12b7d0":"epoch_sayisi=int(input(\"Eposh sayisini giriniz..:\"))\n\nprint('''\n    Uygulanacak algoritmay\u0131 se\u00e7iniz:\n    - CNN\n    - RNN\n    - LSTM\n    - BiLSTM\n    ''')\n\nalgoritma=input(\"Se\u00e7im:\").lower()\nif(algoritma==\"cnn\"):\n    history,model=cnn(epoch_sayisi)\nelif(algoritma==\"rnn\"):\n    history,model=rnn(epoch_sayisi)\nelif(algoritma==\"lstm\"):\n    history,model=lstm(epoch_sayisi)\nelif(algoritma==\"bilstm\"):\n    history,model=bilstm(epoch_sayisi)\nprint(\"**** %s modelinde %s epochluk Acc %.2f  Loss %.2f\" %(algoritma,epoch_sayisi,history.history['acc'][epoch_sayisi-1]*100,history.history['loss'][epoch_sayisi-1]*100))\n\nrnn_test=test(algoritma,model)\n\n","9049f94f":"bilstm_acc = history.history['acc']\nbilstm_val_acc =history.history['val_acc']\nbilstm_loss=history.history['loss']\nbilstm_val_loss=history.history['val_loss']","7ab89c32":"plt.plot(cnn_acc, label='CNN Acc')\nplt.plot(rnn_acc, label='RNN Acc')\nplt.plot(lstm_acc, label='LSTM Acc')\nplt.plot(bilstm_acc, label='BiLSTM Acc')\nplt.title('CNN RNN LSTM BiLSTM Accurary')\nplt.ylabel('Validation Loss')\nplt.xlabel('Epoch Say\u0131s\u0131')\nplt.legend(loc=\"lower left\")\nplt.show()","7d38f538":"plt.plot(cnn_val_acc, label='CNN Val Acc')\nplt.plot(rnn_val_acc, label='RNN Val Acc')\nplt.plot(lstm_val_acc, label='LSTM Val Acc')\nplt.plot(bilstm_val_acc, label='BiLSTM Val Acc')\nplt.title('CNN RNN LSTM BiLSTM Validation Accurary')\nplt.ylabel('Validation Loss')\nplt.xlabel('Epoch Say\u0131s\u0131')\nplt.legend(loc=\"lower left\")\nplt.show()","98dfc4d4":"**Veri setini i\u015flenebilir hale getirmek i\u00e7in tweetleri temizlememiz gerekmektedir. @birolemekli gibi kullan\u0131c\u0131 adlar\u0131n\u0131, https gibi link ifadeleri, rakamlar\u0131... \nBurda regez ile i\u015flemlerimi halledece\u011fim**"}}