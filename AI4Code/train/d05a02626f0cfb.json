{"cell_type":{"4603cc34":"code","6fa09cea":"code","8d4c6fdb":"code","df9e6f4d":"code","dbe3a068":"code","afda27ae":"code","a038ea3f":"code","7719f011":"code","8fb4e2c5":"code","8e95774f":"code","553087df":"code","b9534bcf":"code","ebac5976":"code","01558fbe":"code","a94e0d72":"code","b5cd76eb":"code","b90e9d4d":"code","47debd15":"code","c68b3a4e":"code","c25d7f05":"code","437a419e":"code","798db8f2":"code","02689cac":"code","4f565a00":"code","1331f648":"code","6afe6520":"code","9e80d7ac":"code","8b2fa6b6":"code","b5e4283b":"code","d0da3e23":"code","43ce7209":"code","6ddf9e27":"code","19ebe1b5":"code","4368803e":"code","f056e853":"code","89574b8e":"code","b2127c0d":"code","41678d3b":"code","1caeded7":"code","f612a870":"code","9fa27893":"code","18e1f67e":"code","fe477e03":"code","a84252ae":"code","5f3ffe4b":"code","20360b95":"code","3f1b1fe4":"code","86d4454e":"code","90cadbf5":"code","aabad3c8":"code","9aea5699":"code","bfd77926":"code","12afa3c8":"code","2afd57b2":"code","9e12048e":"code","ccabcae0":"code","347e8187":"code","19508d67":"code","4ba6b379":"code","37162fab":"code","22761e0a":"code","83285d75":"code","73954be6":"code","b5ea0bc6":"code","0b375126":"code","b9ec78ae":"code","cb9775bb":"code","9b05a261":"code","d5297f58":"code","a793ca3a":"code","dbc911b3":"code","e1e31679":"code","1a883997":"code","24e6a27b":"code","a1c86baf":"code","b21a77c9":"code","c788b3fc":"code","5aef341f":"code","5b8e8ca6":"code","5fe489c0":"code","c1458d90":"code","2f97faac":"code","391b705c":"code","5ba2bf6c":"code","3af4ec73":"code","40172b99":"code","d81cf94e":"code","8039112d":"code","efb41dc4":"code","c53dccd3":"code","20c42509":"code","de6a7ede":"code","492d8345":"code","5ad8aad0":"code","7bdafc35":"code","8253cb55":"code","815b78fc":"code","04d9afd6":"code","2e3a9d0b":"code","ef1741d9":"code","2a88b994":"code","6a4f9839":"code","2cf40f83":"code","bdb3de91":"code","762bcb81":"code","6e5980c4":"code","8ff0ea80":"code","17ba41ab":"code","fe844a37":"code","5acde489":"code","78b615e8":"code","e64cb413":"code","22bcbd02":"code","4ba92a6a":"code","63891d32":"code","261adae2":"code","55224334":"code","75e53ec4":"code","072aecf7":"code","7755739e":"code","50a97595":"code","55d9f44e":"code","252dea85":"code","e0b66ff2":"code","5467fee2":"code","f817942c":"code","52d64a2f":"code","3e0d8905":"code","f9d58d5e":"code","9ff014ba":"code","05513d3c":"markdown","ed2b0a6a":"markdown","b0c39158":"markdown","7d0b0043":"markdown","df3f6e4d":"markdown","5fe57185":"markdown","a5faf85b":"markdown","09e0c4fb":"markdown","c2b3bdf9":"markdown","f52c55ab":"markdown","f986c0c6":"markdown","b14878df":"markdown","aee267bc":"markdown","3aedbc14":"markdown","475a6fcb":"markdown","570b6cb8":"markdown","32e4c6f0":"markdown","b1476dd1":"markdown","359f6784":"markdown","7ac54104":"markdown","4e87f019":"markdown","89a53895":"markdown","58bea7e1":"markdown","6d085f7b":"markdown","c44da08e":"markdown","b59c2c36":"markdown","91d46739":"markdown","1fcf228f":"markdown","d0097e9a":"markdown","f2c00d39":"markdown","39a4bde3":"markdown","9e506f90":"markdown","ff56ce61":"markdown","13836939":"markdown","420c435a":"markdown","1d588090":"markdown","a5999353":"markdown","1684e970":"markdown","73cfb134":"markdown","27e70068":"markdown"},"source":{"4603cc34":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6fa09cea":"pd.options.display.max_columns = None\npd.options.display.max_rows = None","8d4c6fdb":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","df9e6f4d":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","dbe3a068":"print(f\"Train dataset has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"Test dataset has {test.shape[0]} rows and {test.shape[1]} columns\")","afda27ae":"#Gives statistical information about numerical variables\ntrain.describe().T","a038ea3f":"#Gives information about the features (like data type etc.)\ntrain.info()","7719f011":"#Gives count of different data types\ntrain.get_dtype_counts()","8fb4e2c5":"#Checking for missing values\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending=False)[df.isnull().sum().sort_values(ascending=False)!=0]\n    percent = (((df.isnull().sum().sort_values(ascending=False)) \/ (df.shape[0])) * 100)[((df.isnull().sum().sort_values(ascending=False)) \/ (df.shape[0])) * 100 != 0]\n    return(pd.concat([total, percent], axis=1, keys=['Total', 'Percent']))\nmissing_percentage(train)","8e95774f":"missing_percentage(test)","553087df":"fig, ax =plt.subplots(2,2, figsize=(12, 8))\nsns.distplot(train[\"SalePrice\"], ax=ax[0][0])\nsns.boxplot(train[\"SalePrice\"], ax=ax[0][1])\nfig.delaxes(ax[1][0])\nfig.delaxes(ax[1][1])\nfig.tight_layout(pad=3.0)","b9534bcf":"print(f\"Skewness value of SalePrice is {train['SalePrice'].skew()}\")\nprint(f\"Kurtosis value of SalePrice is {train['SalePrice'].kurt()}\")","ebac5976":"#Correlation value between target (SalePrice) and other numerical variables\n(train.corr())['SalePrice'].sort_values(ascending=False)[1:]","01558fbe":"len(train.corr()['SalePrice'])","a94e0d72":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='OverallQual', y='SalePrice', data=train)","b5cd76eb":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train)","b90e9d4d":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='GarageCars', y='SalePrice', data=train)","47debd15":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='GarageArea', y='SalePrice', data=train)","c68b3a4e":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=train)","c25d7f05":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='1stFlrSF', y='SalePrice', data=train)","437a419e":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='FullBath', y='SalePrice', data=train)","798db8f2":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='TotRmsAbvGrd', y='SalePrice', data=train)","02689cac":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='YearBuilt', y='SalePrice', data=train)","4f565a00":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='YearRemodAdd', y='SalePrice', data=train)","1331f648":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='GarageYrBlt', y='SalePrice', data=train)","6afe6520":"fig, ax =plt.subplots(figsize=(12, 8))\nsns.scatterplot(x='MasVnrArea', y='SalePrice', data=train)","9e80d7ac":"# fig, ax =plt.subplots(figsize=(12, 8))\n# sns.scatterplot(x='Fireplaces', y='SalePrice', data=train)","8b2fa6b6":"#Deleting the outliers from the given dataset, outliers found using plot of GrLivArea vs SalePrice column\n\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\n\n#save previous train dataset\nprevious_train = train.copy()","b5e4283b":"fig, (ax1, ax2) =plt.subplots(figsize=(12, 8), ncols=2,sharey=False)\n# Scatter plot between GrLivArea and SalePrice\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, ax=ax1)\n# Putting a regression line plot between GrLivArea and SalePrice in the above scatter plot\nsns.regplot(x='GrLivArea', y='SalePrice', data=train, ax=ax1)\n# Scatter plot between MasVnrArea and SalePrice\nsns.scatterplot(x='MasVnrArea', y='SalePrice', data=train, ax=ax2)\n# Putting a regression line plot between MasVnrArea and SalePrice in the above scatter plot\nsns.regplot(x='MasVnrArea', y='SalePrice', data=train, ax=ax2)","d0da3e23":"#Residual plot between GrLivArea and SalePrice\nplt.subplots(figsize = (12,8))\nsns.residplot(train.GrLivArea, train.SalePrice)","43ce7209":"fig, (ax1, ax2) =plt.subplots(figsize=(14, 8), ncols=2,sharey=False)\nsns.distplot(train['SalePrice'], ax=ax1)\nsns.boxplot(train['SalePrice'], ax=ax2)","6ddf9e27":"#Applying numpy's log1p transformation, log1p is aaplied to handle the situation of any zero value present in SalePrice column\ntrain['SalePrice'] = np.log1p(train['SalePrice'])","19ebe1b5":"#Again plotting SalePrice to check whether it is normally distributed now or not\nfig, (ax1, ax2) =plt.subplots(figsize=(14, 8), ncols=2,sharey=False)\nsns.distplot(train['SalePrice'], ax=ax1)\nsns.boxplot(train['SalePrice'], ax=ax2)","4368803e":"#Plotting residual plot between GrLivArea and SalePrice to find whether heteroscedasticity is resolved or not\nfig, (ax1, ax2) =plt.subplots(figsize=(14, 8), ncols=2,sharey=False)\n#Residual plot, without transforming the target column SalePrice\nsns.residplot(previous_train.GrLivArea, previous_train.SalePrice, ax=ax1)\n#Residual plot, after transforming the target column SalePrice\nsns.residplot(train.GrLivArea, train.SalePrice, ax=ax2)","f056e853":"sns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n\n#Generate a mask for the upper triangle\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0);\n\n#Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","89574b8e":"#Dropping Id column from both train and test dataset\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\n#Storing the target variable in y\ny = train['SalePrice'].reset_index(drop=True)\n\nprevious_train = train.copy()","b2127c0d":"#Combining train and test dataset together\nall_data = pd.concat((train, test)).reset_index(drop=True)","41678d3b":"l = []\nfor i in (all_data.select_dtypes(include ='object').columns):\n    if(i != 'SalePrice'):\n        data_crosstab = pd.crosstab(all_data[i], all_data['SalePrice'], margins = False)\n        stat, p, dof, expected = stats.chi2_contingency(data_crosstab)\n        prob=0.95\n        alpha = 1.0 - prob\n        if p <= alpha:\n            print(i, ' : Dependent (reject H0)')\n        else:\n            l.append(i)\n            print(i, ' : Independent (fail to reject H0)')","1caeded7":"for i in all_data[l].columns:\n    print(f'Plot between {i} and SalePrice')\n    sns.boxplot(x=i, y=all_data['SalePrice'], data=all_data)\n    plt.show()","f612a870":"all_data['Utilities'] = all_data['Utilities'].replace(['NoSewr', 'NoSeWa', 'ELO'], 'NoSeWa')","9fa27893":"sns.boxplot(x='Utilities', y='SalePrice', data=all_data)","18e1f67e":"all_data['Utilities'].value_counts()","fe477e03":"sns.boxplot(x='Street', y='SalePrice', data=all_data)","a84252ae":"all_data['YrSold'] = all_data['YrSold'].astype('int64')\nall_data['MoSold'] = all_data['MoSold'].astype('int64')\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype('int64')","5f3ffe4b":"all_data['Old'] = all_data['YrSold'] - all_data['YearRemodAdd']","20360b95":"#From following correlation, we can say that Old column is effecting target column SalePrice, as correlation value is 0.57 (negative)\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"Old\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","3f1b1fe4":"#Finding correlation of newly created variable Old, with the variables using which it is created\nprint(all_data[['Old', 'YrSold']].corr())\nprint(all_data[['Old','YearRemodAdd']].corr())","86d4454e":"#Dropping YearRemodAdd and MoSold\nall_data.drop(['YearRemodAdd', 'MoSold'], axis=1, inplace=True)","90cadbf5":"missing_percentage(all_data)","aabad3c8":"#Replacing null values with None as in this case null values means that particular feature is not present in the house\nmissing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor i in missing_val_col:\n    all_data[i] = all_data[i].fillna('None')","9aea5699":"#The following features also contains null value for a reason, implies that area or square feet is zero, so replacing with 0 value\nmissing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageYrBlt',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor i in missing_val_col2:\n    all_data[i] = all_data[i].fillna(0)","bfd77926":"# Replaced all missing values in LotFrontage by imputing the mean value of each neighborhood\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: \n                                                                                   x.fillna(x.mean()))","12afa3c8":"# Replaced all missing values in MSZoning by imputing the mode value of each MSSubClass\n#Converting MSSubClass to categorical data type \nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: \n                                                                            x.fillna(x.mode()[0]))","2afd57b2":"#Converting YrSold, MoSold to categorical data type \n# all_data['YrSold'] = all_data['YrSold'].astype(str)\n# all_data['MoSold'] = all_data['MoSold'].astype(str)","9e12048e":"#Replace the remaining caegorical columns with their respective mode values\nall_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0]) \nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0]) \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0]) \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","ccabcae0":"missing_percentage(all_data)","347e8187":"numeric_feats = all_data.dtypes[all_data.dtypes != object].index","19508d67":"skew_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskew_feats","4ba6b379":"def fixing_skewness(df):\n    numeric_feats = df.dtypes[df.dtypes != object].index\n    \n    skew_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    \n    high_skew = skew_feats[abs(skew_feats) > 0.5].index\n    \n    for i in high_skew:\n        df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))\n        \nfixing_skewness(all_data)","37162fab":"all_data['TotalSF'] = (all_data['TotalBsmtSF']\n                      + all_data['1stFlrSF']\n                      +all_data['2ndFlrSF'])\n\n# all_data['YrBltAndRemod'] = (all_data['YearBuilt'] \n#                              + all_data['YearRemodAdd'])\n\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF']\n                             + all_data['WoodDeckSF']\n                             + all_data['3SsnPorch']\n                             + all_data['EnclosedPorch']\n                             + all_data['ScreenPorch'])\n\nall_data['Total_Bathrooms'] = (all_data['FullBath']\n                              + 0.5 * all_data['HalfBath']\n                              + all_data['BsmtFullBath']\n                              + 0.5 * all_data['BsmtHalfBath'])\n\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] \n                                 + all_data['BsmtFinSF2'] \n                                 + all_data['1stFlrSF'] \n                                 + all_data['2ndFlrSF']\n                                )","22761e0a":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has1stfloor'] = all_data['1stFlrSF'].apply(lambda x: 1 if x > 0 else 0)","83285d75":"all_data.shape","73954be6":"#Finding correlation with newly created variable TotalSF\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"TotalSF\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","b5ea0bc6":"#Finding correlation of newly created variable 'TotalSF' with the variable using which it is created\nprint(all_data[['TotalSF', 'TotalBsmtSF']].corr())\nprint(all_data[['TotalSF', '1stFlrSF']].corr())\nprint(all_data[['TotalSF','2ndFlrSF']].corr())","0b375126":"print(all_data[['2ndFlrSF', 'SalePrice']].corr())","b9ec78ae":"#Finding correlation with newly created variable Total_porch_sf\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"Total_porch_sf\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","cb9775bb":"print(all_data[['Total_porch_sf', 'OpenPorchSF']].corr())\nprint(all_data[['Total_porch_sf', 'WoodDeckSF']].corr())\nprint(all_data[['Total_porch_sf','3SsnPorch']].corr())\nprint(all_data[['Total_porch_sf','ScreenPorch']].corr())\nprint(all_data[['Total_porch_sf','EnclosedPorch']].corr())","9b05a261":"print(all_data[['WoodDeckSF', 'SalePrice']].corr())","d5297f58":"#Dropping WoodDeckSF column, as it is less correlated with target column as compare to newly created variable, and highly correlated with target variable\nall_data.drop('WoodDeckSF', axis=1, inplace=True)","a793ca3a":"#Finding correlation with newly created variable Total_Bathrooms\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"Total_Bathrooms\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","dbc911b3":"print(all_data[['Total_Bathrooms', 'FullBath']].corr())\nprint(all_data[['Total_Bathrooms', 'HalfBath']].corr())\nprint(all_data[['Total_Bathrooms','BsmtFullBath']].corr())\nprint(all_data[['Total_Bathrooms','BsmtHalfBath']].corr())","e1e31679":"#Finding correlation with newly created variable Total_sqr_footage\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"Total_sqr_footage\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","1a883997":"print(all_data[['Total_sqr_footage', 'BsmtFinSF1']].corr())\nprint(all_data[['Total_sqr_footage', 'BsmtFinSF2']].corr())\nprint(all_data[['Total_sqr_footage','1stFlrSF']].corr())\nprint(all_data[['Total_sqr_footage','2ndFlrSF']].corr())","24e6a27b":"print(all_data[['SalePrice', '2ndFlrSF']].corr())","a1c86baf":"#Dropping 2ndFlrSF column, as it is highly correlated with the newly created variable, and effecting less target variable as compare to the other newly created variables\nall_data.drop('2ndFlrSF', axis=1, inplace=True)","b21a77c9":"#Finding correlation with newly created variable haspool\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"haspool\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","c788b3fc":"#Dropping haspool, as it is correlated with target column but the value is very less 0.077\nall_data.drop('haspool', axis=1, inplace=True)","5aef341f":"#Finding correlation with newly created variable hasgarage\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"hasgarage\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","5b8e8ca6":"print(all_data[['hasgarage', 'GarageArea']].corr())\nprint(all_data[['SalePrice', 'GarageArea']].corr())","5fe489c0":"#Finding correlation with newly created variable hasfireplace\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"hasfireplace\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","c1458d90":"print(all_data[['hasfireplace', 'Fireplaces']].corr())\nprint(all_data[['SalePrice', 'Fireplaces']].corr())","2f97faac":"#Dropping Fireplaces column as it is highly correlated with the newly created variable, and also it is less correlated to the target column as compare to the newly created variable\nall_data.drop('Fireplaces', axis=1, inplace=True)","391b705c":"#Finding correlation with newly created variable hasbsmt\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"hasbsmt\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","5ba2bf6c":"#Dropping hasbsmt column as it is correlated to the target column but the correlation value is very small\nall_data.drop('hasbsmt', axis=1, inplace=True)","3af4ec73":"#Finding correlation with newly created variable has2ndfloor\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"has2ndfloor\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","40172b99":"#Dropping has2ndfloor column as it is correlated to the target column but the correlation value is very small\nall_data.drop('has2ndfloor', axis=1, inplace=True)","d81cf94e":"#Finding correlation with newly created variable has1stfloor\ndf_corr_num_num=all_data.loc[:,[\"SalePrice\",\"has1stfloor\"]]\nsns.heatmap(df_corr_num_num.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')","8039112d":"#Dropping has1stfloor column as it is correlated to the target column but the correlation value is very small\nall_data.drop('has1stfloor', axis=1, inplace=True)","efb41dc4":"#Dropping target variable SalePrice from the whole data\nall_data.drop('SalePrice', axis=1, inplace=True)","c53dccd3":"#We are dropping following columns as in 'PoolQC' only 'NA' is present, in 'Street' only 'Pave' is present, in 'Utilities' only 'AllPub' is present\n#We are dropping 'YearBuilt', 'GarageYrBlt' columns because these are correlated with 'YearRemodAdd' with 83% correlation value\nall_data = all_data.drop(['Utilities', 'Street', 'PoolQC','YearBuilt', 'GarageYrBlt'], axis=1)","20c42509":"#As only two value is present in CentralAir column, so replacing those with 0 and 1, and converting column data type to integer\nall_data['CentralAir'] = all_data['CentralAir'].replace({'Y':1, 'N':0})\nall_data['CentralAir'] = all_data['CentralAir'].astype('int64')","de6a7ede":"#Creating dummy variable\nfinal_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","492d8345":"#Separating the train and test dataset\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]","5ad8aad0":"#Dropping outliers ----research required\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","7bdafc35":"for i in X.columns:\n    counts = X[i].value_counts()\n    print (counts)","8253cb55":"#Function to determine overfitted features, that is feature containing only one value in more than 99.94 cases\ndef overfit_reducer(df):\n    overfit = []\n    for i in df.columns:\n        count = df[i].value_counts()\n        zero_index_value = count.iloc[0]\n        \n        if (((zero_index_value \/ len(df)) * 100) > 99.94):\n            overfit.append(i)\n            \n    overfit = list(overfit)\n    return overfit","815b78fc":"#Finding the list of overfitted features using above user-defined function\noverfitted_features = overfit_reducer(X)\n#Dropping the overfitted columns from the final dataframes\nX.drop(overfitted_features, axis=1, inplace=True)\nX_sub.drop(overfitted_features, axis=1, inplace=True)","04d9afd6":"X.shape, X_sub.shape","2e3a9d0b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","ef1741d9":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","2a88b994":"lin_reg = LinearRegression(normalize=True, n_jobs=-1)","6a4f9839":"lin_reg.fit(X_train, y_train)","2cf40f83":"y_pred = lin_reg.predict(X_test)","bdb3de91":"print((mean_squared_error(y_test, y_pred)))","762bcb81":"from sklearn.linear_model import Ridge\n#Assiging different sets of alpha values to explore which can be the best fit for the model\nalpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\ntemp_rss = {}\ntemp_mse = {}\n\nfor i in alpha_ridge:\n    #Assigin each model\n    ridge = Ridge(alpha=i, normalize=True)\n    # fit the model\n    ridge.fit(X_train, y_train)\n    #Predicting the target value based on X_test\n    y_pred = ridge.predict(X_test)\n    \n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_test - y_pred) ** 2)\n    \n    temp_rss[i] = rss\n    temp_mse[i] = mse\n    ","6e5980c4":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","8ff0ea80":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","17ba41ab":"from sklearn.linear_model import Lasso \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    #Assigin each model. \n    lasso_reg = Lasso(alpha= i, normalize=True)\n    #fit the model. \n    lasso_reg.fit(X_train, y_train)\n    #Predicting the target value based on \"X_test\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","fe844a37":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","5acde489":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","78b615e8":"from sklearn.linear_model import ElasticNet\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    #Assigin each model. \n    lasso_reg = ElasticNet(alpha= i, normalize=True)\n    #fit the model. \n    lasso_reg.fit(X_train, y_train)\n    #Predicting the target value based on \"X_test\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","e64cb413":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","22bcbd02":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","4ba92a6a":"corr_matrix = X.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]","63891d32":"X = X.drop(X[to_drop], axis=1)\nX_sub = X_sub.drop(X_sub[to_drop], axis=1)","261adae2":"from scipy.special import inv_boxcox1p","55224334":"from sklearn.ensemble import GradientBoostingRegressor \ngbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber', \n                                random_state =42)","75e53ec4":"from xgboost import XGBRegressor\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","072aecf7":"rf = RandomForestRegressor(n_estimators=20)","7755739e":"xgb_model_full_data = xgboost.fit(X, y)","50a97595":"gbr_model_full_data = gbr.fit(X, y)","55d9f44e":"rf_fit = rf.fit(X,y)","252dea85":"y_xgb_pred = xgb_model_full_data.predict(X_sub)","e0b66ff2":"y_gbr_pred = gbr_model_full_data.predict(X_sub)","5467fee2":"y_rf_pred = rf_fit.predict(X_sub)","f817942c":"y_xgb_pred = inv_boxcox1p(y_xgb_pred, 0)\ny_gbr_pred = inv_boxcox1p(y_gbr_pred, 0)\ny_rf_pred = inv_boxcox1p(y_rf_pred, 0)","52d64a2f":"submission_df_xgb = pd.DataFrame(y_xgb_pred,columns=['SalePrice'])\nsubmission_df_gbr = pd.DataFrame(y_gbr_pred,columns=['SalePrice'])\nsubmission_df_rf = pd.DataFrame(y_rf_pred,columns=['SalePrice'])","3e0d8905":"test_sub = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_sub.head()","f9d58d5e":"submission_df_xgb['Id'] = test_sub['Id']\nsubmission_df_xgb = submission_df_xgb[['Id', 'SalePrice']]\n\nsubmission_df_gbr['Id'] = test_sub['Id']\nsubmission_df_gbr = submission_df_gbr[['Id', 'SalePrice']]\n\nsubmission_df_rf['Id'] = test_sub['Id']\nsubmission_df_rf = submission_df_rf[['Id', 'SalePrice']]","9ff014ba":"submission_df_xgb.to_csv('\/kaggle\/working\/submission_xgb.csv', index=False)\nsubmission_df_gbr.to_csv('\/kaggle\/working\/submission_gbr.csv', index=False)\nsubmission_df_rf.to_csv('\/kaggle\/working\/submission_rf.csv', index=False)","05513d3c":"As the newly created variable 'Total_Bathrooms' is correlated with the target column SalePrice, and correlation value is 0.68, so, we conclude that this variable is effecting the target variable and thus should not be dropped","ed2b0a6a":"Observations:\n\nThere is 0.83 or 83% correlation between GarageYrBlt and YearBuilt.\n\n83% correlation between TotRmsAbvGrd and GrLivArea.\n\n89% correlation between GarageCars and GarageArea.\n\nSimilarly many other features such asBsmtUnfSF, FullBath have good correlation with other independent feature","b0c39158":"If data is not skewed then above value for skewness should have come between -1 and 1, as skewness value is greater than 1, so SalePrice is not normally distributed\n\nIn positive Skewness the mean and median will be greater than the mode similar to this dataset. Which means more houses were sold by less than the average price.","7d0b0043":"Following notebook will provide rmse value of around 0.11, and it is still under work in progress.\nFeel free to comment","df3f6e4d":"The residuals are randomly scattered around the center line of zero.\nAbove is showing Heteroscedasticity, as with the increase in value of GrLivArea, variance also increases, which is defined by heteroscedasticity. One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation","5fe57185":"From the above graph, we can see that the target variable, SalePrice is not normally distributed, it is right skewed, more number of outliers are there","a5faf85b":"#### Avoiding restrictions on displaying number of rows and columns","09e0c4fb":"Interpretation of above result:\n\n'object' datatype: MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Heating, HeatingQC, CentralAir, Electrical, KitchenQual, Functional, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence, MiscFeature, SaleType, SaleCondition (object(43))\n\n'int64' datatype: Id, MSSubClass, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold, SalePrice (int64(35))\n\n'float64' datatype: LotFrontage, MasVnrArea, GarageYrBlt (float64(3))","c2b3bdf9":"As the newly created variable 'has2ndfloor' is correlated with the target column SalePrice, and correlation value is 0.15 , so, we conclude that this variable is not effecting the target variable and thus should can be dropped","f52c55ab":"#### Removing skewness from data","f986c0c6":"As the newly created variable 'haspool' is correlated with the target column SalePrice, and correlation value is 0.077 which is very small, so, we conclude that this variable is not effecting the target variable and thus should can be dropped","b14878df":"#### Describing dataset","aee267bc":"From above plot we can say that the target variable SalePrice (dependent variable) is not normal, but the linear regression analysis requires the dependent variable to be multivariate normally distributed. So, here we need to apply transformation so that the target variable can be made normally distributed","3aedbc14":"#### Identify Highly Correlated Features","475a6fcb":"So, from above we can conclude that there are no missing values left except for target column, which we are gonna drop in future","570b6cb8":"As the newly created variable 'TotalSF' is correlated with the target column SalePrice, and correlation value is 0.55, so, we conclude that this variable is effecting the target variable and thus should not be dropped","32e4c6f0":"As the newly created variable 'has1stfloor' is correlated with the target column SalePrice, and correlation value is almost 0 , so, we conclude that this variable is not effecting the target variable and thus should can be dropped","b1476dd1":"As the newly created variable 'Total_sqr_footage' is correlated with the target column SalePrice, and correlation value is 0.38, so, we conclude that this variable is effecting the target variable and thus should not be dropped","359f6784":"Interpretation from above result:\n\nLotFrontage, MasVnrArea, GarageYrBlt columns contain missing values\n\nLotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, GrLivArea, GarageArea,WoodDeckSF, MiscVal, SalePrice columns contain skewed values (they are not following normal distribution)","7ac54104":"#### Creating new features","4e87f019":"As the newly created variable 'hasbsmt' is correlated with the target column SalePrice, and correlation value is 0.2 , so, we conclude that this variable is not effecting the target variable and thus should can be dropped","89a53895":"Interpretation of above result:\n\nFrom above we can say that PoolQC, MiscFeature, Alley, Fence columns contain more than 80 percent of null values, so it would be better to drop them unless they are affecting target column (which we'll find later and then decide whether to drop these columns or aplly some kind of treatment on them)","58bea7e1":"From above plot, we can see that after transforming target column SalePrice (right side plot), there is almost an equal amount of variance across the zero lines","6d085f7b":"#### Sample train dataset","c44da08e":"### Importing required libraries","b59c2c36":"As the newly created variable 'hasfirplace' is correlated with the target column SalePrice, and correlation value is 0.51 , so, we conclude that this variable is effecting the target variable and thus should should not be dropped","91d46739":"From above, we find that as the Masonry veneer area in square feet increases, sale price of the house also increases, but there are some outliers also here","1fcf228f":"From the above plot, we can say that, as the square feet area increases, sale price of the house also increases, which is very normal, but there are two outliers here (as with increase in the square feet area sale price of the house decreased drastically, which is not normal)","d0097e9a":"From above we find that with the increase in total square feet of basement area, sale price of the house also increases, which is quite normal, but there is some outlier also as with increase in the area, sale price is decreasing which is not a normal trend","f2c00d39":"As the newly created variable 'Old' is correlated with the target column SalePrice, and correlation value is 0.57 (-ve), so, we conclude that this variable is effecting the target variable and thus should not be dropped","39a4bde3":"From above we find that with the increase in first floor square feet, sale price of the house also increases, which is quite normal, but there is some outlier also as with increase in the square feet, sale price is decreasing which is not a normal trend","9e506f90":"Interpretation of above result:\n\nOverallQual is a categorical variable, that is why scatter plot is not the best way to visualize the relationship of this variable with target variable. But one important thing we can notice from here is that Sale price of house is increasing with the increase in overall quality of the house","ff56ce61":"From above plot we can say that linear relationship between SalePrice and GrLivArea is better than that between SalePrice and MasVnrArea. There are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. ","13836939":"From above, we find that as the garage area increases, sale price of the house also increases, but there are some outliers also here, showing with the increase in garage area, sale price of the house decreased","420c435a":"The above log transformation also helps in solving many other issues like heteroscedasticity. We can have a look on it using the below plot","1d588090":"As the newly created variable 'Total_porch_sf' is correlated with the target column SalePrice, and correlation value is 0.46, so, we conclude that this variable is effecting the target variable and thus should not be dropped","a5999353":"#### Sample test dataset","1684e970":"Now, there should not be any multi collinearity in the data, that is two independent variables should not be correlated with each other, to find the correaltion in each of the variables, we'll be plotting heatmap below:","73cfb134":"From above, we conclude that 2ndFlrSF is correalted with target with a value of 0.32 which is less than that for TotalSF, which is 0.55, so we'll drop 2ndFlrSF","27e70068":"As the newly created variable 'hasgarage' is correlated with the target column SalePrice, and correlation value is 0.32 , so, we conclude that this variable is effecting the target variable and thus should should not be dropped"}}