{"cell_type":{"466d382b":"code","68d57b21":"code","9c50cbf4":"code","a9a3be9c":"code","27b94a8a":"code","37d48ffe":"code","4dc82f42":"code","1540c81b":"code","dfd10f66":"code","36e4bfe5":"code","228750b2":"markdown","0090d0f1":"markdown","eb649a8c":"markdown","12f914f7":"markdown","acf0356b":"markdown","390b92ae":"markdown"},"source":{"466d382b":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport time\n\nfrom gensim.models import Word2Vec\nfrom tqdm import tqdm\n\ntqdm.pandas()","68d57b21":"def preprocessing(titles_array):\n    \n    \"\"\"\n    Take in an array of titles, and return the processed titles.\n    \n    (e.g. input: 'i am a boy', output - 'am boy')  -> since I remove those words with length 1\n    \n    Feel free to change the preprocessing steps and see how it affects the modelling results!\n    \"\"\"\n    \n    processed_array = []\n    \n    for title in tqdm(titles_array):\n        \n        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n        processed = re.sub('[^a-zA-Z ]', '', title)\n        \n        words = processed.split()\n        \n        # keep words that have length of more than 1 (e.g. gb, bb), remove those with length 1.\n        processed_array.append(' '.join([word for word in words if len(word) > 1]))\n    \n    return processed_array","9c50cbf4":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","a9a3be9c":"df_train['processed'] = preprocessing(df_train['title'])\ndf_test['processed'] = preprocessing(df_test['title'])\n\nsentences = pd.concat([df_train['processed'], df_test['processed']],axis=0)\ntrain_sentences = list(sentences.progress_apply(str.split).values)","27b94a8a":"# Parameters reference : https:\/\/www.quora.com\/How-do-I-determine-Word2Vec-parameters\n# Feel free to customise your own embedding\n\nstart_time = time.time()\n\nmodel = Word2Vec(sentences=train_sentences, \n                 sg=1, \n                 size=100,  \n                 workers=4)\n\nprint(f'Time taken : {(time.time() - start_time) \/ 60:.2f} mins')","37d48ffe":"# Total number of vocab in our custom word embedding\n\nlen(model.wv.vocab.keys())","4dc82f42":"# Check out the dimension of each word (we set it to 100 in the above training step)\n\nmodel.wv.vector_size","1540c81b":"# Check out how 'iphone' is represented (an array of 100 numbers)\n\nmodel.wv.get_vector('iphone')","dfd10f66":"# Find words with similar meaning to 'iphone'\n\nmodel.wv.most_similar('iphone')","36e4bfe5":"model.wv.save_word2vec_format('custom_glove_100d.txt')\n\n\n# How to load:\n# w2v = KeyedVectors.load_word2vec_format('custom_glove_100d.txt')\n\n# How to get vector using loaded model\n# w2v.get_vector('iphone')\n","228750b2":"## **Something to take note**\nWord2vec is a **self-supervised** method (well, sort of unsupervised but not unsupervised, since it provides its own labels. check out this [Quora](https:\/\/www.quora.com\/Is-Word2vec-a-supervised-unsupervised-learning-algorithm) thread for a more detailed explanation), so we can make full use of the entire dataset (including test data) to obtain a more wholesome word embedding representation.","0090d0f1":"## **The most important part!**\nLast but not least, save your word embeddings, so that you can used it for modelling. You can load the text file next time using Gensim KeyedVector function.","eb649a8c":"## **Pretty fast isn't it.**\n\nLet's check out some of the features of the customised word vector.","12f914f7":"# **How to train your <del>dragon<\/del> custom word embeddings**\n\nIn the [baseline-keras-lstm-is-all-you-need](https:\/\/www.kaggle.com\/huikang\/baseline-keras-lstm-is-all-you-need) notebook shared by Hui Kang (thanks again!), it was demonstrated that a LSTM model using generic global vector (GLOVE) achieved a pretty solid benchmark results.\n\nAfter playing around with GLOVE, you will quickly find that certain words in your training data are not present in its vocab. These are typically replaced with same-shape zero vector, which essentially means you are 'sacrificing' the word as your input feature, which can potentially be important for correct prediction. Another way to deal with this is to train your own word embeddings, using your training data, so that the semantic relationship of your own training corpus can be better represented.\n\nIn this notebook, I will demonstrate how to train your custom word2vec using Gensim.\n\nFor those who are new to word embeddings and would like to find out more, you can check out the following articles:\n1. [Introduction to Word Embedding and Word2Vec](https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n2. [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https:\/\/skymind.ai\/wiki\/word2vec)","acf0356b":"Well, you will see words similar to 'iphone', sorted based on euclidean distance.\nOf cause, there are also not so intuitive and relevant ones (e.g. jetblack, cpo, ten). If you would like to tackle this, you can do a more thorough pre-processing\/ try other embedding dimensions\n","390b92ae":"## Now, why are word embeddings powerful? \n\nThis is because they capture the semantics relationships between words. In other words, words with similar meanings should appear near each other in the vector space of our custom embeddings.\n\nLets check out an example:"}}