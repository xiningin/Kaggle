{"cell_type":{"d2bd94b4":"code","68df7129":"code","95e62706":"code","5b49d3ca":"code","a23a7cd7":"code","2eda448a":"code","ef8c8bea":"code","abaf339b":"code","4ebf6e88":"code","868e24f2":"code","2dbde76a":"code","3fddc171":"code","090bb4b0":"code","26859213":"code","cf0aaee9":"code","c2d051fc":"code","851adaa0":"code","5361dbd5":"code","f94f6a09":"code","7845599a":"code","b3508f17":"code","a4c08183":"code","79c081ba":"code","46767c79":"code","9e180163":"code","0f58e877":"code","ead6a6c8":"code","0d646e18":"code","62373e5f":"markdown","8d4bc2c0":"markdown","b170bc5e":"markdown","7e575b09":"markdown"},"source":{"d2bd94b4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use(\"fivethirtyeight\")","68df7129":"data = pd.read_csv(\"..\/input\/topic-modeling-for-research-articles\/train.csv\")","95e62706":"data.head()","5b49d3ca":"data.drop(\"ID\", axis=1, inplace=True)\nx = data[[\"ABSTRACT\"]]\ny = data.drop([\"TITLE\",\"ABSTRACT\"], axis=1)","a23a7cd7":"y","2eda448a":"for i in range(y.shape[0]):\n    if y.loc[i, \"Statistics\"]==1:\n        y.loc[i, \"Mathematics\"]=1\ny.drop([\"Quantitative Finance\", \"Quantitative Biology\", \"Statistics\"], axis=1, inplace=True)","ef8c8bea":"unlabeled = pd.read_csv(\"..\/input\/topic-modeling-for-research-articles\/test.csv\")","abaf339b":"unlabeled.shape","4ebf6e88":"unlabeled.index = range(20972,20971+8989+1)","868e24f2":"unlabeled","2dbde76a":"unlabeled.drop(\"ID\", axis=1, inplace=True)","3fddc171":"corpus = pd.concat([x, unlabeled])[\"ABSTRACT\"]","090bb4b0":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","26859213":"from nltk.stem import WordNetLemmatizer\nlemmer=WordNetLemmatizer()","cf0aaee9":"corpus=[' '.join([lemmer.lemmatize(word) for word in text.split(' ')])\n          for text in corpus]\ncorpus = [' '.join([word for word in text.split('\\n')]) for text in corpus]\ncorpus = [text[2:] for text in corpus]","c2d051fc":"tfidf = TfidfVectorizer(stop_words=[\"and\", \"is\", \"are\", \"the\", \"a\", \"on\"], max_features=25000).fit(raw_documents=corpus)\nbow = CountVectorizer(stop_words=[\"and\", \"is\", \"are\", \"the\", \"a\", \"on\"], max_features=25000).fit(raw_documents=corpus)","851adaa0":"bowdata = bow.transform(corpus)\ntfidfdata = tfidf.transform(corpus)","5361dbd5":"from sklearn.decomposition import LatentDirichletAllocation","f94f6a09":"lda = LatentDirichletAllocation(n_components=y.shape[1], random_state=11, learning_method=\"online\")","7845599a":"bowdist = lda.fit_transform(bowdata)\ntfidfdist = lda.fit_transform(tfidfdata)","b3508f17":"bowmaintopics = np.argmax(bowdist, axis=1)\nbowdist = pd.DataFrame(bowdist, columns=[\"Topic \"+str(i) for i in range(1,4)])\nbowdist[\"LDA-MainTopic\"]=bowmaintopics\n\ntfidfmaintopics = np.argmax(tfidfdist, axis=1)\ntfidfdist = pd.DataFrame(tfidfdist, columns=[\"Topic \"+str(i) for i in range(1,4)])\ntfidfdist[\"LDA-MainTopic\"]=tfidfmaintopics","a4c08183":"realtopic=y.idxmax(axis=1)","79c081ba":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n\nsns.heatmap(pd.crosstab(bowdist[\"LDA-MainTopic\"], realtopic), annot=True, fmt=\"d\", annot_kws={\"fontsize\":15}, cmap=\"Blues\", vmin=0, ax=axes[0])\naxes[0].set_xlabel(\"Real Topic\")\naxes[0].set_title(\"\\nBoW + LDA\\n\", fontsize=25)\n\nsns.heatmap(pd.crosstab(tfidfdist[\"LDA-MainTopic\"], realtopic), annot=True, fmt=\"d\", annot_kws={\"fontsize\":15}, cmap=\"Blues\", vmin=0, ax=axes[1])\naxes[1].set_xlabel(\"Real Topic\")\naxes[1].set_title(\"\\nTF-IDF + LDA\\n\", fontsize=25)\n\nplt.show()","46767c79":"from sklearn.decomposition import NMF","9e180163":"nnmf = NMF(n_components=3, random_state=42)","0f58e877":"wbow = nnmf.fit_transform(bowdata)\nhbow = nnmf.components_\n\nwtfidf = nnmf.fit_transform(tfidfdata)\nhtfidf = nnmf.components_","ead6a6c8":"bowmaintopics = np.argmax(wbow, axis=1)\nwbow = pd.DataFrame(wbow, columns=[\"Topic \"+str(i) for i in range(1,4)])\nwbow[\"NMF-MainTopic\"]=bowmaintopics\n\ntfidfmaintopics = np.argmax(wtfidf, axis=1)\nwtfidf = pd.DataFrame(wtfidf, columns=[\"Topic \"+str(i) for i in range(1,4)])\nwtfidf[\"NMF-MainTopic\"]=tfidfmaintopics","0d646e18":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n\nsns.heatmap(pd.crosstab(wbow[\"NMF-MainTopic\"], realtopic), annot=True, fmt=\"d\", annot_kws={\"fontsize\":15}, cmap=\"Blues\", vmin=0, ax=axes[0])\naxes[0].set_xlabel(\"Real Topic\")\naxes[0].set_title(\"\\nBoW + NMF\\n\", fontsize=25)\n\nsns.heatmap(pd.crosstab(wtfidf[\"NMF-MainTopic\"], realtopic), annot=True, fmt=\"d\", annot_kws={\"fontsize\":15}, cmap=\"Blues\", vmin=0, ax=axes[1])\naxes[1].set_xlabel(\"Real Topic\")\naxes[1].set_title(\"\\nTF-IDF + NMF\\n\", fontsize=25)\n\nplt.show()","62373e5f":"# Latent Dirichlet Allocation","8d4bc2c0":"# Text Processing","b170bc5e":"Most common words","7e575b09":"# Non-negative Matrix Factorization"}}