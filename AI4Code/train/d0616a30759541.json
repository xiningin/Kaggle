{"cell_type":{"49db3f29":"code","9e9fbc9f":"code","cbadbcc9":"code","e43d3632":"code","0f4d64b2":"code","61ab6ff9":"code","a8690047":"code","9d716430":"code","47a55741":"code","493ecf43":"code","dde947f6":"code","ea63cad5":"code","09a192ef":"markdown","3a013b1b":"markdown","c85d287c":"markdown","fe0f6740":"markdown","51e87561":"markdown","109c63a3":"markdown","f6d60330":"markdown","06fc32f5":"markdown","bade5890":"markdown","b1fb21af":"markdown","b0f7e3cb":"markdown"},"source":{"49db3f29":"!pip install praat-textgrids\n","9e9fbc9f":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nclass Audio:\n    def __init__(self,samples,sr):\n        self.samples=np.real(samples)\n        self.sr=sr\n    \n    def save(self,direction):\n        librosa.output.write_wav(direction,np.ascontiguousarray(np.real(self.samples)),self.sr)\n    \n    def resample(self,newRate):\n        return Audio(librosa.resample(self.samples,self.sr,newRate),newRate)\n\n    def selectByTime(self,startTime=None,endTime=None):\n        if startTime==None:\n            start=0\n        else:\n            start=int(startTime*self.sr)\n        if endTime==None:\n            end=self.samples.shape[0]\n        else:\n            end=int(endTime*self.sr)\n        return Audio(self.samples[start:end],self.sr)\n    \n    def getLength(self):\n        return self.samples.shape[0]\/self.sr\n    \n    def plot(self,axes=None,imgPath=None):\n        end=self.samples.shape[0]\n        x=np.arange(0,end\/self.sr,1\/self.sr)\n        y=self.samples\n        if axes==None:\n            plt.xlabel(\"t\/s\")\n            plt.ylabel(\"amplitude\")\n            plt.plot(x,y)\n            if imgPath==None:\n                plt.show()\n            else:\n                plt.savefig(imgPath,dpi=500, bbox_inches = 'tight')\n            plt.clf()\n        else:\n            axes.plot(x,y)\n    \n    def spec(self):\n        librosa.display.specshow(librosa.amplitude_to_db(np.abs(self.samples)),sr=self.sr)\n    \n    def __add__(self,other):\n        if type(other)!=Audio:\n            return Audio(self.samples+other,self.sr)\n        if other.sr!=self.sr:\n            raise Exception(\"Sample rate error.\")\n        return Audio(self.samples+other.samples,self.sr)\n        \n    def __sub__(self,other):\n        if type(other)!=Audio:\n            return Audio(self.samples-other,self.sr)\n        if other.__sr!=other.__sr:\n            raise Exception(\"Sample rate error.\")\n        return Audio(self.samples-other.samples,self.sr)\n\n    def __mul__(self,other):\n        if type(other)!=Audio:\n            return Audio(self.samples*other,self.sr)\n        if other.sr!=self.sr:\n            raise Exception(\"Sample rate error.\")\n        return Audio(self.samples*other.samples,self.sr)\n        \n\n\n\ndef read_Audio(direction):\n    y,r=librosa.load(direction)\n    if len(y.shape)==2:\n        return (Audio(y[0],r),Audio(y[1],r))\n    elif len(y.shape)==1:\n        return Audio(y,r)\n    else:\n        raise Exception(\"Wrong number of tracks. \")","cbadbcc9":"def energyCal(frame_w):\n    return 10*np.log(np.sum(np.power(frame_w,2),axis=1)+1e-6)\n\ndef framing(audio,frameDuration,overlapRate):\n    windowLength=int(frameDuration*audio.sr)\n    step=int(windowLength*(1-overlapRate))\n    y=audio.samples\n    l=y.shape[0]\n    indexer = np.arange(windowLength)[None, :] + step*np.arange(int((l-windowLength)\/step))[:, None]\n    flatten=y[indexer]\n    filters=librosa.filters.get_window(window = \"hamming\", Nx = windowLength, fftbins = False)\n    flatten=flatten*filters\n    return flatten\n\ndef overzeroCal(frame_w,fs):\n    sgs=np.sign(frame_w)\n    overzeros=np.sum(np.abs(sgs[:,1:]-sgs[:,:-1]),axis=1)\n    return overzeros\n\ndef VAD(audio,energyPerLow=12.5,energyPerHigh=40,overzeroPer=45,minVoice=0.15,minSilence=0.08,frameDuration=0.03,overlapRate=0.5):\n    flatten=framing(audio,frameDuration,overlapRate)\n    energys=energyCal(flatten)\n    overzeros=overzeroCal(flatten,audio.sr)\n    clip1=np.percentile(energys,energyPerLow)\n    clip2=np.percentile(energys,energyPerHigh)\n    clip3=np.percentile(overzeros,overzeroPer)\n    label=np.where(energys>clip2,1,0)\n    label=np.where((energys>clip1)&(overzeros>clip3),1,label)\n    count=0\n    startPoint=0\n    for i in range(1,len(label)):\n        if label[i]!=label[i-1]:\n            if label[i-1]==0:\n                length=i-startPoint\n                if length*frameDuration<minSilence:\n                    label[startPoint:i-1]=1\n            else:\n                startPoint=i\n            count=0\n        count+=1\n    count=0\n    startPoint=0\n    for i in range(1,len(label)):\n        if label[i]!=label[i-1]:\n            if label[i-1]==1:\n                length=i-startPoint\n                if length*frameDuration<minVoice:\n                    label[startPoint:i-1]=0\n            else:\n                startPoint=i\n            count=0\n        count+=1\n    return label\n\ndef VADPlot(audio,label,frameDuration=0.03,overlapRate=0.5,imgPath=None):\n    windowLength=int(frameDuration*audio.sr)\n    step=int(windowLength*(1-overlapRate))\n    label2=np.array([label[i] for i in range(0,label.shape[0]) for n in range(0,step)])\n    x=np.arange(0,label2.shape[0]\/audio.sr,1\/audio.sr)\n    y=audio.samples[:len(label2)]\n    plt.xlabel(\"t\/s\")\n    plt.ylabel(\"amplitude\")\n    plt.plot(x,np.where(label2==1,y,None),color=\"m\")\n    plt.plot(x,np.where(label2==0,y,None))\n    if imgPath!=None:\n        plt.savefig(imgPath,dpi=500, bbox_inches = 'tight')\n    else:\n        plt.show()\n    plt.clf()","e43d3632":"import textgrids\n\nFRAME_DURATION = 30 # 30 msec\nOVERLAP_RATE = 0.0 # overlap rate between frames: if 0.0 two frames don't overlap, if 1.0 two frames completely overlap\n\n# costants of the labels\nNONSPEECH = 0\nSPEECH = 1\n","0f4d64b2":"\n\ndef readFile(path):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_msec = dur * 1000 # sec -> msec\n        num_frames = int(round(dur_msec \/FRAME_DURATION)) # the audio is divided into 30 msec frames\n        for i in range(num_frames):\n            \n            labeled_list.append(label)\n\n    return np.array(labeled_list)\n","61ab6ff9":"root ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SI2220.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SI2220.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\naudio=read_Audio(audio_path)\n\n# define time axis\nNs = len(audio.samples)  # number of sample\nTs = 1 \/ audio.sr  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis","a8690047":"thresholdLow = 20\nthresholdHigh = 40\nthresholdZero=30\npredicted_label = VAD(audio,thresholdLow,thresholdHigh,thresholdZero,frameDuration=FRAME_DURATION\/1000,overlapRate=OVERLAP_RATE\/1000)","9d716430":"VADPlot(audio,predicted_label,FRAME_DURATION\/1000,OVERLAP_RATE\/1000)\nVADPlot(audio,label_list[:len(predicted_label)],FRAME_DURATION\/1000,OVERLAP_RATE\/1000)","47a55741":"from sklearn.metrics import balanced_accuracy_score\n\nprint(\"Balance accuracy: \", balanced_accuracy_score(predicted_label, label_list[:len(predicted_label)]))","493ecf43":"\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Male\/TMIT\/SI562.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Male\/TMIT\/SI562.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\naudio=read_Audio(audio_path)\n\n# define time axis\nNs = len(audio.samples)  # number of sample\nTs = 1 \/ audio.sr  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis","dde947f6":"predicted_label = VAD(audio,thresholdLow,thresholdHigh,thresholdZero,frameDuration=FRAME_DURATION\/1000,overlapRate=OVERLAP_RATE\/1000)\nVADPlot(audio,predicted_label,FRAME_DURATION\/1000,OVERLAP_RATE\/1000)\nVADPlot(audio,label_list[:len(predicted_label)],FRAME_DURATION\/1000,OVERLAP_RATE\/1000)","ea63cad5":"print(\"Balance accuracy: \", balanced_accuracy_score(predicted_label, label_list[:len(predicted_label)]))","09a192ef":"#### Function extracting the ground truth labels","3a013b1b":"Compute energy and normalize","c85d287c":"Compare the goundtruth labels with the predicted one","fe0f6740":"Load a file with a male speaker","51e87561":"### Install library","109c63a3":"Compute the balance accuracy since we don't know how many SPEECH\/NONSPEECH segments there are ","f6d60330":"With the same threshold we get lower accuracy wtr the first audio signal and considering that such signals are \"noiseless\" it's not a very good result. This means we need to compute the threshold value for each different signal in order to get higher performances. Other features can be considered, such as the **Zero-Crossing-Rate** which is a meaningful feature in the speech processing field.\nHowever the thresholding approach is simple but non very effective when the signals are quite affected by noise (white noise and background noise), the neural networks can provided pretty good results within the right set of short-time features.","06fc32f5":"### Set the threshold and compute labels","bade5890":"## Try same threshold with a different audio","b1fb21af":"# Find speech segments","b0f7e3cb":"### Load an audio from the TIMIT corpus"}}