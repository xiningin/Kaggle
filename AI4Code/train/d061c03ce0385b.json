{"cell_type":{"66973821":"code","964d472e":"code","613ba521":"code","c395ba4d":"code","d36380fd":"code","12817636":"code","d679a67c":"code","6fa7d132":"code","5325c913":"code","459de512":"code","392197f7":"code","ae2f8794":"code","1bdf6bbf":"code","35b09123":"code","3a720707":"code","55af3ea5":"code","f5913061":"code","3ce3f0e7":"code","b09dc8bd":"code","233d7ffd":"code","600313dc":"code","51b38f28":"code","e8142ced":"code","5edd887e":"code","3ebc6b37":"code","bb7ddb24":"code","f791e5a1":"code","2c2b02d5":"code","4e9d9abb":"markdown","4499d695":"markdown","5928a555":"markdown","2b4059fd":"markdown","ed5342d8":"markdown","ffab1e68":"markdown","899f8be5":"markdown","644501f0":"markdown","495e1445":"markdown","74e69ef9":"markdown"},"source":{"66973821":"\nimport warnings\nimport pandas as pd\nimport numpy as np\nwarnings.simplefilter(action='ignore', category=Warning)\nimport joblib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n\n# linear models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n# non-linear models\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n","964d472e":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)","613ba521":"\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf.head()","c395ba4d":"\n\ndf.info()\ndf.shape  # There are 322 observation and 20 variables.\ndf.isnull().sum()  # There are missing values in Salary columns.\n\n\n# Descriptive Analysis\n# When I examine the dataset, we see that there is\n# a difference between the mean and median values of the Assist variable.\n# This difference is also supported by the standard deviation.\ndf.describe([0.05, 0.25, 0.50, 0.75, 0.95, 0.99]).T\n\n\n","d36380fd":"\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n   \n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\n\n# Specifying variable types\ncat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)\ncat_cols, num_cols, cat_but_car","12817636":"def cat_summary(dataframe, col_name, plot=False):\n    import pandas as pd\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()","d679a67c":"for col in cat_cols:\n    cat_summary(df, col, plot=True)","6fa7d132":"\ndef correlation_matrix(df, cols):\n    fig = plt.gcf()\n    fig.set_size_inches(10, 8)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='RdBu')\n    plt.show(block=True)\n\ncorrelation_matrix(df, num_cols)","5325c913":"\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n    \nfor col in cat_cols:\n    target_summary_with_cat(df, \"Salary\", col)","459de512":"df.dropna(inplace=True)","392197f7":"def outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\ndef check_outlier(dataframe, col_name, q1=0.10, q3=0.90):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n","ae2f8794":"for col in num_cols:\n    print(col, check_outlier(df, col))\n\n# We replace with thresholds\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n\n# control for outliers\nfor col in num_cols:\n    print(col, check_outlier(df, col))","1bdf6bbf":"\nsns.boxplot(x=df[\"Salary\"])\nplt.show()","35b09123":"\n# remove salary bigger than up limit\nq3 = 0.90\nsalary_up = int(df[\"Salary\"].quantile(q3))\ndf = df[(df[\"Salary\"] < salary_up)]\n","3a720707":"\nsns.boxplot(x=df[\"Salary\"])\nplt.show()","55af3ea5":"\n# New variables were created with the most appropriate variables according to their proportions.\n\ndf[\"new_Hits\/CHits\"] = df[\"Hits\"] \/ df[\"CHits\"]\ndf[\"new_OrtCHits\"] = df[\"CHits\"] \/ df[\"Years\"]\ndf[\"new_OrtCHmRun\"] = df[\"CHmRun\"] \/ df[\"Years\"]\ndf[\"new_OrtCruns\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf[\"new_OrtCRBI\"] = df[\"CRBI\"] \/ df[\"Years\"]\ndf[\"new_OrtCWalks\"] = df[\"CWalks\"] \/ df[\"Years\"]\n\n\ndf[\"New_Average\"] = df[\"Hits\"] \/ df[\"AtBat\"]\ndf['new_PutOutsYears'] = df['PutOuts'] * df['Years']\ndf[\"new_RBIWalksRatio\"] = df[\"RBI\"] \/ df[\"Walks\"]\ndf[\"New_CHmRunCAtBatRatio\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]\ndf[\"New_BattingAverage\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\ndf.dropna(inplace=True)\n","f5913061":"binary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\nfor col in binary_cols:\n    labelencoder = LabelEncoder()\n    df[col] = labelencoder.fit_transform(df[col])\n","3ce3f0e7":"df.head()","b09dc8bd":"\n# One-Hot Encoding\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=False)\n    return dataframe\n\n\ndf = one_hot_encoder(df, cat_cols)","233d7ffd":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)","600313dc":"# list feature importances for a regressor model like LGBM\npre_model = LGBMRegressor().fit(X, y)\nfeature_imp = pd.DataFrame({'Feature': X.columns, 'Value': pre_model.feature_importances_})\nfeature_imp.sort_values(\"Value\", ascending=False)","51b38f28":"pre_model = RandomForestRegressor().fit(X, y)\nfeature_imp = pd.DataFrame({'Feature': X.columns, 'Value': pre_model.feature_importances_})\nfeature_imp.sort_values(\"Value\", ascending=False)","e8142ced":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          # (\"CatBoost\", CatBoostRegressor(verbose=False))\n          ]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n","5edd887e":"\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [3, 5, 8, 15, 20],\n             \"n_estimators\": [600, 650, 1000]}\n\n\nlightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1, 0.001],\n                   \"n_estimators\": [250, 300, 500, 1500, 2500,3000],\n                   \"colsample_bytree\": [0.1, 0.3, 0.5, 0.7, 1]}\n\nregressors = [(\"RF\", RandomForestRegressor(), rf_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\nbest_models = {}","3ebc6b37":"\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=3, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=3, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\n","bb7ddb24":"\nvoting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('LightGBM', best_models[\"LightGBM\"])])\n\nvoting_reg.fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg,\n                                 X, y,\n                                 cv=10,\n                                 scoring=\"neg_mean_squared_error\")))\n","f791e5a1":"\nX.columns\nrandom_user = X.sample(1, random_state=45)  # 247 index y[y.index == 247] --> 560\nvoting_reg.predict(random_user)","2c2b02d5":"random_user.index\ny[y.index == 247] ","4e9d9abb":"# 4 Automated Hyperparameter Optimization","4499d695":"# 2. Data Preprocessing & Feature Engineering","5928a555":"Data Science and Machine Learning Bootcamp, 2021, https:\/\/www.veribilimiokulu.com\/\n","2b4059fd":"\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n    \nProblem : Estimating baseball players salary applying machine learning algoritms on hitters dataset\n1.  [**Exploratory Data Analysis**](#1)\n2.  [**Data Preprocessing & Feature Engineering**](#2)\n3.  [**Base Models**](#3)\n4.  [**Automated Hyperparameter Optimization**](#4)\n5.  [**Stacking & Ensemble Learning**](#5)\n6.  [**Prediction for a New Observation**](#6)\n","ed5342d8":" # 5 Stacking & Ensemble Learning","ffab1e68":"# 6 Prediction for a New Observation","899f8be5":"# End-to-End Hitters Machine Learning ","644501f0":"\n# 1. Explanatory Data Analysis\n","495e1445":"# Format\n* A data frame with 322 observations of major league players on the following 20 variables.\n* AtBat Number of times at bat in 1986\n* Hits Number of hits in 1986\n* HmRun Number of home runs in 1986\n* Runs Number of runs in 1986\n* RBI Number of runs batted in in 1986\n* Walks Number of walks in 1986\n* Years Number of years in the major leagues\n* CAtBat Number of times at bat during his career\n* CHits Number of hits during his career\n* CHmRun Number of home runs during his career\n* CRuns Number of runs during his career\n* CRBI Number of runs batted in during his career\n* CWalks Number of walks during his career\n* League A factor with levels A and N indicating player\u2019s league at the end of 1986\n* Division A factor with levels E and W indicating player\u2019s division at the end of 1986\n* PutOuts Number of put outs in 1986\n* Assists Number of assists in 1986\n* Errors Number of errors in 1986\n* Salary 1987 annual salary on opening day in thousands of dollars\n* NewLeague A factor with levels A and N indicating player\u2019s league at the beginning of 1987","74e69ef9":"# 3. Base Models"}}