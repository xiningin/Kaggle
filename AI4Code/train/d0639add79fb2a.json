{"cell_type":{"40ef3ef1":"code","8b6386b0":"markdown","f1cd9754":"markdown","63358d43":"markdown","c83700b9":"markdown","8c0dfee8":"markdown","d137be92":"markdown","50a69d94":"markdown"},"source":{"40ef3ef1":"def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n","8b6386b0":"<img src=\"https:\/\/aavella77.github.io\/images\/adam_without_a_clue.png\">","f1cd9754":"And now here is the main code for a Deep Learning Model.  You need to implement the functions.  This is just the main body.","63358d43":"**Some Python code**\n\nHere is the main function of a neural network model.  Libraries such as [Tensorflow](https:\/\/www.tensorflow.org\/tutorials\/), [Pytorch](https:\/\/pytorch.org\/) and [Fast.ai](https:\/\/www.fast.ai\/), do these kinds of things in the background and it is very easy to do stuff, without having a clue.  First, I show you the code how to call a model and then how it could be implemented in Python.","c83700b9":"\n*Published on March 10, 2019\n*\n\nI hope these notes may help someone trying to get started with Deep Learning.  Not sure we need another tutorial, so I'll try to add some value with these Deep Learning Notes.  Hope you follow along...\n\n**Pre-requisites:**\n\nDon't try this tutorial at home if you are new to this area.  If you are an artificial intelligence newbie, I recommend first reading [Jay Alammar's 2 introductory blogs](http:\/\/jalammar.github.io\/visual-interactive-guide-basics-neural-networks\/).  After that, I recommend watching the [3BLUE1BROWN SERIES on Neural Networks](https:\/\/www.youtube.com\/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi).  It is a bit long, but it is the best intuition I have found.  I highly recommend watch them and stop and rewind if you don't get something.  Don't get scared about the math, especially now that there some many tools on the Internet, for example you can try to plot the [sigmoid](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function) or [tanh](https:\/\/reference.wolfram.com\/language\/ref\/Tanh.html) functions on [Desmos graph tool](https:\/\/www.desmos.com\/calculator) or [multiply a matrix with Calcul](http:\/\/www.calcul.com\/show\/calculator\/matrix-multiplication_;4;2;2;4?matrix1=[[%221%22,%220%22],[%220%22,%221%22],[%223%22,%220%22],[%220%22,%225%22]]&matrix2=[[%220%22,%221%22,%220%22,%220%22],[%222%22,%220%22,%221%22,%220%22]]&operator=*).  If you never heard about what a derivative is about, don't worry, check this animation from [Wolfram on derivatives](http:\/\/demonstrations.wolfram.com\/TheTangentLineProblem\/). It is just the slope of the curve at some point. You will hear about gradients. The gradient is a multi-variable generalization of the derivative. While a derivative can be defined on functions of a single variable, for functions of several variables, the gradient takes its place. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued. You will need to learn some linear algebra, which you can understand from these [3BLUE1BROWN SERIES on Linear algebra](https:\/\/www.youtube.com\/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab).  Another thing that you could do is take the [Kaggle free courses](https:\/\/www.kaggle.com\/learn\/overview).\n\n**Steps:**\n\nIn college I passed 7 math courses, but they did not pass through me.  There is some level of math required for deep learning, however, deep Learning for Neural Networks always follows the same steps.  I think in a nutshell these are the steps that it boils down to.\n\n1. Initialize parameters \/ Define hyperparameters\n2. Loop for num_iterations:\n    1. Forward propagation\n    2. Compute cost function\n    3. Backward propagation\n    4. Update parameters (using parameters, and grads from backprop) \n4. Use trained parameters to predict labels\n\n**Deep Neural Network**\n\n[Daniel Smilkov](https:\/\/www.linkedin.com\/in\/dsmilkov\/) and [Shan Carter](http:\/\/shancarter.com\/) created a [wonderful tool](https:\/\/playground.tensorflow.org\/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2,2&seed=0.08014&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) that helps people to play around with neural network parameters.  \n\nThey just created the page and they don't explain much about the different parameters.  So, I'll try to give you some pointers.  First, you have the input X with two features (x1 and x2). Then there are several layers of neurons and weights (W) between them with a bias (b)( Look at the lines between neurons).  These are the parameters W and b that you need to adjust while training the neural network.\n\nThe way it works is kind of cool.  The concept is that you show the neural network some samples and the idea is that the neural networks \"learns\".  By learning, I mean finding the correct weights and biases.  But, how can you do that automatically? Well, with repeated forward and backward propagation steps.  The real trick was initially devised by [Geoffrey Hinton](https:\/\/en.wikipedia.org\/wiki\/Geoffrey_Hinton).  He proposed to use backward propagation by adjusting the weights and biases.  I think [Andrej Karpathy](https:\/\/cs.stanford.edu\/people\/karpathy\/) explained the best in this [Stanford lecture](https:\/\/www.youtube.com\/watch?v=59Hbtz7XgjM). The whole idea is to minimize the cost function while iteraring throught the training examples\n\nFInally, the idea is that we want to predict with a new test set.  So, we show the neural network a new example and it should be able to predict correctly.\n\nThis explanation is how I understand it, with my limited knowledge.  Please feel free to comment below.\n\n<img src=\"https:\/\/aavella77.github.io\/images\/NeuralNetworkExample.png\">\n","8c0dfee8":"**Forward and Backward Propagation**\n\n**Notation**:\n- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n    \nThe linear forward module (vectorized over all the examples) computes the following equations:\n\n$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n\nwhere $A^{[0]} = X$. \n\n Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\u00a0\\tag{7}$$\n\nSuppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n\n<img src=\"https:\/\/aavella77.github.io\/images\/forward_backward_functions.png\">","d137be92":"\n**The following people inspired me to write these notes:**\n1. First, my machine learning professor: [Andrew Ng](https:\/\/www.andrewng.org\/).  I like when he says if don't get the math, \"Don't worry about it\" :-).  He has some very good material on [Coursera's DeepLearning.ai specialization](https:\/\/www.coursera.org\/specializations\/deep-learning)\n2. [Yoshua Bengio](http:\/\/www.iro.umontreal.ca\/~bengioy\/yoshua_en\/index.html) had [an interwiew with Andrew Ng](https:\/\/www.youtube.com\/watch?v=pnTLZQhFpaE) and prompted me to investigate more.  I think Prof. Bengio has very sharp comments!\n3. [Christine Mcleavey Payne](http:\/\/christinemcleavey.com\/) made a career switch to AI and wrote a [page on how she did it](http:\/\/christinemcleavey.com\/learning-about-deep-learning\/). She is currently with [Open.ai](https:\/\/openai.com\/).\n4. [Jeff Dean](https:\/\/ai.google\/research\/people\/jeff) head of [Google Brain](https:\/\/ai.google\/research\/teams\/brain) sent a summary of [Google research on AI on 2018](https:\/\/ai.googleblog.com\/2019\/01\/looking-back-at-googles-research.html) and I especially got interested on [AI for Social Good](https:\/\/ai.google\/social-good\/).\n5. [Surya Ganguli](http:\/\/ganguli-gang.stanford.edu\/surya.html) article from [Stanford Human-Center Artificial Intelligence](https:\/\/hai.stanford.edu\/) group is excellent . The title is [The intertwined quest for understanding biological intelligence and creating artificial intelligence](https:\/\/hai.stanford.edu\/news\/intertwined-quest-understanding-biological-intelligence-and-creating-artificial-intelligence)\n\n**What to do next?**\n\n1. I highly recommend playing with code.  I would suggest you open an account in Kaggle and compete in a challenge.  I did it for the [QuickDraw challenge](https:\/\/aavella77.github.io\/posts\/2018-12-artificial-intelligence-using-fast-ai-and-google-collab-notebooks\/index.html) and it was a lot of fun.\n2. Take the [DeepLearning.ai Coursera specialization](https:\/\/www.coursera.org\/specializations\/deep-learning)\n3. Explore these links from [Montreal AI](https:\/\/montrealartificialintelligence.com\/academy\/)\n4. Try [Michael Nielsen's](http:\/\/michaelnielsen.org\/) book on [Neural Networks and Deep Learning](http:\/\/neuralnetworksanddeeplearning.com\/).\n\nIf you got this far, thank you!  I appreciate if you could vote for this Kaggle Kernel.\n\n**References**\n\nSource: [DeepLearning.ai Coursera specialization](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n**You may listen to these songs in the background for better learning results...:** \n\nI was listening to [Buongiorno Principessa](https:\/\/www.youtube.com\/watch?v=pSmvN0FX6XU&list=RDMMflr8vDQ9Wuc&index=27) from [Life is Beatiful](https:\/\/www.youtube.com\/watch?v=Q1qggoumYi0) while I created this post.  I especially like the first 30 seconds and from 8:15 to 9:00 of the concert.  If you haven't seen the movie I highly recommend it.  When I got bored I listened to [Los \u00c1ngeles Azules - Nunca Es Suficiente ft. Natalia Lafourcade (Live)](https:\/\/www.youtube.com\/watch?v=k76BgIb89-s).  I especially like how fun it looks on 51st second.. checkout how happy the guy playing the accordion looks like!  Finally, I listened to [Benedictus played by 2Cellos](https:\/\/www.youtube.com\/watch?v=f_RjlIPuqyc) while making the figure.\n\n**Author:**\n[Alejandro Avella](https:\/\/aavella77.github.io\/about.html)\n\n\nThe end!\n\n","50a69d94":"\n**How does Adam work?**\n\nThere is a very popular Optimization created by [Durk Kingma](http:\/\/dpkingma.com\/) and [Jimmy Lei Ba](https:\/\/research.fb.com\/fellows\/ba-jimmy-lei\/). I was intrigued about Adam! Seems to make things much better. It combines RMSProp, momentum, Exponential Weighted Averages, learning rate decay and mini-batch. So, I googled the authors and found these pages that provide some background on who they are. I wanted to know more on who can propose such a thing. They are now with Google Brain and Facebook Research.\n\n1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n\nThe update rule is, for $l = 1, ..., L$: \n\n$$\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}$$\nwhere:\n- t counts the number of steps taken of Adam \n- L is the number of layers\n- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n- $\\alpha$ is the learning rate\n- $\\varepsilon$ is a very small number to avoid dividing by zero\n\n"}}