{"cell_type":{"a218e95e":"code","bab39aed":"code","61febec8":"code","5c509cb4":"code","78f7f34a":"code","083a9fbb":"code","e6486a37":"code","3f3386c1":"code","c19d0006":"code","0e5309c9":"code","873ec35c":"code","13945299":"code","5e7c5d44":"code","ca60d2ad":"code","c826c669":"code","c837ccad":"code","12e08b1c":"code","b930deff":"code","d3a68f88":"code","464029f4":"code","c755d3ce":"code","289afb2d":"code","414e56c1":"code","3edfcf9c":"code","b6195470":"code","bc7e959d":"code","fbc0dc2b":"code","8dd24372":"code","59a74ead":"code","b42c574c":"code","d8b7385a":"code","5d3db28c":"code","cc0b2152":"code","88ca0ebf":"code","abbbd830":"code","8608551a":"code","83299002":"code","087833c3":"code","446fdbf1":"code","85aef0ea":"code","17961438":"code","8bbe5357":"code","161605ab":"code","84a0ee43":"code","1721b713":"code","07ed1544":"code","dbaf2d03":"code","6ce6dc04":"code","5832b82a":"code","158e55dc":"code","2eec9fce":"code","98ee1c8b":"code","d3837d85":"code","b708ce28":"code","f7872e9b":"code","15dbf37d":"code","5c03bd3a":"code","56f3a744":"code","346b07c8":"code","8d22d6ba":"code","d2309f50":"code","dcbc15ad":"code","f0cc5a52":"code","8abd4ede":"code","e08c479d":"code","fe2adf3d":"code","008b30bc":"code","fedb555b":"code","d94fdb94":"code","f957a731":"code","e5f2ebd3":"code","9d4f0504":"code","f1ebed0e":"code","48e21e23":"code","c039208b":"code","c3ec86e1":"code","2501d4ea":"code","0fdcc9fc":"code","b684820a":"code","ff47f7ec":"code","7f4db765":"code","ff7d0e04":"code","2c2cd615":"code","113d9ba3":"code","ab67e2a3":"code","89fcb1de":"code","9989330c":"markdown","898022fd":"markdown","515ea86c":"markdown","a4c87efd":"markdown","f26fab8e":"markdown","224e78bc":"markdown","f697776a":"markdown","1bfc5294":"markdown","d3a17d88":"markdown","0c58a4df":"markdown","04e885d1":"markdown","d62c6ed6":"markdown","1d5c6bc2":"markdown","76870285":"markdown","73c04814":"markdown","148140c4":"markdown","45bab92e":"markdown","79f447a2":"markdown","e625342d":"markdown","5fed5dc5":"markdown","78c76694":"markdown","1240f244":"markdown","9079a58d":"markdown","6f4c3e02":"markdown","f1c26c5d":"markdown","4226fde3":"markdown","5de1aa12":"markdown","878ce5f7":"markdown","1dca9b6e":"markdown","225280cb":"markdown","512b2746":"markdown","6b9b345e":"markdown","30e06caf":"markdown","13f7db02":"markdown","fe9cf781":"markdown","9083552e":"markdown","473780b3":"markdown","494791eb":"markdown","089ee1f5":"markdown","784f53da":"markdown","3504c8fc":"markdown"},"source":{"a218e95e":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn.model_selection import learning_curve\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import to_graphviz\nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Reshape, MaxPooling2D, Flatten, Dropout\nfrom numpy.random import seed\nfrom tensorflow import set_random_seed\nfrom keras.models import load_model\nimport matplotlib.pylab as py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n","bab39aed":"# Reading in the training datasets\ndata1 = pd.read_csv(\"..\/input\/Train.csv\", na_values = [\"NA\", \" \", \"?\"])\ndata2 = pd.read_csv(\"..\/input\/Train_HospitalizationData.csv\", na_values = [\"NA\", \" \", \"?\"])\ndata3 = pd.read_csv(\"..\/input\/Train_Diagnosis_TreatmentData.csv\", na_values = [\"NA\", \" \", \"?\"])\ndata4 = pd.read_csv(\"..\/input\/IDs_mapping.csv\", na_values = [\"NA\", \" \", \"?\"], header = None)","61febec8":"# Checking how much training data we have\nprint(\"The number of rows and columns in the patient demographics data is\", data1.shape)\nprint(\"The number of rows and columns in the patient hospitalization data is\", data2.shape)\nprint(\"The number of rows and columns in the patient diagnosis and treatment data is\", data3.shape)\nprint(\"The number of rows and columns in the ID mapping data is\", data4.shape)","5c509cb4":"data1.head(3)","78f7f34a":"data2.head(3)","083a9fbb":"data3.head(3)","e6486a37":"# Merging the first three datasets\n# The sort argument will align data2's patient IDs with data1's and data3's\ndata = data1.merge(data2, on = \"patientID\", sort = True).merge(data3, on = \"patientID\", sort = True)","3f3386c1":"# Printing the number of rows and columns of the combined dataset\nprint(\"The numbers of rows and columns in the combined dataset is {}\".format(data.shape))","c19d0006":"# Separating the ID mapping info via row indexing\nadmission_type_id = data4[1:10]\ndischarge_disposition_id = data4[11:42]\nadmission_source_id = data4[43:68]","0e5309c9":"# Creating dictionaries for the ID mapping\nadmission_type_id_dict = dict(zip(admission_type_id[0], admission_type_id[1]))\ndischarge_disposition_id_dict = dict(zip(discharge_disposition_id[0], discharge_disposition_id[1]))\nadmission_source_id_dict = dict(zip(admission_source_id[0], admission_source_id[1]))","873ec35c":"# Adding new columns to the main dataset that will contain mapped ID info\nid_cols = [\"admission_type_id\", \"discharge_disposition_id\", \"admission_source_id\"]\ndata[id_cols] = data[id_cols].astype(str)\n# Mapping IDs to each patient using the dictionaries created above\ndata[\"admission_type\"]= data[\"admission_type_id\"].map(admission_type_id_dict)\ndata[\"discharge_disposition\"]= data[\"discharge_disposition_id\"].map(discharge_disposition_id_dict)\ndata[\"admission_source\"]= data[\"admission_source_id\"].map(admission_source_id_dict)","13945299":"print(data[\"discharge_disposition\"].value_counts())","5e7c5d44":"print(\"The number of expired patients is {}\".format(len(data[(data[\"discharge_disposition_id\"] == \"11\") | (data[\"discharge_disposition_id\"] == \"19\") | (data[\"discharge_disposition_id\"] == \"20\")])))","ca60d2ad":"# Dropping expired patients \ndata = data[~((data[\"discharge_disposition_id\"] == \"11\") | (data[\"discharge_disposition_id\"] == \"19\") | (data[\"discharge_disposition_id\"] == \"20\"))]","c826c669":"# Counting the missing values in each column\ndata.isnull().sum()","c837ccad":"print(\"The percentage of missing values in the weight column is\", int(33592\/34650 * 100), \"%\")","12e08b1c":"print(\"The percentage of missing values in the payer_code column is\", int(14719\/34650 * 100), \"%\")\nprint(\"The percentage of missing values in the medical_specialty column is\", int(16394\/34650 * 100), \"%\")","b930deff":"# Getting the summary statistics\nwith pd.option_context(\"display.expand_frame_repr\", True):\n    with pd.option_context(\"display.max_columns\", None):\n        print(data.describe(include = \"all\"))\n","d3a68f88":"# Dropping AdmissionID, acetohexamide, weight and setting PatientID as the index\ndata.drop([\"AdmissionID\", \"weight\", \"acetohexamide\"], axis = 1, inplace = True)","464029f4":"# Filling NA values with \"unknown\" since they only occur in categorical columns\ndata.fillna(\"unknown\", inplace = True)\ndata.set_index(\"patientID\", inplace = True)","c755d3ce":"# Converting the date columns to datetime type\ndata[\"Admission_date\"] = pd.to_datetime(data[\"Admission_date\"], format = \"%Y-%m-%d\")\ndata[\"Discharge_date\"] = pd.to_datetime(data[\"Discharge_date\"], format = \"%Y-%m-%d\")\n\n# Subtracting the admission_date and discharged_date columns to get a new column called days_hospitalized\ndata[\"days_hospitalized\"] = ((data[\"Discharge_date\"] - data[\"Admission_date\"]).dt.days)\ndata[\"days_hospitalized\"][:5]","289afb2d":"# Writing a function to group ICD 9 codes based on online information\n\ndef grouper1(i):\n    i = str(i) # to make the code subscriptable\n    if (i[0].isnumeric() == True): # if the code begins with a number\n        i = pd.to_numeric(i) # change the code from categorical to numeric dtype\n        # assign a code group to each code\n        if (1 >= i <= 139):\n            return \"001-139\"\n        elif (140 <= i <= 239):\n            return \"140-239\"\n        elif (240 <= i <= 279):\n            return \"240-279\"\n        elif (280 <= i <= 289):\n            return \"280-289\"\n        elif (290 <= i <= 319):\n            return \"290-319\"\n        elif (320 <= i <= 389):\n            return \"320-389\"\n        elif (390 <= i <= 459):\n            return \"390-459\"\n        elif (460 <= i <= 519):\n            return \"460-519\"\n        elif (520 <= i <= 579):\n            return \"520-579\"\n        elif (580 <= i <= 629):\n            return \"580-629\"\n        elif (630 <= i <= 679):\n            return \"630-679\"\n        elif (680 <= i <= 709):\n            return \"680-709\"\n        elif (710 <= i <= 739):\n            return \"710-739\"\n        elif (740 <= i <= 759):\n            return \"740-759\"\n        elif (760 <= i <= 779):\n            return \"760-779\"\n        elif (780 <= i <= 799):\n            return \"780-799\"\n        else:\n            return \"800-999\"\n    elif (i == \"unknown\"):  \n        return \"unknown\" \n    else: # if the code does not begin with a number\n        return \"EV_code\"\n        ","414e56c1":"data[\"diag_1_grouped\"] = data[\"diagnosis_1\"].apply(grouper1)\ndata[\"diag_2_grouped\"] = data[\"diagnosis_2\"].apply(grouper1)\ndata[\"diag_3_grouped\"] = data[\"diagnosis_3\"].apply(grouper1)\n\n# Dropping the original diagnosis columns\ndata.drop([\"diagnosis_1\", \"diagnosis_2\", \"diagnosis_3\"], axis = 1, inplace = True)","3edfcf9c":"# Writing a function to group medical specialties\ndef grouper2(i):\n    if ((i == \"unknown\") or (i == \"InternalMedicine\") or (i == \"Family\/GeneralPractice\")\n        or (i == \"Emergency\/Trauma\") or (i == \"Cardiology\") or (i == \"Surgery-General\")):\n        return i\n    else:\n        return \"Other\"","b6195470":"# Grouping medical specialty categories with under 1000 samples as Other\ndata[\"medical_specialty\"] = data[\"medical_specialty\"].apply(grouper2)\ndata[\"medical_specialty\"].value_counts()","bc7e959d":"# Writing a function to group payer codes \ndef grouper3(i):\n    if ((i == \"unknown\") or (i == \"MC\") or (i == \"HM\")\n        or (i == \"BC\") or (i == \"SP\") or (i == \"MD\") or (i == \"CP\") or (i == \"UN\")):\n        return i\n    else:\n        return \"Other\"","fbc0dc2b":"# Grouping payer code categories with under 1000 samples as Other\ndata[\"payer_code\"] = data[\"payer_code\"].apply(grouper3)\ndata[\"payer_code\"].value_counts()","8dd24372":"# Creating barplots showing the distributions of some categorical features\n\ntemp = [\"gender\", \"race\", \"age\", \"readmitted\", \"diabetesMed\", \"diag_3_grouped\"]\nfor col in data[temp]:\n    plt.figure(figsize=(10,7))\n    ax = sns.countplot(y = col, data = data, order = data[col].value_counts().index, palette = \"Set2\");\n    ax.set_alpha(0.8)\n    ax.set_title(\"Bar plot : {}\".format(col), fontsize=24)\n    ax.set_xlabel(\"Number of patients\", fontsize=14);\n    ax.set_ylabel(\"\");\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 14)\n    ax.set_xticks(range(0, 30000, 5000))\n    ax.set_facecolor('xkcd:off white')\n    ax.grid(alpha = 0.2)\n\n# Add percentages to individual bars\n    totals = []\n    for i in ax.patches:\n        totals.append(i.get_width())\n    total = sum(totals)\n\n    for i in ax.patches:\n        ax.text(i.get_width()+.3, i.get_y()+.38, \\\n                str(round((i.get_width()\/total)*100, 2))+'%', fontsize=12,\n    color='black')\n\n    plt.show()\n    print()","59a74ead":"# Printing histograms showing the distributions of numeric columns \nfig, ax = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 22))\nsns.set_style(\"darkgrid\")\na = sns.distplot(a = data[\"num_lab_procedures\"], kde = False, ax=ax[0, 0])\na.axes.set_title(\"Number of lab procedures\", fontsize=16)\na.tick_params(labelsize=14)\na.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nb = sns.distplot(a = data[\"days_hospitalized\"], kde = False, ax=ax[0, 1])\nb.axes.set_title(\"Days hospitalized\", fontsize=16)\nb.tick_params(labelsize=14)\nb.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nc = sns.distplot(a = data[\"num_medications\"], kde = False, ax=ax[1, 0])\nc.axes.set_title(\"Number of medications\", fontsize=16)\nc.tick_params(labelsize=14)\nc.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nd = sns.distplot(a = data[\"num_diagnoses\"], kde = False, ax=ax[1, 1])\nd.axes.set_title(\"Number of diagnoses\", fontsize=16)\nd.tick_params(labelsize=14)\nd.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\ne = sns.distplot(a = data[\"num_procedures\"], kde = False, ax=ax[2, 0])\ne.axes.set_title(\"Number of procedures\", fontsize=16)\ne.tick_params(labelsize=14)\ne.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nfig.delaxes(ax[2][1])\nfig.show()","b42c574c":"# Printing a correlation matrix for the numeric variables\ndata.corr()\n\n# Original code for correlation plot (runs into bug while executing on Kaggle)\n# corr = data.corr()\n# cm = sns.light_palette(\"red\", as_cmap=True) \n# corr.style.background_gradient(cmap = cm, axis = None)\\\n# .set_properties(**{\"max-width\": \"100p\", \"font-size\": \"20pt\"})\\\n# .set_precision(2)","d8b7385a":"sns.catplot(x = \"age\", y = \"num_medications\", data=data, jitter = \"0.25\")\nplt.gcf().set_size_inches(12, 8)\nplt.title(\"Age and Number of medications\", fontsize = 22)\nplt.show()","5d3db28c":"sns.catplot(x = \"race\", y = \"num_medications\", data=data, jitter = \"0.25\")\nplt.gcf().set_size_inches(12, 8)\nplt.title(\"Race and Number of medications\", fontsize = 22)\nplt.show()","cc0b2152":"sns.catplot(x = \"gender\", y = \"num_medications\", data=data, jitter = \"0.25\")\nplt.gcf().set_size_inches(8, 5)\nplt.title(\"Gender and Number of medications\", fontsize = 22)\nplt.show()","88ca0ebf":"data[data[\"num_medications\"] > 80]","abbbd830":"sns.catplot(x = \"A1Cresult\", y = \"num_medications\", hue = \"change\", data=data, jitter = \"0.25\")\nplt.gcf().set_size_inches(10, 5)\nplt.title(\"A1Cresult and Number of medications\", fontsize = 22)\nplt.show()","8608551a":"sns.catplot(x = \"discharge_disposition_id\", y = \"num_medications\", hue = \"readmitted\", data=data) \nplt.gcf().set_size_inches(15, 8)\nplt.title(\"Discharge disposition and readmitted status\", fontsize = 22)\nplt.show()","83299002":"fig, ax = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 30))\na = sns.boxplot(y = data[\"num_medications\"], ax=ax[0, 0], palette = \"Set2\")\na.axes.set_title(\"Boxplot: number of medications\", fontsize=16)\na.tick_params(labelsize=14)\na.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nb = sns.boxplot(y = data[\"num_procedures\"], ax=ax[0, 1], palette = \"Set2\")\nb.axes.set_title(\"Boxplot: number of procedures\", fontsize=16)\nb.tick_params(labelsize=14)\nb.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nc = sns.boxplot(y = data[\"num_lab_procedures\"], ax=ax[1, 0], palette = \"Set2\")\nc.axes.set_title(\"Boxplot: number of lab procedures\", fontsize=16)\nc.tick_params(labelsize=14)\nc.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nd = sns.boxplot(y = data[\"days_hospitalized\"], ax=ax[1, 1], palette = \"Set2\")\nd.axes.set_title(\"Boxplot: days in hospital\", fontsize=16)\nd.tick_params(labelsize=14)\nd.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\ne = sns.boxplot(y = data[\"num_diagnoses\"], ax=ax[2, 0], palette = \"Set2\")\ne.axes.set_title(\"Boxplot: number of diagnoses\", fontsize=16)\ne.tick_params(labelsize=14)\ne.set_ylabel(\"Count\", fontsize = 12, rotation = 0)\nfig.delaxes(ax[2][1])\nfig.show()","087833c3":"# Estimate the number of outliers\nprint(\"Approximate number of outliers in num_procedures is {}\".format(len(data[data[\"num_procedures\"] > 5])))\nprint(\"Approximate number of outliers in num_diagnoses is {}\".format(len(data[data[\"num_diagnoses\"] > 15])))\nprint(\"Approximate number of outliers in days_hospitalized is {}\".format(len(data[data[\"days_hospitalized\"] > 12])))\nprint(\"Approximate number of outliers in num_medications is {}\".format(len(data[data[\"num_medications\"] > 35])))\nprint(\"Approximate number of outliers in num_lab_procedures is {}\".format(len(data[data[\"num_lab_procedures\"] > 97])))","446fdbf1":"# Separating the predictors and target\npredictors = data.loc[:, data.columns != \"readmitted\"]\ntarget = data[\"readmitted\"]","85aef0ea":"# Train\/validation split in a 70:30 ratio\nX_train, X_valid, y_train, y_valid = train_test_split(predictors, target, test_size=0.3, random_state=0)","17961438":"# Preprocessing the training set\n\n# Separating the numeric columns\nnum = [\"num_medications\", \"num_procedures\", \"num_lab_procedures\", \"days_hospitalized\", \"num_diagnoses\"]\nnum_cols = X_train[num]\n\n# Separating categorical columns except ID mapping cols and target \ncat = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id',\n       'admission_source_id', 'payer_code', 'medical_specialty',\n       'max_glu_serum', 'A1Cresult', 'metformin',\n       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n       'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n       'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n       'insulin', 'glyburide.metformin', 'glipizide.metformin',\n       'metformin.rosiglitazone', 'metformin.pioglitazone', 'change',\n       'diabetesMed', 'diag_1_grouped',\n       'diag_2_grouped', 'diag_3_grouped']\n\n# Dummifying (one-hot-encoding) the categorical columns\ncat_cols = X_train[cat]\ncat_dummies = pd.DataFrame(pd.get_dummies(cat_cols, drop_first = True))\n\n# Merging the processed columns\nX_train = pd.concat([cat_dummies, num_cols], axis = 1)\n\n# Reformatting column names to avoid XGBoost model fitting errors \n\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\nX_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n\n# Label encoding the target variable\n\nlabel_encoder = LabelEncoder() \ny_train = label_encoder.fit_transform(y_train) \n","8bbe5357":"# Defining a function to plot a learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure(figsize = (10, 5))\n    plt.title(title, fontsize = 14)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training samples\", fontsize = 14)\n    plt.ylabel(\"Recall\", rotation = 0, fontsize = 14)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'recall')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.grid()\n    return plt","161605ab":"title = \"Learning Curves (XGBoost) - imbalanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = XGBClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","84a0ee43":"# Oversampling the positive class to balance the dataset\n\nros = RandomOverSampler(random_state=0)\nX, y_train = ros.fit_resample(X_train, y_train)\nX_train = pd.DataFrame(X, columns = X_train.columns)","1721b713":"title = \"Learning Curves (Logistic Regression) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","07ed1544":"title = \"Learning Curves (Decision Tree) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = DecisionTreeClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","dbaf2d03":"title = \"Learning Curves (Random Forest) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = RandomForestClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","6ce6dc04":"title = \"Learning Curves (NaiveBayes) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","5832b82a":"title = \"Learning Curves (Adaboost) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = AdaBoostClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","158e55dc":"title = \"Learning Curves (Gradient boosting) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = GradientBoostingClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","2eec9fce":"title = \"Learning Curves (XGBoost) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = XGBClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","98ee1c8b":"title = \"Learning Curves (K Nearest Neighbor) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = KNeighborsClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","d3837d85":"title = \"Learning Curves (SVM) - balanced data\"\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = SVC()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 1.01), cv=cv, n_jobs=1)\n\nplt.show()","b708ce28":"# Preprocessing validation data \n\nnum_cols = X_valid[num]\ncat_cols = X_valid[cat]\ncat_dummies = pd.DataFrame(pd.get_dummies(cat_cols, columns = cat_cols.columns, drop_first = True))\nX_valid = pd.concat([cat_dummies, num_cols], axis = 1)\n\n# Reformatting column names \n\nX_valid.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_valid.columns.values]\n\n# Label encoding the target\ny_valid = label_encoder.transform(y_valid) ","f7872e9b":"# Defining a function to fix column difference between train and test sets\n\n# This will go inside the main function\ndef add_missing_dummy_columns(test_set, train_columns):\n    # d = test set, columns = train set columns \n    missing_cols = set(train_columns) - set(test_set.columns)\n    for c in missing_cols:\n        test_set[c] = 0 # add missing columns to test set with empty column values\n        \n# This is the main function     \ndef fix_columns(test_set, train_columns):  \n\n    add_missing_dummy_columns(test_set, train_columns)\n\n    # make sure we have all the columns we need\n    assert(set(train_columns) - set(test_set.columns) == set())\n\n    extra_cols = set(test_set.columns) - set(train_columns) # these are the extra cols in the test set\n    if extra_cols:\n        print (\"extra columns:\", extra_cols)\n\n    test_set = test_set[train_columns] # keep only columns that are in the train set \n    return test_set\n\n# Fixing the test set\nX_valid_fixed = fix_columns(X_valid, X_train.columns)","15dbf37d":"# Best XGBoost classifier\nxgb = XGBClassifier(n_estimators = 50, max_depth = 5, gamma = 100, random_state = 0)\nxgb.fit(X_train, y_train)\npred_train = xgb.predict(X_train)\nprint(\"Classification Report: XGBoost (training data)\")\nprint(classification_report(y_train, pred_train))","5c03bd3a":"# Testing the XGBoost on validation data\npred_valid = xgb.predict(X_valid_fixed)\nprint(\"Classification Report: XGBoost (validation data)\")\nprint(classification_report(y_valid, pred_valid))","56f3a744":"# Checking XGBoost's predictions on the validation set\nprint(\"Confusion Matrix: XGBoost predictions (validation data)\")\npd.DataFrame(confusion_matrix(y_valid, pred_valid), columns = [\"Predicted 0\", \"Predicted 1\"])","346b07c8":"# Best Gradient boosting classifier\ngbm = GradientBoostingClassifier(n_estimators = 50, max_depth = 2, random_state = 0)\ngbm.fit(X_train, y_train)\npred_train = gbm.predict(X_train)\nprint(\"Classification Report: Gradient Boosting (training data)\")\nprint(classification_report(y_train, pred_train))","8d22d6ba":"# Testing the GBM on validation data\npred_valid = gbm.predict(X_valid_fixed)\nprint(\"Classification Report: Gradient Boosting (validation data)\")\nprint(classification_report(y_valid, pred_valid))","d2309f50":"# Visualizing one tree in the XGBoost ensemble\nprint(\"Sample tree visualization from the XGBoost model\")\nto_graphviz(xgb, num_trees = 0) ","dcbc15ad":"# Visualizing another tree in the XGBoost ensemble\nprint(\"Sample tree visualization from the XGBoost model\")\nto_graphviz(xgb, num_trees = 1) ","f0cc5a52":"# Feature importances plot - XGBoost\n\nplt.figure(figsize = (15, 15))\nfeat_importances = pd.Series(xgb.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.title(\"Top 20 patterns for predicting readmission within 30 days - XGBoost (64% validation Recall, outliers untreated)\", fontsize = 16)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 14)\nplt.show()","8abd4ede":"# Feature importances plot - GBM\n\nplt.figure(figsize = (15, 15))\nfeat_importances = pd.Series(gbm.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.title(\"Top 20 patterns for predicting readmission within 30 days - Gradient boosting (60% validation Recall, outliers untreated)\", fontsize = 16)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 14)\nplt.show()","e08c479d":"# Grouping data by important features and calculating readmission prevalence for each\n\ntwenty_two = data[data[\"discharge_disposition_id\"] == \"22\"]\nprint(\"{}% of patients discharged to a rehab facility are readmitted within 30 days\".format(np.around((twenty_two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (twenty_two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + twenty_two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nthree = data[data[\"discharge_disposition_id\"] == \"3\"]\nprint(\"{}% of patients discharged to a Skilled Nursing Facility are readmitted within 30 days\".format(np.around((three.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (three.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + three.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\ndiabetes = data[data[\"diabetesMed\"] == \"Yes\"]\nprint(\"{}% of patients on diabetes medication are readmitted within 30 days\".format(np.around((diabetes.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (diabetes.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + diabetes.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nmetformin = data[data[\"metformin\"] == \"No\"]\nprint(\"{}% of patients not prescribed metformin are readmitted within 30 days\".format(np.around((metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nage = data[data[\"age\"] == \"[50-60)\"]\nprint(\"{}% of 50-60 year old patients are readmitted within 30 days\".format(np.around((age.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (age.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + age.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\ntwo = data[data[\"discharge_disposition_id\"] == \"2\"]\nprint(\"{}% of patients discharged to a short term hospital are readmitted within 30 days\".format(np.around((two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + two.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nfive = data[data[\"discharge_disposition_id\"] == \"5\"]\nprint(\"{}% of patients discharged to another inpatient care institution are readmitted within 30 days\".format(np.around((five.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (five.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + five.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nsix = data[data[\"discharge_disposition_id\"] == \"6\"]\nprint(\"{}% of patients discharged to home with health service are readmitted within 30 days\".format(np.around((six.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (six.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + six.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nfifteen = data[data[\"discharge_disposition_id\"] == \"15\"]\nprint(\"{}% of patients discharged to a swing bed are readmitted within 30 days\".format(np.around((fifteen.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (fifteen.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + fifteen.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\ntwenty_eight = data[data[\"discharge_disposition_id\"] == \"28\"]\nprint(\"{}% of patients discharged to a psychiatric hospital are readmitted within 30 days\".format(np.around((twenty_eight.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (twenty_eight.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + twenty_eight.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nglu_serum = data[data[\"max_glu_serum\"] == \">300\"]\nprint(\"{}% of patients with glu serum > 300 are readmitted within 30 days\".format(np.around((glu_serum.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (glu_serum.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + glu_serum.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\npioglitazone = data[data[\"pioglitazone\"] == \"Up\"]\nprint(\"{}% of patients whose pioglitazone level == Up are readmitted within 30 days\".format(np.around((pioglitazone.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (pioglitazone.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + pioglitazone.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\n","fe2adf3d":"# Calculating the prevalence of each of the above categories in the patient population\nprint(\"{}% of all patients are discharged to an SNF\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"3\"]) \/ len(data)) * 100, 2)))\nprint(\"{}% of all patients are discharged to rehab\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"22\"]) \/ len(data)) * 100, 2))) \nprint(\"{}% of all patients are discharged to a short term hospital\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"2\"]) \/ len(data)) * 100, 2)))\nprint(\"{}% of all patients are discharged to another inpatient care institution\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"5\"]) \/ len(data)) * 100, 2)))\nprint(\"{}% of all patients are discharged to home with health care service\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"6\"]) \/ len(data)) * 100, 2))) \nprint(\"{}% of all patients are discharged to a swing bed\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"15\"]) \/ len(data)) * 100, 2))) \nprint(\"{}% of all patients are discharged to a psychiatric hospital\".format(np.around((len(data[data[\"discharge_disposition_id\"] == \"28\"]) \/ len(data)) * 100, 2))) \nprint(\"{}% of all patients have >300 max glu serum\".format(np.around((len(data[data[\"max_glu_serum\"] == \">300\"]) \/ len(data)) * 100, 2))) \nprint(\"{}% of all patients have pioglitazone == Up\".format(np.around((len(data[data[\"pioglitazone\"] == \"Up\"]) \/ len(data)) * 100, 2))) \n","008b30bc":"# Printing readmission trends for numeric features \nprint(data.groupby([\"num_diagnoses\", \"readmitted\"])[\"readmitted\"].agg(\"count\"))\nprint()\nprint(data.groupby([\"days_hospitalized\", \"readmitted\"])[\"readmitted\"].agg(\"count\"))","fedb555b":"# Combining features and calculating readmission prevalence for the combined groups\ndiabetes_snf = (data[(data[\"discharge_disposition_id\"] == \"3\") & (data[\"diabetesMed\"] == \"Yes\")])\nprint(\"{}% of patients on diabetes medication who are discharged to a Skilled Nursing Facility are readmitted within 30 days\".format(np.around((diabetes_snf.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (diabetes_snf.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + diabetes_snf.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\n\nhigh_counts = (data[(data[\"days_hospitalized\"] > 4) & (data[\"num_diagnoses\"] > 7) & (data[\"num_medications\"] > 12)])\nprint(\"{}% of patients with more than 7 diagnoses and more than 4 days spent in the hospital are readmitted within 30 days\".format(np.around((high_counts.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (high_counts.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + high_counts.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\ndiabetes_metformin = (data[(data[\"diabetesMed\"] == \"Yes\") & (data[\"metformin\"] == \"No\") & (data[\"payer_code\"] == \"unknown\")])\nprint(\"{}% of patients on diabetes medication including metformin are readmitted within 30 days\".format(np.around((diabetes_metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) \/ \n                                                                                             (diabetes_metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[0] + diabetes_metformin.groupby(\"readmitted\")[\"readmitted\"].agg(\"count\")[1]) * 100), 2))\nrehab_hm = (data[(data[\"discharge_disposition_id\"] == \"22\") & (data[\"payer_code\"] == \"HM\")])\nprint(\"Zero rehab patients with payer code HM are readmitted within 30 days\")\n","d94fdb94":"# Preprocessing data with scaling \n# Train validation split\nX_train, X_valid, y_train, y_valid = train_test_split(predictors, target, test_size=0.3, random_state=0)\n\n# Separating the numeric columns\nnum = [\"num_medications\", \"num_procedures\", \"num_lab_procedures\", \"days_hospitalized\", \"num_diagnoses\"]\nnum_cols = X_train[num]\n\n# Scaling the numeric columns to have a mean of 0 and standard deviation of 1\n\nscaler = StandardScaler()\nnum_scaled = pd.DataFrame(scaler.fit_transform(num_cols), columns = num)\n\n# Separating categorical columns except ID mapping cols and target \ncat = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id',\n       'admission_source_id', 'payer_code', 'medical_specialty',\n       'max_glu_serum', 'A1Cresult', 'metformin',\n       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n       'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n       'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n       'insulin', 'glyburide.metformin', 'glipizide.metformin',\n       'metformin.rosiglitazone', 'metformin.pioglitazone', 'change',\n       'diabetesMed', 'diag_1_grouped',\n       'diag_2_grouped', 'diag_3_grouped']\n\n# Dummifying the categorical columns\ncat_cols = X_train[cat]\ncat_dummies = pd.DataFrame(pd.get_dummies(cat_cols, drop_first = True))\n\n# Aligning indexes so the dfs merge properly\nnum_scaled.set_index(cat_dummies.index, inplace = True)\n\n# Merging the processed columns\nX_train = pd.concat([cat_dummies, num_scaled], axis = 1)\n\n# Reformatting column names to avoid some model fitting errors\nimport re\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\nX_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n\n# Label encoding the target variable\nlabel_encoder = LabelEncoder() \ny_train = label_encoder.fit_transform(y_train) \n\n# Oversampling the positive class to balance the dataset\nros = RandomOverSampler(random_state=0)\nX, y_train = ros.fit_resample(X_train, y_train)\nX_train = pd.DataFrame(X, columns = X_train.columns)","f957a731":"# Preprocessing validation data \n\nnum_cols = X_valid[num]\nnum_scaled = pd.DataFrame(scaler.transform(num_cols), columns = num)\ncat_cols = X_valid[cat]\ncat_dummies = pd.DataFrame(pd.get_dummies(cat_cols, cat_cols.columns, drop_first = True))\nnum_scaled.set_index(cat_dummies.index, inplace = True)\nX_valid = pd.concat([cat_dummies, num_scaled], axis = 1)\n\n# Reformatting column names \n\nX_valid.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_valid.columns.values]\n\n# Label encoding the target\ny_valid = label_encoder.transform(y_valid) \n\n# Fixing the validation set columns\nX_valid_fixed = fix_columns(X_valid, X_train.columns)","e5f2ebd3":"# Building a KNN model and predicting on train data\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\npred_train = knn.predict(X_train)\nprint(\"Classification Report: KNN (training data)\")\nprint(classification_report(y_train, pred_train))","9d4f0504":"# Applying KN model to validation data \npred_valid = knn.predict(X_valid_fixed)\nprint(\"Classification Report: KNN (validation data)\")\nprint(classification_report(y_valid, pred_valid))","f1ebed0e":"# Building an SVM model and predicting on train data\nsvm = SVC(kernel = \"rbf\", C = 0.05)\nsvm.fit(X_train, y_train)\npred_train = svm.predict(X_train)\nprint(\"Classification Report: SVM (training data)\")\nprint(classification_report(y_train, pred_train))","48e21e23":"# Applying SVM model to validation data \npred_valid = svm.predict(X_valid_fixed)\nprint(\"Classification Report: SVM (validation data)\")\nprint(classification_report(y_valid, pred_valid))","c039208b":"# Neural network with one hidden layer\n\nseed(3)\nset_random_seed(3)\n\nmodel = Sequential()\nmodel.add(Dense(units=10, input_dim=173, activation='sigmoid', kernel_initializer='normal')) \nmodel.add(Dense(units=10, input_dim=173, activation='sigmoid', kernel_initializer='normal')) \nmodel.add(Dense(units=1, activation='sigmoid', kernel_initializer='normal'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [\"accuracy\"])\n%time model.fit(X_train, y_train, epochs=1, batch_size=64)\n\npred_train = model.predict_classes(X_train)\nprint(\"Classification Report: MLP - one hidden layer (training data)\")\nprint(classification_report(y_train, pred_train))","c3ec86e1":"pred_valid = model.predict_classes(X_valid_fixed)\nprint(\"Classification Report: MLP - one hidden layer (validation data)\")\nprint(classification_report(y_valid, pred_valid))","2501d4ea":"# MLP with two hidden layers\n\nseed(4)\nset_random_seed(1)\n\nmodel2 = Sequential()\nmodel2.add(Dense(units=10, input_dim=173, activation='sigmoid', kernel_initializer='normal')) \nmodel2.add(Dense(units=10, activation='sigmoid', kernel_initializer='normal')) \nmodel2.add(Dense(units=10, activation='sigmoid', kernel_initializer='normal')) \nmodel2.add(Dense(units=1, activation='sigmoid', kernel_initializer='normal'))\nmodel2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [\"accuracy\"])\n%time model2.fit(X_train, y_train, epochs=1, batch_size=64)\n\npred_train = model2.predict_classes(X_train)\nprint(\"Classification Report: MLP - two hidden layers (training data)\")\nprint(classification_report(y_train, pred_train))","0fdcc9fc":"pred_valid = model2.predict_classes(X_valid_fixed)\nprint(\"Classification Report: MLP - two hidden layers (validation data)\")\nprint(classification_report(y_valid, pred_valid))","b684820a":"# MLP with three hidden layers\n\nseed(4)\nset_random_seed(1)\n\nmodel3 = Sequential()\nmodel3.add(Dense(units=10, input_dim=173, activation='sigmoid', kernel_initializer='normal')) \nmodel3.add(Dense(units=10, activation='sigmoid', kernel_initializer='normal')) \nmodel3.add(Dense(units=10, activation='sigmoid', kernel_initializer='normal')) \nmodel3.add(Dense(units=10, activation='sigmoid', kernel_initializer='normal')) \nmodel3.add(Dense(units=1, activation='sigmoid', kernel_initializer='normal'))\nmodel3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [\"accuracy\"])\n%time model3.fit(X_train, y_train, epochs=1, batch_size=64)\n\npred_train = model3.predict_classes(X_train)\nprint(\"Classification Report: MLP - three hidden layers (training data)\")\nprint(classification_report(y_train, pred_train))","ff47f7ec":"pred_valid = model3.predict_classes(X_valid_fixed)\nprint(\"Classification Report: MLP - three hidden layers (validation data)\")\nprint(classification_report(y_valid, pred_valid))","7f4db765":"# Reading in the test data\n\ndata5 = pd.read_csv(\"..\/input\/Test.csv\", na_values = [\"NA\", \" \", \"?\"])\ndata6 = pd.read_csv(\"..\/input\/Test_HospitalizationData.csv\", na_values = [\"NA\", \" \", \"?\"])\ndata7 = pd.read_csv(\"..\/input\/Test_Diagnosis_TreatmentData.csv\", na_values = [\"NA\", \" \", \"?\"])\n\nprint(\"The number of rows and columns in the patient demographics data is\", data5.shape)\nprint(\"The number of rows and columns in the patient hospitalization data is\", data6.shape)\nprint(\"The number of rows and columns in the patient diagnosis and treatment data is\", data7.shape)","ff7d0e04":"# Merging the first three datasets\n# The sort argument will align data2's patient IDs with data1's\ntest_data = data5.merge(data6, on = \"patientID\", sort = True).merge(data7, on = \"patientID\", sort = True)\n\n# Mapping the id cols\ntest_data[id_cols] = test_data[id_cols].astype(str)\n\ntest_data[\"admission_type\"]= test_data[\"admission_type_id\"].map(admission_type_id_dict)\ntest_data[\"discharge_disposition\"]= test_data[\"discharge_disposition_id\"].map(discharge_disposition_id_dict)\ntest_data[\"admission_source\"]= test_data[\"admission_source_id\"].map(admission_source_id_dict)\n\n# Counting expired patients \nprint(\"The number of expired patients in the test dataset is {}\".format(len(test_data[(test_data[\"discharge_disposition_id\"] == \"11\") | (test_data[\"discharge_disposition_id\"] == \"19\") | (test_data[\"discharge_disposition_id\"] == \"20\")])))","2c2cd615":"# Separating expired patients \nexpired = test_data[(test_data[\"discharge_disposition_id\"] == \"11\") | (test_data[\"discharge_disposition_id\"] == \"19\") | (test_data[\"discharge_disposition_id\"] == \"20\")]","113d9ba3":"# Dropping AdmissionID, acetohexamide, weight and setting PatientID as the index\ntest_data.drop([\"AdmissionID\", \"weight\", \"acetohexamide\"], axis = 1, inplace = True)\n\n# Filling NA values with \"unknown\" since they only occur in categorical columns\ntest_data.fillna(\"unknown\", inplace = True)\ntest_data.set_index(\"patientID\", inplace = True)\n\n# Feature engineering\n# Converting the date columns to datetime type\ntest_data[\"Admission_date\"] = pd.to_datetime(test_data[\"Admission_date\"], format = \"%Y-%m-%d\")\ntest_data[\"Discharge_date\"] = pd.to_datetime(test_data[\"Discharge_date\"], format = \"%Y-%m-%d\")\n\n# Subtracting the admission_date and discharged_date columns to get a new column called days_hospitalized\ntest_data[\"days_hospitalized\"] = ((test_data[\"Discharge_date\"] - test_data[\"Admission_date\"]).dt.days)\n\n# Grouping the columns with hundreds of levels\n\ntest_data[\"diag_1_grouped\"] = test_data[\"diagnosis_1\"].apply(grouper1)\ntest_data[\"diag_2_grouped\"] = test_data[\"diagnosis_2\"].apply(grouper1)\ntest_data[\"diag_3_grouped\"] = test_data[\"diagnosis_3\"].apply(grouper1)\n\n# Dropping the original diagnosis columns\ntest_data.drop([\"diagnosis_1\", \"diagnosis_2\", \"diagnosis_3\"], axis = 1, inplace = True)\n\ntest_data[\"medical_specialty\"] = test_data[\"medical_specialty\"].apply(grouper2)\ntest_data[\"payer_code\"] = test_data[\"payer_code\"].apply(grouper3)","ab67e2a3":"# Preprocessing the test data\n\n# Scaling the numeric columns to have a mean of 0 and standard deviation of 1\nnum_cols = test_data[num]\nnum_scaled = pd.DataFrame(scaler.fit_transform(num_cols), columns = num)\n\n# Separating categorical columns except ID mapping cols \ncat = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id',\n       'admission_source_id', 'payer_code', 'medical_specialty',\n       'max_glu_serum', 'A1Cresult', 'metformin',\n       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n       'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',\n       'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n       'insulin', 'glyburide.metformin', 'glipizide.metformin',\n       'metformin.rosiglitazone', 'metformin.pioglitazone', 'change',\n       'diabetesMed', 'diag_1_grouped',\n       'diag_2_grouped', 'diag_3_grouped']\n\n# Dummifying the categorical columns\ncat_cols = test_data[cat]\ncat_dummies = pd.DataFrame(pd.get_dummies(cat_cols, drop_first = True))\n\n# Aligning indexes so the dfs merge properly\nnum_scaled.set_index(cat_dummies.index, inplace = True)\n\n# Merging the processed columns\ntest = pd.concat([cat_dummies, num_scaled], axis = 1)\n\n# Reformatting column names to avoid some model fitting errors\nimport re\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\ntest.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in test.columns.values]\n\n# Fixing any column mismatch\ntest_fixed = fix_columns(test, X_train.columns)\n","89fcb1de":"# Neural network predictions\n\npreds = model.predict_classes(test_fixed)\n# Creating a df of preds and patientID\npatientID = test_fixed.index\npreds_with_id = pd.DataFrame(patientID)\npreds_with_id[\"readmitted\"] = preds\n# Creating a list of expired patient ids\nexpired_patients = expired[\"patientID\"]\n# Setting the predictions for all expired patients to 0\npreds_with_id.loc[preds_with_id[\"patientID\"].isin(expired_patients), \"readmitted\"] = 0\n\n# Writing the predictions to a csv file\npreds_with_id.to_csv(\"preds.csv\", index = False)","9989330c":"KNN does not perform well on the validation data, while SVM performs similarly to XGBoost.\n\nWe previously observed that some of our models have high bias. Adding complexity to the model is one way of dealing with bias, so let's try a more complex type of model now - Multi Layer Perceptrons a.k.a. fully connected artificial Neural Networks.\n\nThe models below were tried with both sigmoid and ReLu activation functions in the middle layers, but sigmoid performed better. Keep in mind that our evaluation metric is still Recall.","898022fd":"#### Observations\n- The column \"acetohexamide\" has only one unique value and therefore adds the same info for each patient. We will drop it as such info has no predictive value\n- Admission ID is not useful since we already have patientID. We'll make patientID the dataframe's index so it remains in sight \n- The diagnosis columns have hundreds of unique ICD 9 codes which will be difficult to analyse individually. In the Feature Engineering step, we will group these codes as per this list on [Wikipedia](http:\/\/en.wikipedia.org\/wiki\/List_of_ICD-9_codes)\n- We'll also group all payer codes and medical specialties with less than 1000 samples into a single category, to reduce the number of categories in those columns\n- We will handle the date columns by converting them to datetime objects","515ea86c":"There are several Caucasians taking a higher number of medications than everyone else. The race barplot above had showed that Caucasians make up 73% of the patients.","a4c87efd":"Most of them have a higher readmission prevalence than the global prevalence of 14%.\n\nWhat proportion of the patient population are these patient groups?","f26fab8e":"We are now ready to build and test models.","224e78bc":"# Learning Curves \n\nWhat is a learning curve? In the simplest terms, it's a graph that compares how a predictive model performs on train and test data as we keep adding more data points. Learning curves can help us identify bias and variance in our models. Let's plot learning curves for this data using various classification algorithms and see the results. \n\nThe first plot shows how the target class imbalance results in a high bias model i.e. a model that is unable to learn the positive (minority) class well. The remaining curves are plotted after rebalancing the target class in a 50:50 ratio using random overampling. \n\nNote: Only X_train and y_train are rebalanced. We won't rebalance X_valid and y_valid as we want the validation data to represent the unseen\/test data.","f697776a":"### Validation data preprocessing\n\nWe repeat the same preprocessing steps for the validation data.\n\nSince dummification can result in \"extra\" or \"missing\" columns in the train and test sets, the last step of our preprocessing will be fixing these differences.","1bfc5294":"### Other models\n\nWe will also try KNN, SVM and Neural Networks as decided earlier. The input data to these models will undergo an additional preprocessing step of scaling, as unscaled data can affect the performance of distance-based algorithms like KNN. (The input data for our tree-based models was left unscaled for better model interpretability).","d3a17d88":"ID mapping\ndata4 contains mapping information for the numerous IDs in the admission_source_id, admission_type_id and discarge_disposition_id columns. For example, it tells us that admission_type_id 1 = Emergency, admission_source_id 1 = Physician Referral, discarge_disposition_id 1 = Discharged to home, and so on.\n\nTo make it easy to interpret these IDs for any patient, we'll add the mapping info to the dataframe with the following code.","0c58a4df":"Remember when we decided not to treat the outliers in the data? If we had capped them, num_diagnoses and days_hospitalized would not be in the top 20 feature importances. \n\nLet's check the readmission trends for the most important features according to our ensemble models, plus some features we noted during EDA:","04e885d1":"# Data preprocessing\n\nWe've done our preliminary data cleaning, feature engineering and exploration. Now we prepare the data for predictive modelling. Since the end goal is to have our models to perform well on unseen\/test data, we will split our data into training and validation sets. ","d62c6ed6":"### Rules \n\nBased on the tree-based feature importances and EDA we can create some rules for identifying patients likely to be readmitted. \n\n\n1. Patients discharged to a rehab facility \n2. Higher number of diagnoses and days hospitalized (the chance of readmission increases steadily)\n3. Patients discharged to a Skilled Nursing Facility\n4. Patients on diabetes medication\n5. Patients discharged to a short term hospital \n6. Patients discharged to another inpatient care institution \n7. Patients discharged to a swing bed \n8. Patients discharged to a psychiatric hospital \n9. Patients with increased pioglitazone levels \n10. Patients with max glu serum > 300 \n\nCombinations of factors can make readmission chances higher. For example, patients discharged to SNF have a chance of 20% and patients on diabetes meds have a chance of 15%, but patients who are on diabetes meds AND are discharged to an SNF have a chance of 22%.\n\nNote: While our analysis also says that rehab patients with payer code HM are not readmitted, we should take this \"rule\" with a pinch of salt as only 26 patients fall under this category.","1d5c6bc2":"It looks like we have hospital data for 34,650 patients in our training set. The fourth csv file contains some extra information that we'll integrate with the main data.\nLet's take a quick look at the individual datasets.","76870285":"# Data preparation\n\nWe have been given multiple csv files containing different kinds of hospital data about the same set of patients. A quick look at the files showed several missing values denoted with ?, which we can specify while reading in the data. \nIn this preparatory stage we'll view snapshots of the csv files and merge them together carefully.","73c04814":"We will drop the weight column since almost all the data in it is missing.","148140c4":"Nearly half the data in payer_code and medical_specialty is missing. It would be acceptable to drop these columns too but I decided to keep them and assign the missing values to a new category called \"unknown\".\n\nBefore we do that, we'll check the summary statistics for the dataset using data.describe()","45bab92e":"We begin with tree-based models and interpret them with sklearn's classification report before trying other models. The hyperparameters for XGBoost are tuned such that it can generalize well to the imbalanced validation and test data.","79f447a2":"Hello! This kernel is an abridged walkthrough of my final project at INSOFE Bangalore. It tackles an interesting data science problem and contains many visualizations, insights and predictive modelling techniques that some (hopefully) may find useful. \n\nWe are given some data from a US hospital and the challenge is to predict which patients will be readmitted within 30 days of their discharge. This is a pesky problem for US hospitals as they are penalized for a high number of readmissions. \n\nFrom a Machine Learning perspective, this is a binary classification problem where the target classes are - readmitted within 30 days (positive class) and not readmitted within 30 days (negative class). We are asked to optimize our models for Recall or true positive rate, as the hospital's priority is to correctly identify all patients who are likely to be readmitted. \n\nWe'll go through the following steps:\n\n- Data preparation\n- Exploratory Data Analysis\n- Feature engineering \n- Data visualizations \n- Data preprocessing \n- Learning Curves \n- Model building and validation\n- Patterns and rules extraction\n- Prediction on test data\n- Summary and conclusion\n","e625342d":"# Exploratory Data Analysis\n\nHere are some interesting finds from the EDA:","5fed5dc5":"The most common ICD group is 390-459 (diseases of the circulatory system) followed by 240-279 (endocrine, nutritional and metabolic diseases, and immunity disorders). Diabetes comes under group 240-279 (ICD code 250).\n\nWe can also see that the target class (\"readmitted\") is highly imbalanced. We'll address this later.\n\nNext, we'll plot histograms to see the distribution of numeric columns.","78c76694":"#### Observations\n- Algorithms are learning better with rebalanced data.\n- The best cross-validated Recall score is by KNN. \n- XGBoost, Gradient Boosting, SVM have more bias but less variance. SVM and KNN have a very high training time.\n- The untuned Decision Tree and Random Forest are overfitting as they reach 100% Recall after training on 30,000 samples. They might not generalise well (high variance). \n- Naive Bayes is overfitting.\n- Logistic Regression and Adaboost have the highest bias. \n- We will tune and test KNN, SVM, GBM, XGB.\n- If a bias problem persists we will try neural networks.","1240f244":"# Model building and validation ","9079a58d":"Let's count the number of missing values in our data:","6f4c3e02":"#### Outliers decision\n\nThe following steps (till a basic model prediction and feature importance plotting) were tried with both untreated and treated (capped) outliers. After treating the outliers, Recall score on training and validation was lower, and the top 20 predictive features changed.\nSince our priority is to improve Recall and since the outliers may be important, we'll proceed without treating the outliers.","f1c26c5d":"# Data Visualizations\n\nNext, we'll create and interpret the following plots to understand the data better:\n\n- Univariate barplots of chosen categorical features and the target variable\n- Histograms and boxplots of the numeric features\n- Catplots plotting chosen features against each other and against the target\n\nLet's begin with the patient demographics. ","4226fde3":"# Feature Engineering \n\nWe will create a new column called days_hospitalized and write some functions to group the categorical features mentioned above.","5de1aa12":"How do the numeric features contribute to the prediction?","878ce5f7":"There are no significant correlations among the numeric features (values close to 1 indicate strong positive correlation and values close to -1 indicate strong negative correlation)\n\nWe will now plot some catplots to look at patterns between demographic attributes and number of medications.","1dca9b6e":"We can see that data1 contains demographic info and the target variable, \"readmitted\". \n\ndata2 contains hospitalization info such as admit\/discharge date, and data3 contains info on each patient's diagnoses and medications. All the medications are diabetes-related, indicating that this dataset or hospital is focused on diabetics.\n\npatientID is the common column across the three training datasets. We'll use this column to merge them. \nHowever this column is ordered differently in data2 so we need to make sure the IDs don't get mixed up.","225280cb":"# Summary & Conclusion \n\nExploratory Data Analysis and visualizations revealed several interesting patterns in the data. We plotted learning curves for various classification algorithms and rebalanced the training data to reduce the high bias. \n\nAmong the models we tried, MLPs gave the highest Recall (up to 100%) with a steep decline in Precision. A tradeoff between Recall and Precision was observed in all models. XGBoost gave a similar performance to SVM and the simplest MLP (64%). XGBoost may be the most practical choice \/ recommendation to a hospital, since it had fewer false positives and higher interpretability. \n\nAll the models got the same Recall scores on test data as on validation data, i.e. they generalized well. \n\nWe used insights from our tree-based models and EDA to generate the top patterns and rules for identifying patients who are likely to be readmitted within 30 days. These rules can help the hospital direct special attention towards such patients, reduce readmissions and avoid penalties.","512b2746":"As would be expected, younger people are on fewer medications. \nThe number of medications increases steadily from age 0 to 70 and then starts decreasing again from 70 to 90. We might be tempted to think it's because there are fewer patients over 70 years old, but our age barplot showed that 70-80 is the most common age group. ","6b9b345e":"We're tried and tested several predictive models. The final step in this project was to predict on the test dataset and get scored via an online tool, like Kaggle competitions.\n\nI am including my code up to the creation of the test predictions because it contains one very important step. Can you guess what that is?","30e06caf":"There's a clear positive correlation between readmissions and these numeric features.\n\nWhat about combinations of factors?","13f7db02":"A1C is a test for diagnosing and monitoring diabetes. The above plot indicates that most patients with A1Cresult > 8 were prescribed a change in medications (blue colour dominates). However, we don't know which happened first (the change or the high test result).\n\nNext, we'll make some catplots with the target variable and find some interesting patterns:","fe9cf781":"Some interesting observations here - the most common number of diagnoses is 9, the most common number of lab procedures is around 45, and the most common number of medications is around 12. Since the most common number of \"procedures\" is 0, we can guess this refers to more serious medical procedures.\n\nAre there any correlations between these numeric features? Let's check with a correlation matrix.","9083552e":"# Prediction on test data\n\nThe following code creates a test predictions file. And this is the important step - we will use replacement to ensure the prediction for all expired patients is 0 (remember the expired patients we dropped from the training data in the beginning?!)","473780b3":"Discharge disposition ids 15, 22 and 28 seem to have a relatively high proportion of patients readmitted within 30 days. The mappings for these ids are: discharged within this institution to swing bed, discharged to rehab facility, and discharged to psychiatric hospital, respectively. \n\nWe've explored some univariate and bivariate distributions. Finally, we will examine the spread of our numeric features with boxplots. We've saved this for the end because we'll need to make a decision based on the boxplots. ","494791eb":"Except for one male outlier the number of medications are similarly distributed for both genders. Looking at the previous two plots, we can deduce that this patient who is taking over 80 medications is a white male aged between 60-70. Let's confirm this:","089ee1f5":"In the next plot we look at how diabetes test results and medications interact.","784f53da":"# Patterns and rules extraction\n\nLet's plot the top features for each of our ensemble models to identify which factors are important in predicting readmissions. We will then measure the contributions of these factors both to readmissions and to the overall data.","3504c8fc":"The discharge dispositions tell us something important which will affect our predictions. Unhide the code output above and see if you can spot it.\n\nSome patients have \"expired\" as their discharge disposition - which means they obviously can't be readmitted! We will count and drop these patients from the dataset."}}