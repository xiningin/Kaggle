{"cell_type":{"728886ed":"code","adcf26e4":"code","ef2b553c":"code","ae5cc91b":"code","7a3b965b":"code","563b45a2":"code","0d72c69c":"code","e99687ff":"code","fe64ae5b":"code","94bdb85a":"code","3d3fcf81":"code","684a1c88":"code","44c3c457":"code","2521a322":"code","0cfafa5e":"code","1d480a7d":"code","496e7987":"code","373fd571":"code","4aeb4ada":"markdown","f5660c52":"markdown","0349e1b6":"markdown","f7d56a2d":"markdown","efbeb107":"markdown","591aae14":"markdown"},"source":{"728886ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","adcf26e4":"# read training data\ndata = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')","ef2b553c":"# first few rows\ndata.head()","ae5cc91b":"# null check\ndata.isna().values.any()","7a3b965b":"# extract labels\nX = data\ny = X.pop('Cover_Type')","563b45a2":"# split training data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","0d72c69c":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","e99687ff":"lgr_simple = LogisticRegression()\nlgr_simple.fit(X_train, y_train)\nlgr_simple.score(X_test, y_test)","fe64ae5b":"# we expect this to do better than the simpler\nlgr = LogisticRegression(solver='newton-cg', multi_class='multinomial')\nlgr.fit(X_train_scaled, y_train)\nlgr.score(X_test_scaled, y_test)","94bdb85a":"lgr = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=50, max_iter=100)\nlgr.fit(X_train_scaled, y_train)\nlgr.score(X_test_scaled, y_test)","3d3fcf81":"lgr_cv = LogisticRegression(multi_class='multinomial')\nparameters = {'solver': ['lbfgs', 'newton-cg'], 'C': [ 0.1, 1, 10, 30, 50, 60, 70, 90]}\ngscv = GridSearchCV(lgr_cv, parameters, cv=5)\ngscv.fit(X_train_scaled, y_train)\nprint(\"Best Accuracy For Logistic Regression: .{:02}\".format(gscv.best_score_))\n","684a1c88":"# Params giving the best accuracy\ngscv.best_estimator_","44c3c457":"# Linear classifier didn't do very well earlier, so lets jump right into a complex non-linear kernel\nsvc = SVC(gamma='scale', kernel='rbf')\n# select a sufficiently big range and hope that find optimal C lies in this range. If not, we will atleast get an idea about the right direction, i.e go higher or lower \nparameters= {'C': [ 0.01, 0.1, 1, 10, 100, 500, 800, 900, 1000, 1100, 3000]} \ngscv_svc = GridSearchCV(svc, parameters, cv=5)\ngscv_svc.fit(X_train_scaled, y_train)\nprint(\"Best Accuracy : .{:02}\".format(gscv_svc.best_score_))","2521a322":"gscv_svc.best_estimator_","0cfafa5e":"best_svc = SVC(gamma='scale', kernel='rbf', C=900)\nt_scaler = preprocessing.StandardScaler().fit(X)\nX_scaled = t_scaler.transform(X)\nbest_svc.fit(X_scaled, y)","1d480a7d":"test_data = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')","496e7987":"test_scaled = t_scaler.transform(test_data)\npredictions = best_svc.predict(test_scaled)","373fd571":"# write predictions to csv in specified format\noutput = pd.DataFrame({'ID': test_data.index,\n                       'Cover_Type': predictions})\n\noutput.to_csv('submission.csv', index=False)","4aeb4ada":"**Simple Logistic Regression**\n\nLogistic Regression is one of the simplest linear classifier, it wouldnt be too good but should do better than chance i.e accuracy > 50%","f5660c52":"**Predict Test Data**","0349e1b6":"**Grid Search**********\n\nIterate through a set of hyperparams and find the ones that perform the best","f7d56a2d":"**Scale Features**\nNormalize features\/columns in data and bring them to a similar scale. This will avoiding improperly large weights to features with larger values","efbeb107":"**SVC**\n\nGiven dataset is not linearly separable thats and a simple classifier like Logistic Regression will probably underfit. Lets go a step deeper and use Support Vector Machines. We dont expect it to be give the best classifier but will do much better than Logistic Regression","591aae14":"**Train a model with found parameters on the whole dataset**"}}