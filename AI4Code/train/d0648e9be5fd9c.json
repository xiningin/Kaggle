{"cell_type":{"31d76965":"code","5dc0d4dc":"code","1130cc7a":"code","f3bbf29d":"code","6cc7daf1":"code","fee9f457":"code","7717d351":"code","a6de19ba":"code","d5c5b0ca":"code","5715205f":"code","4a57a8ef":"code","78080e74":"code","f9dcacfb":"code","fbf04a85":"code","f5e8cd65":"code","eb6ba6f1":"code","b4ad6f82":"code","151a4d18":"code","eabefcce":"code","dd9c269c":"code","4afbdc51":"code","93c6b39a":"code","37f8ca1e":"code","7e0108c3":"code","2fb973d3":"code","14f6f922":"code","599818e6":"code","cb2c3dbb":"code","2ad04160":"code","1ae9a6fc":"code","2d61cc8d":"code","f21ed8af":"code","2891e460":"code","4dd15893":"code","9ca69bb7":"code","e3c1d2d0":"code","91f01aef":"code","47b924dc":"code","419c00b0":"code","86f2e2e5":"code","3e6fb6bf":"code","4097803d":"code","cfb38149":"code","26281d5d":"code","4a8905d2":"code","04a02a42":"code","654b8e23":"code","ef6fd31c":"code","48603577":"code","29c7379c":"markdown","5d76736c":"markdown","d6f28759":"markdown","4c348d8a":"markdown","3bbe92ea":"markdown","4aaf412b":"markdown","aa0a87c8":"markdown","a19acf4e":"markdown","0aa0c9a0":"markdown","f599cbda":"markdown","2fff48f6":"markdown","82688aea":"markdown","d36e6918":"markdown","ca1fdb3d":"markdown","ffa90d59":"markdown","172f7064":"markdown","ef2c45a7":"markdown","8fd66f50":"markdown","d40758c7":"markdown","37172c06":"markdown","245c4326":"markdown","ab153fad":"markdown","996cec5d":"markdown","cccf86b8":"markdown","36390d32":"markdown","89366e10":"markdown","afe7e22e":"markdown","58248c2f":"markdown","4394d117":"markdown","d9442be5":"markdown","ed57af13":"markdown","adafe74a":"markdown","3fcb06aa":"markdown","d85daac0":"markdown","739a9007":"markdown","544aa74d":"markdown","11a8d70d":"markdown","76779a20":"markdown"},"source":{"31d76965":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score","5dc0d4dc":"!pip install factor_analyzer\n\nfrom factor_analyzer import FactorAnalyzer\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer.factor_analyzer import calculate_kmo","1130cc7a":"DATASET = \"..\/input\/student-grade-prediction\/student-mat.csv\"\ndataset =  pd.read_csv(DATASET, sep=\",\")","f3bbf29d":"dataset.info()","6cc7daf1":"dataset[\"school\"].value_counts()","fee9f457":"drop_features = [\"school\"]\n\ndata = dataset.drop(labels=drop_features, axis=1)","7717d351":"def ordinal_encode(series, categories):\n  encoder = OrdinalEncoder(categories=categories, dtype=np.int64)\n  return encoder.fit_transform(series.values.reshape(-1, 1))","a6de19ba":"categories_ordenc = {\"sex\": [[\"F\", \"M\"]],\n                     \"address\": [[\"R\", \"U\"]],\n                     \"famsize\": [[\"LE3\", \"GT3\"]],\n                     \"Pstatus\": [['A', 'T']],\n                     \"schoolsup\": [[\"no\", \"yes\"]],\n                     \"famsup\": [[\"no\", \"yes\"]],\n                     \"paid\": [[\"no\", \"yes\"]],\n                     \"activities\": [[\"no\", \"yes\"]],\n                     \"nursery\": [[\"no\", \"yes\"]],\n                     \"higher\": [[\"no\", \"yes\"]],\n                     \"internet\": [[\"no\", \"yes\"]],\n                     \"romantic\": [[\"no\", \"yes\"]],\n                     }\n\nfor column, category in categories_ordenc.items():\n  data[column] = ordinal_encode(data[column], category)","d5c5b0ca":"def onehot_encode(data, column, categories):\n  encoder = OneHotEncoder(categories=categories, drop=\"first\", sparse=False, dtype=np.int64)\n  encoded_features = encoder.fit_transform(data[column].values.reshape(-1, 1))\n  encoder.transform(data[column].values.reshape(-1, 1))\n\n  data = data.drop(labels=column, axis=1)\n\n  for j, category in enumerate(categories[0][1:]):\n    category_column_name = \"{}_{}\".format(column, category)\n    data[category_column_name] = encoded_features[:, j]\n\n  return data","5715205f":"categories_hotenc = {\"Fjob\": [[\"other\", \"teacher\", \"health\", \"services\", 'at_home']],\n                     \"Mjob\": [[\"other\", \"teacher\", \"health\", \"services\", 'at_home']],\n                     \"reason\": [[\"other\", \"home\", \"reputation\", \"course\"]],\n                     \"guardian\": [[\"other\", \"mother\", \"father\"]]}\n\nfor column, category in categories_hotenc.items():\n  data = onehot_encode(data, column, category)","4a57a8ef":"data.info()","78080e74":"X = data.drop(labels=[\"G1\", \"G2\", \"G3\"], axis=1)\ny = data[[\"G3\"]]","f9dcacfb":"def detect_outlier(df):\n\n  flag_outlier = False\n\n  for feature in df:\n    column = df[feature]\n    mean = np.mean(column)\n    std = np.std(column)\n    z_scores = (column - mean) \/ std\n    outliers = np.abs(z_scores) > 3\n\n    n_outliers = sum(outliers)\n    if n_outliers > 0:\n      print(\"{} has {} outliers\".format(column, n_outliers))\n      flag_outlier = True\n\n    if ~flag_outlier:\n      print(\"The dataset has no outliers.\")\n    \n    return None\n  \ndetect_outlier(X)","fbf04a85":"# Number of variables\nlen(X.columns)","f5e8cd65":"fig, ax = plt.subplots(figsize=(15, 15))\nX.hist(ax=ax)\n\nplt.tight_layout()\nplt.plot()","eb6ba6f1":"fig, ax = plt.subplots(figsize=(25, 15))\n\nax = sns.heatmap(X.corr(), annot=True, fmt=\".2f\", cmap=\"viridis\", ax=ax)\nplt.show()","b4ad6f82":"chi_square_value, p_value = calculate_bartlett_sphericity(X)\nchi_square_value, p_value","151a4d18":"kmo_all, kmo_model = calculate_kmo(X)\n\nprint(\"Overall KMO = {:.3f}\".format(kmo_model))\nprint(\"KMO per feature:\\n{}\".format(kmo_all.round(3)))","eabefcce":"# Variables considered adequate by the KMO test\nkmo_passed = list(X.columns[kmo_all >= 0.5])\nkmo_passed","dd9c269c":"# Variables considered inadequate by the KMO test\nkmo_failed = list(X.columns[kmo_all < 0.5])\nkmo_failed","4afbdc51":"kmo_passed = list(X.columns[kmo_all >= 0.5])\nX_kmo = X[kmo_passed]\n\nkmo_all, kmo_model = calculate_kmo(X_kmo)\n\nprint(\"Overall KMO = {:.3f}\".format(kmo_model))\nprint(\"KMO per feature:\\n{}\".format(kmo_all.round(3)))","93c6b39a":"X = X_kmo.copy()","37f8ca1e":"def train_factor_model(dataset, number_of_factors, rotation=\"varimax\"):\n  factor_model = FactorAnalyzer(n_factors=number_of_factors, rotation=rotation)\n  factor_model.fit(dataset)\n\n  return factor_model","7e0108c3":"def scree_plot(eigenvalues, n_columns):\n  plt.style.use(\"fivethirtyeight\")\n  # Create scree plot using matplotlib\n  fig, ax = plt.subplots(figsize=(10, 6))\n\n  ax.scatter(range(1, n_columns + 1), eigenvalues, s=50)\n  ax.plot(range(1, n_columns + 1), eigenvalues)\n\n  ax.set_title('Scree Plot')\n  ax.set_xlabel('# Factors')\n  ax.set_ylabel('Eigenvalues')\n\n  ax.grid(False)\n\n  ax.axhline(y=1, linestyle=\"--\", color=\"grey\", linewidth=1)\n  # plt.savefig(\"scree_plot.png\", transparent=True, dpi=600)\n  plt.show()","2fb973d3":"def loadings_plot(loadings, variables):\n  # Create loadings plot using matplotlib\n  fig, ax = plt.subplots(figsize=(15, 5))\n\n  image = ax.matshow(loadings, cmap=\"hot\")\n\n  n_factors = loadings.shape[1]\n  factors = [\"Factor{}\".format(index) for index in range(n_factors)]\n\n  ax.set_yticklabels([\"\"] + variables, fontsize=10)\n  ax.set_xticklabels([\"\"] + factors, rotation=\"vertical\", fontsize=10)\n\n  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n  ax.grid(False)\n\n  plt.colorbar(image)\n  plt.tight_layout()\n  plt.show()","14f6f922":"factor_model = train_factor_model(X, len(X.columns), rotation=\"oblimin\")","599818e6":"eigenvalues, common_factor_eigenvalues = factor_model.get_eigenvalues()","cb2c3dbb":"eigenvalues","2ad04160":"n_columns = len(X.columns)\nscree_plot(eigenvalues, n_columns=n_columns)","1ae9a6fc":"number_of_factors = sum(eigenvalues > 1)\nfactor_model = train_factor_model(X, number_of_factors, rotation=\"promax\")","2d61cc8d":"loadings = factor_model.loadings_\nloadings_abs = np.abs(loadings)\n\ncolumns = X.columns.tolist()\nloadings_plot(loadings_abs, columns)","f21ed8af":"n_factors = loadings.shape[1]\nfactors_list = [\"Factor{}\".format(index) for index in range(n_factors)]\nfactors = {}.fromkeys(factors_list, None)\nfor key in factors:\n  factors[key] = []\n\nargsmax = loadings_abs.argmax(axis=1)\nfor argmax, column in zip(argsmax, columns):\n  factors[factors_list[argmax]].append(column)","2891e460":"factors","4dd15893":"# Create target variable for binary and multiclass classification\nseed = 42\n\nX_latent = factor_model.transform(X)\ny_binary = np.array((y[\"G3\"] > 10) + 0)\n\nmulticlass_categories = [\"poor\", \"unsatisfactory\", \"satisfactory\", \"good\", \"very good\"]\ny_multiclass = []\nfor grade in y[\"G3\"]:\n  if grade < 10:\n    y_multiclass.append(0)\n  elif grade < 12:\n    y_multiclass.append(1)\n  elif grade < 14:\n    y_multiclass.append(2)\n  elif grade < 16:\n    y_multiclass.append(3)\n  elif grade <= 20:\n    y_multiclass.append(4)\n\ny_multiclass = np.array(y_multiclass)","9ca69bb7":"def train(X, y, grid_params, seed):\n\n  scaler = StandardScaler()\n  X_scaled = scaler.fit_transform(X)\n\n  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n                                                      test_size=0.2, \n                                                      random_state=seed,\n                                                      stratify=y)\n  \n  # The models dict has same keys as grid_params \n  # pointing to the grid search of each classifier\n  models = {\n      \"SGD\": SGDClassifier(),\n      \"Logistic Regression\": LogisticRegression(),\n      \"Random Forest\": RandomForestClassifier(), \n      \"AdaBoost\": AdaBoostClassifier(), \n      \"GradientBoost\": GradientBoostingClassifier()\n  }\n\n  # store best estimators\n  best_estimators = dict.fromkeys(models.keys())\n\n  for model_name in models:\n    base_clf = models[model_name]\n    params = grid_params[model_name]\n\n    print(\"[INFO] Performing Grid-Search CV ---- {}\".format(model_name))\n    model = GridSearchCV(base_clf, params, scoring='accuracy', cv=5, n_jobs=-1, \n                         verbose=0)\n  \n    model.fit(X_train, y_train)\n    best_estimators[model_name] = model.best_estimator_\n\n  return best_estimators, X_test, y_test","e3c1d2d0":"def evaluate_binary(models, X_test, y_test):\n    \n  results = pd.DataFrame(columns=[\"accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], \n                         index=models.keys())\n  \n  for model_name, model in models.items():\n    y_pred = model.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    results.loc[model_name, :] = accuracy, precision, recall, f1\n\n  return results","91f01aef":"def evaluate_multiclass(models, X_test, y_test):\n  results = pd.DataFrame(columns=[\"Overall Accuracy\"], index=models.keys())\n\n  for model_name, model in models.items():\n    n_samples = len(y_test)\n    y_pred = model.predict(X_test)\n\n    overall_accuracy = np.sum(y_pred == y_test) \/ n_samples\n    results.loc[model_name, :] = overall_accuracy\n\n  return results","47b924dc":"# choose best models based on accuracy\ndef select_binary_model(models, X_test, y_test):\n  best_accuracy = -1\n  best_model = \"\"\n  for model_name, model in models.items():\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    if accuracy > best_accuracy:\n      best_accuracy = accuracy\n      best_model = model_name\n\n  print(\"{}\\naccuracy = {:.4f}\".format(best_model, best_accuracy))\n  return models[best_model]","419c00b0":"def select_multiclass_model(models, X_test, y_test):\n  best_overall_accuracy = -1\n  best_model = \"\"\n  n_samples = len(y_test)\n\n  for model_name, model in models.items():  \n    y_pred = model.predict(X_test)\n\n    overall_accuracy = np.sum(y_pred == y_test) \/ n_samples\n\n    if overall_accuracy > best_overall_accuracy:\n      best_overall_accuracy = overall_accuracy\n      best_model = model_name\n\n  print(\"{}\\nOverall accuracy = {:.4f}\".format(best_model, best_overall_accuracy))\n  return models[best_model]","86f2e2e5":"grid_params_binary = {\n    \"SGD\": {\n        \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n        \"alpha\": np.linspace(1e-5, 1e-3, 5),\n        \"learning_rate\": [\"optimal\", \"invscaling\", \"adaptive\"],\n        \"eta0\": np.linspace(1e-4, 1e-1, 5)\n    },\n    \"Logistic Regression\": {\n        \"penalty\": [\"l1\", \"l2\"],\n        \"C\": [1, 2, 4, 5],\n        \"solver\": [\"liblinear\"]\n    },\n    \"Random Forest\": {    \n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_depth\": np.linspace(3, 12, 4, dtype=np.int64),\n        \"min_samples_split\": np.linspace(0.1, 0.5, 6),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 6),\n        \"max_features\":[\"log2\",\"sqrt\"]\n    }, \n    \"AdaBoost\": {\n        \"base_estimator\": [DecisionTreeClassifier(max_depth=1),\n                           DecisionTreeClassifier(max_depth=2),\n                           DecisionTreeClassifier(max_depth=3)],\n        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n        \"n_estimators\": np.linspace(20, 80, 4, dtype=np.int64),\n    },\n    \"GradientBoost\": {    \n        \"loss\":[\"deviance\"],\n        \"learning_rate\": [0.01, 0.025, 0.05, 0.075],\n        \"min_samples_split\": np.linspace(0.1, 0.5, 5),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n        \"max_depth\":[3, 5, 8],\n        \"max_features\":[\"log2\",\"sqrt\"],\n        \"criterion\": [\"friedman_mse\"],\n        \"subsample\":[0.5, 0.618, 0.85],\n        \"n_estimators\":[5, 10, 15]\n    }\n} ","3e6fb6bf":"# Train and evaluate binary model with factor analysis (latent) variables\nestimators_binary_latent, X_test_latent, y_test_binary = train(X_latent, \n                                                               y_binary, \n                                                               grid_params_binary, \n                                                               seed)\n\nevaluate_binary(estimators_binary_latent, X_test_latent, y_test_binary)","4097803d":"# Train and evaluate binary model with the original variables (without factor analysis)\nestimators_binary, X_test, y_test_binary = train(X, y_binary, \n                                                 grid_params_binary, \n                                                 seed)\n\nevaluate_binary(estimators_binary, X_test, y_test_binary)","cfb38149":"best_latent_binary_model = select_binary_model(estimators_binary_latent, X_test_latent, y_test_binary)","26281d5d":"best_binary_model = select_binary_model(estimators_binary, X_test, y_test_binary)","4a8905d2":"grid_params_multiclass = {\n    \"SGD\": {\n        \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n        \"alpha\": np.linspace(1e-5, 1e-3, 5),\n        \"learning_rate\": [\"optimal\", \"invscaling\", \"adaptive\"],\n        \"eta0\": np.linspace(1e-4, 1e-1, 5)\n    },\n    \"Logistic Regression\": {\n        \"penalty\": [\"l1\", \"l2\"],\n        \"C\": [1, 2, 4, 5],\n        \"solver\": [\"liblinear\"]\n    },\n    \"Random Forest\": {    \n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_depth\": np.linspace(3, 12, 4, dtype=np.int64),\n        \"min_samples_split\": np.linspace(0.1, 0.5, 6),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 6),\n        \"max_features\":[\"log2\",\"sqrt\"]\n    }, \n    \"AdaBoost\": {\n        \"base_estimator\": [DecisionTreeClassifier(max_depth=1),\n                           DecisionTreeClassifier(max_depth=2),\n                           DecisionTreeClassifier(max_depth=3)],\n        \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n        \"n_estimators\": np.linspace(20, 80, 4, dtype=np.int64),\n    },\n    \"GradientBoost\": {    \n        \"loss\":[\"deviance\"],\n        \"learning_rate\": [0.01, 0.025, 0.05, 0.075],\n        \"min_samples_split\": np.linspace(0.1, 0.5, 5),\n        \"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n        \"max_depth\":[3, 5, 8],\n        \"max_features\":[\"log2\",\"sqrt\"],\n        \"criterion\": [\"friedman_mse\"],\n        \"subsample\":[0.5, 0.618, 0.85],\n        \"n_estimators\":[5, 10, 15]\n    }\n} ","04a02a42":"estimators_multiclass_latent, X_test_latent, y_test_multiclass = train(X_latent, y_multiclass, \n                                                                       grid_params_multiclass, seed)\n\nevaluate_multiclass(estimators_multiclass_latent, X_test_latent, y_test_multiclass)","654b8e23":"estimators_multiclass, X_test, y_test_multiclass = train(X, y_multiclass, \n                                                         grid_params_multiclass, \n                                                         seed)\n\nevaluate_multiclass(estimators_multiclass, X_test, y_test_multiclass)","ef6fd31c":"best_latent_multiclass_model = select_multiclass_model(estimators_multiclass_latent, X_test_latent, y_test_multiclass)","48603577":"best_multiclass_model = select_multiclass_model(estimators_multiclass, X_test, y_test_multiclass)","29c7379c":"Since all but one of our variables (absences) are discrete and have few instances, it does not make sense to check for linear relations. Therefore, this point is automatically satisfied.","5d76736c":"To proceed with Factor Analysis, the data must be suited for the model. We are going to check the following characteristics:\n\n2.3.1 Outliers \\\n2.3.2 Sample Size \\\n2.3.3 Linear Relations \\\n2.3.4 Normality \\\n2.3.5 Factorability","d6f28759":"##### 2.3.5.1 Inter-item correlatins (correlation Matrix)\n\nIn this method, we investigate the correlation to matrix to verify that we don't have extreme values of correlation and neither have 0 correlations amongst the values outside the main diagonal of the matrix.","4c348d8a":"#### 3.2 Analysis","3bbe92ea":"Initially, we must decide which variables we are going to use. We are not making distinction among schools. Therefore the variable school will be dropped out.","4aaf412b":"#### 2.3.4 Normality","aa0a87c8":"## 1. Load Data","a19acf4e":"In this notebook, we are going to dive into the exploratory factor analysis technique. This is a dimensionality reduction technique that groups correlated variables that are assumed to share the same kind of information which would be useful to explain the outcomes of the dataset. \n\nThe results obtained using Factor Analysis (FA) were compared to the results without factor Analysis and with the original results from the [dataset paper](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf).\n\nResults for Binary Classification (without using the variables G1 and G2), i.e., predicting if student will succeed or not:\n\n- G3 > 10: pass\n- G3 < 10: fail\n\n| Setup           | Model               | Accuracy |\n|-----------------|---------------------|----------|\n| With Factor Analysis         | Logistic Regression | 0.785 |\n| Without Factor Analysis      | Logistic Regression | 0.646 |\n| Original Results| SVM | 0.706 |\n\nResults for Multi-Class Classification (without using the variables G1 and G2), based on the  5-level classification based on Erasmus grade conversion system:\n- 16-20: very good\n- 14-15: good\n- 12-13: satisfactory\n- 10-11: sufficient\n- 0-9 : fail\n\n| Setup           | Model               | Accuracy |\n|-----------------|---------------------|----------|\n| With Factor Analysis         | SGD | 0.367 |\n| Without Factor Analysis      | AdaBoost | 0.380 |\n| Original Results| Random Forest | 0.335 |\n\nThe notebook is divided as follows:\n\n1. Load Data \n2. Preprocessing \\\n  2.1 Feature Selection \\\n  2.2 Feature Encoding \\\n  2.3 Assumptions \n3. Exploratory Factor analysis \\\n  3.1 Choosing the number of factors \\\n  3.2 Analysis \\\n  3.3 Model\n  \n**References**:\n\nDataset Paper: [Reference 0](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf) \\\nAssumptions: [Reference 1](https:\/\/en.wikiversity.org\/wiki\/Exploratory_factor_analysis\/Assumptions), [Reference 2](https:\/\/en.wikiversity.org\/wiki\/Survey_research_and_design_in_psychology\/Lectures\/Exploratory_factor_analysis\/Notes#Assumption_testing) \\\nSteps of a factor analysis: [Reference 3](https:\/\/www.let.rug.nl\/nerbonne\/teach\/rema-stats-meth-seminar\/Factor-Analysis-Kootstra-04.PDF) \\\nConcepts and Theory: [Reference 4](https:\/\/hal.archives-ouvertes.fr\/hal-02557344\/document) \\\nModel package: [factor_analyzer](https:\/\/factor-analyzer.readthedocs.io\/en\/latest\/factor_analyzer.html#factor-analyzer-analyze-module)","0aa0c9a0":"## 2. Preprocessing","f599cbda":"### 2.2 Feature Encoding\n\nHere we are going to use two techniques to encode the categorical variables:\n\n1. (Dummy) One Hot Encoding\n2. Ordinal Encoding","2fff48f6":"#### 2.3.2 Sample size","82688aea":"We have 3 possible approaches to the problem:\n\n- Binary classification \\\nG3 > 10: pass \\\nG3 < 10: fail\n\n- 5-level classification based on Erasmus grade conversion system \\\n16-20: very good \\\n14-15: good \\\n12-13: satisfactory \\\n10-11: sufficient \\\n0-9 : fail\n\n- Regression (Predicting G3) \\\nWe will not do this one.","d36e6918":"**Sampling Adequacy - Kaiser-Meyer-Olkin (KMO) test**\n\nThe KMO test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variables. Lower proportion is more suitable for factor analysis. KMO values range between 0 and 1. The value of KMO less than 0.6 is considered inadequate.","ca1fdb3d":"# Dimensionality Reduction with Factor Analysis applied to the Student Performance Data Set","ffa90d59":"#### 2.2.1 Ordinal Encoding","172f7064":"We can see from the correlation matrix that we have several small-moderate sized correlations, e.g. > 0.3 or < -0.3. Thus, we can proceed to another methodology of data quality assessment. ","ef2c45a7":"##### 2.3.5.2 Measures of Sampling Adequacy (MSAs) Tests\n\nSampling adequacy provides the researcher with information regarding the grouping of survey items. Grouping items into a set of interpretable factors can better explain the constructs under investigation. Measures of sampling adequacy evaluate how strongly an item is correlated with other items in the EFA correlation matrix. ","8fd66f50":"I hope you understood my work, thanks for reading!","d40758c7":"Using Kaiser Criterion, we must choose just 9 factors to model the variability of the dataset.","37172c06":"Fantastic, we managed to got a reasonable value of 0.638 in the KMO test. Let's use this subset of variables and proceed to the Exploratory Factor Analysis","245c4326":"In Bartlett\u2019s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix. What is in fact what we verified earlier.","ab153fad":"The factors can be translated into:\n\nFactor 0: Alcohol consumption \\\nFactor 1: Parents education \\\nFactor 2: Home Infrastructure \\\nFactor 3: Distance to School \\\nFactor 4: Commitment \\\nFactor 5: Family Educational Financial Incentive \\\nFactor 6: Extra classes \\\nFactor 7: Sociability \\\nFactor 8: Extracurricular activities and attendance","996cec5d":"#### 2.2.2 (Dummy) One Hot Encoding","cccf86b8":"Ideally we should have 38*20 = 760 samples but considering that the number of variables raised considerably because of the one hot encoding strategy. Aggregating all new variables originated from the one hot encoding technique, we would have 29 variables. Thus, the number of ideal samples would shrink to 580.\n\nWe have 395 samples, which corresponds to almost 70% of the ideal amount of variables. However, the minimum ratio of variables:samples is 1:5. Since we have 38 variables, the minimum number of samples is 190.\n\nTherefore, we are not in the ideal setup, but the data fullfil the minimum size requirement to proceed with the factor analysis.","36390d32":"#### 3.3.2 Multiclass Classification","89366e10":"#### 2.3.1 Outliers\n\nTo perform outlier detection, we are going to employ the Z-Score method because it is the simplest one.","afe7e22e":"#### 3.3.1 Binary Classification","58248c2f":"**Suitability of Data - Bartlett\u2019s Test**\n\nBartlett\u2019s test of Sphericity provides a chi-square output that must be significant. It indicates the matrix is not an identity matrix and accordingly it should be significant ($p < .05$) for factor analysis to be suitable. ","4394d117":"### 3.3 Model","d9442be5":"#### 2.3.5 Factorability","ed57af13":"## 3. Exploratory Factor Analysis","adafe74a":"#### 3.1 Choosing the Number of Factors","3fcb06aa":"### 2.1 Feature Selection","d85daac0":"### 2.3 Assumptions","739a9007":"#### 2.3.3 Linear relations","544aa74d":"Factorability is the assumption that there are at least some correlations amongst the variables so that coherent factors can be identified. Basically, there should be some degree of collinearity among the variables but not an extreme degree or singularity among the variables. Factorability means \u201ccan we found the factors in the dataset?\u201d.\n\nThere are some ways of measure factorability: \n\n1. Inter-item correlations (correlation matrix) - are there at least several small-moderate sized correlations e.g., > .3?\n2. Measures of sampling adequacy (MSAs): \n  - Kaiser-Meyer-Olkin (KMO) (should be > ~.5 or .6) and\n  - Bartlett's test of sphericity (should be significant)\n\nWe are going to explore the second way soon in the notebook.","11a8d70d":"Unfortunately, the overall KMO value was 0.489, which is no good. The minimum viable value is 0.5. However, we can try to improve this metric by removing some variables that failed the KMO test. Considering that the variables that failed the KMO test are those built using one hot encoding. Let's remove them and re-compute the KMO test values.","76779a20":"Typical guidelines for factor analysis sample size requirements reported in the research method literature are:\n\nA Total N > 200 is recommended. Comrey and Lee (1992) provide this description of total sample sizes' adequacy for factor analysis:\n\n50 = very poor,\n100 = poor,\n200 = fair,\n300 = good,\n500 = very good\n1000+ = excellent\n\nMin\/Ideal sample size based on variable:factor ratio\n\nMin. N > 5 cases per variable (item)\ne.g., if I have 30 variables, I should have at least 150 cases (i.e., 1:5)\n\nIdeal N > 20 cases per variable\ne.g., if I have 30 variables, I would ideally have at least 600 cases (1:20)"}}