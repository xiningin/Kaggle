{"cell_type":{"6052cb9c":"code","6a6b6037":"code","15b17052":"code","c913da98":"code","c5dc8c44":"code","a7363cd4":"code","259d9a77":"code","352923b9":"code","a48b4abf":"code","885207b2":"code","b18b8a34":"code","2abaf1fd":"code","0382816b":"code","ad1ecc72":"code","48a3642b":"code","3d7e2565":"code","92d2174e":"code","3aeb79ec":"code","3d8c1afd":"code","5fc0f9c2":"code","2040bfdd":"code","328c4477":"markdown","dd3608f6":"markdown","8e74e401":"markdown","896da1b2":"markdown","7adf11d5":"markdown","31f68c4d":"markdown","93eb0b32":"markdown","6aae268b":"markdown","51606500":"markdown","73829348":"markdown","e9e27de4":"markdown","b4584440":"markdown","e6aaec5a":"markdown","363028a6":"markdown","83ad690b":"markdown","eb0ed123":"markdown","e714b291":"markdown","c3a80228":"markdown","a564b5c5":"markdown","68db43cb":"markdown","57b13e1e":"markdown","99da09a1":"markdown","e43e82fc":"markdown","56ec2baf":"markdown","b9a53fe2":"markdown","b131c39c":"markdown","88915e79":"markdown"},"source":{"6052cb9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a6b6037":"# Loading the data\ndf = pd.read_csv(\"..\/input\/housingforca\/housing 2.csv\")\ndf.head(5)","15b17052":"df.info()","c913da98":"df[\"ocean_proximity\"].value_counts()","c5dc8c44":"df.describe()","a7363cd4":"%matplotlib inline \n# just for jupyter\n\nimport matplotlib.pyplot as plt\ndf.hist(bins=50, figsize=(20,15))  ","259d9a77":"from sklearn.model_selection import train_test_split\n\ntrain_test, test_set = train_test_split(df, test_size = 0.2, random_state = 12345)","352923b9":"#Pandas provide cut() function to achieve it\n\ndf[\"income_category\"] = pd.cut(df[\"median_income\"],\nbins=[0., 1.5, 3.0, 4.5, 6., np.inf],\nlabels=[1, 2, 3, 4, 5])","a48b4abf":"df[\"income_category\"].hist()","885207b2":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\nfor train_index, test_index in split.split(df, df[\"income_category\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]","b18b8a34":"strat_test_set[\"income_category\"].value_counts() \/ len(strat_test_set)","2abaf1fd":"# revert back to original data, just to show you these little details I learned\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_category\", axis=1, inplace=True)","0382816b":"df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",alpha =0.1)","ad1ecc72":"df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2,\ns=df[\"population\"]\/100, label=\"population\", figsize=(10,7),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)","48a3642b":"corr_frame = df.corr()\ncorr_frame[\"median_house_value\"].sort_values(ascending=False)","3d7e2565":"from pandas.plotting import scatter_matrix\nimport seaborn as sns","92d2174e":"plt.figure(figsize=(10, 10))\nsns.heatmap(corr_frame, annot=True, linewidths=0.3, fmt= '.2f',cmap=\"RdPu\")\nplt.show()","3aeb79ec":"attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n\"housing_median_age\"]\nscatter_matrix(df[attributes], figsize=(14, 9))","3d8c1afd":"df.ocean_proximity.unique()","5fc0f9c2":"def bar_chart_op(column):\n    ocean = df[df[\"ocean_proximity\"]== \"<1H OCEAN\"][column].value_counts()\n    inland = df[df[\"ocean_proximity\"] == \"INLAND\"][column].value_counts()\n    df2=pd.DataFrame([ocean, inland])\n    df2.index=[\"by Ocean\", \"Not by ocean\"]\n    df2.plot(kind=\"bar\",figsize=(10,5))","2040bfdd":"bar_chart_op(\"income_category\")","328c4477":"### Visualizing","dd3608f6":"Adding alpha visual highlights high-density areas. Try it without on your own!","8e74e401":"Find out what are unique values are stored in ocean_proximity","896da1b2":"Strong netive correlation when it's close to -1","7adf11d5":"# Introduction","31f68c4d":"### Table of content<a class=\"anchor\" id=\"top\"><\/a>\n\n\n* [Set up & Explore](#setup)\n\n* [Creat a Test Set](#test)\n\n* [Correlations](#corr)\n\n* [Correlation Visualized](#corrviz)\n\n* [Bonus](#bo)\n\n* [Conclusion](#conclu)","93eb0b32":"Another visual for sake of simplicity","6aae268b":"### The role of this notebook mainly is to present a way of data exploration. It's not always very relevant, but I try to make my work as clear as possible. If not, please share feedbacks. I'm learning just like you are. The next steps data preprocessing for choosing a Machine Learning Algorithm, select and train a model and fine-tune for your model before imeplementing Classification. \n\n### I have included a link for the file I'm using under Introduction. \n\n### Thank you very much for your time reading. I hope you you learned something! Good day.\n\n* [Back to top](#top)","51606500":"### Bonus <a class=\"anchor\" id=\"bo\"><\/a>","73829348":"The correlation coefficient ranges from \u20131 to 1.","e9e27de4":"Strong positive correlation when it's close to 1","b4584440":"#### Tedious step but necessary for exploration. Notice that the total_bed rooms attribute has only 20,433 nonnull values, meaning that 207 districts are missing this feature. We will take care of that later but now hopefully you see such simple steo could give you a complete overview.","e6aaec5a":"#### if data is large enough, then random sampling is fine. IF not, you run the risk of introducing a sampling bias. The fix is to use stratified sampling. Before going into how to implement stratified sampling method. Notice that many of aboves graphs are skewed. Life is short, no need to be mad at every single one of them. But feature like median_income is important for predicting housing prices. Sufficient number of instances of each stratum is important to have. Let's categorize income.","363028a6":"#### All attributes are numerical, except the ocean_proximity field. Its type is object. It will be fixed later ","83ad690b":"#### If you're a visual person, another good way to get a feel the type of data you are dealing is to plot a hostogram for each attribute (numerical).","eb0ed123":"### The data come from the Kaggle dataset. This is a fairly small dataset that consists of 20,649 instances, which is great to get started. This dataset aims at performing some data manipulations and data exploration based on given features, such as location, age, income, household, etc. You could download zip file directly from [here]( https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/datasets\/housing\/housing.tgz) to extract csv.","e714b291":"### Conclusion <a class=\"anchor\" id=\"conclu\"><\/a>","c3a80228":"No linear relations when it's close to 0","a564b5c5":"##### Time to deal with stratified sampling. For ones who don't know what this term. It makes the population divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population. Representative is the key word. Any of those subgroups can be representitive of the whole population, versus random sampling. Stepped up the game.","68db43cb":"Much better proportion","57b13e1e":"### Creat a test set<a class=\"anchor\" id=\"test\"><\/a>","99da09a1":"California housing prices: red is expensive, blue is cheap, larger circles indicate\nareas with a larger population","e43e82fc":"### Correlation Visualized<a class=\"anchor\" id=\"corrviz\"><\/a>","56ec2baf":"#### Basic but magical","b9a53fe2":"This is can be done without using function. I just thought using function to plot any feature without retyping stuff over and over is cool.","b131c39c":"### Correlations<a class=\"anchor\" id=\"corr\"><\/a>","88915e79":"### Set up & Explore<a class=\"anchor\" id=\"setup\"><\/a>"}}