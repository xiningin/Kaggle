{"cell_type":{"4974b54e":"code","d4494c9b":"code","b1fd51e5":"code","25f903e3":"code","c86a5fcc":"code","df96c086":"code","b8cbcac7":"code","a3e74396":"code","faab5c5f":"code","df6b0641":"code","9889780a":"code","6a036f50":"code","823eb381":"code","078224b7":"code","c9cfb729":"code","8b46a4f0":"code","6ac0f501":"code","84b512c2":"code","22a8a3eb":"code","c1c248d7":"code","3a71eca8":"code","ec69b74a":"code","33d277d8":"code","bcd541c3":"code","1786a6ae":"code","39b78181":"code","c44904ea":"code","ea902d0a":"code","1623c62f":"code","0f212a25":"code","676e1c99":"code","e8ce1d0d":"code","66fb12bb":"code","c7dfe53f":"code","86f94865":"code","a3d87f2f":"code","5fdf1064":"code","2b9eacac":"code","e982393a":"code","080aa295":"code","ff4a575d":"code","c8628374":"code","2529bf15":"code","a69f47ac":"code","432cd870":"code","41f9877b":"code","c679b233":"code","ec49da18":"code","8cc311f3":"code","bdcc0037":"code","fab877c3":"code","00d7cfe5":"code","b33d8aba":"code","253187a2":"code","2504d8b1":"code","e2fb6394":"code","09a7f156":"code","32baceb9":"code","dc5e6d59":"code","e39b56aa":"code","242e450b":"code","e8b7d654":"code","d4bb4c51":"code","a75be7d5":"code","c5aad432":"code","b3128c34":"code","7a70aa57":"code","47f34ba2":"code","e63dace3":"code","8e46ad8a":"code","3ceccab7":"code","c8b69b93":"markdown","2712fb0a":"markdown","93e3ceff":"markdown","66a5c797":"markdown","11378398":"markdown","52a723c4":"markdown","2056dad6":"markdown","89b1831f":"markdown","7cbf8a10":"markdown","70c8bae2":"markdown","8b7c181d":"markdown","c0d2fb49":"markdown","df80cef3":"markdown","51813f90":"markdown","7f58a920":"markdown","09a2abd9":"markdown","d02a2219":"markdown","0b6b8290":"markdown","0559597f":"markdown","17316dad":"markdown","1621fd5e":"markdown","f7a0cbe8":"markdown","8e2757cc":"markdown","a91a86bf":"markdown","f626b16d":"markdown","f918f1a2":"markdown","83aba776":"markdown","dd2a4193":"markdown","9f05ea40":"markdown","60106677":"markdown","95e6ca3e":"markdown","a76dd4cd":"markdown","35b340aa":"markdown","3b73486d":"markdown","a4cf7121":"markdown","be350b8d":"markdown","5926c4a3":"markdown","8ff456bf":"markdown","09d02bc5":"markdown","99a972d7":"markdown","62c2e9a6":"markdown","981b7401":"markdown","e03f5f4b":"markdown","4029e6f7":"markdown","fb1a6506":"markdown","2e2a034a":"markdown","1d620b58":"markdown","14642e6c":"markdown","74d1920e":"markdown","9aa24d3e":"markdown","2ae8a1eb":"markdown","e9434f05":"markdown","3e8376b0":"markdown","f6048854":"markdown","189e62c6":"markdown","dbc0cca0":"markdown","15f3edf3":"markdown","e4ed189d":"markdown","a2a4e785":"markdown","0014dad8":"markdown","0de3df39":"markdown","d8bf490c":"markdown"},"source":{"4974b54e":"import os\nimport gc\nimport json\nimport math\nimport cv2\nimport PIL\nimport re\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n#from sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\n%matplotlib inline\n#from keras.preprocessing import image\nimport glob\nimport tensorflow.keras.applications.densenet as dense\nfrom kaggle_datasets import KaggleDatasets\nimport seaborn as sns","d4494c9b":"tf.__version__","b1fd51e5":"train = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv')\n\nprint('Train: ', train.shape)\nprint(\"Test:\", test.shape)","25f903e3":"train.head()","c86a5fcc":"train.info()","df96c086":"test.head()","b8cbcac7":"test.info()","a3e74396":"train['benign_malignant'].value_counts(normalize=True)","faab5c5f":"sns.countplot(train['benign_malignant'])","df6b0641":"train['sex'].value_counts(normalize=True)","9889780a":"train['target'].groupby(train['sex']).mean()","6a036f50":"sns.countplot(train['sex'], hue=train['target'])","823eb381":"train['target'].groupby(train['age_approx']).mean()","078224b7":"plt.figure(figsize=(8,5))\nsns.countplot(train['age_approx'], hue=train['target'])","c9cfb729":"train['anatom_site_general_challenge'].value_counts(normalize=True)","8b46a4f0":"train['target'].groupby(train['anatom_site_general_challenge']).mean()","6ac0f501":"plt.figure(figsize=(10,5))\nsns.countplot(train['anatom_site_general_challenge'], hue=train['target'])","84b512c2":"train['diagnosis'].value_counts(normalize=True)","22a8a3eb":"train['target'].groupby(train['diagnosis']).mean()","c1c248d7":"plt.figure(figsize=(15,5))\nsns.countplot(train['diagnosis'], hue=train['target'])","3a71eca8":"train_df = train[['sex','age_approx','anatom_site_general_challenge','diagnosis','target']]\ntrain_df.head()","ec69b74a":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df = train_df.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\ntrain_df.head()","33d277d8":"g = sns.pairplot(train_df, hue=\"diagnosis\")","bcd541c3":"sns.heatmap(train_df.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","1786a6ae":"def display_training_curves(training, validation, title, subplot):\n  if subplot%10==1: # set up the subplots on the first call\n    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n    plt.tight_layout()\n  ax = plt.subplot(subplot)\n  ax.set_facecolor('#F8F8F8')\n  ax.plot(training)\n  ax.plot(validation)\n  ax.set_title('model '+ title)\n  ax.set_ylabel(title)\n  ax.set_xlabel('epoch')\n  ax.legend(['train', 'valid.'])\n\ncols, rows = 4, 3\ndef grid_display(list_of_images, no_of_columns=2, figsize=(15,15), title = False):\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    z = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns+1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        if title:\n            if i >= no_of_columns:\n                plt.title(titles[z])\n                z +=1\n            else:\n                plt.title(titles[i])\n        plt.imshow(list_of_images[i])\n        plt.axis('off')","39b78181":"image_list = train[train['target'] == 0].sample(8)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\n#show_images(image_all, cols=1)\ngrid_display(image_all, 4, (15,15))","c44904ea":"image_list = train[train['target'] == 1].sample(8)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","ea902d0a":"image_list = train[train['anatom_site_general_challenge'] == 'torso'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","1623c62f":"image_list = train[train['anatom_site_general_challenge'] == 'lower extremity'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","0f212a25":"image_list = train[train['anatom_site_general_challenge'] == 'upper extremity'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","676e1c99":"image_list = train[train['anatom_site_general_challenge'] == 'head\/neck'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","e8ce1d0d":"image_list = train[train['anatom_site_general_challenge'] == 'palms\/soles'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","66fb12bb":"image_list = train[train['anatom_site_general_challenge'] == 'oral\/genital'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","c7dfe53f":"image_list = train[train['diagnosis'] == 'nevus'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","86f94865":"image_list = train[train['diagnosis'] == 'melanoma'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","a3d87f2f":"image_list = train[train['diagnosis'] == 'seborrheic keratosis'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","5fdf1064":"image_list = train[train['diagnosis'] == 'lentigo NOS'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","2b9eacac":"image_list = train[train['diagnosis'] == 'lichenoid keratosis'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","e982393a":"image_list = train[train['diagnosis'] == 'solar lentigo'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","080aa295":"image_list = train[train['diagnosis'] == 'atypical melanocytic proliferation'].sample(1)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","ff4a575d":"image_list = train[train['diagnosis'] == 'cafe-au-lait macule'].sample(1)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","c8628374":"image_list = train[train['diagnosis'] == 'unknown'].sample()['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","2529bf15":"arr = [15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0]\nimage_all=[]\ntitles = ['At Age 15.0','At Age 20.0','At Age 25.0','At Age 30.0','At Age 35.0','At Age 40.0'\n          ,'At Age 45.0','At Age 50.0','At Age 55.0','At Age 60.0','At Age 65.0','At Age 70.0'\n          ,'At Age 75.0','At Age 80.0','At Age 85.0','At Age 90.0']\nfor i in arr:\n    image_list = train[train['age_approx'] == i].sample()['image_name']\n    for image_id in image_list:\n        image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg' \n        img = np.array(Image.open(image_file))\n        image_all.append(img)\ngrid_display(image_all, 4, (15,15), title = True)","a69f47ac":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Reduced Noise', \"Gaussian Blur\", 'Adjusted Contrast']\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    #Reducing Noise\n    result = cv2.fastNlMeansDenoisingColored(img,None,20,10,7,21)\n    image_all.append(result)\n    #Gaussian Blur\n    blur_image = cv2.GaussianBlur(img, (7,7), 0)\n    image_all.append(blur_image)\n    #Adjusted contrast\n    contrast_img = cv2.addWeighted(img, 2.5, np.zeros(img.shape, img.dtype), 0, 0)\n    image_all.append(contrast_img)\ngrid_display(image_all, 4, (15,15), title = True)","432cd870":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Adaptive thresholding', \"Binary thresholding\"]\nfor image_id in image_list:\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    #Adaptive Thresholding..\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    thresh1 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n    image_all.append(thresh1)\n    #Binary Thresholding...\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) \n    res, thresh = cv2.threshold(hsv[:, :, 0], 0, 255, cv2.THRESH_BINARY_INV)\n    image_all.append(thresh)\ngrid_display(image_all, 3, (15,15), title = True)","41f9877b":"img = cv2.imread('\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/ISIC_0052212.jpg', 0)\n# global thresholding\nret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n\n# Otsu's thresholding\nret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# Otsu's thresholding after Gaussian filtering\nblur = cv2.GaussianBlur(img,(5,5),0)\nret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# plot all the images and their histograms\nimages = [img, 0, th1,\n          img, 0, th2,\n          blur, 0, th3]\ntitles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\nplt.figure(figsize=(15,10))\nfor i in range(3):\n    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\nplt.show()","c679b233":"image = []\ntitles = ['Original', 'Thresold Image', 'Contour Image']\nimg = cv2.imread('\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/ISIC_0052212.jpg', 0)\nimg = img[200:900, 500:1500]\nimage.append(img)\n#Apply thresholding\nblur = cv2.GaussianBlur(img,(5,5),0)\nret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nimage.append(th3)\ncontours, hierarcy = cv2.findContours(th3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\nimg2 = img.copy()\nindex = -1\nthickness = 4\ncolor = (255, 0, 255)\n\nobjects = np.zeros([img.shape[0], img.shape[1], 3], 'uint8')\nfor c in contours:\n    cv2.drawContours(objects, [c], -1, color, -1)\n    \n    area = cv2.contourArea(c)\n    perimeter = cv2.arcLength(c, True)\n    \n    M = cv2.moments(c)\n    if M[\"m00\"] != 0:\n        cx = int(M[\"m10\"] \/ M[\"m00\"])\n        cy = int(M[\"m01\"] \/ M[\"m00\"])\n    else:\n    # set values as what you need in the situation\n        cx, cy = 0, 0\n    cv2.circle(objects, (cx, cy), 4, (0, 0, 255), -1)\n    \n    print(\"AREA:{}, perimeter:{}\".format(area, perimeter))\n\nimage.append(objects)\ngrid_display(image, 3, (15,15), title = True)","ec49da18":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Scale Down', \"Scale Up\"]\nfor image_id in image_list:\n    scaleX = 0.6\n    scaleY = 0.6\n    image_file = f'\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    # Scaling Down the image 0.6 times\n    scaleDown = cv2.resize(img, None, fx= scaleX, fy= scaleY, interpolation= cv2.INTER_LINEAR)\n    image_all.append(scaleDown)\n    # Scaling up the image 1.8 times\n    scaleUp = cv2.resize(img, None, fx= scaleX*3, fy= scaleY*3, interpolation= cv2.INTER_LINEAR)\n    image_all.append(scaleUp)\ngrid_display(image_all, 3, (15,15), title = True)","8cc311f3":"image_all=[]\ntitles = ['original', 'ORB Detected', \"Zoom Image\"]\nimg = cv2.imread('\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/ISIC_0052212.jpg', 1)\nimage_all.append(img)\n# Initiate ORB detector\norb = cv2.ORB_create()\n# find the keypoints with ORB\nkp = orb.detect(img,None)\n# compute the descriptors with ORB\nkp, des = orb.compute(img, kp)\n# draw only keypoints location,not size and orientation\nimg2 = cv2.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\nimage_all.append(img2)\nimg3 = img2[350:800,600:1250]\nimage_all.append(img3)\ngrid_display(image_all, 3, (35,35), title = True)","bdcc0037":"# Detect hardware\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n#If TPU not found try with GPUs\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy for hardware\nif tpu:\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n  print('Running on TPU ', tpu.master())  \nelif len(gpus) > 0:\n  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n  print('Running on ', len(gpus), ' GPU(s) ')\nelse:\n  strategy = tf.distribute.get_strategy()\n  print('Running on CPU')\n\n# How many accelerators do we have ?\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","fab877c3":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')","00d7cfe5":"TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '\/tfrecords\/train*')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '\/tfrecords\/test*')\nBATCH_SIZE = 10 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]\nAUTO = tf.data.experimental.AUTOTUNE\nimSize = 1024\nEPOCHS = 10\n\nVALIDATION_SPLIT = 0.18\nsplit = int(len(TRAINING_FILENAMES) * VALIDATION_SPLIT)\ntraining_filenames = TRAINING_FILENAMES[split:]\nvalidation_filenames = TRAINING_FILENAMES[:split]\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\"\n      .format(len(TRAINING_FILENAMES), len(training_filenames), len(validation_filenames)))\nTRAINING_FILENAMES = training_filenames","b33d8aba":"def read_labeled_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    label = tf.cast(example['target'], tf.int32)\n    return image, label \n\ndef read_unlabeled_tfrecord(example):\n    u_features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, u_features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    idnum = example['image_name']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","253187a2":"def data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    #image = tf.image.random_brightness(x, 0.2)\n    #image = cutmix(image, label)\n    return image, label   \n\ndef get_training_dataset(dataset, do_aug=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(filenames, train=False):\n    dataset = load_dataset(filenames, labeled=True)\n    dataset = dataset.cache() # This dataset fits in RAM\n    if train:\n    # Best practices for Keras:\n    # Training dataset: repeat then batch\n    # Evaluation dataset: do not repeat\n        dataset = dataset.repeat()\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n        dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n    \ndef get_test_dataset(dataset, ordered=False):\n    dataset = load_dataset(dataset, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nvalidation_dataset = get_validation_dataset(validation_filenames, train=False)\ntraining_dataset = get_training_dataset(TRAINING_FILENAMES)\ntest_dataset = get_test_dataset(TEST_FILENAMES)\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nNUM_VALID_IMAGES = count_data_items(validation_filenames)\nvalidation_steps = NUM_VALID_IMAGES \/\/ BATCH_SIZE\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nprint('Dataset: {} training images, {} unlabeled test images, {} validition images'\n      .format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES, NUM_VALID_IMAGES))","2504d8b1":"def get_random_eraser(input_img,p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1\/0.3, v_l=0, v_h=255, pixel_level=False):\n   # def eraser(input_img):\n    img_h, img_w, img_c = input_img.shape\n\n    while True:\n        s = np.random.uniform(s_l, s_h) * img_h * img_w\n        r = np.random.uniform(r_1, r_2)\n        w = int(np.sqrt(s \/ r))\n        h = int(np.sqrt(s * r))\n        left = np.random.randint(0, img_w)\n        top = np.random.randint(0, img_h)\n\n        if left + w <= img_w and top + h <= img_h:\n            break\n\n    if pixel_level:\n        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n    else:\n        c = np.random.uniform(v_l, v_h)\n\n    input_img[top:top + h, left:left + w, :] = c\n\n    return input_img","e2fb6394":"TRAIN = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\nIMAGE_SIZE = 1024\nn_imgs = 12\nimg_filenames = os.listdir(TRAIN)[:n_imgs]\nimg_filenames[:3]\nimage=[]\nfor file_name in img_filenames:\n    img = cv2.imread(TRAIN +file_name)\n    img = get_random_eraser(img)\n    image.append(img)\ngrid_display(image, 4, (15,15))","09a7f156":"# if you have label in images\ndef onehot(image,label):\n    CLASSES = 2 # Define number of classes our model have\n    return image,tf.one_hot(label,CLASSES)\n\ndef cutmix(image, label): #, PROBABILITY = 1.0\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = 1024 #IMAGE_SIZE[0]\n    CLASSES = 2\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32)\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM,y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM,x+WIDTH\/\/2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH\/DIM\/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","32baceb9":"AUG_BATCH = 48\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH\/\/col)\nall_elements = training_dataset.unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(cutmix)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","dc5e6d59":"def transform(image, inv_mat, image_shape):\n    h, w, c = image_shape\n    cx, cy = w\/\/2, h\/\/2\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n    new_zs = tf.ones([h*w], dtype=tf.int32)\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w\/\/2), tf.round(old_coords[1, :] + h\/\/2)\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n    rotated_image_channel = list()\n    for i in range(c):\n        vals = rotated_image_values[:,i]\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n\ndef random_rotate(image, angle, image_shape):\n    def get_rotation_mat_inv(angle):\n        # transform to radian\n        angle = math.pi * angle \/ 180\n        cos_val = tf.math.cos(angle)\n        sin_val = tf.math.sin(angle)\n        one = tf.constant([1], tf.float32)\n        zero = tf.constant([0], tf.float32)\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n        return rot_mat_inv\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\n    rot_mat_inv = get_rotation_mat_inv(angle)\n    return transform(image, rot_mat_inv, image_shape)\n\n\ndef GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n    h, w = image_height, image_width\n    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n    hh = hh+1 if hh%2==1 else hh\n    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n\n    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n\n    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n\n    for i in range(0, hh\/\/d+1):\n        s1 = i * d + st_h\n        s2 = i * d + st_w\n        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n\n    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\n    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\n    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n\n    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n\n    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n    x_ranges = tf.repeat(x_ranges, hh)\n    y_ranges = tf.repeat(y_ranges, hh)\n\n    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n\n    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n\n    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n\n    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n\n    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n    mask = tf.image.crop_to_bounding_box(mask, (hh-h)\/\/2, (hh-w)\/\/2, image_height, image_width)\n\n    return mask\n\ndef apply_grid_mask(image, image_shape):\n    AugParams = {\n        'd1' : 100,\n        'd2': 160,\n        'rotate' : 45,\n        'ratio' : 0.3\n    }\n    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\n    if image_shape[-1] == 3:\n        mask = tf.concat([mask, mask, mask], axis=-1)\n    return image * tf.cast(mask,tf.float32)\n\ndef gridmask(img_batch, label_batch):\n    return apply_grid_mask(img_batch, (1024,1024, 3)), label_batch","e39b56aa":"AUG_BATCH = 48\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH\/\/col)\nall_elements = training_dataset.unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(gridmask)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","242e450b":"# data dump\nprint(\"Training data shapes:\")\nfor image, label in training_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())\n\nprint(\"Validation data shapes:\")\nfor image, label in validation_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", label.numpy())\n\nprint(\"Test data shapes:\")\nfor image, idnum in test_dataset.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U'))","e8b7d654":"# with strategy.scope():\n#     model = tf.keras.Sequential([\n#         dense.DenseNet121(\n#             input_shape=(imSize, imSize, 3),\n#             weights='imagenet',\n#             include_top=False\n#         ),\n#         layers.GlobalAveragePooling2D(),\n#         layers.Dense(1, activation='sigmoid')\n#     ])\n        \n#     model.compile(\n#         optimizer='adam',\n#         loss = 'binary_crossentropy',\n#         metrics=['accuracy']\n#     )\n#     model.summary()","d4bb4c51":"import tensorflow.keras.applications.xception as xcep\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        xcep.Xception(\n            input_shape=(imSize, imSize, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(1024, activation= 'relu'), \n        layers.Dropout(0.2),\n        layers.Dense(512, activation= 'relu'), \n        layers.Dropout(0.2), \n        layers.Dense(256, activation='relu'), \n        layers.Dropout(0.2), \n        layers.Dense(128, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(64, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(1, activation='sigmoid')\n    ])\n        \n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary()","a75be7d5":"# !pip install -q efficientnet","c5aad432":"# import efficientnet.tfkeras as efn\n# with strategy.scope():\n#     model = tf.keras.Sequential([\n#         efn.EfficientNetB5(\n#             input_shape=(imSize, imSize, 3),\n#             weights='imagenet',\n#             include_top=False\n#         ),\n#         layers.GlobalAveragePooling2D(),\n#         layers.Dense(512, activation= 'relu'), \n#         layers.Dropout(0.2), \n#         layers.Dense(256, activation='relu'), \n#         layers.Dropout(0.2), \n#         layers.Dense(128, activation='relu'), \n#         layers.Dropout(0.1),\n#         layers.Dense(64, activation='relu'), \n#         layers.Dropout(0.1),\n#         layers.Dense(1, activation='sigmoid')\n#     ])\n        \n#     model.compile(\n#         optimizer='adam',\n#         loss = 'binary_crossentropy',\n#         metrics=['accuracy']\n#     )\n#     model.summary()","b3128c34":"def lrfn(epoch):\n    LR_START = 0.00001\n    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n    LR_MIN = 0.00001\n    LR_RAMPUP_EPOCHS = 5\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .8\n    \n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n","7a70aa57":"EPOCHS = 10\nhistory = model.fit(training_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n                    validation_data=validation_dataset,callbacks=[lr_schedule])","47f34ba2":"display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)","e63dace3":"test_ds = test_dataset#get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds).flatten()\nprint(probabilities)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, probabilities]), fmt=['%s', '%f'],\n           delimiter=',', header='image_name,target', comments='')\n","8e46ad8a":"sub = pd.read_csv(\"submission.csv\")\nsub.head()","3ceccab7":"sub['target'].hist()","c8b69b93":"We cannnot fill this diagnosis column, because it may affect other variable too. like if we put diagnosis is \"nevus\" but actually it is melanoma, then it may affect it in deciding bengin or, Maligment.","2712fb0a":"## Data shape...","93e3ceff":"## Visualize the Chunk of Non-Melanoma Images...","66a5c797":"## Few Word about dataset...\n\n### What should I expect the data format to be?\nThe images are provided in DICOM format. This can be accessed using commonly-available libraries like **pydicom**, and contains both image and metadata. It is a commonly used medical imaging data format.\n\nImages are also provided in **JPEG** and **TFRecord** format (in the jpeg and tfrecords directories, respectively). Images in TFRecord format have been resized to a uniform 1024x1024.\n\nMetadata is also provided outside of the DICOM format, in CSV files. See the Columns section for a description.\n\n## Columns...\n* **image_name** - unique identifier, points to filename of related DICOM image\n* **patient_id** - unique patient identifier\n* **sex** - the sex of the patient (when unknown, will be blank)\n* **age_approx** - approximate patient age at time of imaging\n* **anatom_site_general_challenge** - location of imaged site\n* **diagnosis** - detailed diagnosis information (train only)\n* **benign_malignant** - indicator of malignancy of imaged lesion\n* **target** - binarized version of the target variable","11378398":"## GridMask data augmentation\nGridMask is belong to Information dropping method. Information dropping might help improving model generalization through enforcing model to learn on remain information. But keeping the deletion and reservation region in a balance relation is needed. Too much information dropping might result in under-fitting, but too few might result in over-fitting.","52a723c4":"Interpreting The Heatmap The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","2056dad6":"## Visualiza the skin cancer nevus....","89b1831f":"## Visualize the skin cancer at Upper extremity...","7cbf8a10":"## What next...\n* Try to use Image Augmentation for model Building like Cutmix, Mixup, Gridmask etc...\n* Try to use Model like Squeezenet, Inception_V3 etc...\n* Try to improve the model performance.\n    * may also try some machine learning model.\n    * Try Gridsearchcv\n    * use ensembales of models.\n    * use of ensembales weights of correlations.\n* And What all I learn in this competition.","70c8bae2":"## Visualiza the skin cancer at Palms\/soles...","8b7c181d":"## Visualize the Chunk of Melanoma Images...","c0d2fb49":"## Symptoms and warning signs\nBasal cell and squamous cell carcinoma are two examples of nonmelanoma skin cancer.The U.S-based Skin Cancer Foundation says that everyone should examine their whole body, from head to toe, once a month, and take note of:\n* any new moles or growths\n* moles or growths that have grown\n* moles or growths that have changed significantly in another way\n* lesions that change, itch, bleed or have not healed\n* The most common sign of skin cancer is an abnormal pink or brown spot, patch, or mole.\n\nThere are different forms of skin cancer, and the most common are:\n* basal cell carcinoma\n* squamous cell carcinoma\n* melanoma\n\nMelanoma is the type most likely to develop in a mole.\n\nEnlarged lymph nodes can also signal skin cancer. Lymph nodes are small, bean-sized collections of immune cells beneath the skin. Many are in the neck, groin, and underarms.\n\n## Staging for basal cell and squamous cell carcinoma\nBasal cell and squamous cell carcinoma usually do not spread to other parts of the body. On rare occasions, a person\u2019s lymph node(s) may be removed to find out if the cancer has spread, which is called metastasis. Lymph nodes are bean-shaped organs that help fight infection. The doctor may recommend other tests to determine the extent of the disease, including blood tests, chest x-rays, and imaging scans of the lymph nodes and nerves, liver, bones, and brain, but this is uncommon.\n\n### Staging for Merkel cell cancer\nDoctors use the TNM system to describe the stage of Merkel cell cancer. Doctors use the results from diagnostic tests and scans to answer these questions:\n\n* Tumor (T): How large is the primary tumor? Where is it located?\n* Node (N): Has the tumor spread to the lymph nodes? If so, where and how many?\n* Metastasis (M): Has the cancer spread to other parts of the body? If so, where and how much?\n\nThe results are combined to determine the stage of Merkel cell cancer for each person.\n\nThere are 5 stages: stage 0 (zero) and stages I through IV (1 through 4). The stage provides a common way of describing the cancer, so doctors can work together to plan the best treatments.\n\n* Stage 0: This is called carcinoma in situ. Cancer cells are found only in the top layers of the skin. The cancer does not involve the lymph nodes, and it has not spread.\n* Stage I: The primary tumor is 2 centimeters (cm) or smaller at its widest part. The cancer has not spread to the lymph nodes or to other parts of the body.\n* Stage IIA: The tumor is larger than 2 cm and has not spread to the lymph nodes or other parts of the body.\n* Stage IIB: The tumor has grown into nearby tissues, such as muscles, cartilage, or bone. It has not spread to the lymph nodes or elsewhere in the body.\n* Stage III: The cancer has spread to the lymph nodes. The tumor can be any size and may have spread to nearby bone, muscle, connective tissue, or cartilage.\n    * Stage IIIA: The tumor is any size or may have grown into nearby tissues. Biopsy or surgery has found that the cancer has spread to nearby lymph nodes. The cancer has not spread to other parts of the body. Or, there is no sign of a tumor, but cancer was found in a nearby lymph node during an exam or with imaging scans. Its presence was confirmed using a microscope.\n    * Stage IIIB: The tumor is any size or may have grown into nearby tissues. The cancer has spread through the lymphatic system, either to a regional lymph node located near where the cancer started or to a skin site on the way to a lymph node, called \u201cin-transit metastasis.\u201d In-transit metastasis may have reached these other lymph nodes. The lymphatic system is part of the immune system and drains fluid from body tissues through a series of tubes or vessels.\n* Stage IV: The tumor has spread to distant parts of the body, such as the liver, lung, bone, or brain.\n\n## How to spot basal and squamous cell skin cancers\nBasal and squamous cell skin cancers are more common and not as dangerous as melanoma. They can develop anywhere, but they are most likely to form on the face, head, or neck. A basal cell carcinoma may look like:\n* a flat, firm, pale or yellow area of skin, similar to a scar\n* a reddish, raised, sometimes itchy patch of skin\n* small shiny, pearly, pink or red translucent bumps, which can have blue, brown, or black areas.\n* pink growths that have raised edges and a lower center, and abnormal blood vessels may spread from the growth like the spokes of a wheel\n* open sores that may ooze or crust, and either do not heal or heal and return\n\nA squamous cell carcinoma may look like:\n* a rough or scaly red patch that may crust or bleed\n* a raised growth or lump, sometimes with a lower center\n* open sores that may ooze or crust, and either do not heal or heal and return\n* a growth that looks like a wart\n\n### Not all skin cancers look alike. The American Cancer Society recommend that a person should contact a doctor if they notice:\n* a mark that does not look like others on the body\n* a sore that does not heal\n* redness or new swelling outside the border of a mole\n* itching, pain, or tenderness in a mole\n* oozing, scaliness, or bleeding in a mole\n","df80cef3":"# Model","51813f90":"## Visualiza the skin cancer unknown...","7f58a920":"## Apply Some Image Augmentation Technique...\n### <font color='red'>I strongly recommend to Read my kernal on Image Augmentation for Detail understanding.<font>[click here](https:\/\/www.kaggle.com\/saife245\/cutmix-vs-mixup-vs-gridmask-vs-cutout\/notebook#MixUp-data-augmentation)","09a2abd9":"## Visualiza the skin cancer cafe-au-lait macule...","d02a2219":"## Visualize the skin cancer at lower extremity...","0b6b8290":"### Writing Helper Fucntion....","0559597f":"# About this notebook....\nSkin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths.This notebook give a overview of these compatition. In this we will see different Augmentation technique, EDA, Model and some cool stuff...\n\nAs always i try my level best to explian each and everything in this competation as simple as possible. And try to update this notebook as I learn new thing in this competation...","17316dad":"## Now the Question is, How to diagnose skin cancer...\nFirst, a doctor will examine the skin and take a medical history.\n\nThey will usually ask when the mark first appeared, if its appearance has changed, if it is ever painful or itchy, and if it bleeds.\n\nThe doctor will also ask about a person\u2019s family history, and any other risk factors, such as lifetime sun exposure.They may also check the rest of the body for other atypical moles and spots. Finally, they may feel the lymph nodes to determine if they are enlarged. The doctor may refer a person to a dermatologist, a skin doctor, who may:\n* examine the mark with a dermatoscope, a handheld magnifying device\n* take a small sample of skin, a biopsy, and send it to a lab to check for signs of cancer","1621fd5e":"## Visualiza the skin cancer Melanoma...","f7a0cbe8":"## Visualiza the skin cancer solar lentigo...","8e2757cc":"# Reading Tensorflow Record...","a91a86bf":"## ORB (Oriented FAST and Rotated BRIEF)...\n\nORB is basically a fusion of FAST keypoint detector and BRIEF descriptor with many modifications to enhance the performance. First it use FAST to find keypoints, then apply Harris corner measure to find top N points among them. It also use pyramid to produce multiscale-features.\n\nIt computes the intensity weighted centroid of the patch with located corner at center. The direction of the vector from this corner point to centroid gives the orientation.","f626b16d":"## Visualiza the skin cancer at Oral\/genital...","f918f1a2":"## Cutout data augmentation...\nCutout is a simple regularization technique for convolutional neural networks that involves removing contiguous sections of input images, effectively augmenting the dataset with partially occluded versions of existing samples.","83aba776":"### Adaptive Thresholding...\n* Adaptive thresholding is a form of thresholding that takes into account spatial variations in illumination. \n* We present a technique for real-time adaptive thresholding using the integral image of the input.\n\n### Binary Thresholding...\n* Image thresholding is a simple, yet effective, way of partitioning an image into a foreground and background. \n* This image analysis technique is a type of image segmentation that isolates objects by converting grayscale images into binary images.\n* Binary (Bi-valued) Image means, only bi or two intensity values can be used to represent the whole image.\n","dd2a4193":"### Use Noise Reduction...\n* Noise means, the pixels in the image show different intensity values instead of true pixel values that are obtained from image. \n* Noise removal algorithm is the process of removing or reducing the noise from the image.\n\n### Use Gaussian Blur...\n* In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). \n* It is a widely used effect in graphics software, typically to reduce image noise and reduce detail.\n\n### Adjusted Contrast...\n* To change the contrast or brightness of an image, the Adjust Contrast tool performs contrast stretching. \n* In this process, pixel values below a specified value are mapped to black and pixel values above a specified value are mapped to white.","9f05ea40":"# Training","60106677":"## Update Version...\n* Version 5 - Updating Visualization\n* Version 4 - Increase the Epochs and adding Dropout....\n* Version 3 - Updating model and Updating IP...\n* version 2 - Adding few other Image processing and Details...","95e6ca3e":"## TPU detection...","a76dd4cd":"## What am I predicting?\n* You are predicting a binary target for each image. \n* Your model should predict the probability (floating point) between 0.0 and 1.0 that the lesion in the image is malignant (the target). \n* In the training data, train.csv, the value 0 denotes benign, and 1 indicates malignant.","35b340aa":"## Melanoma: Latest Research...\nDoctors are working to learn more about melanoma, ways to prevent it, how to best treat it, and how to provide the best care to people diagnosed with this disease. The following areas of research may include new options for patients through clinical trials. This section is not meant to be a complete list of new clinical trials because this field is changing rapidly. Always talk with your doctor about the best diagnostic and treatment options for you.\n\n* **Enhanced prevention and early detection methods.** There is ongoing research on better prevention and early detection strategies for melanoma. Both primary and secondary prevention are important. Primary prevention involves keeping melanoma from developing, such as reducing exposure of ultraviolet (UV) radiation from the sun and avoiding the use of indoor tanning devices. Secondary prevention includes methods of early detection. One promising area is the screening of people with a high risk of developing melanoma.\n\n* **Targeted therapy.** As discussed in Types of Treatment, targeted therapy is a treatment that targets specific genes or proteins. Research has identified a number of molecular pathways and activated or mutated genes in melanoma. Clinical trials are testing new drugs to inhibit the MAP kinase pathway and other pathways that melanoma might use to grow and spread. Strategies to prevent the melanoma from becoming resistant to treatment are also being tested, such as using combinations of drugs or exploring new schedules of giving drugs to patients.\n\n* **Immunotherapy.** Researchers are studying new checkpoint inhibitors and immunotherapies directed at other parts of the immune system. These include TIM3 inhibitors, LAG3 inhibitors, OX40 agonists, CD137 agonists, GITR agonists, and IDO inhibitors. Also, a number of combinations of immunotherapies are being looked at, including all of the above agents with anti-PD-1 or anti-PD-L1 antibodies, as well as combinations of targeted therapies with anti-PD-1 or anti-PD-L1 antibodies.\n\n* **Chimeric antigen receptor T-cell (CAR-T) therapy and T-cell receptor (TCR) therapy.** Another type of experimental immunotherapy involves altering a person\u2019s white blood cells, known as lymphocytes, in a laboratory. This is done to increase their ability to fight the tumor. The changed cells are given back to the patient, often in combination with chemotherapy, interleukin-2, and\/or other immunotherapies.\n\n* **Vaccines.** Therapeutic vaccines that may improve the specific immune response to melanoma have been the focus of multiple clinical trials. Melanoma peptide vaccines are being evaluated in clinical trials for patients with both localized and advanced melanoma. Research has shown that vaccination can cause the immune system to fight melanoma, even in advanced disease, but these therapies are still considered experimental. The vaccines are made using certain proteins found only on a melanoma tumor and are given as an injection. The person\u2019s immune system then recognizes the proteins and destroys melanoma cancer cells. To date, no vaccines have shown a clinical benefit in patients. Learn more about cancer vaccines.\n\n* **Palliative care\/supportive care.** Clinical trials are underway to find better ways of reducing symptoms and side effects of current melanoma treatments to improve comfort and quality of life for patients.","3b73486d":"### I hope You got the Data Exploration.... \n\n# Now Move to data Visualization...\nData visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers of the images. This communication is achieved through the use of a systematic mapping between graphic marks and data values in the creation of the visualization.","a4cf7121":"# End Note...\n\nThis notebook is work in progress.And I try my level best to explain each and every thing as I learn in this competition. And I try to update this kernal as i learn new thing in this.\n\nYaa... I know Kaggle is too busy in Organising new Competation, But Where We have to Go ... It's LockDown so **keep Kaggling.....**\n\n### <font color='blue'>I hope you find this kernel useful and enjoyable, If so Please upvote it. And Your comments and feedback are most welcome. ;-)<font>\n    \n### <font color='red'>STAY TUNED!!!<font>","be350b8d":"## Creating Submission...","5926c4a3":"## Visualiza the skin cancer lentigo NOS...","8ff456bf":"## Data EDA...\n\nLet first analyse the CSV file, and Understand the data contain in it...","09d02bc5":"## Benign and malignant tumors\n* **Benign tumor**:- A benign tumor put simply is one that will not cause any cancerous growth. It will not damage anythin, it's just a small blot on the landscape of your skin.\n\n* **Malignant tumor**:- A malignant tumor is the evil twin of the benign tumor: it causes cancerous growth. ","99a972d7":"## Intialize the Value...","62c2e9a6":"## Scale Up & Scale Down...\n\n* When scaling a raster graphics image, a new image with a higher or lower number of pixels must be generated. \n* In the case of decreasing the pixel number (scaling down) this usually results in a visible quality loss.\n* In the case of Increasing the pixel number (scaling Up) this usually results in a visible quality Increase.","981b7401":"## Melanoma Skin Cancer Stages...\nThe stage of a cancer describes how much cancer is in the body. It helps determine how serious the cancer is and how best to treat it. Doctors also use a cancer's stage when talking about survival statistics.\n\n## Factors used for staging melanoma\nTo determine the stage of a melanoma, the lesion and some surrounding healthy tissue needs to be surgically removed and analyzed using a microscope. Doctors use the melanoma\u2019s thickness, measured in millimeters (mm), and the other characteristics described in the Diagnosis section to help determine the disease\u2019s stage.\n\nDoctors also use results from diagnostic tests to answer these questions about the stage of melanoma:\n* How thick or deep is the original melanoma, often called the primary melanoma or primary tumor?\n* Where is the melanoma located?\n* Has the melanoma spread to the lymph nodes? If so, where and how many?\n* Has the melanoma metastasized to other parts of the body? If so, where and how much?\n\nThe results are combined to determine the stage of melanoma for each person. The stages of melanoma include: stage 0 (zero) and stages I through IV (1 through 4). The stage provides a common way of describing the cancer, so doctors can work together to create the best treatment plan and understand a patient's prognosis.\n\n## Melanoma stage grouping...\n* Stage 0: This refers to melanoma in situ, which means melanoma cells are found only in the outer layer of skin or epidermis. This stage of melanoma is very unlikely to spread to other parts of the body.\n\n* Stage I: The primary melanoma is still only in the skin and is very thin. Stage I is divided into 2 subgroups, IA or IB, depending on the thickness of the melanoma and whether a pathologist sees ulceration under a microscope.\n\n* Stage II: Stage II melanoma is thicker than stage I melanoma, extending through the epidermis and further into the dermis, the dense inner layer of the skin. It has a higher chance of spreading. Stage II is divided into 3 subgroups\u2014A, B, or C\u2014depending on how thick the melanoma is and whether there is ulceration.\n* Stage III: This stage describes melanoma that has spread locally or through the lymphatic system to a regional lymph node located near where the cancer started or to a skin site on the way to a lymph node, called \u201cin-transit metastasis, satellite metastasis, or microsatellite disease.\u201d The lymphatic system is part of the immune system and drains fluid from body tissues through a series of tubes or vessels. Stage III is divided into 4 subgroups\u2014A, B, C, or D\u2014depending on the size and number of lymph nodes involved with melanoma, whether the primary tumor has satellite or in-transit lesions, and if it appears ulcerated under a microscope.\n* Stage IV: This stage describes melanoma that has spread through the bloodstream to other parts of the body, such as distant locations on the skin or soft tissue, distant lymph nodes, or other organs like the lung, liver, brain, bone, or gastrointestinal tract. Stage IV is further evaluated based on the location of distant metastasis:\n    * M1a: The cancer has only spread to distant skin and\/or soft tissue sites.\n    * M1b: The cancer has spread to the lung.\n    * M1c: The cancer has spread to any other location that does not involve the central nervous system.\n    * M1d: The cancer has spread to the central nervous system, including the brain, spinal cord, and\/or cerebrospinal fluid, or lining of the brain and\/or spinal cord.\n\n**NOTE:-** **Recurrent:** Recurrent melanoma is melanoma that has come back after treatment. If the melanoma does return, there will be a round of tests to learn about the extent of the recurrence. These tests and scans may be similar to those done at the time of the original diagnosis.\n\n# How to spot melanoma\nThe medical community has developed two ways to spot the early signs of melanoma, the most dangerous type of skin cancer.\nA person can use the ABCDE method and the ugly duckling method.\n\n1. **The ABCDE method:-** Brown spots, marks, and moles are usually harmless. However, the first sign of melanoma can occur in what doctors call an atypical mole, or dysplastic nevi. To spot an atypical mole, check for the following:\n\n    * A: Asymmetry. If the two halves of a mole do not match, this can be an early indication of melanoma.\n    * B: Border. The edges of a harmless mole are even and smooth. If a mole has uneven edges, this can be an early sign of melanoma. The mole\u2019s border may be scalloped or notched.\n    * C: Color.Harmless moles are a single shade, usually of brown. Melanoma can cause differentiation in shade, from tan, brown, or black to red, blue, or white.\n    * D: Diameter. Harmless moles tend to be smaller than dangerous ones, which are usually larger than a pencil\u2019s eraser \u2014 around one-quarter of an inch, or 6 millimeters.\n    * E: Evolving. If a mole starts to change, or evolve, this can be a warning. Changes may involve shape, color, or elevation from the skin. Or, a mole may start to bleed, itch, or crust.\n![mealona.JPG](attachment:mealona.JPG)\n\n2. **The ugly duckling method:-** The ugly duckling method works on the premise that a person\u2019s moles tend to resemble one another. If one mole stands out in any way, it may be a sign of skin cancer.\n\nOf course, not all moles and growths are cancerous. However, if a person notices any of the above characteristics, they should speak to a healthcare professional.\n\n![Melanoma-Cancer.JPG](attachment:Melanoma-Cancer.JPG)","e03f5f4b":"## CutMix data augmentation\nCutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.","4029e6f7":"## Visualiza the skin cancer lichenoid keratosis...","fb1a6506":"# Causes and risk factors...\nResearchers do not know why certain cells become cancerous. However, they have identified risk factors for skin cancer.\nThe most important risk factor for melanoma is exposure to ultraviolet (UV) rays. These damage the skin cells\u2019 DNA, which controls how the cells grow, divide, and stay alive. Most UV rays come from sunlight, but they also come from tanning beds.\n\nOther risk factors include:\n* Moles \u2013 A person with more than 100 moles is more likely to develop melanoma.\n* Fair skin, light hair, and freckles \u2013 The risk of developing melanoma is higher among people with light skin. Those who burn easily have an increased risk.\n* Family history \u2013 Around 10 percent of people with the disease have a family history of it.\n* Personal history \u2013 Melanoma is likelier to form in a person who has already had it. People who have had basal or squamous cell cancers also have an increased risk of developing melanoma.\n\n## Non-cancerous skin growths...\nIt can be easy to mistake benign growths for skin cancer. The following skin conditions have similar symptoms to skin cancer:\n\n* **Seborrheic keratosis:** brown, black, or tan growths that appear in older adults.\n* **Cherry angioma or hemangioma:** small growths, made up of blood vessels, that are typically red but may rupture and turn brown or black.\n* **Freckles:** flat, darker areas of skin that appear after the skin is exposed to UV light.\n* **Dermatofibroma:** small, firm, round bumps that form under the skin and may change color over time.\n* **Skin tags:** harmless, soft growths.\n\n# Treatment...\n* A doctor usually removes basal cell and squamous cell cancers with minor surgery.\n\n* Radiation therapy is an alternative treatment when a person cannot undergo surgery. A doctor may also recommend this treatment when the cancer is in a place that would make surgery difficult, such as on the eyelids, nose, or ears.\n\n* For melanoma, the best treatment will depend on the stage and location of the cancer. If a doctor diagnoses melanoma early, they can usually remove it with minor surgery.\n\n* In some cases, doctors may suggest other types of surgery or radiation therapy.\n\n## Preventing skin cancer...\nThe best way to reduce the risk of skin cancer is to limit exposure to UV rays. A person can do this by using sunscreen, seeking shade, and covering up when outdoors.\n\nAnyone wishing to prevent skin cancer should also avoid tanning beds and sunlamps.\n\n## Melanoma: Survivorship...\nSurvivorship is one of the most complicated parts of having cancer. This is because it is different for everyone.\n\nSurvivors may experience a range of feelings, including joy, concern, relief, guilt, and fear. Some people say they appreciate life more after a cancer diagnosis and have gained a greater acceptance of themselves. Others become very anxious about their health and uncertain about coping with everyday life.\n\nSurvivors may feel some stress or anxiety when their treatment ends and their visits to the health care team become less frequent. Often, relationships built with the cancer care team provide a sense of security during treatment, and people miss this source of support. This may be especially true when new worries and challenges surface over time, such as any late effects of treatment, emotional challenges including fear of recurrence, sexual health and fertility concerns, and financial and workplace issues.\n\nEvery survivor has individual concerns and challenges. With any challenge, a good first step is being able to recognize your fears and talk about them. Effective coping requires:\n\n* Understanding the challenge you are facing\n\n* Thinking through solutions\n\n* Asking for and allowing the support of others\n\n* Feeling comfortable with the course of action you choose\n\nMany survivors find it helpful to join an in-person support group or an online community of survivors. This allows you to talk with people who have had similar first-hand experiences. Other options for finding support include talking with a friend or member of your health care team, individual counseling, or asking for assistance at the learning resource center of the place where you received treatment.","2e2a034a":"## Otsu\u2019s Binarization....\n* In computer vision and image processing, Otsu's method is used to perform automatic image thresholding. \n* In the simplest form, the algorithm returns a single intensity threshold that separate pixels into two classes, foreground and background.","1d620b58":"## Visualiza the skin cancer atypical melanocytic proliferation...","14642e6c":"## Visualizing training Curve...","74d1920e":"## Introduction...\nSkin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection\u2014potentially aided by data science\u2014can make treatment more effective.\n\nCurrently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or \u201cugly ducklings\u201d that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account \u201ccontextual\u201d images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.","9aa24d3e":"## Choosing tensorflow Record for model...\n* Data is in [.tfrec] format which is tensorflow record Format.\n* **TensorFlow Record:-** A TFRecord file stores your data as a sequence of binary strings.\n* Importances...\n    * Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk. This is especially true if your data is stored on spinning disks, due to the much lower read\/write performance in comparison with SSDs.\n    * Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed.\n    * Another major advantage of TFRecords is that it is possible to store sequence data \u2014 for instance, a time series or word encodings \u2014 in a way that allows for very efficient and (from a coding perspective) convenient import of this type of data.\n* you need to specify the structure of your data before you write it to the file. Tensorflow provides two components for this purpose:\n    * tf.train.Example and\n    * tf.train.SequenceExample.\n* You have to store each sample of your data in one of these structures, then serialize it and use a tf.python_io.TFRecordWriter to write it to disk.","2ae8a1eb":"# Making Prediction...","e9434f05":"## checking the correlation between features and Target...","3e8376b0":"## Calculating Area and Parameter of cancerous part of cell...","f6048854":"## Some other model performance...\n* some of the previous work on melanoma detection and model performance.\n![accuracy.jpg](attachment:accuracy.jpg)","189e62c6":"## Visualiza the skin cancer at head\/neck...","dbc0cca0":"## Skin cancer At different Age Group...","15f3edf3":"## References...\n* Image Augmentation technique Which is Needed in this competation.[Click here](https:\/\/www.kaggle.com\/saife245\/cutmix-vs-mixup-vs-gridmask-vs-cutout\/notebook#MixUp-data-augmentation)\n* Thanks to starter kernal.[click here](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu)\n* Thanks to kernal for some idea.[click here](https:\/\/www.kaggle.com\/manojprabhaakr\/melanoma-tpu-starter-efficientnet-b0)\n\n## My Other work on Kaggle...\n* Image Augmentation technique Which is Needed in this competation.[Click here](https:\/\/www.kaggle.com\/saife245\/cutmix-vs-mixup-vs-gridmask-vs-cutout\/notebook#MixUp-data-augmentation)\n* 100+ Visualization in R.[click here](https:\/\/www.kaggle.com\/saife245\/r-challenge-of-100-visualization-in-r)\n* SqueezeNet-model of size 0.5 MB.[Click here](https:\/\/www.kaggle.com\/saife245\/squeezenet-model-size-of-0-5mb-is-it-true)\n* NeuroImaging Challenge Notebook.[Click here](https:\/\/www.kaggle.com\/saife245\/neuroimaging-in-depth-understanding-eda-model)","e4ed189d":"## Visualiza the skin cancer seborrheic keratosis...","a2a4e785":"# Imports","0014dad8":"## Let see some Stats....\n\nThis year an estimated 100,350 adults (60,190 men and 40,160 women) in the United States will be diagnosed with invasive melanoma of the skin. Melanoma is the fifth most common cancer among men and the sixth most common cancer among women.  \n\nMelanoma is 20 times more common in white people than in black people.**(so you can say that Melanoma is a kind of Racist dieases). Apart from joke,** The average age of diagnosis is 65. Before age 50, more women are diagnosed with melanoma than men. However, by age 65, men are 2 times more likely to be diagnosed with melanoma. By age 80, men are 3 times more likely to be diagnosed with melanoma.  \n\nThe development of melanoma is more common as people grow older, but it also develops in younger people, including those younger than 30 years old. It is one of the most common cancers diagnosed in young adults, particularly for women. This year, about 2,400 cases of melanoma will be diagnosed in people age 15 to 29.   \n\nThe number of people diagnosed with melanoma has risen sharply over the past 3 decades, although this varies by age. In men and women ages 50 and older, the number of people diagnosed with melanoma increased more than 2% per year from 2007 to 2016. However, the rate decreased by more than 1% per year in people younger than 50. \n\nSpecifically, the number of adolescents age 15 to 19 diagnosed with melanoma declined 6% each year between 2007 and 2016. The number of adults in their 20s diagnosed with the disease decreased by 3%. For adults in their 30s, the number of people diagnosed with melanoma remained steady in women and dropped slightly for men. The decrease in melanoma in younger people is likely due in part to increased sun-protection behaviors and a reduction in indoor tanning.  \n\nMelanoma accounts for about 1% of all skin cancers diagnosed in the United States, but it causes most of the skin cancer deaths. It is estimated that 6,850 deaths (4,610 men and 2,240 women) from melanoma will occur this year. However, from 2013 to 2017, deaths from melanoma have decreased by almost 6% in adults older than 50 and by 7% in those younger than 50. \n\nMost people with melanoma are cured by their initial surgery. The 5-year survival rate tells you what percent of people live at least 5 years after the cancer is found. Percent means how many out of 100. Among all people with melanoma of the skin, from the time of initial diagnosis, the 5-year survival is 92%.  \n\nOverall survival at 5 years depends on the thickness of the primary melanoma, whether the lymph nodes are involved, and whether there is spread of melanoma to distant sites. Lymph nodes are small, bean-shaped organs that help fight infection. For people with \"thin melanoma,\" defined as being less than 1 millimeter in maximal thickness, that has not spread to lymph nodes or other distant sites, the 5-year survival is 99%.  \n\nHowever, for people with thicker melanoma, the 5-year survival may be as low as 80%. Survival rates at 5 years for people with melanoma that has spread to the nearby lymph nodes is 65%. However, this number is different for every patient and depends on the number of lymph nodes involved, the amount of tumor in the involved lymph node(s), and the features of the primary melanoma (such as thickness and whether ulceration is present or absent).  \n\nIf melanoma has spread to other, distant parts of the body, the survival rate is lower, about 25%. However, survival varies depending on a number of factors. These factors are explained in detail in the\u202fDiagnosis\u202fand\u202fStages\u202fsections. ","0de3df39":"## Visualize the skin cancer at torso...","d8bf490c":"# Apply some Image Processing...\n\nIn computer science, digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing."}}