{"cell_type":{"c9c55ca6":"code","2684de66":"code","6bd1d33b":"code","572de76c":"code","cc656297":"code","77415c3d":"code","1cd654f4":"code","f0d06a28":"code","668fddb4":"code","d8cf8ea9":"code","89a00587":"code","467a04ab":"code","798a76d7":"markdown","82f54e0c":"markdown","c39576fb":"markdown","2a397ccb":"markdown","34f8bec6":"markdown","c4db468e":"markdown","b84e92c9":"markdown","adfc8f24":"markdown","10f579f6":"markdown","600d2432":"markdown","cbe4a4a4":"markdown","8cec9cbc":"markdown","861af145":"markdown","c3b970aa":"markdown","ac3c6d73":"markdown","9e20c3e1":"markdown","3ed18296":"markdown","02d46e52":"markdown","47af6e03":"markdown","b6db0221":"markdown","fd49f858":"markdown","53c72a2a":"markdown","099d1e24":"markdown","8b520c5a":"markdown","caf5e17e":"markdown","411a754f":"markdown","1c364c79":"markdown","6d18bfc1":"markdown","58c96fe7":"markdown","3cb9b1cc":"markdown"},"source":{"c9c55ca6":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification, make_circles\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense","2684de66":"def create_data():\n  x_1 = np.random.randn(100,2) * 0.1 + 0.5\n  x_2 = np.random.randn(100,2) * 0.1 + np.array([0.5,-0.5])\n  x_4 = np.random.randn(100,2) * 0.1 + np.array([-0.5,0.5])\n  x_3 = np.random.randn(100,2) * 0.1 - 0.5\n  x = np.concatenate((x_1, x_2, x_3, x_4))\n  \n  y_1 = np.ones(100)\n  y_2 = np.ones(100) * 0\n  y_4 = np.ones(100) * 0\n  y_3 = np.ones(100)\n  y = np.concatenate((y_1,y_2,y_3, y_4))\n  y = y.reshape((-1,1))\n  \n  return x, y\n\ndef plot_data(x, y):\n\n  plt.plot(x[y[:,0] == 1,0], x[y[:,0]==1,1], 'bx')\n  plt.plot(x[y[:,0]==0,0], x[y[:,0]==0,1], 'ro')\n\ndef plot_line(c_1, c_2, c):\n  # c_1*x + c_2*y + c = 0 \n\n  plt.ylim(-1.2,1.2)\n  plt.xlim(-1.2,1.2)\n  x = np.linspace(-2,2,100)\n  y = (-c_1*x-c)\/c_2\n  \n  plt.plot(x, y)\n\ndef plot_lines(W, b):\n  for col, c in zip(W.T, b):\n    plot_line(col[0], col[1], c)\n    \nx_train, y_train = create_data()\nplot_data(x_train,y_train)","6bd1d33b":"import tensorflow as tf\n#print(tf.__version__)\n\ntf.keras.backend.clear_session()\n\ntf.random.set_seed(10)\ninputs = tf.keras.Input(shape=(2,))\nx = tf.keras.layers.Dense(2, activation=tf.nn.tanh)(inputs)\noutputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nplot_data(x_train ,y_train)\nb = model.weights[1].numpy()\nw = model.weights[0].numpy()\nplot_lines(w,b)","572de76c":"plot_data(x_train ,y_train)\nb = model.weights[1].numpy().reshape((1,-1))\nw = model.weights[0].numpy()\nplot_line(w[0,0], w[1,0], b[0,0])    \nplot_line(w[0,1], w[1,1], b[0,1]) ","cc656297":"model.fit(x_train, y_train, batch_size = 16, epochs=100)","77415c3d":"plot_data(x_train ,y_train)\nb = model.weights[1].numpy()\nw = model.weights[0].numpy()\nplot_lines(w,b)","1cd654f4":"activations = model.get_layer(name='dense')(x_train)\nplot_data(activations.numpy(), y_train)\nb = model.weights[3].numpy().reshape((1,-1))\nw = model.weights[2].numpy()\nplot_line(w[0,0], w[1,0], b[0,0])  \n#plot_line(w[0,1], w[1,1], b[0,1])  ","f0d06a28":"X, y = make_circles(n_samples=400, factor=0.3, noise=.05)\n\nreds = y == 0\nblues = y == 1\n\nplt.scatter(X[reds, 0], X[reds, 1], c='red', s=20, edgecolor='k')\nplt.scatter(X[blues, 0], X[blues, 1], c='blue', s=20, edgecolor='k')","668fddb4":"model2 = Sequential()\nmodel2.add(Dense(3,input_dim=2, activation='sigmoid'))\nmodel2.add(Dense(1, activation='sigmoid'))\n\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel2.fit(X, y, epochs=1000,)","d8cf8ea9":"w1 = []\nfor i in model2.layers:\n    w1.append(i.get_weights())","89a00587":"#first hidden layer\nwih11 = w1[0][0][0][0]\nwih12 = w1[0][0][0][1]\nwih13 = w1[0][0][0][2]\n\nwih21 = w1[0][0][1][0]\nwih22 = w1[0][0][1][1]\nwih23 = w1[0][0][1][2]\n\nbh1 = w1[0][1][0]\nbh2 = w1[0][1][1]\nbh3 = w1[0][1][2]\n\n#output layer\nwho1 = w1[1][0][0][0]\nwho2 = w1[1][0][1][0]\nwho3 = w1[1][0][2][0]\n\nbho0 = w1[1][1][0]","467a04ab":"x = np.arange(-1,1.5)\n\nplt.plot(x, ((-x*wih11)-bh1)\/wih21, c='y')\nplt.plot(x, ((-x*wih12)-bh2)\/wih22, c='g')\nplt.plot(x, ((-x*wih13)-bh3)\/wih23, c='k')\n\nplt.scatter(X[reds, 0], X[reds, 1], c='red', s=20, edgecolor='k')\nplt.scatter(X[blues, 0], X[blues, 1], c='blue', s=20, edgecolor='k')\n","798a76d7":"# Sigmoid\n\n*It squashes a vector in the range (0, 1). It is applied independently to each element of s(si). It\u2019s also called logistic function.*\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSenBHOyWGmTcj6EbuGAHA0LAMyJFSsAWdQzA&usqp=CAU\"\/>","82f54e0c":"# Non-Linearity: Activation Function\n\n* If the dataset is non-linear separable to make it linear separable , we need to  plot data in N+1 higher dimension to make linearly separable\n* In svm the kernel exactly do this job to make the non-linear separable dataset into linear separable by increasing the N+1 dimension.\n\n<img src=\"https:\/\/miro.medium.com\/max\/630\/1*9tuW9qfFfu9AxW2BGo40TQ.png\"\/>","c39576fb":"**Visualization after passing in Activation Function**","2a397ccb":"# References:\n\n1. https:\/\/medium.com\/@vigneshgig\/why-activation-function-is-used-in-neural-network-fb024b4e4ab3\n1. https:\/\/towardsdatascience.com\/visualizing-the-non-linearity-of-neural-networks-c55b2a14ad7a","34f8bec6":"# Visualization of Working of Activation Function","c4db468e":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*Vriy8ObvBqeqgMW3.gif\"\/>","b84e92c9":"# Relu\n*ReLU stands for a rectified linear unit. It gives an output x if x is positive and 0 otherwise*","adfc8f24":"# Activation Function\n\nHere I used one hidden state and 3 neuron in hidden state\n\n**Diagram: **\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*nkYC04qiCP_EBbe2REp9gg.jpeg\"\/>\n\n**Mathematical Equation: **\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*axvfrp8fDA4tWCy6lhdT1g.jpeg\"\/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*W93IqVXzobq6lu4Ai2EmlA.jpeg\"\/>\n\n**Instead of we solve this equation(o) separatly by split it,So that it is easy to understand How a activation function work in a neural network.**\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*iUwu4MfHOhX-GN_P-YlK3g.jpeg\"\/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*-CM-g6DyjoQwsc7KNrmymg.jpeg\"\/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*BMCu6ENbFC5DxMHWUSQJtA.jpeg\"\/>","10f579f6":"# Types of Activation Function    \n\n1. Sigmoid\n1. Tanh\n1. Relu\n1. Softmax\n\nThere are other activation function also, we will discuss above","600d2432":"**Training Model and Visaulizing Before passing to activation Function**","cbe4a4a4":"# Tanh\n\n*It squashes a vector in the range (0, 1). It is applied independently to each element of s(si). It\u2019s also called logistic function.*\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/freeze\/max\/1000\/1*SBcMLQ2rP77M9uuXYtX0hA.png?q=20\"\/>","8cec9cbc":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*xfhADh-178InHa5a.png\"\/>","861af145":"# Example 1: More Simpler Data","c3b970aa":"**Now we will train a neural network on this data and visualize the features learned by the network.**","ac3c6d73":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*mKs6LUS8LCKZtHT6.png\"\/>","9e20c3e1":"**There are two classes in the data. In the above plot, blue points correspond to the positive class (label 1) and the red points correspond to the negative class (label 0). As you can see from the plot, this data is not linearly separable. In a 2-d input space, this means there is no line that separates the two classes.**","3ed18296":"# Equations according to our Implementation\n\n![image.png](attachment:image.png)","02d46e52":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*CUPBA3fs9IFYZwBJqOgGBA.gif\"\/>","47af6e03":"<img src=\"data:image\/png;base64,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\"\/>","b6db0221":"# Softmax\n\n*Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.*","fd49f858":"# Activation Function","53c72a2a":"# With Activation Function(Sigmoid)","099d1e24":"**Conclusion:** Neural networks learn a new representation of the data which makes it easy to classify with respect to this new representation.","8b520c5a":"# Example 2: Circular data","caf5e17e":". Let\u2019s denote the weights and the biases of the hidden layer as follows:\n\n<img src=\"https:\/\/miro.medium.com\/max\/742\/1*Z72QQq-t_h3153HELQI0Eg.png\"\/>\n\n. The initial values of the weights and biases define two lines in the (x\u2081, x\u2082)-space,\n\n<img src=\"https:\/\/miro.medium.com\/max\/256\/1*L-L-g9vw22blWP5E-5ImsA.png\"\/>","411a754f":"* Now let\u2019s apply the non-linear activation function tanh and visualize the data in the new feature space (a\u2081, a\u2082). The activations of the hidden layer are computed as follows,\n\n<img src=\"https:\/\/miro.medium.com\/max\/666\/1*Kihf42f9t0HT3dJmlRZNtQ.png\"\/>\n\n* For each data point the arguments of the tanh are determined by the position of the data point in relation to the above lines. We can think of a\u2081 and a\u2082 as the new features and (a\u2081,a\u2082)-space as the new feature space. The weights and the bias of the output layer define a line in this new feature space given by the following equation\n\n<img src=\"https:\/\/miro.medium.com\/max\/227\/1*6ed4_7zRsvhceIWf2bV4gA.png\"\/>","1c364c79":"* In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.\n* A ctivation functions are crucial basic components of artificial neural networks (ANN), since they introduce non-linear characteristics to the network. \n* This allows the ANN to learn complicated, non-linear mappings between inputs and response variables.\n","6d18bfc1":"**In this notebook, we will be looking at a toy dataset and a simple neural network to demonstrate the advantage of the neural networks over linear classifiers.**","58c96fe7":"**The following utility functions are for creating random data and plotting.**","3cb9b1cc":"**Visualizing  Data**"}}