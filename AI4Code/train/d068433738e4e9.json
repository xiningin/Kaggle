{"cell_type":{"58aeb367":"code","f35ab65b":"code","52210a47":"code","8716aaf9":"code","00831d4a":"code","63a1540c":"code","f64b04ac":"code","a648ca17":"code","4afa3a37":"code","a5b7c565":"code","4776d96d":"code","8b26a320":"code","71289890":"code","73f2dad0":"code","b5d147fe":"code","64a7a6e3":"code","09da960e":"code","62258df6":"markdown","6541aabf":"markdown","e57fd60d":"markdown"},"source":{"58aeb367":"import pandas as pd\nimport numpy as np\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold , StratifiedKFold\nimport lightgbm","f35ab65b":"path = '..\/input\/tabular-playground-series-mar-2021\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub = pd.read_csv(path + 'sample_submission.csv')","52210a47":"cat = [col for col in train.columns if 'cat' in col]\ncont = [col for col in test.columns if 'cont' in col]\nall_features = cat + cont","8716aaf9":"[x for x in range(19)]","00831d4a":"all_df = pd.concat([train , test]).reset_index(drop = True)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor col in cat:\n    all_df[col] = le.fit_transform(all_df[col])\n\ntrain = all_df[:train.shape[0]]\ntest = all_df[train.shape[0]:]","63a1540c":"data = train[all_features]\ntarget = train['target']","f64b04ac":"def objective(trial , data = data , target = target):\n    train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n            test_size = 0.028059109276941666 , random_state = 42)\n    \n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 10),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-5 , 10),\n        'num_leaves' : trial.suggest_int('num_leaves' , 11 , 300),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.1),\n        'max_depth' : trial.suggest_int('max_depth' , 5 , 20),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n        'min_child_samples' : trial.suggest_int('min_child_samples' , 1 , 100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree' , 1e-5 , 1),\n        'random_state' : trial.suggest_categorical('random_state' , [0,42,2021,555]),\n        'metric' : 'auc',\n        'device_type' : 'gpu',\n    }\n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , early_stopping_rounds = 200 , \\\n             verbose = False)\n    preds = model.predict_proba(test_x)[:,1]\n    auc = roc_auc_score(test_y , preds)\n    return auc","a648ca17":"# study = optuna.create_study(direction = 'maximize' , study_name = 'lgbm')\n# study.optimize(objective , n_trials = 50)\n# print('numbers of the finished trials:' , len(study.trials))\n# print('the best params:' , study.best_trial.params)\n# print('the best value:' , study.best_value)","4afa3a37":"# import plotly\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n","a5b7c565":"# optuna.visualization.plot_optimization_history(study)","4776d96d":"# optuna.visualization.plot_param_importances(study)","8b26a320":"#parameters source:https:\/\/www.kaggle.com\/vitnam\/mar-2021-single-lgbm\nparams = {'reg_alpha': 4.203457823159052, 'reg_lambda': 6.34173530304477, 'num_leaves': 148,\n 'min_child_samples': 55, 'max_depth': 16, 'learning_rate': 0.01, 'colsample_bytree': 0.22290988791359692,\n 'n_estimators': 2703, 'cat_smooth': 37, 'cat_l2': 10, 'min_data_per_group': 97, 'device': 'gpu',\n 'random_state': 26, 'cat_feature': [0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10,  11,  12,  13,  14,\n  15,  16,  17,  18], 'n_jobs': -1, 'boosting_type': 'gbdt', 'metric': 'AUC'}","71289890":"#The score for the best parameter I got is 0.89211\n#the parameters is below\n# params = {\n#     'reg_alpha': 0.014335154764390193, 'reg_lambda': 0.008054411322239597, 'num_leaves': 214, \\\n#     'learning_rate': 0.007986033003932509, 'max_depth': 14, 'n_estimators': 5581, \\\n#     'min_child_samples': 94, 'min_child_weight': 4.579485567290539e-05, \\\n#     'subsample': 0.3380435315962088, 'colsample_bytree': 0.16829994859168315, 'random_state': 555\n# }\n# params['metric'] = 'auc'\n# params['device'] = 'gpu'\n","73f2dad0":"preds = np.zeros(test.shape[0])\noof_preds = np.zeros(train.shape[0])\nkf = StratifiedKFold(n_splits = 20 , random_state = 0 , shuffle = True)\nroc = []\nn = 0\nfor trn_idx , val_idx in kf.split(data , target):\n    train_x = data.iloc[trn_idx]\n    train_y = target.iloc[trn_idx]\n    val_x = data.iloc[val_idx]\n    val_y = target.iloc[val_idx]\n    \n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(train_x , train_y , eval_set = [(val_x , val_y)] , early_stopping_rounds = 200 , \\\n             verbose = False)\n    preds += model.predict_proba(test[all_features])[:,1]\/kf.n_splits\n    oof_preds += model.predict_proba(data[all_features])[:,1]\/kf.n_splits\n    roc.append(roc_auc_score(val_y , model.predict_proba(val_x)[:,1]))\n    print(n+1 , roc[n])\n    n+=1","b5d147fe":"sub['target'] = preds\nsub.to_csv('slgbmsubmission.csv' , index = False)","64a7a6e3":"output = pd.DataFrame({'id':train['id'] , 'target':oof_preds})\noutput.to_csv('slgbmoof_predictions.csv' , index = False)","09da960e":"sub","62258df6":"# optuna","6541aabf":"parameters copy from   https:\/\/www.kaggle.com\/vitnam\/mar-2021-single-lgbm","e57fd60d":"# Label Encoder"}}