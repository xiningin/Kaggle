{"cell_type":{"be3b5fd8":"code","5171f841":"code","d2148042":"code","640a4e63":"code","9b3c0341":"code","b309336c":"code","8a3e9e0d":"code","50b8e334":"code","630baaab":"code","c0723372":"code","a90c1209":"code","f50bbcf2":"code","47b2deb0":"code","dd8aa6ed":"code","53cd606a":"code","65ce9073":"code","7195bc5d":"code","8684551a":"code","218e6d65":"code","54b391d8":"code","64933e00":"code","5f782a91":"code","d343926a":"code","6a617bc6":"code","7194ec74":"code","f16e0698":"code","09e25b39":"code","3701a294":"code","0bc3c0a3":"code","11785a83":"code","502c792c":"code","97aa6e95":"code","f83ab919":"code","b5e62e76":"code","bf586491":"code","11acd3ef":"code","fd28b7c4":"code","88e5350a":"code","ab62a5b1":"code","e92ff888":"code","a7da0bc0":"code","863b9c7c":"code","294501f3":"code","386f546b":"code","d6083fd6":"code","ae6529f5":"code","25495fdb":"code","779e4841":"code","f4b2838e":"code","2cc5797a":"code","6331fc1e":"code","ada2bd10":"code","87d99c39":"markdown","215c5c5b":"markdown","49a39806":"markdown","12770932":"markdown","bbe183f2":"markdown","b4e91599":"markdown"},"source":{"be3b5fd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5171f841":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d2148042":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","640a4e63":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","9b3c0341":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', data=train_data)","b309336c":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n","8a3e9e0d":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n","50b8e334":"#visualizing % of Male and female survived\nsns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Sex', data=train_data)","630baaab":"#cheking for missing values\ntrain_data.isnull().sum()","c0723372":"#cheking for missing values\ntest_data.isnull().sum()","a90c1209":"sns.distplot(train_data['Age'].dropna(), kde=False, color='blue', bins=40)","f50bbcf2":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Pclass', data=train_data)","47b2deb0":"#visualization for relationship between Age and Pclass\nplt.figure(figsize=(15,6))\nsns.boxplot(x='Pclass', y='Age', data = train_data, palette='winter')","dd8aa6ed":"#since the age is corealted with Pclass we have to add mean or average for the age\n\ndef  age_impute(cols):\n  Age = cols[0]\n  Pclass=cols[1]\n  if pd.isnull(Age):\n    if Pclass==1:\n      return 37\n    elif Pclass==2:\n      return 29\n    else:\n      return 24\n  else:\n    return Age","53cd606a":"#impute age\ntrain_data['Age'] = train_data[['Age', 'Pclass']].apply(age_impute, axis=1)\ntest_data['Age'] = train_data[['Age', 'Pclass']].apply(age_impute, axis=1)\n","65ce9073":"train_data.isnull().sum()","7195bc5d":"#droping cabin coloum in train and test set \ntrain_data.drop('Cabin',axis=1, inplace=True)\ntest_data.drop('Cabin',axis=1, inplace=True)\n","8684551a":"train_data['Embarked'].isnull().sum()","218e6d65":"train_data['Embarked']=train_data['Embarked'].replace(np.NaN, train_data['Embarked'].mode())","54b391d8":"train_data.isnull().sum()","64933e00":"test_data.isnull().sum()","5f782a91":"test_data['Fare'].mean()","d343926a":"test_data['Fare']=test_data['Fare'].replace(np.NaN, train_data['Fare'].mean())","6a617bc6":"test_data.isnull().sum()","7194ec74":"#caring categorical values -----Train set------\nsex = pd.get_dummies(train_data['Sex'],drop_first=True)\nembarked = pd.get_dummies(train_data['Embarked'],drop_first=True)","f16e0698":"print(sex.head())\nprint(embarked.head())","09e25b39":"#droping unwanted columns\ntrain_data.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace=True)","3701a294":"train_data.head()","0bc3c0a3":"#Concat\ntrain_data = pd.concat([train_data,sex,embarked],axis=1)\ntrain_data.head()","11785a83":"#caring categorical values -----Test set------\ngender = pd.get_dummies(test_data['Sex'],drop_first=True)\nembark = pd.get_dummies(test_data['Embarked'],drop_first=True)","502c792c":"print(gender.head())\nprint(embark.head())","97aa6e95":"#droping unwanted columns\ntest_data.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace=True)\ntest_data.head()","f83ab919":"#Concat\ntest_data = pd.concat([test_data,gender,embark],axis=1)\ntest_data.head()","b5e62e76":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_data.drop(['Survived','PassengerId'], axis=1), \n                                                    train_data['Survived'], test_size = 0.2, \n                                                    random_state = 0)","bf586491":"#model building\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=500, criterion='entropy')\nclassifier.fit(X_train,y_train)","11acd3ef":"predictions = classifier.predict(X_test)","fd28b7c4":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,predictions)\ncm","88e5350a":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, predictions)\naccuracy","ab62a5b1":"from xgboost import XGBClassifier\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train,y_train)","e92ff888":"xgb_predictions = xgb_classifier.predict(X_test)","a7da0bc0":"#confusion matrix\nxgb_cm = confusion_matrix(y_test, xgb_predictions)\nxgb_cm","863b9c7c":"#checking accuracy\nxgb_accuracy = accuracy_score(y_test,xgb_predictions)\nxgb_accuracy","294501f3":"from sklearn.ensemble import GradientBoostingClassifier\ngbc= GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","386f546b":"#prediciion\ngbc_predictions = gbc.predict(X_test)","d6083fd6":"#confusion_matrix\ngbc_cm = confusion_matrix(y_test, gbc_predictions)\ngbc_cm","ae6529f5":"gbc_accuracy = accuracy_score(y_test, gbc_predictions)\ngbc_accuracy","25495fdb":"from sklearn.model_selection import cross_val_score\ncrossval = cross_val_score(estimator = classifier , X= X_train ,y= y_train, cv = 10)\ncrossval.mean()","779e4841":"#checking accuracy of each model\nmodels={'MODEL':['RANDOM FOREST','XG BOOSTING','GRADIENT BOOSTING'],'ACCURACY':[accuracy,xgb_accuracy,crossval.mean()]}\nmodel_accuracy=pd.DataFrame(models)\nmodel_accuracy","f4b2838e":"train_data.head()","2cc5797a":"test_data.head()","6331fc1e":"passenger_id = test_data['PassengerId']\npredict_values = gbc.predict(test_data.drop('PassengerId', axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : passenger_id, 'Survived': predict_values })\noutput.to_csv('submission.csv', index=False)\n\n","ada2bd10":"predictions","87d99c39":"**Random Forest**","215c5c5b":"Train test split","49a39806":"**Submission(choosing GRADIENT BOOSTING\t)**","12770932":"**Data Cleaning**","bbe183f2":"**XGBoot**","b4e91599":"**Gradient Boosting Classifier**"}}