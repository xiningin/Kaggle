{"cell_type":{"f0e71f4c":"code","a57b1e0c":"code","50837b95":"code","074e6bbc":"code","236c0053":"code","2d678882":"code","57258e9c":"code","67d9f004":"code","c4c9f755":"code","21e95940":"code","f9d5cab2":"code","39466d74":"code","4017fd57":"code","90fe0d0e":"code","5e0cbf5e":"code","f60a8f3a":"code","4e447d07":"code","c8a0768e":"code","d1695d00":"code","e5a27a2f":"code","d0c98322":"code","c5a13c33":"code","77b25220":"code","e6802b0d":"code","798c1991":"code","24a7c663":"code","60e21bc7":"code","1fa5e54d":"code","05fe3da8":"code","66abde3f":"code","5f9c1f38":"code","62985c12":"code","b97600b5":"markdown","7d0bc982":"markdown","04151ac1":"markdown","dc6336d3":"markdown","3e368527":"markdown","ba46e163":"markdown","7c55ca95":"markdown","cb6e870e":"markdown","e7b3f558":"markdown","1882fe64":"markdown","c81dbbc0":"markdown","aa6bdff6":"markdown","3d3b259e":"markdown","b60ad418":"markdown","e6ed48d1":"markdown","ae8e61bf":"markdown","6df0abd5":"markdown","c9fec2c7":"markdown","8acf8f41":"markdown","cfb798fa":"markdown"},"source":{"f0e71f4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a57b1e0c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","50837b95":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n# train.head()\ntest.head()","074e6bbc":"train.info()","236c0053":"test.info()","2d678882":"train.isnull().sum()\/train.shape[0] * 100\n","57258e9c":"test.isnull().sum()\/test.shape[0] * 100","67d9f004":"train.columns\n","c4c9f755":"test.columns\n","21e95940":"ID_train=train['Id']\nID_test=test['Id']\ny=train['SalePrice']","f9d5cab2":"train.drop(columns=['Id','LotFrontage','PoolQC','SalePrice'], axis=1)\ntest=test.drop(columns=['Id','LotFrontage','PoolQC'], axis=1)\ntrain.head()\n# test.head()","39466d74":"cat_train=[col for col in train.columns if train[col].dtype=='object']\nnum_train=[col for col in train.columns if train[col].dtype!='object']\n# cat_train\nnum_train","4017fd57":"cat_test=[col for col in test.columns if test[col].dtype=='object']\nnum_test=[col for col in test.columns if test[col].dtype!='object']\n# cat_test\nnum_test","90fe0d0e":"con_train =[col for col in num_train if train[col].nunique()>25]\ndis_train =[col for col in num_train if train[col].nunique()<25]\nyea_train =[col for col in train.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n\n# con_train\n# dis_train\nyea_train","5e0cbf5e":"con_test =[col for col in num_test if test[col].nunique()>25]\ndis_test =[col for col in num_test if test[col].nunique()<25]\nyea_test =[col for col in test.columns if 'Yr' in col or 'Year' in  col or 'yr' in  col or 'YR' in  col]\n# con_test\n# dis_test\nyea_test","f60a8f3a":"# Filling NAN for missing values in numerical & categorical features.To be handled later by Imputer\nfor col in train.columns:\n    train[col] = train[col].apply(lambda x: 'NAN' if x=='NA' else x)\n\nfor col in test.columns:\n    test[col] = test[col].apply(lambda x: 'NAN' if x=='NA' else x)\n","4e447d07":"from sklearn.impute import SimpleImputer\nnsi = SimpleImputer(strategy='mean')  # For Numerical Features, will replace MISSING NUMERIC values with MEAN\ncsi = SimpleImputer(strategy='most_frequent')  # For Categorical Features, will replace MISSING CATEGORICAL values with MOST FREQUENT value\n\ntrain[cat_train] = csi.fit_transform(train[cat_train])\ntrain[con_train] = nsi.fit_transform(train[con_train])\ntrain[dis_train] = nsi.fit_transform(train[dis_train])\n\n# train.head()\ntest.head()","c8a0768e":"test[cat_test] = csi.fit_transform(test[cat_test])\ntest[con_test] = nsi.fit_transform(test[con_test])\ntest[dis_test] = nsi.fit_transform(test[dis_test])\n\n# train.head()\ntest.head()","d1695d00":"from datetime import date\ntrain[yea_train]=date.today().year - train[yea_train]\ntest[yea_test]=date.today().year - test[yea_test]\ntrain.head()\n# test.head()","e5a27a2f":"# train[con_train]=np.log(train[con_train])\n# test[con_test]= np.log(test[con_test])\n# train.head()\n# test.head()","d0c98322":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\ntrain[dis_train]= ss.fit_transform(train[dis_train])\ntest[dis_test]= ss.fit_transform(test[dis_test])\n\n# train.head()\ntest.head()","c5a13c33":"train1= pd.get_dummies(train, columns=cat_train, drop_first= True)\ntest1= pd.get_dummies(test, columns=cat_test, drop_first= True)\ntrain1.head()\n# test1.head()","77b25220":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)\n# train2.head()\ntest2.head()","e6802b0d":"train=train2.drop(cat_train,axis=1)\ntest=test2.drop(cat_test,axis=1)\n# train.head()\ntest.head()","798c1991":"train=train.dropna(axis=0,how='any') # I have taken all the necessary features thus dropping null values of unnecessary features\ntest=test.dropna(axis=0,how='any') \ntrain.head()\n# test.head()","24a7c663":"from scipy import stats\ntrain[(np.abs(stats.zscore(train)) < 3).all(axis=1)]\ntrain.head()","60e21bc7":"from scipy import stats\ntest[(np.abs(stats.zscore(test)) < 3).all(axis=1)]\ntest.head()","1fa5e54d":"y=train['SalePrice'].iloc[:,0]\n\nX=train.drop(['Id','SalePrice'],axis=1)\ny.head()","05fe3da8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\ny_test.head()","66abde3f":"from sklearn.ensemble import GradientBoostingRegressor\nreg=GradientBoostingRegressor()\nreg.fit(X_train,y_train)\n","5f9c1f38":"predict= reg.predict(X_test)\n# predict","62985c12":"from sklearn.metrics import r2_score\nr2_score(predict, y_test)","b97600b5":"# Apply Log Transform on Continuous Data only\n","7d0bc982":"# Finding the columns in each dataset","04151ac1":"# Imputing the missing values\n### Missing values are one of the most common problems you can encounter when you try to prepareyour data for machine learning. The reason for the missing values might be human errors,interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n","dc6336d3":"# Removing Outliers\n### If you have multiple columns in your dataframe and would like to remove all rows that have outliers in at least one column, the following expression would do that in one shot.","3e368527":"# Using the Trained Model to Predict\n","ba46e163":"# Standardizing the Discrete Values.\n### Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n","7c55ca95":"# Concatenating the Original Dataset & the One after creating Dummies(get_dummies()\n### Get_Dummies() method creates a new DF containing JUST the dummies, MOST People get wrong here)\n","cb6e870e":"# Handling Categorical Data using Get_Dummies()\n### Machine learning models require all input and output variables to be numeric.This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n","e7b3f558":"# Finding Numerical & Categorical Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n","1882fe64":"# Dropping some useless column. Also dropping some columns with more NULL values ","c81dbbc0":"# Finding the Data-type of each column","aa6bdff6":"## Using GBoost to fit the Data\n","3d3b259e":"# Some ways you may show Like by\n### Kaggle - Follow me on Kaggle\n### Twitter - https:\/\/twitter.com\/KumarPython\n### LinkedIn - https:\/\/www.linkedin.com\/in\/kumarpython\/","b60ad418":"# Doing the Train_Test_Split\n","e6ed48d1":"# Finding the Percent of null values in each columns\n","ae8e61bf":"# Scoring the Trained Model\n","6df0abd5":"# Splitting X & y\n","c9fec2c7":"# Dropping the columns already concatenated after Get_Dummies()\n","8acf8f41":"## Transforming Dates\n### If you transform the date column into the extracted columns, the information of them become disclosed and machine learning algorithms can easily understand them.\n","cfb798fa":"# Finding the following Features (to be treated seperately later)\n### This method is called List Comprehension-where a list is created satisfying some condition\n### * Continuous Features\n### * Discreet Features\n### *  Year Features\n"}}