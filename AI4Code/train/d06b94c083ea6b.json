{"cell_type":{"57104c9c":"code","a3787dc0":"code","b9393b29":"code","2765e833":"code","c2d6b6ae":"code","929d475c":"code","c8c1d8b2":"code","7533e51d":"code","bc8b209d":"code","1bdeec98":"code","f25465b6":"code","5e0904b6":"code","e8a432b6":"code","86bfe936":"code","d7f5d64f":"code","a9b87408":"code","5ac67c10":"code","f8859fb2":"code","4d752327":"code","e473214d":"code","14e98f1b":"code","57e205e5":"code","4716f206":"code","31d9bd8a":"code","c69bd140":"markdown","c8669c53":"markdown","1039b54f":"markdown","e1fa0ed5":"markdown","bbd1ad17":"markdown","e76725fd":"markdown","67fa372f":"markdown","30ca7b69":"markdown","01c25771":"markdown","41a9493a":"markdown","b2ed7303":"markdown","3bc455f4":"markdown","ab4dc91a":"markdown","ae77ee96":"markdown","682cd632":"markdown","91362b00":"markdown","817b8d08":"markdown","e954bb28":"markdown","e685e39c":"markdown","80aca16d":"markdown","3a2dcc6d":"markdown","48067579":"markdown","5863cd5f":"markdown","c975d01d":"markdown","acd7fb80":"markdown","563fefae":"markdown","c0b7d4d1":"markdown","4c925bdd":"markdown","e7bc5d06":"markdown","8672458c":"markdown"},"source":{"57104c9c":"# Importing necessary modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\npd.options.mode.chained_assignment = None","a3787dc0":"train = pd.read_csv('..\/input\/loan-data\/Loan_Training_data.csv')\ntest = pd.read_csv('..\/input\/loan-data\/Loan_Test_Data.csv')","b9393b29":"train.head()","2765e833":"train.Loan_Status.value_counts()","c2d6b6ae":"train.isnull().sum()","929d475c":"sns.barplot(train.Gender, train.LoanAmount, hue=train.Loan_Status)\nplt.legend(loc='upper right')\nplt.title('Loan Amount vs Gender - grouped based on Loan Status')","c8c1d8b2":"# Changing categorical names for the sake of easier understanding\ntrain.Married = train.Married.apply(lambda x: 'Married' if x == 'Yes' else 'Unmarried')\n\nsns.barplot(train.Gender, train.LoanAmount, hue=train.Married, hue_order = ['Married', 'Unmarried'])\nplt.legend(loc='upper left')\nplt.title('Loan Amount vs Gender - grouped based on Marital Status')","7533e51d":"sns.scatterplot(train.ApplicantIncome, train.LoanAmount, hue=train.Loan_Status)\nplt.xticks(rotation=45)","bc8b209d":"sns.countplot(train.Education, hue=train.Loan_Status)","1bdeec98":"sns.countplot(train.Property_Area, hue=train.Loan_Status)","f25465b6":"sns.pairplot(train, hue='Loan_Status', palette='Set2', diag_kind='kde')","5e0904b6":"def target_split(train): \n    train_mod = train[~train['LoanAmount'].isnull()] # Loan Amount has few null values but they should not be imputed\n    train_mod.drop('Loan_ID', axis = 1, inplace=True) # Dropping ID column as it is not relevant to the model\n\n    y = train_mod.Loan_Status\n    train_mod.drop('Loan_Status', axis = 1, inplace=True)\n\n    y = y.apply(lambda x: 1 if x == 'Y' else 0) # Changing categories to numerical values\n    \n    return train_mod, y","e8a432b6":"def impute(train):\n    cols = train.columns\n    nan_cols = []\n    for col in cols:\n        if(train[col].isnull().sum() > 0):\n            nan_cols.append(col)\n    # nan_cols contains the list of columns having null values\n    \n    argmax_in_nan = {}\n    for col in nan_cols:\n        argmax_in_nan[col] = None\n        argmax_in_nan[col] = train[col].value_counts().idxmax() # Getting the most frequent value in the column\n        \n        train[col].fillna(argmax_in_nan[col], inplace=True)\n            \n    return train","86bfe936":"def scaler(train):\n    num_cols = [col for col in train.select_dtypes(exclude='object').columns]\n    scaler = MinMaxScaler()\n    for col in num_cols:\n        if (col != 'Credit_History'): # Credit_History belongs to int64 datatype but it is a categorical value. So it should not be scaled.\n            train[col] = scaler.fit_transform(train[[col]])\n            \n    return train","d7f5d64f":"def cat_enc(train):\n    cat_cols = [col for col in train.select_dtypes(include='object').columns]\n    \n    for col in cat_cols:\n        dummies = pd.get_dummies(train[col], prefix=col)\n        train = pd.concat([train,dummies], axis=1)\n        train.drop([col],axis = 1 , inplace=True)\n    \n    return train","a9b87408":"def preprocess(train):\n    train, y = target_split(train)\n    train = impute(train)\n    train = scaler(train)\n    train = cat_enc(train)\n    \n    return train, y","5ac67c10":"train_mod, y = preprocess(train)","f8859fb2":"train_mod.head()","4d752327":"np.random.seed(0)\n\nmodel = Sequential()\n\nmodel.add(Dense(48, kernel_initializer='normal',input_dim = train_mod.shape[1], activation='relu'))\nmodel.add(Dense(96, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(96, kernel_initializer='normal',activation='relu'))\n\nmodel.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","e473214d":"checkpoint_name = 'Weights-{epoch:02d}--{val_loss:.2f}.hdf5'\ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","14e98f1b":"model.fit(train_mod, y, epochs=50, batch_size=37, validation_split = 0.2, callbacks=callbacks_list)","57e205e5":"!ls .","4716f206":"import os\n\nbest_weight_file = str()\nval_loss = 100\nfor filename in os.listdir():\n    if(filename.startswith('W')):\n        name, ext = os.path.splitext(filename)\n        if(int(name[-2:]) < val_loss):\n            val_loss = int(name[-2:])\n            best_weight_file = filename\n            \nprint(best_weight_file)","31d9bd8a":"model.load_weights(best_weight_file)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nscores = model.evaluate(train_mod, y, verbose=0)\nprint(\"Accuracy of model: %.2f%%\" % (scores[1]*100))","c69bd140":"Next, for a full scale visualization, I am employing the pairplot from seaborn.","c8669c53":"# **Data Visualization**","1039b54f":"# Conclusion\n\nIn this kernel, I chose a random problem and built a decent classifier using a shallow neural network. I was able to tackle the problems I faced. If you have any questions or suggestions to improve the kernel, you are welcome to say it.","e1fa0ed5":"The overall model gave 82.09% accuracy on training data, which is pretty decent for a shallow neural network.","bbd1ad17":"Not so surprisingly, people with graduate degree have a higher chance of getting a loan than the applicant's without one.","e76725fd":"There are some interesting things going on here:\n1. As already stated, Applicant's Income does not determine the loan sanction status (plot - row 1, col 1).\n2. But, applicants having credit history have a higher chance of getting a loan (plot - row 5 col 5).\n3. Applicants with higher income **and** credit history have a higher chance of getting a loan (plot row 1 col 5).\n4. Loan Amount KDE (plot - row 3 col 3) peaks around 150 for both sanctioned and rejected loans, with sanctioned overpowering\n   rejected. Beyond the peak, more loan amount invites more rejected applications than sanctioned ones.","67fa372f":"Next step is to fill the null values.","30ca7b69":"# **Predicting Loan Sanction Status using Neural Network**\n\nI am stepping into Deep Learning for the first time. I thought of building a classifier using Keras. In order to go through the full ML experience, I chose an unpopular dataset from Kaggle Datasets. Handled data viz and preprocessing, then built a Sequential NN model. It may seem unnecessary to use Neural Network for numerical prediction, but I would like to see if Deep Learning can perform well in these prediction tasks.","01c25771":"Compiling all the above steps, this function renders a ML-ready dataset.","41a9493a":"Now, the preprocessing part.","b2ed7303":"I'm using keras module to build the neural network. I'm going for a wider, shallow network rather than a narrow deeper one. Adam is currently the most popular optimizer. I'm printing the accuracy score of the model at every epoch.","3bc455f4":"So, in this part, I had a problem. Each time I run this kernel, the name of weight file with best weights change. While committing, the filename changes and throws an error in final kernel. Hence, I could not specify one particular filename when loading weights. So, I wrote a piece of code where I check for the filename with lowest validation loss. I used that file in load_weights function.","ab4dc91a":"There is no discernible correlation between Applicant's Income and the Loan Amount. But the scatter plot shows that most of the applicants are people with low-income and the chance of getting a loan does not depend on the applicant's income.","ae77ee96":"Bottomline from these plots: You have a higher chance of getting a loan if you are a **Married Male Graduate having a Property in Semi-Urban area**.","682cd632":"After filling null values, comes the normalization part. MinMaxScaler maps the values to values in range [0,1]. ","91362b00":"But when it comes to marital status, both men and women can get equal loans when they are married.","817b8d08":"Here, I am using a checkpoint list to store the weights of the model when it performs the best. By this method, I can reuse the best weights for the test data.","e954bb28":"So, our training data has shape of (614, 13). The last column, Loan_Status, will be our target label.\n\nIt has two classes: 'Y' for sanctioned loans & 'N' for rejected loan applications\n\nMost of the other columns are self-explanatory. 'Loan_Amount_Term' is the number of months within which the loan should be repaid; 'Credit_History' has values 1.0 if the applicant has credit history and 0.0 if they do not; 'Property_Area tells' us the category of location of the collateral property which is provided as security for the loan.","e685e39c":"Now, to handle the categorical variables, I am using get_dummies function from pandas. This will create a column for each categorical variable under each feature","80aca16d":"Step 1 is to split the dataset into features and target.","3a2dcc6d":"# **Data Preprocessing**","48067579":"The value counts of the label shows that we have an imbalanced dataset. Most ML models do not work well with imbalanced datasets. To tackle this, we can either collect more data or add synthetic data using popular tools like SMOTE or sklearn.utils.resample. \n","5863cd5f":"The ls command shows the checkpoints at which the model has the highest accuracy. The files contain the weights for that particular epoch (the two digit number after *Weights-*). ","c975d01d":"Surprisingly, property in semi-urban areas render a higher chance of loan sanction than others. This might be due to the fact that semi-urban areas call for more reconstructional capabilities than the urban areas. A semi-urban area is a lot easier to remodel and build than an urban area. And as expected, rural areas score the least here.","acd7fb80":"We have a lot of null values in various columns. Since our training dataset is small, dropping rows is not a viable solution. So, I am gonna replace the null values of a feature with the most frequent value of that feature. We will see that in the preprocessing part.","563fefae":"# **Model development**","c0b7d4d1":"The filename stores the epoch number along with the validation loss at that epoch (it is useful in the coming part). I specified the model to use log_loss\nAfter training the model, the weights file with the least validation loss will be selected for predicting.","4c925bdd":"Dataset source: Loan_data (https:\/\/www.kaggle.com\/pallavi31\/loan-data)\n\nDataset owner: Pallavi Vibhute (https:\/\/www.kaggle.com\/pallavi31)\n","e7bc5d06":"After dropping rows with null LoanAmount and preprocessing, the training data has shape of (592, 20)","8672458c":"This plot tells us that male applicants are lent a larger amount compared to female applicants. But the sanction-to-rejection ratio is higher for female applicants than to that of male applicants."}}