{"cell_type":{"f11e231f":"code","7edec1c6":"code","50a87bd5":"code","7f394c91":"code","2c769229":"code","c0b05b68":"code","413ee9ff":"code","170acff6":"code","7005e230":"code","ff00915a":"code","94b07d8a":"code","1be18b16":"code","693e1601":"code","3ce58561":"code","8b4dbdf3":"code","b34832eb":"code","50c17f91":"code","53dd8ae5":"code","9c24a80d":"code","be6914bc":"code","e5c2741d":"code","bea92e0d":"code","10cefbf3":"code","118c3d34":"code","5694ec22":"code","a3dd86b3":"code","edbec948":"code","b8861c75":"code","d0ae8e47":"markdown","3a4df45b":"markdown","93899c6d":"markdown","2e6d26cf":"markdown","3557f6e0":"markdown","dd4d7b42":"markdown","3459db0d":"markdown","4b619de5":"markdown","bfce8f80":"markdown","b0bc9621":"markdown","f7bedd14":"markdown","6bd248de":"markdown","2b4eb2a0":"markdown","663ce38f":"markdown","5ed260cb":"markdown","6c77c3bd":"markdown","41c39a26":"markdown","bbeba40b":"markdown","678cd7c8":"markdown","14ddb61f":"markdown","4601a425":"markdown","4e8d9a5b":"markdown","92b1f372":"markdown","1378ec29":"markdown","91e6af90":"markdown"},"source":{"f11e231f":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","7edec1c6":"class LogisticsRegression:\n    \n    def sigmoid(self, x):\n    \n        # shape(samples, 1)\n        z = ((np.dot(x, self.weight)) + self.bias)\n\n        # shape(samples, 1)\n        return (1 \/ (1 + np.exp(-z)))\n        \n    def forward(self, x):\n        \n        # shape(samples, 1)\n        return self.sigmoid(x)\n    \n    def binary_crossEntropy(self, y, y_hat):\n        \n        # shape(samples, 1)\n        return ((-1) * y * (np.log(y_hat))) - ((1 - y) * (np.log(1 - y_hat)))\n        \n    def cost(self, y, y_hat):\n        \n        # scalar\n        return np.mean(self.binary_crossEntropy(y, y_hat))\n        \n    def train(self, x, y, alpha, epoch, random_state=-1):\n        \n        # x : shape(#samples, #features)\n        # y : shape(#samples, 1)\n        \n        m, n = x.shape[0], x.shape[1]\n        \n        if random_state != -1:\n            np.random.seed(random_state)\n        \n        # shape(#features, 1)\n        self.weight = np.random.randn(n,1)\n\n        # shape(1,1)\n        self.bias = np.zeros((1,1))\n        \n        self.epoch = epoch\n        \n        self.cost_list = []\n        \n        for i in range(self.epoch):\n            \n            # shape(#samples, 1)\n            y_hat = self.forward(x)\n    \n            # scalar\n            loss = self.cost(y, y_hat)\n\n            self.cost_list.append(loss)\n\n            # Gradient\n            # dL_dw : dLoss\/dweight (#features, 1)\n            dL_dw = (np.dot(x.T, (y_hat - y)))\/m\n\n            # dL_db : dLoss\/dbias (1, 1)\n            dL_db = np.sum((y_hat - y)\/m)\n\n            # shape(#features, 1)\n            self.weight = self.weight - (alpha * dL_dw)\n\n            # shape(1, 1)\n            self.bias = self.bias - (alpha * dL_db)\n            \n    def plot_convergence(self):\n        \n        plt.plot([i for i in range(self.epoch)], self.cost_list)\n        plt.xlabel('Epochs'); plt.ylabel('Binary Cross Entropy')\n        \n    def predict(self, x_test):\n        \n        # shape(samples, 1)\n        y_hat = self.forward(x_test)\n        return np.where(y_hat>=0.5, 1, 0)","50a87bd5":"def randomDataset(m, n, random_state=-1):\n    \n    if random_state != -1:\n        np.random.seed(random_state)\n        \n    x = np.random.randn(m, n)\n    slope = np.random.randn(n, 1)\n    epsilon = np.random.randn(1, 1)\n    y = (1 \/ (1 + np.exp(-(np.dot(x, slope) + epsilon))))\n    y = np.where(y>=0.5, 1, 0)\n    print(slope, epsilon)\n    \n    return x, y","7f394c91":"def train_test_split(x, y, size=0.2, random_state=-1):\n    \n    if random_state != -1:\n        np.random.seed(random_state)\n        \n    x_val = x[:int(len(x)*size)]\n    y_val = y[:int(len(x)*size)]\n    x_train = x[int(len(x)*size):]\n    y_train = y[int(len(x)*size):]\n    \n    return x_train, y_train, x_val, y_val","2c769229":"x, y = randomDataset(1000, 2, random_state=0)","c0b05b68":"x_train, y_train, x_val, y_val = train_test_split(x, y, size=0.2, random_state=0)","413ee9ff":"l = LogisticsRegression()\nlearning_rate = 0.08\nepoch = 250\nl.train(x_train, y_train, learning_rate, epoch, random_state=0)\nl.plot_convergence()\nl.weight, l.bias","170acff6":"y_hat = l.predict(x_val)","7005e230":"confusion_matrix(y_val, y_hat)","ff00915a":"df = pd.read_csv('..\/input\/iris-dataset\/Iris_binary.csv')\ndf.head(2)","94b07d8a":"df.Species.unique()","1be18b16":"df.Species.replace(('Iris-setosa', 'Iris-versicolor'), (0, 1), inplace=True)","693e1601":"df = df.sample(frac=1, random_state=0)","3ce58561":"X, Y = df.drop(['Species'], axis=1).values, df.Species.values\nY = Y.reshape(-1, 1)","8b4dbdf3":"X_train, Y_train, X_val, Y_val = train_test_split(X, Y, size=0.2, random_state=0)","b34832eb":"l = LogisticsRegression()\nlearning_rate = 0.01\nepoch = 100\nl.train(X_train, Y_train, learning_rate, epoch, random_state=0)\nl.plot_convergence()","50c17f91":"Y_hat = l.predict(X_val)\n\nconfusion_matrix(Y_val, Y_hat)","53dd8ae5":"print(classification_report(Y_val, Y_hat))","9c24a80d":"class SoftmaxRegression:\n    \n    def softmax(self, x):\n        \n        # shape(#samples, #classes)\n        z = ((np.dot(x, self.weight)) + self.bias)\n        \n        # shape(#samples, #classes)\n        return (np.exp(z.T) \/ np.sum(np.exp(z), axis=1)).T\n        \n    def forward(self, x):\n        \n        # shape(#samples, #classes)\n        return self.softmax(x)\n    \n    def crossEntropy(self, y, y_hat):\n        \n        # shape(#samples, )\n        return - np.sum(np.log(y_hat) * (y), axis=1)\n    \n    def cost(self, y, y_hat):\n        \n        # scalar\n        return np.mean(self.crossEntropy(y, y_hat))\n        \n    def train(self, x, y, alpha, epoch, random_state=-1):\n        \n        # x : shape(#samples, #features)\n        # y : shape(#samples, #classes)\n        \n        m, n, c = x.shape[0], x.shape[1], y.shape[1]\n        \n        if random_state != -1:\n            np.random.seed(random_state)\n        \n        # shape(#features, #classes)\n        self.weight = np.random.randn(n,c)\n\n        # shape(1, #classes)\n        self.bias = np.zeros((1,c))\n        \n        self.epoch = epoch\n        \n        self.cost_list = []\n        \n        for i in range(self.epoch):\n            \n            # shape(#samples, #classes)\n            y_hat = self.forward(x)\n    \n            # scalar\n            loss = self.cost(y, y_hat)\n\n            self.cost_list.append(loss)\n\n            # Gradient\n            # dL_dw : dLoss\/dweight (#features, #classes)\n            dL_dw = (np.dot(x.T, (y_hat - y)))\/m\n\n            # dL_db : dLoss\/dbias (1, #classes)\n            dL_db = np.sum((y_hat - y)\/m)\n\n            # shape(#features, #classes)\n            self.weight = self.weight - (alpha * dL_dw)\n\n            # shape(1, #classes)\n            self.bias = self.bias - (alpha * dL_db)\n            \n    def plot_convergence(self):\n        \n        plt.plot([i for i in range(self.epoch)], self.cost_list)\n        plt.xlabel('Epochs'); plt.ylabel('Cross Entropy')\n        \n    def predict(self, x_test):\n        \n        # shape(#samples, #classes)\n        y_hat = self.forward(x_test)\n        return y_hat.argmax(axis=1)","be6914bc":"df = pd.read_csv('..\/input\/iris-dataset\/Iris.csv')\ndf.head(2)","e5c2741d":"df.Species.unique()","bea92e0d":"df.Species.replace(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), (0, 1, 2), inplace=True)","10cefbf3":"df = df.sample(frac=1, random_state=0)","118c3d34":"X, Y = df.drop(['Species'], axis=1).values, df.Species.values","5694ec22":"X_train, Y_train, X_val, Y_val = train_test_split(X, Y, size=0.2, random_state=0)\nY_train = (np.arange(np.max(Y_train) + 1) == Y_train[:, None]).astype(float)","a3dd86b3":"s = SoftmaxRegression()\n\ns.train(X_train, Y_train, 0.02, 200, random_state=0)\ns.plot_convergence()","edbec948":"Y_hat = s.predict(X_val)\n\nconfusion_matrix(Y_val, Y_hat)","b8861c75":"print(classification_report(Y_val, Y_hat))","d0ae8e47":"#### Convert dataframe to numpy array","3a4df45b":"### Train","93899c6d":"#### Shuffle data","2e6d26cf":"#### Convert dataframe to numpy array","3557f6e0":"### Basic preprocessing","dd4d7b42":"#### Train","3459db0d":"#### Create data","4b619de5":"#### Train","bfce8f80":"#### Confusion matrix","b0bc9621":"#### Convert to numerical","f7bedd14":"#### Split data","6bd248de":"### Utils","2b4eb2a0":"#### Convert to numerical","663ce38f":"### Evaluate on validation data","5ed260cb":"#### Split","6c77c3bd":"#### Shuffle data","41c39a26":"## Train on Real data","bbeba40b":"### Basic preprocessing","678cd7c8":"### Evaluate on validation data","14ddb61f":"## Train on Dummy data","4601a425":"## Logistic Regression model","4e8d9a5b":"## Softmax Regression","92b1f372":"#### Split","1378ec29":"### Evaluate on validation data","91e6af90":"#### Train"}}