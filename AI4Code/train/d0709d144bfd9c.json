{"cell_type":{"1ee950e6":"code","3fbd318d":"code","49c74deb":"code","715dbec1":"code","63733365":"code","76bfde3d":"code","928669ab":"code","2bb84bdb":"code","1cea0b7c":"code","15c0f6ab":"code","58a858ad":"code","8ae0aa3e":"code","a9550836":"code","ce9eeab3":"code","554ccf19":"code","b269ec53":"code","ec6648cc":"code","6b552962":"code","02531aa4":"code","59aa5bba":"code","ed7ceb73":"code","95fb3b69":"code","0738904e":"code","ac0efe34":"code","7127c8c0":"code","312f4448":"code","7a3a40d7":"code","a92023a2":"code","625215aa":"code","43dd43bd":"code","17a56a71":"code","d0223393":"code","80c2d7dc":"code","d36842f5":"markdown","f3dc7ffc":"markdown","9803dcde":"markdown","0772cf93":"markdown","4198009f":"markdown","208dba4e":"markdown","b7eca59f":"markdown","7f5f4952":"markdown","d6462f48":"markdown","d694d9d5":"markdown","14f64247":"markdown","50ed6a02":"markdown","ee69600a":"markdown"},"source":{"1ee950e6":"import pandas as pd\nimport numpy as np\nimport random\nimport tensorflow as tf","3fbd318d":"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n","49c74deb":"sample = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv')\ntrain = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')","715dbec1":"train.head(2)\ntrain.labels.unique()","63733365":"import matplotlib.pyplot as plt\n%matplotlib inline","76bfde3d":"import cv2","928669ab":"im1 = cv2.imread('..\/input\/plant-pathology-2021-fgvc8\/train_images\/800113bb65efe69e.jpg')\nim2 = cv2.imread('..\/input\/plant-pathology-2021-fgvc8\/train_images\/8002cb321f8bfcdf.jpg')\nim3 = cv2.imread('..\/input\/plant-pathology-2021-fgvc8\/train_images\/80070f7fb5e2ccaa.jpg')\nim4 = cv2.imread('..\/input\/plant-pathology-2021-fgvc8\/train_images\/800edef467d27c15.jpg')","2bb84bdb":"f, axes = plt.subplots(2,2)\naxes[0][0].imshow(im1)\naxes[0][1].imshow(im2)\naxes[1][0].imshow(im3)\naxes[1][1].imshow(im4)\nplt.show()","1cea0b7c":"import os","15c0f6ab":"train = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')","58a858ad":"from sklearn.preprocessing import MultiLabelBinarizer","8ae0aa3e":"df = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv', index_col='image')\n\ndf['labels'] = [x.split(' ') for x in df['labels']]\n\nbinarizer = MultiLabelBinarizer()\nlabels = binarizer.fit_transform(df['labels'].values)\n\ndf = pd.DataFrame(\n    index=df.index,\n    columns=binarizer.classes_,\n    data=labels)","a9550836":"width, height = 400, 400","ce9eeab3":"from cv2 import imread, resize","554ccf19":"paths = ['..\/input\/plant-pathology-2021-fgvc8\/train_images\/'+ path for path in os.listdir('..\/input\/plant-pathology-2021-fgvc8\/train_images')]","b269ec53":"labels = [df.loc[f'{name}'] for name in os.listdir('..\/input\/plant-pathology-2021-fgvc8\/train_images')]","ec6648cc":"labels = np.array(labels)","6b552962":"from cv2 import getRotationMatrix2D, warpAffine, flip","02531aa4":"def rotateImage(image, range_s, range_e):\n    range_random = random.randint(range_s, range_e)\n    rm = getRotationMatrix2D((width\/2,height\/2), range_random, 1)\n    rotated_image = warpAffine(image, rm, (width,height))\n    \n    range_random_2 = random.randint(range_s, range_e)\n    rm = getRotationMatrix2D((width\/2,height\/2), range_random_2, 1)\n    rotated_image_2 = warpAffine(image, rm, (width,height))\n    rotated_image_2 = flip(rotated_image_2, 3)\n    return rotated_image, rotated_image_2","59aa5bba":"def augmentImages(imgpath):\n    image = imread(imgpath)\n    image = resize(image, (width,height)) #1\n    \n    image_rotated_1, image_rotated_2 = rotateImage(image, 10, 50) #2 - 3\n    image_flipped = flip(image, 3) # 4\n    image_flipped_rotated_1, image_flipped_rotated_2 = rotateImage(image_flipped, 10, 50) # 5 - 6\n  #  cont = random.uniform(0.1, 1.5)* image  #7\n  #  bright = image + random.randint(-10,10)#8\n    batch = np.array([image, image_rotated_1, image_rotated_2, image_flipped, image_flipped_rotated_1,\n                     image_flipped_rotated_2])\n    return batch\/255","ed7ceb73":"def showImages(dim1, dim2, imgs):\n    gure, axes = plt.subplots(nrows=dim1, ncols=dim2, figsize=(20,15))\n    for i in range(dim1*dim2):\n        axes[i\/\/dim2, i%dim2].imshow(np.array(imgs.pop()))","95fb3b69":"batch = augmentImages('..\/input\/plant-pathology-2021-fgvc8\/train_images\/800113bb65efe69e.jpg')","0738904e":"showImages(2,3,list(batch))","ac0efe34":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Flatten, GlobalMaxPooling2D, Dense, Input, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam","7127c8c0":"with strategy.scope():\n    Input_tensor = Input((width,height,3))\n\n    en = EfficientNetB0(include_top = False, input_tensor = Input_tensor)\n    en.trainable = False\n\n    model = Sequential()\n    model.add(en)\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n\n    model.add(Dense(2048, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(2048, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(2048, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(2048, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(1024, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(512, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n\n    model.add(Dense(128, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n\n    model.add(Dense(64, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n\n    model.add(Dense(7, activation = 'sigmoid'))\n\n\n    optimizer = Adam(0.0001)\n    model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])","312f4448":"model.summary()","7a3a40d7":"from tqdm import tqdm","a92023a2":"n_samples = len(paths)\nn_samples","625215aa":"epochs = 1","43dd43bd":"from IPython.display import clear_output","17a56a71":"import gc","d0223393":"losses = []\naccs = []\ncnt = 0\nfor label in tqdm(labels):\n    \n    one_img_augmented_6 = augmentImages(paths.pop())\n    \n    for epoch in range(epochs):\n        loss, acc = model.train_on_batch(one_img_augmented_6, np.array([label,label,label,label,label,label]))\n        \n        del one_img_augmented_6\n        losses.append(loss)\n        accs.append(acc)\n        cnt+=1\n        \n    gc.collect()\n    \n    if cnt>=10000:\n        cnt = 0\n        clear_output()\n        print(acc)\n        plt.figure(figsize = (16,16))\n        plt.plot(accs)\n        plt.plot(losses)\n        plt.legend(['accs', 'losses'])\n        plt.show()\n        del loss, acc\n    ","80c2d7dc":"model.save('model.h5py')","d36842f5":"## Defining the Neural Network","f3dc7ffc":"# End\nThank you for reading, if this helped you, or you learnt something new from it, give it an upvote!","9803dcde":"# Data\n## Preprocessing the Data\n#### I used MultiLabelBinarizer in order to one-hot encode the multi-label outputs","0772cf93":"## Setting up Image Augmentation using OpenCV\n#### Out of every image, I decided to generate 5 more by rotating, and flipping the image","4198009f":"## Exploring the data\n#### We'll use matplotlib to visualize a few images from the dataset.","208dba4e":"## Saving the model\n#### Always remember to save your model! so you can use it later in inference or further training.","b7eca59f":"## Importing the labels","7f5f4952":"#### Read the images and organize them using subplots, then display them all at once","d6462f48":"## Training Loop\n#### Since the data can't fit in the RAM, I had to define a custom generator and train the CNN batch by batch.","d694d9d5":"## Accelerator setup\n#### This cell is used to set up the TPU if you want to use it, I'd like to mention that I ran this notebook with a GPU and haven't tried TPU training yet.","14f64247":"# Plant Pathology 2021 - FGVC8\n## Transfer Learning & Image augmentation using OpenCV\n![image.png](attachment:image.png)\n## Identify the category of foliar diseases in apple tree\n\n### Problem Statement\nApples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive. <br \/>\n\nAlthough computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc. <br \/>\n\nPlant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year\u2019s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings.\n### Specific Objectives\nThe main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image. <br \/>\n\n# Table of Contents\n\n1. Preparations\n    1. Importing the necessary libraries\n    2. Accelerator setup\n    3. Importing the labels\n    4. Exploring the data\n2. Data\n    1. Preprocessing the data\n    2. Setting up image augmentation using OpenCV\n3. CNN\n    1. Picking the right pre-trained model\n    2. Defining the Neural Network\n    3. Training loop\n    4. Save the model","50ed6a02":"# Preparations\n## Importing the necessary libraries\n#### We'll use pandas, numpy, random, and tensorflow, we'll import the rest later on","ee69600a":"# CNN\n## Picking the right pre-trained model\n#### The following info was taken from [Google AI Blog](https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html)\n#### Convolutional neural networks (CNNs) are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers, and recently, GPipe achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline CNN by a factor of four. The conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. What if, instead, we could find a more principled method to scale up a CNN to obtain better accuracy and efficiency?\n### EfficientNet Architecture\n#### The effectiveness of model scaling also relies heavily on the baseline network. So, to further improve performance, we have also developed a new baseline network by performing a neural architecture search using the AutoML MNAS framework, which optimizes both accuracy and efficiency (FLOPS). The resulting architecture uses mobile inverted bottleneck convolution (MBConv), similar to MobileNetV2 and MnasNet, but is slightly larger due to an increased FLOP budget. We then scale up the baseline network to obtain a family of models, called EfficientNets.\n![image.png](attachment:image.png)\n### EfficientNet Performance\n#### We have compared our EfficientNets with other existing CNNs on ImageNet. In general, the EfficientNet models achieve both higher accuracy and better efficiency over existing CNNs, reducing parameter size and FLOPS by an order of magnitude. For example, in the high-accuracy regime, our EfficientNet-B7 reaches state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on CPU inference than the previous Gpipe. Compared with the widely used ResNet-50, our EfficientNet-B4 uses similar FLOPS, while improving the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%).\n![image.png](attachment:image.png)\nModel Size vs. Accuracy Comparison. EfficientNet-B0 is the baseline network developed by AutoML MNAS, while Efficient-B1 to B7 are obtained by scaling up the baseline network. In particular, our EfficientNet-B7 achieves new state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy, while being 8.4x smaller than the best existing CNN. For more info read [here](https:\/\/arxiv.org\/abs\/1905.11946)\n### After doing a bit of research, I decided that **EfficientNet** would be the ideal set of pre-trained models that I can play around with, due to it's high performance and low relative computational cost.\n"}}