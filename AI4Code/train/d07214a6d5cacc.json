{"cell_type":{"4b064f83":"code","270aeb54":"code","7cd5c51c":"code","340b1d71":"code","c39ab258":"code","6c123219":"code","7cbac458":"code","e196ddc6":"code","dc3b1f91":"code","65ab9599":"code","f76ffdf0":"code","7eb7dea1":"code","93f903db":"code","21bac786":"code","49a0bdfe":"code","11aa4a48":"code","ceb961a2":"code","257945b5":"code","e3f96e69":"code","479f6065":"code","52d709a5":"code","d594eeec":"code","69e1286a":"code","762e867f":"code","826701d1":"markdown","1424ee67":"markdown"},"source":{"4b064f83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","270aeb54":"df = pd.read_csv(\"\/kaggle\/input\/openintro-possum\/possum.csv\")\ndf.head()","7cd5c51c":"df.info()","340b1d71":"df.describe()","c39ab258":"import seaborn as sns\nimport matplotlib.pyplot as plt","6c123219":"plt.figure(figsize=(10,9))\nsns.heatmap(df.corr(),annot=True)","7cbac458":"sns.lmplot(x=\"hdlngth\",y=\"taill\",data=df,hue=\"sex\",fit_reg=False)","e196ddc6":"plt.figure(figsize=(14,4))\nplt.subplot(1,4,1)\nsns.kdeplot(x=\"hdlngth\",data=df[df[\"sex\"] == \"f\"],shade=True,label=\"f\")\nsns.kdeplot(x=\"hdlngth\",data=df[df[\"sex\"] == \"m\"],shade=True,label=\"m\")\nplt.legend()\nplt.title(\"Head length distributions by sex\")\n\nplt.subplot(1,4,2)\nsns.kdeplot(x=\"taill\",data=df[df[\"sex\"] == \"f\"],shade=True,label=\"f\")\nsns.kdeplot(x=\"taill\",data=df[df[\"sex\"] == \"m\"],shade=True,label=\"m\")\nplt.legend()\nplt.title(\"Tail distributions by sex\")\n\nplt.subplot(1,4,3)\nsns.kdeplot(x=\"footlgth\",data=df[df[\"sex\"] == \"f\"],shade=True,label=\"f\")\nsns.kdeplot(x=\"footlgth\",data=df[df[\"sex\"] == \"m\"],shade=True,label=\"m\")\nplt.legend()\nplt.title(\"Foot length distributions by sex\")\n\nplt.subplot(1,4,4)\nsns.kdeplot(x=\"chest\",data=df[df[\"sex\"] == \"f\"],shade=True,label=\"f\")\nsns.kdeplot(x=\"chest\",data=df[df[\"sex\"] == \"m\"],shade=True,label=\"m\")\nplt.legend()\nplt.title(\"Chest size distributions by sex\")\nplt.tight_layout()","dc3b1f91":"plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.regplot(x=\"hdlngth\",y=\"skullw\",data=df,line_kws={\"color\":\"red\"})\nplt.xlabel(\"Head Length\")\nplt.ylabel(\"Skull Width\")\n\nplt.subplot(1,2,2)\nsns.regplot(x=\"hdlngth\",y=\"chest\",data=df,line_kws={\"color\":\"red\"})\nplt.xlabel(\"Head Length\")\nplt.ylabel(\"Chest Size\")","65ab9599":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,\n           cmap=\"viridis\")","f76ffdf0":"df.head()","7eb7dea1":"print(\"Pop unique values: {}\".format(df[\"Pop\"].unique()))\nprint(\"Sex unique values: {}\".format(df[\"sex\"].unique()))","93f903db":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","21bac786":"# Preprocessing Data\ndef prepro(df):\n    df = df.copy()\n    \n    # Replace str values with binary values\n    df[\"sex\"] = df[\"sex\"].replace({\"m\":1,\"f\":0})\n    df[\"Pop\"] = df[\"Pop\"].replace({\"Vic\":1,\"other\":0})\n    \n    # Filling null values with columns mean values\n    for col in df.columns:\n        df[col].fillna(df[col].mean(),inplace=True)\n    \n    # Splitting and scaling dataset\n    X = df.drop(\"totlngth\",axis=1)\n    y = df[\"totlngth\"]\n    \n    scaler = StandardScaler()\n    scaler.fit(X)\n    \n    # Creating a dataframe containing our tranformed target variables\n    X = pd.DataFrame(scaler.transform(X),index=X.index,columns=X.columns)\n    \n    return train_test_split(X,y,test_size=0.3,shuffle=True)\n    \n    \nX_train,X_test,y_train,y_test = prepro(df)\n    \n    ","49a0bdfe":"# Importing regression models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfmod = RandomForestRegressor()\nlinmod = LinearRegression()\n\nrfmod.fit(X_train,y_train)\nlinmod.fit(X_train,y_train)","11aa4a48":"# Visually comparing model performance\np =linmod.predict(X_test)\npreds = rfmod.predict(X_test)\n\n\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.scatter(y_test, p)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Linear Regression Model\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_test, preds)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Random Forest Regressor Model\")","ceb961a2":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","257945b5":"print(\"Random Forest Regressor Score: {}\".format(r2_score(y_test,preds)))\nprint(\"Linear Regression Score: {}\".format(r2_score(y_test,p)))","e3f96e69":"lr2 = []\nrfr2 = []\nfor i in range(1,21):\n    \n    X_train,X_test,y_train,y_test = prepro(df)\n    \n    linmod = LinearRegression()\n    rfmod = RandomForestRegressor()\n    \n    linmod.fit(X_train,y_train)\n    rfmod.fit(X_train,y_train)\n    \n    lr2.append(linmod.score(X_test,y_test))\n    rfr2.append(rfmod.score(X_test,y_test))","479f6065":"plt.figure(figsize=(12,5))\nplt.plot(range(1,21),lr2,marker=\"o\",ls=\"--\",label=\"Linear Model\")\nplt.plot(range(1,21),rfr2,marker=\"o\",label=\"Random Forest Model\")\nplt.ylabel(\"R^2 Score\")\nplt.xlabel(\"Split, Fit No.\")\nplt.legend(loc=\"lower right\")","52d709a5":"print(\"Linear Model Mean R^2 Score: {:.3f}\".format(np.array(lr2).mean()))\nprint(\"Random Forest Model Mean R^2 Score: {:.3f}\".format(np.array(rfr2).mean()))","d594eeec":"predictions = linmod.predict(X_test)","69e1286a":"# Residuals\nsns.histplot(y_test-predictions,bins=8)","762e867f":"print(\"Mean Absolute Error: {:.3f}\".format(mean_absolute_error(y_test,predictions)))","826701d1":"After running each model several times and getting a fairly wide range of R^2 scores, I believe it would be best to compare scores over a number of splits and tests","1424ee67":"Our Linear Model seems to consistently outperform our random forest model."}}