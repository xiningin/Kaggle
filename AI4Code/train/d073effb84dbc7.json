{"cell_type":{"f4f65814":"code","cb7955c7":"code","68bca4ec":"code","892970e2":"code","5134ae3e":"code","abdc43a3":"code","8e8edca5":"code","c0d3d1e2":"code","f419c79a":"code","c639d47b":"code","1d7710b7":"code","c9b1f201":"code","81316a1f":"code","4b709ffa":"code","1b26b7ad":"code","a105188a":"code","b799c732":"code","3fbea48e":"code","7165b719":"code","5ff29d35":"code","59cd2649":"code","83e5afea":"code","108c8883":"code","387d5586":"code","f0b9ad73":"code","8b3cec1b":"code","8952dad6":"code","eb7502d6":"code","07affd15":"code","a59f5720":"code","dee25af9":"code","1925c9ca":"code","c89e16ff":"code","1d00b993":"code","2422816f":"markdown","95a3dafd":"markdown","2f11ac9e":"markdown","512070d5":"markdown","7ad794b8":"markdown","24473760":"markdown","28efb5ae":"markdown","c1a2cc85":"markdown","dea4db4a":"markdown","e673e863":"markdown"},"source":{"f4f65814":"import os                       # accessing directory structure\nimport numpy as np              # linear algebra\nimport pandas as pd             # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns           # plotting","cb7955c7":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)","68bca4ec":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","892970e2":"from IPython.display import Image\n%matplotlib inline","5134ae3e":"df = pd.read_csv('\/kaggle\/input\/Skyserver_12_30_2019 4_49_58 PM.csv')\ndf.head(10)","abdc43a3":"nRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","8e8edca5":"df.columns","c0d3d1e2":"# Store columns in a list, might will be helpful later\ncols = list(df.columns)\ncols","f419c79a":"# Our labels\ndf['class'].unique()","c639d47b":"# Let's see how the classes are distributed \ndf['class'].value_counts()","1d7710b7":"# Visualization with Matplotlib\ndf['class'].value_counts().plot(kind='bar')","c9b1f201":"# Visualization with Seaborn\nsns.countplot(x='class', data=df, palette=\"brg\")\nplt.show()","81316a1f":"df['mjd'].hist()","4b709ffa":"df['redshift'].hist()","1b26b7ad":"# Let's find missing values\ndf.isnull().sum()","a105188a":"# Mapping classes to 0,1,2 values\nclass_mapping = {label: idx for idx, label in enumerate(np.unique(df['class']))}\nclass_mapping","b799c732":"df['class'] = df['class'].map(class_mapping)\ndf\n\n# now we see class column with numerical (0,1,2) values","3fbea48e":"# Invariant back to original\ninv_class_mapping = {v: k for k, v in class_mapping.items()}\ndf['class'] = df['class'].map(inv_class_mapping)\ndf","7165b719":"# Labels encoding with special Scikit Learn function\nfrom sklearn.preprocessing import LabelEncoder\n\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['class'].values)\nprint(y)\nprint('We have {} values'.format(len(y)))","5ff29d35":"# Our target\nprint(y)\n\n# We prepare data on which we will train and test\n# Labels column should be excluded\ndf = df.drop(columns=['class'])\ndf","59cd2649":"# We need to normalize the data, to not have bias of huge values\n\nfrom sklearn import preprocessing\n\nx = df.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)","83e5afea":"X = df","108c8883":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=1)","387d5586":"X_train","f0b9ad73":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_dtc = DecisionTreeClassifier(random_state=49)\n\nmodel_dtc.fit(X_train, y_train)\n\naccuracies = {}\n\nacc = model_dtc.score(X_test, y_test)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","8b3cec1b":"from sklearn.linear_model import LogisticRegression\n\nmodel_lr = LogisticRegression() # default parameters\n\nmodel_lr.fit(X_train,y_train)\n\ny_pred = model_lr.predict(X_test)\n\nacc = model_lr.score(X_test,y_test)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","8952dad6":"print(y_pred)\nprint(y_test)","eb7502d6":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nlr_train_acc = model_lr.score(X_train, y_train)\nprint('Training Score: ', lr_train_acc)\nlr_test_acc = model_lr.score(X_test, y_test)\nprint('Testing Score: ', lr_test_acc)","07affd15":"from sklearn.svm import SVC\n\nmodel_svm = SVC(random_state = 1)\n\nmodel_svm.fit(X_train, y_train)\n\nacc = model_svm.score(X_test,y_test)*100\n\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))\n\n# Attention: Slow model","a59f5720":"from sklearn.naive_bayes import GaussianNB\n\nmodel_nb = GaussianNB()\n\nmodel_nb.fit(X_train, y_train)\n\nacc = model_nb.score(X_test,y_test)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","dee25af9":"y_pred = model_nb.predict(X_test)","1925c9ca":"print('Classification Report: \\n', classification_report(y_test, y_pred))\nprint('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\nlr_train_acc = model_lr.score(X_train, y_train)\nprint('Training Score: ', lr_train_acc)\nlr_test_acc = model_lr.score(X_test, y_test)\nprint('Testing Score: ', lr_test_acc)","c89e16ff":"from sklearn.neighbors import KNeighborsClassifier\nmodel_knn = KNeighborsClassifier(n_neighbors = 3)  # n_neighbors means k\nmodel_knn.fit(X_train, y_train)\nprediction = model_knn.predict(X_test)\n\nprint(\"{} NN Score: {:.2f}%\".format(3, model_knn.score(X_test, y_test)*100))","1d00b993":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(X_train, y_train)\n    scoreList.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","2422816f":"## Logistic Regression","95a3dafd":"## Support Vector Machines (SVM)","2f11ac9e":"## KNN Neighbours","512070d5":"## Tasks to do:\n* Class weights for class imbalance\n* ANN models","7ad794b8":"## Encoding class labels\nFor some cases, we cannot simply provide categorical values (just strings). Instead, we can convert them to numerical values.\nFor example, since we have 3 classes, we able to assign to each class some values, so that:\n\n* 0 is for GALAXY\n* 1 is for QSO\n* 2 is for STAR.","24473760":"## Naive Bayes","28efb5ae":"## Split","c1a2cc85":"# Models","dea4db4a":"## Variety of models for SDSS data","e673e863":"## Decision Trees"}}