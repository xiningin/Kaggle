{"cell_type":{"af578560":"code","1d92748f":"code","701b767c":"code","289332f0":"code","fd67c2e4":"code","84c8c566":"code","aeef4b0a":"code","04d7ca9e":"code","f34b2705":"code","1df76314":"code","a3d70313":"code","b5a3e671":"code","7c533183":"code","ea3dbd85":"code","3a99d69e":"code","8b6a9d39":"code","52deb468":"code","d1fe2b35":"code","ce805135":"code","e64db360":"code","04a51b78":"code","5907a6d5":"code","70767c87":"code","87cd2e56":"code","14e98efc":"code","f0769dcf":"code","7e37db0e":"code","840147cd":"code","54534db4":"markdown","b4bf201b":"markdown","2eb861ab":"markdown","712fbf00":"markdown","f16fd202":"markdown","ead9d248":"markdown","1c82d6a2":"markdown","04f42c47":"markdown","c4c7a312":"markdown","529f8038":"markdown","dd020d44":"markdown","ff95e478":"markdown","63518a93":"markdown","b42a181e":"markdown","c5ef1b3a":"markdown","6f6fc752":"markdown","36c419db":"markdown","cc5305f7":"markdown","10954191":"markdown","6dc182b3":"markdown","d171ea97":"markdown","1e611abb":"markdown","ea435e4b":"markdown","f1d36452":"markdown","390d6602":"markdown","90eac88f":"markdown","8f76ab3d":"markdown","a0fd1ea4":"markdown","276e4d12":"markdown","5f26c84c":"markdown"},"source":{"af578560":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom scipy.stats import norm\nfrom  category_encoders import MEstimateEncoder\nfrom category_encoders import CatBoostEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\nimport optuna\n\n#mute warnings\nwarnings.filterwarnings('ignore')","1d92748f":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col='Id')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col ='Id')","701b767c":"df_train.head()\ndf_train.describe()\ndf_train.info()","289332f0":"df_test.head()\ndf_test.describe()\ndf_test.info()","fd67c2e4":"def load_data():\n    data_dir = Path('..\/input\/house-prices-advanced-regression-techniques\/')\n    df_train = pd.read_csv(data_dir \/ 'train.csv', index_col = 'Id')\n    df_test = pd.read_csv(data_dir \/ 'test.csv', index_col = 'Id')\n    \n    df = pd.concat([df_train,df_test])\n    #Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    #Reform Splits\n    df_train = df.loc[df_train.index,:]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n    ","84c8c566":"plt.figure(figsize=(20,8))\nsns.distplot(df_train.SalePrice, fit=norm)","aeef4b0a":"num_columns = df_train.columns[df_train.dtypes != 'object']\ncat_columns = df_train.columns[df_train.dtypes == 'object']\n\nprint(num_columns, num_columns.shape)\nprint(cat_columns, cat_columns.shape)","04d7ca9e":"df = pd.concat([df_train, df_test])\nnum_columns = df.columns[df.dtypes != 'object']\ncat_columns = df.columns[df.dtypes == 'object']\n\nfigure1, ax = plt.subplots(7,6, figsize = (25,25))\nfor i, j in enumerate(num_columns):\n    sns.distplot(df[j], fit=norm, ax = ax[i\/\/6, i%6])\nfigure1.text(0.4,.92, 'Distribution of numerical features', size = 25, weight = 'bold')\nplt.show(figure1)\n\nfigure2, ax = plt.subplots(8,6,figsize = (25,25))\nfor k,j in enumerate(cat_columns):\n    plot = sns.barplot(data=pd.DataFrame(df[j].value_counts()).reset_index(), x = 'index', y = j, ax = ax[k\/\/6, k%6])\n    plot.set(title = j)\n    plot.set(xticks=[])\nfigure2.text (0.4, .92, 'Distribution of categorical features', size = 25, weight = 'bold')\nplt.show(figure2)","f34b2705":"def clean(df):\n    df['Exterior2nd'] = df['Exterior2nd'].replace({'Brk Cmn': 'BrkComm'})\n    df['GarageYrBlt'] = df['GarageYrBlt'].where(df.GarageYrBlt <=2010, df.YearBuilt)\n    df.rename(columns = {'1stFlrSF': 'FirstFlrSF', '2ndFlrSF': 'SecondFlrSF', '3SsnPorch':'Threeseasonporch'}, inplace=True)\n    return df","1df76314":"#unordered nominative features\nfeatures_nom = ['MSSubClass', 'MSZoning', 'Street', 'Alley','LandContour', 'LotConfig','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir','GarageType','MiscFeature','SaleType','SaleCondition']\n\n#Pandas levels\nfive_levels = ['Po', 'Fa', 'TA','Gd','Ex']\nten_levels =list(range(10))\n\nordered_levels = {\n    'OverallQual':ten_levels,\n    'OverallCond':ten_levels,\n    'ExterQual':five_levels,\n    'ExterCond':five_levels,\n    'BsmtQual':five_levels,\n    'BsmtCond':five_levels,\n    'HeatingQC':five_levels,\n    'KitchenQual':five_levels,\n    'FireplaceQu':five_levels,\n    'GarageQual': five_levels,\n    'GarageCond':five_levels,\n    'PoolQC':five_levels,\n    'LotShape':['Reg', 'IR1','IR2','IR3'],\n    'LandSlope':['Sev', 'Mod','Gtl'],\n    'BsmtExposure': ['No','Mn','Av','Gd'],\n    'BsmtFinType1':['Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n    'BsmtFinType2':['Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n    'Functional':['Sal','Sev','Maj1','Maj2','Mod','Min2','Min1','Typ'],\n    'GarageFinish': ['Unf','RFn','Fin'],\n    'PavedDrive':['N','P','Y'],\n    'Utilities': ['NoSeWa','NoSewr','AllPub'],\n    'CentralAir':['N','Y'],\n    'Electrical':['Mix','FuseP','FuseF','FuseA','SBrkr'],\n    'Fence':['MnWw','GdWo','MnPrv','GdPrv'],\n}\n\n#add None level for missing values\nordered_levels = {key:['None']+value for key, value in ordered_levels.items()}\n\ndef encode(df):\n    #Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype('category')\n        #Add none category for missing values\n        if 'None' not in df[name].cat.categories:\n            df[name].cat.add_categories('None', inplace=True)\n    #Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels, ordered=True))\n    return df","a3d70313":"def impute(df):\n    for name in df.select_dtypes('number'):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes('category'):\n        df[name] = df[name].fillna('None')\n    return df","b5a3e671":"df = clean(df)\ndf = encode(df)\ndf = impute(df)\n\ndf_train = df.loc[df_train.index,:]\ndf_test = df.loc[df_test.index, :]\n\ndf_train.info()\ndf_test.info()","7c533183":"def score_dataset(X,y,model = XGBRegressor()):\n    #label encoding is good for XGBR, 'cat.codes' attribute holds the category levels.\n    for colname in X.select_dtypes(['category']):\n        X[colname] = X[colname].cat.codes\n    \n    log_y = np.log(y)\n    score = cross_val_score(model, X, log_y, cv=5, scoring = 'neg_mean_squared_error')\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n        ","ea3dbd85":"X = df_train.copy()\ny = X.pop('SalePrice')\nbaseline_score = score_dataset(X,y)\nprint(baseline_score)","3a99d69e":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes(['object','category']):\n        X[colname], _ = X[colname].factorize()\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features = discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name = 'MI Scores', index = X.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    return mi_scores\n    ","8b6a9d39":"X = df_train.copy()\ny = X.pop('SalePrice')\n\nmi_scores = make_mi_scores(X,y)\nmi_scores","52deb468":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores>0.0]\n","d1fe2b35":"X = df_train.copy()\ny = X.pop('SalePrice')\nX = drop_uninformative(X, mi_scores)\nscore_dataset(X,y)","ce805135":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes(['category']):\n        X[colname] = X[colname].cat.codes\n    return X\n\n#label encoding is okay for trees (like RandomForest or XGBoost) if you use linear regression better to use One Hot Encoding especially for unordered features.","e64db360":"#some other features:\n\ndef mathematical_transforms(df):\n    X = pd.DataFrame()\n    X['LivLotRatio'] = df.GrLivArea \/ df.LotArea\n    X['Spaciousness'] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    X['Feet'] = np.sqrt(df.GrLivArea)\n    X['TotalSF'] = df.TotalBsmtSF + df.FirstFlrSF + df.SecondFlrSF\n    X['TotalBathrooms'] = df.FullBath + 0.5* df.HalfBath + df.BsmtFullBath + 0.5 * df.BsmtHalfBath\n    X['TotalPorchSF']= df.OpenPorchSF + df.Threeseasonporch + df.EnclosedPorch + df.ScreenPorch + df.WoodDeckSF\n    #X['Quality'] = df.OverallQual * df.OverallCond\n    #add more if you want\n    \n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix = 'Bldg')\n    X = X.mul(df.GrLivArea, axis = 0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X['PorchTypes'] = df[[\n        'WoodDeckSF',\n        'OpenPorchSF',\n        'EnclosedPorch',\n        'Threeseasonporch',\n        'ScreenPorch',  \n    ]].gt(0.0).sum(axis = 1)\n    \n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X['MedNhbdArea'] = df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    #X['Diff'] = df.groupby('Neighborhood')['GrLivArea'].transform('count') - df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X['MSClass'] = df.MSSubClass.str.split('_', n=1, expand = True)[0]\n    \n    return X\n    ","04a51b78":"cluster_features = ['LotArea', 'TotalBsmtSF','FirstFlrSF','SecondFlrSF','GrLivArea']\n\n\ndef cluster_distance(df, features, n_clusters = 20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0))\/ X_scaled.std(axis=0)\n    kmeans = KMeans ( n_clusters = 20, n_init =50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    X_cd = pd.DataFrame(X_cd, columns = [f'centroid_{i}' for i in range (X_cd.shape[1])])\n    return X_cd\n\ndef cluster_labels ( df, features, n_clusters = 20):\n    X = df.copy()\n    X_scaled = X.loc [:,features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters = n_clusters, n_init = 50, random_state = 0)\n    X_new = pd.DataFrame()\n    X_new['Cluster'] = kmeans.fit_predict(X_scaled)\n    \n    return X_new","5907a6d5":"def apply_pca(X, standarize = True):\n    if standarize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    \n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    #convert to DataFrame\n    component_names = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns = component_names)\n    loadings = pd.DataFrame(pca.components_.T, columns = component_names, index = X.columns)\n    return pca, X_pca, loadings\n\ndef pca_inspired(df):\n    X = pd.DataFrame()\n    X['Feature1'] = df.GrLivArea + df.TotalBsmtSF\n    X['Feature2'] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\npca_features = ['GarageArea', 'YearRemodAdd','TotalBsmtSF','GrLivArea']\n\n    ","70767c87":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new['Outlier'] = (df.Neighborhood == 'Edwards') & (df.SaleCondition == 'Partial')\n    return X_new\n\n#Could improve outliers","87cd2e56":"class CrossFoldEncoder:\n    def __init__(self,encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs\n        self.cv_ = KFold (n_splits = 5)\n        \n    def fit_transform(self,X,y,cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        \n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols = cols, **self.kwargs_)\n            fitted_encoder.fit(X.iloc[idx_encode, :], y.iloc[idx_encode])\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        \n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + '_encoded' for name in X_encoded.columns]\n        \n        return X_encoded\n    \n    def transform(self,X):\n        from functools import reduce\n        \n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce (lambda x,y : x.add(y,fill_value=0), X_encoded_list) \/ len(X_encoded_list)\n        X_encoded.columns = [name + '_encoded' for name in X_encoded.columns]\n        \n        return X_encoded","14e98efc":"def create_features(df, df_test = None):\n    X = df.copy()\n    y = X.pop('SalePrice')\n    mi_scores = make_mi_scores(X,y)\n    \n    #combine splits if data is given, use all data possible\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop('SalePrice')\n        X = pd.concat([X, X_test])\n        \n    #Lesson 2: Mutual Information\n    X = drop_uninformative(X, mi_scores)\n    \n    #Lesson 3: Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n    \n    #Lesson 4 - Clustering\n    X = X.join(cluster_labels(X, cluster_features, n_clusters = 20))\n    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n    \n    #Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n    #X = X.join(indicate_outliers(X))\n    \n    X = label_encode(X)\n    \n    #reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n        \n    #Lesson 6: Target Encoder\n    \n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols= ['MSSubClass']))\n    \n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n    if df_test is not None:\n        return X, X_test\n    \n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, 'SalePrice']\n    \n#Let's see how we score after adding all this features.\n\nscore_dataset(X_train, y_train)","f0769dcf":"X_train = create_features(df_train)\ny_train = df_train.loc[:, 'SalePrice']\n\nxgb_params = dict(max_depth = 6,\n                 learning_rate = 0.01,\n                 n_estimators = 1000,\n                 min_child_weight = 1,\n                 colsample_bytree = 0.7,\n                 subsample = 0.7,\n                 reg_alpha = 0.5,\n                 reg_lambda = 1,\n                 num_parallel_tree = 1)\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","7e37db0e":"def objective(trial):\n    xgb_params = dict(\n        max_depth = trial.suggest_int('max_depth',2,10),\n        learning_rate = trial.suggest_float('learning_rate',1e-4, 1e-1, log=True),\n        n_estimators = trial.suggest_int('n_estimators',1000,8000),\n        min_child_weight = trial.suggest_int('min_child_weight', 1,10),\n        colsample_bytree = trial.suggest_float('colsample_bytree',0.2,1),\n        subsample = trial.suggest_float('subsample',0.2,1),\n        reg_alpha = trial.suggest_float('reg_alpha',1e-4, 1e2, log=True),\n        reg_lambda = trial.suggest_float('reg_lambda', 1e-4, 1e2, log=True))\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 20)\nxgb_params = study.best_params\n    ","840147cd":"X_train, X_test = create_features(df_train,df_test)\ny_train = df_train.loc[:, 'SalePrice']\n\nxgb = XGBRegressor(**study.best_params)\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice':predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint('Your submission was successfully saved!')\n","54534db4":"# Train data and create submission","b4bf201b":"We've cleaned the data and now we don't have null values anymore.\n","2eb861ab":"<b><h4> Clean Data <\/h4> <b>","712fbf00":"Let's create a function to load the data and apply all the transformation we'll do later.","f16fd202":"Target Encoding","ead9d248":"<center> <b> <h3> Load Data <\/h3> <\/b> <\/center>","1c82d6a2":"We can see that the distribution is not quite normal, so we will have to apply log or boxcox transformations to help our algorithm.<br> <br>\nNow let's take a look at the dependent variables or features.\n\nWe have 2 different kind of features: <b>numerical<\/b> and <b>categorical<\/b> features:<br>\n-Numerical features include intervals and ratios<br>\n-Categorical features include nominal (none category is better whithin this feature e.g. Street) and ordered (categories follow a rank) features.","04f42c47":"<center> <b> <h1>Advanced Linear Regression<\/h1> <\/b> <\/center>\n\n\n# **Introduction**\n\n\nHi! <br>\nThis is my first noteboook and I wanted to try to apply all the knowledge I gathered from Kaggle courses (ML, EDA, FE, etc) applying advanced linear regression using XGBoost and Optuna to optimise hyperparameter tuning and get an accurate model which predicts House Prices in the Ames Housing Dataset.<br>\n\nFirst, I will import all the required modules and load the data. Then, I will perform EDA (Exploratory Data Analysis) which uses graphical data analysis to better understanding of the dataset. Later, I will clean the data, create outliers and new features, I'll explore which features contribute most using Mutual information and get the best hyperparameters using Optuna. Finally, I'll create and test the model.\n\nThis code comes mostly from the code by Ryan Holbrook, and his notebook: 'Feature Engineering for House Prices', but I added some modifications to try to improve its performance.\n\nIf you like it, rate it up!\n","c4c7a312":"<b> <h4> Encode Categorical Data <\/h4> <\/b>","529f8038":"To process the data I'm going to follow all the steps learnt during the Feature Engineering Course and I will apply Optuna at the end to see if I can improve even more the result.\n","dd020d44":"<b> <h4> Establish baseline <\/h4> <\/b>","ff95e478":"<center> <b> <h3> Module Import <\/h3> <\/b> <\/center>","63518a93":"<b> <h4> Handle Missing Values <\/h4> <\/b>","b42a181e":"# Data Preprocessing","c5ef1b3a":"Let's take a look at the independent variable (i.e. the target we want to predict) \"Sale Price\"","6f6fc752":"K-Means Clustering (Cluster by labels or by distance)","36c419db":"Now we'd like to use some auto hyperparameter tuner, such as Optuna to improve our hyperparameter selection.","cc5305f7":"PCA","10954191":"# Feature Utility Scores (MI)","6dc182b3":"<b> OPTUNA <\/b>","d171ea97":"Some variables may suggest applying a log or boxcox function while others include 0's so log would require +1 and others peak at 0, which makes them lose importance. Now let's begin the 2nd process.","1e611abb":"Score has lightly improved, let's add new features now.","ea435e4b":"We can see that 37 features are numerical (including target 'SalesPrice') and 43 are categorical, but we can spot <b>'MSSubClass'<\/b> being in the numerical list while it is categorical. So, there are some issues here.<br>\nLet's explore their distributions.","f1d36452":"# Create Features","390d6602":"Final Feature Set","90eac88f":" # Hyperparameter Tuning","8f76ab3d":"PCA application, indicate outliers.","a0fd1ea4":"Let's define label encoding for categorical features.","276e4d12":"# Exploratory Data Analysis (EDA)","5f26c84c":"Some features are uninformative (score = 0), so let's remove them."}}