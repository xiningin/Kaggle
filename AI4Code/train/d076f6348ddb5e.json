{"cell_type":{"def0c16a":"code","53a703d0":"code","122093c6":"code","9af6d416":"code","76607a31":"code","59d9a088":"code","d6bbb658":"code","24b2474e":"code","8eaab4d6":"code","7b9ce910":"code","655b4403":"code","6fb3dd6d":"code","8da8586a":"code","89cc70fe":"code","dfb07f33":"code","73e9a31e":"code","94895728":"code","40b6c557":"code","52489ed7":"code","b6351e66":"code","26106c09":"code","01c82737":"code","ccb0845a":"code","10c0e87b":"code","f24e60b5":"code","88969f3f":"code","61f8cac9":"code","017b132b":"code","5265719a":"code","f2741f13":"code","43540567":"code","a7b96af0":"code","514920d2":"code","2974b68e":"code","680b151b":"code","06e52ba3":"code","162fbc18":"code","1be31249":"markdown","0ab64803":"markdown","14f90c46":"markdown","4634bb86":"markdown","30e5af76":"markdown","1623c2bf":"markdown","2fd20fba":"markdown","f3e4d5e3":"markdown","2bbacab7":"markdown","dedf85ce":"markdown"},"source":{"def0c16a":"import pandas as pd\n\nHOUSING_PATH = '\/kaggle\/input\/housing-in-london\/housing_in_london_monthly_variables.csv'\n\ndef load_housing_data(path: str = HOUSING_PATH) -> pd.DataFrame:\n    return pd.read_csv(path)","53a703d0":"housing = load_housing_data()\nhousing.head()","122093c6":"housing.info()","9af6d416":"housing.describe()","76607a31":"import matplotlib.pyplot as plt\nfrom typing import Tuple\n\nplt.style.use('dark_background')\n\ndef describe_value_counts(df: pd.DataFrame, column_name: str, figsize: Tuple[int, int]) -> None:\n    value_counts: pd.Series = df[column_name].value_counts()\n    value_counts.plot.bar(figsize=figsize)\n    plt.show()\n    \n    print(f'Value counts:\\n{value_counts}', end='\\n\\n')\n    print(f'Value proportions:\\n{value_counts \/ len(df)}')","59d9a088":"describe_value_counts(housing, 'borough_flag', (10, 10))\ndescribe_value_counts(housing, 'code', (30, 10))\ndescribe_value_counts(housing, 'area', (30, 10))","d6bbb658":"housing.hist(bins=50, figsize=(20, 15))\nplt.show()","24b2474e":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_indices, test_indices in split.split(housing, housing['borough_flag']):\n    strat_train_set: pd.DataFrame = housing.loc[train_indices]\n    strat_test_set: pd.DataFrame = housing.loc[test_indices]","8eaab4d6":"housing = strat_train_set.copy()","7b9ce910":"housing.plot(kind='scatter', x='houses_sold', y='average_price', alpha=0.6, figsize=(15, 10), c='no_of_crimes', cmap=plt.get_cmap('cubehelix'), colorbar=True)\nplt.show()","655b4403":"describe_value_counts(housing.dropna(), 'borough_flag', (10, 10))","6fb3dd6d":"housing.plot(kind='scatter', x='houses_sold', y='average_price', alpha=0.2, figsize=(15, 10))","8da8586a":"housing.corr()","89cc70fe":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(housing, figsize=(20, 12))\nplt.show()","dfb07f33":"housing = strat_train_set.drop('average_price', axis=1)\nhousing_labels = strat_train_set['average_price'].copy()","73e9a31e":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')","94895728":"housing_num = housing.drop(['date', 'area', 'code'], axis=1)\nimputer.fit(housing_num)","40b6c557":"for feature_name, feature_median in zip(housing_num.columns, housing_num.median().values):\n    print(f'{feature_name}: {feature_median}')","52489ed7":"import numpy as np\n\nX: np.ndarray = imputer.transform(housing_num)","b6351e66":"housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)","26106c09":"housing_tr.info()","01c82737":"# one hot encoding may be a bit messy here due to the large number of categorical possibilities... but we'll see how it works out\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nhousing_cat = housing.drop(['houses_sold', 'no_of_crimes', 'borough_flag', 'date'], axis=1)\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","ccb0845a":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","10c0e87b":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = list(housing_cat)\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs)\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","f24e60b5":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_predictions, housing_labels)\nlin_rmse = np.sqrt(lin_mse)\n\nprint(f'lin_rmse = {lin_rmse}')","88969f3f":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nlin_rmse_scores = np.sqrt(-scores)","61f8cac9":"def display_scores(scores: np.ndarray) -> None:\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Std:', scores.std())\n    \ndisplay_scores(lin_rmse_scores)","017b132b":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_rmse_scores)","5265719a":"forest_reg.max_features","f2741f13":"import joblib\n\njoblib.dump(forest_reg, 'forest_reg.pkl')","43540567":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30, 60], 'max_features': [1, 2, 3, 4, 5]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [1, 2, 3, 4, 5]}\n]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","a7b96af0":"cvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","514920d2":"feature_importances = grid_search.best_estimator_.feature_importances_\n\nattributes = num_attribs + cat_attribs\n\nsorted(zip(feature_importances, attributes), reverse=True)","2974b68e":"final_model = joblib.load('forest_reg.pkl')\n\nX_test = strat_test_set.drop('average_price', axis=1)\ny_test = strat_test_set['average_price'].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)","680b151b":"final_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint(f'final_rmse = {final_rmse}')","06e52ba3":"print(final_predictions[:5], end='\\n\\n')\nprint(y_test[:5])","162fbc18":"from sklearn.metrics import r2_score\n\nr2_score(y_test, final_predictions)","1be31249":"Good enough (\u30c4)_\/\u00af ","0ab64803":"From this scatter matrix, we can see that it supports what we've seen from the correlation matrix","14f90c46":"With the entire other half of the dataset introduced we can start to clearly see that as `houses_sold` increases, `no_of_crimes` quickly decreases\nto the point where the entire right side of this graph was not included in the above crime colormap graph\n\nIt also didn't show the *very* small values for `houses_sold`, even though `average_price` soars at those low levels","4634bb86":"From this we can understand that for the categorical string features of `area` and `code`, they are all represented equally (mostly) within the data.\n\nFor the categorical numeric variable of `borough_flag`, it is represented very disproportionately, meaning that it would likely be best to use stratified sampling to avoid sampling bias if selected at random","30e5af76":"It seems that a large majority of the crime is being done the higher `average_price` is, and tends to go down as `houses_sold` increases.","1623c2bf":"It's off by 140 thousand...","2fd20fba":"Now it's off by 90k, which isn't *too* bad considering the range of the target value","f3e4d5e3":"It seems that the categorical features barely mattered! (at least when one-hot encoded... maybe encoding them another way would be more helpful...)\n\nThough even the non-encoded `borough_flag` barely did anything","2bbacab7":"**EVERY** training sample which has an defined value for `no_of_crimes` guarantees a `borough_flag` value of 1... hmmmm...\n\nThis does not *necessarily* mean that areas with the `borough_flag` of 1 are more crime-filled, as a defined value can still be 0 and an undefined value does not guarantee it to be 0","dedf85ce":"Looks like the model is off by an average of 90k on the test set"}}