{"cell_type":{"e57a91e5":"code","5e2279d7":"code","825f58d7":"code","76fcb847":"code","f25f63a1":"code","08f08af6":"code","4c8c1ca6":"code","625da42f":"code","625afd8e":"code","ab573cb6":"code","4ab121d9":"code","7c4d6e6a":"markdown","ebb30302":"markdown","570c5c4b":"markdown","86dc59a0":"markdown","792dfd76":"markdown","7b27f4dd":"markdown"},"source":{"e57a91e5":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport random\nimport time\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPRegressor\n\nfrom scipy.optimize import minimize\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_rows = 120\npd.options.display.max_columns = 100\n\nimport warnings\nwarnings.simplefilter('ignore')","5e2279d7":"N_SPLITS = 10\nN_ESTIMATORS = 10000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 299792458","825f58d7":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","76fcb847":"INPUT = Path(\"..\/input\/tabular-playground-series-aug-2021\")\nINPUT_PRED = Path(\"..\/input\/tps08-pred\")\n\ntrain = pd.read_csv(INPUT \/ \"train.csv\")\ntrain['pred'] = np.load(INPUT_PRED \/ \"oof.npy\")\n\ntest = pd.read_csv(INPUT \/ \"test.csv\")\ntest['pred'] = np.load(INPUT_PRED \/ \"pred.npy\")\n\nsubmission = pd.read_csv(INPUT \/ \"sample_submission.csv\")\n\nscale_features = [col for col in test.columns if 'f' in col]\nfeatures = scale_features + ['pred']\ntarget = 'loss'","f25f63a1":"ss = StandardScaler()\ntrain[scale_features] = ss.fit_transform(train[scale_features])\ntest[scale_features] = ss.transform(test[scale_features])","08f08af6":"train.shape, test.shape","4c8c1ca6":"lgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 5e-3,\n    'subsample': 0.8,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.6,\n    'reg_alpha': 6.4,\n    'reg_lambda': 1.8,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'importance_type': 'gain',\n}","625da42f":"mlp_oof = np.zeros(train.shape[0])\nmlp_pred = np.zeros(test.shape[0])\n\nlgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nfeature_importances = pd.DataFrame()\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=train[features])):\n    print(f\"===== fold {fold} =====\")\n    X_train, y_train = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n    X_valid, y_valid = train[features].iloc[val_idx], train[target].iloc[val_idx]\n    X_test = test[features]\n\n    start = time.time()\n    model = MLPRegressor(hidden_layer_sizes=50,\n                         early_stopping=True,\n                         n_iter_no_change=100,\n                         solver='adam',\n                         shuffle=True,\n                         random_state=SEED)\n    model.fit(X_train,y_train)\n\n    mlp_oof[val_idx] = model.predict(X_valid)\n    mlp_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    \n    rmse = mean_squared_error(y_valid, mlp_oof[val_idx], squared=False)\n    print(f\"fold {fold} - mlp rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n    \n    start = time.time()\n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='rmse',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n    )\n\n    fi_tmp = pd.DataFrame()\n    fi_tmp['feature'] = model.feature_name_\n    fi_tmp['importance'] = model.feature_importances_\n    fi_tmp['fold'] = fold\n    fi_tmp['seed'] = SEED\n    feature_importances = feature_importances.append(fi_tmp)\n    \n    lgb_oof[val_idx] = model.predict(X_valid)\n    lgb_pred += model.predict(X_test) \/ N_SPLITS\n    \n    elapsed = time.time() - start\n    \n    rmse = mean_squared_error(y_valid, lgb_oof[val_idx], squared=False)\n    print(f\"fold {fold} - lgb rmse: {rmse:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nrmse = mean_squared_error(train[target], mlp_oof, squared=False)\nprint(f\"oof mlp rmse = {rmse:.6f}\")\n\nrmse = mean_squared_error(train[target], lgb_oof, squared=False)\nprint(f\"oof lgb rmse = {rmse:.6f}\")\n\nnp.save(\"mlp_oof.npy\", mlp_oof)\nnp.save(\"mlp_pred.npy\", mlp_pred)\n\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)","625afd8e":"def class_optimizer(X, a0, a1):\n    oof = X[0]*a0 + (1-X[0])*a1\n    return mean_squared_error(train[target], oof, squared=False)\n\nres = minimize(\n    fun=class_optimizer,\n    x0=[0.2],\n    args=tuple([mlp_oof, lgb_oof]),\n    method='Nelder-Mead',\n    options={'maxiter': 300})\n\nprint(res)\nprint(f\"coef0 {res.x[0]}, coef1 {1-res.x[0]}\")","ab573cb6":"ensemble_oof = res.x[0] * mlp_oof + (1-res.x[0]) * lgb_oof\nensemble_pred = res.x[0] * mlp_pred + (1-res.x[0]) * lgb_pred\n\nprint(mean_squared_error(train[target], ensemble_oof, squared=False))","4ab121d9":"submission['loss'] = ensemble_pred\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","7c4d6e6a":"# Datasets","ebb30302":"# LightGBM","570c5c4b":"# Submission","86dc59a0":"# Libraries","792dfd76":"# Ensemble","7b27f4dd":"# Parameters"}}