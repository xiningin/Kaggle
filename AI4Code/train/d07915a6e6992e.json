{"cell_type":{"6191da75":"code","3342ec49":"code","276128fc":"code","ba48057f":"code","9471a9c3":"code","e5a84990":"code","845d6b8f":"code","867adfd3":"code","343b6149":"code","ce3aefb5":"code","98427f39":"code","f3ed115f":"code","0848fa7d":"code","189d3fbf":"code","8aa67351":"code","7490cea5":"code","0f5e3fa9":"code","a9a4aac0":"code","20d42aed":"code","b8717e0b":"code","02f12d87":"code","fc8eb62a":"code","2042f677":"code","11376532":"code","c3eb4071":"code","70297081":"code","b734a692":"code","21c039f6":"code","54f8d2b8":"code","b0584b80":"code","b5fc3961":"code","f7e468ec":"code","afadf6ed":"code","4cf938e1":"code","169bd791":"code","2fa65eb2":"code","735fbc5d":"code","d38406ad":"code","9d891089":"code","e757c6d7":"code","e24b8c51":"code","ec27a9a3":"code","da64e9fe":"code","05680ee8":"code","dd977138":"code","d05b16a0":"code","8ea75290":"code","46b3ac9f":"code","22225cbf":"code","9ac0fe72":"code","cb2bd417":"code","48ccf143":"code","4e710c1e":"code","75ae01ca":"code","ba5cbad2":"code","7145ddbc":"code","fdca3316":"code","e7611c0d":"code","3e595c9f":"code","459b5c7d":"code","d7bb2ba1":"code","5fc6de2c":"code","eab66ae1":"code","b5c387f7":"code","8a826bf9":"code","a1695e06":"code","8f551662":"code","0f6f2169":"code","e4e52b23":"markdown","3e0f73c2":"markdown","9197a997":"markdown","b81363c3":"markdown","619b33e6":"markdown","6acd7215":"markdown","599abcf5":"markdown","e917aacf":"markdown","ee431fb6":"markdown","703544b0":"markdown","ec3dc192":"markdown","c01035e6":"markdown","f45d7bb7":"markdown","44c903ad":"markdown","4a2d6b1e":"markdown","a7f856c0":"markdown","a923c126":"markdown","bc96ebec":"markdown","aa1f6074":"markdown","88b2d29a":"markdown","a98cfd32":"markdown","918d0dbf":"markdown","fe507f12":"markdown","3e301219":"markdown","3815c798":"markdown","943b7a71":"markdown","719aa497":"markdown","6850caaa":"markdown","51479bde":"markdown","eec228fd":"markdown","8fd75098":"markdown","5597075e":"markdown","d88c1ba7":"markdown","7acff081":"markdown","c93b1bea":"markdown","f87321ba":"markdown","ba49c350":"markdown","e230e597":"markdown","d1fdf2c3":"markdown","c70ba251":"markdown","26fbc3bf":"markdown","faf1c2b3":"markdown","846d8343":"markdown","907cc342":"markdown","67b48bde":"markdown","17255350":"markdown","8eae5eab":"markdown","23957c8c":"markdown","6df8513e":"markdown","726d5a99":"markdown","cd0a069d":"markdown","84581c07":"markdown","1d75e989":"markdown","f4f300d6":"markdown","9c34f3b6":"markdown","b048dcf3":"markdown","d872a74e":"markdown","55bf27ec":"markdown","9d8cacad":"markdown","e1876504":"markdown","b469308e":"markdown","22671dfe":"markdown"},"source":{"6191da75":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","3342ec49":"# Import the basic python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nget_ipython().run_line_magic('matplotlib', 'inline')\nsns.set(style='white', context='notebook', palette='deep')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test[\"PassengerId\"]\ntrain.info()\ntest.info()","276128fc":"train.info() # We have 891 observations & 12 columns. See the mix of variable types.","ba48057f":"test.info() # We have 417 observations & 11 columns (no response 'Survived' column).","9471a9c3":"# Check missing values in train data set\ntrain_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]\nmiss_train = pd.DataFrame({'Train Missing Ratio' :train_na})\nmiss_train.head()\n","e5a84990":"# Check missing values in train data set\ntest_na = (test.isnull().sum() \/ len(test)) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)[:30]\nmiss_test = pd.DataFrame({'Test Missing Ratio' :test_na})\nmiss_test.head()\n\n","845d6b8f":"# Fill empty and NaNs values with NaN\ntrain = train.fillna(np.nan)\ntest = test.fillna(np.nan)","867adfd3":"# Analyze the count of survivors by Pclass\nax = sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\ntrain[['Pclass', 'Survived']].groupby(['Pclass']).count().sort_values(by='Survived', ascending=False)\n\n# Analyze the Survival Probability by Pclass\ng = sns.barplot(x=\"Pclass\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")\ntrain[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False)","343b6149":"# Count the number of passengers by gender\nax = sns.countplot(x=\"Sex\", hue=\"Survived\", data=train)\n# Analyze survival count by gender\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex']).count().sort_values(by='Survived', ascending=False)","ce3aefb5":"# Analyze the Survival Probability by Gender\ng = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False)","98427f39":"# Let's explore the distribution of age by response variable (Survived)\nfig = plt.figure(figsize=(10,8),)\naxis = sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='Survived')\naxis = sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='b',shade=True,label='Did Not Survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 20)\nplt.xlabel(\"Passenger Age\", fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12);","f3ed115f":"sns.lmplot('Age','Survived',data=train)\n\n# We can also say that the older the passenger the lesser the chance of survival","0848fa7d":"# Analyze the count of survivors by SibSP\n\nax = sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train)\ntrain[['SibSp', 'Survived']].groupby(['SibSp']).count().sort_values(by='Survived', ascending=False)","189d3fbf":"# Analyze probability of survival by SibSP\ng  = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False)","8aa67351":"# Analyze the count of survivors by Parch\n\nax = sns.countplot(x=\"Parch\", hue=\"Survived\", data=train)\ntrain[['Parch', 'Survived']].groupby(['Parch']).count().sort_values(by='Survived', ascending=False)","7490cea5":"# Analyze the Survival Probability by Parch\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False)","0f5e3fa9":"train['Ticket'].head()","a9a4aac0":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nsns.distplot(train['Fare'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['Fare'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare distribution')","20d42aed":"# Let's check the unique values\ntrain['Cabin'].unique()","b8717e0b":"# Analyze the count of survivors by Embarked variable\n\nax = sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)\ntrain[['Embarked', 'Survived']].groupby(['Embarked']).count().sort_values(by='Survived', ascending=False)","02f12d87":"# Analyze the Survival Probability by Embarked\n\ng  = sns.factorplot(x=\"Embarked\",y=\"Survived\",data=train,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked']).mean().sort_values(by='Survived', ascending=False)","fc8eb62a":"# Age, Pclass & Survival\n\nsns.lmplot('Age','Survived',data=train,hue='Pclass')","2042f677":"# Age, Embarked, Sex, Pclass\n\ng = sns.catplot(x=\"Age\", y=\"Embarked\",  hue=\"Sex\", row=\"Pclass\",   data=train[train.Embarked.notnull()], \norient=\"h\", height=2, aspect=3, palette=\"Set3\",  kind=\"violin\", dodge=True, cut=0, bw=.2)","11376532":"# Relation among Pclass, Gender & Survival Rate\n\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Pclass\", data=train, saturation=.5, kind=\"bar\", ci=None, aspect=.6)","c3eb4071":"# Relation among SibSP, Gender & Survival Rate\n\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"SibSp\", data=train, saturation=.5,kind=\"bar\", ci=None, aspect=.6)","70297081":"# Relation among Parch, Gender & Survival Rate\ng = sns.catplot(x=\"Sex\", y=\"Survived\", col=\"Parch\", data=train, saturation=.5,kind=\"bar\", ci=None, aspect=.6)","b734a692":"# Let's combining train & test for quick feature engineering. \n# Variable source is a kind of tag which indicates data source in combined data\n\ntrain['source']='train'\ntest['source']='test'\ncombdata = pd.concat([train, test],ignore_index=True)\nprint (train.shape, test.shape, combdata.shape)","21c039f6":"# PassengerID - Drop PassengerID\ncombdata.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","54f8d2b8":"# Pclass \ncombdata['Pclass'].unique()","b0584b80":"# Name - Extract Salutation from Name variable\n\nsalutation = [i.split(\",\")[1].split(\".\")[0].strip() for i in combdata[\"Name\"]]\ncombdata[\"Title\"] = pd.Series(salutation)\ncombdata[\"Title\"].value_counts()","b5fc3961":"# Name - Create 5 major categories & analyze the survival rate\n\ncombdata['Title'] = combdata['Title'].replace('Mlle', 'Miss')\ncombdata['Title'] = combdata['Title'].replace(['Mme','Lady','Ms'], 'Mrs')\ncombdata.Title.loc[ (combdata.Title !=  'Master') & (combdata.Title !=  'Mr') & \n                   (combdata.Title !=  'Miss')  & (combdata.Title !=  'Mrs')] = 'Others'\ncombdata[\"Title\"].value_counts()","f7e468ec":"# inspect the correlation between Title and Survived\ncombdata[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","afadf6ed":"# Create dummy variable \ncombdata = pd.get_dummies(combdata, columns = [\"Title\"])","4cf938e1":"# Fare - Check the number of missing value\ncombdata[\"Fare\"].isnull().sum()\n\n# Only 1 value is missing so we will fill the same with median\ncombdata[\"Fare\"] = combdata[\"Fare\"].fillna(combdata[\"Fare\"].median())\n\n# Use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n# combdata[\"Fare\"] = np.log1p(combdata[\"Fare\"])\n\n#Check the new distribution \n# sns.distplot(combdata['Fare'] , fit=norm);","169bd791":"# bin Fare into five intervals with equal amount of people\ncombdata['Fare-bin'] = pd.qcut(combdata.Fare,5,labels=[1,2,3,4,5]).astype(int)\n\n# inspect the correlation between Fare-bin and Survived\ncombdata[['Fare-bin', 'Survived']].groupby(['Fare-bin'], as_index=False).mean()","2fa65eb2":"## Fill Age with the median age of similar rows according to Sex, Pclass, Parch and SibSp\n# Index of NaN age rows\n# missing_index = list(combdata[\"Age\"][combdata[\"Age\"].isnull()].index)\n\n# for i in missing_index :\n#     median_age = combdata[\"Age\"].median()\n#     filled_age = combdata[\"Age\"][((combdata['Sex'] == combdata.iloc[i][\"Sex\"]) & \n#                                 (combdata['SibSp'] == combdata.iloc[i][\"SibSp\"]) & \n#                                 (combdata['Parch'] == combdata.iloc[i][\"Parch\"]) & \n#                                 (combdata['Pclass'] == combdata.iloc[i][\"Pclass\"]))].median()\n#     if not np.isnan(filled_age) :\n#         combdata['Age'].iloc[i] = filled_age\n#     else :\n#         combdata['Age'].iloc[i] = median_age","735fbc5d":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\ncombdata_temp = combdata[['Age','Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Others','Fare-bin','SibSp']]\n\nX  = combdata_temp.dropna().drop('Age', axis=1)\nY  = combdata['Age'].dropna()\nholdout = combdata_temp.loc[np.isnan(combdata.Age)].drop('Age', axis=1)\n\nregressor = RandomForestRegressor(n_estimators = 300)\n#regressor = GradientBoostingRegressor(n_estimators = 500)\nregressor.fit(X, Y)\ny_pred = np.round(regressor.predict(holdout),1)\ncombdata.Age.loc[combdata.Age.isnull()] = y_pred\n\ncombdata.Age.isnull().sum(axis=0) ","d38406ad":"bins = [ 0, 4, 12, 18, 30, 50, 65, 100] # This is somewhat arbitrary...\nage_index = (1,2,3,4,5,6,7)\n\ncombdata['Age-bin'] = pd.cut(combdata.Age, bins, labels=age_index).astype(int)\ncombdata[['Age-bin', 'Survived']].groupby(['Age-bin'],as_index=False).mean()","9d891089":"# Sex - Create dummy variables\ncombdata[\"Sex\"] = combdata[\"Sex\"].map({\"male\": 0, \"female\":1}) \n# combdata = pd.get_dummies(combdata, columns = [\"Sex\"])","e757c6d7":"# Create a variable representing family size from SibSp and Parch\ncombdata[\"Fsize\"] = combdata[\"SibSp\"] + combdata[\"Parch\"] + 1\n\n# Analyze the correlation between Family and Survived\ncombdata[['Fsize', 'Survived']].groupby(['Fsize'], as_index=False).mean()\n\n# Create new feature of family size\n# combdata['Single'] = combdata['Fsize'].map(lambda s: 1 if s == 1 else 0)\n# combdata['SmallF'] = combdata['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n# combdata['MedF'] = combdata['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n# combdata['LargeF'] = combdata['Fsize'].map(lambda s: 1 if s >= 5 else 0)","e24b8c51":"# Analyze the Survival Probability by Fsize\n\ncombdata.Fsize = combdata.Fsize.map(lambda x: 0 if x > 4 else x)\ng  = sns.factorplot(x=\"Fsize\",y=\"Survived\",data=combdata,kind=\"bar\", size = 7 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")\ng = g.set_xlabels(\"Family Size\")\ncombdata[[\"Fsize\", \"Survived\"]].groupby(['Fsize']).mean().sort_values(by='Survived', ascending=False)","ec27a9a3":"# SibSp - Drop the variable\ncombdata = combdata.drop(labels='SibSp', axis=1)","da64e9fe":"# Parch - Drop the variable\ncombdata = combdata.drop(labels='Parch', axis=1)","05680ee8":"# Ticket - Extracting the ticket prefix. This might be a representation of class\/compartment.\n\ncombdata.Ticket = combdata.Ticket.map(lambda x: x[0])\n\n# inspect the correlation between Ticket and Survived\ncombdata[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean()","dd977138":"# Let's look at the number of people for each type of tickets\ncombdata['Ticket'].value_counts()","d05b16a0":"combdata['Ticket'] = combdata['Ticket'].replace(['A','W','F','L','5','6','7','8','9'], '4')\n\n# check the correlation again\ncombdata[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean()","8ea75290":"# Create dummy variables\ncombdata = pd.get_dummies(combdata, columns = [\"Ticket\"], prefix=\"T\")","46b3ac9f":"# Cabin - Replace the missing Cabin number by the type of cabin unknown 'U'\ncombdata[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'U' for i in combdata['Cabin'] ])","22225cbf":"# Let's plot the survival probability by Cabin\ng  = sns.factorplot(x=\"Cabin\",y=\"Survived\",data=combdata,kind=\"bar\", size = 7 ,\n                    palette = \"muted\",order=['A','B','C','D','E','F','G','T','U'])\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","9ac0fe72":"combdata = combdata.drop(labels='Cabin', axis=1)","cb2bd417":"combdata = combdata.drop(labels='Embarked', axis=1)","48ccf143":"# Drop the variables we don't need\n\ncombdata =combdata.drop(labels=['Age', 'Fare', 'Name'],axis = 1)","4e710c1e":"combdata.info()","75ae01ca":"# Import the required libraries\nfrom sklearn.svm import SVC\nfrom collections import Counter\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier","ba5cbad2":"## Separate train dataset and test dataset using the index variable 'source'\n\ntrain = combdata.loc[combdata['source']==\"train\"]\ntest = combdata.loc[combdata['source']==\"test\"]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)\ntrain.drop(labels=[\"source\"],axis = 1,inplace=True)\ntest.drop(labels=[\"source\"],axis = 1,inplace=True)\n\n# You may want to drop some variables to avoid dummy variable trap\n# test.drop(labels=['source','Sex_male', 'Fsize', 'LargeF', 'SibSp_8','Parch_9','T_WEP','Cabin_T','Emb_Q'],axis = 1,inplace=True)\ntest.shape","7145ddbc":"## Separate train features and label \n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\nY_train = train[\"Survived\"]\nX_train = train.drop(labels = [\"Survived\"],axis = 1)\n\n# You may want to drop some variables to avoid dummy variable trap\n# X_train = train.drop(labels = [\"Survived\", 'Sex_male', 'Fsize', 'LargeF', 'SibSp_8','Parch_9','T_WEP','Cabin_T','Emb_Q'],axis = 1)\nX_train.shape","fdca3316":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","e7611c0d":"# Modeling differents algorithms. \n\nrandom_state = 2\nclassifiers = []\n\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\n                       \"Algorithm\":[\"SVC\",\n                                    \"AdaBoost\",\n                                    \"ExtraTrees\",\n                                    \"KNeighboors\",\n                                    \"DecisionTree\",\n                                    \"RandomForest\",\n                                    \"GradientBoosting\",\n                                    \"LogisticRegression\",\n                                    \"MultipleLayerPerceptron\",\n                                    \"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","3e595c9f":"cv_res","459b5c7d":"# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n                  \"base_estimator__splitter\" :   [\"best\", \"random\"],\n                  \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n                  \"n_estimators\" :[1,2],\n                  \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsadaDTC.fit(X_train,Y_train)\nada_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","d7bb2ba1":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsExtC.fit(X_train,Y_train)\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","5fc6de2c":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsRFC.fit(X_train,Y_train)\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","eab66ae1":"# Gradient boosting \nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGBC.fit(X_train,Y_train)\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","b5c387f7":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(X_train,Y_train)\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","8a826bf9":"# Feature importance\nnrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),\n                     (\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","a1695e06":"# Concatenate all classifier results\ntest_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\ng= sns.heatmap(ensemble_results.corr(),annot=True)","8f551662":"# Use voting classifier to combine the prediction power of all models\nvotingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X_train, Y_train)  ","0f6f2169":"# Predict and export the results\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\nresults = pd.concat([IDtest,test_Survived],axis=1)\nresults.to_csv(\"Final Submission File.csv\",index=False)","e4e52b23":"# Model Evaluation\n![model.png](attachment:model.png)","3e0f73c2":"**Title**","9197a997":"**Support Vector Machines**\n\nSVM builds a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize the error. The idea behind SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.","b81363c3":"**Embarked**\n\nC = Cherbourg, Q = Queenstown, S = Southampton\n\nLet's explore the variable with Survival rate. Embarked represents port of embarkation. As the analysis output below suggests Emabrked C shows high probabilities of survival.","619b33e6":"# Problem Identification","6acd7215":"**Creating Family Size variable using SibSp & Parch**","599abcf5":"**Cross Validation Strategy**\n![CV.png](attachment:CV.png)\n\nCross Validation is one of the most powerful tool in Data Scientist's tool box. It helps you to understand the performance of your model and fight with overfitting. As we all know that Learning the model parameters and testing it on the same data is a big mistake. Such a model would have learned everything about the training data and would give result in a near perfect test score as it has already seen the data. The same model would fail terribly when tested on unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. \n\nThe general approach is as follows:\n\n1. Split the dataset into k groups\n2. For each unique group:\n        a. Kee one group as a hold out or test data set\n        b. Use the remaining groups as training data set\n        c. Build the model on the training set and evaluate it on the test set\n        d. Save the evaluation score \n3. Summarize the performance of the model using the sample of model evaluation scores\n\nYou can access following link and read about Cross Validation in detail.\n\nhttps:\/\/medium.com\/datadriveninvestor\/k-fold-cross-validation-6b8518070833\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/","e917aacf":"**I will be trying the classic \"LESS IS MORE\" approach.**\n\nIn my previous versions, I have used \"**ALL In**\" approach - using all variables with feature engineering for prediction. Please refer to versions 25 or earlier to review the same.","ee431fb6":"\n\nOne important aspect of machine learning is to ensure that the variables show almost the same trend across train & test data. If not, it would lead to overfitting because model is representing a relationship which is not applicable in the test dataset. \n\nI will give you one example here. As we do variable analysis, try to replicate (wherever applicable) the code for test data and see if there is any major difference in data distribution. \n\n**Example** - Let's start with finding the number of missing values. If you compare the output you will see that missing value percentages do not vary much across train & test datasets.\n\nUse the groupby\/univariate\/bivariate analysis method to compare the distribution across Train & Test data","703544b0":"**Gradient Boosting**\n\nGradient boosting is one of the most powerful techniques for building predictive models. Boosting is a method of converting weak learners into strong learners by building an additive model in a forward stage-wise fashion. In boosting, each new tree is a fit on a modified version of the original data set.","ec3dc192":"**Cabin**","c01035e6":"**Name**","f45d7bb7":"**If you liked this notebook and found this notebook helpful, Please upvote and leave a comment**\n![Good%20Bye.png](attachment:Good%20Bye.png)","44c903ad":"Tickets are of 2 types here.\n\nType 1 has only number and Type 2 is a combination of some code followed by the number. Let's extract the first digit and compare it with survival probability.","4a2d6b1e":"# Feature engineering\n![FE.png](attachment:FE.png)","a7f856c0":"**Additional analysis**\n\nLet's create few additional charts to see how different variables are related.","a923c126":"**PassengerID**","bc96ebec":"**Random Forest Classifier**\n\nSimilar to Extra Tree Classifier a Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n\nHow ET differes from RF - \n\n1) When choosing variables at a split, samples are drawn from the entire training set instead of a bootstrap sample of the training set.\n\n2) Splits are chosen completely at random from the range of values in the sample at each split.","aa1f6074":"\n\nWhat we need to do to process following variables  - \n\n**PassengerID** - No action required\n\n**PClass** - Have only 3 numerical values. We will use it as it is.\n\n**Name** - Can be used to create new variable Title by extracting the salutation from name.\n\n**Sex** - Create dummy variables\n\n**Age** - Missing value treatment, followed by creating dummy variables\n\n**SibSP** - Drop the variable\n\n**Parch** - Drop the variable as most of the values are 0\n\n**Ticket** - Create dummy variables post feature engineering\n\n**Fare** - Missing value treatment followed by log normalization\n\n**Cabin** - Create dummy variables post feature engineering\n\n**Embarked** - Create dummy variables","88b2d29a":"**Cabin**\n\nAlphanumeric variable. \n\n687 missing values in train & 327 missing values in test data - which needs to be treated. We can create more features using this Cabin variable. ","a98cfd32":"\n\nTitle, Sex_Female, Fare & PClass seems to be common features preferred for classification.\n\nWhile Title & Age feature represents the Age category of passengers the features like Fare, PClass, Cabin etc. represents the economic status. Based on findings we can conclude that Age, Gender & features representing social\/economic status were primary factors affecting the survival of passenger.\n","918d0dbf":"**Age**\n\n![Age.jpg](attachment:Age.jpg)\nThe insight below connects back to \"Ladies and Kids First\" scene of the movie. It shows that a good number of babies & young kids survived.","fe507f12":"# A Thank You Note..!!!\n\nWherever whenever required I have given due credit to my fellow Kagglers which they deserve for their hard work.Special thanks to Yassine. I tried various ML algorithms but I found the voting method most useful based on the variables I had. ","3e301219":"Most of these tickets belong to category 1, 2, 3, S, P, C. Based on value counts and average survival, we can put all other ticket categories into a new category '4'.\n","3815c798":"**Ticket**\n\nThis variable has alphanumeric value which might not be related to Survival directly but we can use this variable to create some additional features.","943b7a71":"Approximately 62% of Pclass = 1 passenger survived followed by 47% of Pclass2.","719aa497":"Let's plot feature importance by various algorithms","6850caaa":"**Embarked**","51479bde":"\n![Data.jpg](attachment:Data.jpg)\n\n\nLet's import necessary libraries & bring in the datasets in Python environment first. Once we have the datasets in Python environment we can slice & dice the data to understand what we have and what is missing.","eec228fd":"# Conclusion\n![Conclusion.png](attachment:Conclusion.png)","8fd75098":"**Sex**\n\nBased on analysis below, female had better chances of survival. \n\n![](https:\/\/www.ajc.com\/rf\/image_large\/Pub\/p9\/AJC\/2018\/07\/12\/Images\/newsEngin.22048809_071418-titanic_Titanic-Image-7--2-.jpg)","5597075e":"**Parch**\n\nParch indicates number of parents \/ children aboard the Titanic. Note that Parch = 3 and Parch = 1 shows higher survival probabilities. ","d88c1ba7":"**Fare**","7acff081":"\n\nEvaluating multiple models using GridSearch optimization method. \n\nHyper-parameters are key parameters that are not directly learnt within the estimators. We have to pass these as arguments. Different hyper parameters can result in different model with varying performance\/accuracy. To find out what paparmeters are resulting in best score, we can use Grid Search method and use the optimum set of hyper parameters to build and select a good model.\n\nA search consists of:\n\n1. an estimator (regressor or classifier)\n2. a parameter space;\n3. a method for searching or sampling candidates;\n4. a cross-validation scheme; and\n5. a score function.","c93b1bea":"# Creating a Model","f87321ba":"**AdaBoost classifier** \n\nAdaboost begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","ba49c350":"**SibSp**","e230e597":"**ExtraTrees Classifier**\n\nET is a meta estimator that fits a number of randomized decision trees on various sub-samples of the dataset and then uses averaging method to improve the predictive accuracy and control over-fitting.","d1fdf2c3":"**Sex**","c70ba251":"Now we have the training and test datasets available and we can start training the model. We will build couple of base models and then will use Grid Search method to optimize the parameters. There are several classification you can select.\nWe are trying following to develop a baseline - \n\n        1. K Nearest Neighbour\n        2. Linear Discriminant Analysis\n        3. Support Vector Classifier\n        4. Multi-layer Perceptron classifier\n        5. Extra Trees Classifier\n        6. Logistic Regression\n        7. Decision Trees\n        8. Random Forest\n        9. Gradient Boosting Classifier\n        10. AdaBoost Classifier\n","26fbc3bf":"**Name**\n\nNot relevant from analysis & modeling perspective. We will drop this feature later after creating a new variable as Title.","faf1c2b3":"**Pclass**","846d8343":"The Fare variable is right skewed. We need to transform this variable using log function and make it more normally distributed. We will do this during feature engineering process.","907cc342":"**Pclass**\n\nPclass is categorical variable. Let's look at the distribution.","67b48bde":"**2nd approach to treat the Age feature**","17255350":"**SibSP**\n\nThis variable refers to number of siblings\/spouse onboard. SibSP = 1 and SibSP = 2 shows higher chances of survival.","8eae5eab":"There are 2 ways of handling the missing age values.\n\n* Fill the age with median age of similar rows according to Sex, Pclass, Parch & SibSP\n* or use a quick machine learning algorithm to predict the age values based on Age, Title, Fare & SibSP\n\nI used both of them to test which one works better. One of the code will be markdown to avoid confusion.","23957c8c":"Based on data above, female passengers had better chances of survival than male passengers","6df8513e":"# Article on medium publication\n\n\nI also wrote an article on medium on the same topic. You can [click this clink](https:\/\/medium.com\/@rp1611\/model-ensembles-for-survival-prediction-a3ecc9f7c2ae) and access the blog. Please leave comments\/feedback. It would help me improve.\n","726d5a99":"# What would be the workflow?\n\nI will keep it simple & crisp rather than using buzz words & useless data science frameworks. Frankly speaking no one cares. \n\nThis will help you to stay on track. So here is the workflow.\n\n**Problem Identification**\n\n**What data do we have?**\n\n**Exploratory data analysis**\n\n**Data preparation including feature engineering**\n\n**Developing a model**\n\n**Model evaluation**\n\n**Conclusions**\n\nThat's all you need to solve a data science problem.","cd0a069d":"This section of code is for missing value treatment for age. Instead of directly replacing the missing values with the median value of complete data, the script looks for nearby data which is similar in terms of Sex, Pclass, Parch and SibSp and then takes the median values of those observations.\n\nFor example, if my salary information is missing, you can't simply replace the missing value with median salary of my whole organization. You will look for people with similar experience, type of work, department, etc and then will use the median salary of such a group.","84581c07":"# What data do we have?","1d75e989":"# Exploratory data analysis\n![analysis.png](attachment:analysis.png)","f4f300d6":"**Ticket**","9c34f3b6":"The guided approach explained here will help you to understand how you should design and approach Data Science problems. Though there are many ways to do the same analysis, I have used the codes which I found more efficient and helpful.\n\nThe idea is just to show you the path, try your own ways and share the same with others.","b048dcf3":"**PassengerId**\n\nNot relevant from modeling perspective so we will drop this variable later","d872a74e":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","55bf27ec":"**Voting Classifier**\n\nVoting is one of the simplest method of combining the predictions from multiple machine learning models. It is not an actual classifier but a wrapper for set of different ones that are trained and valuated in parallel in order to exploit the different peculiarities of each algorithm. Here we are combining the predictions from  models that we built and predict based on votes.","9d8cacad":"**Age**","e1876504":"**Fare**\n\nLet's check the distribution first.","b469308e":"**Parch**","22671dfe":" \n![Prob%20Ident.png](attachment:Prob%20Ident.png)\n**Best Practice -** The most important part of any project is correct problem identification. Before you jump to \"How to do this\" part like typical Data Scientists, understand \"What\/Why\" part.  \nUnderstand the problem first and draft a rough strategy on a piece of paper to start with. Write down things like what are you expected to do & what data you might need or let's say what all algorithms you plan to use. \n\nNow the <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/\"> Titanic challenge<\/a>  hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing  age, sex, or passenger's class on the boat.\n\n![](http:\/\/www.tyro.com\/content\/uploads\/2016\/04\/blog-twenty-one-business-icebergs-sink-business-280416.jpg)\n\nSo it is a classification problem and you are expected to predict Survived as 1 and Died as 0."}}