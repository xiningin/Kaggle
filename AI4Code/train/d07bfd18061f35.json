{"cell_type":{"4d5901d6":"code","3969f310":"code","2576d071":"code","b66206a3":"code","721b891e":"code","0b996614":"code","f0d49604":"code","925a8124":"code","5765f5ca":"code","2f5fff01":"code","a2bcf25d":"code","f7b4b29c":"code","0111f5a7":"code","0aeae4bf":"markdown","07aa936f":"markdown","8c4d53b6":"markdown","5350db1d":"markdown","f9112698":"markdown","502f3838":"markdown","646db462":"markdown","c333fd76":"markdown","25efb898":"markdown"},"source":{"4d5901d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3969f310":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\npd.set_option('display.max_columns', 10)\ntrain_df.head()","2576d071":"train_df.info()","b66206a3":"# cat_cols = [col for col in train_df.columns if train_df[col].dtype!='float64']\n\n# # Get number of unique entries in each column with categorical data\n# object_nunique = list(map(lambda col: train_df[col].nunique(), cat_cols))\n# d = dict(zip(cat_cols, object_nunique))\n\n# # Print number of unique entries by column, in ascending order\n# sorted(d.items(), key=lambda x: x[1])","721b891e":"pd.set_option('max_rows', None)\nu_df = train_df.nunique().sort_values()\nprint(f'All columns with their unique values:\\n {u_df}\\n\\n')\ncol_could_encode = [col for col in train_df.columns if train_df[col].nunique() < 30]\nc_df = train_df[col_could_encode].nunique().sort_values()\nprint(f'Columns that can be encoded with their unique values:\\n {c_df}')","0b996614":"for index in c_df.index:\n    print(train_df[index].value_counts())","f0d49604":"dropped_columns = ['Street', 'Alley', 'Utilities', 'CentralAir', 'BsmtHalfBath', 'PavedDrive', 'PoolQC',\n                   'LandSlope', 'MiscFeature', 'Fence', 'BsmtCond', 'LandContour', 'KitchenAbvGr',  'FireplaceQu',\n                  'Electrical', 'ExterCond', 'GarageQual', 'GarageCond', 'BldgType', 'Heating', 'BsmtFinType2',\n                  'Functional', 'Condition2', 'RoofMatl', 'PoolArea', 'RoofMatl', 'SaleType', '3SsnPorch',\n                  'MiscVal', 'LowQualFinSF']\ncol_could_encode_remain = [col for col in col_could_encode if col not in dropped_columns]\ncol_could_encode_remain","925a8124":"import seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\n\ncolumns = 4\nsubplots = int(len(col_could_encode_remain))\nrows = math.ceil(subplots\/columns)\n\nfig, ax = plt.subplots(rows, columns, sharey=True, figsize=(15,30))\nfig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.2)\n\n\nfor i, x in zip(range(subplots), col_could_encode_remain):\n    j = i % columns\n    sns.scatterplot(data=train_df, x=x, y='SalePrice', ax=ax[int((i-j)\/columns),j]).tick_params(axis='x', rotation=90)\n","5765f5ca":"object_cols = [col for col in col_could_encode_remain if train_df[col].dtype == 'object']\nint_cols = [col for col in col_could_encode_remain if col not in object_cols]","2f5fff01":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nnumerical_transformer = SimpleImputer(strategy='constant')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown=\"ignore\")),\n])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', numerical_transformer, int_cols),\n        ('cat', categorical_transformer, object_cols)\n    ]\n)\n\nmodel = RandomForestRegressor(n_estimators=100)\n\nmy_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)\n])","a2bcf25d":"from sklearn.model_selection import cross_val_score\n\n\nX = train_df[col_could_encode_remain]\ny = train_df.SalePrice\n\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                             cv=5,\n                             scoring='neg_mean_absolute_error')\n\nscores","f7b4b29c":"X_test = test_df[col_could_encode_remain]\nmy_pipeline.fit(X, y)\npreds = my_pipeline.predict(X_test)\npreds","0111f5a7":"output = pd.DataFrame({'Id': train_df.index[:1459], 'SalePrice':preds})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0aeae4bf":"# Prediction","07aa936f":"# Droping useless features","8c4d53b6":"# Submission","5350db1d":"# Plotting the scatter plot of mentioned column","f9112698":"# Validation","502f3838":"\n\n# Show a brife summary of data","646db462":"# Read the Data","c333fd76":"\n# Examine the features\n\nFirst of all we will dig into columns with dtype of object\n","25efb898":"# Impute, Encode and Model"}}