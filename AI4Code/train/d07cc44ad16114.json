{"cell_type":{"6e0ceba7":"code","f3af406a":"code","dfc071ac":"code","080e8b90":"code","ac6a8986":"code","b6e88860":"code","0432a9df":"code","b1cd16e1":"code","5be6ef0f":"code","44800ee4":"code","0dbf297e":"markdown","fdc03995":"markdown","dd3945c4":"markdown","d295ce2e":"markdown","980762ea":"markdown","3275fdce":"markdown","fb85c48c":"markdown","af2b98e0":"markdown","3c1285c8":"markdown","fcc714ac":"markdown","f32773aa":"markdown","7ad80530":"markdown","1eb89dd2":"markdown","cd291d90":"markdown","0892946b":"markdown","4ede2f5f":"markdown","7b4d1407":"markdown"},"source":{"6e0ceba7":"import torch\nimport torchvision\nimport pandas as pd\nimport os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","f3af406a":"class wheatdataset(torch.utils.data.Dataset):\n    def __init__(self,root,folder='train',transforms=None):\n        self.transforms=[]\n        if transforms!=None:\n            self.transforms.append(transforms)\n        self.root=root\n        self.folder=folder\n        box_data=pd.read_csv(os.path.join(root, \"train.csv\"))\n        self.box_data=pd.concat([box_data,box_data.bbox.str.split('[').str.get(1).str.split(']').str.get(0).str.split(',',expand=True)],axis=1)\n        self.imgs=list(os.listdir(os.path.join(root, self.folder)))\n    def __len__(self):\n        return len(self.imgs)\n        \n    def __getitem__(self,idx):\n        img_path=os.path.join(os.path.join(self.root,self.folder),self.imgs[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        df=self.box_data[self.box_data['image_id']==self.imgs[idx].split('.')[0]]\n        if df.shape[0]!=0:\n            df[2]=df[0].astype(float)+df[2].astype(float)\n            df[3]=df[1].astype(float)+df[3].astype(float)\n            boxes=df[[0,1,2,3]].astype(float).values\n            labels=np.ones(len(boxes))\n        else:\n            boxes=np.asarray([[0,0,0,0]])\n            labels=np.ones(len(boxes))\n        for i in self.transforms:\n            img=i(img)\n            \n        targets={}\n        targets['boxes']=torch.from_numpy(boxes).double()\n        targets['labels']=torch.from_numpy(labels).type(torch.int64)\n        #targets['id']=self.imgs[idx].split('.')[0]\n        return img.double(),targets","dfc071ac":"root='..\/input\/global-wheat-detection'\ndataset=wheatdataset(root,'train',transforms=torchvision.transforms.ToTensor())","080e8b90":"dataset[0]","ac6a8986":"torch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset_train = torch.utils.data.Subset(dataset, indices[:-2500])\ndataset_test = torch.utils.data.Subset(dataset, indices[-2500:])\ndata_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=4, shuffle=True,collate_fn=lambda x:list(zip(*x)))\ndata_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=False,collate_fn=lambda x:list(zip(*x)))","b6e88860":"images,labels=next(iter(data_loader_train))\nfrom matplotlib import patches\ndef view(images,labels,k,std=1,mean=0):\n    figure = plt.figure(figsize=(30,30))\n    images=list(images)\n    labels=list(labels)\n    for i in range(k):\n        out=torchvision.utils.make_grid(images[i])\n        inp=out.cpu().numpy().transpose((1,2,0))\n        inp=np.array(std)*inp+np.array(mean)\n        inp=np.clip(inp,0,1)  \n        ax = figure.add_subplot(2,2, i + 1)\n        ax.imshow(images[i].cpu().numpy().transpose((1,2,0)))\n        l=labels[i]['boxes'].cpu().numpy()\n        l[:,2]=l[:,2]-l[:,0]\n        l[:,3]=l[:,3]-l[:,1]\n        for j in range(len(l)):\n            ax.add_patch(patches.Rectangle((l[j][0],l[j][1]),l[j][2],l[j][3],linewidth=2,edgecolor='w',facecolor='none')) \n\nview(images,labels,4)","0432a9df":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nnum_classes = 2  # 1 class (person) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nmodel=model.to(device)\n\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01)","b1cd16e1":"model.train()\nfrom tqdm.notebook import tqdm\nfor epoch in tqdm(range(1)):\n    for images,targets in tqdm(data_loader_train):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        model=model.double()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        losses.backward()\n\n        optimizer.zero_grad()\n        optimizer.step()\n        \n    print(\"Loss = {:.4f} \".format(losses.item()))\n\ntorch.save(model.state_dict(), '.\/model.pth')","5be6ef0f":"model.load_state_dict(torch.load('.\/model.pth'))","44800ee4":"images,targets=next(iter(data_loader_test))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nmodel.eval()\noutput=model(images)\n\nwith torch.no_grad():\n    view(images,output,2)","0dbf297e":"### Since the model is heavy and it takes lots of time to train. I've trained the model on GPU and saved it as model.pth. Lets load the trained model.","fdc03995":"## And then create the dataset object from wheatdataset class.","dd3945c4":"For this problem, we will be finetuning a pre-trained Faster R-CNN model for wheat head detection in the images.  The reference can be found at the following link.\n[Torchvision Referece](https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html)","d295ce2e":"As we can see, the dataset returns the images, boxes and corresponding labels in tensor format. \n\n**Note:** Here the labels only contains ones as one class is there i.e the wheat heads.","980762ea":"# Defining the Dataset ","3275fdce":"# Time for some model building!!! ","fb85c48c":"# Thank you!! ","af2b98e0":"## Lets define the custom dataset for our wheathead images:","3c1285c8":"We'll create a dataset to read the data and corresponding boxes and labels from the train folder .The dataset should inherit from the standard torch.utils.data.Dataset class, and implement \\_\\_len__ and \\_\\_getitem__.\n\nThe only specificity that we require is that the dataset \\_\\_getitem__ should return:\n\n1. **image**: an image tensor of size (H, W)\n2. **target**: a dict containing the following fields\n    1. boxes (FloatTensor[N, 4]): the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n    2. labels (Int64Tensor[N]): the label for each bounding box. 0 represents always the background class.","fcc714ac":"## Lets train the model ","f32773aa":"# Faster R-CNN Model for Detection of wheat heads in the Images","7ad80530":"## Lets see how our images looks like with the bounding boxes: ","1eb89dd2":"## Looks like more training is needed as very few wheat heads are detected. Will upload better trained model in coming days. ","cd291d90":"![image.png](attachment:image.png)","0892946b":"### We'll take pretrained Faster R-CNN model from torchvision and change the roi head according to our needs","4ede2f5f":"# Take a look at the test images after training:","7b4d1407":"## Lets split train and test data and create Dataloaders to load data in batches:"}}