{"cell_type":{"1154cb5b":"code","b1835184":"code","19ad0e4f":"code","45df2d80":"code","d49bb92b":"code","69237a66":"code","6ed678cf":"code","07d346ba":"code","648a6dda":"code","20d0f45e":"code","64c225be":"code","f06b4d63":"code","735b9882":"code","145cbbfe":"code","73ae5e70":"code","3c2522eb":"code","968552e8":"code","95567e8d":"code","20229c75":"code","e88a93c7":"code","c5433b1c":"code","4a86ccc4":"code","f8496153":"code","ded4d8b2":"code","a144bf71":"code","89a5beca":"code","7164e7cd":"code","455359df":"code","7465788e":"markdown","47365bda":"markdown","42727094":"markdown","bb10888d":"markdown","1b1aa1aa":"markdown","21f67115":"markdown","dba5c38c":"markdown","2265d8c8":"markdown","71fb7964":"markdown","944fda6d":"markdown","1c8f9723":"markdown","473177ed":"markdown","d9c6b28f":"markdown","979d67f6":"markdown","69ec58e7":"markdown","e7c3a0dc":"markdown","4f89deaa":"markdown","70f19a4d":"markdown","448e4937":"markdown","f8e3d6de":"markdown","358600e0":"markdown","f5b97aea":"markdown","d5904a99":"markdown","f8072094":"markdown","1fb734b9":"markdown","43064d65":"markdown","110bca36":"markdown","eb28a5ba":"markdown","a849675e":"markdown","6483bdca":"markdown","9a962f41":"markdown"},"source":{"1154cb5b":"#Importing Libraries\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom datetime import datetime\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nsns.set_theme()","b1835184":"df = pd.read_csv('\/kaggle\/input\/amazon-product-reviews\/ratings_Electronics (1).csv', names = ['User ID','Product ID','Rating','Time Stamp'])\ndf['Time Stamp'] = df['Time Stamp'].apply(lambda x:datetime.fromtimestamp(x))\ndf = df.rename(columns={'Time Stamp':'Date'})\ndf.head()","19ad0e4f":"df['Date'].dt.hour.value_counts()","45df2d80":"df['Date'] = df['Date'].dt.date\r\ndf['Date'] = pd.to_datetime(df['Date'], format = \"%Y-%m-%d\")\r\ndf.head()","d49bb92b":"print('Rows     :',df.shape[0])\r\nprint('Columns  :',df.shape[1])\r\nprint('\\nNumber of Different Users: ', df['User ID'].nunique())\r\nprint('\\nNumber of Different Products: ', df['Product ID'].nunique())\r\nprint('\\nFeatures :\\n     :',df.columns.tolist())\r\nprint('\\nMissing values    :',df.isnull().values.sum())\r\nprint('\\nUnique values :\\n',df.nunique())\r\nprint('\\nMissing values in columns:\\n',df.isnull().sum())\r\nprint('\\nDuplicated rows: \\n',df.duplicated().value_counts())\r\nprint('\\nData Types of each column: \\n', df.dtypes)","69237a66":"plt.figure(figsize=(15,6))\r\nsns.countplot(data = df, x = 'Rating', palette = 'Blues')\r\nplt.title('Count of Ratings',fontdict={'fontsize':20});","6ed678cf":"df.groupby('User ID').count().describe()","07d346ba":"plt.figure(figsize=(15,6))\r\nplt.plot((df.groupby('Rating').count()\/df.shape[0])*100, color='blue')\r\nplt.title('Percentage of Ratings', fontdict={'fontsize':20})\r\nplt.xlabel('Rating')\r\nplt.ylabel('Percentage');","648a6dda":"df.groupby('Product ID')['Rating'].count().sort_values(ascending=False)","20d0f45e":"plt.figure(figsize=(15,6))\r\ndf.groupby('Product ID')['Rating'].count().hist(bins=300)\r\nplt.title('Number of Times Each Product Was Rated', fontdict={'fontsize':'20'})\r\nplt.xlabel('Number of Times Rated')\r\nplt.ylabel('Number of Products');","64c225be":"# Only including those users which gave more than 50 readings\r\nnew_df = df.groupby('Product ID').filter(lambda x:x['Rating'].count() >= 60)","f06b4d63":"new_df_rating_count = pd.DataFrame(new_df.groupby('Product ID')['Rating'].mean())\r\nnew_df_rating_count['Count'] = new_df.groupby('Product ID')['Rating'].count()\r\nnew_df_rating_count","735b9882":"sns.jointplot('Rating','Count',data=new_df_rating_count);","145cbbfe":"#Set the time stamp as the index\r\ndf.reset_index(drop=True,inplace=True)\r\ndf.sort_values('Date', ascending=True,inplace=True)\r\ndf.set_index('Date',inplace=True)\r\ndf.head()","73ae5e70":"df_timeseries = df.loc['2006-01-01':]\r\ndf_timeseries","3c2522eb":"df_timeseries.groupby('Date')['User ID'].count()\r\nplt.figure(figsize=(15,6))\r\nplt.plot(df_timeseries.groupby('Date')['User ID'].count())\r\ndf_timeseries.groupby('Date')['User ID'].count().rolling(window=30).mean().plot()\r\nplt.title('Number of Orders Made per Day From 2006 - 2014', fontdict={'fontsize':20})\r\nplt.legend(['Number of Orders','Mean']);","968552e8":"#Creating month and year column\r\ndf_timeseries['Month'] = df_timeseries.index.month\r\ndf_timeseries['Year'] = df_timeseries.index.year","95567e8d":"# Grabbing values for only 2013\r\ndf_timeseries_2013 = df_timeseries[df_timeseries.index.year == 2013]\r\n\r\nplt.figure(figsize=(15,6))\r\ndf_timeseries_2013.groupby('Date')['User ID'].count().plot()\r\ndf_timeseries_2013.groupby('Date')['User ID'].count().rolling(window=7).mean().plot()\r\nplt.title('Number of Orders per Day in 2013', fontdict={'fontsize':20})\r\nplt.legend(['Number of Orders','Mean']);","20229c75":"# Getting day of week\r\ndf_timeseries_2013['Day of Week'] = df_timeseries_2013.index.weekday\r\ndf_timeseries_2013['Day of Week Name'] = df_timeseries_2013['Day of Week']\r\n\r\n# Converting day of week numbers to names\r\ndf_timeseries_2013['Day of Week Name'] = df_timeseries_2013['Day of Week Name'].replace([0,1,2,3,4,5,6], ['Mon','Tue','Wed','Thurs','Fri','Sat','Sun'])\r\n\r\n# Grouping to get the number of orders in a week per month of 2013\r\ndf_timeseries_2013_grouped = df_timeseries_2013.groupby(['Month','Day of Week','Day of Week Name']).count().reset_index()\r\ndf_timeseries_2013_grouped = df_timeseries_2013_grouped.sort_index(level='Day of Week')\r\nprint(df_timeseries_2013_grouped)\r\n\r\n#Plotting number of orders in a week for 2013\r\nplt.figure(figsize=(15,6))\r\nfor i in df_timeseries_2013_grouped['Month'].unique():\r\n    plt.plot('User ID', data = df_timeseries_2013_grouped[df_timeseries_2013_grouped['Month'] == i].set_index('Day of Week Name'))\r\n\r\nplt.title('Number of Orders Made Throughout the Week', fontdict={'fontsize':20})\r\nplt.legend(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], loc=(1,0.3));","e88a93c7":"#Importing Libraries\r\nimport datetime\r\nfrom surprise import SVD\r\nfrom surprise import SlopeOne\r\nfrom surprise import KNNWithMeans\r\nfrom surprise import NormalPredictor\r\nfrom surprise import accuracy\r\nfrom surprise import Reader\r\nfrom surprise import Dataset\r\nfrom surprise.model_selection import KFold\r\nfrom surprise.model_selection import cross_validate\r\nfrom surprise.model_selection import GridSearchCV","c5433b1c":"#Showing a preview of the data frame\r\ndata = df.reset_index(drop=True)\r\ndata.head()","4a86ccc4":"#Reading and preparing the data\r\ndata = df.reset_index(drop=True)\r\nreader = Reader(rating_scale=(1,5))\r\ndata = Dataset.load_from_df(data,reader)\r\n\r\n#Start time\r\nstart_time = datetime.datetime.now()\r\n\r\n#Running K Fold Cross Validation\r\ncross_validate(NormalPredictor(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True);\r\n\r\n#End time\r\nend_time = datetime.datetime.now()\r\nprint('\\nTotal time taken: \\n', end_time - start_time)","f8496153":"#Reading and preparing the data\r\ndata = df.iloc[0:30000].reset_index(drop=True)\r\nreader = Reader(rating_scale=(1,5))\r\ndata = Dataset.load_from_df(data,reader)\r\n\r\n#Start time\r\nstart_time = datetime.datetime.now()\r\n\r\n#Running K Fold Cross Validation\r\ncross_validate(NormalPredictor(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True);\r\n\r\n#End time\r\nend_time = datetime.datetime.now()\r\nprint('\\nTotal time taken: \\n', end_time - start_time)","ded4d8b2":"#Reading and preparing the data\r\ndata = df.reset_index(drop=True)\r\nreader = Reader(rating_scale=(1,5))\r\ndata = Dataset.load_from_df(data,reader)\r\nraw_ratings = data.raw_ratings\r\n\r\n#Saving 2 different sets of data for un-biased accuracy estimations\r\nthreshold = int(0.9 * len(raw_ratings))\r\nraw_ratings_A = raw_ratings[:threshold]\r\nraw_ratings_B = raw_ratings[threshold:]\r\ndata.raw_ratings = raw_ratings_A\r\n\r\n#Start time\r\nstart_time = datetime.datetime.now()\r\n\r\n#Selecting the best parameters using grid search\r\nparam_grid = {'n_epochs':[22],'lr_all':[0.010]}\r\ngrid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)\r\ngrid_search.fit(data)\r\nprint('Best parameters: ', grid_search.best_params)\r\n\r\nalgo = grid_search.best_estimator['rmse']    #Creates an algorithm instance with the best parameters so we can use it directly\r\n\r\n#Training dataset A on the model\r\ntrainset = data.build_full_trainset()\r\nalgo.fit(trainset)\r\n\r\n#Computing a biased accuracy score\r\npred_A = algo.test(trainset.build_testset())\r\nprint('Biased accuracy on A,', end='   ')\r\naccuracy.rmse(pred_A)\r\n\r\n#Compute unbiased accuracy on B\r\ntestset = data.construct_testset(raw_ratings_B)\r\npred_B = algo.test(testset)\r\nprint('Unbiased accuracy on B,', end=' ')\r\naccuracy.rmse(pred_B)\r\n\r\n#End time\r\nend_time = datetime.datetime.now()\r\nprint('\\nTime taken: ', end_time - start_time)\r\n","a144bf71":"#Reading and preparing the data\r\ndata = df.iloc[0:30000].reset_index(drop=True)\r\nreader = Reader(rating_scale=(1,5))\r\ndata = Dataset.load_from_df(data,reader)\r\nraw_ratings = data.raw_ratings\r\n\r\n#Saving 2 different sets of data for un-biased accuracy estimations\r\nthreshold = int(0.9 * len(raw_ratings))\r\nraw_ratings_A = raw_ratings[:threshold]\r\nraw_ratings_B = raw_ratings[threshold:]\r\ndata.raw_ratings = raw_ratings_A\r\n\r\n#Start time\r\nstart_time = datetime.datetime.now()\r\n\r\n#Selecting the best parameters using grid search\r\nparam_grid = {'n_epochs':[18,19,20,21,22],'lr_all':[0.009,0.010,0.011,0.012,0.013]}\r\ngrid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)\r\ngrid_search.fit(data)\r\nprint('Best parameters: ', grid_search.best_params)\r\n\r\nalgo = grid_search.best_estimator['rmse']    #Creates an algorithm instance with the best parameters so we can use it directly\r\n\r\n#Training dataset A on the model\r\ntrainset = data.build_full_trainset()\r\nalgo.fit(trainset)\r\n\r\n#Computing a biased accuracy score\r\npred_A = algo.test(trainset.build_testset())\r\nprint('Biased accuracy on A,', end='   ')\r\naccuracy.rmse(pred_A)\r\n\r\n#Compute unbiased accuracy on B\r\ntestset = data.construct_testset(raw_ratings_B)\r\npred_B = algo.test(testset)\r\nprint('Unbiased accuracy on B,', end=' ')\r\naccuracy.rmse(pred_B)\r\n\r\n#End time\r\nend_time = datetime.datetime.now()\r\nprint('\\nTime taken: ', end_time - start_time)","89a5beca":"#Reading and preparing the data\r\ndata = df.iloc[0:30000].reset_index(drop=True)\r\nreader = Reader(rating_scale=(1,5))\r\ndata = Dataset.load_from_df(data,reader)\r\nraw_ratings = data.raw_ratings\r\n\r\n#Saving 2 different sets of data for un-biased accuracy estimations\r\nthreshold = int(0.9 * len(raw_ratings))\r\nraw_ratings_A = raw_ratings[:threshold]\r\nraw_ratings_B = raw_ratings[threshold:]\r\ndata.raw_ratings = raw_ratings_A\r\n\r\n#Start Time\r\nstart_time = datetime.datetime.now()\r\n\r\nparam_grid = {'k':[6,7,8,9,10,11,12,13],'sim_options':{'name':['cosine'],'user_based':[True],'min_support':[1]}}\r\ngrid_knn = GridSearchCV(KNNWithMeans,param_grid,measures=['rmse'],cv=5);\r\ngrid_knn.fit(data);\r\nprint('Best parameters: ', grid_knn.best_params)\r\n\r\nalgo_knn = grid_knn.best_estimator['rmse']    #Creates an algorithm instance with the best parameters so we can use it directly\r\n\r\n#Training dataset A on the model\r\ntrainset = data.build_full_trainset()\r\nalgo_knn.fit(trainset)\r\n\r\n#Computing a biased accuracy score\r\npred_A = algo_knn.test(trainset.build_testset())\r\nprint('Biased accuracy on A,', end='   ')\r\naccuracy.rmse(pred_A)\r\n\r\n#Compute unbiased accuracy on B\r\ntestset = data.construct_testset(raw_ratings_B)\r\npred_B = algo_knn.test(testset)\r\nprint('Unbiased accuracy on B,', end=' ')\r\naccuracy.rmse(pred_B)\r\n\r\n#End time\r\nend_time = datetime.datetime.now()\r\nprint('Time taken: ',end_time - start_time)","7164e7cd":"#Reading and preparing the data\ndata = df.iloc[0:300000].reset_index(drop=True)\nreader = Reader(rating_scale=(1,5))\ndata = Dataset.load_from_df(data,reader)\n\n#Start time\nstart_time = datetime.datetime.now()\n\n#Running K Fold Cross Validation\ncross_validate(SlopeOne(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n#End time\nend_time = datetime.datetime.now()\nprint('\\nTime taken: \\n',end_time - start_time)","455359df":"#Creating a dataframe of all accuracy and computational time values\r\nindex = ['Biased Accuray on Set A','Unbiased Accuracy on Set B','Avg RMSE','Time Taken']\r\naccuracy = pd.DataFrame( \r\n    {\r\n        'Normal Predictor' : ['NaN','NaN',1.7661,'00:07:55'],\r\n        'NP Reduced': ['NaN','NaN',1.7303,'00:00:00.9'],\r\n        'SVD' : [0.3572,1.3119,'NaN','01:03:03'],\r\n        'SVD Reduced' : [0.4539,1.3605,'NaN','00:04:13'],\r\n        'KNN' : [0.3915,1.4579,'NaN','00:12:47'],\r\n        'Slope One' : ['Nan','NaN',1.5228,'00:06:12']\r\n    }, index=index\r\n)\r\n\r\naccuracy","7465788e":"# Data Understanding\r\n\r\n### The data used is Amazon Product Reviews which contains 7,824,482 rows and ratings for 476,002 different products made by 4,201,696 different users. The 4 columns are \"User ID\", \"Product ID\", \"Rating\", and \"Time Stamp\".\r\n\r\n### This datset is the courtesy of:\r\n### Justifying recommendations using distantly-labeled reviews and fined-grained aspects\r\n### Jianmo Ni, Jiacheng Li, Julian McAuley\r\n### Empirical Methods in Natural Language Processing (EMNLP), 2019","47365bda":"### How many products are ordered throughout the week. Are there a higher number of orders on the weekends?\r\n### For ease, we will only consider this for the year 2013. The below graph looks at the daily number of orders throughout the year","42727094":"### We can already begin to see that there are gaps in the dates. We do not have data for all days of the months.","bb10888d":"### Since the time part of the time stamp is all at the same exact time we can omit it from the column","1b1aa1aa":"### The number of daily orders have remained fairly consistent through the beginning of the 21st century and saw an exponential increase in 2013 after which it kept seeing a spike at the beginning of the year.","21f67115":"### Since the maximum number of times any given product was rated is 18244. An equal width bin size of 300 would be fair which will give us 60 items in each bar of the histogram we will plot","dba5c38c":"## Total Number of Ratings per User\r\n### Here we observe the number of ratings given by each user. It seems that up until the 75th percentile of users only gave a single rating to products and the maximum rating given by a single user was 103.","2265d8c8":"## How the number of orders have changed throughout the years","71fb7964":"# Data Visualization","944fda6d":"# SVD Reduced","1c8f9723":"# Normal Predictor\r\n\r\n### This is a very basic algorithm which predicts a random rating of an item based on the distribution of the dataset which is assumed to be normal. This is taken as a baseline of sorts so we can see the accuracy we get from a very basic algorithm and how more commonly used and comlpicated algorithms fare in comparison.","473177ed":"# Conclusion\r\n\r\n### Rating of products on Amazon suggest that generally products hosted on their website leave at least 70% of the users satisfied with the product they are reviewing or have purchased. It further shows that the highest number of orders seen are during the new year and that the general trend of the number of electronic products ordered since 2013 has been steadily increasing.\r\n\r\n### Upon trying to predict the ratings of the user for products they have not yet reviewed, the SVD model seems to give us the least amount of error with the second lowest computational time which makes it a very desirable model to be used in this scenario. Note that the models were only trained on 30,000 rows in comparison to it's total 7.8 Million rows due to limited computational and memory resources. But comparing each model to the same dataset gives us a much fairer comparison for use.","d9c6b28f":"### Looking at the 2013 data, we can confirm the highest mean number of orders we get is around January and December. There seems to be a peak in October but that can be just taken as an outlier.\r\n### Now lets break this down to a weekly level and see how it looks for each month","979d67f6":"# Slope One\r\n\r\n### Slope One is another simple but effective algorithm. To explain simply, it takes a commonly rated item between a pair of users (lets call them A and B) and calculates a difference in rating between them. It then computes a predicted rating for an item for B for which the user A has already given a rating. Simple yet effective.","69ec58e7":"### This figure represents the average rating given to a product (which was reviewed at least 50 times or more) and the number of times it was rated. If we split the graph into 4 equal hypothetical quartiles, the top right quartile will represent the highest quality products and the lower left quartile will represent the lowest quality products.","e7c3a0dc":"### The full SVD model gives us the lowest accuracy but also takes the most time taken. For the sake of comparing apples to apples, we will only consider the reduced models for Normal Predictor and SVD. KNN and Slope One are already use the reduced dataset since any larger than 3,000 rows, my machines memory will not be able to handle the large matrices formed.\r\n\r\n### Disregarding the full dataset models, the lowest error is computed by the SVD algorithm with KNN taking second place followed by Slope One and lastly Normal Predictor. SVD also gives us the second fastest computing time which makes it a clear winner for the best model to be used for the dataset we were working with.\r\n\r\n### KNN is second lowest in accuracy but also takes the most amount of time out of all the models so it wouldn't be too desirable to use as much as SVD or Slope One.","4f89deaa":"# Amazon Product Recommendation\r\n\r\n### This capstone project aims to build a product recommendation system in a model-based collaborative filtering approach using different machine learning models and comparing the results between them.\r\n\r\n# Business Understanding\r\n\r\n### By the start of 2020, the amount of data available on the internet was estimated to be 44 zettabytes of data. With this huge of a database and more than 4.8 billion users on the internet, accurate and relevant search and recommendation of products is an ever increasing need to keep user engagement and bring the best results in accordance to the factors which are important to the user. This can vary with demographics, psychographics, search habits, and other such variables.\r\n\r\n### This is especially relevant to large tech companies like Amazon, Netflix, Alibaba and other online platforms which offer a wide variety of products to the audience. They need personalized results to the individual user or more commonly called a recommendation system which filters and displays other products that the user may be interested in to increase basket ratio in the case of Amazon and Alibaba or increase viewing times in the case of Netflix.","70f19a4d":"## Data Exploration","448e4937":"### For ease of visualizations lets take orders from 2006 onwards","f8e3d6de":"# Building a Recommendation System using Machine Learning\r\n\r\n### In this section we will look at 4 ML algorithms. Normal Predictor, SVD, KNN with means, Slope One, and compare each of them on the accuracy of their predictions and computational time.\r\n\r\n### These algorithms are chosen on the basis of their popularity of use in building recommendation systems as well as my current knowledge on how well I understand each of them.\r\n\r\n### For simplicity we will be taking 2 assumptions:\r\n### 1. The accuracy measure we will be using for all will be RMSE which is one of the most widely used offline accuracy measure for recommendation systems\r\n### 2. Each algorithm will be evaluated with using cross validation of 5 folds\r\n### 3. Where parameter tuning is applicable, a set of the data will be used for grid search to find the optimal parameters of the algorithm and another set of the data for accuracy measurment. This prevents biased accuracy measurment where the same set of data is used for both parameter tuning and measuring accuracy of predictions.\r\n### 4. For splitting data into 2 sets for grid search, 90% of the data will be for parameter tuning and 10% for unbiased accuracy measurment","358600e0":"# Normal Predictor Reduced\r\n\r\n### This section tries the algorithm to run on 30,000 rows as is the case for the KNN and Slope One algorithms due to memory space issues. As a result we will see how Normal Predictor and SVD compare with the entire dataset, but for analyzing the accuracy measures, we will use the reduced dataset results.","f5b97aea":"### A mean RMSE of 1.7385 means that our predictions are off quite a bit considering our ratings only range from 1 to 5. But considering the simplicity of the algorithm this is quite a good accuracy to have. ","d5904a99":"# Data Modelling","f8072094":"### The above histogram shows that an absurd amount of products were rated less than 60 times. To cater for this skewness, let's only consider getting average rating of products which are rated more than 60 times to get better average rating per product.","1fb734b9":"### It seems that more than 50% of products received a 5 star review and another 20% received a 4 star review. We can now accurately conclude that atleast 70% of the products reviewed were of high quality, or at least left the user satisfied.\r\n\r\n### Let's look at the ratings in more depth. Let's see the number of times a product was rated and then the average rating given to these products.","43064d65":"# Singular Value Decomposition\r\n\r\n### Popularized by Simon Funk during the NETFLIX prize, SVD comes under the Matrix Decomposition family of algorithms. This is considered quite a complex algorithm with many different parameters to tune it. Parameter tuning is applicable here so we will use 2 seperate sets of data. One for parameter tuning and another for unbiased accuracy measure as stated previously.\r\n\r\n### For sake of simplicity (and my poor laptops computational resources), we will only tune the number of epochs and learning rate parameters and use the RMSE accuracy measure.\r\n\r\n### In the intrest of saving computational time, I have tried multiple grid search tests and came up with the parameters which shows the best accuracies using 100,000 rows of data and will use those resulting parameters for the full dataset.","110bca36":"## Time Series Visualizations","eb28a5ba":"## Percentage of Number of Ratings Given\r\n### We have seen that most products received either a 5 star or 4 star review. But out of all products reveiwed, what do the percentages of these look like? How many received a 5 star review out of the total number of products reveiwed.","a849675e":"### January far exceeds all other months in terms of number of orders made. This confirms our previous hypothesis of orders being higher during the ending and starting period of the year.","6483bdca":"## Total Number of Each Rating\r\n\r\n### Below we can see how many times each rating was given. A very large number of products received a 5 star review and next was the 4 star. So it seems that users who have rated products were generally happy with the products they received and very few in comparison left users unsatisfied.","9a962f41":"# K Nearest Neighbours\r\n\r\n### This comes under the Clustering family of algorithms where it finds the nearest neighbours to a user using a specified similarity metric such as Cosine similarity, Mean Squared Difference, and Pearson correlation to name some of the more well known ones. In our case, we will be using the cosine similarity metric.\r\n\r\n### Parameter tuning is applicable here as well so specific parameter we will look at are 'k' (the number of neighbours), 'Similarity Metric' (we are going with cosine only), 'User or Item Based' (we will go with user based), 'Minimum Support' (this is the minimum number of neighbours a user must have to be included in the algorithm)\r\n\r\n### The trouble we have here is that since the matrices computed here are so large, my laptop runs out of memory and so we can only do a limited number of experiments. We will only be experimenting with 3 different values of 'k'.\r\n\r\n### In the intrest of saving computational time, I have tried multiple grid search tests and came up with the parameters which shows the best accuracies using 100,000 rows of data and will use those resulting parameters for the full dataset."}}