{"cell_type":{"ced24e87":"code","5f576808":"code","e170c434":"code","352acbfb":"code","141e9390":"code","1f92cd15":"code","15e6932e":"code","665d2411":"code","82b6fcb2":"code","42b98d47":"code","608a3480":"markdown","e8ec6622":"markdown","b2c522b3":"markdown","f9e7da62":"markdown","137b858a":"markdown","fb7e3878":"markdown","90f2b4e3":"markdown","3304c182":"markdown"},"source":{"ced24e87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","5f576808":"X = pd.read_csv(\"..\/input\/train.csv\", nrows = 600000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","e170c434":"%time\n\nrows = 150_000\ntrain = X\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min',\n                               'av_change_abs', 'av_change_rate', 'abs_max', 'abs_min',\n                               'std_first_50000', 'std_last_50000', 'std_first_10000', 'std_last_10000',\n                               'avg_first_50000', 'avg_last_50000', 'avg_first_10000', 'avg_last_10000',\n                               'min_first_50000', 'min_last_50000', 'min_first_10000', 'min_last_10000',\n                               'max_first_50000', 'max_last_50000', 'max_first_10000', 'max_last_10000'])\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\ntotal_mean = train['acoustic_data'].mean()\ntotal_std = train['acoustic_data'].std()\ntotal_max = train['acoustic_data'].max()\ntotal_min = train['acoustic_data'].min()\ntotal_sum = train['acoustic_data'].sum()\ntotal_abs_max = np.abs(train['acoustic_data']).sum()\n\nfor segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_tr.loc[segment, 'time_to_failure'] = y\n    X_tr.loc[segment, 'ave'] = x.mean()\n    X_tr.loc[segment, 'std'] = x.std()\n    X_tr.loc[segment, 'max'] = x.max()\n    X_tr.loc[segment, 'min'] = x.min()\n    \n    \n    X_tr.loc[segment, 'av_change_abs'] = np.mean(np.diff(x))\n    X_tr.loc[segment, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_tr.loc[segment, 'abs_max'] = np.abs(x).max()\n    X_tr.loc[segment, 'abs_min'] = np.abs(x).min()\n    \n    X_tr.loc[segment, 'std_first_50000'] = x[:50000].std()\n    X_tr.loc[segment, 'std_last_50000'] = x[-50000:].std()\n    X_tr.loc[segment, 'std_first_10000'] = x[:10000].std()\n    X_tr.loc[segment, 'std_last_10000'] = x[-10000:].std()\n    \n    X_tr.loc[segment, 'avg_first_50000'] = x[:50000].mean()\n    X_tr.loc[segment, 'avg_last_50000'] = x[-50000:].mean()\n    X_tr.loc[segment, 'avg_first_10000'] = x[:10000].mean()\n    X_tr.loc[segment, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_tr.loc[segment, 'min_first_50000'] = x[:50000].min()\n    X_tr.loc[segment, 'min_last_50000'] = x[-50000:].min()\n    X_tr.loc[segment, 'min_first_10000'] = x[:10000].min()\n    X_tr.loc[segment, 'min_last_10000'] = x[-10000:].min()\n    \n    X_tr.loc[segment, 'max_first_50000'] = x[:50000].max()\n    X_tr.loc[segment, 'max_last_50000'] = x[-50000:].max()\n    X_tr.loc[segment, 'max_first_10000'] = x[:10000].max()\n    X_tr.loc[segment, 'max_last_10000'] = x[-10000:].max()","352acbfb":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","141e9390":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\nplt.figure(figsize=(22, 16))\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n        \n    X_test.loc[seg_id, 'av_change_abs'] = np.mean(np.diff(x))\n    X_test.loc[seg_id, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_min'] = np.abs(x).min()\n    \n    X_test.loc[seg_id, 'std_first_50000'] = x[:50000].std()\n    X_test.loc[seg_id, 'std_last_50000'] = x[-50000:].std()\n    X_test.loc[seg_id, 'std_first_10000'] = x[:10000].std()\n    X_test.loc[seg_id, 'std_last_10000'] = x[-10000:].std()\n    \n    X_test.loc[seg_id, 'avg_first_50000'] = x[:50000].mean()\n    X_test.loc[seg_id, 'avg_last_50000'] = x[-50000:].mean()\n    X_test.loc[seg_id, 'avg_first_10000'] = x[:10000].mean()\n    X_test.loc[seg_id, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_test.loc[seg_id, 'min_first_50000'] = x[:50000].min()\n    X_test.loc[seg_id, 'min_last_50000'] = x[-50000:].min()\n    X_test.loc[seg_id, 'min_first_10000'] = x[:10000].min()\n    X_test.loc[seg_id, 'min_last_10000'] = x[-10000:].min()\n    \n    X_test.loc[seg_id, 'max_first_50000'] = x[:50000].max()\n    X_test.loc[seg_id, 'max_last_50000'] = x[-50000:].max()\n    X_test.loc[seg_id, 'max_first_10000'] = x[:10000].max()\n    X_test.loc[seg_id, 'max_last_10000'] = x[-10000:].max()\n    \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","1f92cd15":"X_test_scaled.shape","15e6932e":"from keras.layers import Input, Dense\nfrom keras.models import Model\nimport tensorflow as tf\n\n# This returns a tensor\ninputs = Input(shape=(24,))\n\n# a layer instance is callable on a tensor, and returns a tensor\nx = Dense(128, activation='relu')(inputs)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(1)(x)\n\n# This creates a model that includes\n# the Input layer and three Dense layers\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n              loss='mse',\n              metrics=['accuracy'])\nmodel.fit(X_train_scaled, y_tr,epochs=10)  # starts training","665d2411":"y_pred_nn = model.predict(X_test_scaled).flatten()","82b6fcb2":"sample = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample['time_to_failure'] = y_pred_nn\nsample.to_csv('submission.csv',index=False)","42b98d47":"%time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nn_estimators = [15,16]\nfunctions = ['relu']\nnumbers = [64,128]\nfor funcs in functions:\n  for number in numbers:\n    for n_est in  n_estimators:\n        print(n_est)\n        print(number)\n        print(funcs)\n        cv = KFold(n_splits=5, shuffle=True,random_state=0)\n        for train, valid in cv.split(X_train_scaled, y_tr):\n            x_train = X_train_scaled.iloc[train]\n            x_valid = X_train_scaled.iloc[valid]\n            y_train = y_tr.iloc[train]\n            y_valid = y_tr.iloc[valid]\n        \n            inputs = Input(shape=(24,))\n            x = Dense(number, activation=funcs)(inputs)\n            x = Dense(number, activation=funcs)(x)\n            predictions = Dense(1)(x)\n            model = Model(inputs=inputs, outputs=predictions)\n            model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n                      loss='mse',\n                    metrics=['accuracy'])\n            model.fit(x_train, y_train, epochs=n_est, verbose=0)\n            y_pred_nn = model.predict(x_valid)\n            y_pred = y_pred_nn\n            print(mean_absolute_error(y_valid, y_pred))  ","608a3480":"Making Features.\n\n-----------------------\n\n\u7279\u5fb4\u91cf\u306e\u4f5c\u6210\u3002","e8ec6622":"Making data to submit.\n\n---------------------------\n\n\u63d0\u51fa\u7528\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3002","b2c522b3":"Loading Data.\n\n-------------------\n\n\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3002","f9e7da62":"Making data to predict.\n\n--------------------------------\n\n\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3002","137b858a":"Cross Validation.\n\n---------------------------\n\n\u4ea4\u5dee\u691c\u8a3c\u3002","fb7e3878":"Normalize Features.\n\n------------------------\n\n\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u306e\u6b63\u898f\u5316\u3002","90f2b4e3":"I made too much simple NN model.\nhttps:\/\/www.kaggle.com\/artgor helped me.\n\nI'm beginner, so there may be many strange point.\nPlease give me a advise.\n\n--------------------------------------------------------------\n\nNN\u306b\u3088\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\nhttps:\/\/www.kaggle.com\/artgor\u3000\u3055\u3093\u306ekernels\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u305fkernels\u304c\u307b\u3068\u3093\u3069\u306a\u304b\u3063\u305f\u305f\u3081\n\u5c11\u3057\u3067\u3082\u521d\u5fc3\u8005\u306e\u52a9\u3051\u306b\u306a\u308c\u3070\u3068\u3053\u3061\u3089\u306ekernels\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u79c1\u81ea\u8eab\u3082\u521d\u5fc3\u8005\u3067\u3059\u306e\u3067\u7406\u89e3\u306e\u3067\u304d\u3066\u3044\u306a\u3044\u70b9\u304c\u591a\u304f\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u3054\u6307\u6458\u3084\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3042\u308c\u3070\u662f\u975e\u304a\u9858\u3044\u3057\u307e\u3059\u3002","3304c182":"Trainig and Predict by NN.\n\n--------------------------------------\n\nNN\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002"}}