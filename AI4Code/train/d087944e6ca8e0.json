{"cell_type":{"c97a39f2":"code","792befb6":"code","53c9a94f":"code","28d7e231":"code","abd06258":"code","22a04cc9":"code","b6d6a5e6":"code","f1b46865":"code","a7d633ca":"code","60dbac5a":"code","c2aefb25":"code","cec2196c":"code","00d639ce":"code","23333ed7":"code","54fef0f0":"code","9bffd206":"code","00ed9e6a":"code","1a9941f8":"code","e4440d81":"code","242b4137":"code","3b918951":"code","7e429d84":"code","80a8a772":"code","63a8fb2d":"code","ac51316b":"code","a9a603d5":"code","d2b8c3dd":"code","abb99c0f":"code","95a1d5f3":"markdown","5d145b0e":"markdown","afe0b683":"markdown","d43f8b14":"markdown","17a9193a":"markdown","eda281d7":"markdown","be613cbf":"markdown","f7fe4558":"markdown","0a030cd6":"markdown","8cf79313":"markdown","7a1539b0":"markdown","57689203":"markdown","5f401c50":"markdown","bdf50f74":"markdown","b18f002c":"markdown","8ededb2c":"markdown","b272551c":"markdown","b75b615d":"markdown","d0659117":"markdown"},"source":{"c97a39f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","792befb6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","53c9a94f":"from sklearn.datasets import load_breast_cancer\ncancer=load_breast_cancer()\ncancer.keys()\n#This is a special type of dataset of sklearn","28d7e231":"print(cancer[\"DESCR\"])","abd06258":"cancer[\"data\"]\n# This is the data","22a04cc9":"cancer[\"feature_names\"]\n#This is the feature names of the data","b6d6a5e6":"df=pd.DataFrame(cancer[\"data\"],columns=cancer[\"feature_names\"])\ndf.head()","f1b46865":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(df)\nscaled_data=scaler.transform(df)","a7d633ca":"scaled_data ","60dbac5a":"from sklearn.decomposition import PCA\npca=PCA(n_components=2) # we make an instance of PCA and decide how many components we want to have\n","c2aefb25":"pca.fit(scaled_data) # We make PCA fit to our scaled data","cec2196c":"transformed_data=pca.transform(scaled_data)","00d639ce":"scaled_data.shape\n#This is the original shape of the data with 569 rows and 30 columns","23333ed7":"transformed_data.shape\n#Here we see 569 rows but 2 columns or components after PCA implementation","54fef0f0":"transformed_data","9bffd206":"plt.figure(figsize=(15,10))\nplt.scatter(transformed_data[:,0],transformed_data[:,1])\nplt.xlabel(\"The First Principal Component\")\nplt.ylabel(\"The Second Principal Component\")\n#Here we plot all the rows of columns 1 and column 2 in a scatterplot.","00ed9e6a":"plt.figure(figsize=(15,10))\nplt.scatter(transformed_data[:,0],transformed_data[:,1],c=cancer[\"target\"],cmap=\"plasma\")\nplt.xlabel(\"The First Principal Component\")\nplt.ylabel(\"The Second Principal Component\")\n#Here we plot all the rows of columns 1 and column 2 in a scatterplot.","1a9941f8":"pca.components_","e4440d81":"df_comp=pd.DataFrame(pca.components_,columns=cancer[\"feature_names\"])\ndf_comp","242b4137":"plt.figure(figsize=(15,10))\nsns.heatmap(df_comp,cmap=\"magma\")","3b918951":"X=transformed_data\ny=cancer[\"target\"]","7e429d84":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)","80a8a772":"from sklearn.linear_model import LogisticRegression\nlog_regression=LogisticRegression()\nlog_regression.fit(X_train,y_train)","63a8fb2d":"#Now our model is ready to predict the test data\npredictions=log_regression.predict(X_test)\npredictions","ac51316b":"#Now it is time to evaluate how good the predictions are\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n#The precision and accuracy precentages are over %90, it is very good","a9a603d5":"from sklearn.svm import SVC\nsvm_model=SVC()\nsvm_model.fit(X_train,y_train)","d2b8c3dd":"predictions=svm_model.predict(X_test)\npredictions","abb99c0f":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\n#here we get the classification report to learn how accurate our model is","95a1d5f3":"* The data has been reduced to the two most important components via PCA that we can easily plot out.","5d145b0e":"* It is an unsupervised stastical technique used to examine the interrelations among a set of variables in order to identify the underlying structure of those variables\n\n* It is also known as general factor analysis\n\n* While regression determines a line of best fit to a dataset, factor analysis or principal component analysis determines several orthogonal lines of best fit to the dataset.Orthogonal means at right angles. The lines are perpendicular to each other in n dimensional space where n dimensional space is the variable sample space. There as many dimensions as there are variables,i.e., a dataset with 4 variables the sample space is 4 dimensional.\n\n* If we use this technique on a dataset with large numbers of variables, we can compress the amount of explained variation to just a few components.\n\n","afe0b683":"* The data contains 569 rows and 30 variables or columns\n\n* What we are going to do is to figure out what compenents are important to explain the variance of the data.","d43f8b14":"* In this heatmap above, we see the relation between the principal components and actual features\n\n* The ligh color in the heatmap shows strong correlation between the principal components and actual features while dark colors show the opposite or negative correlation.\n\n* Actually the principal components are the combinations of all these features of the data.","17a9193a":"* Each row represents actual componnents and each column relates back original features.\n\n* We can see the relationship better via a heatmap.\n\n* But first we need to transfor it into a dataframe in order to use the visualization libraries.","eda281d7":"* After we get the principal components of the data, we can feed them into a machine learning algorithm because we have clear and separated components of the data instead of the complex variables.\n\n* For this data we do a logistic regression on tranformed_data instead of doing regression with the entire data.\n\n* Support vector machines can also be a good alternative for this data.","be613cbf":"* It is obvious that we can get pretty good prediction by using just the two principal components of the data instead of using all of the dataset.\n\n* PCA can be very useful tool big data with many features.","f7fe4558":"* PCA is just a transformation of our data and attempts to find out what features excplain the most variance in our data\n\n* We try to get rid of the components that do not explain enough the variance in our data.","0a030cd6":"* This plot shows the power of PCA because based on just two principal components, we can see very clear separation of the target variable, which shows how benign and malignant tumors look like.\n\n* We utilize PCA as compression algorithm to get a clear information about the data instead of analyzing 30 columns or variables as it is case in this dataset.\n\n* However the components that has been decided by PCA do not correspond to a specific column or variable in the dataset.These two components are the combinations of the original variables of the data.","8cf79313":"* Now we are going to combine the feature names with the data and make a dataframes","7a1539b0":"* Before we use PCA in the data, we need to standartize the variables by using standart scaler of sklearn","57689203":"* This plot does not explain much, we can add some paramaters to the same plot in order to show the positions of the components according the target variable of the data","5f401c50":"* In this dataset, there are 30 dimensions or variables, thus it is difficult to visualize all of them. We can utilize PCA to learn the two most important components of the data and visualize the data in this new two dimensional space. ","bdf50f74":"* It can be used as a dimensionality reduction method, which can help to minimize the number of the variables (or columns of a data frame) without losing much of the original information. This is useful especially when you are building machine learning models based on the data with many variables like 100s or 1000s.","b18f002c":"* Because we have already siplitted data in the previous algorithm, we just skip this stage","8ededb2c":"* Standart Scaler transformed our data into a numpy array and standartized all of the variables of the data ","b272551c":"* Now we will also use Support Vector Machines Algorithm with the PCA","b75b615d":"It is obvious that we can get pretty good prediction by using just the two principal components of the data instead of using all of the dataset.","d0659117":"* The precision and accuracy precentages are over %90, it is very good although it is not as good as logistic regression"}}