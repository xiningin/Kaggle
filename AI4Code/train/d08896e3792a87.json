{"cell_type":{"7af2ee03":"code","94c11cd0":"code","df3da958":"code","f8a07be1":"code","a1b3db04":"code","31c4ef67":"code","75818558":"code","580e2366":"code","6aff730f":"code","d2768542":"code","d907056a":"code","096605c4":"code","f5de940f":"code","1c420d7a":"code","5fb038c2":"code","2e6bdfa2":"code","7853a789":"code","b9d6e4ad":"code","0c9ee1e6":"code","cfbabdfa":"code","cb1b6723":"code","a032fe9b":"code","27d57888":"code","1f8130b3":"code","1fcec498":"code","247716b0":"code","b39e18fe":"code","753665cd":"code","5bc52436":"code","d8f816df":"code","7e4dd95d":"code","42dde166":"markdown","c73d6207":"markdown","b1200b63":"markdown","fbaa919e":"markdown","e4a16b57":"markdown","5425fd29":"markdown","1ec7b8ed":"markdown","edee5168":"markdown","9d4d0dcc":"markdown","f326c9f1":"markdown","af093487":"markdown","55ae14ae":"markdown","cb4f635d":"markdown","669bafad":"markdown","d52c7ba4":"markdown","63db7fd6":"markdown","a2f36c56":"markdown","68df9031":"markdown","2a01388e":"markdown","54102336":"markdown","7e4d911e":"markdown","cdf5f4d6":"markdown","09b724a5":"markdown","d223c00a":"markdown","53ca81dd":"markdown","d99e1608":"markdown","14b96c7b":"markdown","0c9ce030":"markdown","e5fed493":"markdown","01c41dc9":"markdown","99072734":"markdown","3b4552d2":"markdown","9cd04954":"markdown"},"source":{"7af2ee03":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd \nimport os\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nimport warnings\nfrom imblearn.over_sampling import SMOTE\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","94c11cd0":"tf.test.gpu_device_name()","df3da958":"df = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep = ';')\ndf.head()","f8a07be1":"df.info()","a1b3db04":"for col in df.columns:\n    print()\n    if df[col].dtype == 'object':\n        print(f'Name of Column is: {col} and unique values are: {df[col].unique()}')","31c4ef67":"#this function returns categorical variables\ndef return_categorical(df):\n\n  categorical_columns = [column_name for column_name in df if df[column_name].dtype == 'O']\n  return categorical_columns\n\n#this function returns numerical variables\ndef return_numerical(df):\n\n  return list(set(df.columns) - set(return_categorical(df)))\n\n\ndef check_normal(df):\n  fig, axes = plt.subplots(1,len(return_numerical(df)), figsize =(70, 10))\n\n  for i,numeric_column_name in enumerate(list(set(df.columns) -set(return_categorical(df)))):\n\n    sns.distplot(df[numeric_column_name], ax=axes[i]);\n    plt.title(f'Distribution of {numeric_column_name}');\n    \ndef classifier(clf, x_train,x_test,y_train,y_test):\n    y_test_pred = clf.predict(x_test)\n    y_train_pred = clf.predict(x_train)\n\n    accuracy_test = accuracy_score(y_test,y_test_pred)\n    accuracy_train =  accuracy_score(y_train,y_train_pred)\n    \n    roc_test = roc_auc_score(y_test, y_test_pred, multi_class='ovr')\n    roc_train = roc_auc_score(y_train, y_train_pred, multi_class='ovr')\n    \n    print('Train accuracy is:',accuracy_train )\n    print('Test accuracy is:',accuracy_test )\n    print()\n    print('Train ROC is:', roc_train)\n    print('Test ROC is:',roc_test )\n    \n    # Fscore, precision and recall on test data\n    f1 = f1_score(y_test, y_test_pred)\n    precision = precision_score(y_test, y_test_pred)\n    recall = recall_score(y_test, y_test_pred) \n    print()\n    print(\"F score is:\",f1 )\n    print(\"Precision is:\",precision)\n    print(\"Recall is:\", recall)\n  \n\ndef random_search(clf,params, x_train,x_test,y_train,y_test):\n    \n    random_search = RandomizedSearchCV(estimator= clf, param_distributions=params, scoring='roc_auc', cv=5)\n    random_search.fit(x_train, y_train)\n    optimal_model = random_search.best_estimator_\n\n    print(\"Best parameters are: \", random_search.best_params_)\n    print()\n    print(\"Best estimator is: \", random_search.best_estimator_)\n    print()\n    print('Scores and accuracies are:')\n    print()\n    classifier(optimal_model, x_train,x_test,y_train,y_test)\n\n\n","75818558":"check_normal(df)","580e2366":"# plotting graphs for all categorical columns\nfor col in return_categorical(df):\n    counts = df[col].value_counts().sort_index()\n    if len(counts) > 10:\n      fig = plt.figure(figsize=(30, 10))\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","6aff730f":"corr = df.corr()\ncorr_greater_than_75 = corr[corr>=.75]\ncorr_greater_than_75","d2768542":"plt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_75, cmap=\"Reds\", annot = True);","d907056a":"df['pdays'].unique()","096605c4":"df['job'] = df['job'].apply(lambda x: -1 if x=='unknown' or x=='unemployed' else (15 if x=='entrepreneur' else (8 if x == 'blue-collar' else ( 6 if x=='technician' or x=='services' or  x=='admin.' or x=='management' else (4 if x== 'self-employed' or x=='student' else (2 if x=='housemaid' or x=='retired' else None) )))))\ndf['housing'] = df['housing'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['loan'] = df['loan'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['y'] = df['y'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['default'] = df['default'].apply(lambda x: 0 if x=='no' else (1 if x=='yes' else -1))\ndf['poutcome'] = df['poutcome'].apply(lambda x: 0 if x=='failure' else (2 if x=='failure' else -1))\ndf['pdays'] = df['pdays'].apply(lambda x: 0 if x==999 else(20 if x<=10 else(6 if x<=20 else 3)))\n\n","f5de940f":"df.drop(['day_of_week', 'contact', 'month'], axis=1, inplace = True)","1c420d7a":"df  = pd.get_dummies(df, drop_first = True)\n","5fb038c2":"x = df.drop(\"y\", axis=1)\ny = df['y']\nx.sample()\n\nx_train,x_test,y_train,y_test = train_test_split(x,y, random_state=42)\n","2e6bdfa2":"\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(x_train, y_train)\n\nprint('Original dataset shape', len(x_train))\nprint('Resampled dataset shape', len(x_smote))","7853a789":"s = StandardScaler()\n","b9d6e4ad":"(x_train.shape[1])","0c9ee1e6":"\nknn = KNeighborsClassifier(n_neighbors = 20)\nknn.fit( s.fit_transform(x_train), y_train)\n\nclassifier(knn, s.fit_transform(x_smote),s.transform(x_test), y_smote,y_test)\n\n","cfbabdfa":"\nerror_rate = []\nfor i in range(1,40):\n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit( s.fit_transform(x_train), y_train)\n pred_i = knn.predict(s.transform(x_test))\n error_rate.append(np.mean(pred_i != y_test))\n\nacc = []\nfor i in range(1,40):\n    neigh = KNeighborsClassifier(n_neighbors = i).fit(s.fit_transform(x_train), y_train)\n    yhat = neigh.predict(s.transform(x_test))\n    acc.append(metrics.accuracy_score(y_test, yhat))\n    ","cb1b6723":"\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate)))\n","a032fe9b":"\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),acc,color = 'blue',linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nprint(\"Maximum accuracy:-\",max(acc),\"at K =\",acc.index(max(acc)))","27d57888":"\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit( s.fit_transform(x_smote), y_smote)\n\nclassifier(knn, s.fit_transform(x_smote),s.transform(x_test),y_smote,y_test)","1f8130b3":"  \n# bagging classifier\nmodel = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors = 10),\n                          n_estimators = 15)\nclassifier(model.fit( s.fit_transform(x_smote), y_smote), s.fit_transform(x_smote),s.transform(x_test),y_smote,y_test)","1fcec498":"\ndtree = DecisionTreeClassifier(random_state=0)\ndtree.fit(x_train, y_train)\nclassifier(dtree, x_train,x_test,y_train,y_test)","247716b0":"param_grid = {'max_depth':np.arange(3,20),\n              'min_samples_split': np.arange(3,20,1),\n             'min_samples_leaf':np.arange(3,30),\n              'min_samples_split' : np.arange(3,30),\n              'criterion': ('gini', 'entropy')}\n\n\n\nrandom_search(DecisionTreeClassifier(random_state=0),param_grid, x_train,x_test,y_train,y_test)","b39e18fe":"# I have chosen tuned hyperparameters here\n\n\nkfold = model_selection.KFold(n_splits = 3)\n\n# bagging classifier\nmodel = BaggingClassifier(base_estimator = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=25, max_depth=8, criterion='gini'),n_estimators = 500,)\n\nclassifier(model.fit(x_train, y_train), x_train,x_test,y_train,y_test)","753665cd":"rforest = RandomForestClassifier(random_state=0)\nclassifier(rforest.fit(x_train, y_train), x_train,x_test,y_train,y_test)","5bc52436":"params = {'n_estimators' : np.arange(100,1000, 100),\n              'max_depth' : np.arange(3,20,1),\n              'min_samples_split' : np.arange(3,20,1),\n              'min_samples_leaf' : np.arange(3,20,1),\n         'max_features': ('sqrt', 'log2'), 'criterion': ('gini', 'entropy')}\n\nrandom_search(RandomForestClassifier(random_state=0),params, x_train,x_test,y_train,y_test)","d8f816df":"\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu',input_dim = x_train.shape[1]))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nmodel_history=classifier.fit(s.fit_transform(x_smote), y_smote,validation_split=0.33, batch_size = 10, epochs = 100)","7e4dd95d":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('Model accuracy Vs Epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('Model loss Vs Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","42dde166":"\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modifying The Columns: Assigning Weight On the Basis Of Importance<\/p>","c73d6207":"\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Checking For Categorical Columns<\/p>","b1200b63":"**Not using balanced data with tree based models because they can handle imbalace pretty well so whynot.**","fbaa919e":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Bagging With KNN As Base Model<\/p>","e4a16b57":"<a id =\"6\"><\/a>\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modelling And Optimizing The Models<\/p>\n","5425fd29":"> Some correlation can be seen here between emp. var rate and nr.employed.\n\n> Also, euribor3m and emp.var rate.\n\n> However,  I would not be removing any. As I am gonna mostly be training on tree models which doesn't require much preprocessing.","1ec7b8ed":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modelling With Decision Tree<\/p>","edee5168":"# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Checking Unique Values For Days Column<\/p>","9d4d0dcc":"\n\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Tuning The HyperParameters Of Random Forest<\/p>","f326c9f1":"\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Checking Distribution For Continuous Variables<\/p>","af093487":"> **Class imbalance observed for target variable**","55ae14ae":"<a id = \"4\"><\/a>\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Utility Functions<\/p>","cb4f635d":"# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Credit Analysis With KNN\/DTree\/RF \/Bagging\ud83c\udfe6<\/p>\n\n<img src=\"https:\/\/i1.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/09\/1-Credit-card-fraud-detection-with-classification-algorithms.png?w=750&ssl=1\">\n\n\n<p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Table Of Contents<\/p>   \n    \n* [1. Importing Modules](#1)\n    \n* [2. Loading Data](#2)\n    \n* [3. Data PreProcessing And Visualizations](#3)\n    \n* [4. Utility Functions](#4) \n\n* [5. Data Balancing](#5)   \n      \n* [6. Modelling And Optimizing The Models](#6)\n    \n* [7. Results And Conclusion](#7)\n \n","669bafad":"\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">For Scaling The Data<\/p>","d52c7ba4":"<a id =\"8\"><\/a>\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Results & Conclusion<\/p>\n\n> **Bagging with Decision tree is performing the best according to recall and roc.**\n\n> **For a credit insurance problem, I would want to go with recall here.**","63db7fd6":"> **999 represents no contact has been done, thus i'll be assigning it a very less weight.**","a2f36c56":"<a id =\"5\"><\/a>\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Balancing The Data Using Smote<\/p>","68df9031":"\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Checking For Highly Correlated Columns<\/p>","2a01388e":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Tuning The Hyperparameters Of Decision Tree<\/p>","54102336":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modelling With Random Forest<\/p>","7e4d911e":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Train-Test Split<\/p>","cdf5f4d6":"<a id=\"1\"><\/a>\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Importing Modules<\/p>","09b724a5":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Using ANN<\/p>\n\n","d223c00a":"\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Tuning The Hyperparameterrs Of KNN<\/p>","53ca81dd":"> **None of the features are following a normal distribution**","d99e1608":"\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modelling With The Value Of K which Gave Least Error<\/p>","14b96c7b":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Modelling With KNN<\/p>","0c9ce030":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">One Hot Encoding Remaining Categorical Features<\/p>","e5fed493":"<a id= \"3\"><\/a>\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Data PreProcessing And Visualizations<\/p>","01c41dc9":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Dropping Columns Which I Feel Won't Affect The Predictions<\/p>","99072734":"<a id = \"2\"><\/a>\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Loading Data<\/p>","3b4552d2":"\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Decision Tree Based Bagging<\/p>","9cd04954":"\n\n\n# <p style=\"background-color:#FA8072;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Plots For Categorical Variables<\/p>"}}