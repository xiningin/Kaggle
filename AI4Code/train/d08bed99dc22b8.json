{"cell_type":{"4b5a794c":"code","58ec28e8":"code","05bd8fad":"code","b4aeedf1":"code","5e59c3a6":"code","f2a11246":"code","7f43c588":"code","762a07ff":"code","eca5ee77":"code","12726953":"code","adc7f1f9":"code","d793d98f":"code","15eb2623":"code","d6e42ae7":"code","5c52bef9":"code","67460617":"code","dd9b17ad":"code","0acb0770":"code","dc55216a":"code","4645f517":"code","3959bc20":"code","76f45d81":"code","1a161ab8":"code","eb0d408c":"code","7c9233bb":"code","82873079":"code","62a13227":"code","9aea8615":"code","fe121e34":"code","2a820aef":"code","1b3d6477":"code","e57ac65f":"code","42963712":"code","447f8100":"code","7369ff2e":"code","0c097686":"code","1540de3d":"code","f950b53c":"code","1bf72e2f":"code","2a79259c":"code","fc966359":"code","01f928ca":"code","3580bc3d":"markdown","f855fd0e":"markdown","119bf32d":"markdown","92b44e62":"markdown","3155cc40":"markdown"},"source":{"4b5a794c":"# ! pip install googletrans","58ec28e8":"# ! pip install langdetect\n# ! pip install translate\n# ! pip install gensim\n# ! python -m spacy download en_core_web_lg","05bd8fad":"import spacy.cli\n# spacy.cli.download(\"en_core_web_lg\")\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","b4aeedf1":"## for data\nimport pandas as pd\nimport numpy as np\n\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## for text processing\nfrom dateutil import parser\nimport string\nimport re\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.stem.porter import PorterStemmer\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom gensim.models import KeyedVectors\n\n## for language detection\n## warnings !!\nimport warnings\nwarnings.filterwarnings('ignore')\n","5e59c3a6":"import zipfile \n\n# zf = zipfile.ZipFile(\"\/content\/drive\/My Drive\/Data\/ZS 2020.zip\")","f2a11246":"train=pd.read_csv('..\/input\/predicting-job-type-category-by-job-description\/train.csv')\ntest=pd.read_csv('..\/input\/predicting-job-type-category-by-job-description\/test.csv')\nsubmission=pd.read_csv('..\/input\/predicting-job-type-category-by-job-description\/sample_submission.csv')","7f43c588":"train.head() # we have 6 classes in 'job_type' and 11 classes in  'category'","762a07ff":"train.isnull().sum() # Verifying if needs any null modification !!","eca5ee77":"train['job_description'][0]","12726953":"# as it can be seen the target class 'job_type' is highly imbalanced !!\n#  'Any' and 'Part-Time' are below the means of consideration.\n\nprint('Class counts for Job_type : \\n',train['job_type'].value_counts())\n\nfig, ax = plt.subplots()\nfig.suptitle('job_type', fontsize=12)\ntrain['job_type'].reset_index().groupby('job_type').count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()\n","adc7f1f9":" # As it can be seen the target class 'category' is highly imbalanced !!\n    \nprint('Class counts for category : \\n',train['category'].value_counts())\n\nfig, ax = plt.subplots()\nfig.suptitle('category', fontsize=12)\ntrain['category'].reset_index().groupby('category').count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","d793d98f":"# Lets focus on job description !!!\n\n# Steps to be taken :\n#\n#     1. Text pre-processing:\n#         1.1 Normalize case\n#         1.2 Special character\n#         1.3 Stop words\n#         1.4 White space\n#         1.5 Stemming\n        \n#     2. Feature extraction:\n#         2.1 Word2vec embedding","15eb2623":"# Cleaning Process:\n\n# For dates removal ~\n\ndef is_valid_date(date_str): # sub function\n    try:\n        parser.parse(date_str)\n        return True\n    except:\n        return False\n    \ndef date_removal(data):\n    new_list = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in data]\n    return (new_list[0])\n\ndef stemmer_and_stopWord(doc):\n    \n    doc= nlp(doc)\n    token_list = []\n    for token in doc:\n      lemma = token.lemma_\n      if lemma == '-PRON-' or lemma == 'be':\n        lemma = token.text\n      token_list.append(lemma)\n\n    stemmed = token_list\n    \n    # Create list of word tokens after removing stopwords\n    \n    filtered_sentence =[] \n    for word in stemmed[:100]:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word)\n    return (' '.join(filtered_sentence))\n\ndef normaliz(filtered_sentence):\n        \n    words = [str(word).lower() for word in filtered_sentence.split()]\n    return  ' '.join(words[:100])\n\n# For number removal ~\n\ndef numbers_removal(data):\n    s = [data]\n    result = ''.join([i for i in s if not i.isdigit()])\n    return (result)\n# For punchuation & double white spaces ~\n\ndef punch_removal(words):\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in [words]]\n    return re.sub(' +', ' ', stripped[:100][0])\n\n\ndef cleaner(data):\n    string = [data]\n    string = date_removal(string)\n    # string = numbers_removal(string)\n    string = punch_removal(string)\n    string = stemmer_and_stopWord(string)\n    string = normaliz(string)\n    return string","d6e42ae7":"def cleaner(data):\n    string = [data]\n    string = date_removal(string)\n    # string = numbers_removal(string)\n    string = punch_removal(string)\n    string = stemmer_and_stopWord(string)\n    string = normaliz(string)\n    return string\n\nprint('* --------------- for train data --------------- *')\n\nlist=[]\nfor i in train['job_description']: # cleansing !!\n    list.append(cleaner(i))\n\ntrain['job_description']=pd.Series(list) # updating the attributee !!\n\nprint('* --------------- for test data --------------- *')\n\nlist=[]\nfor i in test['job_description']: # cleansing !!\n    list.append(cleaner(i))\n\ntest['job_description']=pd.Series(list) # updating the attributee !!\n","5c52bef9":"## Feature extarction or EDA :","67460617":"# adding a new attribute for differencing the language of teh 'job_description' !!\n\nimport langdetect\n\nprint('* ------------------------- for train data-------------------------------- *')\n\ntrain['lang'] = train[\"job_description\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\nfig, ax = plt.subplots()\nfig.suptitle('lang', fontsize=12)\ntrain['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\nplt.show()\n\nprint('* -------------------------- for test data ------------------------------- *')\n\ntest['lang'] = test[\"job_description\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\nfig, ax = plt.subplots()\nfig.suptitle('lang', fontsize=12)\ntest['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\nplt.show()\n","dd9b17ad":"# from translate import Translator\n\nfrom googletrans import Translator\ntranslator = Translator()","0acb0770":"# Translating non english text into english : for train and test sets !!\n\nprint('* ------------------------- for train data-------------------------------- *')\n\nall_eng = []\nfor index, lang in enumerate(train['lang']):\n    if lang != 'en':\n        es = translator.translate(train['job_description'][index])\n        all_eng.append(es.text)\n    else:\n        all_eng.append(train['job_description'][index])\n\ntrain['job_description']=pd.Series(all_eng)\n\nprint('* ------------------------- for test data-------------------------------- *')\n\nall_eng_test = []\nfor index, lang in enumerate(test['lang']):\n    if lang != 'en':\n        es = translator.translate(test['job_description'][index])\n        all_eng_test.append(es.text)\n    else:\n        all_eng_test.append(test['job_description'][index])\n\ntest['job_description']=pd.Series(all_eng_test)","dc55216a":"# Feature Extraction from the job discription :\n\nprint('* ------------------------- for train data-------------------------------- *')\n\n# the relevance of these feature will be tested on the later stages !!\n\ntrain['word_count'] = train[\"job_description\"].apply(lambda x: \n                                                     len(str(x).split(\" \")))\ntrain['char_count'] = train[\"job_description\"].apply(lambda x:\n                                                     sum(len(word) \n                                                     for word in str(x).split(\" \")))\ntrain['sentence_count'] = train[\"job_description\"].apply(lambda x: \n                                                         len(str(x).split(\".\")))\ntrain['avg_word_length'] = train['char_count'] \/ train['word_count']\ntrain['avg_sentence_lenght'] = train['word_count'] \/ train['sentence_count']\n\nprint('* ------------------------- for test data-------------------------------- *')\n\ntest['word_count'] = test[\"job_description\"].apply(lambda x: \n                                                   len(str(x).split(\" \")))\ntest['char_count'] = test[\"job_description\"].apply(lambda x: \n                                                   sum(len(word) \n                                                   for word in str(x).split(\" \")))\ntest['sentence_count'] = test[\"job_description\"].apply(lambda x: \n                                                       len(str(x).split(\".\")))\ntest['avg_word_length'] = test['char_count'] \/ test['word_count']\ntest['avg_sentence_lenght'] = test['word_count'] \/ test['sentence_count']\n","4645f517":"# A bivariate distributions to look at the correlation of a attribute with the target class !!\n\n# conclusion : The categories have a similar length distribution but the samples have different sizes.\n\nx = 'word_count'\ny = 'job_type'\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(x, fontsize=12)\nfor i in train[y].unique():\n    sns.distplot(train[train[y]==i][x], hist=True, kde=False, \n                 bins=10, hist_kws={\"alpha\":0.8}, \n                 axlabel=\"histogram\", ax=ax[0])\n    sns.distplot(train[train[y]==i][x], hist=False, kde=True, \n                 kde_kws={\"shade\":True}, axlabel=\"density\",   \n                 ax=ax[1])\nax[0].grid(True)\nax[0].legend(train[y].unique())\nax[1].grid(True)\nplt.show()","3959bc20":"train.head()","76f45d81":"ig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = train['word_count'].values\ntime_val = train['char_count'].values\n\nsns.distplot(amount_val, ax=ax[0], color='red')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n\nsns.distplot(time_val, ax=ax[1], color='blue')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\n\nplt.show()","1a161ab8":"x = 'word_count'\ny = 'job_type'\n\nsns.displot(train, x=\"word_count\", hue=\"category\")","eb0d408c":"# adding a new attribute for differencing the language of teh 'job_description' !!\n\nimport langdetect\n\nprint('* ------------------------- for train data-------------------------------- *')\n\ntrain['lang'] = train[\"job_description\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\nfig, ax = plt.subplots()\nfig.suptitle('lang', fontsize=12)\ntrain['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\nplt.show()\n\nprint('* -------------------------- for test data ------------------------------- *')\n\ntest['lang'] = test[\"job_description\"].apply(lambda x: langdetect.detect(x) if x.strip() != \"\" else \"\")\n\nfig, ax = plt.subplots()\nfig.suptitle('lang', fontsize=12)\ntest['lang'].reset_index().groupby('lang').count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, ax=ax).grid(axis='x')\nplt.show()\n","7c9233bb":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dropout, GRU, Activation, Embedding, Bidirectional,SpatialDropout1D, BatchNormalization, Conv1D, MaxPool1D","82873079":"MAX_NB_WORDS = 28523 + 1\n# Max number of words in each description.\nMAX_SEQUENCE_LENGTH = 100\n# This is fixed.\n\nEMBEDDING_DIM = 100\n\nJob_type = {\n    'Permanent':0,\n    'Contract\/Interim':1,\n    'Contract\/Temp':2,\n    'Temporary\/Seasonal':3,\n    'Any':4,\n    'Part-Time':6\n    }\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(train['job_description'].values)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","62a13227":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS, \n                      filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', \n                      lower=True, oov_token='OOV')\n\ntokenizer.fit_on_texts(train['job_description'].values)\nX = tokenizer.texts_to_sequences(train['job_description'].values)\n\nMAX_NB_WORDS = len(tokenizer.word_index) + 1 \n# Max number of words in each description.\nMAX_SEQUENCE_LENGTH = 100\n# This is fixed.\nEMBEDDING_DIM = 100\n\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) # Training labels\n\nY = pd.get_dummies(train.replace\n                   ({\"job_type\": Job_type})\n                   ['job_type'].values) # Target class","9aea8615":"X = tokenizer.texts_to_sequences(train['job_description'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","fe121e34":"Job_type = {\n    'Permanent':0,\n    'Contract\/Interim':1,\n    'Contract\/Temp':2,\n    'Temporary\/Seasonal':3,\n    'Any':4,\n    'Part-Time':6\n    }\n\n\nY = pd.get_dummies(train.replace({\"job_type\": Job_type})['job_type'].values)\nprint('Shape of label tensor:', Y.shape)","2a820aef":"# Data Splitting\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, \n                                                    random_state = 42)\n\n# Modeling\njob_model = Sequential()\njob_model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n\njob_model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\njob_model.add(LSTM(80, dropout=0.3, recurrent_dropout=0.3))\n\njob_model.add(Dense(128, activation='relu'))\njob_model.add(Dropout(0.3))\n\njob_model.add(Dense(6, activation='softmax'))\njob_model.compile(loss='categorical_crossentropy', optimizer='adam',\n                  metrics=['accuracy'])","1b3d6477":"class_w = {\n    0:0.18616874,\n    1:2.1041366,\n    2:10.1969697,\n    3:25.80825959,\n    4:116.65333333,\n    5:126.79710145\n}","e57ac65f":"job_model = Sequential()\njob_model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))\n# job_model.add(SpatialDropout1D(0.2)) # flattening the 2D embedding into 1D\n\njob_model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\njob_model.add(Dense(128, activation='relu'))\njob_model.add(Dropout(0.3))\n\njob_model.add(Dense(6, activation='softmax'))\njob_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 10\nbatch_size = 64\n\nhistory = job_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,\n                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)], class_weight = class_w)","42963712":"accr = job_model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","447f8100":"accr = job_model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show();","7369ff2e":"Category = {\n    'Pharmaceutical, Healthcare and Medical Sales':0,\n    'Clinical Research':1,\n    'Pharmaceutical Marketing':2,\n    'Manufacturing & Operations':3,\n    'Science':4,\n    'Medical Affairs \/ Pharmaceutical Physician':5,\n    'Regulatory Affairs':6,\n    'Medical Information and Pharmacovigilance':7,\n    'Data Management and Statistics':8,\n    'Quality-assurance':9,\n    'Pharmacy':10\n    }\n# Class weights :\n\nclass_w ={\n    0:0.35310261, \n    1:0.50903273,\n    2:1.486661  ,\n    3:0.72404519,\n    4:0.54833756,\n    5:3.47320365,\n    6:1.38324111,\n    7:2.84566596,\n    8:1.93283994,\n    9:2.32562467,\n    10:61.18181818\n }\n\nY = pd.get_dummies(train.replace({\"category\": Category})['category'].values)\nprint('Shape of label tensor:', Y.shape)","0c097686":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","1540de3d":"cat_model = Sequential()\ncat_model.add(Embedding(MAX_NB_WORDS, 300, input_length=X.shape[1]))\n# cat_model.add(SpatialDropout1D(0.2))\n\ncat_model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n\ncat_model.add(Dense(128, activation='relu'))\ncat_model.add(Dropout(0.5))\n\ncat_model.add(Dense(11, activation='softmax'))\ncat_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","f950b53c":"epochs = 10\nbatch_size = 64\n\nhistory = cat_model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,\n                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","1bf72e2f":"accr = cat_model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show();","2a79259c":"job_pred = []\ncategory_pred = []\n\nfor i in test['job_description']:\n    seq = tokenizer.texts_to_sequences([i]) # Using the same fitted tokenizer \n                                          # which was trained on the train data\n    padded = pad_sequences(seq, maxlen = MAX_SEQUENCE_LENGTH)\n    pred = job_model.predict(padded) # for job type\n    labels = ['Permanent','Contract\/Interim','Contract\/Temp','Temporary\/Seasonal',\n            'Any','Part-Time']\n    job_pred.append(labels[np.argmax(pred)])\n\n    pred = cat_model.predict(padded) # for category\n    labels = ['Pharmaceutical, Healthcare and Medical Sales','Clinical Research',\n            'Pharmaceutical Marketing','Manufacturing & Operations','Science',\n            'Medical Affairs \/ Pharmaceutical Physician','Regulatory Affairs',\n            'Medical Information and Pharmacovigilance',\n            'Data Management and Statistics','Quality-assurance','Pharmacy']\n    category_pred.append(labels[np.argmax(pred)])\n\n# creating submisison df\nsub = pd.DataFrame()\nsub['job_no'] = test['job_no']\nsub['job_type']=pd.Series(job_pred)\nsub['category']=pd.Series(category_pred)","fc966359":"# creating submisison df\n\nsub = pd.DataFrame()\nsub['job_no'] = test ['job_no']\n\nsub['job_type']=pd.Series(job_pred)\nsub['category']=pd.Series(category_pred)","01f928ca":"sub.head()","3580bc3d":"## 3. Predicting the Type and Category : -->","f855fd0e":"## Modeling :--->","119bf32d":"## 2. Training For Category !!!","92b44e62":"## 1. Training For Type !!!","3155cc40":"## Heyyy Everyyoneeee !! *** pleasee UPVOTE if you liked it OR if you have a good heart *** \ud83e\udd7a \ud83e\udd7a \ud83e\udd7a\n\n<img src=\"https:\/\/ih1.redbubble.net\/image.1600796187.0360\/mp,840x830,matte,f8f8f8,t-pad,1000x1000,f8f8f8.jpg\" width=\"600\" class=\"center\"\/>"}}