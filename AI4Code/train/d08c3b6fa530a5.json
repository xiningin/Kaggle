{"cell_type":{"715cef37":"code","4dc68384":"code","de665579":"code","22589e66":"code","d063a6ad":"code","9bbc9548":"code","c582a731":"code","619e67c2":"code","5969587c":"code","252d7f8a":"code","3abffdc0":"code","bc0d9b01":"code","8a638de1":"code","f746cf50":"code","71fdabe1":"code","fb0864c0":"code","75c97e2e":"code","3f02e8aa":"code","892c4306":"code","77a8cac1":"code","c6971710":"code","e5ae65ac":"code","86643f29":"code","3d9d7f81":"code","c0a8d783":"code","5156ada2":"markdown","87ed786a":"markdown","5f430dc7":"markdown","388ca3a3":"markdown","249e1bb4":"markdown","4af1d52e":"markdown","3b08a902":"markdown","ed73a72b":"markdown","4d7cc278":"markdown","9c24e089":"markdown","149a7706":"markdown","fab88748":"markdown","214ee0b1":"markdown","0a138d86":"markdown","2573e7ca":"markdown","fb0d7659":"markdown","40b0ae68":"markdown","89c4b1b8":"markdown","4335e814":"markdown","b92b279a":"markdown","baac9c87":"markdown","58eab335":"markdown"},"source":{"715cef37":"import logging\nimport os\nfrom os.path import splitext\nfrom os import listdir\nimport sys\nimport scipy.io\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image, ImageFilter, ImageEnhance\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nfrom IPython.display import FileLink","4dc68384":"\"\"\" Parts of the U-Net model \"\"\"\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n            \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n        nn.init.xavier_uniform_(self.conv1.weight)\n        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        \n        self.double_conv = nn.Sequential(\n            self.conv1,\n            #nn.Dropout(p=0.10),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            self.conv2,\n            #nn.Dropout(p=0.10),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        # if you have padding issues, see\n        # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n    \n    \n\"\"\" Full assembly of the parts to form the complete network \"\"\"\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 \/\/ factor)\n        self.up1 = Up(1024, 512 \/\/ factor, bilinear)\n        self.up2 = Up(512, 256 \/\/ factor, bilinear)\n        self.up3 = Up(256, 128 \/\/ factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","de665579":"from skimage.transform import rescale, rotate\nfrom torchvision.transforms import Compose\n\ndef transforms(scale=None, angle=None, flip_prob=None):\n    transform_list = []\n\n    if scale is not None:\n        transform_list.append(Scale(scale))\n    if angle is not None:\n        transform_list.append(Rotate(angle))\n    if flip_prob is not None:\n        transform_list.append(HorizontalFlip(flip_prob))\n\n    return Compose(transform_list)\n\n\nclass Scale(object):\n\n    def __init__(self, scale):\n        self.scale = scale\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        img_size = image.shape[0]\n\n        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n        #scale = np.random.uniform(low=0.5, high=0.8)\n        #print(scale,scale)\n        \n        image = rescale(\n            image,\n            (scale, scale),\n            multichannel=True,\n            preserve_range=True,\n            mode=\"constant\",\n            anti_aliasing=False,\n        )\n        mask = rescale(\n            mask,\n            (scale, scale),\n            order=0,\n            multichannel=True,\n            preserve_range=True,\n            mode=\"constant\",\n            anti_aliasing=False,\n        )\n\n        if scale < 1.0:\n            diff = (img_size - image.shape[0]) \/ 2.0\n            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n        else:\n            x_min = (image.shape[0] - img_size) \/\/ 2\n            x_max = x_min + img_size\n            image = image[x_min:x_max, x_min:x_max, ...]\n            mask = mask[x_min:x_max, x_min:x_max, ...]\n\n        return image, mask\n\n\nclass Rotate(object):\n\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        angle = np.random.uniform(low=-self.angle, high=self.angle)\n        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n        mask = rotate(\n            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n        )\n        return image, mask\n\n\nclass HorizontalFlip(object):\n\n    def __init__(self, flip_prob):\n        self.flip_prob = flip_prob\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        if np.random.rand() > self.flip_prob:\n            return image, mask\n\n        image = np.fliplr(image).copy()\n        mask = np.fliplr(mask).copy()\n\n        return image, mask\n    ","22589e66":"class BasicDataset(Dataset):\n    def __init__(self, imgs_dir, masks_dir, scale=1, enh_factor=None, isval=False, val_ids=[*range(1,11)], isEDES=None, mask_suffix='', transform=None):\n        self.imgs_dir = imgs_dir\n        self.masks_dir = masks_dir\n        self.scale = scale\n        self.enh_factor = enh_factor\n        self.mask_suffix = mask_suffix\n        self.transform = transform\n        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n        \n        val_fids = ['{0:03}'.format(ele) for ele in val_ids]\n        lenval = len(val_fids[0])\n        \n        if isval:\n            self.ids = [splitext(file)[0] for file in listdir(masks_dir)\n                        if not file.startswith('.') and file[:lenval] in val_fids]\n        else:\n            self.ids = [splitext(file)[0] for file in listdir(masks_dir)\n                        if not file.startswith('.') and file[:lenval] not in val_fids]\n        if isEDES=='ED':\n            ED = [x for x in self.ids if x[3:5]=='01']\n            self.ids = ED\n        if isEDES=='ES':\n            ES = [x for x in self.ids if x[3:5]!='01']\n            self.ids = ES\n            \n\n    \n        logging.info(f'Creating dataset with {len(self.ids)} examples')\n\n    def __len__(self):\n        return len(self.ids)\n\n    @classmethod\n    def preprocess(cls, pil_img, pil_mask, scale, enh_factor=None):\n        #resize\n        #w, h = pil_img.size\n        #newW, newH = int(scale * w), int(scale * h)\n        newW, newH = 256, 256\n        assert newW > 0 and newH > 0, 'Scale is too small'\n        pil_img = pil_img.resize((newW, newH))\n        pil_mask = pil_mask.resize((newW, newH))\n        \n        #image enhancement\n        if enh_factor is not None:\n            enh = np.random.uniform(low=1.0\/(1.0 + enh_factor), high=1.0 + enh_factor)\n            #pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n            pil_img = ImageEnhance.Contrast(pil_img).enhance(enh)\n            pil_img = ImageEnhance.Sharpness(pil_img).enhance(enh)\n            #pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=2))\n        \n        img_nd = np.array(pil_img)\n        mask_nd = np.array(pil_mask)\n\n        if len(img_nd.shape) == 2:\n            img_nd = np.expand_dims(img_nd, axis=2)\n            mask_nd = np.expand_dims(mask_nd, axis=2)\n\n        # HWC to CHW\n        img_trans = img_nd.transpose((2, 0, 1))\n        mask_trans = mask_nd.transpose((2, 0, 1))\n        \n        if img_trans.max() > 1:\n            img_trans = img_trans \/ 255\n            \n        mask_trans = 1.* (mask_trans > 0)\n\n        return img_trans, mask_trans\n\n\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n        img_file = glob(self.imgs_dir + idx + '.*')\n        \n\n        assert len(mask_file) == 1, \\\n            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n        assert len(img_file) == 1, \\\n            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n        mask = Image.open(mask_file[0])\n        img = Image.open(img_file[0])\n\n        assert img.size == mask.size, \\\n            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n        \n        img, mask = self.preprocess(img, mask, self.scale, self.enh_factor)\n        \n        if self.transform is not None:          \n            img, mask = self.transform((img.transpose((1, 2, 0)), mask.transpose((1, 2, 0))))\n            img = img.transpose((2, 0, 1))\n            mask = mask.transpose((2, 0, 1))\n\n        return {\n            'image': torch.from_numpy(img).type(torch.FloatTensor),\n            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n        }","d063a6ad":"from torch.autograd import Function\n\nclass DiceCoeff(Function):\n    \"\"\"Dice coeff for individual examples\"\"\"\n\n    def forward(self, input, target):\n        self.save_for_backward(input, target)\n        eps = 0.0001\n        self.inter = torch.dot(input.view(-1), target.view(-1))\n        self.union = torch.sum(input) + torch.sum(target) + eps\n\n        t = (2 * self.inter.float() + eps) \/ self.union.float()\n        return t\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         \/ (self.union * self.union)\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target\n\n\ndef dice_coeff(input, target):\n    \"\"\"Dice coeff for batches\"\"\"\n    if input.is_cuda:\n        s = torch.FloatTensor(1).cuda().zero_()\n    else:\n        s = torch.FloatTensor(1).zero_()\n\n    for i, c in enumerate(zip(input, target)):\n        s = s + DiceCoeff().forward(c[0], c[1])\n\n    return s \/ (i + 1)","9bbc9548":"# Enable dropout of the model\n# used in the function eval_net() and predict_img()\ndef enable_dropout(model):\n  for m in model.modules():\n    if m.__class__.__name__.startswith('Dropout'):\n      m.train()\n\ndef eval_net(net, loader, device, dropout=None):\n    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n    net.eval()\n    \n    if dropout is not None:\n        enable_dropout(net)\n        \n    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n    n_val = len(loader)  # the number of batch\n    tot = 0\n\n    #with tqdm(total=n_val, desc='Validation round', unit='batch', leave=False) as pbar:\n    for batch in loader:\n        imgs, true_masks = batch['image'], batch['mask']\n        imgs = imgs.to(device=device, dtype=torch.float32)\n        true_masks = true_masks.to(device=device, dtype=mask_type)\n\n        with torch.no_grad():\n            mask_pred = net(imgs)\n\n        if net.n_classes > 1:\n            tot += F.cross_entropy(mask_pred, true_masks).item()\n        else:\n            pred = torch.sigmoid(mask_pred)\n            pred = (pred > 0.5).float()\n            tot += dice_coeff(pred, true_masks).item()\n            #pbar.update()\n\n    net.train()\n    return tot \/ n_val","c582a731":"import cv2\nfrom scipy.spatial.distance import directed_hausdorff\n\n# def Hdistance(img1, img2):  \n    \n#     ctrs1, hierarchy = cv2.findContours(np.uint8(img1), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n#     ctrs2, hierarchy = cv2.findContours(np.uint8(img2), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n    \n#     u = None\n#     v = None\n    \n#     if ctrs1 :\n#         for i in range(len(ctrs1)) :\n#             if u is None:\n#                 u = np.squeeze(ctrs1[0])\n#             else:\n#                 u = np.concatenate([u, np.squeeze(ctrs1[0])])\n#     if ctrs2 :\n#         for i in range(len(ctrs2)) :\n#             if v is None:\n#                 v = np.squeeze(ctrs2[0])\n#             else:\n#                 v = np.concatenate([v, np.squeeze(ctrs2[0])])\n                \n#     distance = directed_hausdorff(u,v)[0]\n    \n#     return distance\n\ndef Hdistance(img1, img2):  \n    \n    u = np.transpose(np.nonzero(img1))\n    v = np.transpose(np.nonzero(img2))\n                \n    distance = max(directed_hausdorff(u,v)[0],directed_hausdorff(v,u)[0])\n    \n    return distance","619e67c2":"import random\nls=[*range(1,101)]\nrandom.seed(a=30)\nrandom.shuffle(ls)\nval_ids = [ls[20*i:20*(i+1)] for i in [*range(5)]]\nprint(val_ids[0])\nprint(val_ids[1])\nprint(val_ids[2])\nprint(val_ids[3])\nprint(val_ids[4])","5969587c":"def train_net(dir_img, dir_mask, dir_checkpoint,\n              net, device, epochs=5, batch_size=1, lr=0.001,\n              val_percent=0.1, save_cp=True,\n              img_scale=1, val_set = 0):\n\n    train = BasicDataset(dir_img, dir_mask, img_scale, isval=False, val_ids=val_ids[val_set])\n    for kk in range(5):\n        train_tf = BasicDataset(dir_img, dir_mask, img_scale, isval=False, val_ids=val_ids[val_set], enh_factor=1., transform=transforms(scale=0.1,angle=15, flip_prob=0.5))\n        #train_tf = BasicDataset(dir_img, dir_mask, img_scale, isval=False, val_ids=val_ids[val_set])\n        train = ConcatDataset([train,train_tf])\n        \n    val = BasicDataset(dir_img, dir_mask, img_scale, isval=True, val_ids=val_ids[val_set])\n    n_train = int(len(train))\n    n_val = int(len(val))\n    \n    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n\n    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n    global_step = 0\n\n    print(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {lr}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Checkpoints:     {save_cp}\n        Device:          {device.type}\n        Images scaling:  {img_scale}\n    ''')\n\n    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n    if net.n_classes > 1:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n        \n        \n    # validation score\n    progresses = []\n    valScores = []\n    val_score_max = -1\n    \n    for epoch in range(epochs):\n        net.train()\n\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}\/{epochs}', unit='img') as pbar:\n            for batch in train_loader:\n                imgs = batch['image']\n                true_masks = batch['mask']\n                assert imgs.shape[1] == net.n_channels, \\\n                    f'Network has been defined with {net.n_channels} input channels, ' \\\n                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                imgs = imgs.to(device=device, dtype=torch.float32)\n                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n                true_masks = true_masks.to(device=device, dtype=mask_type)\n\n                masks_pred = net(imgs)\n                loss = criterion(masks_pred, true_masks)\n                epoch_loss += loss.item()\n                writer.add_scalar('Loss\/train', loss.item(), global_step)\n\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n                optimizer.step()\n\n                pbar.update(imgs.shape[0])\n                global_step += 1\n                if global_step % (n_train \/\/ (10 * batch_size)) == 0:\n                    for tag, value in net.named_parameters():\n                        tag = tag.replace('.', '\/')\n                        writer.add_histogram('weights\/' + tag, value.data.cpu().numpy(), global_step)\n                        writer.add_histogram('grads\/' + tag, value.grad.data.cpu().numpy(), global_step)\n                    val_score = eval_net(net, val_loader, device)\n                    scheduler.step(val_score)\n                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n                    progress = 1 + global_step * batch_size \/ n_train\n                    if net.n_classes > 1:\n                        print('Validation cross entropy: {}'.format(val_score))\n                        writer.add_scalar('Loss\/test', val_score, global_step)\n                        progresses.append(progress)\n                        valScores.append(val_score)\n                    else:\n                        print('Validation Dice Coeff: {}'.format(val_score))\n                        writer.add_scalar('Dice\/test', val_score, global_step)\n                        progresses.append(progress)\n                        valScores.append(val_score)\n\n                    writer.add_images('images', imgs, global_step)\n                    if net.n_classes == 1:\n                        writer.add_images('masks\/true', true_masks, global_step)\n                        writer.add_images('masks\/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n                    \n                    if val_score > val_score_max:\n                        val_score_max = val_score\n                        print(val_score_max)\n                        try:\n                            os.mkdir(dir_checkpoint)\n                            logging.info('Created checkpoint directory')\n                        except OSError:\n                            pass\n                        torch.save(net.state_dict(),\n                                   dir_checkpoint + f'maxValScore.pth')\n                        logging.info(f'maxValScore{val_score_max} saved !')\n            \n        if save_cp:\n            try:\n                os.mkdir(dir_checkpoint)\n                logging.info('Created checkpoint directory')\n            except OSError:\n                pass\n            torch.save(net.state_dict(),\n                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n            logging.info(f'Checkpoint {epoch + 1} saved !')\n\n    print(progresses,valScores)\n    df = pd.DataFrame({'progress':progresses,'val_score': valScores}) \n    df.to_csv(f'valScores_batchSize{batch_size}.csv')\n    \n    writer.close() ","252d7f8a":"from torchvision import transforms as trans\nfrom PIL import ImageOps\nimport shutil\n\ndef locCrop(img, mask, alpha=1):\n    \n    sum0 = np.sum(mask,axis=0)\n    left = next((i for i, x in enumerate(sum0) if x), None)\n    right = len(sum0)-1-next((i for i, x in enumerate(np.flipud(sum0)) if x), None)\n\n    sum1 = np.sum(mask,axis=1)\n    up = next((i for i, x in enumerate(sum1) if x), None)\n    down = len(sum1)-1-next((i for i, x in enumerate(np.flipud(sum1)) if x), None)\n\n    center = [(up+down)\/\/2,(left+right)\/\/2]\n\n    figsize_max = min([center[0],len(sum1)-1-center[0],center[1],len(sum0)-1-center[1]])\n    figsize_min = int(2*max(abs(down-up)\/\/2,abs(right-left)\/\/2))\n    figsize = min(figsize_min,figsize_max) + alpha * abs(figsize_max-figsize_min)\n    figsize = int(figsize)\n\n    img_cropped = img[center[0]-figsize:center[0]+figsize+1,center[1]-figsize:center[1]+figsize+1]\n    mask_cropped = mask[center[0]-figsize:center[0]+figsize+1,center[1]-figsize:center[1]+figsize+1]\n    \n    return img_cropped, mask_cropped, center, figsize\n\ndef preCrop(img):\n    \n    shape = img.shape\n    delta = abs(shape[0]-shape[1])\/\/2\n    \n    if delta == 0:\n        return img, shape\n    \n    if shape[0]>shape[1]:\n        img_cropped = img[delta:-delta,:]   \n    else:\n        img_cropped = img[:,delta:-delta]    \n        \n    return img_cropped, shape","3abffdc0":"'''predict the mask for a single image'''\ndef predict_img(net,\n                full_img,\n                full_gt,\n                device,\n                scale_factor=1,#this should be in consistent with the one in train.py\n                out_threshold=0.5):\n    net.eval()\n    enable_dropout(net)\n    \n    img, gt = BasicDataset.preprocess(full_img, full_gt, scale_factor)\n    img = torch.from_numpy(img)\n    gt = torch.from_numpy(gt)\n    \n    img = img.unsqueeze(0)\n    img = img.to(device=device, dtype=torch.float32)\n    gt = gt.unsqueeze(0)\n    gt = gt.to(device=device, dtype=torch.float32)\n\n    with torch.no_grad():\n        output = net(img)\n\n        if net.n_classes > 1:\n            probs = F.softmax(output, dim=1)\n        else:\n            probs = torch.sigmoid(output)\n            \n        dc = DiceCoeff().forward((probs > out_threshold).float(), gt)\n        \n        probs = probs.squeeze(0)\n        tf = trans.Compose(\n            [\n                trans.ToPILImage(),\n                trans.Resize(full_img.size[1]),\n                trans.ToTensor()\n            ]\n        )\n\n        probs = tf(probs.cpu())\n        full_mask = probs.squeeze().cpu().numpy()\n\n    return full_mask > out_threshold, dc","bc0d9b01":"target = {1:'RV',2:'MYO',3:'LV'}\nwhichclass = 2\nval_set = 0\nepochs = 5\n\n\n\nimgs_dir = '..\/input\/acdc-uncropped\/imgs_train\/'\nimgs_dir_out = 'imgs_precropped\/'\nif os.path.isdir(imgs_dir_out):\n    shutil.rmtree(imgs_dir_out)\nos.mkdir(imgs_dir_out)\nimgs = [file for file in listdir(imgs_dir) if not file.startswith('.')]\nfor img in imgs:\n    pil_img = Image.open(imgs_dir+img)\n    img_cropped, shape = preCrop(np.array(pil_img))\n    addname = '_{0:03}'.format(shape[0]) + '_{0:03}'.format(shape[1]) +'_.png'\n    img_gray = ImageOps.grayscale(Image.fromarray(img_cropped).resize((256,256)))\n    img_gray.save(imgs_dir_out+img.replace('.png', addname))\nprint(f'Total number of images for training is: {len(os.listdir(imgs_dir_out))}')\n\n      \n\nfor wc in list(target.keys()):\n    masks_dir = imgs_dir.replace('imgs','masks')\n    masks_dir_out = target[wc] + '_precropped\/'\n    if os.path.isdir(masks_dir_out):\n        shutil.rmtree(masks_dir_out)\n    os.mkdir(masks_dir_out)\n    masks = [file for file in listdir(masks_dir) if not file.startswith('.')]\n    for mask in masks:\n        pil_mask = Image.open(masks_dir+mask)\n        masktg = (np.array(pil_mask)==wc)\n        if np.sum(masktg)>0:\n            mask_cropped, shape = preCrop(masktg)\n            addname = '_{0:03}'.format(shape[0]) + '_{0:03}'.format(shape[1]) +'_.png'\n            mask_gray = ImageOps.grayscale(Image.fromarray(mask_cropped).resize((256,256)))\n            mask_gray.save(masks_dir_out+mask.replace('.png', addname))\n    print(f'Number of nonempty masks for {target[wc]} is: {len(os.listdir(masks_dir_out))}')","8a638de1":"logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device {device}')\n# Change here to adapt to your data\n# n_channels=3 for RGB images\n# n_classes is the number of probabilities you want to get per pixel\n#   - For 1 class and background, use n_classes=1\n#   - For 2 classes, use n_classes=1\n#   - For N > 2 classes, use n_classes=N\nnet = UNet(n_channels=1, n_classes=1, bilinear=True)\nprint(f'Network:\\n'\n             f'\\t{net.n_channels} input channels\\n'\n             f'\\t{net.n_classes} output channels (classes)\\n'\n             f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\nnet.load_state_dict(torch.load('..\/input\/uncropped-models\/uncropped_'+ target[whichclass] + '.pth'))\n#net.load_state_dict(torch.load('checkpoints\/maxValScore.pth'))\nnet.to(device=device)\n\n# dir_img = '..\/input\/dpcprc\/imgs_png_' + target[whichclass] + '\/'\n# dir_mask = dir_img.replace('imgs','masks')\n# dir_checkpoint = 'checkpoints\/'\ndir_img = imgs_dir_out\ndir_mask = dir_img.replace('imgs',target[whichclass])\ndir_checkpoint = 'checkpoints\/'\n\ntrain_net(dir_img, dir_mask, dir_checkpoint,\n          net=net,device=device,\n          lr=0.0005,epochs=epochs,\n          batch_size=8,img_scale=1, val_set=val_set)\n\nFileLink(r'checkpoints\/maxValScore.pth')\n#FileLink(r'checkpoints\/uncropped_' + target[whichclass] + '_fd' + str(val_set) + '.pth')\n#Plot out dice coef v.s. progress\n# val_scores = pd.read_csv(f'valScores_batchSize8.csv')\n# fig, ax = plt.subplots(1, 1, figsize=(4,2))\n# ax.plot(val_scores['progress'],val_scores['val_score'])\n# ax.set_xlabel('progress')\n# ax.set_ylabel('accuracy')\n# plt.show()","f746cf50":"net = UNet(n_channels=1, n_classes=1)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.load_state_dict(torch.load('checkpoints\/maxValScore.pth'))\n#net.load_state_dict(torch.load('..\/input\/uncropped-models\/uncropped_MYO.pth'))\nnet.to(device=device)\n\n# dir_img = '..\/input\/dpcprc\/imgs_png_MYO\/'\n# dir_mask = dir_img.replace('imgs','masks')\n\n# path_imgs = dir_img\n# path_masks = dir_mask\n\npath_imgs_out='imgs_loccropped\/'\npath_masks_out=path_imgs_out.replace('imgs',target[whichclass])   \n\nif os.path.isdir(path_imgs_out):\n    shutil.rmtree(path_imgs_out)\nif os.path.isdir(path_masks_out):\n    shutil.rmtree(path_masks_out)\n    \nos.mkdir(path_imgs_out)\nos.mkdir(path_masks_out)\n\nimgs = os.listdir(dir_mask)\nN = len(imgs)\ncount = 0\ndcs = np.zeros([N,1])\n\nfor ii in range(N):\n    if ii%(N\/\/10)==0:\n        print(f'progress: {ii}\/{N}')\n        print(f'{count}\/{N} are predicted wrong.')\n    try:\n        img = Image.open(dir_img + imgs[ii])\n        mask = Image.open(dir_mask + imgs[ii])\n        pred, dc = predict_img(net=net, full_img=img, full_gt=mask, device=device, scale_factor=1)\n        dcs[ii] = dc.item()\n        img, _, _, _ = locCrop(np.array(img), pred, alpha=0)\n        mask, _, center, figsize = locCrop(np.array(mask), pred, alpha=0)\n        cropinfo = '_{0:03}'.format(center[0]) + '_{0:03}'.format(center[1]) + '_{0:03}_'.format(figsize) \n        oldname = imgs[ii].split('_')\n        newname = oldname[0]+cropinfo+'.png'\n        img_gray = ImageOps.grayscale(Image.fromarray(img).resize((256,256)))\n        img_gray.save(path_imgs_out + newname)\n        mask_gray = ImageOps.grayscale(Image.fromarray(mask).resize((256,256)))\n        mask_gray.save(path_masks_out + newname)\n    except:\n        count += 1\n        \nfiles = os.listdir(path_masks_out)\nprint('Number of files in ' + path_masks_out + ' is ' + str(len(files)))\n# fig, ax = plt.subplots(1, 1, figsize=(8,4))\n# ax.hist(dcs)\n# ax.set_xlabel('dice coefficient')\n# ax.set_ylabel('counts')\n# print(np.mean(dcs))\n\n# files = os.listdir(path_imgs_out)\n# img = Image.open(path_imgs_out+files[0])\n# mask = Image.open(path_masks_out+files[0])\n# fig, ax = plt.subplots(1,2,figsize=(10,8))\n# ax[0].imshow(img)\n# ax[1].imshow(mask)\n# plt.show()\n# print(files[0])","71fdabe1":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet = UNet(n_channels=1, n_classes=1, bilinear=True)\nprint(f'Network:\\n'\n      f'\\t{net.n_channels} input channels\\n'\n      f'\\t{net.n_classes} output channels (classes)\\n'\n      f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n\nnet.load_state_dict(torch.load('checkpoints\/maxValScore.pth'))\n#net.load_state_dict(torch.load('..\/input\/cropped-models\/cropped_MYO_fd1.pth'))\n#net.load_state_dict(torch.load('checkpoints_loc\/maxValScore.pth'))\nnet.to(device=device)\n  \ntrain_net(path_imgs_out, path_masks_out, 'checkpoints_loc\/',\n          net=net,device=device,\n          lr=0.0005,epochs=epochs,\n          batch_size=8,img_scale=1, val_set=val_set)\n\n#FileLink(r'checkpoints_loc\/cropped_' + target[whichclass] + '_fd' + str(val_set) + '.pth')\nFileLink(r'checkpoints_loc\/maxValScore.pth')","fb0864c0":"plt.style.use('classic')\n#mpl.rcParams['text.usetex'] = True\nmpl.rcParams['font.size'] = 16\nmpl.rcParams[\"figure.facecolor\"] = 'white'\nmpl.rcParams['axes.facecolor'] = 'white'\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 20\nmpl.rcParams['lines.linewidth'] = 4\nmpl.rcParams['lines.markersize'] = 10\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams[\"legend.loc\"]='lower right'","75c97e2e":"progress11 = [1.099488609948861, 1.198977219897722, 1.298465829846583, 1.397954439795444, 1.497443049744305, 1.596931659693166, 1.696420269642027, 1.795908879590888, 1.895397489539749, 1.99488609948861, 2.0943747094374707, 2.193863319386332, 2.293351929335193, 2.392840539284054, 2.4923291492329147, 2.591817759181776, 2.691306369130637, 2.790794979079498, 2.8902835890283587, 2.98977219897722, 3.0892608089260807, 3.188749418874942, 3.2882380288238027, 3.387726638772664, 3.4872152487215247, 3.586703858670386, 3.6861924686192467, 3.785681078568108, 3.8851696885169686, 3.98465829846583, 4.084146908414691, 4.183635518363552, 4.283124128312412, 4.382612738261274, 4.482101348210135, 4.581589958158996, 4.681078568107857, 4.780567178056717, 4.880055788005579, 4.97954439795444, 5.079033007903301, 5.178521617852161, 5.278010227801023, 5.377498837749884, 5.476987447698745, 5.576476057647605, 5.675964667596467, 5.775453277545328, 5.874941887494189, 5.974430497443049]\nscore11 = [0.8636284932798269, 0.8776765465736389, 0.867186861378806, 0.8588063607410509, 0.8804032425491177, 0.8780016388211932, 0.8665912090515604, 0.8764826971657422, 0.8777249954184707, 0.8810944180099332, 0.8792837590587382, 0.8785797990098292, 0.8796050049820725, 0.880178516008416, 0.8804260589638535, 0.8799173612983859, 0.8800301795103112, 0.879841651235308, 0.879574387657399, 0.8794345259666443, 0.8798628790037972, 0.8794224967761916, 0.8804717745099749, 0.8793627996833957, 0.8799176933814068, 0.8795922751329384, 0.8800389085497174, 0.8797656504475341, 0.8800505424032405, 0.8803096681225057, 0.8802962461296393, 0.8800350622254975, 0.8801655684198652, 0.8802403321071547, 0.879528786454882, 0.880151783933445, 0.8792021359716143, 0.880732003523379, 0.8798004619929255, 0.8800493928850913, 0.8796448829222698, 0.8792653436563453, 0.8802097172153239, 0.8803299020747749, 0.8794541772531004, 0.8802619345334112, 0.8802654049834426, 0.8796908989244577, 0.879714268810895, 0.8802875742620352]\nprogress21 =[1.099488609948861, 1.198977219897722, 1.298465829846583, 1.397954439795444, 1.497443049744305, 1.596931659693166, 1.696420269642027, 1.795908879590888, 1.895397489539749, 1.99488609948861, 2.0943747094374707, 2.193863319386332, 2.293351929335193, 2.392840539284054, 2.4923291492329147, 2.591817759181776, 2.691306369130637, 2.790794979079498, 2.8902835890283587, 2.98977219897722, 3.0892608089260807, 3.188749418874942, 3.2882380288238027, 3.387726638772664, 3.4872152487215247, 3.586703858670386, 3.6861924686192467, 3.785681078568108, 3.8851696885169686, 3.98465829846583, 4.084146908414691, 4.183635518363552, 4.283124128312412, 4.382612738261274, 4.482101348210135, 4.581589958158996, 4.681078568107857, 4.780567178056717, 4.880055788005579, 4.97954439795444, 5.079033007903301, 5.178521617852161, 5.278010227801023, 5.377498837749884, 5.476987447698745, 5.576476057647605, 5.675964667596467, 5.775453277545328, 5.874941887494189, 5.974430497443049]\nscore21 = [0.8649396592256974, 0.8728411818037227, 0.8732049161074112, 0.8804709072015724, 0.8794895130760816, 0.8808796880196552, 0.884669829388054, 0.8837418604870232, 0.8867632454755355, 0.8857988946291865, 0.8855989137474372, 0.8830434898940884, 0.890810988387283, 0.8895560223229078, 0.8910183286180302, 0.8905407895847243, 0.8900842362520646, 0.8908397166096435, 0.8910987717764718, 0.8912913993913301, 0.8910834752783483, 0.890699028968811, 0.8916924547175972, 0.891591058701885, 0.8916883079373107, 0.8913915181646541, 0.8913232647642797, 0.8911220528641526, 0.8914804823544561, 0.8913716734672079, 0.891605401525692, 0.8906243978714456, 0.8910963231203507, 0.8917624050257157, 0.8916468085074911, 0.8914127568809354, 0.8912546111612903, 0.8914024075683282, 0.8914984464645386, 0.8913899891230525, 0.8914608991875941, 0.8910256976983986, 0.8916520379027542, 0.8912246263757044, 0.8907616746668913, 0.8914485561604403, 0.8909434955947253, 0.8913534952669727, 0.8911190908782336, 0.8917179788861956]\nprogress12 = [1.0994475138121547, 1.1988950276243093, 1.298342541436464, 1.3977900552486187, 1.4972375690607735, 1.5966850828729282, 1.6961325966850829, 1.7955801104972375, 1.8950276243093924, 1.994475138121547, 2.0939226519337018, 2.1933701657458564, 2.292817679558011, 2.3922651933701657, 2.4917127071823204, 2.591160220994475, 2.6906077348066297, 2.790055248618785, 2.889502762430939, 2.988950276243094, 3.088397790055249, 3.1878453038674035, 3.287292817679558, 3.386740331491713, 3.4861878453038675, 3.585635359116022, 3.685082872928177, 3.7845303867403315, 3.883977900552486, 3.983425414364641]\nscore12 = [0.8948776214680774, 0.8968009986775987, 0.8937028912787742, 0.8965728282928467, 0.8907774610722319, 0.8992152239413972, 0.9011993408203125, 0.9013569976421113, 0.9027111162530616, 0.9014488331815029, 0.9026887150520974, 0.9015849296082842, 0.9016351344737601, 0.901884138584137, 0.9022486501551689, 0.9025593564865437, 0.901854322311726, 0.9021950224612622, 0.9021940015731974, 0.9022992621076867, 0.9020021405625851, 0.9020975447715597, 0.9021984148532787, 0.901768648878057, 0.9022896759053494, 0.9021218723439156, 0.9024313206368304, 0.9019861728587049, 0.9026742843871421, 0.9022086047111674]\nprogress22 = [1.0994475138121547, 1.1988950276243093, 1.298342541436464, 1.3977900552486187, 1.4972375690607735, 1.5966850828729282, 1.6961325966850829, 1.7955801104972375, 1.8950276243093924, 1.994475138121547, 2.0939226519337018, 2.1933701657458564, 2.292817679558011, 2.3922651933701657, 2.4917127071823204, 2.591160220994475, 2.6906077348066297, 2.790055248618785, 2.889502762430939, 2.988950276243094, 3.088397790055249, 3.1878453038674035, 3.287292817679558, 3.386740331491713, 3.4861878453038675, 3.585635359116022, 3.685082872928177, 3.7845303867403315, 3.883977900552486, 3.983425414364641]\nscore22 = [0.880016878564307, 0.8894319483574401, 0.8871558004237236, 0.8902061480156919, 0.8923427855714838, 0.892469141077488, 0.8939733429157988, 0.8955392659978664, 0.8940256643802562, 0.8936227915134836, 0.8951473451675253, 0.8998220803889823, 0.8997927003718437, 0.9004315290045231, 0.9002417503519261, 0.9001701448826079, 0.8992868839426243, 0.9001439226434585, 0.900293488451775, 0.9004353348245012, 0.8997154007566736, 0.8999213170497975, 0.9004183056506705, 0.900638240448972, 0.8999398591670584, 0.900075170587986, 0.9004230156857916, 0.9000258991058837, 0.9003503373328675, 0.9000945928248953]\nfig = plt.subplots(1,1,figsize=(8,5))\nplt.plot(progress1+[x+progress1[-1]-1 for x in progress2], score1+score2, label='Mloc',color='b',ls=':')\nplt.plot(progress1, score1, label='M', color='b', ls='-')\n#plt.plot([x+progress1[-1]-1 for x in progress2], score2, label='Mloc')\nplt.xlabel('progress')\nplt.ylabel('accuracy')\nplt.ylim([0.85,0.9])\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show()","3f02e8aa":"progress1 = [1.099488609948861, 1.198977219897722, 1.298465829846583, 1.397954439795444, 1.497443049744305, 1.596931659693166, 1.696420269642027, 1.795908879590888, 1.895397489539749, 1.99488609948861, 2.0943747094374707, 2.193863319386332, 2.293351929335193, 2.392840539284054, 2.4923291492329147, 2.591817759181776, 2.691306369130637, 2.790794979079498, 2.8902835890283587, 2.98977219897722, 3.0892608089260807, 3.188749418874942, 3.2882380288238027, 3.387726638772664, 3.4872152487215247, 3.586703858670386, 3.6861924686192467, 3.785681078568108, 3.8851696885169686, 3.98465829846583, 4.084146908414691, 4.183635518363552, 4.283124128312412, 4.382612738261274, 4.482101348210135, 4.581589958158996, 4.681078568107857, 4.780567178056717, 4.880055788005579, 4.97954439795444, 5.079033007903301, 5.178521617852161, 5.278010227801023, 5.377498837749884, 5.476987447698745, 5.576476057647605, 5.675964667596467, 5.775453277545328, 5.874941887494189, 5.974430497443049, 6.0739191073919105, 6.173407717340772, 6.272896327289633, 6.372384937238493, 6.4718735471873545, 6.571362157136216, 6.670850767085077, 6.770339377033937, 6.8698279869827985, 6.96931659693166, 7.068805206880521, 7.168293816829381, 7.2677824267782425, 7.367271036727104, 7.466759646675965, 7.566248256624825, 7.6657368665736865, 7.765225476522548, 7.864714086471409, 7.964202696420269, 8.06369130636913, 8.163179916317992, 8.262668526266854, 8.362157136215714, 8.461645746164574, 8.561134356113435, 8.660622966062297, 8.760111576011157, 8.85960018596002, 8.95908879590888]\nscore1 = [1.1907330283682832e-07, 1.1907330283682832e-07, 0.02427325332612402, 0.38323991251539213, 0.28456539782334345, 0.6098700153584383, 0.7034092995585227, 0.7257583384611168, 0.735852448307738, 0.767386216290143, 0.7493928232971503, 0.7647900678673569, 0.7790533900260925, 0.7475393268526817, 0.8070465022203873, 0.6034433610585271, 0.8186729416555288, 0.8242861981294594, 0.8324665719149064, 0.8166449057812594, 0.8187563042251431, 0.8032253798173399, 0.8359598432268415, 0.849766440537511, 0.845004349338765, 0.8480028303302064, 0.8459246645168382, 0.8506210896433616, 0.849363140913905, 0.8533017501539114, 0.8498111902451029, 0.8520236282932515, 0.8516871223644334, 0.8530464926544501, 0.8501871301203358, 0.8564872133488558, 0.8535619329433052, 0.8540137775090276, 0.8525879176295533, 0.8511430365698678, 0.8527966603940847, 0.8556524077240302, 0.8511891231244925, 0.8519795670801279, 0.8522080365492373, 0.8506941247959526, 0.8544574063651416, 0.853691324895742, 0.8509884403676403, 0.8556906301148084, 0.852085213271939, 0.8521156700289979, 0.8543028089464927, 0.8550138461346529, 0.8535430455694393, 0.8499389205660138, 0.852263512660046, 0.8556517253116686, 0.8492952281115006, 0.854258120059967, 0.8547801217254327, 0.8533223782266889, 0.8513119755959024, 0.8538461145089598, 0.8497290854551354, 0.851979597490661, 0.85334484066282, 0.8542870319619471, 0.8538557230209818, 0.8543270047830076, 0.8544341435237807, 0.853520581916887, 0.8542065754228708, 0.8550800449994146, 0.8549714672322176, 0.8499473600971456, 0.853166684812429, 0.8520406849530279, 0.8509265573657289, 0.8492035440036229]\nprogress2 = [1.0996275605214152, 1.1992551210428306, 1.2988826815642458, 1.3985102420856612, 1.4981378026070764, 1.5977653631284916, 1.697392923649907, 1.7970204841713222, 1.8966480446927374, 1.9962756052141528, 2.0959031657355682, 2.195530726256983, 2.2951582867783986, 2.394785847299814, 2.494413407821229, 2.5940409683426444, 2.69366852886406, 2.793296089385475, 2.89292364990689, 2.9925512104283056, 3.0921787709497206, 3.191806331471136, 3.2914338919925514, 3.3910614525139664, 3.490689013035382, 3.5903165735567972, 3.689944134078212, 3.7895716945996276, 3.889199255121043, 3.988826815642458]\nscore2 = [0.8215928904864253, 0.8324903882279688, 0.848473956390303, 0.8462184971692611, 0.8366210898574518, 0.8500317079680306, 0.8468478273372261, 0.8472242404003533, 0.8616796780605706, 0.853792527500464, 0.8691203776670962, 0.8702286129095116, 0.8664737197817588, 0.864110208287531, 0.8690099691858097, 0.8760749843655801, 0.8779594265684789, 0.8779666496782886, 0.8774571029507384, 0.8782852559673543, 0.8775566195955082, 0.876864205817787, 0.8767415844664281, 0.8768765123523011, 0.8794354893723313, 0.8781167986441631, 0.8791781213818765, 0.8796007900821919, 0.878403867994036, 0.8789295919087469]\nnoaug=[1.099488609948861, 1.198977219897722, 1.298465829846583, 1.397954439795444, 1.497443049744305, 1.596931659693166, 1.696420269642027, 1.795908879590888, 1.895397489539749, 1.99488609948861, 2.0943747094374707, 2.193863319386332, 2.293351929335193, 2.392840539284054, 2.4923291492329147, 2.591817759181776, 2.691306369130637, 2.790794979079498, 2.8902835890283587, 2.98977219897722, 3.0892608089260807, 3.188749418874942, 3.2882380288238027, 3.387726638772664, 3.4872152487215247, 3.586703858670386, 3.6861924686192467, 3.785681078568108, 3.8851696885169686, 3.98465829846583, 4.084146908414691, 4.183635518363552, 4.283124128312412, 4.382612738261274, 4.482101348210135, 4.581589958158996, 4.681078568107857, 4.780567178056717, 4.880055788005579, 4.97954439795444, 5.079033007903301, 5.178521617852161, 5.278010227801023, 5.377498837749884, 5.476987447698745, 5.576476057647605, 5.675964667596467, 5.775453277545328, 5.874941887494189, 5.974430497443049, 6.0739191073919105, 6.173407717340772, 6.272896327289633, 6.372384937238493, 6.4718735471873545, 6.571362157136216, 6.670850767085077, 6.770339377033937, 6.8698279869827985, 6.96931659693166, 7.068805206880521, 7.168293816829381, 7.2677824267782425, 7.367271036727104, 7.466759646675965, 7.566248256624825, 7.6657368665736865, 7.765225476522548, 7.864714086471409, 7.964202696420269, 8.06369130636913, 8.163179916317992, 8.262668526266854, 8.362157136215714, 8.461645746164574, 8.561134356113435, 8.660622966062297, 8.760111576011157, 8.85960018596002, 8.95908879590888]\nscorew =[1.1907330283682832e-07, 0.3683184080434089, 0.17003002747589227, 0.3522007988423717, 0.5373481098486452, 0.5709804168769291, 0.5893815409164039, 0.5416090324216959, 0.6560262496374092, 0.6550083148236178, 0.7859639415935594, 0.632151190115481, 0.8046263772614148, 0.7730331554704782, 0.8017043532157431, 0.772656111084685, 0.826451301574707, 0.827999899581987, 0.8280082211202505, 0.8295652793378246, 0.8336802076320259, 0.8317816756209548, 0.8370822996509318, 0.8390735898699079, 0.8325791930665776, 0.8400975149504992, 0.8344012036615488, 0.836971507996929, 0.8324971417991482, 0.8417948958825092, 0.838374991806186, 0.8401235986729058, 0.8389935396155532, 0.8420599735513026, 0.8399619411449043, 0.8395010232925415, 0.8382367625528452, 0.8408909963101757, 0.8382195903330433, 0.8400759830766794, 0.8357000265802655, 0.8398763403600576, 0.8402454329996693, 0.8387178365065127, 0.8393859547011706, 0.8403564509080381, 0.8412735182411817, 0.8387672049658639, 0.8397990440835759, 0.8364433840829499, 0.8359072500345658, 0.8365817228142096, 0.841271770243742, 0.8436129287797578, 0.8403777005721111, 0.8383661411246475, 0.839821884826738, 0.8386687064657405, 0.8369723753053315, 0.838394216128758, 0.8393560356023361, 0.8416854289113259, 0.8393667157815428, 0.8381567913658765, 0.8404526029314313, 0.8382811181399287, 0.8366442930941679, 0.8408230664778729, 0.8376644022610723, 0.8371759227343968, 0.8421115388675612, 0.8356834662203886, 0.8392643308152958, 0.8374798747957969, 0.8381880351475307, 0.8362888474853671, 0.8401335307529995, 0.8358709313431565, 0.840711519426229, 0.8426690162444601]\n\nfig = plt.subplots(1,1,figsize=(8,5))\nplt.plot(noaug, scorew, label='no data augmentation')\nplt.plot(progress1, score1, label='data augmentation')\n# plt.plot(progress1+[x+progress1[-1]-1 for x in progress2], score1+score2, label='Mloc')\n# plt.plot(progress1, score1, label='M')\n#plt.plot([x+progress1[-1]-1 for x in progress2], score2, label='Mloc')\nplt.xlabel('progress')\nplt.ylabel('accuracy')\nplt.ylim([0.5,0.9])\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show()","892c4306":"def predict_original_size_img(path1_img, path1_mask, path2_img, path2_mask, filename1):\n    \n    img = Image.open(path1_img+filename1)\n    mask= Image.open(path1_mask+filename1)\n    pred, dc = predict_img(net=net, full_img=img, full_gt=mask, device=device, scale_factor=1)\n    \n    names1 = filename1.split('_')\n    center, figsize = [int(names1[1]),int(names1[2])], int(names1[3])\n    roi = Image.fromarray(pred).resize((2*figsize+1,2*figsize+1))\n    pred1 = np.zeros((256,256))\n    pred1[center[0]-figsize:center[0]+figsize+1,center[1]-figsize:center[1]+figsize+1] = np.array(roi)\n    \n    filename2 = [file for file in os.listdir(path2_mask) if names1[0] in file]\n    names2 = filename2[0].split('_')\n    shape = [int(names2[1]),int(names2[2])]\n    delta = abs(shape[0]-shape[1])\/\/2\n    \n    org_img = np.zeros(shape)\n    org_mask = np.zeros(shape)\n    org_pred = np.zeros(shape)\n    \n    if shape[0] < shape[1]:\n        img2 = Image.open(path2_img+filename2[0]).resize((shape[0],shape[0]))\n        mask2 = Image.open(path2_mask+filename2[0]).resize((shape[0],shape[0]))\n        pred2 = Image.fromarray(pred1).resize((shape[0],shape[0]))\n        org_img[:,delta:shape[1]-delta] = np.array(img2)\n        org_mask[:,delta:shape[1]-delta] = np.array(mask2)\n        org_pred[:,delta:shape[1]-delta] = np.array(pred2)\n    else:\n        img2 = Image.open(path2_img+filename2[0]).resize((shape[1],shape[1]))\n        mask2 = Image.open(path2_mask+filename2[0]).resize((shape[1],shape[1]))\n        pred2 = Image.fromarray(pred1).resize((shape[1],shape[1]))\n        org_img[delta:shape[0]-delta,:] = np.array(img2)\n        org_mask[delta:shape[0]-delta,:] = np.array(mask2)\n        org_pred[delta:shape[0]-delta,:] = np.array(pred2)\n        \n    return org_img, org_mask, org_pred, dc","77a8cac1":"net = UNet(n_channels=1, n_classes=1)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#net.load_state_dict(torch.load('..\/input\/cropped-models\/cropped_MYO_fd5.pth'))\nnet.load_state_dict(torch.load('checkpoints_loc\/maxValScore.pth'))\nnet.to(device=device)\n\n# path_uncrop='..\/input\/dpcprc\/masks_png_MYO\/'\n# path_imgs_out='cropped_imgs_MYO\/'\n# path_masks_out=path_imgs_out.replace('imgs','masks')\n\nfor isEDES in ['ED','ES']:\n    \n    val = BasicDataset(path_imgs_out, path_masks_out, scale=1, isval=True, val_ids=val_ids[0], isEDES=isEDES)\n    print(len(val.ids))\n\n    #calculate mean Hausdorff distance for ED and ES seperately\n    hds = []\n    dcs = []\n    count = 0\n    for idx in val.ids:\n\n        org_img, org_mask, org_pred, dc = predict_original_size_img(path_imgs_out, path_imgs_out, dir_img, dir_mask, idx+'.png')\n        dcs.append(dc)    \n\n        if idx in val.ids[:1]:\n\n    #         org_img, org_mask, org_pred, dc = predict_original_size_img(path_imgs_out, path_imgs_out, dir_img, dir_mask, idx+'.png')\n    #         dcs.append(dc)\n            mpl.rcParams.update(mpl.rcParamsDefault)\n            fig, ax = plt.subplots(1,3,figsize=(10,6))\n            ax[0].imshow(org_img)\n            ax[1].imshow(org_mask>128)\n            ax[2].imshow(org_pred>0.5)\n            plt.show() \n            print(idx,org_pred.shape)\n\n        hds.append(Hdistance(org_mask>128,org_pred>0.5)) \n\n\n    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n    ax.hist(hds)\n    print(f'{count}\/{len(val.ids)} are wrong')\n    print(f'Average Hausdorff distance is {np.mean(hds)}')\n\n\n\n    #calculate mean dice for ED and ES seperately\n    val_loader = DataLoader(val, batch_size=1, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)  \n    N = 1\n    dcs = np.zeros([N,1])\n    for ii in range(N):\n        dcs[ii] = eval_net(net,val_loader,device) #,dropout=True)\n        print(f'Dice coefficient is {dcs[ii]}')\n\n    # fig, ax = plt.subplots(1, 1, figsize=(10,5))\n    # ax.hist(dcs)\n    # ax.set_xlabel('batch averaged dice coefficient')\n    # ax.set_ylabel('counts')\n    # print(np.mean(dcs),np.std(dcs))\n\n#MYO\nED_dc = [0.89394012]\nED_hd = [3.0880430847167193]\nES_dc = [0.91473856]\nES_hd = [3.385555763801318]","c6971710":"# net = UNet(n_channels=1, n_classes=1)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# net.load_state_dict(torch.load('..\/input\/cropped-models\/cropped_MYO_fd5.pth'))\n# #net.load_state_dict(torch.load('..\/input\/dropout015\/dropout0.15_MYO_fd0_1.pth'))\n# net.to(device=device)\n\n# # path_uncrop='..\/input\/dpcprc\/masks_png_MYO\/'\n# # path_imgs_out='cropped_imgs_MYO\/'\n# # path_masks_out=path_imgs_out.replace('imgs','masks')\n\n# #calculate mean Hausdorff distance for ED and ES seperately\n# dcs = []\n# for ii in range(500):\n#     img = Image.open(path_imgs_out+idx+'.png')\n#     mask= Image.open(path_masks_out+idx+'.png')\n#     pred, dc = predict_img(net=net, full_img=img, full_gt=mask, device=device, scale_factor=1)\n#     dcs.append(dc)","e5ae65ac":"#LV\nED=[0.9462,0.9443,0.9451985045316371,0.9570678024622746,0.9546]\nEDh=[3.1288981457606995,3.045574807838474,6.880659150035709,2.3735783089168847,2.6094133416681764]\nES=[0.8947,0.909,0.9108,0.93119632176196937,0.9327]\nESh=[4.869201996536504,3.327,4.296308674749143,2.6232,2.7097230082378547]\nprint(np.mean(ED))\nprint(np.mean(ES))\nprint(np.mean(EDh))\nprint(np.mean(ESh))\n#RV\nED=[0.915,0.912,0.90086896,0.92847941,0.92611341]\nEDh=[8.97292256797219,8.371719867951663,11.57335163197323,5.019068179659404,5.791896375713262]\nES=[0.8604534,0.84788184,0.84088899,0.87287045,0.88587517]\nESh=[9.385108289589429,9.550419508277407,11.744264557052109,7.752770689232926,8.261678977180875]\nprint(np.mean(ED))\nprint(np.mean(ES))\nprint(np.mean(EDh))\nprint(np.mean(ESh))\n#MYO\nED=[0.87930599,0.89374,0.8768134,0.8996050850504396,0.89201334]\nEDh=[4.196813975851807,3.61,4.21,2.584,2.64]\nES=[0.89878689,0.91036824,0.88742615,0.91195765,0.90956352]\nESh=[4.108809377223461,4.087738966126137,4.745608943101348,3.2631633836329916,2.878045816357292]\nprint(np.mean(ED))\nprint(np.mean(ES))\nprint(np.mean(EDh))\nprint(np.mean(ESh))","86643f29":"p=[0.05,0.1,0.15]\ns_mean=[0.8731267429152291,0.869039029,0.8629484493881823]\nsigma=[0.00022759099204778944,0.0006517089053875286,0.0005613714007445122]\nscores_05 = [0.87331342,0.87333143,0.8732863,0.87340288,0.87288952,0.87289357,0.87296747,0.87269603,0.8732761,0.87343681,0.87319477,0.87306098,0.87283402,0.87314631,0.87325575,0.87356689,0.87279015,0.87288194,0.87303729,0.87305472,0.87318868,0.87316056,0.87302827,0.87295709,0.87286149,0.87341638,0.8731365,0.87292554,0.87290065,0.87341309,0.87310909,0.87339313,0.87374544,0.87270255,0.87345979,0.87300217,0.87329002,0.87331728,0.87267972,0.87304084,0.87331457,0.87336918,0.87322015,0.8729096,0.87340271,0.87279351,0.87285423,0.87334867,0.87275539,0.87296747,0.87342451,0.87333014,0.87284233,0.87318678,0.87293003,0.87306413,0.87267714,0.87320023,0.87314106,0.8733636,0.87309104,0.87320482,0.87311574,0.87292176,0.87314028,0.87282322,0.87325132,0.87301823,0.87303616,0.8734009,0.8728462,0.87314371,0.87300764,0.87294975,0.87327676,0.87287998,0.87328415,0.87301052,0.87295519,0.87368164,0.87356092,0.87339108,0.87326908,0.87285096,0.87321546,0.87306212,0.87308202,0.87311798,0.87325014,0.87308527,0.87338681,0.87291477,0.87313492,0.87340304,0.87327668,0.87321046,0.87320059,0.8731605,0.8728588,0.87306366]\nscores_10 = [0.8697967784106732, 0.8693817990645766, 0.8710553497821093, 0.8687466697767376, 0.8684712124429643, 0.8690698056668044, 0.8692041685059667, 0.8695977672934532, 0.8677411967329681, 0.8691929432749749, 0.8675195140950381, 0.8692341388389468, 0.8698640009760856, 0.869625474140048, 0.8684838706254959, 0.8702223702520132, 0.8686109191924334, 0.869550997465849, 0.8678180234879256, 0.868933097999543, 0.8693770147487521, 0.8685399895906448, 0.8687310522049665, 0.8694990168139338, 0.8688508304208518, 0.8689055595546961, 0.8685190906375646, 0.8696642123907804, 0.8687148254364729, 0.8699958860874176, 0.8692010127753019, 0.8683642178028822, 0.868608687967062, 0.8689484779536724, 0.8697110473364592, 0.8693254534900189, 0.869063181951642, 0.8687352734804153, 0.8686322996765375, 0.867580402046442, 0.8683947152644396, 0.868176386244595, 0.8691585700958967, 0.8674692007340491, 0.868741292282939, 0.8694709821790457, 0.8690870878100395, 0.8687619062140584, 0.8688468013703823, 0.8691414667572827, 0.8689496354758739, 0.869881102591753, 0.8699932587891817, 0.8682038147002459, 0.8683469915017485, 0.8692991247028112, 0.8688899119943381, 0.8698239135742187, 0.868507260158658, 0.8680613169819117, 0.8692799863964319, 0.8685752669721842, 0.8691157221049071, 0.8679554709792137, 0.8683901857584715, 0.8690043222904206, 0.8694651550054551, 0.8691477528214455, 0.8690514581650496, 0.8682719898968935, 0.8698485902696848, 0.8688517032936215, 0.8693661823868751, 0.8694078106433153, 0.8705538620054721, 0.8693889805674553, 0.8688534115999937, 0.8697546669840812, 0.867976725101471, 0.8696103118360042, 0.8683804791048169, 0.8693442234396934, 0.8691976804286241, 0.8688010571151972, 0.8690754567086697, 0.8690780286490917, 0.8680252852575889, 0.8690958709269762, 0.8694042455404997, 0.8691308841854334, 0.8693637311086059, 0.8685282526165247, 0.8702637777104973, 0.8695156516134739, 0.869143930003047, 0.8694523039460182, 0.8689988732337952, 0.86930310966447, 0.8696112053096294, 0.8700269440561533]\n#scores_10_1 = [0.8695041617180824, 0.8695819115202147, 0.869921554978337, 0.8694646126735994, 0.8694982252947946, 0.8698973296946962, 0.8699726377172359, 0.8692059874655593, 0.8695959871250855, 0.8699221885430474, 0.8693505477500627, 0.8695090449298298, 0.8693889092082192, 0.8693479714364137, 0.8690055620512211, 0.8693532701752827, 0.8695043024492071, 0.8697922996449041, 0.8694078948133932, 0.8699777959847288, 0.8698058038350903, 0.8694281473293418, 0.8695799793400139, 0.8690699458134611, 0.8697155366435997, 0.8693223776327708, 0.8693599015766128, 0.8697834880463278, 0.8702069995788908, 0.8691727157725577, 0.8691886113033542, 0.8695850595961747, 0.8692518714397791, 0.8693563786877927, 0.8702029786232097, 0.8696609478506038, 0.8694580989879861, 0.8702071247353038, 0.8695613113200632, 0.8695358515206157, 0.8694146290192372, 0.8699480964418324, 0.8697672185491165, 0.8696104233218592, 0.8703169019040716, 0.8696279087033788, 0.8699434115557232, 0.8696980368029452, 0.8690376171460813, 0.8690754087879334, 0.8695617002144994, 0.8691071851807136, 0.869464520257714, 0.8692518390382429, 0.8699106886063519, 0.8694578544596162, 0.8696648157021053, 0.8697054823294937, 0.8696435349214939, 0.8693264063183115, 0.8689589793161278, 0.8697070323414761, 0.869996946833226, 0.8697368759685054, 0.8701242447869456, 0.8693943607046498, 0.8687436954793335, 0.8694093369142407, 0.8691480127715653, 0.869281923485234, 0.8699288432745621, 0.8700401980286078, 0.869844733933604, 0.8692880685461091, 0.8691833309002057, 0.86898835597851, 0.8690079038669157, 0.8692386481760017, 0.8695652967402646, 0.8692005178226831, 0.869600878925575, 0.8696625985506191, 0.8696909831402365, 0.8696444400110228, 0.8689008589220302, 0.8694221834837328, 0.8699720621132072, 0.8699058495746287, 0.8694579730758957, 0.8699018192219491, 0.8700858978764126, 0.8697136565662532, 0.8698822184815047, 0.8698526408160027, 0.8701315800837149, 0.8697505363810066, 0.8698928673827825, 0.8701202650602328, 0.8695017286021781, 0.8693233012309902]\n#scores_15 = [0.87419026,0.87449493, 0.87468625, 0.8744494, 0.87405172, 0.87425401, 0.87425781, 0.87477176, 0.87409505, 0.8734193, 0.87359523, 0.8742911, 0.87487478, 0.87447447, 0.87412666, 0.87439657, 0.87452888, 0.87414008, 0.87437926, 0.87414775, 0.87437481, 0.87430009, 0.87449099, 0.87384319, 0.87483437, 0.87448268, 0.87426891, 0.8742287, 0.87464983, 0.87431562, 0.8736647, 0.87460026, 0.87459545, 0.87410114, 0.87460884, 0.87397896, 0.87425215, 0.8744169, 0.87409821, 0.87417882, 0.87404053, 0.8739908, 0.87389715, 0.87431355, 0.87454532, 0.87440259, 0.87363784, 0.8739239, 0.87421054, 0.87464649, 0.87511009, 0.87406448, 0.87459085, 0.87450476, 0.87453247, 0.8740677, 0.87456093, 0.87469369, 0.87481578, 0.87464547, 0.8742097, 0.87461432, 0.87407086, 0.87394763, 0.87489132, 0.87462682, 0.87449617, 0.87430415, 0.874796, 0.87451398, 0.87427127, 0.8737603, 0.87428971, 0.87440497, 0.87376835, 0.87410355, 0.87387992, 0.87431058, 0.87514257, 0.87470644, 0.87409007, 0.87444854, 0.87489385, 0.8747469, 0.8744604, 0.87393671, 0.87429399, 0.87445799, 0.87442261, 0.87440114, 0.87479824, 0.87423457, 0.87390477, 0.87391397, 0.87409264, 0.87421278, 0.87450037, 0.87460683, 0.87494885, 0.87416295]\nscores_15 = [0.8628673703595996, 0.8624048015847802, 0.8623990073800087, 0.8632287028804422, 0.8607268931930211, 0.8629376422986388, 0.8629292130842805, 0.8627884811162949, 0.8623025111109018, 0.8628406461328268, 0.8621940354630351, 0.86354527246207, 0.8627034287899733, 0.8628176900744439, 0.8634056971222163, 0.8635280176997184, 0.8622370509058237, 0.8630144560337066, 0.8635791182145476, 0.8634279810637235, 0.8629674334824086, 0.8638452585414051, 0.8637461336329579, 0.8628504181280732, 0.8634079901874065, 0.8630818465352058, 0.8630608800426125, 0.8627412924915552, 0.8635623157024384, 0.8625804045796395, 0.862338138744235, 0.8622362869232894, 0.8635695446655154, 0.8627199812605977, 0.8632014139462263, 0.8627511885017156, 0.8630326968058943, 0.8633510715886951, 0.8623301983997226, 0.8629188737645745, 0.8625350360758602, 0.8623312294110655, 0.8627950203418732, 0.8624043329060078, 0.8632362184301019, 0.8642266337573529, 0.8623936504684389, 0.8623649432882666, 0.8625842549279332, 0.8629636375419796, 0.8621334177162499, 0.863564831390977, 0.863170187920332, 0.8627568396553397, 0.8640693670511246, 0.8626588713005185, 0.8633089047670365, 0.8633482159301639, 0.862763608172536, 0.8630064859986305, 0.8624780905805528, 0.8624703116901219, 0.862679332792759, 0.8627989114075899, 0.8637751768529415, 0.863292491659522, 0.8626386575587094, 0.8627015389502048, 0.8626961840316654, 0.8621253264322877, 0.8630137604475021, 0.8635489182174205, 0.8638475822471082, 0.8633016800135374, 0.8621791492402554, 0.8638886228576302, 0.8632199138775468, 0.8638071192428469, 0.8627064261212944, 0.8629739084467292, 0.8633303272351622, 0.8632696686312556, 0.8630903960205615, 0.8630421718023717, 0.8639113697037101, 0.8631205774471163, 0.8623588394001126, 0.862848153039813, 0.862566682510078, 0.8635766552761197, 0.8641947719827294, 0.862480667643249, 0.8620175444707274, 0.8634789062291384, 0.8631041492530493, 0.8627016406133771, 0.8629758520796895, 0.8618405134603381, 0.8632346680760383, 0.8627712093293667]\n#print(len(scores_05),len(scores_10),len(scores_15),np.mean(scores_15))\nfig = plt.subplots(1,1,figsize=(8,5))\nplt.hist(scores_05, histtype=u'step', label='p=0.05')\nplt.hist(scores_10, histtype=u'step', label='p=0.10')\nplt.hist(scores_15, histtype=u'step', label='p=0.15')\nplt.xlabel('accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()\n#print(np.array(dcs).reshape((100,)).tolist())","3d9d7f81":"from IPython.display import FileLink\nFileLink(r'checkpoints\/maxValScore.pth')","c0a8d783":".\/imgs_precropped","5156ada2":"# Dropout(uncertainty quantification)","87ed786a":"# Download output files from Kaggle\n1. Copy the target file to the directory \/kaggle\/working\n2. Run the FileLink code\n","5f430dc7":"# Dataloader","388ca3a3":"# Predict","249e1bb4":"4. Second training using the dataset after localized cropping","4af1d52e":"# Localized cropping","3b08a902":"# Import modules","ed73a72b":"# Train net","4d7cc278":"# Data augmentation (transforms)","9c24e089":"2. Results for dropout experiments","149a7706":"# Dice loss","fab88748":"# Main code stars from here\n","214ee0b1":"# Results\n1. Results for cross validation","0a138d86":"5. Fit the mask predicted by the second model into to the original-size background and calculate averaged dice coefficient and Hausdorff distance of the validation set","2573e7ca":"# Unet Model","fb0d7659":"# Figure setup","40b0ae68":"1. Precropping, resizing, bad data removal","89c4b1b8":"# Train-val split by patient number","4335e814":"# Evaluation","b92b279a":"3. Generate the predicted mask using model trained above and use it to do localized cropping","baac9c87":"2. First training using data without localized cropping","58eab335":"# Hausdorff distance"}}