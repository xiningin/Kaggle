{"cell_type":{"b8a7d45b":"code","3f268c58":"code","7a605a7c":"code","638eeba8":"code","67cd70c0":"code","4c7ab906":"code","008eb81b":"code","5874856b":"code","a1e9ee35":"code","8070b41b":"code","14529ace":"code","a7e9905b":"code","eac90343":"code","629739c5":"code","b0d396f9":"code","7c3a7e03":"code","9f193c2f":"code","ba1f74a7":"code","19f2e751":"code","26875dc1":"code","c3fcee33":"code","705f590f":"code","9fa6ced3":"code","bcd2aa82":"code","e3958da3":"code","e904a059":"code","7468abb7":"code","73b95e50":"code","e24ff948":"code","58ca980f":"code","203d531c":"code","102a8181":"code","94fa8497":"code","b936e3b9":"code","0e09617c":"code","94cdfeb3":"code","d353c7c3":"code","7f10abd5":"code","83d91ffa":"code","1d78f070":"code","34a32e19":"code","382a6a9b":"code","4279ff87":"code","8dd1b2c0":"code","b03d74f0":"code","0f72cda1":"code","c5218a8f":"code","f35c3003":"code","36c69d2a":"code","256bd22c":"code","0310fe27":"code","1527ecf1":"code","a020883c":"code","ce455210":"code","5e517b28":"code","5f49ac64":"code","82b103fc":"code","9ca8f629":"code","94e6ee18":"code","bfee57e5":"code","7491e199":"code","7108f667":"code","df0147cc":"code","6e103a3d":"code","d43eb54f":"code","8c357d07":"code","5ca8f2d5":"code","18726200":"code","9904b80d":"code","64e4c6c2":"code","2f7e8ca1":"code","cb9f6ae5":"code","04617848":"code","6717bd71":"code","5b660568":"code","543ee153":"code","fd85d78f":"code","674c8b26":"code","6f28602e":"markdown","6619d2cf":"markdown","4d9b4a50":"markdown","e1684548":"markdown","8d2e19b4":"markdown","941530bf":"markdown","6e61eecb":"markdown","95943a09":"markdown","56d9837d":"markdown","b165a671":"markdown","3b628317":"markdown","86c4ea8a":"markdown","acbc0c69":"markdown","9dffaa58":"markdown","9aedb7e7":"markdown","49404e41":"markdown","acf23495":"markdown","c39e85f2":"markdown","113c4af7":"markdown","fe3cf36b":"markdown","f66cf621":"markdown","0c3f50cd":"markdown","33fcbf41":"markdown"},"source":{"b8a7d45b":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3f268c58":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"Shape of trainig set:\",train.shape)\nprint(\"Shape of test set:\",test.shape)","7a605a7c":"y = train['SalePrice']\ntotal = train.append(test)\nprint(total.shape)\n# Appended train data and test data for cleaning purpose","638eeba8":"mask = total.isnull().sum()>0\nprint(sum(mask))\ntotal.isnull().sum()","67cd70c0":"total.columns","4c7ab906":"print(\"mean of SalePrice\", np.mean(y))\nprint(\"Median of Sale Price\", np.median(y))\nprint(\"Minimum Sale Price\", np.min(y))\nprint(\"Maximum Sale Price\", np.max(y))","008eb81b":"# Let's see the distribution\nplt.figure(figsize=(10,8))\nsns.distplot(y, label='Distribution of Sale Price')\nplt.show()","5874856b":"# Let's plot the whiskers plot \nplt.figure(figsize=(10,8))\nsns.boxplot(data=y)\nplt.xlabel('Outlier detection from Sale Price')\nplt.show()","a1e9ee35":"# Let's find out how many outlier points are present\nprint('Number of possible outlier points above 600000=',sum(y>600000))\nprint('Number of possible outlier points above 700000=',sum(y>700000))","8070b41b":"plt.figure(figsize=(10,8))\nsns.countplot(x='MSSubClass', data=train[train['SalePrice']<700000])\nplt.show()","14529ace":"plt.figure(figsize=(8,6))\nsns.countplot(x='MSZoning', data=train[train['SalePrice']<700000])\nplt.show()","a7e9905b":"train['SalePrice'].groupby(train.MSZoning).median()","eac90343":"plt.figure(figsize=(8,6))\nsns.countplot(x='SaleType', data=train[train['SalePrice']<700000])\nplt.show()","629739c5":"plt.figure(figsize=(8,6))\nsns.countplot(x='SaleCondition', data=train[train['SalePrice']<700000])\nplt.show()","b0d396f9":"plt.figure(figsize=(8,6))\nsns.countplot(x='BldgType', data=train[train['SalePrice']<700000])\nplt.show()","7c3a7e03":"plt.figure(figsize=(10,8))\nsns.distplot(train['YearRemodAdd'])","9f193c2f":"plt.figure(figsize=(15,15))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","ba1f74a7":"# Let's see how sales price varies with overall quality\nplt.figure(figsize=(10,10))\nax1 = plt.subplot(2,2,1)\nax2 = plt.subplot(2,2,2)\nsns.pointplot(x=train['OverallQual'],y=train['SalePrice'],ax=ax1)\nsns.lineplot(x=train['OverallCond'],y=train['SalePrice'],ax=ax2)","19f2e751":"sns.scatterplot(x=train.GrLivArea,y=train.SalePrice)","26875dc1":"temp_y=y.copy()\ntemp_train=train.copy()\ntemp_test=test.copy()\n# temp_train.drop('SalePrice', axis=1,inplace=True)\ntemp_total = temp_train.append(temp_test)\nprint(\"temp_y shape\", temp_y.shape)\nprint(\"temp_train shape\", temp_train.shape)\nprint(\"temp_test shape\", temp_test.shape)\nprint(\"temp_total shape\", temp_total.shape)","c3fcee33":"temp_total.shape","705f590f":"# s = temp_total.isnull().sum()\n# s[s>0]\nsns.heatmap(temp_total.isnull())","9fa6ced3":"print(temp_train['MiscFeature'].value_counts())\n# Lets just make Miscfeature such that if any value is present it is replaced as 1 otherwise 0\ntemp_total.loc[~temp_total['MiscFeature'].isnull(),'MiscFeature']=1\ntemp_total.loc[temp_total['MiscFeature'].isnull(),'MiscFeature']=0\nprint(temp_total['MiscFeature'].value_counts())","bcd2aa82":"# Lets do the same for PoolQC and Alley\ntemp_total.loc[~temp_total['PoolQC'].isnull(),'PoolQC']=1\ntemp_total.loc[temp_total['PoolQC'].isnull(),'PoolQC']=0\n\nprint(\"Null PoolQC in trainig set:\",temp_total['PoolQC'].isnull().sum())","e3958da3":"# For Alley and Fence\ntemp_total.loc[~temp_total['Alley'].isnull(),'Alley']=1\ntemp_total.loc[temp_total['Alley'].isnull(),'Alley']=0\n\nprint(\"Null Alley in test set:\",temp_total['Alley'].isnull().sum())\n\ntemp_total.loc[~temp_total['Fence'].isnull(),'Fence']=1\ntemp_total.loc[temp_total['Fence'].isnull(),'Fence']=0\n\nprint(\"Null Fence in trainig set:\",temp_total['Fence'].isnull().sum())","e904a059":"# I tried including these features but dropiing is better option.\ntemp_total.drop('MiscFeature', axis=1,inplace=True)\ntemp_total.drop('PoolQC', axis=1,inplace=True)\ntemp_total.drop('Alley', axis=1,inplace=True)\ntemp_total.shape","7468abb7":"s = temp_total.isnull().sum()\ns[s>0]","73b95e50":"(temp_total[temp_total['FireplaceQu'].isnull()])['Fireplaces'].value_counts()","e24ff948":"# In FireplaceQu, null means no fireplace, so let's put NF for No-Fireplace\n# Although same meant for above changed attributes, we changed them to binary values because they had too many null values\ntemp_total.loc[temp_total['FireplaceQu'].isnull(),'FireplaceQu']='NA'\nplt.figure(figsize=(10,8))\n\nsns.countplot(x='FireplaceQu',data=temp_total)\n\nprint(\"Null Fireplace in trainig set:\",temp_total['FireplaceQu'].isnull().sum())","58ca980f":"# Let's fill up the remaining null values\ns = temp_total.isnull().sum()\ns[s>0]","203d531c":"(temp_total[temp_total['GarageYrBlt'].isnull()])['GarageArea'].value_counts()","102a8181":"# If there is no garage why should there be anything related to garage\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageCars']=0\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageArea']=0.0\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageQual']='NA'\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageCond']='NA'\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageFinish']='NA'\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageType']='NA'\ntemp_total.loc[temp_total['GarageYrBlt'].isnull(),'GarageYrBlt']=0\ns = temp_total.isnull().sum()\ns[s>0]","94fa8497":"temp_total[temp_total['LotFrontage'].isnull()].head(10)","b936e3b9":"# There is some interesting pattern out there, let's make some plots.\nplt.figure(figsize=(20,10))\nax1 = plt.subplot(3,3,1)\nax2 = plt.subplot(3,3,2)\nax3 = plt.subplot(3,3,3)\nsns.countplot(x='MSZoning', data=temp_total[temp_total['LotFrontage'].isnull()],ax=ax1)\nsns.countplot(x='Street', data=temp_total[temp_total['LotFrontage'].isnull()], ax=ax2)\nsns.countplot(x='Utilities', data=temp_total[temp_total['LotFrontage'].isnull()], ax=ax3)","0e09617c":"# Lets see what are mean and median for these values.\nprint((temp_train[temp_train['MSZoning']=='RL'])['LotFrontage'].mean())\nprint((temp_train[temp_train['MSZoning']=='RL'])['LotFrontage'].median())\nprint((temp_train[temp_train['Street']=='Pave'])['LotFrontage'].mean())\nprint((temp_train[temp_train['Street']=='Pave'])['LotFrontage'].median())\nprint((temp_train[temp_train['Utilities']=='AllPub'])['LotFrontage'].mean())\nprint((temp_train[temp_train['Utilities']=='AllPub'])['LotFrontage'].median())\nprint((temp_train[(temp_train['MSZoning']=='RL')&(temp_train['Street']=='Pave')&(temp_train['Utilities']=='AllPub')])['LotFrontage'].mean())\nprint((temp_train[(temp_train['MSZoning']=='RL')&(temp_train['Street']=='Pave')&(temp_train['Utilities']=='AllPub')])['LotFrontage'].median())","94cdfeb3":"meanForLotFrontage = np.mean([72.0,69.0,69.0])\n# Not a good idea to hardcode the values but for now its okay.\nprint(meanForLotFrontage)\n# Let's fill the missing values.\ntemp_total.loc[temp_total['LotFrontage'].isnull(),'LotFrontage']=meanForLotFrontage\ntemp_total['LotFrontage'].isnull().sum()","d353c7c3":"(temp_total[temp_total['BsmtQual'].isnull()])['TotalBsmtSF'].value_counts()\n# All these should be filled with zero or NA because if there is no basement there should be no other value for basement.","7f10abd5":"# All remaining missing values are easy to fill, let's examine Basement properties where null means no basement\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtCond']='NA'\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtExposure']='NA'\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtFinType1']='NA'\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtFinType2']='NA'\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtFinSF1']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtFinSF2']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtUnfSF']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'TotalBsmtSF']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtFullBath']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtHalfBath']=0.0\ntemp_total.loc[temp_total['BsmtQual'].isnull(),'BsmtQual']='NA'\ns = temp_total.isnull().sum()\ns[s>0]","83d91ffa":"print((temp_train[(temp_train['Street']=='Pave') & (temp_train['Utilities']=='AllPub') ])['MasVnrArea'].mean())\nprint((temp_train[(temp_train['Street']=='Pave') & (temp_train['Utilities']=='AllPub') ])['MasVnrArea'].median())\nprint((temp_train[(temp_train['Street']=='Pave') & (temp_train['Utilities']=='AllPub') ])['MasVnrArea'].value_counts())\n(temp_train[(temp_train['Street']=='Pave') & (temp_train['Utilities']=='AllPub') ])['MasVnrType'].value_counts()","1d78f070":"# Let's fill all other missing values\ntemp_total.loc[temp_total['MasVnrType'].isnull(),'MasVnrType']='None'\ntemp_total.loc[temp_total['MasVnrArea'].isnull(),'MasVnrArea']=0.0\ntemp_total.loc[temp_total['MSZoning'].isnull(), 'MSZoning']='RL'\ntemp_total.loc[temp_total['Utilities'].isnull(), 'Utilities']='AllPub'\ntemp_total.loc[temp_total['Exterior1st'].isnull(), 'Exterior1st']='VinylSd'\ntemp_total.loc[temp_total['Exterior2nd'].isnull(), 'Exterior2nd']='VinylSd'\ntemp_total.loc[temp_total['BsmtFinSF1'].isnull(), 'BsmtFinSF1']=0.0\ntemp_total.loc[temp_total['BsmtFinSF2'].isnull(), 'BsmtFinSF2']=0.0\ntemp_total.loc[temp_total['BsmtUnfSF'].isnull(), 'BsmtUnfSF']=0.0\ntemp_total.loc[temp_total['TotalBsmtSF'].isnull(), 'TotalBsmtSF']=0.0\ntemp_total.loc[temp_total['Electrical'].isnull(), 'Electrical']='SBrkr'\ntemp_total.loc[temp_total['BsmtFullBath'].isnull(), 'BsmtFullBath']=0.0\ntemp_total.loc[temp_total['BsmtHalfBath'].isnull(), 'BsmtHalfBath']=0.0\ntemp_total.loc[temp_total['BsmtFullBath'].isnull(), 'BsmtFullBath']=0.0\ntemp_total.loc[temp_total['KitchenQual'].isnull(), 'KitchenQual']='TA'\ntemp_total.loc[temp_total['Functional'].isnull(), 'Functional']='Typ'\ntemp_total.loc[temp_total['GarageCars'].isnull(), 'GarageCars']=2.0\ntemp_total.loc[temp_total['GarageArea'].isnull(), 'GarageArea']=480.0\ntemp_total.loc[temp_total['SaleType'].isnull(), 'SaleType']='WD'","34a32e19":"# temp_total=temp_total.ffill(axis=0)\ns=temp_total.isnull().sum()\ns[s>0]","382a6a9b":"temp_total.loc[temp_total['BsmtCond'].isnull(),'BsmtCond']='TA'\ntemp_total.loc[temp_total['BsmtExposure'].isnull(),'BsmtExposure']='No'\ntemp_total.loc[temp_total['BsmtFinType2'].isnull(),'BsmtFinType2']='Unf'","4279ff87":"s=temp_total.isnull().sum()\ns[s>0]","8dd1b2c0":"# Lets plot ditribution of garage area and if it is not skewed we shall drop GarageCars\n# Otherwise we shall drop GarageArea\n# We can keep the product of GarageArea and GarageCars\nprint(temp_total['GarageArea'].corr(temp_total['GarageCars']))\nplt.figure(figsize=(10,8))\nax1=plt.subplot(2,2,1)\nax2=plt.subplot(2,2,2)\nsns.distplot(temp_train['GarageArea'],ax=ax1)\nsns.distplot(temp_train['GarageArea']*temp_train['GarageCars'],ax=ax2)","b03d74f0":"# Lets make a new feature by mutiplying the GarageCars and GarageArea\n# We will create a new variable summing all areas\ntemp_total['TotalArea']=temp_total['TotalBsmtSF'] + temp_total['1stFlrSF'] + temp_total['2ndFlrSF'] + temp_total['GrLivArea'] +temp_total['GarageArea']\ntemp_total['Bathrooms'] = temp_total['FullBath'] + temp_total['HalfBath']*0.5+temp_total['BsmtFullBath']+0.5*temp_total['BsmtHalfBath'] \nprint(temp_total.shape)\ntemp_total['Garage'] = temp_total['GarageArea']*temp_total['GarageCars']\n# temp_total.drop(['GarageArea','GarageCars'],axis=1,inplace=True)\n# temp_total.drop(['FullBath','HalfBath','BsmtHalfBath','BsmtFullBath'], axis=1, inplace=True)\nprint(temp_total.shape)","0f72cda1":"(temp_total.dtypes)","c5218a8f":"temp_total.columns","f35c3003":"# These features need to be categorical instead.\n# convert_features = {'LotFrontage':float,'LotArea':float,'YearBuilt':int,'YearRemodAdd':int,'MasVnrArea':float,'BsmtFinSF1':float,'BsmtFinSF2':float,'BsmtUnfSF':float,'TotalBsmtSF':float,'1stFlrSF':float,'2ndFlrSF':float,'LowQualFinSF':float,'GrLivArea':float,'TotalArea':float,'Bathrooms':float,'TotRmsAbvGrd':int,'Fireplaces':int,'GarageYrBlt':float,'Garage':float,'WoodDeckSF':float,'OpenPorchSF':float,'EnclosedPorch':float,'3SsnPorch':float,'ScreenPorch':float,'PoolArea':float}\n# temp_total=temp_total.astype(convert_features)","36c69d2a":"# Some actually categorical features are misinterpreted as categorical\nconvert_features={\n    'MSSubClass':str,\n    'OverallCond':str,\n    'OverallQual':str,\n    'SaleCondition':str\n}","256bd22c":"temp_total=temp_total.astype(convert_features)","0310fe27":"temp_total = pd.get_dummies(temp_total,drop_first=True)\ntemp_total.shape","1527ecf1":"temp_total.drop('Id',axis=True, inplace=True)","a020883c":"plt.figure(figsize=(20,10))\nax1=plt.subplot(2,2,1)\nax2=plt.subplot(2,2,2)\nprint(\"GrLivArea skew:\",temp_total['GrLivArea'].skew())\nprint(\"GrLivArea kurtosis:\",temp_total['GrLivArea'].kurtosis())\nsns.distplot(temp_total['GrLivArea'],ax=ax1)\nprint(\"LotArea skew:\",temp_total['LotArea'].skew())\nprint(\"LotArea kurtosis:\",temp_total['LotArea'].kurtosis())\nsns.distplot(temp_total['LotArea'],ax=ax2)","ce455210":"plt.figure(figsize=(20,10))\nax1=plt.subplot(2,2,1)\nax2=plt.subplot(2,2,2)\nprint(\"1stFlrSF skew:\",temp_total['1stFlrSF'].skew())\nprint(\"1stFlrSF kurtosis:\",temp_total['1stFlrSF'].kurtosis())\nsns.distplot(temp_total['1stFlrSF'],ax=ax1)\nprint(\"2ndFlrSF skew:\",temp_total['1stFlrSF'].skew())\nprint(\"2ndFlrSF kurtosis:\",temp_total['1stFlrSF'].kurtosis())\nsns.distplot(temp_total['2ndFlrSF'],ax=ax2)","5e517b28":"plt.figure(figsize=(20,10))\nax1=plt.subplot(2,2,1)\nax2=plt.subplot(2,2,2)\nprint(\"TotalArea skew:\",temp_total['TotalArea'].skew())\nprint(\"TotalArea kurtosis:\",temp_total['TotalArea'].kurtosis())\nsns.distplot(temp_total['TotalArea'],ax=ax1)\nprint(\"Garage skew:\",temp_total['Garage'].skew())\nprint(\"Garage kurtosis:\",temp_total['Garage'].kurtosis())\nsns.distplot(temp_total['Garage'],ax=ax2)","5f49ac64":"# Lets apply log1p transform to all of these\ntemp_total['GrLivArea']=np.log1p(temp_total['GrLivArea'])\ntemp_total['LotArea']=np.log1p(temp_total['LotArea'])\ntemp_total['1stFlrSF']=np.log1p(temp_total['1stFlrSF'])\ntemp_total['2ndFlrSF']=np.log1p(temp_total['2ndFlrSF'])\ntemp_total['TotalArea']=np.log1p(temp_total['TotalArea'])\ntemp_total['Garage']=np.log1p(temp_total['Garage'])","82b103fc":"temp_train = temp_total.iloc[:1460,:]\ntemp_test = temp_total.iloc[1460:,:]\nprint(temp_train.shape)\nprint(temp_test.shape)","9ca8f629":"temp_test.drop('SalePrice', axis=1,inplace=True)\ntemp_train=temp_train[temp_train['SalePrice']<700000]\ntemp_y = temp_train['SalePrice']\ntemp_train.drop('SalePrice', axis=1,inplace=True)\ntemp_y = np.log1p(temp_y)\nprint(temp_test.shape)\nprint(temp_train.shape)","94e6ee18":"sns.distplot(temp_y)","bfee57e5":"#For the purpose of preprocessing we need to divide the training set into validation set and train set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train, y_test = train_test_split(temp_train, temp_y,test_size=0.2, random_state=42)\nprint(\"X_train size:\", X_train.shape)\nprint(\"X_test size:\",X_test.shape)","7491e199":"from sklearn.preprocessing import RobustScaler\nscaler=RobustScaler()\nscaler.fit(X_train)\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)\n\ntemp_test=scaler.transform(temp_test)","7108f667":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV","df0147cc":"ridge=Ridge()\nparams = {'alpha':[i for i in range (1,50)]}\nridge_cv=GridSearchCV(estimator=ridge, param_grid=params, n_jobs=-1)\nridge_cv.fit(X_train,y_train)\nprint(\"The best Alpha is: \", ridge_cv.best_params_)\nprint(\"The best score achieved: \",np.sqrt(ridge_cv.best_score_))","6e103a3d":"# We obtained best ridge model with alpha=11. Let's keep this model for future use if any.\nridge = Ridge(alpha=0.25)\nridge.fit(X_train,y_train)\nprint(\"R2 score on training set\", ridge.score(X_train,y_train))\npred_ridge_train = ridge.predict(X_train)\nprint(\"RMSE Ridge on training set\", str(np.sqrt(mean_squared_error(y_train, pred_ridge_train))))\n# On test set\npred_ridge_test = ridge.predict(X_test)\nprint(\"R2 score on test set\", ridge.score(X_test,y_test))\nprint(\"RMSE Ridge on test set\", str(np.sqrt(mean_squared_error(y_test, pred_ridge_test))))","d43eb54f":"lasso=Lasso()\nparams = {'alpha':[0.0001,0.0004,0.0009,0.01,0.1,1.0,10,100,1000]}\nlasso_cv=GridSearchCV(estimator=lasso, param_grid=params, n_jobs=-1)\nlasso_cv.fit(X_train,y_train)\nprint(\"The best Alpha is: \", lasso_cv.best_params_)\nprint(\"The best score achieved: \",np.sqrt(lasso_cv.best_score_))","8c357d07":"# We obtained best ridge model with alpha=11. Let's keep this model for future use if any.\nlasso = Lasso(alpha=0.000098, max_iter=1e7)\nlasso.fit(X_train,y_train)\nprint(\"R2 score on training set\", lasso.score(X_train,y_train))\npred_lasso_train = lasso.predict(X_train)\nprint(\"RMSE Lasso on training set\", str(np.sqrt(mean_squared_error(y_train, pred_lasso_train))))\n# On test set\npred_lasso_test = lasso.predict(X_test)\nprint(\"R2 score on training set\", lasso.score(X_test,y_test))\nprint(\"RMSE Lasso on training set\", str(np.sqrt(mean_squared_error(y_test, pred_lasso_test))))","5ca8f2d5":"# Elastic Net\nfrom sklearn.linear_model import ElasticNetCV\nalphas = [10,1,0.1,0.01,0.001,0.002,0.003,0.004,0.005]\nl1_ratio=[0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\nelastic_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, cv=15, max_iter=1e6)\nelastic = elastic_cv.fit(X_train, y_train.ravel())\npred_elastic=elastic.predict(X_test)\nprint('RMSE test = ' + str(np.sqrt(mean_squared_error(y_test, pred_elastic))))\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)","18726200":"from sklearn.linear_model import ElasticNet\nelasticnet = ElasticNet(alpha=0.0000985, l1_ratio=1.0, max_iter=1e7)\nelastic = elasticnet.fit(X_train, y_train.ravel())\npred_elastic=elastic.predict(X_test)\nprint('RMSE test = ' + str(np.sqrt(mean_squared_error(y_test, pred_elastic))))","9904b80d":"# from xgboost.sklearn import XGBRegressor\n\n# xg_reg = XGBRegressor()\n# xgparam_grid= {'learning_rate' : [0.01],'n_estimators':[2000, 3460, 4000],\n#                                     'max_depth':[3], 'min_child_weight':[3,5],\n#                                     'colsample_bytree':[0.5,0.7],\n#                                     'reg_alpha':[0.0001,0.001,0.01,0.1,10,100],\n#                                    'reg_lambda':[1,0.01,0.8,0.001,0.0001]}\n\n# xg_grid=GridSearchCV(xg_reg, param_grid=xgparam_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n# xg_grid.fit(x_train,y_train)\n# print(xg_grid.best_estimator_)\n# print(xg_grid.best_score_)","64e4c6c2":"from xgboost.sklearn import XGBRegressor\nxgboost= XGBRegressor(base_score=0.5, booster='gbtree',gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=3, min_child_weight=0.1, n_estimators=1000,\n             n_jobs=-1, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.000098, reg_lambda=0.001)\nxgb_model=xgboost.fit(X_train,y_train)\nxgb_pred=xgb_model.predict(X_test)\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test, xgb_pred)))","2f7e8ca1":"from sklearn.ensemble import VotingRegressor\nvr = VotingRegressor([('Ridge', ridge),('Lasso',lasso),('ElasticNet',elastic),('XGBoost',xgb_model)])\nvoting_model = vr.fit(X_train, y_train.ravel())\nvr_pred = voting_model.predict(X_test)\nprint(\"RMSE:\",np.sqrt(mean_squared_error(vr_pred, y_test)))","cb9f6ae5":"from mlxtend.regressor import StackingRegressor\n\nstack_reg = StackingRegressor(regressors=[elastic,ridge, lasso, voting_model], \n                           meta_regressor=xgb_model, use_features_in_secondary=True\n                          )\n\nstack_model=stack_reg.fit(X_train, y_train.ravel())\nstacking_pred=stack_model.predict(X_test)\nprint(\"RMSE Stacking:\", np.sqrt(mean_squared_error(stacking_pred, y_test)))","04617848":"avg_test = (0.15*vr_pred+0.35*pred_lasso_test+0.15*stacking_pred+0.35*pred_ridge_test)\nprint(\"RMSE on averaged models:\", np.sqrt(mean_squared_error(avg_test,y_test)))","6717bd71":"# Let's train each of these models on full train set rather than X_train\ncombined_train_X = np.concatenate((X_train, X_test), axis=0)\ncombined_train_y = np.concatenate((y_train, y_test), axis=0)\nprint(combined_train_X.shape)\nprint(combined_train_y.shape)","5b660568":"ridge.fit(combined_train_X, combined_train_y)\nlasso.fit(combined_train_X, combined_train_y)\nelastic=elasticnet.fit(combined_train_X, combined_train_y.ravel())\nxgb_model = xgboost.fit(combined_train_X, combined_train_y)\nvoting_model = vr.fit(combined_train_X, combined_train_y.ravel())\nstack_model=stack_reg.fit(combined_train_X, combined_train_y.ravel())","543ee153":"final_lasso = np.expm1(lasso.predict(temp_test))\nfinal_ridge = np.expm1(ridge.predict(temp_test))\nfinal_vr = np.expm1(voting_model.predict(temp_test))\nfinal_stck = np.expm1(stack_model.predict(temp_test))","fd85d78f":"final_pred = (0.15*final_vr+0.35*final_lasso+0.15*final_stck+0.35*final_ridge)","674c8b26":"final_submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": final_pred\n    })\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","6f28602e":"Its clear that latter two properties are more likely. We will fill the missing values with mean of median value just to include all. Median is less sensitive to outliers, so we chose median. We use training set values so that we do not over optimize on test set.","6619d2cf":"## Bivariate Analysis","4d9b4a50":"### Dummy variables","e1684548":"There are some obvious correlations like GarageArea is highly correlated to GarageCars, GarageYrBuilt and YearBuilt, OverallCond is correlated to yearBuilt, etc.","8d2e19b4":"We can do the same for every variable but that would be too much. We will go for bivariate analysis for some most important variable. ","941530bf":"So we may take 600000 or 700000 as outlier point after further analysis","6e61eecb":"Although, we can do more analysis, for now I will copy the data into temporary variables and move on to data preparation.","95943a09":"#### This is not the final notebook, we have not done outlier removal and there is scope of dimensionality reduction.\nHowever, this is a begginer friendly notebook.\nI shall keep adding more to the notebook.","56d9837d":"#### Let's start with Ridge Regression and Lasso Regression","b165a671":"We have now removed the skewed data and we can now apply the normalization if needed but since all values are in same range after log we can go without normalization.","3b628317":"We will keep working on feature engineering but for now let's proceed.","86c4ea8a":"We have significantly reduced number of features, earlier it was more than 350 but now its 275. Since we are using linear model, lesser dimensions helps.","acbc0c69":"## Univariate Analysis","9dffaa58":"There are cetain outliers which can be filtered out by filtering data above 700000","9aedb7e7":"If there is no garage there is a null value for all garage fields, so let's put NA to all such fields. Also there is an outlier with 360.0 which we convert to 0.0 because if there is no garage there should be no Garage Area which means it may be an artificial error(error in data collection, etc.)","49404e41":"### Skewness removal","acf23495":"### Data Preparation","c39e85f2":"#### Scaling\nUsing Robust Scalar","113c4af7":"20 is most popularly sold and then 60 is sold. One common thing between them is that they represent 1945 and newer.","fe3cf36b":"We will use lasso or elastic net in final predictions","f66cf621":"We shall preprocesses total data based on training data not on the basis of total data as that would make the data biased towards test set.","0c3f50cd":"### Model Selection","33fcbf41":"So there are no more null values left.\nLet's now deal with correlations and perform feature Engineering\n### Feature Engineering"}}