{"cell_type":{"9123e19c":"code","c12e4667":"code","0475a014":"code","d34bcdb8":"code","43594f10":"code","16d91e70":"code","6b1daa80":"code","0116d133":"code","a823c391":"code","76d8835b":"code","23ae3d72":"code","ad266075":"code","44e05a9f":"code","6ae5fcfb":"code","1e3b879e":"code","e329b997":"code","4a1a5490":"code","7ffe584e":"markdown","4572e4a3":"markdown","b76e46b9":"markdown","ded26eb9":"markdown","537ee6ad":"markdown","5e58be94":"markdown","98eed376":"markdown","72442482":"markdown","8809a867":"markdown","af2160fb":"markdown","db15dbb1":"markdown","38a14a13":"markdown","4a0331f0":"markdown"},"source":{"9123e19c":"import os \nimport random\nimport math\n\nimport torch \nimport torch.nn as nn \nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lrs\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.swa_utils import SWALR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam, SGD\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.cuda.amp import autocast, GradScaler\n\n\n\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.models import resnet\n\n\nimport cv2\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport json\nimport time\nfrom contextlib import contextmanager\nimport pandas as pd\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n","c12e4667":"class CFG:\n    shape=(224,224,3)\n    model='ResNet50'\n    norm='batchnorm'\n    num_classes=6\n    pretrained=1\n    num_workers=4\n    n_fold=5\n    swa_lr=0.05\n    decay_type='cosine_warmup'\n    optimizer='ADAM'\n    weight_decay=0.0001\n    learning_rate=0.0001\n    checkpoint_name='swa'\n    checkpoint_dir='checkpoint'\n    pretrained_path=None\n    target_col='labels'\n    debug=False\n    apex=True\n    device='GPU' # ['TPU', 'GPU']\n    nprocs=1 # [1, 8]\n    print_freq=100\n    num_workers=4\n    freeze_epo = 0 # GradualWarmupSchedulerV2\n    warmup_epo = 1 # GradualWarmupSchedulerV2\n    cosine_epo = 2 # GradualWarmupSchedulerV2  ## 19\n    epochs = freeze_epo + warmup_epo + cosine_epo \n    scheduler='GradualWarmupSchedulerV2' \n    criterion='CrossEntropyLoss' # ['CrossEntropyLoss']\n    T_0=10 # CosineAnnealingWarmRestarts\n    lr=1e-4\n    min_lr=1e-6\n    batch_size=128 #[32, 64]\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    rand_augment=True\n    seed=2021\n    n_fold=5\n    train=True\n    swa_start = 1\n    swa = True\n    trn_fold=[0] #[0, 1, 2, 3, 4]\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","0475a014":"def get_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\ndef seed_torch(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)\nnp.random.seed(CFG.seed)","d34bcdb8":"def get_transforms(data):\n   \n    if data == 'train':\n        \n        return A.Compose([\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.Normalize([0.485,0.456,0.406],[0.299,0.224,0.225]),\n            ToTensorV2(),\n            \n        ])\n    elif data == 'valid':\n        \n        return A.Compose([\n            A.Resize(256, 256),\n            A.Normalize([0.485,0.456,0.406],[0.299,0.224,0.225]),\n            ToTensorV2(),\n            \n        ])\n    elif data == 'test':\n        \n        return A.Compose([\n            A.Resize(256, 256),\n            A.Normalize([0.485,0.456,0.406],[0.299,0.224,0.225]),\n            A.ToTensorV2(),\n            \n        ])","43594f10":"trainset = torchvision.datasets.ImageFolder(root=\"..\/input\/intel-image-classification\/seg_train\/seg_train\")\ntestset = torchvision.datasets.ImageFolder(root=\"..\/input\/intel-image-classification\/seg_test\/seg_test\")","16d91e70":"folds = pd.DataFrame(columns=['image_id','labels','fold'])\nfolds['labels'] = trainset.targets\nfolds['image_id'] = [s[0] for s in trainset.samples ]\n\nFold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds['labels'])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.groupby(['fold', 'labels']).size())","6b1daa80":"folds","0116d133":"class TrainDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['image_id'].values\n        self.labels = df['labels'].values\n        self.transform = transform \n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        \n        label = torch.tensor(self.labels[idx]).long()\n        return image, label","a823c391":"train_dataset = TrainDataset(folds, transform=get_transforms('train'))\n\nfor i in range(2):\n    image, label = train_dataset[i]\n    plt.imshow(np.transpose(image, (1,2,0)))\n    plt.show()\n","76d8835b":"\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val\n\ndef get_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","23ae3d72":"class GradualWarmupScheduler(_LRScheduler):\n    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n        total_epoch: target learning rate is reached at total_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    \"\"\"\n\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        self.multiplier = multiplier\n        if self.multiplier < 1.:\n            raise ValueError('multiplier should be greater thant or equal to 1.')\n        self.total_epoch = total_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_last_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]\n\n    def step(self, epoch=None, metrics=None):\n        if self.finished and self.after_scheduler:\n            if epoch is None:\n                self.after_scheduler.step(None)\n            else:\n                self.after_scheduler.step(epoch - self.total_epoch)\n            self._last_lr = self.after_scheduler.get_last_lr()\n        else:\n            return super(GradualWarmupScheduler, self).step(epoch)\n\nclass GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]","ad266075":"\nclass Flatten(nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n\ndef conv1x1(in_channel, out_channel, stride=1):\n    return nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride, bias=False)\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layer_config, num_classes=2, norm='batch', zero_init_residual=False):\n        super(ResNet, self).__init__()\n        norm = nn.BatchNorm2d\n\n        self.in_channel = 64\n\n        self.conv = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False)\n        self.norm = norm(self.in_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self.make_layer(block,  64*block.expansion, layer_config[0], stride=1, norm=norm)\n        self.layer2 = self.make_layer(block, 128*block.expansion, layer_config[1], stride=2, norm=norm)\n        self.layer3 = self.make_layer(block, 256*block.expansion, layer_config[2], stride=2, norm=norm)\n        self.layer4 = self.make_layer(block, 512*block.expansion, layer_config[3], stride=2, norm=norm)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.flatten = Flatten()\n        self.dense = nn.Linear(512*block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BasicBlock):\n                    nn.init.constant_(m.norm2.weight, 0)\n                elif isinstance(m, Bottleneck):\n                    nn.init.constant_(m.norm3.weight, 0)\n\n    def make_layer(self, block, out_channel, num_blocks, stride=1, norm=None):\n        norm = nn.BatchNorm2d\n\n        downsample = None\n        if stride != 1 or self.in_channel != out_channel:\n            downsample = nn.Sequential(\n                conv1x1(self.in_channel, out_channel, stride),\n                norm(out_channel),\n            )\n        layers = []\n        layers.append(block(self.in_channel, out_channel, stride, downsample, norm))\n        self.in_channel = out_channel\n        for _ in range(1, num_blocks):\n            layers.append(block(self.in_channel, out_channel, norm=norm))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.norm(out)\n        out = self.relu(out)\n        out = self.maxpool(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = self.flatten(out)\n        out = self.dense(out)\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, norm=None):\n        super(BasicBlock, self).__init__()\n        norm = nn.BatchNorm2d\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride)\n        self.norm1 = norm(out_channel)\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.norm2 = norm(out_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.norm2(out)\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n        out += identity\n        out = self.relu(out)\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, norm=None):\n        super(Bottleneck, self).__init__()\n        norm = nn.BatchNorm2d\n\n        mid_channel= out_channel \/\/ self.expansion\n        self.conv1 = conv1x1(in_channel, mid_channel)\n        self.norm1 = norm(mid_channel)\n        self.conv2 = conv3x3(mid_channel, mid_channel, stride)\n        self.norm2 = norm(mid_channel)\n        self.conv3 = conv1x1(mid_channel, out_channel)\n        self.norm3 = norm(out_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.norm3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass ResNet18(nn.Module):\n    def __init__(self, shape, num_classes=2, checkpoint_dir='checkpoint', checkpoint_name='ResNet18',\n                 pretrained=False, pretrained_path=None, norm='batch', zero_init_residual=False):\n        super(ResNet18, self).__init__()\n\n        if len(shape) != 3:\n            raise ValueError('Invalid shape: {}'.format(shape))\n        self.shape = shape\n        self.num_classes = num_classes\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint_name = checkpoint_name\n        self.H, self.W, self.C = shape\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        self.checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name, 'model.pt')\n\n        model = ResNet(BasicBlock, [2,2,2, 2], num_classes, norm, zero_init_residual)\n        \n        if pretrained:\n            print(\"Pretrained Weight is loaded!!\")\n            if pretrained_path is None:\n                print(\"Loading from torchvision models\")\n                model = resnet.resnet18(pretrained=True)\n                if zero_init_residual:\n                    for m in model.modules():\n                        if isinstance(m, resnet.Bottleneck):\n                            nn.init.constant_(m.bn3.weight, 0)\n                        elif isinstance(m, BasicBlock):\n                            nn.init.constant_(m.bn2.weight, 0)\n            else:\n                checkpoint = torch.load(pretrained_path)\n                model.load_state_dict(checkpoint)\n\n        self.features = nn.Sequential(*list(model.children())[:-2])\n        self.num_features = 512 * BasicBlock.expansion\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            nn.Linear(self.num_features, num_classes)\n        )\n\n    def save(self, checkpoint_name=''):\n        if checkpoint_name == '':\n            torch.save(self.state_dict(), self.checkpoint_path)\n        else:\n            checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name, checkpoint_name + '.pt')\n            torch.save(self.state_dict(), checkpoint_path)\n\n    def load(self, checkpoint_name=''):\n        if checkpoint_name == '':\n            self.load_state_dict(torch.load(self.checkpoint_path))\n        else:\n            checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name, checkpoint_name + '.pt')\n            self.load_state_dict(torch.load(checkpoint_path))\n\n    def forward(self, x):\n        out = x\n        out = self.features(out)\n        out = self.classifier(out)\n        return out\n\nclass ResNet50(nn.Module):\n    def __init__(self, shape, num_classes=2, checkpoint_dir='checkpoint', checkpoint_name='ResNet50',\n                 pretrained=False, pretrained_path=None, norm='batch', zero_init_residual=False):\n        super(ResNet50, self).__init__()\n\n        if len(shape) != 3:\n            raise ValueError('Invalid shape: {}'.format(shape))\n        self.shape = shape\n        self.num_classes = num_classes\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint_name = checkpoint_name\n        self.H, self.W, self.C = shape\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        self.checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name, 'model.pt')\n\n        model = ResNet(Bottleneck, [3,4,6,3], num_classes, norm, zero_init_residual)\n        \n        if pretrained:\n            print(\"Pretrained Weight is loaded!!\")\n            if pretrained_path is None:\n                print(\"Loading from torchvision models\")\n                model = resnet.resnet50(pretrained=True)\n                if zero_init_residual:\n                    for m in model.modules():\n                        if isinstance(m, resnet.Bottleneck):\n                            nn.init.constant_(m.bn3.weight, 0)\n            else:\n                checkpoint = torch.load(pretrained_path)\n                model.load_state_dict(checkpoint)\n\n        self.features = nn.Sequential(*list(model.children())[:-2])\n        self.num_features = 512 * Bottleneck.expansion\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            nn.Linear(self.num_features, num_classes)\n        )\n\n    def save(self, checkpoint_name=''):\n        if checkpoint_name == '':\n            torch.save(self.state_dict(), self.checkpoint_path)\n        else:\n            checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name, checkpoint_name + '.pt')\n            torch.save(self.state_dict(), checkpoint_path)\n\n    def load(self, checkpoint_name=''):\n        if checkpoint_name == '':\n            self.load_state_dict(torch.load(self.checkpoint_path))\n        else:\n            checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name, checkpoint_name + '.pt')\n            self.load_state_dict(torch.load(checkpoint_path))\n\n    def forward(self, x):\n        out = x\n        out = self.features(out)\n        out = self.classifier(out)\n        return out","44e05a9f":"def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, fold):\n    if CFG.device == 'GPU':\n        scaler = GradScaler()\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to train mode\n    model.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        if CFG.device == 'GPU':\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds, labels)\n                # record loss\n                losses.update(loss.item(), batch_size)\n                if CFG.gradient_accumulation_steps > 1:\n                    loss = loss \/ CFG.gradient_accumulation_steps\n                scaler.scale(loss).backward()\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n                if (step + 1) % CFG.gradient_accumulation_steps == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    global_step += 1\n                        \n        elif CFG.device == 'TPU':\n            y_preds = model(images)\n            loss = criterion(y_preds, labels)\n            # record loss\n            losses.update(loss.item(), batch_size)\n            if CFG.gradient_accumulation_steps > 1:\n                loss = loss \/ CFG.gradient_accumulation_steps\n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n                xm.optimizer_step(optimizer, barrier=True)\n                optimizer.zero_grad()\n                global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if CFG.device == 'GPU':\n            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n                print('Epoch: [{0}][{1}\/{2}] '\n                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      'Grad: {grad_norm:.4f}  '\n                      'Fold: {fold:4f}'\n                      #'LR: {lr:.6f}  '\n                      .format(\n                       epoch+1, step, len(train_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses,\n                       remain=timeSince(start, float(step+1)\/len(train_loader)),\n                       grad_norm=grad_norm,\n                       fold=fold\n                       #lr=scheduler.get_lr()[0],\n                       ))\n        elif CFG.device == 'TPU':\n            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n                xm.master_print('Epoch: [{0}][{1}\/{2}] '\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                                'Elapsed {remain:s} '\n                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                                'Grad: {grad_norm:.4f}  '\n                                'Fold: {fold:4f}'\n                                #'LR: {lr:.6f}  '\n                                .format(\n                                epoch+1, step, len(train_loader), batch_time=batch_time,\n                                data_time=data_time, loss=losses,\n                                remain=timeSince(start, float(step+1)\/len(train_loader)),\n                                grad_norm=grad_norm,\n                                fold=fold\n                                #lr=scheduler.get_lr()[0],\n                                ))\n    return losses.avg","6ae5fcfb":"def valid_fn(valid_loader, model, criterion, device, fold):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to evaluation mode\n    model.eval()\n    trues = []\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds, labels)\n        losses.update(loss.item(), batch_size)\n        # record accuracy\n        trues.append(labels.to('cpu').numpy())\n        preds.append(y_preds.softmax(1).to('cpu').numpy())\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if CFG.device == 'GPU':\n            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n                print('EVAL: [{0}\/{1}] '\n                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      'Fold: {fold:4f}'\n                      .format(\n                       step, len(valid_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses,\n                       remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                       fold=fold\n                       ))\n        elif CFG.device == 'TPU':\n            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n                xm.master_print('EVAL: [{0}\/{1}] '\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                                'Elapsed {remain:s} '\n                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                                'Fold: {fold:4f}'\n                                .format(\n                                step, len(valid_loader), batch_time=batch_time,\n                                data_time=data_time, loss=losses,\n                                remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                                fold=fold\n                                ))\n    trues = np.concatenate(trues)\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions, trues","1e3b879e":"# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n\n    train_dataset = TrainDataset(train_folds, transform=get_transforms(data='train'))\n    valid_dataset = TrainDataset(valid_folds, transform=get_transforms(data='valid'))\n    \n    train_loader = DataLoader(train_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=True, \n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=False, \n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n    \n    valid_labels = valid_folds[CFG.target_col].values\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='GradualWarmupSchedulerV2':\n            scheduler_cosine=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, CFG.cosine_epo)\n            scheduler_warmup=GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=CFG.warmup_epo, after_scheduler=scheduler_cosine)\n            scheduler=scheduler_warmup\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    if CFG.device == 'TPU':\n        device = xm.xla_device(fold+1)\n    elif CFG.device == 'GPU':\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    \n    def get_model(shape, num_classes):\n        model = eval(CFG.model)(\n            shape,\n            num_classes,\n            checkpoint_dir=CFG.checkpoint_dir,\n            checkpoint_name=CFG.checkpoint_name,\n            pretrained=CFG.pretrained,\n            pretrained_path=CFG.pretrained_path,\n            norm=CFG.norm,\n        )\n        return model\n    \n\n    model = get_model(CFG.shape, CFG.num_classes)    \n    model = model.to(device)\n    \n    \n\n    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n    \n    # ====================================================\n    # Stochastic Weighted Average\n    # ====================================================\n    \n    if CFG.swa:\n        swa_model = AveragedModel(model)\n        swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n    else:\n        swa_model = None\n        \n    # ====================================================\n    # Criterion - ['CrossEntropyLoss', ]\n    # ====================================================\n    def get_criterion():\n        if CFG.criterion=='CrossEntropyLoss':\n            criterion = nn.CrossEntropyLoss()\n        return criterion\n\n\n    # ====================================================\n    # loop \n    # ====================================================\n    criterion = get_criterion()\n    LOGGER.info(f'Criterion: {criterion}')\n\n    best_score = 0.\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        if CFG.device == 'TPU':\n            if CFG.nprocs == 1:\n                avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, fold)\n            elif CFG.nprocs > 1:\n                para_train_loader = pl.ParallelLoader(train_loader, [device])\n                avg_loss = train_fn(para_train_loader.per_device_loader(device), model, criterion, optimizer, epoch, scheduler, device, fold)\n        elif CFG.device == 'GPU':\n            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, fold)\n        \n        \n            \n        # eval baseline\n        if CFG.device == 'TPU':\n            if CFG.nprocs == 1:\n                avg_val_loss, preds, _ = valid_fn(valid_loader, model, criterion, device, fold)\n            elif CFG.nprocs > 1:\n                para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n                avg_val_loss, preds, valid_labels = valid_fn(para_valid_loader.per_device_loader(device), model, criterion, device, fold)\n                preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n                valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n        elif CFG.device == 'GPU':\n            avg_val_loss, preds, _ = valid_fn(valid_loader, model, criterion, device, fold)\n        \n        \n            \n        if isinstance(scheduler, GradualWarmupSchedulerV2):\n            if epoch >= CFG.swa_start and CFG.swa:\n                swa_model.update_parameters(model)\n                swa_scheduler.step()\n            else:\n                scheduler.step(epoch)\n\n        # scoring\n        score = get_score(valid_labels, preds.argmax(1))\n\n        elapsed = time.time() - start_time\n\n        if CFG.device == 'GPU':\n            LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n            LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - Score: {score:.4f}')\n        elif CFG.device == 'TPU':\n            if CFG.nprocs == 1:\n                LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n                LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - Score: {score:.4f}')\n            elif CFG.nprocs > 1:\n                xm.master_print(f'Epoch {epoch+1} - Fold {fold} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n                xm.master_print(f'Epoch {epoch+1} - Fold {fold} - Score: {score:.4f}')\n        \n        if score > best_score:\n            best_score = score\n            if CFG.device == 'GPU':\n                LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - Save Best Score: {best_score:.4f} Model')\n                torch.save({'model': model.state_dict(), \n                            'preds': preds},\n                           OUTPUT_DIR+f'{CFG.model}_fold{fold}_best_score.pth')\n            elif CFG.device == 'TPU':\n                if CFG.nprocs == 1:\n                    LOGGER.info(f'Epoch {epoch+1} - Fold {fold} - Save Best Score: {best_score:.4f} Model')\n                elif CFG.nprocs > 1:\n                    xm.master_print(f'Epoch {epoch+1} - Fold {fold} - Save Best Score: {best_score:.4f} Model')\n                xm.save({'model': model, \n                         'preds': preds}, \n                        OUTPUT_DIR+f'{CFG.model}_fold{fold}_best_score.pth')\n    \n    # Update bn statistics for the swa_model\n    if CFG.swa:\n        torch.cuda.empty_cache()\n        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n    \n    #==========================\n    # Compare SWA and Baseline\n    #==========================\n    \n    if CFG.swa:\n        avg_val_loss_baseline, preds_baseline, _ = valid_fn(valid_loader, model, criterion, device, fold)\n        score_baseline = get_score(valid_labels, preds_baseline.argmax(1))\n\n        avg_val_loss_swa, preds_swa, _ = valid_fn(valid_loader, swa_model, criterion, device, fold)\n        score_swa = get_score(valid_labels, preds_swa.argmax(1))\n\n        if CFG.device == 'GPU':\n            LOGGER.info(f'Fold {fold} - avg_val_loss_baseline: {avg_val_loss_baseline:.4f} ')\n            LOGGER.info(f'Fold {fold} - Score_Baseline: {score_baseline:.4f}')\n            LOGGER.info('=====================================')\n            LOGGER.info(f'Fold {fold} - avg_val_loss_swa: {avg_val_loss_swa:.4f}')\n            LOGGER.info(f'Fold {fold} - Score_SWA: {score_swa:.4f}')\n    \n    \n    if CFG.nprocs != 8:\n        check_point = torch.load(OUTPUT_DIR+f'{CFG.model}_fold{fold}_best_score.pth')\n        valid_folds['preds'] = check_point['preds'].argmax(1)\n\n    return valid_folds","e329b997":"# ====================================================\n# main\n# ====================================================\ndef main():\n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df[CFG.target_col].values\n        score = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.5f}')\n    \n    if CFG.train:\n        # train \n        oof_df = pd.DataFrame()\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(folds, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                if CFG.nprocs != 8:\n                    LOGGER.info(f\"========== fold: {fold} result ==========\")\n                    get_result(_oof_df)\n                    \n        if CFG.nprocs != 8:\n            # CV result\n            LOGGER.info(f\"========== CV ==========\")\n            get_result(oof_df)\n            # save result\n            oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)","4a1a5490":"if __name__ == '__main__':\n    if CFG.device == 'TPU':\n        torch.set_default_tensor_type('torch.FloatTensor')\n        Parallel(n_jobs=5, backend=\"threading\")(delayed(train_loop)(folds, i) for i in range(5))\n    elif CFG.device == 'GPU':\n        main()","7ffe584e":"## DataSet","4572e4a3":"## Utils","b76e46b9":"### Folding","ded26eb9":"## Models","537ee6ad":"## Main","5e58be94":"### ResNet Original","98eed376":"## Evaluator","72442482":"SWA has a wide range of applications and features:\n\n* SWA significantly improves performance compared to standard training techniques in computer vision (e.g., VGG, ResNets, Wide ResNets and DenseNets on ImageNet and CIFAR benchmarks [1, 2]).\n* SWA provides state-of-the-art performance on key benchmarks in semi-supervised learning and domain adaptation [2].\n* SWA was shown to improve performance in language modeling (e.g., AWD-LSTM on WikiText-2 [4]) and policy-gradient methods in deep reinforcement learning [3].\n* SWAG, an extension of SWA, can approximate Bayesian model averaging in Bayesian deep learning and achieves state-of-the-art uncertainty calibration results in various settings. Moreover, its recent generalization MultiSWAG provides significant additional performance gains and mitigates double-descent [4, 10]. Another approach, Subspace Inference, approximates the Bayesian posterior in a small subspace of the parameter space around the SWA solution [5].\n* SWA for low precision training, SWALP, can match the performance of full-precision SGD training, even with all numbers quantized down to 8 bits, including gradient accumulators [6].\n* SWA in parallel, SWAP, was shown to greatly speed up the training of neural networks by using large batch sizes and, in particular, set a record by training a neural network to 94% accuracy on CIFAR-10 in 27 seconds [11].\n\n[More info ](https:\/\/pytorch.org\/blog\/pytorch-1.6-now-includes-stochastic-weight-averaging\/)","8809a867":"## Data Loader","af2160fb":"## Trainer","db15dbb1":"## Scheduler","38a14a13":"Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it\u2019s easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model.","4a0331f0":"## Import Libs"}}