{"cell_type":{"e9131cfa":"code","535db5b4":"markdown"},"source":{"e9131cfa":"# this notebook makes it very easy for you to create\n# a file with sentiment from the subreddit of your\n# choosing about a stock ticker which you select.\n#\n# it also pulls down daily stock market data for the\n# same stock ticker via yahoo finace\n# you can change the number of submissions you\n# wish you analyze as well & for what time period\n#\n# this notebook is ready to run, and will produce two\n# .csv files automatically\n# the main parameters to change are all highlighted in the code below\n# they are:-\n#\n# selectedsubreddit = the name of the subreddit you want to carry out the analysis on\n# howmanysumbissions = how many submissions do you want to analyse. takes about 5 seconds for each one.\n# selectedTickerSymbol = the ticker symbol you want to explore. in this case 'GME' of course\n#\n# i was pleased that i was able to pull real live data from the web\n# and performing sentiment analysis i found most of this code on the internet\n# and then customised it to get the data I wanted from the sub reddit\n# r\/wallstreetsbets. My idea was to carry out sentiment analysis\n# against the top x number of comments\n# for a month, for a specific stock. I only record the sentiment from that comment, if there is at least one mention\n# of that stock. This wasn't a serious project, more just to prove the process works.\n\n# I decided to leave this as a dataset creation tool for sentiment analysis on reddit posts vs the chosen stock data\n# and have two files created. I will then add another notebook for EDA and possibly train a predictive model,\n# in the unlikely event of correlation ! I need to add the following as features on the extract as well:-\n\n# text blob: add polarity and subjectivity\n\n# i left in these commands below. I comment them out on Pycharm, then activated for Kaggle. They install the required\n# libraries\n\n!pip install --upgrade pip\n!pip install yfinance --no-dependencies\n!pip install multitasking --no-dependencies\n!pip install praw\n\n#import libraries. yfinance is yahoo finance and praw handles the reddit requests\nimport yfinance as yf\nimport pandas as pd\nfrom datetime import datetime\nimport praw\nimport nltk\n\n# some global variables. sorry !\nfrom numpy.core.defchararray import rstrip\n\nglobal selectedTickerSymbolCount\nglobal selectedTickerSymbol\nselectedTickerSymbolCount = 0\n\n# kaggle secrets functionality below. Such a great Kaggle function and only added because it was commented on\n# so thank you\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"secret_client_id\")\nsecret_value_1 = user_secrets.get_secret(\"secret_client_secret\")\nsecret_value_2 = user_secrets.get_secret(\"secret_user_agent\")\n\n# <<<<< Change the string below to the ticker you want to analyze, the program only records comments with a mention of\n# and it records how many mentions per post, which is added to the data as a feature. You might screen out all postings\n# with less than 5 mentions of the chosen ticker for example >>>>>\nselectedTickerSymbol = 'GME'\n\n# <<<<< select your subreddit below >>>>\nselectedsubreddit = 'wallstreetbets'\n\n# <<<<< how many submissions to analyze? >>>>>\n# 100000 should be the majority for the month, if not all, that's average 3,333 a day\n# 10000 should suffice.  I did the math and I think 100K would be 6 days, so 10K half a day?\nhowmanysumbissions = 25000\n\n#create a reddit object with PRAW\nreddit = praw.Reddit(client_id=secret_value_0,\n                     client_secret=secret_value_1,\n                     user_agent=secret_value_2)\n\n# get x number of top posts from the subreddit\ntop_posts = reddit.subreddit(selectedsubreddit).top('month', limit=howmanysumbissions)\n\n# install textblob if not already installed using \"pip install -U textblob\"\nfrom textblob import TextBlob\n\n# Download VADER, if not downloaded\n# nltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# create object for VADER sentiment function interaction\nsia = SentimentIntensityAnalyzer()\n\n# Sentiment analysis function for TextBlob tool\ndef text_blob_sentiment(review, sub_entries_textblob):\n    analysis = TextBlob(review)\n    if analysis.sentiment.polarity >= 0.0001:\n        if analysis.sentiment.polarity > 0:\n            sub_entries_textblob['positive'] = sub_entries_textblob['positive'] + 1\n            return 'Positive'\n\n    elif analysis.sentiment.polarity <= -0.0001:\n        if analysis.sentiment.polarity <= 0:\n            sub_entries_textblob['negative'] = sub_entries_textblob['negative'] + 1\n            return 'Negative'\n    else:\n        sub_entries_textblob['neutral'] = sub_entries_textblob['neutral'] + 1\n        return 'Neutral'\n\n\n# sentiment analysis function for VADER tool\ndef nltk_sentiment(review, sub_entries_nltk):\n    vs = sia.polarity_scores(review)\n    if not vs['neg'] > 0.05:\n        if vs['pos'] - vs['neg'] > 0:\n            sub_entries_nltk['positive'] = sub_entries_nltk['positive'] + 1\n            return 'Positive'\n        else:\n            sub_entries_nltk['neutral'] = sub_entries_nltk['neutral'] + 1\n            return 'Neutral'\n\n    elif not vs['pos'] > 0.05:\n        if vs['pos'] - vs['neg'] <= 0:\n            sub_entries_nltk['negative'] = sub_entries_nltk['negative'] + 1\n            return 'Negative'\n        else:\n            sub_entries_nltk['neutral'] = sub_entries_nltk['neutral'] + 1\n            return 'Neutral'\n    else:\n        sub_entries_nltk['neutral'] = sub_entries_nltk['neutral'] + 1\n        return 'Neutral'\n\n# replication of comment section of reddit post\ndef replies_of(top_level_comment, count_comment, sub_entries_textblob, sub_entries_nltk):\n    global selectedTickerSymbolCount\n    if len(top_level_comment.replies) == 0:\n        count_comment = 0\n        return\n    else:\n        for num, comment in enumerate(top_level_comment.replies):\n            try:\n                count_comment += 1\n                # print('-' * count_comment, comment.body)\n                text_blob_sentiment(comment.body, sub_entries_textblob)\n                nltk_sentiment(comment.body, sub_entries_nltk)\n\n                # Search for stock ticker symbol in comment body and increment counter if found\n                if comment.body.find(selectedTickerSymbol) != -1:\n                    selectedTickerSymbolCount += 1\n            except:\n                continue\n            replies_of(comment, count_comment, sub_entries_textblob, sub_entries_nltk)\n\n\ndef main():\n    # global int for recording how many times the program finds the stock ticker in the reddit comments\n    # i only wanted to extract records with a mention of the selected stock. I also will pull that number into my model\n    # as a feature.\n\n    # creates empty pandas dataframe, which will be filled with data from the reddit comments as the program runs\n    global selectedTickerSymbolCount\n    commentDF = pd.DataFrame()\n\n    # this creates a data frame called 'hist' and fills it with stock data for the selected stock. I am taking\n    # all the data as there are less than 10,000 records. It then prints the data set.\n    selectedTicker = yf.Ticker(selectedTickerSymbol)\n    hist = selectedTicker.history(period=\"max\")\n    print('\\n')\n    print('\\n')\n    print('\\n')\n    print(hist)\n\n    # loop through the submissions\n    submissioncounter = 1\n    for submission in top_posts:\n        sub_entries_textblob = {'negative': 0, 'positive': 0, 'neutral': 0}\n        sub_entries_nltk = {'negative': 0, 'positive': 0, 'neutral': 0}\n        print('Submission:', submissioncounter,'   |  Title of the post :', submission.title)\n        submissioncounter = submissioncounter + 1\n        text_blob_sentiment(submission.title, sub_entries_textblob)\n        nltk_sentiment(submission.title, sub_entries_nltk)\n        # print(\"\\n\")\n        submission_comm = reddit.submission(id=submission.id)\n\n        for count, top_level_comment in enumerate(submission_comm.comments):\n            # print(f\"-------------{count} top level comment start--------------\")\n            count_comm = 0\n            try:\n                # print(top_level_comment.body)\n                text_blob_sentiment(top_level_comment.body, sub_entries_textblob)\n                nltk_sentiment(top_level_comment.body, sub_entries_nltk)\n                replies_of(top_level_comment,\n                           count_comm,\n                           sub_entries_textblob,\n                           sub_entries_nltk)\n            except:\n                continue\n\n        # fetching the Unix timestamp from the comment\n        unix_time = submission_comm.created_utc\n        commentPostedUnixTime = str(datetime.fromtimestamp(unix_time))\n        commentYMD = commentPostedUnixTime[0:11:1]\n\n        # get the senitment values. These are from a dictionary so have to be access via the key\n        # and then array location. The code uses both textblob and vader.\n        textblobnegative = sub_entries_textblob.get('negative',0)\n        textblobpositive = sub_entries_textblob.get('positive',0)\n        textblobneutral = sub_entries_textblob.get('neutral',0)\n\n        vadernegative = sub_entries_nltk.get('negative',0)\n        vaderpositive = sub_entries_nltk.get('positive',0)\n        vaderneutral = sub_entries_nltk.get('neutral',0)\n\n        # if the program finds 1 or more mentions of the stock ticker in the comments, then it will record that record\n        # and append it to the data set\n        if selectedTickerSymbolCount > 0:\n\n            commentYMD = commentYMD.rstrip()\n            commentYMD = datetime.strptime(commentYMD, '%Y-%m-%d')\n\n            rowsToAppend = {'Title': submission.title, 'Ticker': selectedTickerSymbol, 'Date': commentYMD,\n                            'NumberOfTickerMentions': selectedTickerSymbolCount, 'Vader Neg': vadernegative,\n                            'Vader Pos': vaderpositive, 'Vader Neut': vaderneutral, 'textblob Negative': textblobnegative\n                            , 'textblob Positive': textblobpositive, 'textblob Neut': textblobneutral}\n            commentDF = commentDF.append(rowsToAppend, ignore_index = True)\n            selectedTickerSymbolCount = 0\n            # reset the counter that records how many times the stock symbol is found in the comments\n\n\n    # export the files as a .csv\n    # comment_analysis: this has the results of the sentiment analysis\n    # stockticker_history: this has the extract of the stock price information for the selected stock\n\n    commentDF.to_csv(r'comment_analysis.csv', index=False, header=True)\n    hist.to_csv(r'stockticker_history.csv', index=True,header=True)\n    print('\\n')\n    print('Sentiment analysis is below:')\n    print('\\n')\n    print(commentDF)\n    print('\\n')\n    print('Process complete. Files have been saved.')\nif __name__ == '__main__':\n    main()","535db5b4":"I have written another notebook, which takes the output from the sentiment analysis, merges the stockdata and sentiment, then creates an EDA report using the wonderful Pandas Profiling functionality ! It also saves that as a .html report !!!\n\nLink to that notebook is below. It is already setup with ~ 800 submissions ready to go as the input data (Wall Street Bets - GME submissions for the month Jan-21).\n\nhttps:\/\/www.kaggle.com\/iainmcintosh\/fork-of-reddit-sentiment-vs-stockprice-eda"}}