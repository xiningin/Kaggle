{"cell_type":{"580e8b07":"code","5ea673db":"code","8a46c7c9":"code","2c9ccac4":"code","38922c1c":"code","a889aacb":"code","84026281":"code","b617d584":"code","c3a35736":"code","e48d62c4":"code","fd58f159":"code","c34d75ad":"code","e343bd59":"code","ba14e684":"code","955c239d":"code","2e6f9fd5":"code","4d1a7ecb":"code","3c1b7e0c":"code","596db8d2":"code","1034577a":"code","f96614ac":"code","6de34538":"code","528b2db5":"code","16109d4f":"code","3ac41605":"code","2539b00a":"code","be81a540":"code","be2fa873":"code","884c06ff":"code","aaef20fb":"code","427793ac":"code","3276e8fa":"code","d5f55666":"code","d3a4f778":"markdown","177fb116":"markdown","e462347f":"markdown","f881db09":"markdown","51e5f1f6":"markdown","fee72edd":"markdown","66fb0869":"markdown","8e579336":"markdown","5d0283ea":"markdown","4268779a":"markdown","a855c1a3":"markdown","c9b9dea4":"markdown"},"source":{"580e8b07":"from IPython.display import Image\nimport os\n!ls ..\/input\/image211\nImage(\"..\/input\/image211\/banking.jpg\")","5ea673db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting libraries\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes = True)\n\nimport sklearn\nimport scipy\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a46c7c9":"df =pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","2c9ccac4":"df.info()","38922c1c":"df.isnull().sum()  #checking null value","a889aacb":"df.shape","84026281":"df.describe()","b617d584":"sns.set_style(\"darkgrid\")\nsns.countplot(data=df,x=\"Class\")\n","c3a35736":"df[\"Class\"].value_counts()","e48d62c4":"## Seprating data for analysis\n\nfraud = df[df.Class==1]\n\nnormal = df[df.Class==0]\n","fd58f159":"print(fraud.shape)\nprint(normal.shape)","c34d75ad":"## we will analyze transition amount during fraud cases\nfraud.Amount.describe()","e343bd59":"normal.Amount.describe()","ba14e684":"## comparing the values for both trasactions\ndf.groupby(\"Class\").mean()","955c239d":"## Now will try to make dataset balance by taking same value of fradulant data and normal data\nnormal_sample = normal.sample(n=492)","2e6f9fd5":"new_dataset = pd.concat([normal_sample,fraud],axis=0)","4d1a7ecb":"new_dataset.head()","3c1b7e0c":"## now we made new dataframe with Uniform distribution of fraud data and normal data\nnew_dataset[\"Class\"].value_counts()","596db8d2":"new_dataset.hist(bins=30,figsize=(15,15))\nplt.show()","1034577a":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","f96614ac":"# correlation gives us relation between each varibale. how much each variable is contributing.\ncorrelation =new_dataset.corr()\n\nplt.figure(figsize=(22,15))\nsns.heatmap(correlation,annot=True,cmap=\"RdYlGn\")","6de34538":"## Seprating independent and dependent variable\ninputs = new_dataset.drop(\"Class\",axis=\"columns\")\ntarget = new_dataset.Class","528b2db5":"inputs","16109d4f":"target","3ac41605":"from sklearn.model_selection import train_test_split","2539b00a":"##for proper distribution we are using Sratify\nX_train,X_test,y_train,y_test = train_test_split(inputs,target,test_size=0.2,stratify= target ,random_state=2)","be81a540":"print(\"Training data:{}\".format(X_train.shape))\nprint(\"Test data:{}\".format(X_test.shape))","be2fa873":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","884c06ff":"clf = LogisticRegression(max_iter=10000)\nclf.fit(X_train,y_train)","aaef20fb":"## Accuracy on training data\nX_train_prediction = clf.predict(X_train)\ntraining_data_accuracy = accuracy_score(X_train_prediction, y_train)","427793ac":"print(\"Accuracy on training data :\",training_data_accuracy)","3276e8fa":"## Accuracy on test data\nX_test_prediction =clf.predict(X_test)\ntesting_data_accuracy =accuracy_score(X_test_prediction,y_test)","d5f55666":"print(\"Accuracy on test data :\",testing_data_accuracy)","d3a4f778":"## Here we can see that there is huge difference between values of fraud and normal transaction","177fb116":"## Concating two dataframes","e462347f":"## this is highly unbalanced dataset\n0 -- Normal Transaction\n1 -- Fraud Transaction","f881db09":"## 3.Exploratory Data Analysis","51e5f1f6":"## training_data_accuracy = 94.79%\n## testing_data_accuracy  = 92.89%\n## So our model giving good accuracy (No Overfitting)","fee72edd":"## Content\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","66fb0869":"## Conclusion","8e579336":"## 2. Load Data","5d0283ea":"1. This dataset was highly un-balance, so with help of sampling technique made balance dataset.\n2. At the starting find out accuracy for training dataset and then for test dataset.","4268779a":"## Under-sampling","a855c1a3":"## 1. Load Libraries","c9b9dea4":"## Keypoints"}}