{"cell_type":{"b6cbaf26":"code","c19a0881":"code","55cc9d8f":"code","b70d5ac1":"code","4e1b67b3":"code","d763aa7c":"code","7e28d140":"code","530f9263":"code","ed9cefd2":"code","bc2dd8f9":"code","129f1cc0":"code","e2a84200":"code","59a1a59e":"markdown","1d148d25":"markdown","0f6c5c7b":"markdown","d3b9b107":"markdown","4e394676":"markdown","40d96f48":"markdown","2c420d72":"markdown","319d738a":"markdown","5a16388e":"markdown","75af175a":"markdown","f51a3982":"markdown","59d34739":"markdown","7ce0d664":"markdown","79d7aab8":"markdown","c3991502":"markdown"},"source":{"b6cbaf26":"from matplotlib import pyplot as plt\nimport math\nimport numpy as np\n\nlrs = np.zeros((1000, ))\nfor epoch in range(1000):\n    cos_inner = (math.pi * (epoch % 100)) \/ 100\n    lrs[epoch] = 0.01 \/ 2 * (math.cos(cos_inner) + 1)\n    \nplt.figure(1, figsize=(16, 8))\nplt.xlabel('epoch')\nplt.ylabel('learning rate')\nplt.plot(lrs)\nplt.show()","c19a0881":"from keras.callbacks import Callback\nfrom keras import backend\nfrom keras.models import load_model\n\n# this callback applies cosine annealing, saves snapshots and allows to load them\nclass SnapshotEnsemble(Callback):\n    \n    __snapshot_name_fmt = \"snapshot_%d.hdf5\"\n    \n    def __init__(self, n_models, n_epochs_per_model, lr_max, verbose=1):\n        \"\"\"\n        n_models -- quantity of models (snapshots)\n        n_epochs_per_model -- quantity of epoch for every model (snapshot)\n        lr_max -- maximum learning rate (snapshot starter)\n        \"\"\"\n        self.n_epochs_per_model = n_epochs_per_model\n        self.n_models = n_models\n        self.n_epochs_total = self.n_models * self.n_epochs_per_model\n        self.lr_max = lr_max\n        self.verbose = verbose\n        self.lrs = []\n \n    # calculate learning rate for epoch\n    def cosine_annealing(self, epoch):\n        cos_inner = (math.pi * (epoch % self.n_epochs_per_model)) \/ self.n_epochs_per_model\n        return self.lr_max \/ 2 * (math.cos(cos_inner) + 1)\n\n    # when epoch begins update learning rate\n    def on_epoch_begin(self, epoch, logs={}):\n        # update learning rate\n        lr = self.cosine_annealing(epoch)\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrs.append(lr)\n\n    # when epoch ends check if there is a need to save a snapshot\n    def on_epoch_end(self, epoch, logs={}):\n        if (epoch + 1) % self.n_epochs_per_model == 0:\n            # save model to file\n            filename = self.__snapshot_name_fmt % ((epoch + 1) \/\/ self.n_epochs_per_model)\n            self.model.save(filename)\n            if self.verbose:\n                print('Epoch %d: snapshot saved to %s' % (epoch, filename))\n                \n    # load all snapshots after training\n    def load_ensemble(self):\n        models = []\n        for i in range(self.n_models):\n            models.append(load_model(self.__snapshot_name_fmt % (i + 1)))\n        return models\n","55cc9d8f":"import pandas as pd\nimport os\n\n# read train and test csvs\npath = '\/kaggle\/input\/digit-recognizer\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\n\n# extract target and remove from train\ntarget = train['label']\ntrain.drop(columns=['label'], inplace=True)\n\n# reshape datasets according to image size (with 1 channel)\nim_size = 28\ntrain = train.to_numpy().reshape((-1, im_size, im_size, 1))\ntest = test.to_numpy().reshape((-1, im_size, im_size, 1))\n\n# adjust pixels brightnesses to range 0..1\ntrain = train \/ 255\ntest = test \/ 255\n\ntrain.shape, test.shape","b70d5ac1":"from sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\ntarget = to_categorical(target)\n\nx_train, x_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=289)\n\nx_train.shape, x_test.shape","4e1b67b3":"plt.figure(1, figsize=(14, 10))\nfor i in range(18):\n    plt.subplot(3, 6, i + 1)\n    plt.imshow(x_train[i].reshape((im_size, im_size)), cmap='gray')\n    plt.title(str(np.argmax(y_train[i])))\nplt.show()","d763aa7c":"from keras.models import Sequential\nfrom keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Conv2D(16, 3, activation='relu', padding='same', input_shape=(im_size, im_size, 1)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(16, 3, activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(16, 5, activation='relu', padding='same'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(32, 3, activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(32, 3, activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(32, 5, activation='relu', padding='same'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64, 3, activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(64, 3, activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(64, 5, activation='relu', padding='same'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['acc']\n)","7e28d140":"from keras.preprocessing.image import ImageDataGenerator\n\nimagegen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2\n)","530f9263":"se_callback = SnapshotEnsemble(n_models=7, n_epochs_per_model=50, lr_max=.001)\n\nhistory = model.fit_generator(\n    imagegen.flow(x_train, y_train, batch_size=32),\n    steps_per_epoch=len(x_train) \/ 32,\n    epochs=se_callback.n_epochs_total,\n    verbose=0,\n    callbacks=[se_callback],\n    validation_data=(x_test, y_test)\n)","ed9cefd2":"h = history.history\nplt.figure(1, figsize=(16, 10))\n\nplt.subplot(121)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(h['loss'], label='training')\nplt.plot(h['val_loss'], label='validation')\nplt.legend()\n\nplt.subplot(122)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.plot(h['acc'], label='training')\nplt.plot(h['val_acc'], label='validation')\nplt.legend()\n\nplt.show()","bc2dd8f9":"from sklearn.metrics import accuracy_score\n\n# makes prediction according to given models and given weights\ndef predict(models, data, weights=None):\n    if weights is None:\n        # default weights provide voting equality\n        weights = [1 \/ (len(models))] * len(models)\n    pred = np.zeros((data.shape[0], 10))\n    for i, model in enumerate(models):\n        pred += model.predict(data) * weights[i]\n    return pred\n    \n# returns accuracy for given predictions\ndef evaluate(preds, weights=None):\n    if weights is None:\n        weights = [1 \/ len(preds)] * len(preds)\n    y_pred = np.zeros((y_test.shape[0], 10))\n    for i, pred in enumerate(preds):\n        y_pred += pred * weights[i]\n    y_pred = np.argmax(y_pred, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    return accuracy_score(y_true, y_pred)\n\n# load list of snapshots\nmodels = se_callback.load_ensemble()\n# precalculated predictions of all models\npreds = []\n# evaluate every model as single\nfor i, model in enumerate(models):\n    pred = predict([model], x_test)\n    preds.append(pred)\n    score = evaluate([pred])\n    print(f'model {i + 1}: accuracy = {score:.4f}')\n\n# evaluate ensemble (with voting equality)\nensemble_score = evaluate(preds)\nprint(f'ensemble: accuracy = {ensemble_score:.4f}')","129f1cc0":"best_score = ensemble_score\nbest_weights = None\nno_improvements = 0\nwhile no_improvements < 5000: #patience\n    \n    # generate normalized weights\n    new_weights = np.random.uniform(size=(len(models), ))\n    new_weights \/= new_weights.sum()\n    \n    # get the score without predicting again\n    new_score = evaluate(preds, new_weights)\n    \n    # check (and save)\n    if new_score > best_score:\n        no_improvements = 0\n        best_score = new_score\n        best_weights = new_weights\n        print(f'improvement: {best_score:.4f}')\n    else:\n        no_improvements += 1\n\nprint(f'best weights are {best_weights}')","e2a84200":"pred = predict(models, test, best_weights)\n\nres = pd.DataFrame()\nres['ImageId'] = np.arange(test.shape[0]) + 1\nres['Label'] = np.argmax(pred, axis=1)\nres.to_csv('submission.csv', index=False)\nres.head(15)","59a1a59e":"<h3>keras callback code<\/h3>\n\ni found [this wonderful article](https:\/\/machinelearningmastery.com\/snapshot-ensemble-deep-learning-neural-network\/) and improve author's callback. this callback is passed to keras' ```fit``` function as an usual callback.","1d148d25":"there are some useful functions for making predictions and evaluating models. i also checked single models and their ensemble\n\nyou can see ```load_ensemble()```, useful function to load all snapshots","0f6c5c7b":"ensemble may contain models that make close predictions due to possibility to have models stuck in close local minimas. to reduce their negative influence we need to exclude models like that. i suggest an experimental way to do it: randomly generate weights and evaluate the ensemble with found weights. if any voting imbalance causes better score, i accept it. that's the code ","d3b9b107":"i built that convolutional neural network using keras. i think this configuration is powerful enough for current task","4e394676":"we start training from large enough learning rate and descend to a local minimum reducing learning rate on the go. when learning rate reaches its minimum value (after finishing needed quantity of epochs), model's state is being saved as snapshot, learning rate returns to its maximal value, and all actions repeats until we save enough snapshots. that's all!","40d96f48":"here is a code that splits train dataset into train and validation sets. ```to_categorical``` performs one-hot encoding","2c420d72":"<h1>theory<\/h1>\n\n**snapshot ensemble** is a way of ensebmling of neural network. the main idea is to obtain several nn models of the same architecture but stuck in different local minimas of loss function. \n\nin usual way we train only one model until it reaches a local minimum. to achieve this we may use techiques of manipulating learning rate. once we get closer and closer to the local minimum we can't get up and switch to the other one. found point can be far from optimal and results with high bias (fig. 1. left). using a technique of snapshot ensembling we train one model with reducing learning rate to reach a local minimum, make a snapshot of current model's state, increase learning rate to quit current local minimum, train again, and again, and again (fig. 1. right) \n\n<img src=\"https:\/\/raw.githubusercontent.com\/koki0702\/models\/images\/snapshot_ensemble.png\" \/>\n\ndue to difference in local minimas the models give different predictions which in average cause less bias and better score. pseudo-code of trainig algorithm:\n```\nmodel <- create_model()\nfor i in 1..M\n    model.set_learning_rate(large_learning_rate) \/\/ large lr to get out from local minimum\n    model.train_with_reducing_learning_rate() \/\/ descend to local minimum\n    save_snapshot(model) \/\/ save current states (model's weights)\n\nensemble = load_list_of_snapshots() \/\/ ensemble contains list of M models\n```\n\n[here](https:\/\/arxiv.org\/abs\/1704.00109) is a link to the original paper. all images refer to this paper","319d738a":"let's look at the history. left plot contains loss function's values. loss' peaks are the points of increasing learning rate. right plot contains accuracies. in the same way, accuracy decreases when the model searched new local minimum","5a16388e":"data augmentation allows the model to be more robust and reduce overfitting","75af175a":"good news is that described behavior can be obtained using only one (not ordinary) formula:\n<img src=\"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2018\/10\/Equation-for-the-Cosine-Annealing-Learning-Rate-Schedule.png\" \/>\n\nyou need to learn not this formula but its idea. its idea is to apply learning rate like this:","f51a3982":"<h1>practice<\/h1>\n\ni use classic mnist digits dataset for this tutorial. there is a code that loads data","59d34739":"here goes the ```SnapshotEnsemble``` callback. i define it with desired ```n_models``` and ```n_epochs_per_model```. ```lr_max``` should be big enough to easily quick a local minimum of previous snapshot but not too large to run away from any optimal loss\n\npay attention to ```n_epochs_total```: i have to use this in ```fit```.","7ce0d664":"<h3>references<\/h3>\n\n1. [original paper](https:\/\/arxiv.org\/abs\/1704.00109)\n2. [article with another example](https:\/\/machinelearningmastery.com\/snapshot-ensemble-deep-learning-neural-network\/)","79d7aab8":"finally, i make a prediction using best weights","c3991502":"<h3>thanks for readng till the end! i hope you found this notebook helpful!<\/h3>"}}