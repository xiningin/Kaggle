{"cell_type":{"1d0cb3a5":"code","0232a463":"code","944ccb56":"code","7098f8b7":"code","d96d85a7":"code","3aaf947e":"code","5e6f30b5":"code","fecf72f1":"code","3b7f4e0b":"code","db79296f":"code","ba66c747":"code","b6aae35b":"code","d4dee64b":"code","862bb9f6":"markdown","20e348cc":"markdown","bcaca64b":"markdown","744b4d9b":"markdown"},"source":{"1d0cb3a5":"!pip install ..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master\/ > \/dev\/null # no output\n!pip install torch_optimizer --no-index --find-links=file:\/\/\/kaggle\/input\/torch-optimizer\/torch_optimizer","0232a463":"import os\nimport gc\ngc.enable()\nimport sys\nimport math\nimport json\nimport time\nimport random\nfrom glob import glob\nfrom datetime import datetime\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport multiprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nimport torchvision\nfrom torch import Tensor\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom tqdm import tqdm\n\nimport efficientnet_pytorch\n\nimport torch_optimizer as optim\nimport albumentations as A\n\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","944ccb56":"IN_KERNEL = os.environ.get('KAGGLE_WORKING_DIR') is not None\nBATCH_SIZE = 64\nNUM_WORKERS = multiprocessing.cpu_count()\nNUM_TOP_PREDICTS = 1","7098f8b7":"test_dir = '..\/input\/landmark-recognition-2021\/test\/'","d96d85a7":"class ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, image_dir:str, mode: str):\n        self.df = dataframe\n        self.mode = mode\n        self.image_dir = image_dir\n        \n        transforms_list = []\n        if self.mode == 'train':\n            # Increase image size from (64,64) to higher resolution,\n            # Make sure to change in RandomResizedCrop as well.\n            transforms_list = [\n                transforms.Resize((64,64)),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomChoice([\n                    transforms.RandomResizedCrop(64),\n                    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n                    transforms.RandomAffine(degrees=15, translate=(0.2, 0.2),\n                                            scale=(0.8, 1.2), shear=15,\n                                            resample=Image.BILINEAR)\n                ]),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225]),\n            ]\n        else:\n            transforms_list.extend([\n                # Keep this resize same as train\n                transforms.Resize((64,64)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225]),\n            ])\n        self.transforms = transforms.Compose(transforms_list)\n\n    def __getitem__(self, index: int):\n        image_id = self.df.iloc[index].id\n        image_path = f\"{self.image_dir}\/{image_id[0]}\/{image_id[1]}\/{image_id[2]}\/{image_id}.jpg\"\n        image = Image.open(image_path)\n        image = self.transforms(image)\n\n        if self.mode == 'test':\n            return {'image':image}\n        else:\n            return {'image':image, \n                    'target':self.df.iloc[index].landmark_id}\n\n    def __len__(self) -> int:\n        return self.df.shape[0]","3aaf947e":"class EfficientNetEncoderHead(nn.Module):\n    def __init__(self, depth, num_classes):\n        super(EfficientNetEncoderHead, self).__init__()\n        self.depth = depth\n        self.base = efficientnet_pytorch.EfficientNet.from_name(f'efficientnet-b{self.depth}')#tropicbird\n        pretrained_file = glob(f'..\/input\/efficientnet-pytorch\/efficientnet-b{self.depth}*')[0]\n        checkpoint = torch.load(pretrained_file)\n        self.base.load_state_dict(checkpoint)\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.output_filter = self.base._fc.in_features\n        self.classifier = nn.Linear(self.output_filter, num_classes)\n    def forward(self, x):\n        x = self.base.extract_features(x)\n        x = self.avg_pool(x).squeeze(-1).squeeze(-1)\n        x = self.classifier(x)\n        return x","5e6f30b5":"def inference(data_loader, model):\n    model.eval()\n\n    activation = nn.Softmax(dim=1)\n    all_predicts, all_confs, all_targets = [], [], []\n\n    with torch.no_grad():\n        for i, data in enumerate(tqdm(data_loader, disable=IN_KERNEL)):\n            if data_loader.dataset.mode != 'test':\n                input_, target = data['image'], data['target']\n            else:\n                input_, target = data['image'], None\n\n            output = model(input_.cuda())\n            output = activation(output)\n\n            confs, predicts = torch.topk(output, NUM_TOP_PREDICTS)\n            all_confs.append(confs)\n            all_predicts.append(predicts)\n\n            if target is not None:\n                all_targets.append(target)\n\n    predicts = torch.cat(all_predicts)\n    confs = torch.cat(all_confs)\n    targets = torch.cat(all_targets) if len(all_targets) else None\n\n    return predicts, confs, targets","fecf72f1":"def generate_submission(test_loader, model, label_encoder):\n    predicts_gpu, confs_gpu, _ = inference(test_loader, model)\n    predicts, confs = predicts_gpu.cpu().numpy(), confs_gpu.cpu().numpy()\n\n    #The modified labels are inversed to the original labels\n    labels = [label_encoder.inverse_transform(pred) for pred in predicts]\n    print('labels')\n    print(np.array(labels))\n    print('confs')\n    print(np.array(confs))\n\n    sub = test_loader.dataset.df\n\n    def concat(label: np.ndarray, conf: np.ndarray):\n        return ' '.join([f'{str(L)} {str(np.round(c,4))}' for L, c in zip(label, conf)])\n    sub['landmarks'] = [concat(label, conf) for label, conf in zip(labels, confs)]\n\n    sub = sub.set_index('id')\n    sub.to_csv('submission.csv')","3b7f4e0b":"test_filenames=[]\nfor dirname, _, filenames in os.walk('..\/input\/landmark-recognition-2021\/test'):\n    for filename in filenames:\n        test_filenames.append(filename.split(\".\")[0])\ntest=pd.DataFrame({\"id\":test_filenames,\"landmarks\":\"\"})","db79296f":"test_dataset = ImageDataset(test, test_dir, mode='test')\ntest_loader = DataLoader(test_dataset, \n                         batch_size=BATCH_SIZE,\n                         shuffle=False, num_workers=NUM_WORKERS)","ba66c747":"label_encoder = LabelEncoder()\nlabel_encoder.classes_ = np.load('..\/input\/pytorch-starter-train-efficientnet\/classes.npy')","b6aae35b":"model = EfficientNetEncoderHead(depth=0, num_classes=len(label_encoder.classes_))\nmodel.cuda()\nmodel.load_state_dict(torch.load(\"..\/input\/pytorch-starter-train-efficientnet\/new_weight_efficientnet.pth\"))","d4dee64b":"generate_submission(test_loader, model, label_encoder)\npd.read_csv(\".\/submission.csv\")","862bb9f6":"# Make prediction","20e348cc":"# Load model weights","bcaca64b":"# Introduction\nThis notebook uses my trained model generated from https:\/\/www.kaggle.com\/hdsk38\/pytorch-starter-train-efficientnet to predict the hidden test dataset. Please go to the notebook to see the details of trained model. Many codes are forked from https:\/\/www.kaggle.com\/rhtsingh\/pytorch-training-inference-efficientnet-baseline. Thank you [\n@rhtsingh](https:\/\/www.kaggle.com\/rhtsingh)!\n\nI hope this notebook will help more kagglers to join this competition. Cheers! :)","744b4d9b":"# Load label encoder"}}