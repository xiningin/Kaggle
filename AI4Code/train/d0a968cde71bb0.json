{"cell_type":{"2378b1d2":"code","f7909256":"code","164c835d":"code","b01b7dcc":"code","acf6c840":"code","392a7fa4":"code","d51c8eb0":"code","4b0ab270":"code","b88db6f0":"code","fc7bab75":"code","66907961":"code","7c17309f":"code","af27245b":"code","de5936bb":"code","51954ddd":"code","d012fe6b":"code","4b433f40":"code","bebb8a17":"code","efb316e1":"code","563f4512":"code","867ad98b":"code","eb00ccb3":"code","2de5346a":"code","4b2f1558":"code","e28cf76e":"code","ee5f7e38":"code","22a6fded":"code","1a9b1390":"code","e2117444":"code","268f07d4":"code","88a521f7":"code","1ecce237":"code","d1fac3a6":"code","4df270ef":"code","b7ac7852":"markdown","2ec28a93":"markdown","4606e33d":"markdown","16b10736":"markdown","ca28338c":"markdown","f5edce21":"markdown","51dda521":"markdown","a3185af0":"markdown","5b003045":"markdown","03dca9b0":"markdown","46363adc":"markdown","e091a26e":"markdown","62a55f8a":"markdown","a7343922":"markdown","e920aad3":"markdown","440e6bf9":"markdown","5067ec14":"markdown","89c8a11c":"markdown","c141b177":"markdown","2a0dc59e":"markdown","d27c0a57":"markdown","8402f1b9":"markdown","42324442":"markdown","3776fea4":"markdown","1d15162f":"markdown","7fe31ef7":"markdown","43e3f163":"markdown","42f61c7c":"markdown","72fe4da9":"markdown","e7565d1e":"markdown","318bffd5":"markdown","15b8e62a":"markdown","45a1a193":"markdown","a11960f4":"markdown","2f6a2f86":"markdown","918f8164":"markdown","24433fa1":"markdown","b946c750":"markdown","4974b708":"markdown","7d7b6107":"markdown","2acb3dd6":"markdown","edc437fd":"markdown","88f15cd2":"markdown","3deb0927":"markdown","f91d5564":"markdown","9745ad5d":"markdown","5ecfb01b":"markdown","44b1e574":"markdown","aedc8cee":"markdown","435988b7":"markdown","7ddb8c43":"markdown","7b34ceff":"markdown","6974297d":"markdown","30f015d3":"markdown","a1229053":"markdown","b366654b":"markdown","43e38985":"markdown","eef98b49":"markdown","24cb4bf4":"markdown","5bac461c":"markdown","940ce038":"markdown","d4d6d165":"markdown","4889036c":"markdown","4741d85f":"markdown","f27a4bac":"markdown","18e16b0f":"markdown","166c98ce":"markdown","2c04f88a":"markdown","3d2c8114":"markdown","ef8f814b":"markdown"},"source":{"2378b1d2":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.base import TransformerMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, KFold, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom fancyimpute import KNN\nfrom fancyimpute import IterativeImputer\npd.options.mode.chained_assignment = None\nimport warnings\nwarnings.filterwarnings('ignore')","f7909256":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv', header=0)\ntest = pd.read_csv('..\/input\/test.csv', header=0)\n\n# Merge train and test sets\ntest.insert(1,'Survived',np.nan)\nall = pd.concat([train, test])","164c835d":"# Perform corrections\ncorr_dict = {248: pd.Series([0,1], index=['SibSp', 'Parch'],),\n             313: pd.Series([1,0], index=['SibSp', 'Parch'],),\n             418: pd.Series([0,0], index=['SibSp', 'Parch'],),\n             756: pd.Series([0,1], index=['SibSp', 'Parch'],),\n             1041: pd.Series([1,0], index=['SibSp', 'Parch'],),\n             1130: pd.Series([0,0], index=['SibSp', 'Parch'],),\n             1170: pd.Series([2,0], index=['SibSp', 'Parch'],),\n             1254: pd.Series([1,0], index=['SibSp', 'Parch'],),\n             1274: pd.Series([1,0], index=['SibSp', 'Parch'],),\n             539: pd.Series([1,0], index=['SibSp', 'Parch'],)\n             }\n\nall[['SibSp','Parch']] = all.apply(lambda s: corr_dict[s['PassengerId']]\n    if s['PassengerId'] in [248,313,418,756,1041,1130,1170,1254,1274,539] else s[['SibSp','Parch']], axis = 1)","b01b7dcc":"# Add Title\nall['Title'] =  all.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","acf6c840":"# Replace rare titles\nall.loc[all['Title'].isin(['Ms','Mlle']), 'Title'] = 'Miss'\nall.loc[all['Title'].isin(['Mme','Lady','Dona','Countess']), 'Title'] = 'Mrs'\nall.loc[all['Title'].isin(['Col','Major','Sir','Rev','Capt','Don','Jonkheer']), 'Title'] = 'Mr'\nall.loc[(all['Title'] == 'Dr') & (all['Sex'] == 'male'),'Title'] = 'Mr'\nall.loc[(all['Title'] == 'Dr') & (all['Sex'] == 'female'),'Title'] = 'Mrs'","392a7fa4":"# Add Family Size and is-Alone\nall['FamSize'] = all.apply(lambda s: 1+s['SibSp']+s['Parch'], axis = 1)\nall['isAlone'] = all.apply(lambda s: 1 if s['FamSize'] == 1 else 0, axis = 1)","d51c8eb0":"# Add Group Size\nticket_counts = all['Ticket'].value_counts()\nall['GrSize'] = all.apply(lambda s: ticket_counts.loc[s['Ticket']], axis=1)\n\n# Add has-Cabin\nall['Cabin'].fillna('U',inplace=True)\nall['hasCabin'] = all.apply(lambda s: 0 if s['Cabin'] == 'U' else 1,axis = 1)","4b0ab270":"# Add Family Name\nall['Fname'] =  all.Name.str.extract('^(.+?),', expand=False)\n\n# Search for passengers with siblings\nPas_wSib = []\nall_x_0 = all[(all['SibSp'] > 0) & (all['Parch'] == 0)]\nname_counts_SibSp = all_x_0['Fname'].value_counts()\nfor label, value in name_counts_SibSp.items():\n    entries = all_x_0[all_x_0['Fname'] == label]\n    if (entries.shape[0] > 1 and (not (entries['Title'] == 'Mrs').any())) or \\\n       (entries.shape[0] == 1 and entries['Title'].values[0] == 'Mrs'):\n            Pas_wSib.extend(entries['PassengerId'].values.tolist())\n    else:\n        Pas_wSib.extend( \\\n            entries[(entries['Title'] == 'Miss')|(entries['GrSize'] == 1)]['PassengerId'].values.tolist())\n\n# Search for Mrs-es with parents\nMrs_wPar = []\nall_x_y = all[all['Parch'] > 0]\nname_counts_Parch = all_x_y['Fname'].value_counts()\nfor label, value in name_counts_Parch.items():\n    entries = all_x_y[all_x_y['Fname'] == label]\n    if entries.shape[0] == 1:\n        if entries['Title'].values[0] == 'Mrs' and entries['Age'].values[0] <= 30:\n            Mrs_wPar.extend(entries['PassengerId'].values.tolist())\n\ndef get_features(row):\n\n    features = pd.Series(0, index = ['wSib','wSp','wCh','wPar'])\n\n    if row['PassengerId'] in Pas_wSib:\n        features['wSib'] = 1\n    else:\n        if (row['SibSp'] != 0) & (row['Parch'] == 0):\n            features['wSp'] = 1\n        else:\n            if  ( (row['Title']=='Mrs')&(not row['PassengerId'] in Mrs_wPar) )| \\\n                ( (row['Title']=='Mr')&(not row['PassengerId'] == 680)&\n                                        ( ((row['Pclass']==1)&(row['Age']>=30))|\n                                          ((row['Pclass']==2)&(row['Age']>=25))|\n                                          ((row['Pclass']==3)&(row['Age']>=20)) ) ):\n                features['wCh'] = 1\n            else:\n                features['wPar'] = 1\n\n    return features\n\nall[['wSib','wSp','wCh','wPar']] = all.apply(lambda s: get_features(s) if s['isAlone'] == 0 else [0,0,0,0], axis = 1)","b88db6f0":"all = all.drop(['Fname','Name','Cabin','Ticket','Fare','SibSp','Parch'], axis = 1)","fc7bab75":"all[all['Pclass'] == 1].groupby(['Title','isAlone','wSib','wSp','wCh','wPar'])['Survived'].agg(['count','size','mean'])","66907961":"all[(all['Pclass'] == 1)&(all['Title'] == 'Mr') ].groupby(['hasCabin','isAlone','wSib','wSp','wCh','wPar'])['Survived'].agg(['count','size','mean'])","7c17309f":"all[all['Pclass'] == 2].groupby(['Title','isAlone','wSib','wSp','wCh','wPar'])['Survived'].agg(['count','size','mean'])","af27245b":"all[all['Pclass'] == 3].groupby(['Title','isAlone','wSib','wSp','wCh','wPar'])['Survived'].agg(['count','size','mean'])","de5936bb":"all[(all['Pclass'] == 3)&(all['Title'] != 'Mr')].groupby(['Title','FamSize'])['Survived'].agg(['count','size','mean'])","51954ddd":"# Make FamSize bins\nall['FamSizeBin'] = pd.cut(all['FamSize'], bins = [0,4,11], labels = False)\nall = all.drop(['FamSize'], axis = 1)","d012fe6b":"all[(all['Pclass'] == 3)&(all['Title'] != 'Mr')].groupby(['Title','FamSizeBin','isAlone','wSib','wSp','wCh','wPar'])['Survived'].agg(['count','size','mean'])","4b433f40":"def get_survived_1(row):\n    if row['Pclass'] in [1,2]:\n        if row['Title'] == 'Mr':\n            survived = 0\n        else:\n            survived = 1\n    else:\n        if row['Title'] == 'Mr' or row['FamSizeBin'] == 1:\n            survived = 0\n        else:\n            survived = 1\n\n    return survived","bebb8a17":"# Form train and test sets\nX_train = all.iloc[:891,:]\nX_test = all.iloc[891:,:]\ny_train = all.iloc[:891,:]['Survived']\n\n# Make predictions (train)\ny_train_hat = X_train.apply(lambda s: get_survived_1(s), axis = 1)\n\n# Make predictions (test)\npredictions = pd.DataFrame( {'PassengerId': test['PassengerId'], 'Survived': 0} )\npredictions['Survived'] = X_test.apply(lambda s: get_survived_1(s), axis = 1)\npredictions.to_csv('submission-1.csv', index=False)\n\n# Train score\nscore = metrics.accuracy_score(y_train_hat, y_train)\nprint('Train Accuracy: {}'.format(score))","efb316e1":"all[(all['Pclass'] == 3)&(all['Title'] != 'Mr')&(all['FamSizeBin'] == 0)].groupby(['Title','Embarked'])['Survived'].agg(['count','size','mean'])","563f4512":"def get_survived_2(row):\n    if row['Pclass'] in [1,2]:\n        if row['Title'] == 'Mr':\n            survived = 0\n        else:\n            survived = 1\n    else:\n        if row['Title'] == 'Mr' or row['FamSizeBin'] == 1 or (row['Title'] == 'Miss' and row['Embarked'] == 'S'):\n            survived = 0\n        else:\n            survived = 1\n\n    return survived","867ad98b":"# Make predictions (train)\ny_train_hat = X_train.apply(lambda s: get_survived_2(s), axis = 1)\n\n# Make predictions (test)\npredictions['Survived'] = X_test.apply(lambda s: get_survived_2(s), axis = 1)\npredictions.to_csv('submission-2.csv', index=False)\n\n# Train score\nscore = metrics.accuracy_score(y_train_hat, y_train)\nprint('Train Accuracy: {}'.format(score))","eb00ccb3":"all[(all['Pclass'] == 3)&(all['Title'] == 'Miss')&(all['FamSizeBin'] == 0)].groupby(['Title','wPar','Embarked'])['Survived'].agg(['count','size','mean'])","2de5346a":"def get_survived_3(row):\n    if row['Pclass'] in [1,2]:\n        if row['Title'] == 'Mr':\n            survived = 0\n        else:\n            survived = 1\n    else:\n        if row['Title'] == 'Mr' or row['FamSizeBin'] == 1 or \\\n        (row['Title'] == 'Miss' and row['Embarked'] == 'S' and row['wPar'] == 0):\n            survived = 0\n        else:\n            survived = 1\n\n    return survived","4b2f1558":"# Make predictions (train)\ny_train_hat = X_train.apply(lambda s: get_survived_3(s), axis = 1)\n\n# Make predictions (test)\npredictions['Survived'] = X_test.apply(lambda s: get_survived_3(s), axis = 1)\npredictions.to_csv('submission-3.csv', index=False)\n\n# Train score\nscore = metrics.accuracy_score(y_train_hat, y_train)\nprint('Train Accuracy: {}'.format(score))","e28cf76e":"# Select and convert categorical features into numerical ones (1)\nall['Sex'] = all['Sex'].map( {'male': 0, 'female': 1} ).astype(int)\nall['Embarked'].fillna(all['Embarked'].value_counts().index[0], inplace=True)\nall_dummies =  pd.get_dummies(all, columns = ['Title','Pclass','Embarked'],\\\n                                 prefix=['Title','Pclass','Embarked'], drop_first = True)\nall_dummies = all_dummies.drop(['PassengerId','Survived'], axis = 1)","ee5f7e38":"# KNN imputation\nall_dummies_i = pd.DataFrame(data=KNN(k=3, verbose = False).fit_transform(all_dummies).astype(int),\n                            columns=all_dummies.columns, index=all_dummies.index)","22a6fded":"# Convert categorical features into numerical ones (2)\nall_dummies_i['isAlwSib'] = all_dummies_i.apply(lambda s: 1 if (s['isAlone'] == 1)|(s['wSib'] == 1) else 0 ,axis = 1)\nall_dummies_i = all_dummies_i.drop(['isAlone','wSib','Sex','GrSize'], axis = 1)","1a9b1390":"# Form train and test sets\nX_train = all_dummies_i.iloc[:891,:]\nX_test = all_dummies_i.iloc[891:,:]","e2117444":"# Perform scaling\nscaler = StandardScaler()\nscaler.fit(X_train[['Age']])\nX_train['Age'] = scaler.transform(X_train[['Age']])\nX_test['Age'] = scaler.transform(X_test[['Age']])","268f07d4":"# Cross-validation parameters\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)","88a521f7":"# Grid search parameters\nsvm_grid = {'C': [10,11,12,13,14,15,16,17,18,19,20], 'gamma': ['auto']}\nsvm_search = GridSearchCV(estimator = SVC(), param_grid = svm_grid, cv = cv, refit=True, n_jobs=1)","1ecce237":"# Apply grid search\nsvm_search.fit(X_train, train['Survived'])\nsvm_best = svm_search.best_estimator_\nprint(\"Cross-validation accuracy: {}, standard deviation: {}, with parameters {}\"\n       .format(svm_search.best_score_, svm_search.cv_results_['std_test_score'][svm_search.best_index_],\n               svm_search.best_params_))","d1fac3a6":"y_train_hat = svm_best.predict(X_train)\nprint('Train Accuracy: {}'\n        .format(metrics.accuracy_score(y_train_hat, y_train)))\n\npredictions['Survived'] = svm_best.predict(X_test)\npredictions.to_csv('submission-svm.csv', index=False)","4df270ef":"def get_survived_svm_rule(row):\n    if row['Pclass'] in [1,2]:\n        if row['Title'] == 'Mr':\n            survived = 0\n        else:\n            survived = 1\n    else:\n        if row['Title'] == 'Mr' or row['FamSizeBin'] == 1 or \\\n        (row['Title'] == 'Miss' and row['Embarked'] == 'S' and row['Age'] >= 18):\n            survived = 0\n        else:\n            survived = 1\n\n    return survived","b7ac7852":"Similarly to classes 1 and 2, the fate of adult males is certain: they are more likely to perish. The fate of children and adult females, however, is much less certain. Let us try to resolve this uncertainty by using the 'FamSize' feature.","2ec28a93":"According to the statistic, the ones having 'FamSize' > 4 have significantly lower chances to survive than the remaining ones. Let us clean the statistic by introducing two family size bins: less than or equal to \/ greater than 4.","4606e33d":"In class 3, the situation is quite different. First, children have fairly low survival rates between 0.34-0.37. Adult females have the rates greater than 0.5, with the exception of 0.42 for ones traveling with children (the survival rate of 0 for those with wSib == 1 is statistically insignificant). Adult males have the lowest survival rates of 0-0.15, surprisingly however, this is slightly greater than that for class 2.","16b10736":"Following the standard practice, we create the 'Title' feature.","ca28338c":"As already mentioned in other kernels, having groups of people with high uncertainity in the survival (survival rate close to 0.5) is going to limit the prediction accuracy of the classifier. Hence, an effort has to be put to resolve such uncertainties.  Let us check, whether 'hasCabin' feature can help us to do so for the adult males.","f5edce21":"Additionally, we create the feature representing the number of people sharing the same ticket. Also, we extract information about the availability of the cabin information.","51dda521":"As we already mentioned, Model 3 and SVM yield almost identical predictions on the test. The difference between the two is in a single entry which is a young Miss in class 3, non-child and embarked in S: SVM decides her rather to survive. Thus, the SVM decision rule can be interpreted as a corrected Model 3, where now all females in class 3 younger than 18 survive.","a3185af0":"## Model 3 <a class=\"anchor\" id=\"model-3\"><\/a>","5b003045":"In order to apply SVM correctly, 'Age' feature has to be normalized. First, we re-build train and test sets.","03dca9b0":"As one can see, the family size feature helps to clarify the fate of children and adult females. The survival rate of the ones with families greater than 4 is remarkably low: 0.05-0.13. For all the rest, the forecast is now rather positive: children have the survival rates of 0.62 (female) and 1 (male), and all adult females have the rates above 0.5.","46363adc":"## SVM Modeling <a class=\"anchor\" id=\"SVM\"><\/a>","e091a26e":"Here, we create additional binary features.\n\n* 'wSib': whether person is traveling ONLY with siblings\n* 'wSp': whether person is traveling ONLY with spouse\n* 'wCh': whether person is traveling with children\n* 'wPar': whether person is traveling with parents","62a55f8a":"To find the parameter of SVM (regularization factor 'C'), we use an exhaustive grid search.","a7343922":"The kernel is organized as follows. First, feature engineering is described. Then, exploratory data analysis (EDA) is performed. Building on top of EDA, models based on simple decision rules are derived. In the end, modeling using SVM is described.","e920aad3":"The features 'wSib', 'wSp', 'wPar', 'wCh' and 'isAlone' are mutually exclusive, where 'wSib' + 'wSp' + 'wCh' + 'wPar'  + 'isAlone' = 1 holds. Obviously, 'wCh' == 1 identifies parents. 'wPar' == 1, in turn, can help to identify children, although it also identifies adults travelling with parents. To distinguish children from adults travelling with parents, we will use the 'Title' feature: children are said to be those with title 'Master' or title 'Miss' and 'wPar' == 1.","440e6bf9":"## Models Based on Simple Rules  <a class=\"anchor\" id=\"models\"><\/a>","5067ec14":"In general, 'Embarked' feature has weak predictive power with the exception for those with 'Title' == 'Miss': with survival rate 0.44, the Misses embarked in Southampton (S) have now more chances to perish. This judgment seems to be statistically significant, as it is based on 39 samples. Based on this new information, we create the new model.","89c8a11c":"## Class 3 <a class=\"anchor\" id=\"EDA-3\"><\/a>","c141b177":"# Feature Engineering <a class=\"anchor\" id=\"feature\"><\/a>","2a0dc59e":"## Model 2 <a class=\"anchor\" id=\"model-2\"><\/a>","d27c0a57":"We start with loading the train and test sets and building the joint train+test set.","8402f1b9":"# Table of Contents\n* [Standard Routines](#standard)\n* [Feature Engineering](#feature)\n    * [Title, Family Size, Group Size, is Alone, has Cabin](#feature-1)\n    * [with Siblings, with Spouse, with Children, with Parents](#feature-2)\n* [Exploratory Data Analysis](#EDA)\n    * [Class 1](#EDA-1)\n    * [Class 2](#EDA-2)\n    * [Class 3](#EDA-3)\n* [Models Based on Simple Rules](#models)\n    * [Model 1](#model-1)\n    * [Model 2](#model-2)\n    * [Model 3](#model-3)\n* [SVM Modeling](#SVM)\n    * [Data Preparation](#SVM-1)\n    * [Training](#SVM-2)\n    * [Relationship to Model 3](#SVM-3)\n* [Conclusions](#conclusions)","42324442":"# Exploratory Data Analysis <a class=\"anchor\" id=\"EDA\"><\/a>","3776fea4":"## Training <a class=\"anchor\" id=\"SVM-2\"><\/a>","1d15162f":"Let us recapture first, what we got so far. For classes 1 and 2, all adult females and children ('Title' != 'Mr') are more likely to survive and the remaining adult males are more likely to perish. Now, we consider class 3.","7fe31ef7":"The function above summarizes the SVM decision rule, feel free to test it by forking the script. Note, that Model 3 and SVM differ only in single prediction which is for the public part of the test set and for the private part of the test set the predictions are identical. Additional insight here: from the large set of features which were used to build the SVM classifier, in total only 4 features 'Title', 'Age', 'FamSizeBin' and 'Embarked' were important for the test set accuracy.","43e3f163":"The 'hasCabin' feature has some predictive power as it differentiates the survival rate for the ones traveling alone: 0.39 and 0.22 for those with and without cabin information, respectively. Unfortunately, as both survival rates are less than 0.5, this information alone is not helpful for building decision rules. Let us now move to class 2.","42f61c7c":"Let us apply our model to the train and test sets and see the accuracy.","72fe4da9":"<b>Model 3.<\/b>\n<cite> All adult males are deemed to perish as well as the ones in class 3 with families greater than 4. Also, Misses in class 3, non-chlidren and embarked in S perish. The rest all survive. <\/cite>","e7565d1e":"Now we make the predictions on the train and test sets and print the train set accuracy.","318bffd5":"We add the feature representing the size of the family. The 'isAlone' feature identifies whether the person is traveling alone.","15b8e62a":"We perform EDA in order to derive the simple rules for the models. In the following, survival statistics for each passenger class are considered.","45a1a193":"For the models based on simple rules, only few features necessary. However, for the derivation of the models, a larger set of features is considered.","a11960f4":"Now we calculate the scaling based on the train set and apply it to both train and test sets.","2f6a2f86":"## with Siblings, with Spouse, with Children, with Parents <a class=\"anchor\" id=\"feature-2\"><\/a>","918f8164":"## Class 2 <a class=\"anchor\" id=\"EDA-2\"><\/a>","24433fa1":"We group 'isAlone' and 'wSib' together. Also we get rid of the 'Sex' (redundant) and 'GrSize' (not used) features.","b946c750":"Before proceeding further, let us clean our dataframe a bit. We remove 'Fname', 'Name', 'Cabin', 'Ticket', 'Fare', 'SibSp' and 'Parch' features as they are not used for the rest of the kernel.","4974b708":"During the preparation of this kernel, few errors in the features 'SibSp' and 'ParCh' have been identified. We correct them below, although this is not going to change the accuracy of modeling and is done solely for the sake of more precise EDA.","7d7b6107":"<b>Model 2.<\/b>\n<cite> All adult males are deemed to perish as well as the ones in class 3 with families greater than 4. Also, Misses in class 3 embarked in S perish. The rest all survive. <\/cite>","2acb3dd6":"<b>Model 1.<\/b>\n<cite> All adult males are deemed to perish as well as the ones in class 3 with families greater than 4. The rest all survive. <\/cite>","edc437fd":"In this kernel, classification based on simple decision rule is proposed. Using in total 2 features 'Title' and 'FamSize' = 1 + 'SibSp' + 'ParCh', the public score of 0.78947 is achieved. Adding 'Embarked' as a third feature on top allows to push the public score to 0.80382. Additionally, modeling using support vector machine (SVM) is considered. By comparing the predictions on the test set between the models relying on simple decision rules and SVM, insights into decision rules generated by SVM are obtained.","88f15cd2":"## Class 1 <a class=\"anchor\" id=\"EDA-1\"><\/a>","3deb0927":"Checking the test set predictions on Kaggle yields now the accuracy of 0.80382, which at the time of writing this kernel is within the top 12 percent results on the public leaderboard. Need to be said, that we are possibly dealing here with a slight overfit on the public part of the test set. We elaborate more on this in the following.","f91d5564":"Now, we are ready to apply SVM. First, let us define the cross-validation strategy. We form for 80\/20 percent train\/test splits, in total 10 times. ","9745ad5d":"As we can see, the female children embarked in S are more likely to survive, although, we are dealing here with a small number of samples. Let us correct the model and see whether the accuracy is going to increase.","5ecfb01b":"Now, let try to improve Model 1 by adding the 'Embarked' feature on top. As we have seen, females in class 3 with family sizes less than 5 have still high uncertainty in the survival. Let us print their survival statistic for different embarkment ports.","44b1e574":"As we can see, the train accuracy is slightly greater than that we achieved with Model 3 (0.8418). Uploading the predictions to Kaggle yields the accuracy of 0.80382, which is as good as the best accuracy we got with Model 2.","aedc8cee":"We start with preprocessing the features. First, we apply dummy encoding. To reduce the number of features, we map K categories onto (K - 1)-sized space, where K is a natural number.","435988b7":"Due to rare occurrence of some titles, we replace them by the more frequent ones.","7ddb8c43":"In this kernel, 3 models based on simple decision rules and SVM modeling were proposed. It has been shown, that the public score of 0.78947 can be achieved by using Model 1 based on in total only 2 features: 'Title' and 'FamSize'. Models 2 and 3 employ 'Embarked' feature in addition and yield the accuracy of 0.80382 and 0.79904, respectively, where Model 3 is potentially more robust. SVM achieves 0.80382 and produces the predictions on the test set almost identical to that of Model 3, where to increase the accuracy, it exploits the 'Age' feature.","7b34ceff":"Summarizing the conducted study, we can build our models. We start with the simplest one relying on in total 2 features.","6974297d":"Checking the test set predictions on Kaggle yields the accuracy of 0.78947, which is already a decent score, improving upon the simple gender-based model and achieving the same accuracy as the kernels using complex learning algorithms (e.g. Random Forest). Our proposed model, however, relies on simple decision rules, hence it is less likely to overfit on the public part of the test .","30f015d3":"In this part of the kernel, SVM modeling is considered. We employ the same set of features which was used for EDA: 'Title', 'Age', 'FamSizeBin', 'Embarked', ('isAlone' + 'wSib'), 'wSp', 'wCh', 'wPar' and 'hasCabin'. ","a1229053":"## Relationship to Model 3 <a class=\"anchor\" id=\"SVM-3\"><\/a>","b366654b":"## Title, Family Size, Group Size, is Alone, has Cabin <a class=\"anchor\" id=\"feature-1\"><\/a>","43e38985":"For the sake of readability, we summarize all imports below.","eef98b49":"As we can see, the training accuracy increases, while the public test score decreases to 0.79904, which 1 correct classification less than Model 2. Model 3 however creates the test set predictions which are almost identical (up to 1 classification entry) to those which will be obtained with SVM later on. That been said, Models 2 and 3 have almost identical accuracy and to decide between the two, one need to test the accuracy on the private part of test set. We leave this out to the reader.","24cb4bf4":"## Model 1 <a class=\"anchor\" id=\"model-1\"><\/a>","5bac461c":"## Data Preparation <a class=\"anchor\" id=\"SVM-1\"><\/a>","940ce038":"In our kernel, we use KNN age imputation.","d4d6d165":"# Standard Routines <a class=\"anchor\" id=\"standard\"><\/a>","4889036c":"As we have seen, male children (Master)  in class 3 with family sizes less than 5 have all survived. Normally, we would expect this also to be true for female children, irrespective of embarkment port. Let us print the survival statistic to confirm our hypothesis.","4741d85f":"<b>SVM rule.<\/b>\n<cite> All adult males are deemed to perish as well as the ones in class 3 with families greater than 4. Also, 18 and older Misses in class 3 embarked in S perish. The rest all survive. <\/cite>","f27a4bac":"'Mean' defines the survival rate: the percentage of the survived passengers in the train set, where 'NaN' corresponds to the case when all samples of a particular group are in the test set. 'count' and 'size' stand for the size of the train and joint train+test sets for particular groups, i.e. 'size' - 'count' is the size of the test set. The findings here: adult females and children ('Title' != 'Mr') mostly survive. The fate of the remaining adult males is different: the survival rate is somewhere between 0.23 (parents) and 0.46 (traveling only with spouses).","18e16b0f":"We run our grid search and print the cross-validation statistics: mean and standard deviation.","166c98ce":"# Conclusions <a class=\"anchor\" id=\"conclusions\"><\/a>","2c04f88a":"Similarly to class 1, children all survive. The survival rate of adult females is close to 1, with the exception of 0.77 for ones traveling only with spouses. The fate of adult males with the survival rates 0-0.1 is certain: they are more likely to perish. Notice, the survival for those traveling alone and only with siblings is similar, i.e. these groups can be merged (which we will later do when applying SVM).","3d2c8114":"# Summary","ef8f814b":"Let us update the predictions on the train and test sets to see how well Model 2 performs."}}