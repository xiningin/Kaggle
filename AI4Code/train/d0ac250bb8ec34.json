{"cell_type":{"3466e38a":"code","3c0c0b12":"code","ac803e18":"code","f6947f64":"code","f7b014e3":"code","560395f5":"code","0b29588a":"code","1bf7a28d":"code","1e3683f3":"code","62a37d08":"code","80847c18":"code","aee3458d":"code","33706530":"code","36638657":"code","ec29260e":"code","7945033c":"code","0ef0c112":"code","4d6dcad0":"code","72d29799":"code","edf2fb81":"code","fed53225":"code","c07224e7":"code","423bc7e0":"code","4c4f1436":"code","8b382715":"code","4eb03631":"code","171e0388":"code","00f4d796":"code","bf39b5fd":"code","c70ef63d":"code","676c15ef":"code","04a02e6e":"code","b0e4ac7d":"code","d218b0d3":"code","c7dc163c":"code","0979420d":"code","fe58cb0f":"code","ec27ccd7":"code","ef33ffe0":"code","eaf631d3":"code","4748ea09":"code","1f524e84":"code","b32db1cb":"code","093b8465":"code","1b98d3d6":"code","c5b2b26f":"code","4dde382b":"code","de798eed":"code","22f6991a":"code","e20018c9":"code","7f8d604e":"code","459de10d":"code","01a1b8c9":"code","67c4a2e3":"code","818d9993":"code","37ff2e61":"code","893f9f4f":"code","348c24b3":"code","79edc4f0":"code","f158f496":"code","7b487ade":"code","e8bf78fc":"code","b446f500":"code","e9c2287a":"code","df8c97f7":"markdown","1a318223":"markdown","4a8ec3fb":"markdown","4bbe2ea6":"markdown","2d5f33a8":"markdown","e02e39e6":"markdown","d279f75c":"markdown","6278d6ce":"markdown","dfebdaf0":"markdown","1a861a17":"markdown","e95a3a5c":"markdown","15e44062":"markdown","6e3e1271":"markdown","2bedec70":"markdown","a175e2a3":"markdown","c2b025eb":"markdown","fdc7dfa4":"markdown","419400c3":"markdown","4e951951":"markdown","4dddbd72":"markdown","ecdab286":"markdown","0e9288f6":"markdown","8393ed9a":"markdown","4562469e":"markdown","de547fb4":"markdown","255bcc39":"markdown","17c996c2":"markdown","ca945cc2":"markdown","14c03427":"markdown","37c140ae":"markdown","8f4d45a8":"markdown","8e93f68d":"markdown","ab461b95":"markdown","eb22726d":"markdown","3e6a1885":"markdown","08539d65":"markdown","e8b40ed9":"markdown","3f4db971":"markdown","728aaad1":"markdown","202c1fbd":"markdown","ea16dfc7":"markdown","c4f45522":"markdown","d05461b4":"markdown","76ddaf78":"markdown","3fe8baaf":"markdown","791b1400":"markdown","3f785f34":"markdown","434e0d00":"markdown","ef7f40ee":"markdown","f7e3764c":"markdown","26b619d8":"markdown","1d8ac6a0":"markdown","3c7a2e28":"markdown","e8165cf0":"markdown","646066f8":"markdown","2fe52e65":"markdown"},"source":{"3466e38a":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.linear_model import LinearRegression\nfrom pandas import date_range\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\n\n# Model 1 (trend)\nfrom pyearth import Earth\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\n\n# Model 2\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.multioutput import RegressorChain\nimport warnings","3c0c0b12":"# switch off the warnings\nwarnings.filterwarnings(\"ignore\")","ac803e18":"df_holidays = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv', header = 0)\ndf_oil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv', header = 0)\ndf_stores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv', header = 0)\ndf_trans = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv', header = 0)\n\ndf_train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv', header = 0)\ndf_test = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv', header = 0)","f6947f64":"df_holidays['date'] = pd.to_datetime(df_holidays['date'], format = \"%Y-%m-%d\")\ndf_oil['date'] = pd.to_datetime(df_oil['date'], format = \"%Y-%m-%d\")\ndf_trans['date'] = pd.to_datetime(df_trans['date'], format = \"%Y-%m-%d\")\ndf_train['date'] = pd.to_datetime(df_train['date'], format = \"%Y-%m-%d\")\ndf_test['date'] = pd.to_datetime(df_test['date'], format = \"%Y-%m-%d\")","f7b014e3":"df_holidays.head(10) # check data","560395f5":"df_oil.head(3) # check data","0b29588a":"df_stores.head(10) # check data","1bf7a28d":"df_trans.head(3) # check data","1e3683f3":"df_train.head(10) # check data","62a37d08":"df_test.head(5) # check data","80847c18":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(25,15))\ndf_oil.plot.line(x=\"date\", y=\"dcoilwtico\", color='b', title =\"dcoilwtico\", ax = axes, rot=0)\nplt.show()","aee3458d":"def grouped(df, key, freq, col):\n    \"\"\" GROUP DATA WITH CERTAIN FREQUENCY \"\"\"\n    df_grouped = df.groupby([pd.Grouper(key=key, freq=freq)]).agg(mean = (col, 'mean'))\n    df_grouped = df_grouped.reset_index()\n    return df_grouped","33706530":"# check grouped data\ndf_grouped_trans_w = grouped(df_trans, 'date', 'W', 'transactions')\ndf_grouped_trans_w","36638657":"def add_time(df, key, freq, col):\n    \"\"\" ADD COLUMN 'TIME' TO DF \"\"\"\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['time'] = np.arange(len(df_grouped.index))\n    column_time = df_grouped.pop('time')\n    df_grouped.insert(1, 'time', column_time)\n    return df_grouped","ec29260e":"df_grouped_train_w = add_time(df_train, 'date', 'W', 'sales')\ndf_grouped_train_m = add_time(df_train, 'date', 'M', 'sales')\n\ndf_grouped_train_w.head() # check results","7945033c":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(30,20))\n\n# TRANSACTIONS (WEEKLY)\naxes[0].plot('date', 'mean', data=df_grouped_trans_w, color='grey', marker='o')\naxes[0].set_title(\"Transactions (grouped by week)\", fontsize=20)\n\n# SALES (WEEKLY)\naxes[1].plot('time', 'mean', data=df_grouped_train_w, color='0.75')\naxes[1].set_title(\"Sales (grouped by week)\", fontsize=20)\n# linear regression\naxes[1] = sns.regplot(x='time', \n                      y='mean', \n                      data=df_grouped_train_w, \n                      scatter_kws=dict(color='0.75'), \n                      ax = axes[1])\n\n# SALES (MONTHLY)\naxes[2].plot('time', 'mean', data=df_grouped_train_m, color='0.75')\naxes[2].set_title(\"Sales (grouped by month)\", fontsize=20)\n# linear regression\naxes[2] = sns.regplot(x='time', \n                      y='mean', \n                      data=df_grouped_train_m, \n                      scatter_kws=dict(color='0.75'), \n                      line_kws={\"color\": \"red\"},\n                      ax = axes[2])\n\nplt.show()","0ef0c112":"def add_lag(df, key, freq, col, lag):\n    \"\"\" ADD LAG \"\"\"\n    df_grouped = grouped(df, key, freq, col)\n    name = 'Lag_' + str(lag)\n    df_grouped['Lag'] = df_grouped['mean'].shift(lag)\n    return df_grouped","4d6dcad0":"df_grouped_train_w_lag1 = add_lag(df_train, 'date', 'W', 'sales', 1)\ndf_grouped_train_m_lag1 = add_lag(df_train, 'date', 'W', 'sales', 1)\n\ndf_grouped_train_w_lag1.head() # check data","72d29799":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,20))\naxes[0].plot('Lag', 'mean', data=df_grouped_train_w_lag1, color='0.75', linestyle=(0, (1, 10)))\naxes[0].set_title(\"Sales (grouped by week)\", fontsize=20)\naxes[0] = sns.regplot(x='Lag', \n                      y='mean', \n                      data=df_grouped_train_w_lag1, \n                      scatter_kws=dict(color='0.75'), \n                      ax = axes[0])\n\n\naxes[1].plot('Lag', 'mean', data=df_grouped_train_m_lag1, color='0.75', linestyle=(0, (1, 10)))\naxes[1].set_title(\"Sales (grouped by month)\", fontsize=20)\naxes[1] = sns.regplot(x='Lag', \n                      y='mean', \n                      data=df_grouped_train_m_lag1, \n                      scatter_kws=dict(color='0.75'), \n                      line_kws={\"color\": \"red\"},\n                      ax = axes[1])\n\nplt.show()","edf2fb81":"def plot_stats(df, column, ax, color, angle):\n    \"\"\" PLOT STATS OF DIFFERENT COLUMNS \"\"\"\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=18)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","fed53225":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\nfig.autofmt_xdate()\nfig.suptitle(\"Stats of df_holidays\".upper())\nplot_stats(df_holidays, \"type\", axes[0], \"pastel\", 45)\nplot_stats(df_holidays, \"locale\", axes[1], \"rocket\", 45)\nplt.show()","c07224e7":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,40))\nplot_stats(df_stores, \"city\", axes[0], \"mako_r\", 45)\nplot_stats(df_stores, \"state\", axes[1], \"rocket_r\", 45)\nplot_stats(df_stores, \"type\", axes[2], \"magma\", 0)\nplot_stats(df_stores, \"cluster\", axes[3], \"viridis\", 0)\nplt.show()","423bc7e0":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ncount_classes = df_train['family'].value_counts()\nplt.title(\"Stats of df_train\".upper())\ncolors = ['#ff9999','#66b3ff','#99ff99',\n          '#ffcc99', '#ffccf9', '#ff99f8', \n          '#ff99af', '#ffe299', '#a8ff99',\n          '#cc99ff', '#9e99ff', '#99c9ff',\n          '#99f5ff', '#99ffe4', '#99ffaf']\n\nplt.pie(count_classes, \n        labels = count_classes.index, \n        autopct='%1.1f%%',\n        shadow=True, \n        startangle=90, \n        colors=colors)\n\nplt.show()","4c4f1436":"def plot_boxplot(palette, x, y, hue, ax, title):\n    sns.set_theme(style=\"ticks\", palette=palette)\n    ax = sns.boxplot(x=x, y=y, hue=hue, ax=ax)\n    ax.set_title(title, fontsize=18)","8b382715":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30,60))\nplot_boxplot(\"pastel\", df_oil['date'].dt.year, df_oil['dcoilwtico'], df_oil['date'].dt.month, axes[0], \"df_oil\")\nplot_boxplot(\"pastel\", df_oil['date'].dt.year, df_oil['dcoilwtico'], df_oil['date'].dt.year, axes[1], \"df_oil\")\nplot_boxplot(\"pastel\", df_trans['date'].dt.year, df_trans['transactions'], df_trans['date'].dt.month, axes[2], \"df_trans\")\nplot_boxplot(\"pastel\", df_trans['date'].dt.year, df_trans['transactions'], df_trans['date'].dt.year, axes[3], \"df_trans\")\nplt.show()","4eb03631":"def plot_moving_average(df, key, freq, col, window, min_periods, ax, title):\n    df_grouped = grouped(df, key, freq, col)\n    moving_average = df_grouped['mean'].rolling(window=window, center=True, min_periods=min_periods).mean()   \n    ax = df_grouped['mean'].plot(color='0.75', linestyle='dashdot', ax=ax)\n    ax = moving_average.plot(linewidth=3, color='g', ax=ax)\n    ax.set_title(title, fontsize=18)","171e0388":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,20))\nplot_moving_average(df_trans, 'date', 'W', 'transactions', 7, 4, axes[0], 'Transactions Moving Average')\nplot_moving_average(df_train, 'date', 'W', 'sales', 7, 4, axes[1], 'Sales Moving Average')\nplt.show()","00f4d796":"def plot_deterministic_process(df, key, freq, col, ax1, title1, ax2, title2):\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\") \n    dp = DeterministicProcess(index=df_grouped['date'], constant=True, order=1, drop=True)\n    dp.index.freq = freq # manually set the frequency of the index\n    # 'in_sample' creates features for the dates given in the `index` argument\n    X1 = dp.in_sample()\n    y1 = df_grouped[\"mean\"]  # the target\n    y1.index = X1.index\n    # The intercept is the same as the `const` feature from\n    # DeterministicProcess. LinearRegression behaves badly with duplicated\n    # features, so we need to be sure to exclude it here.\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X1, y1)\n    y1_pred = pd.Series(model.predict(X1), index=X1.index)\n    ax1 = y1.plot(linestyle='dashed', label=\"mean\", color=\"0.75\", ax=ax1, use_index=True)\n    ax1 = y1_pred.plot(linewidth=3, label=\"Trend\", color='b', ax=ax1, use_index=True)\n    ax1.set_title(title1, fontsize=18)  \n    _ = ax1.legend()\n    \n    # forecast Trend for future 30 steps\n    steps = 30 \n    X2 = dp.out_of_sample(steps=steps)\n    y2_fore = pd.Series(model.predict(X2), index=X2.index)\n    y2_fore.head()\n    ax2 = y1.plot(linestyle='dashed', label=\"mean\", color=\"0.75\", ax=ax2, use_index=True)\n    ax2 = y1_pred.plot(linewidth=3, label=\"Trend\", color='b', ax=ax2, use_index=True)\n    ax2 = y2_fore.plot(linewidth=3, label=\"Predicted Trend\", color='r', ax=ax2, use_index=True)\n    ax2.set_title(title2, fontsize=18)  \n    _ = ax2.legend()","bf39b5fd":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30,30))\nplot_deterministic_process(df_trans, 'date', 'W', 'transactions', \n                           axes[0], \"Transactions Linear Trend\",  \n                           axes[1], \"Transactions Linear Trend Forecast\")\nplot_deterministic_process(df_train, 'date', 'W', 'sales', \n                           axes[2], \"Sales Linear Trend\", \n                           axes[3], \"Sales Linear Trend Forecast\")\nplt.show()","c70ef63d":"def seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(x=X[freq], \n                      y=X[y],\n                      ax=ax, \n                      hue=X[period],\n                      palette=palette, \n                      legend=False)\n    ax.set_title(f\"Seasonal Plot ({period}\/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(name, \n                    xy=(1, y_), \n                    xytext=(6, 0), \n                    color=line.get_color(), \n                    xycoords=ax.get_yaxis_transform(), \n                    textcoords=\"offset points\", \n                    size=14, \n                    va=\"center\")\n    return ax","676c15ef":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"365D\") \/ pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(ts, fs=fs, detrend=detrend, window=\"boxcar\", scaling='spectrum')\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels([\"Annual (1)\", \"Semiannual (2)\", \"Quarterly (4)\", \n                        \"Bimonthly (6)\", \"Monthly (12)\", \"Biweekly (26)\", \n                        \"Weekly (52)\", \"Semiweekly (104)\"], rotation=30)\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","04a02e6e":"def seasonality(df, key, freq, col):\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\")\n    df_grouped.index = df_grouped['date'] \n    df_grouped = df_grouped.drop(columns=['date'])\n    df_grouped.index.freq = freq # manually set the frequency of the index\n    \n    X = df_grouped.copy()\n    X.index = pd.to_datetime(X.index, format = \"%Y-%m-%d\") \n    X.index.freq = freq \n    # days within a week\n    X[\"day\"] = X.index.dayofweek   # the x-axis (freq)\n    X[\"week\"] = pd.Int64Index(X.index.isocalendar().week)  # the seasonal period (period)\n    # days within a year\n    X[\"dayofyear\"] = X.index.dayofyear\n    X[\"year\"] = X.index.year\n    fig, (ax0, ax1, ax2) = plt.subplots(3, 1, figsize=(20, 30))\n    seasonal_plot(X, y='mean', period=\"week\", freq=\"day\", ax=ax0)\n    seasonal_plot(X, y='mean', period=\"year\", freq=\"dayofyear\", ax=ax1)\n    X_new = (X['mean'].copy()).dropna()\n    plot_periodogram(X_new, ax=ax2)","b0e4ac7d":"# df_trans, grouped by day\nseasonality(df_trans, 'date', 'D', 'transactions')","d218b0d3":"# df_train, grouped by day\nseasonality(df_train, 'date', 'D', 'sales')","c7dc163c":"def predict_seasonality(df, key, freq, col, ax1, title1):\n    fourier = CalendarFourier(freq=\"A\", order=10)  # 10 sin\/cos pairs for \"A\"nnual seasonality\n    df_grouped = grouped(df, key, freq, col)\n    df_grouped['date'] = pd.to_datetime(df_grouped['date'], format = \"%Y-%m-%d\") \n    df_grouped['date'].freq = freq # manually set the frequency of the index\n    dp = DeterministicProcess(index=df_grouped['date'], \n                              constant=True, \n                              order=1, \n                              period=None, \n                              seasonal=True, \n                              additional_terms=[fourier], \n                              drop=True)\n    dp.index.freq = freq # manually set the frequency of the index\n\n    # 'in_sample' creates features for the dates given in the `index` argument\n    X1 = dp.in_sample()\n    y1 = df_grouped[\"mean\"]  # the target\n    y1.index = X1.index\n\n    # The intercept is the same as the `const` feature from\n    # DeterministicProcess. LinearRegression behaves badly with duplicated\n    # features, so we need to be sure to exclude it here.\n    model = LinearRegression(fit_intercept=False)\n    model.fit(X1, y1)\n    y1_pred = pd.Series(model.predict(X1), index=X1.index)\n    X1_fore = dp.out_of_sample(steps=90)\n    y1_fore = pd.Series(model.predict(X1_fore), index=X1_fore.index)\n    \n    ax1 = y1.plot(linestyle='dashed', style='.', label=\"init mean values\", color=\"0.4\", ax=ax1, use_index=True)\n    ax1 = y1_pred.plot(linewidth=3, label=\"Seasonal\", color='b', ax=ax1, use_index=True)\n    ax1 = y1_fore.plot(linewidth=3, label=\"Seasonal Forecast\", color='r', ax=ax1, use_index=True)\n    ax1.set_title(title1, fontsize=18)  \n    _ = ax1.legend()","0979420d":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(30,30))\npredict_seasonality(df_trans, 'date', 'W', 'transactions', axes[0], \"Transactions Seasonal Forecast\")\npredict_seasonality(df_train, 'date', 'W', 'sales', axes[1], \"Sales Seasonal Forecast\")\nplt.show()","fe58cb0f":"store_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean() \n    .unstack('family')\n    .loc['2017', ['sales', 'onpromotion']]\n)\n\nmag_sales = family_sales.loc(axis=1)[:, 'MAGAZINES']","ec27ccd7":"store_sales.head()","ef33ffe0":"mag_sales.head()","eaf631d3":"y = mag_sales.loc[:, 'sales'].squeeze()\n\nfourier = CalendarFourier(freq='M', order=4)\ndp = DeterministicProcess(\n    constant=True,\n    index=y.index,\n    order=1,\n    seasonal=True,\n    drop=True,\n    additional_terms=[fourier],\n)\nX_time = dp.in_sample()\nX_time['NewYearsDay'] = (X_time.index.dayofyear == 1)\n\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_time, y)\ny_deseason = y - model.predict(X_time)\ny_deseason.name = 'sales_deseasoned'\n\nax = y_deseason.plot()\nax.set_title(\"Magazine Sales (deseasonalized)\");","4748ea09":"def lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) \/ x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) \/ y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(alpha=0.75,s=3)\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_, y=y_, scatter_kws=scatter_kws, line_kws=line_kws, lowess=True, ax=ax, **kwargs)\n    at = AnchoredText(f\"{corr:.2f}\",prop=dict(size=\"large\"), frameon=True, loc=\"upper left\")\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags \/ nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2 + 10, nrows * 2 + 5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig","1f524e84":"_ = plot_lags(y_deseason, lags=8, nrows=2)","b32db1cb":"_ = plot_pacf(y_deseason, lags=8)","093b8465":"onpromotion = mag_sales.loc[:, 'onpromotion'].squeeze().rename('onpromotion')\n\n# Drop the New Year outlier\nplot_lags(x=onpromotion.iloc[1:], y=y_deseason.iloc[1:], lags=3, nrows=1)","1b98d3d6":"def make_lags(ts, lags):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(1, lags + 1)\n        },\n        axis=1)","c5b2b26f":"X = make_lags(y_deseason, lags=4)\nX = X.fillna(0.0)","4dde382b":"# Create target series and data splits\ny = y_deseason.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False)\n\n# Fit and predict\nmodel = LinearRegression()  # `fit_intercept=True` since we didn't use DeterministicProcess\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nax = y_train.plot(color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", ax=ax)\nax = y_test.plot(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", ax=ax)\nax = y_pred.plot(ax=ax)\n_ = y_fore.plot(ax=ax, color='C3')\nplt.show()","de798eed":"store_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']\n)","22f6991a":"# we'll add fit and predict methods to this minimal class\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method","e20018c9":"def fit(self, X_1, X_2, y):\n    # train model_1\n    self.model_1.fit(X_1, y)\n\n    # make predictions\n    y_fit = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=y.columns,\n    )\n\n    # compute residuals\n    y_resid = y - y_fit\n    y_resid = y_resid.stack().squeeze() # wide to long\n\n    # train model_2 on residuals\n    self.model_2.fit(X_2, y_resid)\n\n    # save column names for predict method\n    self.y_columns = y.columns\n    # Save data for question checking\n    self.y_fit = y_fit\n    self.y_resid = y_resid\n\n\n# Add method to class\nBoostedHybrid.fit = fit","7f8d604e":"def predict(self, X_1, X_2):\n    # Predict with model_1\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1), \n        index=X_1.index, columns=self.y_columns,\n    )\n    y_pred = y_pred.stack().squeeze()  # wide to long\n\n    # Add model_2 predictions to model_1 predictions\n    y_pred += self.model_2.predict(X_2)\n\n    return y_pred.unstack()\n\n\n# Add method to class\nBoostedHybrid.predict = predict","459de10d":"# Target series\ny = family_sales.loc[:, 'sales']\n\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\n\n# X_2: Features for XGBoost\nX_2 = family_sales.drop('sales', axis=1).stack()  # onpromotion feature\n\n# Label encoding for 'family'\nle = LabelEncoder()  # from sklearn.preprocessing\nX_2 = X_2.reset_index('family')\nX_2['family'] = le.fit_transform(X_2['family'])\n\n# Label encoding for seasonality\nX_2[\"day\"] = X_2.index.day  # values are day of the month","01a1b8c9":"# Create model\nmodel = BoostedHybrid(\n    model_1=LinearRegression(),\n    model_2=XGBRegressor())\n\nmodel.fit(X_1, X_2, y)\n\ny_pred = model.predict(X_1, X_2)\ny_pred = y_pred.clip(0.0)","67c4a2e3":"# Boosted Hybrid\nmodel = BoostedHybrid(\n    model_1=Ridge(),\n    model_2=KNeighborsRegressor(),\n)","818d9993":"y_train, y_valid = y[:\"2017-07-01\"], y[\"2017-07-02\":]\nX1_train, X1_valid = X_1[: \"2017-07-01\"], X_1[\"2017-07-02\" :]\nX2_train, X2_valid = X_2.loc[:\"2017-07-01\"], X_2.loc[\"2017-07-02\":]\n\n# Some of the algorithms above do best with certain kinds of\n# preprocessing on the features (like standardization), but this is\n# just a demo.\nmodel.fit(X1_train, X2_train, y_train)\ny_fit = model.predict(X1_train, X2_train).clip(0.0)\ny_pred = model.predict(X1_valid, X2_valid).clip(0.0)\n\nfamilies = y.columns[0:6]\naxs = y.loc(axis=1)[families].plot(subplots=True, \n                                   sharex=True, \n                                   figsize=(30, 20), \n                                   color=\"0.75\",\n                                   style=\".-\",\n                                   markeredgecolor=\"0.25\",\n                                   markerfacecolor=\"0.25\",\n                                   alpha=0.5)\n_ = y_fit.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C0', ax=axs)\n_ = y_pred.loc(axis=1)[families].plot(subplots=True, sharex=True, color='C3', ax=axs)\nfor ax, family in zip(axs, families):\n    ax.legend([])\n    ax.set_ylabel(family)","37ff2e61":"# train data\nstore_sales = df_train.copy()\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']\n)","893f9f4f":"# test data\ntest = df_test.copy()\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()","348c24b3":"def make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)","79edc4f0":"y = family_sales.loc[:, 'sales']\n\n# make 4 lag features\nX = make_lags(y, lags=5).dropna()\n\n# make multistep target\ny = make_multistep_target(y, steps=16).dropna()\n\ny, X = y.align(X, join='inner', axis=0)","f158f496":"le = LabelEncoder()\nX = (X\n    .stack('family')  # wide to long\n    .reset_index('family')  # convert index to column\n    .assign(family=lambda x: le.fit_transform(x.family))  # label encode\n)\ny = y.stack('family')  # wide to long\n\ndisplay(y) ","7b487ade":"# init model\nmodel = RegressorChain(base_estimator=XGBRegressor())","e8bf78fc":"# train model\nmodel.fit(X, y)\ny_pred = pd.DataFrame(model.predict(X), index=y.index,columns=y.columns).clip(0.0)","b446f500":"# helpful function\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax","e9c2287a":"FAMILY = 'BEAUTY'\nSTART = '2017-04-01'\nEVERY = 16\n\ny_pred_ = y_pred.xs(FAMILY, level='family', axis=0).loc[START:]\ny_ = family_sales.loc[START:, 'sales'].loc[:, FAMILY]\n\nfig, ax = plt.subplots(1, 1, figsize=(11, 4))\nax = y_.plot(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\", markerfacecolor=\"0.25\",ax=ax, alpha=0.5)\nax = plot_multistep(y_pred_, ax=ax, every=EVERY)\n_ = ax.legend([FAMILY, FAMILY + ' Forecast'])","df8c97f7":"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let's build same plots, but with ***'lag'*** feature:","1a318223":"Also, we need to convert all ***'date'*** columns to datetime Pandas format:","4a8ec3fb":"After that, we can **predict seasonality**, using **DeterministicProcess**, as we used for Trend. We are going to forecast seasonality for **Transactions** and **Sales**.","4bbe2ea6":"# Store Sales. Time Series Forecast & Visualization\nHello everyone! In this my new notebook we are going to explore Time Series (dataset: Store Sales). \n\n\n### References\nFor this notebook I would like to say thank you some authors for their notebooks that have inspired me to write own notebook:\n\n1. [KASHISH RASTOGI. \ud83d\udcddStore Sales Analysis\u23f3 Time Serie](https:\/\/www.kaggle.com\/kashishrastogi\/store-sales-forecasting)\n\n2. [HOWOO JANG. First kaggle notebook. Following TS tutorial](https:\/\/www.kaggle.com\/howoojang\/first-kaggle-notebook-following-ts-tutorial)\n\n3. [EKREM BAYAR. Store Sales TS Forecasting - A Comprehensive Guide](https:\/\/www.kaggle.com\/ekrembayar\/store-sales-ts-forecasting-a-comprehensive-guide)\n\n4. [Ryan Holbrook. Time Series](https:\/\/www.kaggle.com\/learn\/time-series)","2d5f33a8":"Let's take a look at **seasonal plots** over a week and over a year. Here we can see the plots for **df_trans**.","e02e39e6":"Here we can see **df_stores**:","d279f75c":"# 3.2 Lag feature\nTo make a lag feature we shift the observations of the target series so that they appear to have occured later in time. Here we've created a 1-step lag feature, though shifting by multiple steps is possible too. So, firstly, we should **add lag** to our data:","6278d6ce":"Here we can see **df_holidays**:","dfebdaf0":"Also, we need to create **fit** method:","1a861a17":"Here we can **check data store_sales**:","e95a3a5c":"Here we can **check data mag_sales**:","15e44062":"Here we can see the plots for **df_train**.","6e3e1271":"# 5. Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. Here we'll learn how to create **\"hybrid\" forecasters** that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other.","2bedec70":"Also, we need to define helpfull function, **plot_multistep**:","a175e2a3":"# 3.1. Linear Regression\nAfter that, we can build some more plots. **Linear regression** is widely used in practice and adapts naturally to even complex forecasting tasks. The linear regression algorithm learns how to make a weighted sum from its input features.","c2b025eb":"After that, we can look and check our different dataframes:","fdc7dfa4":"Let's take a look at the **lag** and **autocorrelation plots** first:","419400c3":"**IF YOU LIKED THIS ARTICLE ABOUT TIME SERIES, PLEASE, MAKE AN UPVOTE \u2764\ufe0f**","4e951951":"Here we can see stats for **df_holidays**:","4dddbd72":"# 3.7 Seasonality\nTime series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times. Just like we used a moving average plot to discover the trend in a series, we can use a **seasonal plot** to discover seasonal patterns.","ecdab286":"Here we can see **df_oil**:","0e9288f6":"Here we can see **df_train**:","8393ed9a":"# 6. Machine learning forecasting","4562469e":"Here we can see **Linear Trend** & **Linear Trend Forecast** for **Transactions** (plots 1,2) and **Sales** (plots 3,4).","de547fb4":"Here we can **check grouped data**:","255bcc39":"Here we can **train** our model:","17c996c2":"# 3. Visualize data\nOne of the biggest parts of the notebook. Here we can look through some variables and see some dependencies. Firstly, let's check the **dependency of the oil from the date**:","ca945cc2":"Here we examine leading and lagging values for **onpromotion** plotted against **magazine sales**.","14c03427":"After that we train and plot","37c140ae":"So, now, we can **plot results**:","8f4d45a8":"And we can build plot with **predictions**:","8e93f68d":"# 3.6. Trend. Forecasting Trend\nWe'll use a function from the **statsmodels** library called **DeterministicProcess**. Using this function will help us avoid some tricky failure cases that can arise with time series and linear regression. The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.","ab461b95":"Here we can **plot data**:","eb22726d":"Here we count values for some columns of **df_stores**:","3e6a1885":"Here we can see **df_trans**:","08539d65":"# 3.4 BoxPlot\nIn addition, we can build some **boxplots**: for **df_oil** & **df_trans**.","e8b40ed9":"Firslty, we should create **Boosted Hybrid class**:","3f4db971":"# 3.3 Some more statistics & visualizations\nIn this block we are going to explore data. Firstly, let's count for each category in each dataset, ***value_counts()***:","728aaad1":"Here we can **set up data for training**:","202c1fbd":"And **predict** method:","ea16dfc7":"And, for better forecasting we'll add ***'time'*** column to our dataframe.","c4f45522":"Here we can see **df_test**:","d05461b4":"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict (in the same row, in other words). This makes lagged series useful as features for modeling serial dependence. To forecast series, we could use y_lag_1 and y_lag_2 as features to predict the target y.","76ddaf78":"# 6.1 Forecast with the DirRec strategy\nInstatiate a model that applies the DirRec strategy to XGBoost.","3fe8baaf":"# 3.5 Trend. Moving Average\nThe **trend** component of a time series represents a persistent, long-term change in the mean of the series. The trend is the slowest-moving part of a series, the part representing the largest time scale of importance. In a time series of product sales, an increasing trend might be the effect of a market expansion as more people become aware of the product year by year.\n\nTo see what kind of trend a time series might have, we can use a **moving average** plot. To compute a moving average of a time series, we compute the average of the values within a sliding window of some defined width. Each point on the graph represents the average of all the values in the series that fall within the window on either side. The idea is to smooth out any short-term fluctuations in the series so that only long-term changes remain.\n\nBelow we can see the moving average plots for **Transactions** and **Sales**, colored in green.","791b1400":"So, now we can check the results of grouping on the example of **df_train (grouped by weeks on sales, after that, mean was counted)**.","3f785f34":"# 2. Read data","434e0d00":"Let's plot **pie chart** for ***'family'*** of **df_train**:","ef7f40ee":"# 4. Time Series as Features","f7e3764c":"Here we can **check grouped data with lag**:","26b619d8":"Here we **prepare the data** for XGBoost:","1d8ac6a0":"# 4.2 Lags. Forecasting\nAfter that, we can **make lags** for future plots.","3c7a2e28":"# 1. Import libraries","e8165cf0":"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by **mean**.","646066f8":"# 4.1 Lag plot\nA lag plot of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. The most commonly used measure of serial dependence is known as **autocorrelation**, which is simply the correlation a time series has with one of its lags. The **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of \"new\" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use.\n","2fe52e65":"# 7. Conclusion\nThank you for reading my new article!\n\nHope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https:\/\/www.kaggle.com\/maricinnamon\/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https:\/\/www.kaggle.com\/maricinnamon\/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https:\/\/www.kaggle.com\/maricinnamon\/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https:\/\/www.kaggle.com\/maricinnamon\/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https:\/\/www.kaggle.com\/maricinnamon\/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https:\/\/www.kaggle.com\/maricinnamon\/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https:\/\/www.kaggle.com\/maricinnamon\/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https:\/\/www.kaggle.com\/maricinnamon\/retail-trade-report-department-stores-lstm)"}}