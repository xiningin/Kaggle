{"cell_type":{"cf30fdfd":"code","18df4e26":"code","1fbe18cb":"code","6d01e515":"code","350b520c":"code","6234246a":"code","33a2a0f3":"code","e3653b88":"code","3bf41064":"code","af2efa0f":"code","e29b5ac0":"code","274f0aef":"code","de0176ef":"code","2f857963":"markdown","75bb50a2":"markdown","6e51b1ce":"markdown","1a28c9a4":"markdown","3edf2758":"markdown","be9299af":"markdown","f3030820":"markdown","35467d5d":"markdown","8ed58d1c":"markdown"},"source":{"cf30fdfd":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf","18df4e26":"#Training Dataset\ntrain=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nprint(\"Training Dataset Shape:\",train.shape)\ntrain.head()","1fbe18cb":"#Testing Dataset\ntest=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nprint('Test Dataset Shape:',test.shape)\ntest.head()","6d01e515":"#Extracting the dependent and independent variables\ny=train['label'].values #To be predicted\nx=train.drop('label',axis=1).values #Independent Variables\nprint('Variables Shape:',x.shape)\nprint('Labels Shape:',y.shape)","350b520c":"#Displaying some images from the dataset\nimport matplotlib.pyplot as plt\nx=x.reshape(x.shape[0],28,28)\nfig=plt.figure(figsize=(12,5))\nfor i in range(30):\n    plt.subplot(3,10,i+1)\n    plt.axis('off')\n    plt.imshow(x[i],cmap=plt.cm.binary)","6234246a":"#Reshaping and Normalization\nfrom keras.utils.np_utils import to_categorical\n\n#Reshape images to input into CNN layers as a 4D Tensor\nx=x.reshape(-1,28,28,1)\ntest=test.values.reshape(-1,28,28,1)\n#Normalization\nx=x\/255\ntest=test\/255\n#One Hot Encoding the labels\ny=to_categorical(y)","33a2a0f3":"#Data Augmentation to increase number of input images\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen=ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    fill_mode='nearest'\n)","e3653b88":"#CNN Ensemble Models\n#To learn about deep learning ensemble models:\n# https:\/\/machinelearningmastery.com\/ensemble-methods-for-deep-learning-neural-networks\/\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,GlobalAveragePooling2D,MaxPool2D,Flatten,BatchNormalization\n\nensem=10\nmodel=[0]*ensem\nfor i in range(ensem):\n    model[i]=Sequential()\n    model[i].add(Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu',input_shape=(28,28,1)))\n    model[i].add(BatchNormalization())\n    model[i].add(Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(MaxPool2D(2,2))\n    model[i].add(Dropout(0.2))\n\n    model[i].add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n    model[i].add(BatchNormalization())\n    model[i].add(MaxPool2D(2,2))\n    model[i].add(Dropout(0.2))\n\n    model[i].add(GlobalAveragePooling2D())\n    model[i].add(Dense(128,activation='relu'))\n    model[i].add(Dropout(0.2))\n    model[i].add(Dense(10,activation='softmax'))\n    \n    model[i].compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","3bf41064":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\ncallback=tf.keras.callbacks.EarlyStopping(monitor='accuracy',min_delta=0,patience=5,mode='auto',restore_best_weights=True,verbose=0)\nlrs=tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\n\nhistory=[0]*ensem\nfor i in range(ensem):\n    train_x,valid_x,train_y,valid_y=train_test_split(x,y,test_size=0.2)\n    history[i]=model[i].fit_generator(train_datagen.flow(train_x,train_y,batch_size=128),\n                      epochs=100,\n                      steps_per_epoch=train_x.shape[0]\/\/128,\n                      verbose=0,\n                      validation_data=(valid_x,valid_y),\n                      validation_steps=valid_x.shape[0]\/\/128,\n                      callbacks=[callback,lrs])\n    print('Model {}: Epochs=100, Train_Accuracy:{}, Val_Accuracy:{}'.format(i+1,max(history[i].history['accuracy']),max(history[i].history['val_accuracy'])))","af2efa0f":"#Models Accuracy\nimport matplotlib.pyplot as plt\n\nstyles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']\nnames=['Model {}'.format(i) for i in range(ensem)]\nplt.figure(figsize=(15,5))\nfor i in range(ensem):\n    plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\nplt.title('Models accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('# Epoch')\nplt.legend(names, loc='lower right')\naxes = plt.gca()\naxes.set_ylim([0.8,1])\nplt.show()","e29b5ac0":"#Plotting the loss and accuracy for the first model.\n#All the modesl follow a similar trend\n\ndef plot_model(history):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,4))\n    fig.suptitle('Model Accuracy and Loss')\n\n    ax1.plot(history[0].history['accuracy'])\n    ax1.plot(history[0].history['val_accuracy'])\n    ax1.title.set_text('Accuracy')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.legend(['Train','Valid'],loc=4)\n\n    ax2.plot(history[0].history['loss'])\n    ax2.plot(history[0].history['val_loss'])\n    ax2.title.set_text('Loss')\n    ax2.set_ylabel('Loss')\n    ax2.set_xlabel('Epoch')\n    ax2.legend(['Train','Valid'],loc=1)\n\n    fig.show()\n\nplot_model(history)","274f0aef":"#Classification Report and Confusion Matrix for the first model\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n#Predicting values from the validation dataset and one hot encoding the predicted and true labels\ny_pred=model[0].predict(valid_x)\ny_pred_classes=np.argmax(y_pred, axis=1)\ny_true=np.argmax(valid_y, axis=1)\n\n#Classification Report\nprint('Classification Report')\nreport=classification_report(y_true, y_pred_classes)\nprint(report)\n\n#Computuing and Plotting Confusion Matrix\nconfusion_mtx=confusion_matrix(y_true, y_pred_classes)\nf,ax=plt.subplots(figsize=(16,8))\nsns.heatmap(confusion_mtx,annot=True,fmt='')\nplt.xlabel(\"Predicted\",size=12)\nplt.ylabel(\"True\",size=12)\nplt.title(\"Confusion Matrix\",size=20)\nplt.show()","de0176ef":"prediction=np.zeros((test.shape[0],10)) \nfor i in range(ensem):\n    prediction=prediction+model[i].predict(test)\nprediction=np.argmax(prediction,axis = 1)\nprediction=pd.Series(prediction,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name=\"ImageId\"),prediction],axis=1)\nsubmission.to_csv(\"digit_recognizer.csv\",index=False)","2f857963":"* All the models follow a similar trend for Validation Accuracy","75bb50a2":"* Most of the values are correctly predicted with high precision\n* Highest error rate is between 7-2 and 4-9","6e51b1ce":"# Thank You \ud83d\ude4f","1a28c9a4":"### Hope this helps \ud83d\ude04","3edf2758":"# MNIST Digit Recognizer\n**\"Hello World\"**.\nThis notebook will help in understanding the basics of Computer Vision and design a CNN model to identify hand written numbers with high accuracy. Special thanks to **Chris Deotte** for his notebook https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n","be9299af":"# Plotting","f3030820":"# Submission","35467d5d":"# CNN Model","8ed58d1c":"# Data Reading and Cleaning"}}