{"cell_type":{"0db99346":"code","c39bffc0":"code","f4b36044":"code","2a5bb6d2":"code","88b725c8":"code","e23b8f51":"code","b1539207":"code","fbae9c00":"code","97b059f9":"code","dd15eda2":"code","f28b3893":"code","18edddf7":"code","f78d3d4b":"code","a4abcffc":"code","d96db4a0":"code","884bf957":"code","46e9f80b":"code","57df20a4":"code","ab92f7b8":"code","26913614":"code","ad48c815":"code","037ff5ad":"code","cbe8cd62":"code","bf3e8bbf":"code","bbc59026":"code","73ad7c19":"code","c3afcb08":"code","a3ae572b":"code","0e23c6b3":"code","3c5eacad":"code","973c884b":"code","44c81494":"code","258b7b90":"code","4aef4f34":"code","2b8774d7":"code","e3577dab":"code","02fa6a60":"code","4957dcd6":"code","54d7aa33":"code","305ddd98":"code","78842964":"code","d0412d2e":"code","1deb8efd":"code","5af6194c":"code","ce1d8835":"code","b35ea636":"code","65ad0bf2":"code","6f7e4c5f":"code","d86aab0b":"code","ddf06a80":"code","701d0f08":"code","83558bf1":"code","5cfbbdf5":"code","87deeaa4":"code","9c2b1e9b":"code","37b90a15":"code","843a22c0":"code","778df0b4":"code","8ff30be7":"code","56dd6bd8":"code","3286a9bb":"code","2555a7d0":"code","73910761":"code","ec51eb88":"code","6a511b8f":"code","703fb92e":"code","af2e01c3":"code","23a6fc4c":"code","7b30ea8f":"code","f78d4aca":"code","91f7145c":"code","e2ad6bc0":"code","d55b2075":"code","4c2c2a11":"code","a172253c":"code","3631c91d":"code","00dd80b1":"code","297b181c":"code","23be029e":"code","d1c8f887":"code","c637b341":"code","96b2c49b":"code","6282896f":"code","1e091988":"code","23da2afd":"code","6ce7e43d":"code","fc2def5f":"code","cbeb1fd4":"code","36bd56c3":"code","af389c23":"code","403c75b1":"code","dd877a4a":"code","47bf0037":"code","c122e4e4":"code","a756e41a":"code","cad9bf65":"code","0405e796":"code","8aef08ff":"code","8ed7b466":"code","55519f55":"code","a7d75599":"code","d370bf1f":"code","e193cd77":"code","709825b9":"code","06139cad":"code","e3df6475":"code","5794deca":"code","d0a1de7a":"code","af3a9c5e":"code","57b501f5":"code","13e26482":"code","f3e492d3":"code","42cb785c":"code","7f1c98e2":"code","08487fa2":"code","e2c3db67":"code","1ab649da":"code","1a0db9e6":"code","504a1f45":"code","482e9247":"code","219d03f5":"code","e88fc440":"markdown","f97249ba":"markdown"},"source":{"0db99346":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import Normalizer\nfrom tensorflow.keras import utils\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n#import tensorflow_federated as tff\nfrom tensorflow import keras\nfrom sklearn import preprocessing\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.backend import set_floatx\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.preprocessing import timeseries_dataset_from_array\nfrom tensorflow.keras.layers import InputLayer, BatchNormalization, LSTM, Dropout, Dense, Bidirectional\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\nfrom keras import optimizers\n#!pip install --upgrade nest_asyncio\n#!pip install --upgrade tensorflow_federated\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c39bffc0":"str_time = '10:15:41.0000001'\n\nsplit = str_time.split(':')\nnew_value = float(split[0]) * 60 * 60\nnew_value += float(split[1]) * 60\nnew_value += float(split[2])","f4b36044":"new_value","2a5bb6d2":"df1 = pd.read_csv('..\/input\/smartilizer-dataset\/Terra-D1-multi-labeled-interpolated.csv')\nprint('Before: ', df1['label'].size)\ndf1 = df1[np.all((df1.label.apply(float.is_integer), df1.label != 0), axis=0)]\nprint('After: ', df1['label'].size)\n\ndf2 = pd.read_csv('..\/input\/commercial-vehicles-sensor-data-set\/Terra-D2-multi-labeled-interpolated.csv')\nprint('Before: ', df2['label'].size)\ndf2 = df2[np.all((df2.label.apply(float.is_integer), df2.label != 0), axis=0)]\nprint('After: ', df2['label'].size)\n\n\n\nprint('Summary: ', df1['label'].size + df2['label'].size)","88b725c8":"df1 = df1[:300000]","e23b8f51":"df2 = df2.drop(['time'], axis = 1)\ndf2['label'] = df2['label'].astype(int) - 1\ndf2.to_csv('smartilizer-data.csv', sep = ',', index = False)","b1539207":"df2[:865000]","fbae9c00":"df1 = df1.drop(['time'], axis = 1)\ndf2 = df2.drop(['time'], axis = 1)\n\ndf1['label'] = df1['label'].astype(int) - 1\ndf2['label'] = df2['label'].astype(int) - 1","97b059f9":"subset1_1 = df2[:570000]\nsubset1_2 = df2[570000:]","dd15eda2":"subset1_1_train = subset1_1[:int(subset1_1.shape[0] * 0.8)]\nsubset1_1_test = subset1_1[int(subset1_1.shape[0] * 0.8):]\n\n\nsubset1_2_train = subset1_2[:int(subset1_2.shape[0] * 0.8)]\nsubset1_2_test = subset1_2[int(subset1_2.shape[0] * 0.8):]\n\nset2_train = df1[:int(df1.shape[0] * 0.8)]\nset2_test = df1[int(df1.shape[0] * 0.8):]","f28b3893":"set2_train","18edddf7":"all_test = pd.concat([subset1_1_test,subset1_2_test,set2_test], axis = 0)","f78d3d4b":"all_test","a4abcffc":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nt = subset1_1_train['time']\nl = subset1_1_train['label']\n\nt1 = subset1_1_test['time']\nl1 = subset1_1_test['label']\n\n\n\nt2 = subset1_2_train['time']\nl2 = subset1_2_train['label']\n\nt3 = subset1_2_test['time']\nl3 = subset1_2_test['label']\n\n\nt4 = set2_train['time']\nl4 = set2_train['label']\n\nt5 = set2_test['time']\nl5 = set2_test['label']\n\n\n\n","d96db4a0":"t5 = df1['time']\nl5 = df1['label']","884bf957":"plt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(0, 17200)\nplt.grid()      \n\nplt.plot(t1, l1, 'r')\nplt.plot(t2, l2, 'b')\nplt.plot(t3, l3, 'r')\n","46e9f80b":"plt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(0, 6500)\nplt.grid()      \nplt.plot(t4, l4, 'b')\nplt.plot(t5, l5, 'r')","57df20a4":"subset1_1_train","ab92f7b8":"#subset1_1_train.to_csv('subset1_1_train_6k.csv',sep = ',', index=True)\n#subset1_1_test.to_csv('subset1_1_test_6k.csv',sep = ',', index=True)\n\n#subset1_2_train.to_csv('subset1_2_train_6k.csv',sep = ',', index=True)\n#subset1_2_test.to_csv('subset1_2_test_6k.csv',sep = ',', index=True)\n\nset2_train.to_csv('set2_train_3k.csv',sep = ',', index=True)\nset2_test.to_csv('set2_test_3k.csv',sep = ',', index=True)","26913614":"subset1_1_train","ad48c815":"big_data_train = df2[:int(df2.shape[0] * 0.8)]\nbig_data_test =  df2[int(df2.shape[0] * 0.8):]\nsmall_data_train = df1[:int(df1.shape[0] * 0.8)]\nsmall_data_test =  df1[int(df1.shape[0] * 0.8):]","037ff5ad":"df2['time'].max() * 0.8","cbe8cd62":"e = big_data_test['label'].unique()","bf3e8bbf":"e","bbc59026":"small_data_test.shape[0] + big_data_test.shape[0]","73ad7c19":"big_data_test.to_csv('sw_1_big_test.csv',sep = ',', index=False)\nbig_data_train.to_csv('sw_1_big_train.csv',sep = ',', index=False)\n\nsmall_data_test.to_csv('sw_2_small_test.csv',sep = ',', index=False)\nsmall_data_train.to_csv('sw_2_small_train.csv',sep = ',', index=False)","c3afcb08":"a = '5.0'\nint(a)","a3ae572b":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\"\"\"\nt = df1['time']\nl = df1['label']\nx = df1['wx']\ny = df1['wy']\nz = df1['wz']\n\nt2 = df2['time']\nl2 = df2['label']\nx2 = df2['wx']\ny2 = df2['wy']\nz2 = df2['wz']\n\"\"\"\n\nt = df1['time']\nl = df1['label']\nx = df1['gFx']\ny = df1['gFy']\nz = df1['gFz']\n\n\n\nt2 = df2['time']\nl2 = df2['label']\nx2 = df2['gFx']\ny2 = df2['gFy']\nz2 = df2['gFz']\n\n\n","0e23c6b3":"l2","3c5eacad":"4500 * 0.8","973c884b":"plt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(0, 4000)\nplt.grid()      \nplt.plot(t, x)\nplt.plot(t, y)\nplt.plot(t, z)\nplt.plot(t, l)","44c81494":"plt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(0, 17000)\nplt.grid()      \nplt.plot(t2, x2)\nplt.plot(t2, y2)\nplt.plot(t2, z2)\nplt.plot(t2, l2)","258b7b90":"dataset = pd.concat([df1,df2],axis=0)\ndataset = dataset.drop(['time'], axis = 1)","4aef4f34":"grouped = dataset.label.unique()","2b8774d7":"grouped.astype(int)","e3577dab":"for i in grouped:\n    name = \"label%d\" % i\n    print(type(name))\n    globals()[name] = dataset[dataset['label'] == i]    ","02fa6a60":"def get_name(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name","4957dcd6":"labels = [label1,label2, label3, label4, label5]\n\nfor example in labels:\n    percent = example.shape[0] \/  2053015\n    print(get_df_name(example), 'ratio is ', percent)","54d7aa33":"def check_ratio(labels):\n    grouped = labels.label.unique()\n    grouped = grouped.astype(int)\n    grouped.sort()\n    for example in grouped:\n        tmp = labels.loc[labels['label'] == example]\n        ratio = tmp.shape[0] \/ labels.shape[0]\n        print('Label ', example,' ratio is', ratio )\n    print(labels.shape[0])\n    \n    ","305ddd98":"def divide_data(df, ratio1 = 0.2, ratio2 = 0.3):\n    \n    Y = df['label']\n    Y = Y.to_frame()\n    X = df.drop(['label'], axis = 1)\n    \n    shape = X.shape[0]\n    \n    train_X1, x, train_Y1, y = train_test_split(X, Y, test_size=(1-ratio1), random_state=1)\n    train_X2, x, train_Y2, y = train_test_split(x,y, test_size = 1 - (train_X1.shape[0] \/ x.shape[0]))\n    \n    train_X3, x, train_Y3, y = train_test_split(x,y, test_size = (1-ratio2), random_state=1)\n    train_X4, test_x, train_Y4, test_y = train_test_split(x,y, test_size = 1 - (train_X3.shape[0] \/ x.shape[0]))\n\n    \n    print('train_1 is ', train_X1.shape[0] \/ shape * 100, '% of all data')\n    print('train_2 is ', train_X2.shape[0] \/ shape * 100, '% of all data')\n    print('train_3 is ', train_X3.shape[0] \/ shape * 100, '% of all data')\n    print('train_4 is ', train_X4.shape[0] \/ shape * 100, '% of all data')\n    print('test is ', test_x.shape[0] \/ shape * 100, '% of all data')\n    \n    \n    return train_X1, train_Y1, train_X2,train_Y2,  train_X3,train_Y3, train_X4,train_Y4, test_x, test_y","78842964":"def process_labels(arr):\n    print('***************')\n    print('Check ratio for', get_name(arr))\n    check_ratio(arr)\n    print('***************')\n    arr = arr['label'] - 1\n    arr = utils.to_categorical(arr, 5)\n    return arr","d0412d2e":"train_X","1deb8efd":"train_X1, train_Y1, train_X2,train_Y2,  train_X3,train_Y3, train_X4,train_Y4, test_x, test_y = divide_data(dataset, ratio1=0.1)\n\ntrain_Y1 = process_labels(train_Y1)\ntrain_Y2 = process_labels(train_Y2)\ntrain_Y3 = process_labels(train_Y3)\ntrain_Y4 = process_labels(train_Y4)\ntest_y = process_labels(test_y)\n\n","5af6194c":"train_X = np.array(train_X)","ce1d8835":"\ntrain_X0 = subset1_1_train.drop(['label'], axis = 1)\ntrain_Y0 = subset1_1_train['label']\ntrain_Y0 = utils.to_categorical(train_Y0, 5)\n\ntest_X0 = subset1_1_test.drop(['label'], axis = 1)\ntest_Y0 = subset1_1_test['label']\ntest_Y0 = utils.to_categorical(test_Y0, 5)\n\n\n\ntrain_X1 = subset1_2_train.drop(['label'], axis = 1)\ntrain_Y1 = subset1_2_train['label']\ntrain_Y1 = utils.to_categorical(train_Y1, 5)\n\ntest_X1 = subset1_2_test.drop(['label'], axis = 1)\ntest_Y1 = subset1_2_test['label']\ntest_Y1 = utils.to_categorical(test_Y1, 5)\n\n\n\ntrain_X2 = set2_train.drop(['label'], axis = 1)\ntrain_Y2 = set2_train['label']\ntrain_Y2 = utils.to_categorical(train_Y2, 5)\n\ntest_X2 = set2_test.drop(['label'], axis = 1)\ntest_Y2 = set2_test['label']\ntest_Y2 = utils.to_categorical(test_Y2, 5)","b35ea636":"train_X0","65ad0bf2":"a = np.append(a, train_X1)","6f7e4c5f":"x = get_name(train_X1)[-1]","d86aab0b":"train_X1","ddf06a80":"model_v = 'Dense'\ndef create_model(train_X, train_Y):\n  model = tf.keras.Sequential([\n      Dense(train_X.shape[1], input_shape=(train_X.shape[1],)),\n      BatchNormalization(),\n      Dense(256, activation='relu'),\n      Dropout(.2),\n      BatchNormalization(),\n      Dense(64, activation='relu'),\n      Dropout(.2),\n      Dense(train_Y.shape[1], activation='softmax')\n  ])\n  print(train_X.shape[1], train_Y.shape[1])\n  return model","701d0f08":"for i in range(3):\n    exec('x = train_Y%s' % i)\n    print(x.shape)\n    ","83558bf1":"for i in range(3):\n    exec('train_X = train_X%s' % i)\n    exec('test_X = test_X%s' % i)\n    \n    exec('train_Y = train_Y%s' % i)\n    exec('test_Y = test_Y%s' % i)\n    \n    print('Training on set # ', i+1)\n    \n    print('Train data shape: ', train_X.shape)\n    print('Test data shape: ', test_X.shape)\n    \n\n\n    model_1 = create_model(train_X, train_Y)\n    opt = optimizers.SGD(learning_rate=0.01)\n    _opt = optimizers.Adam(learning_rate=0.01)\n\n    model_1.compile(optimizer=_opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n    print('Fitting model...')\n    #norm = Normalizer()\n    #train_X1 = norm.fit_transform(train_X1)\n    history = model_1.fit(train_X,train_Y, epochs=20, batch_size=32)\n\n\n    score, acc = model_1.evaluate(test_X, test_Y, batch_size = 32)\n    print('Loss: ', score, 'Acc: ', acc)\n","5cfbbdf5":"\nmodel_test = create_model(train_X2, train_Y2)\nopt = optimizers.SGD(learning_rate=0.01)\n_opt = optimizers.Adam(learning_rate=0.01)\n\nmodel_test.compile(optimizer=_opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nprint('Fitting model...')\n#norm = Normalizer()\n#train_X1 = norm.fit_transform(train_X1)\nhistory = model_test.fit(train_X2,train_Y2, epochs=20, batch_size=32)","87deeaa4":"\nscore, acc = model_test.evaluate(test_X2, test_Y2, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","9c2b1e9b":"model_2 = create_model(train_X, train_Y)\nopt = optimizers.SGD(learning_rate=0.01)\n_opt = optimizers.Adam(learning_rate=0.01)\n\nmodel_2.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nprint('Fitting model...')\n#norm = Normalizer()\n#train_X1 = norm.fit_transform(train_X1)\nhistory = model_2.fit(train_X0,train_Y0, epochs=20, batch_size=32)\n\n\nscore, acc = model_2.evaluate(test_X1, test_Y1, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","37b90a15":"score, acc = model_2.evaluate(test_X0, test_Y0, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","843a22c0":"plt.plot(history.history['accuracy'],\n        label = 'train')\nplt.plot(history.history['val_accuracy'],\n        label = 'validation')\nplt.xlabel('Epoch')\nplt.ylabel('% success')\nplt.legend()\nplt.show()","778df0b4":"test_x = norm.fit_transform(test_x)","8ff30be7":"test_x","56dd6bd8":"score, acc = model_1.evaluate(test_X, test_Y, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","3286a9bb":"test_Y.shape","2555a7d0":"%matplotlib inline\nimport matplotlib.pyplot as plt\nt = set2_test['time']\nx = set2_test['label']\nplt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(3000, 4000)\nplt.grid()      \n#plt.plot(t, x, 'ro')\nplt.plot(t, x)","73910761":"pred = model_1.predict(test_X)","ec51eb88":"x = np.array([])\ny = np.array([])\nfor i in range(test_Y.shape[0]):\n    x = np.append(x, np.argmax(test_Y[i]))\n    y = np.append(y, np.argmax(pred[i]))","6a511b8f":"from sklearn.metrics import classification_report, confusion_matrix\n\n\nprint(confusion_matrix(x, y))\n\nprint('Classification Report')\ntarget_names = ['1', '2', '3','4','5']\nprint(classification_report(x, y, target_names=target_names))","703fb92e":"import pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Defining and fitting a DecisionTreeClassifier instance\ntree = DecisionTreeClassifier(max_depth = 19)\ntree.fit(train_X, train_Y)","af2e01c3":"from sklearn.tree import export_graphviz\n\n# Creates dot file named tree.dot\nexport_graphviz(\n            tree,\n            out_file =  \"myTreeName.dot\",\n            feature_names = list(train_X.columns),\n            filled = True,\n            rounded = True)","23a6fc4c":"pred = tree.predict(test_X)\n\n","7b30ea8f":"test_Y","f78d4aca":"pred","91f7145c":"train_X.shape","e2ad6bc0":"pred.shape\n","d55b2075":"x = np.array([])\ny = np.array([])\nfor i in range(test_Y.shape[0]):\n    x = np.append(x, np.argmax(test_Y[i]))\n    y = np.append(y, np.argmax(pred[i]))\n    ","4c2c2a11":"x.shape","a172253c":"t = np.array(t)","3631c91d":"t","00dd80b1":"y","297b181c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nt = subset1_2_test['time']\nplt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(14000, 17000)\nplt.grid()      \n#plt.plot(t, x, 'ro')\nplt.plot(t,y)","23be029e":"plt.figure(figsize=(23, 5))\nplt.title(\"\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0439\") \nplt.xlabel(\"\u0412\u0440\u0435\u043c\u044f\") \nplt.ylabel(\"\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\") \nplt.xlim(0, 17000)\nplt.grid()      \nplt.plot(t2, x2)\nplt.plot(t2, y2)\nplt.plot(t2, z2)\nplt.plot(t2, l2)","d1c8f887":"pred[1000]","c637b341":"model_1.save('model1')","96b2c49b":"0.11001388674587255 * 77772 ","6282896f":"model_2 = keras.models.load_model('model1')","1e091988":"train_X3 = norm.fit_transform(train_X3)","23da2afd":"train_X3","6ce7e43d":"history2 = model_2.fit(train_X3, train_Y3, epochs=20, validation_split = 0.1,batch_size=32)","fc2def5f":"plt.plot(history2.history['accuracy'],\n        label = 'train')\nplt.plot(history2.history['val_accuracy'],\n        label = 'validation')\nplt.xlabel('Epoch')\nplt.ylabel('% success')\nplt.legend()\nplt.show()","cbeb1fd4":"score, acc = model_2.evaluate(test_x, test_y, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","36bd56c3":"data  =  df2.drop(['time'], axis = 1)\ndf2[:1050000]","af389c23":"number = 1050000\n\ntrain_data = data[:number]\ntest_data = data[number:]\n\ntrain_data = train_data.sample(number)\n\ntest_Y = test_data['label']\ntest_Y = test_Y.to_frame()\ntest_X = test_data.drop(['label'], axis = 1)\n\ntrain_Y = train_data['label']\ntrain_Y = train_Y.to_frame()\ntrain_X = train_data.drop(['label'], axis = 1)\n\ntest_Y = process_labels(test_Y)\ntrain_Y = process_labels(train_Y)","403c75b1":"model = create_model(train_X, train_Y)\nopt = optimizers.SGD(learning_rate=0.01)\n\nmodel.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n","dd877a4a":"history = model.fit(train_X, train_Y, epochs=10, batch_size=32)","47bf0037":"score, acc = model.evaluate(test_X, test_Y, batch_size = 32)\nprint('Loss: ', score, 'Acc: ', acc)","c122e4e4":"pred = model.predict(test_X)","a756e41a":"from tensorflow.keras.preprocessing import timeseries_dataset_from_array\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom scipy import stats\nfrom sklearn.preprocessing import OneHotEncoder","cad9bf65":"time_points = 600\nBATCH_SIZE = 32\n\n#Sweden\narg_num = 8\nclass_num = 5\n","0405e796":"df1 = pd.read_csv('..\/input\/smartilizer-dataset\/Terra-D1-multi-labeled-interpolated.csv')\nprint('Before: ', df1['label'].size)\ndf1 = df1[np.all((df1.label.apply(float.is_integer), df1.label != 0), axis=0)]\nprint('After: ', df1['label'].size)\n\ndf2 = pd.read_csv('..\/input\/commercial-vehicles-sensor-data-set\/Terra-D2-multi-labeled-interpolated.csv')\nprint('Before: ', df2['label'].size)\ndf2 = df2[np.all((df2.label.apply(float.is_integer), df2.label != 0), axis=0)]\nprint('After: ', df2['label'].size)\n\n\n\nprint('Summary: ', df1['label'].size + df2['label'].size)","8aef08ff":"df2['label'] = df2['label'].astype(int) - 1\nsubset1_1 = df2[:570000]\nsubset1_2 = df2[570000:]","8ed7b466":"\nsubset1_2_train = subset1_2[:int(subset1_2.shape[0]*0.8)]\nsubset1_2_test = subset1_2[int(subset1_2.shape[0]*0.8):]","55519f55":"subset1_2_train","a7d75599":"\ndef create_dataset(X, y, time_steps=1, step=1):\n    Xs, ys = [], []\n    for i in range(0, len(X) - time_steps, step):\n        v = X.iloc[i:(i + time_steps)].values\n        labels = y.iloc[i: i + time_steps]\n        Xs.append(v)\n        ys.append(stats.mode(labels)[0][0])\n    return np.array(Xs), np.array(ys).reshape(-1, 1)","d370bf1f":"TIME_STEPS = 300\nSTEP = 100\n\ntrain_X, train_Y = create_dataset(\n    subset1_2_train[['gFx', 'gFy', 'gFz','wx', 'wy', 'wz', 'speed']],\n    subset1_2_train.label,\n    TIME_STEPS,\n    STEP\n)\n\ntest_X, test_Y = create_dataset(\n    subset1_2_test[['gFx', 'gFy', 'gFz','wx', 'wy', 'wz', 'speed']],\n    subset1_2_test.label,\n    TIME_STEPS,\n    STEP\n)","e193cd77":"train_X.shape","709825b9":"train_X.shape","06139cad":"enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nenc = enc.fit(train_Y)\n\ntrain_Y = enc.transform(train_Y)\ntest_Y = enc.transform(test_Y)","e3df6475":"train_Y.max()","5794deca":"enc.categories_[0]","d0a1de7a":"opt = optimizers.SGD(learning_rate=0.01)\n\nmodel_v = '0.1' #CONST\ndef create_model():\n  model = Sequential([\n    Bidirectional(\n    LSTM(128, activation='tanh',input_shape=[train_X.shape[1], train_X.shape[2]], dropout=0.5)),\n    Dense(128, activation='relu'),\n    Dense(train_Y.shape[1], activation='softmax')\n  ])\n\n  model.compile(\n    loss='categorical_crossentropy',\n    optimizer=opt,\n    metrics=['acc'],\n  )\n  return model","af3a9c5e":"model_test = create_model()","57b501f5":"history = model_test.fit(\n    train_X, train_Y,\n    epochs=20,\n    batch_size=32,\n    shuffle=False\n)","13e26482":"model_test.evaluate(test_X, test_Y)","f3e492d3":"a = np.array([[[-34.1320000e-01, -2.0000000e-04,  9.5040000e-01, -6.2600000e-02,\n       -2.1800000e-02,  1.5560000e-01,  25.9824322e-01]]])","42cb785c":"y_pred = model_test.predict(test_X)","7f1c98e2":"y_pred","08487fa2":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_cm(y_true, y_pred, class_names):\n  cm = confusion_matrix(y_true, y_pred)\n  fig, ax = plt.subplots(figsize=(18, 16)) \n  ax = sns.heatmap(\n      cm, \n      annot=True, \n      fmt=\"d\", \n      cmap=sns.diverging_palette(220, 20, n=7),\n      ax=ax\n  )\n\n  plt.ylabel('Actual')\n  plt.xlabel('Predicted')\n  ax.set_xticklabels(class_names)\n  ax.set_yticklabels(class_names)\n  b, t = plt.ylim() # discover the values for bottom and top\n  b += 0.5 # Add 0.5 to the bottom\n  t -= 0.5 # Subtract 0.5 from the top\n  plt.ylim(b, t) # update the ylim(bottom, top) values\n  plt.show() # ta-da!","e2c3db67":"enc.categories_[0]","1ab649da":"a = np.array([0, 1, 2, 3])","1a0db9e6":"a","504a1f45":"a = np.array([])\nfor i in test_Y:\n    a = np.append(a,np.argmax(i))","482e9247":"np.unique(a)","219d03f5":"plot_cm(\n  enc.inverse_transform(test_Y),\n  enc.inverse_transform(y_pred),\n  a\n)","e88fc440":"# **lstm network**","f97249ba":"# Classes\n\n**1 -  idle;** the vehicle is not busy with the engine on.\n\n**2 - driving;** the dumper is moving forward or backwards\n(both hauling and driving empty).\n\n**3 -  loading;** the dumper is being loaded with mass, generally\nidle with the engine on. The time between each bucket is\nusually around 25 seconds. Such in-between time is also\nlabeled as loading, i.e. not as idle.\n\n**4 - dumping;** the vehicle is loosing its mass from the bed.\nThis activity starts when the dumper begins tilting its bed\nand ends when the bed is back at its initial position.\n\n**5 - engine-off;** the dumper is inactive with the engine off."}}