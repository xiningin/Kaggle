{"cell_type":{"6f9ee459":"code","c6707a9b":"code","17a1545c":"code","238bed1e":"code","d014d54a":"code","e3ae36a1":"code","a9e7d778":"code","377e6bab":"code","734c39a2":"code","0f2b2e1d":"code","7092dec3":"code","c190f2d5":"code","59c8fbf7":"code","243a141e":"code","facd6037":"code","59547975":"code","efe2ceba":"code","77edbed5":"code","9e6d78db":"code","10c2f919":"code","829e582a":"code","73829c33":"code","5be22b13":"code","f4e779e7":"code","54f5b726":"code","363f1629":"code","31878da2":"code","294441db":"code","126ac668":"code","1b964483":"code","bddb8e55":"code","a396d370":"code","fb1c85cc":"code","4e61f455":"code","e38382aa":"code","08343f8e":"code","6126cc49":"code","a061d308":"code","0843e8ba":"code","3373e711":"code","9a671070":"code","1dc41b8a":"code","cb33af2c":"code","ee11c7f5":"code","bae71b38":"code","bd2a5e64":"code","7e4978a0":"code","7a709650":"code","cff56d88":"code","10388420":"code","0fa9f66f":"code","52066026":"markdown","54ba7f50":"markdown"},"source":{"6f9ee459":"## ----------- Part 1: Define and categorize the problem statement --------------\n#### The problem statement is to \"Predict the daily bike rental count based on the environmental and seasonal settings\"\n##### This is clearly a 'Supervised machine learning regression problem' to predict a number based on the input features\n\n## ----------- Part 1 ends here ----------------- ","c6707a9b":"##------------- Import all the required libraries--------------\n\n## Import all the required libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\n#---- for model building\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n#from sklearn.cross_validation import train_test_split\n\n#---- for visualization---\nimport matplotlib.pyplot as plt \nimport seaborn as sn\n\n#------ for model evaluation -----\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#---- For handling warnings\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","17a1545c":"## ------------------- Part 2: Gather the data -----------------\n\n### Here data is provided as .csv file with the problem.\n### Let's import the data \n\n#bike = pd.read_csv(\"\/input..\/day.csv\")\nbike =pd.read_csv('..\/input\/train.csv')\nbike_test =pd.read_csv('..\/input\/test.csv')\nbike.head()\n\n##---------- Part 2 ends here --------------------------","238bed1e":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ---------------\n#### 3a.) Check the shape\/properties of the data\n#### 3b.) Completing -- Perform missing value analysis and impute missing values if necessary\n#### 3c.) Correcting -- Check for any invalid data inputs , for outliers or for any out of place data\n#### 3d.) Creating -- Feature extraction . Extract any new features from existing features if required\n#### 3e.) Converting -- Converting data to proper formats","d014d54a":"#### --------3a.) Check the shape\/properties of the data\n## Check the shape of the data\nbike.shape\n\n# what we can infer:\n## ->the dataset has 731 observations and 16 features","e3ae36a1":"## Check the properties of the data\nbike.info()\n# what we can infer:\n# ->There are no null values in the dataset\n# -> The datatypes are int,float and object ","a9e7d778":"# -------------- 3b.) Completing -- Perform missing value analysis and impute missing values if necessary\n# Although we have already seen above thatthere are no null values in the dataset. Lets try other way to confirm this\n#Checking nulls\nbike.isnull().sum().sort_values(ascending=False)\n\n# what we can infer:\n# ->There are no null values in the dataset.If it had, then eithere the rows\/columns had to be dropped or the null values be imputed based on the % of null values","377e6bab":"#### ------------------3c.) Correcting -- Check for any invalid data inputs , for outliers or for any out of place data\n# From above observations data doesnot seem to have any invalid datatypes to be handled\n# Let's check for the outliers in EDA step","734c39a2":"#### -----------------3d.) Creating -- Feature extraction . Extract any new features from existing features if required\nbike.head()\nbike.datetime.describe()\n## We can see that here we have 'datetime', which gives us the exact date. This features has 2 years of data(2011, 2012), all through 12 months(1 to 12) of a year\n## However, date(day of month) information is not saperately given.\n## Lets extract 'date','mnth','weekday' and 'yr' from 'datetime' column\nbike['datetime'] = pd.to_datetime(bike['datetime'])\nbike['date'] = bike['datetime'].dt.day\nbike['mnth'] = bike['datetime'].dt.month\nbike['yr'] = bike['datetime'].dt.year\nbike['weekday'] = bike['datetime'].dt.weekday\n\n#--convert year 2011 : 1 and 2012 : 2\nbike['yr']=bike.yr.replace({2011,2012},{1,2})\n\n\n## Now, 'dteday' column is not required, since we already have year, month, date info in other columns. So lets drop it.\nbike = bike.drop(columns=['datetime'])\n\n#--------repeating the same operation for test data ------------\nbike_test\nbike_test['datetime'] = pd.to_datetime(bike_test['datetime'])\nbike_test['date'] = bike_test['datetime'].dt.day\nbike_test['mnth'] = bike_test['datetime'].dt.month\nbike_test['yr'] = bike_test['datetime'].dt.year\nbike_test['weekday'] = bike_test['datetime'].dt.weekday\nbike_test['yr']=bike_test.yr.replace({2011,2012},{1,2})\nbike_test = bike_test.drop(columns=['datetime'])\n#---------------------------------------------------------------\n\nbike.tail()","0f2b2e1d":"#### 3e.) ------- Converting -- Converting data to proper formats\n#We can clearly see that \"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weather\",\"date\" are categories,rather than continous variable.\n#Let them convert to categories\ncategoryFeatureList = [\"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weather\",\"date\"]\nfor var in categoryFeatureList:\n    bike[var] = bike[var].astype(\"category\")\n    bike_test[var] = bike[var].astype(\"category\")\nbike.info()","7092dec3":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ENDS here---------------","c190f2d5":"# ------------Part 4 : Exploratory Data Analysis(EDA) STARTS here -----------","59c8fbf7":"#----- 4 a.) Outlier Analysis -----------","243a141e":"## -- Lets do the outlier analysis ----\n## -- Visualize continous variables(cnt,temp,atemp,humidity,windspeed) and \n##  count with respect to categorical variables(\"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weathersit\",\"date\")with boxplots ---\nfig, axes = plt.subplots(nrows=3,ncols=4)\nfig.set_size_inches(20,15)\n\n#-- Plot total counts on y bar\nsn.boxplot(data=bike, y=\"count\",ax=axes[0][0])\n\n#-- Plot temp on y bar\nsn.boxplot(data=bike, y=\"temp\",ax=axes[0][1])\n\n#-- Plot atemp on y bar\nsn.boxplot(data=bike, y=\"atemp\",ax=axes[0][2])\n\n#-- Plot hum on y bar\nsn.boxplot(data=bike, y=\"humidity\",ax=axes[0][3])\n\n#-- Plot windspeed on y bar\nsn.boxplot(data=bike, y=\"windspeed\",ax=axes[1][0])\n\n#-- Plot total counts on y-bar and 'yr' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"yr\",ax=axes[1][1])\n\n#-- Plot total counts on y-bar and 'mnth' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"mnth\",ax=axes[1][2])\n\n#-- Plot total counts on y-bar and 'date' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"date\",ax=axes[1][3])\n\n#-- Plot total counts on y-bar and 'season' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"season\",ax=axes[2][0])\n\n#-- Plot total counts on y-bar and 'weekday' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"weekday\",ax=axes[2][1])\n\n#-- Plot total counts on y-bar and 'workingday' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"workingday\",ax=axes[2][2])\n\n#-- Plot total counts on y-bar and 'weathersit' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"weather\",ax=axes[2][3])","facd6037":"# what we can infer from above boxplots:\n# -> There are many outliers.\n# Lets keep these outliers for now, till we complete full EDA(will remove the outliers in next update of kernel)","59547975":"#---- 4b.) Correlation Analysis\n#--- Explore continous features\n#--- Explore categorical features","efe2ceba":"#------------- Explore continous features -----------------\n##Explore the correlation btwn the independent continous features with target variabe\ncorr=bike[['temp','atemp','humidity','windspeed']].corrwith(bike['count'])\ncorr.plot.bar(figsize=(8,8), title='Correlation of features with the response variable count_of_rented_bikes', grid=True, legend=False, style=None, fontsize=None, colormap=None, label=None)","77edbed5":"##------heatmap for correlation matrix---------##\n##to check multicollinearity ---##\n\n#correlation matrix\nsn.set(style='white')\n#compute correlation matrix\ncorr =bike.drop(columns=['count']).corr()\n#generate a mask for upper triangle#\nmask =np.zeros_like(corr, dtype=np.bool)\nmask[np.tril_indices_from(mask)]=True\n#setuop the matplotlab figure\nf,ax=plt.subplots(figsize=(10,10))\n#generate a custom diverging colormap\ncmap=sn.diverging_palette(220, 10, s=75, l=50, sep=10, n=6, center='light', as_cmap=True)\n#heatmap\nsn.heatmap(corr, vmin=None, vmax=None, cmap=cmap, center=0, robust=False, fmt='.2g', linewidths=0, linecolor='white', square=True, mask=mask, ax=None)","9e6d78db":"#Clearly, from above heatmap, we can se that the dataset has multicolinearity. 'temp' and 'atemp' are highly correlated.\n#Will need to drop one of them.","10c2f919":"#Visualize the relationship among all continous variables using pairplots\nNumericFeatureList=[\"temp\",\"atemp\",\"humidity\",\"windspeed\"]\nsn.pairplot(bike,hue = 'yr',vars=NumericFeatureList)","829e582a":"#Lets explore some more, the relationship btwn independent continous variables and dependent variable using JOINT PLOTs\n#graph individual numeric features by count of rented bikes\nfor i in NumericFeatureList:\n    sn.jointplot(i, \"count\", data=bike, kind='reg', color='g', size=4, ratio=2, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None)","73829c33":"# Check the distribution plot of target variable 'count'\nsn.distplot(bike[\"count\"],color ='r')","5be22b13":"# what we can infer from above analysis of continous variables:\n# -> Target variable 'cnt' is almost normally distributed, which is a good thing.\n# -> From correlation with dependent variable cnt, we can see that 'casual','registered' are very highly correlated to cnt. These are actually 'leak variablles'. Needs to be dropped from the dataset\n# -> 'hum' has low correlation with 'cnt'. For ow, lets keep it.\n# -> atemp and temp has good correlation with 'cnt'\n# -> From heatmap, we can see that atemp and temp are highly correlated. So we need to drop 1 to remove multicollinearity.\n# -> Since, as seen from jointplot,p(atemp) < p(temp), we can drop 'temp' and retain 'atemp' in the dataset","f4e779e7":"#------------- Explore categorical features ------------------","54f5b726":"##checking the pie chart distribution of categorical variables\n#bike_piplot = bike.drop(columns=['instant','dteday','temp','atemp','hum','windspeed','casual','registered','cnt'])\nbike_piplot=bike[categoryFeatureList]\nplt.figure(figsize=(15,12))\nplt.suptitle('pie distribution of categorical features', fontsize=20)\nfor i in range(1,bike_piplot.shape[1]+1):\n    plt.subplot(3,3,i)\n    f=plt.gca()\n    f.set_title(bike_piplot.columns.values[i-1])\n    values=bike_piplot.iloc[:,i-1].value_counts(normalize=True).values\n    index=bike_piplot.iloc[:,i-1].value_counts(normalize=True).index\n    plt.pie(values,labels=index,autopct='%1.1f%%')\n#plt.tight_layout()","363f1629":"#What we can infer from above piplot:\n#-> Most of the categorical variables are uniformally distributed, except 'holiday','weathersit','workingday'\n#-> This makes sense for 'weathersit', as extreme weather is rare and hence %percentage of extreme weather in whole dataset is low\n#-> This makes sense for 'holiday', as number of holidays are less in comparison to working days\n#-> This makes sense for 'workingday' for the same reason as above\n#-> So, categorical data seems o be pretty much uniformly distributed","31878da2":"#graph individual categorical features by count\nfig, saxis = plt.subplots(3, 3,figsize=(16,12))\n\nsn.barplot(x = 'season', y = 'count',hue= 'yr', data=bike, ax = saxis[0,0], palette =\"Blues_d\")\nsn.barplot(x = 'yr', y = 'count', order=[0,1,2,3], data=bike, ax = saxis[0,1], palette =\"Blues_d\")\nsn.barplot(x = 'mnth', y = 'count', data=bike, ax = saxis[0,2])\nsn.barplot(x = 'holiday', y = 'count',  data=bike, ax = saxis[1,0])\nsn.barplot(x = 'weekday', y = 'count',  data=bike, ax = saxis[1,1])\nsn.barplot(x = 'workingday', y = 'count', data=bike, ax = saxis[1,2])\nsn.barplot(x = 'weather', y = 'count', data=bike, ax = saxis[2,0])\nsn.barplot(x = 'date', y = 'count' , data=bike, ax = saxis[2,1])\n#sn.pointplot(x = 'weathersit', y = 'cnt', data=bike, ax = saxis[2,0])\nsn.pointplot(x='date', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[2,2])\n#sn.pointplot()","294441db":"#--- Lets see how these categorical variables individually ffects the count of rented bikes\n# Does 'yr' affects count of rented bikes\n#--> YES. the count have an upward trend wrt year\n\n#Does 'season' affects count of rented bikes\n#--> YES, it seems ppl rent more bikes during season 3 and 2, i.e. highest in fall and summer and less in winter and springs. This makes sense as weather is good to ride during summer and fall.\n\n#Does 'month' affects count of rented bikes\n#-->YES.ppl are likely to rent bikes more btwn the months May- October and lowest in month of Jan,Feb and Dec(in that order). This again makes sense, as this trend is in sync with favourable weather conditions\n\n#Does 'holiday' affects count of rented bikes\n#--> YES. ppl rent more bikes on non-holiday than holiday. It makes sense as bikers who commute to work\/school will be less on holiday.\n\n#Does 'weekday' affects count of rented bikes\n#--> To some extent Yes. ppl seems to rent lesser bikes on Sat\/ Sun. ie. over the weekend. Again makes sense as school and offices are closed on weekend.\n#Monday also has lesser count of rented bikes. It may be possible the ppl visit to other places\/cities over weekend and travel back in car on Monday, istead of renting bikes.\n\n#Does 'weather' affects count of rented bikes\n#--> Most definately YES. noone rented bike on extreme weather(season=4). ppl rent maximum bikes during a clear day (weathersit=1)\n\n#Does 'date' affects count of rented bikes\n#--> Well there is no set trends. It seems to be random. Let explore bit more of it over the 12 months using pointplot\n#-->","126ac668":"#-- exploring some more pairplots, to see the trends over the years\nfig, saxis = plt.subplots(2, 2,figsize=(16,12))\nsn.pointplot(x='season', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,0])\nsn.pointplot(x='holiday', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,1])\nsn.pointplot(x='weekday', y='count', hue='mnth', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,0])\nsn.pointplot(x='workingday', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,1])","1b964483":"#------ Exploratory Data Analysis ENDS Here------------------\n# Final observations:\n#1.) 'casual' and 'registered' are leak variables. They need to be dropped from the dataset\n#2.) 'atemp' and 'temp' are very strongly correlated . Drop 'atemp' from the dataset(since it has higher p-value than 'temp')\n#3.) 'date' does not seem to have any affect on count of bikes, it can be dropped from the dataset\n#------------------------------------------------------------","bddb8e55":"#---- Drop the features mentioned above(as part of feature engineering)\ntrain = bike.drop(columns=['temp','casual','registered'])\ntest = bike_test.drop(columns=['temp'])\ntrain.head()","a396d370":"#----------Part 5 : Model Builing starts here ----------------------\n#Train the models with both datasets(before and after feature engineering)\n#Note: Just to show how feature engineering improves the result, I am going to train and test 1st model(linear regression model) with both 'before feature engineering' and 'after feature engineering' data and compare the results\n# For subsequent models,I'll only use the dataset with feature engineering implemented","fb1c85cc":"# 1.) I am selecting 3 models to test and evaluate\n #   -> Linear Regression Model\n #   -> Random Forrest (ensemble method using bagging technique)\n #   -> Gradient Boosting (ensemble method using boosting technique)\n#2.) Cross validation    \n#3.) All these 3 models will be compared and evaluated(with and without feature engineering)\n#4.) We'll choose the best out of 3","4e61f455":"#----- 5a.) -- Selecting train and test datasets for cross validations\n#split train data in to test and train(after featr engineering)\n#train, test = train_test_split(bike_aftr_ftr_eng, test_size=0.20, random_state = 5)\n\ntrain_data = train[:80]\ntest_data = train[20:]\nX_train = train_data.drop(columns=['count'])\nY_train = train_data['count']\nX_test = test_data.drop(columns=['count'])\nY_test = test_data['count']\n\n#--- *AFT <=> After Feature Engineering------","e38382aa":"#------- 5b.) Define a dataframe to store performance metrices of the models ","08343f8e":"#--- define a function which takes model, predicted and test values and returns evalution matrix: R-squared value,RootMeanSquared,MeanAbsoluteError\ndef model_eval_matrix(model,X_test,Y_test,Y_predict):\n    r_squared = model.score(X_test, Y_test)\n    mse = mean_squared_error(Y_predict, Y_test)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(Y_predict, Y_test)\n    return r_squared,mse,rmse, mae","6126cc49":"#-------- 5c.) Define and fit models ---------------","a061d308":"#--Define Linear regession model --\nlrm_regressor = LinearRegression()\nlrm_regressor.fit(X_train, Y_train)\nY_predict_lrm =lrm_regressor.predict(X_test)","0843e8ba":"#------- Random Forest Model (Ensemble method using Bagging technique) --------------\nforest_reg = RandomForestRegressor(random_state=1)\nforest_reg.fit(X_train, Y_train)\nY_predict_forest =forest_reg.predict(X_test)","3373e711":"## ----------- Building XGBoost Model (Ensemble method using Boosting technique) ---------------\n#xgb_reg = GradientBoostingRegressor(random_state=1) # without parameter hypertuning\n# Following model is with parameter hypertuning\nxgb_reg = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=300, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=1, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=100, warm_start=False, presort='auto')\nxgb_reg.fit(X_train, Y_train)\nY_predict_xgb = xgb_reg.predict(X_test)\n","9a671070":"#-------Part 5 ENDS here ------------------------------------------------","1dc41b8a":"#-------Part 6 : Model comparisions STARTS here---------------------------","cb33af2c":"#---Stroring all model performances in dataframe to compare----\nmetric=[]\nml_models=['Linear Regression','Random Forest','Gradient Boosting']\nfitted_models= [lrm_regressor,forest_reg,xgb_reg]\nY_Predict =[Y_predict_lrm,Y_predict_forest,Y_predict_xgb]\ni=0\nfor mod in ml_models:\n    R_SQR,MSE,RMSE,MAE = model_eval_matrix(fitted_models[i],X_test,Y_test,Y_Predict[i])\n    metric.append([mod,R_SQR,MSE,RMSE,MAE])\n    i=i+1\ndf_mod_performance=pd.DataFrame(metric,columns =['Model','R-Squared','MeanSquaredError','RootMeanSquaredError','MeanAbsoluteError'])","ee11c7f5":"df_mod_performance","bae71b38":"#------ Comparing the performance matrix values of the models-----\n#fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n#a=sn.pointplot(y='Model', x='R-Squared', rotate =90,data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,0])\n#a.set_xticklabels(a.get_xticklabels(), rotation=45)\n#sn.pointplot(y='Model', x='MeanSquaredError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,1])\n#sn.pointplot(y='Model', x='RootMeanSquaredError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,0])\n#sn.pointplot(y='Model', x='MeanAbsoluteError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,1])\n#plt.tight_layout()","bd2a5e64":"#What can be inferred from above observations:\n#-->It is evidently clear that gradient boost gives the best performance out of all the models\n#-->Hence we'll consider Gradient Boosting as our final model","7e4978a0":"#---------Part 6 : Model comparisions ENDS here ---------------------","7a709650":"#---------Part 7 : Hypertune the selected model starts here ------------\n\n#Now, Gradient Boosting is the final model, parameter hypertuning can be performed on the model to find the best parameters which will give the maximum performance.\n#Functions like GRIDSearchCV from GridSearch library of python can be used for this.\n\n#However, I tried here simple approach of \u2018hit and trial\u2019, where I changed parameter few times and found a set which gave me maximum performance.\n\n#Before parameter tuning:\n#-----> Gradient Boosting\n#-----> R-Squared :0.897838\n#-----> MSE: 387939.616482\n#-----> RMSE: 622.847988\n#-----> MAE: 460.576495\n\n#Before parameter tuning:\n#-----> Gradient Boosting\n#-----> R-Squared :0.913779\n#-----> MSE: 327408.191428\n#-----> RMSE: 572.195938\n#-----> MAE: 415.264316\n\n#Evident here, hypertuning the parameter boosted the model performance. So, we lock the parameters as below:\n#-->loss='ls',\n#-->learning_rate=0.1, \n#-->n_estimators=300, \n#-->subsample=1.0, \n#-->criterion='friedman_mse', \n#-->min_samples_split=2, \n#-->min_samples_leaf=1, \n#-->min_weight_fraction_leaf=0.0, \n#-->max_depth=3, \n#-->min_impurity_decrease=0.0, \n#-->min_impurity_split=None, \n#-->init=None, \n#-->random_state=1, \n#-->max_features=None, \n#-->alpha=0.9, \n#-->verbose=0, \n#-->max_leaf_nodes=100, \n#-->warm_start=False, \n#-->presort='auto'\n\n#Lets produce the output using this model\n\n#---------Part 7 : Hypertune the selected model ENDS here ------------","cff56d88":"#--------Part 8 : Produce sample output with tuned model STARTS here----------------------\n","10388420":"Y_predict_xgb_final = xgb_reg.predict(test)\nfinal_bike_prediction_df=test\n#final_bike_prediction_df['ActualCount'] = Y_test\nfinal_bike_prediction_df['PredictedCount'] = Y_predict_xgb_final\nfinal_bike_prediction_df['PredictedCount'] = round(final_bike_prediction_df['PredictedCount'])\n#--- Sample output(with actual counts and predicted counts) ---\n#final_bike_prediction_df\nfinal_bike_prediction_df.head()","0fa9f66f":"#-----Plotting the distributions of 'ActualCount' and 'PredictedCount'\n#fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n#sn.distplot(final_bike_prediction_df[\"ActualCount\"],color ='r', ax = saxis[0,0])\n#sn.distplot(final_bike_prediction_df[\"PredictedCount\"],color ='g',ax = saxis[0,1])\n\n#--- As clearly evident from the below charts the distributions of both the counts are very similar.\n#--This seems a fair model","52066026":"### I am going to divide whole project in to 8 parts:\n#### 1.) Define and categorize problem statement\n#### 2.) Gather the data\n#### 3.) Prepare data for consumption\n#### 4.) Perform Exploratory Data Analysis\n#### 5.) Models Building\n#### 6.) Evaluate and compare Model performances and choose the best model\n#### 7.) Hypertune the selected model\n#### 8.) Produce sample output with tuned model","54ba7f50":"# This python code is for project : Bike Renting \n#### The objective of this Case is Predication of bike rental count on daily based on the environmental and seasonal settings\n#### This is more to demonstarte all the steps involved in any machine learning project\n#### For the sake of simplicity, I am using day.csv only for the analysis."}}