{"cell_type":{"a1db2376":"code","a264ceab":"code","cca3581d":"code","eec28b3d":"code","eea95f49":"code","bd379855":"code","f5b4f289":"code","e978f74c":"code","7a41c1bc":"code","68533a7a":"code","1c15d5e9":"code","1d26a3da":"code","8cf766a5":"code","ab863552":"code","605e428c":"code","07173826":"code","c0c86309":"code","795ffd7b":"code","22f90905":"code","efa7e2bb":"code","e3d7b9bb":"code","fb1a8ac6":"code","163091f8":"code","05d7dfcd":"code","e25c4cf5":"code","72e10b25":"code","f321c7a4":"code","9c67babf":"code","ad907a5c":"code","525dfc98":"code","b562eb08":"code","2fc8553a":"code","34de0af5":"code","84e8970b":"code","3df53246":"code","7a6ccd7f":"code","cda14b4f":"code","b2022fc2":"code","16effeb5":"code","071f7203":"code","2567542a":"code","ea83dd35":"code","82bfe767":"code","28fe9096":"markdown","cd0f6189":"markdown","cdd35539":"markdown","dc9efafa":"markdown","cc99550a":"markdown","2e696dd5":"markdown","2d307937":"markdown","9fc8ed2e":"markdown","4399a96f":"markdown","c00087eb":"markdown","f7452e96":"markdown","556ff4dc":"markdown","6a0c120e":"markdown","beb1b409":"markdown","8b8b39e9":"markdown","2acfca0e":"markdown","813bd7b5":"markdown","cd27ca41":"markdown","0f99d12a":"markdown","a3ed95a2":"markdown","8193fb90":"markdown","51697154":"markdown"},"source":{"a1db2376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a264ceab":"import warnings\nwarnings.filterwarnings('ignore')","cca3581d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, auc, classification_report, roc_curve, roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport statsmodels\n\n%matplotlib inline","eec28b3d":"df = pd.read_csv('..\/input\/star-type-classification\/Stars.csv')","eea95f49":"df","bd379855":"df.info()","f5b4f289":"df.describe()","e978f74c":"g = sns.heatmap(df[['Type', 'Temperature', 'L', 'R', 'A_M']].corr(), annot=True, cmap='coolwarm')","7a41c1bc":"y = df.Type\n\n#Break off test set from training data\ndftrain, dftest, ytrain, ytest = train_test_split(df, y,\n                                                 train_size=0.8, test_size=0.2, \n                                                 random_state=0)","68533a7a":"def comparing (dftrain, variable1, variable2):\n    print(dftrain[[variable1, variable2]][dftrain[variable2].isnull()==False].\n         groupby([variable1], as_index=False). mean().sort_values(by=variable2, ascending=False))\n\n    g=sns.FacetGrid(dftrain, col=variable2).map(sns.distplot,variable1)","1c15d5e9":"def counting_values(dftrain, variable1, variable2):\n    return dftrain[[variable1, variable2]][dftrain[variable2].isnull()==False].groupby([variable1], as_index=False).mean().sort_values(by=variable2, ascending=False)","1d26a3da":"plt.figure(figsize=(16,7))\ncomparing(dftrain,'Temperature','Type')","8cf766a5":"fig,axs = plt.subplots(figsize=(30,5))\nsns.histplot(data=dftrain, x='Temperature').set_title('Temperature Distribution', fontdict={'fontsize':24, 'fontweight':'bold'});\nsns.despine()","ab863552":"LRange = dftrain.groupby(['Type']).L.agg([len, min, max])\nLRange","605e428c":"plt.figure(figsize=(20,7))\ncomparing(dftrain, 'L', 'Type')","07173826":"fig,axs = plt.subplots(figsize=(30,5))\nsns.histplot(data=dftrain, x='L').set_title('L Distribution', fontdict={'fontsize':24, 'fontweight':'bold'});\nsns.despine()","c0c86309":"comparing(dftrain, 'R', 'Type')","795ffd7b":"fig,axs = plt.subplots(figsize=(30,5))\nsns.histplot(data=dftrain, x='R').set_title('R Distribution', fontdict={'fontsize':24, 'fontweight':'bold'});\nsns.despine()","22f90905":"comparing(dftrain, 'A_M', 'Type')","efa7e2bb":"fig,axs = plt.subplots(figsize=(30,5))\nsns.histplot(data=dftrain, x='A_M').set_title('A_M Distribution', fontdict={'fontsize':24, 'fontweight':'bold'});\nsns.despine()","e3d7b9bb":"dftrain.Color.value_counts()","fb1a8ac6":"counting_values(dftrain, 'Color', 'Type')","163091f8":"# Categorize colors with the same name\ndftrain.Color = dftrain.Color.replace(['Blue white', 'Blue-white', 'Blue-White'], 'Blue White')\ndftrain.Color = dftrain.Color.replace(['Whitish', 'white', 'Yellowish White', 'White-Yellow'], 'White')\ndftrain.Color = dftrain.Color.replace(['yellow-white', 'yellowish', 'Pale yellow orange', 'Orange-Red'], 'Yellowish')\ndftrain.Color.unique()","05d7dfcd":"dftrain.Color.value_counts()","e25c4cf5":"g = pd.DataFrame(dftrain['Color'].value_counts())\nplt.figure(figsize=(10,7))\nsns.barplot(g['Color'], g.index, palette='Set2')\nplt.title('Star Color Analysis')","72e10b25":"g1 = pd.DataFrame(dftrain['Spectral_Class'].value_counts())\nplt.figure(figsize=(10,7))\nsns.barplot(g1['Spectral_Class'], g1.index, palette='Spectral')\nplt.title('Spectral Class Analysis')","f321c7a4":"dftrain.set_index('Spectral_Class')","9c67babf":"g2 = pd.DataFrame(dftrain['Type'].value_counts())\nplt.figure(figsize=(10,7))\nplt.pie(g2['Type'], labels=g2.index, autopct='%1.1f%%')\nplt.title('Percent Distribution of the Star type')","ad907a5c":"my_features = ['Temperature', 'L','R', 'Color', 'A_M', 'Spectral_Class']","525dfc98":"df_train = dftrain[my_features].copy()\ndf_test = dftest[my_features].copy()","b562eb08":"dftrain.drop(['Type'], axis=1, inplace=True)\n\n# Select categorical columns\ncategorical_cols = [cname for cname in df_train.columns if\n                   df_train[cname].nunique() <10 and\n                   df_train[cname].dtype == 'object']\n\n# Select numerical columns\nnumerical_cols = [cname for cname in df_train.columns if\n                 df_train[cname].dtype in ['int64', 'float64']]","2fc8553a":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n#preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n    \n# Feature SAcaling\nsc = StandardScaler()\n    \npreprocessor = ColumnTransformer(\n      transformers=[\n          ('num', numerical_transformer, numerical_cols),\n          ('cat', categorical_transformer, categorical_cols)\n      ])   \n","34de0af5":"Results = pd.DataFrame({'Model': [], 'Accuracy Score': []})","84e8970b":"# Define a model\nlrc = LogisticRegression()\n\n# Bundle preprocessing and modeling code in a pipeline\nlr = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('sc', StandardScaler()),\n                    ('model', lrc\n)])\n\n# Preprocessing of training data, fit model\nlr.fit(df_train, ytrain)\n\n# Get predictions\npredsLR = lr.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predsLR))\nprint('CR:', classification_report(ytest, predsLR))\nprint('CM:', confusion_matrix(ytest, predsLR))\n\nrest = pd.DataFrame({'Model': ['LogisticRegression'],\n                    'Accuracy Score': [accuracy_score(ytest, predsLR)]})\n\nResults = Results.append(rest)","3df53246":"# Define a model\ndtc = DecisionTreeClassifier()\n\n# Bundle preprocessing and modeling code in a pipeline\ndt = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('sc', StandardScaler()),\n                    ('model', dtc\n)])\n\n# Preprocessing of training data, fit model\ndt.fit(df_train, ytrain)\n\n# Get predictions\npredsDT = dt.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predsDT))\nprint('CR:', classification_report(ytest, predsDT))\nprint('CM:', confusion_matrix(ytest, predsDT))\n\nrest = pd.DataFrame({'Model': ['DecisionTree'],\n                    'Accuracy Score': [accuracy_score(ytest, predsDT)]})\n\nResults = Results.append(rest)","7a6ccd7f":"# Define a model\nrfc = RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy')\n\n# Bundle preprocessing and modeling code in a pipeline\nrf = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('sc', StandardScaler()),\n                    ('model', rfc\n)])\n\n# Preprocessing of training data, fit model\nrf.fit(df_train, ytrain)\n\n# Get predictions\npredsRF = rf.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predsRF))\nprint('CR:', classification_report(ytest, predsRF))\nprint('CM:', confusion_matrix(ytest, predsRF))\n\nrest = pd.DataFrame({'Model': ['RandomForest'],\n                    'Accuracy Score': [accuracy_score(ytest, predsRF)]})\n\nResults = Results.append(rest)","cda14b4f":"# Define a model\nknnc = KNeighborsClassifier()\n\n# Bundle preprocessing and modeling code in a pipeline\nknn = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('sc', StandardScaler()),\n                    ('model', knnc\n)])\n\n# Preprocessing of training data, fit model\nknn.fit(df_train, ytrain)\n\n# Get predictions\npredsKNN = knn.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predsKNN))\nprint('CR:', classification_report(ytest, predsKNN))\nprint('CM:', confusion_matrix(ytest, predsKNN))\n\nrest = pd.DataFrame({'Model': ['KNeighborsClassifier'],\n                    'Accuracy Score': [accuracy_score(ytest, predsKNN)]})\n\nResults = Results.append(rest)","b2022fc2":"# Define a model\nsv = SVC()\n\n# Bundle preprocessing and modeling code in a pipeline\nsvc = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('sc', StandardScaler()),\n                    ('model', sv\n)])\n\n# Preprocessing of training data, fit model\nsvc.fit(df_train, ytrain)\n\n# Get predictions\npredssvc = svc.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predssvc))\nprint('CR:', classification_report(ytest, predssvc))\nprint('CM:', confusion_matrix(ytest, predssvc))\n\nrest = pd.DataFrame({'Model': ['SVM'],\n                    'Accuracy Score': [accuracy_score(ytest, predssvc)]})\n\nResults = Results.append(rest)","16effeb5":"# Define a model\nxgbc = XGBClassifier(n_estimators = 100, learning_rate = 0.1, max_depth = 2, min_child_weight = 1, nthread=4, seed=27, subsample=0.8, colsample_bytree=0.9, max_delta_step=0,\n                    objective='multi:softmax', gamma =0, reg_alpha=0.001, reg_lambda =0.5, eval_metric ='auc', random_state=0, num_class =6)\n\n\n# Bundle preprocessing and modeling code in a pipeline\nxgb = Pipeline(steps=[('preprocessor', preprocessor),\n                    ('model', xgbc\n)])\n\n# Preprocessing of training data, fit model\nxgb.fit(df_train, ytrain)\n\n# Get predictions\npredsxgb = xgb.predict(df_test)\n\nprint('Accuracy:', accuracy_score(ytest, predsxgb))\nprint('CR:', classification_report(ytest, predsxgb))\nprint('CM:', confusion_matrix(ytest, predsxgb))\n\nrest = pd.DataFrame({'Model': ['XGB'],\n                    'Accuracy Score': [accuracy_score(ytest, predsxgb)]})\n\nResults = Results.append(rest)","071f7203":"Results","2567542a":"rfc.feature_importances_\n","ea83dd35":"dtc.feature_importances_","82bfe767":"xgbc.feature_importances_","28fe9096":"## DecisionTree","cd0f6189":"### A_M (Absolute Magnitude) vs Type","cdd35539":"## KNN","dc9efafa":"### Temperature vs Type","cc99550a":"### Luminosity Ratio(L) vs Type","2e696dd5":"## Logistic Regression","2d307937":"## XGB","9fc8ed2e":"### R (Solar Radius) vs Type","4399a96f":"# Model Selection","c00087eb":"### Spectral Class vs Type","f7452e96":"### Color vs Type","556ff4dc":"## RandomForest","6a0c120e":"# Results","beb1b409":"**The best models are RandomForest and XGB**","8b8b39e9":"# Explore the data","2acfca0e":"# Import the Libraries","813bd7b5":"### Preprocessing","cd27ca41":"L, R and Temperature are highly correlated with the type of the star. Correlation of A_M with type of the star is very low. ","0f99d12a":"There's no pattern or clear range of L.","a3ed95a2":"## SVM","8193fb90":"# Import Data","51697154":"### Percent Distribution of Star Type"}}