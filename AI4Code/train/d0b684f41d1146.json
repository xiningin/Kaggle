{"cell_type":{"f83799c7":"code","05b1100d":"code","105bf00b":"code","9f0ac74b":"code","1a51df56":"code","86c996ad":"code","c0a4c212":"code","2455ab87":"code","b4960c7b":"code","e7444a4b":"code","1280ddff":"code","b60b691b":"code","9b61cb42":"code","60049703":"code","e8ddfd23":"code","9d4b9a6d":"code","d1000c3c":"code","5b9e11ab":"code","f7e5152d":"code","74eb5d2a":"code","82495ae3":"code","67ec290f":"code","cd65f4af":"code","80d21691":"code","1f886433":"code","7ba4e32c":"code","3599297a":"code","9f57aced":"code","9f4bae51":"code","811d7359":"code","8adc582f":"code","b38b299b":"markdown","90b0607d":"markdown","26f4df69":"markdown","681354c8":"markdown","342531f2":"markdown","d777362f":"markdown","bc5f7060":"markdown"},"source":{"f83799c7":"import pandas as pd\nimport sklearn.model_selection\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","05b1100d":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata.shape","105bf00b":"data.isna().sum()","9f0ac74b":"data.hist(figsize=(10,10))","1a51df56":"data.describe()","86c996ad":"data['Sex_n'] = data['Sex'].apply(lambda sex: 0 if sex == 'female' else 1)","c0a4c212":"data.info()","2455ab87":"data.corr()","b4960c7b":"train, test = sklearn.model_selection.train_test_split(data, test_size=0.2)\n\n# impute age based on nearest based on Pclass and SibSp since those have highest correlation to age\nimputer = KNNImputer()\ntrain.insert(0, \"Age_imputed\", imputer.fit_transform(train[['Age','Pclass','SibSp']])[:,0])\n","e7444a4b":"train","1280ddff":"train['Fare'].hist(bins=30)","b60b691b":"train.columns","9b61cb42":"#mostly\ncategorical_columns = ['Survived','Pclass','Sex','SibSp','Parch','Embarked']\nfor column in categorical_columns:\n    print(column)\n    print(train[column].value_counts())","60049703":"train.dtypes","e8ddfd23":"train.corr()","9d4b9a6d":"train_columns = ['Pclass', 'Age_imputed', 'SibSp',\n       'Parch', 'Fare', 'Sex_n']\nX = train[train_columns]\ny = train['Survived']","d1000c3c":"X.shape, y.shape","5b9e11ab":"scoring = ['accuracy', 'precision', 'recall']\nforest = RandomForestClassifier()\ncross_validate(forest, X, y, scoring=scoring, return_estimator=True)","f7e5152d":"forest.fit(X, y)","74eb5d2a":"forest.feature_importances_","82495ae3":"cross_val_score(forest, X, y).mean()","67ec290f":"X.columns","cd65f4af":"#tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n#                     'C': [1, 10, 100, 1000]},\n#                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n#clf = GridSearchCV(\n#    SVC(), tuned_parameters, scoring='accuracy'\n#)\n#clf.fit(X, y)\n#^^ takes ages to run. best result is {'C': 10, 'kernel': 'linear'}\nsvc = SVC(C=10, kernel=\"linear\")\nsvc.fit(X, y)\ncross_val_score(svc, X, y).mean()","80d21691":"boosting_params = [{'learning_rate':[0.1,0.03,0.01,0.003],\n                    'n_estimators':[150,300,500,700,1000],\n                    'subsample':[1.0,0.5,0.2,0.05],\n                    'max_features':['auto', 'sqrt', 'log2',None]\n                   }]\nboosting_search = GridSearchCV(\n    GradientBoostingClassifier(), boosting_params, scoring='accuracy'\n)\nboosting_search.fit(X, y)","1f886433":"cross_val_score(boosting_search.best_estimator_, X, y).mean()","7ba4e32c":"boosting_search.best_params_","3599297a":"boosting_search.best_estimator_.score(X, y)","9f57aced":"test.insert(0, \"Age_imputed\", imputer.transform(test[['Age','Pclass','SibSp']])[:,0])","9f4bae51":"test_X = test[train_columns]\ntest_y = test['Survived']\nboosting_search.best_estimator_.score(test_X, test_y)","811d7359":"def build_age_imputer(train):\n    imputer = KNNImputer()\n    imputer.fit(train[['Age','Pclass','SibSp']])\n    return imputer\n\ndef build_fare_imputer(train):\n    imputer = KNNImputer()\n    imputer.fit(train[['Fare','Pclass','Parch']])\n    return imputer\n\ndef prepare(df, age_imputer, fare_imputer):\n    copy_columns = ['Pclass', 'SibSp', 'Parch']\n    prepared = pd.DataFrame()\n    prepared['Sex_n'] = df['Sex'].apply(lambda sex: 0 if sex == 'female' else 1)\n    prepared.insert(0, \"Age_imputed\", age_imputer.transform(df[['Age','Pclass','SibSp']])[:,0])\n    prepared.insert(0, \"Fare_imputed\", fare_imputer.transform(df[['Fare','Pclass','Parch']])[:,0])\n    prepared[copy_columns] = df[copy_columns]\n    return prepared\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nage_imputer = build_age_imputer(train_data)\nfare_imputer = build_fare_imputer(train_data)\nprepared_train = prepare(train_data, age_imputer, fare_imputer)\nfinal_model = GradientBoostingClassifier(learning_rate=0.03,\n max_features='log2',\n n_estimators=300,\n subsample=0.5)\nfinal_model.fit(prepared_train, train_data[\"Survived\"])\n#make sure I didn't mess anything up in obvious ways\n#print(cross_val_score(final_model, prepared_train, train_data[\"Survived\"]).mean())\n","8adc582f":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprepared_test = prepare(test_data, age_imputer, fare_imputer)\nresult = final_model.predict(prepared_test)\nsubmission = pd.DataFrame(result, index=test_data['PassengerId'], columns=['Survived'])\nsubmission.to_csv(\"submission.csv\")","b38b299b":"## bring it together and clean it up","90b0607d":"## train some models","26f4df69":"## Exploration","681354c8":"Nice! The train accuracy is way higher than the validation accuracy. That suggests to me there may be high variance. But I tried adding in various regularization parameters to the boosting_params search and while they helped, never pushed train acc close to val acc.\nBest one I've got, lets look at the test set to see if it generalizes, and if so, use this for the submission!","342531f2":"Just about the same. People are going nuts over gradient boosted trees, let's try one of those","d777362f":"Not bad at all. Maybe we can squeeze out a few more percentage points","bc5f7060":"### Modify some data, still exploring"}}