{"cell_type":{"7f5dc31e":"code","19464b0e":"code","cd6ea1ff":"code","dd36b84d":"code","73dbd8f2":"code","b413acd4":"code","42fa51a6":"code","b1b22e68":"code","ff06f209":"code","d4b4774e":"code","ebf6d9c8":"code","b7957641":"code","4e7bc502":"code","12ca36d6":"code","e94a0d14":"code","dc3d80bc":"code","751500b2":"code","da1681ee":"code","68fa4295":"code","27f61f13":"code","c9733f15":"code","9f2ae6f8":"code","2f30d5e4":"code","d4de479b":"code","a506c9bf":"code","dd72df4f":"code","4ba5862c":"code","cb5ddcea":"code","c37d6666":"code","9fb4d1a2":"code","072a53e0":"code","f34ec57f":"code","ae3ede5f":"code","54406bf4":"code","04fa99c1":"code","08a603d6":"code","239251dd":"code","1aaacc6a":"code","33e0454d":"code","a44db08b":"code","0f65a344":"code","9c1cd716":"code","e3614b69":"markdown","677625d2":"markdown","be4464e8":"markdown","ef40ac42":"markdown","c29c2d81":"markdown","5a525111":"markdown","ddc83aec":"markdown","10fa30bd":"markdown","02beff28":"markdown","845884fd":"markdown","2230f921":"markdown","243c843a":"markdown","7b1e48bc":"markdown","b76c4951":"markdown","ec7d4847":"markdown"},"source":{"7f5dc31e":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","19464b0e":"data = pd.read_csv(\"..\/input\/ireland-historical-news\/irishtimes-date-text.csv\")","cd6ea1ff":"data.shape","dd36b84d":"data.isna().sum()","73dbd8f2":"data.duplicated().sum()","b413acd4":"data.head()","42fa51a6":"data.drop_duplicates(inplace=True) ","b1b22e68":"data.shape","ff06f209":"year = [] \nmonth = [] \nday = [] \n\ndates = data.publish_date.values\n\nfor date in dates:\n    str_date = list(str(date))\n    year.append(int(\"\".join(str_date[0:4]))) \n    month.append(int(\"\".join(str_date[4:6])))\n    day.append(int(\"\".join(str_date[6:8])))","d4b4774e":"data['year'] = year\ndata['month'] = month\ndata['day'] = day\n\ndata.drop(['publish_date'] , axis=1,inplace=True) ","ebf6d9c8":"data.head()","b7957641":"print('Unique Headlines Categories: {}'.format(len(data.headline_category.unique())))","4e7bc502":"set([category for category in data.headline_category if \".\" not in category] ) ","12ca36d6":"data.headline_category = data.headline_category.apply(lambda x: x.split(\".\")[0]) ","e94a0d14":"plt.figure(figsize=(10,5))\nax = sns.countplot(data.headline_category) ","dc3d80bc":"from nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\nimport regex\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\nstop = stopwords.words('english')\n\nfor punct in punctuation:\n    stop.append(punct)\n\ndef filter_text(text, stop_words):\n    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha()]\n    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n    return \" \".join(filtered_text)","751500b2":"data[\"filtered_text\"] = data.headline_text.apply(lambda x : filter_text(x, stop)) ","da1681ee":"data.head()","68fa4295":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=data.year.value_counts().index.values,y=data.year.value_counts().values)\nax = plt.title('Number of Published News by Year')","27f61f13":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=data.month.value_counts().index.values,y=data.month.value_counts().values)\nax = plt.title('Number of Published News by Month')","c9733f15":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=data.day.value_counts().index.values,y=data.day.value_counts().values)\nax = plt.title('Number of Published News by Day')","9f2ae6f8":"from wordcloud import WordCloud\n\ndef make_wordcloud(words,title):\n    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n    plt.figure(figsize=(20,20))\n    plt.imshow(cloud, interpolation=\"gaussian\")\n    plt.axis(\"off\") \n    plt.title(title, fontsize=60)\n    plt.show()","2f30d5e4":"all_text = \" \".join(data[data.headline_category == \"news\"].filtered_text) \nmake_wordcloud(all_text, \"News\") ","d4de479b":"all_text = \" \".join(data[data.headline_category == \"culture\"].filtered_text) \nmake_wordcloud(all_text, \"Culture\")","a506c9bf":"all_text = \" \".join(data[data.headline_category == \"opinion\"].filtered_text) \nmake_wordcloud(all_text, \"Opinion\")","dd72df4f":"all_text = \" \".join(data[data.headline_category == \"business\"].filtered_text) \nmake_wordcloud(all_text, \"Business\")","4ba5862c":"all_text = \" \".join(data[data.headline_category == \"sport\"].filtered_text) \nmake_wordcloud(all_text, \"Sport\")","cb5ddcea":"all_text = \" \".join(data[data.headline_category == \"lifestyle\"].filtered_text) \nmake_wordcloud(all_text, \"Lifestyle\")","c37d6666":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(lowercase=False)\nml_data = tfidf.fit_transform(data['filtered_text'])","9fb4d1a2":"ml_data.shape","072a53e0":"data['classification'] = data['headline_category'].replace(['news','culture','opinion','business','sport','lifestyle'],[0,1,2,3,4,5])","f34ec57f":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(ml_data,data['classification'], stratify=data['classification'], test_size=0.2)","ae3ede5f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix\n\nmodel = LogisticRegression(solver='lbfgs',multi_class='auto', max_iter=1000)\nmodel.fit(x_train,y_train)","54406bf4":"predicted = model.predict(x_test)\nprint(\"Test score: {:.2f}\".format(accuracy_score(y_test,predicted)))\nprint(\"Cohen Kappa score: {:.2f}\".format(cohen_kappa_score(y_test,predicted)))\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_test,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","04fa99c1":"def get_most_important_words(model,index,category):\n    base = {'news':0,'culture':1,'opinion':2,'business':3,'sport':4,'lifestyle':5}\n    t=pd.DataFrame(model.coef_[base[category]].T, index=tfidf.get_feature_names()) \n    return pd.concat([t.nlargest(5,0),t.nsmallest(5,0)])","08a603d6":"index = tfidf.get_feature_names()","239251dd":"get_most_important_words(model,index,'news')","1aaacc6a":"get_most_important_words(model,index,'culture')","33e0454d":"get_most_important_words(model,index,'opinion')","a44db08b":"get_most_important_words(model,index,'business')","0f65a344":"get_most_important_words(model,index,'sport')","9c1cd716":"get_most_important_words(model,index,'lifestyle')","e3614b69":"# Predicting the Headlines Categories","677625d2":"Here are the final results:","be4464e8":"# Most Important Words for the classifier","ef40ac42":"In the next lines of code I separated the year, month and day into 3 other columns.","c29c2d81":"Im using the TFIDF Vectorizer to create the input data for the Machine Learning algorithm.","5a525111":"Im going to use the WordNetLemmatizer and stopwords with punctuation for filtering the text.","ddc83aec":"As we can see in the word clouds above, each category have very different words, that is very good for the next part which is text classification.","10fa30bd":"# Cleaning the Data","02beff28":"## Date analysis ","845884fd":"## Word Clouds","2230f921":"I chose to work with the Logistic Regression.","243c843a":"# Exploring the Data","7b1e48bc":"We can merge some headlines categories, let's use the most common ones. ","b76c4951":"# Checking the Dataset","ec7d4847":"### Context\nThis news dataset is a collection of 1.42 million news headlines published by The Irish Times based in Ireland.\n\nCreated over 159 Years ago the agency provides a long term birds eye view of the happenings of Europe.\n\nAgency Website: https:\/\/www.irishtimes.com\n\nThe historical reels can be explored thoroughly via the archives portal."}}