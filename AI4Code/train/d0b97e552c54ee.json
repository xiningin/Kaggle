{"cell_type":{"396b16aa":"code","addcde14":"code","352a1dcc":"code","8630e6f1":"code","49538b9b":"code","c58f181b":"code","a0520f7e":"code","e7ddabf0":"code","c2f36662":"code","392c3a07":"code","b015a259":"code","f8afcc0d":"code","b3929427":"code","c99ccbc2":"code","b4726430":"code","d6c126ca":"code","73de5e49":"code","c1ff79e9":"code","40e9e8fd":"code","8acdea98":"code","e61bbf5a":"code","2d47214b":"code","62625213":"code","a56055b3":"code","d0a6f351":"code","4515907f":"code","2d4e12e7":"code","16661b64":"code","d4efb959":"code","cd33947f":"code","8864e3b4":"code","2c9a598b":"code","5f610c3d":"code","4d6ecce7":"code","2dcec619":"code","8ce573c3":"code","360d4d41":"code","5a247ac4":"markdown","20bfb59f":"markdown","6b394c07":"markdown","4bcbc1fb":"markdown","6c6257bf":"markdown","f948af13":"markdown","2ab7c440":"markdown","0c05bb08":"markdown","e76325b1":"markdown","39e22098":"markdown","d7924bdd":"markdown","619624fc":"markdown"},"source":{"396b16aa":"!git clone https:\/\/github.com\/siddharthchaini\/gpvae-raw.git","addcde14":"!mv -v .\/gpvae-raw\/* .\/","352a1dcc":"!rm -rf gpvae-raw\n!rm gpvaeraw.ipynb","8630e6f1":"!wget https:\/\/www.dropbox.com\/s\/651d86winb4cy9n\/physionet.npz?dl=1 -O physionet.npz","49538b9b":"import sys\nimport os\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\n\ntf.compat.v1.enable_eager_execution()\n\nfrom sklearn.metrics import average_precision_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","c58f181b":"from lib.models import *","a0520f7e":"latent_dim = 35 # 'Dimensionality of the latent space'\nencoder_sizes = [128, 128] # 'Layer sizes of the encoder'\ndecoder_sizes = [256, 256] # 'Layer sizes of the decoder'\nwindow_size = 24 # 'Window size for the inference CNN: Ignored if model_type is not gp-vae'\nsigma = 1.005 # 'Sigma value for the GP prior: Ignored if model_type is not gp-vae'\nlength_scale = 7.0 # 'Length scale value for the GP prior: Ignored if model_type is not gp-vae'\nbeta = 0.2 # 'Factor to weigh the KL term (similar to beta-VAE'\nnum_epochs = 40 # 'Number of training epochs'\n\n# Flags with common default values for all three datasets\nlearning_rate = 1e-3 # 'Learning rate for training'\ngradient_clip = 1e4 # 'Maximum global gradient norm for the gradient clipping during training'\nnum_steps = 0 # 'Number of training steps: If non-zero it overwrites num_epochs'\nprint_interval = 0 # 'Interval for printing the loss and saving the model during training'\nexp_name = \"reproduce_physionet\" # 'Name of the experiment'\nbasedir = \"models\" # 'Directory where the models should be stored'\ndata_dir = \"\" # 'Directory from where the data should be read in'\ndata_type = 'physionet' # ['hmnist', 'physionet', 'sprites'], 'Type of data to be trained on'\nseed = 1337 # 'Seed for the random number generator'\nmodel_type = 'gp-vae' # ['vae', 'hi-vae', 'gp-vae'], 'Type of model to be trained'\ncnn_kernel_size = 3 # 'Kernel size for the CNN preprocessor'\ncnn_sizes = [256] # 'Number of filters for the layers of the CNN preprocessor'\ntesting = True # 'Use the actual test set for testing'\nbanded_covar = True # 'Use a banded covariance matrix instead of a diagonal one for the output of the inference network: Ignored if model_type is not gp-vae'\nbatch_size = 64 # 'Batch size for training'\n\nM = 1 # 'Number of samples for ELBO estimation'\nK = 1 # 'Number of importance sampling weights'\n\nkernel = 'cauchy' # ['rbf', 'diffusion', 'matern', 'cauchy'], 'Kernel to be used for the GP prior: Ignored if model_type is not (mgp-vae'\nkernel_scales = 1 # 'Number of different length scales sigma for the GP prior: Ignored if model_type is not gp-vae'","e7ddabf0":"np.random.seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(\"Testing: \", testing, f\"\\t Seed: {seed}\")","c2f36662":"encoder_sizes = [int(size) for size in encoder_sizes]\ndecoder_sizes = [int(size) for size in decoder_sizes]\n\nif 0 in encoder_sizes:\n    encoder_sizes.remove(0)\nif 0 in decoder_sizes:\n    decoder_sizes.remove(0)","392c3a07":"# Make up full exp name\ntimestamp = datetime.now().strftime(\"%y%m%d\")\nfull_exp_name = \"{}_{}\".format(timestamp, exp_name)\noutdir = os.path.join(basedir, full_exp_name)\nif not os.path.exists(outdir): os.mkdir(outdir)\ncheckpoint_prefix = os.path.join(outdir, \"ckpt\")\nprint(\"Full exp name: \", full_exp_name)","b015a259":"data_type","f8afcc0d":"if data_type == \"hmnist\":\n    data_dir = \"data\/hmnist\/hmnist_mnar.npz\"\n    data_dim = 784\n    time_length = 10\n    num_classes = 10\n    decoder = BernoulliDecoder\n    img_shape = (28, 28, 1)\n    val_split = 50000\nelif data_type == \"physionet\":\n    if data_dir == \"\":\n        data_dir = \"physionet.npz\"\n    data_dim = 35\n    time_length = 48\n    num_classes = 2\n    decoder = GaussianDecoder\nelif data_type == \"sprites\":\n    if data_dir == \"\":\n        data_dir = \"data\/sprites\/sprites.npz\"\n    data_dim = 12288\n    time_length = 8\n    decoder = GaussianDecoder\n    img_shape = (64, 64, 3)\n    val_split = 8000\nelse:\n    raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","b3929427":"data = np.load(data_dir)","c99ccbc2":"x_train_full = data['x_train_full']\nx_train_miss = data['x_train_miss']\nm_train_miss = data['m_train_miss']","b4726430":"if data_type in ['hmnist', 'physionet']:\n    y_train = data['y_train']","d6c126ca":"if testing:\n    if data_type in ['hmnist', 'sprites']:\n        x_val_full = data['x_test_full']\n        x_val_miss = data['x_test_miss']\n        m_val_miss = data['m_test_miss']\n    if data_type == 'hmnist':\n        y_val = data['y_test']\n    elif data_type == 'physionet':\n        x_val_full = data['x_train_full']\n        x_val_miss = data['x_train_miss']\n        m_val_miss = data['m_train_miss']\n        y_val = data['y_train']\n        m_val_artificial = data[\"m_train_artificial\"]\nelif data_type in ['hmnist', 'sprites']:\n    x_val_full = x_train_full[val_split:]\n    x_val_miss = x_train_miss[val_split:]\n    m_val_miss = m_train_miss[val_split:]\n    if data_type == 'hmnist':\n        y_val = y_train[val_split:]\n    x_train_full = x_train_full[:val_split]\n    x_train_miss = x_train_miss[:val_split]\n    m_train_miss = m_train_miss[:val_split]\n    y_train = y_train[:val_split]\nelif data_type == 'physionet':\n    x_val_full = data[\"x_val_full\"]  # full for artificial missings\n    x_val_miss = data[\"x_val_miss\"]\n    m_val_miss = data[\"m_val_miss\"]\n    m_val_artificial = data[\"m_val_artificial\"]\n    y_val = data[\"y_val\"]\nelse:\n    raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","73de5e49":"tf_x_train_miss = tf.data.Dataset.from_tensor_slices((x_train_miss, m_train_miss))\\\n                                 .shuffle(len(x_train_miss)).batch(batch_size).repeat()\ntf_x_val_miss = tf.data.Dataset.from_tensor_slices((x_val_miss, m_val_miss)).batch(batch_size).repeat()\ntf_x_val_miss = tf.compat.v1.data.make_one_shot_iterator(tf_x_val_miss)","c1ff79e9":"# Build Conv2D preprocessor for image data\nif data_type in ['hmnist', 'sprites']:\n    print(\"Using CNN preprocessor\")\n    image_preprocessor = ImagePreprocessor(img_shape, cnn_sizes, cnn_kernel_size)\nelif data_type == 'physionet':\n    image_preprocessor = None\nelse:\n    raise ValueError(\"Data type must be one of ['hmnist', 'physionet', 'sprites']\")","40e9e8fd":"if model_type == \"vae\":\n    model = VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n                decoder_sizes=decoder_sizes, decoder=decoder,\n                image_preprocessor=image_preprocessor, window_size=window_size,\n                beta=beta, M=M, K=K)\nelif model_type == \"hi-vae\":\n    model = HI_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                   encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n                   decoder_sizes=decoder_sizes, decoder=decoder,\n                   image_preprocessor=image_preprocessor, window_size=window_size,\n                   beta=beta, M=M, K=K)\nelif model_type == \"gp-vae\":\n    encoder = BandedJointEncoder if banded_covar else JointEncoder\n    model = GP_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n                   encoder_sizes=encoder_sizes, encoder=encoder,\n                   decoder_sizes=decoder_sizes, decoder=decoder,\n                   kernel=kernel, sigma=sigma,\n                   length_scale=length_scale, kernel_scales = kernel_scales,\n                   image_preprocessor=image_preprocessor, window_size=window_size,\n                   beta=beta, M=M, K=K, data_type=data_type)\nelse:\n    raise ValueError(\"Model type must be one of ['vae', 'hi-vae', 'gp-vae']\")","8acdea98":"print(\"GPU support: \", tf.test.is_gpu_available())","e61bbf5a":"print(\"Training...\")\n_ = tf.compat.v1.train.get_or_create_global_step()\ntrainable_vars = model.get_trainable_vars()\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\nprint(\"Encoder: \", model.encoder.net.summary())\nprint(\"Decoder: \", model.decoder.net.summary())","2d47214b":"if model.preprocessor is not None:\n    print(\"Preprocessor: \", model.preprocessor.net.summary())\n    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net,\n                                          decoder=model.decoder.net, preprocessor=model.preprocessor.net,\n                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())\nelse:\n    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net, decoder=model.decoder.net,\n                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())","62625213":"summary_writer = tf.compat.v2.summary.create_file_writer(logdir=outdir, flush_millis=10000)","a56055b3":"if num_steps == 0:\n    num_steps = num_epochs * len(x_train_miss) \/\/ batch_size\nelse:\n    num_steps = num_steps","d0a6f351":"if print_interval == 0:\n    print_interval = num_steps \/\/ num_epochs","4515907f":"losses_train = []\nlosses_val = []","2d4e12e7":"t0 = time.time()","16661b64":"with summary_writer.as_default(), tf.compat.v2.summary.record_if(True):\n    for i, (x_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n        try:\n            with tf.GradientTape() as tape:\n                tape.watch(trainable_vars)\n                loss = model.compute_loss(x_seq, m_mask=m_seq)\n                losses_train.append(loss.numpy())\n            grads = tape.gradient(loss, trainable_vars)\n            grads = [np.nan_to_num(grad) for grad in grads]\n            grads, global_norm = tf.clip_by_global_norm(grads, gradient_clip)\n            optimizer.apply_gradients(zip(grads, trainable_vars),\n                                      global_step=tf.compat.v1.train.get_or_create_global_step())\n\n            # Print intermediate results\n            if i % print_interval == 0:\n                print(\"================================================\")\n                print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(optimizer._lr, global_norm))\n                print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n                loss, nll, kl = model.compute_loss(x_seq, m_mask=m_seq, return_parts=True)\n                print(\"Train loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(loss, nll, kl))\n\n                saver.save(checkpoint_prefix)\n                tf.compat.v2.summary.scalar(name=\"loss_train\", data=loss, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"kl_train\", data=kl, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"nll_train\", data=nll, step=tf.compat.v1.train.get_or_create_global_step())\n\n                # Validation loss\n                x_val_batch, m_val_batch = tf_x_val_miss.get_next()\n                val_loss, val_nll, val_kl = model.compute_loss(x_val_batch, m_mask=m_val_batch, return_parts=True)\n                losses_val.append(val_loss.numpy())\n                print(\"Validation loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(val_loss, val_nll, val_kl))\n\n                tf.compat.v2.summary.scalar(name=\"loss_val\", data=val_loss, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"kl_val\", data=val_kl, step=tf.compat.v1.train.get_or_create_global_step())\n                tf.compat.v2.summary.scalar(name=\"nll_val\", data=val_nll, step=tf.compat.v1.train.get_or_create_global_step())\n\n                if data_type in [\"hmnist\", \"sprites\"]:\n                    # Draw reconstructed images\n                    x_hat = model.decode(model.encode(x_seq).sample()).mean()\n                    tf.compat.v2.summary.image(name=\"input_train\", data=tf.reshape(x_seq, [-1]+list(img_shape)), step=tf.compat.v1.train.get_or_create_global_step())\n                    tf.compat.v2.summary.image(name=\"reconstruction_train\", data=tf.reshape(x_hat, [-1]+list(img_shape)), step=tf.compat.v1.train.get_or_create_global_step())\n                elif data_type == 'physionet':\n                    # Eval MSE and AUROC on entire val set\n                    x_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\n                    x_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)\n                    m_val_artificial_batches = np.array_split(m_val_artificial, batch_size, axis=0)\n                    get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_artificial_batches)\n\n                    n_missings = m_val_artificial.sum()\n                    mse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m).numpy()\n                                       for x, y, m in get_val_batches()]) \/ n_missings\n\n                    x_val_imputed = np.vstack([model.decode(model.encode(x_batch).mean()).mean().numpy()\n                                               for x_batch in x_val_miss_batches])\n                    x_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]  # impute gt observed values\n\n                    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n                    val_split = len(x_val_imputed) \/\/ 2\n                    cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n                    cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n                    probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n                    auroc = roc_auc_score(y_val[val_split:], probs)\n                    print(\"MSE miss: {:.4f} | AUROC: {:.4f}\".format(mse_miss, auroc))\n\n                    # Update learning rate (used only for physionet with decay=0.5)\n                    if i > 0 and i % (10*print_interval) == 0:\n                        optimizer._lr = max(0.5 * optimizer._lr, 0.1 * learning_rate)\n                t0 = time.time()\n        except KeyboardInterrupt as e:\n            print(\"KeyboardInterrupt\")\n            saver.save(checkpoint_prefix)\n#             if debug:\n#                 import ipdb\n#                 ipdb.set_trace()\n            break","d4efb959":"# Split data on batches\nx_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\nx_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)","cd33947f":"if data_type == 'physionet':\n    m_val_batches = np.array_split(m_val_artificial, batch_size, axis=0)\nelse:\n    m_val_batches = np.array_split(m_val_miss, batch_size, axis=0)","8864e3b4":"get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_batches)","2c9a598b":"# Compute NLL and MSE on missing values\nn_missings = m_val_artificial.sum() if data_type == 'physionet' else m_val_miss.sum()\nnll_miss = np.sum([model.compute_nll(x, y=y, m_mask=m).numpy()\n                   for x, y, m in get_val_batches()]) \/ n_missings\nmse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m, binary=data_type==\"hmnist\").numpy()\n                   for x, y, m in get_val_batches()]) \/ n_missings","5f610c3d":"print(\"NLL miss: {:.4f}\".format(nll_miss))\nprint(\"MSE miss: {:.4f}\".format(mse_miss))","4d6ecce7":"# Save imputed values\nz_mean = [model.encode(x_batch).mean().numpy() for x_batch in x_val_miss_batches]\nnp.save(os.path.join(outdir, \"z_mean\"), np.vstack(z_mean))\nx_val_imputed = np.vstack([model.decode(z_batch).mean().numpy() for z_batch in z_mean])\nx_val_imputed_std = np.vstack([model.decode(z_batch).stddev().numpy() for z_batch in z_mean])\nnp.save(os.path.join(outdir, \"imputed_no_gt\"), x_val_imputed)\nnp.save(os.path.join(outdir, \"x_val_imputed_std\"), x_val_imputed_std)","2dcec619":"# impute gt observed values\nx_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]\nnp.save(os.path.join(outdir, \"imputed\"), x_val_imputed)","8ce573c3":"if data_type == \"hmnist\":\n    # AUROC evaluation using Logistic Regression\n    x_val_imputed = np.round(x_val_imputed)\n    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n\n    cls_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', tol=1e-10, max_iter=10000)\n    val_split = len(x_val_imputed) \/\/ 2\n\n    cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n    probs = cls_model.predict_proba(x_val_imputed[val_split:])\n\n    auprc = average_precision_score(np.eye(num_classes)[y_val[val_split:]], probs)\n    auroc = roc_auc_score(np.eye(num_classes)[y_val[val_split:]], probs)\n    print(\"AUROC: {:.4f}\".format(auroc))\n    print(\"AUPRC: {:.4f}\".format(auprc))\n\nelif data_type == \"sprites\":\n    auroc, auprc = 0, 0\n\nelif data_type == \"physionet\":\n    # Uncomment to preserve some z_samples and their reconstructions\n    # for i in range(5):\n    #     z_sample = [model.encode(x_batch).sample().numpy() for x_batch in x_val_miss_batches]\n    #     np.save(os.path.join(outdir, \"z_sample_{}\".format(i)), np.vstack(z_sample))\n    #     x_val_imputed_sample = np.vstack([model.decode(z_batch).mean().numpy() for z_batch in z_sample])\n    #     np.save(os.path.join(outdir, \"imputed_sample_{}_no_gt\".format(i)), x_val_imputed_sample)\n    #     x_val_imputed_sample[m_val_miss == 0] = x_val_miss[m_val_miss == 0]\n    #     np.save(os.path.join(outdir, \"imputed_sample_{}\".format(i)), x_val_imputed_sample)\n\n    # AUROC evaluation using Logistic Regression\n    x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n    val_split = len(x_val_imputed) \/\/ 2\n    cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n    cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n    probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n    auprc = average_precision_score(y_val[val_split:], probs)\n    auroc = roc_auc_score(y_val[val_split:], probs)\n\n    print(\"AUROC: {:.4f}\".format(auroc))\n    print(\"AUPRC: {:.4f}\".format(auprc))","360d4d41":"# Visualize reconstructions\nif data_type in [\"hmnist\", \"sprites\"]:\n    img_index = 0\n    if data_type == \"hmnist\":\n        img_shape = (28, 28)\n        cmap = \"gray\"\n    elif data_type == \"sprites\":\n        img_shape = (64, 64, 3)\n        cmap = None\n\n    fig, axes = plt.subplots(nrows=3, ncols=x_val_miss.shape[1], figsize=(2*x_val_miss.shape[1], 6))\n\n    x_hat = model.decode(model.encode(x_val_miss[img_index: img_index+1]).mean()).mean().numpy()\n    seqs = [x_val_miss[img_index:img_index+1], x_hat, x_val_full[img_index:img_index+1]]\n\n    for axs, seq in zip(axes, seqs):\n        for ax, img in zip(axs, seq[0]):\n            ax.imshow(img.reshape(img_shape), cmap=cmap)\n            ax.axis('off')\n\n    suptitle = model_type + f\" reconstruction, NLL missing = {mse_miss}\"\n    fig.suptitle(suptitle, size=18)\n    fig.savefig(os.path.join(outdir, data_type + \"_reconstruction.pdf\"))\n\nresults_all = [seed, model_type, data_type, kernel, beta, latent_dim,\n               num_epochs, batch_size, learning_rate, window_size,\n               kernel_scales, sigma, length_scale,\n               len(encoder_sizes), encoder_sizes[0] if len(encoder_sizes) > 0 else 0,\n               len(decoder_sizes), decoder_sizes[0] if len(decoder_sizes) > 0 else 0,\n               cnn_kernel_size, cnn_sizes,\n               nll_miss, mse_miss, losses_train[-1], losses_val[-1], auprc, auroc, testing, data_dir]\n\nwith open(os.path.join(outdir, \"results.tsv\"), \"w\") as outfile:\n    outfile.write(\"seed\\tmodel\\tdata\\tkernel\\tbeta\\tz_size\\tnum_epochs\"\n                  \"\\tbatch_size\\tlearning_rate\\twindow_size\\tkernel_scales\\t\"\n                  \"sigma\\tlength_scale\\tencoder_depth\\tencoder_width\\t\"\n                  \"decoder_depth\\tdecoder_width\\tcnn_kernel_size\\t\"\n                  \"cnn_sizes\\tNLL\\tMSE\\tlast_train_loss\\tlast_val_loss\\tAUPRC\\tAUROC\\ttesting\\tdata_dir\\n\")\n    outfile.write(\"\\t\".join(map(str, results_all)))\n\nwith open(os.path.join(outdir, \"training_curve.tsv\"), \"w\") as outfile:\n    outfile.write(\"\\t\".join(map(str, losses_train)))\n    outfile.write(\"\\n\")\n    outfile.write(\"\\t\".join(map(str, losses_val)))\n\nprint(\"Training finished.\")","5a247ac4":"### Training","20bfb59f":"## Kaggle stuff","6b394c07":"### Prep","4bcbc1fb":"### Flags","6c6257bf":"### Load data","f948af13":"### Define data specific parameters","2ab7c440":"### Build Model","0c05bb08":"### Training preparation","e76325b1":"### Imports","39e22098":"- https:\/\/www.kaggle.com\/siddharthchaini\/gp-vae-intro\n- https:\/\/github.com\/siddharthchaini\/GP-VAE\/blob\/master\/train.py","d7924bdd":"### Download Physionet data","619624fc":"### Evaluation"}}