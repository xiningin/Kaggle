{"cell_type":{"447239eb":"code","d98fcb3c":"code","0829a255":"code","fe072b67":"code","77d1dbaa":"code","184c44c8":"code","04fb9cf7":"code","f0ddf1b2":"code","f36aafe8":"code","ffe057bc":"code","dd9d60db":"code","88d5c0bb":"code","dc982296":"code","6bb0fa2e":"code","4cef3851":"code","534b5056":"code","7418b566":"code","8f64710a":"code","728d82ec":"code","71d15f79":"code","4ef7742c":"code","f54ff8f7":"code","2da361ea":"code","045a49c3":"code","8639e620":"code","6b601ff0":"code","d9693b7d":"code","67bd3ce7":"code","3fc9c726":"code","b8e9eb37":"code","b4433103":"code","194a4dbf":"code","72301f85":"code","5b005cfe":"markdown","97557692":"markdown","3dfba70e":"markdown","f769f3a1":"markdown","4df06126":"markdown","42d5c71b":"markdown","80c123c7":"markdown","06810c31":"markdown","f3687b08":"markdown","554db795":"markdown","9c4d3aaf":"markdown","49f11599":"markdown","7aadf202":"markdown","fd82c889":"markdown","f590d75f":"markdown","6f216d93":"markdown","4be82992":"markdown","903580a1":"markdown","c0d43b90":"markdown","bde85613":"markdown","18aafef9":"markdown","5e513841":"markdown","42f03246":"markdown","c6360b22":"markdown"},"source":{"447239eb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize, sent_tokenize, pos_tag, ne_chunk, FreqDist\nfrom textblob import TextBlob\nimport collections\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import normalize\nfrom wordcloud import WordCloud\n%matplotlib inline\npy.init_notebook_mode(connected=True)","d98fcb3c":"train_data=pd.read_csv(\"..\/input\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/test.csv\")\nsubmission_data=pd.read_csv(\"..\/input\/sample_submission.csv\")","0829a255":"train_data.shape","fe072b67":"test_data.shape","77d1dbaa":"submission_data.shape","184c44c8":"train_data.head()","04fb9cf7":"train_data.columns","f0ddf1b2":"train_data['question_text'].isnull().value_counts()","f36aafe8":"train_data['target'].isnull().value_counts()","ffe057bc":"train_data['question_text'].drop_duplicates().count()","dd9d60db":"# Unique target\ntrain_data['target'].unique()","88d5c0bb":"insicere_quiz=train_data[train_data['target']==1]\nsincere_quiz=train_data[train_data['target']==0]","dc982296":"insicere_quiz['target'].count()\/train_data['target'].count() * 100","6bb0fa2e":"sincere_quiz['target'].count()\/train_data['target'].count() * 100","4cef3851":"target_counts = train_data['target'].value_counts()\ntarget_counts","534b5056":"pie_labels = (np.array(target_counts.index))\npie_sizes = (np.array((target_counts \/ target_counts.sum())*100))\n\ntrace = go.Pie(labels=pie_labels, values=pie_sizes)\npie_layout = go.Layout(title='Target distribution',font=dict(size=16),width=500,height=500)\nfig = go.Figure(data=[trace], layout=pie_layout)\npy.iplot(fig, filename=\"file_name\")","7418b566":"bar_graph = go.Bar(\n        x=target_counts.index,\n        y=target_counts.values,\n        marker=dict(\n        color=target_counts.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nbar_layout = go.Layout(title='Target Distrinution',font=dict(size=20))\nfig = go.Figure(data=[bar_graph], layout=bar_layout)\npy.iplot(fig, filename=\"file_name\")","8f64710a":"stop_words = stopwords.words('english')\n\ntokens=[]\nfor i in train_data[0:10]['question_text']:\n    for j in word_tokenize(i):\n        tokens.append(j)\n\nfiltered_text = [token for token in tokens if not token in stop_words]  \n\nfiltered_text = [] \n  \nfor i in tokens: \n    if i in stop_words: \n        filtered_text.append(i) \n        \nprint(np.array(filtered_text))\nlen(np.array(filtered_text))","728d82ec":"token=[]\nfor i in train_data[0:2]['question_text']:\n    token.append(pos_tag(word_tokenize(i)))\n\nprint (token)","71d15f79":"for i in train_data[0:10]['question_text']:\n    print(TextBlob(i).noun_phrases)","4ef7742c":"# lng=[]\n# for i in train_data[0:5]['question_text']:\n#     lng.append(TextBlob(i).detect_language())\n    \n# set(lng)","f54ff8f7":"for i in train_data[0:5]['question_text']:\n    print(TextBlob(i).word_counts[\"quebec\"])","2da361ea":"for i in train_data[0:10]['question_text']:\n    print(i,\" => \",len(word_tokenize(i)))","045a49c3":"for i in train_data[0:10]['question_text']:\n    print(i,\" => \",len(sent_tokenize(i)))","8639e620":"tokens=[]\nfor i in train_data[0:10]['question_text']:\n    for j in word_tokenize(i):\n        tokens.append(j)\n\n\nfrequency_distribution=FreqDist(tokens).most_common()\nprint(frequency_distribution)","6b601ff0":"tokens=[]\nfor i in train_data[0:10]['question_text']:\n    for j in word_tokenize(i):\n        tokens.append(j)\n\n\nfrequency_distribution=FreqDist(tokens)\nword_c={}\nfor i in frequency_distribution:\n    word_c[i]=token\n    word_c[i]=frequency_distribution[i]\n\nsorted(word_c.items(), key=lambda x: x[1], reverse=True)","d9693b7d":"text=str(train_data[0:1000]['question_text'])\nwordcloud = WordCloud(width=1600, height=800).generate(text)\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.figure( figsize=(40,30) )\nplt.show()","67bd3ce7":"# Calculating Sentment Analysis with TextBlob\nfor i in train_data[0:5]['question_text']:\n    print(i,\" => \",TextBlob(i).sentiment)","3fc9c726":"# Extracting the sentiment polarity of a text\nfor i in train_data[0:5]['question_text']:\n    print(TextBlob(i).sentiment.polarity)","b8e9eb37":"# Extracting the sentiment subjectivity of a text\nfor i in train_data[0:5]['question_text']:\n    print(TextBlob(i).sentiment.subjectivity)","b4433103":"for i in train_data[0:2]['question_text']:\n    print(TextBlob(i).ngrams(n=3))","194a4dbf":"corpus=[]\nfor i in train_data[0:5]['question_text']:\n    corpus.append(i)\n\ncvect = CountVectorizer(ngram_range=(1,1))\ncounts = cvect.fit_transform(corpus)\nnormalized_counts = normalize(counts, norm='l1', axis=1)\n\ntfidf = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False)\ntfs = tfidf.fit_transform(corpus)\nnew_tfs = normalized_counts.multiply(tfidf.idf_)\n\nfeature_names = tfidf.get_feature_names()\ncorpus_index = [n for n in corpus]\ndf = pd.DataFrame(new_tfs.T.todense(), index=feature_names, columns=corpus_index)\n\nprint(df)","72301f85":"#Bow with collection\ntoken=[]\nfor i in train_data[0:5]['question_text']:\n    token.append(i)\n\nbow = [collections.Counter(words.split(\" \")) for words in token]\ntotal_bow=sum(bow,collections.Counter())\nprint(total_bow)","5b005cfe":"**To Continue .....**","97557692":"8. Most Common Words","3dfba70e":"**9. WordCloud**","f769f3a1":"**3. Extracting Noun Phrases**","4df06126":"**5. Word frequency**","42d5c71b":"**Inspect the data**\n\nThe training data set has a total of 1306122 records with three columns. testing data has a total of 56370 records with 2 columns. Submission data has a total of 56370 with 2 columns","80c123c7":"**Focus on Training data**","06810c31":"**Percentage of sincere questions in the dataset**","f3687b08":"**Check for null values**","554db795":"**12. Bag of Words (BoW)**","9c4d3aaf":"**1. Checking for stop words**","49f11599":"**Simple visualization for target distribution**","7aadf202":"**Percentage of insincere questions  in the dataset**","fd82c889":"**4. Detecting language**","f590d75f":"7. Sentence count","6f216d93":"**11. Term Frequency Inverse Document Frequency (tf-idf)**","4be82992":"**Notebook Objective:**\nThe objective of this notebook is to explore how the data looks without any processing.\n\n**Competition Objective :**\nThe main objective of this competition is to predict whether the asked question on Quora is sincere or not. ","903580a1":"**2. Part Of Speech Tagging (POS)**","c0d43b90":"**Import all required libraries**","bde85613":"**6. Words count**","18aafef9":"**Load data**","5e513841":"**Simple Natural Language Processing (NLP) Tasks**","42f03246":"**10. N-grams (tri-gram)**","c6360b22":"**9. Sentiment Analysis**"}}