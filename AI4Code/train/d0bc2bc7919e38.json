{"cell_type":{"ab7839b5":"code","eb062f35":"code","22668493":"code","ccdd00c1":"code","ac1a26b9":"code","7f5d5d96":"code","78b957e2":"code","d5af747d":"code","e9ad1e7a":"code","8eaf1e83":"code","b5af1a21":"code","cf5de3bf":"code","c70590c4":"code","aae57299":"code","82cfd6f2":"code","17688515":"code","12d11513":"markdown","6755c987":"markdown","3c002d7b":"markdown","aaedc645":"markdown","e795d8d0":"markdown","5e3c2dcc":"markdown","41f02657":"markdown","bc70d9c8":"markdown","9a0c4b63":"markdown","ce6fd7a6":"markdown","87edfc45":"markdown","e9d9a410":"markdown","7d73e20a":"markdown"},"source":{"ab7839b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb062f35":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix","22668493":"# Read in data as pandas dataframe and display first 5 rows\nfeatures_raw = pd.read_csv('\/kaggle\/input\/datareduced\/DataCalculator_Red.CSV', sep=';', decimal=',', nrows=100000)\nfeatures_raw.head(5)","ccdd00c1":"features_raw.corr()","ac1a26b9":"import seaborn as sns\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = features_raw.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","7f5d5d96":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer, RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Featurematrix X: alle Zeilen, Spalten ohne tba\nX_data = features_raw.drop([\"tba\"], axis=1)\n\n# Zielvektor y: tba\ny_data = features_raw['tba']\n\n# KLassifizierung der Spalten\ncat_columns =  ['RKZ', 'bzw', 'Sex']\nnum_columns = list(X_data.drop(cat_columns,axis=1).columns)\n\nX_data[num_columns] = X_data[num_columns].astype('float64')\n#y_data = y_data.astype('float64')\n\n# Pipeline definieren\n#scaler = StandardScaler()\nscaler = RobustScaler()\nnum_pipeline = Pipeline([('scaler', scaler)])\ncat_pipeline = Pipeline([('onehot', OneHotEncoder())])\n\ndata_preproc_pipeline = ColumnTransformer([\n    ('cat_values', cat_pipeline, cat_columns),\n    ('num_values', num_pipeline, num_columns)\n])\n\n# Aufteilung: 80% Training, 20% Test\nX_train_raw, X_test_raw, train_labels, test_labels = train_test_split(X_data, y_data, test_size=0.20, random_state=1)\n\n# Preprocessing (Klassifizierung und Scaling)\ndata_preproc_pipeline.fit(X_train_raw, train_labels)\nfeature_names = list(data_preproc_pipeline.named_transformers_[\"cat_values\"].named_steps[\"onehot\"].get_feature_names()) \\\n+ data_preproc_pipeline.transformers_[1][2]\n\n#train_features, test_features, train_labels, test_labels\n\ntrain_features = pd.DataFrame(data_preproc_pipeline.transform(X_train_raw), columns=feature_names)\ntest_features = pd.DataFrame(data_preproc_pipeline.transform(X_test_raw), columns=feature_names)\n\nfeature_list = list(train_features.columns)","78b957e2":"print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","d5af747d":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model \nrf = RandomForestRegressor(random_state=42, verbose=1, n_jobs = -1)\nrf.fit(train_features, train_labels)\n\n","e9ad1e7a":"# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint('Mean Squared Error:', mean_squared_error(test_labels, predictions))\n\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', mean_absolute_error(test_labels, predictions))\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ test_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\n\n# Calculate Score\nscore = rf.score(test_features, test_labels)\n\nprint('Accuracy:', round(accuracy, 2), '%')\n\nprint('Score:', round(score*100, 2), '%')","8eaf1e83":"x_ax = range(len(test_labels))\nplt.scatter(x_ax, test_labels, s=5, color=\"blue\", label=\"original\")\nplt.scatter(x_ax, predictions, s=2, color=\"red\", label=\"predicted\")\nplt.legend()\nplt.show()","b5af1a21":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","cf5de3bf":"#df_results = pd.DataFrame(test_features)\ndf_results = pd.DataFrame(X_test_raw)\n\ntest_labels = np.array(test_labels)\n\ndf_results[\"TBA erwartet\"] = test_labels\ndf_results[\"TBA berechnet\"] = np.round(predictions,2)\ndf_results[\"abs. Differenz\"] = np.round(abs(predictions - test_labels),2)\ndf_results[\"abs. Differenz%\"] = np.round(df_results[\"abs. Differenz\"] \/ test_labels * 100,3)\n\nprint('Zuf\u00e4llige Auswahl:')\ndisplay(df_results.sample(n=10))\n\n\nprint('F\u00fcnf gr\u00f6\u00dfte und kleinste abs. Differenzen%:')\n# sortiert nach abs. Differenz%\ndf_sort = df_results.sort_values('abs. Differenz%', ascending=False)\n#display(df_sort.head(10))\ndisplay(df_sort)\n\ndisplay(df_results[[\"abs. Differenz\", \"abs. Differenz%\"]].describe())","c70590c4":"from pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","aae57299":"# Instantiate model \nrf = RandomForestRegressor(n_estimators=700, min_samples_split=2, min_samples_leaf=1, max_features='auto', max_depth=110, bootstrap=True, random_state=42, verbose=1, n_jobs = -1)\nrf.fit(train_features, train_labels)\n\n# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nprint('Mean Squared Error:', mean_squared_error(test_labels, predictions))\n\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', mean_absolute_error(test_labels, predictions))\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ test_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\n\n# Calculate Score\nscore = rf.score(test_features, test_labels)\n\nprint('Accuracy:', round(accuracy, 2), '%')\n\nprint('Score:', round(score*100, 2), '%')","82cfd6f2":"from sklearn.ensemble import GradientBoostingRegressor\ngrb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n                                max_depth=1, random_state=0, loss='ls', verbose=0).fit(train_features, train_labels)\n\npredictions = grb.predict(test_features)\nprint(\"GradientBoost:\")\nprint('Mean Squared Error:', mean_squared_error(test_labels, predictions))\nprint('Mean Absolute Error:', mean_absolute_error(test_labels, predictions))\n","17688515":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=100, random_state=0)\n\nregr_1.fit(train_features, train_labels)\nregr_2.fit(train_features, train_labels)\n\n# Predict\ny_1 = regr_1.predict(test_features)\ny_2 = regr_2.predict(test_features)\n\nprint(\"Decision Tree: \")\nprint('Mean Squared Error:', mean_squared_error(test_labels, y_1))\nprint('Mean Absolute Error:', mean_absolute_error(test_labels, y_1))\n\nprint(\"AdaBoost: \")\nprint('Mean Squared Error:', mean_squared_error(test_labels, y_2))\nprint('Mean Absolute Error:', mean_absolute_error(test_labels, y_2))\n","12d11513":"**Datenaufbereitung**","6755c987":"Ab hier versuchen wir noch weitere Verfahren mit weitgehend Default-Parametern.","3c002d7b":"**Make Predictions on Test Data**","aaedc645":"**Gradient Boost**","e795d8d0":"**Beispiel-Liste f\u00fcr Testwerte vs. vorhergesagte Werte**","5e3c2dcc":"Mit einem Random Search Cross Validation wurden \"optimale\" Hyperparameter wie folgt ermittelt:\n\n**Tuning-Modell**\nParameter:\n{'n_estimators': 700,\n 'min_samples_split': 2,\n 'min_samples_leaf': 1,\n 'max_features': 'auto',\n 'max_depth': 110,\n 'bootstrap': True}\n \n Diese werden nachfolgend verwendet und erneut trainiert und vorhergesagt.","41f02657":"**Datenaufbereitung**\n* Scaling der X-Werte\n* Kategorisierung der nicht-numerischen Werte","bc70d9c8":"**Einfluss der Variablen**\n\nWie zu erwarten, haben die Beitragszahlweise (wegen Berechnung technischer Beitrag) und das Geschlecht (Unisex-Tafel) keinen Einfluss.","9a0c4b63":"**Korrelationsmatrix**","ce6fd7a6":"**Decision Tree Regression with AdaBoost**","87edfc45":"**Predictions vs. Testlabels**","e9d9a410":"**Bisher verwendete Hyper-Parameter**","7d73e20a":"**Training the Forest**\n\nHier versuchen wir erstmal ein einfaches Modell mit den Default-Parametern."}}