{"cell_type":{"955d9596":"code","c7d2918b":"code","86da7e2c":"code","21bbb71d":"code","91dc7dfb":"code","36799877":"code","b53324fa":"code","d401ce0c":"code","382f4d80":"code","762eda35":"code","e94531b0":"code","3d236b80":"code","3fa2a122":"markdown"},"source":{"955d9596":"!pip install \/kaggle\/input\/pyfftwwheel\/pyFFTW-0.12.0-cp37-cp37m-manylinux1_x86_64.whl","c7d2918b":"import os\nfrom pathlib import Path\nimport pandas as pd\nimport librosa\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch import nn\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom pyfftw.builders import rfft as rfft_builder\nfrom pyfftw import empty_aligned\n\nMEL_BANDS=80\nMEL_MIN=27.5\nMEL_MAX=10000\nSAMPLE_RATE=32_000\nTHRESHOLD = 0.9","86da7e2c":"classes = pd.read_pickle('..\/input\/esp-starter-pack-v2-weights\/classes.pkl')","21bbb71d":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cnn = nn.Sequential(*list(torchvision.models.resnet34(False).children())[:-2])\n        self.classifier = nn.Sequential(*[\n            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.BatchNorm1d(512),\n            nn.Linear(512, len(classes))\n        ])\n    \n    def forward(self, x):\n        bs, im_num, ch, y_dim, x_dim = x.shape\n        x = self.cnn(x.view(-1, ch, y_dim, x_dim))\n        x = x.mean((2,3))\n        x = self.classifier(x)\n        x = x.view(bs, im_num, -1)\n        x = x.mean(-2)\n        return x","91dc7dfb":"model = Model()\nmodel.load_state_dict(torch.load('..\/input\/esp-starter-pack-v2-weights\/220_0.65.pth'))\nmodel.cuda()\nmodel.eval();","36799877":"def spectrogram(samples, sample_rate, frame_len, fps, batch=48, dtype=None,\n                bins=None, plans=None):\n    \"\"\"\n    Computes a magnitude spectrogram for a given vector of samples at a given\n    sample rate (in Hz), frame length (in samples) and frame rate (in Hz).\n    Allows to transform multiple frames at once for improved performance (with\n    a default value of 48, more is not always better). Returns a numpy array.\n    Allows to return a limited number of bins only, with improved performance\n    over discarding them afterwards. Optionally accepts a set of precomputed\n    plans created with spectrogram_plans(), required when multi-threading.\n    \"\"\"\n    if dtype is None:\n        dtype = samples.dtype\n    if bins is None:\n        bins = frame_len \/\/ 2 + 1\n    if len(samples) < frame_len:\n        return np.empty((0, bins), dtype=dtype)\n    if plans is None:\n        plans = spectrogram_plans(frame_len, batch, dtype)\n    rfft1, rfft, win = plans\n    hopsize = int(sample_rate \/\/ fps)\n    num_frames = (len(samples) - frame_len) \/\/ hopsize + 1\n    nabs = np.abs\n    naa = np.asanyarray\n    if batch > 1 and num_frames >= batch and samples.flags.c_contiguous:\n        frames = np.lib.stride_tricks.as_strided(\n                samples, shape=(num_frames, frame_len),\n                strides=(samples.strides[0] * hopsize, samples.strides[0]))\n        spect = [nabs(rfft(naa(frames[pos:pos + batch:], dtype) * win)[:, :bins])\n                 for pos in range(0, num_frames - batch + 1, batch)]\n        samples = samples[(num_frames \/\/ batch * batch) * hopsize::]\n        num_frames = num_frames % batch\n    else:\n        spect = []\n    if num_frames:\n        spect.append(np.vstack(\n                [nabs(rfft1(naa(samples[pos:pos + frame_len:],\n                                dtype) * win)[:bins:])\n                 for pos in range(0, len(samples) - frame_len + 1, hopsize)]))\n    return np.vstack(spect) if len(spect) > 1 else spect[0]\n\n\ndef create_mel_filterbank(sample_rate, frame_len, num_bands, min_freq,\n                          max_freq):\n    \"\"\"\n    Creates a mel filterbank of `num_bands` triangular filters, with the first\n    filter starting at `min_freq` and the last one stopping at `max_freq`.\n    Returns the filterbank as a matrix suitable for a dot product against\n    magnitude spectra created from samples at a sample rate of `sample_rate`\n    with a window length of `frame_len` samples.\n    \"\"\"\n    # prepare output matrix\n    input_bins = (frame_len \/\/ 2) + 1\n    filterbank = np.zeros((input_bins, num_bands))\n\n    # mel-spaced peak frequencies\n    min_mel = 1127 * np.log1p(min_freq \/ 700.0)\n    max_mel = 1127 * np.log1p(max_freq \/ 700.0)\n    spacing = (max_mel - min_mel) \/ (num_bands + 1)\n    peaks_mel = min_mel + np.arange(num_bands + 2) * spacing\n    peaks_hz = 700 * (np.exp(peaks_mel \/ 1127) - 1)\n    fft_freqs = np.linspace(0, sample_rate \/ 2., input_bins)\n    peaks_bin = np.searchsorted(fft_freqs, peaks_hz)\n\n    # fill output matrix with triangular filters\n    for b, filt in enumerate(filterbank.T):\n        # The triangle starts at the previous filter's peak (peaks_freq[b]),\n        # has its maximum at peaks_freq[b+1] and ends at peaks_freq[b+2].\n        left_hz, top_hz, right_hz = peaks_hz[b:b + 3]  # b, b+1, b+2\n        left_bin, top_bin, right_bin = peaks_bin[b:b + 3]\n        # Create triangular filter compatible to yaafe\n        filt[left_bin:top_bin] = ((fft_freqs[left_bin:top_bin] - left_hz) \/\n                                  (top_bin - left_bin))\n        filt[top_bin:right_bin] = ((right_hz - fft_freqs[top_bin:right_bin]) \/\n                                   (right_bin - top_bin))\n        filt[left_bin:right_bin] \/= filt[left_bin:right_bin].sum()\n\n    return filterbank\n\ndef spectrogram_plans(frame_len, batch=48, dtype=np.float32):\n    \"\"\"\n    Precompute plans for spectrogram(), for a given frame length, batch size\n    and dtype. Returns two plans (single spectrum and batch), and a window.\n    \"\"\"\n    input_array = empty_aligned((batch, frame_len), dtype=dtype)\n    win = np.hanning(frame_len).astype(dtype)\n    return (rfft_builder(input_array[0]), rfft_builder(input_array), win)","b53324fa":"filterbank = create_mel_filterbank(SAMPLE_RATE, 256, MEL_BANDS, MEL_MIN, MEL_MAX)\n\ndef audio_to_melspec(audio):\n    spec = spectrogram(audio, SAMPLE_RATE, 256, 128)\n    return (spec @ filterbank).T","d401ce0c":"TEST_PATH = Path('..\/input\/birdsong-recognition') if os.path.exists('..\/input\/birdsong-recognition\/test_audio') else Path('..\/input\/birdcall-check')\n\nTEST_AUDIO_PATH = TEST_PATH\/'test_audio'\ntest_df = pd.read_csv(TEST_PATH\/'test.csv')\ntest_df.head()","382f4d80":"class AudioDataset(Dataset):\n    def __init__(self, items, classes, rec):\n        self.items = items\n        self.vocab = classes\n        self.rec = rec\n    \n    def __getitem__(self, idx):\n        _, rec_fn, start = self.items[idx]\n        x = self.rec[start*SAMPLE_RATE:(start+5)*SAMPLE_RATE]\n        example = self.get_specs(x)\n        example = self.normalize(example)\n        imgs = example.reshape(-1, 3, 80, 212)\n        return imgs.astype(np.float32)\n    \n    def get_specs(self, x):\n        xs = []\n        for i in range(3):\n            start_frame = int(i * 1.66 * SAMPLE_RATE)\n            xs.append(x[start_frame:start_frame+int(1.66*SAMPLE_RATE)])\n\n        specs = []\n        for x in xs:\n            specs.append(audio_to_melspec(x))\n        return np.stack(specs)\n    \n    def normalize(self, example):\n        return (example - 0.12934518) \/ 0.5612393\n    \n    def show(self, idx):\n        x = self[idx][0]\n        return plt.imshow(x.transpose(1,2,0)[:, :, 0])\n    \n    def __len__(self):\n        return len(self.items)","762eda35":"%%time\n\nrow_ids = []\nresults = []\n\nfor audio_id in test_df[test_df.site.isin(['site_1', 'site_2'])].audio_id.unique():\n    items = [(row.row_id, row.audio_id, int(row.seconds)-5) for idx, row in test_df[test_df.audio_id == audio_id].iterrows()]\n    rec = librosa.load(TEST_AUDIO_PATH\/f'{audio_id}.mp3', sr=SAMPLE_RATE, res_type='kaiser_fast')[0]\n    test_ds = AudioDataset(items, classes, rec)\n    dl = DataLoader(test_ds, batch_size=64)\n    for batch in dl:\n        with torch.no_grad():\n            preds = model(batch.cuda()).cpu().detach()\n            for row in preds:\n                birds = []\n                for idx in np.where(row > THRESHOLD)[0]:\n                    birds.append(classes[idx])\n                if not birds: birds = ['nocall']\n                results.append(' '.join(birds)) \n    row_ids += [item[0] for item in items]","e94531b0":"%%time\nfor audio_id in test_df[test_df.site=='site_3'].audio_id.unique():\n    rec = librosa.load(TEST_AUDIO_PATH\/f'{audio_id}.mp3', sr=SAMPLE_RATE, res_type='kaiser_fast')[0]\n    current_row = test_df[test_df.audio_id == audio_id]\n    duration = rec.shape[0] \/\/ SAMPLE_RATE\n    duration = duration - (duration % 5) - 5\n    items = [(current_row.row_id.item(), current_row.audio_id.item(), start_sec) for start_sec in [0 + i * 5 for i in range(duration \/\/ 5)]]\n    test_ds = AudioDataset(items, classes, rec)\n    dl = DataLoader(test_ds, batch_size=64)\n    \n    birds = []\n    for batch in dl:\n        with torch.no_grad():\n            preds = model(batch.cuda()).cpu().detach()\n            for row in preds:\n                for idx in np.where(row > THRESHOLD)[0]:\n                    birds.append(classes[idx])\n    row_ids.append(current_row.row_id.item())\n    if not birds: birds = ['nocall']\n    results.append(' '.join(list(set(birds))))             ","3d236b80":"predicted = pd.DataFrame(data={'row_id': row_ids, 'birds': results})\nsub = pd.DataFrame(data={'row_id': test_df.row_id})\nsub = sub.merge(predicted, 'left', 'row_id')\nsub.to_csv('submission.csv', index=False)","3fa2a122":"The below adapts [code](https:\/\/github.com\/f0k\/birdclef2018\/blob\/master\/experiments\/audio.py) by Jan Schl\u00fcter."}}