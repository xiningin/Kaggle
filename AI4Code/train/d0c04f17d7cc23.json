{"cell_type":{"ea7503f3":"code","1c809e84":"code","7606cc00":"code","f797280a":"code","1f4a0c11":"code","96e18828":"code","65681969":"code","06006c4f":"code","d092cf96":"code","e7f5f48a":"code","7521a06d":"markdown","db4ff4c4":"markdown","3f835209":"markdown","2d156b9a":"markdown","068401d7":"markdown","b0b03af6":"markdown"},"source":{"ea7503f3":"# libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tensorflow as tf\nimport transformers as hf\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","1c809e84":"sarcasm_df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\nsarcasm_df.drop(\"article_link\", axis=1, inplace=True)\nsarcasm_df['headline'] = sarcasm_df['headline'].apply(lambda x: x.lower())\nsarcasm_df['headline'] = sarcasm_df['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\nsarcasm_df['headline'] = sarcasm_df['headline'].apply((lambda x: re.sub('\\s+',' ',x)))\nsarcasm_df = sarcasm_df.sample(4000) # memory problems\nsarcasm_df.head()","7606cc00":"# train\/test split\nx_train, x_test, y_train, y_test = train_test_split(sarcasm_df['headline'].values, sarcasm_df['is_sarcastic'].values, train_size=0.5)","f797280a":"word_limit = 100","1f4a0c11":"tokenizer = tf.keras.preprocessing.text.Tokenizer(word_limit, split=' ')\ntokenizer.fit_on_texts(x_train)\n\ntrain_sequence = tokenizer.texts_to_sequences(x_train)\ntrain_sequence = tf.keras.preprocessing.sequence.pad_sequences(train_sequence)\n\ntest_sequence = tokenizer.texts_to_sequences(x_test)\ntest_sequence = tf.keras.preprocessing.sequence.pad_sequences(test_sequence)","96e18828":"baseline_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(word_limit, 128, input_length=train_sequence.shape[1]),\n    tf.keras.layers.SpatialDropout1D(0.3),\n    tf.keras.layers.LSTM(32, recurrent_dropout=0.2),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nbaseline_model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\nbaseline_model.fit(train_sequence, tf.cast(y_train, tf.int32), epochs=50, batch_size=100)\nbaseline_predictions = tf.squeeze(baseline_model.predict(test_sequence));","65681969":"dbert_tokenizer = hf.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndbert_model = hf.DistilBertModel.from_pretrained('distilbert-base-uncased')\n\ndef dbert_tokenization(word_list):\n    tokens = list(map(lambda x: dbert_tokenizer.encode(x, add_special_tokens=True), word_list))\n    padded = np.array([i + [0]*(word_limit-len(i)) for i in tokens])\n    attention_mask = np.where(padded != 0, 1, 0)\n    return padded, attention_mask\n\ndbert_train_tokenized, train_mask = dbert_tokenization(x_train)\ndbert_test_tokenized, test_mask = dbert_tokenization(x_test)","06006c4f":"# train data\ninput_ids = torch.tensor(dbert_train_tokenized)  \nattention_mask = torch.tensor(train_mask)\nwith torch.no_grad():\n    train_last_hidden_states = dbert_model(input_ids, attention_mask=attention_mask)\n\ntrain_dbert_features = train_last_hidden_states[0][:,0,:].numpy()\n\ninput_ids = torch.tensor(dbert_test_tokenized)  \nattention_mask = torch.tensor(test_mask)\nwith torch.no_grad():\n    test_last_hidden_states = dbert_model(input_ids, attention_mask=attention_mask)\n    \ntest_dbert_features = test_last_hidden_states[0][:,0,:].numpy()","d092cf96":"lr = LogisticRegression()\nlr.fit(train_dbert_features, y_train)\nbert_predictions = np.squeeze(lr.predict_proba(test_dbert_features)[:, 1])","e7f5f48a":"{\n    \"bert\": roc_auc_score(y_test, bert_predictions),\n    \"baseline\": roc_auc_score(y_test, baseline_predictions),\n}","7521a06d":"# Comparison","db4ff4c4":"# Simple Baseline\n\nNote, I'm relying on EDA done by other Kagglers!","3f835209":"# Setup","2d156b9a":"# BERT-Based Classifier\n\nUsing the DistilBERT from HuggingFace to predict sarcasm","068401d7":"BERT is doing far better!","b0b03af6":"# BERT"}}