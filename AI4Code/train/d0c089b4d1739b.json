{"cell_type":{"491d8b5d":"code","4dda2156":"code","06bd31ff":"code","0f03ec16":"code","e94f1c0c":"code","0e317836":"code","d7a17f43":"code","6df66f87":"code","44629945":"code","9449c5cd":"code","27ad65c4":"code","2f5889f7":"code","cfbfa4fe":"code","e0c84139":"code","2f2630f7":"code","6d851f81":"code","30d76c5b":"code","eb903650":"code","5efa43fc":"code","5893b4f0":"code","f2170af2":"code","d44844bd":"code","57358dc8":"code","74ec6d95":"code","3bfe1f2c":"code","a75991b1":"code","5c75c0a6":"code","4e319673":"code","55fe6d23":"code","56b405ec":"code","a853d3b0":"code","28e88509":"code","13bf0765":"code","aa23b562":"code","72998290":"code","975cea07":"code","73b05d27":"code","4cd3d970":"code","d2ccea38":"code","bc6e58db":"code","f5301293":"code","a0d08684":"markdown","86be98be":"markdown","e468f1b2":"markdown","2be90b29":"markdown","cd579f5c":"markdown","e8d196bf":"markdown","d309bc81":"markdown","4acb8c82":"markdown","9ae96a3f":"markdown","d2811393":"markdown","338a0ccc":"markdown","d882b8d4":"markdown","9cbb2e9e":"markdown","ac29e232":"markdown"},"source":{"491d8b5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4dda2156":"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nimport scipy.stats as stats\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier#\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport sys\n\nimport category_encoders as ce\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')","06bd31ff":"df = pd.read_csv('..\/input\/toyota-used-car-listing\/toyota.csv')\n\ndf_copy = df.copy()\ndf_copy.head()","0f03ec16":"df_copy.info()","e94f1c0c":"df_copy.shape","0e317836":"df_copy.describe()","d7a17f43":"df_copy.isnull().sum()","6df66f87":"df_copy['model'].value_counts()","44629945":"df_copy['transmission'].value_counts()","9449c5cd":"df_copy['fuelType'].value_counts()","27ad65c4":"df_copy['year'].value_counts()","2f5889f7":"numerical_features = ['price','mileage','tax','mpg','engineSize']\n\nRP=plt.figure(figsize=(14,8))\nfor i, feature in enumerate(numerical_features):\n    r=RP.add_subplot(2,3,i+1)\n    x_min = int(df_copy[feature].min())\n\n    x_max = int(df_copy[feature].max())\n\n    range_bin_width = range(x_min, x_max, 5)\n    sns.distplot(\n        df_copy[feature], color='#123456',\n        kde=True\n    )\n    r.set_title(feature+\" Histogram Plot\")\nRP.tight_layout()","cfbfa4fe":"g_price = df_copy.groupby('model')['price'].sum().sort_values(ascending=False).head(10)\nbarplot1 = px.bar(x=g_price.index,y=g_price.values, title=\"price by model (sum totals)\",color=g_price.values)\nbarplot1.show()","e0c84139":"box_features = ['model','transmission','fuelType']\n\nRP=plt.figure(figsize=(14,20))\nfor i, feature in enumerate(box_features):\n    r=RP.add_subplot(3,1,i+1)\n    sns.boxplot(data = df_copy, x=feature, y='price')\n    r.set_title(feature+\" Histogram Plot\")\nRP.tight_layout()","2f2630f7":"plt.figure(figsize=(6,6))\nplt.pie(df_copy['model'].value_counts(),autopct='%.1f', labels= df_copy['model'].unique())\nplt.title('model Weigtage')\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.pie(df_copy['year'].value_counts(),autopct='%.1f', labels= df_copy['year'].unique())\nplt.title('year Weigtage')\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.pie(df_copy['transmission'].value_counts(),autopct='%.1f', labels= df_copy['transmission'].unique())\nplt.title('transmission Weigtage')\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.pie(df_copy['fuelType'].value_counts(),autopct='%.1f', labels= df_copy['fuelType'].unique())\nplt.title('fuelType Weigtage')\nplt.show()","6d851f81":"y = df_copy.groupby(['year']).agg({'price':'mean'}).sort_values('price',ascending=False).reset_index()\nplt.figure(figsize=(8,5))\nsns.lineplot(data=y, x=\"year\", y=\"price\")","30d76c5b":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10,24))\n\nsns.scatterplot(data=df_copy, x='mileage', y='price',ax=ax1)\nsns.scatterplot(data=df_copy, x='tax', y='price',ax=ax2)\nsns.scatterplot(data=df_copy, x='mpg', y='price',ax=ax3)\nsns.scatterplot(data=df_copy, x='engineSize', y='price',ax=ax4)","eb903650":"df_copy['transmission'] = df_copy.transmission.str.replace('Other', 'Automatic')\nf = plt.figure(figsize=(20, 15))\nsns.swarmplot(x='transmission', y='price', data=df_copy)","5efa43fc":"df_copy[df_copy['price'] > 30000]","5893b4f0":"cats = []\n\nfor col in df_copy.columns:\n    if df_copy[col].dtype == 'object':\n        cats.append(col)\n\nce_ohe = ce.OneHotEncoder(cols=cats,handle_unknown='impute')\ndf_session_ce_onehot = ce_ohe.fit_transform(df_copy)\n\ndf_session_ce_onehot.head()","f2170af2":"X = df_session_ce_onehot.drop('price', axis = 1)\ny = df_session_ce_onehot.price","d44844bd":"import xgboost as xgb\nfrom xgboost import XGBRegressor","57358dc8":"params = {\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'max_depth': 5,\n    'subsample': 0.9,\n    'colsample_bytree': 1.0,\n}","74ec6d95":"class XGBoostRegressor():\n    def __init__(self, num_boost_round=10):\n        self.X = None\n        self.y = None\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        self.dtrain = None\n        self.pred = None\n        self.g_clf = None\n        self.clf = None\n        self.params = params\n        #self.params.update({'objective': 'reg:linear'})\n    \n    def set_param(self,params):\n        self.params = params\n    \n    def train_test_split(self,t_size=0.3, state =42):\n        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(self.X,self.y,test_size = t_size,random_state = state)\n        \n    def preprocess(self,X,y):\n        self.X = X\n        self.y = y\n    \n    def fit(self, num_boost_round = 100):\n        self.dtrain = xgb.DMatrix(self.X_train, label=self.y_train)\n        self.clf = xgb.train(params=self.params, dtrain=self.dtrain, num_boost_round=num_boost_round)\n        return self.clf\n        \n    def predict(self):\n        dtest = xgb.DMatrix(self.X_test)\n        self.pred = self.clf.predict(dtest)\n        return self.clf.predict(dtest)\n    \n    def rmse(self):\n        return np.sqrt(mean_squared_error(self.y_test, self.pred))\n    \n    def accuracy(self):\n        score = accuracy_score(self.y_test, self.pred)\n        print('score:{0:.4f}'.format(score))\n    \n    def importance(self):\n        _, ax = plt.subplots(figsize=(12, 8))\n        xgb.plot_importance(\n            self.clf,\n            ax=ax,\n            importance_type='gain',\n            show_values=False\n)","3bfe1f2c":"XGB_model = XGBoostRegressor()\nXGB_model.set_param(params)\nXGB_model.preprocess(X,y)\nXGB_model.train_test_split()\nXGB_model.fit()","a75991b1":"XGB_model.predict()","5c75c0a6":"XGB_model.rmse()","4e319673":"XGB_model.importance()","55fe6d23":"parameters = {\n    'eta': [0.05, 0.1, 0.3],\n    'max_depth': [3, 4, 5],\n    'subsample': [0.9, 1.0],\n    'colsample_bytree': [0.9, 1.0],\n}\n\nclass Parameters_Tuning():\n    def __init__(self):\n        self.X = None\n        self.y = None\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        self.params = params\n        self.clf = None\n        self.g_clf = None\n        self.r_clf = None\n    \n    def set_param(self,params):\n        self.params = params\n    \n    def preprocess(self,X,y):\n        self.X = X\n        self.y = y\n    \n    def train_test_split(self,t_size=0.3, state =42):\n        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(self.X,self.y,test_size = t_size,random_state = state)\n    \n    def randomizedsearch(self,n_jobs=1, cv=2):\n        self.r_clf = RandomizedSearchCV(self.clf, self.params, n_jobs=n_jobs, cv=cv)\n        return self.r_clf\n    \n    def gridsearch(self,n_jobs=1, cv=2):\n        self.g_clf = GridSearchCV(self.clf, self.params, n_jobs=n_jobs, cv=cv)\n        return self.g_clf\n    \n    def fit(self,grid_flg = True):\n        if grid_flg == True:\n            self.g_clf.fit(self.X_train, self.y_train)\n        else:\n            self.r_clf.fit(self.X_train, self.y_train)\n            \n    def show_best_score(self,grid_flg = True):\n        if grid_flg == True:\n            print(\"Best score is %s with the GridSearchCV.\" %(self.g_clf.best_score_))\n        else:\n            print(\"Best score is %s with the RandomizedSearchCV.\" %(self.r_clf.best_score_))\n    \n    def show_best_params(self,grid_flg = True):\n        if grid_flg == True:\n            print(\"Best parameters is %s with the GridSearchCV.\" %(self.g_clf.best_params_))\n        else:\n            print(\"Best parameters is %s with the RandomizedSearchCV.\" %(self.r_clf.best_params_))\n    \n    def xgbregressor(self):\n        self.clf = XGBRegressor(\n            eval_metric = 'rmse',\n            nthread = 4,\n            eta = 0.1,\n            num_boost_round = 80,\n            max_depth = 5,\n            subsample = 0.5,\n            colsample_bytree = 1.0,\n            #silent = 1,\n)","56b405ec":"parameters_tuning = Parameters_Tuning()\nparameters_tuning.set_param(parameters)\nparameters_tuning.preprocess(X,y)\nparameters_tuning.train_test_split()\nparameters_tuning.xgbregressor()\nparameters_tuning.gridsearch()\nparameters_tuning.fit()","a853d3b0":"parameters_tuning.show_best_score(grid_flg = True)","28e88509":"parameters_tuning.show_best_params(grid_flg = True)","13bf0765":"parameters_tuning.randomizedsearch()\nparameters_tuning.fit(grid_flg = False)","aa23b562":"parameters_tuning.show_best_score(grid_flg = False)","72998290":"parameters_tuning.show_best_params(grid_flg = False)","975cea07":"fix_clf = XGBRegressor(\n        eval_metric = 'rmse',\n        nthread = 4,\n        eta = 0.1,\n        num_boost_round = 80,\n        max_depth = 4,\n        subsample = 0.9,\n        colsample_bytree = 1.0,\n        #silent = 1,\n)","73b05d27":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state =42)","4cd3d970":"fix_clf.fit(X_train, y_train)\nscores = []\n\nscores.append(fix_clf.score(X_test, y_test))\n\n# \u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\npred_train = fix_clf.predict(X_train)\npred_test = fix_clf.predict(X_test)","d2ccea38":"print (mean_squared_error(y_train, pred_train))","bc6e58db":"print (mean_squared_error(y_test, pred_test))","f5301293":"scores","a0d08684":"Enginesize is way too influencial on other dats , secondly fueltype is impactful , otherwise taper off .","86be98be":"# **OneHotEncodering**","e468f1b2":"I check outliers that they have higher price more than 30,000.\nBut according to extracted datas , I think they are not skeptical datas or something extraordinary , in my opinion , it's just spec of normal used car .\nSo I do not handle those outliers at all .\nI'm not familiar with car spec and price of used car , so if someone who is way familiar with car industry may think that they are totally wrongly estimated...\n\nPlease never mind.","2be90b29":"Seemingly price and mileage and mpg are right-skewed distribution on those chart , otherwise enginesize is categorical, so it does not matter so far.","cd579f5c":"# **Parameters Tuning**","e8d196bf":"# **Toyota Used Car Price Prediction**","d309bc81":"![](https:\/\/toyota.jp\/pages\/_system\/common\/image\/noimage.png)","4acb8c82":"# **amendment of original model based on above tuner class**","9ae96a3f":"# **RMSE**","d2811393":"There are some outliers on charts , especially I guess it converge upon above 30,000 , I keep it in mind.","338a0ccc":"# **Model**","d882b8d4":"# **RandomizedSearchCV**","9cbb2e9e":"# **EDA**","ac29e232":"# **GridSearchCV**"}}