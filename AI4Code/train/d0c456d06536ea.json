{"cell_type":{"635abfa9":"code","01c9093f":"code","f12ab051":"code","04ca87da":"code","e98554bd":"code","682fd016":"code","bc419e5b":"code","380cc32e":"code","4073f450":"code","c8a109a2":"code","2e38941c":"code","cbce8d86":"code","a351963b":"code","6b950b3e":"code","5a0ec18f":"code","b6a4cc39":"code","4cd4890e":"code","51582b02":"code","ef0e2d7c":"code","37e13431":"code","22852d81":"code","b28622f0":"code","f9c43080":"code","a285e7cf":"code","104733fc":"code","96210b54":"code","3216716e":"code","db80da79":"code","b23e493b":"code","d4574cbc":"code","b64be05d":"code","45aba226":"code","0f7f01e4":"code","d69b2819":"code","ce0b81f7":"code","cd1ec9be":"code","2ea24853":"code","eef49f24":"code","7cdfea19":"code","ff2d595c":"code","ed8632af":"code","795c5670":"code","c442fc26":"code","c6d79c91":"code","8fa3d77c":"code","5f622345":"code","677a8d85":"markdown","ab2f07c1":"markdown","1fd949b9":"markdown","3d96ae3f":"markdown","7afea0c2":"markdown","4e544d69":"markdown","2cc76f3d":"markdown","0be932ba":"markdown","dc1232f4":"markdown","a9cd6615":"markdown","2889c76a":"markdown","0d34d81f":"markdown","4d9551fd":"markdown","4b14a8a5":"markdown","b5a10bf0":"markdown","e17b6e23":"markdown","23726e14":"markdown","0b3f38d9":"markdown","77e04181":"markdown","49af19b9":"markdown","e80317c8":"markdown","e5935149":"markdown","1c2047e9":"markdown","798ee82a":"markdown","d898aeb6":"markdown","31161108":"markdown","4e5742e3":"markdown","25df7835":"markdown","9afc034e":"markdown","89cde8dd":"markdown","58f987f0":"markdown","60055236":"markdown","86d8f9f6":"markdown","acec6635":"markdown","36729de2":"markdown","0a818bfa":"markdown","4a17655e":"markdown","8d585aa7":"markdown","8db8a1c4":"markdown","e3fa4889":"markdown"},"source":{"635abfa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","01c9093f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nimport keras","f12ab051":"train_data = pd.read_csv('..\/input\/csvTrainImages 13440x1024.csv', header = None)\ntrain_label = pd.read_csv('..\/input\/csvTrainLabel 13440x1.csv', header = None)\ntest_data = pd.read_csv('..\/input\/csvTestImages 3360x1024.csv', header = None)\ntest_label = pd.read_csv('..\/input\/csvTestLabel 3360x1.csv', header = None)","04ca87da":"train_data = train_data.iloc[:,:].values.astype('float32')\ntrain_label = train_label.iloc[:,:].values.astype('int32')-1\ntest_data = test_data.iloc[:,:].values.astype('float32')\ntest_label = test_label.iloc[:,:].values.astype('int32')-1","e98554bd":"pd.DataFrame(train_label).head(10)","682fd016":"pd.DataFrame(train_label).info()","bc419e5b":"def row_calculator(number_of_images, number_of_columns):\n    if number_of_images % number_of_columns != 0:\n        return (number_of_images \/ number_of_columns)+1\n    else:\n        return (number_of_images \/ number_of_columns)","380cc32e":"def display_image(x, img_size, number_of_images):\n    plt.figure(figsize = (8, 7))\n    if x.shape[0] > 0:\n        n_samples = int(x.shape[0]\/4)\n        x = x.reshape(n_samples, img_size, img_size)\n        number_of_rows = row_calculator(number_of_images, 4)\n        for i in range(number_of_images):\n            plt.subplot(number_of_rows, 4, i+1)\n            plt.imshow(x[i])","4073f450":"display_image(train_data, 64,16)\nplt.show()","c8a109a2":"labelencoder_X = LabelEncoder()\ntrain_label = labelencoder_X.fit_transform(train_label)","2e38941c":"train_data.shape","cbce8d86":"train_data = train_data.reshape([-1, 32, 32, 1])\ntest_data = test_data.reshape([-1, 32, 32, 1])","a351963b":"train_data.shape","6b950b3e":"train_label =keras.utils.np_utils.to_categorical(train_label,28)\n\ntest_label =keras.utils.np_utils.to_categorical(test_label,28)\n\ntrain_label_data = pd.DataFrame(train_label)\n\n","5a0ec18f":"train_label_data.head()","b6a4cc39":"\n    print(train_label_data[train_label_data[0]==1].count())\n    ","4cd4890e":"datagen = ImageDataGenerator(rescale=1.0\/255.0,\n        featurewise_center=False, \n        samplewise_center=False,  \n        featurewise_std_normalization=False,\n        samplewise_std_normalization=False,\n        zca_whitening=False,\n        rotation_range=10,\n        zoom_range = 0.1,  \n        width_shift_range=0.1, \n        height_shift_range=0.1,\n        horizontal_flip=False,\n        vertical_flip=False)","51582b02":"# prepare an iterators to scale images\ntrain_iterator = datagen.flow(train_data, train_label, batch_size=64)\ntest_iterator = datagen.flow(test_data, test_label, batch_size=64)\nprint('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n# confirm the scaling works\nbatchX, batchy = train_iterator.next()\nprint('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n# define mode","ef0e2d7c":"recognizer = Sequential()\n\nrecognizer.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (32,32,1)))\nrecognizer.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nrecognizer.add(MaxPool2D(pool_size=(2,2)))\nrecognizer.add(Dropout(0.5))\n\n\nrecognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nrecognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nrecognizer.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nrecognizer.add(Dropout(0.5))\n\n\nrecognizer.add(Flatten())\nrecognizer.add(Dense(units = 256, input_dim = 1024, activation = 'relu'))\nrecognizer.add(Dense(units = 256, activation = \"relu\"))\nrecognizer.add(Dropout(0.5))\nrecognizer.add(Dense(28, activation = \"softmax\"))","37e13431":"recognizer.summary()\n","22852d81":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n","b28622f0":"recognizer.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","f9c43080":" recognizer.fit_generator(train_iterator,epochs = 30, verbose = 2, steps_per_epoch=train_data.shape[0] \/\/ 100)","a285e7cf":"test_acc1=[]","104733fc":"for i in range(10):\n    _, acc = recognizer.evaluate_generator(test_iterator, steps=len(test_iterator))\n    print('Test Accuracy {0}: {1}' .format(i+1 , np.round((acc * 100),2)))\n    test_acc1.append(acc * 100)\n    ","96210b54":"basic_code_results = pd.Series(test_acc1)","3216716e":"np.round((basic_code_results.mean()),2)","db80da79":"recognizer2 = Sequential()\n\nrecognizer2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (32,32,1)))\nrecognizer2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nrecognizer2.add(MaxPool2D(pool_size=(2,2)))\nrecognizer2.add(Dropout(0.2))\n\n\nrecognizer2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nrecognizer2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nrecognizer2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nrecognizer2.add(Dropout(0.2))\n\n\nrecognizer2.add(Flatten())\nrecognizer2.add(Dense(units = 256, input_dim = 1024, activation = 'relu'))\nrecognizer2.add(Dense(units = 256, activation = \"relu\"))\nrecognizer2.add(Dropout(0.5))\nrecognizer2.add(Dense(28, activation = \"softmax\"))","b23e493b":"recognizer2.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","d4574cbc":"recognizer2.fit_generator(train_iterator,epochs = 30, verbose = 2, steps_per_epoch=train_data.shape[0] \/\/ 100)","b64be05d":"test_acc2 = []","45aba226":"for i in range(10):\n    _, acc = recognizer2.evaluate_generator(test_iterator, steps=len(test_iterator))\n    print('Test Accuracy {0}: {1}' .format(i+1 , np.round((acc * 100),2)))\n    test_acc2.append(np.round((acc * 100),2))","0f7f01e4":"final_code_results=pd.Series(test_acc2)","d69b2819":"np.round((final_code_results.mean()),2)","ce0b81f7":"test_predictions = recognizer2.predict(test_data)\ntest_predictions = np.round(test_predictions)","cd1ec9be":"test_predictions1=pd.DataFrame(test_predictions)","2ea24853":"test_predictions1.head()","eef49f24":"test_label1=pd.DataFrame(test_label)","7cdfea19":"incorrect_label_indices= (test_predictions1.idxmax(axis=1) != test_label1.idxmax(axis=1))","ff2d595c":"misclassified_data=test_data[incorrect_label_indices]\nnew_labels = test_label1[incorrect_label_indices]\nincorrect_prediction = test_predictions1[incorrect_label_indices]","ed8632af":"plt.imshow(misclassified_data[0].squeeze().T)\n","795c5670":"pd.Series(incorrect_prediction.idxmax(axis=1)).reset_index()[0][0]","c442fc26":"pd.Series(new_labels.idxmax(axis=1)).reset_index()[0][0]","c6d79c91":"plt.imshow(misclassified_data[1].squeeze().T)","8fa3d77c":"pd.Series(incorrect_prediction.idxmax(axis=1)).reset_index()[0][1]","5f622345":"pd.Series(new_labels.idxmax(axis=1)).reset_index()[0][1]","677a8d85":"### Generally , Natural language processing is , according to Wikipedia concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data , it helps machines to understand the natural language. Despite natural language processing depending on machine learning is from the late1980's but Arabic language processing was not considered before 2010 when Nezar Habash introduced series of lectures titled by \"Introduction to arabic natural language processing\" and generally references about handling Arabic Language in natural language processing are rare so we should search more about that due to this is one of the 5 official languages of UN and almost 400 million human speak it.","ab2f07c1":"In this case we try some modification, let's modify drop out parameter, in our model, we drop out 0.5 of the input nodes in all layers, but this may lead to relatively weak performance, on other words \"Under fitting\u201f So we will modify it in the 2 drop out layers that deal with image data before being flattened to produce probability of each of the 28 letters.","1fd949b9":"so we need to reshape it to demonstrate the 32*32 pixels shape of each image","3d96ae3f":"### First we need to introduce some concepts \n\n#### Convolutional layer: the main building block of CNN it consists of set of filters each of them study the properties of a specific part of the image.\n\n#### Kernel: the size of filters that consists the convolutional layer. \n\n#### Padding : if layer size did not cover the whole photo we will have parts of it not used in learning process, so we may add part from the beginning of the photo (here left side) to its ending to make it covered by an exact filter\u201fs size. \n\n#### Activation: the function we insert our layer\u201fs results into to have the final output of a layer. \n\n### These parameters are for convolutional layer but we have other layers needed Pooling \n\n#### layer: used to avoid inflation of layers, we divide the specific filter into certain number of grids taking the grid that has the highest value (in our case we use max pooling) so if we have pool size =(2,2) we take the maximum of each 4 neighbor grids.\n\n#### Dropout layer : off course not each filter (node) related to each node of the input layer so we drop out a random proportion of the weights so if we have dropout=0.2 we drop 0.2 of nodes in the layer. \n\n#### one epoch = one forward pass and one backward pass of all the training examples \n\n#### batch size = the number of training examples in one forward\/backward pass. The higher the batch size, the more memory space you'll need. In the second stage we apply model on test set to make sure of its performance in recognizing letters in data other from data it was trained on.\n\n### Our strategy is a pair of convolutional layers after that a max pooling layer and dropout layer, we repeat that 2 times, then we has a dense layer to make the output consist of the probabilities of each of 28 letters for an image.","7afea0c2":"This gives better result in the final epoch 95.6% accuracy.","4e544d69":"the correct label","2cc76f3d":"the incorrect label","0be932ba":"since labels are recorded as integers, we need to encode them as categories , as in the following line","dc1232f4":"the correct label","a9cd6615":"Let\u201fs check performance against test set we run the model on test set for 10 times and get their average","2889c76a":"### then we try building our first neural network to learn from train data","0d34d81f":"then we define the optimizer , we use RMSprop","4d9551fd":"### Importing necessary liberaries","4b14a8a5":"then we fit the recognizer to the training data for 30 epochs","b5a10bf0":"let's check how the modified model performs aganist test set ","e17b6e23":"the incorrect label","23726e14":"so we have each of the 28 letters written for 480 times in training data so we have (480*28=13440) training case","0b3f38d9":"We see that the average of 10 runs on the test set is 95.5% accuracy which is more by 0.06% than 94.9%, the score they got in the benchmark paper.","77e04181":"#### checking that labels became one dimensioned\n","49af19b9":"so we find that each of 28 arabic letters has a column that has a binary labeling ","e80317c8":"### let's check another example of mis classified letters","e5935149":"so we have the model with accuracy score of 92.5% on the training data by the 30 th Epoch","1c2047e9":"#### then we visualize the different hand writings for some letters to demonestrate the difference in hand writings of the same letter","798ee82a":"### then we select the non correctly classified images by taking the character that had the largest probability for each image","d898aeb6":"# Recognizing Arabic Letters\n\nwith some modifications  on the kernel \n\nhttps:\/\/www.kaggle.com\/bbloggsbott\/understanding-convolutional-neural-network","31161108":"#### from this second plot , we see the letter \"Haa \u062d\" that has the label [5] but it was misclassified as the letter\"Jeem \u062c\" that has the label [4] as we see , difference is only in the dot on the right of the letter, this suggests that we may need to add more convolutional layers to detect such small details ","4e5742e3":"we make labels as categories","25df7835":"####  Assessing Data","9afc034e":"#### checking the data type of labels","89cde8dd":"so we have the average of 95.2 % accuracy for 10 runs on the test set ,but when we look at the model referrenced in dataset page , we see it has result of 94.9% accuracy on the test set, so we must have an improvement on our model","58f987f0":"#### we see that is is the letter \"Thaa \u062b\" which is the letter with the label [3] but we see it was classified with the label [19] which belongs to the letter \"Faa \u0641\"  as we can see the model mis-read the 3 dots above the letter and considered them the closed curve of the letter \"\u0641\" as we can see the letters seem like to each other , as well hand writing is not so clear","60055236":"### let's take the first misclassified letter","86d8f9f6":"#### we notice that each of the eight letters differs from a person to an other like in the letter \"Dal \u062f\" in the bottom right , we see difference can occure in the angle of the letter or in the placing of the dot , as we will prove later.","acec6635":"#### we need to rescale data pixels from 1 to zero as preferred for processing in convolutional neural networks \n#### also we need to add some rotation since in the real life letters are expected not only to appear in the midle of the sheet , we need our model to recognize letters wherever they are placed in the sheet","36729de2":"### then we select mis classified images and , their real labels , and their misclassified labels  ","0a818bfa":"### Now we need to know what makes model perform better, what we need to get better classification\n\n#### for this purpose , we run model on the test set another time plotting a sample of  letters that were mis classified ","4a17655e":"#### Collecting Data","8d585aa7":"we check the structure of label data","8db8a1c4":"#### then we make labels one dimensioned \n\n","e3fa4889":"### Data preprocessing"}}