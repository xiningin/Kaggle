{"cell_type":{"1f839079":"code","5e4beff8":"code","86eee39d":"code","ca16f89e":"code","cc0d5f3b":"code","58eaf051":"code","fba9cb65":"code","8981a0ba":"code","df44a025":"code","2a5004d1":"code","6099f240":"code","7bedd6da":"code","55d43435":"code","e69c2ee2":"code","19b50a33":"code","44b4f495":"code","0aa3527e":"code","1897cd3d":"code","861c61de":"code","9824ecce":"code","2dbe3d83":"code","e1a74a1b":"code","5b30e214":"code","8658ab56":"code","438e7993":"code","c8feadd8":"code","32f4f603":"code","f5d07e72":"code","edb4bd8c":"markdown","6a71d10c":"markdown","1e20423c":"markdown","30d0f12b":"markdown","0b418429":"markdown","09e8d1be":"markdown","0a4ba0df":"markdown","e3978f36":"markdown","0ab22de4":"markdown","17621382":"markdown","0ed03c25":"markdown","61ffb92e":"markdown","a7dca726":"markdown","5107d1c1":"markdown","ea946d15":"markdown","b86e4460":"markdown","471fc5c5":"markdown","9a815da0":"markdown","7e2f7dcd":"markdown","28c0edd7":"markdown","f3071814":"markdown","de096791":"markdown","d2cd2a44":"markdown","4efbe679":"markdown"},"source":{"1f839079":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom ml_metrics import rmse\n\n#importing models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\n#from sklearn.tree import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5e4beff8":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntest2 = pd.read_csv(\"..\/input\/test.csv\")\nhouses = pd.concat([train,test],sort=False)\nprint(train.shape)\nprint(test.shape)","86eee39d":"houses.select_dtypes(include='object').head()","ca16f89e":"houses.select_dtypes(exclude='object').head()","cc0d5f3b":"train.select_dtypes(include='object').isnull().sum()","58eaf051":"train.select_dtypes(exclude='object').isnull().sum()","fba9cb65":"train.drop(['Alley', 'PoolQC','Fence', 'MiscFeature'], axis=1, inplace=True)\ntest.drop(['Alley', 'PoolQC','Fence', 'MiscFeature'], axis=1, inplace=True)\n","8981a0ba":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', \n            'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', \n            'GarageQual', 'GarageCond', 'PavedDrive', 'Electrical','MasVnrType'):\n    train[col]=train[col].fillna('Unknown')\n    test[col]=test[col].fillna('Unknown')\n","df44a025":"print(train.GarageType.isnull().sum())\nprint(test.GarageType.isnull().sum())","2a5004d1":"train['LotFrontage']=train['LotFrontage'].fillna(train.LotFrontage.mean())\ntest['LotFrontage']= test['LotFrontage'].fillna(test.LotFrontage.mean())\ntrain['MasVnrArea']= train['MasVnrArea'].fillna(train.MasVnrArea.mean())\ntest['MasVnrArea']= test['MasVnrArea'].fillna(test.MasVnrArea.mean())\ntrain['GarageYrBlt']= train['GarageYrBlt'].fillna(0)\ntest['GarageYrBlt']= test['GarageYrBlt'].fillna(0)\n\ntest['BsmtFinSF1']= test['BsmtFinSF1'].fillna(test.BsmtFinSF1.mean())\ntest['BsmtFinSF2']= test['BsmtFinSF2'].fillna(test.BsmtFinSF2.mean())\ntest['BsmtUnfSF']= test['BsmtUnfSF'].fillna(test.BsmtUnfSF.mean())\ntest['BsmtFullBath']= test['BsmtFullBath'].fillna(test.BsmtFullBath.mean())\ntest['BsmtHalfBath']= test['BsmtHalfBath'].fillna(test.BsmtHalfBath.mean())\ntest['GarageCars']= test['GarageCars'].fillna(test.GarageCars.mean())\ntest['TotalBsmtSF']= test['TotalBsmtSF'].fillna(test.TotalBsmtSF.mean())\ntest['GarageArea'] = test['GarageArea'].fillna(test.GarageArea.mean())\n\n","6099f240":"\ntest.columns[test.isnull().any()].tolist()\n","7bedd6da":"plt.figure(figsize=[80,40])\nsns.heatmap(train.corr(),annot=True)\n","55d43435":"corr = train.corr()\nmost_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.3]\nplt.figure(figsize=(10,10))\nsns.heatmap(train[most_corr_features].corr(), annot=True, cmap=\"RdYlGn\")","e69c2ee2":"#removing outliers recomended by author\ntrain = train[train['GrLivArea']<4000]","19b50a33":"train.describe()","44b4f495":"print(train.select_dtypes(exclude='object').columns)\n\n#These features are highly correlated with other features and themselves have lower correlation with SalePrice\n#'GarageArea','1stFlrSF','TotRmsAbvGrd','2ndFlrSF'\n","0aa3527e":"features = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold']\nx_train, x_test, y_train, y_test = train_test_split(train[features],train['SalePrice'], train_size = 0.8, \n                                                    test_size = 0.2, random_state=3)\nlm = LinearRegression()\nlm.fit(x_train, y_train)\nprint(lm.score(x_train, y_train))\nprint(lm.score(x_test, y_test))","1897cd3d":"most_corr_features = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', 'GrLivArea',\n       'FullBath', 'Fireplaces', 'GarageCars']\nsteps = [\n   ('scalar', StandardScaler()),\n   ('poly', PolynomialFeatures(degree=3)),\n   ('model', Lasso(alpha=1000, fit_intercept=True))\n]\npipeline = Pipeline(steps)\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(train[most_corr_features], train[\"SalePrice\"], train_size=0.8, test_size=0.2, random_state=3)\n\npipeline.fit(x_train2, y_train2)\nprint(pipeline.score(x_train2, y_train2))\nprint(pipeline.score(x_test2, y_test2))\n","861c61de":"len_train=train.shape[0]\nhouses=pd.concat([train,test], sort=False)\n\nfor col in ('MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional', 'SaleType'):\n    houses[col]=houses[col].fillna('Unknown')\n\n\n","9824ecce":"houses[\"MSSubClass\"] = houses[\"MSSubClass\"].apply(str)\n#houses[\"OverallCond\"] = houses[\"OverallCond\"].apply(str)\n\nhouses[\"MoSold\"] = houses[\"MoSold\"].apply(str)\n","2dbe3d83":"#add any new features you make over here.\n#houses[\"yearDiff\"] = houses[\"YearRemodAdd\"] - houses[\"YearBuilt\"]\n#houses[\"IsRemod\"] = houses.apply(lambda row: 0 if row['yearDiff'] == 0 else 1,axis=1)\n#houses[\"HasGarage\"] = houses.apply(lambda row: 0 if row['GarageArea'] == 0 else 1,axis=1)\nhouses[\"YrSold\"] = houses[\"YrSold\"].apply(str)\n","e1a74a1b":"#houses.drop(\"yearDiff\", axis=1, inplace=True)\nhouses.drop(\"GarageArea\", axis=1, inplace=True)\n\nhouses=pd.get_dummies(houses)\ntrain=houses[:len_train]\ntest=houses[len_train:]\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\ntrain['SalePrice']=np.log(train['SalePrice'])*100\nx=train.drop('SalePrice', axis=1)\ny=train['SalePrice']\ntest=test.drop('SalePrice', axis=1)","5b30e214":"\nsteps = [\n   \n   ('poly', PolynomialFeatures(degree=1)),\n   ('model', Lasso(alpha=0.15, fit_intercept=True))\n]\npipeline = Pipeline(steps)\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=3)\n\npipeline.fit(x_train2, y_train2)\n#print(pipeline.score(x_train2, y_train2))\n#print(pipeline.score(x_test2, y_test2))\nprint(rmse(actual=y_test2\/100, predicted=pipeline.predict(x_test2)\/100))","8658ab56":"#regressor = DecisionTreeRegressor(random_state = 0)\n#regressor.fit(x_train2, y_train2)\n#print(rmse(actual=y_test2\/100, predicted=regressor.predict(x_test2)\/100))\n#print(rmse(actual=y_test2\/100, predicted=(pipeline.predict(x_test2) + regressor.predict(x_test2))\/200))\n#train[\"predictions\"] = (pipeline.predict(x)\/100)\n#train[\"error\"] = train.apply(lambda row: row[\"predictions\"] - (row[\"SalePrice\"]\/100), axis=1)\n","438e7993":"#model = SVR(epsilon=0.01, kernel='poly')\n#print(model)\n#model.fit(x_train2, y_train2)\n#print(rmse(actual=y_test2\/100, predicted=model.predict(x_test2)\/100))","c8feadd8":"predictions1 = pipeline.predict(test)","32f4f603":"params = {\n    'n_estimators': 300,\n    'max_depth': 3,\n    'learning_rate': 0.1,\n    'criterion': 'mae',\n    'min_impurity_decrease': 0.005,\n    'loss': 'huber',\n    'alpha': 0.99\n}\n\ngradient_boosting_regressor = GradientBoostingRegressor(**params)\ngradient_boosting_regressor.fit(x_train2, y_train2)\nprint(rmse(actual=y_test2\/100, predicted=gradient_boosting_regressor.predict(x_test2)\/100))","f5d07e72":"predictions2 = gradient_boosting_regressor.predict(test)\npredictions = np.exp((predictions1 + predictions2)\/200)\nmy_sub = pd.DataFrame({'Id':test2.Id, 'SalePrice':predictions})\nmy_sub.to_csv('submission.csv', index = False)\n","edb4bd8c":"Let's make sure nothing null is left","6a71d10c":"filling categorical features' missing values with Unknown as it may still have some data hidden.","1e20423c":"Let's look at categorical and numerical features separately","30d0f12b":"tried many derived features but they didn't add anything. Converting year sold to string as well.","0b418429":"Tried out decision tree regressor but it did much worse.","09e8d1be":"Getting to understand correlations with price for for various variables by plotting correlation heatmap","0a4ba0df":"plotting heatmap of most correlated features which look useful.\nThe value of 0.3 was chosen after trying out numerous values.","e3978f36":"Tried the gradient boosting model and it does very well after training the hyper-parameters. It does almost as good as the lasso model, so now we will try ensembling the two models.","0ab22de4":"fitting our final lasso model after tuning our hyper parameters.\nAlso, we are now going to predict the log of Sale Price as that is what is being used in the evaluation.","17621382":"It's time to build the model","0ed03c25":"trying out lasso model. Lasso model is better than plain linear regression.","61ffb92e":"garage area is making the model worse so we will remove it. Also for all the categorical features we have done one hot encoding.","a7dca726":"making the first set of predictions using lasso.","5107d1c1":"Making sure all categorical missing values have been filled","ea946d15":"BsmtQual, BsmtCond, BsmtExposure,BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, Garagequal, GarageCond, PavedDrive are other categorical variables missing some values. Let's replace. ","b86e4460":"Taking average of gradient boosting and lasso predictions to give the final predictions.","471fc5c5":"Now lets handle missing values of numerical features","9a815da0":"basic linear regression. The r2 score is not very good","7e2f7dcd":"In this kernel, we will do:\n* Basic data exploration\n* Trying to create new derived features\n* Handle missing values and use one hot encoding for the categorical variables.\n* Try out various models-\n    1. Linear Regression\n    2. Lasso\n    3. Decision Tree Regressor\n    4. SVR\n    5. Gradient Boosting Regressor\n* Use ensembling for the final output.","28c0edd7":"importing all the data","f3071814":"Looks like MSSubclass is actually a categorical variable and should be treated accordingly\n\nLet's look at null values first and understand deapth of data\nCategorical first","de096791":"tried svr also but it did much worse. Also using the poly kernel takes too long to run.","d2cd2a44":"converting mosold and subclass to categorical as it doesn't make sense to keep them linear.","4efbe679":"Looks like we have most of the data missing for following categorical variables. Probably it is better to get rid of these columns as they might not have predictive power. Let's remove.\nAlley, PoolQC, Fense, MiscFeature"}}