{"cell_type":{"c8fd4dc7":"code","d3c921f2":"code","2c72e59d":"code","0c95b8db":"code","6fbe8da5":"code","a791ec72":"code","ceba6b7d":"code","4f3cb7d7":"code","6f07b3ce":"code","3c664648":"code","ecd94dd2":"code","57ada685":"code","8924e7eb":"code","f00fc39b":"code","63e0ed92":"code","6114ab86":"code","a81a7c8b":"code","372978a0":"code","566b981b":"code","9d5e5f5a":"code","ade42cc7":"code","73f3f495":"code","4422fd15":"code","5e57531b":"code","b51b23f4":"code","02ac69ce":"code","f14fe2c3":"code","e5562b70":"code","1bbc7d7f":"code","b0bd87de":"code","738d433f":"code","f40ee1fd":"code","3324795e":"code","6037c594":"code","02559df1":"code","81e85a4a":"code","f1020483":"code","cea0f8b3":"code","1f8ec1c5":"code","8046fd06":"markdown","c25ec313":"markdown","88108c52":"markdown","81b23d8d":"markdown","d4f6bf3b":"markdown","7b345682":"markdown","d7f62130":"markdown","f93655b8":"markdown","36333bba":"markdown","72289af1":"markdown","69d01632":"markdown","f644974f":"markdown","30744cd0":"markdown","da2ae62d":"markdown","f74df35c":"markdown","f6c4ff08":"markdown","8695cfdf":"markdown","7fc3770d":"markdown","6253a89b":"markdown","e43408f0":"markdown","5b2090a5":"markdown","ab165032":"markdown","e7108268":"markdown","42409da3":"markdown","82a5aeb7":"markdown","6ebccd7f":"markdown","11635ca1":"markdown","d470ae76":"markdown","ae9844f3":"markdown","c476cf98":"markdown","e82c1f7c":"markdown","0e63784b":"markdown","aa1831ce":"markdown","08f08f4d":"markdown"},"source":{"c8fd4dc7":"#libraries\nimport numpy as np \nimport pandas as pd \n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Let's be rebels and ignore warnings for now\nimport warnings\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3c921f2":"#uploading our datasets into the notebook ( datasets were already given )\nimport pandas as pd\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ngender_submission= pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","2c72e59d":"#View the train dataset. Head function selects first 5 lines\ntrain.head()","0c95b8db":"#View the test dataset\ntest.head()","6fbe8da5":"gender_submission.head()","a791ec72":"len(train)","ceba6b7d":"len(test)","4f3cb7d7":"\n#calculating missing value rate\ntrain.isnull().mean().round(4) * 100","6f07b3ce":"missingno.matrix(train, figsize=(30,5))","3c664648":"train[\"Age\"].fillna(train[\"Age\"].mean(), inplace=True)\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace=True)","ecd94dd2":"#Survived people comparison\nfig= plt.figure(figsize = (20,1))\n#countplot show the total amount of numbers\nsns.countplot(y=\"Survived\", data=train)\n","57ada685":"#distribution by gender \nfig= plt.figure(figsize = (20,1))\nsns.countplot(y=\"Sex\", data=train)","8924e7eb":"#graph\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#% of men survived\nmen = train.loc[train.Sex == 'male'][\"Survived\"]\nrate_men = round((sum(men)\/len(men))*100,2)\nprint(\"% of men who survived:\", rate_men)\n\n#% of women survived \nwomen = train.loc[train.Sex==\"female\"][\"Survived\"]\nrate_women=round((sum(women)\/len(women))*100,2)\nprint(\"% of women survived\", rate_women)","f00fc39b":"#graph\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n#Survival ratio based on Pclass\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","63e0ed92":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6114ab86":"train.Age.plot.hist()","a81a7c8b":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","372978a0":"#Bins are like group filters and labels are the names of the groups\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()\n","566b981b":"#Same thing what we did with Pclass \nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train)\nprint(\"Percentage of Embarked = S who survived:\", train[\"Survived\"][train[\"Embarked\"] == \"S\"].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Embarked = Q who survived:\", train[\"Survived\"][train[\"Embarked\"] == \"Q\"].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Embarked = C who survived:\", train[\"Survived\"][train[\"Embarked\"] == \"C\"].value_counts(normalize = True)[1]*100)\n","9d5e5f5a":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","ade42cc7":"bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['0', '1', '2', '3', '4', '5', '6', '7']\ntrain['AgeGroupNumber'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroupNumber'] = pd.cut(test[\"Age\"], bins, labels = labels)\ntest.head()","73f3f495":"\nNames=train.Name.str.split(\" \").map(lambda x: x[1])\nNames_2=test.Name.str.split(\" \").map(lambda x: x[1])\n# If you want to see all unique values type this - Names.value_counts()\ntrain[\"Title\"]=Names\ntest[\"Title\"]=Names_2\n\n","4422fd15":"train[\"Title\"] = train[\"Title\"].replace(['Ms.','Mrs','Mulder,','Capt.','Pelsmaeker,','Cruyssen,','Walle,','Don.','Jonkheer.','Melkebeke,','der',\n                                        'Velde,','Messemaeker,','Billiard,','Shawah,','the','Steen,','Carlo,','Gordon,', \n                                        'Mlle.','Major.','Col.', 'Impe,', 'Planke,','y','Rev.', 'Rev.','Mme.'],\n                                        'Rare')\ntest[\"Title\"] = test[\"Title\"].replace(['Ms.','Mrs','Mulder,','Capt.','Pelsmaeker,','Cruyssen,','Walle,','Don.','Jonkheer.','Melkebeke,','der',\n                                        'Velde,','Messemaeker,','Billiard,','Shawah,','the','Steen,','Carlo,','Gordon,', \n                                        'Mlle.','Major.','Col.', 'Impe,', 'Planke,','y','Rev.', 'Rev.','Mme.'],\n                                        'Rare')\n#Changing words into numbers\ntitle_mapping = {\"Mr.\": 0, \"Miss.\": 1, \"Mrs.\": 2,\"Rare.\": 3,\"Master\": 4,\"Dr.\": 5, }\ntrain['TitleGroup'] = train['Title'].map(title_mapping)\ntest['TitleGroup'] = test['Title'].map(title_mapping)","5e57531b":"#Changing words into numbers \nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['SexGroup'] = train['Sex'].map(sex_mapping)\ntest[\"SexGroup\"] = test[\"Sex\"].map(sex_mapping)\ntest.head()","b51b23f4":"#Changing words into numbers \nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\" : 2}\ntrain[\"EmbarkedGroup\"]=train[\"Embarked\"].map(embarked_mapping)\ntest[\"EmbarkedGroup\"]=test[\"Embarked\"].map(embarked_mapping)\ntest.head()","02ac69ce":"fare=train[\"Fare\"]\nmax(fare)","f14fe2c3":"bins = [0, 10, 50, 100, 200, 550, np.inf]\nlabels = ['0', '1', '2', '3', '4', '5']\ntrain['FareGroup'] = pd.cut(train[\"Fare\"], bins, labels = labels)\ntest['FareGroup'] = pd.cut(test[\"Fare\"], bins, labels = labels)\n","e5562b70":"train=train.drop([\"Name\",'Sex','Age','Ticket','Fare','Cabin','Embarked','AgeGroup','Title'], axis=1)\ntest=test.drop([\"Name\",'Sex','Age','Ticket','Fare','Cabin','Embarked','AgeGroup','Title'], axis=1)\ntest.head()","1bbc7d7f":"train.isnull().mean().round(4)*100","b0bd87de":"df=train[\"FareGroup\"]\ndf1=test[\"FareGroup\"]\ntrain[\"FareGroup\"] = df.fillna(method = 'ffill') \ntest[\"FareGroup\"] = df1.fillna(method = 'ffill') \n\nEf=train[\"TitleGroup\"]\nEf1=test[\"TitleGroup\"]\ntest[\"TitleGroup\"] = df.fillna(method = 'ffill')\ntrain[\"TitleGroup\"] = df.fillna(method = 'ffill')\n\nKf=train[\"EmbarkedGroup\"]\nKf1=test[\"EmbarkedGroup\"]\ntest[\"EmbarkedGroup\"] = df.fillna(method = 'ffill')\ntrain[\"EmbarkedGroup\"] = df.fillna(method = 'ffill')\ntrain.isnull().mean().round(4)*100","738d433f":"x = np.array(train[\"TitleGroup\"])\n\ny=np.array(train[\"EmbarkedGroup\"])\ntrain[\"TitleGroup\"]=x.astype(int)\ntrain[\"EmbarkedGroup\"]=y.astype(int)\ntrain[\"AgeGroupNumber\"]=train[\"AgeGroupNumber\"].astype(int)\ntrain[\"FareGroup\"]=train[\"FareGroup\"].astype(int)\n\nx1 = np.array(test[\"TitleGroup\"])\ny1=np.array(test[\"EmbarkedGroup\"])\ntest[\"TitleGroup\"]=x1.astype(int)\ntest[\"EmbarkedGroup\"]=y1.astype(int)\ntest[\"AgeGroupNumber\"]=train[\"AgeGroupNumber\"].astype(int)\ntest[\"FareGroup\"]=train[\"FareGroup\"].astype(int)\n\n\n","f40ee1fd":"#Splitting our data \nfrom sklearn.model_selection import train_test_split\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","3324795e":"#Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)\n","6037c594":"#Decision Tree Clasifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","02559df1":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","81e85a4a":"#Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","f1020483":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","cea0f8b3":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","1f8ec1c5":"ids = test['PassengerId']\npredictions = logreg.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","8046fd06":"**Age exploration**","c25ec313":"From this chart we can see that there was a lot more males than females","88108c52":"# 1.Import Necessary Libraries","81b23d8d":"SibSp means the number of siblings \n\nSo here are survival ratio based on siblings number\n","d4f6bf3b":"You can see in the Name column there are some repeating words like Mr. Mrs and so on. So this is a great way to group them!","7b345682":"We have some missing data and when it comes to building models there cant be any nann. I wanted to fill in with mean() method but it didnt work so I did it a little bit differently but still easy. ","d7f62130":"# Titanic Survival predictions for begineers\n\nHi! Welcome to my notebook! I hope you will find it useful and easy to understand!\n\nWorkflow stages:\n- Import Necessary Libraries\n- Read In and Explore the Data\n- Data Analysis and visualization\n- Cleaning and preparing data for machine learning\n- Choosing the Best Model\n- Creating Submission File","f93655b8":"Yes it does!","36333bba":"How much values do we have in our datasets?","72289af1":"Survival ratio based on age\n","69d01632":"**How much data is missing? **\n","f644974f":"Because there are a lot of unique values I decided to put all values that have been only used once or twice into one group. (Rare)","30744cd0":"From this analysis we can make some valuable conclusions. \n\nThe passenger is more likely to survive if he is :\n- is in pclass 1\n- is female\n- is child or not older than 30\n- Embarked in c \n- Have one or two siblings \n\nOthervise the passenger has higher chance to die.  \n","da2ae62d":"I decided to put age into groups because that way I can get more useful insights and also Machine learning models will be more accurate. why ? read this - https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114","f74df35c":"# 2.Read In and Explore the Data","f6c4ff08":"Survived = 0, means that the person did not survived and 1 that survived. ","8695cfdf":"Before grouping Fare we need to know what is the highest value","7fc3770d":"# 7)Building models\n\n**First of all we will split our training data to test different models**","6253a89b":"So what we want to do here :\n- Group data that has a lot of values \n- Change words into numbers \n- Fill in missing values \nAgain, for more information I highly recommend reading this article https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114","e43408f0":"# Data cleaning and preparation","5b2090a5":"From this chart we can see that if our model will show that more people survived than didn't then we will know that we did something wrong. ","ab165032":"Fill in missing values using mean() function so we could get more accurate vizualizations but we will get back to data cleaning later","e7108268":"# **Testing Models**\nI will be testing 5 different models:\n- Gaussian Naive Bayes\n- logistic regression model\n- Random Forest Classifier\n- Gradient Boosting Classifier\n- Decision Tree Classifier","42409da3":"Is fare column has some kind of influence for survival?","82a5aeb7":"Also we have mixed data types (float, int and etc)so I thought that it should be good to make them all the same","6ebccd7f":"Survival ratio based on gender\n","11635ca1":"* So we can see that Age column has 19,87% and Cabin column has 77.10% of missing data\n\nLet's visualize it to get more understanding. ","d470ae76":"Survival ratio based on Pclass","ae9844f3":"- We can see that the test set doesn't have a \"Survived\" column so that's what we have to predict using machine learning models\n- In the end our data should look like gender_submission dataset. ","c476cf98":"You can see that test set doesn't have a survived column(that's what we have to predict)\n","e82c1f7c":"# Submmiting results","0e63784b":"**Data exploration**\n\nLet's get a better understading of our data","aa1831ce":"From this we see that cabin column is not even worth analyzing it or filling missing values because there is just to many. So we can drop it and forget that it ever excited.","08f08f4d":"**Dropping columns**\n\nDrop the columns that wont be needed. For example Cabin has too much missing values and Ticket dont give any useful info and all other columns we already have. "}}