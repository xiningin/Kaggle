{"cell_type":{"0d7f4ec8":"code","3d1c4cf1":"code","a4025dca":"code","40c6890d":"code","b0ff0287":"code","6306a32e":"code","2928157f":"code","4561ca06":"code","df4c14dc":"code","23f4eb8f":"code","aa13a13e":"code","8c8367eb":"code","ef8023b5":"code","4ae098d6":"code","c8cd6c25":"code","94c02047":"code","aa793d1e":"code","1b3b9dd5":"code","9abd22fd":"code","be77f7f5":"code","aaf6ebf1":"code","55d12675":"code","46b72aad":"code","0ebab22d":"code","62c48459":"code","6b90479c":"code","c9e7d729":"code","7984ceb2":"code","6b9d97ec":"code","9357319a":"code","ab49ff30":"code","db47dcce":"code","4e25bfae":"code","9b6c5783":"code","107f2956":"code","cb826bdb":"code","efa9d90d":"code","a9cca167":"code","210e37d5":"code","22afc422":"code","ed56ed81":"code","b2b9bb4b":"code","999f6ae9":"code","584d1cb1":"code","af985cb8":"code","5737e7c2":"code","09fa0ee2":"code","0d5bd810":"code","76a226fe":"code","6a64651e":"markdown","28c44fbe":"markdown","0e580e56":"markdown","d4e4d7d1":"markdown","6d9f64a1":"markdown"},"source":{"0d7f4ec8":"import sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\nimport numpy as np # linear algebra\nprint(\"NumPy version: {}\". format(np.__version__))\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nprint(\"pandas version: {}\". format(pd.__version__))\nimport matplotlib # collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d1c4cf1":"# skip the first 3 rows because garbage\ngdp_data = pd.read_csv('..\/input\/gdp2017\/GDP.csv',skiprows = 3)\n# Drop the last column Unnamed\ngdp_data.drop(['Unnamed: 62'],axis=1, inplace=True)\ngdp_data.shape # (264, 62)","a4025dca":"gdp_data.head(3)","40c6890d":"id_vars=['Country Name','Country Code', 'Indicator Name', 'Indicator Code']\ndf = pd.melt(frame=gdp_data, id_vars=id_vars, var_name='year', value_name='GDP') # country_x_2018_forecast\n# df.describe()\ndf['year'] = df['year'].astype(int) # convert from object to int or float\ndf.info() # confirm data types","b0ff0287":"df.shape","6306a32e":"df = df.dropna() # drop rows where GDP is NaN\ndf.shape","2928157f":"df.drop(['Country Code', 'Indicator Name', 'Indicator Code'], axis=1, inplace=True)\ndf.rename(columns={'Country Name':'Country'}, inplace=True)\ndf.head(50)","4561ca06":"values = ['Arab World',\n          'Caribbean small states',\n          'Central Europe and the Baltics',\n          'Early-demographic dividend',\n          'East Asia & Pacific (excluding high income)',\n          'Early-demographic dividend',\n          'East Asia & Pacific',\n          'East Asia & Pacific (IDA & IBRD countries)',\n          'Europe & Central Asia',\n          'Europe & Central Asia (IDA & IBRD countries)',\n          'Europe & Central Asia (excluding high income)',\n          'Euro area',\n          'European Union',\n          'Fragile and conflict affected situations',\n          'Heavily indebted poor countries (HIPC)',\n          'High income',\n          'IBRD only',\n          'IDA & IBRD total',\n          'IDA total',\n          'IDA blend',\n          'IDA only',\n          'Late-demographic dividend',\n          'Latin America and Caribbean',\n          'Latin America & Caribbean',\n          'Latin America & Caribbean (excluding high income)',\n          'Latin America & the Caribbean (IDA & IBRD countries)',\n          'Lower middle income',\n          'Low & middle income',\n          'Middle income',\n          'Middle East & North Africa (IDA & IBRD countries)',\n          'Middle East & North Africa',\n          'Middle East & North Africa (excluding high income)',\n          'North America',\n          'OECD members',\n          'Pacific island small states',\n          'Post-demographic dividend',\n          'Pre-demographic dividend',\n          'South Asia (IDA & IBRD)',\n          'Sub-Saharan Africa (IDA & IBRD countries)',\n          'Sub-Saharan Africa (excluding high income)',\n          'Sub-Saharan Africa',\n          'Small states',\n          'Upper middle income',\n          'World']\nfor i in range(0, 60):\n    for value in values:\n        condition = df[df.Country == value].index\n        df.drop(condition, inplace=True)\n\n# df[df['column name'].map(lambda x: str(x)!=\".\")]\n\n# df.where(m, -df)\ndf.head(50)","df4c14dc":"df.tail(50)","23f4eb8f":"df['Country'].value_counts() # Pre-demographic dividend?","aa13a13e":"df.shape","8c8367eb":"filename = 'GDP_tidy.csv'\ndf.to_csv(filename, index=False)\nprint(\"{} saved\".format(filename))","ef8023b5":"# Adding a default\ncountry = 'United States'\nfilter = df['Country'] != country\ndfus = df.drop(df[filter].index, inplace=False) # filter by country\n# df.shape\ndfus.tail(5)","4ae098d6":"print('UNITED STATES')\nx = dfus[['year']].values\ny = dfus.GDP.values\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"USA Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\nplt.ylim(0) # start at zero\nplt.show()","c8cd6c25":"import matplotlib.pyplot as plt\nfrom scipy import stats\nX = dfus.year\ny = dfus.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP US (2018) = $16,904,994,673,321.25 USD\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\ntitle='GDP US (2018) = ${} USD'.format(y_pred)\nplt.title(title)\nprint('SciPy')\nplt.scatter(X, y) # Scatter Plot\nplt.plot(X, model, color='red') # linestyle='dashed', marker='o', markersize=12, markerfacecolor='blue'\nplt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP US data (1960-2017)'])\nplt.show()","94c02047":"gdpus_pred = y_pred # 2018\ngdpus_pred = gdpus_pred \/ 1000000000000\nround(gdpus_pred, 2)\ngdpus_pred","aa793d1e":"gdpus = y[15297] # 2017\ngdpus = gdpus \/ 1000000000000\nround(gdpus, 2)\ngdpus","1b3b9dd5":"# Adding a default\ncountry = 'China'\nfilter = df['Country'] != country\ndfch = df.drop(df[filter].index, inplace=False) # filter by country\n# df.shape\ndfch.tail(5)","9abd22fd":"print('CHINA')\nx = dfch[['year']].values\ny = dfch.GDP.values\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"CHINA Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\nplt.ylim(0) # start at zero\nplt.show()","be77f7f5":"from scipy import stats\nX = dfch.year\ny = dfch.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP CHINA (2018) = $6,347,500,525,036.9375\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\ntitle='GDP CHINA (2018) = ${}'.format(y_pred)\nplt.title(title)\nprint('SciPy')\nplt.scatter(X, y, color='red') # Scatter Plot\nplt.plot(X, model, color='orange') # linestyle='dashed', marker='o', markersize=12\nplt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP CHINA data (1960-2017)'])\nplt.show()","aaf6ebf1":"gdpch_pred = y_pred # 2018\ngdpch_pred = gdpch_pred \/ 1000000000000\nround(gdpch_pred, 2)\ngdpch_pred","55d12675":"gdpch = y[15086] # 2017\ngdpch = gdpch \/ 1000000000000\nround(gdpch, 2)\ngdpch","46b72aad":"# Adding a default\ncountry = 'Mexico'\nfilter = df['Country'] != country\ndfmx = df.drop(df[filter].index, inplace=False) # filter by country\n# df.shape\ndfmx.tail(5)","0ebab22d":"import sklearn\nfrom sklearn.linear_model import LinearRegression\nprint('MEXICO')\nx = dfmx[['year']].values\ny = dfmx.GDP.values\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"MEXICO Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\nplt.ylim(0) # start at zero\nplt.show()","62c48459":"from scipy import stats\nX = dfmx.year\ny = dfmx.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP Mexico (2018) = $1,131,888,421,568.4062 MXN\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\ntitle='GDP Mexico (2018) = ${} MXN'.format(y_pred)\nplt.title(title)\nprint('SciPy')\nplt.scatter(X, y, color='green') # Scatter Plot\nplt.plot(X, model, color='red') # linestyle='dashed', marker='o', markersize=12\nplt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP Mexico data (1960-2017)'])\nplt.show()","6b90479c":"gdpmx_pred = y_pred # 2018\ngdpmx_pred = gdpmx_pred \/ 1000000000000\nround(gdpmx_pred, 2)\ngdpmx_pred","c9e7d729":"gdpmx = y[15200] # 2017\ngdpmx = gdpmx \/ 1000000000000\nround(gdpmx, 2)\ngdpmx","7984ceb2":"# Fixing random state for reproducibility\nplt.rcdefaults()\nfig, ax = plt.subplots()\ny = ('United States', 'China', 'Mexico')\ny_pos = np.arange(len(y))\nx = (gdpus, gdpch, gdpmx)\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(y)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('GDP')\nax.set_title('GDP per Country 2017')\nfor i, v in enumerate(x):\n    ax.text(v + 1, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","6b9d97ec":"# Fixing random state for reproducibility\nplt.rcdefaults()\nfig, ax = plt.subplots()\ny = ('United States', 'China', 'Mexico')\ny_pos = np.arange(len(y))\nx = (gdpus_pred, gdpch_pred, gdpmx_pred)\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(y)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('GDP')\nax.set_title('GDP per Country 2018')\nfor i, v in enumerate(x):\n    ax.text(v + 1, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","9357319a":"print(\"Skewness: %f\" % dfus['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfus['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfus['GDP'], ax=ax[0])\nsns.boxplot(dfus['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfus['GDP'], plot=ax[2])\nplt.show()","ab49ff30":"dfus['GDP'] = np.log(dfus['GDP'])","db47dcce":"print(\"Skewness: %f\" % dfus['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfus['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfus['GDP'], ax=ax[0])\nsns.boxplot(dfus['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfus['GDP'], plot=ax[2])\nplt.show()","4e25bfae":"print('UNITED STATES')\nx = dfus[['year']].values\ny = dfus.GDP.values\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"USA Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\n# plt.ylim(0) # start at zero\nplt.show()","9b6c5783":"import matplotlib.pyplot as plt\nfrom scipy import stats\nX = dfus.year\ny = dfus.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP US (2018) = $16,904,994,673,321.25 USD\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\nprint('Model Prediction GDP US (2018) = ${} USD'.format(y_pred))\nprint('SciPy')\nplt.scatter(X, y) # Scatter Plot\nplt.plot(X, model, color='red') # linestyle='dashed', marker='o', markersize=12, markerfacecolor='blue'\n# plt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP US data (1960-2017)'])\nplt.show()","107f2956":"gdpus_pred = y_pred # 2018\n# gdpus_pred = gdpus_pred \/ 1000000000000\n# round(gdpus_pred, 2)\ngdpus_pred","cb826bdb":"print(\"Skewness: %f\" % dfch['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfch['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfch['GDP'], ax=ax[0])\nsns.boxplot(dfch['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfch['GDP'], plot=ax[2])\nplt.show()","efa9d90d":"dfch['GDP'] = np.log(dfch['GDP'])","a9cca167":"print(\"Skewness: %f\" % dfch['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfch['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfch['GDP'], ax=ax[0])\nsns.boxplot(dfch['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfch['GDP'], plot=ax[2])\nplt.show()","210e37d5":"print('CHINA')\nx = dfch[['year']].values\ny = dfch.GDP.values\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"CHINA Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\n# plt.ylim(0) # start at zero\nplt.show()","22afc422":"from scipy import stats\nX = dfch.year\ny = dfch.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP CHINA (2018) = $6,347,500,525,036.9375\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\ntitle='GDP CHINA (2018) = ${}'.format(y_pred)\nplt.title(title)\nprint('SciPy')\nplt.scatter(X, y, color='red') # Scatter Plot\nplt.plot(X, model, color='orange') # linestyle='dashed', marker='o', markersize=12\n# plt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP CHINA data (1960-2017)'])\nplt.show()","ed56ed81":"gdpch_pred = y_pred # 2018\n# gdpch_pred = gdpch_pred \/ 1000000000000\n# round(gdpch_pred, 2)\ngdpch_pred","b2b9bb4b":"print(\"Skewness: %f\" % dfmx['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfmx['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfmx['GDP'], ax=ax[0])\nsns.boxplot(dfmx['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfmx['GDP'], plot=ax[2])\nplt.show()","999f6ae9":"dfmx['GDP'] = np.log(dfmx['GDP'])","584d1cb1":"print(\"Skewness: %f\" % dfmx['GDP'].skew())\nprint(\"Kurtosis: %f\" % dfmx['GDP'].kurt())\nimport seaborn as sns\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(dfmx['GDP'], ax=ax[0])\nsns.boxplot(dfmx['GDP'], ax=ax[1])\nfrom scipy import stats\nstats.probplot(dfmx['GDP'], plot=ax[2])\nplt.show()","af985cb8":"import sklearn\nfrom sklearn.linear_model import LinearRegression\nprint('MEXICO')\nx = dfmx[['year']].values\ny = dfmx.GDP.values\nregr = sklearn.linear_model.LinearRegression()\nmodel = regr.fit(x,y) # SciKit-Learn\nscore = regr.score(x, y)\nscore = round(score*100,2)\ntitle = f\"MEXICO Linear Regression Score = {score}\"\nplt.title(title)\nprint('score = {}'.format(score))\ncoef = regr.coef_\nprint('coef = {}'.format(coef)) # 1.0\nintercept = regr.intercept_\nprint('intercept = {}'.format(intercept)) # 3.0000...\ny_pred = model.predict(x)\nprint('SciKit-Learn')\nplt.scatter(x, y, color='gray') # sklearn\nplt.plot(x, y_pred, color='orange') # model\n# plt.ylim(0) # start at zero\nplt.show()","5737e7c2":"from scipy import stats\nX = dfmx.year\ny = dfmx.GDP\nslope, intercept, r, p, std_err = stats.linregress(X, y) # scipy\ndef modelPrediction(x):\n  return slope * x + intercept\n# Model Prediction GDP Mexico (2018) = $1,131,888,421,568.4062 MXN\nmodel = list(map(modelPrediction, X)) # scipy\nx_pred = 2018\ny_pred = modelPrediction(x_pred)\ntitle='GDP Mexico (2018) = ${} MXN'.format(y_pred)\nplt.title(title)\nprint('SciPy')\nplt.scatter(X, y, color='green') # Scatter Plot\nplt.plot(X, model, color='red') # linestyle='dashed', marker='o', markersize=12\n# plt.ylim(ymin=0) # starts at zero\nplt.legend(['Model Prediction using Linear Regression', 'GDP Mexico data (1960-2017)'])\nplt.show()","09fa0ee2":"gdpmx_pred = y_pred # 2018\n# gdpmx_pred = gdpmx_pred \/ 1000000000000\n# round(gdpmx_pred, 2)\ngdpmx_pred","0d5bd810":"x = (gdpus_pred, gdpch_pred, gdpmx_pred)\nx","76a226fe":"plt.rcdefaults()\nfig, ax = plt.subplots()\ny = ('United States', 'China', 'Mexico')\ny_pos = np.arange(len(y))\nx = (gdpus_pred, gdpch_pred, gdpmx_pred)\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(y)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('GDP')\nax.set_title('GDP per Country 2018')\nfor i, v in enumerate(x):\n    ax.text(v + 1, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","6a64651e":"## Transformation\nvariable transformation improve model accuracy using log","28c44fbe":"Conclusion: score = 97% is a great improvement from our previous score which was 93%.","0e580e56":"## Conclusion\n\n[Linear Regression](https:\/\/towardsdatascience.com\/a-summary-of-the-basic-machine-learning-models-e0a65627ecbe) tends to be the Machine Learning algorithm that all teachers explain first, most books start with, and most people end up learning to start their career with.\n\nIt is a very simple algorithm that takes a vector of features (the variables or characteristics of our data) as an input, and gives out a numeric, continuous output. \n\nAs its name and the previous explanation outline, it is a regression algorithm, and the main member and father of the family of linear algorithms where Generalised Linear Models (GLMs) come from.\n\nIt can be trained using a closed form solution, or, as it is normally done in the Machine Learning world, using an iterative optimisation algorithm like Gradient Descent.\n\nLinear Regression is a parametric machine learning model (with a fixed number of parameters that depend on the n\u00ba of features of our data and that trains quite quickly) that works well for data that is linearly correlated with our target variable (the continuous numeric feature that we want to later predict), that is very intuitive to learn, and easy to explain. \n\nIt is what we call an \u2018explainable AI model\u2019, as the predictions it makes are very easy to explain knowing the model weights.\n\nAn example of a Linear Regression model could be a model that predicts house prices taking into account the characteristics of each home like the surface area, location, number of rooms, or if it has an elevator or not.\n\nThe following figure shows how Linear Regression would predict the price of a certain house using only 1 feature: \n\nThe surface area in squared meters of the house. \n\nIn the case of more variables being included in our model, the X axis would reflect a weighted linear combination of these features.\n\nThe line from the previous figure would have been fit in the training process using an optimisation algorithm, like gradient descent, that iteratively changes the slope of the line until the best possible line for our task is obtained.","d4e4d7d1":"Conclusion: score = 93% is good but we can improve it later in the next steps.","6d9f64a1":"Variable transformation is a way to make the data work better in your model. Compare before and after."}}