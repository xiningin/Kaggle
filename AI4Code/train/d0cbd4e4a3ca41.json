{"cell_type":{"8c6fead2":"code","52953509":"code","bbf98bee":"code","82ac50f9":"code","11f84478":"code","e179c408":"code","1d10f1d8":"code","1ecdfcf2":"code","77847a03":"code","4f6360be":"code","1e73f152":"code","a5d6d2d6":"code","0bcc9f71":"code","f75fc287":"code","6ef33934":"code","72fa305e":"code","4025b13b":"code","806b010a":"code","d1ecdea4":"code","3bfaa179":"code","268d3cfe":"code","3d0ddf9c":"code","568f68e8":"code","d986d7c6":"code","99413a02":"code","35130c23":"code","7fbd7c51":"code","6fbdaf06":"code","83dc49c9":"code","fa198d4d":"code","b224b395":"code","f7874745":"code","750b94f3":"code","239c4e76":"code","25ebd345":"code","890f39dc":"code","cb56399a":"code","9361a65e":"code","cc0a2090":"code","a90ffea7":"code","46184679":"code","b96bd3d0":"code","51d0ad5f":"code","b9f6e140":"code","f711e023":"code","f95bcad1":"code","ccdead36":"code","4f9ebd8c":"code","d203687f":"code","9e5123e7":"code","fca70a0b":"code","f7e7dd89":"code","dbebd0a1":"code","e78f1d5d":"code","00a43c45":"code","103319a7":"code","efda29fd":"code","4a6c1e8c":"code","b98f9654":"code","d8688212":"code","417dbe80":"code","85df324e":"code","f9123d5c":"code","16c6cb9e":"code","bc2dd992":"code","db9541e5":"code","76f3bf47":"code","04a577de":"code","d349dc8c":"code","69e2f679":"code","713e1fcd":"code","16641327":"code","7f0fdbc7":"code","666349d3":"code","f2e897aa":"code","be6d9c22":"code","dd8446aa":"code","8602a4de":"code","2c99564d":"code","1c3cf97a":"markdown","78ef5408":"markdown","8edbcc04":"markdown","bc78acaf":"markdown","36e4aa88":"markdown","e7638eb4":"markdown","1958243d":"markdown","37e83479":"markdown","ca2d95a7":"markdown","51527012":"markdown","1a8206ae":"markdown","a737916f":"markdown","0339b8ae":"markdown","ed9096e6":"markdown","50d339c9":"markdown","748e4c25":"markdown","4533c33b":"markdown","52cf1073":"markdown","aa1c3a88":"markdown","c16f8a1b":"markdown","db0625ee":"markdown","02dc9147":"markdown","3b463322":"markdown","cd5cae07":"markdown","15dfb334":"markdown","7037ec50":"markdown","a85aad41":"markdown","fc7b9624":"markdown"},"source":{"8c6fead2":"import pandas as pd\nimport numpy as np\nimport gc, warnings\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize \n\nimport numpy as np                   # Multi-dimensional array object\nimport pandas as pd                  # Data Manipulation\nimport seaborn as sns                # Data Visualization\nimport matplotlib.pyplot as plt      # Data Visualization\nimport plotly.express as px          # Interactive Data Visualization\n\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot # Offline version of the Plotly modules.\n\n","52953509":"DATA_FOLDER = '..\/input\/competitive-data-science-predict-future-sales\/'\n\ntransactions    = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\nitems           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\n\ntest            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\nsubmission      = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))","bbf98bee":"items.shape","82ac50f9":"items.isnull().sum()","11f84478":"print(items.item_category_id.unique())\nprint(\" \")\nprint(\"Number of Unique ID : \", len(items.item_category_id.unique()))","e179c408":"import plotly.express as px                                          # Interactive Data Visualization\n\nfig = px.histogram(items, x = \"item_category_id\",\n                  labels = {\"item_category_id\":\"Category_id\"},\n                  title = \"Item_Category_Id\",\n                  #color_discrete_sequence = [\"Blue\"]\n                  )\n                  \nfig.show()","1d10f1d8":"item_categories.head()","1ecdfcf2":"shops.head()","77847a03":"submission.head()","4f6360be":"transactions","1e73f152":"print(\"----------Top-5- Record----------\")\nprint(transactions.head(5))\nprint()\nprint(\"-----------Information-----------\")\nprint(transactions.info())\nprint()\nprint(\"-----------Data Types-----------\")\nprint(transactions.dtypes)\nprint()\nprint(\"----------Missing value-----------\")\nprint(transactions.isnull().sum())\nprint()\nprint(\"----------Null value-----------\")\nprint(transactions.isna().sum())\nprint()\nprint(\"----------Shape of Data----------\")\nprint(transactions.shape)","a5d6d2d6":"# Number of Duplicated Row\nprint('Number of duplicates:', len(transactions[transactions.duplicated()]))","0bcc9f71":"test","f75fc287":"# create all date from 2013.01.01 to 2015.10.31\nfull_period = []\n\n# create period from 2013.01.01 to 2014.01.31\nfor year in range(2013, 2015):\n    \n    for month in range(1, 13):\n        \n        for day in range(1, 32):\n            \n            full_period.append(str(day)+\".\"+str(month)+\".\"+str(year))\n\n# create period from 2015.01.01 to 2015.10.31\nfor year in range(2015, 2016):\n    \n    for month in range(1, 11):\n        \n        for day in range(1, 32):\n            \n            full_period.append(str(day)+\".\"+str(month)+\".\"+str(year))\n            \nday_list = [\"31.2.2013\", \"30.2.2013\", \"29.2.2013\", \"31.4.2013\", \"31.6.2013\", \"31.9.2013\", \"31.11.2013\",\n            \"31.2.2014\", \"30.2.2014\", \"29.2.2014\", \"31.4.2014\", \"31.6.2014\", \"31.9.2014\", \"31.11.2014\",\n            \"31.2.2015\", \"30.2.2015\", \"29.2.2015\", \"31.4.2015\", \"31.6.2015\", \"31.9.2015\",]\n\nfor w in day_list:\n\n    full_period.remove(w)","6ef33934":"Russian_public_holiday = pd.DataFrame()\n\nFull_TimeFrame = pd.DataFrame()\n\n# List of all public hoilday between period 2013.01.01 to 2015.10.31\npublic_holiday = ['1.1.2013', '2.1.2013', '3.1.2013', '4.1.2013', '5.1.2013',\n                  '6.1.2013', '7.1.2013', '8.1.2013', '9.1.2013', '10.1.2013',\n                  '21.2.2013', '22.2.2013', '23.2.2013', '6.3.2013', '7.3.2013',\n                  '8.3.2013', '1.5.2013', '2.5.2013', '3.5.2013', '8.5.2013',\n                  '9.5.2013', '10.5.2013', '12.6.2013', '4.11.2013', \n                  '1.1.2014', '2.1.2014', '3.1.2014', '4.1.2014', '5.1.2014',\n                  '6.1.2014', '7.1.2014', '8.1.2014', '9.1.2014', '10.1.2014',\n                  '21.2.2014', '22.2.2014', '23.2.2014', '6.3.2014', '7.3.2014',\n                  '8.3.2014', '1.5.2014', '2.5.2014', '3.5.2014', '8.5.2014',\n                  '9.5.2014', '10.5.2014', '12.6.2014', '4.11.2014', \n                  '1.1.2015', '2.1.2015', '3.1.2015', '4.1.2015', '5.1.2015',\n                  '6.1.2015', '7.1.2015', '8.1.2015', '9.1.2015', '10.1.2015',\n                  '21.2.2015', '22.2.2015', '23.2.2015', '6.3.2015', '7.3.2015',\n                  '8.3.2015', '1.5.2015', '2.5.2015', '3.5.2015', '8.5.2015',\n                  '9.5.2015', '10.5.2015', '12.6.2015', \n                 ]\n\nFull_TimeFrame['date'] = full_period\n\nRussian_public_holiday['date'] = public_holiday\n\nRussian_public_holiday['public_holiday'] = 1\n\n# Merge item_category_id with main dataframe\nFull_df = pd.merge(Full_TimeFrame, Russian_public_holiday, on=['date'], how='outer')\n","72fa305e":"Full_TimeFrame","4025b13b":"Russian_public_holiday","806b010a":"# Fill up non public_holiday to 0.0\nFull_df = Full_df.fillna(0.0)\n\nFull_df['year'] = pd.DatetimeIndex(Full_df['date']).year\nFull_df['month'] = pd.DatetimeIndex(Full_df['date']).month\nFull_df['day'] = pd.DatetimeIndex(Full_df['date']).day\n\nFull_df.drop(['date'], axis=1, inplace=True)\n\nFull_df","d1ecdea4":"transactions['year'] = pd.DatetimeIndex(transactions['date']).year\ntransactions['month'] = pd.DatetimeIndex(transactions['date']).month\ntransactions['day'] = pd.DatetimeIndex(transactions['date']).day","3bfaa179":"transactions.head()","268d3cfe":"# Merge transactions with Full_df dataframe\ntransactions_with_public_holiday = pd.merge(transactions, Full_df, on=['year', 'month', 'day'],how='inner')","3d0ddf9c":"transactions_with_public_holiday.drop(['date'], axis=1, inplace=True)","568f68e8":"transactions_with_public_holiday.isnull().sum()","d986d7c6":"transactions_with_public_holiday['revenue'] = transactions_with_public_holiday['item_price'] * transactions_with_public_holiday['item_cnt_day']","99413a02":"transactions_with_public_holiday","35130c23":"Average_item_price = transactions_with_public_holiday.groupby(['shop_id','item_id'])['item_price'].mean().reset_index()\n\nAverage_item_price = Average_item_price.rename(columns={'item_price':'average_item_price'}, inplace=False)","7fbd7c51":"transactions_with_average_item_price = pd.merge(transactions_with_public_holiday, Average_item_price,\n                                                on=['shop_id','item_id'],\n                                                how='inner'\n                                               )","6fbdaf06":"transactions_with_average_item_price['item_price_changed'] = transactions_with_average_item_price['item_price'] - transactions_with_average_item_price['average_item_price']","83dc49c9":"transactions_with_average_item_price","fa198d4d":"sales_by_item_shop_id = transactions.pivot_table(index=['item_id','shop_id'],  # x-axis: item_id\n                                            values=['item_cnt_day'],      # y-axis: month\n                                            columns='date_block_num',     # x & y Intersection: Sale\n                                            aggfunc=np.sum,               # Sum\n                                            fill_value=0\n                                           ).reset_index()\n\nsales_by_item_shop_id.columns = sales_by_item_shop_id.columns.droplevel().map(str)\n\nsales_by_item_shop_id = sales_by_item_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\n\nsales_by_item_shop_id.columns.values[0] = 'item_id'\n\nsales_by_item_shop_id.columns.values[1] = 'shop_id'\n\nsales_by_item_shop_id","b224b395":"def downcast_dtypes(df):\n    \n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    \n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    \n    df[float_cols] = df[float_cols].astype(np.float32)\n    \n    df[int_cols] = df[int_cols].astype(np.int16)\n    \n    return df\n\ntransactions = downcast_dtypes(transactions)\n\nprint(transactions.info())","f7874745":"sales_by_item_shop_id.sum()[2:].plot(legend=True, label=\"Monthly sum\")","750b94f3":"sales_by_item_shop_id.mean()[2:].plot(legend=True, label=\"Monthly mean\")","239c4e76":"# Number of outdated item in training set\noutdated_items_and_shop = sales_by_item_shop_id[sales_by_item_shop_id.loc[:,'27':].sum(axis=1)==0]\n\nprint('Number of Zero Shop & Sale Item since month 27 :', len(outdated_items_and_shop))","25ebd345":"outdated_items_and_shop","890f39dc":"outdated_items_and_shop['id_shop_item'] = outdated_items_and_shop['shop_id'].astype(str).add('_').add(outdated_items_and_shop['item_id'].astype(str))\n\noutdated_items_and_shop","cb56399a":"# Number of outdated item and shop in test set (no sales for the last 6 months)\n\ntest['id_shop_item'] = test['shop_id'].astype(str).add('_').add(test['item_id'].astype(str))\n\nprint('Number of outdate item in test set :', len(test[test['id_shop_item'].isin(outdated_items_and_shop['id_shop_item'])]))","9361a65e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,4))\n\nplt.xlim(transactions['item_cnt_day'].min(), transactions['item_cnt_day'].max())\n\nsns.boxplot(x = transactions['item_cnt_day'])\n\nprint('Item_id with Sale volume outliers:',transactions['item_id'][transactions['item_cnt_day']>680].unique())\n\n\n\nplt.figure(figsize=(10,4))\n\nplt.xlim(transactions['item_price'].min(), transactions['item_price'].max())\n\nsns.boxplot(x = transactions['item_price'])\n\nprint('Item_id with price outliers:',transactions['item_id'][transactions['item_price']>60000].unique())","cc0a2090":"# Cut off the outliers\ntransactions = transactions[transactions.item_price < 60000]\n\ntransactions = transactions[transactions.item_cnt_day < 680]","a90ffea7":"## -1 means a product that has  been returned. We are replacing it for now with the median\ntransactions[transactions['item_price']<0]","46184679":"median = transactions[(transactions.shop_id==32) & (transactions.item_id==2973) & (transactions.date_block_num==4)].item_price.median()\n\ntransactions.loc[transactions.item_price<0, 'item_price'] = median","b96bd3d0":"sales_by_shop_id = transactions.pivot_table(index=['shop_id'],\n                                            values=['item_cnt_day'], \n                                            columns='date_block_num', \n                                            aggfunc=np.sum, \n                                            fill_value=0\n                                           ).reset_index()\n\nsales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)\n\nsales_by_shop_id = sales_by_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\n\nsales_by_shop_id.columns.values[0] = 'shop_id'\n\nfor i in range(6,34):\n    \n    print('Not exists in month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,'0':str(i)].sum(axis=1)==0].unique())\n\nfor i in range(6,28):\n    \n    print('Shop is outdated for month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,str(i):].sum(axis=1)==0].unique())","51d0ad5f":"good_sales = test.merge(transactions, on=['item_id','shop_id'], how='left').dropna()\ngood_pairs = test[test['ID'].isin(good_sales['ID'])]\nno_data_items = test[~(test['item_id'].isin(transactions['item_id']))]\n\nprint('1. Number of good pairs:', len(good_pairs))\nprint('2. No Data Items:', len(no_data_items))\nprint('3. Only Item_id Info:', len(test)-len(no_data_items)-len(good_pairs))","b9f6e140":"from itertools import product\n\nFull_set = []\n\ncols = ['date_block_num','shop_id','item_id']\n\n# Create product of 3 columns into Full_set \nfor i in range(34):\n    \n    month_num = transactions_with_average_item_price[transactions_with_average_item_price.date_block_num == i]   \n    \n    Full_set.append(np.array(list(product([i], month_num.shop_id.unique(), month_num.item_id.unique())), dtype='int16'))\n\n    \n    \n# Convert array into dataframe   \nFull_set = pd.DataFrame(np.vstack(Full_set), columns=cols)\n\nFull_set['date_block_num'] = Full_set['date_block_num'].astype(np.int8)\n\nFull_set['shop_id'] = Full_set['shop_id'].astype(np.int8)\n\nFull_set['item_id'] = Full_set['item_id'].astype(np.int16)\n\nFull_set.sort_values(cols,inplace=True)\n\n","f711e023":"Full_set","f95bcad1":"transactions_with_average_item_price","ccdead36":"# Sum up daily item sold ('item_cnt_day') and merge into Full_set dataframe\ngroup = transactions_with_average_item_price.groupby(['date_block_num',\n                                                'shop_id',\n                                                'item_id'\n                                               ])['item_cnt_day'].sum().reset_index()","4f9ebd8c":"group = group.rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\ngroup","d203687f":"Full_set = pd.merge(Full_set, group, on=cols, how='left')\n\n\n\nFull_set['item_cnt_month'] = (Full_set['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)              # Set 'item_cnt_month' value minimum = 0, max = 20  \n                                .astype(np.float16))     ## in the test set has also been clipped and therefore it is advisable\n\n## set floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. \n## If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs. \n\n","9e5123e7":"test = test.drop(\"id_shop_item\", axis=1)","fca70a0b":"# Merge test into Full_set\ntest['date_block_num'] = 34\n\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\n\ntest['shop_id'] = test['shop_id'].astype(np.int8)\n\ntest['item_id'] = test['item_id'].astype(np.int16)\n\nFull_set = pd.concat([Full_set, test], ignore_index=True, sort=False, keys=cols)\n\nFull_set.fillna(0, inplace=True) # 34th month (prediction month)","f7e7dd89":"Full_set","dbebd0a1":"def lag_feature(df, lags, col):\n    \n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    \n    for i in lags:\n        \n        shifted = tmp.copy()\n        \n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        \n        shifted['date_block_num'] += i\n        \n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        \n    return df","e78f1d5d":"Full_set_lag = lag_feature(Full_set, [1,2,3,6,12], 'item_cnt_month')","00a43c45":"Full_set_lag","103319a7":"# Apply Mean Encoded into 'date_block_num'\ngroup = Full_set_lag.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n\ngroup.columns = [ 'date_avg_item_cnt' ]\n\ngroup.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group, on=['date_block_num'], how='left')\n\nFull_set_lag['date_avg_item_cnt'] = Full_set_lag['date_avg_item_cnt'].astype(np.float16)\n\n# Use defined lag_feature function above\nFull_set_lag = lag_feature(Full_set_lag, [1], 'date_avg_item_cnt')\n\nFull_set_lag.drop(['date_avg_item_cnt'], axis=1, inplace=True)","efda29fd":"# Apply Mean Encoded into 'date_block_num' and 'item_id'\ngroup_1 = Full_set_lag.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n\ngroup_1.columns = [ 'date_item_avg_item_cnt' ]\n\ngroup_1.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group_1, on=['date_block_num', 'item_id'], how='left')\n\nFull_set_lag['date_item_avg_item_cnt'] = Full_set_lag['date_item_avg_item_cnt'].astype(np.float16)\n\n# Use defined lag_feature function above \nFull_set_lag = lag_feature(Full_set_lag, [1,2,3,6,12], 'date_item_avg_item_cnt')\n\nFull_set_lag.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","4a6c1e8c":"# Apply Mean Encoded into 'date_block_num' and 'shop_id'\ngroup_2 = Full_set_lag.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\n\ngroup_2.columns = [ 'date_shop_avg_item_cnt' ]\n\ngroup_2.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group_2, on=['date_block_num', 'shop_id'], how='left')\n\nFull_set_lag['date_shop_avg_item_cnt'] = Full_set_lag['date_shop_avg_item_cnt'].astype(np.float16)\n\n# Use defined lag_feature function above\nFull_set_lag = lag_feature(Full_set_lag, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n\nFull_set_lag.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","b98f9654":"public_holiday = ['1.1.2013', '2.1.2013', '3.1.2013', '4.1.2013', '5.1.2013',\n                  '6.1.2013', '7.1.2013', '8.1.2013', '9.1.2013', '10.1.2013',\n                  '21.2.2013', '22.2.2013', '23.2.2013', '6.3.2013', '7.3.2013',\n                  '8.3.2013', '1.5.2013', '2.5.2013', '3.5.2013', '8.5.2013',\n                  '9.5.2013', '10.5.2013', '12.6.2013', '4.11.2013', \n                  '1.1.2014', '2.1.2014', '3.1.2014', '4.1.2014', '5.1.2014',\n                  '6.1.2014', '7.1.2014', '8.1.2014', '9.1.2014', '10.1.2014',\n                  '21.2.2014', '22.2.2014', '23.2.2014', '6.3.2014', '7.3.2014',\n                  '8.3.2014', '1.5.2014', '2.5.2014', '3.5.2014', '8.5.2014',\n                  '9.5.2014', '10.5.2014', '12.6.2014', '4.11.2014', \n                  '1.1.2015', '2.1.2015', '3.1.2015', '4.1.2015', '5.1.2015',\n                  '6.1.2015', '7.1.2015', '8.1.2015', '9.1.2015', '10.1.2015',\n                  '21.2.2015', '22.2.2015', '23.2.2015', '6.3.2015', '7.3.2015',\n                  '8.3.2015', '1.5.2015', '2.5.2015', '3.5.2015', '8.5.2015',\n                  '9.5.2015', '10.5.2015', '12.6.2015', \n                 ]\n\nholiday_month = [10, 3, 3, 0, 6, 1, 0, 0, 0, 0, 1, 0,\n                 10, 3, 3, 0, 6, 1, 0, 0, 0, 0, 1, 0,\n                 10, 3, 3, 0, 6, 1, 0, 0, 0, 0, 1\n                ] \n\nholiday_num_month = []\n\nfor d in Full_set_lag['date_block_num']:\n    \n    holiday_num_month.append(holiday_month[d])\n\n\nFull_set_lag['holiday_num_month'] = holiday_num_month\n    ","d8688212":"Full_set_lag.head()","417dbe80":"# Apply Mean Encoded into 'date_block_num' and 'shop_id'\ngroup_0 = Full_set_lag.groupby(['date_block_num', 'holiday_num_month']).agg({'item_cnt_month': ['mean']})\n\ngroup_0.columns = [ 'date_holiday_avg_item_cnt' ]\n\ngroup_0.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group_0, on=['date_block_num', 'holiday_num_month'], how='left')\n\nFull_set_lag['date_holiday_avg_item_cnt'] = Full_set_lag['date_holiday_avg_item_cnt'].astype(np.float16)\n\n# Use defined lag_feature function above\nFull_set_lag = lag_feature(Full_set_lag, [1,2,3,6,12], 'date_holiday_avg_item_cnt')\n\nFull_set_lag.drop(['date_holiday_avg_item_cnt'], axis=1, inplace=True)","85df324e":"Full_set_lag","f9123d5c":"## Creating a column to get the average price for each item based on its item_id\ngroup_3 = transactions_with_average_item_price.groupby(['item_id']).agg({'item_price': ['mean']})\n\ngroup_3.columns = ['item_avg_item_price']\n\ngroup_3.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group_3, on=['item_id'], how='left')\n\nFull_set_lag['item_avg_item_price'] = Full_set_lag['item_avg_item_price'].astype(np.float16)\n\n\n\n## Creating a column to get the average price for each item based on grouping by date_block_num and item_id\ngroup_4 = transactions_with_average_item_price.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n\ngroup_4.columns = ['date_item_avg_item_price']\n\ngroup_4.reset_index(inplace=True)\n\nFull_set_lag = pd.merge(Full_set_lag, group_4, on=['date_block_num','item_id'], how='left')\n\nFull_set_lag['date_item_avg_item_price'] = Full_set_lag['date_item_avg_item_price'].astype(np.float16)\n\n## This is an important step because here columns are created for a lag of 1-6 months based on the combination of date_block\n## and item_id. Basically this tell us what was the price last month, last 2 months...upto last 6 months of the given item\n## of a given date block. \nlags = [1,2,3,4,5,6]\n\nFull_set_lag = lag_feature(Full_set_lag, lags, 'date_item_avg_item_price')","16c6cb9e":"for i in lags:\n    \n    Full_set_lag['delta_price_lag_'+str(i)] = \\\n        (Full_set_lag['date_item_avg_item_price_lag_'+str(i)] - Full_set_lag['item_avg_item_price']) \/ Full_set_lag['item_avg_item_price']","bc2dd992":"# Create a new column to understand latest trend of a given product\ndef select_trend(row):\n    \n    for i in lags:\n        \n        if row['delta_price_lag_'+str(i)]:\n            \n            return row['delta_price_lag_'+str(i)]\n        \n    return 0\n\nFull_set_lag['delta_price_lag'] = Full_set_lag.apply(select_trend, axis=1)\n\nFull_set_lag['delta_price_lag'] = Full_set_lag['delta_price_lag'].astype(np.float16)\n\nFull_set_lag['delta_price_lag'].fillna(0, inplace=True)","db9541e5":"# Dropping columns such as date_item_avg_price_lag and delta_price_lag\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n\nfor i in lags:\n    \n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    \n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nFull_set_lag.drop(fetures_to_drop, axis=1, inplace=True)","76f3bf47":"Full_set_lag","04a577de":"transactions_with_average_item_price.head()","d349dc8c":"## Grouping by date_block_num and shop_id and summing the total revenue\ngroup_revenue = transactions_with_average_item_price.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n\ngroup_revenue.columns = ['date_shop_revenue']\n\ngroup_revenue.reset_index(inplace=True)\n\n## Creating a column for the revenue date_shop_revenue\nFull_set_lag = pd.merge(Full_set_lag, group_revenue, on=['date_block_num','shop_id'], how='left')\nFull_set_lag['date_shop_revenue'] = Full_set_lag['date_shop_revenue'].astype(np.float32)\n\n\n## Here we are grouping the revenue values by finding the mean over the shop id\ngroup_revenue = group_revenue.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n\ngroup_revenue.columns = ['shop_avg_revenue']\n\ngroup_revenue.reset_index(inplace=True)\n\n\n## Adding this to the Full_set_lag\nFull_set_lag = pd.merge(Full_set_lag, group_revenue, on=['shop_id'], how='left')\n\nFull_set_lag['shop_avg_revenue'] = Full_set_lag['shop_avg_revenue'].astype(np.float32)\n\n## Doing the same as above that we did for price\nFull_set_lag['delta_revenue'] = (Full_set_lag['date_shop_revenue'] - Full_set_lag['shop_avg_revenue']) \/ Full_set_lag['shop_avg_revenue']\n\nFull_set_lag['delta_revenue'] = Full_set_lag['delta_revenue'].astype(np.float16)\n\n## Adding only lag for the previous month\nFull_set_lag = lag_feature(Full_set_lag, [1], 'delta_revenue')\n\nFull_set_lag.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","69e2f679":"Full_set_lag['month'] = Full_set_lag['date_block_num'] % 12\n\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n\nFull_set_lag['days'] = Full_set_lag['month'].map(days).astype(np.int8)","713e1fcd":"Full_set_lag['item_shop_first_sale'] = Full_set_lag['date_block_num'] - Full_set_lag.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n\nFull_set_lag['item_first_sale'] = Full_set_lag['date_block_num'] - Full_set_lag.groupby('item_id')['date_block_num'].transform('min')","16641327":"Full_set_lag","7f0fdbc7":"# Fill nulls with zeros\n\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nFull_set_lag = fill_na(Full_set_lag)","666349d3":"Full_set_lag.to_pickle('Full_set_lag.pkl')","f2e897aa":"data = pd.read_pickle('Full_set_lag.pkl')","be6d9c22":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\n\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\n\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","dd8446aa":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n","8602a4de":"from xgboost import plot_importance\n\nimport time\nimport sys\nimport gc\nimport pickle\n\n## Note: We are clipping values here because the evaluation rules of the competition had said to clip the values\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission_211115.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","2c99564d":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","1c3cf97a":"## Shop Closed - No Sales for last 6 months","78ef5408":"## 1. Feature Pre-processing\n\n1. Use One-Hot-Vector for categorical features","8edbcc04":"Full_set_lag = Full_set_lag[matrix.date_block_num > 11]","bc78acaf":"## Target Lags\n- Lag is a time series concept where we hope try to get patters of sales from the previous months. \n\n- That is more of like saying does the total items sold in a given month have a pattern to the previous month or 2 months before or a year before.\n\n- Now the question how do we know many months of lag should we take into consideration. This more of a trial and error thing. In SARIMA models we could plot ACF and PACF plots and consider how many months of lag that we need to take. Taking 12 months lag is saying what was the patten the same time the previous year.\n\n- On how to choose lags is more of a trial and error and usually people consider a [1,2,3,6,12] lags period but this is just a wild guess and it is upto the individual to expeirment. \n\nNote: \n- Here we are joining the Full_set dataframe and shifted dataframe by columns of date_block_num,shop_id and item_id. \n- This would lead to a question whether a in a given month the shops and items sold wont be the same when compared to the previous month. \n- This is correct and this would lead to a lot Nan's. We would be dealing with this towards the end and turning them to mostly 0","36e4aa88":"## Apply Mean Encoded Features\n\n1. Average Sales per month\n- The value of the previous months average sales as feature to current month","e7638eb4":"2. Calculate the average number of items that are sold for a given item_id in a given date block","1958243d":"## Let's see how many products and shops are outdated (no sales for the last 6 months)","37e83479":"3. Calculate the average number of items that are sold for a given shop_id in a given date block","ca2d95a7":"## Average Item Price during the Period","51527012":"### Data fields\n\n- ID - an Id that represents a (Shop, Item) tuple within the test set\n- shop_id - unique identifier of a shop\n- item_id - unique identifier of a product\n- item_category_id - unique identifier of item category\n- item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n- item_price - current price of an item\n- date - date in format dd\/mm\/yyyy\n- date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., -October 2015 is 33\n- item_name - name of item\n- shop_name - name of shop\n- item_category_name - name of item category","1a8206ae":"## Months since the first sale for each shop\/item pair and for item only.","a737916f":"## Get the Price Trend\n\n- That is it indicates the ratio of the avg item price (let it be x1) for the last month to the avg item price (x) for the whole set.\n\n- Hence, for the last month trend_feature = (x1 - x)\/x. for example, if last month price is greather then avg price, then the final value is more then zero (positive trend).\n\n- if last month price is lesser than avg price, then the value is less than zero (negative trend). \n\n- Finaly, if last month price is close to avg price then the trend will be close to zero.\n\n**because of the existence of items wich were not sold last month we use lags for date_item_avg_item_price property. Thus, if We cant get last month trend we take a trend for two months (x2-x)\/x and so on.probably the name of the feature 'delta_price_lag' should be 'price_trend'.**","0339b8ae":"## Removing all rows from that are in the date_block_num for the first 11 months","ed9096e6":"## Add number of public holiday for each month as feature","50d339c9":"## Add feature Russian_public_holiday","748e4c25":"## Create Full (Train, Validation, and Test) Set","4533c33b":"## Revenue of Each Transaction","52cf1073":"## To Predict Total sales for every product and store in the next month\n\nWe are provided with daily historical sales data. \n\n***The task is to forecast the total amount of products sold in every shop for the test set.***\n\nNote that the list of shops and products slightly changes ***every month***. Creating a robust model that can handle such situations is part of the challenge.\n\n- Submissions are evaluated by root mean squared error (RMSE). \n- True target values are clipped into [0,20] range.","aa1c3a88":"## Outliers by price and sales volume","c16f8a1b":"## Addition Feature - Number of Days for each month","db0625ee":"## Item Price compare to Average Item Price","02dc9147":"## Analysis Test Set ","3b463322":"## Downcasting ","cd5cae07":"## Last month shop revenue trend\n\n- Calculate the revenue trend and then create a lag for only the previous month.\n- Unlike the previous case where we created it for the last months (1-6). This is obvious because ceach shops latest trend can be seen in the previous month of revenue as there wont be any shops with having a revenue the previous month unlike items that could have been sold the previous month or the one before or upto 6 months.","15dfb334":"## Trend Feature - Price trend for last six months","7037ec50":"## XGBoost","a85aad41":"## Create Table to Sum Up Sale for each item by month","fc7b9624":"4. Calculate the average number of items that are sold for a given public holiday in a given date block"}}