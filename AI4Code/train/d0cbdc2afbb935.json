{"cell_type":{"49a07c36":"code","aa7d0aaf":"code","ee5def7f":"code","79bec1b1":"code","c8c47af2":"code","1f723040":"code","f5ab62f9":"code","5b2d0d97":"code","a23a74b7":"code","f27ecdf8":"code","448b720a":"code","1df279fc":"code","47217f01":"code","fa08f0b2":"code","9eccb7c2":"code","3efb6ae6":"code","902d2bb1":"markdown","9a2beeef":"markdown","0e8fb387":"markdown","8b2010e0":"markdown","357d7896":"markdown","9405d7ec":"markdown","d3f81db4":"markdown","9929c2f0":"markdown","b0759175":"markdown","07839b75":"markdown","905a8545":"markdown","839b11ce":"markdown","02662115":"markdown","cf9efa5e":"markdown","ca003762":"markdown","0ef6c8f0":"markdown"},"source":{"49a07c36":"import torch\n\n\ndef get_device():\n    # \u0415\u0441\u043b\u0438 \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435 \u0435\u0441\u0442\u044c GPU ...\n    if torch.cuda.is_available():\n        # \u0422\u043e\u0433\u0434\u0430 \u0433\u043e\u0432\u043e\u0440\u0438\u043c PyTorch \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c GPU.\n        device = torch.device(\"cuda\")\n        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n        print('We will use the GPU:', torch.cuda.get_device_name(0))\n    # \u0415\u0441\u043b\u0438 \u043d\u0435\u0442 GPU, \u0442\u043e \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043d\u0430 \u043e\u0431\u044b\u0447\u043d\u043e\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0435 ...\n    else:\n        print('No GPU available, using the CPU instead.')\n        device = torch.device(\"cpu\")\n    return device\n\n\ndevice = get_device()","aa7d0aaf":"!pip install wget\n\ndef download_dataset():\n    import wget\n    import os\n    import zipfile\n\n    print('Downloading dataset...')\n    # URL \u0434\u043e zip-\u0444\u0430\u0439\u043b\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442.\n    url = 'https:\/\/nyu-mll.github.io\/CoLA\/cola_public_1.1.zip'\n    out_file = '.\/cola_public_1.1.zip'\n\n    # \u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0444\u0430\u0439\u043b (\u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0435\u0441\u043b\u0438 \u043d\u0435 \u0441\u043a\u0430\u0447\u0430\u043b\u0438 \u0440\u0430\u043d\u044c\u0448\u0435)\n    if not os.path.exists(out_file):\n        wget.download(url, out_file)\n    # Unzip\n    if not os.path.exists('.\/cola_public\/'):\n        with zipfile.ZipFile(out_file, 'r') as zip_ref:\n            zip_ref.extractall(os.path.dirname(out_file))\n    print('Complete')\n\n\ndownload_dataset()","ee5def7f":"def get_sentences_and_labels():\n    import pandas as pd\n\n    # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c dataset \u0432 pandas dataframe.\n    df = pd.read_csv(\".\/cola_public\/raw\/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n\n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0447\u0438\u0441\u043b\u043e \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n    print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n\n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 10 \u0440\u044f\u0434\u043e\u0432 \u0438\u0437 \u0442\u0430\u0431\u043b\u0438\u0447\u043a\u0438.\n    print(df.sample(10))\n\n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c 5 \u0433\u0440\u0430\u043c\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435\u0432\u0435\u0440\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n    print(df.loc[df.label == 0].sample(5)[['sentence', 'label']])\n\n    sentences = df['sentence'].values\n    labels = df['label'].values\n\n    # \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u043a \u043d\u0438\u043c.\n    return sentences, labels\n\n\nsentences, labels = get_sentences_and_labels()","79bec1b1":"from transformers import BertTokenizer\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nsentence_number = 0\n# \u041d\u0430\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435.\nprint('Original:', sentences[sentence_number])\n# \u041d\u0430\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u0431\u0438\u0442\u043e\u0435 \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\nprint('Tokenized: ', tokenizer.tokenize(sentences[sentence_number]))\n# \u041d\u0430\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u0431\u0438\u0442\u043e\u0435 \u043d\u0430 \u043d\u043e\u043c\u0435\u0440\u0430 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[sentence_number])))\n","c8c47af2":"max_len = 0\n# \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u0430\u043a\u043e\u0439 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0438\u043c\u0435\u0435\u0442 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u0431\u0438\u0442\u043e\u0435 \u043d\u0430 \u0442\u043e\u043a\u0435\u043d\u044b \u0438 \u0440\u0430\u0437\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0435 \u0441\u043f\u0435\u0446. \u0442\u043e\u043a\u0435\u043d\u0430\u043c\u0438.\nfor sent in sentences:\n    # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c `[CLS]` \u0438 `[SEP]` \u0442\u043e\u043a\u0435\u043d\u044b.\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c.\n    max_len = max(max_len, len(input_ids))\nprint('Max sentence length: ', max_len)","1f723040":"input_ids, attention_masks = [], []\n\n# \u0414\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439...\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,  # \u0422\u0435\u043a\u0441\u0442 \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438.\n        add_special_tokens=True,  # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c '[CLS]' \u0438 '[SEP]'\n        max_length=64,  # \u0414\u043e\u043f\u043e\u043b\u043d\u044f\u0435\u043c [PAD] \u0438\u043b\u0438 \u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0434\u043e 64 \u0442\u043e\u043a\u0435\u043d\u043e\u0432.\n        pad_to_max_length=True,\n        return_attention_mask=True,  # \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0442\u0430\u043a\u0436\u0435 attn. masks.\n        return_tensors='pt',  # \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 \u0432\u0438\u0434\u0435 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 pytorch.\n    )\n\n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a\n    input_ids.append(encoded_dict['input_ids'])\n    # \u0418 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c attention mask \u0432 \u0441\u043f\u0438\u0441\u043e\u043a\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043a\u0438 \u0432 \u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u044b\u0435 \u0442\u0435\u043d\u0437\u043e\u0440\u044b Pytorch.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# \u041f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0441 \u043d\u043e\u043c\u0435\u0440\u043e\u043c 0, \u0435\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u044b (\u0442\u0435\u043f\u0435\u0440\u044c \u0432 \u0432\u0438\u0434\u0435 \u043d\u043e\u043c\u0435\u0440\u043e\u0432 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435) \u0438.\u0442.\u0434.\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])\nprint('Attention masks:', attention_masks[0])\nprint('Labels:', labels[0])\n","f5ab62f9":"from torch.utils.data import TensorDataset, random_split\n\n# \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043e\u0434\u0438\u043d TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# \u0414\u0435\u043b\u0430\u0435\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 90% - \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 10% - \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f.\n\n# \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0438 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\n# \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","5b2d0d97":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# DataLoader \u0434\u043e\u043b\u0436\u0435\u043d \u0437\u043d\u0430\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u043c\u044b \u0437\u0430\u0434\u0430\u0435\u043c \u0435\u0433\u043e \u0437\u0434\u0435\u0441\u044c.\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u2013 \u044d\u0442\u043e \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0441\u0435\u0442\u044c \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432\n# \u0410\u0432\u0442\u043e\u0440\u044b BERT \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442 \u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0435\u0433\u043e 16 \u0438\u043b\u0438 32. \nbatch_size = 32\n\n# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 DataLoaders \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0433\u043e \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u043e\u0432\n\n# \u0414\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u043c\u044b \u0431\u0435\u0440\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u044b \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435.\ntrain_dataloader = DataLoader(\n        train_dataset,  # \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445.\n        sampler = RandomSampler(train_dataset), # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0431\u0430\u0442\u0447\u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\n        batch_size = batch_size # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u0441 \u0442\u0430\u043a\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0431\u0430\u0442\u0447\u0430.\n)\n\n# \u0414\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u043d\u0435 \u0432\u0430\u0436\u0435\u043d, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0430\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0438\u0445 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e.\nvalidation_dataloader = DataLoader(\n        val_dataset, # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445.\n        sampler = SequentialSampler(val_dataset), # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0431\u0430\u0442\u0447\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e.\n        batch_size = batch_size # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u0442\u0430\u043a\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0431\u0430\u0442\u0447\u0430.\n)\n","a23a74b7":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c BertForSequenceClassification. \u042d\u0442\u043e \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c BERT \u0441 \u043e\u0434\u0438\u043d\u043e\u0447\u043d\u044b\u043c \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u043c \u0441\u043b\u043e\u0435\u043c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c 12-\u0441\u043b\u043e\u0439\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c BERT, \u0441\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u043c \u0431\u0435\u0437 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0430.\n    num_labels = 2, # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u0441\u043b\u043e\u0451\u0432 \u2013 2 \u0434\u043b\u044f \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438. \u041c\u043e\u0436\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0434\u043b\u044f \u043c\u0443\u043b\u044c\u0442\u0438\u043a\u043b\u0430\u0441\u0441\u043e\u0432\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438.\n    output_attentions = False, # \u0411\u0443\u0434\u0435\u0442 \u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0432\u0435\u0441\u0430 \u0434\u043b\u044f attention-\u0441\u043b\u043e\u0451\u0432. \u0412 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0435\u0442.\n    output_hidden_states = False, # \u0411\u0443\u0434\u0435\u0442 \u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043a\u0440\u044b\u0442\u044b\u0445 \u0441\u043b\u043e\u0451\u0432. \u0412 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0435\u0442.\n)\n\n# \u0417\u0434\u0435\u0441\u044c \u043c\u044b \u0433\u043e\u0432\u043e\u0440\u0438\u043c PyTorch \u0447\u0442\u043e \u0445\u043e\u0442\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU.\nif torch.cuda.is_available():\n    model.cuda()\n\n# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u0430\u043a \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439 \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0441\u0432\u043e\u0434\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u043c\u043e\u0434\u0435\u043b\u0438.\nparams = list(model.named_parameters())\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\nprint('==== Embedding Layer ====\\n')\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","f27ecdf8":"optimizer = AdamW(model.parameters(),\n    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n)\n\nfrom transformers import get_linear_schedule_with_warmup\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438. \u0410\u0432\u0442\u043e\u0440\u044b BERT \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e\u0442 \u043e\u0442 2 \u0434\u043e 4.\n# \u041c\u044b \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c 4, \u043d\u043e \u0443\u0432\u0438\u0434\u0438\u043c \u043f\u043e\u0437\u0436\u0435, \u0447\u0442\u043e \u044d\u0442\u043e \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043a \u043e\u0432\u0435\u0440\u0444\u0438\u0442\u0443 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435.\nepochs = 1\n\n# \u041e\u0431\u0449\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u0448\u0430\u0433\u043e\u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0440\u0430\u0432\u043d\u043e [\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0431\u0430\u0442\u0447\u0435\u0439] x [\u0447\u0438\u0441\u043b\u043e \u044d\u043f\u043e\u0445].\ntotal_steps = len(train_dataloader) * epochs\n\n# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a learning rate (LR). LR \u0431\u0443\u0434\u0435\u0442 \u043f\u043b\u0430\u0432\u043d\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0442\u044c\u0441\u044f \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n","448b720a":"import numpy as np\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0440\u0430\u0441\u0447\u0451\u0442\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438. \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u0440\u0435\u0430\u043b\u044c\u043d\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0430 \u043a \u0434\u0430\u043d\u043d\u044b\u043c\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\n\nimport time\nimport datetime\nimport random \n\n# \u041d\u0430 \u0432\u0445\u043e\u0434 \u0432\u0440\u0435\u043c\u044f \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445 \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0441\u0442\u0440\u043e\u043a\u0430 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 hh:mm:ss\ndef format_time(elapsed):\n    # \u041e\u043a\u0440\u0443\u0433\u043b\u044f\u0435\u043c \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0439 \u0441\u0435\u043a\u0443\u043d\u0434\u044b.\n    elapsed_rounded = int(round((elapsed)))\n\n    # \u0424\u043e\u0440\u043c\u0430\u0442\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043a hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","1df279fc":"def train_step(device, model, train_dataloader, optimizer, scheduler):\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    t0 = time.time()\n    total_train_loss = 0\n    # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0440\u0435\u0436\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438.\n    model.train()\n\n    # \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u0430\u0442\u0447\u0430 \u0438\u0437 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445...\n    for step, batch in enumerate(train_dataloader):\n        if step % 40 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0431\u0430\u0442\u0447\u0430\n        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n        # \u041e\u0447\u0438\u0449\u0430\u0435\u043c \u0432\u0441\u0435 \u0440\u0430\u043d\u0435\u0435 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b (\u044d\u0442\u043e \u0432\u0430\u0436\u043d\u043e)\n        model.zero_grad()\n        # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c\n        loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        # \u041d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u0435\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e \u0432\u0441\u0435\u043c \u0431\u0430\u0442\u0447\u0430\u043c\n        total_train_loss += loss.item()\n        # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043a\u0438 \u0447\u0442\u043e \u0431\u044b \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b.\n        loss.backward()\n        # \u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u0435\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u0434\u043e 1.0. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0437\u0431\u0435\u0436\u0430\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \"exploding gradients\".\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0438 \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e learning rate.\n        optimizer.step()\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c learning rate.\n        scheduler.step()\n\n    # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e \u0432\u0441\u0435\u043c \u0431\u0430\u0442\u0447\u0430\u043c.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)\n    # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0438.\n    training_time = format_time(time.time() - t0)\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    return avg_train_loss, training_time","47217f01":"def validation_step(device, model, validation_dataloader):\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0440\u0435\u0436\u0438\u043c evaluation \u2013 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043b\u043e\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 dropout \u0432\u0435\u0434\u0443\u0442 \u0441\u0435\u0431\u044f \u043f\u043e \u0434\u0440\u0443\u0433\u043e\u043c\u0443.\n    model.eval()\n\n    # \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    # \u041f\u0440\u043e\u0433\u043e\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\n    for batch in validation_dataloader:\n        # \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0431\u0430\u0442\u0447\u0430.\n        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n\n        # \u0413\u043e\u0432\u043e\u0440\u0438\u043c pytorch \u0447\u0442\u043e \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u0435\u043d \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0433\u0440\u0430\u0444 \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 (\u0432\u0441\u0451 \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u0431\u044b\u0441\u0442\u0440\u0435\u0435)\n        with torch.no_grad():\n            # \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n            (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n        # \u041d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.\n        total_eval_loss += loss.item()\n\n        # \u041f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441 GPU \u043d\u0430 CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u0442\u0447\u0430 \u0441 \u0442\u0435\u043a\u0441\u0442\u0430\u043c\u0438 \u0438 \u043d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.\n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u044e\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0431\u0430\u0442\u0447\u0435\u0439.\n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    # \u0418\u0437\u043c\u0435\u0440\u044f\u0435\u043c \u043a\u0430\u043a \u0434\u043e\u043b\u0433\u043e \u0441\u0447\u0438\u0442\u0430\u043b\u0430\u0441\u044c \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f.\n    validation_time = format_time(time.time() - t0)\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    return avg_val_loss, avg_val_accuracy, validation_time\n","fa08f0b2":"# \u0412 \u044d\u0442\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0432\u0441\u044f\u043a\u0443\u044e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u043f\u043e \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0435: \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c, \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0446\u0435\u043d\u044b (\u043f\u043e\u0442\u0435\u0440\u044c) \u0438 \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f.\ntraining_stats = []\n# \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0447\u0442\u043e \u0431\u044b \u0438\u0437\u043c\u0435\u0440\u0438\u0442\u044c \u0432\u0440\u0435\u043c\u044f \u0432\u0441\u0435\u0439 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438.\ntotal_t0 = time.time()\n\n# \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0438...\nfor epoch_i in range(0, epochs):\n    # \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043e\u0434\u043d\u0443 \u044d\u043f\u043e\u0445\u0443 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 (\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0441\u043b\u0430\u0439\u0434) \n    avg_train_loss, training_time = train_step(device, model, train_dataloader, optimizer, scheduler)\n    # \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e \u0447\u0442\u043e \u0431\u044b \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u043c \u044d\u0442\u0430\u043f\u0435 (\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0441\u043b\u0430\u0439\u0434)\n    avg_val_loss, avg_val_accuracy, validation_time = validation_step(device, model, validation_dataloader)\n\n    # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0435.\n    training_stats.append(\n        {\n            'Epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Validation Loss': avg_val_loss,\n            'Validation Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"Training complete! Total training took {:} (hh:mm:ss)\".format(format_time(time.time() - total_t0)))\n","9eccb7c2":"import os\n\n# \u0417\u0430\u0434\u0430\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u043d\u0443\u044e \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e\noutput_dir = '.\/model_save\/'\n# \u0415\u0441\u043b\u0438 \u043e\u043d\u0430 \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0435\u0451\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043d\u0430\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0435\u0451 \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f `save_pretrained()`.\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed\/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","3efb6ae6":"from transformers import BertTokenizer, BertForSequenceClassification\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0430\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0435\u0451 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\nmodel = BertForSequenceClassification.from_pretrained(output_dir)\ntokenizer = BertTokenizer.from_pretrained(output_dir)\n\n# \u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU.\nif torch.cuda.is_available():\n    model.to(device)","902d2bb1":"**PART 15: \u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0446\u0438\u043a\u043b \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438**","9a2beeef":"**PART 3: \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438 \u043a \u043d\u0438\u043c**","0e8fb387":"**PART 9: \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c BERT \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u0435\u0451 \u0441\u043b\u043e\u0451\u0432 \u0434\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430**","8b2010e0":"**PART 16: \u0412\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043f\u0438\u0438**","357d7896":"**PART 11: \u0414\u0432\u0435 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438: \u0440\u0430\u0441\u0447\u0451\u0442 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0438 \u0432\u044b\u0432\u043e\u0434 \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438**","9405d7ec":"**PART 15: \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438**","d3f81db4":"**PART 1: \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430 \u0434\u043b\u044f \u0440\u0430\u0441\u0447\u0451\u0442\u043e\u0432**","9929c2f0":"**PART 7: \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435**","b0759175":"**PART 5: \u041f\u043e\u0434\u0441\u0447\u0435\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 (\u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432)**","07839b75":"**PART 8: \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 (Data Loaders)**","905a8545":"**PART 6: \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 (\u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u043e)**","839b11ce":"**PART 12: \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u0445\u043e\u0434 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0432\u0441\u0435\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c.**","02662115":"**PART 4: \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440\u0430 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0435\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u044b**","cf9efa5e":"**PART 2: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438\u0437 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0430**","ca003762":"**PART 10: \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 Adam, \u0437\u0430\u0434\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0438 \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a learning rate**","0ef6c8f0":"**PART 14: \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u043c\u0435\u0442\u0440\u0438\u043a \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438**"}}