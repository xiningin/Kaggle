{"cell_type":{"f0104794":"code","ecbd4f0f":"code","ae9de619":"code","8d1bc666":"code","019664d1":"code","14737ffe":"code","1e783a54":"code","3e58905e":"code","b5e1942c":"code","d764ed83":"code","47c23054":"code","f5daee4e":"code","5124cd84":"code","e8939a2c":"code","316a98a4":"code","d6f8ca56":"code","2c81f699":"code","0e47e7c8":"code","b5463263":"code","f0704205":"code","fd200969":"code","c7e0d23b":"markdown","63822481":"markdown","1ff4742a":"markdown","f64107a4":"markdown","6d14250b":"markdown","0fdeb3bf":"markdown","1b595bb5":"markdown","7fd2212a":"markdown","dc57d08d":"markdown","99898b6a":"markdown","fa85ae0b":"markdown","c4d94687":"markdown","639f621a":"markdown","d5ea0e44":"markdown","6be22b7d":"markdown"},"source":{"f0104794":"import os\nimport pandas as pd\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","ecbd4f0f":"MODEL_NAME = \"roberta-large\"\n\n# The model is a roberta large from the HF model hub with a modified token classification head\n# The modification is done in the training notebook\nMODEL_PATH = '..\/input\/feedback-prize-roberta-weights\/model'\n\n# Weights from the training notebook\nMODEL_CHECKPOINT = '..\/input\/feedback-prize-roberta-weights\/pytorch_model_e3.bin'\n\n# Test batch size\nBATCH_SIZE = 4\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# The stride (overlap) between chunks of texts during the split\nDOC_STRIDE = 128\n\n# Max model length, 512 for roberta\nMAX_LENGTH = 512","ae9de619":"def load_df_test():\n    test_names, df_test = [], []\n    for f in list(os.listdir('..\/input\/feedback-prize-2021\/test')):\n        test_names.append(f.replace('.txt', ''))\n        df_test.append(open('..\/input\/feedback-prize-2021\/test\/' + f, 'r').read())\n    df_test = pd.DataFrame({'id': test_names, 'text': df_test})\n    df_test['text_split'] = df_test.text.str.split()\n    return df_test\n\ndf_test = load_df_test()\ndf_test.head()","8d1bc666":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nLABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\nIDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\n\nLABELS_TO_IDS","019664d1":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","14737ffe":"def get_labels(word_ids, word_labels):\n    label_ids = []\n    for word_idx in word_ids:                            \n        if word_idx is None:\n            label_ids.append(-100)\n        else:\n            label_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n    return label_ids\n\n\ndef tokenize(df, to_tensor=True, with_labels=True):\n    \n    encoded = tokenizer(df['text_split'].tolist(),\n                        is_split_into_words=True,\n                        return_overflowing_tokens=True,\n                        stride=DOC_STRIDE,\n                        max_length=MAX_LENGTH,\n                        padding=\"max_length\",\n                        truncation=True)\n\n    if with_labels:\n        encoded['labels'] = []\n\n    encoded['wids'] = []\n    \n    n = len(encoded['overflow_to_sample_mapping'])\n    \n    for i in range(n):\n\n        # Map back to original row\n        text_idx = encoded['overflow_to_sample_mapping'][i]\n        \n        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n        word_ids = encoded.word_ids(i)\n        \n        if with_labels:\n            # Get word labels of the full un-chunked text\n            word_labels = df['entities'].iloc[text_idx]\n        \n            # Get the labels associated with the word indexes\n            label_ids = get_labels(word_ids, word_labels)\n            encoded['labels'].append(label_ids)\n        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n    \n    if to_tensor:\n        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n    return encoded","1e783a54":"# Tokenize the df_test\ntokenized_test = tokenize(df_test, with_labels=False)","3e58905e":"# Original number of rows\nlen(df_test)","b5e1942c":"# Number of samples to feed the model\nlen(tokenized_test['input_ids'])","d764ed83":"# Back-reference. \n# The first 2 zeroes mean that the first row was split into 2 samples\n# And the 4 twos mean that one big ass motherfucker was split into 4 :P\ntokenized_test['overflow_to_sample_mapping']","47c23054":"# The one in position \"2\" has 1056 words and has to fit into 512, with overlaps of 200.\ndf_test['text_split'].str.len()","f5daee4e":"# Actually, it has 1304 tokens (which are subwords)\nn_tokens = len(tokenizer(df_test.iloc[2]['text'])['input_ids'])\nn_tokens","5124cd84":"# Further exploration of the case for those who are interested:\n\n## Verification that 4 chunks of 512 with a stride of 200 is the correct number of chunks to fit 1304 tokens in\n# 512 + 2*(512-200) < n_tokens < 512 + 3*(512-200)\n\n## Original text:\n# df_test.iloc[2]['text']\n\n## The four 512-token chunks generated by the tokenization procedure:\n# tokenizer.decode(tokenized_test['input_ids'][3])\n# tokenizer.decode(tokenized_test['input_ids'][4])\n# tokenizer.decode(tokenized_test['input_ids'][5])\n# tokenizer.decode(tokenized_test['input_ids'][6])","e8939a2c":"class FeedbackPrizeDataset(Dataset):\n    def __init__(self, tokenized_ds):\n        self.data = tokenized_ds\n\n    def __getitem__(self, index):\n        item = {}\n        for k in self.data.keys():\n            item[k] = self.data[k][index]\n        return item\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n\n    \n# Create Dataset and DataLoader\nds_test = FeedbackPrizeDataset(tokenized_test)\ndl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, \n                              num_workers=2, pin_memory=True)","316a98a4":"def load_model():\n    model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n    model.to(DEVICE)\n    model.load_state_dict(torch.load(MODEL_CHECKPOINT))\n    model.eval()\n    print('Model loaded.')\n    return model\n\nmodel = load_model()","d6f8ca56":"\ndef inference(dl):\n    \n    # These 2 dictionaries will hold text-level data\n    # Helping in the merging process by accumulating data\n    # Through all the chunks\n    predictions = defaultdict(list)\n    seen_words_idx = defaultdict(list)\n    \n    for batch in dl:\n        ids = batch[\"input_ids\"].to(DEVICE)\n        mask = batch[\"attention_mask\"].to(DEVICE)\n        outputs = model(ids, attention_mask=mask, return_dict=False)\n        \n        del ids, mask\n        \n        batch_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n    \n        # Go over each prediction, getting the text_id reference\n        for k, (chunk_preds, text_id) in enumerate(zip(batch_preds, batch['overflow_to_sample_mapping'].tolist())):\n            \n            # The word_ids are absolute references in the original text\n            word_ids = batch['wids'][k].numpy()\n            \n            # Map from ids to labels\n            chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]        \n            \n            for idx, word_idx in enumerate(word_ids):                            \n                if word_idx == -1:\n                    pass\n                elif word_idx not in seen_words_idx[text_id]:\n                    # Add predictions if the word doesn't have a prediction from a previous chunk\n                    predictions[text_id].append(chunk_preds[idx])\n                    seen_words_idx[text_id].append(word_idx)\n    \n    final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n    return final_predictions\n\n\n# https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer\n# code has been modified from original\n# I moved the iteration over the batches to inference because  \n# samples from the same text might have be split into different batches\ndef get_predictions(df, dl):\n    \n    all_labels = inference(dl)\n    final_preds = []\n    \n    for i in range(len(df)):\n        idx = df.id.values[i]\n        pred = all_labels[i]\n        preds = []\n        j = 0\n        \n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': pass\n            else: cls = cls.replace('B','I')\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            if cls != 'O' and cls != '' and end - j > 7:\n                final_preds.append((idx, cls.replace('I-',''), \n                                    ' '.join(map(str, list(range(j, end))))))\n            j = end\n        \n    df_pred = pd.DataFrame(final_preds)\n    df_pred.columns = ['id','class','predictionstring']\n    return df_pred","2c81f699":"df_sub = get_predictions(df_test, dl_test)\ndisplay(df_sub.head())","0e47e7c8":"# Add: https:\/\/www.kaggle.com\/vuxxxx\/tensorflow-longformer-ner-postprocessing#Postprocessing\nmap_clip = {'Lead':9, \n            'Position':5, \n            'Evidence':14, \n            'Claim':3, \n            'Concluding Statement':11,\n            'Counterclaim':6, \n            'Rebuttal':4}\n\ndef threshold(df):\n    df = df.copy()\n    df['len'] = df['predictionstring'].apply(lambda x:len(x.split()))\n    for key, value in map_clip.items():\n    # if df.loc[df['class']==key,'len'] < value \n        index = df.loc[df['class']==key].query(f'len<{value}').index\n        df.drop(index, inplace = True)\n    df = df.drop('len', axis=1)\n    return df","b5463263":"def post_process(df_sub):\n    df_post = [df_sub.iloc[0].copy()]\n    for i in range(1, len(df_sub)):\n        prev_row = df_post[-1]\n        row = df_sub.iloc[i].copy()\n        \n        # Does the row belong to the same text as the previous and to the same discourse class?\n        if row['id'] == prev_row['id'] and row['class'] == prev_row['class']:\n            try:\n                first_pos_row = int(row['predictionstring'].split(\" \")[0])\n                last_pos_prev_row = int(prev_row['predictionstring'].split(\" \")[-1])\n                \n                # Is the row starting 2 words after the previous?\n                if last_pos_prev_row + 2 == first_pos_row:\n                    # In that case, merge with the previous one\n                    new_id = last_pos_prev_row + 1\n                    row['predictionstring'] = prev_row['predictionstring'] + f' {new_id} ' + row['predictionstring']\n                    df_post = df_post[:-1]\n                    \n                df_post.append(row)\n            except:\n                df_post.append(row)\n        else:\n            df_post.append(row)\n    df_post = pd.DataFrame(df_post).reset_index(drop=True)\n    df_post = threshold(df_post)\n    return df_post\n\n\n","f0704205":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","fd200969":"#df_post = post_process(df_sub)\ndf_post = threshold(df_sub)\ndf_post = link_evidence(df_post)\ndf_post.to_csv(\"submission.csv\", index=False)\ndf_post.head()","c7e0d23b":"# Load model with fine-tuned weights\n\nTraining notebook: https:\/\/www.kaggle.com\/julian3833\/pytorch-roberta-w-chunks-train-0-604","63822481":"## Create mapping for output labels","1ff4742a":"# Load test data\n\nCode from [Fine-Tunned on Roberta-base as NER problem [0.533]](https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533)","f64107a4":"# Submit!","6d14250b":"# \ud83d\udcd6 PyTorch \"ShortFormer\" - RoBERTa w\/Chunks - Infer [0.607]\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31779\/logos\/header.png)\n\n### A NER \"ShortFormer\" with chunks, strides, and all the clumsy stuff\n\n**This notebook is a baseline model for the competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021). It approaches the problems as a token classification problem (\"NER\"-like) and builds a RoBERTa base model with `max_length=512`. In order to do so, it manages the chunking with stride of the texts with length greater than 512 (and the posterior merge).**\n\nIt is a kind of follow-up of the public work, and relies heavily on the awesome public BigBird baseline by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte): [PyTorch - BigBird - NER - [CV 0.615]](https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615). That notebook, in turn, uses code from the following ones:\n* [Fine-Tunned on Roberta-base as NER problem [0.533]](https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533) by [RAGHAVENDRAKUTTALA](https:\/\/www.kaggle.com\/raghavendrakotala)\n* [\ud83c\udf93 Student Writing Competition [Twitch Stream]](https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch) by [Rob Mulla](https:\/\/www.kaggle.com\/robikscube\/)\n* [Pytorch NER infer](https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer) by [zzy](https:\/\/www.kaggle.com\/zzy990106)\n\nDon't forget to upvote all these excellent kernels.\n\n\n\n### This is the inference notebook.\n### The training notebook is here: [\ud83d\udcd6 PyTorch- \"ShortFormer\" w\/Chunks - Train [0.604]](https:\/\/www.kaggle.com\/julian3833\/pytorch-roberta-w-chunks-train-0-604)\n\n\n&nbsp;\nI loved the dual training\/inference nature of Chris' notebook, but it was too much for me right now -I'm learning pytorch- so I unrolled it into the old Training\/Inference way that we are used to. \n\n\nBoth mostly follow Chris'. The main differences are:\n1. At the tokenizing step, where I used the hugging face tokenizer functionality to leverage the chunking. See that step for details about the implementation\n2. Validation is now performed on a per-epoch fashion\n3. The `inference` and `get_predictions` functions had to be adapted to the chunking as well.\n\n\n\n# Please _DO_ upvote if you found this kernel useful or interesting! \ud83e\udd17\n\n&nbsp;\n&nbsp;\n\n&nbsp;\n&nbsp;\n\n---\n\n# Oh, ($n^2$)oo!: Some context\n\n\nTransformer models are great. We all love them. _But_ the self-attention mechanism - the core of the Transformer architecture - has a matrix multiplication that scales quadratically with the input sequence length (at least) in terms of memory. The $QK^T$ costs a lot. And it makes the vanilla Transformer prohibitive for long sequences. This lead to the `512` tokens max length in the BERT-like models we see and use constantly.\n\nThere is research in the direction of reducing the cost of the attention operation so it scales in a slower fashion with the input length. Two recent models from this research are [LongFormer](https:\/\/arxiv.org\/abs\/2004.05150) and [BigBird](https:\/\/arxiv.org\/abs\/2007.14062), both put on the table by Chris Deotte in this competition (at least for me). \nThose models propose both slight variations of the self-attention mechanism that reduce the memory dependency to $O(n)$, this is, to scale linearly with the length of the input sequences. Both methods are \"sparse attention\" methods, meaning that, instead of each token attending to (and receiving attention from) all of the others, this cross-attention is pruned to a small number of tokens. In Longformer, there are 2 flavors of local windows (normal and dilated) and a global per-task attention, while in BigBird there is a window, a random and a global attention.\n\n\n   <center><img src=\"https:\/\/i.imgur.com\/t4MYmbj.png\" width=\"50%\"><\/center>\n      <center><i>From the LongFormer <a href=\"https:\/\/arxiv.org\/abs\/2004.05150\">paper<\/a><\/i><\/center>\n\n\n   <center><img src=\"https:\/\/i.imgur.com\/4bkL2JA.png\" width=\"50%\"><\/center>\n      <center><i>From the BigBird <a href=\"https:\/\/arxiv.org\/abs\/2007.14062\">paper<\/a><\/i><\/center>\n\n&nbsp;\n&nbsp;\n\nThe lower cost of these sparse self-attention mechanisms allows these models to handle up to `4096` in a normal GPU, this is, `8x` what a normal Transformer can.\n\nGiven the lengths of the texts in this competition, it is no surprise that the current public work is focused on those so called \"Longformer\" models and it is probable that they will be an important part of the final ensemble solutions.\n\nBut... but..., on the other hand, the old-fashioned 512-token models _do have_ a mechanism to cope with their sequence length limitations. For a given sequence of length greater than `512`, before longformer-like models, the NLP community would:\n\n1. Split it into chunks of `512` tokens (possibly with some overlap)\n2. Use the model to process those chunks\n3. Merge back the predictions over the chunks to obtain predictions over the full text\n\nThis mechanism was used, for example, during the recently finished [ChaII competition](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering), starting from [Darek K\u0142eczek](https:\/\/www.kaggle.com\/thedrcat)'s [baseline](https:\/\/www.kaggle.com\/thedrcat\/chaii-eda-baseline).\n\n\nIt is possible that this mechanism is still relevant although the sparse-attention models.\n\nIt is possible that [\"ShortFormers\"](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/297461) have something to say in this competition? Maybe add some variance to an ensemble? ... Or even more?\n\n\n---\n\n\n&nbsp;\n&nbsp;\n\n&nbsp;\n&nbsp;\n\n\nOk, let's go!","0fdeb3bf":"# Link Evidence","1b595bb5":"# Please _DO_ upvote if you found this kernel useful or interesting! \ud83e\udd17","7fd2212a":"# Imports","dc57d08d":"# Tokenization and chunking\n\nThis is the main added value of these notebooks.\n\nIn particular, the call to the `tokenizer` with the following parameters:\n\n* The text split already into words, in combination with `is_split_into_words`, as used by Chris Deotte and explained [here](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/tokenizer#transformers.PreTrainedTokenizer.prepare_for_tokenization.is_split_into_words).\n* `return_overflowing_tokens=True`, which activates the \"chunking\" mechanism (aka: will generate more than one tokenized sample for texts with more than 512 tokens\n* `stride`: the size of the overlap between chunked parts of a text\n\n`return_overflowing_tokens=True` sets the key `overflow_to_sample_mapping` which has the index of the original text that generated each sample for each of them.\n\nMoreover, the `word_ids(idx)` method returns a back-reference to the word index in the original text, indexed correctly no matter the chunk, doing a lot of the heavy-lifting. This is, for each token in the tokenized output, it says which word of the original text generated that token. ","99898b6a":"## Dataset class\n\nWith the functional tokenization we performed above, the dataset class is trivial.","fa85ae0b":"# A short exploration of the tokenization procedure","c4d94687":"# Inference code\n\nWe will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop. \n\nCode taken and adapted from Chris Deotte. In turn his work is based on [this][1] and [this][2].\n\n\nThe adaptions are the minimal required to handle the fact that one text might have generated more than one model sample.\n\nThe key `overflow_to_sample_mapping` is a mapping from the sample back to the original text.\n\n\nDuring inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n\nMoreover, since there are a large overlaps, for long texts there will be more than one prediction for various token. In this version, we are using the first prediction found and dropping all the rest. A voting mechanism could be implemented.\n\n\n\n[1]: https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer","639f621a":"# Post processing\n\nThe model currently is breaking some discourse blocks by a dot. In `df_sub` above, for example, the row 1 ends in the word `64` and the row 2 starts in the word `66`. An analysis of the cases showed that the word inbetween (`65`) is a dot. \n\nThe following code takes care of those case (and only of those) leading to a +`0.02` LB score.\n\n","d5ea0e44":"# Configuration","6be22b7d":"# Inference\nWe will now infer the test data and write submission CSV"}}