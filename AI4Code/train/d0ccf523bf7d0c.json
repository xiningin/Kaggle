{"cell_type":{"11107ec4":"code","2bd974d1":"code","a233522d":"code","c456898a":"code","7f4e7e8f":"code","3db5d609":"code","dae27923":"code","c69e2a4c":"code","a34bdd4e":"code","3f538a72":"code","0c771e15":"code","29680604":"code","0f99cdb1":"code","60910d7d":"code","65f44158":"code","f6f62db5":"code","b13c1aa0":"markdown","af418f79":"markdown","59f45081":"markdown","4f12be0e":"markdown","35192e43":"markdown"},"source":{"11107ec4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom zipfile import ZipFile\nfrom subprocess import check_output\n\nprint(check_output([\"ls\", \"..\/input\/hps-data\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","2bd974d1":"from pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport urllib.request, json\nimport os\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler","a233522d":"    df = pd.read_csv(os.path.join(\"..\/input\/hps-data\/hpq.us.txt\"),delimiter=',',usecols=['Date','Open','High','Low','Close'])","c456898a":"df = df.sort_values('Date')","7f4e7e8f":"df.head()","3db5d609":"df.shape[0]","dae27923":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),(df['Low']+df['High'])\/2.0)\nplt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Mid Price',fontsize=18)\nplt.show()\n","c69e2a4c":"high_prices = df.loc[:,'High'].as_matrix()\nlow_prices = df.loc[:,'Low'].as_matrix()\nmid_prices = (high_prices+low_prices)\/2.0","a34bdd4e":"train_data = mid_prices[:11000]\ntest_data = mid_prices[11000:]\n","3f538a72":"scaler = MinMaxScaler()\ntrain_data = train_data.reshape(-1,1)\ntest_data = test_data.reshape(-1,1)","0c771e15":"smoothing_window_size = 2500\nfor di in range(0,10000,smoothing_window_size):\n    scaler.fit(train_data[di:di+smoothing_window_size,:])\n    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n\n# normalizing the last bit of remaining data\nscaler.fit(train_data[di+smoothing_window_size:,:])\ntrain_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n","29680604":"train_data = train_data.reshape(-1)\ntest_data = scaler.transform(test_data).reshape(-1)","0f99cdb1":"EMA = 0.0\nalpha = 0.1\nfor ti in range(11000):\n  EMA = alpha*train_data[ti] + (1-alpha)*EMA\n  train_data[ti] = EMA\n    \nall_mid_data = np.concatenate([train_data,test_data],axis=0)","60910d7d":"window_size = 100\nN = train_data.size\nstd_avg_predictions = []\nstd_avg_x = []\nmse_errors = []\n\nfor it in range(window_size, N):\n    \n    if it>=N:\n        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n    else:\n        date = df.loc[it, 'Date']\n        \nstd_avg_predictions.append(np.mean(train_data[it-window_size:it])) \nmse_errors.append((std_avg_predictions[-1]-train_data[it])**2)\nstd_avg_x.append(date)\n\nprint('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))\n","65f44158":"window_size = 100\nN = train_data.size\n\nrun_avg_predictions = []\nrun_avg_x = []\n\nmse_errors = []\n\nrunning_mean = 0.0\nrun_avg_predictions.append(running_mean)\n\ndecay = 0.5\n\nfor pred_idx in range(1,N):\n\n    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n    run_avg_predictions.append(running_mean)\n    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n    run_avg_x.append(date)\n\nprint('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))","f6f62db5":"plt.figure(figsize = (18,9))\nplt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\nplt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Mid Price')\nplt.legend(fontsize=18)\nplt.show()","b13c1aa0":"# Exponential Moving Average","af418f79":"Performing exponential moving average smoothing<br>\nNow the data will have smoother curve than the original ragged data","59f45081":"**** Different time periods of data have different value ranges. So, Normalize the data by splitting the full series into windows. Otherwise, the earlier data is close to zero and will not add much value to the learning process. Here, window size taken = 2500.","4f12be0e":"****Reshape the data as[data_size, number_of_features]","35192e43":"****reshape the databack into [data_size]"}}