{"cell_type":{"1718cf55":"code","0aa60047":"code","c18bc182":"code","63abdea7":"code","1189364b":"code","486bff59":"code","b6e581a0":"code","7842f2bd":"code","387f6c48":"code","14191d3e":"code","49464335":"code","0e9a1f4c":"code","ff5829f5":"code","39e46e3d":"code","10baf2e6":"code","6984c141":"code","e6343017":"code","d00b1038":"code","4cb697ef":"code","166a58b1":"code","c15fe1ce":"code","0f023076":"code","b4d1994f":"code","cb42f4f5":"code","70780109":"code","df3bee66":"code","d41187c3":"code","4630fa1f":"code","25ea7b12":"code","dceea8ed":"code","a00a1d85":"code","ac5eac7e":"code","bc0d67d6":"code","48fe1dfa":"code","02fe610d":"code","57df0649":"markdown","386db0ee":"markdown","9388791e":"markdown","c8aa22a9":"markdown","c393eb7f":"markdown","1a7d9444":"markdown","b51bc6e2":"markdown","d4fd7efb":"markdown","3b636040":"markdown","0a8dc7d4":"markdown","d2d233ab":"markdown","1b052c71":"markdown","c3028f3d":"markdown"},"source":{"1718cf55":"# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0aa60047":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy import stats #qqplot\nimport statsmodels.api as sm #for decomposing the trends, seasonality etc.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX #the big daddy","c18bc182":"#Import the data and parse dates.\ndf  = pd.read_csv('..\/input\/productdemandforecasting\/Historical Product Demand.csv', parse_dates=['Date'])","63abdea7":"#Don't judge it by the cover.\ndf.head(5)","1189364b":"#Check the cardinality.\ndf.shape","486bff59":"#Check the data types.\ndf.dtypes","b6e581a0":"# Check any number of columns with NaN\nprint(df.isnull().any().sum(), ' \/ ', len(df.columns))\n# Check any number of data points with NaN\nprint(df.isnull().any(axis=1).sum(), ' \/ ', len(df))","7842f2bd":"#Lets check where these nulls are.\nprint (df.isna().sum())\nprint ('Null to Dataset Ratio in Dates: ',df.isnull().sum()[3]\/df.shape[0]*100)\n#There are missing values in Dates.","387f6c48":"#Drop na's.\n\n#Since the number of missing values are about 1%, I am taking an 'executive decision' of removing them. ;) \ndf.dropna(axis=0, inplace=True) #remove all rows with na's.\ndf.reset_index(drop=True)\ndf.sort_values('Date')[10:20] #Some of the values have () in them.","14191d3e":"#Target Feature - Order_Demand\n#Removing () from the target feature.\ndf['Order_Demand'] = df['Order_Demand'].str.replace('(',\"\")\ndf['Order_Demand'] = df['Order_Demand'].str.replace(')',\"\")\n\n#Next step is to change the data type.\ndf['Order_Demand'] = df['Order_Demand'].astype('int64')","49464335":"#Get the lowest and highest dates in the dataset.\ndf['Date'].min() , df['Date'].max()\n#There is data for 6 years. great.","0e9a1f4c":"#Lets start with 2012 and cap it 2016 december. Since the dates before 2012 have a lot of missing values - inspected and checked using basic time series plot.\ndf = df[(df['Date']>='2012-01-01') & (df['Date']<='2016-12-31')].sort_values('Date', ascending=True)","ff5829f5":"#Lets check the ditribution of the target variable (Order_Demand)\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 4,2\n\nsb.distplot(df['Order_Demand'], fit=norm)\n\n#Get the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['Order_Demand'], plot=plt)\nplt.show()","39e46e3d":"#The data is highly skewed, but since we'll be applying ARIMA, it's fine.\ndf['Order_Demand'].skew()","10baf2e6":"#Just in case if there needs to be some transformation, it can be done by either taking log values or using box cox.\n\n## In case you need to normalize data, use Box Cox. Pick the one that looks MOST like a normal distribution.\n# for i in [1,2,3,4,5,6,7,8]:\n#     plt.hist(df['Order_Demand']**(1\/i), bins= 40, normed=False)\n#     plt.title(\"Box Cox transformation: 1\/{}\". format(str(i)))\n#     plt.show()","6984c141":"#Warehouse shipping by orders.\ndf['Warehouse'].value_counts().sort_values(ascending = False)","e6343017":"#The amount of orders shipped by each warehouse.\ndf.groupby('Warehouse').sum().sort_values('Order_Demand', ascending = False)\n#Warehouse J is clearly shipping most orders. Although S is shipping more quantity within fewer requested orders.","d00b1038":"#Product Category.\n\nprint (len(df['Product_Category'].value_counts()))\n\nrcParams['figure.figsize'] = 50,14\nsb.countplot(df['Product_Category'].sort_values(ascending = True))\n\n#There's a lot of orders on category19.","4cb697ef":"#Lets check the orders by warehouse.\n\n#Checking with Boxplots\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 16,4\nf, axes = plt.subplots(1, 2)\n#Regular Data\nfig3 = sb.boxplot( df['Warehouse'],df['Order_Demand'], ax = axes[0])\n#Data with Log Transformation\nfig4 = sb.boxplot( df['Warehouse'], np.log1p(df['Order_Demand']),ax = axes[1])\n\ndel fig3, fig4","166a58b1":"#Lets check the Orders by Product Category.\nrcParams['figure.figsize'] = 50,12\n#Taking subset of data temporarily for in memory compute.\ndf_temp = df.sample(n=20000).reset_index()\nfig5 = sb.boxplot( df_temp['Product_Category'].sort_values(),np.log1p(df_temp['Order_Demand']))\ndel df_temp, fig5","c15fe1ce":"df = df.groupby('Date')['Order_Demand'].sum().reset_index()\n#This gives us the total orders placed on each day.","0f023076":"#Index the date\ndf = df.set_index('Date')\ndf.index #Lets check the index","b4d1994f":"#Averages daily sales value for the month, and we are using the start of each month as the timestamp.\ny = df['Order_Demand'].resample('MS').mean()","cb42f4f5":"#In case there are Null values, they can be imputed using bfill.\n#y = y.fillna(y.bfill())","70780109":"#Visualizing time series.\n\ny.plot(figsize=(12,5))\nplt.show()\n\n#Takeaway: The sales are always low for the beginning of the year and the highest peak in demand every year is in the\n#last quarter. The observed trend shows that orders were higher during 2014-2016 then reducing.\n\n#Lets check it by decomposition.","df3bee66":"#The best part about time series data and decomposition is that you can break down the data into the following:\n#Time Series Decomposition. \nfrom pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\nfig = decomposition.plot()\nplt.show()","d41187c3":"#GRID SEARCH for Param Tuning.\n#Sample params for seasonal arima. (SARIMAX).\n\n#For each combination of parameters, we fit a new seasonal ARIMA model with the SARIMAX() function \n#from the statsmodels module and assess its overall quality.\n\nimport itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","4630fa1f":"#Get the best params for the data. Choose the lowest AIC.\n\n# The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a \n# given set of data. \n# AIC measures how well a model fits the data while taking into account the overall complexity of the model.\n# Large AIC: Model fits very well using a lot of features.\n# Small AIC: Model fits similar fit but using lesser features. \n# Hence LOWER THE AIC, the better it is.\n\n#The code tests the given params using sarimax and outputs the AIC scores.\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","25ea7b12":"#Fit the model with the best params.\n#ARIMA(1, 1, 1)x(1, 1, 0, 12)12 - AIC:960.5164122018646\n\n\n#The above output suggests that ARIMA(1, 1, 1)x(1, 1, 0, 12) yields the lowest AIC value: 960.516\n#Therefore we should consider this to be optimal option.\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nmod = sm.tsa.statespace.SARIMAX(y,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","dceea8ed":"#Plotting the diagnostics.\n\n#The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.\nresults.plot_diagnostics(figsize=(16, 8))\nplt.show()\n\n#What to look for?\n#1. Residuals SHOULD be Normally Distributed ; Check\n#Top Right: The (orange colored) KDE line should be closely matched with green colored N(0,1) line. This is the standard notation\n#for normal distribution with mean 0 and sd 1.\n#Bottom Left: The qq plot shows the ordered distribution of residuals (blue dots) follows the linear trend of the samples \n#taken from a standard normal distribution with N(0, 1). \n\n#2. #Residuals are not correlated; Check\n#Top Left: The standard residuals don\u2019t display any obvious seasonality and appear to be white noise. \n#Bottom Right: The autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have \n#low correlation with its own lagged versions.\n","a00a1d85":"#Lets get the predictions and confidence interval for those predictions.\n#Get the predictions. The forecasts start from the 1st of Jan 2017 but the previous line shows how it fits to the data.\npred = results.get_prediction(start=pd.to_datetime('2014-05-01'), dynamic=False) #false is when using the entire history.\n#Confidence interval.\npred_ci = pred.conf_int()\n\n#Plotting real and forecasted values.\nax = y['2013':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='blue', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Takeaway: The forecats seems to be fitting well to the data. The Blue\/purple thicker plot shows the confidence level in the forecasts. ","ac5eac7e":"#Getting the mean squared error (average error of forecasts).\ny_forecasted = pred.predicted_mean\ny_truth = y['2016-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('MSE {}'.format(round(mse, 2)))\n\n#Smaller the better.","bc0d67d6":"print('RMSE: {}'.format(round(np.sqrt(mse), 2)))","48fe1dfa":"#The time can be changed using steps.\npred_uc = results.get_forecast(steps=50)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Order_Demand')\nplt.legend()\nplt.show()\n\n#Far out values are naturally more prone to variance. The grey area is the confidence we have in the predictions.","02fe610d":"#And that is how you build a somewhat accurate forecast of the demand. Next steps include checking and modeling demand by category type.","57df0649":"Univariate Analysis - Warehouse, Product Category.\n","386db0ee":"**Theory on ARIMA**\n\nAn ARIMA model is characterized by 3 terms: p, d, q where these three parameters account for seasonality (p), trend (d), and noise in data (q):\n\n* p is the order of the AR term (number of lags of Y to be used as predictors). If it snowed for the last wee, it is likely it will snow tomorrow.\n* q is the order of the MA term (moving average).  \n* d is the number of differencing required to make the time series stationary. if already stationary d=0.\n* \nBut when dealing with SEASONALITY, it is best to incorporate it as 's'. \nARIMA(p,d,q)(P,D,Q)s.\nWhere 'pdq' are non seasonal params and 's' is the perdiocity of the time series. 4:quarter, 12:yearly etc.\n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, \nshort for \u2018Seasonal ARIMA\u2019.","9388791e":"Model Validation","c8aa22a9":"**Grid Search**\n\nSince ARIMA has hyper params that can be tuned, the objective here is to find the best params using Grid Search.","c393eb7f":"***STEPS\/Work Flow***\n\n* Import data and libaries: Self Explanatory\n* Explore the Dataset: Check Nulls, Skew, Data Types, Univariate and Bivariate Analysis.\n* Explore the Dataset as a Time Series: Min\/Max Dates, Check Seasonality, Trends etc.\n* ARIMA Model - Some Theory, Choose Best Params (Grid Seach), Build\/Fit, Validation, Forecast Accuracy. \n* The END.\n\nI will try and implement some other models including deep network and Facebook's Prophet in later commits.","1a7d9444":"### Author : Sanjoy Biswas\n### Project : Forecast Order Demand and Visualization\n### Email : sanjoy.eee32@gmail.com","b51bc6e2":"Fit the Model","d4fd7efb":"Decomposition is a great way to check the seasonality, trends and residuals.","3b636040":"**Exploring the Data as TIME SERIES**","0a8dc7d4":"Bivariate Analysis - Warehouse, Product Category with target variable.\n","d2d233ab":"Forecast Accuracy","1b052c71":"Lets inspect the data to see why a linear model without any transformation would fail to capture the essence of the data.","c3028f3d":"Interpreting the table:\n\ncoeff: Shows weight\/impotance how each feature impacts the time series.\nPvalue: Shows the significance of each feature weight. Can test hypothesis using this.\nIf p value is <.05 then they are statitically significant.\n\nRefresher on null hyp and pvalues.\nBy default we take the null hyp as 'there is no relationship bw them'\nIf p value < .05 (significance level) then you reject the Null Hypthesis\nIf p value > .05 , then you fail to reject the Null Hypothesis.\n\nSo, if the p-value is < .05 then there is a relationship between the response and predictor. Hence, significant."}}