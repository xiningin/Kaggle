{"cell_type":{"32fbea27":"code","f35013c5":"code","0c3e8e1c":"code","a6284c1f":"code","353ddff7":"code","b6de87a8":"code","ed94c384":"code","d9077a0d":"code","24b128c3":"markdown","84ba3c3e":"markdown"},"source":{"32fbea27":"#import require modules\/packages\/libraries\nimport pandas as pd #for reading csv files\nimport numpy as np \nimport tensorflow as tf \n\nfrom tensorflow import keras #we are going to code using keras\nfrom keras.models import Sequential, Model #we can use Sequential or Model to define model\nfrom keras.layers import Dense, BatchNormalization, Activation, Input #Importing layers which we will use in the model\nfrom keras.utils.np_utils import to_categorical #for encoding target labels\nfrom keras.callbacks import ReduceLROnPlateau #this will allow learning rate to be changed during training\n","f35013c5":"#Let us read the data first\ntrdf=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv', header='infer')\ntsdf=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv', header='infer')\n\n#Separate X_train (image) and y_train (class label) and get numpy arrays\ny_train=trdf[\"label\"].values\nX_train=trdf.iloc[:,1:].values\n\n#numpy arrays of test images\nX_test=tsdf.values\n\n#delete dataframe, they are no longer reuired\ndel trdf, tsdf\n\n#Check the data type and size\nprint(type(X_train), X_train.shape) #ndarray of size 42K X 784\nprint(type(X_test), X_test.shape) #ndarray of size 28K X 784\nprint(type(y_train), y_train.shape) #ndarray of size 42K","0c3e8e1c":"#Normalized images such that pixel intensities are in the range of -1 to 1. At present images are\n#8 bit gray-scale images with intensities between 0 to 255.\n\nX_train=(X_train-127.5)\/127.5\nX_test=(X_test-127.5)\/127.5\n\n#One-hot encode class labels, I mean y_train\ny_train=to_categorical(y_train)\n\nprint(type(y_train), y_train.shape) #Notice the output, y_train is now one-hot encoded\nprint(X_train.dtype, y_train.dtype, X_test.dtype)","a6284c1f":"#setup callback\nreduceLROnPlateau=ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n#We are going to monitor validation accuracy and going to have patience of 3 epochs (that is we will wait for 3 epochs without improvement)\n#before we set new lr = lr*factor. \n#Verbose is 1 which indicates that the update messages will be displayed, in case if it 0, no messages will\n#be displayed","353ddff7":"#Create model using Sequential\nffnn=Sequential() #allows us to add layers sequrntially\nffnn.add(Dense(units=512, input_shape=(784,))) #Dense (Fully connected) layer with 512 neurons\nffnn.add(BatchNormalization()) #Batchnormalization before activation\nffnn.add(Activation('relu')) #applying relu activation\nffnn.add(Dense(units=256))\nffnn.add(BatchNormalization())\nffnn.add(Activation('relu'))\nffnn.add(Dense(units=128))\nffnn.add(BatchNormalization())\nffnn.add(Activation('relu'))\nffnn.add(Dense(units=64))\nffnn.add(BatchNormalization())\nffnn.add(Activation('relu'))\nffnn.add(Dense(units=32))\nffnn.add(BatchNormalization())\nffnn.add(Activation('relu'))\nffnn.add(Dense(units=10))\nffnn.add(BatchNormalization())\nffnn.add(Activation('softmax')) #to output probability vector\n\n#Set compile settings\nffnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#display summary of the model\nffnn.summary()\n\n#display the model\ntf.keras.utils.plot_model(ffnn)","b6de87a8":"'''\n#Alternatively model can be created using Model\n\ninp=Input(shape=(784,)) #start with input layer\n\nd1=Dense(units=512)(inp)\nb1=BatchNormalization()(d1)\na1=Activation('relu')(b1)\n\nd2=Dense(units=256)(a1)\nb2=BatchNormalization()(d2)\na2=Activation('relu')(b2)\n\nd3=Dense(units=128)(a2)\nb3=BatchNormalization()(d3)\na3=Activation('relu')(b3)\n\nd4=Dense(units=64)(a3)\nb4=BatchNormalization()(d4)\na4=Activation('relu')(b4)\n\nd5=Dense(units=32)(a4)\nb5=BatchNormalization()(d5)\na5=Activation('relu')(b5)\n\nd6=Dense(units=10)(a5)\nb6=BatchNormalization()(d6)\na6=Activation('softmax')(b6)\n\nffnn=Model(inputs=inp, outputs=a6)\nffnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nffnn.summary()\ntf.keras.utils.plot_model(ffnn)\n\n'''","ed94c384":"#fit the model. We are setting number of epochs = 150 and batch_size is 120. validation_split is 0.2 that\n#is 20% data will be used for validation\nffnn.fit(x=X_train, y=y_train, epochs=150, callbacks=[reduceLROnPlateau], batch_size=120, validation_split=0.2)","d9077a0d":"predictions=ffnn.predict(X_test) #make predictions on test images\nprint(predictions.shape) #This will be 28K x 10. For each test image, probability of 10 classes\n\n#Decide class label for each test image based on max probability.\n#This will be vector of size 28K. \npredictions=np.argmax(predictions,axis=1) \nprint(predictions.shape) \n\n\n#read sample_submission.csv in the dataframe. The dataframe will have 2 columns ImageId and Label\nsub=pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv', header='infer')\n\n#\nsub[\"Label\"]=predictions\n\nsub.to_csv('submission.csv', index=False)\n\n#Please Upvote the notebook, if you find it useful.","24b128c3":"In this notebook, we use feed forward neural network (FFNN) and create a baseline. In the next notebook, we will use convolutional neural network with data augmentation.\n\nSteps:\n1. Import require modules\/packages\/libraries.\n2. Read training and test data.\n3. Separate X_train (image) and y_train (class label) and get numpy arrays from training and test data.\n4. Normalize train and test images.\n5. One-hot encode class labels.\n6. Setup callbacks. We only set ReduceLROnPlateau which helps in reducing learning rate (LR) during training best on specified conditions in setup.\n7. Define FFNN model using Sequential or Model. Compile model.\n8. Display model summary and model plot. \n9. Fit (train) Model.\n10. Predict for test images.\n11. Read sample_submission.csv in a dataframe.\n12. Overwrite \"Label\" column in the dataframe with predictions.\n13. Write dataframe as submission.csv.\n\nPlease Upvote the notebook, if you find it useful.","84ba3c3e":"Digit Recognizer:\n\nThe competion is about classifying popular MNIST images. These images are of handwritten English digits. Ten classes from 0 to 9. Images are of size 28 x 28 but they are flatten and given as row vectors in train and test file. Train and test files have 42K and 28K rows respectively corresponding to 42K train images and 28K test images. Training file has 785 columns.  The first column is class label and next 784 are pixel intensity of an image (flatten image). Test file has 784 columns as it does not have class label for the images. Sample Submission file has 2 columns, the first one is ImageId and the second one is Label. Read this file in a dataframe. We then need to overwrite 28K predictied labels on Label column of the dataframe and save the dataframe file as submission.csv"}}