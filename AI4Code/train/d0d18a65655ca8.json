{"cell_type":{"04d53524":"code","b14872a8":"code","b603bf85":"code","eb423d83":"code","814b11a1":"code","130fc33f":"code","bc62bfca":"code","b2b73cdf":"markdown","c100cc5b":"markdown","c30e02ff":"markdown","2fe902e6":"markdown","a6e29b97":"markdown","b555c8a8":"markdown"},"source":{"04d53524":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\nimport gc\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa","b14872a8":"train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]","b603bf85":"train.fillna(train.mean(),inplace=True)\nfeatures = [c for c in train.columns if 'feature' in c]\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\n#y_train = (train.loc[:, 'action'])\n\n# resp_1 > 0 resp_2 > 0 ...\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","eb423d83":"# from the comments https:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn\n# 1111 gives the best result\n#  I tried other seed, and got high score that is more than 8000. \n# I have tried a couple of random seeds and the worst only gives 5000 score. \nSEED = 1112\nnp.random.seed(SEED)\n\n#  initial parameters from Keras Tuner bayesian optimization\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate), # RectifiedAdam Optimizer (known to be robust to the choice in learning rate)\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    ) \n\n    return model\n\nepochs = 2000\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\n#Label Smoothing is a regularization technique that introduces noise for the labels. \n#This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of directly can be harmful.\n#Assume for a small constant e, the training set label y is correct with probability 1 - e and incorrect otherwise. \n#Label Smoothing regularizes a model based on a softmax with k output values by replacing the hard 0 and 1 classification targets \n#with e\/k-1 targets of and 1 - e respectively.\nlearning_rate = 1e-5\n#normally the model training with a batch size of 4096 and learning rate 1e-3 starts to overfit \n#on the train set after only 10 epochs. \n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\n\nfrom keras.callbacks import ModelCheckpoint\nfilepath=\"weights-improvement-{epoch:02d}-{val_AUC:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_AUC', verbose=1, save_best_only=True,mode='max')\n\nclf = create_mlp(len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)\nhistory = clf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0,validation_split=0.2,callbacks=[checkpoint])\n\nauc = history.history['AUC']\nval_auc = history.history['val_AUC']\nloss_score = history.history['loss']\nval_loss_score = history.history['val_loss']\n\nhistory_df = pd.DataFrame({'AUC':auc,'val_AUC':val_auc,'loss':loss_score,'val_loss':val_loss_score})\n\nhistory_df.to_csv('history_df.csv')\n\n# save model\nclf.save(f'model_v2.h5')","814b11a1":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nplt.plot(history.history['AUC'])\nplt.plot(history.history['val_AUC'])\nplt.legend(['train', 'test'], loc='upper left') \nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.title('model accuracy')\nplt.show()","130fc33f":"fig = plt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left') \nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.title('model loss')\nplt.show()","bc62bfca":"th = 0.503 # https:\/\/www.kaggle.com\/gkoundry\/the-most-important-model-parameter\n# This parameter controls the ratio of ones and zeros in the prediction. If the market is going up, then you will want to predict more ones than zeros.\n# However it's a little more complicated than this as you need to take the weights into consideration and not just the direction of the whole market:$\n# In the training data the overall market is going up, but the weighted returns are trending downwards so you would want to predict fewer ones for this time period.\n# => It's the threshold, and yes it's an intuitive experience. The model predicts probabilities of resp between 0 and 1, \n# compares its median to the th, and then assigns 0 or 1. \n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nimport janestreet\nenv = janestreet.make_env()\n\nfor (test_df, pred_df) in tqdm(env.iter_test()): \n    if test_df['weight'].item() > 0:\n        \n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            # np.nan_to_num : Replace NaN with zero and infinity with large finite numbers\n            # np.isnan : return a boolean list\n            \n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            \n        pred = np.median(clf(x_tt))\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","b2b73cdf":"Score of the version 1 : 10609.091 (slightly better than the original 10606.782)\n\n.................... 2 : 7340.576\n\n.................... 3 : 7521.166\n\n.................... 6 : 6567.191\n\n.................... 7 : Pending\n\n## Kudos: \ud83d\udd25\ud83d\udd25\ud83d\udd25\n\n[jane street with keras nn overfit](https:\/\/www.kaggle.com\/code1110\/jane-street-with-keras-nn-overfit\/)\n\n[OWN Jane Street with Keras NN](https:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn)\n\n\n\nThis notebook overfits after almost 10 epochs, but it's good to make some experimentation.\n\nYou can add a validation set, experiment with things like Purged Time Series...\n\n\n## If you change the following your score will changes a lot in some case:\n\n1\/ Seed : I changed from 1111 to 73 and 1112, and the score changes, so having a good seed guess will work for your advantage.\n\n2\/ th : it's the threshold, parameter, and the best is 0.503 (the detail are below).\n\n3\/ optimizers : Comparing the two notebook mentionned above, RectifiedAdam really gives better results.\n\n4\/ epochs : starts over fitting around 15 epochs, so a higher epoch number will give you a better score on the public dataset, but probably won't generilize well. If you want to use a high number of epochs, and a lot of parameters you should have a validation dataset to watch out for overfitting.\n\n5\/ hidden_units : I have increased the hidden units number, but the score went down.\n\n6\/ date :  date > 85 because the data changes after around 4 months, and here we can make two models, one for the first 4 months, and another one for the rest, after that we stack them together.\n\n7\/ feature_0 :  1 for buy and -1 for sell (Bid\/Ask), and weight for trading volume.  \n=> Buy less, Sell more, so you increase the treshold for buy orders to 0.503 while for sell order reduce the threshold to 0.498, and we can drop this feature before training the model, and change it with other instructions in the prediction phase.\n\n### P.S.\nThose were a couple of my observations, and some of the comments that I have found usefull in the comment section on the above mentionned notebooks.\n\nPlease feel free to comment, and if You feel there is an added value of this NoteBook, consider to vote it up =)","c100cc5b":"# Libs","c30e02ff":"# Data Import and Processing","2fe902e6":"Copied from the awesome notebook \"Simple MLP\".\nJust add simple plot to show kaggle novices like me why it is overfitting. ","a6e29b97":"# Prediction","b555c8a8":"# Model"}}