{"cell_type":{"e9c49433":"code","0393ed1d":"code","e3e03e47":"code","c6e2ce80":"code","90295f10":"code","437a542e":"code","cda5908c":"code","1d40ec0c":"code","6d117d66":"code","e7a24aef":"code","4ba5efc3":"code","a562c407":"code","c80c8a83":"code","98608200":"code","6e4749d7":"code","bf7ee20d":"code","d9c5fc49":"code","07d07f3f":"code","2ef73c78":"code","e1d40743":"code","281f537b":"code","0171ff07":"code","25d43317":"code","59e7b335":"code","3558f082":"code","64dd805c":"code","d063f037":"code","a09b9bc5":"code","2e45a524":"code","55ee7d2c":"code","81be50de":"code","f745c76c":"code","fc2dd417":"code","dce318d4":"code","29aa5d45":"code","a6be93a4":"code","d8f61aab":"code","3ecadd87":"code","69ca514d":"code","ecbf5a0a":"code","8b5a3892":"code","9e15eb5e":"code","ef012e85":"code","217a4b6a":"code","9d3e8690":"code","90ceb965":"code","e0c411c8":"code","574f9bd3":"code","0c35c56c":"code","a3585d1f":"code","28e4a107":"code","3eed33ef":"code","4888fdcf":"code","16d6517d":"code","10d87c67":"code","6ec98377":"code","8f92fb42":"code","77aeb121":"code","ce7968a0":"code","eb2faa5d":"code","dc354c5e":"code","31f2c445":"code","924e92e6":"code","e4d5f8a8":"code","dc74b857":"markdown","dc17c1ee":"markdown","8c682a05":"markdown","953050d2":"markdown","a7fad56e":"markdown","d57ccd27":"markdown","3361b313":"markdown","4bc17fb9":"markdown","228c31cf":"markdown","6320e582":"markdown","f209a882":"markdown","14ae81db":"markdown","50e85eb2":"markdown","7f04483c":"markdown","92c13fe3":"markdown","a5ac1e97":"markdown","37d4160a":"markdown","2ba406e0":"markdown","25eb812e":"markdown","3f7b860a":"markdown","ea47a8b5":"markdown","aa92f3ac":"markdown","df1ce3a5":"markdown","d65f5eea":"markdown","fe2be8f3":"markdown","daae199a":"markdown","cdf0a43d":"markdown","6267d525":"markdown","5ac9701f":"markdown","6d9da24e":"markdown","c8c2b201":"markdown","9457bb56":"markdown","a04141a9":"markdown","387b49e9":"markdown","9cc8f8fe":"markdown","d0f27dae":"markdown","ef9873aa":"markdown","7e5a5d4b":"markdown","c9638382":"markdown","0aeac446":"markdown","63301e2f":"markdown","3a8cec35":"markdown","ea35daae":"markdown"},"source":{"e9c49433":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0393ed1d":"import time\nnotebookstart= time.time()\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n# Simply works with Numpy, Matplotlib, Pandas, Sympy etc. \n# SciPy provides numerical integral routines and differential equations interpreters, algorithms to root out equations, standard continuous\/differentiated probability distributions, and various statistical tools.","e3e03e47":"df_train =  pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows = 100000, parse_dates=[\"pickup_datetime\"])\n\ndf_train.head()","c6e2ce80":"df_test =  pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv', parse_dates=[\"pickup_datetime\"])\n\ndf_test.head()","90295f10":"df_train.info()","437a542e":"df_test.info()","cda5908c":"df_train.shape, df_test.shape","1d40ec0c":"sns.distplot(df_train['fare_amount'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['fare_amount'], plot=plt)\nplt.show()","6d117d66":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"fare_amount\"] = np.log1p(df_train[\"fare_amount\"])\n\n#Check the new distribution \nsns.distplot(df_train['fare_amount'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['fare_amount'], plot=plt)\nplt.show()","e7a24aef":"df_train.isnull().sum()","4ba5efc3":"df_train = df_train.dropna()","a562c407":"df_train.isnull().sum()","c80c8a83":"df_train.shape","98608200":"fig, ax = plt.subplots()\nax.scatter(x = df_train['passenger_count'], y = df_train['fare_amount'])\nplt.ylabel('fare_amount', fontsize=13)\nplt.xlabel('passenger_count', fontsize=13)\nplt.show()","6e4749d7":"df_train['passenger_count'].value_counts()","bf7ee20d":"df_train[df_train['fare_amount'] > 600]['fare_amount'].value_counts()","d9c5fc49":"df_train[df_train['fare_amount'] < 0]['fare_amount'].value_counts()","07d07f3f":"df_train[\"year\"] = df_train[\"pickup_datetime\"].dt.year\ndf_train[\"month\"] = df_train[\"pickup_datetime\"].dt.month\ndf_train[\"day\"] = df_train[\"pickup_datetime\"].dt.day\ndf_train[\"hour\"] = df_train[\"pickup_datetime\"].dt.hour\ndf_train[\"dayofweek\"] = df_train[\"pickup_datetime\"].dt.dayofweek","2ef73c78":"df_train","e1d40743":"fig, axes = plt.subplots(nrows=5)\nfig.set_size_inches(18,14)\n\nplt.sca(axes[0])\nplt.xticks(rotation=30, ha='right')\naxes[0].set(ylabel='fare_amount',title=\"Annual Fare\")\nsns.pointplot(data = df_train, x=\"year\", y=\"fare_amount\", ax=axes[0])\n\nplt.sca(axes[1])\nplt.xticks(rotation=30, ha='right')\naxes[1].set(ylabel='fare_amount',title=\"Monthly Fare\")\nsns.pointplot(data = df_train, x=\"month\", y=\"fare_amount\", ax=axes[1])\n\nplt.sca(axes[2])\nplt.xticks(rotation=30, ha='right')\naxes[2].set(ylabel='fare_amount',title=\"Daily Fare\")\nsns.pointplot(data = df_train, x=\"day\", y=\"fare_amount\", ax=axes[2])\n\nplt.sca(axes[3])\nplt.xticks(rotation=30, ha='right')\naxes[3].set(ylabel='fare_amount',title=\"hourly Fare\")\nsns.pointplot(data = df_train, x=\"hour\", y=\"fare_amount\", ax=axes[3])\n\nplt.sca(axes[4])\nplt.xticks(rotation=30, ha='right')\naxes[4].set(ylabel='fare_amount',title=\"Fare by Day\")\nsns.pointplot(data = df_train, x=\"dayofweek\", y=\"fare_amount\", ax=axes[4])","281f537b":"df_train[(df_train['pickup_longitude'] > 180) | (df_train['pickup_longitude'] < -180)]['pickup_longitude'].value_counts()","0171ff07":"df_train[(df_train['pickup_latitude'] > 90) | (df_train['pickup_latitude'] < -90)]['pickup_latitude'].value_counts()","25d43317":"df_train[(df_train['dropoff_longitude'] > 180) | (df_train['dropoff_longitude'] < -180)]['dropoff_longitude'].value_counts()","59e7b335":"df_train[(df_train['dropoff_latitude'] > 90) | (df_train['dropoff_latitude'] < -90)]['dropoff_latitude'].value_counts()","3558f082":"train = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows = 5000, index_col = \"key\")\ntrain = train.dropna()\ntest_df = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv', index_col = \"key\")\ntestdex = test_df.index","64dd805c":"def prepare_distance_features(df):\n    # Distance is expected to have an impact on the fare\n    df['longitude_distance'] = abs(df['pickup_longitude'] - df['dropoff_longitude'])\n    df['latitude_distance'] = abs(df['pickup_latitude'] - df['dropoff_latitude'])\n\n    # Straight distance\n    df['distance_travelled'] = (df['longitude_distance'] ** 2 + df['latitude_distance'] ** 2) ** .5\n    df['distance_travelled_sin'] = np.sin((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5)\n    df['distance_travelled_cos'] = np.cos((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5)\n    df['distance_travelled_sin_sqrd'] = np.sin((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5) ** 2\n    df['distance_travelled_cos_sqrd'] = np.cos((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5) ** 2\n\n    # Haversine formula for distance\n    # Haversine formula:\ta = sin\u00b2(\u0394\u03c6\/2) + cos \u03c61 \u22c5 cos \u03c62 \u22c5 sin\u00b2(\u0394\u03bb\/2)\n    R = 6371e3 # Metres\n    phi1 = np.radians(df['pickup_latitude'])\n    phi2 = np.radians(df['dropoff_latitude'])\n    phi_chg = np.radians(df['pickup_latitude'] - df['dropoff_latitude'])\n    delta_chg = np.radians(df['pickup_longitude'] - df['dropoff_longitude'])\n    a = np.sin(phi_chg \/ 2) + np.cos(phi1) * np.cos(phi2) * np.sin(delta_chg \/ 2)\n    c = 2 * np.arctan2(a ** .5, (1-a) ** .5)\n    d = R * c\n    df['haversine'] = d\n\n    # Bearing\n    # Formula:\t\u03b8 = atan2( sin \u0394\u03bb \u22c5 cos \u03c62 , cos \u03c61 \u22c5 sin \u03c62 \u2212 sin \u03c61 \u22c5 cos \u03c62 \u22c5 cos \u0394\u03bb )\n    y = np.sin(delta_chg * np.cos(phi2))\n    x = np.cos(phi1) * np.sin(phi2) - np.sin(phi1) * np.cos(phi2) * np.cos(delta_chg)\n    df['bearing'] = np.arctan2(y, x)\n\n    return df\n\ndef prepare_time_features(df):\n    df['pickup_datetime'] = df['pickup_datetime'].str.replace(\" UTC\", \"\")\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n    df['hour_of_day'] = df.pickup_datetime.dt.hour\n    df['week'] = df.pickup_datetime.dt.week\n    df['month'] = df.pickup_datetime.dt.month\n    df[\"year\"] = df.pickup_datetime.dt.year\n    df['day_of_year'] = df.pickup_datetime.dt.dayofyear\n    df['week_of_year'] = df.pickup_datetime.dt.weekofyear\n    df[\"weekday\"] = df.pickup_datetime.dt.weekday\n    df[\"quarter\"] = df.pickup_datetime.dt.quarter\n    df[\"day_of_month\"] = df.pickup_datetime.dt.day\n    \n    return df\n\n# Airport Features - By Albert van Breenmen\n# https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration\ndef dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance\n\ndef airport_feats(train,test_df):\n    for data in [train,test_df]:\n        nyc = (-74.0063889, 40.7141667)\n        jfk = (-73.7822222222, 40.6441666667)\n        ewr = (-74.175, 40.69)\n        lgr = (-73.87, 40.77)\n        data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n        data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                             data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n        data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                              data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n        data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                              data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n    return train, test_df\n\n# Percentile\ndef percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\n\n# Build ime Aggregate Features\ndef time_agg(train, test_df, vars_to_agg, vars_be_agg):\n    for var in vars_to_agg:\n        agg = train.groupby(var)[vars_be_agg].agg([\"sum\",\"mean\",\"std\",\"skew\",percentile(80),percentile(20)])\n        if isinstance(var, list):\n            agg.columns = pd.Index([\"fare_by_\" + \"_\".join(var) + \"_\" + str(e) for e in agg.columns.tolist()])\n        else:\n            agg.columns = pd.Index([\"fare_by_\" + var + \"_\" + str(e) for e in agg.columns.tolist()]) \n        train = pd.merge(train,agg, on=var, how= \"left\")\n        test_df = pd.merge(test_df,agg, on=var, how= \"left\")\n    \n    return train, test_df\n\n# Clean dataset from https:\/\/www.kaggle.com\/gunbl4d3\/xgboost-ing-taxi-fares\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45)]\nprint(\"Cleaning Functions Defined..\")","d063f037":"print(\"Percent of Training Set with Zero and Below Fair: \", round(((train.loc[train[\"fare_amount\"] <= 0, \"fare_amount\"].shape[0]\/train.shape[0]) * 100),5))\nprint(\"Percent of Training Set 200 and Above Fair: \", round((train.loc[train[\"fare_amount\"] >= 200, \"fare_amount\"].shape[0]\/train.shape[0]) * 100,5))\ntrain = train.loc[(train[\"fare_amount\"] > 0) & (train[\"fare_amount\"] <= 200),:]\nprint(\"\\nPercent of Training Set with Zero and Below Passenger Count: \", round((train.loc[train[\"passenger_count\"] <= 0, \"passenger_count\"].shape[0]\/train.shape[0]) * 100,5))\nprint(\"Percent of Training Set with Nine and Above Passenger Count: \", round((train.loc[train[\"passenger_count\"] >= 9, \"passenger_count\"].shape[0]\/train.shape[0]) * 100,5))\ntrain = train.loc[(train[\"passenger_count\"] > 0) & (train[\"passenger_count\"] <= 9),:]\n\n# Clean Training Set\ntrain = clean_df(train)\n\n# Distance Features\ntrain = prepare_distance_features(train)\ntest_df = prepare_distance_features(test_df)\ntrain,test_df = airport_feats(train,test_df)\n\n# Time Features\ntrain = prepare_time_features(train)\ntest_df = prepare_time_features(test_df)\n\n# Ratios\ntrain[\"fare_to_dist_ratio\"] = train[\"fare_amount\"] \/ ( train[\"distance_travelled\"]+0.0001)\ntrain[\"fare_npassenger_to_dist_ratio\"] = (train[\"fare_amount\"] \/ train[\"passenger_count\"]) \/( train[\"distance_travelled\"]+0.0001)\n\n# Time Aggregate Features\ntrain, test_df = time_agg(train, test_df,\n                          vars_to_agg  = [\"passenger_count\", \"weekday\", \"quarter\", \"month\", \"year\", \"hour_of_day\",\n                                          [\"weekday\", \"month\", \"year\"], [\"hour_of_day\", \"weekday\", \"month\", \"year\"]],\n                          vars_be_agg = \"fare_amount\")","a09b9bc5":"train_time_start = train.pickup_datetime.min()\ntrain_time_end = train.pickup_datetime.max()\nprint(\"Train Time Starts: {}, Ends {}\".format(train_time_start,train_time_end))\ntest_time_start = test_df.pickup_datetime.min()\ntest_time_end = test_df.pickup_datetime.max()\nprint(\"Test Time Starts: {}, Ends {}\".format(test_time_start,test_time_end))","2e45a524":"train_data_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_data_na = train_data_na.drop(train_data_na[train_data_na == 0].index).sort_values(ascending=False)[:30]","55ee7d2c":"missing_data = pd.DataFrame({'Missing Ratio' :train_data_na})\nmissing_data.head(20)","81be50de":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='45')\nsns.barplot(x=train_data_na.index, y=train_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","f745c76c":"test_data_na = (test_df.isnull().sum() \/ len(test_df)) * 100\ntest_data_na = test_data_na.drop(test_data_na[test_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({'Missing Ratio' :test_data_na})\nmissing_data.head(20)","fc2dd417":"# df.mode() : The value that occurs most frequently\n\n# train['haversine'] = train['haversine'].fillna(train['haversine'].mode()[0])\n# test_df['haversine'] = test_df['haversine'].fillna(test_df['haversine'].mode()[0])\n\ntrain['fare_by_weekday_month_year_skew'] = train['fare_by_weekday_month_year_skew'].fillna(train['fare_by_weekday_month_year_skew'].mode()[0])\ntest_df['fare_by_weekday_month_year_skew'] = test_df['fare_by_weekday_month_year_skew'].fillna(test_df['fare_by_weekday_month_year_skew'].mode()[0])\n\ntrain['fare_by_weekday_month_year_std'] = train['fare_by_weekday_month_year_std'].fillna(train['fare_by_weekday_month_year_std'].mode()[0])\ntest_df['fare_by_weekday_month_year_std'] = test_df['fare_by_weekday_month_year_std'].fillna(test_df['fare_by_weekday_month_year_std'].mode()[0])\n\n# train['fare_by_hour_of_day_weekday_month_year_skew'] = train['fare_by_hour_of_day_weekday_month_year_skew'].fillna(train['fare_by_hour_of_day_weekday_month_year_skew'].mode()[0])\n# test_df['fare_by_hour_of_day_weekday_month_year_skew'] = test_df['fare_by_hour_of_day_weekday_month_year_skew'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_skew'].mode()[0])\n\n# train['fare_by_hour_of_day_weekday_month_year_std'] = train['fare_by_hour_of_day_weekday_month_year_std'].fillna(train['fare_by_hour_of_day_weekday_month_year_std'].mode()[0])\n# test_df['fare_by_hour_of_day_weekday_month_year_std'] = test_df['fare_by_hour_of_day_weekday_month_year_std'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_std'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_sum'] = test_df['fare_by_hour_of_day_weekday_month_year_sum'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_sum'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_mean'] = test_df['fare_by_hour_of_day_weekday_month_year_mean'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_mean'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'] = test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'] = test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'].mode()[0])","dce318d4":"test_data_na = (test_df.isnull().sum() \/ len(test_df)) * 100\ntest_data_na = test_data_na.drop(test_data_na[test_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({'Missing Ratio' :test_data_na})\nmissing_data.head(20)","29aa5d45":"train_data_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_data_na = train_data_na.drop(train_data_na[train_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_data_na})\nmissing_data.head(20)","a6be93a4":"sns.distplot(train['fare_amount'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['fare_amount'], plot=plt)\nplt.show()","d8f61aab":"train[\"fare_amount\"] = np.log1p(train[\"fare_amount\"])\n\nsns.distplot(train['fare_amount'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['fare_amount'], plot=plt)\nplt.show()","3ecadd87":"train.head(5)","69ca514d":"y_train = train.fare_amount.values\nfeatures_drop = ['pickup_datetime','fare_by_hour_of_day_weekday_month_year_skew', 'fare_by_hour_of_day_weekday_month_year_std', 'fare_by_hour_of_day_weekday_month_year_sum', 'fare_by_hour_of_day_weekday_month_year_mean','fare_by_hour_of_day_weekday_month_year_percentile_80','fare_by_hour_of_day_weekday_month_year_percentile_20','haversine']\ntest_df.drop(features_drop, axis = 1, inplace=True)\ntrain = train[test_df.columns]\nprint(\"Does Train feature equal test feature?: \", all(train.columns == test_df.columns))","ecbf5a0a":"train.shape, test_df.shape, len(y_train)","8b5a3892":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","9e15eb5e":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","ef012e85":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=45))","217a4b6a":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","9d3e8690":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","90ceb965":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","e0c411c8":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","574f9bd3":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","0c35c56c":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a3585d1f":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","28e4a107":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3eed33ef":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4888fdcf":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","16d6517d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","10d87c67":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6ec98377":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","8f92fb42":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","77aeb121":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","ce7968a0":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df.values))\nprint(rmsle(y_train, stacked_train_pred))","eb2faa5d":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","dc354c5e":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","31f2c445":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","924e92e6":"ensemble = stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15","e4d5f8a8":"sub = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/sample_submission.csv')\nsubmission = pd.DataFrame()\nsubmission['key'] = sub['key']\nsubmission['fare_amount'] = ensemble\nsubmission.to_csv('submission_ensemble_1.csv',index=False)","dc74b857":"* **LightGBM**","dc17c1ee":"# **Collecting Data**","8c682a05":"* **Ensemble prediction**","953050d2":"* **Base models scores**","a7fad56e":"I thank the kernel for allowing me to study improved modeling.\n(https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)","d57ccd27":"**Stacking models**\n\nSimplest Stacking approach : Averaging base models\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","3361b313":"The target variable is skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","4bc17fb9":"* **Elastic Net Regression**","228c31cf":"* **pickup_longitude** - float for longitude coordinate of where the taxi ride started.\n* **pickup_latitude** - float for latitude coordinate of where the taxi ride started.\n* **dropoff_longitude** - float for longitude coordinate of where the taxi ride ended.\n* **dropoff_latitude** - float for latitude coordinate of where the taxi ride ended.","6320e582":"# **Final Training and Prediction**","f209a882":"* **Fare amount**\n* **pickup datetime** : value indicating when the taxi ride started.","14ae81db":"* **Target Variable**\n\n**Fare amount** is the variable we need to predict. So let's do some analysis on this variable first.","50e85eb2":"Missing values are very small, so it is believed to have a small impact on predictions. Therefore, it seems safe to remove it from the dataset.","7f04483c":"Let's find out the rate for each time zone!\nData will be divided by year, month, date, time, and day.","92c13fe3":"* **Time Range**","a5ac1e97":"Let's interpret the results!\n\n1. Fare increases over the years.\n2. Fare is high at the beginning of the month (4th to 6th)\n\nPlease let me know if you have any other information you can find out.","37d4160a":"**Note** :\n\nLatitude and longitude are a pair of numbers (coordinates) used to describe a position on the plane of a geographic coordinate system. The numbers are in decimal degrees format and range from -90 to 90 for latitude and -180 to 180 for longitude.","2ba406e0":"* **XGBoost**","25eb812e":"* Target Variable","3f7b860a":"# **Base models**\n\n* **LASSO Regression**\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","ea47a8b5":"* **StackedRegressor**","aa92f3ac":"* **Imputing missing values**","df1ce3a5":"* **Gradient Boosting Regression**","d65f5eea":"* **Fare amount** : dollar amount of the cost of the taxi ride. \n* **Passenger count** :  indicating the number of passengers in the taxi ride.","fe2be8f3":"* **LightGBM**","daae199a":"* **Keep Relevant Variables**","cdf0a43d":"* **Stacking averaged Models Class**","6267d525":"**Define a cross validation strategy**\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","5ac9701f":"* **Missing values**","6d9da24e":"* **Kernel Ridge Regression**","c8c2b201":"* **Averaged base models class**","9457bb56":"* **XGBoost**","a04141a9":"* **Log-transformation of the target variable**","387b49e9":"* Reference : https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\n\nThis work has already referred to well-organized kernels.","9cc8f8fe":"Values outside the latitude and longitude range can be determined as outliers, so we decided to remove them.","d0f27dae":"* **missing values**","ef9873aa":"* **Ensembling StackedRegressor, XGBoost and LightGBM**","7e5a5d4b":"**Stacking Averaged models Score**\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","c9638382":"# **Data Cleaning and Feature Engineering**","0aeac446":"We can find the outliers.\n1. Passenger_count == 200 in the bottom right can be judged by the outlier value.\n2. The high fare in the upper left is far from the distribution, so it is judged to be outlier.\n3. Fare determines that a rate below zero cannot exist as an outlier.\n\n\n**Note** :\n\nEliminating outliers is stable for creating robust models. Therefore, we will remove the abnormalities found above later.","63301e2f":"# **Import necessary librairies**","3a8cec35":"# **Mdelling**","ea35daae":"# **Exploratory data analysis**"}}