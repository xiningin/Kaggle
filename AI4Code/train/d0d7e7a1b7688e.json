{"cell_type":{"48fa41e3":"code","8dddaa3b":"code","3590fb3a":"code","6bc324a3":"code","1205b29a":"code","6bd9b35c":"code","61233f1c":"code","1fbdb335":"code","ca0d26af":"code","a719de94":"code","dd824b48":"code","bbdfb3ee":"code","5ebe3f78":"code","8db1b726":"code","43d1a1c2":"code","2328cd15":"code","b7092958":"code","4247e169":"code","120f22d3":"code","d7d2b1d3":"code","5d9b3e79":"code","602b5b0a":"code","bf118606":"code","5256d571":"code","6ec3810b":"code","db9694b6":"markdown","0a70a8fa":"markdown","d6414f16":"markdown","5174e11c":"markdown","6ce793fb":"markdown","c5e00c4e":"markdown","0cd9a197":"markdown","90777aa3":"markdown","23347d10":"markdown","54fd6a87":"markdown","ce832ff7":"markdown","e6d5a6ff":"markdown","0c3b9e78":"markdown","cd517f7b":"markdown","f1540726":"markdown"},"source":{"48fa41e3":"import numpy as np\nimport pandas as pd \nimport joblib\n        \nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8dddaa3b":"train = pd.read_csv('..\/input\/titanic\/train.csv')\n\ntrain.head() #Getting a quick glimpse at the dataset","3590fb3a":"print(train.shape) # Seeing the shape as well","6bc324a3":"y_train = train.Survived.values # Creating y_train that stores the \"correct answers\"\n\ntrain.drop(['PassengerId', 'Survived'], axis=1, inplace=True) \n#Dropping Survived since that would be cheating leaving that in my training set as well as passengerId since it provides no insight when training my models","1205b29a":"train.isnull().sum().to_frame().T # From this line I see I need to work on three columns in dealing with their missing values","6bd9b35c":"# Since there are 891 values in this dataset and 687 of those are missing the Cabin feature (77%) I have decided to not use that feature.\n# Will drop the name column here as well as it doesn't provide any statistical advantage when processing it\n\ntrain.drop(['Cabin', 'Name'], axis=1, inplace=True)\n\n# Fixing Age Missing Values\n\ntrain.Age.mean() # Average age of 29.699\n\n# Applying that Average to all Missing columns of Age\n\ntrain['Age'].fillna(value=train['Age'].mean(), inplace=True)\n\ntrain.isnull().sum().to_frame().T","61233f1c":"train.Embarked.unique()\n\n(train.Embarked.value_counts() \/ len(train)).to_frame() # With 72 % of Emabarked being S, I will fill in the two missing values as those.","1fbdb335":"train['Embarked'].fillna(value='S', inplace=True)\n\ntrain.isnull().sum().to_frame().T # Now we have no missing values are can start to preprocess and work with the data","ca0d26af":"train.dtypes.values == 'O' # False return a Numerical Column while True returns a Categorical Column\n# Must split features when using OneHotEncoding later on","a719de94":"cat_features = train.columns.values[train.dtypes.values == 'O'].tolist()\nprint(cat_features)","dd824b48":"train[cat_features].nunique() # With 681 unique varibales for ticket we won't get any useful information out of it thus we need to remove it from cat features","bbdfb3ee":"num_features = [c for c in train.columns if c not in cat_features]\nprint(num_features)","5ebe3f78":"cat_features.remove('Ticket')\ntrain.drop('Ticket', axis=1, inplace=True)","8db1b726":"features = cat_features + num_features\n\nprint(features)\n\n# May have seem weird to split just to rejoin them but will to use all three in the future","43d1a1c2":"numerical_transformer = Pipeline(\n    steps = [\n        ('scaler', StandardScaler())\n    ])\n\ncategorical_transformer = Pipeline(\n    steps = [\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('numerical', numerical_transformer, num_features),\n        ('categorical', categorical_transformer, cat_features)\n    ])\n\npreprocessor.fit(train)\nx_train = preprocessor.transform(train)\n\nprint(x_train.shape)","2328cd15":"lr_class = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio': [0, 1],\n    'C': [0.5, 0.6, 0.7, 1, 10]\n}\n\nlr_grid = GridSearchCV(lr_class, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(x_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:', lr_grid.best_score_)\nprint('Training Accuracy:', lr_model.score(x_train, y_train))","b7092958":"lr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nfor p, s in zip(lr_grid.cv_results_['params'], lr_grid.cv_results_['mean_test_score']):\n    print(f\"l1: {p['l1_ratio']:<.3f},  C: {p['C']:>8.3f},  score: {s:.4f}\")","4247e169":"dt_class = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [2, 3, 4, 6, 8, 10, 12],\n    'min_samples_leaf': [1, 2, 4, 8]\n}\n\ndt_grid = GridSearchCV(dt_class, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\ndt_grid.fit(x_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:', dt_grid.best_score_)\nprint('Training Accuracy:', dt_model.score(x_train, y_train))","120f22d3":"dt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximun Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nfor p, s in zip(dt_grid.cv_results_['params'], dt_grid.cv_results_['mean_test_score']):\n    print(f\"depth: {p['max_depth']:<.3f},  min_inst: {p['min_samples_leaf']:>8.3f},  score: {s:.4f}\")","d7d2b1d3":"rf_class = RandomForestClassifier(random_state=1, n_estimators=50)\n\nrf_parameters = {\n    'max_depth': [2, 4, 6, 8, 20, 32],\n    'min_samples_leaf': [1, 2, 4, 8]\n}\n\nrf_grid = GridSearchCV(rf_class, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nrf_grid.fit(x_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:', rf_grid.best_score_)\nprint('Training Accuracy:', rf_model.score(x_train, y_train))","5d9b3e79":"rf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximun Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nfor p, s in zip(rf_grid.cv_results_['params'], rf_grid.cv_results_['mean_test_score']):\n    print(f\"depth: {p['max_depth']:<.3f},  min_inst: {p['min_samples_leaf']:>8.3f},  score: {s:.4f}\")","602b5b0a":"print('Logistic Regression Best CV Score:', lr_grid.best_score_)\nprint('Decision Tree Best CV Score:', dt_grid.best_score_)\nprint('Random Forest Best CV Score:', rf_grid.best_score_)\n\n# With a CV Score of 0.835 the Random Forest Model seems to be the best fit for the data set","bf118606":"#Getting the best parameter values for that Random Forest Model\nprint('Random Forest Best Parameters:', rf_grid.best_params_)","5256d571":"final_model = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=8, min_samples_leaf=2)\nfinal_model.fit(x_train, y_train)","6ec3810b":"joblib.dump(preprocessor, 'AS_preprocessor_titanic_new.joblib')\njoblib.dump(final_model, 'AS_model_titanic_new.joblib')","db9694b6":"## Preprocessing","0a70a8fa":"## Combining Features into List","d6414f16":"## Picking The Best Model","5174e11c":"## Splitting the Columns into Cat and Num ","6ce793fb":"## Finding Missing Values","c5e00c4e":"## Dropping Target Values and Placing Them in Another Dataframe","0cd9a197":"## Saving Final Model","90777aa3":"## Dealing With Missing Values","23347d10":"## Loading Data","54fd6a87":"## Performing GridSearch for a Logistic Regression Model","ce832ff7":"## Num Features","e6d5a6ff":"## Making a Final Model","0c3b9e78":"## Decision Tree Model","cd517f7b":"## Cat Features","f1540726":"## Random Forest Model"}}