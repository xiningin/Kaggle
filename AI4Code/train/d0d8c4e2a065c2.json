{"cell_type":{"ba94b173":"code","65bec64c":"code","6d94ff16":"code","6f4b78ca":"code","488a0c4f":"code","7cd5b5f2":"code","bcf6571d":"code","b522f23e":"code","31b864d3":"code","315bc797":"code","0c2f9fc2":"code","bd808db7":"code","e164b0b7":"code","6283f582":"code","98940472":"code","13572223":"code","cc0db84f":"code","f7145e93":"code","13433958":"code","3cf76474":"code","6baa8505":"code","43d9ee5f":"code","380cde41":"code","063c80bd":"code","177f82e2":"markdown","6f33f506":"markdown","bb6bd973":"markdown"},"source":{"ba94b173":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plot\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","65bec64c":"# Importing the dataset\ntrain_dataset = pd.read_csv('..\/input\/train.csv')\ntest_dataset = pd.read_csv('..\/input\/test.csv')\ntrain_dataset.describe()","6d94ff16":"train_dataset.head()","6f4b78ca":"test_dataset.head()","488a0c4f":"y_train = train_dataset.iloc[:, 1].values\n#X = dataset[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']]\nX_train = train_dataset.iloc[:, [0, 2, 4, 5, 6, 7, 11]].values\nX_test = test_dataset.iloc[:, [0, 1, 3, 4, 5, 6, 10]].values\n\nm = X_train.shape[0]\nfamily_size_column = np.zeros((m, 1))\nX_train = np.append(X_train, family_size_column, axis=1)\nX_train[:, 7] = 1 + X_train[:, 4] + X_train[:, 5]\nX_train[:, 1] = X_train[:, 1] * X_train[:, 7]\nX_train = np.delete(X_train, [4, 5, 7], 1)\n\nm = X_test.shape[0]\nfamily_size_column = np.zeros((m, 1))\nX_test = np.append(X_test, family_size_column, axis=1)\nX_test[:, 7] = 1 + X_test[:, 4] + X_test[:, 5]\nX_test[:, 1] = X_test[:, 1] * X_test[:, 7]\nX_test = np.delete(X_test, [4, 5, 7], 1)\n\nm = X_test.shape[0]\npred_column = np.zeros((m, 1))\nresult = test_dataset.iloc[:, [0]].values\nresult = np.append(result, pred_column, axis=1)\nresult = result.astype(int)\nresult","7cd5b5f2":"# NaN value count for Age column in training dataset\nnan_age_train = train_dataset[train_dataset['Age'].isnull()]\nnan_age_train.shape[0]","bcf6571d":"# NaN value count for Age column in testing dataset\nnan_age_test = test_dataset[test_dataset['Age'].isnull()]\nnan_age_test.shape[0]","b522f23e":"# NaN value count for Embarked column in training dataset\nnan_embarked_train = train_dataset[train_dataset['Embarked'].isnull()]\nnan_embarked_train.shape[0]","31b864d3":"# NaN value count for Embarked column in testing dataset\nnan_embarked_test = test_dataset[test_dataset['Embarked'].isnull()]\nnan_embarked_test.shape[0]","315bc797":"# Taking care of missing data\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values = np.nan, strategy= 'mean')\n\n# Fill mean values for Age where age value is NaN\nimputer = imputer.fit(X_train[:, 3:4])\nX_train[:, 3:4] = imputer.transform(X_train[:, 3:4])\n\nimputer = imputer.fit(X_test[:, 3:4])\nX_test[:, 3:4] = imputer.transform(X_test[:, 3:4])\n\n# Fill most_frequent values for Embarked where its value is NaN\n#imputer = SimpleImputer(missing_values = np.nan, strategy= 'most_frequent')\n#imputer = imputer.fit(X_train[:, 4:5])\n#X_train[:, 4:5] = imputer.transform(X_train[:, 4:5])\n\n#imputer = imputer.fit(X_test[:, 4:5])\n#X_test[:, 4:5] = imputer.transform(X_test[:, 4:5])\nX_train = np.delete(X_train, [4], 1)\nX_test = np.delete(X_test, [4], 1)","0c2f9fc2":"# Encoding categorical data\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Column transformer for \"Sex\" and \"Embarked\" columns\nct = ColumnTransformer(\n    [('oh_enc', OneHotEncoder(sparse=False), [2]),],  # the column numbers for which we want to apply this\n    remainder='passthrough'  # This leaves the rest of my columns in place\n)\nX_train = ct.fit_transform(X_train)\nX_test = ct.fit_transform(X_test)\n\nlabelencoder_y = LabelEncoder()\ny_train = labelencoder_y.fit_transform(y_train)","bd808db7":"print(X_train[0:5, :])\nX_train.shape","e164b0b7":"print(X_test[0:5, :])\nX_test.shape","6283f582":"# Delete dummy variable columns.\nX_train = np.delete(X_train, [1], 1)\nprint(X_train[0:5, :])\nX_train.shape","98940472":"# Delete dummy variable columns.\nX_test = np.delete(X_test, [1], 1)\nprint(X_test[0:5, :])\nX_test.shape","13572223":"# Feature Scalling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","cc0db84f":"# Fitting the Logistic Regression to the training set.\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nacc_log = round(classifier.score(X_train, y_train) * 100, 2)\nacc_log\n\n#result[:, 1] = y_pred\n#result","f7145e93":"# Support Vector Machines\n\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc\n\nresult[:, 1] = y_pred\nresult","13433958":"# k-Nearest Neighbors algorithm (or k-NN for short)\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n\n#result[:, 1] = y_pred\n#result\nacc_knn","3cf76474":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc","6baa8505":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","43d9ee5f":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest\n\n#result[:, 1] = y_pred\n#result","380cde41":"# Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':result[:, 0],'Survived':result[:, 1]})\n\n# Visualize the first 5 rows\nsubmission.head()","063c80bd":"# Create csv file for submission.\nfilename = 'Titanic_Survival_Trial_9.csv'\nsubmission.to_csv(filename, index=False)\n\nprint('Saved file: ' + filename)","177f82e2":"**Encode Categorical Data**","6f33f506":"**Taking care of Missing Data**","bb6bd973":"**Import Dataset**"}}