{"cell_type":{"047acec5":"code","56be657c":"code","6f66dbe3":"code","310afdd4":"code","efb7deb8":"code","1f26219a":"code","782ec97b":"code","983f899c":"code","9f5796bd":"code","66498022":"code","4ec58f2d":"markdown"},"source":{"047acec5":"\n!git clone https:\/\/github.com\/Dena\/HandyRL.git\n\n!cp -r HandyRL\/. .","56be657c":"!pip install -r requirements.txt\n!pip install catalyst\n!pip install kaggle_environments\n!mkdir models","6f66dbe3":"%%writefile handyrl\/envs\/kaggle\/hungry_geese.py\n# Copyright (c) 2020 DeNA Co., Ltd.\n# Licensed under The MIT License [see LICENSE for details]\n\n# kaggle_environments licensed under Copyright 2020 Kaggle Inc. and the Apache License, Version 2.0\n# (see https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/LICENSE for details)\n\n#changed\n# wrapper of Hungry Geese environment from kaggle\n\nimport random\nimport itertools\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# You need to install kaggle_environments, requests\nfrom kaggle_environments import make\n\nfrom ...environment import BaseEnvironment\n\n\nclass TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] \/\/ 2, kernel_size[1] \/\/ 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x, _=None):\n        h = F.hardswish(self.conv0(x),inplace=True)\n        for block in self.blocks:\n            h = F.hardswish(h + block(h),inplace=True)\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n\n        return {'policy': p, 'value': v}\n\n\nclass Environment(BaseEnvironment):\n    ACTION = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    DIRECTION = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n    NUM_AGENTS = 4\n\n    def __init__(self, args={}):\n        super().__init__()\n        self.env = make(\"hungry_geese\")\n        self.reset()\n\n    def reset(self, args={}):\n        obs = self.env.reset(num_agents=self.NUM_AGENTS)\n        self.update((obs, {}), True)\n\n    def update(self, info, reset):\n        obs, last_actions = info\n        if reset:\n            self.obs_list = []\n        self.obs_list.append(obs)\n        self.last_actions = last_actions\n\n    def action2str(self, a, player=None):\n        return self.ACTION[a]\n\n    def str2action(self, s, player=None):\n        return self.ACTION.index(s)\n\n    def direction(self, pos_from, pos_to):\n        if pos_from is None or pos_to is None:\n            return None\n        x, y = pos_from \/\/ 11, pos_from % 11\n        for i, d in enumerate(self.DIRECTION):\n            nx, ny = (x + d[0]) % 7, (y + d[1]) % 11\n            if nx * 11 + ny == pos_to:\n                return i\n        return None\n\n    def __str__(self):\n        # output state\n        obs = self.obs_list[-1][0]['observation']\n        colors = ['\\033[33m', '\\033[34m', '\\033[32m', '\\033[31m']\n        color_end = '\\033[0m'\n\n        def check_cell(pos):\n            for i, geese in enumerate(obs['geese']):\n                if pos in geese:\n                    if pos == geese[0]:\n                        return i, 'h'\n                    if pos == geese[-1]:\n                        return i, 't'\n                    index = geese.index(pos)\n                    pos_prev = geese[index - 1] if index > 0 else None\n                    pos_next = geese[index + 1] if index < len(geese) - 1 else None\n                    directions = [self.direction(pos, pos_prev), self.direction(pos, pos_next)]\n                    return i, directions\n            if pos in obs['food']:\n                return 'f'\n            return None\n\n        def cell_string(cell):\n            if cell is None:\n                return '.'\n            elif cell == 'f':\n                return 'f'\n            else:\n                index, directions = cell\n                if directions == 'h':\n                    return colors[index] + '@' + color_end\n                elif directions == 't':\n                    return colors[index] + '*' + color_end\n                elif max(directions) < 2:\n                    return colors[index] + '|' + color_end\n                elif min(directions) >= 2:\n                    return colors[index] + '-' + color_end\n                else:\n                    return colors[index] + '+' + color_end\n\n        cell_status = [check_cell(pos) for pos in range(7 * 11)]\n\n        s = 'turn %d\\n' % len(self.obs_list)\n        for x in range(7):\n            for y in range(11):\n                pos = x * 11 + y\n                s += cell_string(cell_status[pos])\n            s += '\\n'\n        for i, geese in enumerate(obs['geese']):\n            s += colors[i] + str(len(geese) or '-') + color_end + ' '\n        return s\n\n    def step(self, actions):\n        # state transition\n        obs = self.env.step([self.action2str(actions.get(p, None) or 0) for p in self.players()])\n        self.update((obs, actions), False)\n\n    def diff_info(self, _):\n        return self.obs_list[-1], self.last_actions\n\n    def turns(self):\n        # players to move\n        return [p for p in self.players() if self.obs_list[-1][p]['status'] == 'ACTIVE']\n\n    def terminal(self):\n        # check whether terminal state or not\n        for obs in self.obs_list[-1]:\n            if obs['status'] == 'ACTIVE':\n                return False\n        return True\n\n    def outcome(self):\n        # return terminal outcomes\n        # 1st: 1.0 2nd: 0.33 3rd: -0.33 4th: -1.00\n        rewards = {o['observation']['index']: o['reward'] for o in self.obs_list[-1]}\n        outcomes = {p: 0 for p in self.players()}\n        for p, r in rewards.items():\n            for pp, rr in rewards.items():\n                if p != pp:\n                    if r > rr:\n                        outcomes[p] += 1 \/ (self.NUM_AGENTS - 1)\n                    elif r < rr:\n                        outcomes[p] -= 1 \/ (self.NUM_AGENTS - 1)\n        return outcomes\n\n    def legal_actions(self, player):\n        # return legal action list\n        return list(range(len(self.ACTION)))\n\n    def action_length(self):\n        # maximum action label (it determines output size of policy function)\n        return len(self.ACTION)\n\n    def players(self):\n        return list(range(self.NUM_AGENTS))\n\n    # def rule_based_action(self, player):\n    #     from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, GreedyAgent\n    #     action_map = {'N': Action.NORTH, 'S': Action.SOUTH, 'W': Action.WEST, 'E': Action.EAST}\n\n    #     agent = GreedyAgent(Configuration({'rows': 7, 'columns': 11}))\n    #     agent.last_action = action_map[self.ACTION[self.last_actions[player]][0]] if player in self.last_actions else None\n    #     obs = {**self.obs_list[-1][0]['observation'], **self.obs_list[-1][player]['observation']}\n    #     action = agent(Observation(obs))\n    #     return self.ACTION.index(action)\n\n    def rule_based_action(self, player):\n\n        import random\n\n        from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\n        def get_nearest_cells(x,y):\n            # returns all cells reachable from the current one\n            result = []\n            for i in (-1,+1):\n                result.append(((x+i+7)%7, y))\n                result.append((x, (y+i+11)%11))\n            return result\n\n        def find_closest_food(table):\n            # returns the first step toward the closest food item\n            new_table = table.copy()\n            \n            \n            # (direction of the step, axis, code)\n            possible_moves = [\n                (1, 0, 1),\n                (-1, 0, 2),\n                (1, 1, 3),\n                (-1, 1, 4)\n            ]\n            \n            # shuffle possible options to add variability\n            random.shuffle(possible_moves)\n            \n            \n            updated = False\n            for roll, axis, code in possible_moves:\n\n                shifted_table = np.roll(table, roll, axis)\n                \n                if (table == -2).any() and (shifted_table[table == -2] == -3).any(): # we have found some food at the first step\n                    return code\n                else:\n                    mask = np.logical_and(new_table == 0,shifted_table == -3)\n                    if mask.sum() > 0:\n                        updated = True\n                    new_table += code * mask\n                if (table == -2).any() and shifted_table[table == -2][0] > 0: # we have found some food\n                    return shifted_table[table == -2][0]\n                \n                # else - update new reachible cells\n                mask = np.logical_and(new_table == 0,shifted_table > 0)\n                if mask.sum() > 0:\n                    updated = True\n                new_table += shifted_table * mask\n\n            # if we updated anything - continue reccurison\n            if updated:\n                return find_closest_food(new_table)\n            # if not - return some step\n            else:\n                return table.max()\n\n        \n        #self.last_step = None\n\n        def agent(obs_dict, config_dict):\n            self.last_step = None\n            #global self.\n            \n            observation = Observation(obs_dict)\n            configuration = config_dict\n            player_index = observation.index\n            player_goose = observation.geese[player_index]\n            player_head = player_goose[0]\n            player_row, player_column = row_col(player_head, configuration.columns)\n\n\n            table = np.zeros((7,11))\n            # 0 - emply cells\n            # -1 - obstacles\n            # -4 - possible obstacles\n            # -2 - food\n            # -3 - head\n            # 1,2,3,4 - reachable on the current step cell, number is the id of the first step direction\n            \n            legend = {\n                1: 'SOUTH',\n                2: 'NORTH',\n                3: 'EAST',\n                4: 'WEST'\n            }\n            \n            # let's add food to the map\n            for food in observation.food:\n                x,y = row_col(food, configuration.columns)\n                table[x,y] = -2 # food\n                \n            # let's add all cells that are forbidden\n            for i in range(4):\n                opp_goose = observation.geese[i]\n                if len(opp_goose) == 0:\n                    continue\n                    \n                is_close_to_food = False\n                    \n                if i != player_index:\n                    x,y = row_col(opp_goose[0], configuration.columns)\n                    possible_moves = get_nearest_cells(x,y) # head can move anywhere\n                    \n                    for x,y in possible_moves:\n                        if table[x,y] == -2:\n                            is_close_to_food = True\n                    \n                        table[x,y] = -4 # possibly forbidden cells\n                \n                # usually we ignore the last tail cell but there are exceptions\n                tail_change = -1\n                if obs_dict['step'] % 40 == 39:\n                    tail_change -= 1\n                \n                # we assume that the goose will eat the food\n                if is_close_to_food:\n                    tail_change += 1\n                if tail_change >= 0:\n                    tail_change = None\n                    \n\n                for n in opp_goose[:tail_change]:\n                    x,y = row_col(n, configuration.columns)\n                    table[x,y] = -1 # forbidden cells\n            \n            # going back is forbidden according to the new rules\n            x,y = row_col(player_head, configuration.columns)\n            if self.last_step is not None:\n                if self.last_step == 1:\n                    table[(x + 6) % 7,y] = -1\n                elif self.last_step == 2:\n                    table[(x + 8) % 7,y] = -1\n                elif self.last_step == 3:\n                    table[x,(y + 10)%11] = -1\n                elif self.last_step == 4:\n                    table[x,(y + 12)%11] = -1\n                \n            # add head position\n            table[x,y] = -3\n            \n            # the first step toward the nearest food\n            step = int(find_closest_food(table))\n            \n            # if there is not available steps try to go to possibly dangerous cell\n            if step not in [1,2,3,4]:\n                x,y = row_col(player_head, configuration.columns)\n                if table[(x + 8) % 7,y] == -4:\n                    step = 1\n                elif table[(x + 6) % 7,y] == -4:\n                    step = 2\n                elif table[x,(y + 12)%11] == -4:\n                    step = 3\n                elif table[x,(y + 10)%11] == -4:\n                    step = 4\n                        \n            # else - do a random step and lose\n                else:\n                    step = np.random.randint(4) + 1\n            \n            self.last_step = step\n            return legend[step]\n        obs = {**self.obs_list[-1][0]['observation'], **self.obs_list[-1][player]['observation']}\n        action = agent(obs, Configuration({'rows': 7, 'columns': 11}))\n        return self.ACTION.index(action)\n\n\n    def rule_based_action2(self, player):\n\n        from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n        import numpy as np\n        import random\n        import copy\n\n        self.frame = 0\n        self.opposites = {Action.EAST: Action.WEST, Action.WEST: Action.EAST, Action.NORTH: Action.SOUTH, Action.SOUTH: Action.NORTH}\n        self.action_meanings = {Action.EAST: (1, 0), Action.WEST: (-1, 0), Action.NORTH: (0, -1), Action.SOUTH: (0, 1)}\n        self.action_names = {(1, 0): Action.EAST, (-10, 0): Action.EAST, (-1, 0): Action.WEST, (10, 0): Action.WEST, (0, -1): Action.NORTH, (0, 6): Action.NORTH, (0, -6): Action.SOUTH, (0, 1): Action.SOUTH}\n        strValue = {Action.EAST: 'EAST', Action.WEST: 'WEST', Action.NORTH: 'NORTH', Action.SOUTH: 'SOUTH'}\n        self.all_last_actions = [None, None, None, None]\n        self.revert_last_actions = [None, None, None, None]\n        self.last_observation = None\n\n        class Obs:\n            pass\n\n        def setLastActions(observation, configuration):\n            #global frame, revert_last_actions, all_last_actions\n            if not self.frame == 0:\n                for i in range(4):\n                    setLastAction(observation, configuration, i)\n            self.revert_last_actions = copy.deepcopy(self.all_last_actions)\n\n\n        def revertLastActions():\n            #global revert_last_actions, all_last_actions\n            self.all_last_actions = copy.deepcopy(self.revert_last_actions)\n\n\n        def setLastAction(observation, configuration, gooseIndex):\n            #global last_observation, all_last_actions, action_names\n            if len(observation.geese[gooseIndex]) > 0:\n                oldGooseRow, oldGooseCol = row_col(self.last_observation.geese[gooseIndex][0], configuration.columns)\n                newGooseRow, newGooseCol = row_col(observation.geese[gooseIndex][0], configuration.columns)\n                self.all_last_actions[gooseIndex] = self.action_names[\n                    ((newGooseCol - oldGooseCol) % configuration.columns, (newGooseRow - oldGooseRow) % configuration.rows)]\n\n\n        def getValidDirections(observation, configuration, gooseIndex):\n            #global all_last_actions, opposites\n            directions = [Action.EAST, Action.WEST, Action.NORTH, Action.SOUTH]\n            returnDirections = []\n            for direction in directions:\n                row, col = getRowColForAction(observation, configuration, gooseIndex, direction)\n                if not willGooseBeThere(observation, configuration, row, col) and not self.all_last_actions[gooseIndex] == self.opposites[\n                    direction]:\n                    returnDirections.append(direction)        \n            if len(returnDirections) == 0:\n                return directions\n            return returnDirections\n\n\n        def randomTurn(observation, configuration, actionOverrides, rewards, fr):\n            newObservation = cloneObservation(observation)\n            for i in range(4):\n                if len(observation.geese[i]) > 0:\n                    if i in actionOverrides.keys():\n                        newObservation = performActionForGoose(observation, configuration, i, newObservation, actionOverrides[i])\n                    else:\n                        newObservation = randomActionForGoose(observation, configuration, i, newObservation)\n\n            checkForCollisions(newObservation, configuration)\n            updateRewards(newObservation, configuration, rewards, fr)\n            hunger(newObservation, fr)\n            return newObservation\n\n\n        def hunger(observation, fr):\n            if fr % 40 == 0:\n                for g, goose in enumerate(observation.geese):\n                    goose = goose[0:len(goose)-1]\n                    \n\n\n        def updateRewards(observation, configuration, rewards, fr):\n            for g, goose in enumerate(observation.geese):\n                if len(goose) > 0:\n                    rewards[g] = 2 * fr + len(goose)\n\n        def checkForCollisions(observation, configuration):\n            killed = []\n            for g, goose in enumerate(observation.geese):\n                if len(goose) > 0:\n                    for o, otherGoose in enumerate(observation.geese):\n                        for p, part in enumerate(otherGoose):\n                            if not (o == g and p == 0):\n                                if goose[0] == part:\n                                    killed.append(g)\n\n            for kill in killed:\n                observation.geese[kill] = []\n\n\n        def cloneObservation(observation):\n            newObservation = Obs()\n            newObservation.index = observation.index\n            newObservation.geese = copy.deepcopy(observation.geese)\n            newObservation.food = copy.deepcopy(observation.food)\n            return newObservation\n\n\n        def randomActionForGoose(observation, configuration, gooseIndex, newObservation):\n            validActions = getValidDirections(observation, configuration, gooseIndex)\n            action = random.choice(validActions)\n            row, col = getRowColForAction(observation, configuration, gooseIndex, action)\n            newObservation.geese[gooseIndex] = [row * configuration.columns + col] + newObservation.geese[gooseIndex]\n            if not isFoodThere(observation, configuration, row, col):\n                newObservation.geese[gooseIndex] = newObservation.geese[gooseIndex][0:len(newObservation.geese[gooseIndex])-1]  \n            return newObservation\n\n        def performActionForGoose(observation, configuration, gooseIndex, newObservation, action):\n            row, col = getRowColForAction(observation, configuration, gooseIndex, action)\n            newObservation.geese[gooseIndex][:0] = [row * configuration.columns + col]\n            if not isFoodThere(observation, configuration, row, col):\n                newObservation.geese[gooseIndex] = newObservation.geese[gooseIndex][0:len(newObservation.geese[gooseIndex])-1]  \n            return newObservation\n                \n\n        def isFoodThere(observation, configuration, row, col):\n            for food in observation.food:\n                foodRow, foodCol = row_col(food, configuration.columns)\n                if foodRow == row and foodCol == col:\n                    return True\n            return False\n\n\n        def willGooseBeThere(observation, configuration, row, col):\n            for goose in observation.geese:\n                for p, part in enumerate(goose):\n                    if not p == len(goose) - 1:\n                        partRow, partCol = row_col(part, configuration.columns)\n                        if partRow == row and partCol == col:\n                            return True\n            return False\n\n\n        def getRowColForAction(observation, configuration, gooseIndex, action):\n            #global action_meanings\n            gooseRow, gooseCol = row_col(observation.geese[gooseIndex][0], configuration.columns)\n            actionRow = (gooseRow + self.action_meanings[action][1]) % configuration.rows\n            actionCol = (gooseCol + self.action_meanings[action][0]) % configuration.columns\n            return actionRow, actionCol\n\n\n        def simulateMatch(observation, configuration, firstMove, depth):\n            #global frame\n            actionOverrides = {observation.index: firstMove}\n            revertLastActions()\n            simulationFrame = self.frame + 1\n            newObservation = cloneObservation(observation)\n            rewards = [0, 0, 0, 0]\n            count = 0\n            while count < depth:\n                newObservation = randomTurn(newObservation, configuration, actionOverrides, rewards, simulationFrame)\n                actionOverrides = {}\n                simulationFrame += 1\n                count += 1\n            return rewards\n\n\n        def simulateMatches(observation, configuration, numMatches, depth):\n            options = getValidDirections(observation, configuration, observation.index)\n            rewardTotals = []\n            for o, option in enumerate(options):\n                rewardsForOption = [0, 0, 0, 0]\n                for i in range(numMatches):\n                    matchRewards = simulateMatch(observation, configuration, option, depth)\n                    for j in range(4):\n                        rewardsForOption[j] += matchRewards[j]\n                rewardTotals.append(rewardsForOption)\n            scores = []\n            for o, option in enumerate(options):\n                rewards = rewardTotals[o]\n                if len(rewards) <= 0:\n                    mean = 0\n                else:\n                    mean = sum(rewards) \/ len(rewards)\n                if mean == 0:\n                    scores.append(0)\n                else:\n                    scores.append(rewards[observation.index] \/ mean)\n            \n            # print('frame: ', frame)\n            # print('options: ', options)\n            # print('scores: ', scores)\n            # print('reward totals: ', rewardTotals)\n            # print('lengths: ')\n            # print('0: ', len(observation.geese[0]))\n            # print('1: ', len(observation.geese[1]))\n            # print('2: ', len(observation.geese[2]))\n            # print('3: ', len(observation.geese[3]))\n\n            return options[scores.index(max(scores))]\n\n\n\n        def agent(obs_dict, config_dict):\n            #global last_observation, all_last_actions, opposites, frame\n            observation = Observation(obs_dict)\n            configuration = config_dict\n            setLastActions(observation, configuration)\n            myLength = len(observation.geese[observation.index])\n            if myLength < 5:\n                my_action = simulateMatches(observation, configuration, 300, 3)\n            elif myLength < 9:\n                my_action = simulateMatches(observation ,configuration, 120, 6)\n            else:\n                my_action = simulateMatches(observation, configuration, 85, 9)\n            \n            self.last_observation = cloneObservation(observation)\n            self.frame += 1\n            return strValue[my_action]\n\n        obs = {**self.obs_list[-1][0]['observation'], **self.obs_list[-1][player]['observation']}\n        action = agent(obs, Configuration({'rows': 7, 'columns': 11}))\n        return self.ACTION.index(action)    \n\n\n    def net(self):\n        return GeeseNet\n\n    def observation(self, player=None):\n        if player is None:\n            player = 0\n\n        b = np.zeros((self.NUM_AGENTS * 4 + 1, 7 * 11), dtype=np.float32)\n        obs = self.obs_list[-1][0]['observation']\n\n        for p, geese in enumerate(obs['geese']):\n            # head position\n            for pos in geese[:1]:\n                b[0 + (p - player) % self.NUM_AGENTS, pos] = 1\n            # tip position\n            for pos in geese[-1:]:\n                b[4 + (p - player) % self.NUM_AGENTS, pos] = 1\n            # whole position\n            for pos in geese:\n                b[8 + (p - player) % self.NUM_AGENTS, pos] = 1\n\n        # previous head position\n        if len(self.obs_list) > 1:\n            obs_prev = self.obs_list[-2][0]['observation']\n            for p, geese in enumerate(obs_prev['geese']):\n                for pos in geese[:1]:\n                    b[12 + (p - player) % self.NUM_AGENTS, pos] = 1\n\n        # food\n        for pos in obs['food']:\n            b[16, pos] = 1\n\n        return b.reshape(-1, 7, 11)\n\n\nif __name__ == '__main__':\n    e = Environment()\n    for _ in range(100):\n        e.reset()\n        while not e.terminal():\n            print(e)\n            actions = {p: e.legal_actions(p) for p in e.turns()}\n            print([[e.action2str(a, p) for a in alist] for p, alist in actions.items()])\n            e.step({p: random.choice(alist) for p, alist in actions.items()})\n        print(e)\n        print(e.outcome())\n","310afdd4":"%%writefile handyrl\/evaluation.py\n# Copyright (c) 2020 DeNA Co., Ltd.\n# Licensed under The MIT License [see LICENSE for details]\n\n# evaluation of policies or planning algorithms\n\nimport random\nimport time\nimport multiprocessing as mp\n\nfrom .environment import prepare_env, make_env\nfrom .connection import send_recv, accept_socket_connections, connect_socket_connection\nfrom .agent import RandomAgent, RuleBasedAgent, Agent, EnsembleAgent, SoftAgent\n\n\nnetwork_match_port = 9876\n\n\ndef view(env, player=None):\n    if hasattr(env, 'view'):\n        env.view(player=player)\n    else:\n        print(env)\n\n\ndef view_transition(env):\n    if hasattr(env, 'view_transition'):\n        env.view_transition()\n    else:\n        pass\n\n\nclass NetworkAgentClient:\n    def __init__(self, agent, env, conn):\n        self.conn = conn\n        self.agent = agent\n        self.env = env\n\n    def run(self):\n        while True:\n            command, args = self.conn.recv()\n            if command == 'quit':\n                break\n            elif command == 'outcome':\n                print('outcome = %f' % args[0])\n            elif hasattr(self.agent, command):\n                if command == 'action' or command == 'observe':\n                    view(self.env)\n                ret = getattr(self.agent, command)(self.env, *args, show=True)\n                if command == 'action':\n                    player = args[0]\n                    ret = self.env.action2str(ret, player)\n            else:\n                ret = getattr(self.env, command)(*args)\n                if command == 'update':\n                    reset = args[1]\n                    if reset:\n                        self.agent.reset(self.env, show=True)\n                    view_transition(self.env)\n            self.conn.send(ret)\n\n\nclass NetworkAgent:\n    def __init__(self, conn):\n        self.conn = conn\n\n    def update(self, data, reset):\n        return send_recv(self.conn, ('update', [data, reset]))\n\n    def outcome(self, outcome):\n        return send_recv(self.conn, ('outcome', [outcome]))\n\n    def action(self, player):\n        return send_recv(self.conn, ('action', [player]))\n\n    def observe(self, player):\n        return send_recv(self.conn, ('observe', [player]))\n\n\ndef exec_match(env, agents, critic, show=False, game_args={}):\n    ''' match with shared game environment '''\n    if env.reset(game_args):\n        return None\n    for agent in agents.values():\n        agent.reset(env, show=show)\n    while not env.terminal():\n        if show:\n            view(env)\n        if show and critic is not None:\n            print('cv = ', critic.observe(env, None, show=False)[0])\n        turn_players = env.turns()\n        actions = {}\n        for p, agent in agents.items():\n            if p in turn_players:\n                actions[p] = agent.action(env, p, show=show)\n            else:\n                agent.observe(env, p, show=show)\n        if env.step(actions):\n            return None\n        if show:\n            view_transition(env)\n    outcome = env.outcome()\n    if show:\n        print('final outcome = %s' % outcome)\n    return outcome\n\n\ndef exec_network_match(env, network_agents, critic, show=False, game_args={}):\n    ''' match with divided game environment '''\n    if env.reset(game_args):\n        return None\n    for p, agent in network_agents.items():\n        info = env.diff_info(p)\n        agent.update(info, True)\n    while not env.terminal():\n        if show:\n            view(env)\n        if show and critic is not None:\n            print('cv = ', critic.observe(env, None, show=False)[0])\n        turn_players = env.turns()\n        actions = {}\n        for p, agent in network_agents.items():\n            if p in turn_players:\n                action = agent.action(p)\n                actions[p] = env.str2action(action, p)\n            else:\n                agent.observe(p)\n        if env.step(actions):\n            return None\n        for p, agent in network_agents.items():\n            info = env.diff_info(p)\n            agent.update(info, False)\n    outcome = env.outcome()\n    for p, agent in network_agents.items():\n        agent.outcome(outcome[p])\n    return outcome\n\n\ndef build_agent(raw, env):\n    if raw == 'random':\n        return RandomAgent()\n    elif raw == 'rulebase':\n        return RuleBasedAgent()\n    return None\n\n\nclass Evaluator:\n    def __init__(self, env, args):\n        self.env = env\n        self.args = args\n        self.default_opponent = 'rulebase'\n\n    def execute(self, models, args):\n        opponents = self.args.get('eval', {}).get('opponent', [])\n        if len(opponents) == 0:\n            opponent = self.default_opponent\n        else:\n            #opponent = random.choice(opponents)\n            opponent = self.default_opponent\n\n        agents = {}\n        for p, model in models.items():\n            if model is None:\n                agents[p] = build_agent(opponent, self.env)\n            else:\n                agents[p] = Agent(model, self.args['observation'])\n\n        outcome = exec_match(self.env, agents, None)\n        if outcome is None:\n            print('None episode in evaluation!')\n            return None\n        return {'args': args, 'result': outcome, 'opponent': opponent}\n\n\ndef wp_func(results):\n    games = sum([v for k, v in results.items() if k is not None])\n    win = sum([(k + 1) \/ 2 * v for k, v in results.items() if k is not None])\n    if games == 0:\n        return 0.0\n    return win \/ games\n\n\ndef eval_process_mp_child(agents, critic, env_args, index, in_queue, out_queue, seed, show=False):\n    random.seed(seed + index)\n    env = make_env({**env_args, 'id': index})\n    while True:\n        args = in_queue.get()\n        if args is None:\n            break\n        g, agent_ids, pat_idx, game_args = args\n        print('*** Game %d ***' % g)\n        agent_map = {env.players()[p]: agents[ai] for p, ai in enumerate(agent_ids)}\n        if isinstance(list(agent_map.values())[0], NetworkAgent):\n            outcome = exec_network_match(env, agent_map, critic, show=show, game_args=game_args)\n        else:\n            outcome = exec_match(env, agent_map, critic, show=show, game_args=game_args)\n        out_queue.put((pat_idx, agent_ids, outcome))\n    out_queue.put(None)\n\n\ndef evaluate_mp(env, agents, critic, env_args, args_patterns, num_process, num_games, seed):\n    in_queue, out_queue = mp.Queue(), mp.Queue()\n    args_cnt = 0\n    total_results, result_map = [{} for _ in agents], [{} for _ in agents]\n    print('total games = %d' % (len(args_patterns) * num_games))\n    time.sleep(0.1)\n    for pat_idx, args in args_patterns.items():\n        for i in range(num_games):\n            if len(agents) == 2:\n                # When playing two player game,\n                # the number of games with first or second player is equalized.\n                first_agent = 0 if i < (num_games + 1) \/\/ 2 else 1\n                tmp_pat_idx, agent_ids = (pat_idx + '-F', [0, 1]) if first_agent == 0 else (pat_idx + '-S', [1, 0])\n            else:\n                tmp_pat_idx, agent_ids = pat_idx, random.sample(list(range(len(agents))), len(agents))\n            in_queue.put((args_cnt, agent_ids, tmp_pat_idx, args))\n            for p in range(len(agents)):\n                result_map[p][tmp_pat_idx] = {}\n            args_cnt += 1\n\n    network_mode = agents[0] is None\n    if network_mode:  # network battle mode\n        agents = network_match_acception(num_process, env_args, len(agents), network_match_port)\n    else:\n        agents = [agents] * num_process\n\n    for i in range(num_process):\n        in_queue.put(None)\n        args = agents[i], critic, env_args, i, in_queue, out_queue, seed\n        if num_process > 1:\n            mp.Process(target=eval_process_mp_child, args=args).start()\n            if network_mode:\n                for agent in agents[i]:\n                    agent.conn.close()\n        else:\n            eval_process_mp_child(*args, show=True)\n\n    finished_cnt = 0\n    while finished_cnt < num_process:\n        ret = out_queue.get()\n        if ret is None:\n            finished_cnt += 1\n            continue\n        pat_idx, agent_ids, outcome = ret\n        if outcome is not None:\n            for idx, p in enumerate(env.players()):\n                agent_id = agent_ids[idx]\n                oc = outcome[p]\n                result_map[agent_id][pat_idx][oc] = result_map[agent_id][pat_idx].get(oc, 0) + 1\n                total_results[agent_id][oc] = total_results[agent_id].get(oc, 0) + 1\n\n    for p, r_map in enumerate(result_map):\n        print('---agent %d---' % p)\n        for pat_idx, results in r_map.items():\n            print(pat_idx, {k: results[k] for k in sorted(results.keys(), reverse=True)}, wp_func(results))\n        print('total', {k: total_results[p][k] for k in sorted(total_results[p].keys(), reverse=True)}, wp_func(total_results[p]))\n\n\ndef network_match_acception(n, env_args, num_agents, port):\n    waiting_conns = []\n    accepted_conns = []\n\n    for conn in accept_socket_connections(port):\n        if len(accepted_conns) >= n * num_agents:\n            break\n        waiting_conns.append(conn)\n\n        if len(waiting_conns) == num_agents:\n            conn = waiting_conns[0]\n            accepted_conns.append(conn)\n            waiting_conns = waiting_conns[1:]\n            conn.send(env_args)  # send accept with environment arguments\n\n    agents_list = [\n        [NetworkAgent(accepted_conns[i * num_agents + j]) for j in range(num_agents)]\n        for i in range(n)\n    ]\n\n    return agents_list\n\n\ndef get_model(env, model_path):\n    import torch\n    from .model import ModelWrapper\n    model = env.net()()\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    return ModelWrapper(model)\n\n\ndef client_mp_child(env_args, model_path, conn):\n    env = make_env(env_args)\n    model = get_model(env, model_path)\n    NetworkAgentClient(Agent(model), env, conn).run()\n\n\ndef eval_main(args, argv):\n    env_args = args['env_args']\n    prepare_env(env_args)\n    env = make_env(env_args)\n\n    model_path = argv[0] if len(argv) >= 1 else 'models\/latest.pth'\n    num_games = int(argv[1]) if len(argv) >= 2 else 100\n    num_process = int(argv[2]) if len(argv) >= 3 else 1\n\n    agent1 = Agent(get_model(env, model_path))\n    critic = None\n\n    print('%d process, %d games' % (num_process, num_games))\n\n    seed = random.randrange(1e8)\n    print('seed = %d' % seed)\n\n    agents = [agent1] + [RandomAgent() for _ in range(len(env.players()) - 1)]\n\n    evaluate_mp(env, agents, critic, env_args, {'default': {}}, num_process, num_games, seed)\n\n\ndef eval_server_main(args, argv):\n    print('network match server mode')\n    env_args = args['env_args']\n    prepare_env(env_args)\n    env = make_env(env_args)\n\n    num_games = int(argv[0]) if len(argv) >= 1 else 100\n    num_process = int(argv[1]) if len(argv) >= 2 else 1\n\n    print('%d process, %d games' % (num_process, num_games))\n\n    seed = random.randrange(1e8)\n    print('seed = %d' % seed)\n\n    evaluate_mp(env, [None] * len(env.players()), None, env_args, {'default': {}}, num_process, num_games, seed)\n\n\ndef eval_client_main(args, argv):\n    print('network match client mode')\n    while True:\n        try:\n            host = argv[1] if len(argv) >= 2 else 'localhost'\n            conn = connect_socket_connection(host, network_match_port)\n            env_args = conn.recv()\n        except EOFError:\n            break\n\n        model_path = argv[0] if len(argv) >= 1 else 'models\/latest.pth'\n        mp.Process(target=client_mp_child, args=(env_args, model_path, conn)).start()\n        conn.close()\n","efb7deb8":"%%writefile handyrl\/train.py\n# Copyright (c) 2020 DeNA Co., Ltd.\n# Licensed under The MIT License [see LICENSE for details]\n\n# training\n\nimport os\nimport time\nimport copy\nimport threading\nimport random\nimport bz2\nimport base64\nimport pickle\nimport warnings\nfrom collections import deque\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as dist\nimport torch.optim as optim\nimport psutil\n\nfrom .environment import prepare_env, make_env\nfrom .util import map_r, bimap_r, trimap_r, rotate\nfrom .model import to_torch, to_gpu, RandomModel, ModelWrapper\nfrom .losses import compute_target\nfrom .connection import MultiProcessJobExecutor\nfrom .connection import accept_socket_connections\nfrom .worker import WorkerCluster, WorkerServer\nfrom torch.cuda.amp import autocast\nfrom joblib import dump, load\n\ndef make_batch(episodes, args):\n    \"\"\"Making training batch\n\n    Args:\n        episodes (Iterable): list of episodes\n        args (dict): training configuration\n\n    Returns:\n        dict: PyTorch input and target tensors\n\n    Note:\n        Basic data shape is (B, T, P, ...) .\n        (B is batch size, T is time length, P is player count)\n    \"\"\"\n\n    obss, datum = [], []\n\n    def replace_none(a, b):\n        return a if a is not None else b\n\n    for ep in episodes:\n        moments_ = sum([pickle.loads(bz2.decompress(ms)) for ms in ep['moment']], [])\n        moments = moments_[ep['start'] - ep['base']:ep['end'] - ep['base']]\n        players = list(moments[0]['observation'].keys())\n        if not args['turn_based_training']:  # solo training\n            players = [random.choice(players)]\n\n        obs_zeros = map_r(moments[0]['observation'][moments[0]['turn'][0]], lambda o: np.zeros_like(o))  # template for padding\n        p_zeros = np.zeros_like(moments[0]['policy'][moments[0]['turn'][0]])  # template for padding\n\n        # data that is chainge by training configuration\n        if args['turn_based_training'] and not args['observation']:\n            obs = [[m['observation'][m['turn'][0]]] for m in moments]\n            p = np.array([[m['policy'][m['turn'][0]]] for m in moments])\n            act = np.array([[m['action'][m['turn'][0]]] for m in moments], dtype=np.int64)[..., np.newaxis]\n            amask = np.array([[m['action_mask'][m['turn'][0]]] for m in moments])\n        else:\n            obs = [[replace_none(m['observation'][player], obs_zeros) for player in players] for m in moments]\n            p = np.array([[replace_none(m['policy'][player], p_zeros) for player in players] for m in moments])\n            act = np.array([[replace_none(m['action'][player], 0) for player in players] for m in moments], dtype=np.int64)[..., np.newaxis]\n            amask = np.array([[replace_none(m['action_mask'][player], p_zeros + 1e32) for player in players] for m in moments])\n\n        # reshape observation\n        obs = rotate(rotate(obs))  # (T, P, ..., ...) -> (P, ..., T, ...) -> (..., T, P, ...)\n        obs = bimap_r(obs_zeros, obs, lambda _, o: np.array(o))\n\n        # datum that is not changed by training configuration\n        v = np.array([[replace_none(m['value'][player], [0]) for player in players] for m in moments], dtype=np.float32).reshape(len(moments), len(players), -1)\n        rew = np.array([[replace_none(m['reward'][player], [0]) for player in players] for m in moments], dtype=np.float32).reshape(len(moments), len(players), -1)\n        ret = np.array([[replace_none(m['return'][player], [0]) for player in players] for m in moments], dtype=np.float32).reshape(len(moments), len(players), -1)\n        oc = np.array([ep['outcome'][player] for player in players], dtype=np.float32).reshape(1, len(players), -1)\n\n        emask = np.ones((len(moments), 1, 1), dtype=np.float32)  # episode mask\n        tmask = np.array([[[m['policy'][player] is not None] for player in players] for m in moments], dtype=np.float32)\n        omask = np.array([[[m['value'][player] is not None] for player in players] for m in moments], dtype=np.float32)\n\n        progress = np.arange(ep['start'], ep['end'], dtype=np.float32)[..., np.newaxis] \/ ep['total']\n\n        # pad each array if step length is short\n        if len(tmask) < args['forward_steps']:\n            pad_len = args['forward_steps'] - len(tmask)\n            obs = map_r(obs, lambda o: np.pad(o, [(0, pad_len)] + [(0, 0)] * (len(o.shape) - 1), 'constant', constant_values=0))\n            p = np.pad(p, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            v = np.concatenate([v, np.tile(oc, [pad_len, 1, 1])])\n            act = np.pad(act, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            rew = np.pad(rew, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            ret = np.pad(ret, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            emask = np.pad(emask, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            tmask = np.pad(tmask, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            omask = np.pad(omask, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=0)\n            amask = np.pad(amask, [(0, pad_len), (0, 0), (0, 0)], 'constant', constant_values=1e32)\n            progress = np.pad(progress, [(0, pad_len), (0, 0)], 'constant', constant_values=1)\n\n        obss.append(obs)\n        datum.append((p, v, act, oc, rew, ret, emask, tmask, omask, amask, progress))\n\n    p, v, act, oc, rew, ret, emask, tmask, omask, amask, progress = zip(*datum)\n\n    obs = to_torch(bimap_r(obs_zeros, rotate(obss), lambda _, o: np.array(o)))\n    p = to_torch(np.array(p))\n    v = to_torch(np.array(v))\n    act = to_torch(np.array(act))\n    oc = to_torch(np.array(oc))\n    rew = to_torch(np.array(rew))\n    ret = to_torch(np.array(ret))\n    emask = to_torch(np.array(emask))\n    tmask = to_torch(np.array(tmask))\n    omask = to_torch(np.array(omask))\n    amask = to_torch(np.array(amask))\n    progress = to_torch(np.array(progress))\n\n    return {\n        'observation': obs,\n        'policy': p, 'value': v,\n        'action': act, 'outcome': oc,\n        'reward': rew, 'return': ret,\n        'episode_mask': emask,\n        'turn_mask': tmask, 'observation_mask': omask,\n        'action_mask': amask,\n        'progress': progress,\n    }\n\n\ndef forward_prediction(model, hidden, batch, args):\n    \"\"\"Forward calculation via neural network\n\n    Args:\n        model (torch.nn.Module): neural network\n        hidden: initial hidden state (..., B, P, ...)\n        batch (dict): training batch (output of make_batch() function)\n\n    Returns:\n        tuple: batch outputs of neural network\n    \"\"\"\n\n    observations = batch['observation']  # (B, T, P, ...)\n\n    if hidden is None:\n        # feed-forward neural network\n        obs = map_r(observations, lambda o: o.view(-1, *o.size()[3:]))\n        outputs = model(obs, None)\n    else:\n        # sequential computation with RNN\n        outputs = {}\n        for t in range(batch['turn_mask'].size(1)):\n            obs = map_r(observations, lambda o: o[:, t].reshape(-1, *o.size()[3:]))  # (..., B * P, ...)\n            omask_ = batch['observation_mask'][:, t]\n            omask = map_r(hidden, lambda h: omask_.view(*h.size()[:2], *([1] * (len(h.size()) - 2))))\n            hidden_ = bimap_r(hidden, omask, lambda h, m: h * m)  # (..., B, P, ...)\n            if args['turn_based_training'] and not args['observation']:\n                hidden_ = map_r(hidden_, lambda h: h.sum(1))  # (..., B * 1, ...)\n            else:\n                hidden_ = map_r(hidden_, lambda h: h.view(-1, *h.size()[2:]))  # (..., B * P, ...)\n            outputs_ = model(obs, hidden_)\n            for k, o in outputs_.items():\n                if k == 'hidden':\n                    next_hidden = outputs_['hidden']\n                else:\n                    outputs[k] = outputs.get(k, []) + [o]\n            next_hidden = bimap_r(next_hidden, hidden, lambda nh, h: nh.view(h.size(0), -1, *h.size()[2:]))  # (..., B, P or 1, ...)\n            hidden = trimap_r(hidden, next_hidden, omask, lambda h, nh, m: h * (1 - m) + nh * m)\n        outputs = {k: torch.stack(o, dim=1) for k, o in outputs.items() if o[0] is not None}\n\n    for k, o in outputs.items():\n        o = o.view(*batch['turn_mask'].size()[:2], -1, o.size(-1))\n        if k == 'policy':\n            # gather turn player's policies\n            outputs[k] = o.mul(batch['turn_mask']).sum(2, keepdim=True) - batch['action_mask']\n        else:\n            # mask valid target values and cumulative rewards\n            outputs[k] = o.mul(batch['observation_mask'])\n\n    return outputs\n\n\ndef compose_losses(outputs, log_selected_policies, total_advantages, targets, batch, args):\n    \"\"\"Caluculate loss value\n\n    Returns:\n        tuple: losses and statistic values and the number of training data\n    \"\"\"\n\n    tmasks = batch['turn_mask']\n    omasks = batch['observation_mask']\n\n    losses = {}\n    dcnt = tmasks.sum().item()\n    turn_advantages = total_advantages.mul(tmasks).sum(2, keepdim=True)\n\n    losses['p'] = (-log_selected_policies * turn_advantages).sum()\n    if 'value' in outputs:\n        losses['v'] = ((outputs['value'] - targets['value']) ** 2).mul(omasks).sum() \/ 2\n    if 'return' in outputs:\n        losses['r'] = F.smooth_l1_loss(outputs['return'], targets['return'], reduction='none').mul(omasks).sum()\n\n    entropy = dist.Categorical(logits=outputs['policy']).entropy().mul(tmasks.sum(-1))\n    losses['ent'] = entropy.sum()\n\n    base_loss = losses['p'] + losses.get('v', 0) + losses.get('r', 0)\n    entropy_loss = entropy.mul(1 - batch['progress'] * (1 - args['entropy_regularization_decay'])).sum() * -args['entropy_regularization']\n    losses['total'] = base_loss + entropy_loss\n\n    return losses, dcnt\n\n\ndef compute_loss(batch, model, hidden, args):\n    outputs = forward_prediction(model, hidden, batch, args)\n    actions = batch['action']\n    emasks = batch['episode_mask']\n    clip_rho_threshold, clip_c_threshold = 1.0, 1.0\n\n    log_selected_b_policies = F.log_softmax(batch['policy']  , dim=-1).gather(-1, actions) * emasks\n    log_selected_t_policies = F.log_softmax(outputs['policy'], dim=-1).gather(-1, actions) * emasks\n\n    # thresholds of importance sampling\n    log_rhos = log_selected_t_policies.detach() - log_selected_b_policies\n    rhos = torch.exp(log_rhos)\n    clipped_rhos = torch.clamp(rhos, 0, clip_rho_threshold)\n    cs = torch.clamp(rhos, 0, clip_c_threshold)\n    outputs_nograd = {k: o.detach() for k, o in outputs.items()}\n\n    if 'value' in outputs_nograd:\n        values_nograd = outputs_nograd['value']\n        if args['turn_based_training'] and values_nograd.size(2) == 2:  # two player zerosum game\n            values_nograd_opponent = -torch.stack([values_nograd[:, :, 1], values_nograd[:, :, 0]], dim=2)\n            values_nograd = (values_nograd + values_nograd_opponent) \/ (batch['observation_mask'].sum(dim=2, keepdim=True) + 1e-8)\n        outputs_nograd['value'] = values_nograd * emasks + batch['outcome'] * (1 - emasks)\n\n    # compute targets and advantage\n    targets = {}\n    advantages = {}\n\n    value_args = outputs_nograd.get('value', None), batch['outcome'], None, args['lambda'], 1, clipped_rhos, cs\n    return_args = outputs_nograd.get('return', None), batch['return'], batch['reward'], args['lambda'], args['gamma'], clipped_rhos, cs\n\n    targets['value'], advantages['value'] = compute_target(args['value_target'], *value_args)\n    targets['return'], advantages['return'] = compute_target(args['value_target'], *return_args)\n\n    if args['policy_target'] != args['value_target']:\n        _, advantages['value'] = compute_target(args['policy_target'], *value_args)\n        _, advantages['return'] = compute_target(args['policy_target'], *return_args)\n\n    # compute policy advantage\n    total_advantages = clipped_rhos * sum(advantages.values())\n\n    return compose_losses(outputs, log_selected_t_policies, total_advantages, targets, batch, args)\n\n\nclass Batcher:\n    def __init__(self, args, episodes):\n        self.args = args\n        self.episodes = episodes\n        self.shutdown_flag = False\n\n        self.executor = MultiProcessJobExecutor(self._worker, self._selector(), self.args['num_batchers'], num_receivers=2)\n\n    def _selector(self):\n        while True:\n            yield [self.select_episode() for _ in range(self.args['batch_size'])]\n\n    def _worker(self, conn, bid):\n        print('started batcher %d' % bid)\n        while not self.shutdown_flag:\n            episodes = conn.recv()\n            batch = make_batch(episodes, self.args)\n            conn.send(batch)\n        print('finished batcher %d' % bid)\n\n    def run(self):\n        self.executor.start()\n\n    def select_episode(self):\n        while True:\n            ep_idx = random.randrange(min(len(self.episodes), self.args['maximum_episodes']))\n            accept_rate = 1 - (len(self.episodes) - 1 - ep_idx) \/ self.args['maximum_episodes']\n            if random.random() < accept_rate:\n                break\n        ep = self.episodes[ep_idx]\n        turn_candidates = 1 + max(0, ep['steps'] - self.args['forward_steps'])  # change start turn by sequence length\n        st = random.randrange(turn_candidates)\n        ed = min(st + self.args['forward_steps'], ep['steps'])\n        st_block = st \/\/ self.args['compress_steps']\n        ed_block = (ed - 1) \/\/ self.args['compress_steps'] + 1\n        ep_minimum = {\n            'args': ep['args'], 'outcome': ep['outcome'],\n            'moment': ep['moment'][st_block:ed_block],\n            'base': st_block * self.args['compress_steps'],\n            'start': st, 'end': ed, 'total': ep['steps']\n        }\n        return ep_minimum\n\n    def batch(self):\n        return self.executor.recv()\n\n    def shutdown(self):\n        self.shutdown_flag = True\n        self.executor.shutdown()\n\n\nclass Trainer:\n    def __init__(self, args, model):\n        self.args = args\n        if self.args['restart_with_saved_epochs']:\n            print('start load saved episodes....')\n            self.episodes = load('geese2genepisodes.job', mmap_mode=None)\n            print('loaded saved episodes', len(self.episodes))\n        if not self.args['restart_with_saved_epochs']:\n            self.episodes = deque()\n        restart_epoch = self.args['restart_epoch']\n        self.gpu = torch.cuda.device_count()\n        self.model = model\n        \n        if self.args['restart_with_saved_env_states']:\n          self.batch_cnt = load('batch_cntload.job', mmap_mode=None)\n          print('loaded saved batch_cntload')\n        if not self.args['restart_with_saved_env_states']:\n          self.batch_cnt = 0\n        if self.args['restart_with_saved_env_states']:\n          self.data_cnt = load('data_cntload.job', mmap_mode=None)\n          print('loaded saved data_cntload')\n        if not self.args['restart_with_saved_env_states']:\n          self.data_cnt = 0\n        \n        self.default_lr = 3e-8\n        if self.args['restart_with_saved_env_states']:\n          self.data_cnt_ema = load('data_cnt_emaload.job', mmap_mode=None)\n          print('loaded saved data_cnt_ema')\n        if not self.args['restart_with_saved_env_states']:\n          self.data_cnt_ema = self.args['batch_size'] * self.args['forward_steps']\n        self.params = list(self.model.parameters())\n        lr = self.default_lr * self.data_cnt_ema\n        self.optimizer = optim.Adam(self.params, lr=lr, weight_decay=1e-5) if len(self.params) > 0 else None\n        if self.args['restart_with_saved_env_states']:\n          self.steps = load('stepsload.job', mmap_mode=None)\n          self.steps_old = load('stepsload.job', mmap_mode=None)\n          print('loaded saved steps')\n        if not self.args['restart_with_saved_env_states']:\n          self.steps = 0\n          self.steps_old = 0\n        self.lock = threading.Lock()\n        self.batcher = Batcher(self.args, self.episodes)\n        self.updated_model = None, 0\n        self.update_flag = False\n        self.shutdown_flag = False\n        if self.args['restart_with_saved_env_states']:\n          self.scaler = torch.cuda.amp.GradScaler()\n          self.scaler.load_state_dict(pickle.loads(bz2.decompress(base64.b64decode(load('scalerload.job', mmap_mode=None)))))\n          print('loaded saved GradScaler')\n        if not self.args['restart_with_saved_env_states']:\n          self.scaler = torch.cuda.amp.GradScaler()\n        self.wrapped_model = ModelWrapper(self.model)\n        self.once = True\n        self.trained_model = self.wrapped_model\n        if self.gpu > 1:\n            self.trained_model = nn.DataParallel(self.wrapped_model)\n\n    def update(self):\n        if len(self.episodes) < self.args['minimum_episodes']:\n            return None, 0  # return None before training\n        self.update_flag = True\n        while True:\n            time.sleep(0.1)\n            model, steps = self.recheck_update()\n            if model is not None:\n                break\n        return model, steps\n\n    def report_update(self, model, steps):\n        self.lock.acquire()\n        self.update_flag = False\n        self.updated_model = model, steps\n        self.lock.release()\n\n    def recheck_update(self):\n        self.lock.acquire()\n        flag = self.update_flag\n        self.lock.release()\n        return (None, -1) if flag else self.updated_model\n\n    def shutdown(self):\n        self.shutdown_flag = True\n        self.batcher.shutdown()\n\n    def train(self):\n        if self.optimizer is None:  # non-parametric model\n            print()\n            return\n\n        # if self.once:\n        #   self.optimizer.load_state_dict(pickle.loads(bz2.decompress(base64.b64decode(load('optimload.job', mmap_mode=None)))))\n        #   print('loaded saved optimizer')\n        #   self.once = False    \n\n        loss_sum = {}\n        if self.gpu > 0:\n            self.trained_model.cuda()\n        self.trained_model.train()\n\n        while self.data_cnt == 0 or not (self.update_flag or self.shutdown_flag):\n            batch = self.batcher.batch()\n            batch_size = batch['value'].size(0)\n            player_count = batch['value'].size(2)\n            hidden = self.wrapped_model.init_hidden([batch_size, player_count])\n            if self.gpu > 0:\n                batch = to_gpu(batch)\n                hidden = to_gpu(hidden)\n            with autocast():\n                losses, dcnt = compute_loss(batch, self.trained_model, hidden, self.args)\n\t\t\n            self.optimizer.zero_grad()\n            self.scaler.scale(losses['total']).backward()\n            #losses['total'].backward()\n            self.scaler.unscale_(self.optimizer)\n            nn.utils.clip_grad_norm_(self.params, 4.0)\n            #self.optimizer.step()\n            self.scaler.step(self.optimizer)\n\n            self.batch_cnt += 1\n            self.data_cnt += dcnt\n            for k, l in losses.items():\n                loss_sum[k] = loss_sum.get(k, 0.0) + l.item()\n\n            self.steps += 1\n            self.scaler.update()\n\n        print('loss = %s' % ' '.join([k + ':' + '%.3f' % (l \/ self.data_cnt) for k, l in loss_sum.items()]))\n\n        self.data_cnt_ema = self.data_cnt_ema * 0.8 + self.data_cnt \/ (1e-2 + self.batch_cnt) * 0.2\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = self.default_lr * self.data_cnt_ema \/ (1 + self.steps * 1e-5)\n        self.model.cpu()\n        self.model.eval()\n        return copy.deepcopy(self.model)\n\n    def run(self):\n        print('waiting training')\n        while not self.shutdown_flag:\n            if len(self.episodes) < self.args['minimum_episodes']:\n                time.sleep(1)\n                continue\n            if self.steps == 0 or self.steps == self.steps_old:\n                self.batcher.run()\n                print('started training')\n            model = self.train()\n            self.report_update(model, self.steps)\n        print('finished training')\n\n\nclass Learner:\n    def __init__(self, args, net=None, remote=False):\n        train_args = args['train_args']\n        env_args = args['env_args']\n        train_args['env'] = env_args\n        args = train_args\n\n        self.args = args\n        random.seed(args['seed'])\n\n        self.env = make_env(env_args)\n        eval_modify_rate = (args['update_episodes'] ** 0.85) \/ args['update_episodes']\n        self.eval_rate = max(args['eval_rate'], eval_modify_rate)\n        self.shutdown_flag = False\n        self.flags = set()\n\n        # trained datum\n        self.model_epoch = self.args['restart_epoch']\n        self.model_class = net if net is not None else self.env.net()\n        train_model = self.model_class()\n        if self.model_epoch == 0:\n            self.model = RandomModel(self.env)\n        else:\n            self.model = train_model\n            try:\n              print('loaded prev trained saved model')\n              self.model.load_state_dict(pickle.loads(bz2.decompress(base64.b64decode(load('modelload.job', mmap_mode=None)))), strict=False)\n              #self.model.load_state_dict(torch.load(self.model_path(self.model_epoch))['model'], strict=False)\n            except:\n              self.model.load_state_dict(torch.load(self.model_path(self.model_epoch)), strict=False)\n\n        # generated datum\n        self.generation_results = {}\n        self.num_episodes = 0\n\n        # evaluated datum\n        self.results = {}\n        self.results_per_opponent = {}\n        self.num_results = 0\n\n        # multiprocess or remote connection\n        self.worker = WorkerServer(args) if remote else WorkerCluster(args)\n\n        # thread connection\n        self.trainer = Trainer(args, train_model)\n\n        self.runs = 0\n\n    def shutdown(self):\n        self.shutdown_flag = True\n        self.trainer.shutdown()\n        self.worker.shutdown()\n        self.thread.join()\n\n    def model_path(self, model_id):\n        return os.path.join('models', str(model_id) + '.pth')\n\n    def latest_model_path(self):\n        return os.path.join('models', 'latest.pth')\n\n\n    def update_model(self, model, steps):\n        # get latest model and save it\n        print('updated model(%d)' % steps)\n        self.model_epoch += 1\n        self.model = model\n        os.makedirs('models', exist_ok=True)\n        torch.save({'model': model.state_dict(), \n            'optim': self.trainer.optimizer.state_dict(),\n            'scaler': self.trainer.scaler.state_dict(),\n            'data_cnt_ema': self.trainer.data_cnt_ema,\n            'batch_cnt': self.trainer.batch_cnt,\n            'data_cnt': self.trainer.data_cnt,\n            'steps': self.trainer.steps},\n            self.model_path(self.model_epoch))\n        #torch.save(model.state_dict(), self.latest_model_path())\n        torch.save({'model': model.state_dict(), \n            'optim': self.trainer.optimizer.state_dict(),\n            'scaler': self.trainer.scaler.state_dict(),\n            'data_cnt_ema': self.trainer.data_cnt_ema,\n            'batch_cnt': self.trainer.batch_cnt,\n            'data_cnt': self.trainer.data_cnt,\n            'steps': self.trainer.steps},\n            self.latest_model_path())\n\n\n        \n    def feed_episodes(self, episodes):\n        # analyze generated episodes\n        for episode in episodes:\n            self.runs = self.runs + 1\n            if episode is None:\n                continue\n            for p in episode['args']['player']:\n                model_id = episode['args']['model_id'][p]\n                outcome = episode['outcome'][p]\n                n, r, r2 = self.generation_results.get(model_id, (0, 0, 0))\n                self.generation_results[model_id] = n + 1, r + outcome, r2 + outcome ** 2\n\n        # store generated episodes\n        self.trainer.episodes.extend([e for e in episodes if e is not None])\n\n        if self.runs>self.args['save_no_episodes']:\n          \n            # with open('geese2genepisodes.job', 'wb') as f:\n            #       dump(self.trainer.episodes, f, compress=('lzma', 3))\n            \n            dump(self.trainer.episodes, \"geese2genepisodes.job\")\n            \n            no_epi = len(self.trainer.episodes)\n\n            np.save('no_ep.npy', no_epi)\n\n            self.runs = 0\n\n\n        mem_percent = psutil.virtual_memory().percent\n        mem_ok = mem_percent <= 95\n        maximum_episodes = self.args['maximum_episodes'] if mem_ok else int(len(self.trainer.episodes) * 95 \/ mem_percent)\n\n        if not mem_ok and 'memory_over' not in self.flags:\n            warnings.warn(\"memory usage %.1f%% with buffer size %d\" % (mem_percent, len(self.trainer.episodes)))\n            self.flags.add('memory_over')\n\n        while len(self.trainer.episodes) > maximum_episodes:\n            self.trainer.episodes.popleft()\n\n    def feed_results(self, results):\n        # store evaluation results\n        for result in results:\n            if result is None:\n                continue\n            for p in result['args']['player']:\n                model_id = result['args']['model_id'][p]\n                res = result['result'][p]\n                n, r, r2 = self.results.get(model_id, (0, 0, 0))\n                self.results[model_id] = n + 1, r + res, r2 + res ** 2\n\n                if model_id not in self.results_per_opponent:\n                    self.results_per_opponent[model_id] = {}\n                opponent = result['opponent']\n                n, r, r2 = self.results_per_opponent[model_id].get(opponent, (0, 0, 0))\n                self.results_per_opponent[model_id][opponent] = n + 1, r + res, r2 + res ** 2\n\n    def update(self):\n        # call update to every component\n        print()\n        print('epoch %d' % self.model_epoch)\n\n        if self.model_epoch not in self.results:\n            print('win rate = Nan (0)')\n        else:\n            def output_wp(name, results):\n                n, r, r2 = results\n                mean = r \/ (n + 1e-6)\n                name_tag = ' (%s)' % name if name != '' else ''\n                print('win rate%s = %.3f (%.1f \/ %d)' % (name_tag, (mean + 1) \/ 2, (r + n) \/ 2, n))\n\n            if len(self.args.get('eval', {}).get('opponent', [])) <= 1:\n                output_wp('', self.results[self.model_epoch])\n            else:\n                output_wp('total', self.results[self.model_epoch])\n                for key in sorted(list(self.results_per_opponent[self.model_epoch])):\n                    output_wp(key, self.results_per_opponent[self.model_epoch][key])\n\n        if self.model_epoch not in self.generation_results:\n            print('generation stats = Nan (0)')\n        else:\n            n, r, r2 = self.generation_results[self.model_epoch]\n            mean = r \/ (n + 1e-6)\n            std = (r2 \/ (n + 1e-6) - mean ** 2) ** 0.5\n            print('generation stats = %.3f +- %.3f' % (mean, std))\n\n        model, steps = self.trainer.update()\n        if model is None:\n            model = self.model\n        self.update_model(model, steps)\n\n        # clear flags\n        self.flags = set()\n\n    def server(self):\n        # central conductor server\n        # returns as list if getting multiple requests as list\n        print('started server')\n        prev_update_episodes = self.args['minimum_episodes']\n        while self.model_epoch < self.args['epochs'] or self.args['epochs'] < 0:\n            # no update call before storing minimum number of episodes + 1 age\n            next_update_episodes = prev_update_episodes + self.args['update_episodes']\n            while not self.shutdown_flag and self.num_episodes < next_update_episodes:\n                conn, (req, data) = self.worker.recv()\n                multi_req = isinstance(data, list)\n                if not multi_req:\n                    data = [data]\n                send_data = []\n\n                if req == 'args':\n                    for _ in data:\n                        args = {'model_id': {}}\n\n                        # decide role\n                        if self.num_results < self.eval_rate * self.num_episodes:\n                            args['role'] = 'e'\n                        else:\n                            args['role'] = 'g'\n\n                        if args['role'] == 'g':\n                            # genatation configuration\n                            args['player'] = self.env.players()\n                            for p in self.env.players():\n                                if p in args['player']:\n                                    args['model_id'][p] = self.model_epoch\n                                else:\n                                    args['model_id'][p] = -1\n                            self.num_episodes += 1\n                            if self.num_episodes % 100 == 0:\n                                print(self.num_episodes, end=' ', flush=True)\n\n                        elif args['role'] == 'e':\n                            # evaluation configuration\n                            args['player'] = [self.env.players()[self.num_results % len(self.env.players())]]\n                            for p in self.env.players():\n                                if p in args['player']:\n                                    args['model_id'][p] = self.model_epoch\n                                else:\n                                    args['model_id'][p] = -1\n                            self.num_results += 1\n\n                        send_data.append(args)\n\n                elif req == 'episode':\n                    # report generated episodes\n                    self.feed_episodes(data)\n                    send_data = [None] * len(data)\n\n                elif req == 'result':\n                    # report evaluation results\n                    self.feed_results(data)\n                    send_data = [None] * len(data)\n\n                elif req == 'model':\n                    for model_id in data:\n                        model = self.model\n                        if model_id != self.model_epoch:\n                            try:\n                                model = self.model_class()\n                                model.load_state_dict(torch.load(self.model_path(model_id))['model'], strict=False)\n                            except:\n                                # return latest model if failed to load specified model\n                                pass\n                        send_data.append(pickle.dumps(model))\n\n                if not multi_req and len(send_data) == 1:\n                    send_data = send_data[0]\n                self.worker.send(conn, send_data)\n            prev_update_episodes = next_update_episodes\n            self.update()\n        print('finished server')\n\n    def run(self):\n        try:\n            # open training thread\n            self.thread = threading.Thread(target=self.trainer.run)\n            self.thread.start()\n            # open generator, evaluator\n            self.worker.run()\n            self.server()\n\n        finally:\n            self.shutdown()\n\n\ndef train_main(args):\n    prepare_env(args['env_args'])  # preparing environment is needed in stand-alone mode\n    learner = Learner(args=args)\n    learner.run()\n\n\ndef train_server_main(args):\n    learner = Learner(args=args, remote=True)\n    learner.run()\n","1f26219a":"%%writefile handyrl\/agent.py\n# Copyright (c) 2020 DeNA Co., Ltd.\n# Licensed under The MIT License [see LICENSE for details]\n# changed\n# agent classes\n\nimport random\n\nimport numpy as np\n\nfrom .util import softmax\n\n\nclass RandomAgent:\n    def reset(self, env, show=False):\n        pass\n\n    def action(self, env, player, show=False):\n        actions = env.legal_actions(player)\n        return random.choice(actions)\n\n    def observe(self, env, player, show=False):\n        return [0.0]\n\n\nclass RuleBasedAgent(RandomAgent):\n    def action(self, env, player, show=False):\n        #if hasattr(env, 'rule_based_action'):\n        return env.rule_based_action(player)\n        #return random.choices([env.rule_based_action(player), env.rule_based_action2(player)], k=1, weights=[1, 1])[0]\n        # else:\n        #     return random.choice(env.legal_actions(player))\n\n\ndef print_outputs(env, prob, v):\n    if hasattr(env, 'print_outputs'):\n        env.print_outputs(prob, v)\n    else:\n        print('v = %f' % v)\n        print('p = %s' % (prob * 1000).astype(int))\n\n\nclass Agent:\n    def __init__(self, model, observation=False, temperature=0.0):\n        # model might be a neural net, or some planning algorithm such as game tree search\n        self.model = model\n        self.hidden = None\n        self.observation = observation\n        self.temperature = temperature\n\n    def reset(self, env, show=False):\n        self.hidden = self.model.init_hidden()\n\n    def plan(self, obs):\n        outputs = self.model.inference(obs, self.hidden)\n        self.hidden = outputs.pop('hidden', None)\n        return outputs\n\n    def action(self, env, player, show=False):\n        outputs = self.plan(env.observation(player))\n        actions = env.legal_actions(player)\n        p = outputs['policy']\n        v = outputs.get('value', None)\n        mask = np.ones_like(p)\n        mask[actions] = 0\n        p = p - mask * 1e32\n\n        if show:\n            print_outputs(env, softmax(p), v)\n\n        if self.temperature == 0:\n            ap_list = sorted([(a, p[a]) for a in actions], key=lambda x: -x[1])\n            return ap_list[0][0]\n        else:\n            return random.choices(np.arange(len(p)), weights=softmax(p \/ self.temperature))[0]\n\n    def observe(self, env, player, show=False):\n        v = None\n        if self.observation:\n            outputs = self.plan(env.observation(player))\n            v = outputs.get('value', None)\n            if show:\n                print_outputs(env, None, v)\n        return v if v is not None else [0.0]\n\n\nclass EnsembleAgent(Agent):\n    def reset(self, env, show=False):\n        self.hidden = [model.init_hidden() for model in self.model]\n\n    def plan(self, obs):\n        outputs = {}\n        for i, model in enumerate(self.model):\n            o = model.inference(obs, self.hidden[i])\n            for k, v in o:\n                if k == 'hidden':\n                    self.hidden[i] = v\n                else:\n                    outputs[k] = outputs.get(k, []) + [o]\n        for k, vl in outputs:\n            outputs[k] = np.mean(vl, axis=0)\n        return outputs\n\n\nclass SoftAgent(Agent):\n    def __init__(self, model, observation=False):\n        super().__init__(model, observation=observation, temperature=1.0)\n","782ec97b":"%%writefile config.yaml\n\nenv_args:\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n\n\ntrain_args:\n    turn_based_training: False  # always False for Hungry Geese\n    observation: False\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 800\n    minimum_episodes: 50000\n    maximum_episodes: 200000\n    eval_rate: 0.1\n    epochs: 5000\n    num_batchers: 28\n    worker:\n        num_parallel: 36\n    lambda: 0.7\n    policy_target: 'VTRACE'\n    value_target: 'VTRACE'\n    seed: 0\n    restart_epoch: 0\n    save_no_episodes: 200000\n    restart_with_saved_epochs: False\n    restart_with_saved_env_states: False\n\n\nworker_args:\n    server_address: ''\n    num_parallel: 48","983f899c":"import os\nimport sys\nimport yaml\n\nfrom handyrl.train import train_main \nfrom handyrl.train import train_server_main\nfrom handyrl.worker import worker_main\nfrom handyrl.evaluation import eval_main\nfrom handyrl.evaluation import eval_server_main\nfrom handyrl.evaluation import eval_client_main\nwith open('config.yaml') as f:\n    args = yaml.safe_load(f)\nprint(args)","9f5796bd":"import pickle\nimport bz2\nimport base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom joblib import dump, load\n\nif args['train_args']['restart_with_saved_env_states']:\n    restart_epoch = args['train_args']['restart_epoch']\n    modelload = base64.b64encode(bz2.compress(pickle.dumps(torch.load(f'models\/{restart_epoch}.pth')['model']))) \n    optimload = base64.b64encode(bz2.compress(pickle.dumps(torch.load(f'models\/{restart_epoch}.pth')['optim'])))\n    scalerload = base64.b64encode(bz2.compress(pickle.dumps(torch.load(f'models\/{restart_epoch}.pth')['scaler'])))\n    data_cnt_emaload = torch.load(f'models\/{restart_epoch}.pth')['data_cnt_ema']\n    stepsload = torch.load(f'models\/{restart_epoch}.pth')['steps']#\n    batch_cntload = torch.load(f'models\/{restart_epoch}.pth')['batch_cnt']\n    data_cntload = torch.load(f'models\/{restart_epoch}.pth')['data_cnt']#\n    print('loaded model, opt, scaler, steps, data_cnt_ema...')\n    with open('modelload.job', 'wb') as f:\n      dump(modelload, f, compress=('lzma', 3))\n    with open('optimload.job', 'wb') as f:\n      dump(optimload, f, compress=('lzma', 3))\n    with open('scalerload.job', 'wb') as f:\n      dump(scalerload, f, compress=('lzma', 3))\n    with open('data_cnt_emaload.job', 'wb') as f:\n      dump(data_cnt_emaload, f, compress=('lzma', 3))\n    with open('stepsload.job', 'wb') as f:\n      dump(stepsload, f, compress=('lzma', 3))\n    with open('batch_cntload.job', 'wb') as f:\n      dump(batch_cntload, f, compress=('lzma', 3))\n    with open('data_cntload.job', 'wb') as f:\n      dump(data_cntload, f, compress=('lzma', 3))\n    del modelload, optimload, scalerload, data_cnt_emaload, stepsload\n    import gc\n    gc.collect()\n    print('saved model, opt, scaler, steps, data_cnt_ema...')\n","66498022":"import warnings\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n\ntrain_main(args)","4ec58f2d":"Thanks for a very interesting and fun competition and great public contributions, like the HandyRL.\n\nI first started some training with replays and imitation learning, with my best in the ~1050 zone but wanted to learn more within the RL area so I left the imitation learning and started to use the RL\/HandyRL kernel https:\/\/github.com\/DeNA\/HandyRL\n\nAfter some practice with HandyRL and after studied the code closer it was time for the training, this was around two-three weeks prior the deadline. During this time the team behind HandyRL also released more information and a presentation from a conference https:\/\/www.kaggle.com\/c\/hungry-geese\/discussion\/247266, after this reading I implemented the https:\/\/www.kaggle.com\/ilialar\/risk-averse-greedy-goose as opponent, I think the implementation worked, and also used V-trace. \n\nThe idea was to train both a small model, and use the Alpha Zero with a thought that more analyses could be done with approximal half the prediction time vs the standard, and also a large model with the though that more parameters would create a better RL. But started to feel the time pressure and ended both ideas to favor a longer training with a medium sized model instead, with the thought that a longer trained model is the best choice.\n\nFor the longer training I needed to save the episodes and other information and states to continue the learning, so did an extra code for the feature. Started with joblib lzma compress that saved ~50% of org size but took much more time so changed to standard npy after a while, time pressure vs diskspace.\n\nI also did some other minor changes, \n-\tAdded Mixed Precision to double the batch size.\n-\tChanged Adam to Adamp https:\/\/arxiv.org\/abs\/2006.08217\n-\tChanged the relu to hardswish with inplace feature, based on that swish is a good replacement for relu in general and then using the faster version h-swish from https:\/\/arxiv.org\/abs\/1905.02244. After some feedback this change might not make big difference in RL. Just a bad habit changing this function from my part.\n\nI finally trained a model for 3310 epochs (not enough for the top 50 I guess), which I posted the final day.  I used 200k episodes in the saved resumed part, shifting with new episodes after 200k as in original code after max episodes is reached. More max episodes are preferable but due to testing and time pressure I used a smaller size, also wanted to try this option to use only the 200k latest.\n\nI have shared the changes and training in this notebook.\n\nAll credits to the original contributors and creators of HandyRL! A manual prediction is that they will win.\n"}}