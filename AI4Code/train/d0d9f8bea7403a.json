{"cell_type":{"e3bf8b83":"code","b0c69863":"code","1571e281":"code","0039d030":"code","7d8bf626":"code","13928c61":"code","dbfda495":"code","f5a84472":"code","ec3dbee0":"code","e4a66f65":"markdown","2c27cfd8":"markdown","97606940":"markdown","210a9782":"markdown"},"source":{"e3bf8b83":"import os\nimport time\nimport copy\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\nimport torch\nfrom torch import nn, optim\nimport torchvision\nfrom torchvision import datasets, models, transforms","b0c69863":"# Use GPU\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name())\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nprint(device)","1571e281":"os.environ[\"TORCH_HOME\"] = \"..\/working\"\ntrain_path = \"..\/input\/fruits\/fruits-360\/Training\"\ntest_path = \"..\/input\/fruits\/fruits-360\/Test\"\nnum_classes = 131","0039d030":"def set_parameter_requires_grad(model):\n    for param in model.parameters():\n        param.requires_grad = False","7d8bf626":"def init_model(model_name, num_classes, feature_extract=True, use_pretrained=True):\n    '''\n    batch_size:int - Optimal batch_size for each model identified for this fruits dataset \n                     to ensure maximum GPU utilization (for 16GB RAM)\n                     Note consider both validation and training batches\n    '''\n    is_inception = False\n    if \"alexnet\" in model_name:\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = None\n        batch_size = 10000\n\n    elif \"resnet\" in model_name:\n        if 'wide' in model_name:\n            if '50' in model_name:\n                model_ft = models.wide_resnet50_2(pretrained=use_pretrained)\n            elif '101' in model_name:\n                model_ft = models.wide_resnet101_2(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        elif '18' in model_name:\n            model_ft = models.resnet18(pretrained=use_pretrained)\n            batch_size = 5000    # \n        elif '34' in model_name:\n            model_ft = models.resnet34(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        elif '50' in model_name:\n            model_ft = models.resnet50(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        elif '101' in model_name:\n            model_ft = models.resnet101(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        else:\n            model_ft = models.resnet152(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n\n    elif \"vgg\" in model_name:\n        if '11' in model_name:\n            if 'bn' in model_name:\n                model_ft = models.vgg11_bn(pretrained=use_pretrained)\n            else:\n                model_ft = models.vgg11(pretrained=use_pretrained)\n            batch_size = 2000\n        elif '13' in model_name:\n            if 'bn' in model_name:\n                model_ft = models.vgg13_bn(pretrained=use_pretrained)\n                batch_size = 1000\n            else:\n                model_ft = models.vgg13(pretrained=use_pretrained)\n                batch_size = 2000\n        elif '16' in model_name:\n            if 'bn' in model_name:\n                model_ft = models.vgg16_bn(pretrained=use_pretrained)\n                batch_size = 2000\n            else:\n                model_ft = models.vgg16(pretrained=use_pretrained)\n                batch_size = 1000\n        elif '19' in model_name:\n            if 'bn' not in model_name:\n                model_ft = models.vgg19(pretrained=use_pretrained)\n            else:\n                model_ft = models.vgg19_bn(pretrained=use_pretrained)\n            batch_size = 2000\n        else:\n            model_ft = models.vgg16_bn(pretrained=use_pretrained)\n            batch_size = 2000\n\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = None\n\n    elif \"squeezenet\" in model_name:\n        if '1' in model_name and '0' in model_name:\n            model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n            batch_size = 5000    # Yet to determine\n        else:\n            model_ft = models.squeezenet1_1(pretrained=use_pretrained)\n            batch_size = 10000\n        \n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = None\n\n    elif 'shufflenet' in model_name:\n        if '0' in model_name and '5' in model_name:\n            model_ft = models.shufflenet_v2_x0_5(pretrained=use_pretrained)\n            batch_size = 20000    # 16.1GB\n        elif '1' in model_name and '5' in model_name:\n            #model_ft = models.shufflenet_v2_x1_5(pretrained=use_pretrained)\n            print(\"ShuffleNet 2.0 not implemented yet, using 1.0 instead\")\n            model_ft = models.shufflenet_v2_x1_0(pretrained=use_pretrained)\n            batch_size = 20000\n        elif '2' in model_name and '0' in model_name:\n            #model_ft = models.shufflenet_v2_x2_0(pretrained=use_pretrained)\n            print(\"ShuffleNet 2.0 not implemented yet, using 1.0 instead\")\n            model_ft = models.shufflenet_v2_x1_0(pretrained=use_pretrained)\n            batch_size = 20000\n        else:\n            model_ft = models.shufflenet_v2_x1_0(pretrained=use_pretrained)\n            batch_size = 20000    # Yet to determine\n\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n\n    elif \"densenet\" in model_name:\n        if '121' in model_name:\n            model_ft = models.densenet121(pretrained=use_pretrained)\n            batch_size = 5000\n        elif '169' in model_name:\n            model_ft = models.densenet169(pretrained=use_pretrained)\n            batch_size = 5000\n        elif '201' in model_name:\n            model_ft = models.densenet201(pretrained=use_pretrained)\n            batch_size = 5000\n        else:\n            model_ft = models.densenet161(pretrained=use_pretrained)\n            batch_size = 3000    # 15.963GB\n\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n\n    elif \"inception\" in model_name or 'googlenet' in model_name:\n        # Be careful, expects (299,299) sized images and has auxiliary output\n        if '1' in model_name or 'googlenet' in model_name:\n            model_ft = models.googlenet(pretrained=use_pretrained)\n            if feature_extract:\n                set_parameter_requires_grad(model_ft)\n            num_ftrs = model_ft.fc.in_features\n            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n            input_size = None\n            batch_size = 6000    # Usage 12GB\n        else:\n            is_inception = True\n            model_ft = models.inception_v3(pretrained=use_pretrained)\n            if feature_extract:\n                set_parameter_requires_grad(model_ft)\n            # Handle the auxilary net\n            num_ftrs1 = model_ft.AuxLogits.fc.in_features\n            model_ft.AuxLogits.fc = nn.Linear(num_ftrs1, num_classes)\n            # Last Layer\n            num_ftrs = model_ft.fc.in_features\n            model_ft.fc = nn.Linear(num_ftrs,num_classes)\n            input_size = 299\n            batch_size = 500    # 13GB\n\n    elif 'mnasnet' in model_name:\n        if '0' in model_name and '75' in model_name:\n            #model_ft = models.mnasnet0_75(pretrained=use_pretrained)\n            print(\"Mnasnet 0.75 not implemented yet, using 1.0 instead\")\n            model_ft = models.mnasnet1_0(pretrained=use_pretrained)\n            batch_size = 8000\n        elif '0' in model_name and '5' in model_name:\n            model_ft = models.mnasnet0_5(pretrained=use_pretrained)\n            batch_size = 16000\n        elif '1' in model_name and '3' in model_name:\n            #model_ft = models.mnasnet1_3(pretrained=use_pretrained)\n            print(\"Mnasnet 1.3 not implemented yet, using 1.0 instead\")\n            model_ft = models.mnasnet1_0(pretrained=use_pretrained)\n            batch_size = 8000\n        else:\n            model_ft = models.mnasnet1_0(pretrained=use_pretrained)\n            batch_size = 8000\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.classifier[1].in_features\n        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n\n    elif 'mobilenet' in model_name:\n        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.classifier[1].in_features\n        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n        batch_size = 5000    # Usage 14.7 GB\n    \n    elif 'resnext' in model_name:\n        if '50' in model_name:\n            model_ft = models.resnext50_32x4d(pretrained=use_pretrained)\n            batch_size = 5000\n        else:\n            model_ft = models.resnext101_32x8d(pretrained=use_pretrained)\n            batch_size = 3000    # 15.983GB\n        if feature_extract:\n            set_parameter_requires_grad(model_ft)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = None\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, batch_size, input_size, is_inception","13928c61":"class CustomSampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    def __len__(self):\n        return len(self.indices)","dbfda495":"model_names = ['mobilenet', 'densenet161', 'resnet101']\nfeature_extract = False\nuse_pretrained = False\nval_split = 0.2\nrandom_seed = 42\nnum_epochs = 50\ndiff = 1000","f5a84472":"fig, ((train_acc, val_acc), (train_loss, val_loss)) = plt.subplots(2, 2, figsize=(15, 15))\nfor model_name in model_names:\n    print(\"-\"*20,model_name,\"-\"*20)\n    # # INITIALIZE MODEL\n    model, BATCH_SIZE, input_size, is_inception = init_model(model_name, num_classes=num_classes, \n                                                             feature_extract=feature_extract, use_pretrained=use_pretrained)\n    optimize = {\n        'name': 'adam',\n        'lr': 0.01\n    }\n    # # Parameters to update\n    params_to_update = list(model.parameters())\n    n_p = 0\n    for p in model.parameters():\n        n_p += p.numel()\n\n    # # Transforming image data\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    if input_size is not None:\n        transformer = transforms.Compose([transforms.Resize(input_size), transforms.ToTensor(), normalize])\n    else:\n        transformer = transforms.Compose([transforms.ToTensor(), normalize])\n\n    # # TRAINING MODEL    \n    patience = 5    # Early stopping patience\n    tol = 0    # Early stopping tolerance\n    \n    model = model.to(device)\n\n    metrics = {\n        'n_epochs': num_epochs,\n        'optimizer': optimize,\n        'train_acc': [],\n        'train_loss': [],\n        'val_acc': [],\n        'val_loss': [],\n        'test_acc': 0,\n        'test_loss': 0,\n        'train_time': 0,\n        'n_params': n_p\n    }\n    best_model_wts = copy.deepcopy(model.state_dict())\n    \n    start_time = time.time()\n    while True:\n        try:\n            #  # LOSS AND OPTIMIZER\n            criterion = nn.CrossEntropyLoss()\n            if optimize['name'] == 'adam':\n                optimizer = optim.Adam(params_to_update, lr=optimize['lr'])\n            elif optimize['name'] == 'sgd':\n                optimizer = optim.SGD(params_to_update, lr=optimize['lr'], momentum=optimize['m'])\n            # # DATA LOADER\n            dataset = datasets.ImageFolder(root=train_path, transform=transformer)\n            #labels_mapping = pd.Series(dataset.classes)\n            targets = dataset.targets\n            train_idx, val_idx = train_test_split(np.arange(len(targets)), test_size=val_split, shuffle=True, \n                                                  random_state=random_seed, stratify=targets)\n            n_train, n_val = len(train_idx), len(val_idx)\n            # Don't shuffle again since above shuflling is already done\n            train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, sampler=CustomSampler(train_idx))\n            val_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, sampler=CustomSampler(val_idx))\n\n            test_dataset = datasets.ImageFolder(root=test_path, transform=transformer)\n            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n            wait = 0\n            for epoch in range(num_epochs):\n                # ## Training\n                model.train()  # Set model to training mode\n                running_loss = 0.0\n                running_acc = 0\n                for inputs, labels in iter(train_loader):\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n                    if is_inception:\n                        # From https:\/\/discuss.pytorch.org\/t\/how-to-optimize-inception-model-with-auxiliary-classifiers\/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model.forward(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    loss.backward()\n                    optimizer.step()\n                    # loss.item() gives averaged loss for this batch, hence get back the original sum of losses by multiplying the batch size\n                    running_loss += loss.item() * inputs.size(0)\n                    running_acc += torch.sum(preds == labels.data).item()\n                # Mean accuracy of epoch\n                metrics['train_acc'].append(100*running_acc \/ n_train)\n                # Mean loss of epoch\n                metrics['train_loss'].append(running_loss \/ n_train)\n                del inputs, labels, loss\n                if 'cuda' in str(device):\n                    torch.cuda.empty_cache()\n                # ## Validation\n                model.eval()\n                running_loss = 0.0\n                running_acc = 0\n                for inputs, labels in iter(val_loader):\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    with torch.set_grad_enabled(False):\n                        outputs = model.forward(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n                    # item() to return only value, otherwise cuda tensor is returned\n                    # loss.item() gives averaged loss for this batch, hence get back the original sum of losses by multiplying the batch size\n                    running_loss += loss.item() * inputs.size(0)\n                    running_acc += torch.sum(preds == labels.data).item()\n                # Mean validation accuracy of epoch\n                metrics['val_acc'].append(100*running_acc \/ n_val)\n                # Mean validation loss of epoch\n                metrics['val_loss'].append(running_loss \/ n_val)\n\n                # ## Early Stopping\n                if metrics['val_acc'][-1] >= max(metrics['val_acc'])-tol:\n                    wait = 0\n                    # Store weights for restoring best weights in case early stopped\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                else:\n                    wait += 1\n                    # Stop if validation accuracy is not increasing for the past 'wait' epochs\n                    if wait >= patience:\n                        # Load best model weights\n                        model.load_state_dict(best_model_wts)\n                        print(\"Early stopping at epoch %d out of total %d\" % (epoch+1, num_epochs))\n                        break\n\n            metrics['train_time'] = time.time() - start_time\n\n            # Plot train and validation metrics\n            train_acc.plot(metrics['train_acc'], label=model_name)\n            train_loss.plot(metrics['train_loss'])\n            val_acc.plot(metrics['val_acc'])\n            val_loss.plot(metrics['val_loss'])\n\n            del inputs, labels, loss, optimizer\n            if 'cuda' in str(device):\n                torch.cuda.empty_cache()\n\n            # # TEST DATA\n            model.eval()\n            running_loss = 0.0\n            running_acc = 0\n            for inputs, labels in iter(test_loader):\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                with torch.set_grad_enabled(False):\n                    outputs = model.forward(inputs)\n                    loss = criterion(outputs, labels)\n                _, preds = torch.max(outputs, 1)\n\n                running_loss += loss.item() * inputs.size(0)\n                running_acc += torch.sum(preds == labels.data).item()\n            metrics['test_acc'] = 100*running_acc \/ len(test_loader.dataset)\n            metrics['test_loss'] = running_loss \/ len(test_loader.dataset)\n\n            print(\"Best train accuracy = %.2f, Best validation accuracy = %.2f, Test accuracy = %.2f\" % \n                  (max(metrics['train_acc']), max(metrics['val_acc']), metrics['test_acc']))\n\n            # # Save model weights and related metrics\n            torch.save({'metrics': metrics,\n                        'model_state_dict': model.state_dict()}, \n                        f'{os.environ[\"TORCH_HOME\"]}\/{model_name}')\n\n            # # Clear GPU Memory\n            del inputs, labels, loss, model\n            if 'cuda' in str(device):\n                torch.cuda.empty_cache()\n\n            # # Delete downloaded model\n            pretrained_model_dir = os.path.join(os.environ['TORCH_HOME'], 'checkpoints')\n            l = os.listdir(pretrained_model_dir)\n            for filename in l:\n                os.remove(os.path.join(pretrained_model_dir, filename))\n            print(\"Best Batch Size = \", BATCH_SIZE)\n            break\n        except RuntimeError:\n            print(\"Error with Batch Size \", BATCH_SIZE)\n            if BATCH_SIZE - diff > 0:\n                BATCH_SIZE = int(BATCH_SIZE - diff)\n            else:\n                diff = int(diff \/ 2)\n                BATCH_SIZE = int(BATCH_SIZE - diff)\n            \n        except ValueError as ve:\n            print(f\"Error in {model_name}: {ve}\")\n        finally:\n            # # Clear GPU Memory\n            try:\n                del inputs, labels, loss, optimizer\n                torch.cuda.empty_cache()\n            except:\n                torch.cuda.empty_cache()\n\n# Prettifying plot\nyticks = range(50, 110, 10)\ntrain_acc.set_title('Training Accuracy')\ntrain_acc.set_yticks(yticks)\ntrain_loss.set_title('Training Loss')\nval_acc.set_title('Validation Accuracy')\nval_acc.set_yticks(yticks)\nval_loss.set_title('Validation Loss')\n\nfig.legend()\n# add a big axis, hide frame\nfig.add_subplot(111, frameon=False)\n# hide tick and tick label of the big axis\nplt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\nplt.xlabel(\"Epochs ->\")\nplt.ylabel(\"Metric\")\nplt.tight_layout()","ec3dbee0":"os.rmdir(os.path.join(os.environ['TORCH_HOME'], 'checkpoints'))","e4a66f65":"## Process","2c27cfd8":"# Run the process","97606940":"## Specify models","210a9782":"# Function to initialize Pretrained Models"}}