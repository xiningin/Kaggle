{"cell_type":{"fe3a1d3f":"code","ac26c630":"code","df47998e":"code","b68fc93a":"code","7394667c":"code","fd9a7d1f":"code","39e9338e":"code","1edb94d7":"code","f7ad7644":"code","7a5d64c1":"code","d1b3e1ea":"code","d3ac9425":"markdown","e6b6784d":"markdown","c3161d28":"markdown","eb2719c3":"markdown","c3ebf4be":"markdown","1e294dff":"markdown","24bdcb77":"markdown","fdeb872d":"markdown"},"source":{"fe3a1d3f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os \n#The functions that the OS module provides allows you to interface with the operating system that Python is running on\nfrom sklearn.pipeline import Pipeline \n#Pipeline is used to assemble several steps that can be cross-validated together while setting different parameters. \n\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers \n#for building up the different layers of neural network\n#Keras tensor flow deep learning library to create a deep learning model for both regression and classification problems.\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Embedding,  Flatten\n#importing necessary dunctions for building the network\nfrom tensorflow.keras.models import Model, Sequential\n#Sequential model allows us to create a deep learning model by adding layers to it. \n#Here, every unit in a layer is connected to every unit in the previous layer. \nfrom keras.callbacks import ReduceLROnPlateau\n#ReduceLROnPlateau is used to Reduce learning rate when a metric has stopped improving.\nfrom keras.optimizers import RMSprop\n#Optimizer that implements the RMSprop algorithm.\n\nfrom tensorflow.data import Dataset\n#It handles downloading and preparing the data deterministically and constructing a tf.data.Dataset \nfrom sklearn.model_selection import train_test_split\n#From splitting the dataset for training and testing\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\n#QuantileTransformer method transforms the features to follow a uniform or a normal distribution\n#KBinsDiscretizer bins continuous data into intervals.\nfrom tensorflow import keras\n\nfrom sklearn.impute import SimpleImputer\n#Imputation transformer for completing missing values.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n#For quantifying the quality of predictions","ac26c630":"#You need to first install datatable with pip command\n#pip install datatable","df47998e":"#datatable package is like pandas which can read data up to 10 times faster than pandas.\n#import datatable as dt ","b68fc93a":"%%time\n# %%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsub   = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n#We are again coverting it to pandas.\n#Check refrence (10) for details","7394667c":"train.head()","fd9a7d1f":"%%time\ntrain['n_missing'] = train.isna().sum(axis=1) \n#Checking the total null value row wise and storing it to a new column\n\ntest['n_missing'] = test.isna().sum(axis=1)\n#Checking the total null value row wise and storing it to a new column\ntrain['claim'] = train['claim'].astype(str)\n#Converting the int datatypes to string datatype\n\nfeatures = [col for col in train.columns if col not in ['claim', 'id']]\n#Here we are taking only the features . We are not invluding the output(claim) and id.\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=128,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=128, encode='ordinal',strategy='uniform'))\n        ])\n#Now we are ready to create a pipeline object by providing with the list of steps. \n#Our steps are \u2014 SimpleImputer,QuantileTransformer and KBinsDiscretizer\n#These steps are list of tuples consisting of name and an instance of the transformer or estimator. \n#For imputing the missing value, we have used median.\n\ntrain[features] = pipe.fit_transform(train[features])\ntest[features] = pipe.transform(test[features])\n#transforming the features with pipeline","39e9338e":"train","1edb94d7":"model = Sequential([\n    Input(train[features].shape[1:]), #train[features].shape[1:] is used to get the input shape\n    #The model needs to know what input shape it should expect. \n    #For this reason, the first layer in a Sequential model needs to receive information about its input shape\n    Embedding(input_dim=512, output_dim=4),\n    #we use an embedding layer to compress the input feature space into a smaller one.\n    #There are three parameters to the embedding layer\n    #input_dim : Size of the vocabulary\n    #output_dim : Length of the vector for each word\n    #input_length : Maximum length of a sequence\n    \n    Flatten(),\n    Dense(64,  activation='relu'),\n    Dropout(0.5),\n    Dense(32,  activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid'),\n])\n\nauc = tf.keras.metrics.AUC(name='aucroc') #Defining how we want to evaluate our model\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=5e-2,\n        decay_steps = 3000,\n        decay_rate= 0.8)\n#We are using a learning rate schedule to modulate how the learning rate of your optimizer changes over time\n\noptimizer = RMSprop(lr=1e-3, rho=0.9, epsilon=1e-08, decay=0.0) \n#This optimizer is usually a good choice for recurrent neural networks.\n#\"rho\" is the decay factor or the exponentially weighted average over the square of the gradients.\n#\"decay\" decays the learning rate over time, so we can move even closer to the local minimum in the end of training.\n#Check reference (8) for details\nmodel.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[auc]) \n#Keras model provides a method, compile() to compile the model.\n#The important arguments are as follows \u2212loss function,Optimizer and metrics\n#Check reference (9) for details","f7ad7644":"model.fit(x = np.float32(train[features]), y = np.float32(train.claim),\n          batch_size = 512, shuffle = True, epochs = 10) \n#Models are trained by NumPy arrays using fit(). \n#The main purpose of this fit function is used to evaluate your model on training.\n#Check reference (9) for details","7a5d64c1":"sub['claim'] = model.predict(np.float32(test[features]))\nsub=sub.set_index('id')\nsub.to_csv('submission.csv')\n#and lastly we are creating our submission model","d1b3e1ea":"sub","d3ac9425":"# Modeling","e6b6784d":"# Load Dataset","c3161d28":"# Preprocessing","eb2719c3":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/28009\/logos\/header.png?)","c3ebf4be":"# If you find the notebook useful, you can thumbs it up.","1e294dff":"# Your task should be,\n# **Focus on the hyperparameters of the neural network. Try to change the values and check whether your auc value is improving or not.**","24bdcb77":"References:\n\n1)https:\/\/www.tensorflow.org\/datasets\/overview\n\n2)https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n\n3)https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.KBinsDiscretizer.html\n\n4)https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html\n\n5)https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html\n\n6)https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n\n7)https:\/\/keras.io\/api\/optimizers\/learning_rate_schedules\/\n\n8)https:\/\/keras.io\/api\/optimizers\/\n\n9)https:\/\/www.tutorialspoint.com\/keras\/keras_model_compilation.htm\n\n10)https:\/\/www.kdnuggets.com\/2019\/08\/overview-python-datatable-package.html#:~:text=Modern%20machine%20learning%20applications%20need%20to%20process%20a,a%20single-node%20machine%2C%20at%20the%20maximum%20possible%20speed.","fdeb872d":"# **This is a updated version of the public notebook. I have updated it with full explanation. I have also added all the refereneces to each line, so t will be easier for evryone to modify it.**"}}