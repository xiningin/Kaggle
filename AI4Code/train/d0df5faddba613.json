{"cell_type":{"5e72461a":"code","02a846e6":"code","d6b940d2":"code","b7271b80":"code","9cde2e49":"code","6f256bc9":"code","6322c7dd":"code","db44f277":"code","3e9bfbd9":"code","1c266d4d":"code","88c111cd":"code","3f2fcb63":"code","3cc37b95":"code","1914e611":"code","b3bc25fb":"code","5016b88d":"markdown","57d702c9":"markdown","4d5b23cc":"markdown","bc40d2e2":"markdown","9cbdad9a":"markdown","35abcf18":"markdown","0e1b8ad7":"markdown","e57ff825":"markdown"},"source":{"5e72461a":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport sys\nimport random\nimport math\nimport subprocess\nfrom glob import glob\nfrom collections import OrderedDict\nimport numpy as np\nimport cv2\nfrom skimage import measure\nimport pandas as pd\nfrom tqdm import tqdm_notebook, tqdm\nfrom matplotlib import pyplot as plt\nimport scipy.ndimage as ndi\nfrom PIL import Image\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import model_zoo\n\nimport torch.backends.cudnn as cudnn\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.optim.optimizer import Optimizer, required\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n        \n        num_classes = 5\n        self.label_linear = nn.Linear(512 * block.expansion, num_classes)\n        self.relu_ = nn.ReLU(inplace=True)\n\n        self.emb = nn.Embedding(num_classes, num_classes)\n        self.emb.weight = nn.Parameter(torch.eye(num_classes))\n        \n#         self.emb_mask = nn.Embedding(num_classes, num_classes)\n#         self.emb_mask.weight =  nn.Parameter(torch.Tensor([\n#             [1,1,0,0,0],\n#             [1,1,1,0,0],\n#             [0,1,1,1,0],\n#             [0,0,1,1,1],\n#             [0,0,0,1,1]]))\n#         self.emb_mask.weight.requires_grad_(False)\n        \n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x2 = x\n        x = self.last_linear(x)\n        x2 = self.label_linear(x2.detach())\n        return x, x2\n\n    def forward(self, x, targets=None):\n        x = self.features(x)\n        x, x2 = self.logits(x)\n        if targets is not None:\n            tar = self.emb(targets).cuda()\n#             tar = tar * (self.emb_mask(targets).cuda())\n        else:\n            tar=None\n        return x, x2, tar, self.emb.weight\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    \n    pretrained_dict=model_zoo.load_url(settings['url'])\n    model_dict = model.state_dict()\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n    # model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n#     if pretrained is not None:\n#         settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n#         initialize_pretrained_model(model, num_classes, settings)\n    return model","02a846e6":"def get_modified_model(model_name='se_resnext50_32x4d', num_outputs=None, pretrained=True, \n            freeze_bn=False, dropout_p=0, **kwargs):\n    model=se_resnext50_32x4d()\n    model.avg_pool = nn.AdaptiveAvgPool2d(1)\n    in_features = model.last_linear.in_features\n    if dropout_p == 0:\n        model.last_linear = nn.Linear(in_features, num_outputs)\n    else:\n        model.last_linear = nn.Sequential(\n            nn.Dropout(p=dropout_p),\n            nn.Linear(in_features, num_outputs))\n    if freeze_bn:\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.weight.requires_grad = False\n                m.bias.requires_grad = False\n\n    return model","d6b940d2":"def scale_radius(src, img_size, padding=False):\n    x = src[src.shape[0] \/\/ 2, ...].sum(axis=1)\n    r = (x > x.mean() \/ 10).sum() \/\/ 2\n    yx = src.sum(axis=2)\n    region_props = measure.regionprops((yx > yx.mean() \/ 10).astype('uint8'))\n    yc, xc = np.round(region_props[0].centroid).astype('int')\n    x1 = max(xc - r, 0)\n    x2 = min(xc + r, src.shape[1] - 1)\n    y1 = max(yc - r, 0)\n    y2 = min(yc + r, src.shape[0] - 1)\n    dst = src[y1:y2, x1:x2]\n    dst = cv2.resize(dst, dsize=None, fx=img_size\/(2*r), fy=img_size\/(2*r))\n    if padding:\n        pad_x = (img_size - dst.shape[1]) \/\/ 2\n        pad_y = (img_size - dst.shape[0]) \/\/ 2\n        dst = np.pad(dst, ((pad_y, pad_y), (pad_x, pad_x), (0, 0)), 'constant')\n    return dst\n\n    \nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_paths, labels, transform=None, img_size=288, save_img=True):\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transform = transform\n        self.img_size = img_size\n        self.save_img = save_img\n\n    def __getitem__(self, index):\n        img_path, label = self.img_paths[index], self.labels[index]\n        \n        if 'train' in img_path:\n            img = cv2.imread('..\/input\/aptos2019-resized\/images_288_scaled\/%s' %os.path.basename(img_path))\n        \n        else:\n            if os.path.exists('processed\/%s' %os.path.basename(img_path)):\n                img = cv2.imread('processed\/%s' %os.path.basename(img_path))\n\n            else:\n                img = cv2.imread(img_path)\n                try:\n                    img = scale_radius(img, img_size=self.img_size, padding=False)\n                except Exception as e:\n                    img = img\n                if self.save_img:\n                    cv2.imwrite('processed\/%s' %os.path.basename(img_path), img)\n        \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = Image.fromarray(img)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, label\n\n\n    def __len__(self):\n        return len(self.img_paths)","b7271b80":"class RAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef quadratic_weighted_kappa(y_pred, y_true):\n    if torch.is_tensor(y_pred):\n        y_pred = y_pred.data.cpu().numpy()\n    if torch.is_tensor(y_true):\n        y_true = y_true.data.cpu().numpy()\n    if y_pred.shape[1] == 1:\n        y_pred = y_pred[:, 0]\n    else:\n        y_pred = np.argmax(y_pred, axis=1)\n    return metrics.cohen_kappa_score(y_pred, y_true, weights='quadratic')\n\ndef comp_Loss(out1, out2, tar, emb_w, targets):\n    \"\"\"Compute the total loss\"\"\"\n    out2_prob = F.softmax(out2, dim=1)\n    tau2_prob = F.softmax(out2 \/ 2., dim=1).detach()\n    soft_tar = F.softmax(tar, dim=1).detach()# + 0.5*F.softmax(tar1).detach()\n#     soft_tar = (tar\/torch.sum(tar, dim=1).unsqueeze(1)).detach()\n    L_o1_y = F.cross_entropy(out1, targets)\n\n    alpha = 0.9 # default:0.9\n    beta = 0.5 # default:0.5#adjust_alpha(epoch)\n    _, pred = torch.max(out2,1)\n    mask = pred.eq(targets).float().detach()\n    L_o1_emb = -torch.mean(my_loss(out1, soft_tar))\n\n    L_o2_y = F.cross_entropy(out2, targets)\n    L_emb_o2 = -torch.sum(my_loss(tar, tau2_prob)*mask)\/(torch.sum(mask)+1e-8)\n    gap = torch.gather(out2_prob, 1, targets.view(-1,1))-alpha\n    L_re = torch.sum(F.relu(gap))\n    #L2_loss = F.mse_loss(emb_w.t(), emb_w.detach())\n    \n    loss = beta*L_o1_y + (1-beta)*L_o1_emb +L_o2_y +L_emb_o2 +L_re\n    return loss\n\ndef my_loss(logit, prob):\n    \"\"\" Cross-entropy function\"\"\"\n    soft_logit = F.log_softmax(logit, dim=1)\n    loss = torch.sum(prob*soft_logit, 1)\n    return loss\n\ndef compute_accuracy(y_pred,y_true):\n    if torch.is_tensor(y_pred):\n        y_pred = y_pred.data.cpu().numpy()\n    if torch.is_tensor(y_true):\n        y_true = y_true.data.cpu().numpy()\n    if y_pred.shape[1] == 1:\n        y_pred = y_pred[:, 0]\n    else:\n        y_pred = np.argmax(y_pred, axis=1)\n    return metrics.accuracy_score(y_true, y_pred, normalize=True)","9cde2e49":"def my_train(train_loader, model, criterion, optimizer, epoch):\n    losses = AverageMeter()\n    scores = AverageMeter()\n    ac_scores = AverageMeter()\n\n    model.train()\n\n    for i, (input, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        input = input.cuda()\n        target = target.cuda()\n\n        output = model(input, target)\n        out1, out2, tar, emb_w = output\n        \n        loss = comp_Loss(out1, out2, tar, emb_w, target)\n        # compute gradient and do optimizing step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        score = quadratic_weighted_kappa(out1, target)\n        ac_score = compute_accuracy(out1, target)\n\n\n        losses.update(loss.item(), input.size(0))\n        scores.update(score, input.size(0))\n        ac_scores.update(ac_score, input.size(0))\n\n    return losses.avg, scores.avg, ac_scores.avg\n\ndef my_validate(val_loader, model, criterion):\n    losses = AverageMeter()\n    scores = AverageMeter()\n    ac_scores = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        for i, (input, target) in tqdm(enumerate(val_loader), total=len(val_loader)):\n            input = input.cuda()\n            target = target.cuda()\n\n            output = model(input, target)\n            out1, out2, tar, emb_w = output\n\n            loss = comp_Loss(out1, out2, tar, emb_w, target)\n\n            score = quadratic_weighted_kappa(out1, target)\n            ac_score = compute_accuracy(out1, target)\n\n            losses.update(loss.item(), input.size(0))\n            scores.update(score, input.size(0))\n            ac_scores.update(ac_score, input.size(0))\n\n    return losses.avg, scores.avg, ac_scores.avg","6f256bc9":"os.makedirs('processed', exist_ok=True)","6322c7dd":"pseudo_probs = {}\n\naptos2019_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\naptos2019_img_paths = '..\/input\/aptos2019-blindness-detection\/train_images\/' + aptos2019_df['id_code'].values + '.png'\naptos2019_labels = aptos2019_df['diagnosis'].values\n\ntest_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\ndir_name = '..\/input\/aptos2019-blindness-detection\/test_images'\ntest_img_paths = dir_name + '\/' + test_df['id_code'].values + '.png'\ntest_labels = np.zeros(len(test_img_paths))\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\ntest_set = Dataset(\n    test_img_paths,\n    test_labels,\n    transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(\n    test_set,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2)","db44f277":"# create model\nmodel = get_modified_model(model_name='se_resnext50_32x4d',\n                  num_outputs=5,\n                  pretrained=False,\n                  freeze_bn=True,\n                  dropout_p=0)\nmodel = model.cuda()\nmodel.eval()\n\nprobs = []\nfor fold in range(5):\n    print('Fold [%d\/%d]' %(fold+1, 5))\n\n    model.load_state_dict(torch.load('..\/input\/my-se-resnext50-32x4d-10211607\/model_1_%d.pth' % (fold+1)))\n\n    probs_fold = []\n    with torch.no_grad():\n        for i, (input, _) in tqdm(enumerate(test_loader), total=len(test_loader)):\n            input = input.cuda()\n            output,_,_,_ = model(input)\n            probs_fold.extend(output.data.cpu().numpy())\n    probs_fold = np.array(probs_fold)\n#     print(probs_fold.shape)\n    probs.append(probs_fold)\ndel model\ntorch.cuda.empty_cache()\n!nvidia-smi","3e9bfbd9":"result = np.mean(probs,axis=0)\nresult = np.argmax(result,axis=1)\npseudo_probs['se_resnext50_32x4d'] = result","1c266d4d":"l1_probs = {}\n\ntrain_transform = []\ntrain_transform = transforms.Compose([\n    transforms.Resize((288, 288)),\n    transforms.RandomAffine(\n        degrees=(-180, 180),\n        scale=(0.8889, 1.0),\n        shear=(-36, 36),\n    ),\n    transforms.CenterCrop(256),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.ColorJitter(\n        brightness=0,\n        contrast=(0.9, 1.1),\n        saturation=0,\n        hue=0),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])","88c111cd":"aptos2019_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\naptos2019_img_paths = '..\/input\/aptos2019-blindness-detection\/train_images\/' + aptos2019_df['id_code'].values + '.png'\naptos2019_labels = aptos2019_df['diagnosis'].values\n\ntest_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\ntest_img_paths = '..\/input\/aptos2019-blindness-detection\/test_images\/' + test_df['id_code'].values + '.png'\ntest_labels = pseudo_probs['se_resnext50_32x4d']\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=41)\nkf = KFold(n_splits=5, shuffle=True, random_state=41)\nimg_paths = []\nlabels = []\nfor (train_idx1, val_idx1), (train_idx2, val_idx2) in zip(skf.split(aptos2019_img_paths, aptos2019_labels), kf.split(test_img_paths)):\n    img_paths.append((np.hstack((aptos2019_img_paths[train_idx1], test_img_paths[val_idx2])), aptos2019_img_paths[val_idx1]))\n    labels.append((np.hstack((aptos2019_labels[train_idx1], test_labels[val_idx2])), aptos2019_labels[val_idx1]))","3f2fcb63":"# create model\nmodel = get_modified_model(model_name='se_resnext50_32x4d',\n                  num_outputs=5,\n                  pretrained=False,\n                  freeze_bn=True,\n                  dropout_p=0,\n                  )\nmodel = model.cuda()\n\ncriterion = nn.CrossEntropyLoss().cuda()\n\nbest_losses = []\nbest_scores = []\n\nfor fold, ((train_img_paths, val_img_paths), (train_labels, val_labels)) in enumerate(zip(img_paths, labels)):\n    print('Fold [%d\/%d]' %(fold+1, len(img_paths)))\n\n    # train\n    train_set = Dataset(\n        train_img_paths,\n        train_labels,\n        transform=train_transform,\n        img_size=288,\n        save_img=True)\n    train_loader = torch.utils.data.DataLoader(\n        train_set,\n        batch_size=32,\n        shuffle=True,\n        num_workers=2)\n\n    val_set = Dataset(\n        val_img_paths,\n        val_labels,\n        transform=val_transform,\n        save_img=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_set,\n        batch_size=32,\n        shuffle=False,\n        num_workers=2)\n\n    model.load_state_dict(torch.load('..\/input\/my-se-resnext50-32x4d-10211607\/model_1_%d.pth' % (fold+1)))\n\n    optimizer = RAdam(\n        filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n\n    scheduler = lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=10, eta_min=1e-5)\n    \n    os.makedirs('se_resnext50_32x4d', exist_ok=True)\n\n    best_loss = float('inf')\n    best_score = 0\n    best_ac_score = 0\n    for epoch in range(10):\n        print('Epoch [%d\/%d]' % (epoch + 1, 10))\n\n        # train for one epoch\n        train_loss, train_score, train_ac_score = my_train(train_loader, model, criterion, optimizer, epoch)\n        # evaluate on validation set\n        val_loss, val_score, val_ac_score = my_validate(val_loader, model, criterion)\n\n        scheduler.step()\n\n        print('loss %.4f - score %.4f - ac_score %.4f - val_loss %.4f - val_score %.4f - val_ac_score %.4f'\n              % (train_loss, train_score, train_ac_score, val_loss, val_score, val_ac_score))\n\n        if val_loss < best_loss:\n            torch.save(model.state_dict(), 'se_resnext50_32x4d\/model_%d.pth' %(fold+1))\n            best_loss = val_loss\n            best_score = val_score\n            best_ac_socre = val_ac_score\n            print(\"=> saved best model\")\n\n    print('val_loss:  %f' % best_loss)\n    print('val_score: %f' % best_score)\n    print('val_ac_score: %f' % best_ac_score)\n    \ndel model\ntorch.cuda.empty_cache()\n!nvidia-smi","3cc37b95":"test_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')\ndir_name = '..\/input\/aptos2019-blindness-detection\/test_images'\ntest_img_paths = dir_name + '\/' + test_df['id_code'].values + '.png'\ntest_labels = np.zeros(len(test_img_paths))\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\ntest_set = Dataset(\n    test_img_paths,\n    test_labels,\n    transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(\n    test_set,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2)\n\n# create model\nmodel = get_modified_model(model_name='se_resnext50_32x4d',\n                  num_outputs=5,\n                  pretrained=False,\n                  freeze_bn=True,\n                  dropout_p=0)\nmodel = model.cuda()\nmodel.eval()\n\nprobs = []\nfor fold in range(5):\n    print('Fold [%d\/%d]' %(fold+1, 5))\n\n    model.load_state_dict(torch.load('se_resnext50_32x4d\/model_%d.pth' % (fold+1)))\n\n    probs_fold = []\n    with torch.no_grad():\n        for i, (input, _) in tqdm(enumerate(test_loader), total=len(test_loader)):\n            input = input.cuda()\n            output,_,_,_ = model(input)\n            probs_fold.extend(output.data.cpu().numpy())\n    probs_fold = np.array(probs_fold)\n    probs.append(probs_fold)\n\nprobs = np.mean(probs, axis=0)\nprobs = np.argmax(probs, axis=1)\nl1_probs['se_resnext50_32x4d'] = probs\n\ndel model\ntorch.cuda.empty_cache()\n!nvidia-smi","1914e611":"preds = l1_probs['se_resnext50_32x4d']\npreds = preds.astype('int')\n\ntest_df['diagnosis'] = preds\ntest_df.to_csv('submission.csv', index=False)","b3bc25fb":"!rm processed\/*","5016b88d":"# Training","57d702c9":"# Defination","4d5b23cc":"## Metrics","bc40d2e2":"# Pseudo Label ","9cbdad9a":"## SE-ResNeXt50_32x4d with Soft Label Learning","35abcf18":"## Dataset","0e1b8ad7":"## SE-ResNeXt50_32x4d","e57ff825":"## Train and Validation Function"}}