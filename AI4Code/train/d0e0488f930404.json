{"cell_type":{"f179cb02":"code","3baf7684":"code","f5f9b433":"code","cf44d8bc":"code","28edeab7":"code","ece2b6a5":"code","36f19eb5":"code","9e8c4981":"code","a7c9d19d":"code","24055c6f":"code","c38df89a":"code","f3ce3ba2":"code","4f142c0d":"code","88a46ae9":"code","9534a42f":"code","1d1606da":"code","6f06e945":"code","80ebe273":"code","7a66fbc3":"code","4c312583":"code","4643f949":"markdown","9336d4a7":"markdown","439bbd60":"markdown","a11ddc6a":"markdown","f52cd959":"markdown"},"source":{"f179cb02":"import numpy as np\nimport pandas as pd\nimport glob, gc\n\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\nimport os","3baf7684":"data_path = '..\/input\/optiver-realized-volatility-prediction\/'","f5f9b433":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain.head()","cf44d8bc":"def calculate_wap(df):\n    a1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    b1 = df['bid_size1'] + df['ask_size1']\n    a2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    b2 = df['bid_size2']+ df['ask_size2']\n    return (a1 + a2) \/ (b1 + b2), (a1 \/ b1 + a2 \/ b2) \/ 2\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))","28edeab7":"def rmspe(predictions, targets):\n    return np.sqrt((((predictions - targets) \/ targets) ** 2).mean())","ece2b6a5":"def optimise_num(df : pd.DataFrame, stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(data_path + f'book_{dataType}.parquet\/stock_id={stock_id}\/')\n    book_train_subset.sort_values(by = ['time_id', 'seconds_in_bucket'])\n    book_train_subset['wap1'], book_train_subset['wap2'] = calculate_wap(book_train_subset)\n\n    book_train_subset['log_return1'] = (book_train_subset.groupby(by = ['time_id'])['wap1'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train_subset['log_return2'] = (book_train_subset.groupby(by = ['time_id'])['wap2'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    best_rmspe = np.inf\n    for sec in tqdm(np.arange(0, 600, 10), leave = False):\n        book_train = book_train_subset[book_train_subset['seconds_in_bucket'] >= sec]\n        stock_stat = pd.concat([\n            book_train.groupby(['time_id'])['log_return1'].agg(realized_volatility).rename('rv1_new'),\n            book_train.groupby(['time_id'])['log_return2'].agg(realized_volatility).rename('rv2_new'),\n            ], \n            axis = 1, \n        ).reset_index()\n        stock_stat['rv_new'] = (stock_stat['rv1_new'] + stock_stat['rv2_new']) \/ 2\n        stock_stat = stock_stat.merge(df.loc[df['stock_id'] == stock_id, ['time_id', 'target']], on = 'time_id', how = 'left')\n        rmspe_score = rmspe(stock_stat['rv_new'], stock_stat['target'])\n        if rmspe_score < best_rmspe:\n            best_rmspe = rmspe_score\n            best_sec = sec\n    print(stock_id, best_sec, best_rmspe)\n    return best_sec","36f19eb5":"def generate_stock_nums(df : pd.DataFrame, stock_ids : list, dataType = 'train', parallel = False):\n    if parallel:\n        nums = Parallel(n_jobs = -1)(\n            delayed(optimise_num)(df, stock_id, dataType) \n            for stock_id in tqdm(stock_ids, total = len(stock_ids))\n        )\n    else:\n        nums = []\n        for stock_id in tqdm(stock_ids, total = len(stock_ids)):\n            nums.append(optimise_num(df, stock_id, dataType))\n    stock_nums = dict(zip(stock_ids, nums))\n    return stock_nums","9e8c4981":"stock_secs = generate_stock_nums(df = train, stock_ids = train['stock_id'].unique(), dataType = 'train', parallel = True)","a7c9d19d":"print(stock_secs)","24055c6f":"x = gc.collect()","c38df89a":"def get_stock_stat(stock_id : int, dataType = 'train'):\n    \n    book_train_subset = pd.read_parquet(data_path + f'book_{dataType}.parquet\/stock_id={stock_id}\/')\n    book_train_subset.sort_values(by = ['time_id', 'seconds_in_bucket'])\n    book_train_subset['wap1'], book_train_subset['wap2'] = calculate_wap(book_train_subset)\n\n    book_train_subset['log_return1'] = (book_train_subset.groupby(by = ['time_id'])['wap1'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train_subset['log_return2'] = (book_train_subset.groupby(by = ['time_id'])['wap2'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    \n    book_train = book_train_subset[book_train_subset['seconds_in_bucket'] >= stock_secs[stock_id]]\n    stock_stat = pd.concat([\n        book_train.groupby(['time_id'])['log_return1'].agg(realized_volatility).rename('rv1_new'),\n        book_train.groupby(['time_id'])['log_return2'].agg(realized_volatility).rename('rv2_new'),\n        ], \n        axis = 1, \n    ).reset_index()\n    stock_stat['rv_new'] = (stock_stat['rv1_new'] + stock_stat['rv2_new']) \/ 2\n    stock_stat['stock_id'] = stock_id\n    return stock_stat[['stock_id', 'time_id', 'rv_new']]","f3ce3ba2":"def get_dataSet(stock_ids : list, dataType = 'train', parallel = False):\n    if parallel:\n        stock_stat = Parallel(n_jobs = -1)(\n            delayed(get_stock_stat)(stock_id, dataType) \n            for stock_id in tqdm(stock_ids, total = len(stock_ids))\n        )\n    else:\n        stock_stat = []\n        for stock_id in tqdm(stock_ids, total = len(stock_ids)):\n            stock_stat.append(get_stock_stat(stock_id, dataType))\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df","4f142c0d":"train_dataSet = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train', parallel = True)\ntrain_dataSet = pd.merge(train, train_dataSet, on = ['stock_id', 'time_id'], how = 'left')","88a46ae9":"print(rmspe(train_dataSet['rv_new'], train_dataSet['target']))","9534a42f":"del train_dataSet\nx = gc.collect()","1d1606da":"test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ntest.head()","6f06e945":"test_dataSet = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test', parallel = True)\ntest_dataSet = pd.merge(test, test_dataSet, on = ['stock_id', 'time_id'], how = 'left')","80ebe273":"x = gc.collect()","7a66fbc3":"sub = pd.DataFrame()\nsub[['row_id', 'target']] = test_dataSet[['row_id', 'rv_new']]\nsub.head()","4c312583":"sub.to_csv('submission.csv', index = False)","4643f949":"# Submit","9336d4a7":"# Optimise the Portion Number","439bbd60":"# Go Deeper and Deeper! [Optimise RV]\n\nHow far can we go without using any machine learning (ML) models and just try the realised volatility (RV) function? The answer is it is much deeper than you think.\n\nIn this notebook, I show how to optimise the number of seconds in bucket we should consider for calculating the RV value for each stock. The insight is that old data may provide less importance to the future RV value that can be ignored.\n\nYou may consider this as a feature into your model.\n\nReferences:\n\n[We need to go deeper - and validate!][1]\n\n[1]: https:\/\/www.kaggle.com\/konradb\/we-need-to-go-deeper-and-validate","a11ddc6a":"# Predict on the Test Set","f52cd959":"# Validate on the Train Set"}}