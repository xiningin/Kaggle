{"cell_type":{"12c65a05":"code","6d99de7f":"code","ac2fa54e":"code","69c6f377":"code","fc389839":"code","b8a57e8d":"code","af76bd42":"code","39fe3d6a":"code","446ff70c":"code","22599bdc":"code","aef4524e":"code","6cb87841":"code","c0803fc4":"code","10e54b9c":"code","fb80ff89":"code","fb2f98d2":"code","6b50eb9b":"code","308d48c0":"code","a94fa330":"code","9a80348b":"markdown","ead17707":"markdown","17a2f903":"markdown","f8c110c7":"markdown","cf8c0251":"markdown","99925f90":"markdown","84cba242":"markdown","e814ee6b":"markdown","49a9ed08":"markdown","62f5ae3f":"markdown","d7dbb0e8":"markdown","a34e93cc":"markdown","1b2e7adf":"markdown","cf8477a7":"markdown","9da88e71":"markdown","1f5e62fe":"markdown","eda0df5e":"markdown","915a6049":"markdown"},"source":{"12c65a05":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy.stats import zscore\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import pie\n%matplotlib inline\nimport time\nimport pandas_profiling\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import f_classif\nfrom xgboost import XGBClassifier\nfrom hyperopt import hp,Trials,fmin,tpe","6d99de7f":"df = pd.read_csv('..\/input\/globalterrorismdb_0718dist.csv',encoding='ISO-8859-1')\ndf.head()","ac2fa54e":"df.shape","69c6f377":"print(df.isnull().sum())","fc389839":"pandas_profiling.ProfileReport(df)","b8a57e8d":"# Dropping rows with more than 75% missing values\nrow = df.isnull().sum(axis=1)\njunk_25 = int(df.shape[1] * 0.75)\nrow = row[row > junk_25]\nindex_to_delete = row.index.tolist()\ndf = df.drop(index_to_delete, axis=0)\n\n\n\n# Finding columns with more than 25% missing\nnull_count_dict = df.isnull().sum().to_dict()\nforty_percent = int(df.shape[0] * .25)\ncolumn_to_delete = []\nfor key, value in null_count_dict.items():\n    if (value > forty_percent):\n        column_to_delete.append(key)\n\n# delete the column in column 40% missing\ndf = df.drop(column_to_delete, axis=1)\n\nprint(df.shape)\n\n\n\n#Dump csv \n# df.to_csv(\"..\/input\/new.csv\",index=False)\n","af76bd42":"# df = pd.read_csv(\"\/home\/user\/Downloads\/gtd\/new.csv\")\ny = df['gname']\nX = df.drop(['gname'], axis=1)\nprint(type(y))","39fe3d6a":"y.value_counts()","446ff70c":"class_count = df['gname'].value_counts()\nkeep_class = []\nfor i, v in class_count.items():\n    if (v > 500):\n        keep_class.append(i)\nframe = pd.DataFrame(columns=df.columns)\nfor i in keep_class:\n    frame = frame.append(df[df.gname == i])\n\ny = frame['gname']\nX = frame.drop(['gname'], axis=1)","22599bdc":"for i in X.columns:\n    if X[i].dtype == object:\n        X[i] = X[i].fillna(X[i].mode()[0])\n\n    else:\n        X[i] = X[i].fillna(X[i].mean())\n\n","aef4524e":"label = []\nfor i in X.columns:\n    if X[i].dtype == object:\n        l = LabelEncoder()\n        X[i] = l.fit_transform(X[i])\n        label.append(l)","6cb87841":"X.boxplot(figsize=(30,10))","c0803fc4":"def detect_n_treat_outliers(data_frame, y,space, treat=True):\n    print(\"==================================================\")\n    print(\"Outlier detection and treatment started ...\")\n    print(\"Space:\", space)\n    #     print(\"Label column name:\", data_frame.columns[-1])\n    # y = data_frame[data_frame.columns[-1]]\n    # data_frame.drop([data_frame.columns[-1]], axis=1, inplace=True)\n    X = data_frame\n\n    dtypesss = X.dtypes\n\n    #     print(\"Dtypes:;;\", dtypesss)\n\n    y_predicted = None\n    params = space['params']\n\n    if space['model'] == \"DBSCAN\":\n\n        db = DBSCAN(**params)\n        y_predicted = db.fit_predict(X)\n        y_predicted = list(map(lambda x: 1 if x < 0 else 0, y_predicted))\n\n    elif space['model'] == \"EllipticEnvelope\":\n        elliptic = EllipticEnvelope(**params)\n        for i in range(0, X.shape[0], 10000):\n            elliptic.fit(X[i:10000 + i], y[i:10000 + i])\n        y_predicted = elliptic.predict(X)\n        y_predicted = list(map(lambda x: 1 if x == -1 else 0, y_predicted))\n\n    elif space['model'] == \"IsolationForest\":\n        iso = IsolationForest(**params)\n        for i in range(0, X.shape[0], 10000):\n            iso.fit(X[i:10000 + i], y[i:10000 + i])\n        y_predicted = iso.predict(X)\n\n        y_predicted = list(map(lambda x: 1 if x == -1 else 0, y_predicted))\n\n    elif space['model'] == \"OneClassSVM\":\n        ocv = OneClassSVM(**params)\n        for i in range(0, X.shape[0], 10000):\n            ocv.fit(X[i:10000 + i], y[i:10000 + i])\n        y_predicted = ocv.predict(X)\n        y_predicted = list(map(lambda x: 1 if x == -1 else 0, y_predicted))\n\n    elif space['model'] == \"LocalOutlierFactor\":\n        lof = LocalOutlierFactor(**params)\n        for i in range(0, X.shape[0], 10000):\n            lof.fit(X[i:10000 + i], y[i:10000 + i])\n        y_predicted = lof._predict(X)\n        y_predicted = list(map(lambda x: 1 if x == -1 else 0, y_predicted))\n\n    elif space['model'] == \"zscore\":\n        threshold = params['threshold']\n        print(\"thold\", threshold)\n        score_frame = pd.DataFrame()\n        for i in X.columns:\n            score = zscore(X[i], axis=0, ddof=1)\n            score_frame[i] = score\n        score_frame = score_frame.abs()\n        predicted_outliers = []\n        for i in range(len(score_frame)):\n            if any(score_frame.iloc[i] > threshold):\n                predicted_outliers.append(1)\n            else:\n                predicted_outliers.append(0)\n        y_predicted = predicted_outliers\n\n    return y_predicted\n\n\ndef auto_detect_n_treat_outliers(df,y,hyperopt_trained_space, voting_percentage, treat=True):\n    # Voting starts\n    all_votes = [0] * df.shape[0]\n    all_votes = np.array(all_votes)\n\n    # iterating over each space and getting y_predicted\n    for i in hyperopt_trained_space:\n        y_predicted = detect_n_treat_outliers(df.copy(), y,i, treat=treat)\n        y_predicted = np.array(y_predicted)\n        all_votes += y_predicted\n\n    voting_criteria = voting_percentage * (len(hyperopt_trained_space) ) * 1.0\n    remove_rows = []\n    for i in range(len(all_votes)):\n        if all_votes[i] >= voting_criteria:\n            remove_rows.append(i)\n\n    final_remove_index = sorted(np.unique(remove_rows))\n    return final_remove_index\nhyperspace = [\n             {\"model\": \"IsolationForest\", \"params\": {\"n_estimators\": 100,\"contamination\":0.05,\"n_jobs\": -1}},\n             {\"model\": \"LocalOutlierFactor\", \"params\": {'n_neighbors': 50,\"contamination\":0.05,'novelty':True}},\n             {\"model\": \"DBSCAN\", \"params\": {'eps': 0.7}},\n             {\"model\": \"OneClassSVM\", \"params\": {'max_iter': 10}},\n             {\"model\": \"EllipticEnvelope\", \"params\": {'contamination':0.05}},\n             {\"model\": \"zscore\", \"params\": {'threshold':3 }}\n            ]\n\nfinal_remove_index = auto_detect_n_treat_outliers(X,y,hyperopt_trained_space=hyperspace,voting_percentage=0.83)\nX.drop(X.index[final_remove_index], axis=0, inplace=True)\ny.drop(y.index[final_remove_index], axis=0, inplace=True)","10e54b9c":"# Feature selection\nf = f_classif(X, y)\n\nclass_f = dict(zip(X.columns, f[0]))\nsorted_idx_class_f = sorted(class_f, key=(lambda key: class_f[key]), reverse=True)\n\n#Taking top 25 features contributing\nj = 25\nX = X[sorted_idx_class_f[0:j]]","fb80ff89":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)","fb2f98d2":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","6b50eb9b":"clf = RandomForestClassifier()\nfor i in range(0, X_train.shape[0], 10000):\n    clf.fit(X_train[i:10000 + i], y_train[i:10000 + i])\n    print(i)\npredicted = clf.predict(X_test)    ","308d48c0":"def model_evalueation(y_test, y_pred):\n    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n    f1score = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n    cf = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    return {\"accuracy\": accuracy, \"f1_score\": f1score, \"confusion_matrix\":cf}\nscore = model_evalueation(y_test,predicted)\nprint(score)","a94fa330":"space = hp.choice(\"algo\", [\n\n    {\"model\": \"RandomForestClassifier\",\n     \"params\": {\n         \"min_samples_split\": hp.choice(\"min_samples_split\", range(2, 10, 1)),\n         \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", range(1, 5, 1)),\n         \"n_jobs\": hp.choice(\"n_jobs\", [-1]),\n\n     }},\n\n])\n\n\ndef optimization(space):\n    params = space['params']\n    model = space['model']\n    print(params)\n    clf = RandomForestClassifier(**params)\n\n    for i in range(0, X_train.shape[0], 10000):\n        clf.fit(X_train[i:10000 + i], y_train[i:10000 + i])\n\n    predicted = clf.predict(X_test)\n    f1 = f1_score(y_test, predicted, average='macro')\n    print(f1)\n    loss = (1 - f1)\n    # print(loss)\n\n    print(\"==============================================================\")\n    return loss\n\n\ntrials = Trials()\n\nbest = fmin(optimization, space, algo=tpe.suggest, max_evals=100, trials=trials, verbose=False)\nmin_loss = min(trials.losses())\nprint(best)\nprint('Minimum loss:', min_loss)\nprint(\"Best f1\",1- min_loss)","9a80348b":"Loading csv after deleting missing values","ead17707":"Dealing with class Imbalance","17a2f903":"\nDropping Missing values in rows and columns","f8c110c7":"Feature scaling","cf8c0251":"Model Evalueation","99925f90":"Model fitting in batches for memory optimization","84cba242":"Still having missing values so treating them by mean and mode","e814ee6b":"outlier detection and removal","49a9ed08":"Encoding the categorical variable","62f5ae3f":"Feature selection based on scores","d7dbb0e8":"# Dataset :- Global Terrorism Database 1970-2017","a34e93cc":"Loss optimization using hyperopt","1b2e7adf":"Counting null values in each column","cf8477a7":"Splitting Dataset into train and test sets\n","9da88e71":"Importing Dataset","1f5e62fe":"Boxplot for contamination estimation","eda0df5e":"EDA using Pandas Profiling","915a6049":"All Imports"}}