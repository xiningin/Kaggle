{"cell_type":{"6cf9ace2":"code","ab545773":"code","447fe688":"code","3c4f4e12":"code","58dac638":"code","de1dc804":"code","9c58f2f9":"code","cabde9da":"code","665746e5":"code","78b8d8ac":"code","dc006179":"code","a8d832a9":"code","833e4590":"code","6b24e32d":"code","385a0172":"code","6cb171b8":"code","f4e974c7":"code","25b10c2c":"code","8e469bc7":"code","28596a29":"code","114deaa4":"code","0769d403":"code","6aa8f5df":"code","e524ceac":"code","c6049af6":"markdown","dd2c9b74":"markdown"},"source":{"6cf9ace2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import decomposition\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC, LinearSVC","ab545773":"titanic = pd.read_csv('..\/input\/titanic\/train.csv', sep=',')\ntitanictest = pd.read_csv('..\/input\/titanic\/test.csv', sep=',')\ntitanic.head(5)","447fe688":"# Dropping columns that don't matter.\ntitanic = titanic.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntitanictest = titanictest.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntitanic.isnull().sum()","3c4f4e12":"nage = titanic.Age.isnull()\ntitanic[nage]","58dac638":"print(titanictest.isnull().sum())\nprint(titanictest[titanictest['Fare'].isnull()])","de1dc804":"titanic.info()","9c58f2f9":"#Changing Pclass to categorical\/object instead of int\n#titanic['Pclass'] = titanic.Pclass.astype('object')\n#titanictest['Pclass'] = titanictest.Pclass.astype('object')","cabde9da":"\"\"\"I'm not sure whether I should be interpolating the test data on its own, or using the train data to help\ndefine the age of the test data. I would think it's the latter, similar to scalers\/transforming data.\"\"\"\n\ntitanic['Age'] = titanic['Age'].interpolate()\ntitanictest['Age'] = titanic['Age'].interpolate()\n\ntitanictest['Fare'] = titanic['Fare'].interpolate()","665746e5":"col = list(titanic.columns)\ncatv = []\nnumv = []\nnumvcol = []\n\nfor i in col:\n    if titanic.dtypes[i] == object:\n        catv.append(i)\n    elif str(titanic.dtypes[i]) in ['int64','float64']:\n            numv.append(i)\n            numvcol.append(i)\n            \nprint(f'The categorical variables are:\\t{catv}\\n')\nprint(f'The numeric variables are:\\t{numv}')\n\nfor i in catv:\n    print(f'{i} has {titanic[i].nunique()} unique values.')","78b8d8ac":"titanic = pd.get_dummies(titanic, columns=catv, prefix_sep='_', drop_first=True)\ntitanictest = pd.get_dummies(titanictest, columns=catv, prefix_sep='_', drop_first=True)","dc006179":"X = titanic.iloc[:,1:]\ny = titanic.iloc[:,0]\n\nX2 = titanictest\n\n#X_train, X_hold, y_train, y_hold = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n#X_valid, X_test, y_valid, y_test = train_test_split(X_hold, y_hold, test_size=0.5, random_state=1, stratify=y_hold)\n\nscaler = StandardScaler()\nscaler.fit(X)\nx_scaled = scaler.transform(X)\nx2_scaled = scaler.transform(X2)","a8d832a9":"print(X.shape)\nprint(X2.shape)","833e4590":"pca = decomposition.PCA(n_components=(len(X.columns)))\n#pca = decomposition.PCA(n_components=(3))\npca.fit(x_scaled)\nx_pca = pca.transform(x_scaled)\n\nx2_pca = pca.transform(x2_scaled)","6b24e32d":"x_pca.shape","385a0172":"print(X.shape)\nprint(X2.shape)","6cb171b8":"#labels=['PC' + str(i) for i in range(0,len(X.columns))]\nlabels=['PC' + str(i) for i in range(0,(x_pca.shape[1]))]\n\ntitanic_comp = pd.DataFrame(pca.components_,columns=X.columns,index=list(labels))\ncomponentsT = titanic_comp.sort_values(by =labels, axis=1,ascending=False).round(decimals=6).T\ncomponents = componentsT.reindex(componentsT.PC0.abs().sort_values(ascending=False).index)\ncomponents","f4e974c7":"sum(pca.explained_variance_ratio_)","25b10c2c":"cumVar = 0\nfor i in range(0,len(labels)):\n    var = pca.explained_variance_ratio_[i]\n    cumVar += var\n    print(f'PC{i:02} explains {100*var:.2f}% for a cumulative total of {100*cumVar:.2f}%')","8e469bc7":"x_pca = pd.DataFrame(x_pca)\nx2_pca = pd.DataFrame(x2_pca)\nx_pca","28596a29":"# There has to be a better way at getting the PCA into a DF. Maybe a function? Gonna think over ways to standardize and make this better.\n# This is good enough for now.\ndef PCADF(dataframe,y_value,PCsWanted):\n    headers = ['PC' + str(x) for x in range(PCsWanted)]\n    dataframe.columns = ['PC' + str(x) for x in range(len(dataframe.columns))]\n    data = [y_value, dataframe[headers]]\n    df_pca = pd.concat(data, axis=1)\n    return df_pca","114deaa4":"df_pc1 = PCADF(x_pca,y,1)\ndf_pc2 = PCADF(x_pca,y,2)\ndf_pc3 = PCADF(x_pca,y,3)\ndf_pc5 = PCADF(x_pca,y,5)\n\ndftest_pc1 = x2_pca.iloc[:,:1]\ndftest_pc2 = x2_pca.iloc[:,:2]\ndftest_pc3 = x2_pca.iloc[:,:3]\ndftest_pc5 = x2_pca.iloc[:,:5]","0769d403":"X_train = df_pc5.iloc[:,1:]\ny_train = df_pc5.iloc[:,0]\n\nX_test = dftest_pc5\n\nprint(X_train.head(5))\nprint(X_test.head(5))","6aa8f5df":"svc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint(f'Support Vector Classifier Training Accuracy: {svc.score(X_train, y_train):.2f}')","e524ceac":"titanicresults = pd.read_csv('..\/input\/titanic\/test.csv', sep=',', index_col='PassengerId')\ntitanicresults.insert(0,'Survived_Pred', y_pred)\ntitanicresults = titanicresults.iloc[:,:1]\ntitanicresults.to_csv('TitanicPredictions.csv')\ntitanicresults","c6049af6":"# Beginning PCA","dd2c9b74":"# Data Preprocessing\n### I'm going to do preprocessing on the train and test data at the same time while only showing the train data, so it might look a bit confusing. Any scalers and the like are fit to train but also run on test."}}