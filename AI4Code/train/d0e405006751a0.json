{"cell_type":{"f9c3b8a6":"code","db38b124":"code","11545fc2":"code","ca2b5247":"code","a9391128":"code","dba1e919":"code","bf2d4cfc":"code","6728afde":"code","e2f8cfd8":"code","0d2c9bf3":"code","12677e5b":"code","806539c6":"code","735e2a01":"code","fb09909b":"code","91fe9903":"code","670cd6c1":"code","696a7822":"code","af4cdbf2":"code","d287e74a":"code","a75d5efd":"code","e26d5601":"code","b4733210":"code","4a25de97":"code","e1dc13f1":"code","7d4f63ab":"code","aac579cf":"code","4fdca998":"code","0e1202b9":"code","0f6411c3":"code","f6f3afb0":"code","0021185e":"code","52a8c775":"code","68698684":"code","a7aa8069":"code","d29cf575":"code","9f064d13":"code","a6a3bcd8":"code","c82940cd":"code","c967638a":"code","afe1343d":"code","39e8ca0f":"code","8bdd0d8d":"code","840b15c4":"markdown","fb168f24":"markdown","005f10f7":"markdown","073bc3cf":"markdown","5a7bd115":"markdown","fb0dd40c":"markdown","e54ffdb1":"markdown","ab89cbfa":"markdown","2f1d3efb":"markdown","e8606eed":"markdown","cf73905d":"markdown","2adadbed":"markdown","cef26e23":"markdown","14169b0a":"markdown","d87b06ed":"markdown","390cd464":"markdown","bd790ee2":"markdown","8c83b739":"markdown","dab04d8c":"markdown","45aa1c56":"markdown","211b6d5d":"markdown","0f2f8a11":"markdown","fca2027e":"markdown","76323621":"markdown","edf8c5e3":"markdown","6bf5c4f5":"markdown","c5daed42":"markdown","4317918c":"markdown","7308927a":"markdown","9d788bc9":"markdown","fca81e4c":"markdown","3e84e182":"markdown","579b83b8":"markdown","a66f9d7c":"markdown","f773daa1":"markdown","00c4a1db":"markdown","2cda93b8":"markdown","5ce7fad0":"markdown","f91c3463":"markdown","f001b011":"markdown","56a35d4c":"markdown"},"source":{"f9c3b8a6":"##importing toolkit\n##basics\nimport pandas as pd\nimport numpy as np\n\n##ploting\nimport matplotlib as plt\n\n##balancing\/ spliting\nfrom imblearn.over_sampling import ADASYN\nfrom sklearn.model_selection import train_test_split\n\n##tunning & metrics\nfrom skopt import dummy_minimize\nfrom skopt import gp_minimize\nfrom sklearn import metrics\n\n##models\n#import sklearn as skit\n#from sklearn.svm import SVC\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n#from sklearn.naive_bayes import GaussianNB\nimport lightgbm as lgb\n\n","db38b124":"train_data = pd.read_csv(r'..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_data = pd.read_csv(r'..\/input\/santander-customer-transaction-prediction\/test.csv')","11545fc2":"train_data.shape, test_data.shape","ca2b5247":"print('train data info:')\ntrain_data.info() \nprint('test data info:')\ntest_data.info()","a9391128":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df.head(5)","dba1e919":"reduce_mem_usage(train_data)","bf2d4cfc":"train_data.head(10)","6728afde":"test_data.head(10)","e2f8cfd8":"train_data.describe()","0d2c9bf3":"test_data.describe()","12677e5b":"def missing_values(df):\n    total = df.isnull().sum()\n    percent = (df.isnull().sum()\/df.isnull().count()*100)\n    tt = pd.concat([total, percent], axis = 1, keys = ['total', 'percent'])\n    types = []\n    for i in df.columns:\n        dtype = str(df[i].dtype)\n        types.append(dtype)\n    tt['types'] = types\n    \n    if (df[i].isnull().sum() > 0):\n        print('There is missing data')\n        return (np.transpose(tt))\n    else:\n        print('There is no missing data')\n        return (np.transpose(tt))","806539c6":"missing_values(train_data)","735e2a01":"missing_values(test_data)","fb09909b":"train_data['target'].hist(grid = False, figsize = (15, 7), color = 'black')","91fe9903":"y = train_data.target\nX = train_data.drop(columns = ['target', 'ID_code'])\n\n# setting up testing and training sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify = y)\n\n#sm = ADASYN()\n#X_train, y_train = sm.fit_sample(X_train, y_train)","670cd6c1":"X.describe()","696a7822":"def train_model(params):\n    learning_rate = params[0]\n    num_leaves = params[1]\n    min_child_samples = params[2]\n    subsample = params[3]\n    colsample_bytree = params[4]\n    \n    min_data_in_leaf = params[5]\n    min_sum_hessian_in_leaf = params[6]\n    bagging_fraction = params[7]\n    bagging_freq = params[8]\n    feature_fraction = params[9]\n    lambda_l1 = params[10]\n    lambda_l2 = params[11]\n    min_gain_to_split = params[12]\n    max_depth = params[13]\n    verbosity = params[14]\n    max_bin = params[15]\n    \n    \n    print(params, '\\n')\n    \n    mdl = lgb.LGBMClassifier(learning_rate=learning_rate, num_leaves=num_leaves, min_child_samples=min_child_samples,\n                        subsample=subsample, colsample_bytree=colsample_bytree, min_data_in_leaf = min_data_in_leaf, \n                        min_sum_hessian_in_leaf = min_sum_hessian_in_leaf, bagging_fraction = bagging_fraction,\n                        bagging_freq = bagging_freq, feature_fraction = feature_fraction, lambda_l1 = lambda_l1, \n                        lambda_l2 = lambda_l2, min_gain_to_split = min_gain_to_split, max_depth = max_depth, \n                        verbosity = verbosity, max_bin = max_bin,\n                        subsample_freq=1, n_estimators=100, n_jobs = -1, \n    objective = 'binary',\n    metric = 'auc',\n    boosting = 'gbdt')\n    \n    \n    mdl.fit(X_train, y_train)\n    \n    p = mdl.predict_proba(X_val)[:,1]\n    \n    # Queremos minimizar o auc score\n    return -metrics.roc_auc_score(y_val, p)\n\n# Definindo nosso espa\u00e7o de busca rand\u00f4mica. N\u00e3o s\u00e3o tuplas, s\u00e3o ranges!\nspace = [(1e-3, 1e-1, 'log-uniform'), #learning rate\n         (20, 80), # num_leaves\n         (1, 100), # min_child_samples\n         (0.05, 1.0), # subsample\n         (0.1, 1.0), # colsample bytree\n         (10, 20), #min_data_in_leaf\n         (7, 14), #min_sum_hessian_in_leaf\n         (0.5, 1), #bagging_fraction\n         (1, 4), #bagging_freq\n         (0.5, 1), #feature_fraction\n         (0.3, 1.2), #lambda_l1\n         (0.15, 0.6), #lambda_l2\n         (0.01, 0.4), #min_gain_to_split\n         (-2, 2), #max_depth\n         (-2, 2), #verbosity\n         (255, 500) #max_bin\n        ] \n\nresultado = gp_minimize(train_model, space, verbose=1, n_calls=50, n_random_starts = 10, n_jobs = -1)","af4cdbf2":"resultado.x","d287e74a":"params = {\n    'learning_rate': 0.05,\n    'num_leaves': 70,\n    'min_child_samples': 1,\n    'subsample': 0.6279956574567085,\n    'colsample_bytree': 1.0,\n   \n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'is_unbalanced': True,\n\n    'min_data_in_leaf': 13,\n    'min_sum_hessian_in_leaf': 14,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 4,\n    'feature_fraction': 0.7123317972623503,\n    'lambda_l1': 0.37253228768374314,\n    'lambda_l2': 0.45639156898526567,\n    'min_gain_to_split': 0.1778400339886555,\n    'max_depth': 2,\n    'verbosity': 2,\n    'max_bin': 346\n}\n\nprint('Started training the model...')\n##traing the LGBM with 2000 iterations and an early stop if model won't improove in 200 iterations.\n\nmdl = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\n#mdl = lgb.LGBMClassifier(**params, n_jobs = -1)\nmdl.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n        verbose=1000, early_stopping_rounds= 200, eval_metric = 'roc_auc')\n#mdl.fit(X_train, y_train)","a75d5efd":"name = mdl.__class__.__name__\nprint(\"=\"*30)\nprint(name)\nprint('****Results****')\np = mdl.predict_proba(X_val)[:, 1]\n#print(\"Accuracy:\", metrics.accuracy_score(y_val, p))\n#print(\"Precision:\", metrics.precision_score(y_val, p))\n#print(\"Recall:\", metrics.recall_score(y_val, p))\nprint(\"AUC:\", metrics.roc_auc_score(y_val, p))","e26d5601":"#test_to_sub = test_data.drop(columns=['ID_code'])\n#print('test_to_sub ID Drop done.')\n#print('Starting ID Separation...')\n#ID = test_data['ID_code']\n#print('ID Separation Done.')\n#print('Started submission prediction...')\n#Submission = mdl.predict_proba(test_to_sub)[:, 1]\n#print('Prediction done.')\n#print('Creating the .csv file...')\n#CSV_to_Sub = pd.DataFrame({'ID_code': ID, 'target': Submission})\n#print('Wrigting the .csv file...')\n#CSV_to_Sub.to_csv(r'C:\\Users\\paulo\\Desktop\\customer_transaction_prediction\\SUB6.csv', index = False)\n#print(r'.csv file created at - C:\\Users\\paulo\\Desktop\\customer_transaction_prediction\\SUB6.csv')","b4733210":"f = train_data.columns.values[2:202]\nfor df in [test_data, train_data]:\n    df['sum'] = df[f].sum(axis=1)\n    df['min'] = df[f].min(axis=1)\n    df['max'] = df[f].max(axis=1)\n    df['mean'] = df[f].mean(axis=1)\n    df['std'] = df[f].std(axis=1)\n    df['skew'] = df[f].skew(axis=1)\n    df['kurt'] = df[f].kurtosis(axis=1)\n    df['med'] = df[f].median(axis=1)","4a25de97":"train_data[train_data.columns[202:]].head(5)","e1dc13f1":"test_data[test_data.columns[202:]].head(5)","7d4f63ab":"##balancing data with ADASYN technique\n# Separate input features and target\ny = train_data.target\nX = train_data.drop(columns = ['target', 'ID_code'])\n\n# setting up testing and training sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify = y)\n\n#sm = ADASYN()\n#X_train, y_train = sm.fit_sample(X_train, y_train)","aac579cf":"X_train.head(2)","4fdca998":"X_val.head(2)","0e1202b9":"def train_model(params):\n    learning_rate = params[0]\n    num_leaves = params[1]\n    min_child_samples = params[2]\n    subsample = params[3]\n    colsample_bytree = params[4]\n    \n    min_data_in_leaf = params[5]\n    min_sum_hessian_in_leaf = params[6]\n    bagging_fraction = params[7]\n    bagging_freq = params[8]\n    feature_fraction = params[9]\n    lambda_l1 = params[10]\n    lambda_l2 = params[11]\n    min_gain_to_split = params[12]\n    max_depth = params[13]\n    verbosity = params[14]\n    max_bin = params[15]\n    \n    \n    print(params, '\\n')\n    \n    mdl = lgb.LGBMClassifier(num_leaves=num_leaves, min_child_samples=min_child_samples,\n                        subsample=subsample, colsample_bytree=colsample_bytree, min_data_in_leaf = min_data_in_leaf, \n                        min_sum_hessian_in_leaf = min_sum_hessian_in_leaf, bagging_fraction = bagging_fraction,\n                        bagging_freq = bagging_freq, feature_fraction = feature_fraction, lambda_l1 = lambda_l1, \n                        lambda_l2 = lambda_l2, min_gain_to_split = min_gain_to_split, max_depth = max_depth, \n                        verbosity = verbosity, max_bin = max_bin,\n                        subsample_freq=1, n_estimators=100, learning_rate = learning_rate, n_jobs = -1, \n    objective = 'binary',\n    metric = 'auc',\n    boosting = 'gbdt')\n    \n    \n    mdl.fit(X_train, y_train)\n    \n    p = mdl.predict_proba(X_val)[:,1]\n    \n    # Queremos minimizar o auc score\n    return -metrics.roc_auc_score(y_val, p)\n\n# Definindo nosso espa\u00e7o de busca rand\u00f4mica. N\u00e3o s\u00e3o tuplas, s\u00e3o ranges!\nspace = [(1e-3, 1e-1, 'log-uniform'), #learning rate\n         (20, 80), # num_leaves\n         (1, 100), # min_child_samples\n         (0.05, 1.0), # subsample\n         (0.1, 1.0), # colsample bytree\n         (10, 20), #min_data_in_leaf\n         (7, 14), #min_sum_hessian_in_leaf\n         (0.5, 1), #bagging_fraction\n         (1, 4), #bagging_freq\n         (0.5, 1), #feature_fraction\n         (0.3, 1.2), #lambda_l1\n         (0.15, 0.6), #lambda_l2\n         (0.01, 0.4), #min_gain_to_split\n         (-2, 2), #max_depth\n         (-2, 2), #verbosity\n         (255, 500) #max_bin\n        ] \n\nresultado = gp_minimize(train_model, space, verbose=1, n_calls=100, n_random_starts = 10, n_jobs = -1)","0f6411c3":"resultado.x","f6f3afb0":"params = {\n    'learning_rate': 0.01,\n    'num_leaves': 80,\n    'min_child_samples': 1,\n    'subsample': 0.07014119574453273,\n    'colsample_bytree': 0.30442008827557787,\n   \n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting': 'gbdt',\n    'is_unbalanced': True,\n    'seed' : 0,\n\n    'min_data_in_leaf': 17,\n    'min_sum_hessian_in_leaf': 14,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 1,\n    'feature_fraction': 0.5,\n    'lambda_l1': 1.2,\n    'lambda_l2': 0.5493135486189045,\n    'min_gain_to_split': 0.4,\n    'max_depth': 2,\n    'verbosity': 2,\n    'max_bin': 500\n}\n\nprint('Started training the model...')\n##traing the LGBM with 2000 iterations and an early stop if model won't improove in 200 iterations.\n\nmdl = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\n#mdl = lgb.LGBMClassifier(**params, n_jobs = -1)\nmdl.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n        verbose=1000, early_stopping_rounds= 200, eval_metric = 'roc_auc')\n#mdl.fit(X_train, y_train)\n\n##changed seed = 0 \/\/ was none","0021185e":"name = mdl.__class__.__name__\nprint(\"=\"*30)\nprint(name)\nprint('****Results****')\np = mdl.predict_proba(X_val)[:, 1]\n#print(\"Accuracy:\", metrics.accuracy_score(y_val, p))\n#print(\"Precision:\", metrics.precision_score(y_val, p))\n#print(\"Recall:\", metrics.recall_score(y_val, p))\nprint(\"AUC:\", metrics.roc_auc_score(y_val, p))","52a8c775":"#test_to_sub = test_data.drop(columns=['ID_code'])\n#print('test_to_sub ID Drop done.')\n#print('Starting ID Separation...')\n#ID = test_data['ID_code']\n#print('ID Separation Done.')\n#print('Started submission prediction...')\n#Submission = mdl.predict_proba(test_to_sub)[:, 1]\n#print('Prediction done.')\n#print('Creating the .csv file...')\n#CSV_to_Sub = pd.DataFrame({'ID_code': ID, 'target': Submission})\n#print('Wrigting the .csv file...')\n#CSV_to_Sub.to_csv(r'C:\\Users\\paulo\\Desktop\\customer_transaction_prediction\\SUB_F_E2.csv', index = False)\n#print(r'.csv file created at - C:\\Users\\paulo\\Desktop\\customer_transaction_prediction\\SUB6_F_E2.csv')","68698684":"##balancing data with ADASYN technique\n# Separate input features and target\ny = train_data.target\nX = train_data.drop(columns = ['target', 'ID_code'])\n\n# setting up testing and training sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify = y)\n\nsm = ADASYN()\nX_train, y_train = sm.fit_sample(X_train, y_train)","a7aa8069":"params = {'num_leaves': 128,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","d29cf575":"model = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=1000, early_stopping_rounds=200)","9f064d13":"sample_y_train = y_train.sample(frac=0.2)\nsample_x_train = X_train.sample(frac=0.2)\nsample_x_val = X_val.sample(frac=0.2)\nsample_y_val = y_val.sample(frac=0.2)","a6a3bcd8":"X_train.shape, sample_x_train.shape","c82940cd":"X_train.describe()","c967638a":"sample_x_train.describe()","afe1343d":"sample_y_train.hist(grid = False, figsize = (15, 7), color = 'black')","39e8ca0f":"# defining a list with all models \nclassifiers = [\n    #KNeighborsClassifier(3, n_jobs = -1),\n    GaussianNB(),\n    lgb.LGBMClassifier(n_jobs = -1),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_jobs = -1),\n    GradientBoostingClassifier()]\n\n# rotina para instanciar, predizer e medir os rasultados de todos os modelos\nfor clf in classifiers:\n    # instanciando o modelo\n    clf.fit(sample_x_train, sample_y_train)\n    # armazenando o nome do modelo na vari\u00e1vel name\n    name = clf.__class__.__name__\n    # imprimindo o nome do modelo\n    print(\"=\"*30)\n    print(name)\n    # imprimindo os resultados do modelo\n    print('****Results****')\n    sample_y_pred = clf.predict(sample_x_val)\n    print(\"Accuracy:\", metrics.accuracy_score(sample_y_val, sample_y_pred))\n    print(\"Precision:\", metrics.precision_score(sample_y_val, sample_y_pred))\n    print(\"Recall:\", metrics.recall_score(sample_y_val, sample_y_pred))\n    print(\"ROC:\", metrics.roc_auc_score(sample_y_val, sample_y_pred))","8bdd0d8d":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")","840b15c4":"#### *Spliting data for modeling:*\n*note:* Looking at this distribution you could think of applyng an balancing technique (like ADASYN) to have the same amount of positive and negative values for modeling our classification. But this is not always the better option. In this case [I've tried both](#cellB), and impiricaly, the m\u00e9trics whore better without balancing. Since it is a Kaggle competition, I choose to stay unbalanced.","fb168f24":"**We did it.**\n\nOur memory usage has decreased in 72.8% and that is great because we can process a lot faster and cheaper.\n\nNow let's look at our data. In any project, I always like to acctually look at the data for at least once before sart the next steps of the process. When I do it, I'm basicly checking it's scale, variables and mostly, becoming more confortable by knowing the face of what I am dealing with.\n\n#### *Printing first 10 rows of Train DataFrame:*","005f10f7":"#### *Printing Model's Metrics:*","073bc3cf":"#### *Checking it's values:*","5a7bd115":"#### *Creating .csv file to submmit*","fb0dd40c":"<a id=\"cell5\"><\/a>\n#### *Submmiting to Keagle to see if there's real improvment:*","e54ffdb1":"<a id=\"cell1\"><\/a>\n# The Challenge\n\nSantander - *Customer Transaction Prediction* Is a **Santander Bank\u2019s** challenge hosted on **Kaggle** (really important Data Science online community). It has one binary objective: given a person's known data, is he or she going to make a specific transaction? (yes\/no). We need an algorithm to predict that. To create\/ train\/ test our algorithm we are provided 2 .csv files. They have enough entries, lots of variables and no labels. \n\n*The fun part:* there are no labels for the variables on the datasets, so we need to find other strategies than business logics to see which ones are relevant to the prediction or not. Which means going all in with math and model resources.  \n\n*The not so fun part:* by not knowing the labels of variables, we lose interpretability, and therefore we can\u2019t take any insight of the analyses.\n\n*The cool part:* lots of people have already done this challenge, so we will find no trouble looking for reference. And our job becomes to read, understand, and incorporate it with our personal knowlege added, focusing on making it better.\n","ab89cbfa":"#### *Finding the best params for our model with Baesyan Optimization:*\n*note:* first I've tried an [function](#cellC) that tests a number of selected models with their default hiperparams and print each metric, my idea was to tune the one that presents best raw metrics. But all of then was similarly bad, with not enought diferences to take as base for a decision. I choose to stay the LGBM because it's eficiency on processing large amounts of data, it's tree based learning, that discards the need of regularizing variable's scale, and too because most of the reference was using this model with some great results, wich means a big oportunity to dig into this one model and learn a lot. \n\nThe Baesyan Otimizator is an method to search for hiperparameters that uses Baesyan Probability logics to search the best optimal points. You set the params and it's search range of numbers. And define the number o iteractions and number of ramdom starts. It will run for days if you say so always storing the best iteraction that can be called using **your_iterations_variable.x**","2f1d3efb":"#### *Spliting for train \/ val Data with our new feature engeneered DataFrame:*","e8606eed":"# **Santander Customer Transaction Prediction**\n\n## ***Can You Identify Who Will Make a Transaction?***\n\n#### *A simple way of doing it* by: Paulo Mancini\n\n\n### index:\n*[The Challenge](#cell1)\n\n*[Loading and Exploring the Data](#cell2)\n\n*[Modeling \/ Submmiting](#cell3)\n\n*[Feature Engeneering](#cell4)\n\n*[Submmiting v2](#cell5)\n\n*[Historic](#cell6)\n\n*[Reference](#cell7)","cf73905d":"#### *Searching for new params for the new Data:*","2adadbed":"#### *Training the Algorithm with the params:*\nps: After the first round I've changed some params manually to try enhance model's performance after analyzing the metrics. The learning rate was changed from 0.1 to 0.05 and the max depth was changed from 0 to 2., the NUM_OF_LEAVES from 80 to 70. It has enhanced the AUC_SCORE in 0.02%","cef26e23":"#### *Check Samples:*","14169b0a":"<a id=\"cell2\"><\/a>\n# Loading and Exploring the Data\n\nAs said, we are provided 2 .csv files for this challenge. They\u2019re already divided into Train and Test files. The train.csv file is shaped (200k entries with 202 columns) and the test.csv is shaped (200k entries with 201 columns). The column that is missing in the test file is the 0\/1 for making or not making the transaction, that is our target. \n","d87b06ed":"#### *Training the model with our new params:*\n*note:* some params had been changed manually after first round to try improving the model. If it's value is diferent from the respective one on the list above, it's means it had worked.","390cd464":"As we can see, it has 200k entries, with 200 variables, plus target and ID in the train file and just ID on the test one. All the variables are in **float64** data format (wich is quite heavy to process). Let's try a method to see if the **float64** is really needed for the information we got and, if not, substitute it for the smaller size possible without lossing information. We want to do it beacause processing data is expensive (not so in this case) and takes time (especially when processing in home computers).\n\n#### *Creating a function to try reduce memory usage:*","bd790ee2":"#### *Loading and storing the .csv files on DataFrames:*","8c83b739":"#### *Creating a function to look for empty data entries that could crash the model:*","dab04d8c":"<a id=\"cell4\"><\/a>\n## Done. Now we have a model that is scoring .89722 on Keagle.\n### *let's improve it*\nTo do it I'm going to bid on even more fine tunning and basic feature engeneering. First step is to simply combine some features with basic math, than storing they as new columns in both train and test DataFrames.","45aa1c56":"#### *Checking targuet distribution:*","211b6d5d":"#### *Applying the function to our train DataSet:*","0f2f8a11":"#### *Printing first 10 rows of test DataFrame:*","fca2027e":"##","76323621":"#### *Printing model's metrics:*","edf8c5e3":"<a id=\"cellC\"><\/a>\n## Test diferent Classifiers function","6bf5c4f5":"#### *Get samples:*","c5daed42":"#### *Applying the function to our train DataFrame:*","4317918c":"#### *Printing some basic metrics of the train DataFrame:*","7308927a":"##","9d788bc9":"## The new score on Kagle is .89871%. ","fca81e4c":"#### *Printing some basic metrics of the test DataFrame:*","3e84e182":"<a id=\"cellB\"><\/a>\n## Training our model with ADASYN Balanced Data:\n*note:* the search for hiperparameters was done using the same technique and the only diference is balanced vs not balanced Data. As you can see the best validation AUCs in this case was **0.83** in compare to the unbalanced data that has reached **0.89**. ","579b83b8":"##","a66f9d7c":"#### *Testing diferent classifiers:*","f773daa1":"<a id=\"cell7\"><\/a>\n## reference\n##\n#### [https:\/\/www.kaggle.com\/artgor\/santander-eda-fe-fs-and-models]\n#### [https:\/\/towardsdatascience.com\/automated-feature-engineering-in-python-99baf11cc219]\n#### [https:\/\/towardsdatascience.com\/feature-engineering-techniques-in-python-97977ecaf6c8]\n#### [https:\/\/medium.com\/@aganirbanghosh007\/santander-customer-transaction-prediction-a-simple-machine-learning-solution-771613633843]\n#### [https:\/\/medium.com\/coinmonks\/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167]","00c4a1db":"#### *Creating our new features:*","2cda93b8":"<a id=\"cell6\"><\/a>\n# Historic","5ce7fad0":"<a id=\"cell3\"><\/a>\n# Modeling","f91c3463":"#### *Checking the shape and size info of the DataFrames:*","f001b011":"#### *Applying the function to our test DataSet:*","56a35d4c":"##"}}