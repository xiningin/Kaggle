{"cell_type":{"31730957":"code","5b74114a":"code","5ce93be9":"code","214cfafe":"code","88b1ec2b":"code","986ff9b5":"code","2631e864":"markdown","c8cef535":"markdown","3ef7d8dd":"markdown","abbe83c1":"markdown","ceeac192":"markdown","7f47f1f7":"markdown"},"source":{"31730957":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nclass SingleLayerPerceptron:\n    def __init__(self, my_weights, my_bias, learningRate=0.05):\n        self.weights = my_weights\n        self.bias = my_bias\n        self.learningRate = learningRate\n        \n    def activation(self, net):\n        answer = 1 if net > 0 else 0\n        return answer\n    \n    def neuron(self, inputs):\n        neuronArchitecture = np.dot(self.weights, inputs) + self.bias\n        return neuronArchitecture\n    \n    def neuron_propagate(self, inputs):\n        processing = self.neuron(inputs)\n        return self.activation(processing) \n    \n    def training(self, inputs, output):\n        output_prev = self.neuron_propagate(inputs)\n        self.weights = [W + X * self.learningRate * (output - output_prev)\n                       for (W, X) in zip(self.weights, inputs)]\n        self.bias += self.learningRate * (output - output_prev)\n        error_calculation = np.abs(output_prev - output)\n        return error_calculation","5b74114a":"data = pd.DataFrame(columns=('x1', 'x2'), data=np.random.uniform(size=(600,2)))\ndata.head()","5ce93be9":"def show_dataset(data, ax):\n    data[data.y==1].plot(kind='scatter', ax=ax, x='x1', y='x2', color='blue')\n    data[data.y==0].plot(kind='scatter', ax=ax, x='x1', y='x2', color='red')\n    plt.grid()\n    plt.title(' My Dataset')\n    ax.set_xlim(-0.1,1.1)\n    ax.set_ylim(-0.1,1.1)\n    \ndef testing(inputs):\n    answer = int(np.sum(inputs) > 1)\n    return answer\n\ndata['y'] = data.apply(testing, axis=1)","214cfafe":"fig = plt.figure(figsize=(10,10))\nshow_dataset(data, fig.gca())","88b1ec2b":"InitialWeights = [0.1, 0.1]\nInitialBias = 0.01\nLearningRate = 0.1\nSLperceptron = SingleLayerPerceptron(InitialWeights, \n                                     InitialBias,\n                                     LearningRate)","986ff9b5":"import random, itertools\n\ndef showAll(perceptron, data, threshold, ax=None):\n    if ax==None:\n        fig = plt.figure(figsize=(5,4))\n        ax = fig.gca()\n        \n    show_dataset(data, ax)\n    show_threshold(perceptron, ax)\n    title = 'training={}'.format(threshold + 1)\n    ax.set_title(title)\n    \ndef trainingData(SinglePerceptron, inputs):\n    count = 0 \n    for i, line in inputs.iterrows():\n        count = count + SinglePerceptron.training(line[0:2], \n                                                  line[2])\n    \n    return count\n\ndef limit(neuron, inputs):\n    weights_0 = neuron.weights[0]\n    weights_1 = neuron.weights[1]\n    bias = neuron.bias\n    threshold = -weights_0 * inputs - bias\n    threshold = threshold \/ weights_1\n    return threshold\n\ndef show_threshold(SinglePerceptron, ax):\n    xlim = plt.gca().get_xlim()\n    ylim = plt.gca().get_ylim()\n    \n    x2 = [limit(SinglePerceptron, x1) for x1 in xlim]\n    \n    ax.plot(xlim, x2, color=\"yellow\")\n    ax.set_xlim(-0.1,1.1)\n    ax.set_ylim(-0.1,1.1)\n\nf, axarr = plt.subplots(3, 4, sharex=True, sharey=True, figsize=(12,12))\naxs = list(itertools.chain.from_iterable(axarr))\nuntil = 12\nfor interaction in range(until):\n    showAll(SLperceptron, data, interaction, ax=axs[interaction])\n    trainingData(SLperceptron, data)","2631e864":"## The SingleLayer Perceptron Learning\nLearning goes by calculating the prediction of the perceptron:\n\n### Basic Neuron \n$$\n\\hat{y} = f\\left(\\vec{w}\\cdot\\vec{x} + b) = f( w_{1}x_{1} + w_2x_{2} + \\cdots + w_nx_{n}+b\\right)\\,\n$$\n\nAfter that, we update the weights and the bias using as:\n\n$$\n\\hat{w_i} = w_i + \\alpha (y - \\hat{y}) x_{i} \\,,\\  i=1,\\ldots,n\\,;\\\\\n$$\n$$\n\\hat{b} = b + \\alpha (y - \\hat{y})\\,.\n$$","c8cef535":"Example using Multilayer Perceptron (no libraries): https:\/\/www.kaggle.com\/vitorgamalemos\/iris-flower-using-multilayer-perceptron\/","3ef7d8dd":"## Biologic Model\n\n<img src=\"https:\/\/www.neuroskills.com\/images\/photo-500x500-neuron.png\">\n<p style=\"text-align: justify;\">Artificial neurons are designed to mimic aspects of their biological counterparts. The neuron is one of the fundamental units that make up the entire brain structure of the central nervous system; such cells are responsible for transmitting information through the electrical potential difference in their membrane. In this context, a biological neuron can be divided as follows.<\/p>\n\n**Dendrites** \u2013 are thin branches located in the nerve cell. These cells act on receiving nerve input from other parts of our body.\n\n**Soma** \u2013 acts as a summation function. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites they are added together.\n\n**Axon** \u2013 gets its signal from the summation behavior which occurs inside the soma. It is formed by a single extended filament located throughout the neuron. The axon is responsible for sending nerve impulses to the external environment of a cell.","abbe83c1":"## Artificial Neuron as Mathematic Notation\nIn general terms, an input X is multiplied by a weight W and added a bias b producing the net activation. \n<img style=\"max-width:60%;max-height:60%;\" src=\"https:\/\/miro.medium.com\/max\/1290\/1*-JtN9TWuoZMz7z9QKbT85A.png\">\n\nWe can summarize an artificial neuron with the following mathematical expression:\n$$\n\\hat{y} = f\\left(\\text{net}\\right)= f\\left(\\vec{w}\\cdot\\vec{x}+b\\right) = f\\left(\\sum_{i=1}^{n}{w_i x_i + b}\\right)\n$$","ceeac192":"## The SigleLayer Perceptron\n\n<p style=\"text-align: justify;\">The Perceptron and its learning algorithm pioneered the research in neurocomputing. the perceptron is an algorithm for supervised learning of binary classifiers [1]. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.<p>\n    \n<img src=\"https:\/\/www.edureka.co\/blog\/wp-content\/uploads\/2017\/12\/Perceptron-Learning-Algorithm_03.gif\">\n    \n#### References\n    \n- Freund, Y.; Schapire, R. E. (1999). \"Large margin classification using the perceptron algorithm\" (PDF). Machine Learning\n\n- Aizerman, M. A.; Braverman, E. M.; Rozonoer, L. I. (1964). \"Theoretical foundations of the potential function method in pattern recognition learning\". Automation and Remote Control. 25: 821\u2013837.\n \n- Mohri, Mehryar and Rostamizadeh, Afshin (2013). Perceptron Mistake Bounds.","7f47f1f7":"# Artificial Neural Networks \n\n## About this notebook\n\nThis notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n\nContact me here: https:\/\/www.linkedin.com\/in\/vitorgamalemos\/\n\n## Introduction \n\n<img src=\"https:\/\/media.springernature.com\/original\/springer-static\/image\/art%3A10.1007%2Fs40846-016-0191-3\/MediaObjects\/40846_2016_191_Fig1_HTML.gif\">\n\n<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes.<\/p>"}}