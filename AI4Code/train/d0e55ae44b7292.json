{"cell_type":{"c62b8672":"code","7ae133ce":"code","cb481916":"code","29524db6":"code","3082c762":"code","4c5b61c6":"code","2cb633e2":"code","c6840bb8":"code","7871d1a5":"code","972bc61e":"code","01156f39":"code","3cb4024b":"code","a3b14ec1":"code","b5089fff":"code","b9ed5cb7":"code","b8bad8f1":"markdown","180fbea5":"markdown","755ad9ce":"markdown","1f0c46cd":"markdown","aab83f84":"markdown"},"source":{"c62b8672":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","7ae133ce":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","cb481916":"def build_model(transformer, max_len=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy')\n    \n    return model","29524db6":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","3082c762":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","4c5b61c6":"\n\n# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer\n\n","2cb633e2":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nPATH = '..\/input\/spooky-author-identification'\ntrain = pd.read_csv(f'{PATH}\/train.zip')\ntest = pd.read_csv(f'{PATH}\/test.zip')\nsample = pd.read_csv(f'{PATH}\/sample_submission.zip')\n\n\n#data preprocssing\nencoder = preprocessing.LabelEncoder()\ny = encoder.fit_transform(train[\"author\"].values)\n\n#data split\nx_train, x_valid, y_train, y_valid = train_test_split(train.text.values,y,random_state=42,test_size=0.1,shuffle=True)","c6840bb8":"X_train = fast_encode(x_train.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nX_valid = fast_encode(x_valid.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nX_test = fast_encode(test.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)","7871d1a5":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)","972bc61e":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","01156f39":"n_steps = X_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","3cb4024b":"# sub['toxic'] = model.predict(test_dataset,verbose=1)","a3b14ec1":"sub = model.predict(test_dataset,verbose=1)","b5089fff":"sample.iloc[:,-3:] = sub ","b9ed5cb7":"sample.to_csv(\"submission.csv\",index=False)","b8bad8f1":"# Encoder-Decoder and Attention\n\nEncoder-Decoder are an amazing method of sequence modeling. When I read about it in various places, I was enthalled by<br\/>\nHow amazing this things works. \n\n**NOTE**: My way of explaining encoder-decoder is slight different from what I have read, so you may castigate me for my weired explanation<br\/>\n\n**Encoder-Decoder**.\nLet's say encoder and decoder are persons we will call them Dalinar and Kaladin.<br\/>\nDalinar lives in a place named Alethkar and Kaladin lives in a place named Hearthstone and they are traders.<br\/>\nBoth the places speaks different language and so people of both the places used to face problem of language while trading<br\/>.\nNow as Dalinar and Kaladin known each other through trading, they decided to solve this problem.<br\/>\nThe idea was that each individual will create drawings of various processes of trading.<br\/>\nand will write down the basic sentence used for that process in both the languages and will create a book<br\/>\nso everyone can learn form it. So that's the story of encoder and decoder.\n\nAs Dalinar and Kaladin used image as their intermediate language, encoder-decoder uses NN hidden layers as their<br\/>\nintermediate language.\n\nBut Encoder-Decoder are no real person so, how do they learn an intermediate language ?<br\/>\n\nDo you remember word embeddings are gist of whole word in vector form.<br\/>\n\nSimilarly Enocder-Decoder are RNNs in which  **encoder** figures out gist of the Sequence<br\/>\nand sends it to **Decoder** to convert that into desired Sequence.\n\nNow to more serious explanation.<br\/>\n\nLet's understand what trained encoder-decoder do.<br\/>\n\nSuppose we want to translate english sentence \"Hey, How are you?\" to french \"Salut, comment vas tu\" <br\/>\nEncoder's RNN will take each word one by one in vector form and use embeddings to convert it to word embedding, and <br\/>\ncreate an hidden state pass it as input along with next word to next RNN iteration.<br\/>\n\nIt will continue this for last word \"you\" so we will get total four hidden state for each iteration<br\/>.\nAa RNN's are passed previous state, last state is dependent on all previous state indirectly, so last hidden state.<br\/>\nacts like gist of whole sentence.<br\/>\n\nAnd so last hidden state is passed to decoder which is also an RNN which will Generate a words step by step in French language.\n\n**Attention Models**\n\nWhat are attention models and how are they differnt from encoder-decoder ?<br\/>\n\nWell as we saw in encoder-decoder we only passed last hidden state in encoder to decoder.<br\/>\nIn Attention models each hidden state of encoder is passed to decoder.\n\nNow what decoder will do is it has **weight** attach to each of the **hidden-state** passed to it by encoder.<br\/>\n\nSo it will multipy each **hidden-state** by those **weights** to each hidden state, calculate **softmax** for each them, so it will highlight.<br\/>\nmost important **hidden-state** of them all and then each **softmax value** is multiplied again to **orignal-state**<br\/>\nand then all of them are added to get single **hidden-state** which is used to output single word in sequence.<br\/>\n\nsteps perfomed by decoder\n\n1. w1 x h1, w2 x h2, w3 x h3 <br\/>\n2. s1 = Softmax(w1xh1),s2 = Softmax(w2xh3),s3 = Softmax(w3xh3)<br\/>\n3. H1 = s1 x h1, H2 = s2 x h2, H3= s3 x h3<br\/>\n4. H_final = (H1 + H2 + H3)\n\nThis H_final will be used to generate one word.<br\/>\nNow there will be other set of weights which will be connected to this hidden states for gererating next word.\n\nYou might have got the idea that what Attention model basically does is while outputting each word in a sentence<br\/>\nIt will look at which part of the orignal sentence is important for the translation and which part is not.<br\/>\nSo Decoder will focus on the words in orignal sentence which are important to it.\n\nAttention model works better because it gets to see whole sentence as oppose to encoder-decoder where only.<br\/>\nlast hidden state was passed to it.\n\n**Note**: If you did't get what I explained here please read 1st article it has far better explanation.\n\n\nvideos\n\n1. [Attention in Neural Networks](https:\/\/www.youtube.com\/watch?v=W2rWgXJBZhU)\n\nArticles\n\n1. [Visualizing A Neural Machine Translation Model](https:\/\/jalammar.github.io\/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention\/)\n2. [Sequence Models](https:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/attention-model-intuition-RDXpX)\n3. [How Does Attention Work in Encoder-Decoder Recurrent Neural Networks](https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/)\n\n","180fbea5":"# BERT MODEL","755ad9ce":"# NLP Challange (part 3)\n\nI have challanged myself to learn NLP in one week and this is my third notebook for challange.\n\n[Here](https:\/\/www.kaggle.com\/maunish\/nlp-challenge-part-1?scriptVersionId=32391796) is first one.\n[Here](https:\/\/www.kaggle.com\/maunish\/nlp-challenge-part-2?scriptVersionId=32391910) is second one.\n\nIn first notebook we learned about topics like\n\n1. What is NLP?\n2. Tf (term frequency) and idf (inverse document frequency)\n3. CountVectorizer\n4. TfidfVectorizer and TfidfTransformer\n5. Training Logistic Regression, SVM, XGboost, Navie Bayes,\n6. GridSearch.\n\nIn second notebook we covered some advance NLP topics like\n\n1. What is Word vectors and word embeddings?\n2. Simple NN\n3. RNN\n4. LSTM\n5. GRU\n6. Bidirectional LSTM\n\nIn third Notebook we are going to learn about topics like.\n1. Attention models\n2. What is transformer\n3. BERT.","1f0c46cd":"# Sequence Modeling\n\nThere are basically various types of architecture in which RNN works.\n\n![image.png](attachment:image.png)\n\n**one to one**: one to one means one input is given and it produces one output<br\/>\nit is just work like basic Neural Network.\n\n**Many to One**: In many to one architecture RNN is provided series of inputs and at the end<br\/>\nof series it gives one output. This type of architecture could be used in sentiment analysis <br\/> \nwhere we have to predict negativity or positivity of sentence.\n\n**One to Many**: This type of architecture just take one input and based on that generates many output.<br\/>\nOne example on one to many is image captioning where we provide one single image and based on that.<br\/>\nIt generates a series of words.\n\n**Many to Many**: This type of architecture basically takes sequence and outputs sequence.<br\/>\nMostly this type of architecture is used in language translation.\n\n\nWatch the below video to understand differnt types of sequence models.<br\/>\n[Sequence Models](https:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/different-types-of-rnns-BO8PS)\n\nI am not writing code for sequence translation or generation.<br\/>\nBut various resources of implementation are available on internet.\n","aab83f84":"# Transformer and BERT\n\nAs explaining Tansromfer and BERT is far above my writting skills i will link the videos and articles below.\n\n\n### Transformer\n1. [The Illustrated Transformer](http:\/\/jalammar.github.io\/illustrated-transformer\/)\n\nLinks  to videos.\n1. [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https:\/\/www.youtube.com\/watch?v=TQQlZhbC5ps)\n\nOrignal paper \"Attention is all you need\"\n1. [Attention is all you need](https:\/\/arxiv.org\/pdf\/1706.03762v4.pdf)\n\n### BERT\n\nLink to articles\n1. [The Illustrated BERT, ELMo, and co. ](http:\/\/jalammar.github.io\/illustrated-bert\/)\n2. [A Visual Guide to Using BERT for the First Time](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/)\n\n"}}