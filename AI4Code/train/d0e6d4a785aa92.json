{"cell_type":{"0359fa08":"code","512e3c94":"code","c3d364a6":"code","38c8d7f4":"code","8ceeec21":"code","a2cdd27d":"code","6c429bf4":"code","fe0cbb19":"code","ecf42ec5":"code","f5934375":"code","ec965601":"code","2586a35c":"code","e021c0ac":"code","70f4bd8a":"code","8c3d4e15":"code","c9689c69":"code","73d7eb33":"code","2e97af3f":"code","108c2c52":"code","17c5720e":"code","4ca803de":"code","dd33565e":"code","dcd63eae":"code","95688cf4":"code","557e245e":"code","56fc4b60":"code","1ae4c0c5":"code","b6697e07":"code","a7731c17":"code","702d605d":"code","37fb9a52":"code","34af6797":"code","be406203":"code","0563d278":"code","427d13b2":"code","9dc0d093":"code","24629667":"code","2e4c7884":"code","5d9c6c22":"code","1fd4cbb9":"code","cfe31904":"code","b9044373":"code","738dd8c3":"code","f1cc7586":"code","87d1f0d7":"code","8af8606a":"code","112e4357":"code","1d3ff13e":"code","73dead7c":"code","c144543a":"code","d5e9d3e8":"code","7735eafb":"code","200a3326":"code","7751e6f1":"code","aa988a64":"code","add1af93":"code","0e8ee8e3":"code","c586ad49":"code","05052e82":"code","19ca403e":"code","30ceeb7e":"code","d5edd999":"code","219ff287":"code","f2a41b87":"code","0f6cf5d4":"code","13d28570":"code","9bc4952f":"code","d8a61791":"code","c6aaef08":"code","62f42d0f":"code","1ffa4c73":"code","ee9fa0ed":"markdown","78a47234":"markdown","88c4a885":"markdown","2c56bc55":"markdown","7df98d58":"markdown","c417d770":"markdown","07a5b967":"markdown","7ab04916":"markdown","f588ecb6":"markdown","9be0ef92":"markdown","58e244a5":"markdown","ce1fe41f":"markdown","539c5ad7":"markdown","1cb22e40":"markdown","b9963617":"markdown","acff64dd":"markdown","3adb2be8":"markdown","edef9c25":"markdown","6b9e4f71":"markdown","1c83004a":"markdown","287e1b64":"markdown","166e15ee":"markdown","02fdd6df":"markdown","e1019c25":"markdown","8755374c":"markdown","b61d4cd8":"markdown","64165686":"markdown","bd6515b2":"markdown","2fd2065b":"markdown","9df3302f":"markdown","6cfa1104":"markdown","de24024c":"markdown"},"source":{"0359fa08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","512e3c94":"import numpy as np\nimport pandas as pd\nimport datetime\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\n","c3d364a6":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.shape, test.shape","38c8d7f4":"train.head()","8ceeec21":"train.describe()","a2cdd27d":"train.columns","6c429bf4":"num_train = train.shape[0]  #store number of train and test examples\nnum_test = test.shape[0]\ny_train = train.SalePrice.values","fe0cbb19":"y_train.shape","ecf42ec5":"trainid = train['Id']\ntestid = test['Id']\n#since each id is unique for a rpw so it makes sense to drop it\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","f5934375":"dataa = pd.concat((train, test)).reset_index(drop = True)\n","ec965601":"dataa.shape","2586a35c":"dataa['SalePrice'].fillna(dataa['SalePrice'].mean(), inplace=True)\n","e021c0ac":"dataa['SalePrice'].isna().sum()","70f4bd8a":"dataa.shape","8c3d4e15":"sns.set_style(\"darkgrid\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(dataa['SalePrice'], color=\"PURPLE\");\nax.xaxis.grid(True)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n","c9689c69":"print(\"Skewness: %f\" % dataa['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % dataa['SalePrice'].kurt())\n","73d7eb33":"dataa['SalePrice'] = np.log(dataa['SalePrice'])\nsns.distplot(dataa['SalePrice'], fit=norm,  color=\"PURPLE\");\nfig = plt.figure()\n","2e97af3f":"print(\"Skewness: %f\" % dataa['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % dataa['SalePrice'].kurt())\n","108c2c52":"#Check the datatypes in the columns\nn=[]\nfor i in train.columns:\n    #print((i), type(train[i][0]))\n    n.append(type(train[i][0]))\nset(n)\n","17c5720e":"numtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_features = []\nfor i in dataa.columns:\n    if train[i].dtype in numtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric_features.append(i)  \nnumeric_features","4ca803de":"#specify the position of individual graphs\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2) #spacing between subplots\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 10)\n\nfor i, feature in enumerate(list(dataa[numeric_features]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric_features)), 4, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Reds', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","dd33565e":"print(numeric_features)","dcd63eae":"numeric_feats = dataa.dtypes[dataa.dtypes != \"object\"].index\nprint(numeric_feats)","95688cf4":"\nskewed_feats = dataa[(numeric_feats)].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","557e245e":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    dataa[feat] = boxcox1p(dataa[feat], lam)\n\ndataa[skewed_features] = np.log1p(dataa[skewed_features])","56fc4b60":"data=dataa.copy()","1ae4c0c5":"data.drop(['SalePrice'], axis = 1, inplace = True)\n","b6697e07":"def mvalues(tr):\n    sns.set_style(\"white\")\n    f, ax = plt.subplots(figsize=(8, 7))\n    sns.set_color_codes(palette='deep')\n    missing = (tr.isnull().sum())\n    missing = missing[missing > 0]\n    missing.sort_values(inplace=True)\n    missing.plot.bar(color=\"r\")\n    ax.xaxis.grid(False)\n    ax.set(ylabel=\"Number of missing values\")\n    ax.set(xlabel=\"Features\")\n    ax.set(title=\"Number of missing data\")\n\n    ","a7731c17":" mvalues(data)","702d605d":"data.head()","37fb9a52":"#train[\"PoolQC\"] = train[\"PoolQC\"].fillna(\"None\")\n\n#train[\"MiscFeature\"] = train[\"MiscFeature\"].fillna(\"None\")\n\n#train[\"Alley\"] = train[\"Alley\"].fillna(\"None\")\n\n#train[\"Fence\"] = train[\"Fence\"].fillna(\"None\")\n\ndata[\"FireplaceQu\"] = data[\"FireplaceQu\"].fillna(\"None\")\n\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')\n    \ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\n\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n\ndata = data.drop(['Utilities'], axis=1)\n\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\n\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\n\ndata['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","34af6797":"data['MiscFeature']=data['MiscFeature'].fillna(\"None\")","be406203":"sum(data.isna().sum())","0563d278":"\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['OverallCond'] = data['OverallCond'].astype(str)\n\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","427d13b2":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(dataa[c].values)) \n    dataa[c] = lbl.transform(list(dataa[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(dataa.shape))","9dc0d093":"cor=dataa.corr()\nplt.subplots(figsize=(12,10))\nsns.heatmap(cor, vmax=0.9,  square=True)","24629667":"cor=train.corr()\ncols = cor.nlargest(10, 'SalePrice')['SalePrice'].index\nsp = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(sp, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2e4c7884":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(dataa[cols], size = 2.5)\nplt.show();","5d9c6c22":"data.columns","1fd4cbb9":"data.head()","cfe31904":"category=[]\nc= 'str'\nfor i in train.columns:\n    a=train[i][0]\n    if type(a) == type(c):\n        category.append(i)\n","b9044373":"print(category)  \n","738dd8c3":"data['Total_Bathrooms'] = (data['FullBath'] + (data['HalfBath']) +\n                               data['BsmtFullBath'] + (data['BsmtHalfBath']))","f1cc7586":"# Adding total square footage feature \ndata['TotalSqFootage'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","87d1f0d7":"sum(data.isna().sum())","8af8606a":"data.head()","112e4357":"dt=data.copy()","1d3ff13e":"dt.head()","73dead7c":"dt = pd.get_dummies(dt) #.reset_index(drop=True)\ndt.shape","c144543a":"dt.head()","d5e9d3e8":"(dt.isna().sum())","7735eafb":"trainn = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrainn[\"SalePrice\"] = np.log1p(trainn[\"SalePrice\"])\n#Check the new distribution \nsns.distplot(trainn['SalePrice'] , fit=norm);\ny_train = trainn.SalePrice.values","200a3326":"sns.distplot(y_train , fit=norm);","7751e6f1":"y_train","aa988a64":"train = dt[:num_train] #here num_train is the number of columns of training data\ntest = dt[num_train:]","add1af93":"train.head()","0e8ee8e3":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","c586ad49":"#Define cross validation strategy\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","05052e82":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","19ca403e":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n","30ceeb7e":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","d5edd999":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","219ff287":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","f2a41b87":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","0f6cf5d4":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","13d28570":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","9bc4952f":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","d8a61791":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","c6aaef08":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","62f42d0f":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","1ffa4c73":"sub = pd.DataFrame()\nsub['Id'] = testid\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","ee9fa0ed":"### Scatterplot of Top Numerical Features","78a47234":"# Dealing with missing values","88c4a885":"**Removing skewness of all numerical features**","2c56bc55":" Drop the target column","7df98d58":"#Since ID is unique for each row so we drop it\ndata=data.drop(columns=['Id'])","c417d770":"data.drop(columns=['PoolQC', 'MiscFeature', 'Fence', 'Alley' ], inplace=True)","07a5b967":"**Dropping highest missing data columns**","7ab04916":"Ensembling StackedRegressor, XGBoost and LightGBM\nWe add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","f588ecb6":"Reference: \n1.  pipeline : https:\/\/rk1993.tistory.com\/entry\/Python-sklearnpipeline-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8Pipeline%EC%9D%B4%EB%9E%80\nsklearn.base : https:\/\/suuntree.tistory.com\/307","9be0ef92":"**Stacking models** Ref: https:\/\/zhuanlan.zhihu.com\/p\/67711746","58e244a5":"**Stacking models**\n\n**Simplest Stacking approach : Averaging base models**\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n**Averaged base models class**","ce1fe41f":"**Transform some variables into their correct type**","539c5ad7":"#### Top 10 Highly Correlated Features with SalesPrice","1cb22e40":"### Concantenate the train and tests","b9963617":"Stacking Averaged models Score\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","acff64dd":"## Data Preprocessing\n","3adb2be8":"Baseline Model","edef9c25":"Top correlated features from the heatmap are: YearBuilt, TotalBsmtSF, 1stFlrSF, YearBuilt, and Garage Variables.\nNow calculate correlation with SalesPrice","6b9e4f71":"### Now all the missing values will be handled according to their type","1c83004a":"Final Training and Prediction","287e1b64":"LABEL ENCODE Reference : https:\/\/2-chae.github.io\/category\/1.ai\/30","166e15ee":"### For the purpose of visualization, we will use Saleprice for some time but later drop it for prediction","02fdd6df":"**Less simple Stacking : Adding a Meta-model\n**\nIn this approach, we add a meta-model(model6) on averaged base models(model1-model5) and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\n1. Split the total training set into two disjoint sets (here train and .holdout )![download.png](attachment:download.png)\n\n2. Train several base models on the first part (train)\n\n3. Test these base models on the second part (holdout)\n\n4. Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.\n\n\nRef: https:\/\/zhuanlan.zhihu.com\/p\/67711746","e1019c25":"### Plot Correlation Matrix ","8755374c":"**Creating New Features**","b61d4cd8":"Averaged base models score\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","64165686":"#### Pairplots of SalesPrice with Highly Correlated Features\n","bd6515b2":"## Visualizing the Numeric features by a Scatterplot","2fd2065b":"**Checking for null values**","9df3302f":"**Encode the Category Variables**","6cfa1104":"### Now apply log transformation to get a normal distribution","de24024c":"**Stacking averaged Models Class**"}}