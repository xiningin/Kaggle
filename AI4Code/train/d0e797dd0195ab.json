{"cell_type":{"81a42c96":"code","7185540c":"code","53dc6eb2":"code","48f34cd8":"code","800cb9ca":"code","6197d81e":"markdown","b571212b":"markdown","8246e29f":"markdown"},"source":{"81a42c96":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport base64\nimport io\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport time\nimport IPython\nimport PIL\n\nfrom kaggle_secrets import UserSecretsClient\n\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Device:\", tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)\nprint(tf.__version__)","7185540c":"user_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","53dc6eb2":"DS_PATH = \"..\/input\/wikipedia-train-0\"\n\nDESC_COLUMN = \"caption_title_and_reference_description\"\nIMG_COLUMN = \"b64_bytes\"\nFEAT_COLUMN = \"wit_features\"\nURL_COLUMN = \"image_url\"\n\nfilenames = sorted(os.listdir(DS_PATH))\njson_content = []\n\nstart_time = time.time()\nstep_time = start_time\nfor file in filenames:\n    filename = os.path.join(DS_PATH, file)\n    with open(filename, \"rb\") as fr:\n        for line in fr:\n            if line:\n                obj = json.loads(line)                \n                content = {}\n                content[URL_COLUMN] = obj[URL_COLUMN]\n                content[DESC_COLUMN] = []\n                content[IMG_COLUMN] = obj[IMG_COLUMN]\n                \n                for element in obj[FEAT_COLUMN]:\n                    if element.get(DESC_COLUMN):\n                        content[DESC_COLUMN].append(element[DESC_COLUMN])\n                \n                # Only keep content if both description and image are not empty\n                if content[URL_COLUMN] != \"\" and content[IMG_COLUMN] != \"\" and len(content[DESC_COLUMN]) > 0:\n                    json_content.append(content)\n    cur_time = time.time()\n    print(f\"Import took {(cur_time - step_time):.3f} seconds from file: {file}\")\n    step_time = cur_time\n\nend_time = time.time()\nprint(f\"Time to read {len(filenames)} files: {(end_time - start_time):.3f} seconds\")","48f34cd8":"print(json_content[0].keys())\nprint(f\"Total items: {len(json_content)}\")","800cb9ca":"def display_from_json(content):\n    decoded = base64.b64decode(content[IMG_COLUMN])\n    image = PIL.Image.open(io.BytesIO(decoded)).convert(\"RGB\")\n    print(f\"\\nImage URL: {content[URL_COLUMN]}\")\n    print(f\"Description: {content[DESC_COLUMN]}\\n\")\n    IPython.display.display(image)\n\n    \nfor _ in range(3):\n    rand_index = np.random.randint(0, len(json_content))\n    display_from_json(json_content[rand_index])","6197d81e":"### Import data\nData consists of first 10 joined datasets (00000-00009) from the archive here: https:\/\/analytics.wikimedia.org\/published\/datasets\/one-off\/caption_competition\/training\/joined\/","b571212b":"# Wikipedia Image-Caption Competition\n\n### Import libraries","8246e29f":"### Visualize data"}}