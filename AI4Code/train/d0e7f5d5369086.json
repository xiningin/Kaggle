{"cell_type":{"3ad7a5d5":"code","3ee74bf3":"code","6bedda5e":"code","66bc5a43":"code","1c4e3e4f":"code","ed2944df":"code","636b2d4d":"code","a74908cf":"code","9ce9c99b":"code","e3f2b065":"code","08de959e":"code","e5233230":"code","03d42dc3":"code","b91cc28a":"code","d79deb07":"code","de76aef1":"code","25b73f67":"code","979aec1a":"code","61150fbe":"code","8605a23e":"code","9e638cca":"code","504f490e":"code","f66b9540":"code","17870282":"code","346ac2ed":"code","e3950cab":"code","1b1f40f6":"code","4f90fd32":"code","24e59401":"code","81f44390":"code","46ed3468":"code","910f1fce":"code","bc46604b":"markdown","187a973e":"markdown","9715db7c":"markdown","e65ca720":"markdown","f6bab754":"markdown","b9cb6bbf":"markdown","7746bc1b":"markdown","28d992ce":"markdown","05b6d859":"markdown","01aec359":"markdown","01277a2a":"markdown","ec0f2f8e":"markdown","8a0c4566":"markdown","36b166f7":"markdown","d2008cd7":"markdown","755d887b":"markdown","9daa0464":"markdown","efd7192b":"markdown","89925dc5":"markdown","85f2c01b":"markdown"},"source":{"3ad7a5d5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n","3ee74bf3":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain.head()","6bedda5e":"print('There are {} rows and {} columns in the train data'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in the test data'.format(test.shape[0],test.shape[1]))","66bc5a43":"print('Number of real disaster tweets {} , {} %'.format(train[train.target==1].shape[0],train[train.target==1].shape[0]\/train.shape[0] *100))\nprint('Number of fake disaster tweets {} , {} %'.format(train[train.target==0].shape[0],train[train.target==0].shape[0]\/train.shape[0] *100))","1c4e3e4f":"tweet_len_disaster     =train[train['target']==1]['text'].str.len()\ntweet_len_non_disaster =train[train['target']==0]['text'].str.len()","ed2944df":"fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster',\"Non-disaster\"))\n\ntrace0= go.Histogram(\n    \n    x=tweet_len_disaster,\n    name=\"Disaster\",\n    opacity=0.75\n)\n\ntrace1= go.Histogram(\n    \n    x=tweet_len_non_disaster,\n    name=\"Non-Disaster\",\n    opacity=0.75\n)\n\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\n\nfig.update_layout(template=\"plotly_dark\",title_text='<b>Distribution of length of characters in tweets<\/b>',font=dict(family=\"Arial,Balto,Courier new,Droid sans\",color='white'))\nfig.show()","636b2d4d":"tweet_len_1=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\ntweet_len_0=train[train['target']==0]['text'].str.split().map(lambda x: len(x))","a74908cf":"fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster',\"Non-disaster\"))\n\ntrace0= go.Histogram(\n    \n    x=tweet_len_1,\n    name=\"Disaster\",\n    opacity=0.75\n)\n\ntrace1= go.Histogram(\n    \n    x=tweet_len_0,\n    name=\"Non-Disaster\",\n    opacity=0.75\n)\n\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\n\nfig.update_layout(template=\"plotly_dark\",title_text='<b>Distribution of length of words in tweets<\/b>',font=dict(family=\"Arial,Balto,Courier new,Droid sans\",color='white'))\nfig.show()","9ce9c99b":"word_1=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nword_0=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])","e3f2b065":"word_1=word_1.map(lambda x: np.mean(x))\nword_0=word_0.map(lambda x: np.mean(x))","08de959e":"hist_data = [word_1,word_0]\ngroup_labels = ['disaster','non-disaster']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\nfig.update_layout(title_text='Average word length in a tweet',width=900,height=450)\nfig.show()","e5233230":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","03d42dc3":"corpus_0=create_corpus(0)\n\ndic_0=defaultdict(int)\nfor word in corpus_0:\n    if word in stop:\n        dic_0[word]+=1\n\ntop_0=sorted(dic_0.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx_0,y_0=zip(*top_0)\n\n\ncorpus_1=create_corpus(1)\n\ndic_1=defaultdict(int)\nfor word in corpus_1:\n    if word in stop:\n        dic_1[word]+=1\n        \ntop_1=sorted(dic_1.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx_1,y_1=zip(*top_1)\n","b91cc28a":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x_0, y=y_0),\n    go.Bar(name='diaster', x=x_1, y=y_1)\n])\nfig.update_layout(title_text='common stop words')\nfig.show()\n","d79deb07":"import string\nspecial = string.punctuation\n\n\n\ncorpus0=create_corpus(0)\n\ndic0=defaultdict(int)\n\nfor i in (corpus0):\n    if i in special:\n        dic0[i]+=1\n        \n\ncorpus1=create_corpus(1)\n\ndic1=defaultdict(int)\nfor i in (corpus1):\n    if i in special:\n        dic1[i]+=1\n      \n\n    \nx0,y0=zip(*dic0.items())\n\nx1,y1=zip(*dic1.items())    \n        \n\n    \n    \nfig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.update_layout(title_text='Punctuations')\nfig.show()\n    ","de76aef1":"counter0=Counter(corpus0)\nmost0=counter0.most_common()\nx0=[]\ny0=[]\nfor word,count in most0[:100]:\n    if (word not in stop) :\n        x0.append(word)\n        y0.append(count)\n\ncounter1=Counter(corpus1)\nmost1=counter1.most_common()\nx1=[]\ny1=[]\nfor word,count in most1[:100]:\n    if (word not in stop) :\n        x1.append(word)\n        y1.append(count)\n","25b73f67":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.update_layout(title_text='common  words')\nfig.show()","979aec1a":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","61150fbe":"top_tweet_bigrams=get_top_tweet_bigrams(train[train.target==0]['text'])[:10]\nx0,y0=map(list,zip(*top_tweet_bigrams))\ntop_tweet_bigrams=get_top_tweet_bigrams(train[train.target==1]['text'])[:10]\nx1,y1=map(list,zip(*top_tweet_bigrams))","8605a23e":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.show()","9e638cca":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","504f490e":"remove_URL(\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\")","f66b9540":"train['text']=train['text'].apply(lambda x : remove_URL(x))\n","17870282":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","346ac2ed":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\nprint(remove_html(example))","e3950cab":"train['text']=train['text'].apply(lambda x : remove_html(x))","1b1f40f6":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","4f90fd32":"remove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","24e59401":"train['text']=train['text'].apply(lambda x: remove_emoji(x))","81f44390":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n","46ed3468":"\nexample=\"I am a #king\"\nprint(remove_punct(example))\n","910f1fce":"train['text']=train['text'].apply(lambda x : remove_punct(x))","bc46604b":"### Average world length","187a973e":"### Removing Emojis","9715db7c":"### Common words","e65ca720":"### Removing urls","f6bab754":"### Removing punctuations","b9cb6bbf":"Around 43% of data is real tweets data","7746bc1b":"#### Train data","28d992ce":"### Common stop words in tweets","05b6d859":"### Ngram analysis","01aec359":"## Exploratory data analysis","01277a2a":"### Removing html tags","ec0f2f8e":"### Please give a upvote","8a0c4566":"# EDA and preparing data","36b166f7":"#### Data distribution","d2008cd7":"### Punctuations","755d887b":"### Number of characters in tweets","9daa0464":"### Number od words in tweets","efd7192b":"## Data cleaning","89925dc5":"### Loading data","85f2c01b":"### Data frequency"}}