{"cell_type":{"5c9e540d":"code","eb67e7ff":"code","13f47c6b":"code","15a8379f":"code","7f88ab13":"code","9acfe510":"code","eede6a9e":"code","1c920d3e":"code","06884f7d":"code","73700df2":"code","955063f9":"code","c0cd2abc":"code","7e8cf3d0":"code","857a7323":"code","afa4f055":"code","9b2cd03e":"code","ac476647":"code","cc3bd7e1":"code","9a575411":"code","a9ae604b":"code","d6b19f30":"code","5115bf62":"code","f59e1adc":"code","c063c660":"code","e966e3b3":"markdown","435c2c16":"markdown","38b3a0c9":"markdown","86e0d9c5":"markdown","f5ddcfb6":"markdown","cda7fdad":"markdown","3871a304":"markdown","132c03ad":"markdown","479b4834":"markdown","67337f7a":"markdown","950d7a41":"markdown","c636bbc5":"markdown","4de293ad":"markdown","c5aeaf55":"markdown"},"source":{"5c9e540d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport pickle\nfrom tqdm import tqdm\nimport gc\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model , model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nfrom transformers import TFBertModel, BertTokenizerFast , BertTokenizer , RobertaTokenizerFast , TFRobertaModel , RobertaConfig , TFAutoModel , AutoTokenizer","eb67e7ff":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu,True)","13f47c6b":"max_len = 250\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 123\n\nMODEL=['bert-base-uncased' , 'roberta-base']\n\nmodel_name = MODEL[1]","15a8379f":"path=[\n    \"..\/input\/commonlitreadabilityprize\/sample_submission.csv\",\n    \"..\/input\/commonlitreadabilityprize\/test.csv\",\n    \"..\/input\/commonlitreadabilityprize\/train.csv\"\n]\n\ndf_train = pd.read_csv(path[2])\ndf_test = pd.read_csv(path[1])\ndf_ss = pd.read_csv(path[0])","7f88ab13":"df_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')","9acfe510":"X= df_train['excerpt']\ny=df_train['target'].values\n\nX_test = df_test['excerpt']","eede6a9e":"tokenizer1 = AutoTokenizer.from_pretrained(\"..\/input\/huggingface-roberta-variants\/roberta-base\/roberta-base\")\ntokenizer1","1c920d3e":"print('tokenization')\ntrain_embeddings = tokenizer1(X.to_list(), truncation = True , padding = 'max_length' , max_length=max_len)\ntest_embeddings = tokenizer1(X_test.to_list() , truncation = True , padding = 'max_length' , max_length = max_len)","06884f7d":"@tf.function\ndef map_function(encodings):\n    input_ids = encodings['input_ids']\n    \n    return {'input_word_ids': input_ids}\n\nprint(\"generating train and test\")    \ntrain = tf.data.Dataset.from_tensor_slices((train_embeddings))\ntrain = (\n            train\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(16)\n            .prefetch(AUTOTUNE)\n        )\n\n\ntest = tf.data.Dataset.from_tensor_slices((test_embeddings))\ntest = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(16)\n        .prefetch(AUTOTUNE)\n    )","73700df2":"def build_roberta_base_model(max_len=max_len ):\n    \n    transformer = TFAutoModel.from_pretrained(\"..\/input\/huggingface-roberta-variants\/roberta-base\/roberta-base\")\n    \n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    \n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = output)\n    \n    return model","955063f9":"ragnar_model = build_roberta_base_model()","c0cd2abc":"ragnar_model.summary()","7e8cf3d0":"def feature_extractor(path):\n    print(\"loading weights\")\n    ragnar_model.load_weights(path)\n    x= ragnar_model.layers[-3].output\n    model = Model(inputs = ragnar_model.inputs , outputs = x)\n    return model","857a7323":"def get_preds(model,train,test):\n    print(\"Extracting Features from train data\")\n    train_features = model.predict( train , verbose =1)\n    train_features = train_features.last_hidden_state\n    train_features = train_features[: , 0 , :]\n    print(\"Extracting Features from train data\")\n    test_features = model.predict( test , verbose =1)\n    test_features = test_features.last_hidden_state\n    test_features = test_features[: , 0 , :]\n    \n    return np.array(train_features , dtype= np.float16) , np.array(test_features , dtype= np.float16) ","afa4f055":"#model weight paths\npaths=[\"..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_1.h5\",\n       \"..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_2.h5\",\n       \"..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_3.h5\",\n       \"..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_4.h5\",\n       \"..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_5.h5\"\n      ]","9b2cd03e":"#1\nextraction_model = feature_extractor(paths[0])\ntrain_embeddings1 , test_embeddings1 = get_preds(extraction_model , train , test)","ac476647":"#2\nextraction_model = feature_extractor(paths[1])\ntrain_embeddings2 , test_embeddings2 = get_preds(extraction_model , train , test)","cc3bd7e1":"#3\nextraction_model = feature_extractor(paths[2])\ntrain_embeddings3 , test_embeddings3 = get_preds(extraction_model , train , test)","9a575411":"#4\nextraction_model = feature_extractor(paths[3])\ntrain_embeddings4 , test_embeddings4 = get_preds(extraction_model , train , test)","a9ae604b":"#5\nextraction_model = feature_extractor(paths[4])\ntrain_embeddings5 , test_embeddings5 = get_preds(extraction_model , train , test)","d6b19f30":"def get_preds(train_embeddings , test_embeddings):\n    scores=[]\n    kfold = KFold(n_splits=5, shuffle= True , random_state= SEED)\n    iteration=1\n    preds = np.zeros((test_embeddings.shape[0]))\n    for train_idx, test_idx in kfold.split(train_embeddings,y):\n        print(f'running iteration {iteration}')\n        X_train = train_embeddings[train_idx]\n        X_test = train_embeddings[test_idx]\n        y_train = y[train_idx]\n        y_test = y[test_idx]\n\n        regression_model = Ridge()\n        \n        regression_model.fit(X_train,y_train)\n        y_pred = regression_model.predict(X_test)\n\n        score = np.sqrt(mse(y_pred,y_test))\n        scores.append(score)\n        print(f'Fold {iteration} , rmse score: {score}')\n        y_preds = regression_model.predict(test_embeddings)\n        y_preds=y_preds.reshape(-1)\n        preds+=y_preds  \n        iteration += 1\n\n    print(f\"the average rmse is {np.mean(scores)}\")\n    return np.array(preds)\/5  ","5115bf62":"print(\"***********predicting***********\")\npreds1 = get_preds(train_embeddings1,test_embeddings1)\nprint(\"***********predicting***********\")\npreds2 = get_preds(train_embeddings2,test_embeddings2)\nprint(\"***********predicting***********\")\npreds3 = get_preds(train_embeddings3,test_embeddings3)\nprint(\"***********predicting***********\")\npreds4 = get_preds(train_embeddings4,test_embeddings4)\nprint(\"***********predicting***********\")\npreds5 = get_preds(train_embeddings5,test_embeddings5)","f59e1adc":"preds=(preds1+preds2+preds3+preds4+preds5)\/5\npreds = preds.tolist()","c063c660":"sub=pd.DataFrame({'id':df_ss['id'],'target':preds})\nsub.to_csv('submission.csv',index=False)\nsub.head()","e966e3b3":"# \ud83d\udcdd Abstract\n\n* This Notebook shows how to leverage model trained by others to get a decent score.\n* With the amount of quality kaggle kernels published by kagglers one can easily use finetuned model to get decent score. \n* This Notebook uses [Ragnar's](https:\/\/www.kaggle.com\/ragnar123) finetuned model weights as a Feature Extractor, kindly check out his [Training Notebook](https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf) and [Inference Notebook](https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf-inference\/data) \n\n\n### Future Work\n* Hyperparameter optimization of Ridge model\n* Using other Feature Extractors\n\n### Versions\n* Version 3 : CV- 0.37 LB- 0.475\n* Version 4 : Setting seed = 123 (same as ragnars seed to avoid data leak) CV - 0.3702 LB- 0.474 ","435c2c16":"# Feature Extraction Model","38b3a0c9":"#### Using Ridge","86e0d9c5":"# Tokenization","f5ddcfb6":"#### Thanks for viewing, drop your suggestions down in the comments below. \ud83d\ude42","cda7fdad":"# \ud83e\udde0 Modelling","3871a304":"# \ud83d\udcaf Submission","132c03ad":"# \ud83d\udd04 Kfold Training","479b4834":"#### Replicating Ragnars model architecture ","67337f7a":"# \u2699\ufe0f Parameters","950d7a41":"# \ud83d\udcca Dataset Preperation","c636bbc5":"# \ud83c\udfa3 Feature Extraction","4de293ad":"# Defining Tokenizer","c5aeaf55":"# \ud83d\ude9a Imports"}}