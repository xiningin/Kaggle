{"cell_type":{"19c8a5fd":"code","a224df1d":"code","f38e5197":"code","fccbd974":"code","5c7006da":"code","46a5ee07":"code","faf266aa":"code","c6b5f913":"code","da7c3e85":"code","f5bcecb8":"code","925b2b58":"code","4653a39a":"code","61879e02":"code","d5c99045":"code","b552ffb4":"code","e1cb7dbf":"markdown","9f3639c3":"markdown","7ace37af":"markdown","a17f14b5":"markdown","d3886002":"markdown"},"source":{"19c8a5fd":"import collections\nimport gc\nimport pickle\nimport sys\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import resample\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nimport lightgbm\n\n%matplotlib notebook\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bokeh.models import HoverTool\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\n\nfrom tqdm.auto import tqdm","a224df1d":"# Seaborn advanced settings\n\nsns.set(style='ticks',          # 'ticks', 'darkgrid'\n        palette='colorblind',   # 'colorblind', 'pastel', 'muted', 'bright'\n        #palette=sns.color_palette('Accent'),   # 'Set1', 'Set2', 'Dark2', 'Accent'\n        rc = {\n           'figure.autolayout': True,\n           'figure.figsize': (14, 8),\n           'legend.frameon': True,\n           'patch.linewidth': 2.0,\n           'lines.markersize': 6,\n           'lines.linewidth': 2.0,\n           'font.size': 20,\n           'legend.fontsize': 20,\n           'axes.labelsize': 16,\n           'axes.titlesize': 22,\n           'axes.grid': True,\n           'grid.color': '0.9',\n           'grid.linestyle': '-',\n           'grid.linewidth': 1.0,\n           'xtick.labelsize': 20,\n           'ytick.labelsize': 20,\n           'xtick.major.size': 8,\n           'ytick.major.size': 8,\n           'xtick.major.pad': 10.0,\n           'ytick.major.pad': 10.0,\n           }\n       )\n\nplt.rcParams['image.cmap'] = 'viridis'","f38e5197":"# Set general parameters here\nN_samples = int(1e5)   # Double of this amount is used (train and test)\ndrop_max_columns = -1   # -1 is for: repeat until all but one columns are dropped\nallowed_time = 7.5 * 60 * 60\n\nN_folds = 2   # Minimum is 2, but you can fork and set different CV \n\nn_estimators = 50                                                                                                                                                       \nverbose = 100                                                                                                                                                               \nearly_stopping_rounds = 50 ","fccbd974":"# Set LightGBM hyperparameters here\nparams = {'num_leaves': 1000,\n          'min_child_weight': 0.03,\n          'feature_fraction': 1,\n          'subsample': 0.5,\n          'min_data_in_leaf': 100,\n          'objective': 'binary',\n          'max_depth': 500,\n          'learning_rate': 0.007,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 312,\n          \"metric\": 'auc',\n#           \"verbosity\": -1,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 312,\n          'use_missing': True,\n         }","5c7006da":"# Load your train and test data in this cell\nwith open(\"..\/input\/ieee-the-most-basic-preprocessing\/Train-dtypes.pkl\", \"rb\") as f:\n    train_dtypes = pickle.load(f)\nwith open(\"..\/input\/ieee-the-most-basic-preprocessing\/Test-dtypes.pkl\", \"rb\") as f:\n    test_dtypes = pickle.load(f)\n    \ntrain = pd.read_csv(\"..\/input\/ieee-the-most-basic-preprocessing\/Train.csv\", dtype=train_dtypes)\ntest = pd.read_csv(\"..\/input\/ieee-the-most-basic-preprocessing\/Test.csv\", dtype=test_dtypes)","46a5ee07":"features = test.columns\ntrain = train[features]","faf266aa":"thing_to_learn = \"target\"\n\ntrain['target'] = 0\ntest['target'] = 1\n\ntrain[\"target\"] = train[\"target\"].astype(\"int16\")\ntest[\"target\"] = test[\"target\"].astype(\"int16\")\ntrain_dtypes[\"target\"] = train[\"target\"].dtype","c6b5f913":"dset = pd.concat([train.sample(N_samples), test.sample(N_samples)], ignore_index=True).reindex()\nfor column in tqdm(dset.columns):\n    dset[column] = dset[column].astype(train_dtypes[column])\nprint(dset.shape)\ndel train, test\ngc.collect()","da7c3e85":"drop_columns = list()\ndropped_values = list()\nCVs = list()\nif drop_max_columns == -1:\n    drop_max_columns = dset.shape[1] - 1\nif drop_max_columns > dset.shape[1]:\n    drop_max_columns = dset.shape[1] - 1","f5bcecb8":"dropped_already = -1\nstart_time = time.time()\ncurrent_time = time.time()\nwhile dropped_already < drop_max_columns and (current_time - start_time) < allowed_time:\n    CV = 0\n    feature_importance = pd.DataFrame() \n    kf = KFold(n_splits=N_folds, random_state=312, shuffle=True)\n    fold_n = 0\n    for train_index, valid_index in kf.split(dset):\n        X_train, X_valid = dset.drop([thing_to_learn]+drop_columns, axis=1).loc[train_index], dset.drop([thing_to_learn]+drop_columns, axis=1).loc[valid_index]\n        y_train, y_valid = dset.loc[train_index, thing_to_learn], dset.loc[valid_index, thing_to_learn]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dvalid = lgb.Dataset(X_valid, label=y_valid)\n        model = lgb.train(params, dtrain, n_estimators, valid_sets=[dtrain, dvalid], verbose_eval=verbose, early_stopping_rounds=500)\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = dset.drop([thing_to_learn]+drop_columns, axis=1).columns                                                                                                                                     \n        fold_importance[\"importance\"] = model.feature_importance()                                                                                                              \n        fold_importance[\"fold\"] = fold_n + 1                  \n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)                                                                                        \n        CV += model.best_score[\"valid_1\"][\"auc\"] \n        fold_n += 1\n    CV \/= N_folds\n    fi = pd.DataFrame({\"Feature\":feature_importance.pivot(index=\"fold\", columns=\"feature\", values=\"importance\").columns.values,                                   \n                      \"Value\":feature_importance.pivot(index=\"fold\", columns=\"feature\", values=\"importance\").mean().values})\n    row = fi.iloc[fi['Value'].idxmax()]\n    feature = row[\"Feature\"]\n    value = row[\"Value\"]\n    drop_columns.append(feature)\n    dropped_values.append(value)\n    CVs.append(CV)\n    dropped_already += 1\n    current_time = time.time()","925b2b58":"print(list(drop_columns))","4653a39a":"drop_columns = drop_columns[:-1]\ndropped_values = dropped_values[:-1]\ndrop_columns.insert(0, \"Nothing\")\ndropped_values.insert(0, 0)\ndeltas = list(np.array(CVs)[:-1] - np.array(CVs)[1:])\ndeltas.insert(0, 0)\ndf = pd.DataFrame({\"Dropped_feature\":drop_columns, \"CV_change\":deltas, \"N_dropped\":np.arange(len(CVs)), \"CV\":CVs})\ndf.to_csv(\"Results.csv\")","61879e02":"output_notebook()\n\nfig = figure(plot_height=600,\n             plot_width=700,\n             x_axis_label='Number of dropped features',\n             x_range=(0, drop_max_columns),\n             y_axis_label='AUC',\n             title='AUC vs number of dropped features',\n             toolbar_location='below',\n             tools='save, box_zoom, pan, wheel_zoom, reset')\n\ntooltips = [\n            ('Dropped feature','@Dropped_feature'),\n            ('CV change', '@CV_change'),\n            (\"Number of dropped features\", \"@N_dropped\"),\n            (\"Current CV\", \"@CV\")\n           ]\n\nscatter = fig.circle(x='N_dropped', y='CV', source=df,\n        size=15, alpha=1,\n        hover_fill_color='cyan', hover_alpha=0.3)\n\nfig.add_tools(HoverTool(tooltips=tooltips, renderers=[scatter]))\n\nshow(fig)","d5c99045":"pd.set_option('display.max_rows', 100)\ndf.head(100)","b552ffb4":"plt.figure()\nplt.plot(np.arange(len(CVs)), CVs, \"o\")\nplt.ylabel(\"AUC\")\nplt.xlabel(\"Number of dropped features\")\nplt.savefig(\"AUCvsDroppedFeatures.png\")\nplt.show()","e1cb7dbf":"# Validation","9f3639c3":"# Results","7ace37af":"# Settings","a17f14b5":"# Intro\n\nAdversarial validation is a method for comparing distributions of train and test data using feature importance of a regression forest. We label all train data with 0 and test data with 1 (or vice versa). Then we train a lightGBM to distinguish train samples from test samples. Ideally ROC AUC should be 0.5 - this would mean that distributions are identical and based on the features it is not possible to distinguish between train and test. If the ROC AUC is higher, then the model finds it easy to distinguish train and test samples and this means that training and testing distributions are different. By excluding the most important features (i.e. those which help the most to distinguish between train and test and are most certainly different for train and test) we can lower the ROC AUC, ideally getting to 0.5.\n\nThis kernel is fork from [Bojan Tunguz's kernel](https:\/\/www.kaggle.com\/tunguz\/adversarial-ieee), but it is automated, so that the most important feature is discarded and then LightGBM is run again. This is run iteratively, until pre-specified number of dropped features is reached or until time limit is reached. Bojan Tunguz's kernel use as source this [standalone train and test preprocessing](https:\/\/www.kaggle.com\/tunguz\/standalone-train-and-test-preprocessing). I have my data ready with encodings etc. so this kernel is really just about adversarial validation, no manipulation with data. As I won't publish my features until end of the competition, I made a very simple preprocessing in another kernel, so that this notebook just runs without an error. ","d3886002":"# Preprocessing"}}