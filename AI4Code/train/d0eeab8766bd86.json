{"cell_type":{"02c5272d":"code","7ae36f11":"code","5ba3f491":"code","b2f2fdb9":"code","a4545a4c":"code","2a3cbf70":"code","4b6aff9c":"code","3c41eeba":"code","ad9a2f42":"code","4e467ad0":"code","cf89cd33":"code","b1fd2174":"code","fc3aff4b":"code","6a50e649":"code","d4cf8740":"code","adb249c7":"code","985fef09":"code","5790cfba":"code","25039a5a":"code","2d83b49f":"code","bd179f60":"code","bf7c2310":"code","e94a9a12":"code","dd41adf7":"code","376f2a58":"code","6195cadd":"code","0661fb12":"code","5d7cffba":"code","65dca459":"code","f8147f63":"code","a51743fc":"code","783e4c30":"code","017b2196":"code","7a8ef3cf":"markdown","3f44ac1c":"markdown","71d1cd41":"markdown","89709cf3":"markdown","d59a6a77":"markdown","e417ac5e":"markdown","c162940b":"markdown","d1eb048f":"markdown","363ce0f2":"markdown","2ff5060b":"markdown","beb5e65f":"markdown","75a74359":"markdown","45110df0":"markdown","62ddfec9":"markdown","01a5a391":"markdown","b9d5065e":"markdown","900116e7":"markdown","a25543ac":"markdown","f1d540c4":"markdown","9b10a6e9":"markdown","3889fb54":"markdown","8744e4df":"markdown","610c46d9":"markdown","8af288d7":"markdown","9f1d3ee5":"markdown"},"source":{"02c5272d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ae36f11":"df_diabetes = pd.read_csv('..\/input\/diabetes-data-set\/diabetes.csv')\ndf_diabetes.head()","5ba3f491":"df_diabetes.info()","b2f2fdb9":"df_diabetes.describe()","a4545a4c":"#get the number of 0s or, the number of non diabetes\nprint((df_diabetes == 0).sum())\nprint('')\nprint('There are', (df_diabetes['Outcome'] == 0).sum(axis = 0), 'non diabetes')\nprint('Which represent', ((df_diabetes['Outcome'] == 0).sum(axis = 0) \/ len(df_diabetes)) * 100, 'of the total')\n\n#get the number of 1s or, the number of diabetes\nprint('There are', (df_diabetes['Outcome'] == 1).sum(axis = 0), 'diabetes')\nprint('Which represent', ((df_diabetes['Outcome'] == 1).sum(axis = 0) \/ len(df_diabetes)) * 100, 'of the total')","2a3cbf70":"#define a new data set just for statistical analysis\n#keep only features with 0 values where it's not supposed to be.\ndf_diabetes_stat = df_diabetes[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']]","4b6aff9c":"#compute the % of 0 values in each column\nzeros = (df_diabetes_stat == 0).astype(int).sum(axis = 0)\npercentage = ((df_diabetes_stat == 0).astype(int).sum(axis = 0) \/ len(df_diabetes)) * 100\ndf_zeros = pd.DataFrame(zeros)\ndf_zeros['0 rate'] = percentage\ndf_zeros.columns = ['Count 0s', 'Percentage 0s']\ndf_zeros","3c41eeba":"print('-----------------Statistical Analysis-----------------')\ndisplay(df_diabetes_stat.describe())","ad9a2f42":"# plt.figure()\n# sns.pairplot(df_diabetes)\n# plt.show()","4e467ad0":"groupby_outcome = df_diabetes.groupby('Outcome').agg({'Glucose': ['median'], \n                                                      'BloodPressure': ['median'], \n                                                      'SkinThickness': ['median'], \n                                                      'Insulin': ['median'],\n                                                      'BMI': ['median'], \n                                                      'DiabetesPedigreeFunction': ['median'],\n                                                      'Age': ['median'], \n                                                      'Pregnancies': ['median']})\ngroupby_outcome = groupby_outcome.reset_index()\ngroupby_outcome.columns = ['Outcome','Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n                           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Pregnancies'] \ngroupby_outcome","cf89cd33":"cols = groupby_outcome.columns\n#drop Outcome, Age, Pregnancies because we don't want to replace their 0 values\ncols = cols.drop(['Outcome','Age', 'Pregnancies'])\n\ndef fill_median(row):\n    condition = (groupby_outcome['Outcome'] == row['Outcome']) #take the value based on the Outcome (0 or 1) \n    return groupby_outcome[condition][cols].values[0]","b1fd2174":"#replace all 0 values by NaN\ndf_diabetes[cols]=df_diabetes[cols].replace(0, np.nan)","fc3aff4b":"#for loop for each columns \n#apply the function fill_median and replace NaN by median value\nfor cols in cols:\n    df_diabetes[cols] = df_diabetes.apply(lambda row: fill_median(row) if np.isnan(row[cols]) else row[cols], axis = 1)","6a50e649":"plt.figure(figsize = (15, 7))\nplt.subplot(2,3,1)\nsns.histplot(df_diabetes['Glucose'], kde = True)\nplt.subplot(2,3,2)\nsns.histplot(df_diabetes['BloodPressure'], kde = True)\nplt.subplot(2,3,3)\nsns.histplot(df_diabetes['SkinThickness'], kde = True)\nplt.subplot(2,3,4)\nsns.histplot(df_diabetes['Insulin'], kde = True)\nplt.subplot(2,3,5)\nsns.histplot(df_diabetes['BMI'], kde = True)\nplt.subplot(2,3,6)\nsns.histplot(df_diabetes['DiabetesPedigreeFunction'], kde = True)\nplt.show()","d4cf8740":"cols = df_diabetes.columns\ncols = cols.drop(['Outcome', 'Age', 'Pregnancies'])\nfor cols in cols:\n    skew = df_diabetes[cols].skew() #compute the skewness of each column\n    skew = np.abs(skew) #to avoid negative values, compute the absolute value\n    print(cols, 'skewness', skew)","adb249c7":"corr_before = df_diabetes.corr()\ncorr_before = pd.DataFrame(corr_before['Outcome'])","985fef09":"df_diabetes[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']] = np.log(df_diabetes[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']])","5790cfba":"cols = df_diabetes.columns\ncols = cols.drop(['Outcome', 'Age', 'Pregnancies'])\nfor cols in cols:\n    skew = df_diabetes[cols].skew() #compute the skewness of each column\n    skew = np.abs(skew) #to avoid negative values, compute the absolute value\n    print(cols, 'skewness', skew)","25039a5a":"corr_after = df_diabetes.corr()\ncorr_after = pd.DataFrame(corr_after['Outcome'])\n\n#add the correlation before so it's easier to compare\ncorr = corr_before.join(corr_after, lsuffix = '_before', rsuffix = '_after')\ncorr","2d83b49f":"#visualize a heatmap of the correlation\nplt.figure(figsize = (10,7))\nsns.heatmap(df_diabetes.corr(), cmap = 'flare', annot = True)\nplt.show()","bd179f60":"plt.figure(figsize = (15, 7))\nplt.subplot(2,3,1)\nsns.histplot(df_diabetes['Glucose'], kde = True)\nplt.subplot(2,3,2)\nsns.histplot(df_diabetes['BloodPressure'], kde = True)\nplt.subplot(2,3,3)\nsns.histplot(df_diabetes['SkinThickness'], kde = True)\nplt.subplot(2,3,4)\nsns.histplot(df_diabetes['Insulin'], kde = True)\nplt.subplot(2,3,5)\nsns.histplot(df_diabetes['BMI'], kde = True)\nplt.subplot(2,3,6)\nsns.histplot(df_diabetes['DiabetesPedigreeFunction'], kde = True)\nplt.show()","bf7c2310":"df_diabetes.replace([np.inf, -np.inf], 0, inplace=True)","e94a9a12":"#SelectKBest with chi2 doesn't deal with negative values\n#I turned all the values to their absolute values\n#by doing that we actually have a more accurate model lol (cross val : 0.866 -> 0.883)\ndf_diabetes['DiabetesPedigreeFunction'] = df_diabetes['DiabetesPedigreeFunction'].abs()","dd41adf7":"from sklearn.feature_selection import SelectKBest, chi2 #for feature selection\n\nX = df_diabetes.drop('Outcome', axis = 1)\ny = df_diabetes['Outcome']\n\nfor k in np.arange(1, 9, 1):\n    X_new = SelectKBest(score_func = chi2, k = k)\n    X_new.fit_transform(X, y)\n    features = X.columns[X_new.get_support()]\n    print('---------------For k =', k, '---------------')\n    display(features)","376f2a58":"from xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.metrics import mean_absolute_error, confusion_matrix, classification_report, f1_score, accuracy_score, precision_score, recall_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.dummy import DummyClassifier","6195cadd":"#first, let's create and split the test and train set\nX = df_diabetes.drop('Outcome', axis = 1) #features\n#X = df_diabetes[['Insulin', 'Glucose', 'BMI']]\ny = df_diabetes['Outcome'] #target\n\n#scaler = StandardScaler()\nrobust = RobustScaler()\n#X_trainScaled = scaler.fit_transform(X)\nX_trainScaled = robust.fit_transform(X)\n\n#split the train and test set with train size equal to 0.8 (80%)\nX_trainScaled, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 1)","0661fb12":"display(X.shape)\ndisplay(y.shape)","5d7cffba":"# # #first, define the hyperparameters to be implemented in XGBClassifier\n# # #since I don't have a supercomputer, I'll limit the number of hyperparameter for the tuning\n# parameters_grid = {'n_estimators':[500, 1000], #how many times to go through the modeling cycle\n#                    'learning_rate':[0.05, 0.3], #Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number before adding them in.\n#                    'n_jobs':[3,6], #On larger datasets where runtime is a consideration, you can use parallelism to build your models faster\n#                    'max_depth':[3,4,5,10]\n#                   }\n\n# grid_search = GridSearchCV(xgb, parameters_grid, cv = 10, scoring = 'f1', error_score = 0)\n# grid_result = grid_search.fit(X, y)\n\n# # #print the optimal hyperparameters and accuracy\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","65dca459":"xgb = XGBClassifier(n_estimators = 500, n_jobs = 3, max_depth = 4, learning_rate = 0.3) #set up the model\nxgb.fit(X_trainScaled, y_train) #fit the model\npredictions = xgb.predict(X_test) #run prediction\nxgb","f8147f63":"print('--------------------XGBoost--------------------')\nprint()\n#ACCURACY\nprint('Training accuracy is', xgb.score(X_trainScaled, y_train))\nprint('Test accuracy is', xgb.score(X_test, y_test))\nprint('-' * 40)\n\n#CROSS VALIDATION\nscore_accuracy = cross_val_score(xgb, X_trainScaled, y_train, cv = 10, scoring = 'f1')\nprint('Average f1 score is', score_accuracy.mean())\nprint('-' * 40)\n\n#CONFUSIN MATRIX\ny_predicted = xgb.predict(X_test)\nconfusion = confusion_matrix(y_test, y_predicted)\nprint('The confusion matrix is')\ndisplay(confusion)\nprint('-' * 40)\n\n#CLASSIFICATION REPORT\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, predictions)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, predictions)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, predictions)))\nprint('F1: {:.2f}'.format(f1_score(y_test, predictions)))\nprint('-' * 40)\nprint('The classification report is')\nprint(classification_report(y_test, predictions))","a51743fc":"models = [] #create a list\n#append each model to the list\n#models.append(('LR', LogisticRegression(random_state = 1)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DTC', DecisionTreeClassifier(random_state = 1)))\nmodels.append(('RFC', RandomForestClassifier(random_state = 1)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 1)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 1)))","783e4c30":"results = []\nnames = []\n\nfor name, model in models:\n    #kfold = KFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_trainScaled, y_train, cv = 10, scoring = \"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","017b2196":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","7a8ef3cf":"## Model","3f44ac1c":"## Comparison with other algorithm\nI will not perform any tuning or other stuff to improve my accuracy","71d1cd41":"Analyze the median of each feature grouped by the Outcome.","89709cf3":"The RandomForest and GradientBoost gives the best cross validation score, and close to the XGBoost. Additionnally, the RandomForest as a satisfying standard deviation to say that among the list of model, it is the best. Of course, it can be improved.","d59a6a77":"Visualize how the log on skewed features acted","e417ac5e":"## Some base data explanatory","c162940b":"<b>To be continued...<\/b>","d1eb048f":"### Hyperparameter tuning","363ce0f2":"We can see that we have a lot of 0 values.\n<br>\n<br>\nFor columns like <i>Outcome<\/i> and <i>Pregnancies<\/i> it is totally normal.\n<br>\nFor columns like <i>DiabetesPedigreeFunction<\/i> and <i>Age<\/i>, there are no 0 values, we're good.\n<br>\nFor the rest, we need to clean that.\n<br>\n<br>\nThe error can be human or other, no idea. For sure, the SkinThickness of the BloodPressure cannot be equal to 0. We need to find a solution to get rid of those 0s.","2ff5060b":"However, we still have a problem with the Insulin feature. For the Outcome 1, the Insulin median is 0. But, for this Outcome we have other values than 0.","beb5e65f":"Some information about the data set:\n<br>\n<ul>\n    <li>Pregnancies: Number of times pregnant<\/li>\n    <li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test<\/li>\n    <li>BloodPressure: Diastolic blood pressure (mm Hg)<\/li>\n    <li>SkinThickness: Triceps skin fold thickness (mm)<\/li>\n    <li>Insulin: 2-Hour serum insulin (mu U\/ml)<\/li>\n    <li>BMI: Body mass index (weight in kg\/(height in m)^2)<\/li>\n    <li>DiabetesPedigreeFunction: Diabetes pedigree function<\/li>\n    <li>Age: Age (years)<\/li>\n    <li>Outcome: Class variable (0 or 1)<\/li>\n<\/ul>\nImportant piece of information !!!! There is a feature called 'Pregnancies' which means the data set is about a women sample","75a74359":"## Feature Engineering","45110df0":"SkinThickness and the Insulin features count respictively 29.6% and 48.7% of 0 values.","62ddfec9":"First, I'll replace the 0s value in the columns where it matters. \n<br>\nThe columns concerned are:\n<ul>\n    <li>Glucose<\/li>\n    <li>BloodPressure<\/li>\n    <li>SkinThickness<\/li>\n    <li>Insulin<\/li>\n    <li>BMI<\/li>\n<\/ul>\nThese parameteres have a high influence on the final Outcome. According to some source, just by checking the Glucose or the Insulin is sufficient to tell the Outcome. That is why, the 0 values will be replaced by the median of the column grouped by the Outcome target.\n<br>\n<br>\nIn other words: median of Outcome = 0, and median for Outcome = 1","01a5a391":"To take care of them, I'm gonna perform a log correction","b9d5065e":"#### XGBoost","900116e7":"#### Set up the environment","a25543ac":"#### Model evaluation","f1d540c4":"# Project 2\/52\n<b>Task details:<\/b>\n<br>\nEvery data has lot of hidden information. These hidden information was required to be investigated to find out the hidden patterns . These patterns can be helpful in making decisions on the procedure , removal of any ambiguity and also in getting key business insights. To solve all this questions, exploratory data analysis was introduced.\n<br>\n<br>\n<b>Expectation:<\/b>\n<br>\nHighst AC score need it! and please explain with comments!","9b10a6e9":"Insulin and Glucose have the strongest correlation with the outcome. The BMI and SkinThickness give satisfying correlation that can be used for the model.","3889fb54":"It takes some time to run this code so I commented it. The results obtained is the following:\n<br>\nBest: 0.835342 using {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 500, 'n_jobs': 3}\n<br>\n<br>\nHowever, this is not satisfying for my overfitting problem. But the idea is there. Being able to test more parameters would surely give a better model. But I don't have a super computer so I won't do it.","8744e4df":"After running my model mutliple times (for different feature engineering), every single time I ran on a perfect train set of 1. While my test set went from 0.75 to 0.902. My train set was obviously overfitted.\n<br>\n<br>\nI tried many options to fix this overfitting:\n<ul>\n    <li>feature selection using k best. The train set accuracy varies between 0.97 and 0.99. But the cross validation\n    and F1 scores are not satisfying<\/li>\n    <li>hyperparameter tuning with gridsearch -> running it takes ages, so I reduced the number of parameter to test<\/li>\n    <li>Regularization (coming soon)<\/li>\n<\/ul>","610c46d9":"## Feature Selection","8af288d7":"#### Skew features","9f1d3ee5":"### Model fitting"}}