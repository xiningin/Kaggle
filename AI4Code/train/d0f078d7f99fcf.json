{"cell_type":{"b8f0c1e5":"code","039c2110":"code","3e7f2ee1":"code","0f52d057":"code","3f4a6425":"markdown","0114ebfd":"markdown","60896c39":"markdown","88e54f06":"markdown"},"source":{"b8f0c1e5":"import numpy as np\nimport matplotlib.pyplot as plt","039c2110":"mu1 = [7,7]\nsigma1 = [[8,3],[3,2]]\nc1 = np.random.multivariate_normal(mu1,sigma1,300)\n\nmu2 = [15,6]\nsigma2 = [[2,0],[0,2]]\nc2 = np.random.multivariate_normal(mu2,sigma2,100)\n\nplt.scatter(c1[:,0],c1[:,1],s= 9,color='blue',marker='*',label='class1')\nplt.scatter(c2[:,0],c2[:,1],s= 9,color='orange',marker='o',label='class2')\n\n#################################################################################################\n# Batch Gradient Descent\nphi = np.concatenate((c1,c2),axis=0)\nx = np.concatenate((np.ones((np.size(phi,0),1)),phi),axis=1)\ny = np.concatenate((np.ones((np.size(c1,0),1)),np.zeros((np.size(c2,0),1))))\n\nmax_ite = 5000; weight_vec = np.random.randn(3,1); lr = 0.1\ncost_val_batch = np.zeros((max_ite,1))\n\nfor i in range(max_ite):\n    # logistic function in vectorized form\n    h_x = 1\/(1+np.exp(-x.dot(weight_vec)))\n    #log-loss a.k.a cost function\n    cost_val_batch[i] = -(1\/(np.size(x,0)))*sum(y*np.log(h_x) + (1-y)*np.log(1-h_x))\n    #gradient of cost function\n    grad = (1\/(np.size(x,0)))*(x.T.dot((h_x-y)))\n    #update weight vectors\n    weight_vec = weight_vec - lr*grad\n\nx_plot = np.linspace(5,20,7)\ny_plot = -weight_vec[0]\/weight_vec[2] - (weight_vec[1]\/weight_vec[2])*x_plot\nplt.plot(x_plot,y_plot,'k',label='Batch Gradient Descent')\n\n#################################################################################################\n#2nd order Newton's method\nmax_ite_newton = 20;\ncost_val_newton = np.zeros((max_ite_newton,1))\n\nfor i in range(max_ite_newton):\n    h_x = 1\/(1+np.exp(-x.dot(weight_vec)))\n    cost_val_newton[i] = -(1\/(np.size(x,0)))*sum(y*np.log(h_x) + (1-y)*np.log(1-h_x))\n    grad = (1\/(np.size(x,0)))*(x.T.dot((h_x-y)))\n    # hessian matrix\n    H = (1\/(np.size(x,0)))*(x.T.dot(np.diag(h_x.reshape(np.size(x,0),))).dot(np.diag((1-h_x).reshape(np.size(x,0),))).dot(x))\n    weight_vec = weight_vec - np.linalg.pinv(H).dot(grad)\n    \ny_plot = -weight_vec[0]\/weight_vec[2] - (weight_vec[1]\/weight_vec[2])*x_plot\nplt.plot(x_plot,y_plot,'r',label='Newtons method' )\nplt.xlabel('x1');plt.ylabel('x2')\nplt.legend()\nplt.show()","3e7f2ee1":"# plot cost fucntion\nplt.plot(cost_val_batch,'k')\nplt.title('Batch Gradient Descent');plt.xlabel('# of iterations');plt.ylabel('cost')\nplt.xlim([0,100])\nplt.show()\n\nplt.plot(cost_val_newton,'b--')\nplt.plot(cost_val_newton,'ro')\nplt.title('2nd order Newtons method');plt.xlabel('# of iterations');plt.ylabel('cost')\nplt.show()","0f52d057":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='newton-cg')\nlog_reg.fit(phi,y.ravel())\n\ny_plot = -log_reg.intercept_\/log_reg.coef_[:,1] - (log_reg.coef_[:,0]\/log_reg.coef_[:,1])*x_plot\nplt.plot(x_plot,y_plot,'g',label='sklearn newton-cg')\ny_plot = -weight_vec[0]\/weight_vec[2] - (weight_vec[1]\/weight_vec[2])*x_plot\nplt.plot(x_plot,y_plot,'r',label='Newtons method' )\nplt.scatter(c1[:,0],c1[:,1],s= 9,color='blue',marker='*',label='class1')\nplt.scatter(c2[:,0],c2[:,1],s= 9,color='orange',marker='o',label='class2')\nplt.xlabel('x1');plt.ylabel('x2')\nplt.legend()\nplt.show()","3f4a6425":"Look at convergence time between two different solvers.\nAs you can see, Newton's method normally converges around 4-5 iterations and you don't neet to assign learning parameter. \nThis is because, in algorithm we compute the Hessian matrix(second -derivative of cost) which makes it quadratic solution and converges much faster.","0114ebfd":"Below, I am using sklearn to solve the same problem. If you read the documentation, you will see different solvers. Here, I picked 'newton-cg', there are also other solvers with different configurations. I will refer readers to check out the sklearn documentation for more details. Try to undersrand their advantages and disadvantages. ","60896c39":"1. Let's create two synthetic data cluster.\n2. Implement logistic regression with BGD solver\n3. Implement logistic regression with Newton's method\n\n*Note: These codes are not fully optimized yet, but it will give you the main picture. For example, I only used quick BGD solver here, but it can be modified to Stochastic or mini_batch solver.\n*","88e54f06":"**           Implement logistic regression (classification algorithm) algorithms from scracth!\n**\n\nOutline:\n\n* Generate synthetic data: two cluster data\n* Implement batch gradient descent (BGD) logistic regression from scrath\n* Implement 2nd order newtons method logistic regression from scrath\n* Cost function comparision\n* Scikit-learn linear regression"}}