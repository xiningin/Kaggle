{"cell_type":{"8265ca30":"code","e0d11916":"code","ce208842":"code","b3ff127f":"code","457f5b2e":"code","09b2b9c6":"code","4a0046ee":"code","ff89b91c":"code","e19fb766":"code","8dbc915c":"code","610ff88c":"code","6e16bcb2":"code","de5a5abb":"code","02e5d5f9":"code","d654dcea":"code","65643649":"code","aba0dca2":"code","74787c12":"code","43097e93":"code","09d16b12":"code","a60d3252":"code","285c0c31":"code","e51b4d58":"code","bb39efbe":"code","8f863178":"code","4dbe33ec":"code","a69cf5d3":"code","63f756d3":"code","b215781b":"code","f7ef93a6":"code","575ed6d0":"code","b0e9c68b":"code","f4484cc8":"code","84647581":"code","03c3681a":"code","4b164bc3":"code","f7cfc65e":"code","bf1c0d02":"code","937f658a":"code","d7e7fb95":"code","0c5e781e":"code","a62da1e9":"code","bf7334f3":"code","b7f5ea80":"code","b0fa8a6c":"code","8bd46bba":"code","a5f546f3":"code","5e57b094":"code","43da4433":"code","3b558b78":"code","1252e990":"code","00b66b52":"code","867378fc":"code","9a821c8e":"code","e56aa039":"markdown","fca7ec8f":"markdown","9b7b1bca":"markdown","7fdf36f4":"markdown","5fb44fb2":"markdown","30907ece":"markdown","589f041b":"markdown","6390665b":"markdown","ffd11d65":"markdown","cef86df1":"markdown","d1a2e9c3":"markdown","b8ea6c48":"markdown","a24099bf":"markdown","06e47d2e":"markdown","cfcced96":"markdown","2b69fa8e":"markdown","42e3145f":"markdown","7981cab6":"markdown","fe6648df":"markdown","30a6504c":"markdown","2ae8ce1f":"markdown","d61940ed":"markdown","7aa29274":"markdown","f5006ea1":"markdown","111c2f1e":"markdown","e99529eb":"markdown","3786a56b":"markdown","ca9fc132":"markdown","a33f0113":"markdown","af48adae":"markdown","a3886b32":"markdown","b2445dc7":"markdown","8753fbd0":"markdown","351f8331":"markdown","26c92811":"markdown","a6672891":"markdown","c557952d":"markdown","b3aa015e":"markdown"},"source":{"8265ca30":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n","e0d11916":"df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\ndf.head()","ce208842":"df.info()\ndf.describe()","b3ff127f":"# Let's see who made one off purchase of $40761!\ndf[df['ONEOFF_PURCHASES']==40761.250000]","457f5b2e":"df['CASH_ADVANCE'].max()","09b2b9c6":"# Let's see who made cash advance of $47137!\n# This customer made 123 cash advance transactions!!\n# Never paid credit card in full\ndf[df['CASH_ADVANCE']==47137.211760000006]","4a0046ee":"# Let's see if we have any missing data, luckily we don't!\nsns.heatmap(df.isnull(),yticklabels=False,cmap='Blues',cbar=False)","ff89b91c":"df.isnull().sum()","e19fb766":"# Fill up the missing elements with mean of the 'MINIMUM_PAYMENT' \ndf['MINIMUM_PAYMENTS'] = df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].mean())","8dbc915c":"# Fill up the missing elements with mean of the 'CREDIT_LIMIT' \ndf['CREDIT_LIMIT'] = df['CREDIT_LIMIT'].fillna(df['CREDIT_LIMIT'].mean())","610ff88c":"sns.heatmap(df.isnull(),cbar=False,cmap='Blues',yticklabels=False)","6e16bcb2":"# Let's see if we have duplicated entries in the data\ndf.duplicated().sum()","de5a5abb":"# Let's drop Customer ID since it has no meaning here \ndf.drop(\"CUST_ID\",axis=1,inplace=True)\ndf.head(2)","02e5d5f9":"n = len(df.columns)\nn","d654dcea":"df.columns","65643649":"# distplot combines the matplotlib.hist function with seaborn kdeplot()\n# KDE Plot represents the Kernel Density Estimate\n# KDE is used for visualizing the Probability Density of a continuous variable. \n# KDE demonstrates the probability density at different values in a continuous variable. \nplt.figure(figsize=(10,60))\nfor i in range(n):\n    plt.subplot(17,1,i+1)\n    sns.distplot(df[df.columns[i]],kde_kws={'color':'b','bw': 0.1,'lw':3,'label':'KDE'},hist_kws={'color':'r'})\n    plt.title(df.columns[i])\nplt.tight_layout()","aba0dca2":"correlations = df.corr()","74787c12":"plt.figure(figsize=(16,12))\nsns.heatmap(correlations,annot=True)\n# 'PURCHASES' have high correlation between one-off purchases, 'installment purchases, purchase transactions, credit limit and payments. \n# Strong Positive Correlation between 'PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY'\n","43097e93":"# Let's scale the data first\nscaler = StandardScaler()","09d16b12":"scaled_data = scaler.fit_transform(df)\nscaled_data.shape","a60d3252":"scores_1 = []\n\nrange_values = range(1,20)\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(scaled_data)\n    scores_1.append(kmeans.inertia_)\nplt.plot(scores_1, 'bx-')\nplt.style.use('ggplot')\nplt.title('Finding the right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('Scores') \nplt.show()","285c0c31":"# From this we can observe that, 4th cluster seems to be forming the elbow of the curve. \n# However, the values does not reduce linearly until 8th cluster. \n# Let's choose the number of clusters to be 8.","e51b4d58":"kmeans = KMeans(8)\nkmeans.fit(scaled_data)\nlabels = kmeans.labels_","bb39efbe":"kmeans.cluster_centers_.shape","8f863178":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_,columns = [df.columns])\ncluster_centers","4dbe33ec":"# In order to understand what these numbers mean, let's perform inverse transformation\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers,columns = [df.columns])\ncluster_centers\n\n# First Customers cluster (Transactors): Those are customers who pay least amount of intrerest charges and careful with their money, Cluster with lowest balance ($104) and cash advance ($303), Percentage of full payment = 23%\n# Second customers cluster (revolvers) who use credit card as a loan (most lucrative sector): highest balance ($5000) and cash advance (~$5000), low purchase frequency, high cash advance frequency (0.5), high cash advance transactions (16) and low percentage of full payment (3%)\n# Third customer cluster (VIP\/Prime): high credit limit $16K and highest percentage of full payment, target for increase credit limit and increase spending habits\n# Fourth customer cluster (low tenure): these are customers with low tenure (7 years), low balance \n","a69cf5d3":"labels.shape # Labels associated to each data point","63f756d3":"labels.max()","b215781b":"labels.min()","f7ef93a6":"y_kmeans = kmeans.fit_predict(scaled_data)\ny_kmeans","575ed6d0":"# concatenate the clusters labels to our original dataframe\ncreditcard_df_cluster = pd.concat([df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster.head()","b0e9c68b":"# Plot the histogram of various clusters\nfor i in df.columns:\n  plt.figure(figsize = (35, 5))\n  for j in range(8):\n    plt.subplot(1,8,j+1)\n    cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{}    \\nCluster {} '.format(i,j))\n  \n  plt.show()","f4484cc8":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(scaled_data)\nprincipal_comp","84647581":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data=principal_comp,columns=['pca1','pca2'])\npca_df.sample(5)","03c3681a":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","4b164bc3":"plt.figure(figsize=(10,10))\nplt.style.use('ggplot')\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink','yellow','gray','purple', 'black'])\nplt.show()","f7cfc65e":"from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom keras.optimizers import SGD","bf1c0d02":"encoding_dim = 7\n\ninput_df = Input(shape=(17,))\n\n\n# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution \n\nx = Dense(encoding_dim, activation='relu')(input_df)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nencoded = Dense(10, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\ndecoded = Dense(17, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n#encoder - used for our dimention reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer= 'adam', loss='mean_squared_error')","937f658a":"scaled_data.shape","d7e7fb95":"autoencoder.fit(scaled_data,scaled_data,batch_size=128,epochs=25,verbose=1)","0c5e781e":"#autoencoder.save_weights('autoencoder.h5')","a62da1e9":"autoencoder.summary()","bf7334f3":"pred_ac = encoder.predict(scaled_data)","b7f5ea80":"pred_ac.shape","b0fa8a6c":"scores_2 = []\n\nrange_values = range(1,20)\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(pred_ac)\n    scores_2.append(kmeans.inertia_)\nplt.plot(scores_2, 'bx-')\nplt.style.use('ggplot')\nplt.title('Finding the right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('Scores') \nplt.show()","8bd46bba":"plt.plot(scores_1, 'bx-', color = 'r',label='Without Autoencode')\nplt.plot(scores_2, 'bx-', color = 'g',label='With Autoencode')","a5f546f3":"kmeans = KMeans(4)\nkmeans.fit(pred_ac)\nlabels = kmeans.labels_\nkmeans.cluster_centers_.shape","5e57b094":"y_kmeans = kmeans.fit_predict(scaled_data)\ny_kmeans","43da4433":"# concatenate the new reduced clusters labels to our original dataframe\ncreditcard_df_cluster_new = pd.concat([df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster_new.head()","3b558b78":"# Plot the histogram of various clusters\nfor i in df.columns:\n  plt.figure(figsize = (20, 5))\n  for j in range(4):\n    plt.subplot(1,4,j+1)\n    cluster = creditcard_df_cluster_new[creditcard_df_cluster_new['cluster'] == j]\n    cluster[i].hist(bins = 20)\n    plt.title('{}    \\nCluster {} '.format(i,j))\n  \n  plt.show()","1252e990":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp_new = pca.fit_transform(pred_ac)\nprincipal_comp_new","00b66b52":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data=principal_comp_new,columns=['pca1','pca2'])\npca_df.sample(5)","867378fc":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","9a821c8e":"plt.figure(figsize=(10,10))\nplt.style.use('ggplot')\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink'])\nplt.show()","e56aa039":"#### So as per my observations I find the optimal value of clusters to be 4 as the curve seems to be linear after 4","fca7ec8f":"# TASK #6: APPLY K-MEANS METHOD","9b7b1bca":"![alt text](https:\/\/drive.google.com\/uc?id=1bLRDIZRda0NSTAdcbugasIjDjvgw4JIU)","7fdf36f4":"# TASK #5: FIND THE OPTIMAL NUMBER OF CLUSTERS USING ELBOW METHOD","5fb44fb2":"![alt text](https:\/\/drive.google.com\/uc?id=1uS6vsccMt3koetsp3k9cAIfbpJw7Z1J8)","30907ece":"# TASK #9: APPLY AUTOENCODERS (PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)","589f041b":"![alt text](https:\/\/drive.google.com\/uc?id=1EBCmP06GuRjVfPgTfH85Yhv9xIAZUj-K)","6390665b":"# TASK #3: VISUALIZE AND EXPLORE DATASET","ffd11d65":"# TASK #4: UNDERSTAND THE THEORY AND INTUITON BEHIND K-MEANS","cef86df1":"![alt text](https:\/\/drive.google.com\/uc?id=1Yfi-dpWW3keU5RLgwAT4YmQ2rfY1GxUh)","d1a2e9c3":"## So to summarize all the steps:\n#### 1. Load the data & just have a brief look at it. Try to find information(.info), use .describe. By doing so you will be able to get good understanding of the data. Try to understand all the features and what do they mean as this is very important to understand which features are the most important or which are the least important. If possible try to ask questions to the team\/person who has provided you the dataset. This step is important in a real world project. \n\n#### 2. Do some exploratory data analysis (EDA). Find missing values. Handling missing values is a critical step. You have to ask youe self this question. \n\nIs this value missing becuase it wasn't recorded or becuase it dosen't exist?\n\nIf a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\") :)\n\n#### Make some really good graphs by extracting information from the data. As a data scientist you might be asked for presentations of your work\/product. Beautiful graphs really helps a lot.\n\n#### 3. Now comes the Machine learning part. For this dataset I used unsupervised learning. Find the optimum number of clusters by using 'Elbow Method'. Apply Kmeans clustering and then use PCA dimensionality reduction technique to make a graph of your clusters.\n\n#### 4. Use Autoencoding technique to encode the original data and reduce its dimensions. Then use the reduced encoded data as a new input and follow step 3 again.","b8ea6c48":"![alt text](https:\/\/drive.google.com\/uc?id=1vMr3ouoZ6Pc1mba1mBm2eovlJ3tfE6JA)","a24099bf":"![alt text](https:\/\/drive.google.com\/uc?id=1ppL-slQPatrmHbPBEaT3-8xNH01ckoNE)","06e47d2e":"![alt text](https:\/\/drive.google.com\/uc?id=1VvqzWWY8wFGeP4cl-rVtWVOg1P6saHfZ)","cfcced96":"#### Mean balance is 1564 \n#### Balance frequency is frequently updated on average ~0.9\n#### Purchases average is 1000\n#### one off purchase average is 600\n#### Average purchases frequency is around 0.5\n#### average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low\n#### Average credit limit ~ 4500\n#### Percent of full payment is 15%\n#### Average tenure is 11 years","2b69fa8e":"\n<table>\n  <tr><td>\n    <img src=\"https:\/\/drive.google.com\/uc?id=1OjWCpwRHlCSNYaJoUUd2QGryT9CoQJ5e\"\n         alt=\"Fashion MNIST sprite\"  width=\"1000\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <b>Figure 1. Customers Segmentation\n  <\/td><\/tr>\n<\/table>\n","42e3145f":"# TASK 7: APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS","7981cab6":"### As seen that the shape of our input has reduced. Now using this as an input and repeting the whole process with the reduced input. ","fe6648df":"#### Mean of balance is 1500\n#### 'Balance_Frequency' for most customers is updated frequently ~1\n#### For 'PURCHASES_FREQUENCY', there are two distinct group of customers\n#### For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently \n#### Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0\n#### Credit limit average is around 4500\n#### Most customers are ~11 years tenure","30a6504c":"# TASK #2: IMPORT LIBRARIES AND DATASETS","2ae8ce1f":"### Comparing both the results, using Autoencoders and by not using Autoencoders","d61940ed":"![alt text](https:\/\/drive.google.com\/uc?id=1r1FjdO8duujUoI904Oy4vbza6KktxSXo)","7aa29274":"## APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS of the new encoded reduced data","f5006ea1":"![alt text](https:\/\/drive.google.com\/uc?id=1xk1D5uldId0DWywRJ3-OAVBcIr5NGCq_)","111c2f1e":"![alt text](https:\/\/drive.google.com\/uc?id=1g0tWKogvKaCrtsfzjApi6m8yGD3boy4x)","e99529eb":"![alt text](https:\/\/drive.google.com\/uc?id=1rBQziDU0pS1Fz0m8VQRjQuBoGFSX1Spb)","3786a56b":"# TASK #8: UNDERSTAND THE THEORY AND INTUITION BEHIND AUTOENCODERS","ca9fc132":"![alt text](https:\/\/drive.google.com\/uc?id=1BOX2q8R_8E4Icb4v1tpn1eymCTJY2b5o)","a33f0113":"Data Source: https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata","af48adae":"# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE","a3886b32":"## Here we can see that by using autoencoders I was able to make clusters of data with very less overlapping. This is more meaningful clustering\/segmentations of the customers. I will now be able to tell my clients that they have 4 different types of customers and each can be targeted in a different way. Autoencoding really helped in this case. ","b2445dc7":"## Apply Kmeans algorithm","8753fbd0":"![alt text](https:\/\/drive.google.com\/uc?id=1Q43AkxxDy4g-zl5lIX4_PBJtTguh4Ise)","351f8331":"![alt text](https:\/\/drive.google.com\/uc?id=1EYWyoec9Be9pYkOaJTjPooTPWgRlJ_Xz)","26c92811":"![alt text](https:\/\/drive.google.com\/uc?id=1v7hJEPiigSeTTaYo0djbO-L4uEnTpcAU)","a6672891":"![alt text](https:\/\/drive.google.com\/uc?id=1LpdL0-4E9lbc4s-x6eJ5zkyIVw_OpHuJ)","c557952d":"![alt text](https:\/\/drive.google.com\/uc?id=1AcyUL_F9zAD2--Hmyq9yTkcA9mC6-bwg)","b3aa015e":"![alt text](https:\/\/drive.google.com\/uc?id=1xDuvEnbuNqIjX5Zng39TCfGCf-BBDGf0)"}}