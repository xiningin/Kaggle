{"cell_type":{"8c47e6dd":"code","30003d0b":"code","21d8a79a":"code","0abbbc36":"code","07a4aae5":"code","a4d6ffe1":"code","b47c0f7f":"code","e6403f47":"code","1ce258d7":"code","5312398d":"code","c33d0875":"code","0583eacc":"code","d0b86413":"code","75eb0352":"code","1a859f66":"code","6ef91d3c":"code","c68be6f0":"code","ebe946b9":"code","13bfe292":"code","f7f9aaf0":"code","d8b9e473":"code","94e87375":"code","164c0b91":"code","09b6c929":"code","472f8c20":"code","d4228a5c":"code","84340d00":"code","390dde04":"code","336ad33e":"code","ccdc595d":"code","4e40c9ae":"code","c2d16e70":"code","58e5ab3d":"code","04f81241":"code","cbd6e307":"code","dcd693d7":"code","9c74ebb2":"code","4dba7b6e":"code","cc6dfebc":"code","380b54b1":"code","3041aa29":"code","0decf674":"code","bf783c93":"code","0faeeccc":"code","a1b0b1cc":"code","6e8fffc8":"code","cf0236b4":"code","10456a26":"code","374c8786":"code","499eaf10":"code","0209851d":"code","2d50c8af":"code","9932e1ee":"code","7ca1138a":"code","3aa5ad71":"code","c7d72e39":"code","52c6ff00":"code","0abb5690":"code","ed2a0154":"code","61c23cfd":"code","545c7bd7":"code","5333e393":"code","36e46148":"code","cdbe72f9":"code","d5eb563a":"code","46f1a5fb":"code","ba60b3e9":"code","0de89e01":"code","97e8ce6c":"code","4104b088":"code","9b5b50cd":"code","a6951124":"code","84ab5ad4":"code","bba19a91":"code","0a6ea3f4":"code","0b5a12c2":"code","5649c2df":"code","731e079d":"code","27d46d28":"code","23d43e46":"code","9b829b49":"code","ed9c8ed5":"code","601e0ff2":"code","e32188f5":"code","5d1d1b11":"code","333ce1b9":"code","d51cb217":"markdown","65e8e1ef":"markdown","c32213f7":"markdown","8955ed5c":"markdown","cfbe9dbb":"markdown","0606aed8":"markdown","8dda46fb":"markdown","1d66879d":"markdown","0bb30f9f":"markdown","b388225e":"markdown","548ee8ac":"markdown","6f0a5669":"markdown","5894beda":"markdown","ea2c585f":"markdown","0f852158":"markdown","c0bb21dd":"markdown","65fdd94c":"markdown","5da73cce":"markdown","4458051f":"markdown","5d134120":"markdown","3e3a9d08":"markdown","2151c44f":"markdown","a7684abe":"markdown","179da560":"markdown","4002bc2c":"markdown","5bef00b0":"markdown","cfd30e63":"markdown","f82d5af0":"markdown","220a9d57":"markdown","3d0d2c94":"markdown","836352cb":"markdown","69495c6f":"markdown","20438661":"markdown","ad8f1424":"markdown","81314240":"markdown","01ed794c":"markdown"},"source":{"8c47e6dd":"import numpy as np \nimport pandas as pd \nimport seaborn as sns                                   \nimport matplotlib as mpl\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","30003d0b":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","21d8a79a":"titanic.shape","0abbbc36":"test.shape","07a4aae5":"print('no of rows',titanic.shape[0])\nprint('no of columns',titanic.shape[1])","a4d6ffe1":"titanic.info()","b47c0f7f":"test.info()","e6403f47":"titanic.describe()","1ce258d7":"titanic.describe(include='all')","5312398d":"sns.countplot(x='Pclass',data = titanic)\n","c33d0875":"sns.countplot(x='Survived',data=titanic)","0583eacc":"sns.countplot(x='Pclass', hue='Survived',data=titanic)\n\n## hue parameter splits the graph into defined parameter\n\n","d0b86413":"sns.countplot(x='Sex',hue='Survived',data=titanic)","75eb0352":"sns.countplot(x='Embarked',hue='Survived',data=titanic)","1a859f66":"sns.violinplot(\"Pclass\",'Sex',data=titanic)\n","6ef91d3c":"sns.violinplot('Pclass',data=titanic)","c68be6f0":"sns.violinplot(x='Pclass',y='Sex',hue='Survived',data=titanic)","ebe946b9":"pd.crosstab(titanic['Sex'],titanic['Survived'])","13bfe292":"sns.factorplot('Sex','Survived',data=titanic)","f7f9aaf0":"sns.factorplot('Pclass','Survived',data=titanic)","d8b9e473":"sns.barplot(x='Pclass',y='Fare',data=titanic)","94e87375":"sns.catplot(x='Embarked',y=\"Fare\", data=titanic,kind='swarm')","164c0b91":"plt.figure(figsize=(15,6))\ncorr = titanic.corr()\n\nsns.heatmap(corr,annot=True) ","09b6c929":"sns.pairplot(titanic)","472f8c20":"titanic.isnull().sum()","d4228a5c":"titanic.groupby('SibSp')['Age'].describe()\n\n## To see null values in 'SibSp' with relative to 'Age'","84340d00":"titanic.groupby('SibSp')['Age'].median()\n","390dde04":"titanic['Age'].median()","336ad33e":"titanic['Age']=np.where((titanic['SibSp']==8) & (titanic['Age'].isnull()),28.0,titanic['Age'])\ntitanic['Age']=np.where((titanic['SibSp']==0) & (titanic['Age'].isnull()),29.0,titanic['Age'])\ntitanic['Age']=np.where((titanic['SibSp']==1) & (titanic['Age'].isnull()),30.0,titanic['Age'])\ntitanic['Age']=np.where((titanic['SibSp']==2) & (titanic['Age'].isnull()),23.0,titanic['Age'])\ntitanic['Age']=np.where((titanic['SibSp']==3) & (titanic['Age'].isnull()),10.0,titanic['Age'])","ccdc595d":"titanic.isnull().sum()\n","4e40c9ae":"titanic['Embarked'].value_counts()","c2d16e70":"titanic['Embarked'].fillna('S',inplace=True)","58e5ab3d":"titanic.head(20)","04f81241":"titanic.drop(columns=['Name','Ticket','Cabin'],inplace=True)","cbd6e307":"titanic.info()","dcd693d7":"sns.boxplot(titanic['Age'])","9c74ebb2":"sns.scatterplot('Age','Survived',data=titanic)","4dba7b6e":"sns.boxplot(titanic['Fare'])","cc6dfebc":"sns.scatterplot('Fare','Survived',data=titanic)","380b54b1":"titanic.sort_values(by='Fare',ascending=False)[:10]","3041aa29":"titanic.drop(titanic[titanic['Fare']>500].index, inplace = True)","0decf674":"sns.boxplot(titanic['Fare'])","bf783c93":"df_num = titanic.select_dtypes(include=[np.number]).copy()","0faeeccc":"df_num","a1b0b1cc":"df_cat = titanic.select_dtypes(include='object').copy()","6e8fffc8":"df_cat","cf0236b4":"df_cat = pd.get_dummies(df_cat)","10456a26":"df_cat","374c8786":"df = pd.concat([df_num,df_cat],axis=1)","499eaf10":"df","0209851d":"df.drop(columns=['Sex_male','Embarked_C','PassengerId'],inplace=True)","2d50c8af":"df.shape","9932e1ee":"X = df.drop(columns=['Survived'])\nY = df['Survived']","7ca1138a":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=.30,random_state=0)","3aa5ad71":"X_train.shape","c7d72e39":"X_test.shape","52c6ff00":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,Y_train)\nY_pred = lr.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(Y_test,Y_pred)\nprint('Accuracy is',\"{:.2f}%\".format(100*accuracy))","0abb5690":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='entropy',max_depth=15,random_state=0,max_leaf_nodes=10)\ndt.fit(X_train,Y_train)\nY_pred = dt.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_pred)\nprint('Accuracy of Decision Tree model is',\"{:.2f}%\".format(100*accuracy))","ed2a0154":"from sklearn import *\nmodel_params = {\n               'max_leaf_nodes':range(10,20),\n               'criterion':['gini','entropy'],\n               'max_depth':range(1,10),\n               'min_impurity_decrease':[0.00005,0.0001,0.0002,0.0005,0.001,0.0015,0.002,0.005,0.01]}\n\ndt_model_improved = DecisionTreeClassifier()\n\nrandom_search_object = model_selection.RandomizedSearchCV(dt_model_improved,model_params,\n                                     n_iter=10,cv=5,random_state=0)\n\n\nrandom_search_best_model = random_search_object.fit(X_train,Y_train)\n\nY_pred = random_search_best_model.predict(X_test)\n\naccuracy= accuracy_score(Y_test,Y_pred)\n\nprint('Accuracy of Decision Tree improved Model is:',\"{:.2f}%\".format(100*accuracy))","61c23cfd":"random_search_best_model.best_params_","545c7bd7":"from sklearn import *\nrf=ensemble.RandomForestClassifier(n_estimators=150,criterion='entropy',\n                                        random_state=0)\nrf.fit(X_train,Y_train)\n\nY_pred = rf.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_pred)\n\nprint('Accuracy of Random Forest Model is:',\"{:.2f}%\".format(100*accuracy))","5333e393":"model_params=  {'n_estimators':[140,145,150,155,160],\n               'max_leaf_nodes':range(10,30),\n               'criterion':['gini','entropy'],\n                'max_depth':range(1,10),\n               'min_impurity_decrease':[0.00005,0.0001,0.0002,0.0005,0.001,0.0015,0.002,0.005,0.01]}\n\nrf_model_improved = ensemble.RandomForestClassifier(random_state=0)\n\nrandom_search_object = model_selection.RandomizedSearchCV(rf_model_improved,model_params,\n                                     n_iter=10,cv=5,random_state=0)\n\nrandom_search_best_model = random_search_object.fit(X_train,Y_train)\n\nY_pred = random_search_best_model.predict(X_test)\n\naccuracy= accuracy_score(Y_test,Y_pred)\n\nprint('Accuracy of Random Forest Model is:',\"{:.2f}%\".format(100*accuracy))","36e46148":"random_search_best_model.best_params_","cdbe72f9":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3, metric='euclidean')\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\nknn.fit(X_train,Y_train)\nY_pred = knn.predict(X_test)\naccuracy= accuracy_score(Y_test,Y_pred)\nprint('Accuracy of KNN algorithm is',\"{:.2f}%\".format(100*accuracy))","d5eb563a":"from sklearn import *\n\nmodel_params = {'leaf_size':range(1,50),\n               'n_neighbors':range(1,30),\n               'p':[1,2]}\n\nknn_improved = KNeighborsClassifier()\n\ngrid_search_object = model_selection.GridSearchCV(knn_improved, model_params,cv=10)\n\ngrid_search_best_model = grid_search_object.fit(X_train,Y_train)\n\nY_pred = grid_search_best_model.predict(X_test)\n\naccuracy= accuracy_score(Y_test,Y_pred)\n\nprint('Accuracy of KNN improved Model is:',\"{:.2f}%\".format(100*accuracy))","46f1a5fb":"grid_search_best_model.best_params_\n","ba60b3e9":"## Let's Start submission process","0de89e01":"test.info()","97e8ce6c":"test.describe()","4104b088":"test['Fare'].mean()","9b5b50cd":"test['Fare'].fillna(35.6271884892086,inplace=True)","a6951124":"test.isnull().sum()","84ab5ad4":"test.groupby('SibSp')['Age'].median()","bba19a91":"test.groupby('SibSp')['Age'].describe()","0a6ea3f4":"test['Age'].isnull().value_counts()","0b5a12c2":"test['Age']=np.where((test['SibSp']==8) & (test['Age'].isnull()),14.5,test['Age'])\ntest['Age']=np.where((test['SibSp']==0) & (test['Age'].isnull()),27.0,test['Age'])\ntest['Age']=np.where((test['SibSp']==1) & (test['Age'].isnull()),30.0,test['Age'])\ntest['Age']=np.where((test['SibSp']==2) & (test['Age'].isnull()),21.0,test['Age'])\ntest['Age']=np.where((test['SibSp']==3) & (test['Age'].isnull()),28.5,test['Age'])","5649c2df":"test['Age'].isnull().value_counts()","731e079d":"test.drop(columns=['Cabin','Name','Ticket'],inplace=True)","27d46d28":"test.info()","23d43e46":"test.head()","9b829b49":"test_num = test.select_dtypes(exclude='object').copy()\ntest_dummies = test.select_dtypes(include='object').copy()\ntest_dummies = pd.get_dummies(test_dummies)","ed9c8ed5":"test_dummies.drop(columns=['Sex_male','Embarked_C'],inplace=True)\ndf_test = pd.concat([test_num,test_dummies],axis=1)","601e0ff2":"df_test.head()","e32188f5":"X_test = df_test.drop('PassengerId',axis=1)\nY_test_pred = random_search_best_model.predict(X_test)","5d1d1b11":"submission = pd.DataFrame(df_test['PassengerId'])\nsubmission['Survived'] = Y_test_pred ","333ce1b9":"submission.head()\nsubmission.to_csv('titanic_result.csv',index=False)","d51cb217":"## Improving the score using RandomSearchCV","65e8e1ef":"### Conclusion of above plot: Summary of number of Male and Female survived and not survived","c32213f7":"### In this instance we can see that we are using group by to find out in Titanic dataset value of 'SibSp' equal to 8 and 'Age' equal to \"NaN\"\n### If found out then fill the value of 28 in its place.\n### Similiarly we will do for other values also","8955ed5c":"### Conclusion of above plot: More Female survived than Male","cfbe9dbb":"### Conclusion of above plot: Male and Female in Pclass 3 did not survive the most ","0606aed8":"### Conclusion of above plot: There were more Female in Pclass 1 than Male in Pclass 1. Also there were more Male in Pclass 3 than Female in same class.  ","8dda46fb":"## ML Algorithms Start","1d66879d":"### Conclusion of above plot: Maximum people were in Pclass 3 followed by Pclass 1 and then least in Pclass 2","0bb30f9f":"## Random Forest ","b388225e":"### Conclusion of above plot: Maximum fare was of people who embarked from C then from S followed by Q","548ee8ac":"## RandomSearchCV for hyperparameter tuning","6f0a5669":"### There are few missing values and we will handle it now","5894beda":"### We will use random forest as our preffered model for score submission as we got maximum accuracy in this model i.e. 80%","ea2c585f":"### We will remove null values in 'Age' variable equilvalent to the train data.\n\n","0f852158":" ## Missing value treatment","c0bb21dd":"### Conclusion of above plot: More people died i.e. 0 and less people survived i.e. 1 in this ship wreakge","65fdd94c":"## Decision Tree","5da73cce":"### Conclusion of above plot: The no of Males deceased were more than female ","4458051f":"### Conclusion of above plot: More people were there in class 3 as compared to class 1 and class 2","5d134120":"## Dividing the numerical and categorical variables   ","3e3a9d08":"### **.info()** shows important information about the data like datatypes,column names, and number of null values.Here in this dataset we can see we have null\/missing values in '*Age*', '*Embarked*' and '*Cabin*' columns. \n","2151c44f":"### Conclusion of above plot: Pclass 1 had the highest fare then Pclass 2 followed by Pclass 3","a7684abe":"### Dropping the above found out values","179da560":"## Hyperparameter tuning is required and we will use GridsearchCV ","4002bc2c":"### Conclusion of above plot: People who embarked from S and Q were most likely to not survive and People who embarked from C had more chance of survival.","5bef00b0":"## Final Results\n### Logistic Regression     : 77.90%\n### Decision Trees          : 78.60%\n### Random Forest (improved): 80.52%\n### KNN (improved)          : 79.78%\n#### Any thoughts and suggestions are always welcome. Please do leave a comment and upvote if you liked the solution.\n\n## Thanks","cfd30e63":"### Seeing the outlier value of Fare variable\n\n","f82d5af0":"## Combining the numrical and categorical dataframes","220a9d57":"## Outlier Treatment","3d0d2c94":"## Dropping unnecessary columns","836352cb":"### Conclusion of above plot: Most of the people in class 1 survived followed by class 2 And majority of the people died were from class 3.","69495c6f":"## Logistic Regression","20438661":"## Data Visualization","ad8f1424":"### Conclusion of above plot: Most people from Pclass 1 followed by Pclass 2 and then Pclass 3 survived this ordeal","81314240":"## KNN Algorithm with scaling of Data ","01ed794c":"## Dummy variable for categorical data"}}