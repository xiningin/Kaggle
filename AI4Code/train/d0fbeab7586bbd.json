{"cell_type":{"ae292c40":"code","8537e896":"code","a6adee12":"code","26a1ce4b":"code","6e0a1b86":"code","46b2bdfa":"code","c7eab03e":"code","279400dd":"code","a2151f01":"code","0175463e":"code","73df0211":"code","fea43392":"code","94043307":"code","17901e9e":"markdown","81430b5c":"markdown","e3e07e88":"markdown","08f8f993":"markdown","e3e87a13":"markdown","ae613fc1":"markdown"},"source":{"ae292c40":"# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","8537e896":"# Import and read dataset\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(input_)\n\ndf.head(10)","a6adee12":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","26a1ce4b":"df.describe()","6e0a1b86":"x = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nmodel = GradientBoostingClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","46b2bdfa":"# Delete outlier\ndf = df[df['ejection_fraction']<70]","c7eab03e":"#inp_data = df.drop(df[['DEATH_EVENT']], axis=1)\ninp_data = df.iloc[:,[4,7,11]]\nout_data = df[['DEATH_EVENT']]\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=0, shuffle=True)\n\n## Applying Transformer\nsc= StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","279400dd":"## X_train, X_test, y_train, y_test Shape\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","a2151f01":"## I coded this method for convenience and to avoid writing the same code over and over again\n\ndef result(clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    print('Accuracy Score    : {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('GBM f1-score      : {:.4f}'.format(f1_score( y_test , y_pred)))\n    print('GBM precision     : {:.4f}'.format(precision_score(y_test, y_pred)))\n    print('GBM recall        : {:.4f}'.format(recall_score(y_test, y_pred)))\n    print(\"GBM roc auc score : {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n    print(\"\\n\",classification_report(y_pred, y_test))\n    \n    plt.figure(figsize=(6,6))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")\n    plt.title(\"GBM Confusion Matrix (Rate)\")\n    plt.show()\n    \n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                xticklabels=[\"FALSE\",\"TRUE\"],\n                yticklabels=[\"FALSE\",\"TRUE\"],\n                cbar=False)\n    plt.title(\"GBM Confusion Matrix (Number)\")\n    plt.show()\n    \ndef sample_result(\n    loss='deviance',\n    learning_rate=0.1,\n    n_estimators=100,\n    subsample=1.0,\n    criterion='friedman_mse',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_depth=3,\n    max_features=None\n):    \n    \n    scores = [] \n    for i in range(0,500): # 500 samples\n        n_estimators, max_features, max_depth, min_samples_split\n        X_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2)\n        clf = GradientBoostingClassifier(\n            loss=loss,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            subsample=subsample,\n            criterion=criterion,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            max_depth=max_depth,\n            max_features=max_features\n        ) \n        sc=StandardScaler()\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.fit_transform(X_test)\n        clf.fit(X_train, y_train)\n        scores.append(accuracy_score(clf.predict(X_test), y_test)) \n    \n    plt.hist(scores)\n    plt.show()\n    print(\"Best Score: {}\\nMean Score: {}\".format(np.max(scores), np.mean(scores)))","0175463e":"clf = GradientBoostingClassifier(random_state=0)\nresult(clf)\nsample_result()","73df0211":"param_grid = {\n    \"learning_rate\": [0.1, 0.5, 0.01],\n    \"min_samples_split\": [1,3,5],\n    \"max_depth\": [2,4,6],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.1, 0.5, 1],\n    \"n_estimators\":[500,1000]\n}\n\nclf = GradientBoostingClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=4, verbose=3, cv=5)\ngrid.fit(X_train, y_train)\ngrid.best_params_","fea43392":"clf = GradientBoostingClassifier(\n    learning_rate=0.01,\n    min_samples_split=5,\n    max_depth=2,\n    max_features='sqrt',\n    criterion='friedman_mse',\n    subsample=0.5,\n    n_estimators=500,\n    random_state=0\n)\n\nresult(clf)\nsample_result(\n    learning_rate=0.01,\n    min_samples_split=5,\n    max_depth=2,\n    max_features='sqrt',\n    criterion='friedman_mse',\n    subsample=0.5,\n    n_estimators=500\n)","94043307":"Importance = pd.DataFrame({'Importance':clf.feature_importances_*100},index=df.iloc[:,[4,7,11]].columns)\nImportance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='lightblue')\nplt.xlabel('Importance for variable');","17901e9e":"## Reporting\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n**Correctly predicted -> %93.33 (278 of 297 predict are correct)**\n- True Negative -> %68.33 (41 people) -> Those who were predicted not to die and who did not die\n- True Positive -> %25.00 (15 people) -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %6.66 (19 of 297 predict are wrong)**\n- False Positive -> %03.33 (2 people) -> Those who were predicted to die but who did not die\n- False Negative -> %03.33 (2 people) -> Those who were predicted to not die but who did die","81430b5c":"When we examine the graph above, we can predict that time, serum_creatinine, ejection_fraction  values will increase accuracy in education.","e3e07e88":"## What is GBM?\n\nLet\u2019s start by understanding Boosting! Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. The gradient boosting algorithm (gbm) can be most easily explained by first introducing the AdaBoost Algorithm.The AdaBoost Algorithm begins by training a decision tree in which each observation is assigned an equal weight. After evaluating the first tree, we increase the weights of those observations that are difficult to classify and lower the weights for those that are easy to classify. The second tree is therefore grown on this weighted data. Here, the idea is to improve upon the predictions of the first tree. Our new model is therefore Tree 1 + Tree 2. We then compute the classification error from this new 2-tree ensemble model and grow a third tree to predict the revised residuals. We repeat this process for a specified number of iterations. Subsequent trees help us to classify observations that are not well classified by the previous trees. Predictions of the final ensemble model is therefore the weighted sum of the predictions made by the previous tree models.\n\nGradient Boosting trains many models in a gradual, additive and sequential manner. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees). While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function (y=ax+b+e , e needs a special mention as it is the error term). The loss function is a measure indicating how good are model\u2019s coefficients are at fitting the underlying data. A logical understanding of loss function would depend on what we are trying to optimise. For example, if we are trying to predict the sales prices by using a regression, then the loss function would be based off the error between true and predicted house prices. Similarly, if our goal is to classify credit defaults, then the loss function would be a measure of how good our predictive model is at classifying bad loans. One of the biggest motivations of using gradient boosting is that it allows one to optimise a user specified cost function, instead of a loss function that usually offers less control and does not essentially correspond with real world applications.\n\n\n## How does GBM work?\n\nThis algorithm is based on the work of Leo Breiman.\n\nSuppose we have a regression problem where we try to predict the target values as above.\nGradient Boosting creates an \"F\" function that generates predictions in the first iteration. Calculates the difference between the estimates and the target value and creates the \"h\" function for these differences. In the second iteration, it combines the \"F\" and \"h\" functions and again calculates the difference between predictions and targets. In this way, it tries to increase the success of the \"F\" function by constantly adding on it, and therefore to reduce the difference between predictions and targets to zero.\n\nIn the image below, you can see the model's prediction in the 1st iteration as a red line on the left graph. I have also shown the difference between the predictions and the target value for each x value in the graph on the right.\n\n![](https:\/\/miro.medium.com\/max\/1125\/1*6M_WOL_-ZM9VHNbMTJ1Kqg.png)\n\nThe success of the model will increase as the iterations progress. You can see the same graph in the 10th iteration result below.\n\n![](https:\/\/miro.medium.com\/max\/1125\/1*MNKP3INB89Sft1oxlRltGQ.png)\n\nIn the 25th and 50th iteration results, you can now see that the model's prediction and target difference approaches zero. In fact, the 50th iteration results show that the model is starting to overfit a bit. To prevent overfit, it would be useful to compare the results with a separate validation set and find the appropriate number of iterations for you.\n\n![](https:\/\/miro.medium.com\/max\/1125\/1*fxlV-MohyZm-R5KKcSxgcw.png)\n\n![](https:\/\/miro.medium.com\/max\/1125\/1*vhCCMYp7G9xhDQaP2Mv4PA.png)\n\n\n## Advantages\n- Often provides predictive accuracy that cannot be beat.\n- Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.\n- No data pre-processing required - often works great with categorical and numerical values as is.\n- Handles missing data - imputation not required.\n\n## Disdvantages\n- GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.\n- Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.\n- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.\n- Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).\n\n\n## Hyperparameters\n- Number of trees: The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation.\n- Depth of trees: The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1  works well, in which case each tree is a stump consisting of a single split. More commonly, d is greater than 1 but it is unlikely d > 10 will be required.\n- Learning rate: Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called shrinkage.\n- Subsampling: Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing stochastic gradient descent. This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradient.\n\n\n\n## Tuning a GBM Model and Early Stopping\nHyperparameter tuning is especially significant for gbm modelling since they are prone to overfitting. The special process of tuning the number of iterations for an algorithm such as gbm and random forest is called \u201cEarly Stopping\u201d. Early Stopping performs model optimisation by monitoring the model\u2019s performance on a separate test data set and stopping the training procedure once the performance on the test data stops improving beyond a certain number of iterations.\n\nIt avoids overfitting by attempting to automatically select the inflection point where performance on the test dataset starts to decrease while performance on the training dataset continues to improve as the model starts to overfit. In the context of gbm, early stopping can be based either on an out of bag sample set (\u201cOOB\u201d) or cross- validation (\u201ccv\u201d). Like mentioned above, the ideal time to stop training the model is when the validation error has decreased and started to stabilise before it starts increasing due to overfitting.\n\n\n\n## Comparison with Other Models\n\n### GBM vs Random Forest\nEvery algorithm consists of two steps:\n- Producing a distribution of simple ML models on subsets of the original data.\n- Combining the distribution into one \"aggregated\" model.\n\nNow, Random Forest uses Bagging(Bootstrapped Aggregating) for sampling.\n- It aims to decrease variance not bias.\n- It is low bias- high variance model.\n- It doesn\u2019t overfit.\n- It uses parallel ensembling.\n- In final prediction, It uses simple majority vote for classification.\n\nwhile GBT uses Boosting method for sampling.\n- It aims to decrease bias not variance.\n- It is high bias-low variance algorithm.\n- It overfits.\n- It uses sequential ensembling.\n- In final prediction, It uses weighted majority vote for classification.\n\n### GBM vs Neural Net\nGradient Boosting (LGB, XGB, Catboost):\n- work well on categorical features\n- easy to tune\n- work well on small datasets like we have here\n- hard to include information from images\n- run on CPU hence 6h time\n\nNeural Net:\n- hard to include categorical features\n- work well with images and text\n- need GPU hence only 2h time\n- hard to combine different features (categorical, numerical, images)\n- not good on small datasets","08f8f993":"---\n\n## Simple Metod\nI applied GBM directly without changing anything and the result is as follows:","e3e87a13":"![Gradient Boosting Machines](https:\/\/i.ibb.co\/C7nDs5b\/Gradient-Boosting-Machines.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7 - Gradient Boosting Machines (GBM)**\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost","ae613fc1":"## Advanced Method"}}