{"cell_type":{"f4ba6e7c":"code","40631454":"code","48a32caa":"code","d89a360a":"code","00c1ccf9":"code","664d6f5a":"code","d7c2e814":"code","a2110cfe":"code","95501cfb":"code","e0aa7618":"code","6421fce9":"code","3d30753e":"code","a697cbba":"code","ec20047f":"code","6def207b":"code","5e8552a1":"code","232e9540":"code","e314fede":"code","4d009c3e":"code","00a661b3":"code","3008bc16":"markdown","41c44ec6":"markdown","4f4a18a1":"markdown","026a2680":"markdown","4e2f40e2":"markdown","667b805d":"markdown","6a8da3a6":"markdown","ab06a410":"markdown","eabe37a8":"markdown","fac0312f":"markdown","60d02335":"markdown","18b85fdf":"markdown","079598f3":"markdown","9621d268":"markdown","a78cba2b":"markdown","c984299a":"markdown","a8edd8fc":"markdown","cdbcc62a":"markdown","7be44eb5":"markdown","2f3db009":"markdown"},"source":{"f4ba6e7c":"import math\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#warnings supression\nimport warnings\nwarnings.filterwarnings('ignore')","40631454":"data = pd.read_csv (\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")\nprint(\"The data shape is : {} \".format(data.shape))\ndata.head()","48a32caa":"str1 = \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\"\nstr2 = \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"\ndata.rename(columns={str1 : \"pred1\", str2 : \"pred2\"}, inplace=True)\ndata.info()","d89a360a":"cat_columns = data.select_dtypes(include = ['object'])\nunique_values = cat_columns.nunique(dropna=False)\nprint (unique_values)","00c1ccf9":"col = np.unique(data['Attrition_Flag'].values)\nprint (col)\n# Change \"Existing customer\" to 1 and \"Attrited Customer\" to 0\ndata.loc[data['Attrition_Flag'] == 'Attrited Customer', 'Attrition_Flag'] = 0\ndata.loc[data['Attrition_Flag'] == 'Existing Customer', 'Attrition_Flag'] = 1\ndata['Attrition_Flag'] = data['Attrition_Flag'].astype(int)","664d6f5a":"col = np.unique(data['Gender'].values)\nprint (col)\n# Change \"M\" to 0 and \"F\" to 1\ndata.loc[data['Gender'] == 'M', 'Gender'] = 0\ndata.loc[data['Gender'] == 'F', 'Gender'] = 1\ndata['Gender'] = data['Gender'].astype(int)","d7c2e814":"col = np.unique(data['Education_Level'].values)\nprint (col)\n# Change 'College'=14 'Doctorate'=21 'Graduate'=16 'High School'=12 'Post-Graduate'=18 'Uneducated'=8 'Unknown'= Mode\ndata.loc[data['Education_Level'] == 'College', 'Education_Level'] = 14\ndata.loc[data['Education_Level'] == 'Doctorate', 'Education_Level'] = 21\ndata.loc[data['Education_Level'] == 'Graduate', 'Education_Level'] = 16\ndata.loc[data['Education_Level'] == 'High School', 'Education_Level'] = 12\ndata.loc[data['Education_Level'] == 'Post-Graduate', 'Education_Level'] = 18\ndata.loc[data['Education_Level'] == 'Uneducated', 'Education_Level'] = 8\ndata.loc[data['Education_Level'] == 'Unknown', 'Education_Level'] = 0 # Will be fixed later\ndata['Education_Level'] = data['Education_Level'].astype(int)","a2110cfe":"col = np.unique(data['Income_Category'].values)\nprint (col)\ndata.loc[data['Income_Category'] == 'Less than $40K', 'Income_Category'] = 30\ndata.loc[data['Income_Category'] == '$40K - $60K', 'Income_Category'] = 50\ndata.loc[data['Income_Category'] == '$60K - $80K', 'Income_Category'] = 70\ndata.loc[data['Income_Category'] == '$80K - $120K', 'Income_Category'] = 100\ndata.loc[data['Income_Category'] == '$120K +', 'Income_Category'] = 150\ndata.loc[data['Income_Category'] == 'Unknown', 'Income_Category'] = 0 # Will be fixed later\ndata['Income_Category'] = data['Income_Category'].astype(int)","95501cfb":"col = np.unique(data['Card_Category'].values)\nprint (col)\ndata.loc[data['Card_Category'] == 'Blue', 'Card_Category'] = 1\ndata.loc[data['Card_Category'] == 'Silver', 'Card_Category'] = 2\ndata.loc[data['Card_Category'] == 'Gold', 'Card_Category'] = 3\ndata.loc[data['Card_Category'] == 'Platinum', 'Card_Category'] = 4\ndata['Card_Category'] = data['Card_Category'].astype(int)","e0aa7618":"data = pd.get_dummies(data, drop_first = True)\n\ndata.info()\nprint (data.isnull().sum())","6421fce9":"col = data['Income_Category']\nmode = col.mode()[0]\ndata.loc[data['Income_Category'] == 0, 'Income_Category'] = mode\n\ncol = data['Education_Level']\nmode = col.mode()[0]\ndata.loc[data['Education_Level'] == 0, 'Education_Level'] = mode","3d30753e":"corrmat = data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\nplt.show()","a697cbba":"used_data = data.drop (['CLIENTNUM', 'pred1', 'pred2', 'Attrition_Flag'], axis=1)\nX = used_data.values \ny = data['Attrition_Flag'].values\ny2 = data['pred1'].values\ny3 = data['pred2'].values","ec20047f":"print (\"% of 1s in label:\", y.mean())","6def207b":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample (X,y)\nprint (\"After SMOTE: % of 1s in label:\", y_res.mean())","5e8552a1":"for feature in range (X.shape[1]):\n    min = X_res[:,feature].min()\n    max = X_res[:,feature].max()\n    X_res[:,feature] = (X_res[:,feature]-min) \/ (max-min)","232e9540":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split (X_res, y_res, test_size=0.2, random_state=42)\nprint(\"The training data size is : {} \".format(X_train.shape))\nprint(\"The test data size is : {} \".format(X_test.shape))","e314fede":"from sklearn.tree import DecisionTreeClassifier\n\ndct = DecisionTreeClassifier(max_depth=None)\ndct.fit(X_train,y_train)\ndct_training_score = 100*dct.score(X_train, y_train)\nprint ('Tree Depth:', dct.get_depth())\nprint ('Tree Leaves:', dct.get_n_leaves())\ndct_test_score = 100*dct.score(X_test, y_test)\nprint(\"Decision Tree accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(dct_training_score, dct_test_score))","4d009c3e":"max_d = dct.get_depth()\ndct_training_score, dct_test_score = np.zeros(max_d), np.zeros(max_d)\nfor i in range (max_d):\n  dct = DecisionTreeClassifier(max_depth=i+1)\n  dct.fit(X_train,y_train)\n  dct_training_score[i] = 100*dct.score(X_train, y_train)\n  dct_test_score[i] = 100*dct.score(X_test, y_test)\n\nprint (np.around (dct_training_score, decimals=2))  \nprint (np.around (dct_test_score, decimals=2))\nplt.plot (dct_training_score)\nplt.plot(dct_test_score)\nplt.show()","00a661b3":"features = used_data.columns\nimportances = dct.feature_importances_\nleading_indices = (-importances).argsort()[:23]\nprint (\"Leading features sorted by importance:\")\nfor i in range (21):\n    print (i+1, features[leading_indices[i]], round(100*importances[leading_indices[i]],2), '%')","3008bc16":"84% existing customers, only 16% attrited. This is not god enough. Let's use SMOTE","41c44ec6":"# Modeling\nWe are ready to model! Let's start with a decision tree calssifier","4f4a18a1":"Is the data balanced?","026a2680":"# Arranging the data\nThe last 2 columns seem to be predictions. Change their name to be more manageable","4e2f40e2":"Most of the columns are numerical but 6 are categorical.\nLet's start by taking a better look at categorical data\n","667b805d":"Split into training and test:","6a8da3a6":"Not a lot of outstanding correlations: \nThe two predictors are anti correlated to each other, and one of them  has a good correlation with the label.\nmarital status catergories are obvilusly anti-correlated\nOther than that \"Avg_Open_to_Buy is correlated with \"Credit limit\"\n\nWe will remove the client number and the two predictors and create our X and y matrices\n","ab06a410":"The optimal depth is around 7. Let's get most important tree features","eabe37a8":"Next, let's normalize the data:","fac0312f":"Attrition flag (our label) - has 2 unique values","60d02335":"Card Category - 4 unique values. There seems to be a clear scale","18b85fdf":"# Initial analysis\nGenerate a correlation map\n","079598f3":"Income category - 6 unique values. Take a representative value of each category, and the mode for \"Unknown\"","9621d268":"Finally, marital status - 4 unique values but no clear order - use Hotkey encoding","a78cba2b":"Let's start with some imports:","c984299a":"Good - all fileds are now numeric and we have no missing values\nWe just need to fix the unknown income category and education levels according to the mode","a8edd8fc":"Next - gender also has 2 unique values","cdbcc62a":"This obviously includes some overfitting. Let's see what is the optimal depth","7be44eb5":"Now let's read the data","2f3db009":"Education_Level - 7 unique values"}}