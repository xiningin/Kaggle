{"cell_type":{"3a00b33c":"code","d9e9b044":"code","388029f1":"code","1f4ab6b9":"code","775ef25f":"code","ffd892b4":"code","973ca13c":"code","69172ade":"code","5b709c16":"code","2392fc55":"code","c9048b14":"code","c969de24":"code","ddd18caf":"code","9d80caf1":"code","5eb7af84":"code","e19ac694":"code","fc065649":"code","73363b29":"code","b3a80fed":"code","f3dbaf90":"code","45a7535e":"code","6fb3ce82":"code","8d7c25d3":"code","6cb3af1c":"code","7ca0c648":"code","c0f8f00c":"code","3d836381":"code","01e7464b":"code","95ae3400":"code","7455ec4c":"code","ca57d41e":"code","66597840":"code","57ae0469":"code","08d32749":"code","c849d592":"code","0fce3d55":"code","ee018fcb":"code","f64432da":"code","35c07f32":"code","b6b57499":"code","f15f7a47":"code","68999dc0":"code","15b86056":"code","4fe6a295":"code","3f31216f":"code","80b55cc0":"code","fa446523":"code","33fb8592":"markdown","ca43f67a":"markdown","4734ad20":"markdown","49ad170c":"markdown","800c5dbf":"markdown","4b051b73":"markdown","8be6709e":"markdown","ec19786c":"markdown","7954ba77":"markdown","3d8939af":"markdown","c07a39cb":"markdown","15dd9b54":"markdown","07d46e0c":"markdown","9d1647f7":"markdown","eece2976":"markdown","a018848c":"markdown","96b8de99":"markdown","c24da3a0":"markdown","05e2f3bd":"markdown","73c5a32d":"markdown","bdcb5383":"markdown","e11ce4cb":"markdown","a55f99c0":"markdown","68134196":"markdown","c149e701":"markdown","706dc37f":"markdown","6f4b49ed":"markdown","a57b2559":"markdown","2484b854":"markdown","963319d3":"markdown","12af90f6":"markdown","e969a614":"markdown","cea3f098":"markdown","10fe4aa3":"markdown","9967c7c6":"markdown","82884497":"markdown","8c4b2421":"markdown","ef9f65b8":"markdown"},"source":{"3a00b33c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9e9b044":"# Import the necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nsns.set(color_codes=True)\n%matplotlib inline\n%config InlineBackend.figure_formats = {'png', 'retina'}\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","388029f1":"# Load the dataset\ndf=pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","1f4ab6b9":"# According to the data details, \"The PLEASE IGNORE THE LAST 2 COLUMNS (NAIVE BAYES CLAS\u2026)\" so we should delete the last 2 columns.\n# First display the column names\ndf.columns","775ef25f":"# Delete the last two columns\n# \"CLIENTNUM\" is not needed for prediction. So, let's delete it.\ndf=df.drop([\"CLIENTNUM\",\n            \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n            \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"],\n           axis = 1)","ffd892b4":"# Display first five rows\ndf.head()","973ca13c":"# Display dataset shape\ndf.shape","69172ade":"# Display descriptive statistics\ndf.describe()","5b709c16":"# Display data information\ndf.info()","2392fc55":"# Show the unique values of categorical variables\nprint(\"Attrition_Flag :\",df[\"Attrition_Flag\"].unique())\nprint(\"Gender         :\",df[\"Gender\"].unique())\nprint(\"Education_Level:\",df[\"Education_Level\"].unique())\nprint(\"Marital_Status :\",df[\"Marital_Status\"].unique())\nprint(\"Income_Category:\",df[\"Income_Category\"].unique())\nprint(\"Card_Category  :\",df[\"Card_Category\"].unique())","c9048b14":"# Convert variables with two categories into binary variables\ndf.loc[df[\"Attrition_Flag\"] == \"Existing Customer\", \"Attrition_Flag\"] = 0\ndf.loc[df[\"Attrition_Flag\"] == \"Attrited Customer\", \"Attrition_Flag\"] = 1\ndf[\"Attrition_Flag\"] = df[\"Attrition_Flag\"].astype(int)\n\ndf.loc[df[\"Gender\"] == \"F\", \"Gender\"] = 0\ndf.loc[df[\"Gender\"] == \"M\", \"Gender\"] = 1\ndf[\"Gender\"] = df[\"Gender\"].astype(int)","c969de24":"df.head()","ddd18caf":"#One hot encoding for Categorical variables\ndf = pd.get_dummies(df)\ndf.head()","9d80caf1":"# Split data into train and test Datasets\n\n# Separate the dataset into features and target\nX = df.drop([\"Attrition_Flag\"],axis=1)\ny = df[\"Attrition_Flag\"]\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, stratify=y, random_state=101)","5eb7af84":"# Scale the data\n\n# Standardize the columns the values of which are out of 0-1 range\nscaler = StandardScaler().fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","e19ac694":"# Check the data balance\n\n# Count the number of churn (=1)\ny.value_counts()","fc065649":"### Check the churn rate\nchurn_rate = (sum(df['Attrition_Flag'])\/len(df['Attrition_Flag'].index))*100\nchurn_rate","73363b29":"# Initiate the model\nbase_lm = LogisticRegression()\n# Fit the model\nbase_lm_model = base_lm.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_lm_pred=base_lm_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_lm_accuracy = accuracy_score(y_test, base_lm_pred)\nbase_lm_precision = precision_score(y_test, base_lm_pred)\nbase_lm_recall = recall_score(y_test, base_lm_pred)\nbase_lm_f1 = 2 * (base_lm_precision * base_lm_recall) \/ (base_lm_precision + base_lm_recall)\n\n# Calculate AUC score\nbase_lm_probs = base_lm.predict_proba(X_test)\nbase_lm_probs = base_lm_probs[:,1]\nbase_lm_auc = roc_auc_score(y_test, base_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_lm_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_lm_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_lm_pred))","b3a80fed":"# Initiate the model\nbase_knn = KNeighborsClassifier()\n# Fit the model\nbase_knn_model = base_knn.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_knn_pred=base_knn_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_knn_accuracy = accuracy_score(y_test, base_knn_pred)\nbase_knn_precision = precision_score(y_test, base_knn_pred)\nbase_knn_recall = recall_score(y_test, base_knn_pred)\nbase_knn_f1 = 2 * (base_knn_precision * base_knn_recall) \/ (base_knn_precision + base_knn_recall)\n\n# Calculate AUC score\nbase_knn_probs = base_knn.predict_proba(X_test)\nbase_knn_probs = base_knn_probs[:,1]\nbase_knn_auc = roc_auc_score(y_test, base_knn_probs)\n\n# Display the metrics\nprint(\"KNN Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_knn_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_knn_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_knn_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_knn_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_knn_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_knn_pred))","f3dbaf90":"# Initiate the model\nbase_svc = SVC(kernel='rbf',probability=True)\n# Fit the model\nbase_svc_model = base_svc.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_svc_pred = base_svc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_svc_accuracy = accuracy_score(y_test, base_svc_pred)\nbase_svc_precision = precision_score(y_test, base_svc_pred)\nbase_svc_recall = recall_score(y_test, base_svc_pred)\nbase_svc_f1 = 2 * (base_svc_precision * base_svc_recall) \/ (base_svc_precision + base_svc_recall)\n\n# Calculate AUC score\nbase_svc_probs = base_svc.predict_proba(X_test)\nbase_svc_probs = base_svc_probs[:,1]\nbase_svc_auc = roc_auc_score(y_test, base_svc_probs)\n\n# Display the metrics\nprint(\"SVM Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_svc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_svc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_svc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_svc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_svc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_svc_pred))","45a7535e":"# Initiate the model\nbase_tree = DecisionTreeClassifier()\n# Fit the model\nbase_tree_model = base_tree.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_tree_pred=base_tree_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_tree_accuracy = accuracy_score(y_test, base_tree_pred)\nbase_tree_precision = precision_score(y_test, base_tree_pred)\nbase_tree_recall = recall_score(y_test, base_tree_pred)\nbase_tree_f1 = 2 * (base_tree_precision * base_tree_recall) \/ (base_tree_precision + base_tree_recall)\n\n# Calculate AUC score\nbase_tree_probs = base_tree.predict_proba(X_test)\nbase_tree_probs = base_tree_probs[:,1]\nbase_tree_auc = roc_auc_score(y_test, base_tree_probs)\n\n# Display the metrics\nprint(\"Decision Tree Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_tree_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_tree_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_tree_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_tree_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_tree_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_tree_pred))","6fb3ce82":"# Initiate the model\nbase_rfc = RandomForestClassifier()\n# Fit the model\nbase_rfc_model = base_rfc.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_rfc_pred=base_rfc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_rfc_accuracy = accuracy_score(y_test, base_rfc_pred)\nbase_rfc_precision = precision_score(y_test, base_rfc_pred)\nbase_rfc_recall = recall_score(y_test, base_rfc_pred)\nbase_rfc_f1 = 2 * (base_rfc_precision * base_rfc_recall) \/ (base_rfc_precision + base_rfc_recall)\n\n# Calculate AUC score\nbase_rfc_probs = base_rfc.predict_proba(X_test)\nbase_rfc_probs = base_rfc_probs[:,1]\nbase_rfc_auc = roc_auc_score(y_test, base_rfc_probs)\n\n# Display the metrics\nprint(\"Random Forest Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_rfc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_rfc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_rfc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_rfc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_rfc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_rfc_pred))","8d7c25d3":"# Initiate the model\nbase_adb = AdaBoostClassifier()\n# Fit the model\nbase_adb_model = base_adb.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_adb_pred=base_adb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_adb_accuracy = accuracy_score(y_test, base_adb_pred)\nbase_adb_precision = precision_score(y_test, base_adb_pred)\nbase_adb_recall = recall_score(y_test, base_adb_pred)\nbase_adb_f1 = 2 * (base_adb_precision * base_adb_recall) \/ (base_adb_precision + base_adb_recall)\n\n# Calculate AUC score\nbase_adb_probs = base_adb.predict_proba(X_test)\nbase_adb_probs = base_adb_probs[:,1]\nbase_adb_auc = roc_auc_score(y_test, base_adb_probs)\n\n# Display the metrics\nprint(\"AdaBoost Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_adb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_adb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_adb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_adb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_adb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_adb_pred))","6cb3af1c":"# Initiate the model\nbase_xgb = XGBClassifier()\n# Fit the model\nbase_xgb_model = base_xgb.fit(X_train, y_train.ravel())\n# Make Predictions\nbase_xgb_pred = base_xgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nbase_xgb_accuracy = accuracy_score(y_test, base_xgb_pred)\nbase_xgb_precision = precision_score(y_test, base_xgb_pred)\nbase_xgb_recall = recall_score(y_test, base_xgb_pred)\nbase_xgb_f1 = 2 * (base_xgb_precision * base_xgb_recall) \/ (base_xgb_precision + base_xgb_recall)\n\n# Calculate AUC score\nbase_xgb_probs = base_xgb.predict_proba(X_test)\nbase_xgb_probs = base_xgb_probs[:,1]\nbase_xgb_auc = roc_auc_score(y_test, base_xgb_probs)\n\n# Display the metrics\nprint(\"XGBoost Classifier: Imbalanced Data\")\nprint(\" - Accuracy : \",'{:.3f}'.format(base_xgb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(base_xgb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(base_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(base_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(base_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,base_xgb_pred))","7ca0c648":"# Since this is an imbalanced data, apply SMOTE to the training set\nfrom imblearn.over_sampling import SMOTE\nsmote=SMOTE()\nsmote_X_train, smote_y_train = smote.fit_sample(X_train,y_train)\n\n# Check if SMOTE were properly applied\nsmote_y_train.value_counts()","c0f8f00c":"# Initiate the model\nsmote_lm = LogisticRegression()\n\n# Fit the model\nsmote_lm_model = smote_lm.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_lm_pred=smote_lm_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_lm_accuracy = accuracy_score(y_test, smote_lm_pred)\nsmote_lm_precision = precision_score(y_test, smote_lm_pred)\nsmote_lm_recall = recall_score(y_test, smote_lm_pred)\nsmote_lm_f1 = 2 * (smote_lm_precision * smote_lm_recall) \/ (smote_lm_precision + smote_lm_recall)\n\n# Calculate AUC score\nsmote_lm_probs = smote_lm.predict_proba(X_test)\nsmote_lm_probs = smote_lm_probs[:,1]\nsmote_lm_auc = roc_auc_score(y_test, smote_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_lm_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_lm_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_lm_pred))","3d836381":"# Initiate the model\nsmote_knn = KNeighborsClassifier()\n\n# Fit the model\nsmote_knn_model = smote_knn.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_knn_pred=smote_knn_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_knn_accuracy = accuracy_score(y_test, smote_knn_pred)\nsmote_knn_precision = precision_score(y_test, smote_knn_pred)\nsmote_knn_recall = recall_score(y_test, smote_knn_pred)\nsmote_knn_f1 = 2 * (smote_knn_precision * smote_knn_recall) \/ (smote_knn_precision + smote_knn_recall)\n\n# Calculate AUC score\nsmote_knn_probs = smote_knn.predict_proba(X_test)\nsmote_knn_probs = smote_knn_probs[:,1]\nsmote_knn_auc = roc_auc_score(y_test, smote_knn_probs)\n\n# Display the metrics\nprint(\"KNN Classifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_knn_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_knn_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_knn_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_knn_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_knn_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_knn_pred))","01e7464b":"# Initiate the model\nsmote_svc = SVC(kernel='rbf',probability=True)\n\n# Fit the model\nsmote_svc_model = smote_svc.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_svc_pred=smote_svc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_svc_accuracy = accuracy_score(y_test, smote_svc_pred)\nsmote_svc_precision = precision_score(y_test, smote_svc_pred)\nsmote_svc_recall = recall_score(y_test, smote_svc_pred)\nsmote_svc_f1 = 2 * (smote_svc_precision * smote_svc_recall) \/ (smote_svc_precision + smote_svc_recall)\n\n# Calculate AUC score\nsmote_svc_probs = smote_svc.predict_proba(X_test)\nsmote_svc_probs = smote_svc_probs[:,1]\nsmote_svc_auc = roc_auc_score(y_test, smote_svc_probs)\n\n# Display the metrics\nprint(\"SVM Classifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_svc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_svc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_svc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_svc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_svc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_svc_pred))","95ae3400":"# Initiate the model\nsmote_tree = DecisionTreeClassifier()\n\n# Fit the model\nsmote_tree_model = smote_tree.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_tree_pred=smote_tree_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_tree_accuracy = accuracy_score(y_test, smote_tree_pred)\nsmote_tree_precision = precision_score(y_test, smote_tree_pred)\nsmote_tree_recall = recall_score(y_test, smote_tree_pred)\nsmote_tree_f1 = 2 * (smote_tree_precision * smote_tree_recall) \/ (smote_tree_precision + smote_tree_recall)\n\n# Calculate AUC score\nsmote_tree_probs = smote_tree.predict_proba(X_test)\nsmote_tree_probs = smote_tree_probs[:,1]\nsmote_tree_auc = roc_auc_score(y_test, smote_tree_probs)\n\n# Display the metrics\nprint(\"Decision Tree Classifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_tree_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_tree_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_tree_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_tree_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_tree_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_tree_pred))","7455ec4c":"# Initiate the model\nsmote_rfc = RandomForestClassifier()\n\n# Fit the model\nsmote_rfc_model = smote_rfc.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_rfc_pred = smote_rfc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_rfc_accuracy = accuracy_score(y_test, smote_rfc_pred)\nsmote_rfc_precision = precision_score(y_test, smote_rfc_pred)\nsmote_rfc_recall = recall_score(y_test, smote_rfc_pred)\nsmote_rfc_f1 = 2 * (smote_rfc_precision * smote_rfc_recall) \/ (smote_rfc_precision + smote_rfc_recall)\n\n# Calculate AUC score\nsmote_rfc_probs = smote_rfc.predict_proba(X_test)\nsmote_rfc_probs = smote_rfc_probs[:,1]\nsmote_rfc_auc = roc_auc_score(y_test, smote_rfc_probs)\n\n# Display the metrics\nprint(\"Random Forest Classifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_rfc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_rfc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_rfc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_rfc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_rfc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_rfc_pred))","ca57d41e":"# Initiate the model\nsmote_adb = AdaBoostClassifier()\n\n# Fit the model\nsmote_adb_model = smote_adb.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_adb_pred = smote_adb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_adb_accuracy = accuracy_score(y_test, smote_adb_pred)\nsmote_adb_precision = precision_score(y_test, smote_adb_pred)\nsmote_adb_recall = recall_score(y_test, smote_adb_pred)\nsmote_adb_f1 = 2 * (smote_adb_precision * smote_adb_recall) \/ (smote_adb_precision + smote_adb_recall)\n\n# Calculate AUC score\nsmote_adb_probs = smote_adb.predict_proba(X_test)\nsmote_adb_probs = smote_adb_probs[:,1]\nsmote_adb_auc = roc_auc_score(y_test, smote_adb_probs)\n\n# Display the metrics\nprint(\"AdaBoost Classifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_adb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_adb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_adb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_adb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_adb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_adb_pred))","66597840":"# Initiate the model\nsmote_xgb = XGBClassifier()\n\n# Fit the model\nsmote_xgb_model = smote_xgb.fit(smote_X_train, smote_y_train.ravel())\n\n# Make Predictions\nsmote_xgb_pred = smote_xgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmote_xgb_accuracy = accuracy_score(y_test, smote_xgb_pred)\nsmote_xgb_precision = precision_score(y_test, smote_xgb_pred)\nsmote_xgb_recall = recall_score(y_test, smote_xgb_pred)\nsmote_xgb_f1 = 2 * (smote_xgb_precision * smote_xgb_recall) \/ (smote_xgb_precision + smote_xgb_recall)\n\n# Calculate AUC score\nsmote_xgb_probs = smote_xgb.predict_proba(X_test)\nsmote_xgb_probs = smote_xgb_probs[:,1]\nsmote_xgb_auc = roc_auc_score(y_test, smote_xgb_probs)\n\n# Display the metrics\nprint(\"XGBClassifier: SMOTE\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smote_xgb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smote_xgb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smote_xgb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smote_xgb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smote_xgb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smote_xgb_pred))","57ae0469":"# Combined SMOTE and Edited Nearest Neighbors sampling for imbalanced classification\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\n# define sampling\nsmoteenn_X_train, smoteenn_y_train=SMOTEENN(\n    enn=EditedNearestNeighbours(sampling_strategy='majority')).fit_sample(X_train,y_train)\n\n# Check if SMOTE-ENN were properly applied\nsmoteenn_y_train.value_counts()","08d32749":"# Initiate the model\nsmoteenn_lm = LogisticRegression()\n\n# Fit the model\nsmoteenn_lm_model = smoteenn_lm.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_lm_pred=smoteenn_lm_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_lm_accuracy = accuracy_score(y_test, smoteenn_lm_pred)\nsmoteenn_lm_precision = precision_score(y_test, smoteenn_lm_pred)\nsmoteenn_lm_recall = recall_score(y_test, smoteenn_lm_pred)\nsmoteenn_lm_f1 = 2 * (smoteenn_lm_precision * smoteenn_lm_recall) \/ (smoteenn_lm_precision + smoteenn_lm_recall)\n\n# Calculate AUC score\nsmoteenn_lm_probs = smoteenn_lm.predict_proba(X_test)\nsmoteenn_lm_probs = smoteenn_lm_probs[:,1]\nsmoteenn_lm_auc = roc_auc_score(y_test, smoteenn_lm_probs)\n\n# Display the metrics\nprint(\"Logistic Regression: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_lm_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_lm_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_lm_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_lm_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_lm_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_lm_pred))","c849d592":"# Initiate the model\nsmoteenn_knn = KNeighborsClassifier()\n\n# Fit the model\nsmoteenn_knn_model = smoteenn_knn.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_knn_pred=smoteenn_knn_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_knn_accuracy = accuracy_score(y_test, smoteenn_knn_pred)\nsmoteenn_knn_precision = precision_score(y_test, smoteenn_knn_pred)\nsmoteenn_knn_recall = recall_score(y_test, smoteenn_knn_pred)\nsmoteenn_knn_f1 = 2 * (smoteenn_knn_precision * smoteenn_knn_recall) \/ (smoteenn_knn_precision + smoteenn_knn_recall)\n\n# Calculate AUC score\nsmoteenn_knn_probs = smoteenn_knn.predict_proba(X_test)\nsmoteenn_knn_probs = smoteenn_knn_probs[:,1]\nsmoteenn_knn_auc = roc_auc_score(y_test, smoteenn_knn_probs)\n\n# Display the metrics\nprint(\"KNN Classifier: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_knn_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_knn_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_knn_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_knn_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_knn_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_knn_pred))","0fce3d55":"# Initiate the model\nsmoteenn_svc = SVC(kernel='rbf',probability=True)\n\n# Fit the model\nsmoteenn_svc_model = smoteenn_svc.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_svc_pred=smoteenn_svc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_svc_accuracy = accuracy_score(y_test, smoteenn_svc_pred)\nsmoteenn_svc_precision = precision_score(y_test, smoteenn_svc_pred)\nsmoteenn_svc_recall = recall_score(y_test, smoteenn_svc_pred)\nsmoteenn_svc_f1 = 2 * (smoteenn_svc_precision * smoteenn_svc_recall) \/ (smoteenn_svc_precision + smoteenn_svc_recall)\n\n# Calculate AUC score\nsmoteenn_svc_probs = smoteenn_svc.predict_proba(X_test)\nsmoteenn_svc_probs = smoteenn_svc_probs[:,1]\nsmoteenn_svc_auc = roc_auc_score(y_test, smoteenn_svc_probs)\n\n# Display the metrics\nprint(\"SVM Classifier: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_svc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_svc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_svc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_svc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_svc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_svc_pred))","ee018fcb":"# Initiate the model\nsmoteenn_tree = DecisionTreeClassifier()\n\n# Fit the model\nsmoteenn_tree_model = smoteenn_tree.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_tree_pred=smoteenn_tree_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_tree_accuracy = accuracy_score(y_test, smoteenn_tree_pred)\nsmoteenn_tree_precision = precision_score(y_test, smoteenn_tree_pred)\nsmoteenn_tree_recall = recall_score(y_test, smoteenn_tree_pred)\nsmoteenn_tree_f1 = 2 * (smoteenn_tree_precision * smoteenn_tree_recall) \/ (smoteenn_tree_precision + smoteenn_tree_recall)\n\n# Calculate AUC score\nsmoteenn_tree_probs = smoteenn_tree.predict_proba(X_test)\nsmoteenn_tree_probs = smoteenn_tree_probs[:,1]\nsmoteenn_tree_auc = roc_auc_score(y_test, smoteenn_tree_probs)\n\n# Display the metrics\nprint(\"Decision Tree Classifier: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_tree_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_tree_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_tree_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_tree_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_tree_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_tree_pred))","f64432da":"# Initiate the model\nsmoteenn_rfc = RandomForestClassifier()\n\n# Fit the model\nsmoteenn_rfc_model = smoteenn_rfc.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_rfc_pred = smoteenn_rfc_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_rfc_accuracy = accuracy_score(y_test, smoteenn_rfc_pred)\nsmoteenn_rfc_precision = precision_score(y_test, smoteenn_rfc_pred)\nsmoteenn_rfc_recall = recall_score(y_test, smoteenn_rfc_pred)\nsmoteenn_rfc_f1 = 2 * (smoteenn_rfc_precision * smoteenn_rfc_recall) \/ (smoteenn_rfc_precision + smoteenn_rfc_recall)\n\n# Calculate AUC score\nsmoteenn_rfc_probs = smoteenn_rfc.predict_proba(X_test)\nsmoteenn_rfc_probs = smoteenn_rfc_probs[:,1]\nsmoteenn_rfc_auc = roc_auc_score(y_test, smoteenn_rfc_probs)\n\n# Display the metrics\nprint(\"Random Forest Classifier: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_rfc_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_rfc_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_rfc_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_rfc_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_rfc_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_rfc_pred))","35c07f32":"# Initiate the model\nsmoteenn_adb = AdaBoostClassifier()\n\n# Fit the model\nsmoteenn_adb_model = smoteenn_adb.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n\n# Make Predictions\nsmoteenn_adb_pred = smoteenn_adb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_adb_accuracy = accuracy_score(y_test, smoteenn_adb_pred)\nsmoteenn_adb_precision = precision_score(y_test, smoteenn_adb_pred)\nsmoteenn_adb_recall = recall_score(y_test, smoteenn_adb_pred)\nsmoteenn_adb_f1 = 2 * (smoteenn_adb_precision * smoteenn_adb_recall) \/ (smoteenn_adb_precision + smoteenn_adb_recall)\n\n# Calculate AUC score\nsmoteenn_adb_probs = smoteenn_adb.predict_proba(X_test)\nsmoteenn_adb_probs = smoteenn_adb_probs[:,1]\nsmoteenn_adb_auc = roc_auc_score(y_test, smoteenn_adb_probs)\n\n# Display the metrics\nprint(\"AdaBoost Classifier: SMOTE-ENN\")\nprint(\" - Accuracy : \",'{:.3f}'.format(smoteenn_adb_accuracy))\nprint(\" - Precision: \",'{:.3f}'.format(smoteenn_adb_precision))\nprint(\" - Recall   : \",'{:.3f}'.format(smoteenn_adb_recall))\nprint(\" - F1 score : \",'{:.3f}'.format(smoteenn_adb_f1))\nprint(\" - AUC score: \",'{:.3f}'.format(smoteenn_adb_auc))\n\n# Display the confusion matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(y_test,smoteenn_adb_pred))","b6b57499":"# Initiate the model\nsmoteenn_xgb = XGBClassifier()\n# Fit the model\nsmoteenn_xgb_model =smoteenn_xgb.fit(smoteenn_X_train, smoteenn_y_train.ravel())\n# Make Predictions\nsmoteenn_xgb_pred = smoteenn_xgb_model.predict(X_test)\n\n# Calculate Accuracy, Precision, Recall, and F1 score\nsmoteenn_xgb_accuracy = accuracy_score(y_test, smoteenn_xgb_pred)\nsmoteenn_xgb_precision = precision_score(y_test, smoteenn_xgb_pred)\nsmoteenn_xgb_recall = recall_score(y_test, smoteenn_xgb_pred)\nsmoteenn_xgb_f1 = 2 * (smoteenn_xgb_precision * smoteenn_xgb_recall) \/ (smoteenn_xgb_precision + smoteenn_xgb_recall)\n\n# Calculate AUC score\nsmoteenn_xgb_probs = smoteenn_xgb.predict_proba(X_test)\nsmoteenn_xgb_probs = smoteenn_xgb_probs[:,1]\nsmoteenn_xgb_auc = roc_auc_score(y_test, smoteenn_xgb_probs)\n\n# Display the metrics\nprint(\"XGB Classifier: SMOTEENN\")\nprint(\" - Accuracy: \", smoteenn_xgb_accuracy)\nprint(\" - Precision: \", smoteenn_xgb_precision)\nprint(\" - Recall: \", smoteenn_xgb_recall)\nprint(\" - F1 score: \", smoteenn_xgb_f1)\nprint(\" - AUC score: \", smoteenn_xgb_auc)\n\n# Display the confusion matrix\nprint(confusion_matrix(y_test,smoteenn_xgb_pred))","f15f7a47":"# Create Results Table: Accuracy \npd.options.display.float_format = '{:.3f}'.format\n\nacc_list = [\n    [base_lm_accuracy, base_knn_accuracy, base_svc_accuracy, base_tree_accuracy, base_rfc_accuracy,\n     base_adb_accuracy, base_xgb_accuracy],\n    [smote_lm_accuracy, smote_knn_accuracy, smote_svc_accuracy, smote_tree_accuracy, smote_rfc_accuracy,\n     smote_adb_accuracy, smote_xgb_accuracy],\n    [smoteenn_lm_accuracy, smoteenn_knn_accuracy, smoteenn_svc_accuracy, smoteenn_tree_accuracy, \n     smoteenn_rfc_accuracy,smoteenn_adb_accuracy, smoteenn_xgb_accuracy]]\n\nacc_df = pd.DataFrame(acc_list)\nacc_df.index = ['BASE','SMOTE','SMOTE-ENN']\nacc_df.columns = ['Logistic Regression','KNN','SVM','Decision Tree','Random Forest','AdaBoost','XGBoost']   \n\n# Create Results Table: Recall\nrec_list = [\n    [base_lm_recall, base_knn_recall, base_svc_recall, base_tree_recall, base_rfc_recall,\n     base_adb_recall, base_xgb_recall],\n    [smote_lm_recall, smote_knn_recall, smote_svc_recall, smote_tree_recall, smote_rfc_recall,\n     smote_adb_recall, smote_xgb_recall],\n    [smoteenn_lm_recall, smoteenn_knn_recall, smoteenn_svc_recall, smoteenn_tree_recall, \n     smoteenn_rfc_recall,smoteenn_adb_recall, smoteenn_xgb_recall]]\n\nrec_df = pd.DataFrame(rec_list)\nrec_df.index = ['BASE','SMOTE','SMOTE-ENN']\nrec_df.columns = ['Logistic Regression','KNN','SVM','Decision Tree','Random Forest','AdaBoost','XGBoost']\n\n# Create Results Table: F1\nf1_list = [\n    [base_lm_f1, base_knn_f1, base_svc_f1, base_tree_f1, base_rfc_f1,base_adb_f1, base_xgb_f1],\n    [smote_lm_f1, smote_knn_f1, smote_svc_f1, smote_tree_f1, smote_rfc_f1,smote_adb_f1, smote_xgb_f1],\n    [smoteenn_lm_f1, smoteenn_knn_f1, smoteenn_svc_f1, smoteenn_tree_f1,smoteenn_rfc_f1,smoteenn_adb_f1, \n     smoteenn_xgb_f1]]\n\nf1_df = pd.DataFrame(f1_list)\nf1_df.index = ['BASE','SMOTE','SMOTE-ENN']\nf1_df.columns = ['Logistic Regression','KNN','SVM','Decision Tree','Random Forest','AdaBoost','XGBoost']\n\n# Create Results Table: AUC\nauc_list = [\n    [base_lm_auc, base_knn_auc, base_svc_auc, base_tree_auc, base_rfc_auc,base_adb_auc, base_xgb_auc],\n    [smote_lm_auc, smote_knn_auc, smote_svc_auc, smote_tree_auc, smote_rfc_auc,smote_adb_auc, smote_xgb_auc],\n    [smoteenn_lm_auc, smoteenn_knn_auc, smoteenn_svc_auc, smoteenn_tree_auc,smoteenn_rfc_auc,smoteenn_adb_auc, \n     smoteenn_xgb_auc]]\n\nauc_df = pd.DataFrame(auc_list)\nauc_df.index = ['BASE','SMOTE','SMOTE-ENN']\nauc_df.columns = ['Logistic Regression','KNN','SVM','Decision Tree','Random Forest','AdaBoost','XGBoost']","68999dc0":"print(\"Accuracy\")\nacc_df","15b86056":"print(\"Recall\")\nrec_df","4fe6a295":"print(\"F1 score\")\nf1_df","3f31216f":"print(\"AUC\")\nauc_df","80b55cc0":"# Import the library for ROC curve.\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nns_probs = [0 for _ in range(len(y_test))]\n\nprob_lm = base_lm_model.predict_proba(X_test)\nprob_lm = prob_lm[:,1]\n\nprob_knn = base_knn_model.predict_proba(X_test)\nprob_knn = prob_knn[:,1]\n\nprob_svc = base_svc_model.predict_proba(X_test)\nprob_svc = prob_svc[:,1]\n\nprob_tree = base_tree_model.predict_proba(X_test)\nprob_tree = prob_tree[:,1]\n\nprob_rfc = base_rfc_model.predict_proba(X_test)\nprob_rfc = prob_rfc[:,1]\n\nprob_adb = base_adb_model.predict_proba(X_test)\nprob_adb = prob_adb[:,1]\n\nprob_xgb = base_xgb_model.predict_proba(X_test)\nprob_xgb = prob_xgb[:,1]","fa446523":"ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlm_fpr, lm_tpr, _ = roc_curve(y_test, prob_lm)\nknn_fpr, knn_tpr, _ = roc_curve(y_test, prob_knn)\nsvc_fpr, svc_tpr, _ = roc_curve(y_test, prob_svc)\ntree_fpr, tree_tpr, _ = roc_curve(y_test, prob_tree)\nrfc_fpr, rfc_tpr, _ = roc_curve(y_test, prob_rfc)\nadb_fpr, adb_tpr, _ = roc_curve(y_test, prob_adb)\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, prob_xgb)\n\nplt.figure(figsize=(8, 6))\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Based on Majority')\nplt.plot(lm_fpr, lm_tpr, marker='.', label='Logistic')\nplt.plot(knn_fpr, knn_tpr, marker='.', label='KNN')\nplt.plot(svc_fpr, svc_tpr, marker='.', label='SVM')\nplt.plot(tree_fpr, tree_tpr, marker='.', label='Decision Tree')\nplt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest')\nplt.plot(adb_fpr, adb_tpr, marker='.', label='AdaBoost')\nplt.plot(xgb_fpr, xgb_tpr, marker='.', label='XGBoost')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.legend()\nplt.show()","33fb8592":"# 5. Prediction with SMOTE-ENN","ca43f67a":"- There are no missing values in this dataset. \n- Some are categorical variables. So we need to encode them.\n  - They are \"Attrition_Flag\",\"Gender\",\"Education_Level\",\"Marital_Status\",\"Income_Category\",\"Card_Category\"","4734ad20":"### 3.7. XGBoost with Imbalance Data","49ad170c":"## ROC Curve","800c5dbf":"## Perform SMOTE-ENN","4b051b73":"## 3.2. KNN with Imbalanced Data","8be6709e":"## 4.5. Random Forest Classifier with SMOTE","ec19786c":"# 2.1. Convert categorical variables to numerical","7954ba77":"# 2. Data preprocessing","3d8939af":"### 3.3. SVM with Imbalanced Data","c07a39cb":"## 5.1. Logistic Regression with SMOTE-ENN","15dd9b54":"The employee churn rate is 23.8. We can say that it is an imbalanced dataset.","07d46e0c":"## 4.6. AdaBoost Classifier with SMOTE","9d1647f7":"## 4.3. SVM with SMOTE","eece2976":"## 4. Prediction with SMOTE","a018848c":"- Baseline model achieved the highest performance.\n- Among them, XGBoost achieve the highest performance. AUC score = 0.994","96b8de99":"## 3. Prediction with Imbalanced Data: Baseline models","c24da3a0":"## 4.1. Logisgic Regression with SMOTE","05e2f3bd":"## 4.7. XGBoost Classifier with SMOTE","73c5a32d":"### 3.4. Decision Tree with Imbalanced Data","bdcb5383":"# Customer Churn Prediction with XGBoost and SMOTE-ENN","e11ce4cb":"- Customer churn prediction is a binary classification problem to be solved by supervised learning.\n- Let's use major supervised learning algorithm and compare the results.\n  - Logistic regression\n  - KNN\n  - SVM\n  - Decision Tree\n  - Random Forest\n  - AdaBoost\n  - XGBoost\n- The data is imbalanced, so apply rebalancing methods and compare their results  with those of baseline models.\n  - Baseline models: Imbalanced data\n  - SMOTE\n  - SMOTE-ENN\n- Performance measure\n  - Accuracy\n  - Recall\n  - F1 score\n  - AUC score","a55f99c0":"## 5.3. SVM Classifier with SMOTE-ENN","68134196":"SMOTE successfully made the dataset balanced","c149e701":"## 4.4. Decision Tree Classifier with SMOTE","706dc37f":"## 5.4. Decision Tree Classifier with SMOTE-ENN","6f4b49ed":"### 3.6. AdaBoost with Imbalanced Data","a57b2559":"## 5.2. KNN with SMOTE-ENN","2484b854":"# 6. Summary of Results","963319d3":"## 4.2. KNN with SMOTE","12af90f6":"## 3.1.Logistic Regression with Imbalanced Data","e969a614":"## Perform SMOTE","cea3f098":"### 3.5. Random Forest with Imbalanced Data","10fe4aa3":"## 5.7. XGBoost Classifier with SMOTE-ENN","9967c7c6":"## 2.2. Train-Test split","82884497":"## 5.5. Random Forest Classifier with SMOTE-ENN","8c4b2421":"# 1. Load and Explore Dataset","ef9f65b8":"## 5.6. AdaBoost with SMOTE-ENN"}}