{"cell_type":{"526a2421":"code","0e3083bf":"code","d777b3d4":"code","b8b10ade":"code","2746c9a4":"code","a1a37533":"code","56007437":"code","be61c3f4":"code","1933c7f2":"code","f6548b6d":"code","bedc196e":"code","75af7a2d":"code","7940cc2f":"code","8f8dc73c":"code","53587234":"code","879a3ab5":"code","a40e8166":"code","6bec7b8a":"code","41c9baee":"code","ee4deef5":"code","9905add8":"code","b636906e":"code","3ef7fa78":"code","58265f06":"code","88b2488b":"code","7e4ed1d8":"code","1c09aca0":"code","80d389ce":"code","991f248c":"code","b56c2701":"code","8ce9ecbe":"code","a6c6636e":"code","a075f891":"code","16113773":"code","1dbe8ba5":"code","52a3bb79":"code","0d409089":"code","793a311d":"code","0fa628a1":"code","08b78c6d":"code","68a70eca":"code","3646d4cb":"code","06c352d6":"code","6a1fb3be":"code","3a4ad9b9":"code","9d8603d5":"code","ad90b279":"code","0936f0fa":"code","6dc5f0ee":"code","8effbfea":"code","10a1c54b":"code","f9c73cb3":"code","9175e567":"code","b67ea6e9":"code","5a438211":"code","4f31d815":"code","e64dc861":"code","2626d1db":"code","bc55765a":"code","3c3bfb36":"code","0e011188":"code","64a70849":"code","9e894d35":"code","1d54ad05":"code","4628bc4d":"code","f4e65b1c":"code","e420a4dc":"code","beaa4bfe":"code","c525602b":"code","9214fb33":"code","db8f8408":"code","1e44839b":"code","74f9005e":"code","5f9a91fb":"code","187a0143":"code","81ce479a":"code","b4d0cd2a":"code","3748f88a":"code","9048685c":"code","7ee19a01":"code","125af657":"code","f2d776ee":"code","fcc66f73":"code","00ea12ff":"code","ccfbb336":"markdown","94b4913c":"markdown","a93ca0e7":"markdown","1a8d08ba":"markdown","157fe09f":"markdown","7357106b":"markdown","b45a4735":"markdown","bfb5aded":"markdown","5a1661cf":"markdown","46c7cabe":"markdown","437517a6":"markdown","aff8c1ef":"markdown","d72ad20a":"markdown","913bef21":"markdown","552eaa77":"markdown","2281726c":"markdown","e486e3ee":"markdown","68ea93e7":"markdown","7b458c3b":"markdown","f5cd8a33":"markdown","5c5c4d23":"markdown","cc645599":"markdown","dd663ef2":"markdown","e13b1bed":"markdown","33a179c2":"markdown","5d0bc94e":"markdown","cb5c987c":"markdown","4b507e8b":"markdown","6ded3728":"markdown","7f959e89":"markdown","c728118e":"markdown","d4f29e15":"markdown","5dbc516f":"markdown","1f9daceb":"markdown","6dfd9216":"markdown","a9315f49":"markdown","aedf87a1":"markdown","ea9ab9f6":"markdown","a0286661":"markdown","76b80ca7":"markdown","8805c1d9":"markdown","b494c4a4":"markdown","c70f91e4":"markdown","97be26ea":"markdown","e12e83f8":"markdown","6ec5e3cd":"markdown"},"source":{"526a2421":"from IPython.display import Image\nImage(url = 'https:\/\/scx2.b-cdn.net\/gfx\/news\/2016\/mountingtens.jpg')","0e3083bf":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport scipy\n%matplotlib inline","d777b3d4":"train_labels = pd.read_csv('..\/input\/richters-predictor-modeling-earthquake-damage\/train_labels.csv')\ntest = pd.read_csv('..\/input\/richters-predictor-modeling-earthquake-damage\/test_values.csv')\ntrain = pd.read_csv('..\/input\/richters-predictor-modeling-earthquake-damage\/train_values.csv')","b8b10ade":"train_labels.head()","2746c9a4":"train_labels.info()","a1a37533":"train.head()","56007437":"train.info()","be61c3f4":"res = train_labels.building_id.equals(train.building_id)\nprint(\"Statement 'all building IDs match is'\", res)","1933c7f2":"train['damage_grade'] = train_labels.damage_grade","f6548b6d":"train.describe()","bedc196e":"fig = plt.subplots(figsize = (9,5))\nsns.countplot(train.damage_grade)\nplt.show()","75af7a2d":"pd.value_counts(train.damage_grade)","7940cc2f":"train.iloc[:,[i for i in range(15, 26)]].head()","8f8dc73c":"train.iloc[:,[i for i in range(28, 39)]].head()","53587234":"train.has_secondary_use.mean()","879a3ab5":"total = 0\nfor i in range(29, 39):\n    col = train.columns[i]\n    total+=train[col].mean()\nprint(f'The sum of means of the secondary_use columns is {total}')","a40e8166":"plt.hist(train.age)\nplt.show()","6bec7b8a":"plt.hist(train.age,range=(0,175), bins = 15)\nplt.show()","41c9baee":"sns.barplot('damage_grade', 'age', data = train)","ee4deef5":"sns.barplot('damage_grade', 'land_surface_condition', data= train)","9905add8":"train.drop('building_id', inplace = True, axis = 1) #this column isn't needed","b636906e":"#14- 24 are columns containing superstructure info\nsuperstructure_cols = []\nfor i in range(14, 25):\n    superstructure_cols.append(train.columns[i])","3ef7fa78":"corr = train[superstructure_cols+['damage_grade']].corr()","58265f06":"sns.heatmap(corr)","88b2488b":"sns.barplot('damage_grade', 'has_superstructure_adobe_mud', data=train,)","7e4ed1d8":"#pearsonr correlation implies normal distribution\n#The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a\n#Pearson correlation at least as extreme as the one computed from these datasets\nscipy.stats.pearsonr(train.damage_grade, train.has_superstructure_mud_mortar_stone)","1c09aca0":"scipy.stats.pearsonr(train.damage_grade, train.has_superstructure_cement_mortar_brick)","80d389ce":"superstructure_cols = []\nfor i in range(14, 25):\n    superstructure_cols.append(train.columns[i])","991f248c":"secondary_use = []\nfor i in range(27, 37):\n    secondary_use.append(train.columns[i])","b56c2701":"corr = train[secondary_use +['damage_grade']].corr()","8ce9ecbe":"sns.heatmap(corr)","a6c6636e":"additional_num_data = []\nfor i in range(7):\n    additional_num_data.append(train.columns[i])\nadditional_num_data.append(train.columns[26])","a075f891":"corr = train[additional_num_data+['damage_grade']].corr()","16113773":"sns.heatmap(corr)","1dbe8ba5":"train.dtypes.value_counts()","52a3bb79":"print('Object data types:\\n')\n#we'll use the function later, without wanting to print anything\ndef get_obj(train, p = False):\n    obj_types = []\n    for column in train.columns:\n        if train[column].dtype == 'object': \n            if p: print(column)\n            obj_types.append(column)\n    return obj_types\nobj_types = get_obj(train, True)","0d409089":"def transform_to_int(train, obj_types):\n    #Assign dictionaries with current values and replacements for each column\n    d_lsc = {'n':0, 'o':1, 't':2}\n    d_ft = {'h':0, 'i':1, 'r':2, 'u':3, 'w':4}\n    d_rt = {'n':0, 'q':1, 'x':2}\n    d_gft = {'f':0, 'm':1, 'v':2, 'x':3, 'z':4}\n    d_oft = {'j':0, 'q':1, 's':2, 'x':3}\n    d_pos = {'j':0, 'o':1, 's':2, 't':3}\n    d_pc = {'a':0, 'c':1, 'd':2, 'f':3, 'm':4, 'n':5, 'o':6, 'q':7, 's':8, 'u':9}\n    d_los = {'a':0, 'r':1, 'v':2, 'w':3}\n    #Each positional index in replacements corresponds to the column in obj_types\n    replacements = [d_lsc, d_ft, d_rt, d_gft, d_oft, d_pos, d_pc, d_los]\n\n    #Replace using lambda Series.map(lambda)\n    for i,col in enumerate(obj_types):\n        train[col] = train[col].map(lambda a: replacements[i][a]).astype('int64')\ntransform_to_int(train, obj_types)","793a311d":"train.dtypes.value_counts()","0fa628a1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","08b78c6d":"#separate column to be predicted from the rest\ny = train.pop('damage_grade') \nx = train.copy()","68a70eca":"x_train, x_test, y_train, y_test = train_test_split(x, y)\n\nrcf = RandomForestClassifier()\nmodel = rcf.fit(x_train, y_train)\n\nmodel.score(x_test, y_test)","3646d4cb":"y_pred = model.predict(x_test)","06c352d6":"f1_score(y_test, y_pred,average='micro')","6a1fb3be":"def get_conf_matrix(y_test, y_pred):    \n    data = confusion_matrix(y_test, y_pred) #get confusion matrix\n    cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test)) #build the confusion matrix as a dataframe table\n    cm.index.name = 'Observed'\n    cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    sns.heatmap(cm, annot=True, fmt=\"d\", annot_kws={\"size\": 12}) #plot a heatmap\n    plt.title(\"Confusion Matrix\")\n    plt.show()\nget_conf_matrix(y_test, y_pred)","3a4ad9b9":"importance = pd.DataFrame({\"Feature\":list(x), \"Importance\": rcf.feature_importances_}) # build a dataframe with features and their importance\nimportance = importance.sort_values(by=\"Importance\", ascending=False) #sort by importance\nimportance","9d8603d5":"#import the fitting methods to try\nimport sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\n\nclassifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()]\n\ndef model_and_test(X, y, classifiers):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n    for model in classifiers:\n        this_model = model.__class__.__name__ #get the name of the classifier\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        \n        print(f'{this_model} f1 score:')\n        score = f1_score(y_test, y_pred,average='micro')\n        print(f'{score:.4f}')\n        print('\\n')","ad90b279":"model_and_test(x, y, classifiers)","0936f0fa":"boxplot_cols=[\"geo_level_3_id\",\"geo_level_2_id\",\"geo_level_1_id\",\"age\", \"area_percentage\", \"height_percentage\"]\nq=1\nplt.figure(figsize=(20,20))\nfor j in boxplot_cols:\n    plt.subplot(3,3,q)\n    ax=sns.boxplot(train[j].dropna())\n    plt.xlabel(j)\n    q+=1\nplt.show()","6dc5f0ee":"def remove_outliers(df, col_cutoff = 0.01, z_score = 3.5): #define a function to get rid of all outliers of the most important columns\n    important_cols = importance[importance.Importance>col_cutoff]['Feature'].tolist() #get all columns with importance > 0.01.  \n    df_new = df.copy() #init the new df\n    for col in important_cols: df_new = df_new[np.abs(scipy.stats.zscore(df_new[col]))<z_score] #removing all rows where a z-score is >3\n    return df_new","8effbfea":"df = pd.concat([x, y], axis = 1)","10a1c54b":"df_new = remove_outliers(df)","f9c73cb3":"y = df_new.pop('damage_grade')\nx=df_new","9175e567":"sns.countplot(y)","b67ea6e9":"import xgboost as xgb","5a438211":"#Bring up original data\ndef get_original():\n    df = pd.read_csv('..\/input\/richters-predictor-modeling-earthquake-damage\/train_values.csv')\n    df.drop('building_id', axis =1, inplace=True)\n    obj_types = get_obj(df)\n    transform_to_int(df, obj_types)\n    df['damage_grade'] = train_labels.damage_grade\n\n    return df\ndf = get_original()\n\n# a function that will later be used to divide dataframe into x(independent variables) and y(dependent variable)\ndef get_xy(df):\n    y = df.pop('damage_grade')\n    x= df\n    return x, y","4f31d815":"y = df.damage_grade\nx = df.drop('damage_grade', axis = 1)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)","e64dc861":"pd.value_counts(y_test) #to confirm that the original proportion of damage grades is preserved","2626d1db":"def test_model(model, removing = False, col_cutoff = 0.01, z_score = 3.5):\n    df_train = pd.concat([x_train, y_train], axis = 1) #combine them together, so outliers are simultaneously removed from both\n    if removing: df_train = remove_outliers(df_train, col_cutoff, z_score) \n    x, y =get_xy(df_train)\n    model.fit(x, y)\n\n    y_pred = model.predict(x_test)\n    print(f1_score(y_test, y_pred, average='micro'))\ntest_model(xgb.XGBRFClassifier())","bc55765a":"models = [xgb.XGBRFClassifier(), xgb.XGBClassifier()]\nfor model in models:\n    print(model.__class__.__name__, 'score:',end =' ')\n    test_model(model, True)","3c3bfb36":"xgbc = xgb.XGBClassifier()","0e011188":"'''\nxgbc = xgb.XGBClassifier() #init xgbc \nfor a in [0.01, 0.02, 0.05]:\n    for b in [2.5, 3, 3.5]:\n        print('removing outliers on columns with importance >,',a,'on z scores >',b,'. Score =', end=' ')\n        test_model(xgbc, True, a, b) '''","64a70849":"'''for b in [2.5, 3, 3.5]:\n    print('removing outliers on columns with importance > 0.1,','on z scores >',b,'. Score =', end=' ')\n    test_model(xgbc, True, 0.1, b)'''","9e894d35":"print('No outlier removal score:', end = ' ')\ntest_model(xgbc, False)","1d54ad05":"x, y = get_xy(df)","4628bc4d":"df_train = pd.concat([x, y], axis = 1) #combine them together, so outliers are simultaneously removed from both x and y\ndf_train = remove_outliers(df_train, 0.1, 3)\nx, y =get_xy(df_train)","f4e65b1c":"xgbc = xgb.XGBRFClassifier()","e420a4dc":"parameters = {'max_depth' : [5, 10, 20, 40]} #first looking for an optimal max_depth","beaa4bfe":"from sklearn.model_selection import GridSearchCV\n#grid search cv tries all the parameters individually using cross validation, default set to 5 folds\ngrid_search = GridSearchCV(xgbc, parameters, scoring=\"f1_micro\", n_jobs=-1, verbose=3)\n# grid_result = grid_search.fit(x, label_encoded_y)","c525602b":"def plot_score(grid_result, parameters, name):    \n    means = grid_result.cv_results_['mean_test_score'] #get the means of the scores from the 5 folds\n    stds = grid_result.cv_results_['std_test_score'] #standard error of scores for plotting error bars\n\n    # plot scores vs parameter\n    plt.errorbar(parameters[name], means, yerr=stds)\n    pyplot.xlabel(name)\n    pyplot.ylabel('f1 score')\n#plot_score(grid_result,parameters, 'max_depth')","9214fb33":"Image(url = 'https:\/\/imgur.com\/16Lya4M.png')","db8f8408":"xgbc = xgb.XGBRFClassifier(max_depth = 20)","1e44839b":"n_estimators = [50, 100, 150, 200]\nparam2 = {'n_estimators':n_estimators}","74f9005e":"grid_search_estimators = GridSearchCV(xgbc, param2, scoring=\"f1_micro\", n_jobs=-1, verbose=3)\n# grid_result_estimators = grid_search.fit(x, label_encoded_y)","5f9a91fb":"# plot_score(grid_result_estimators,param2, 'n_estimators')","187a0143":"Image(url = 'https:\/\/i.imgur.com\/FwgVGgk.png')","81ce479a":"xgbc = xgb.XGBClassifier(max_depth = 20, n_estimators = 150)","b4d0cd2a":"params={\n \"learning_rate\"    : [0.1, 0.2, 0.3] ,\n \"min_child_weight\" : [ 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.05, 0.1, 0.2 , 0.3],\n \"colsample_bylevel\" :[0.2, 0.5, 0.8, 1.0],\n \"colsample_bynode\": [0.2, 0.5, 0.8, 1.0],\n \"subsample\": [0.2, 0.5, 0.8, 1.0],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    }","3748f88a":"from sklearn.model_selection import RandomizedSearchCV\nrand_search = RandomizedSearchCV(xgbc,param_distributions=params ,n_iter=10,scoring='f1_micro',n_jobs=-1, verbose = 3)","9048685c":"# rand_res = rand_search.fit(x, y)","7ee19a01":"# best_params = rand_res.best_params_","125af657":"best_params = {'subsample': 0.8,\n 'min_child_weight': 5,\n 'learning_rate': 0.1,\n 'gamma': 0.05,\n 'colsample_bytree': 0.3,\n 'colsample_bynode': 0.8,\n 'colsample_bylevel': 0.8}","f2d776ee":"xgbc = xgb.XGBClassifier( min_child_weight= 5, learning_rate= 0.1, gamma= 0.05, subsample= 0.8,colsample_bytree= 0.3, colsample_bynode= 0.8,\n colsample_bylevel= 0.8, max_depth = 20, n_estimators = 150)\n\n# xgbc.fit(x, y) #final model","fcc66f73":"def submit_model(model, file_name): #I defined a function because I was submitting multiple models\n    test = pd.read_csv('..\/input\/richters-predictor-modeling-earthquake-damage\/test_values.csv') #get the test csv into a dataframe\n    submission_ids = test.pop('building_id') #get the building ids\n    transform_to_int(test, get_obj(test)) #transform obj_types to int to predict damage grades\n    submission_predictions = model.predict(test) #predict\n    subbmission = pd.DataFrame({'building_id':submission_ids, 'damage_grade':submission_predictions}) #save buildings_ids and predicted damage grades to a data frame\n    subbmission.to_csv(file_name, index = False) #save as a csv file","00ea12ff":"# submit_model(xgbc, 'submission_xgb4.csv')\n#0.7477 score","ccfbb336":"removing outliers on columns with importance > 0.1, on z scores > 2.5 . Score = 0.7273320287018918\n\nremoving outliers on columns with importance > 0.1, on z scores > 3 . Score = 0.7285982886305207\n\nremoving outliers on columns with importance > 0.1, on z scores > 3.5 . Score = 0.727600629292813","94b4913c":"This is slightly larger than the previous mean, so there is a very small number of buildings with multiple secondary uses","a93ca0e7":"Z scores can be used instead to filter data, with the risk however of removing some useful data points. To avoid losing useful data, the importance data-frame will be used in order to only drop rows which contain outliers for relevant variables.","1a8d08ba":"Before moving on with outlier removal, let's look at a generally better method of predicting tabular data: XGBoost.\n\nXGradientBoost is an ensemble learning method built on top of Decision Trees, imporving upon other Gradient boosting methods.\nMore details can be found here https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html","157fe09f":"The majority, >50% buildings were labeled with damage_grade = 2, while only ~25,000, <10% buildings were labeled as mildly damaged (damage_grade = 1)","7357106b":"Random Forest Classifier outputs the best prediction, followed by knn  ","b45a4735":"A ~0.02 f1 score improvement has been achieved. It is now time to fit the test data and submit it.","bfb5aded":"Not only is no improvement observed following outlier removal, but the score decreased by ~0.001.\n\nHowever, XGBClassifier is giving a much better score, even better than rand forest classifier, so it will be the chosen model for this submission.\nLet's test a few general ways of removing outliers:","5a1661cf":"No outliers for the geolocation data, however there are some for the age and normalized area\/height of the building footprint","46c7cabe":"In order to further improve the model, some of the following steps can be taken:\n<ul>\n    <li> Assesing the relation between outliers of specific variables such as age or height percentage and damage grade<\/li>\n    <li> Grouping some of the variables together to create one which correlates more strongly with damage grade. Some candidates to consider include:\n        <ol>  <li> superstructure parameters <\/li>\n            <li> secondary_use parameters <\/li>\n            <li> other possibly correlated variables, such as ground_floor_type and superstructure used <\/li>\n        <\/ol>  \n    <\/li>\n    <li> Further tuning the hyper-parameters; this may however come at a significant computational cost <\/li>\n<\/ul>","437517a6":"Mud mortar stone homes were the worse, being strongly correlated with a higher damage grade, while cement mortar brick were least damaged","aff8c1ef":"Some of the superstrcture values are strongly positively or negatively correlated between them. For example, buildings made from timber are also largely composed of bamboo. ","d72ad20a":"F1 score will be used to assess the model accuracy for this competiton\nF1 score combines accuracy and precision. As there are 3 possible labels, micro averaged F1 score is used.","913bef21":"Given the size of our dataset, ~ 260,000 samples, considering all variables with z scores > 3, as outliers, corresponding to 0.27% percentile, might be removing some useful data. \n\nA z score of 3.5, corresponding with the 0.0465% could also be good enough to remove outliers, while preserving more samples. This way, the original distrbituion between damage grades may be better preserved too.","552eaa77":"# Earthquake damage prediction\n\n\nThe result of this prediction was submitted for the competition hosted at https:\/\/www.drivendata.org\/competitions\/57\/nepal-earthquake.\n\nAs of 10\/07, it was ranked 112.","2281726c":"The mean damage grade is ~2.38, with a standard deviation fo 0.6, indicating that most buildings were severly damaged","e486e3ee":"Currently the best result is achieved when leaving all the data in. However, it is likely that following hyper-parameter tuning, a superior result can be achieved by dropping some outliers. Since dropping outliers on columns with importance >0.1, using a z score of 3 achieved the best result, that is what will be used. \n\nIt will only drop on outliers from area and height percentage, ensuring no important rows are dropped, and the original distribution between damage grades is maintained. ","68ea93e7":"Let's see what accuracy we can get with the raw data, and then we will try to modify it and decide on the best models\n\nFirst let's change the object type columns to int","7b458c3b":"removing outliers on columns with importance >, 0.01 on z scores > 2.5 . Score = 0.6931813821418978\n\nremoving outliers on columns with importance >, 0.01 on z scores > 3 . Score = 0.7232262768120948\n\nremoving outliers on columns with importance >, 0.01 on z scores > 3.5 . Score = 0.7284831740915544\n\nremoving outliers on columns with importance >, 0.02 on z scores > 2.5 . Score = 0.7151682590844557\n\nremoving outliers on columns with importance >, 0.02 on z scores > 3 . Score = 0.7249913664095775\n\nremoving outliers on columns with importance >, 0.02 on z scores > 3.5 . Score = 0.7272936571889029\n\nremoving outliers on columns with importance >, 0.05 on z scores > 2.5 . Score = 0.72460765127969\n\nremoving outliers on columns with importance >, 0.05 on z scores > 3 . Score = 0.7252599670004989\n\nremoving outliers on columns with importance >, 0.05 on z scores > 3.5 . Score = 0.7273704002148804","f5cd8a33":"The current settings are not very good, they are removing too many columns with damage_grade = 1, so a milder method of removing outliers is required","5c5c4d23":"Close to no relation between seconday use and damage grade","cc645599":"Moreover, there are no null values. Let's just check that all Building IDs from the train_labels dataset match those of train_values","dd663ef2":"Hyper-parameter tuning is a crucial part of ML if we want to find the best model.\nFirst, testing is performed to find an optimal max depth and estimator numbers.\n\nMax depth represents how deep the decision trees in the model will be, while number of estimators indicates how many weak lernears(decision trees) are to be used.\n\nGenerally, the larger these are, the better the accuracy of the model is, however going too deep may lead to overfitting and consuming too much computational power for a negligible score increase in return.","e13b1bed":"Let's examine superstructure info","33a179c2":"Before we commence cleaning the data, let's have a look at the f1 scores given by some other models:","5d0bc94e":"We can see just from the first 5 rows, that some buildings have superstructures from multiple materials. We'll leave the columns as such for now, however there could be some merit in trying to group them together in some way","cb5c987c":"Let's see whether there are buildings with multiple secondary uses by summing up the means and see whether it's equal to the has_secondary_use mean","4b507e8b":"One way to improve model accuracy is to remove outliers. Plotting boxplots for the most important parameters could help visualize how these look like","6ded3728":"The best result at max_depth = 20.","7f959e89":"Best result at n_estimators = 150\n\nWe could try to further narrow down the optimal max-depth and n_estimators, however it is taking too long. \n\nWhile we have optimised two important parameters, there are many more xgboost hyper-parameters that can optimised. \nIn order to test many at once, randomized search cross-validation is employed. It randomly tests a given number of iterations from the given params and finds the ones outputting the best score.","c728118e":"Younger buildings seem more likely to have been less likely affected; however there's no mean age difference between medium and severly affected buildings","d4f29e15":"Get confusion matrix","5dbc516f":"The geographic location has the highest impact","1f9daceb":"Thus, the damage grade column can simply be added on to the train values data frame","6dfd9216":"### In this kernel an overall view of the Nepal Earthquake dataset is provided.\n\n### Models are trained using Random Forest Classification and XGBoost Classification methods. The f1_score is maximised by removing outliers and tuning xgboost hyper-parameters. ","a9315f49":"The model is overestimating the buildings with damage_grade = 2 : 37,201 observed and 42,243 predicted: 5,042 buildings more, or ~13.5%.\n\nIt underestimates the other two damage ranks. Damage_grade = 1 is predicted the worst, out of 6,170 observed, only 4,625 were predicted, 25% less. Moreover, the model correctly predicted only 3,015 buildings, having an accuracy of <50% for damage grade=1, as opposed to an accuracy of 30,724\/37,201 * 100 ~= 82.5% \n\nWhile this doesn't come as a big surprise given the damage grades distribution, it indicates that focusing on refining damage_grade = 1 predictions may lead to significant f1 score improvement","aedf87a1":"Credit: Roger Bilham\/CIRES","ea9ab9f6":"All columns are now integer types","a0286661":"The damage_grade column, which is the variable we're trying to predict, comes in a different file:'train_labels' Fortunately, it has the same length and indexes as the 'train_values' file","76b80ca7":"Some small correlation between land surface condition and damage grade","8805c1d9":"I'll be commenting out some of the code, as it takes quite some time to run the. The outputs are copied in the kernel","b494c4a4":"Most buildings are aged 0-50, there is a small cluster of ancient outliers that were labeled as being 995 years old","c70f91e4":"I hope you enjoyed this project !!\n\nI'd be more than happy to respond to any comments\/suggestions you may have !","97be26ea":"The feature importance dataframe helps us focus on relevant variables","e12e83f8":"Let's start by running a RandomForestClassifier on the data","6ec5e3cd":"Let's have a look at the superstrctures"}}