{"cell_type":{"54ae437b":"code","d17a8f92":"code","20fef8e5":"code","e8c9bff7":"code","5e359ba5":"code","f0582c31":"code","2f1ceb05":"code","f0748086":"code","0956ba42":"code","146fd6d6":"code","d0c116b8":"code","b8c63fb6":"code","080632e0":"code","f5c85bba":"markdown","d65344a4":"markdown","849dfbb4":"markdown","5535917d":"markdown","20e5bc31":"markdown","d5bf1567":"markdown","4040038b":"markdown","ecbce9f4":"markdown","d60bd02f":"markdown","7cd61df3":"markdown","6da143d2":"markdown","957faa4a":"markdown","f981153c":"markdown","a356b1d6":"markdown","37683e5d":"markdown","7abb8bac":"markdown","cf097b6d":"markdown","95b5a356":"markdown","3e9a234a":"markdown"},"source":{"54ae437b":"#----------------------------Preprocessing\n#WORKING USING THIS\nimport tensorflow as tf\nimport cv2\nimport numpy as np \nimport math as mt\nimport cv2\nimport pylab as p_\nimport matplotlib.cm as cm\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np \nimport math as mt\nimport cv2\nimport pylab as p_\nimport matplotlib.cm as cm\nfrom keras.datasets import mnist\n\n \n# downloading dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n\n#flattening training set vectors to feed into neural network\nX_train_new=np.zeros(shape=(60000,784),dtype='float64')\nX_train_2d=np.zeros(shape=(784),dtype='float64')\n\nfor i in range(60000):\n  if(i%10000==0):\n    print(i)\n  X_train_2d=X_train[i].flatten()\n  X_train_new[i]=X_train_2d\n\n\n#flattening testing set vectors to feed into neural network\nX_test_new=np.zeros(shape=(10000,784),dtype='float64')\nX_test_2d=np.zeros(shape=(784),dtype='float64')\n\nfor i in range(10000):\n  if(i%10000==0):\n    print(i)\n  X_test_2d=X_test[i].flatten()\n  X_test_new[i]=X_test_2d\n\n\nprint(X_test_new.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X_train_new.shape)\n\nX_TRAIN=X_train_new\nY_TRAIN=y_train\nX_TEST=X_test_new\nY_TEST=y_test\n\n\n#adding noise of standard deviation 50\n#factor which is multiplied is standard_deviation \n\nX_TRAIN_NOISY=X_TRAIN+50*np.random.normal(0,1,size=X_TRAIN.shape)\nX_TEST_NOISY=X_TEST+50*np.random.normal(0,1,size=X_TEST.shape)\n\n\n#to make sure values are in (0,1) range\n\nX_TRAIN_NOISY=X_TRAIN_NOISY\/255.0\nX_TEST_NOISY=X_TEST_NOISY\/255.0\nX_TEST=X_TEST\/255.0\nX_TRAIN=X_TRAIN\/255.0\n\nX_TRAIN_NOISY=np.clip(X_TRAIN_NOISY,0,1)\nX_TEST_NOISY=np.clip(X_TEST_NOISY,0,1)\n\n","d17a8f92":"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nactual_image_scaled_train=np.zeros((60000,784))\nnoisy_image_scaled_train=np.zeros((60000,784))\nfor i in range(60000):\n  scaler=MinMaxScaler(feature_range=(0,1))\n  actual_image_scaled_train[i]=np.reshape(scaler.fit_transform(X=np.reshape(X_TRAIN[i],(-1,1))),(784))\n\n\nfor i in range(60000):\n  scaler=MinMaxScaler(feature_range=(0,1))\n  noisy_image_scaled_train[i]=np.reshape(scaler.fit_transform(X=np.reshape(X_TRAIN_NOISY[i],(-1,1))),(784))\n\nprint(actual_image_scaled_train.shape)\nprint(noisy_image_scaled_train.shape)","20fef8e5":"print(actual_image_scaled_train.shape)\nprint(np.max(actual_image_scaled_train))\nprint(np.min(actual_image_scaled_train))\n\nprint(noisy_image_scaled_train.shape)\nprint(np.max(noisy_image_scaled_train))\nprint(np.min(noisy_image_scaled_train))","e8c9bff7":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(noisy_image_scaled_train[5],(28,28)),cmap='gray')","5e359ba5":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(actual_image_scaled_train[5],(28,28)),cmap='gray')","f0582c31":"X=noisy_image_scaled_train\nY=actual_image_scaled_train\nprint(X.shape)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\nprint(X_test.shape)\n","2f1ceb05":"from skimage.metrics import mean_squared_error\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\n\nhidden_layer=100\naccuracies_final=[]\n\nmse_observations=[]\n\nmse_epoch=[]\n\nlearning_rate=0.06 # learning_rate\nepochs=500 # no. of epochs\nn_x=784 # no. of inputs\nn_h_1=hidden_layer # no. of nodes in Hidden Layer\nbatch_size=2048 \ndigits=784 # no. of nodes in Output Layer\nlmbda=0.01 # Regularization Parameter\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n\n# initialization\nparams = {\"W1\": tf.random.normal([n_x,n_h_1],0,1,tf.float64,seed=1),# normal distribution of mean 0 and variance 1 with dimension n_h_1 * n_x (no. of weights = no. of links)\n          \"b1\": tf.random.normal([1,n_h_1],dtype=tf.float64),\n          \"W2\": tf.random.normal([n_h_1,digits],0,1,tf.float64,seed=1),   # normal distribution of mean 0 and variance 1 with dimension n_h_2 * n_h_1 (no. of weights = no. of links)\n          \"b2\": tf.random.normal([1,digits],dtype=tf.float64)}   # normal distribution of mean 0 and variance 1 with dimension n_h_2 * n_h_1 (no. of weights = no. of links)\n\n\ndef feed_forward(X, params):\n    \"\"\"\n    feed forward network: 2 - layer neural net\n\n    inputs:\n        params: dictionay a dictionary contains all the weights and biases\n\n    return:\n        cache: dictionay a dictionary contains all the fully connected units and activations\n    \"\"\"\n    cache = {}\n\n    # Z1 = W1.dot(x) + b1\n    cache[\"Z1\"] = tf.matmul(X.astype('float64'),params[\"W1\"]) + params[\"b1\"]\n\n    # A1 = RELU(Z1)\n    cache[\"A1\"] = tf.nn.relu(cache[\"Z1\"])\n\n    # Z2 = W2.dot(A1) + b2\n    cache[\"Z2\"] = tf.matmul(cache[\"A1\"],params[\"W2\"]) + params[\"b2\"]\n\n    # A2 = SIGMOID(Z2)\n    cache[\"A2\"] =tf.nn.sigmoid(cache[\"Z2\"])\n    \n    #The final output is stored in cache dictionary with key \"A2\"\n    return cache\n\n\n\ndef back_propagate(X, Y, params, cache, m_batch):\n    \"\"\"\n    back propagation\n\n    inputs:\n        params: dictionay a dictionary contains all the weights and biases\n        cache: dictionay a dictionary contains all the fully connected units and activations\n\n    return:\n        grads: dictionay a dictionary contains the gradients of corresponding weights and biases\n    \"\"\"\n\n    #(Derivative of Loss function * Derivative of Sigmoid)\n    dZ2 = (cache[\"A2\"] - Y)  * (cache[\"A2\"]) * (1-cache[\"A2\"])\n\n    # gradients for Weight Matrices present in Decoder Section\n    dW2 = (1 \/ m_batch) * tf.matmul(tf.transpose(dZ2), cache[\"A1\"])\n    db2 = (1. \/ m_batch) * tf.reduce_sum(dZ2, axis=0, keepdims=True)\n\n    # back propgate to the Hidden layer\n    dA1 = tf.transpose(tf.matmul(params[\"W2\"], tf.transpose(dZ2)))\n    \n    # Derivative of RELU is used here\n    dZ1 = dA1 * np.where(cache[\"A1\"]>0,1,0.001)\n\n\n    # gradients for Weight Matrices present in Encoder Section\n    dW1 = (1 \/ m_batch) * tf.matmul(tf.transpose(dZ1), X.astype('float64'))\n    db1 = (1. \/ m_batch) * tf.reduce_sum(dZ1, axis=0, keepdims=True)\n\n    dW1=tf.transpose(dW1)\n    dW2=tf.transpose(dW2)\n\n    \n    # Regulaization performed\n    dW1=tf.add(dW1 ,lmbda*params[\"W1\"]);\n    dW2=tf.add(dW2 ,lmbda*params[\"W2\"]);\n    \n    \n    grads = {\"dW1\": dW1, \"dW2\": dW2,\n            \"db1\": db1, \"db2\": db2}\n\n    return grads\n\n\n\n\nacc=[]\n# training\nfor k in range(epochs):\n\n    batches=X_train.shape[0]\/\/batch_size;\n\n    for j in range(batches):\n\n        # get mini-batch\n        begin = j * batch_size\n        end = min(begin + batch_size, X_train.shape[0] - 1)\n        \n        \n        X = X_train[begin:end,:]\n        Y = y_train[begin:end,:]\n        m_batch = end - begin\n        \n        \n        #forward propagation\n        cache = feed_forward(X, params)\n        #backward propagation\n        grads = back_propagate(X, Y, params, cache, m_batch)\n\n\n\n        # gradient descent , Weights and biases updation\n        params[\"W1\"] = params[\"W1\"] - learning_rate * grads[\"dW1\"]\n        params[\"b1\"] = params[\"b1\"] - learning_rate * grads[\"db1\"]\n        params[\"W2\"] = params[\"W2\"] - learning_rate * grads[\"dW2\"]\n        params[\"b2\"] = params[\"b2\"] - learning_rate * grads[\"db2\"]\n\n    # forward pass on training set\n    cache = feed_forward(X_train, params)\n    y_eval=cache[\"A2\"];\n    mse_epoch.append(mean_squared_error(y_train,y_eval))\n    print(\"epoch \",(k+1),\"train_loss = \",mse_epoch[k])\n    \n    # forward pass on testing set\n    cache = feed_forward(X_test, params)\n    y_pred=cache[\"A2\"];\n    print(\"epoch \",(k+1),\"test_loss = \",mean_squared_error(y_test,y_pred))\n\n#making final train and test predictions\ncache = feed_forward(X_train, params)\ny_eval=cache[\"A2\"];\ncache = feed_forward(X_test, params)\ny_pred=cache[\"A2\"];\n\nmse_observations.append((mean_squared_error(y_test,y_pred),mean_squared_error(y_train,y_eval)))\nprint(mse_observations)","f0748086":"cache = feed_forward(X_test, params)\ny_pred=cache[\"A2\"];","0956ba42":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(X_test[5],(28,28)),cmap='gray')","146fd6d6":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(y_test[5],(28,28)),cmap='gray')","d0c116b8":"#loss 0.14\nimport matplotlib.pyplot as plt\nplt.imshow(np.reshape(y_pred[5],(28,28)),cmap='gray')","b8c63fb6":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(y_pred[5],(28,28)),cmap='gray')","080632e0":"import matplotlib.pyplot as plt\nplt.imshow(np.reshape(y_pred[5],(28,28)),cmap='gray')","f5c85bba":"## Woah! Finally we get the Denoised Image\n\n## Congratulations! You just learnt how to design an Auto-Encoder from Scratch.","d65344a4":"# Prediction on Test Set\n\nFinally, predicting Denoised Images on Test Set","849dfbb4":"Here we execute several trials to arrive at the final tuned model of learning_rate 0.06 and no. of epochs as 500\n\nBut we see how the images are formed for mean_square_error values of 0.14 , 0.11 and 0.01. It can be observed that how it is turning into the final denoised image of figure '9'","5535917d":"Epochs=100\nLearning_Rate=0.06\nLoss=0.11","20e5bc31":"# Motivation behind Developing code from scratch\n\nWe get to know the Mathematics behind what is going on in the neural networks. Both the forward and backpropagation are implemented from scratch.","d5bf1567":"Epochs=100\nLearning_Rate=0.01\nLoss=0.14","4040038b":"Noisy Image sample","ecbce9f4":"# Image Visualization\n\nVisualizing Noisy and Original Image","d60bd02f":"# Pre-processing\n\nWe scale the data using the MinMaxScaler function to make sure values are in the range (0,1)\nBy doing this, the neural networks are trained fast and efficiently","7cd61df3":"Epochs=500\nLearning_Rate=0.06\nLoss=0.01","6da143d2":"# Observations","957faa4a":"# Data Split\n\nSplit the Train and Test data in a ratio of 7:3","f981153c":"![Basic Idea of a Denoising Autoencoder](https:\/\/drive.google.com\/uc?export=view&id=1Dh83zJlxBFbWXnigZeCYQc8iHCGd1o0z)\n\nImage is taken from https:\/\/www.pyimagesearch.com\/2020\/02\/24\/denoising-autoencoders-with-keras-tensorflow-and-deep-learning\/","a356b1d6":"Cross-checking of maximum and minimum values","37683e5d":"The target Original Image","7abb8bac":"# Problem Statement: \n\n* We are given MNIST Images. After adding Noise to it, denoised image needs to be retrieved\n\n* An Autoencoder can be used to teach the model, how to denoise the image by taking as Input the Noisy Image while training against the Original Image (Target).","cf097b6d":"# Preparing the Data\n\n* Our first job is to prepare the Data\n\n* Downloading the data will give the 60000 training and 10000 test images of dimension (28,28) each\n\n* As we will use Artifical Neural Network concept, the images are flattened into 1D vectors of dimension (784)\n\n* Noise of standard deviation 50 is added with mean 0\n\n* After getting 1D vectors, we divide all the training and testing vectors by 255 to normalize it.\n\n* Lastly we clip the negative values, if any.","95b5a356":"### ^^^ UPVOTE the Notebook if you really like it ! ^^^","3e9a234a":"# Architecture\n\n![Auto-Encoder-Architecture](https:\/\/drive.google.com\/uc?export=view&id=1ym561d5y7XU09YqPcx04G1WnfNq8ahOq)\n\nImage is taken from https:\/\/medium.com\/machine-learning-researcher\/auto-encoder-d942a29c9807\n\nThe Architecture which I have designed has the Size of Input and Output Layer as 784 while that of Hidden-Layer is kept as 100\n\nThus, there will be two Matrices both for Weights and Bias.\n\n## Usage of Activation functions:\n\nIn the first layer we use RELU, while SIGMOID is used in the final layer to retrieve the output."}}