{"cell_type":{"c5917c9e":"code","13d3ced3":"code","b0855b6e":"code","e3dc5598":"code","25c2fb77":"code","e884d329":"code","a921b8c0":"code","3b5149f3":"code","fd52feb4":"code","ae66c4a3":"code","59f484c1":"code","6c04e1de":"code","2e4968f3":"code","848553f1":"code","e326cee0":"code","083f1849":"code","1cdd20fc":"code","3841e541":"code","ffbc660b":"code","cb6282d7":"code","acb09d29":"code","76f93e06":"code","3b5f968c":"code","3c539eaa":"code","1dcc6d0d":"code","b2f1774b":"code","c173eebe":"code","90481885":"code","3568b420":"code","baaf3bf5":"code","77b09709":"code","8b18ddc0":"code","c1ed6d98":"code","4a532997":"code","614c7525":"code","70d9c589":"code","c29e8efc":"code","7c726464":"code","2861cd71":"code","0de677a0":"code","56c6bea2":"code","9bd29fd2":"code","dd24c924":"markdown","a901c00e":"markdown","b4341b92":"markdown","63986135":"markdown","f7b14cc7":"markdown","a785979f":"markdown","6eae5e70":"markdown","26146de1":"markdown","cb9afe31":"markdown","295637d4":"markdown","30f80211":"markdown","4c8b9d7f":"markdown","20582713":"markdown","92c28732":"markdown","bd71d300":"markdown","cac06d7c":"markdown","8226b436":"markdown","b1d33589":"markdown","9c9a882e":"markdown","8833585b":"markdown","0bc6b81f":"markdown","284ff237":"markdown","34f15bde":"markdown","b1423be9":"markdown","463868b9":"markdown","719f7199":"markdown","07d31725":"markdown","7ff25658":"markdown","87047865":"markdown","a84058b0":"markdown","5da83177":"markdown","8b9b8026":"markdown","89d95eef":"markdown","566ff776":"markdown","9aba1c25":"markdown","fd65f532":"markdown","0518cffa":"markdown","383d44e4":"markdown"},"source":{"c5917c9e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","13d3ced3":"adult=pd.read_csv('..\/input\/adult-census-income\/adult.csv', na_values=[\"?\"])\nprint(adult.head())","b0855b6e":"adult_new=adult.dropna(axis=0)\nprint(adult_new.head())\nprint('Dimensions:',adult.shape)","e3dc5598":"col=adult_new.columns\ndata_type=adult_new.dtypes\nuniq=adult_new.nunique()\n\nprint(\"\\n%30s  %10s   %10s\\n \" % (\"Column Name\", \"Data Type\", \"Unique Values\"))\nfor i in range(len(adult_new.columns)):\n    print(\"%30s  %10s   %10s \" % (col[i],data_type[i],uniq[i]))\n\nprint(\"\\nDimensions:\",adult_new.shape[0],'rows and ',adult_new.shape[1],'columns')","25c2fb77":"\nadult_new['income'].replace({'<=50K':0,'>50K':1},inplace=True)\nadult_new=adult_new.drop('education.num',axis=1)","e884d329":"from collections import Counter\noccupatn=dict(Counter(adult_new['occupation'])).keys()\nprint('Occupation types:','\\n',list(occupatn),'\\n')\nrace=dict(Counter(adult_new['race'])).keys()\nprint('Race types:','\\n',list(race),'\\n')\nrelation=dict(Counter(adult_new['relationship'])).keys()\nprint('Relation types:','\\n',list(relation),'\\n')\neducate=dict(Counter(adult_new['education'])).keys()\nprint('Education levels:','\\n',list(educate),'\\n')\nmarital=dict(Counter(adult_new['marital.status'])).keys()\nprint('Marital status levels:','\\n',list(marital),'\\n')\nwork=dict(Counter(adult_new['workclass'])).keys()\nprint('Workclass levels:','\\n',list(work),'\\n')\ncountry=dict(Counter(adult_new['native.country'])).keys()\nprint('Native countries:','\\n',list(country),'\\n')","a921b8c0":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = adult_new.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(adult_new[top_corr_features].corr(),annot=True,cmap=\"twilight_shifted_r\")","3b5149f3":"import scipy.stats as stats\na=['age','capital.loss','capital.gain','hours.per.week','fnlwgt']\nfor i in a:\n    print(i,':',stats.pointbiserialr(adult_new['income'],adult_new[i])[0])\n","fd52feb4":"adult_new=adult_new.drop('fnlwgt',axis=1)\nadult_new.dtypes","ae66c4a3":"categorical_cols = adult_new.columns[adult_new.dtypes==object].tolist()\ncategorical_cols","59f484c1":"\ndef cross_tab(obs1=[]):\n    observed=pd.crosstab(obs1,adult_new['income'])\n    val=stats.chi2_contingency(observed)\n    return(val[1])","6c04e1de":"alpha=0.01\ndf=adult_new.drop('income',axis=1)\ncount=0\nattributes2=[]\nfor i in categorical_cols:\n    p_value=cross_tab(adult_new[i])\n    if p_value<=alpha:\n        count+=1\n        attributes2.append(i)\nprint('Number of attributes contributing:',count,'\\n')\nprint(attributes2)","2e4968f3":"pd.crosstab(adult.relationship,adult_new['income'])","848553f1":"categorical_cols","e326cee0":"adult_new1=pd.get_dummies(adult_new,columns=categorical_cols)\nadult_new1.head()","083f1849":"adult_new1.columns","1cdd20fc":"from sklearn.preprocessing import MinMaxScaler\ncolumns_to_scale = ['age', 'capital.gain', 'capital.loss', 'hours.per.week']\nmms = MinMaxScaler()\nmin_max_scaled_columns = mms.fit_transform(adult_new1[columns_to_scale])\n#processed_data = np.concatenate([min_max_scaled_columns, adult_new], axis=1)\nadult_new1['age'],adult_new1['capital.gain'],adult_new1['capital.loss'],adult_new1['hours.per.week']=min_max_scaled_columns[:,0],min_max_scaled_columns[:,1],min_max_scaled_columns[:,2],min_max_scaled_columns[:,3]\nadult_new1.head()","3841e541":"category=adult_new1.columns[adult_new1.dtypes!=object].tolist()[5:]\n#category\nalpha=0.01\n#df=adult_new.drop('income',axis=1)\ncount=0\nfeatures=[]\nfor i in category:\n    p_value=cross_tab(adult_new1[i])\n    if p_value<=alpha:\n        count+=1\n        features.append(i)\n        #print(i,' has a relation')\n        #print('p-value for ',i,' is ',cross_tab(adult_new[i]),'\\n')\nprint('Number of contributing attributes:',count,'\\n')\nprint(features)","ffbc660b":"features.append('age')\nfeatures.append('capital.gain')\nfeatures.append('capital.loss')\nfeatures.append('hours.per.week')\nfeatures.append('income')","cb6282d7":"adult_new1[features].head()\n","acb09d29":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = adult_new1[features].corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(adult_new1[top_corr_features].corr(),annot=True,cmap=\"twilight_shifted_r\")","76f93e06":"f,ax=plt.subplots(1,2,figsize=(18,10))\n#plt.figure(figsize=(7,10))\nincome1=adult_new1['income'].value_counts()\nax[0].pie(income1,explode=(0,0.05),autopct='%1.1f%%',startangle=90,labels=['<=50K','>50K'])\nax[0].set_title('Income Share')\nax[1]=sns.countplot(x='income',data=adult_new1,palette='pastel')\nax[1].legend(labels=['<=50K','>50K'])\nax[1].set(xlabel=\"INCOME CATEGORIES\")\nax[1].set(ylabel='COUNT OF THE CATEGORIES')\nax[1].set_title('COUNT OF THE TWO LEVELS')\n\nfor p in ax[1].patches:\n    ax[1].annotate(p.get_height(),(p.get_x()+0.3,p.get_height()+500))","3b5f968c":"plt.figure(figsize=(10,10))\nax=sns.countplot(x='income',hue='sex',data=adult_new,palette='Set1')\nax.set(xlabel='INCOME 50')\nax.set(ylabel='COUNT WITH AGE')\nax.set_title('INCOME WITH RESPECT TO SEX')\nfor p in ax.patches:\n    ax.annotate(p.get_height(),(p.get_x()+0.15,p.get_height()+200))","3c539eaa":"f, ax = plt.subplots(figsize=(15, 8))\nax = sns.countplot(x=\"income\", hue=\"race\", data=adult_new, palette=\"Set1\")\nax.set_title(\"FREQUENCY DISTRIBUTION OF INCOME WITH RESPECT TO AGE\")\nax.set(xlabel='INCOME RANGE',ylabel='FREQUENCY OF AGES')\n\nfor p in ax.patches:\n    ax.annotate(p.get_height(),(p.get_x()+0.05,p.get_height()+200))\nplt.show()","1dcc6d0d":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"income\", data=adult_new, palette=\"Set2\")\nax.set_title(\"FREQUENCY DISTRIBUTION OF WORKCLASS WITH RESPECT TO INCOME\")\nax.set(xlabel='WORKCLASS RANGE',ylabel='FREQUENCY OF WORKCLASS')\nax.legend(labels=['<=50K','>50K'],loc='upper right',fontsize='large')\nplt.show()","b2f1774b":"#adult1=sns.load_dataset(\"adult.csv\")\ng = sns.FacetGrid(adult, col=\"occupation\")\ng.map(sns.countplot,'sex',alpha=0.7)\n","c173eebe":"plt.figure(figsize=(10,10))\n#sns.regplot(x='hours.per.week', y='fnlwgt',data=adult_new);\nx=adult_new['hours.per.week']\nplt.hist(x,bins=8,histtype='step')\nplt.ylabel('FREQUENCY')#,xlabel='Hours per week')\nplt.xlabel('HOURS PER WEEK')\nplt.title('HISTOGRAM OF HOURS PER WEEK')\nimport statistics as stat\nplt.axvline(stat.mode(x),color='red')\nplt.show()","90481885":"f,ax=plt.subplots(1,2,figsize=(15,10))\n\nless=adult_new[adult_new['income']==0]\nage_mode1=stat.mode(less.age)\nmore=adult_new[adult_new['income']!=0]\nage_mode2=stat.mode(more.age)\n#ax.axvline(age_mode1,age_mode2)\nprint('Maximum people around age ',age_mode1,' earn <=50K \\n')\nprint('Maximum people around age ',age_mode2,' earn >50K \\n')\nax[0].hist(less['age'],bins=15,histtype='step',color='green')\nax[0].set(xlabel='AGE RANGE',ylabel='FREQUENCY OF AGE')\nax[0].set_title('AGE DISTRIBUTION FOR INCOME <=50K')\nax[0].axvline(age_mode1,color='red')\nax[1].hist(more['age'],bins=8,histtype='step',color='red')\nax[1].set(xlabel='AGE RANGE',ylabel='FREQUENCY OF AGE')\nax[1].set_title('AGE DISTRIBUTION FOR INCOME >50K')\nax[1].axvline(age_mode2,color='black')\nplt.show()","3568b420":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nchi2=adult_new1[features]\n","baaf3bf5":"def train_print(clf,x_test,y_test):\n    predictions = clf.predict(x_test)\n    print('Precision report:\\nprecision \\t\\t\\t recall \\t\\t\\t f-score \\t\\t\\t support\\n',\n          precision_recall_fscore_support(y_test, predictions)[0],'\\t',\n          precision_recall_fscore_support(y_test, predictions)[1],\n          '\\t',precision_recall_fscore_support(y_test, predictions)[2],'\\t',\n          precision_recall_fscore_support(y_test, predictions)[3],'\\n')\n    print('Confusion matrix:\\n',confusion_matrix(y_test, predictions),'\\n')\n    print('Accuracy score:',accuracy_score(y_test, predictions)*100,'\\n')","77b09709":"from sklearn.linear_model import LogisticRegression\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\nX_train, Y_train = SMOTE().fit_sample(x_train, y_train)\n\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, Y_train)\n\ntrain_print(logmodel,x_test,y_test)","8b18ddc0":"# predict probabilities\nlr_probs = logmodel.predict_proba(x_test)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\n#print(lr_probs)\nns_probs = [0 for _ in range(len(y_test))]\n#print(ns_probs)","c1ed6d98":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Logistic: ROC AUC=%.3f' % (lr_auc))","4a532997":"# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","614c7525":"from sklearn.tree import DecisionTreeClassifier\nx = chi2.drop('income', axis=1)\ny = chi2['income']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion='entropy',min_samples_split=8,max_depth=10)\n\n# Train Decision Tree Classifer\nclf.fit(x_train,y_train)\n\ntrain_print(clf,x_test,y_test)","70d9c589":"# predict probabilities\ndt_probs = clf.predict_proba(x_test)\n# keep probabilities for the positive outcome only\ndt_probs1 = dt_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\ndt_auc = roc_auc_score(y_test, dt_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Deciison Tree: ROC AUC=%.3f' % (dt_auc))","c29e8efc":"# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\ndt_fpr, dt_tpr, _ = roc_curve(y_test, dt_probs1)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(dt_fpr, dt_tpr, linestyle='--',marker='*',label='Deciison Tree: ROC AUC=%.3f' % (dt_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","7c726464":"from sklearn.naive_bayes import GaussianNB\n#Create a Gaussian Classifier\nmodel = GaussianNB()\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Train the model using the training sets\ngnb = model.fit(x_train,y_train)\n\ntrain_print(gnb,x_test,y_test)","2861cd71":"# predict probabilities\nnb_probs = model.predict_proba(x_test)\n# keep probabilities for the positive outcome only\nnb_probs1 = nb_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nnb_auc = roc_auc_score(y_test, nb_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nnb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs1)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(nb_fpr, nb_tpr, linestyle='--',marker='*',label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n","0de677a0":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier(min_samples_split=30)\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Train the model using the training sets\nrf.fit(x_train,y_train)\n\ntrain_print(rf,x_test,y_test)","56c6bea2":"# predict probabilities\nrf_probs = rf.predict_proba(x_test)\n# keep probabilities for the positive outcome only\nrf_probs1 = rf_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nrf_auc = roc_auc_score(y_test, rf_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Random Forest: ROC AUC=%.3f' % (rf_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs1)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(rf_fpr, rf_tpr, linestyle='--',marker='*',label='Random Forest: ROC AUC=%.3f' % (rf_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n","9bd29fd2":"plt.figure(figsize=(15,10))\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\nplt.plot(dt_fpr, dt_tpr, linestyle='--',marker='*',label='Deciison Tree: ROC AUC=%.3f' % (dt_auc))\nplt.plot(nb_fpr, nb_tpr, linestyle='--',marker='*',label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\nplt.plot(rf_fpr, rf_tpr, linestyle='--',marker='*',label='Random Forest: ROC AUC=%.3f' % (rf_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\nplt.title('ROC CURVES')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","dd24c924":"Some jargons explained-\n\nprecision->true positives predicted\n\nrecall->true positives actually present\n\nf score->harmonic mean between precision and recall\n\nsupport->total number of samples in each class \n\nconfusion matrix->gives the true positives and negatives and false positives and negatives.\n\nThe aim is to increase both the precision and recall. F score gives the balance between the two.","a901c00e":"<h3><p>5. FITTING APPROPRIATE MODEL<\/p><\/h3>","b4341b92":"Visualising the various values under the categorical columns.\n","63986135":"Replace the dependent column into appropriate format for fitting models.\nThe education.num column is the numerical representation of education column and hence redundant.","f7b14cc7":"Finding the correlation of all the other variables with the dependent variable. I will create a heatmap for the same.\nLook at the last row which is the 'income' and check the correlation. The variables which seem to have no or negative correlation with the dependent variable, I will drop it.","a785979f":"Below is how a contingency table looks like.","6eae5e70":"The ROC curve gives how well the model predicted. Higher the percentage of area inside the arc better the model.","26146de1":"<p><h3>5.3 Naive Bayes<\/h3><\/p>","cb9afe31":"A statistical test called point biserial correlation is used to measure the relationship between a binary variable x and a continuous variable y. It works on the same concept of a correlation coefficient. I will print the correlation coefficient to check which variables to drop. Since it works only between continuous and binary variables, I will use this for the 5 continuous variables in the dataset.","295637d4":"According to chi square test, only 60 attributes contribute to the dependent variable. I add the continuous variables to the set of selected attributes.","30f80211":"<p><h3>6. COMPARING ALL MODELS BY THEIR ROC <\/h3> <\/p>","4c8b9d7f":"As seen in the first graph, there is a class imbalance problem. Models do not fit well when there is a class imbalance. There are some methods like oversampling, undersampling and mixture. I will do class oversampling using SMOTE(Synthetic Minority Oversampling  Technique). It creates new samples along the lines of the existing samples.","20582713":"<p><h3>2.1 Level 1 feature selection using chi square estimate<\/h3><\/p>","92c28732":"<h3><p>3. ENCODING THE CATEGORICAL VARIABLES<\/p><\/h3>","bd71d300":"I see that all the categorical variables contribute to 'income'. Hence, I keep all of them.","cac06d7c":"<p><h3>3.1 Normalizing the variables<\/h3><\/p>","8226b436":"<p><h3>5.4 Random Forest<\/h3><\/p>","b1d33589":"The range of correlation is from -1 to +1. On the right hand side there is a reference given so that by the colour on the heatmap the correlation can be easily determined. Here, 'fnlwgt' correlate nagatively with the 'income' and hence I will drop it. Also, in a correlation heatmap all the diagonal elements will be 1 indicating high correlation as they correlate with themselves.","9c9a882e":"<p><h3>5.2 Decision Tree<\/h3><\/p>","8833585b":"The dataset has 15 columns. The missing values are ony present in the categorical columns. Several predictive supervise models like KNN or other methods can be used to predict these values and impute them. I will remove them in this case.","0bc6b81f":"To measure the correlation between 2 categorical variables, I will use the chi-square estimate. The chi-square gives me a contingency table and calculates the p-value and the chi-square estimate. \nThe hypothesis of the chi-square test is \n\nH0: variable not related.\n\nH1: variables related.\n\nI set the value of alpha as 0.01. If the p-value is less than alpha, I will reject H0 and hence the variables are related.","284ff237":"<p>Around 18K people work between 35 to 50 hours a day<\/p>","34f15bde":"As above, the 'fnlwgt' has a negative correation with 'income' and hence I will drop the column and the remaining varibales are such.","b1423be9":"The missing values are shows as '?'. Hence I will replace them with NAN values.","463868b9":"<p><h3>4. EXPLORATORY DATA ANALYSIS<\/h3><\/p>\n","719f7199":"<h2>1. IMPORTING LIBRARIES<\/h2>","07d31725":"I use the min max normalization which by default scales all the variables between 0 and 1 but the range can be specified for other scales.","7ff25658":"<h2>2. CLEANING THE DATA<\/h2>","87047865":"<h3><p>3.3 Heatmap of all selected variables<\/p><\/h3>\n","a84058b0":"<p><h3>3.2 Level 2 feature selection using chi square estimate<\/h3><\/p>","5da83177":"Now, I have 103 columns. Fitting this huge set of attributes is not recommended and hence the next step is feature selection. Also, since almost all varibales are either 0 or 1, we normalize the continuous variables to be between 0 and 1.","8b9b8026":"<p>15% women are in >50 category and the rest are 85% men<\/p>\n<p>38% women are in <=50 and the rest are 62% men<\/p>\n<p>69% of all men earn <=50K while only 31% earn more than 50K<\/p>\n<p>Only 10% of all women in the sample earn more than 50K while the rest 90% earn less than 50K<\/p>","89d95eef":"<p>There is a total of 30162 observations with 7508 people earning more than 50K and 22654 earning less than or equal to 50K.\n<\/p>\n<p>\nThe percentage division can be clearly understood by the pie chart.<\/p>","566ff776":"<p><h3>5.1 Logistic Regression<\/h3><\/p>","9aba1c25":"Fitting the categorical columns into models without encoding them will be a problem and hence I encode them using get_dummies. It gives me the dummy variables. For eg- if race has 4 types, for each row one type will have '1' under its category and '0' under the others. Thus, all the categorical variables will split into their respective 'n' types. This process can also be done using OneHotEncoder or LabelEncoder","fd65f532":"First I will import and visualize the data.","0518cffa":"\n<p style=\"font-family: Arial; font-size:2em;color:green; font-style:bold\"><br>\n    Income Classification <\/p>\n    <p style=\"font-family: Arial;color:black; font-style:bold\">Hello,I have worked on the Adult Income daatset. This notebook is based on classifying the income into two groups.This is a dichotomous classification notebook. The goal is to predict whether a certain individual earns more or less than $50,000. <\/p>\n    <p>This notebook has the following sections and subsections<\/p><br>\n    \n <ul >\n <li>IMPORTING LIBRARIES<\/li>\n <li>CLEANING THE DATA<\/li>\n <ul><li>Level 1 feature selection using chi-square<\/li><\/ul>\n <li>ENCODING THE CATEGORICAL VARIABLES<\/li>\n <ul><li>Normalizing the variables<\/li>\n <li>Level 2 feature selection using chi square <\/li>\n <li>Heatmap of all selected variables<\/li><\/ul>\n <li>EXPLORATORY DATA ANALYSIS<\/li>\n <li>FITTING APPROPRIATE MODEL<\/li>\n <ul><li>Logistic Regression<\/li>\n <li>Decision Tree<\/li>\n <li>Naive Bayes<\/li>\n <li>Random Forest<\/li><\/ul>\n <li>COMPARING ALL MODELS BY THEIR ROC<\/li>\n\n   ","383d44e4":"<p> Random Forest performs the best with area of 0.91 under the curve.<\/p>"}}