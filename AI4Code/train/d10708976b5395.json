{"cell_type":{"50de1327":"code","bb640a83":"code","3f035caf":"code","8303896b":"code","ea65c77c":"code","e8a0ba42":"code","429458da":"code","670c05e2":"code","24401df3":"code","4e1bafde":"code","efcf1468":"code","99f83069":"code","98663f31":"code","7951f9b7":"code","4b2590ef":"code","6e5571ea":"code","733f92c2":"code","49bc7416":"code","6769a82e":"code","f45dbf55":"code","e543626e":"code","cac2a35c":"code","d3316273":"code","70a029d7":"code","f0194a3c":"code","b84c4c87":"code","e0cc21a4":"code","5e13020e":"code","48464651":"code","a93b8bb9":"code","53720a20":"code","dfeaa758":"code","82d3ce94":"code","45f70913":"code","ba3373c1":"code","75be23bb":"code","61b195bc":"code","f91077c5":"code","3fbc2e93":"code","90051d26":"code","c4a0ab63":"code","5f734059":"code","a93b748a":"code","c5f88505":"code","559620f7":"code","89160662":"code","d78ef39d":"code","560322e4":"code","82f3fff3":"code","53c43cf8":"code","64cc4a2f":"code","224543e0":"code","e330dbf8":"code","f485cd0e":"code","b9ae55b0":"code","dc0794e6":"code","8de0fa44":"code","23033f40":"code","52d8e44e":"code","ae0280ed":"code","4cc8d7f2":"code","945b1e39":"code","1cbdbf1c":"code","e5930b08":"code","e86a8a55":"code","361c3fa1":"code","a914ec9d":"code","f2d2e16b":"code","8969fddf":"code","273d8e94":"code","33f411ce":"code","2cf42a80":"code","4cf6ca4e":"code","3c53b101":"code","074ac4b2":"code","bd2859e1":"code","7e2e3711":"code","62bd6fac":"code","648878ce":"code","ef05aecb":"markdown","e3648a0d":"markdown","8fbb8243":"markdown","22b2b6af":"markdown","8edc9e3b":"markdown","a7c330aa":"markdown","33790dc4":"markdown","1ff86f59":"markdown","2ece21c7":"markdown","b52102c5":"markdown","6af13647":"markdown","c5e14996":"markdown","3938ccc6":"markdown","4ba8da10":"markdown","4dd70ef0":"markdown","dded2557":"markdown","2d75260c":"markdown","d1ab34fd":"markdown","82b90ce7":"markdown","a1e23277":"markdown","b698e181":"markdown","3a0c0dc7":"markdown","e1758f29":"markdown","539941f8":"markdown","3cb44841":"markdown","380b6a07":"markdown","2843df2e":"markdown","38e43bc0":"markdown","3e015f6c":"markdown","f9bd0a89":"markdown","9c06799f":"markdown","fb85800b":"markdown","e9139d59":"markdown","07c4f768":"markdown","073afd08":"markdown","39adc410":"markdown","193c4442":"markdown","47b51ca6":"markdown","e0c99f28":"markdown","9ef69cc5":"markdown","3724fdd8":"markdown","1fc305a7":"markdown","b3334b35":"markdown","ecd78cce":"markdown","271ed15c":"markdown","e200ffe3":"markdown","a62243c5":"markdown","eb92fed0":"markdown","2b64d479":"markdown","f6dcfbc1":"markdown","45b96d0e":"markdown","7a7a5166":"markdown","dba616b5":"markdown","698cf7ee":"markdown","efc86ae4":"markdown"},"source":{"50de1327":"# importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n%matplotlib inline\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans\nfrom itertools import combinations\n\ndf_raw = pd.read_csv(\"..\/input\/bike-sharing-dataset\/hour.csv\", parse_dates = [\"dteday\"])\ndf_raw.head()","bb640a83":"print(\"Number of rows: {}\\nNumber of columns: {}\".format(*df_raw.shape))","3f035caf":"df = df_raw.drop([\"casual\", \"registered\"], axis = 1)","8303896b":"print(\"Num of duplicates: {}\".format(df[[\"dteday\", \"hr\"]].duplicated().sum()))\nprint(\"Num of nulls: {}\".format(df.isnull().sum().sum()))","ea65c77c":"#Adding hours to dteday datetime column\nimport datetime\ndf.dteday = df.dteday + pd.to_timedelta(df.hr, unit = \"h\")\n\nprint(\"Years classification:\")\npd.concat([df.dteday.dt.year, df.yr], axis = 1).drop_duplicates()","e8a0ba42":"print(\"Number of errors in 'mnth' labels: {}\".format((df.dteday.dt.month != df.mnth).sum()))","429458da":"print('Number of logs by month by season')\ndf.pivot_table(index = 'season', columns = 'mnth', values = 'dteday', aggfunc = 'count').fillna(0).astype(int)","670c05e2":"#creating DataFrame to help us find the dates where change of seasons happened\nseasons_shifted = pd.concat([df.season, df.season.reindex(df.index - 1).reset_index(drop = True),\n                             df.season.reindex(df.index + 1).reset_index(drop = True)],\n                            join = \"outer\", axis = 1, ignore_index = True)\n\n#finding seasons' date intervals\nseasons_intervals_filter = (seasons_shifted[0] != seasons_shifted[1]) | (seasons_shifted[0] != seasons_shifted[2])\nseasons_intervals_df = df.loc[seasons_intervals_filter,['dteday', 'season']].reset_index(drop = True)\n\nint_start_ix = pd.RangeIndex(start = 0, stop = seasons_intervals_df.shape[0], step=2)\nint_end_ix = pd.RangeIndex(start = 1, stop = seasons_intervals_df.shape[0], step=2)\n\nseasons_intervals_start_df = seasons_intervals_df.loc[int_start_ix].reset_index(drop = True)\nseasons_intervals_end_df = seasons_intervals_df.loc[int_end_ix].reset_index(drop = True)\n\nseasons_intervals = seasons_intervals_start_df.merge(seasons_intervals_end_df, left_index=True, right_index=True, suffixes = ['_start','_end'])\nseasons_intervals = seasons_intervals.reindex(['dteday_start','dteday_end', 'season_start'], axis = 1)\nseasons_intervals.columns = ['dteday_start','dteday_end', 'season']\nprint('Classification of seasons based on date intervals')\nseasons_intervals","24401df3":"df[\"weekday\"] = df.dteday.dt.weekday","4e1bafde":"datetimes = pd.date_range(\n    start=df.dteday.min(),\n    end=df.dteday.max(),\n    freq=\"1H\",\n    name=\"Datetime\")\ndatetimes\n\nmissing_datetimes = ~datetimes.isin(df.dteday)\nprint('Numer of missing hourly logs in the data: {}'.format(missing_datetimes.sum()))","efcf1468":"print('Number of cnt <= 0: {}'.format((df.cnt <= 0).sum()))","99f83069":"def missing_dates_neighbors(row):\n    try:\n        i_left = df.dteday[df.dteday < row].index[-1]\n    except:\n        i_left = np.nan\n    try:\n        i_right = df.dteday[df.dteday > row].index[0]\n    except:\n        i_right = np.nan\n    return pd.Series({\"left\" : i_left,\n                     \"right\": i_right})\nmd_series = pd.Series(datetimes[missing_datetimes], name = \"Miss DT\")\n#md_series.apply(missing_dates_neighbors, axis = 1)\n\nmd_neighbors = md_series.apply(missing_dates_neighbors)\nmd_neighbors_counts = md_neighbors[\"left\"].value_counts().sort_index()\nmissing_hrs = pd.concat([df.dteday,md_neighbors_counts],\n          axis = 1, join = \"inner\").reset_index()\nmissing_hrs.columns = [\"index_log_before\",\"datetime_since\",\"nr missing hrs\"]\nmissing_hrs[\"datetime_since\"] = missing_hrs[\"datetime_since\"] + datetime.timedelta(hours = 1)\nmissing_hrs = missing_hrs.set_index(\"datetime_since\")\nmissing_hrs['nr missing hrs'].value_counts().sort_index(ascending = False)","98663f31":"missing_hrs[missing_hrs['nr missing hrs'] <= 2].index.hour.value_counts().sort_index()","7951f9b7":"missing_hrs[missing_hrs[\"nr missing hrs\"] > 2].sort_values(by = \"nr missing hrs\", ascending = False)","4b2590ef":"#we'll split in 4 subplots to better see the bars on the figure.\n#As a split criteria, we'll use a middle point of a dteday field of df\nmin_date = df.dteday.dt.date.min()\nmax_date = df.dteday.dt.date.max()\ncenter_date = min_date + (max_date - min_date)\/2\nsecond_quarter_date = min_date + (center_date - min_date)\/2\nthird_quarter_date = center_date + (max_date - center_date)\/2\n\ndt_lims = [\n    [min_date,second_quarter_date],\n    [second_quarter_date,center_date],\n    [center_date,third_quarter_date],\n    [third_quarter_date,max_date + datetime.timedelta(days = 1)]\n]\n\n#aggregating rentals cnt at day level\nseries_for_trend_plot = df.set_index('dteday').resample('D').cnt.sum().asfreq('D')\n\n#subplots generation\nfig, axs = plt.subplots(nrows=len(dt_lims),figsize=(17,8), sharey = True)\n\n#pandas needs this\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n#the bar chart will be on secondary y, on primary we'll place cnt trend\n#let's create list for twin axs\naxs2 = [i.twinx() for i in axs]\n#set shared y for secondary y axes\naxs2[0].get_shared_y_axes().join(*axs2)\n\nfor i in range(len(dt_lims)):\n    \n    trendline = series_for_trend_plot[dt_lims[i][0]:dt_lims[i][1]]\n    missing_logs = missing_hrs.loc[dt_lims[i][0]:dt_lims[i][1],'nr missing hrs']\n\n    #adding cnt trend on the primary y\n    l = axs[i].plot(trendline.index,\n                trendline,\n                color = 'grey',\n                alpha = 0.7,\n                label = 'daily bike rentals'\n               )\n    \n    #adding missing_logs bars on secondary y\n    b = axs2[i].bar(missing_logs.index,\n                missing_logs,\n                color = 'orange',\n                alpha = 0.7,\n                label = 'number of missing hours - right axis (logarithmic)')\n\n    #make 2nd y axis on log scale to increase visibility\n    axs2[i].set_yscale('log')\n    \n    #adding legend just on the first plot\n    if i == 0:\n        l_b = l+[b]\n        labels = [lab.get_label() for lab in l_b]\n        axs2[i].legend(l_b, labels)\n\nplt.suptitle(\"Daily bike rentals and number of consecutive hours without logs\")\nplt.show()","6e5571ea":"#Adding weekday and month to the tabme with missing hours\nmissing_hrs[\"wk_day\"] = missing_hrs.index.weekday\nmissing_hrs[\"mnth\"] = missing_hrs.index.month\nmissing_hrs[missing_hrs[\"nr missing hrs\"] > 2]","733f92c2":"ix_filter = missing_hrs[missing_hrs[\"nr missing hrs\"] > 2].index_log_before\nix_filter = sorted(ix_filter.tolist() + (ix_filter + 1).tolist())\nfiltered = df.iloc[ix_filter].drop([\"season\",\"yr\",\"mnth\",\"hr\"], axis = 1)\nfiltered","49bc7416":"filtered[\"YYYYMMHH\"] = (filtered.dteday.dt.year * 10000 + filtered.dteday.dt.month*100 + filtered.dteday.dt.hour) \ndf1 = df.copy()\ndf1[\"YYYYMMHH\"] = (df1.dteday.dt.year * 10000 + df1.dteday.dt.month*100 + df1.dteday.dt.hour)\ndf1 = df1.pivot_table(values = \"cnt\", index = \"YYYYMMHH\", aggfunc = np.mean).reset_index()\nfiltered.merge(df1,\n               left_on = \"YYYYMMHH\",\n               right_on = \"YYYYMMHH\",\n               how = \"inner\")[[\"dteday\",\"cnt_x\",\"cnt_y\"]]","6769a82e":"import calendar\n\n#we'll write a function to return slices of df with specified days before and after\ndef find_avg_trend(date_instant, days_before = 2, days_after = 2, df = df):\n\n    date_instant_year = date_instant.year\n    date_instant_month = date_instant.month\n    date_instant_day = date_instant.day\n    date_instant_weekday = date_instant.weekday()\n\n    left = datetime.datetime(year = date_instant_year,\n                             month = date_instant_month,\n                             day = date_instant_day) - datetime.timedelta(days = days_before)\n\n    monthrange = calendar.monthrange(date_instant_year,date_instant_month)\n\n    next_month = (datetime.datetime(year = date_instant_year, month = date_instant_month, day = monthrange[1])\n                  + datetime.timedelta(days = 1))\n\n    monthrange_series = pd.date_range(start = datetime.datetime(year = date_instant.year,\n                                                month = date_instant.month,\n                                                day = 1),\n                                      end = next_month,\n                                      closed = 'left',\n                                      freq = '1D')\n    \n    left_similar = list(monthrange_series[monthrange_series.weekday == left.weekday()])\n\n    df_segments = []\n    df_frequency = df.reset_index().set_index('dteday').asfreq('1H').sort_index()\n\n    for l in left_similar:\n\n        r = l + datetime.timedelta(hours = (days_before + days_after + 1)*24-1)\n        df_segment = df_frequency[l:r].reset_index().set_index('index')\n        df_segments.append(df_segment)\n\n    main_period_df = df_segments.pop(left_similar.index(left))\n    return(main_period_df, df_segments)","f45dbf55":"dates_before_mis_logs = df.iloc[missing_hrs[missing_hrs[\"nr missing hrs\"] > 2].index_log_before][\"dteday\"].copy()\nfig, axs = plt.subplots(nrows = dates_before_mis_logs.shape[0], figsize = (17,8), sharex = True)\n\nfor i in range(dates_before_mis_logs.shape[0]):\n    \n    slice_main, slices_other = find_avg_trend(dates_before_mis_logs.iloc[i], days_before = 2, days_after = 2, df = df)\n    main_period_cnt = slice_main.cnt.values\n    other_periods_cnt_mean = np.mean([table.cnt.fillna(0).values for table in slices_other], axis=0)\n    \n    axs[i].plot(main_period_cnt, color = 'grey', label = 'Nr rentals')\n    axs[i].plot(other_periods_cnt_mean, color = 'orange', alpha = 0.5, label = 'Mean nr rentals in similar weekdays of the month')\n    axs[i].set_title('Missing logs start at: ' + str(dates_before_mis_logs.iloc[i]))\n    \n    if i == 0:\n        axs[i].legend()\nplt.suptitle(\"Bike rentals trend 2 days before and 2 days after the start of top interruptions\")\nplt.xlabel('hr since period start')\nplt.show()","e543626e":"fig, axs = plt.subplots(nrows = dates_before_mis_logs.shape[0], figsize = (17,8), sharex = True, sharey = True)\n\nfor i in range(dates_before_mis_logs.shape[0]):\n    \n    slice_main, slices_other = find_avg_trend(dates_before_mis_logs.iloc[i], days_before = 2, days_after = 2, df = df)\n    main_period_temp = slice_main.temp.values\n    main_period_hum = slice_main.hum.values\n    main_period_ws = slice_main.windspeed.values\n\n    axs[i].plot(main_period_temp, color = 'red', alpha = 0.5, label = 'Relative temperature')\n    axs[i].plot(main_period_hum, color = 'green', alpha = 0.5, label = 'Relative humidity')\n    axs[i].plot(main_period_ws, color = 'blue', alpha = 0.5, label = 'Relative windspeed')\n    axs[i].set_title('Missing logs start at: ' + str(dates_before_mis_logs.iloc[i]))\n    \n    if i == 0:\n        axs[i].legend()\nplt.suptitle(\"Temperature and humidity trend 2 days before and 2 days after the start of top interruptions\")\nplt.xlabel('hr since period start')\nplt.show()","cac2a35c":"df['instant'] = df.set_index('dteday').asfreq('1H').reset_index().dropna().index\ndf","d3316273":"df_for_trend = df.copy().set_index(\"dteday\")\n\n#plotting lines\nax = df_for_trend.cnt.plot(figsize = (17,7), alpha = 0.2, color = 'grey', linewidth = 0.5, label = 'Hourly rentals')\ndf_for_trend.resample('D').mean().cnt.plot(ax = ax, alpha = 0.4, linewidth = 0.8, color = \"red\", label = 'Mean hourly rentals by day')\ndf_for_trend.resample('W').mean().cnt.plot(ax = ax, alpha = 0.4, linewidth = 1, color = \"red\", marker = '.', label = 'Mean hourly rentals by week')\ndf_for_trend.rolling('56D').mean().cnt.plot(ax = ax, alpha = 0.4, linewidth = 1, color = \"black\", label = '56D rolling avg hourly rentals')\n\n#plotting trendline\nlr_matrix = pd.concat([pd.Series(1, index=df_for_trend.index), df_for_trend.instant], axis = 1)\nX = lr_matrix\nY = df_for_trend.cnt\n\ncoeffs = np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T, Y))\nY1 = pd.Series(np.dot(X, coeffs.T), index = df_for_trend.index,name = 'lin_approx')\nY1.plot(ax = ax, alpha = 0.6, linestyle = \"--\", color = \"orange\",\n        label = \"Linear Approx.\\nIntercept: {:.3f}\\nSpline: {:.3f}\".format(coeffs[0],coeffs[1]))\n\nax.legend()\nplt.show()","70a029d7":"fig, ax = plt.subplots(figsize = (17,7))\nsns.boxplot(x=df_for_trend.index.to_period('M'), y = df_for_trend[\"cnt\"], ax = ax, color = \"white\")\nplt.xticks(rotation = 45)\nplt.title(\"Daily bike rentals distribution by month\")\nax.set_xlabel (\"Month\")\nplt.show()","f0194a3c":"seasonality_c = (Y \/ Y1)\n\nseasonality_yearly = seasonality_c.groupby(seasonality_c.index.month).mean()\nseasonality_wd = seasonality_c.groupby(seasonality_c.index.weekday).mean()\nseasonality_d = seasonality_c.groupby(seasonality_c.index.hour).mean()\n\nseasonality_srs = [seasonality_yearly,seasonality_wd,seasonality_d]\nlbls = ['month','weekday','hour']\nfig,axs = plt.subplots(ncols = 3,figsize = (15,5), sharey = True)\n\nfor i in range(3):\n    seasonality_srs[i].plot(ax = axs[i], legend = False, color = \"grey\", alpha = 0.5, marker = '.')\n    axs[i].set_xlabel(lbls[i])\naxs[0].set_ylabel(\"Seasonality coefficient\")\nplt.suptitle(\"Seasonalities\")\n\nplt.show()","b84c4c87":"seasonality_c1 = (Y \/ df_for_trend.rolling('14D').mean().cnt)\nseasonality_c1.name = 'seasonality_c'\ndf_for_trend1 = pd.concat([df_for_trend,seasonality_c1], axis = 1)\n\nwh_seasonalities = df_for_trend1.groupby(['season','workingday',df_for_trend1.index.hour.rename('hr')]).cnt.mean()\nwh_seasonalities_pivot = wh_seasonalities.reset_index().pivot_table(values = 'cnt', columns = ['workingday', 'season'], index = 'hr')\n\nfig,axs = plt.subplots(ncols = 4, nrows = 2, figsize = (17,7), sharey = True, sharex = True)\n\nfor i in range(2):\n    wh_seasonalities_pivot[i].plot(subplots = True, ax = axs[i-1], legend = False, color = \"grey\", alpha = 0.5, marker = '.')\n    axs[i-1, 0].set_ylabel(\"Workingday = {}\".format(i))\nfor i in range (1,5):\n    axs[0,i-1].set_title(\"Season = {}\".format(i))\nplt.suptitle(\"Mean hourly bike rentals in different seasons\")\nplt.show()","e0cc21a4":"df1 = df_for_trend\ndf1['daytime_lbl'] = \"[0:6)\"\ndf1.loc[(df1.index.hour >= 6) & (df1.index.hour < 9), 'daytime_lbl'] = \"[6:9)\"\ndf1.loc[(df1.index.hour >= 9) & (df1.index.hour < 12), 'daytime_lbl'] = \"[9:12)\"\ndf1.loc[(df1.index.hour >= 12) & (df1.index.hour < 16), 'daytime_lbl'] = \"[12:16)\"\ndf1.loc[(df1.index.hour >= 16) & (df1.index.hour < 20), 'daytime_lbl'] = \"[16:20)\"\ndf1.loc[(df1.index.hour >= 20) & (df1.index.hour < 24), 'daytime_lbl'] = \"[20:23]\"\n\nfig, axs = plt.subplots(ncols = 2, figsize = (12,5), sharey = True)\nc = sns.color_palette(\"BrBG\", 6)\nsns.boxplot(data = df1, x = \"daytime_lbl\", y = \"cnt\", ax = axs[0], palette = c)\naxs[0].set_title(\"Distribution of bikerental count split by daytime label\")\n\ndf2 = df1.pivot_table(values = \"cnt\", columns = \"daytime_lbl\", index = df1.index.to_period('D'))\ndf2.plot(marker='.', alpha=0.7, linestyle='None', ax = axs[1], color = c)\naxs[1].set_title(\"Rentals by day hour category\")\nplt.show()","5e13020e":"fig,axs = plt.subplots(ncols = 2, figsize = (12,5))\nc = sns.color_palette(\"colorblind\", 4)\nseason_weather_ctab = pd.crosstab(df1['season'],\n                                   df1['weathersit'],\n                                   values = df1['cnt'],\n                                   aggfunc = [np.size,np.mean]).fillna(0).astype(int)\nseason_weather_ctab['size'].plot.bar(alpha = 0.6, color = c, logy = True, ax = axs[0],\n                                     title = 'Number of hours with different weather situations', legend = False, rot = 0)\nseason_weather_ctab['mean'].plot.bar(alpha = 0.6, color = c, ax = axs[1],\n                                     title = 'Mean bike rentals in different weather situations', rot = 0),\nplt.show()","48464651":"#instantiating subplots\nfig,axs = plt.subplots(ncols = 4, nrows = 2, figsize = (16,8), sharex = True)\n\n#set shared y\naxs[0][0].get_shared_y_axes().join(*axs[0])\naxs[1][0].get_shared_y_axes().join(*axs[1])\n\n#some labels for text on plots\nseasons_labels = sorted(df1['season'].unique().tolist())\nweathersit_labels = sorted(df1['weathersit'].unique().tolist())\n\nfor i in range(len(seasons_labels)):\n    \n    #weight based on share of weathersit in season\n    weights = df1[df1['season'] == seasons_labels[i]]['weathersit'].value_counts(normalize = 1)\n        \n    for j in range(len(weathersit_labels)):\n        df1_ij_indexer = ((df1['weathersit'] == weathersit_labels[j]) & \n                          (df1['season'] == seasons_labels[i]))\n\n        #if there is no weathersit label in season\n        #there's nothing to plot\n        if df1_ij_indexer.sum() > 0:\n            df_ij = df1[df1_ij_indexer]\n            df_ij_freq = df_ij.index.hour.value_counts(normalize = True).sort_index()\n            df_ij_weighted = df_ij_freq*weights[weathersit_labels[j]]\n            df_ij_mean_cnt = df_ij.groupby(df_ij.index.hour)['cnt'].mean()\n            \n            l1 = axs[0][i].plot(df_ij_weighted,\n                             label = 'Weathersit = {}'.format(weathersit_labels[j]),\n                             color = c[j],\n                             alpha = 0.6)\n            \n            axs[1][i].plot(df_ij_mean_cnt,\n                             label = 'Weathersit = {}'.format(weathersit_labels[j]),\n                             color = c[j],\n                             alpha = 0.6)\n\n    #adding text and formatting    \n    axs[0][i].set_title('Season = {}'.format(seasons_labels[i])) \n    axs[1][i].set_xlabel('hr')\n\n    if i != 0:\n        axs[0][i].yaxis.set_visible(False)\n        axs[1][i].yaxis.set_visible(False)\n\n#some other formatting\naxs[0][1].set_xlim((0,23))\naxs[0][0].set_ylabel('Weathersit distribution')\naxs[1][0].set_ylabel('Mean rentals')\n    \n# Put a legend below\nlabels = [lab.get_label() for lab in axs[0][0].lines]\n\nfig.legend(axs[0][0].lines, labels, bbox_to_anchor=(0.43, 0), loc='lower center',ncol=5)\n\nfig.suptitle('Weathersit distribution by hour vs mean rentals')\nplt.show()","a93b8bb9":"fig, axs = plt.subplots(ncols = 2, figsize = (17,7), sharey = True)\nsns.violinplot(data = df1, x = \"weathersit\", y = \"cnt\", ax = axs[0], palette = c)\n\ndf.pivot_table(values = \"cnt\", columns = \"weathersit\", index = \"dteday\").plot(marker = \".\",\n                                                                              linestyle = \"None\",\n                                                                              alpha = 0.3,\n                                                                              figsize = (17,7),\n                                                                              color = c,\n                                                                              ax = axs[1])\nfig.suptitle(\"Distributions of hourly bike rentals in different weather situations\")\nplt.show()","53720a20":"fig,axs = plt.subplots(ncols = 3, nrows = 2, figsize = (15,10))\nweather_continuous_cols = [\"temp\", \"atemp\",\"hum\",\"windspeed\"]\nweather_cols_comb = combinations(weather_continuous_cols, 2)\nweather_cols_comb = [list(i) for i in list(weather_cols_comb)]\n\ncenters = df1.pivot_table(values = weather_continuous_cols, index = \"weathersit\").sort_index()\n\nfor i in range(len(weather_cols_comb)):\n    if i > 0:\n        legend = False\n    else:\n        legend = \"full\"\n    sns.scatterplot(data = df1,\n                    x = weather_cols_comb[i][0],\n                    y = weather_cols_comb[i][1],\n                    hue = \"weathersit\",\n                    alpha = 0.3,\n                    ax = axs.flatten()[i],\n                    palette = c,\n                    marker = \"D\",\n                    s = 10,\n                    legend = False)\n    sns.scatterplot(data = centers,\n                    x = weather_cols_comb[i][0],\n                    y = weather_cols_comb[i][1],\n                    hue = centers.index,\n                    ax = axs.flatten()[i],\n                    palette = c,\n                    legend = legend,\n                    marker = \"P\",\n                    s = 200)\nfig.suptitle(\"Analysis of weathersit label vs weather parameters\\n('+' represents centers of clusters)\")\nplt.show()","dfeaa758":"fig,axs = plt.subplots(ncols = 4, figsize = (16,4), sharey = True, sharex = True)\nfor i in range(len(weather_continuous_cols)):\n    if i == len(weather_continuous_cols) - 1:\n        legend_visible = True\n    else:\n        legend_visible = False\n    for j in range(len(weathersit_labels)):\n        df_ij = df1[df1['weathersit'] == weathersit_labels[j]][weather_continuous_cols[i]]\n        sns.kdeplot(df_ij,\n                    ax = axs[i],\n                    label = 'Weathersit = {}'.format(weathersit_labels[j]),\n                    color = c[j],\n                    legend = legend_visible,\n                    alpha = 0.6)\n        axs[i].set_xlabel(weather_continuous_cols[i])\n        axs[i].set_xlim((0,1))\nfig.suptitle('Weather parameters distributions in different weather situations')\nplt.show()","82d3ce94":"weather_ranges = []\nfor i in weather_continuous_cols:\n    ranges = pd.cut(df1[i], bins=np.arange(0,1.01,0.05),include_lowest = True)\n    mids = pd.Series(pd.IntervalIndex(ranges).mid, name = i, index = df1.index)\n    weather_ranges.append(mids)\n\nmean_cnt_by_weather = pd.concat(weather_ranges +[df1.cnt], axis = 1)","45f70913":"from scipy.stats import gaussian_kde as kde\ncols_num = len(weather_continuous_cols)\n\nfig,axs = plt.subplots(ncols = cols_num, figsize = (16,4), sharex = True, sharey = True)\n\n#setting shared secondary y\naxs2 = [i.twinx() for i in axs]\naxs2[0].get_shared_y_axes().join(*axs2)\n\n#instantiate X_mash and freq for kdes\nX_mash = np.mgrid[0:1:.05]\nfreq = df1.cnt\n\n#start plotting\nfor i in range(cols_num):\n    #primary axis - mean\n    XY = mean_cnt_by_weather.groupby(weather_continuous_cols[i]).cnt.mean()\n    XY.plot(ax = axs[i],\n            color = 'grey',\n            alpha = 0.8,\n            label = 'Mean rentals')\n    \n    #2nd axis - kde\n    X1_freq = mean_cnt_by_weather[weather_continuous_cols[i]].value_counts(normalize = 1).sort_index()\n    axs2[i].fill_between(X1_freq.index,X1_freq,\n                        color = 'silver',\n                        alpha = 0.1,\n                        label = 'Perameter frequency')\n            \n    #2nd axis - kde\n    X2_freq = mean_cnt_by_weather[weather_continuous_cols[i]].repeat(mean_cnt_by_weather.cnt).value_counts(normalize = 1).sort_index()\n    axs2[i].fill_between(X2_freq.index,X2_freq,\n                 color = 'orange',\n                 alpha = 0.1,\n                 label = 'Rentals frequency')\n    \n    #add xlabel\n    axs[i].set_xlabel(weather_continuous_cols[i])\naxs[0].set_xlim((0,1))\n#add ylabel\naxs[0].set_ylabel('Mean rentals')\naxs2[-1].set_ylabel('Density')\n#add legend\nhandles, labels = axs[0].get_legend_handles_labels()\nhandles1, labels2 = axs2[0].get_legend_handles_labels()\naxs[-1].legend(handles + handles1, labels + labels2, loc = 'upper right')\nplt.suptitle('Mean hourly bike rentals variation with weather parameters')\nplt.show()","ba3373c1":"#let's aggregate on higher level\nweather_ranges = []\nfor i in weather_continuous_cols:\n    ranges = pd.cut(df1[i], bins=np.arange(0,1.01,0.07),include_lowest = True)\n    mids = pd.Series(pd.IntervalIndex(ranges).mid, name = i, index = df1.index)\n    weather_ranges.append(mids)\n#let's plot without atemp\nmean_cnt_by_weather1 = pd.concat(weather_ranges +[df1.cnt], axis = 1)\nweather_cols_wo_atemp = [i for i in weather_continuous_cols if i != 'atemp']\ncols_num = len(weather_cols_wo_atemp)\n\nweather_cols_wo_atemp_comb = combinations(weather_cols_wo_atemp, 2)\nweather_cols_wo_atemp_comb = [list(i) for i in list(weather_cols_wo_atemp_comb)]\ncols_comb_num = len(weather_cols_wo_atemp_comb)\n\nfig,axs = plt.subplots(ncols = cols_comb_num, nrows = 1,\n                       figsize = (15,4), sharex = True, sharey = True)\n\n#start plotting\nfor i in range(cols_comb_num):\n    Z = mean_cnt_by_weather1.pivot_table(index = weather_cols_wo_atemp_comb[i][1],\n                               columns = weather_cols_wo_atemp_comb[i][0],\n                               values = 'cnt',\n                               aggfunc = sum)\n    Z_flat = Z.melt()\n    vmin = Z_flat[Z_flat.value != 0].value.quantile(0.3)\n    axs[i].contour(Z.columns,\n                 Z.index,\n                 Z,\n                 colors='k',\n                 alpha = 0.5,\n                 linewidths=0.2,\n                 levels=10,\n                 vmin = vmin)\n    axs[i].contourf(Z.columns,\n                     Z.index,\n                     Z,\n                     cmap=\"Greys\",\n                     alpha = 0.7,\n                     levels=10,\n                     vmin = vmin)\n    axs[i].set_xlabel(weather_cols_wo_atemp_comb[i][0])\n    axs[i].set_ylabel(weather_cols_wo_atemp_comb[i][1])\nplt.suptitle(\"Bike rentals density at combination of weather parameters\")\nplt.show()","75be23bb":"from scipy.stats import mode\nprint('Bike rentals weather stats:')\nfor i in weather_continuous_cols:\n    weather_params = mean_cnt_by_weather1[i].repeat(mean_cnt_by_weather1.cnt)\n    print(i,\n          '-- mode: ',\n          round(mode(weather_params)[0][0],3),\n          'median: ',\n          round(weather_params.median(),3))\nprint()\nweather_params1 = mean_cnt_by_weather1.groupby(weather_continuous_cols).cnt.sum().sort_values(ascending = False)\nweather_params_top = weather_params1[weather_params1 >= weather_params1.quantile(0.99)].reset_index()\nprint('Top 1% of rentals happened at:')\nfor i in weather_continuous_cols:\n    print(i, round(weather_params_top[i].min(),3), '-', round(weather_params_top[i].max(),3))","61b195bc":"df_clustering = df1[weather_continuous_cols + ['cnt']].copy()\n# normalizing weathersit\ndf_clustering['weathersit'] = (df1['weathersit'] - df1['weathersit'].min())\/df1['weathersit'].max()","f91077c5":"from sklearn.metrics import silhouette_score\nn_clusters = 72\n\n#let's track the time it takes for clusterization cycles\nimport time\nstart_time = time.time()\n\nkm = KMeans(n_clusters=n_clusters, random_state = 0, algorithm = \"full\").fit(df_clustering[weather_continuous_cols\n                                                                                           + ['weathersit']])\n\n#print time in seconds\nprint(\"--- {:.2f} seconds ---\".format(time.time() - start_time))","3fbc2e93":"df_clustering[\"wthr_lbl\"] = km.labels_\n\n#as new labels are random, let's sort them by mean temperature of the cluster in ascending order \n#this should help in visualization\nlbl_ord = list(km.cluster_centers_[:,0])\nZ = [x for _,x in sorted(zip(lbl_ord,list(range(n_clusters))))]\n\n#let's assign the ordered labels to a new column\ndf_clustering[\"new_wthr_lbl_ordered_temp\"] = np.nan\n\nfor i in range(n_clusters):\n    df_clustering.loc[df_clustering[\"wthr_lbl\"] == i,\"new_wthr_lbl_ordered_temp\"] = Z.index(i)","90051d26":"c = sns.color_palette(\"RdBu_r\", n_clusters)\n\nfig,axs = plt.subplots(ncols = 3, nrows = 2, figsize = (15,10))\n\ncenters = pd.DataFrame(km.cluster_centers_, columns = weather_continuous_cols + ['weathersit']).sort_values(by = \"temp\")\n\nfor i in range(len(weather_cols_comb)):\n    sns.scatterplot(data = df_clustering,\n                    x = weather_cols_comb[i][0],\n                    y = weather_cols_comb[i][1],\n                    hue = \"new_wthr_lbl_ordered_temp\",\n                    alpha = 0.3,\n                    ax = axs.flatten()[i],\n                    palette = c,\n                    marker = \"D\",\n                    s = 10,\n                    legend = False)\n    sns.scatterplot(data = centers,\n                    x = weather_cols_comb[i][0],\n                    y = weather_cols_comb[i][1],\n                    hue = centers.index,\n                    ax = axs.flatten()[i],\n                    palette = c,\n                    legend = False,\n                    marker = \"P\",\n                    s = 150)\nplt.suptitle(\"New weather clusters analysis\\n('+' represents centers of clusters)\")\nplt.show()","c4a0ab63":"#to simplify, we will create a table with cnt field of new labels in separate columns\nk1 = df_clustering.pivot_table(values = 'cnt', columns = \"new_wthr_lbl_ordered_temp\", index = df_clustering.index)\n\nfig, axs = plt.subplots(ncols = 2, nrows = 2, figsize = (17,10), sharey = True, sharex = True)\nfor i in range(1,5):\n    k1[df1['weathersit'] == i].plot(marker = \".\",\n                                    linestyle = \"None\",\n                                    alpha = 0.5,\n                                    ax = axs.flatten()[i-1],\n                                    color = c,\n                                    legend = False,\n                                    rot = 0)\n    axs.flatten()[i-1].set_title('Weathersit = {}'.format(i))\naxs.flatten()[0].set_xlim((k1.index.min(),k1.index.max()))\nplt.suptitle(\"New weather labels ordered by temperature on time series\")\nplt.show()","5f734059":"new_lbl_sort_cnt = df_clustering.groupby(\"new_wthr_lbl_ordered_temp\").cnt.mean().sort_values().reset_index()[\"new_wthr_lbl_ordered_temp\"].astype(int)\nnew_lbl_sort_cnt = new_lbl_sort_cnt.reset_index()\nnew_lbl_sort_cnt = new_lbl_sort_cnt.set_index(new_lbl_sort_cnt['new_wthr_lbl_ordered_temp']).iloc[:,0].rename('new_wthr_lbl_ordered_cnt')\nnew_lbl_sort_cnt.sort_index()\ndf_clustering = df_clustering.merge(new_lbl_sort_cnt,left_on = 'new_wthr_lbl_ordered_temp', right_index = True)","a93b748a":"fig, ax = plt.subplots(ncols = 2, nrows = 1, figsize = (17,7), sharex = True, sharey = True)\n\nlabels = [\"new_wthr_lbl_ordered_temp\", \"new_wthr_lbl_ordered_cnt\"]\n\nfor j in range(2):\n    for i in range(1,5):\n        df_index_test = ((df1['weathersit'] == i))\n        df_test = df_clustering[df_index_test][labels[j]].value_counts()\n        ax[j].bar(df_test.index, df_test,\n                  color = sns.color_palette(\"colorblind\", 4)[i-1],\n                  alpha = 0.5)\n    ax2 = ax[j].twinx()\n    df_clustering.groupby(labels[j]).cnt.mean().plot(marker = \".\",\n                                                     ax = ax2,\n                                                     color = 'grey')\n    ax[j].set_ylim((0,600))\n    ax[j].set_title(labels[j])\n    ax[j].set_xlabel('label')\nax[0].set_ylabel('Number of labeled items')\nax2.set_ylabel('Mean rentals')\nplt.suptitle('Reordering weather labels')\nax[0].set_xlim((-1, n_clusters))\nplt.show()","c5f88505":"c2 = sns.color_palette(\"summer\", n_clusters)\nk2 = df_clustering.pivot_table(values = 'cnt', columns = \"new_wthr_lbl_ordered_cnt\", index = df_clustering.index)\n\nfig, axs = plt.subplots(ncols = 2, nrows = 2, figsize = (17,10), sharey = True, sharex = True)\nfor i in range(1,5):\n    k1[df1['weathersit'] == i].plot(marker = \".\",\n                                    linestyle = \"None\",\n                                    alpha = 0.5,\n                                    ax = axs.flatten()[i-1],\n                                    color = c2,\n                                    legend = False,\n                                    rot = 0)\n    axs.flatten()[i-1].set_title('Weathersit = {}'.format(i))\naxs.flatten()[0].set_xlim((k1.index.min(),k1.index.max()))\nplt.suptitle(\"New weather labels ordered by count on time series\")\nplt.show()","559620f7":"# adding anomaly label\nanomaly_dates = filtered.dteday.dt.date\nanomaly_dates_index = df1[pd.Series(df1.index.date, index = df1.index).isin(anomaly_dates)].index\ndf1['anomaly'] = 0\ndf1.loc[anomaly_dates_index, 'anomaly'] = 1\n\n# adding weather label\ndf1['weather_lbl'] = df_clustering[\"new_wthr_lbl_ordered_cnt\"].astype(int)\n\n# converting daytime label to category\ndf1['daytime_lbl'] = df1['daytime_lbl'].astype(\"category\")","89160662":"df2 = pd.get_dummies(df1, columns = ['weathersit'], drop_first = True, prefix = 'weathersit')","d78ef39d":"df2 = pd.get_dummies(df2, columns = ['daytime_lbl'], drop_first = True, prefix = 'daytime_lbl')","560322e4":"calendar_cols = ['instant', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n       'workingday', \"daytime_lbl_[12:16)\", \"daytime_lbl_[16:20)\",\n                 \"daytime_lbl_[20:23]\", \"daytime_lbl_[6:9)\", \"daytime_lbl_[9:12)\"]\nweather_cols = ['temp', 'atemp', 'hum', 'windspeed', \"weather_lbl\", 'weathersit_2','weathersit_3','weathersit_4']\ndrop = ['anomaly']\ntarget_col = \"cnt\"\n\ncolumns_sorted = calendar_cols + weather_cols + drop + [target_col]\ndteday = df2.index\ndf_for_regression = df2[columns_sorted].reset_index(drop = True)","82f3fff3":"corr = df_for_regression.corr()\nf, ax = plt.subplots(figsize=(7,7))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.1, cbar_kws={\"shrink\": .7}, ax = ax)\nax.set_title(\"Fields correlation\")\nplt.show()\nprint(\"correlation of bike rentals (cnt field) with other fields\")\nprint(corr[target_col].sort_values(ascending = False))","53c43cf8":"#dropping anomalies\ndf_for_regression = df_for_regression.drop(df_for_regression.loc[(df_for_regression['anomaly'] == 1)].index).drop(drop, axis = 1)\n","64cc4a2f":"#resetting index for lin reg to work\ndf_for_regression = df_for_regression.reset_index(drop = True)","224543e0":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nkf = KFold(n_splits=5, random_state = 0)\n\ntarget = target_col\nfeats = df_for_regression.columns.drop(target).tolist()\nlen(feats)\n\n# train = df.sample(frac = 0.8, random_state = 1)\n# test = df[~df.index.isin(train.index)]\n","e330dbf8":"# linear regression\nlr_rmses = []\nfor train_index, test_index in kf.split(df_for_regression):\n    train_x = df_for_regression.reindex(index = train_index, columns = feats)\n    train_y = df_for_regression.reindex(index = train_index, columns = [target])\n    test_x = df_for_regression.reindex(index = test_index, columns = feats)\n    test_y = df_for_regression.reindex(index = test_index, columns = [target])\n\n    lr = LinearRegression()\n    lr.fit(train_x,train_y)\n    lr_prediction = lr.predict(test_x)\n\n    lr_rmses.append(np.sqrt(mean_squared_error(test_y, lr_prediction)))\nlr_rmse = np.mean(lr_rmses)\nprint(lr_rmses)\nprint(lr_rmse)","f485cd0e":"null_prediction_rmses = []\nfor train_index, test_index in kf.split(df_for_regression):\n    train_y = df_for_regression.reindex(index = train_index, columns = [target])\n    test_y = df_for_regression.reindex(index = test_index, columns = [target])\n    null_prediction_rmses.append(np.sqrt(mean_squared_error(test_y, [train_y.mean() for i in range(len(test_y))])))\nnull_prediction_rmse = np.mean(null_prediction_rmses)\nprint(null_prediction_rmses)\nprint(null_prediction_rmse)","b9ae55b0":"comb = []\nfor j in range(18, len(feats)+1):\n    comb += [list(i) for i in list(combinations(feats,j))]\nlen(comb)","dc0794e6":"rmse_df = pd.DataFrame(index = [str(i) for i in comb], columns = [\"LR\"])\nstart_time = time.time()\nfor feat_comb in comb:\n    lr_rmses = []\n    for train_index, test_index in kf.split(df_for_regression):\n        \n        train_x = df_for_regression.reindex(index = train_index, columns = feat_comb)\n        train_y = df_for_regression.reindex(index = train_index, columns = [target])\n        test_x = df_for_regression.reindex(index = test_index, columns = feat_comb)\n        test_y = df_for_regression.reindex(index = test_index, columns = [target])\n\n        lr = LinearRegression()\n        lr.fit(train_x,train_y)\n        lr_prediction = lr.predict(test_x)\n\n        lr_rmses.append(np.sqrt(mean_squared_error(test_y, lr_prediction)))\n    lr_rmse = np.mean(lr_rmses)\n    rmse_df.loc[str(feat_comb), \"LR\"] = lr_rmse\nprint(\"--- {:.2f} seconds ---\".format(time.time() - start_time))","8de0fa44":"rmse_df[\"LR\"].sort_values().head(1)","23033f40":"import ast\nlist(set(feats) - set(ast.literal_eval(rmse_df[\"LR\"].sort_values().index[0])))","52d8e44e":"#we'll convert all datetime variables that hide seasonality to dummies\nvars_seasonality = ['season', 'mnth', 'weekday','hr']\ndf_for_regression_dummies = pd.get_dummies(df_for_regression, columns = vars_seasonality,\n               drop_first = True, prefix = vars_seasonality)\n#we dont need daytime labels now\ndf_for_regression_dummies = df_for_regression_dummies.drop(['daytime_lbl_[12:16)', 'daytime_lbl_[16:20)', 'daytime_lbl_[20:23]', 'daytime_lbl_[6:9)', 'daytime_lbl_[9:12)'], axis = 1)\nfeats1 = df_for_regression_dummies.columns.drop(target)","ae0280ed":"lr_rmses = []\nfor train_index, test_index in kf.split(df_for_regression_dummies):\n    train_x = df_for_regression_dummies.reindex(train_index)[feats1]\n    train_y = df_for_regression_dummies.reindex(train_index)[target]\n    test_x = df_for_regression_dummies.reindex(test_index)[feats1]\n    test_y = df_for_regression_dummies.reindex(test_index)[target]\n\n    lr = LinearRegression()\n    lr.fit(train_x,train_y)\n    lr_prediction = lr.predict(test_x)\n\n    lr_rmses.append(np.sqrt(mean_squared_error(test_y, lr_prediction)))\nlr_rmse = np.mean(lr_rmses)\nprint(lr_rmses)\nprint(lr_rmse)","4cc8d7f2":"lr_rmses1 = []\nfor train_index, test_index in kf.split(df_for_regression_dummies):\n    \n    #splitting to train and test\n    train_x = df_for_regression_dummies.reindex(train_index)[feats1]\n    test_x = df_for_regression_dummies.reindex(test_index)[feats1]\n    train_y = df_for_regression_dummies.reindex(train_index)[target]\n    test_y = df_for_regression_dummies.reindex(test_index)[target]\n\n    #pulling the trend\n    lr = LinearRegression()\n    lr.fit(train_x['instant'].values.reshape(-1,1),train_y)\n    \n    linear_approx_s1 = lr.predict(train_x['instant'].values.reshape(-1,1))\n    linear_approx_s2 = lr.predict(test_x['instant'].values.reshape(-1,1))\n\n    \n    #pulling trend-corrected prediction\n    train_x1 = train_x[list(set(feats1) - set('instant'))]\n    test_x1 = test_x[list(set(feats1) - set('instant'))]\n    train_y1 = train_y \/ linear_approx_s1\n    test_y1 = test_y \/ linear_approx_s2\n\n    lr1 = LinearRegression()\n    lr1.fit(train_x1,train_y1)\n    \n    trend_corrected_approx_s1 = lr1.predict(train_x1)\n    trend_corrected_approx_s2 = lr1.predict(test_x1)\n    \n    #combining the predictions\n    total_approx_s2 = linear_approx_s2 * trend_corrected_approx_s2\n    \n    #calculating rmses\n    rmse = np.sqrt(mean_squared_error(test_y, total_approx_s2))\n    lr_rmses1.append(rmse)\n\nlr_rmse1 = np.mean(lr_rmses1)\nprint(lr_rmses1)\nprint(lr_rmse1)","945b1e39":"#we have done several manipulations with df, and now it is not in best shape for decision tree\n#let's recompile it\ndf_for_trees = df_for_regression.merge(df1[['weathersit','daytime_lbl']], left_index = True, right_on = df1.reset_index().index, how = 'inner')\ndf_for_trees = df_for_trees.drop(['daytime_lbl_[12:16)', 'daytime_lbl_[16:20)', 'daytime_lbl_[20:23]', 'daytime_lbl_[6:9)', 'daytime_lbl_[9:12)'], axis = 1)\ndf_for_trees = df_for_trees.drop(['key_0','weathersit_2', 'weathersit_3', 'weathersit_4'], axis = 1)\ndf_for_trees = df_for_trees.reset_index(drop = True)\ndf_for_trees['daytime_lbl'] = df_for_trees['daytime_lbl'].cat.codes\nfeats_dt = df_for_trees.columns.drop(target)","1cbdbf1c":"dt_rmses_train = []\ndt_rmses_test = []\nfor train_index, test_index in kf.split(df_for_trees):\n    train_x = df_for_trees.reindex(train_index)[feats_dt]\n    train_y = df_for_trees.reindex(train_index)[target]\n    test_x = df_for_trees.reindex(test_index)[feats_dt]\n    test_y = df_for_trees.reindex(test_index)[target]\n\n    dt = DecisionTreeRegressor(random_state = 0)\n    dt.fit(train_x,train_y)\n    dt_prediction_train = dt.predict(train_x)\n    dt_prediction_test = dt.predict(test_x)\n\n    dt_rmses_train.append(np.sqrt(mean_squared_error(train_y, dt_prediction_train)))\n    dt_rmses_test.append(np.sqrt(mean_squared_error(test_y, dt_prediction_test)))\n    \ndt_rmse_train = np.mean(dt_rmses_train)\ndt_rmse_test = np.mean(dt_rmses_test)\n\nprint('Train error:')\nprint(dt_rmses_train)\nprint(dt_rmse_train)\nprint()\nprint('Test error:')\nprint(dt_rmses_test)\nprint(dt_rmse_test)","e5930b08":"lf = np.arange(1,51, 5)\nspl = np.arange(2,51, 5)\n\nspl_lf_df = pd.DataFrame(columns = lf, index = spl)\n\nstart_time = time.time()\nfor i in spl:\n    for j in lf:\n        dt_rmses_train = []\n        dt_rmses_test = []\n        for train_index, test_index in kf.split(df_for_trees):\n            train_x = df_for_trees.reindex(train_index)[feats_dt]\n            train_y = df_for_trees.reindex(train_index)[target]\n            test_x = df_for_trees.reindex(test_index)[feats_dt]\n            test_y = df_for_trees.reindex(test_index)[target]\n\n            dt = DecisionTreeRegressor(min_samples_split = i,\n                                       min_samples_leaf = j, random_state = 0)\n            dt.fit(train_x,train_y)\n            dt_prediction_train = dt.predict(train_x)\n            dt_prediction_test = dt.predict(test_x)\n\n            dt_rmses_train.append(np.sqrt(mean_squared_error(train_y, dt_prediction_train)))\n            dt_rmses_test.append(np.sqrt(mean_squared_error(test_y, dt_prediction_test)))\n\n        dt_rmse_train = np.mean(dt_rmses_train)\n        dt_rmse_test = np.mean(dt_rmses_test)\n        spl_lf_df.loc[i,j] = (round(dt_rmse_test,2),round(dt_rmse_train,2))\nprint(\"--- {:.2f} seconds ---\".format(time.time() - start_time))","e86a8a55":"spl_lf = spl_lf_df.reset_index().melt(id_vars = \"index\")\nspl_lf = pd.concat([spl_lf,pd.DataFrame(spl_lf['value'].tolist(), index=spl_lf.index)], axis = 1)\nspl_lf = spl_lf.drop(\"value\", axis = 1)\nspl_lf.columns = [\"min_samples_split\", \"min_samples_leaf\", \"rmse_test\",\"rmse_train\"]\nspl_lf [\"abs_diff\"] = (spl_lf[\"rmse_test\"] - spl_lf[\"rmse_train\"]).abs()","361c3fa1":"print('Optimal parameters (top 3):')\nprint(spl_lf.sort_values(by = [\"abs_diff\",\"rmse_test\",\n                               \"min_samples_leaf\",\"min_samples_split\"]).head(3))\nprint()\nprint('Lowest rmse for test set parameters (top 3):')\nprint(spl_lf.sort_values(by = [\"rmse_test\",\"min_samples_leaf\",\"min_samples_split\"]).head(3))","a914ec9d":"def combined_lin_dt(spl = 2, lf = 1):\n    dt_rmses_train = []\n    dt_rmses_test = []\n    for train_index, test_index in kf.split(df_for_regression_dummies):\n\n        #splitting to train and test\n        train_x = df_for_regression_dummies.reindex(train_index)[feats1]\n        test_x = df_for_regression_dummies.reindex(test_index)[feats1]\n        train_y = df_for_regression_dummies.reindex(train_index)[target]\n        test_y = df_for_regression_dummies.reindex(test_index)[target]\n\n        #pulling the trend\n        lr = LinearRegression()\n        lr.fit(train_x['instant'].values.reshape(-1,1),train_y)\n\n        linear_approx_s1 = lr.predict(train_x['instant'].values.reshape(-1,1))\n        linear_approx_s2 = lr.predict(test_x['instant'].values.reshape(-1,1))\n\n\n        #pulling trend-corrected prediction\n        train_x1 = train_x[list(set(feats1) - set('instant'))]\n        test_x1 = test_x[list(set(feats1) - set('instant'))]\n        train_y1 = train_y \/ linear_approx_s1\n        test_y1 = test_y \/ linear_approx_s2\n\n        dt = DecisionTreeRegressor(min_samples_split = spl,\n                                   min_samples_leaf = lf,\n                                   random_state = 0)\n        dt.fit(train_x1,train_y1)\n        trend_corrected_approx_s1 = dt.predict(train_x1)\n        trend_corrected_approx_s2 = dt.predict(test_x1)\n\n        #combining the predictions\n        total_approx_s1 = linear_approx_s1 * trend_corrected_approx_s1\n        total_approx_s2 = linear_approx_s2 * trend_corrected_approx_s2\n\n\n        #calculating rmses\n        dt_rmses_train.append(np.sqrt(mean_squared_error(train_y, total_approx_s1)))\n        dt_rmses_test.append(np.sqrt(mean_squared_error(test_y, total_approx_s2)))\n    return(dt_rmses_train,dt_rmses_test)","f2d2e16b":"dt_rmses_train,dt_rmses_test = combined_lin_dt(spl = 2, lf = 1)\ndt_rmse_train = np.mean(dt_rmses_train)\ndt_rmse_test = np.mean(dt_rmses_test)\nprint('Train error:')\nprint(dt_rmses_train)\nprint(dt_rmse_train)\nprint()\nprint('Test error:')\nprint(dt_rmses_test)\nprint(dt_rmse_test)","8969fddf":"lf1 = np.arange(1,51,10)\nspl1 = np.arange(2,51,10)\n\nspl_lf_df1 = pd.DataFrame(columns = lf1, index = spl1)\n\nstart_time = time.time()\nfor i in spl1:\n    for j in lf1:\n        dt_rmses_train = []\n        dt_rmses_test = []\n        for train_index, test_index in kf.split(df_for_trees):\n            rmses_train, rmses_test = combined_lin_dt(spl = i, lf = j)\n            dt_rmses_train.append(np.mean(rmses_train))\n            dt_rmses_test.append(np.mean(rmses_test))\n\n        dt_rmse_train = np.mean(dt_rmses_train)\n        dt_rmse_test = np.mean(dt_rmses_test)\n        spl_lf_df1.loc[i,j] = (round(dt_rmse_test,2),round(dt_rmse_train,2))\nprint(\"--- {:.2f} seconds ---\".format(time.time() - start_time))","273d8e94":"spl_lf1 = spl_lf_df1.reset_index().melt(id_vars = \"index\")\nspl_lf1 = pd.concat([spl_lf1,pd.DataFrame(spl_lf1['value'].tolist(), index=spl_lf1.index)], axis = 1)\nspl_lf1 = spl_lf1.drop(\"value\", axis = 1)\nspl_lf1.columns = [\"min_samples_split\", \"min_samples_leaf\", \"rmse_test\",\"rmse_train\"]\nspl_lf1 [\"abs_diff\"] = (spl_lf1[\"rmse_test\"] - spl_lf1[\"rmse_train\"]).abs()\nprint('Optimal parameters (top 3):')\nprint(spl_lf1.sort_values(by = [\"abs_diff\",\"rmse_test\",\n                               \"min_samples_leaf\",\"min_samples_split\"]).head(3))\nprint()\nprint('Lowest rmse for test set parameters (top 3):')\nprint(spl_lf1.sort_values(by = [\"rmse_test\",\"min_samples_leaf\",\"min_samples_split\"]).head(3))","33f411ce":"rf_rmses_train = []\nrf_rmses_test = []\nfor train_index, test_index in kf.split(df_for_trees):\n    train_x = df_for_trees.reindex(train_index)[feats_dt]\n    train_y = df_for_trees.reindex(train_index)[target]\n    test_x = df_for_trees.reindex(test_index)[feats_dt]\n    test_y = df_for_trees.reindex(test_index)[target]\n\n    rf = RandomForestRegressor(n_estimators = 10, random_state = 0)\n    rf.fit(train_x,train_y)\n    rf_prediction_train = rf.predict(train_x)\n    rf_prediction_test = rf.predict(test_x)\n\n    rf_rmses_train.append(np.sqrt(mean_squared_error(train_y, rf_prediction_train)))\n    rf_rmses_test.append(np.sqrt(mean_squared_error(test_y, rf_prediction_test)))\n    \nrf_rmse_train = np.mean(rf_rmses_train)\nrf_rmse_test = np.mean(rf_rmses_test)\n\nprint('Train error:')\nprint(rf_rmses_train)\nprint(rf_rmse_train)\nprint()\nprint('Test error:')\nprint(rf_rmses_test)\nprint(rf_rmse_test)","2cf42a80":"rf_rmse_train_plot = []\nrf_rmse_test_plot = []\n\nlf = range(1,11)\n\nfor i in lf:\n    rf_rmses_train = []\n    rf_rmses_test = []\n    for train_index, test_index in kf.split(df_for_trees):\n        train_x = df_for_trees.reindex(train_index)[feats_dt]\n        train_y = df_for_trees.reindex(train_index)[target]\n        test_x = df_for_trees.reindex(test_index)[feats_dt]\n        test_y = df_for_trees.reindex(test_index)[target]\n\n        rf = RandomForestRegressor(n_estimators = 10, random_state = 0, min_samples_leaf = i)\n        rf.fit(train_x,train_y)\n        rf_prediction_train = rf.predict(train_x)\n        rf_prediction_test = rf.predict(test_x)\n\n        rf_rmses_train.append(np.sqrt(mean_squared_error(train_y, rf_prediction_train)))\n        rf_rmses_test.append(np.sqrt(mean_squared_error(test_y, rf_prediction_test)))\n\n    rf_rmse_train = np.mean(rf_rmses_train)\n    rf_rmse_test = np.mean(rf_rmses_test)\n    \n    rf_rmse_train_plot.append(rf_rmse_train)\n    rf_rmse_test_plot.append(rf_rmse_test)","4cf6ca4e":"fig, ax = plt.subplots(figsize = (7,5))\nax.plot(lf, rf_rmse_test_plot, color = 'grey', alpha = 0.7, label = 'test set prediction rmse')\nax.plot(lf, rf_rmse_train_plot, color = 'orange', alpha = 0.7, label = 'train set prediction rmse')\nplt.suptitle('Error of Random Forest model with k-number of min samples leaf')\nax.set_xlabel('k-number of min samples leaf')\nax.set_ylabel('rmse')\nax.legend()\nplt.show()","3c53b101":"print('min rmse: {:.2f} at min_samples_leaf = {}'.format(min(rf_rmse_test_plot),\n                                                        (rf_rmse_test_plot.index(min(rf_rmse_test_plot)) + 1)))","074ac4b2":"def combined_lin_rf(spl = 2, lf = 1):\n    rf_rmses_train = []\n    rf_rmses_test = []\n    for train_index, test_index in kf.split(df_for_regression_dummies):\n\n        #splitting to train and test\n        train_x = df_for_regression_dummies.reindex(train_index)[feats1]\n        test_x = df_for_regression_dummies.reindex(test_index)[feats1]\n        train_y = df_for_regression_dummies.reindex(train_index)[target]\n        test_y = df_for_regression_dummies.reindex(test_index)[target]\n\n        #pulling the trend\n        lr = LinearRegression()\n        lr.fit(train_x['instant'].values.reshape(-1,1),train_y)\n\n        linear_approx_s1 = lr.predict(train_x['instant'].values.reshape(-1,1))\n        linear_approx_s2 = lr.predict(test_x['instant'].values.reshape(-1,1))\n\n\n        #pulling trend-corrected prediction\n        train_x1 = train_x[list(set(feats1) - set('instant'))]\n        test_x1 = test_x[list(set(feats1) - set('instant'))]\n        train_y1 = train_y \/ linear_approx_s1\n        test_y1 = test_y \/ linear_approx_s2\n        \n        rf = RandomForestRegressor(n_estimators = 10,\n                                   min_samples_split = spl,\n                                   min_samples_leaf = lf,\n                                   random_state = 0)\n        \n        rf.fit(train_x1,train_y1)\n        trend_corrected_approx_s1 = rf.predict(train_x1)\n        trend_corrected_approx_s2 = rf.predict(test_x1)\n\n        #combining the predictions\n        total_approx_s1 = linear_approx_s1 * trend_corrected_approx_s1\n        total_approx_s2 = linear_approx_s2 * trend_corrected_approx_s2\n\n\n        #calculating rmses\n        rf_rmses_train.append(np.sqrt(mean_squared_error(train_y, total_approx_s1)))\n        rf_rmses_test.append(np.sqrt(mean_squared_error(test_y, total_approx_s2)))\n    return(rf_rmses_train,rf_rmses_test)","bd2859e1":"combined_lin_rf(spl = 2, lf = 1)\n\nrf_rmses_train,rf_rmses_test = combined_lin_rf(spl = 2, lf = 1)\nrf_rmse_train = np.mean(rf_rmses_train)\nrf_rmse_test = np.mean(rf_rmses_test)\nprint('Train error:')\nprint(rf_rmses_train)\nprint(rf_rmse_train)\nprint()\nprint('Test error:')\nprint(rf_rmses_test)\nprint(rf_rmse_test)","7e2e3711":"rf_rmse_train_plot = []\nrf_rmse_test_plot = []\n\nlf = range(1,11)\n\nfor i in lf:\n    rf_rmses_train, rf_rmses_test = combined_lin_rf(lf = i)\n    \n    rf_rmse_train = np.mean(rf_rmses_train)\n    rf_rmse_test = np.mean(rf_rmses_test)\n    \n    rf_rmse_train_plot.append(rf_rmse_train)\n    rf_rmse_test_plot.append(rf_rmse_test)","62bd6fac":"fig, ax = plt.subplots(figsize = (7,5))\nax.plot(lf, rf_rmse_test_plot, color = 'grey', alpha = 0.7, label = 'test set prediction rmse')\nax.plot(lf, rf_rmse_train_plot, color = 'orange', alpha = 0.7, label = 'train set prediction rmse')\nplt.suptitle('Error of combined Linear Regression - Random Forest model with k-number of min samples leaf')\nax.set_xlabel('k-number of min samples leaf')\nax.set_ylabel('rmse')\nax.legend()\nplt.show()","648878ce":"print('min rmse: {:.2f} at min_samples_leaf = {}'.format(min(rf_rmse_test_plot),\n                                                        (rf_rmse_test_plot.index(min(rf_rmse_test_plot)) + 1)))","ef05aecb":"We see an improvement of accuracy from rmse 117 to 106. The only problem here is that if we create too many dummies it might make our model less smooth and the computation will be longer as well.\n\n3. Pulling the trend\n\nIf the function is more complex than linear, we can decompose it by different methonds. Here, for example, one of the basic improvements that we could make would be calculating the general trend by linear regression, then normalizing the values to it and using the linear regression once again. In the end we will need to reverse the steps. Let's see how it could help.","e3648a0d":"The model fed with all the feats has rmse 117.7. Let's check if it is better than a simple guess","8fbb8243":"From the above we see that the model performs better than our 'best' linear regression model. However, it is significancly overfitted: rmse for the training set prediction is 0 whereas for test set it is not. We should generalize the model a bit: let's test different model parameters.\n\n### Decision tree model optimization\n\nWe will optimize the model in several ways: by finding the optimal parameters and by combining it with linear regression.\n\n1. Finding optimal parameters for a simple decision tree\n\nWe will test variation of min samples split and min samples leaf parameters of the model in order to generalize it. What we are looking for is the minimal difference between rmse for test and train sets' predictions. ","22b2b6af":"We have 17379 rows and 17 columns in our data set:\n\n- Calendar fields\n\n    For convenience, we will combine 'dteday' and 'hr' as a datetime field. Fields 'season', 'yr', 'mnth' and 'weekday' could be obtained from field 'dteday'. We can already notice that 'weekday' is not in the standard format where 0 is Monday and 6 is Sunday. To avoid any error in our analysis, we will substitute it with the standard.\n\n    'Workingday' feld was obtained by combining weekends with holidays in a way that would allow to classify days as working or non-working.\n    \n\n- Weather fields\n\n    From the Hadi Fanaee-T's article, we know that there were missing weather reports for some hours, which were filled by the closest reports.\n\n    'Weathersit' column was also created by the authors of the data set: it grades the weather conditions provided in the weather data as good, cloudy, bad and very bad. The methodology of ranking was not described, but the discription of the field suggests that it was produced by aggregating qualitative data from the weather web site. We will study the weathersit labels in the next part of our analysis.\n    \n\n- Bike rentals fields\n\n    We have 3 fields: 'casual', 'registered' and their sum 'cnt'. Whe will not use the first two parameters in this study; therefore, we can drop them.\n    \n\n- Instant field\n\n    The 'instant' field might be useful for us as it shows the order of logs. However, we need to check if there is any missing hourly log in the dataset and decide what to do with it. Missing logs might occur if there were no rentals during that hour, if there was a downtime, or in case of change from summer to winter time.\n\n\nIn this part of the analysis we will do the following:\n\n1. remove the unnecessary fields\n2. check for duplicates and nulls\n3. add 'hr' to 'dteday' datetime field and check correctness if calendar data labels\n4. inspect missing logs\n\n___\n1. Let's remove unnecessary fields","8edc9e3b":"Looking at bike rentals densities at combined weather parameters, we see that rides happened more frequently at around temp 0.665, hum 0.455 and windspeed 0.105.\n\n### Labels for weather\n\nLet's move forwards with creating another weather categorization based on the combination of the four continuous weather representation fields that are present in the dataset, weathersit and daytime labels. We'll utilize k-means clustering technique for that.\n\nWe will not spend time here on the selection of the best number of clusters using the silhouette score, as this will be too time consuming. However, we can use the following logic for choosing it: (6 day times * 4 seasons * 3 weather situations) = 72. \n\nIt is worth mentioning here that we will not use the bike rentals numbers in our clusterization excercise. However, we will order the labels by the mean rentals in order to get better performance of linear regression. It might be one of the flaws of this study, but we'll still do this. Afterall, this excersize could have been done similarly on a train subset of data, if labels ordered by trend-corrected rentals (rentals devided by the estimated value of the trend component). Still, we acknowledge that it would be better to split the dataset to test and train sets, do clusterization for the train set and look how it improves predictions on the test set.\n\nLet's create these labels now.","a7c330aa":"From the above chart we see that the error of train set increases gradually, whereas the error of test set prediction stays at around the same level. However, there is an optimal point in the tested range.\n\nLet's test the combined model now.","33790dc4":"From the above figure we see that each daytime segment has its distinct features and is well separated from the other segments.\n\n## Weather parameters analysis\n\nWe  have an ordinal data field 'weahersit', which represents weather situation label. In the [data specification](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset), it is stated, that this field classifies the weather by the following parameters:\n\n- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\nThis classification does not provide full information about the weather. For a more complete picture, we have 4 other continuous normalized fields (temp, atemp, hum, windspeed), which we could use for clustering.\n\nFirst, let's analyze the 'weathersit' field to understend how it affects bike rentals and how it varies with the change of other weather paremeters.","1ff86f59":"From the numbers we see that not only the error became less, but it is much more generalized as well.\n\nLet's try random forest model next.\n\n\n## Random forest model\n\nLet's finish the project with re-iteration of the previous analysis - now with random forest model. The model should be better generalized vs decision tree, and the accuracy should be even higher. It is interesting to see how better it can be than the previous ones.\n\nLike before, we will look at basic performance, and then we will try to optimize it.","2ece21c7":"There is an overlap of seasons in March, June, September and December. Let's find the dates when the seasons change.","b52102c5":"On the above charts we can see that the shapes remain the same with variation of seasons and workingdays. The only noticeable features are the increase of overall level of demand and a slightly more spread graph for non-working summer days.\n\n\n## Labeling hourly data\n\nThe last exercise of this project will be focused on the assessment of different prediction models for bike rentals data. In order to have a better prediction accuracy, we need to find some patterns in our data that would be useful.\n\nWe have already found 5 cases with long breaks in logs, and noticed that the demand for bikes was very low at those days vs average. We can either label them as anomalies or drop as noize.\n\nThere are also other labels, which we could add:\n\n- hourly segmentation of demand trend, which we'll do now\n- segmentation by weather conditions, which we'll do after studying weather parameters\n\nBased on the hourly bike rentals trend from the above figure, we can distinguish 6 hour segments:\n\n- 00hr - 06hr\n- 09hr - 12hr\n- 12hr - 16hr\n- 16hr - 20hr\n- 20hr - 24hr\n\nLet's add these labels to the data and visually check if they coud be useful for increasing the accuracy of further prediction exercise.","6af13647":"We can conclude that in missing hours there were no rentals. Let's spend some time on studying the number of consecutive hours without bike rentals and their frequencies.","c5e14996":"We see a clear monthly seasonality. In late Spring and early Autumn the number of bike rentals is 2 times higher on average. We can also notice that the number of bike rentals increased by more than 2 times over 2 years.\n\nLet's find seasonality coefficients for months, weekdays and hours.","3938ccc6":"We have a dataset of 17379 rows without nulls and duplicates.\n\n3. Let's check calendar labels","4ba8da10":"We will check linear model performance for 1562 combinations of features. That's a lot. Let's track how much time it takes.","4dd70ef0":"Some happened in the night, and some in the evening. Let's plot the above cases to see their distribution on the timeline.","dded2557":"On the above plot we see that the new labels capture not only the weather situation, but also the mean number of rentals in the cluster. Once again, this reordering should be better done as a part of a prediction model (at fit stage). But we'll assume that the results would not change significantly for simplicity. \n\n## Preparation of the dataset for ML exercise and study of variables correlation\n\nNow, we will append all the extra features, which we created, and prepare the dataset for machine learning.","2d75260c":"The months are correct. Let's see if there are errors in seasons labels. 1 should be winter, 4 should be fall.","d1ab34fd":"In the beginning it was usual to have 1 - 2 hr break in rentals somewhere in the night. We can think of 2 reasons: either it was dictated by natural demand drop, or by rental availability (e.g. maintenance of all the bikes, system downtime).\n\nWe can see that in late spring - early autumn there were no breaks in logs. This speaks of a demand decrease in low season's nights, which caused missing logs.\n\nLet's investigate the peaks. Most probably they were the result of some anomaly. Looking at the number of daily rentals on the above figure, we see that there was a decrease in rentals prior the cases with long breaks. Let's zoom in to see what was going on there.","82b90ce7":"The results are very close to the best that was obtained in the previous model. We see that the model is well generalized already. Let's check how change of min_samples_leaf parameter would affect the result.","a1e23277":"That's actually a good result, because we managed to predict test with error close to the previous model.\n\n3. Finding optimal parameters for the combined model\n\nNow, let's finish with optimization of the model by selecting the optimal parameters.","b698e181":"The above figure tells us that there were extremely little number of cases with weathersit = 4 - only 3 hours. The weather situation is tipically better in the summer. Looking at the mean number of rentals, we can say that in the spring there were relatively more overall rentals with weathersit = 1 vs other seasons. It might be either due to social reaction to better weather after the winter, or due weather = 1 better aligning with peak hours for bike rentals in spring. Let's add hour dimension to the above plot in order to see this.","3a0c0dc7":"## Understanding the trend\n\nWe have already seen that the bkie rentals are affected by both time and weather parameters. It is also greatly reliant on the external events, such as holidays, weather anomalies, political events. Moreover, it is dependant on the number of stations, bicycles, city planning, transport situation, pollution, change of social habits etc. \n\nIf we think of bike rentals as a function of time and weather, we can come to the following thoughts:\n1. Bikerentals should be more dependent on weather, than on time. But weather parameters change with time: there is yearly and daily seasonalities, and might be some more, which could be dictated by relative earth positions in space and processes in the Sun and the Earth's core.\n2. Most people show different behavioral patterns on working days, weekends and holidays. But these patterns vary with the weather conditions and the length of a day. \n3. The actual numbers of bike rentals might be constrained by the capacity of the system. It might grow with the development of the bike rental system. It can also be changing over time due to the changes in social demands and perception of bike rentals. \n\nOverall, we can expect to see some general trendline, which is time dependant, and seasonalities, which are time and weather dependant. \n\nLet's look at the change of bike rentals over time at different aggregation levels to study the patterns. First, we need to understand the general trend.","e1758f29":"Elimination of month, hum and instant variables resulted in some reduction of rmse. Hum and month parameters are indeed already well captured in weather_lbl, and instant we kept for a later excercise - it will help us with extraction of trend. Absense of the month predictor also makes sense in a way that with increase of the month number, the bike rentals change non linearly. This is also true for other daytime parameters that hide seasonality. In order for linear model to work better with them, they should either be ordered by the mean rentals on the train stage, or we could find the seasonality coefficients and apply them to the trendline. Another possible solution might be the creation of dummy variables out of them. Let's see how this works next.\n\n2. Converting fields to dummy\n\nLet's check the performance of linear model with dummy variables for all datetime features. We will need to remove our daytime labels now because it will not add value when there will be dummy hour variables.","539941f8":"Indeed, the actual number of rented bikes was much lower than the mean for the same hour in the same month. Let's create a visualization of the change of bike rentals within some timeperiod which should include the period with missing logs. For this, we can write a function that will create a slice of data for the period n-days before and k-days after the specified date. In order to better understand how the numbers deviate from norm, the function will also return  the slices of data for other same weekdays within the month of the specified date.","3cb44841":"What we see is that humidity it the most important factor for weathersit - weathersit 3 occur more often with higher humidity. A bit lower temperature is also a feature for such conditions. As for the windspeed, we see that the distribution for weathersit 3 is a bit less sqewed to the right than for weathersit 1 and 2.\n\nLet's see how bike rentals vary with weather parameters.","380b6a07":"We are now ready to plot the zoomed-in graphs for all the cases with prolonged missing logs of bike rentals. With the help of the written function, we can actually plot not only the 'cnt' column, but also the weather columns as well.","2843df2e":"We were able to slightly improve the model's performance. With that, we will finish our excercise of optimization of regression models. Overall, we see that a random forest produces better results than the other models, but it can be improved further by combination with other regression methods in a meaningful way. \n\n## As a conclusion\n\nIn this project we studied the data on hourly bike rentals in Washington D.C. in 2011-2012: cleaned the data set, conducted an exploratory analysis, created some new labels and tried to fit 3 machine learning algorythms to it: linear regression, decision tree and random forest.\n\nA combination of a linear and a random forest regresions resulted a much more accurate prediction of demand for hourly bike rentals.\n\nThe findings of this study could be a good basis for building more complex prediction models for different purposes based on bike rentals data.","38e43bc0":"We can check the average demand for rented bikes in the same year-month-hour to see if there was a drop in numbers on log breaks - this will indicate that something was happening there. We see that relative humidity is on the higher level for these rows, and for row 5635 the wind speed is very high as well. For all of the cases weathersit label is also high in the last hour before the break. The temperatures, on the other hand, vary from low to mid-level as some of the cases happened in the winter, and some - in the summer. This might indicate that weather might have been a factor, which caused the decrease in bike rentals. \n\nLet's compare the number of rentals right before and immediately after the missing logs for all the cases vs average level for the same year-month-hour combinations. Then, we'll plot the trend lines 2 days before and 2 days after the missing logs for them.","3e015f6c":"As we see from the specification, this field is ordinal, but the difference between different levels is, well, different. If we think about it, thunderstorms or snowing affect the decision for using bikes much more than knowing that it will not be sunny (weathersit 1), but rather a bit cloudy (weathersit 2). We can see this on the violin plot above: the median value for bike rentals does not decrease linearly - it is more affeted by higher number of weathersit indicator. Moreover, the distributions change differently as well: for weathersit 1 and 2 they are very similar, whereas for 3 it is much more right-skewed. For weathersit 4 it is hard to tell anything because there were registered only 3 such cases.\n\nThat being said, we should already get the feeling that transforming these ordinal labels to dummy variables should provide us better overall in a linear regression model.\n\nFrom the distribution of weathersit on the timeline, we see that it does not distinctly separate winter from summer. Let's check how it varies with other weather parameters and then we'll create some new labels out of them.","f9bd0a89":"Most of the interruptions happened for not more than 1-2 hrs. And there were 5 cases when the logs were missing for 6-36 hours.\nLet's find the hours after which logs disappearred for the cases with short intervals (<= 2 hr):","9c06799f":"### Linear regression\n\nAs the first step, we will use all features as predictors and calculate the error. Next, we will use different optimization methods. Let's do the first prediction.","fb85800b":"Let's create dummy variables for weathersit, as we discussed above. We will also drop weathersit = 1 - we'll think of it as a default weather situation.","e9139d59":"On the chart we see that the mean hourly bike rentals have almost linear dependency from temperature. However, after temp around 0.8, the number of mean rentals starts decreasing. Looking at atemp, we see that this drop is even more vivid - this might indicate that at higher temperatures people start choosing activities other than cycling. Bike rentals frequency distribution by temperature (orange area) is more skewed to the left than the frequency of temperatures (grey area) - this speaks of the fact that people prefer to cycle at higher temperatures.\n\nWe also see that lower humidity is also better for cycling. Yet, such cases are relatively rare. When they occur, there is more demand for bicycle rental services.\n\nLooking at the change of rentals with the increase of windspeed, we can think of the following explanation: wind is on average slower in the nights, or at mild weather conditions, when the demand for bike sharing is also lower on average. This explains the mean rentals at less windy times being not the highest. However, when it is more windy, rentals start decreasing as well. We see mean rentals going up at windspeed 0.8 and higher, but such cases are so rare that we could assume them to be statistical errors.","07c4f768":"It is also interesting to see how the trend changes with season and what is the difference between working and non-working days. Let's plot the mean nr of hourly rentals now to also see the change in level of demand. ","073afd08":"Our new cluster separate weathersit as well: the bars are colored blue for weathersit 1, orange - for 2, green - for 3. The label number increases with the increase of mean rentals almost linearly. We'll keep this labeling. Let's plot it on the timeline now.","39adc410":"Linar model produced rmse 117.9, whereas null prediction - 182.3. This is a significant improvement.\n\n### Tweaking linear model\n\nLet's see how we could improve the model. There are several ways how to do this: addition of new feats, selection of fewer and more meaningful predictors, or by turning some ordinal variables to dummy. We can also use math methods for pulling different components of the function and then combining their approximations. Let's spend some time on studying how basic manipulations improve the result.\n\n1. Selecting less features\n\nLet's check if there is any field, without which the prediction would be even more accurate. For this, we'll create combinations of features of different sizes. As the number of columns is relatively high, we will limit ourselves by reducing the number of feats to not lower than 18.","193c4442":"From correlation coefficients we can see that weather label has the highest effect on cnt now.\n\n\n## Regression models test\n\nIn this last exercise, we will test 3 ML regression methods: Linear Model, Decision Tree and Random Forest. As an error metric we'll use rmse because it works well with continuous numeric data. We will use k Fold x-validation for estimating performance of the models. Let's initialize it below.","47b51ca6":"So we have 165 missing hourly logs.  Let's see if there is a pattern behind. First, we need to understand if there are 0 values in 'cnt' column. Their absence will give us a hint that missing logs are an indication of zero rentals.","e0c99f28":"We have found that the seasons are mapped according to the following date intervals:\n\n- \\[Dec 21, Mar 21) - Season 1\n- \\[Mar 21, Jun 21) - Season 2\n- \\[Jun 21, Sep 23) - Season 3\n- \\[Sep 23, Dec 21) - Season 4\n\nNow, let's change 'weekday' column to label days as 0 - Monday, 6 - Sunday.","9ef69cc5":"4. Next, we'll check if we have missing logs, i.e. if some of the hourly information is missing in the dataset.","3724fdd8":"We see that calculating only one simple clusterization took several seconds. That's why we are not experimenting with the number of clusters.\n\nLet's visualize the result of labeling the data based on weather conditions. But first, we should redo our clusterization and order the procuced labels in a meaningful way (by temperature, for example) for more appealing graphs.","1fc305a7":"Looks like the clusters do a good job in separating some of the features of the dataset. Let's order them by mean rentals now.","b3334b35":"The increase of min_samples_splin and min_samples_leaf allowed us to generalize the model and slightly improve the accurac as well. Now we have error around 80.\n\n2. Combination with linear regression\n\nLet's repeat the last exercise of the linear model's optimization: we'll find the trend first with linear regression, and then use decision tree to predict the rest. We'll do 2 iterations with standard parameters of decision tree and then we'll try to optimize the model.","ecd78cce":"2. Let's check for duplicates and nulls","271ed15c":"# Analysis of Bike Rentals and study of ML algorythms performance\n\n## Introduction and data description\n\nIn this study we will conduct exploratory analysis of bicycle rentals in Washington D.C. in 2011-2012. We will also explore the performance of several ML algorythms for prediction of the number of hourly rentals. We will use the following models: linear regression ,decision tree and random forest. We will also utilize k-means clustering algorythm for grouping the weather parameters.\n\nThe dataset was pulled from the [UCI website](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Bike+Sharing+Dataset). It contains information on hourly bike rentals, weather parameters and some datetime features. It was combined by Hadi Fanaee-T for his research article [Event labeling combining ensemble detectors and background knowledge](https:\/\/link.springer.com\/article\/10.1007\/s13748-013-0040-3), which was done in collaboration with Joao Gama. \n\nThe data set was obtained by combining bike rental logs in Washington D.C. from [Capital Bikeshare](https:\/\/www.capitalbikeshare.com\/system-data) with [weather parameters](http:\/\/www.freemeteo.com) and [holiday schedule](http:\/\/dchr.dc.gov\/page\/holiday-schedule). All the logs are aggregated on an hourly level.\n\nData fields:\n\n- instant: record index\n- dteday : date\n- season : season (1:winter, 2:spring, 3:summer, 4:fall)\n- yr : year (0: 2011, 1:2012)\n- mnth : month ( 1 to 12)\n- hr : hour (0 to 23)\n- holiday : weather day is holiday or not (extracted from [Web Link])\n- weekday : day of the week\n- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n- weathersit :\n    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)\/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)\/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)\n- casual: count of casual users\n- registered: count of registered users\n- cnt: count of total rental bikes including both casual and registered\n\n## Preliminary data exploration\n\nBefore we start with analysis of bike rentals, we need to understand some specifics of the data and do data cleaning if necessary. As the dataset was used for a [research](https:\/\/link.springer.com\/article\/10.1007\/s13748-013-0040-3), it should be already cleaned. However, we might still encounter some pecularities, which we should take into account in our analysis.\n\nFirst, let's load all libraries and read the data:","e200ffe3":"From above we can conclude that years are labeled as per dataset specification. Let's see if month is also determined correctly.","a62243c5":"As the daytime_lbl is ordinal but with different periods, we should split it to dummies as well.","eb92fed0":"We see that all such cases happened in the night - early morning. This indicates that the logs were missing due to the drop of demand. Let's look at the 6 cases, where the logs were interrupted for more than 2 hrs.","2b64d479":"The error decreased below 100. We can make the model more and more accurate by this technique. But let's move forwards and check how other regression models tackle this problem.\n\n\n## Decision tree model\n\nLet's see if decision tree model would peform better.","f6dcfbc1":"Let's study correlations of variables in the dataset:","45b96d0e":"We are now ready to visualize the new clusters. I propose to choose blue-red color scheme for new labels: blue for low temperatures, red for high.","7a7a5166":"In the above figures we can see that in case 1 and 2 bike rentals do not follow the average trend for the same week days within their months. This might be an indication of holidays, and indeed there was Martin Luther King Jr. Day on Jan 17 and Washington's Birthday on Feb 21. In both cases the average bike rentals are lower than for Sundays before them, which might be a pattern for holidays happening after weekends. Or, more probably, it might be due to the significant humidity increase, which is obvious on the second figure. With these cases happening in low season, it should not be a surprise that the the demand for bikerentals  have disappeared in the night hours with bad weather.\n\nFor cases 4 and 5 we see that bikerentals follow the average trend closely and then it starts decreasing. In the case 5, the trend does not recover immediately - we know that there was a hurricane Sandy that day, which explains such prolonged absense of rentals. Searching the date 2011-08-27 on the internet gave us the info that there was a Hurricane Irine. \n\nFor 2011-01-26 Google says that there was heavy snowing, which surely should have affected bikes usage - it was so severe that it was in the news headlines.\n\nFor all the cases, we see worsening of the weather conditions prior the breaks in logs.\n\nOverall, now we know that these anomalies were the effect of bad weather and some other events. We also know that the dataset is not a datetime frequency, and 'instant' field does not really make much sense. We will not try to convert it to the format of data to datetime frequency because it will require us to fill in the missing weather info. However, we will change 'instant' column so that it contains the information that there were missing logs. It might be helpful for regression models. \n\nMoreover, we can label the found events as anomalies in order to gain some marginal improvement of prediction models in the future by dropping these rows as noize. We'll not spend more time on events labeling as it is not the main focuse in this analysis.\n\nLet's finish our preliminary study with transformation of the 'instant' column.","dba616b5":"The error indeed decreased  - it is 2 times lower now than at the simplest linear regression's. Let's see if it can be improved by changing  min_leaf_splits parameter.","698cf7ee":"We plotted 8 graphs, which are showing the same information as previous charts, but with hourly perspective:\n\nThe first row represents frequency distributions of weathersit by hour. The frequencies are weighted by the share of weathersit in the season (count of hours). The sum of all values on a single plot is 1.\n\nThe second row shows mean hourly bike rentals in different weathersit. \n\nWe can see that morning peak hours are very similar in terms the demand for rentals for weathersit 1 and 2, whereas the evening peak is higher on average for weathersit = 1. And for winter and spring times it has more separated from the next line.\nIt can be due to weathersit 2 in winter and spring being more severe than in summer and fall. Or it could be due to the effect of suppressed demand for physical outdoors activity due to a bit worse on average weather in the morning for these seasons, which results in an increase of rentals in the evening time, when it becomes better.","efc86ae4":"The table above contains all the rows which were the last before the breaks in rentals more than 2hrs happened. For a more complete understanding of the situation, we'll also need the rows, where the log restarted after break."}}