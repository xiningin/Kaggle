{"cell_type":{"9a17c4e7":"code","2cfa9d14":"code","522bbe83":"code","31d6bb55":"code","d9791f74":"code","67e062d3":"code","e218a161":"code","a451fba2":"code","5cb806e9":"code","973950ec":"code","00f18764":"code","08846a4b":"code","c33772d0":"code","314ff228":"code","bf8bbeb5":"code","b497d5c9":"code","1513838e":"code","004e47f4":"code","f00ebe9a":"code","555db283":"code","2936779e":"code","6d8e0d86":"code","def8d75b":"code","78cbb075":"code","e843a877":"code","00f79fd2":"code","3d55a9a8":"code","6b3806d4":"code","3d711343":"code","08e0be25":"code","563f162e":"code","2dc05b1b":"code","7985ae7f":"code","848e3602":"code","67fdf2fd":"code","f76bb44d":"code","ade15de2":"code","30cf065f":"code","e47bad85":"markdown","c5e676d1":"markdown","899da2b2":"markdown","115d8f66":"markdown","9f797c94":"markdown","e0e71cbc":"markdown","d1bc413a":"markdown","1960abee":"markdown","add00335":"markdown","f4afcd90":"markdown","d7aa844d":"markdown","3a4f1ca6":"markdown","ed300120":"markdown","4e3baa8d":"markdown","96f6d694":"markdown","3ac6e323":"markdown","c2bfc99e":"markdown","559303dc":"markdown","a23f8833":"markdown","5bf097aa":"markdown","ff1292a9":"markdown","d0671d3f":"markdown","f70b70f2":"markdown","9409ce4c":"markdown","6eb829f3":"markdown","fd22c0c3":"markdown","709e05cc":"markdown","f547f4fb":"markdown","85ae64cc":"markdown","55f2ef1c":"markdown"},"source":{"9a17c4e7":"import pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn import linear_model, neighbors, svm, model_selection, preprocessing, tree, feature_selection, ensemble, metrics\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nnp.random.seed(1234)","2cfa9d14":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.head()\n\ndata.head()","522bbe83":"data.describe(include='all')","31d6bb55":"f = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r')\nprint(f.read())\nprint(data.info())","d9791f74":"missing_scores = pd.DataFrame(((data.isna().sum())\/1460).sort_values(ascending=False)*100, columns=['missing values %'])\nmissing_scores = missing_scores[(missing_scores.T != 0).any()]\nplt.figure(figsize=(10,5))\nax =sns.barplot(data = missing_scores,x = missing_scores.index, y = 'missing values %', orient='v')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\n","67e062d3":"#For 'PoolQC' NaN measn No Pool in the house...\ndata['PoolQC'] = data['PoolQC'].fillna('NoPool')\n\n#For 'MiscFeature' NaN means that there is no special feature among the listed ones.\ndata['MiscFeature'] = data['MiscFeature'].fillna('None')\n\n#For 'Alley' NaN means 'NoAccess', so no access to the alley\ndata['Alley'] = data['Alley'].fillna('NoAlley')\n\n#For Fence means 'NoFence'\ndata['Fence'] = data['Fence'].fillna('NoFence')\n\n#For Fireplace means no fireplace\ndata['FireplaceQu'] = data['FireplaceQu'].fillna('NoFireplace')\n\n#For Lot frontage we have a numerical value and about 300 missing values. We could decide to substitute them with the mean value:\ndata['LotFrontage'] = data['LotFrontage'].fillna(0)\n\n#For Garage Condition, garage type etc... it means no Garage\ndata['GarageCond'] = data['GarageCond'].fillna('NoGarage')\ndata['GarageType'] = data['GarageType'].fillna('NoGarage')\ndata['GarageYrBlt'] = data['GarageYrBlt'].fillna(0)\ndata['GarageFinish'] = data['GarageFinish'].fillna(0)\ndata['GarageQual'] = data['GarageQual'].fillna('NoGarage')\n\n#For BsmtExposure, condition etc...\ndata['BsmtExposure'] = data['BsmtExposure'].fillna('NoBsmt')\ndata['BsmtFinType1'] = data['BsmtFinType1'].fillna('NoBsmt')\ndata['BsmtFinType2'] = data['BsmtFinType2'].fillna('NoBsmt')\ndata['BsmtCond'] = data['BsmtCond'].fillna('NoBsmt')\ndata['BsmtQual'] = data['BsmtQual'].fillna('NoBsmt')\n\n#For the other ones\n\ndata['MasVnrArea'] = data['MasVnrArea'].fillna(0)\ndata['MasVnrType'] = data['MasVnrType'].fillna('None')\n\n\n\n#For electrical data but more importantly for SalePrice we need to drop the rows with NaN values since they have no meaning.\n\ndata = data.dropna()\ndata = data.drop(columns=['Id'])\n\ndata.describe(include='all')","e218a161":"#Divide the SalePrice for 10.000 in the original dataset\ndata['SalePrice'] = np.log(data['SalePrice'])\ny = data['SalePrice']\ndata = data.drop(columns=['SalePrice'])\n\ncategories = {}\n\n#Discretize all the categorical variables\ncategorical_variables = data.select_dtypes('object')\nfor c in categorical_variables:\n    data[c] = data[c].astype('category')\n    categories[c] = data[c].cat.codes\n    data[c] = data[c].cat.codes\n\n#Store the info of the features in a DataFrame\ninfo_features = pd.DataFrame({'mean': data.mean(), 'std': data.std()})\n    \n\n#zscore over all the variables\nzscorer = preprocessing.StandardScaler()\nzscorer.fit(data)\ndata = pd.DataFrame(zscorer.transform(data), columns=data.columns)","a451fba2":"plt.figure(figsize=(10,10))\nsns.clustermap(data.corr(), cmap = \"Blues\")","5cb806e9":"plt.figure(figsize=(20,10))\nax = sns.boxplot(data = data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\nplt.tight_layout()\nplt.show()","973950ec":"#Perform a fitting for the lasso and plot the weights assigned to the features.\nlasso = linear_model.Lasso(alpha = 0.1)\nlasso_model = lasso.fit(data,y)\nvariables_selected = pd.DataFrame(lasso_model.coef_, index = data.columns,columns=['weight'])\nvariables_selected = variables_selected.sort_values(by='weight',ascending=False)\nplt.figure(figsize=(15,5))\nax =sns.barplot(data = variables_selected,x = variables_selected.index, y = 'weight')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\nplt.tight_layout()\nplt.show()","00f18764":"lasso_scores = model_selection.cross_validate(lasso,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)\nprint('Lasso Model scored an R squared of: mean ' + str(lasso_scores['train_r2'].mean()) + ' std: ' + str(lasso_scores['train_r2'].std()))","08846a4b":"rand_f = ensemble.ExtraTreesRegressor()\nrand_f_model = rand_f.fit(data,y)\nsel_vars = pd.DataFrame(rand_f_model.feature_importances_, index = data.columns, columns=['importance'])\nsel_vars = sel_vars.sort_values(by='importance', ascending=False)\nplt.figure(figsize=(15,5))\nax =sns.barplot(data = sel_vars,x = sel_vars.index, y = 'importance')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\nplt.tight_layout()\nplt.show()","c33772d0":"randf_scores = model_selection.cross_validate(rand_f,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)\nprint('Random Forest scored an R squared of: mean ' + str(randf_scores['train_r2'].mean()) + ' std: ' + str(randf_scores['train_r2'].std()))","314ff228":"ridge = linear_model.Ridge(alpha=0.1)\nridge_model = ridge.fit(data,y)\nvariables_selected = pd.DataFrame(ridge_model.coef_, index = data.columns,columns=['weight'])\nvariables_selected = variables_selected.sort_values(by='weight',ascending=False)\nplt.figure(figsize=(15,5))\nax =sns.barplot(data = variables_selected,x = variables_selected.index, y = 'weight')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\nridge_scores = model_selection.cross_validate(ridge,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","bf8bbeb5":"lr = linear_model.LinearRegression()\nlr_model = lr.fit(data,y)\nlr_scores = model_selection.cross_validate(lr,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","b497d5c9":"ply = preprocessing.PolynomialFeatures(degree=2)\nply_oi = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True)\n\nply_data = pd.DataFrame(ply.fit_transform(data), columns = ply.get_feature_names())\nply_data_oi = pd.DataFrame(ply_oi.fit_transform(data), columns = ply_oi.get_feature_names())\n\nply_data = ply_data[np.setdiff1d(ply_data.columns,ply_data_oi.columns)]\n\nlr_quad = linear_model.LinearRegression()\nlr_quad_model = lr_quad.fit(ply_data,y)\nlr_quad_scores = model_selection.cross_validate(lr_quad,ply_data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)\n","1513838e":"selected_columns = sel_vars[sel_vars >= 0.025].dropna().index\n\nrestricted_data = data[selected_columns] #The new dataset\nlr_fs = linear_model.LinearRegression()\nlr_fs_model = lr_fs.fit(restricted_data,y)\nlr_fs_scores = model_selection.cross_validate(lr_fs,restricted_data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","004e47f4":"ply_fs = preprocessing.PolynomialFeatures(degree=2)\nply_fs_oi = preprocessing.PolynomialFeatures(degree=2, interaction_only = True)\n\nply_fs_data = pd.DataFrame(ply_fs.fit_transform(restricted_data), columns = ply_fs.get_feature_names())\nply_fs_oi_data = pd.DataFrame(ply_fs_oi.fit_transform(restricted_data),columns=ply_fs_oi.get_feature_names())\nply_fs_data = ply_fs_data[np.setdiff1d(ply_fs_data.columns,ply_fs_oi_data.columns)]\nlr_fs_q = linear_model.LinearRegression()\nlr_fs_q_model = lr_quad.fit(ply_fs_data,y)\nlr_fs_q_scores = model_selection.cross_validate(lr_fs_q,ply_fs_data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","f00ebe9a":"knn = neighbors.KNeighborsRegressor(n_neighbors = 5,weights='uniform')\nknn_model = knn.fit(data,y)\nknn_scores = model_selection.cross_validate(knn,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","555db283":"knn_ds = neighbors.KNeighborsRegressor(n_neighbors = 5,weights='distance')\nknn_ds_model = knn_ds.fit(data,y)\nknn_ds_scores = model_selection.cross_validate(knn_ds,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","2936779e":"svr = svm.SVR(kernel='rbf')\nsvr_model = svr.fit(data,y)\nsvr_scores = model_selection.cross_validate(svr,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","6d8e0d86":"gb = ensemble.GradientBoostingRegressor()\ngb_model = gb.fit(data,y)\ngb_scores = model_selection.cross_validate(gb,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","def8d75b":"xgbreg = xgb.XGBRegressor()\nxgb_model = xgbreg.fit(data,y)\nxgb_scores = model_selection.cross_validate(xgbreg,data,y,cv=10,scoring=['r2','neg_root_mean_squared_error'], return_train_score=True)","78cbb075":"# #Let's create the data frame for the statistics to be used for the comparisons of our models.\nmeans_r2 = [lasso_scores['train_r2'].mean(), randf_scores['train_r2'].mean(), ridge_scores['train_r2'].mean(), lr_scores['train_r2'].mean(), lr_quad_scores['train_r2'].mean(), lr_fs_scores['train_r2'].mean(),lr_fs_q_scores['train_r2'].mean(), knn_scores['train_r2'].mean(), knn_ds_scores['train_r2'].mean(), svr_scores['train_r2'].mean(),gb_scores['train_r2'].mean(), xgb_scores['train_r2'].mean()]\nstds_r2 = [lasso_scores['train_r2'].std(), randf_scores['train_r2'].std(), ridge_scores['train_r2'].std(), lr_scores['train_r2'].std(), lr_quad_scores['train_r2'].std(), lr_fs_scores['train_r2'].std(),lr_fs_q_scores['train_r2'].std(), knn_scores['train_r2'].std(), knn_ds_scores['train_r2'].mean(), svr_scores['train_r2'].std(),gb_scores['train_r2'].std(), xgb_scores['train_r2'].std()]\n# mse = [metrics.mean_squared_error(y,lasso_model.predict(data)),\n#        metrics.mean_squared_error(y,rand_f_model.predict(data)),\n#        metrics.mean_squared_error(y,ridge_model.predict(data)),\n#        metrics.mean_squared_error(y,lr_model.predict(data)),\n#        metrics.mean_squared_error(y,lr_quad_model.predict(ply_data)),\n#        metrics.mean_squared_error(y,lr_fs_model.predict(restricted_data)),\n#        metrics.mean_squared_error(y,lr_fs_q_model.predict(ply_fs_data)),\n#        metrics.mean_squared_error(y,knn_model.predict(data)),\n#        metrics.mean_squared_error(y,knn_ds_model.predict(data)),\n#        metrics.mean_squared_error(y,svr_model.predict(data)),\n#        metrics.mean_squared_error(y,gb_model.predict(data))]\nmeans_rmse = [lasso_scores['test_neg_root_mean_squared_error'].mean(), randf_scores['test_neg_root_mean_squared_error'].mean(), ridge_scores['test_neg_root_mean_squared_error'].mean(), lr_scores['test_neg_root_mean_squared_error'].mean(), lr_quad_scores['test_neg_root_mean_squared_error'].mean(), lr_fs_scores['test_neg_root_mean_squared_error'].mean(),lr_fs_q_scores['test_neg_root_mean_squared_error'].mean(), knn_scores['test_neg_root_mean_squared_error'].mean(), knn_ds_scores['test_neg_root_mean_squared_error'].mean(), svr_scores['test_neg_root_mean_squared_error'].mean(),gb_scores['test_neg_root_mean_squared_error'].mean(), xgb_scores['test_neg_root_mean_squared_error'].mean()]\nstds_rmse = [lasso_scores['test_neg_root_mean_squared_error'].std(), randf_scores['test_neg_root_mean_squared_error'].std(), ridge_scores['test_neg_root_mean_squared_error'].std(), lr_scores['test_neg_root_mean_squared_error'].std(), lr_quad_scores['test_neg_root_mean_squared_error'].std(), lr_fs_scores['test_neg_root_mean_squared_error'].std(),lr_fs_q_scores['test_neg_root_mean_squared_error'].std(), knn_scores['test_neg_root_mean_squared_error'].std(), knn_ds_scores['test_neg_root_mean_squared_error'].mean(), svr_scores['test_neg_root_mean_squared_error'].std(),gb_scores['test_neg_root_mean_squared_error'].std(), xgb_scores['test_neg_root_mean_squared_error'].std()]\n\n\nmodel_scores = pd.DataFrame({'model': ['lasso','random_forest','ridge','lr','lr_quad','lr_fs','lr_fs_q','knn','knn_ds','svr','gb','xgb'], 'mean_r2': means_r2, 'std_r2': stds_r2,\n                            'means_RMSE': means_rmse,'std_RMSE':stds_rmse})\nmodel_scores","e843a877":"plt.ylim([-0.33,0])\nplt.xticks(rotation=60)\nplt.axhline(y=max(model_scores['means_RMSE']))\nsns.barplot(data = model_scores, x='model', y='means_RMSE')","00f79fd2":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_data.head()","3d55a9a8":"test_data.columns[np.where(test_data.isna().any() == True)]","6b3806d4":"#For 'PoolQC' NaN measn No Pool in the house...\ntest_data['PoolQC'] = test_data['PoolQC'].fillna('NoPool')\n\n#For 'MiscFeature' NaN means that there is no special feature among the listed ones.\ntest_data['MiscFeature'] = test_data['MiscFeature'].fillna('None')\n\n#For 'Alley' NaN means 'NoAccess', so no access to the alley\ntest_data['Alley'] = test_data['Alley'].fillna('NoAlley')\n\n#For Fence means 'NoFence'\ntest_data['Fence'] = test_data['Fence'].fillna('NoFence')\n\n#For Fireplace means no fireplace\ntest_data['FireplaceQu'] = test_data['FireplaceQu'].fillna('NoFireplace')\n\n#For Lot frontage we have a numerical value and about 300 missing values. We could decide to substitute them with the mean value:\ntest_data['LotFrontage'] = test_data['LotFrontage'].fillna(0)\n\n#For Garage Condition, garage type etc... it means no Garage\ntest_data['GarageCond'] = test_data['GarageCond'].fillna('NoGarage')\ntest_data['GarageType'] = test_data['GarageType'].fillna('NoGarage')\ntest_data['GarageYrBlt'] = test_data['GarageYrBlt'].fillna(0)\ntest_data['GarageFinish'] = test_data['GarageFinish'].fillna(0)\ntest_data['GarageQual'] = test_data['GarageQual'].fillna('NoGarage')\n\n#For BsmtExposure, condition etc...\ntest_data['BsmtExposure'] = test_data['BsmtExposure'].fillna('NoBsmt')\ntest_data['BsmtFinType1'] = test_data['BsmtFinType1'].fillna('NoBsmt')\ntest_data['BsmtFinType2'] = test_data['BsmtFinType2'].fillna('NoBsmt')\ntest_data['BsmtCond'] = test_data['BsmtCond'].fillna('NoBsmt')\ntest_data['BsmtQual'] = test_data['BsmtQual'].fillna('NoBsmt')\n\n#For the other ones\n\ntest_data['MasVnrArea'] = test_data['MasVnrArea'].fillna(0)\ntest_data['MasVnrType'] = test_data['MasVnrType'].fillna('None')\n\n#For electrical data but more importantly for SalePrice we need to drop the rows with NaN values since they have no meaning.\ntest_data.drop(columns=['Id'],inplace=True)\n\ntest_data.describe(include='all')","3d711343":"test_data.columns[np.where(test_data.isna().any() == True)]","08e0be25":"for c in categorical_variables:\n    test_data[c] = test_data[c].astype('category')\n    test_data[c] = categories[c]\ntest_data.head()\ntest_data.astype('double')","563f162e":"test_data = test_data.fillna(-1) #In this way we consider as negative the missing values for the predictions","2dc05b1b":"#Now standardize the data from the original distribution\ntest_data = pd.DataFrame(zscorer.transform(test_data), columns = test_data.columns)","7985ae7f":"test_data.head()","848e3602":"test_data.isna().sum()","67fdf2fd":"predictions_gb = gb_model.predict(test_data)\npredictions_rf = rand_f_model.predict(test_data)\npredictions_xgb = xgb_model.predict(test_data)\n\nrestricted_data_test = test_data[selected_columns] #The new dataset \nply_test = preprocessing.PolynomialFeatures(degree=2)\nply_test_oi = preprocessing.PolynomialFeatures(degree=2, interaction_only = True)\n\nply_test_data = pd.DataFrame(ply_test.fit_transform(restricted_data_test), columns = ply_test.get_feature_names())\nply_test_oi_data = pd.DataFrame(ply_test_oi.fit_transform(restricted_data_test), columns = ply_test_oi.get_feature_names())\n\nply_test_data = ply_test_data[np.setdiff1d(ply_test_data.columns,ply_test_oi_data.columns)]\npredictions_lrqfs = lr_fs_q_model.predict(ply_test_data)","f76bb44d":"sub1 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub2 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub3 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub4 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub1['SalePrice'] = np.exp(predictions_gb).astype(np.int64)\nsub2['SalePrice'] = np.exp(predictions_rf).astype(np.int64)\nsub3['SalePrice'] = np.exp(predictions_lrqfs).astype(np.int64)\nsub4['SalePrice'] = np.exp(predictions_xgb).astype(np.int64)","ade15de2":"sub1.set_index('Id',inplace=True)\nsub2.set_index('Id',inplace=True)\nsub3.set_index('Id',inplace=True)\nsub4.set_index('Id',inplace=True)","30cf065f":"sub1.to_csv('submission_gb.csv') ## Score: 0.16\nsub2.to_csv('submission_rf.csv') ## Score: 0.20\nsub3.to_csv('submission_lrqfs.csv') ## Score: 0.22\nsub4.to_csv('submission_xgb.csv') ## Score: ?","e47bad85":"We can highlight all the categorical data that have very low variability and that are not really meaningful for the decision of a price (for example the presence of a Pool should be a very good price increasing feature, don't you think?):\n\n\nCategorical: [Street, Utilities, MiscFeature, Condition2, RoofMatl, Heating, Functional, MiscFeature]\n\n\nNumerical: []\n","c5e676d1":"### Linear Regression with Quadratic features","899da2b2":"It seems that the best model for our purposes is the Gradient Boosting model.\nIn order to be really sure of it, we can do a paired t-test.\n\nTBD\n\nWe will try submitting four different files. XGB or GB will probably be the best ones.","115d8f66":"### Linear Regression with Quadratic Restricted Features","9f797c94":"# Predictions and Submission of results\n\nNow we have to do the predictions over the test set and see how the results score over the real data. \nFirst, we have to zscore the original data with the mean and std of the original features, that we have stored in a dedicated dataframe.\nAfter that we have to store the predictions for our model into an array and then create a dataframe with the prediction. Then write a submission .csv file.","e0e71cbc":"## Data loading\n","d1bc413a":"# Data cleaning\n\nNow we plot the data that have the worst number of missing values:","1960abee":"We now try to fit a Random Forest and see what is the score for the model and also how can we select the best features by plotting the importances for the features.","add00335":"We can also see and analyze the correlation matrix, even though in this particular case does not give use any particular information.","f4afcd90":"### Gradient Boosting for regression","d7aa844d":"# Advanced House Regression\n\n### Abstract\n\nIn this notebook we try to solve a regression problem on the price of a dataset of houses placed in Boston. We then want to find a good model for regression, that is, a model that has a good R squared score and that has a good RMSE evaluation error. We will try for now to do some Exploratory Data Analysis in order to see statistics over the data, clean the data and eventually select the best feature. Later we will try some models and then decide among them through a paired t-test with a Bonferroni correction which of them stands better for this problem.\nIMPORTANT: This notebook does not make use of Neural Nets for Regression but tries to address the problem with other families of learners (Random Forests, Lasso, GB etc...)","3a4f1ca6":"# Model Selection and Comparison\n\nFor now we tried to clean and analyse data. We didn't plot them because we had a big number of features and plotting them in 2D would result in insignificant inspections.\nWe now try different models, save their results and then select the best one through the paired t-test with a Bonferroni correction with a 5% confidence interval.\nWe are going to use the following models:\n- Lasso Linear Regression\n- Random Forest\n- Ridge Linear Regression\n- Linear Regression\n- Linear Regression with Quadratic features\n- Linear Regression with Features selected by the importance of the Random Forest\n- Naive Bayes\n- kNN (k=5)\n- Support Vector Regressor\n\nWe already fitted the Lasso and Random Forest, let's see the other ones.\n","ed300120":"We can see that there are a few cases that are not covered in our training phase... We need to do something about this.\nIn particular we do not have missing values for these variables ","4e3baa8d":"Now it is time to make the prediction: We take our fitted Random Forest and predict over all the values that we have.\nLet's find out the predictions.","96f6d694":"### Linear Regression with Feature Selection","3ac6e323":"We now can see what are the variables that require more attention for the data cleansing. Let's examine those one by one and try to adequately fill those missing values.","c2bfc99e":"Particular cases are the one of LotArea and SalePrice (the target variable).\nNow we need to perform some feature selection. In order to do so, we can act in more ways, but in this case we will try first to apply Lasso Regression in order to see how much are the impact of the features and next we will plot the feature importance with a regression on a decision tree. \nWe could also use a wrapper approach by trying several regression by increasing or decreasing the features and plotting the scores.\n","559303dc":"### XGB","a23f8833":"In the next figure instead we can see for every variable all the outliers data. They seems to be many but, for many variables they are a big number of outlier data, so it could negatively affect our predictions.","5bf097aa":"# Data Preparation\n\n### Steps\n\nAfter the cleansing of the data from the NaN values we need to perform two more actions:\n1. We can discretize the categorical values, normalize the data and save for each one of them mean and std. For the target divide by 10.000 in order to have a smaller range or pass to a minmax scaler.\n2. Plot the boxplots in order to detect and delete the outliers that could eventually negatively affect our model.\n3. Perform some feature selection in order to check whether to mantain all features or discard some of them.\n4. See the correlation between the variables in order to check if there are some that could be aggregated. ","ff1292a9":"### Support Vector Machine for Regression","d0671d3f":"### k-NN Regression with distance weights","f70b70f2":"And we have to pass to a categorical representation WITH THE LABELS ON THE ORIGINAL DATASET:","9409ce4c":"Of course, we have to apply the SAME exact steps of preprocessing to these data, but we also need to pay attentions to the NaN values in the training data: in fact, there could be cases in which we also have missing values in the test data and we need to assign to these values some particular meaning. ","6eb829f3":"### k-NN Regression","fd22c0c3":"### Linear Model","709e05cc":"### Inspection\n\nNow that we have some statistics over the data the first thing that we clearly see is that many values have missing values.\nBefore proceeding by filling these values is VERY IMPORTANT to check if these missing values are really missing or instead represent something else. In order to do so we print a description of the various features.","f547f4fb":"We can see here that the Random Forest is quite a good model, even though we should estimate also the Out of Bag error for it.","85ae64cc":"### Ridge","55f2ef1c":"From this last barplot we could select all the features that are over a give percentage, for example 2.5% of importance. In this way we would select only A small part of the features. Later we will try to use these info in order to compute a simple Linear Regression problem also with more complex features.\nFor now let's just analyse."}}