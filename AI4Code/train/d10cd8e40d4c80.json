{"cell_type":{"a818e469":"code","3f85890d":"code","eed3d5db":"code","e9c637b5":"code","7abb5d03":"code","5a79fac2":"code","83a1d66e":"code","94ed5767":"code","c5f629e7":"code","bb714c6d":"code","c39013dd":"code","3e6bd1c8":"code","61f787ff":"code","c31fb0b0":"code","c920b0b8":"code","03734aa0":"code","649c2ce6":"code","934a634a":"code","4bcc5927":"code","80922d81":"code","16e88022":"code","b0095aa1":"code","1be43993":"code","a469b5b4":"code","daad6e04":"code","50ea7b4b":"code","5336be57":"code","792403ff":"code","fe07ee7e":"code","757b89f6":"code","49d6af40":"code","cdecb1c5":"code","bad79285":"code","43320ff9":"code","f7485b9c":"code","b003292f":"code","5ca41c47":"code","26fe09c1":"code","9b9ed5ac":"code","6ac8aba8":"code","34ffca05":"code","837b86d8":"code","8cc09aa0":"code","5f19032a":"code","ae3cb875":"code","72265e9e":"code","51b9d29c":"code","4c4c3644":"markdown"},"source":{"a818e469":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\", 100)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nsns.set()\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f85890d":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","eed3d5db":"print(train_data.shape)\nprint(test_data.shape)\n","e9c637b5":"train_data.head()","7abb5d03":"test_data.head()","5a79fac2":"# function: Dataset characteristics\ndef data_characteristics(dataset):\n    space = \"\"\"\n    \n    \"\"\"\n    #shape of dataset\n    print('Shape of dataset :                {}'.format(dataset.shape))\n    print('Number of Rows in the dataset:    {}'.format(dataset.shape[0]))\n    print('Number of Columns in the dataset: {}'.format(dataset.shape[1]))\n    print(\"\")\n    print('|' +'-'*60 + \"|\")\n    print(space)\n    #Understanding the Number of Numerical and Categorical features in dataset\n    \n    numerical_features = dataset.select_dtypes(include = [np.number])\n    categorical_features = dataset.select_dtypes(exclude = [np.number])\n    print(\"Number of Numerical features: {}\".format(numerical_features.shape[1]))\n    print(\"Number of Categorical features: {}\".format(categorical_features.shape[1]))\n    print(\"\")\n    print('|' +'-'*60 + \"|\")\n    print(space)\n    \n    \n    #list of features based on category or numeric\n    print(\"Categorical Features: \")\n    print(categorical_features.columns)\n    print(space)\n    print(\"numerical features: \")\n    print(numerical_features.columns)\n    print(space)\n    \n    \n    #unique values\n    print(\"No of unique values : \")\n    print('Columns   count_unique_values')\n    print(format(dataset.nunique()))\n    print(\"\")\n    print('|' +'-'*60 + \"|\")\n    print(space)\n    \n    #number of not null values\n    print(\"No. of NON-NAN's : \")\n    print(format(dataset.count()))\n    print(\"\")\n    print('|' +'-'*60 + \"|\")\n    print(space)\n    \n    #understanding the dataset\n    print(\"Information of Dataset: {}\".format(dataset.info(verbose = False,memory_usage = 'deep')))\n    print(\" \")\n    print('|' +'-'*60 + \"|\")\n    print(space)\n    \n    #statistical summary of dataset\ndata_characteristics(train_data)\nprint(\"Statistical Summary of the dataset : \")\ntrain_data.describe(percentiles = [.15,.25,.50,.75,.85])","83a1d66e":"train_data.isnull().sum()","94ed5767":"# function: check percentage of missing values\ndef null_check(dataset):\n    \n    # Calculating the percentage of missing values\n    null_per = (dataset.isnull().sum() \/ len(dataset)) * 100\n    try:\n        # dropping null_per == 0\n        null_per = round(null_per.drop(null_per[null_per == 0].index)).sort_values(ascending = False)\n        \n        \n        # Making the bar plot of the Null values\n        plt.figure(figsize = (12,8))\n        null_plot = sns.barplot(x = null_per.index , y = null_per , color= 'red')\n        plt.title(\"Percentage Of Null values in the Dataset\")\n        plt.xticks(rotation = \"45\")\n        plt.ylabel(\"Percentage Of Null values in the Dataset\")\n        plt.xlabel('Column name')\n        print(null_plot)\n        \n    except:\n        print(\"There is NO null values in the dataset\")\n        print(\"Returning the Dataset...\")\n        return dataset\n\nnull_check(train_data)","c5f629e7":"null_check(train_data)","bb714c6d":"sns.countplot('Survived',hue= 'Sex', data = train_data , color= 'red')\nplt.show()","c39013dd":"sns.countplot('Embarked', data = train_data , color= 'red')\nplt.show()","3e6bd1c8":"sns.countplot(\"Survived\", data= train_data , hue = 'Pclass' , color = 'red')","61f787ff":"sns.boxenplot(train_data.Fare , color= 'red')\n#shows few people payed for more than $100","c31fb0b0":"sns.boxenplot( train_data.Age , color = 'red')\n#most people were from age between 20-49\n#there were few above 65","c920b0b8":"corr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()\n","03734aa0":"((train_data.groupby(['Sex','Survived']).Survived.count() * 100) \/ train_data.groupby('Sex').Survived.count())","649c2ce6":"(train_data.groupby(['Pclass','Survived']).Survived.count() * 100) \/ train_data.groupby('Pclass').Survived.count()\n","934a634a":"train_data.groupby(by = ['Survived']).mean()['Age']\n#so the average age of the people survivved was 28.34","4bcc5927":"train_data.drop('Cabin' , axis = 1 , inplace = True)\ntest_data.drop('Cabin' , axis = 1 , inplace = True)","80922d81":"null_check(train_data)","16e88022":"train_data[\"Age\"].fillna(train_data['Age'].mean() , inplace = True)\n","b0095aa1":"test_data[\"Age\"].fillna(test_data['Age'].mean() , inplace = True)\n","1be43993":"train_data.dropna(inplace= True)","a469b5b4":"test_data[\"Fare\"].fillna(test_data['Fare'].mean() , inplace = True)\n","daad6e04":"train_data.head()","50ea7b4b":"def change_gender(x):\n    if x == 'male':\n        return 0\n    elif x == 'female':\n        return 1\ntrain_data.Sex = train_data.Sex.apply(change_gender)\ntest_data.Sex = test_data.Sex.apply(change_gender)","5336be57":"change = {'S':1,'C':2,'Q':0}\ntrain_data.Embarked = train_data.Embarked.map(change)\ntest_data.Embarked = test_data.Embarked.map(change)\n","792403ff":"combined_data = [train_data, test_data]","fe07ee7e":"train_data['Alone'] = train_data.SibSp + train_data.Parch\ntest_data['Alone'] = test_data.SibSp + test_data.Parch\n\ntrain_data.Alone = train_data.Alone.apply(lambda x: 1 if x == 0 else 0)\ntest_data.Alone = test_data.Alone.apply(lambda x: 1 if x == 0 else 0)","757b89f6":"# now lets drop SibSp and Parch column for both training and testing data\ntrain_data.drop(['SibSp','Parch'], axis = 1, inplace = True)\ntest_data.drop(['SibSp','Parch'], axis = 1, inplace = True )","49d6af40":"train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False).unique().size\n# there are total 17 unique title","cdecb1c5":"# lets create the Title feature which contain the title of the passenger and drop Name column\nfor data in combined_data:\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n    data.drop('Name', axis = 1, inplace = True)","bad79285":"train_data.Title.value_counts()","43320ff9":"#lets replace least occuring title in the data with rare\nleast_occuring = [ 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\nfor data in combined_data:\n    data.Title = data.Title.replace(least_occuring, 'Rare')","f7485b9c":"# lets perform title mapping in order to change to ordinal\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor data in combined_data:\n    data['Title'] = data['Title'].map(title_mapping)","b003292f":"columns_to_drop = ['PassengerId','Ticket']\ntrain_data.drop(columns_to_drop, axis = 1, inplace = True)\ntest_data.drop(columns_to_drop[1], axis = 1, inplace = True)","5ca41c47":"for dataset in combined_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","26fe09c1":"for data in combined_data:\n    data.loc[data['Fare'] < 30, 'Fare'] = 1\n    data.loc[(data['Fare'] >= 30) & (data['Fare'] < 50),'Fare'] = 2\n    data.loc[(data['Fare'] >= 50) & (data['Fare'] < 100),'Fare'] = 3\n    data.loc[(data['Fare'] >= 100),'Fare'] = 4","9b9ed5ac":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test = test_data.drop(\"PassengerId\", axis = 1)\nprint(\"shape of X_train\",X_train.shape)\nprint(\"Shape of Y_train\",Y_train.shape)\nprint(\"Shape of x_test\",X_test.shape)","6ac8aba8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","34ffca05":"from sklearn.ensemble import VotingClassifier","837b86d8":"KNN=KNeighborsClassifier()\nNAIVE=GaussianNB()\nSVM=SVC()\nDT=DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nEnsemble = VotingClassifier( estimators= [('KNN',KNN),('NB',NAIVE),('SVM',SVM),('DT',DT),('LR',LR),('RF',RF)], voting = 'hard')","8cc09aa0":"Ensemble.fit(X_train,Y_train)","5f19032a":"predict = Ensemble.predict(X_test)\n\npredict","ae3cb875":"submit = pd.DataFrame({\"PassengerId\":test_data.PassengerId, 'Survived':predict})\nsubmit.to_csv(\"final_submission.csv\",index = False)\n","72265e9e":"from sklearn import metrics\nY_pred_rand = (Ensemble.predict(X_train) > 0.5).astype(int)\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred_rand)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred_rand)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred_rand)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred_rand)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred_rand)*100,2))","51b9d29c":"# plotting the confusion matrix in heatmap\nmatrix = metrics.confusion_matrix(Y_train, Y_pred_rand)\nsns.heatmap(matrix, annot = True,fmt = 'g')\nplt.show()","4c4c3644":"Dealing with missing data"}}