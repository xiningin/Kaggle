{"cell_type":{"da0d3bcd":"code","0bda4f41":"code","ae3cd146":"code","c9248b27":"code","41404644":"code","638b95f5":"code","083761e1":"code","35a702cf":"code","98e6dd3b":"code","4c6ab76a":"code","78a5e6c1":"code","7593f6eb":"code","7b501b00":"code","c5efe82b":"code","6f39f3df":"code","0c53f07a":"code","53e32f9e":"code","e9323140":"code","5b36e240":"code","ec881bb4":"code","5a97cfb6":"code","c9446daa":"code","72f0d1be":"code","7151aa71":"code","2fe7067f":"code","45fd8cee":"code","321e9797":"code","117acf9e":"code","daf0bc8f":"code","3f14ea2a":"code","6a941c53":"code","cf6e9d29":"code","d75fecb0":"code","4e9d969b":"code","63983aae":"markdown","6ee83991":"markdown","81f41775":"markdown","2ece8643":"markdown","c01a9827":"markdown","284ae533":"markdown","09ee4031":"markdown","b5b2f50d":"markdown","b2219fc1":"markdown","b127d27b":"markdown","c4f359c4":"markdown","7af0b8e8":"markdown","f6191ed4":"markdown","bcaf5488":"markdown","2ef44349":"markdown","525b77c9":"markdown"},"source":{"da0d3bcd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","0bda4f41":"# read csv (comma separated value) into data\ndata = pd.read_csv('..\/input\/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","ae3cd146":"# to see features and target variable\ndata.head()","c9248b27":"# Null de\u011ferler ve veri uzunluklar\u0131na bakal\u0131m\ndata.info()","41404644":"data.describe()","638b95f5":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()\n","083761e1":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","35a702cf":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","98e6dd3b":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","4c6ab76a":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","78a5e6c1":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = data[data['class'] =='Abnormal']\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","7593f6eb":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","7b501b00":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","c5efe82b":"# Ridge\nfrom sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint('Ridge score: ',ridge.score(x_test,y_test))","6f39f3df":"# Lasso\nfrom sklearn.linear_model import Lasso\nx = np.array(data1.loc[:,['pelvic_incidence','pelvic_tilt numeric','lumbar_lordosis_angle','pelvic_radius']])\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\nlasso = Lasso(alpha = 0.1, normalize = True)\nlasso.fit(x_train,y_train)\nridge_predict = lasso.predict(x_test)\nprint('Lasso score: ',lasso.score(x_test,y_test))\nprint('Lasso coefficients: ',lasso.coef_)","0c53f07a":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","53e32f9e":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","e9323140":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n# abnormal = 1 and normal = 0\ndata['class_binary'] = [1 if i == 'Abnormal' else 0 for i in data.loc[:,'class']]\nx,y = data.loc[:,(data.columns != 'class') & (data.columns != 'class_binary')], data.loc[:,'class_binary']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\n","5b36e240":"from sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n","ec881bb4":"param_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))\n","5a97cfb6":"\ndata = pd.read_csv('..\/input\/column_2C_weka.csv')\n\ndf = pd.get_dummies(data)\ndf.head(10)","c9446daa":"\ndf.drop(\"class_Normal\",axis = 1, inplace = True) \ndf.head(10)","72f0d1be":"# SVM, pre-process and pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nsteps = [('scalar', StandardScaler()),\n         ('SVM', SVC())]\npipeline = Pipeline(steps)\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 1)\ncv = GridSearchCV(pipeline,param_grid=parameters,cv=3)\ncv.fit(x_train,y_train)\n\ny_pred = cv.predict(x_test)\n\nprint(\"Accuracy: {}\".format(cv.score(x_test, y_test)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n","7151aa71":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","2fe7067f":"data2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","45fd8cee":"df = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","321e9797":"inertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","117acf9e":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\ndata3 = data.drop('class',axis = 1)","daf0bc8f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","3f14ea2a":"from scipy.cluster.hierarchy import linkage,dendrogram\n\nmerg = linkage(data3.iloc[200:220,:],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\nplt.show()","6a941c53":"from sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=100)\ntransformed = model.fit_transform(data2)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list )\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","cf6e9d29":"# PCA\nfrom sklearn.decomposition import PCA\nmodel = PCA()\nmodel.fit(data3)\ntransformed = model.transform(data3)\nprint('Principle components: ',model.components_)","d75fecb0":"# PCA variance\nscaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler,pca)\npipeline.fit(data3)\n\nplt.bar(range(pca.n_components_), pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.show()\n","4e9d969b":"# apply PCA\npca = PCA(n_components = 2)\npca.fit(data3)\ntransformed = pca.transform(data3)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list)\nplt.show()","63983aae":"<a id=\"16\"><\/a> <br>\n### PRINCIPLE COMPONENT ANALYSIS (PCA)","6ee83991":"<a id=\"6\"><\/a> <br>\n### CROSS VALIDATION\n\n","81f41775":"<a id=\"9\"><\/a> <br>\n### PRE-PROCESSING DATA","2ece8643":"Scatter Matrix","c01a9827":"<a id=\"11\"><\/a> <br>\n### KMEANS","284ae533":"<a id=\"5\"><\/a> <br>\n### REGRESSION","09ee4031":"<a id=\"10\"><\/a> <br>\n## UNSUPERVISED LEARNING\n","b5b2f50d":"<a id=\"12\"><\/a> <br>\n### EVALUATING OF CLUSTERING","b2219fc1":"<a id=\"8\"><\/a> <br>\n### HYPERPARAMETER TUNING\n\n","b127d27b":"<a id=\"14\"><\/a> <br>\n### HIERARCHY","c4f359c4":"<a id=\"7\"><\/a> <br>\n### ROC Curve with Logistic Regression ","7af0b8e8":"### Regularized Regression\n\n","f6191ed4":"<a id=\"15\"><\/a> <br>\n### T - Distributed Stochastic Neighbor Embedding (T - SNE)","bcaf5488":"# SVM","2ef44349":"<a id=\"4\"><\/a> <br>\n###  K-NEAREST NEIGHBORS (KNN)\n","525b77c9":"<a id=\"13\"><\/a> <br>\n### STANDARDIZATION"}}