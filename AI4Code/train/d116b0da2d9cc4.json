{"cell_type":{"30da9764":"code","ced4ad6d":"code","f159e305":"code","8c3278b6":"code","fe1b545c":"code","2b176345":"code","b93a44bb":"code","9e2fe2d3":"code","16bd1ebc":"code","dae0ee04":"code","9e75ead1":"code","d1b1afb1":"code","a19b676a":"code","0cadd5a1":"code","0f5c9db1":"code","8be5215b":"code","b15022e1":"code","753bf086":"markdown","91802d48":"markdown","2d84b317":"markdown","c2f9850d":"markdown","ee420706":"markdown","628d85dc":"markdown","18abdc12":"markdown","8f00fa90":"markdown","479f26fe":"markdown","845c1d6e":"markdown","f9e7afaa":"markdown","bdcfdb54":"markdown","8050afc3":"markdown","a7508861":"markdown","9a6203a0":"markdown","ded3c94e":"markdown","301a6474":"markdown","b93d8dae":"markdown","a951091a":"markdown","1c16f610":"markdown","4af67c88":"markdown","347bc2cf":"markdown","fd084eb5":"markdown","d91c5aac":"markdown","96335fb2":"markdown","aea72f47":"markdown"},"source":{"30da9764":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(123)","ced4ad6d":"def minmaxscale(x):\n    return (x-x.min()) \/ (x.max()-x.min())\n\n# plot 1\nx1 = np.random.uniform(4, 5, 50)\nx2 = np.random.uniform(0, 10, 50)\n\n# plot 2\nerrors = np.random.uniform(0, 2, 50)\nx3 = np.random.uniform(5, 3, 50)\nx4 = 3*x3 + 3 + errors\n\n# subplots\nfig, axarr = plt.subplots(1,2)\nfig.set_size_inches(10,4)\n\naxarr[0].scatter(x1, x2)\naxarr[0].arrow(4.5, 2, 0, 6, color=\"red\", width=0.10)\narrow = axarr[0].arrow(4.5, 6, 0, -4, color=\"red\", width=0.10)\n\naxarr[0].title.set_text(\"Consider reducing 2-D data to 1-D\")\naxarr[0].set_xlabel(\"feat1 (on x-component of cartesian plane)\")\naxarr[0].set_ylabel(\"feat2 (on y-component of cartesian plane)\")\naxarr[0].legend([\"original data\"])\naxarr[0].legend([arrow,], ['Optimal component',])\naxarr[0].set_xticks(range(0, 10))\naxarr[0].set_yticks(range(0, 10))\n\naxarr[1].scatter(x3, x4)\narrow = axarr[1].arrow(3, 13, 1.6, 5, color=\"red\", width=0.10)\naxarr[1].arrow(4.6, 18, -1.6, -5, color=\"red\", width=0.10)\n\naxarr[1].title.set_text(\"Consider reducing 2-D data to 1-D\")\naxarr[1].set_xlabel(\"feat3 (on x-component of cartesian plane)\")\naxarr[1].set_ylabel(\"fea4 (on y-component of cartesian plane)\")\naxarr[1].legend([\"original data\", \"optimal component\"])\naxarr[1].legend([arrow,], ['Optimal component',])\naxarr[1].set_xticks(range(0, 10))\naxarr[1].set_yticks(range(10, 20))\n\nplt.show()","f159e305":"fig = plt.figure(figsize=(7,4))\n\n# cartesian plane start------------------------------------------\nbegx, begy = 0,0\ndx, dy = 4,0\nplt.arrow(begx, begy, dx, dy, color=\"black\", width=0.001, head_width=0.04, head_length=0.04)\n\nbegx, begy = 0,0\ndx, dy = -4,0\nplt.arrow(begx, begy, dx, dy, color=\"black\", width=0.001, head_width=0.04, head_length=0.04)\n\nbegx, begy = 0,0\ndx, dy = 0,3\nplt.arrow(begx, begy, dx, dy, color=\"black\", width=0.001, head_width=0.04, head_length=0.04)\n\nbegx, begy = 0,0\ndx, dy = 0,-4\nplt.arrow(begx, begy, dx, dy, color=\"black\", width=0.001, head_width=0.04, head_length=0.04)\n# cartesian plane end------------------------------------------\n\n\n# optimal component\nbegx, begy = 0,0\nendx, endy = 5,3\nplt.plot([begx, endx], [begy, endy],linestyle=\"-\", color=\"red\", label=\"optimal component u_1\")\n\n# points\nplt.scatter([2.5], [2.5], color=\"blue\", label=\"original datapoint x_i\")\nplt.scatter([3], [1.8], color=\"green\", label=\"projection of x_i\")\n\n# distance d\nbegx, begy = 2.5, 2.5\nendx, endy = 3, 1.8\nplt.plot([begx, endx], [begy, endy],linestyle=\"--\", color=\"orange\", label=\"distance d_i\")\n\n# original point vector\nbegx, begy = 0, 0\nendx, endy = 2.5, 2.5\nplt.plot([begx, endx], [begy, endy],linestyle=\"-\", color=\"blue\", label=\"x_i datapoint vector magnitude\")\n\n# magnitude of projection\nbegx, begy = 0, 0\nendx, endy = 3, 1.8\nplt.plot([begx, endx], [begy, endy],linestyle=\"--\", color=\"green\", label=\"x_i projection magnitude\")\n\nplt.title(\"Right-angled Triangle\")\nplt.axis('off')\nplt.legend()\nplt.show()","8c3278b6":"fig, axarr = plt.subplots(1,4)\nfig.set_size_inches(20,4)\n\n# 1. perfect slope\nx1 = np.random.uniform(3,4,100)\nx2 = 3*x1 + 4\n\naxarr[0].set_title(\"Fig1: Perfect slope\\n\u03bb2 = 0 \")\naxarr[0].scatter(x1, x2)\naxarr[0].plot([3.2, 3.88], [15.75,13], color=\"red\")\naxarr[0].plot(x1, x2, color=\"red\")\naxarr[0].grid()\naxarr[0].set_xlabel(\"feat1\")\naxarr[0].set_ylabel(\"feat2\")\n\n\n# 2. Fig2:\n_x1 = np.random.uniform(3,4,100)\nerrors = np.random.normal(0,0.13,100)\n_x2 = 3*_x1 + 4 + errors\n\naxarr[1].set_title(\"Fig2: \u03bb1>>\u03bb2\")\naxarr[1].scatter(_x1, _x2)\naxarr[1].plot(x1, x2, color=\"red\")\naxarr[1].plot([3.2, 3.83], [15.75,13], color=\"red\")\naxarr[1].grid()\naxarr[1].set_xlabel(\"feat1\")\naxarr[1].set_ylabel(\"feat2\")\n\n\n# 3. Fig3:\n_x1 = np.random.uniform(3,4,100)\nerrors = np.random.normal(0,0.6,100)\n_x2 = 3*_x1 + 4 + errors\n\naxarr[2].set_title(\"Fig3: \u03bb1>\u03bb2\")\naxarr[2].scatter(_x1, _x2)\naxarr[2].plot(x1, x2, color=\"red\")\naxarr[2].plot([3.3, 3.66], [17,11], color=\"red\")\naxarr[2].grid()\naxarr[2].set_xlabel(\"feat1\")\naxarr[2].set_ylabel(\"feat2\")\n\n\n# 4. Fig4: Perfect circle\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=1000, centers=1, n_features=2, random_state=0)\n\naxarr[3].set_title(\"Fig4: Perfect circle\\n\u03bb1=\u03bb2\")\naxarr[3].scatter(X.T[0], X.T[1])\naxarr[3].plot([-1,3],[2,7], color=\"red\")\naxarr[3].plot([-2,4],[7,2], color=\"red\")\naxarr[3].grid()\naxarr[3].set_xlabel(\"feat1\")\naxarr[3].set_ylabel(\"feat2\")\n\nplt.show()","fe1b545c":"# REPRODUCABLE CODE: Visualisation only\n# =====================================\n\n#libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy.linalg import eigh \nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef get_pca_df(X, labels):\n    \n    # 1.standardize\n    # ==============\n    standardized_data = StandardScaler().fit_transform(X)\n    covar_matrix = np.matmul(standardized_data.T , standardized_data)\n\n    # original dims of original data (m)\n    m = len(covar_matrix)\n    \n    # 2.Get first-two eigen vectors\n    # =============================\n    values, vectors = eigh(covar_matrix, eigvals=(m-2, m-1)) #(62, 63) for MNIST_8X8 i.e from end to front\n    U = vectors.T # shape (d, m) where\n\n    #print(values)    \n\n    # 3. Genereate new 2-D data (projections)\n    # ======================================\n    projections = np.matmul(U, standardized_data.T) #(d,m) x (m, n) => (d, n)  [where, n - num_of_samples ]\n    # appending label to the new 2-D data \n    projections = np.vstack((projections, labels)).T \n\n    # creating a new data frame for ploting the labeled points.\n    dataframe = pd.DataFrame(data=projections, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\n    \n    return dataframe\n\n\"\"\" # Plotting\n# plot bc \nsns.FacetGrid(df2, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n\nplt.title(\"Breast Cancer Dataset \\nm={} n={}\".format(X_bc.shape[1], X_bc.shape[0]))\nplt.show()\n\"\"\"","2b176345":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX_mnist = digits.data\nlabels_mnist = digits.target","b93a44bb":"df = get_pca_df(X_mnist, labels_mnist)\n# X - all numerical rvs\n# label - categorical rv","9e2fe2d3":"# plot\nsns.FacetGrid(df, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.title(\"MNIST Dataset \\nm={} n={}\".format(X_mnist.shape[1], X_mnist.shape[0]))\nplt.show()","16bd1ebc":"# load bc data\nfrom sklearn.datasets import load_breast_cancer\nbc = load_breast_cancer()\nX_bc = bc.data\nlabels_bc = bc.target","dae0ee04":"df2 = get_pca_df(X_bc, labels_bc)\n# X - all numerical rvs\n# label - categorical rv","9e75ead1":"# plot bc \nsns.FacetGrid(df2, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n\nplt.title(\"Breast Cancer Dataset \\nm={} n={}\".format(X_bc.shape[1], X_bc.shape[0]))\nplt.show()","d1b1afb1":"# REPRODUCIBLE CODE:\n# PCA for dimensionality redcution (non-visualization)\n# ====================================================\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import decomposition\npca = decomposition.PCA()\n\ndef pca_anaysis(sample_data):\n    \"\"\"Sample data is `X` with all numerical rvs\"\"\"\n    # connfig and transform\n    pca.n_components = sample_data.shape[1]\n    pca_data = pca.fit_transform(sample_data)\n\n    # calculate cumulative variance\n    percentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n    cum_var_explained = np.cumsum(percentage_var_explained)\n\n    # Plot the PCA spectrum\n    plt.figure(1, figsize=(6, 4))\n\n    plt.clf()\n    plt.plot(cum_var_explained, linewidth=2)\n    plt.axis('tight')\n    plt.grid()\n    plt.xlabel('n_components')\n    plt.ylabel('Cumulative_explained_variance')\n    plt.title(\"PCA Analysis\")\n    plt.show()\n    \nimport pandas as pd\ndef pca_get_df(sample_data, labels, new_dims=2):\n    \"\"\"Sample data is `X` with all numerical rvs\"\"\"\n    pca.n_components = new_dims\n    pca_data = pca.fit_transform(sample_data)\n    \n    df = pd.DataFrame(pca_data)\n    df['labels'] = labels\n    return df","a19b676a":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX_mnist = digits.data\nlabels_mnist = digits.target","0cadd5a1":"pca_anaysis(X_mnist)","0f5c9db1":"pca_get_df(X_mnist, labels_mnist, new_dims=20).head()","8be5215b":"df = pca_get_df(X_mnist, labels_mnist, new_dims=2)","b15022e1":"# plot bc \nimport seaborn as sns\nsns.FacetGrid(df, hue=\"labels\", height=6).map(plt.scatter, 0, 1).add_legend()\n\nplt.title(\"Mnist Dataset \\nm={} n={}\".format(X_mnist.shape[1], X_mnist.shape[0]))\nplt.show()","753bf086":"## Breast Cancer Dataset","91802d48":"## 01. Visualisation","2d84b317":"$$\\text{By pythagoras theorem,}\\,\\,\\, d^2 = ||\\vec{x_i}||^2 - p^2 = \\bigg( (x_i^T x_i) \\,-\\, (u_1^T x_i)^2 \\bigg) \\,\\, \\because ||u_1||=1$$","c2f9850d":"# **Procedure:**\n\n1. Colum standardize data into ${}^{standardized}{X_{nxm}}$\n2. Find optimal components (col vectors with size (mx1)) $u_1, u_2, u_3 \\dots u_d$ (d<m)\n3. Generate ${X_{nxd}}^{dim\\, reduced}$ w\/ help of projections\n\n$${}^{dim\\, reduced}{X_{n\\text{x}d}} = {}^{standardized}{X_{n\\text{x}m}} \\cdot {U_{m\\text{x}d}}$$ ","ee420706":"## Method 1of2: \n\n> Find direction  \ud835\udc62\u0302 (comonent) such that variance of projected points on  \ud835\udc62\u0302   is MAXIMIZED","628d85dc":"## **Finding Optimal Components**\n\n**OBJECTIVE:** To find *direction $\\hat{u_1}$* (optimal component) \n\nThis is an optimisation problem. It can be solve in two ways\n\n- Find *direction $\\hat{u_1}$* such that **variance of projected points on $\\hat{u_1}$ is MAXIMIZED**\n- Find *direction $\\hat{u_1}$* such that **distance b\/w original datapoints and $\\hat{u_1}$ is MINIMIZED**","18abdc12":"Right angled triangle is formed by --\n- distance $d_i$ b\/w original data-point $x_i$ and it's projected point on $\\vec{u_1}$ \n- magnitude of original datapoint vector i.e $||\\vec{x_1}||^2$ \n- magnitude of projection $||\\vec{p}||^2$","8f00fa90":"## Method 2of2: \n\n> Find direction  \ud835\udc62\u0302 (component) such that distance b\/w original datapoints and $\\hat{u_1}$ is MINIMIZED","479f26fe":"### MNIST Data","845c1d6e":"# **Principal Component Analysis**\n\n> *Reducing dimensions by*\n>\n> - **Maximizing variance** of projections on *optimal component* (or)\n> - **Minimizing distance** between an *optimal component* and original data samples*","f9e7afaa":"> ","bdcfdb54":"## Eigen Vectors\n\n> *Linear algebra technique. Interpreted differently in differenet areas of study* [(know more)](https:\/\/www.math.ubc.ca\/~pwalls\/math-python\/linear-algebra\/eigenvalues-eigenvectors\/)\n\n$$ S \\vec{v} = \\lambda \\vec{v} \\,\\,\\,\\, \\text{where S is covariance matrix (symetric square) and } \\lambda \\text{ is a scalar quantity}$$\n\nSolving the above equation gives different $\\lambda$ values called **EIGEN VALUES** such that,\n\n$$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3 \\ge \\lambda_4 \\ge \\dots \\lambda_d$$\n\nEach $\\lambda$ value has an **EIGEN VECTOR** $\\vec{v}_k$ assosciated with it. $\\vec{v}_k$ is column vector of size $\\text{(1 x d)}$ where d is size of $S$\n\n<br>\n\n---\n**Significance of $\\vec{v}$ values**\n> All are mutually perpendicular\n\n**Significance of $\\lambda$ values**\n\n> $$\\text{var % covered by eigen } \\vec{v}_k \\text{ compared to other GENERATED eigenvetors under consideration} = \\frac{\\lambda_k}{\\sum_{i=1}^d{\\lambda_i}}$$\n>\n> Used to **decide the num of dimensions** given, the amount of variability we want to retain\n> \n> For example, if **`x`% variability must be retained**, find minimum dims `k` such that **$\\sum^k_{i=1}{\\lambda_i} \\ge x $**","8050afc3":"The projections(shadows) of original data points on red-lines(components) will be our new data whose dimension has been reduced for 2-D to 1-D\n\n> Because, these **projected** points will have maximum variance. Thus, information is retained","a7508861":"### MNIST Dataset","9a6203a0":"## **Background**","ded3c94e":"**Optimisation Problem:**\n<span style=\"color:blue\">\n$$Minimize_{u_1}\\,\\, \\Bigg\\{ \\text{dist_metric(between x_i and it's projection)} \\Bigg\\} \\,\\,\\, \\text{such that } u_1=\\hat{u_1}\\,\\, \\text{i.e } {||u_1||}^{2} = 1 (constant)$$\n<\/span>\n\nHere, dist_metric is euclidean distance $d^2$ <br>\nThus the optimisation problem with constraints is,\n\n<span style=\"color:green\">\n$$ Min_{u_1} \\Bigg\\{  (x_i^T x_i) \\,-\\, (u_1^T x_i)^2  \\Bigg\\} \\,\\,\\, \\text{such that } ||u_1||^2 = 1 $$\n<\/span>\n\n\n> <br>\n> <b>Note that both optimisation problems look almost similar and can be proved that they are same [proof]()<\/b><br>\n> \n> Solving the above two optimisation problems are same as finding **Eigen vectors**","301a6474":"**Optimisation Problem:**\n<span style=\"color:blue\">\n$$Maximize_{u_1}\\,\\, \\Bigg\\{ \\text{var(projection of original-data on u1)} \\Bigg\\} \\,\\,\\, \\text{such that } u_1=\\hat{u_1}\\,\\, \\text{i.e } {||u_1||}^{2} = 1 (constant)$$\n<\/span>\n    \n$$\n\\Rightarrow Max_{u_1}\\,\\, {{\\Bigg\\{ var {\\bigg( \\frac{x_i \\cdot u_1}{||u_1||^2} \\bigg)}^{i=1,2\\dots n}} \\Bigg\\}} \\,\\,\\, \\text{where, } x_i \\in \\text{X - High dim original data and n - num of samples} \\\\\n\\Rightarrow Max_{u_1}\\,\\, {\\Bigg\\{ var {\\bigg( x_i \\cdot u_1 \\bigg)}^{i=1,2\\dots n}} \\Bigg\\}\\,\\,\\, \\because \\text{||u||=1} \\\\\n\\Rightarrow Max_{u_1}\\,\\, \\Bigg\\{   \\frac{1}{n}\\sum{({u_1}^T x_i - {u_1}^T \\cdot \\mu)^2}    \\Bigg\\}\n\\Rightarrow Max_{u_1}\\,\\, \\Bigg\\{   \\frac{1}{n}\\sum{({u_1}^T x_i)^2}    \\Bigg\\} \\,\\,\\, \\because \\text{col-standardized(mean=0)}\n$$ \n\n\nHence, the optimisation problem with constraints is,\n\n<span style=\"color:green\">\n$$ Max_{u_1}\\,\\, \\Bigg\\{   \\frac{1}{n}\\sum_{i=1}^{n}{({u_1}^T x_i)^2}    \\Bigg\\} \\,\\,\\, \\text{such that } ||u_1||^2 = 1 $$\n<\/span>\n\n\n> $||u_1||^2 = 1$ because, it must be constant. Otherwise, we can simply put $u_1 = \\infty$ for maxizing variance","b93d8dae":"# Limitations\n\n![image.png](attachment:image.png)","a951091a":"## 02. Analytical dimensionality reduction","1c16f610":"Consider the above case, converting **2-D data to new 2-D data (instead of dim reduction)**,\n\nThe two new components will be $\\hat{v_1}$ and $\\hat{v_2}$ corresponding to $\\lambda_1$ and $\\lambda_2$ (found using above formula)\n> *But what does $\\lambda_1$ and $\\lambda_2$ mean?*\n\n- $\\lambda_1$ $\\ge \\lambda_2$ Does **NOT** imply $\\text{variance-of-projections-of}\\, \\hat{v_1}$ $\\ge \\text{variance-of-projections-of}\\, \\hat{v_2}$ (Cannot be used for comparison)\n- $\\frac{\\lambda_1}{\\sum_{i=1}^d{\\lambda_i}}$ $\\ge \\frac{\\lambda_2}{\\sum_{i=1}^d{\\lambda_i}} \\Rightarrow \\text{variance-of-projections-of}\\, \\hat{v_1}$ $\\ge \\text{variance-of-projections-of}\\, \\hat{v_2}$ (Can be used for comparison)\n\n> Note: we are [normalizing by division by sum](https:\/\/www.kaggle.com\/l0new0lf\/02-08-normalisation-vs-standardisation-vs-probs)","4af67c88":"### Projections\n\nProjection of vector a on vector b,<br>\n$$\\text{Vector:}\\,\\, \\vec{p_{a,b}} = \\frac{a \\cdot b}{{||b||}^{2}} \\hat{b}$$\n$$\\text{Scalar:}\\,\\, |p_{a,b}| = \\frac{a \\cdot b}{{||b||^2}} $$\n\nNote that projections **conserve mean** i.e *projection of mean will be a mean*. And $||x||^2 = x \\cdot x = (x^T x)$","347bc2cf":"# **Implementation**\n\n- For visualisation\n- For dimension reduction","fd084eb5":"\n> ## Just 'Cause You Can't See It, Doesn't Mean It, Isn't There.\n>\n> <p style=\"text-align: right\"> <a href=\"https:\/\/www.youtube.com\/watch?v=Tm8LGxTLtQk\">\u2014 One More Light<\/a> <\/p>","d91c5aac":"**Observation**\n> If we take 20-dimensions, approx. 90% of variance is expalined.","96335fb2":"> **High variance $\\Rightarrow$ High information**\n>\n> While reducing dimensions, make sure **information is lost as less as possible i.e variance is as high as possible**\n>\n> **Intuition:**<br>\n> Imagine we have car dataset with 2 features `colors` and `num_of_wheels`. Which of the two featues can help us differentiate between *different cars*? `colors` because of **high variability (has large amount of info)**. Because of **zero variability(less\/no info)**, we cannot use `num_of_wheels`","aea72f47":"**PONDER**\n\n1. Use of NN to find $v_1$ in $x_i^{new} = v_1^T x_i$ (simple back propagation w\/ metric -- maximize variance of $x_i^{new}$)\n2. As all $v_1$, $v_2$, $ ... $ are mutually perpendicular, wouldn't it be sufficient to just find one of them and then use slope formula for perpendicular lines to find others"}}