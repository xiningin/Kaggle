{"cell_type":{"9577c9af":"code","34e8d093":"code","2dc8322c":"code","033c4b77":"code","b00cbd49":"code","dd6c9bfe":"code","bc7f07a2":"code","a245bafe":"code","49551416":"code","2c69f23b":"code","75f57500":"code","8788993e":"code","5d430b70":"code","44d2259c":"code","22cb7e82":"code","de2e2f4b":"code","473c4f15":"code","cff8ba64":"code","cdab5f4e":"code","b381d5fc":"code","902e0f54":"code","2a7ae192":"code","277da755":"code","2e875355":"code","6651afb5":"code","d5334aca":"code","2b07862f":"code","f73af524":"code","4eda7a74":"code","d08119dc":"code","b6930b21":"code","788f6dac":"code","266eb8ab":"code","8dcc851b":"code","f7a60afa":"code","843806e0":"code","b142f5f7":"code","c1a6498f":"code","d9dd2e5c":"code","6078215b":"code","77f74cea":"code","23fc1ee6":"code","efe6ca72":"code","a32112eb":"code","bf14788d":"code","8b8a3454":"code","89854853":"code","8c1faeac":"code","a0bd61a5":"code","9f598347":"code","5fe67b8f":"code","84319ceb":"code","d93c57c8":"code","1b494ccc":"code","8ae9999e":"code","0f451762":"code","5cfce9dc":"code","c0c4396d":"code","e686cc0b":"code","26731560":"code","14b745f3":"code","0e0f586b":"code","2063cd8f":"code","605dff24":"code","a3f87afa":"code","790404e4":"code","96120924":"code","9ebb2c1c":"code","c57d137d":"code","329849b4":"code","2ee6d474":"code","8327125c":"code","7ff0b305":"code","48127ec1":"code","5a15df50":"code","9880d9c7":"code","e8379deb":"markdown","46a72b25":"markdown","523c8fb5":"markdown","3842ea39":"markdown","cf699ab4":"markdown","6ee732ab":"markdown","b3abc061":"markdown","3a406915":"markdown","fe9a2519":"markdown","c411f3a1":"markdown","0a5506f8":"markdown","2dae22b0":"markdown","af96faeb":"markdown","7b220caa":"markdown","09a3670f":"markdown","5291286f":"markdown","dbce5b03":"markdown","b9466437":"markdown","45b684f2":"markdown","9d3b35c7":"markdown","1415a3f3":"markdown","ee022566":"markdown","3d2a5244":"markdown","0aa2c690":"markdown","003db7d1":"markdown","c383e0a9":"markdown","e1748723":"markdown","19904f36":"markdown","aefcb70a":"markdown","197c8252":"markdown","503d47c6":"markdown","c5782dc6":"markdown","9621b14b":"markdown","dd370624":"markdown","c7bc81d2":"markdown","85899564":"markdown","cf1e2f10":"markdown","9bf20b34":"markdown","53caa6b9":"markdown","8827a081":"markdown","00967175":"markdown","24452394":"markdown","500ad854":"markdown"},"source":{"9577c9af":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom scipy.special import expit  \n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Perceptron, SGDClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","34e8d093":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import expit  # sigmoid\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Perceptron, SGDClassifier","2dc8322c":"X = np.array([1, 2])\nW = np.array([0.1, 0.2])","033c4b77":"X * W ","b00cbd49":"np.sum(X * W)","dd6c9bfe":"np.dot(X, W)","bc7f07a2":"X @ W","a245bafe":"X.dot(W)","49551416":"def and_gate(x1, x2): \n    w1, w2, th = 0.5, 0.5, 0.7 \n    result = x1 * w1 + x2 * w2 \n    if result > th: \n        y = 1\n    else: \n        y = 0 \n\n    return y ","2c69f23b":"and_gate(x1=0, x2=0)","75f57500":"and_gate(x1=0, x2=1)","8788993e":"and_gate(x1=1, x2=0)","5d430b70":"and_gate(x1=1, x2=1)","44d2259c":"def and_gate2(x1, x2):\n    w1, w2, th = 0.1, 0.1, 0.1 \n    result = x1 * w1 + x2 * w2 \n    y = 1 if result > th else 0 \n    return y ","22cb7e82":"and_gate2(1, 1)","de2e2f4b":"def and_gate3(x):\n    w = np.array([0.5, 0.5])\n    th = 0.7\n    # y = 1 if np.sum(x * w) > th else 0 \n    y = 1 if np.dot(x, w) > th else 0 \n    return y ","473c4f15":"and_gate3(np.array([0.5, 0.5]))","cff8ba64":"def and_gate4(X):\n    w = np.array([0.5, 0.5])  # \uac00\uc911\uce58(weight)\n    th = 0.7\n    y = (np.dot(X, w) > th).astype(np.int_)\n    return y ","cdab5f4e":"X = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])","b381d5fc":"and_gate4(X)","902e0f54":"def and_gate5(X):\n    w = np.array([0.5, 0.5])  # \uac00\uc911\uce58 \n    b = -0.7  # \ud3b8\ud5a5 \n    result = np.dot(X, w) + b # Linear equation \n    y = result > 0 \n    return y.astype(np.int_)","2a7ae192":"and_gate5(X)","277da755":"def and_gate6(X):\n    w_bias = np.array([-0.7, 0.5, 0.5])  # bias\ub97c \ud3ec\ud568\ud55c weights: (bias, w1, w2)\n    n = len(X)\n    X_bias = np.c_[np.ones(n), X]\n    result = np.dot(X_bias, w_bias)  # X_bias @ w_bias\n    y = result > 0\n    return y.astype(np.int_)","2e875355":"and_gate6(X)","6651afb5":"def nand_gate(X):\n    w_bias = np.array([-0.7, 0.5, 0.5])  \n    n = len(X)\n    X_bias = np.c_[np.ones(n), X]\n    result = np.dot(X_bias, w_bias)  \n    y = (result <= 0)\n    return y.astype(np.int_)","d5334aca":"nand_gate(X[0:1])","2b07862f":"def or_gate(X):\n    w_bias = np.array([-0.5, 1, 1])  \n    n = len(X)\n    X_bias = np.c_[np.ones(n), X]\n    y = np.dot(X_bias, w_bias) > 0\n    return y.astype(np.int_)","f73af524":"or_gate(X)","4eda7a74":"def xor_gate(X):\n    y1 = nand_gate(X)  # perceptron, neuron\n    y2 = or_gate(X)\n    Y = np.c_[y1, y2]  # layer\n    z = and_gate6(Y)\n    return z","d08119dc":"xor_gate(X)","b6930b21":"def step(x): \n    # y = 1 if x > 0 else 0\n    y = x > 0\n\n    return y.astype(np.int_) ","788f6dac":"x = np.arange(-5, 5, 0.001)\ny = step(x)\n\nprint(x[:10])\nprint(y[:10])\n\nprint(x[-10:])\nprint(y[-10:])\n\nplt.plot(x, y)\nplt.grid()\nplt.show()","266eb8ab":"def plot_activation_fn(activation):\n    x = np.arange(-5, 5, 0.0001)  # X on the graph\n    y = activation(x)     # Return values from activation function\n    plt.plot(x, y)\n    plt.grid()\n    plt.show()","8dcc851b":"plot_activation_fn(activation=step)","f7a60afa":"def sigmoid(x):\n    y = 1 \/ (1 + np.exp(-x))\n    return y\n\nplot_activation_fn(sigmoid)\n\nplot_activation_fn(expit)","843806e0":"plot_activation_fn(np.tanh)","b142f5f7":"def relu(x):\n    y = np.maximum(x, 0)\n    return y","c1a6498f":"plot_activation_fn(relu)","d9dd2e5c":"X, y = load_iris(return_X_y=True)","6078215b":"X.shape  #> (n_sample, n_features)","77f74cea":"y.shape  #> (n_samples)","23fc1ee6":"X[:5]   #> sepal-length, sepal-width, petal-length, petal-width","efe6ca72":"y[:5]","a32112eb":"np.unique(y, return_counts=True)  #> 0: setosa, 1: versicolor, 2: virginica","bf14788d":"data = X[:, 2:4]","8b8a3454":"data[:5]","89854853":"target = (y != 0).astype(np.int_)\ntarget","8c1faeac":"plt.scatter(data[target== 0, 0], data[target == 0, 1], label='setosa')\nplt.scatter(data[target== 1, 0], data[target == 1, 1], label='non-setosa')\n\nplt.legend()\nplt.xlabel('petal length')\nplt.ylabel('petal width')\nplt.show()","a0bd61a5":"b = -2.0  # bias\nw = np.array([0.7, 1.2])  # weight ","9f598347":"z = np.dot(data, w) + b\nz","5fe67b8f":"predictions = step(z)\npredictions\n\nnp.mean(target == predictions)","84319ceb":"class SimplePerceptron:\n    def fit(self, X, y):\n        self.intercept_ = np.array([-2.0])  # bias\n        self.coef_ = np.array([0.7, 1.2])   # weights \n\n    def forward(self, X):\n        z = np.dot(X, self.coef_) + self.intercept_\n        return z \n\n    def activation(self, x):  # step function \n        y = x > 0\n        return y.astype(np.int_)\n\n    def predict(self, X):     # \uc774 \ud558\ub098\uac00 \ub274\ub7f0 \ud558\ub098 \uc774\ub2e4\n        z = self.forward(X)\n        y = self.activation(z)\n        return y \n\n    def score(self, X, y):\n        y_pred = self.predict(X)\n        acc = np.mean(y == y_pred)\n        return acc\n    ","d93c57c8":"neuron = SimplePerceptron()  # create neuron incidence","1b494ccc":"neuron.fit(data, target)   # model train","8ae9999e":"neuron.intercept_, neuron.coef_  # coefficient values are given after model training","0f451762":"neuron.forward(data)","5cfce9dc":"neuron.predict(data)","c0c4396d":"neuron.score(data, target)","e686cc0b":"perceptron = Perceptron(random_state=42)  # model incidence","26731560":"perceptron.fit(X=data, y=target)  # model train","14b745f3":"perceptron.intercept_, perceptron.coef_","0e0f586b":"perceptron.predict(data)","2063cd8f":"perceptron.score(data, target)","605dff24":"class SimpleNeuron:\n    def fit(self, X, y): \n        self.intercept_ = np.array([-35.6])\n        self.coef_ = np.array([11.8, 17.0])\n\n    def forward(self, X): \n        z = np.dot(X, self.coef_) + self.intercept_\n        return z \n\n    def activation(self, x):          \n        y = expit(x)   # sigmoid \ud568\uc218 \ub9ac\ud134\uac12 \n        return y \n\n    def predict_proba(self, X):\n        z = self.forward(X)  # \uc120\ud615 \ubc29\uc815\uc2dd\n        y = self.activation(z)  # sigmoid \ud65c\uc131\ud654\n        probas = np.c_[1 - y, y]  # [0(setosa)\uc774 \ub420 \ud655\ub960, 1(non-setosa)\uc774 \ub420 \ud655\ub960]\n        return probas \n\n    def predict(self, X): \n        probas = self.predict_proba(X)\n        preds = np.argmax(probas, axis=1)\n        return preds\n\n    def score(self, X, y):\n        preds = self.predict(X)\n        acc = np.mean(y == preds)\n        return acc","a3f87afa":"neuron = SimpleNeuron()","790404e4":"neuron.fit(data, target)","96120924":"neuron.intercept_, neuron.coef_\n\nz = neuron.forward(data)\ny = neuron.activation(z) \ny   # probability of target 1 (non-setosa)","9ebb2c1c":"pred_probs = neuron.predict_proba(data)\npred_probs[:5], pred_probs[-5:]","c57d137d":"neuron.predict(data)","329849b4":"neuron.score(data, target)","2ee6d474":"sgd_clf = SGDClassifier(loss='log', random_state=42)  ","8327125c":"sgd_clf.fit(data, target)  ","7ff0b305":"sgd_clf.intercept_, sgd_clf.coef_","48127ec1":"probas = sgd_clf.predict_proba(data)\nprobas[:5]","5a15df50":"sgd_clf.predict(data)","9880d9c7":"sgd_clf.score(data, target)","e8379deb":"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.\n\nSingle-layer perceptrons are only capable of learning linearly separable patterns. For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve a lot of otherwise non-separable problems. ([wiki](https:\/\/en.wikipedia.org\/wiki\/Perceptron))\n\nThe purpose to explain the logic behind how the values are being changed in simple terms.","46a72b25":"## Sigmoid neuron activation","523c8fb5":"From our knowledge of logic gates, we know that an AND logic table is given by the diagram below.","3842ea39":"| $X_1$ | $X_2$ |NAND($y_1$)| OR($y_2$) |z($y_1$ AND $y_2$)|\n|---|---|---|---|---|\n|0|0|1|0|0|\n|0|1|1|1|1|\n|1|0|1|1|1|\n|1|1|0|1|0|","cf699ab4":"The OR gate is 0 only if both inputs are 0.","6ee732ab":"* Wikipedia","b3abc061":"## Reference","3a406915":"# 2. Perceptron","fe9a2519":"## XOR(Exclusive OR) Gate","c411f3a1":"## Table of Contents  \n\n1. [Introduction](#1.-Introduction)\n2. [Perceptron](#2.-Perceptron)\n    - AND gate\n    - NAND gate\n    - OR gate\n    - XOR gate\n3. [Activation Function](#3.-Activation-Function)\n    - Step function\n    - Sigmoid function\n    - tanh function\n    - ReLu function\n4. [Artificial Neuron Practice](#4.-Artificial-Neuron-Practice)\n5. [Conclusion](#5.-Conclusion) ","0a5506f8":"## ReLu(Rectified Linear Unit) function","2dae22b0":"The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.","af96faeb":"## AND Gate","7b220caa":"Weights(w1, w2) which could make AND gate and threshold vary with combinations. ","09a3670f":"# 5. Conclusion","5291286f":"From the diagram, the NAND gate is 0 only if both inputs are 1.","dbce5b03":"| $X_1$ | $X_2$ | $y$ |\n|---|---|---|\n|0|0|0|\n|0|1|1|\n|1|0|1|\n|1|1|1|","b9466437":"# <center> [Tutorial] Perceptron Algorithm and Activation Function","45b684f2":"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities. ([wiki](https:\/\/en.wikipedia.org\/wiki\/Activation_function))","9d3b35c7":"Also, the steps in this method are very similar to how Neural Networks learn, which is as follows;\n\n* Initialize weight values and bias\n* Forward Propagate\n* Check the error\n* Backpropagate and Adjust weights and bias\n* Repeat for all training examples\n\nNow that we know the steps, let\u2019s get up and running:\n","1415a3f3":"| $X_1$ | $X_2$ | $y$ |\n|---|---|---|\n|0|0|1|\n|0|1|1|\n|1|0|1|\n|1|1|0|","ee022566":"A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula.","3d2a5244":"## NAND Gate","0aa2c690":"---","003db7d1":"## tanh(Hyperbolic tangent)","c383e0a9":"In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input \n\n${\\displaystyle f(\\mathbf {x} )={\\begin{cases}1&{\\text{if }}\\ \\mathbf {w} \\cdot \\mathbf {x} +b>0,\\\\0&{\\text{otherwise}}\\end{cases}}}$\n\nwhere ${\\displaystyle \\mathbf {w} }$  is a vector of real-valued weights, ${\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }$ is the dot product ${\\displaystyle \\sum _{i=1}^{m}w_{i}x_{i}}$, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.","e1748723":"---","19904f36":"hyperbolic functions are analogues of the ordinary trigonometric functions, but defined using the hyperbola rather than the circle. Just as the points (cos t, sin t) form a circle with a unit radius, the points (cosh t, sinh t) form the right half of the unit hyperbola. Also, just as the derivatives of sin(t) and cos(t) are cos(t) and \u2013sin(t), the derivatives of sinh(t) and cosh(t) are cosh(t) and +sinh(t).","aefcb70a":"## Step function","197c8252":"## Sigmoid function","503d47c6":"# 3. Activation Function","c5782dc6":"## sklearn.linear_module.SGDClassifier","9621b14b":"## OR Gate","dd370624":"In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.\n\nThanks for your watching and hope your kind Upvote! Thanks!","c7bc81d2":"XOR gate (sometimes EOR, or EXOR and pronounced as Exclusive OR) is a digital logic gate that gives a true (1 or HIGH) output when the number of true inputs is odd. An XOR gate implements an exclusive or; that is, a true output results if one, and only one, of the inputs to the gate is true. If both inputs are false (0\/LOW) or both are true, a false output results. XOR represents the inequality function, i.e., the output is true if the inputs are not alike otherwise the output is false. A way to remember XOR is \"must have one or the other but not both\".\n\nXOR can also be viewed as addition modulo 2. As a result, XOR gates are used to implement binary addition in computers. A half adder consists of an XOR gate and an AND gate. Other uses include subtractors, comparators, and controlled inverters.[1]\n\nThe algebraic expressions ${\\displaystyle A\\cdot {\\overline {B}}+{\\overline {A}}\\cdot B}$ or ${\\displaystyle (A+B)\\cdot ({\\overline {A}}+{\\overline {B}})}$ or ${\\displaystyle A\\oplus B}$ all represent the XOR gate with inputs A and B. The behavior of XOR is summarized in the truth table shown below.","85899564":"In mathematics, a function on the real numbers is called a step function (or staircase function) if it can be written as a finite linear combination of indicator functions of intervals. Informally speaking, a step function is a piecewise constant function having only finitely many pieces.","cf1e2f10":"# 1. Introduction","9bf20b34":"Below is the function to visualize after getting the activation function as a parameter. ","53caa6b9":"| $X_1$ | $X_2$ | $y$ |\n|---|---|---|\n|0|0|0|\n|0|1|0|\n|1|0|0|\n|1|1|1|","8827a081":"The reasons we use activation functions are :\n\n* To limit the output to the specific values(i.g. 0, 1) or the range(i.g. 0 ~ 1)\n* To differentiate the multi-layer neural network or deep neural network from the single-layer neural network","00967175":"## sklearn.linear_model.Perceptron ","24452394":"# 4. Artificial Neuron Practice","500ad854":"To mkae this simple, I will\n* select petal length and width\n* for binary classification between setosa(0) \/ non-setosa(1)"}}