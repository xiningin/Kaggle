{"cell_type":{"ef8b7504":"code","0565328c":"code","210d10ab":"code","43fe9258":"code","84b675d4":"code","46b6ca9d":"code","15b9e93a":"code","9fd38e07":"code","d7d52d2f":"code","06dee72b":"code","5457c872":"code","c3518636":"code","8995ca31":"code","8d513acc":"code","3eb96e27":"code","598ba7a1":"code","bed540c9":"code","706a8d8e":"code","e376631f":"markdown","debd79ee":"markdown","3526cc08":"markdown","c934991a":"markdown","5775f921":"markdown","84b8c8b0":"markdown","19cf2f5a":"markdown","41f2f34f":"markdown","3cb4a0f3":"markdown","b4447591":"markdown","4373ece3":"markdown","ea903a5f":"markdown","b4f56ca8":"markdown","554a5a08":"markdown","a5ffe7c1":"markdown"},"source":{"ef8b7504":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom sklearn.decomposition import PCA\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout, BatchNormalization\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import  train_test_split\nfrom keras import backend as K\nfrom keras import optimizers\nimport keras as k\nimport time\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.utils import class_weight\nimport warnings\nwarnings.filterwarnings('ignore')","0565328c":"sampledf = False  #Uses a specific amount of rows , use this for faster training and testing functionalities and features\nfrac_sample = 0.01 #fraction of the data to use\naugmnt = False #use an augmented data set\nfold_train = True\nagmnt_between = True #augment training data between folds\nkfold_shuffle = False\nuse_perc = False #using percentiles in feature engineering\nlog_transf = False\nsq_data = True\ndim_red = False\nn_components = 60\nstandradize=False\ntrain_between = True\nfreq = True\nfeature_eng_cv = True #to prevent leakage we apply feature engineering inside the K-fold cv for each split independently\n#--------------------------------------------------------------\n#Keras options\n#Weighted Classes when training \nweighted = False\nbalanced = False #balanced weights\n#-------------------\n#train test Split\ntst_size = 0.3\n\nsub_name = 'submission'\nprint('Options Active: \\n\\t SampledDF: {} frac: {} \\n\\t Augmentation: {}\\n\\t Weighted: {}\\n\\t Balanced: {}\\\n      \\n\\t agmnt_between:{}\\n\\t Percentiles: {}\\n\\t LOG_Transform: {}\\n\\t PCA: {}\\n\\t Square: {}\\n\\t Standradize: {}\\n\\t Train_Between: {}\\n\\t Freq: {}'.format(sampledf,frac_sample,augmnt,\n                                                                                                                             weighted,balanced,agmnt_between,use_perc,log_transf\n                                                                                                                            ,dim_red,sq_data,standradize,train_between,freq))","210d10ab":"df_t = pd.read_csv('..\/input\/train.csv')\ndf_tst = pd.read_csv('..\/input\/test.csv')\n\nif sampledf:\n    sub_name = sub_name+'_sampled'\n    df_train = df_t.sample(frac=(frac_sample))\n    df_test = df_tst.sample(frac=(frac_sample))\n    print('Loading Sampled df..')\nelse:\n    df_train = df_t.copy()\n    df_test = df_tst.copy()\n\nprint('Training df shape',df_train.shape)\nprint('Test df shape',df_test.shape)","43fe9258":"df_train.head()","84b675d4":"def vis_classes(labels,values,title='Target Percentages'):\n    trace=go.Pie(labels=labels,values=values)\n    layout = go.Layout(\n        title=title,\n        height=600,\n        margin=go.Margin(l=0, r=200, b=100, t=100, pad=4)   # Margins - Left, Right, Top Bottom, Padding\n        )\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \n    \nlabels = [str(x) for x in list(df_train['target'].unique())]\nvalues = [(len(df_train[df_train['target'] == 0])\/len(df_train))*100,(len(df_train[df_train['target'] > 0])\/len(df_train))*100]    \nvis_classes(labels,values)","46b6ca9d":"df_ones = df_train[df_train['target'] > 0]\nprint('Ones',df_ones.shape)\ndf_zeros = df_train[df_train['target'] == 0].sample(frac=0.25)\nprint('Zeros',df_zeros.shape)\n#we concat both to the sampling dataframe\ndf_sampling = pd.concat([df_ones, df_zeros]).sample(frac=1) #shuffling\nprint(df_sampling.shape)","15b9e93a":"#thanks to https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","9fd38e07":"#part of it Inspired by Gabriel Preda 's Kernel'\n\ndef eng_features_preprocess(df,idx,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99]):\n    #data metrics\n    print('  * Loading new data metrics: ')\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n    #moving average\n    print('  * Loading moving average metric: ')\n    df['ma'] =  df[idx].apply(lambda x: np.ma.average(x), axis=1)\n    \n    \n    #\n    if standradize:\n        print('  * Standradizing the data..')\n        #inf values can result from squaring\n        scaler = StandardScaler()\n        df_train.iloc[:,2:] = scaler.fit_transform(df_train.iloc[:,2:])\n        df_test.iloc[:,1:] = scaler.fit_transform(df_test.iloc[:,1:])\n        print('  * Data Standradized!')\n\n\n    if use_perc:\n        print('  * Loading percentiles: ')\n        for i in perc_list:\n            df['perc_'+str(i)] =  df[idx].apply(lambda x: np.percentile(x, i), axis=1)\n    \n    #thanks to  https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/87486#latest-506429\n    if freq:\n        print('  * Loading frequency: ')\n        for var in idx:\n            hist, bin_edges = np.histogram(df[var], bins=1000, density=True)\n            hist_test, bin_edges_test = np.histogram(df[var], bins=1000, density=True)\n            df['test_'+var] = [ hist[np.searchsorted(bin_edges,ele)-1] for ele in df[var]]\n\n    if log_transf:\n        print('  * Loading log transformations')\n        for i in range(200):\n            df['var_log_'+str(i)] = np.log(df['var_'+str(i)])\n\n    if sq_data:\n        print('  * Loading Squared data: ')\n        for i in range(200):\n            df['var_sq_'+str(i)] = np.square(df['var_'+str(i)])","d7d52d2f":"#part of it Inspired by Gabriel Preda 's Kernel'\nif not feature_eng_cv: #full scale feature engineering on train and test data\n    def feature_creation(df,idx,use_perc,perc_list,freq,log_transf,name_num='_1'):\n        #data metrics\n        print('  * Loading new data metrics: ', name_num)\n        df['sum'] = df[idx].sum(axis=1)  \n        df['min'] = df[idx].min(axis=1)\n        df['max'] = df[idx].max(axis=1)\n        df['mean'] = df[idx].mean(axis=1)\n        df['std'] = df[idx].std(axis=1)\n        df['skew'] = df[idx].skew(axis=1)\n        df['kurt'] = df[idx].kurtosis(axis=1)\n        df['med'] = df[idx].median(axis=1)\n        #moving average\n        print('  * Loading moving average metric: ', name_num)\n        df['ma'] =  df[idx].apply(lambda x: np.ma.average(x), axis=1)\n        #percentiles\n        if use_perc:\n            print('  * Loading percentiles: ', name_num)\n            for i in perc_list:\n                df['perc_'+str(i)] =  df[idx].apply(lambda x: np.percentile(x, i), axis=1)\n\n    #interactions\n    #coming..\n\n\n    perc_size  = 0\n    perc_list = [1,2,5,10,25,50,60,75,80,85,95,99]\n    if use_perc:\n        perc_size = len(perc_list)\n    start_time = time.time()\n    for i,df in enumerate([df_train,df_test]):\n        print('Loading more features for df: {}\/{}'.format(i+1,3))\n        print('Creating Metrics Part 1')\n        features_1 = df_train.columns.values[2:202]\n        feature_creation(df,features_1,use_perc,perc_list,name_num='_1') #adding columns using the train features (#200)\n        print('Creating Metrics Part 2')\n        features_2 = df_train.columns.values[2:211+perc_size] #all features included the ones added\n        feature_creation(df,features_2,use_perc,perc_list,name_num='_2') #adding columns using the train features + the new features\n        #drop repeated columns\n        df.drop(['min_2','max_2'],axis=1,inplace=True)\n        print('-'*50)\n\n    #thanks to  https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/87486#latest-506429\n    if freq:\n        print('Creating freq..')\n        for var in df_train.columns.values[2:202]:\n            hist, bin_edges = np.histogram(df_train[var], bins=1000, density=True)\n            hist_test, bin_edges_test = np.histogram(df_train[var], bins=1000, density=True)\n            df_train['test_'+var] = [ hist[np.searchsorted(bin_edges,ele)-1] for ele in df_train[var] ]\n            df_test['test_'+var] = [ hist_test[np.searchsorted(bin_edges_test,ele)-1] for ele in df_test[var] ]\n\n    if log_transf:\n        print('Loading log transformations')\n        for i in range(200):\n            df_train['var_log_'+str(i)] = np.log(df_train['var_'+str(i)])\n            df_test['var_log_'+str(i)] = np.log(df_test['var_'+str(i)])\n\n    if sq_data:\n        print('Loading Squared data..')\n        for i in range(200):\n            df_train['var_sq_'+str(i)] = np.square(df_train['var_'+str(i)])\n            df_test['var_sq_'+str(i)] = np.square(df_test['var_'+str(i)])\n\n    print('Features loaded !')\n    print(\"Execution --- %s seconds ---\" % (time.time() - start_time))\n    print('Train df: ', df_train.columns)\n    print('Test df: ', df_test.columns)\n    print('Number of Features: ', len(df_train.columns[2:]))","06dee72b":"X = df_train.iloc[:,2:]\nY = df_train['target']\nX_target = df_test.iloc[:,1:]\n\n#Applying this before the fold split can cause data leakage and that's why the score you get CV will not be \n#a good indicator of the one you submit for the competition\nif augmnt:\n    print('Data Augmentation: Enabled')\n    X,Y = augment(X.values,Y.values,t=2)\n    X = pd.DataFrame(X,columns=df_train.columns[2:])\n    Y = pd.Series(Y)\n    print('Augmentation Succeeded')\n    labels = [\"0\",\"1\"]\n    values = [(sum(Y == 0)\/len(Y))*100,(sum(Y > 0)\/len(Y))*100]    \n    vis_classes(labels,values,title ='Target Percentages After Augmentation')\n    sub_name = sub_name+'_agmted'    ","5457c872":"if dim_red :\n    print('Reducing Dimension to:',n_components)\n    pca = PCA(n_components= n_components)\n    xnew =pca.fit_transform(X)\n    xtest = pca.fit_transform(X_target)\n    X =pd.DataFrame(xnew,columns=['pc_'+str(i) for i in range(n_components)])\n    X_target = pd.DataFrame(xtest,columns=['pc_'+str(i) for i in range(n_components)])\n    print(X.shape)\n    X.head()","c3518636":"#test train split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=tst_size, random_state=6666)\n\n#Sampling train test split\nX_smple = df_sampling.iloc[:,2:]\ny_smple = df_sampling['target']\nX_train_smple, X_test_smple, y_train_smple, y_test_smple = train_test_split(X_smple, y_smple, test_size=0.4, random_state=6)\n\nprint(\"X_train: \", X_train.shape)\nprint(\"X_test: \" ,X_test.shape)","8995ca31":"#Model LGBM \nparam = {\n    'bagging_freq': 10, #handling overfitting\n    'bagging_fraction': 0.2,#handling overfitting - adding some noise\n     #'boost': 'dart', \n    #'boost': 'goss',\n     'boost_from_average':False,\n     'boost': 'gbdt',   \n    'feature_fraction': 0.15, #handling overfitting\n    'learning_rate': 0.01, #the changes between one auc and a better one gets really small thus a small learning rate performs better\n    'max_depth':2, \n    'metric':'auc',\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'xentropy', \n    'verbosity':1,\n    \"bagging_seed\" : 122,\n    \"seed\": 20,\n    }\ndef create_model_lgbm(param,X_train,y_train,X_val=None,y_val=None):\n    dtrain = lgb.Dataset(X_train,label=y_train)\n    if not X_val is None:\n        dval = lgb.Dataset(X_val,label=y_val)\n        valid_sets = (dtrain,dval)\n        valid_names = ['train','valid']\n        num_boost_round = 200000\n    else:\n        valid_sets = (dtrain)\n        valid_names = ['train']\n        num_boost_round = 60000\n    model = lgb.train(param,dtrain,num_boost_round=num_boost_round,valid_sets=valid_sets,valid_names=valid_names,\n                      verbose_eval=3000,\n                     early_stopping_rounds=3000)\n    return model\n\n\n#Setting up\nprint('Processing and FE Test Data..')\neng_features_preprocess(X_target,X_target.columns,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99])\nlgbm_test_x = X_target\npredictions = df_test[['ID_code']]\nprint('Using Params:\\n',param)\nval_aucs = []\nval_pred = 0\ntarget_pred = 0\nimportand_folds = 0\nsub_train_n =2\nkf = StratifiedKFold(n_splits=5,shuffle = kfold_shuffle, random_state=546)\nif fold_train:\n    for _fold, (trn_idx, val_idx) in enumerate(kf.split(X.values, Y.values)):\n            Xtrn, ytrn = X.iloc[trn_idx], Y.iloc[trn_idx]\n            Xval, y_val = X.iloc[val_idx], Y.iloc[val_idx]\n            #FE \n            if feature_eng_cv: #applying feature engieering and preprocessing of each train val split\n                print('FE on training data..')\n                eng_features_preprocess(Xtrn,Xtrn.columns,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99])\n                print('FE on val data..')\n                eng_features_preprocess(Xval,Xval.columns,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99])\n                print('Num of features : ', len(Xtrn.columns))\n            #------------------------------------------------------------------------------------------------------------------------\n            #Just for info\n            ones_train = (sum(ytrn>0) \/ len(ytrn))*100\n            ones_val = (sum(y_val>0) \/ len(y_val))*100\n            print('-'*50)\n            print(\"Fold num:{}\".format(_fold + 1))\n            print('\\tTrain Perc: 1: {:.2f}%, 0: {:.2f}%'.format(ones_train,100-ones_train))\n            print('\\tValid Perc: 1 : {:.2f}%, 0:{:.2f}%'.format(ones_val,100-ones_val))\n            #augmentation for each training \n            #thanks to https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment\n            if train_between:\n                val_pred = 0\n                target_pred = 0\n                for i in range(sub_train_n):\n                    print('\\tSub-Train: {}'.format(i+1))\n                    if agmnt_between:\n                        X_t, y_t = augment(Xtrn.values, ytrn.values)\n                        print('\\tAugmentation Succeeded..')\n                        X_t = pd.DataFrame(X_t)\n                        X_t = X_t.add_prefix('var_')\n                        print('\\tFitting Model')\n                        clf = create_model_lgbm(param,X_t,y_t,Xval,y_val)\n                    else:\n                         clf = create_model_lgbm(param,Xtrn,ytrn,Xval,y_val)\n                    \n                    target_pred += clf.predict(lgbm_test_x)\n                    val_pred += clf.predict(Xval)\n            #this part could be used when the augmentation is fully applied to the training data\n            else:\n                clf = create_model_lgbm(param,Xtrn,ytrn,Xval,y_val)\n                val_pred  = clf.predict(Xval)\n                target_pred = clf.predict(X_target)\n            \n            print('-' * 50)\n            \n            importand_folds += clf.feature_importance()\n            if train_between:\n                val_score = roc_auc_score(y_val, val_pred\/sub_train_n)\n                predictions['fold{}'.format(_fold+1)] = target_pred\/sub_train_n\n            else:\n                val_score = roc_auc_score(y_val, val_pred)\n                predictions['fold{}'.format(_fold+1)] = target_pred\n                \n            val_aucs.append(val_score)\n            print('\\tVal CV score : {}'.format(val_score))\n            print('-' * 50)\n            \n\nmean_cv_score = np.mean(val_aucs)\nprint ('-----   Mean CV Score: {:.2} ------'.format(mean_cv_score))","8d513acc":"if feature_eng_cv:\n     print('FE on training data..')\n     eng_features_preprocess(X_train,X_train.columns,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99])\n     print('FE on val data..')\n     eng_features_preprocess(X_test,X_test.columns,use_perc,freq,log_transf,sq_data,perc_list =[1,2,5,10,25,50,60,75,80,85,95,99])\n        \nclf_non_cv = create_model_lgbm(param,X_train,y_train,X_test,y_test)\nlgbm_pred_noncv = clf_non_cv.predict(X_target)\n#full val score\nprint('LGBM NO-CV Val Score: {}'.format(roc_auc_score(y_test,clf_non_cv.predict(X_test))))","3eb96e27":"num_features = 60\nif fold_train:\n    indxs = np.argsort(importand_folds\/ kf.n_splits)\nelse:\n    indxs = np.argsort(clf.feature_importance())[:num_features]\n    \nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance()[indxs],X_target.columns[indxs])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Top {} LightGBM Features accorss folds'.format(num_features))\nplt.tight_layout()\nplt.show()","598ba7a1":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\n\n#Model NN definition\ndef create_model_nn(in_dim,layer_size=200):\n    model = Sequential()\n    model.add(Dense(layer_size,input_dim=in_dim, kernel_initializer='normal'))\n    if not standradize:\n        model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    for i in range(2):\n        model.add(Dense(layer_size))\n        if not standradize:\n            model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(optimizer=adam,loss='binary_crossentropy',metrics = [auc])    \n    return model\n\n#Class weights to handle the unbalanced dataset\nclass_weights = None\nif weighted:\n    sub_name = sub_name+'_weighted'\n    if balanced:\n        class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\n    else:\n        class_weights = {\n            1:50, \n            0:1\n                }\n\n\nmodel_nn = create_model_nn(X_train.shape[1])\ncallback = EarlyStopping(monitor=\"val_auc\", patience=50, verbose=0, mode='max')\nhistory = model_nn.fit(X_train, y_train, validation_data = (X_test ,y_test),epochs=50,batch_size=64,verbose=1,callbacks=[callback],class_weight=class_weights)\ntarget_pred_nn = model_nn.predict(X_target)[:,0]\nprint('\\n Validation Max score : {}'.format(np.max(history.history['val_auc'])))","bed540c9":"#Ditribution Plots from both models \nnn_val_pred = model_nn.predict(X_test,batch_size=64)[:,0]\npredictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1) \npredictions['target_noncv'] = lgbm_pred_noncv\npredictions['target_keras'] = target_pred_nn\n\ncomb_approach_test = (0.1*target_pred_nn)+(0.9*predictions['target'])\ncomb_approach_test[comb_approach_test>1]=1\ncomb_approach_test[comb_approach_test<0]=0\n\nif fold_train:\n    plt.figure(figsize=(13, 9))\n    #validations sets\n    sns.distplot(nn_val_pred,label='NN Val Score:{:.3f}'.format(roc_auc_score(y_test,nn_val_pred)))\n    sns.distplot(val_pred,label='LGBM Val Score : {:.3f}'.format(mean_cv_score))\n    plt.title('Validation set target predictions')\n    plt.legend()\n    plt.show()\n    plt.savefig('combination_val.png')\n\nplt.figure(figsize=(13, 9))\n#target final test set\nsns.distplot(target_pred_nn,label='NN Target')\nsns.distplot(predictions['target'],label='LGBM Target')\nsns.distplot(lgbm_pred_noncv,label='Non-CV LGBM Target')\nsns.distplot(comb_approach_test,label='Combination Prediction Target')\nplt.title('Test set target predictions')\nplt.legend()\nplt.show()\nplt.savefig('combination_target_test.png')","706a8d8e":"def sub_pred(preds,df_test,name='submission.csv'):\n    sub_df = pd.DataFrame({'ID_code':df_test['ID_code'],'target':preds})\n    sub_df.to_csv(name, index=False)\n\nsub_file =  sub_name +'.csv'\nsub_pred(predictions['target'],df_test,name=sub_file)\nprint(sub_file+'   --submitted successfully')\n\nprint('Submitting Combination File..')\nsub_pred(comb_approach_test,df_test,name='comb_submission.csv')\nprint('comb_submission.csv   --submitted successfully')\n\n\n\nprint('Submitting non-cv File..')\nsub_pred(predictions['target_noncv'] ,df_test,name='lgbm_noncv_submission.csv')\nprint('lgbm_noncv_submission.csv   --submitted successfully')\n\n\nprint('Submitting non-cv File..')\nsub_pred(predictions['target_keras'] ,df_test,name='nn_submission.csv')\nprint('nn_submission.csv   --submitted successfully')","e376631f":"# 2.Keras NN Model\n<a id='keras'><\/a>","debd79ee":"# 1. LGBM Model\n<a id='lgbm'><\/a>","3526cc08":"## Sampling from the full dataset (more work on this later)\n<a id='sample'><\/a>","c934991a":"**DistPlot Analysis:** \n\nWe can see from the plots how the predictions for the validation sets, and the final target test set follows closely a similar distribution.\nWhich tells us that the test set can be a resemblance of validation sets we are using. \nThen we can proceed with improving our scores in the validation set knowing that there is a high chance they will also improve in the test set.\n\n**Combination Analysis**\n\nWe can also see that the combination of both is adding some noise to the prediction, which in some cases can prove helpful when each model\nwas able to predict with some features better than the others\n\nmore testing is going on here to see how effecient a combination model can get.","5775f921":"A Faster way to go up again and check what caused the results.\n[Training Options](#options)","84b8c8b0":"## Running Models Options\n<a id='options'><\/a>\nThe options here helps you check the different combinations for training and check which fits best.","19cf2f5a":"# Submissions\n<a id='sub'><\/a>","41f2f34f":"We will Reduce our dimensions to n number of components and test whether this approach can also help in providing better results","3cb4a0f3":"# Deep Learning + LGBM + Weighted Combination\n\nThis kernel will always be running with different parameters and approaches until before the competition deadline.\n\nFeel free to upvote,fork and test the presented models with different training options, to see if a better score with the following models is possible.\n\nIf forked Please try the different combinations:\n- Only Feature Engineering ( omitting some features maybe)\n- Only Augmented\n- Augmented + Feature Engineering (Augment before or after FE)\n- Augmented + Feature Engineering + folds\n- Augmented + Feature Engineering + full\n- Combination of different prediction weights\n- etc..\n\n\n**Don't forget that with each combination you might need different hyper-parameters for the models**\n\n\nYou can also check here for weighted CV approach that will make a minor better prediction that you might need in the competition:\nhttps:\/\/www.kaggle.com\/hjd810\/introducing-weighted-cross-validation\n\nEnjoy ! \n\nAny comments are appreciated (added motivation <3)\n\n1. [Training Options](#options)\n2. [Sampling](#sample)\n3. [Feature Engineering](#fe)\n4. [Dim Reduction](#dim)\n5. [LGBM Model](#lgbm)\n6. [Keras Model](#keras)\n7. [Combination Vis](#vis)\n8. [Submission](#sub)","b4447591":"# Dimensionality Reduction\n<a id='dim'><\/a>","4373ece3":"As you can see we are dealing with an unbalanced targets (10% vs 90%)","ea903a5f":"As we can see , many of the engineered features are present within the top 60 important features","b4f56ca8":"## Combination Vis\n<a id='vis'><\/a>","554a5a08":"## Simple Feature Engineering and Pre-processing\n* sum\n* min\n* max\n* mean\n* std\n* skew\n* kurt\n* med\n* Moving Average\n* percentiles\n* Augmentation\n* Log transformation\n* normalization\n<a id='fe'><\/a>","a5ffe7c1":"\n### Non-CV LGBM Approach"}}