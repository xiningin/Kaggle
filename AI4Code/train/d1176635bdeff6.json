{"cell_type":{"b30edf59":"code","4976715e":"code","2c48938b":"code","d70628f7":"code","c7fd54bd":"code","4e61566c":"code","1d06c106":"code","1694c248":"code","efc5d37f":"code","c2802c6c":"code","44dc5ba4":"code","839bc56a":"code","3f000e53":"code","1d8c43b0":"code","3a43a22b":"code","bbeca950":"code","634ff457":"code","d859173d":"code","c16cf448":"code","63e27ae4":"code","924b5ebe":"code","b04c83a9":"code","c3e77c01":"code","ee487003":"code","d5270021":"code","cba739f1":"code","52815a58":"code","0e42fc7e":"code","0dbad75c":"code","0358e13d":"code","5eac923e":"code","e17d7147":"code","991d38a2":"code","218874c0":"code","c7422886":"code","39d5a4e2":"markdown","4bc75a1b":"markdown","1ffe999a":"markdown","fdb24ae8":"markdown","a98f53d4":"markdown","e1c88506":"markdown","9a0b46c6":"markdown","8b643c2c":"markdown","56dd25ed":"markdown","cbe6ca81":"markdown","b924b883":"markdown","688b927c":"markdown","52956929":"markdown","4338a2f2":"markdown","6175a304":"markdown","20de0229":"markdown","aee5ae79":"markdown","533eaaa7":"markdown","de9ad5cb":"markdown","582b5c51":"markdown","49e095c8":"markdown"},"source":{"b30edf59":"from gensim.models import KeyedVectors\nimport pandas as pd\nimport os\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\n\n########################################\n# if stopwords are not downloaded to your environment\n# import nltk\n# nltk.download('stopwords')\n########################################\nimport gensim.downloader as api\nword_vectors = api.load(\"glove-wiki-gigaword-100\")\n\n########################################\n# if you want to save \/ load from local vectors\n#word_vectors.save('vectors.kv')\n#word_vectors = KeyedVectors.load('vectors.kv')\n########################################","4976715e":"%config Completer.use_jedi = False\nLOWER_CASE = True","2c48938b":"df = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")\ndf_ss = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")","d70628f7":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = \"..\/input\/feedback-prize-2021\/train\"):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext","c7fd54bd":"from collections import defaultdict\n\naam_misspell_dict = {'colour':'color',\n                'centre':'center',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                \"genericname\": \"someone\",\n                 \"driveless\" : \"driverless\",\n                 \"canidates\" : \"candidates\",\n                 \"electorial\" : \"electoral\",\n                 \"genericschool\" : \"school\",\n                 \"polution\" : \"pollution\",\n                 \"enviorment\" : \"environment\",\n                 \"diffrent\" : \"different\",\n                 \"benifit\" : \"benefit\",\n                 \"schoolname\" : \"school\",\n                 \"artical\" : \"article\",\n                 \"elctoral\" : \"electoral\",\n                 \"genericcity\" : \"city\",\n                 \"recieves\" : \"receives\",\n                 \"completly\" : \"completely\",\n                 \"enviornment\" : \"environment\",\n                 \"somthing\" : \"something\",\n                 \"everyones\" : \"everyone\",\n                 \"oppurtunity\" : \"opportunity\",\n                 \"benifits\" : \"benefits\",\n                 \"benificial\" : \"beneficial\",\n                 \"tecnology\" : \"technology\",\n                 \"paragragh\" : \"paragraph\",\n                 \"differnt\" : \"different\",\n                 \"reist\" : \"resist\",\n                 \"probaly\" : \"probably\",\n                 \"usuage\" : \"usage\",\n                 \"activitys\" : \"activities\",\n                 \"experince\" : \"experience\",\n                 \"oppertunity\" : \"opportunity\",\n                 \"collge\" : \"college\",\n                 \"presedent\" : \"president\",\n                 \"dosent\" : \"doesnt\",\n                 \"propername\" : \"name\",\n                 \"eletoral\" : \"electoral\",\n                 \"diffcult\" : \"difficult\",\n                 \"desicision\" : \"decision\"\n }","4e61566c":"txt = []\nfor i in tqdm( df[\"id\"].unique() ):\n    txt.append( read_train_file(i) )","1d06c106":"from collections import defaultdict\n\ninitial_vocab = defaultdict(int)\n\nfor i in tqdm(txt, total = len(txt)):\n    words = i.split()\n    for word in words:\n        initial_vocab[word.lower()] += 1","1694c248":"print(\"Total vocabulary including stopwords is : \", len(initial_vocab))","efc5d37f":"word_df = pd.DataFrame(initial_vocab.items(),\n            columns = [\"raw_words\", \"raw_words_counts\"])\nprint(\"-\"*80)\nprint(\"Displaying Head of the words dataframe\")\ndisplay(word_df.head())\nprint(\"-\"*80)\nprint(\"Displaying Tail of the words dataframe\")\ndisplay(word_df.tail())\nprint(\"-\"*80)","c2802c6c":"stops = stopwords.words(\"english\")\n\nword_df[\"is_stop_word\"] = word_df[\"raw_words\"].apply(lambda x: 0 if x not in stops else 1)","44dc5ba4":"word_df[\"is_stop_word\"].value_counts()","839bc56a":"def apply_coverage(x):\n    if x in word_vectors:\n        return 1\n    return 0\n        \nword_df[\"raw_in_vectors\"] = word_df[\"raw_words\"].apply(apply_coverage)","3f000e53":"def get_coverage(column_words,\n                 column_word_counts,\n                 column_word_presence,\n                 df, exc_stop = True):\n    '''\n        column_words : the column containing the words for which we check coverage\n        column_word_counts : the column containing pre-computed word counts for the words in question\n        column_word_presence : Just an output column name where we will output if word exists in word_vectors\n        exc_stop : Should we exclude stopwrods from coverage analysis or not\n    '''\n    word_df = df.copy()\n    word_df[column_word_presence] = word_df[column_words].apply(apply_coverage)\n    print(\"-\" * 80)\n    \n    #display(word_df[column_word_presence].value_counts(normalize = True))\n    #print(\"-\" * 80)\n    if exc_stop == False:\n        word_coverage = 100*word_df[column_word_presence].value_counts(normalize = True)[1]\n        text_coverage = 100*word_df.groupby([column_word_presence])[column_word_counts].sum()[1] \/ (word_df.groupby([column_word_presence])[column_word_counts].sum()[0] + \n                         word_df.groupby([column_word_presence])[column_word_counts].sum()[1])\n    else:\n        print(\"EXCLUDING STOP WORD FROM ANALYSIS...\")\n        word_coverage = 100*word_df[word_df[\"is_stop_word\"] == 0][column_word_presence].value_counts(normalize = True)[1]\n        text_coverage = 100*word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[1] \/ (word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[0] + \n                         word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[1])\n        \n    if exc_stop:\n        print(\"Total words in {} were {} and {:.2f}% words were found in the word_vectors.\".format(column_words,\n                                                                                               len(word_df[word_df[\"is_stop_word\"] == 0]),\n                                                                                               word_coverage))\n    else:\n        print(\"Total words in {} were {} and {:.2f}% words were found in the word_vectors.\".format(column_words,\n                                                                                               len(word_df),\n                                                                                               word_coverage))\n        \n    print(\"-\" * 80)\n    print(\"From text coverage, {:.2f}% text is coverage in word_vectors.\".format(text_coverage))\n    print(\"-\" * 80)\n    return word_df\n    \n\n","1d8c43b0":"word_df = get_coverage( \"raw_words\", \"raw_words_counts\", \"raw_in_vectors\", word_df)","3a43a22b":"def preprocess(x):\n    x = x.replace(\"n't\", \"nt\")\n    \n    x = str(x)\n    if LOWER_CASE:\n        x = x.lower()\n        \n    if len(x.strip()) == 1:\n        return x #special case if a punctuation was the only alphabet in the token.\n    \n    for punct in \"\/-'&\":\n        x = x.replace(punct, '')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n        \n    x = re.sub('[0-9]{1,}', '#', x) #replace all numbers by #\n    if len(x.strip()) < 1:\n        x = '.' #if it was all punctuations like ------ or ..... or .;?!!. Then we return only a period to keep token consistent performance.\n    return x\n\nword_df[\"clean_words_01\"] = word_df[\"raw_words\"].progress_apply(lambda x: preprocess(x))","bbeca950":"word_df.head()","634ff457":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_01\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_01\", \"clean_words_01_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_01\"], how = 'left')","d859173d":"word_df = get_coverage( \"clean_words_01\", \"clean_words_01_counts\", \"clean_01_in_vectors\", word_df)","c16cf448":"# Reference: \n# https:\/\/itqna.net\/questions\/9818\/how-remove-accented-expressions-regular-expressions-python\nimport re\n\n# char codes: https:\/\/unicode-table.com\/en\/#basic-latin\naccent_map = {\n    u'\\u00c0': u'A',\n    u'\\u00c1': u'A',\n    u'\\u00c2': u'A',\n    u'\\u00c3': u'A',\n    u'\\u00c4': u'A',\n    u'\\u00c5': u'A',\n    u'\\u00c6': u'A',\n    u'\\u00c7': u'C',\n    u'\\u00c8': u'E',\n    u'\\u00c9': u'E',\n    u'\\u00ca': u'E',\n    u'\\u00cb': u'E',\n    u'\\u00cc': u'I',\n    u'\\u00cd': u'I',\n    u'\\u00ce': u'I',\n    u'\\u00cf': u'I',\n    u'\\u00d0': u'D',\n    u'\\u00d1': u'N',\n    u'\\u00d2': u'O',\n    u'\\u00d3': u'O',\n    u'\\u00d4': u'O',\n    u'\\u00d5': u'O',\n    u'\\u00d6': u'O',\n    u'\\u00d7': u'x',\n    u'\\u00d8': u'0',\n    u'\\u00d9': u'U',\n    u'\\u00da': u'U',\n    u'\\u00db': u'U',\n    u'\\u00dc': u'U',\n    u'\\u00dd': u'Y',\n    u'\\u00df': u'B',\n    u'\\u00e0': u'a',\n    u'\\u00e1': u'a',\n    u'\\u00e2': u'a',\n    u'\\u00e3': u'a',\n    u'\\u00e4': u'a',\n    u'\\u00e5': u'a',\n    u'\\u00e6': u'a',\n    u'\\u00e7': u'c',\n    u'\\u00e8': u'e',\n    u'\\u00e9': u'e',\n    u'\\u00ea': u'e',\n    u'\\u00eb': u'e',\n    u'\\u00ec': u'i',\n    u'\\u00ed': u'i',\n    u'\\u00ee': u'i',\n    u'\\u00ef': u'i',\n    u'\\u00f1': u'n',\n    u'\\u00f2': u'o',\n    u'\\u00f3': u'o',\n    u'\\u00f4': u'o',\n    u'\\u00f5': u'o',\n    u'\\u00f6': u'o',\n    u'\\u00f8': u'0',\n    u'\\u00f9': u'u',\n    u'\\u00fa': u'u',\n    u'\\u00fb': u'u',\n    u'\\u00fc': u'u'\n}\n\ndef accent_remove (m):\n    return accent_map[m.group(0)]\n\nstring_velha = \"Ol\u00e1 voc\u00ea est\u00e1 ????   \"\nstring_nova = re.sub(u'([\\u00C0-\\u00FC])', accent_remove, string_velha.encode().decode('utf-8'))\nstring_nova","63e27ae4":"word_df[\"clean_words_02\"] = word_df[\"clean_words_01\"].apply( lambda x: re.sub(u'([\\u00C0-\\u00FC])', \n                                                  accent_remove, \n                                                  x.encode().decode('utf-8'))\n                               )","924b5ebe":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_02\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_02\", \"clean_words_02_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_02\"], how = 'left')","b04c83a9":"word_df = get_coverage( \"clean_words_02\", \"clean_words_02_counts\", \"clean_02_in_vectors\", word_df)","c3e77c01":"word_df[ word_df[\"clean_words_01_counts\"] != word_df[\"clean_words_02_counts\"]].head(10)","ee487003":"word_df[\"clean_words_03\"] = word_df[\"clean_words_02\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])","d5270021":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_03\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_03\", \"clean_words_03_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_03\"], how = 'left')","cba739f1":"word_df = get_coverage( \"clean_words_03\", \"clean_words_03_counts\", \"clean_03_in_vectors\", word_df)","52815a58":"word_df[word_df[\"clean_03_in_vectors\"] == 0].sort_values(by = [\"clean_words_03_counts\"], ascending = False).head(20)","0e42fc7e":"aam_misspell_dict.update( {\"shouldnt\" : \"shant\" })","0dbad75c":"word_df[\"clean_words_04\"] = word_df[\"clean_words_03\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])\ntemp = pd.DataFrame(word_df.groupby( [\"clean_words_04\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_04\", \"clean_words_04_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_04\"], how = 'left')","0358e13d":"word_df = get_coverage( \"clean_words_04\", \"clean_words_04_counts\", \"clean_04_in_vectors\", word_df)","5eac923e":"word_df[word_df[\"clean_04_in_vectors\"] == 0].sort_values(by = [\"clean_words_04_counts\"], ascending = False)[:10]","e17d7147":"aam_misspell_dict.update( {\"teacherdesigned\" : \"designed\",\n                      \"studentname\" : \"myself\",\n                      \"studentdesigned\" : \"designed\",\n                      \"teachername\" : \"teacher\",\n                      \"winnertakeall\" : \"winner-take-all\"})","991d38a2":"word_df[\"clean_words_05\"] = word_df[\"clean_words_04\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])\ntemp = pd.DataFrame(word_df.groupby( [\"clean_words_05\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_05\", \"clean_words_05_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_05\"], how = 'left')","218874c0":"word_df = get_coverage( \"clean_words_05\", \"clean_words_05_counts\", \"clean_05_in_vectors\", word_df)","c7422886":"print(\"Exporting the created dictionary now. \")\nword_df.to_csv(\"cleaned_word_dict.csv\")","39d5a4e2":"---\n# Analysis #3\n\n* **Amazing**, we improved the text coverage from **80**% to **99.76**%\n* Lets try to do more\n---","4bc75a1b":"# Exclude Stopwords\n\n* This means that to report coverage of text we will not count stop words in the analysis.\n* It also helps because in the keyed vectors from gensim stop words wont be included","1ffe999a":"# Introduction\n\nThe essays for this challenge have a lot of spelling mistakes and punctuations which can cause out of vocabulary errors for tokenizers.\n\nIn this notebook, we will see that initially we have around 80% coverage of word tokens compared to the vocabulary of glove embeddings and then we will improve it to beyond 99%.\n\nWe wont be using any lemmatization or stemming to achieve this feat. You are welcome to stem as it may help with some words. Lemmatize probably wont help as it depends on a valid word in the first place.\n\n---\n## TLDR;\n\n* Get the exported csv file from notebook output. For the words in raw_words, replace them with clean_words_05 from the text you encounter from your training and testing dataframes. This improves the word embeddings as common mistakes have been clarified and count-wise improvement lifts it from 80% current to 99% new.\n\n---\n## Disclaimer\n\nAll ideas are taken from the below excellent reference and then adopted for our train\/test sets. Amazing work on simple ideas to improve the vocabulary.\n\n\nfrom @christofhenkel\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings","fdb24ae8":"Have a nice day fellows. Happy kaggling!","a98f53d4":"# Total Vocabulary Words\n\nLength of the vocabulary ~ 101K words right now","e1c88506":"---\n---\n\n# Match Vocab\n\nLet us now see how many words have valid entries in the word_vectors\n\n* apply_coverage only checks if a word exists in glove vectors obtained earlier.","9a0b46c6":"---\n---\n\n# Misspellings\n\n* I created a dictionary in the beginning of notebook **aam_misspell_dict** to include common errors that I can see. You can improve upon it\n* There are definitely a lot of **misspellings** at work\n* There are also anonymous names playing. (like **genericschool**, **genericname** etc..)\n* Seeing that mainly there are a **limited number of topics**, we can perform some basic spelling corrections on the essay topics\n","8b643c2c":"# Viola\n\n* Another improvement from **99.76%** to **99.84%**\n* Can we do better ???","56dd25ed":"* get_coverage checks the amount of vocabulary and text coverage when comparing to glove vectors\n* It creates a new column named column_word_presence to indicate if the target word exists or not.\n* It also displays the coverage statistics and returns a dataframe.\n\n    '''\n        column_words : the column containing the words for which we check coverage\n        column_word_counts : the column containing pre-computed word counts for the words in question\n        column_word_presence : Just an output column name where we will output if word exists in word_vectors\n        exc_stop : Should we exclude stopwrods from coverage analysis or not\n    '''","cbe6ca81":"---\n---\n# Conclusion\n\n* After application of several transformations, we have created a dictionary which improves the text-coverage from **80%** to **99.92%**\n* Hopefully this can improve the prediction performance from different models.\n* Do post critique\/feedback\n\n* You can use the exported dataframe to create \/ use as dictionary for your tokens.","b924b883":"## Build Initial Vocabulary","688b927c":"---\n---\n# Warning\n\nMake sure that if you use anything like below, then dont train on character positions. Because after spelling corrections the character positions will change. \nI have tried to keep the token counts same as before so training on token positions can potentially work.","52956929":"# Analysis 1:\n1) After all lower case words, we see there are 170 stop words detected from train set","4338a2f2":"# Pandas for all heavylifting\n\n* We will use pandas so that all results achieved can be in different columns and you can download the data in the end and use any columns you would like based on your own tokenizer preferences.\n","6175a304":"---\n---\n\n# Analysis 6 - Risky Choices Ahead\n\n* Now we can see that the vocabulary with most trouble is sort of problem-specific and not general enough\n* We can replace the **studentdesigned** or **teacherdesigned** as designed.. This would probably change the contextual meaning but can improve tokenize performance as well.\n* I will replace **studentname** as **myself**\n* I will replace **teachername** as **teacher**\n* I will replace **winnertakeall** as **winner-take-all**","20de0229":"---\n---\n\n# Wow\n\n* Another 0.01% improvement\n* Lets see the top words still giving us issues","aee5ae79":"# Vocabulary Flow (Lower Case)\n\n1. Read all text from train files and combine together.","533eaaa7":"---\n---\n# Analysis 2\n\n* So we have missing vocabulary for around **76%** of the total words\n* In terms of usage frequency, we have coverage of 80% of the words if repetitions are taken into account\n\n---\n---\n\n# Objective\n\n* Our objective is to improve the **text coverage** so that the tokenizers can improve their performances\n\n\n**Remember** we DONT WANT TO INCREASE or DECREASE the number of tokens. As this will make training and predictionstring too difficult and cause problems on the submission dataset","de9ad5cb":"# Analysis #5\n\n* **Shouldnt** is giving us some problems. Well it shouln't (pun-intended)\n* The vocabulary contains should and not separately but I dont want to increase number of tokens, so we will replace all shouldnt with **shant** which has similar meaning","582b5c51":"# Accented alphabet replacements\n\n* \u00e1 to a and so on....","49e095c8":"# Analysis 4\n\n* Seems that accented character changes did not improve score too much. Lets see some examples where these replacements were made"}}