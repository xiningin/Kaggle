{"cell_type":{"d3e29776":"code","fe1391eb":"code","ff47f58a":"code","fb9fdf3d":"code","8eec3b5d":"code","a199c4a1":"code","c5ff2fe3":"code","1d0c0577":"code","4b1f33be":"code","0ff1d8b8":"code","8b5a40e5":"code","7975f8b2":"code","e2242d2d":"code","4f55c1d8":"code","7404d830":"code","95f029aa":"code","d8093676":"code","0e1f9a4a":"code","7c5b70d3":"code","89f6d162":"code","2259d168":"code","0e989182":"code","fce32d12":"code","349d052b":"code","4a08d289":"code","3ca4db63":"code","78443573":"code","19b3da5b":"code","f3e47382":"code","4a31e0c9":"code","c8c1a8c6":"code","e47cb32d":"code","bdb02e4f":"code","30d97fcd":"markdown","6c336b35":"markdown","988c4b2b":"markdown","d91d670f":"markdown","ef6ce12f":"markdown","1136fee7":"markdown"},"source":{"d3e29776":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA","fe1391eb":"from sklearn.datasets import load_breast_cancer","ff47f58a":"cancer = load_breast_cancer()","fb9fdf3d":"cancer.keys()","8eec3b5d":"print(cancer['DESCR'])","a199c4a1":"df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])","c5ff2fe3":"df","1d0c0577":"df.describe()","4b1f33be":"from sklearn.preprocessing import StandardScaler","0ff1d8b8":"scaler = StandardScaler()","8b5a40e5":"scaler.fit(df)","7975f8b2":"df_scaled = scaler.transform(df)","e2242d2d":"covariance_matrix = PCA(n_components=30)","4f55c1d8":"covariance_matrix.fit(df_scaled)","7404d830":"variance = covariance_matrix.explained_variance_ratio_\n# simply printing variance would give decimal values, hence we use np.round\nvar = np.round(variance, decimals=3)\n# printing var would give individual scores, hence we use np.cumsum\ncumulative_var = np.cumsum((var)*100)","95f029aa":"cumulative_var","d8093676":"plt.plot(cumulative_var)\nplt.title('SCREE PLOT (selecting optimal features)')\nplt.xlabel('No of features')\nplt.ylabel('% of variance explained')\nplt.show()","0e1f9a4a":"pca = PCA(n_components=5)","7c5b70d3":"pca.fit(df_scaled)","89f6d162":"df_pca = pca.transform(df_scaled)","2259d168":"df_scaled.shape","0e989182":"df_pca.shape","fce32d12":"plt.scatter(df_pca[:,0], df_pca[:,1], c=cancer['target'], cmap='rainbow')\nplt.show()","349d052b":"plt.scatter(df_pca[:,1], df_pca[:,2], c=cancer['target'], cmap='plasma')\nplt.show()","4a08d289":"plt.scatter(df_pca[:,2], df_pca[:,3], c=cancer['target'], cmap='cool')  # inferno isn't a good fit :(\nplt.show()","3ca4db63":"plt.scatter(df_pca[:,3], df_pca[:,4], c=cancer['target'], cmap='viridis')\nplt.show()","78443573":"from mpl_toolkits.mplot3d import Axes3D","19b3da5b":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,0], df_pca[:,1], df_pca[:,2], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nax.set_zlabel('Third Principal Component')\nax.view_init(30, 120)\n","f3e47382":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,0], df_pca[:,1], df_pca[:,3], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nax.set_zlabel('fourth Principal Component')\nax.view_init(30, 120)\n","4a31e0c9":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,0], df_pca[:,1], df_pca[:,4], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nax.set_zlabel('fifth Principal Component')\nax.view_init(30, 120)\n","c8c1a8c6":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,1], df_pca[:,2], df_pca[:,3], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('second Principal Component')\nax.set_ylabel('third Principal Component')\nax.set_zlabel('fourth Principal Component')\nax.view_init(30, 120)\n","e47cb32d":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,1], df_pca[:,2], df_pca[:,4], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('second Principal Component')\nax.set_ylabel('third Principal Component')\nax.set_zlabel('fifth Principal Component')\nax.view_init(30, 120)\n","bdb02e4f":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df_pca[:,2], df_pca[:,3], df_pca[:,4], c=cancer['target'],s=60)\n#ax.legend(['Malign'])\nax.set_xlabel('third Principal Component')\nax.set_ylabel('fourth Principal Component')\nax.set_zlabel('fifth Principal Component')\nax.view_init(30, 120)\n","30d97fcd":"## Plot the graph","6c336b35":"### it is difficult to visualize soo many features, i.e., 30, hence we use PCA to reduce number of columns so it can be represented by a scatter plot. to perform further, we need to standardize the data so that all the data has single unit variance","988c4b2b":"## acc to the graph, the saturation point reaches when the no. of features are 5 and after that, the explained variance becomes saturated. hence, we select 5 as the number of optimal features. Running PCA with 5 features","d91d670f":"## Standardizing","ef6ce12f":"## Plotting all 6 graphs for 5 features, in 3D plot","1136fee7":"## Checking optimal number of clusters"}}